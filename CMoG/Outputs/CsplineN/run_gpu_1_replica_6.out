2023-09-26 13:23:47.120269: Importing os...
2023-09-26 13:23:47.120339: Importing sys...
2023-09-26 13:23:47.120355: Importing and initializing argparse...
Visible devices: [1]
2023-09-26 13:23:47.138606: Importing timer from timeit...
2023-09-26 13:23:47.139202: Setting env variables for tf import (only device [1] will be available)...
2023-09-26 13:23:47.139248: Importing numpy...
2023-09-26 13:23:47.295133: Importing pandas...
2023-09-26 13:23:47.491551: Importing shutil...
2023-09-26 13:23:47.491574: Importing subprocess...
2023-09-26 13:23:47.491581: Importing tensorflow...
Tensorflow version: 2.12.0
2023-09-26 13:23:48.936576: Importing tensorflow_probability...
Tensorflow probability version: 0.20.1
2023-09-26 13:23:49.206885: Importing textwrap...
2023-09-26 13:23:49.206906: Importing timeit...
2023-09-26 13:23:49.206912: Importing traceback...
2023-09-26 13:23:49.206917: Importing typing...
2023-09-26 13:23:49.206924: Setting tf configs...
2023-09-26 13:23:49.293360: Importing custom module...
Successfully loaded GPU model: NVIDIA A40
2023-09-26 13:23:49.969755: All modues imported successfully.
Directory ../../results/CsplineN_new/ already exists.
Directory ../../results/CsplineN_new/run_1/ already exists.
Skipping it.
===========
Run 1/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_2/ already exists.
Skipping it.
===========
Run 2/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_3/ already exists.
Skipping it.
===========
Run 3/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_4/ already exists.
Skipping it.
===========
Run 4/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_5/ already exists.
Skipping it.
===========
Run 5/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_6/ already exists.
Skipping it.
===========
Run 6/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_7/ already exists.
Skipping it.
===========
Run 7/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_8/ already exists.
Skipping it.
===========
Run 8/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_9/ already exists.
Skipping it.
===========
Run 9/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_10/ already exists.
Skipping it.
===========
Run 10/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_11/ already exists.
Skipping it.
===========
Run 11/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_12/ already exists.
Skipping it.
===========
Run 12/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_13/ already exists.
Skipping it.
===========
Run 13/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_14/ already exists.
Skipping it.
===========
Run 14/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_15/ already exists.
Skipping it.
===========
Run 15/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_16/ already exists.
Skipping it.
===========
Run 16/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_17/ already exists.
Skipping it.
===========
Run 17/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_18/ already exists.
Skipping it.
===========
Run 18/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_19/ already exists.
Skipping it.
===========
Run 19/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_20/ already exists.
Skipping it.
===========
Run 20/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_21/ already exists.
Skipping it.
===========
Run 21/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_22/ already exists.
Skipping it.
===========
Run 22/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_23/ already exists.
Skipping it.
===========
Run 23/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_24/ already exists.
Skipping it.
===========
Run 24/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_25/ already exists.
Skipping it.
===========
Run 25/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_26/ already exists.
Skipping it.
===========
Run 26/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_27/ already exists.
Skipping it.
===========
Run 27/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_28/ already exists.
Skipping it.
===========
Run 28/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_29/ already exists.
Skipping it.
===========
Run 29/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_30/ already exists.
Skipping it.
===========
Run 30/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_31/ already exists.
Skipping it.
===========
Run 31/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_32/ already exists.
Skipping it.
===========
Run 32/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_33/ already exists.
Skipping it.
===========
Run 33/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_34/ already exists.
Skipping it.
===========
Run 34/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_35/ already exists.
Skipping it.
===========
Run 35/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_36/ already exists.
Skipping it.
===========
Run 36/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_37/ already exists.
Skipping it.
===========
Run 37/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_38/ already exists.
Skipping it.
===========
Run 38/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_39/ already exists.
Skipping it.
===========
Run 39/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_40/ already exists.
Skipping it.
===========
Run 40/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_41/ already exists.
Skipping it.
===========
Run 41/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_42/ already exists.
Skipping it.
===========
Run 42/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_43/ already exists.
Skipping it.
===========
Run 43/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_44/ already exists.
Skipping it.
===========
Run 44/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_45/ already exists.
Skipping it.
===========
Run 45/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_46/ already exists.
Skipping it.
===========
Run 46/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_47/ already exists.
Skipping it.
===========
Run 47/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_48/ already exists.
Skipping it.
===========
Run 48/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_49/ already exists.
Skipping it.
===========
Run 49/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_50/ already exists.
Skipping it.
===========
Run 50/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_51/ already exists.
Skipping it.
===========
Run 51/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_52/ already exists.
Skipping it.
===========
Run 52/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_53/ already exists.
Skipping it.
===========
Run 53/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_54/ already exists.
Skipping it.
===========
Run 54/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_55/ already exists.
Skipping it.
===========
Run 55/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_56/ already exists.
Skipping it.
===========
Run 56/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_57/ already exists.
Skipping it.
===========
Run 57/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_58/ already exists.
Skipping it.
===========
Run 58/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_59/ already exists.
Skipping it.
===========
Run 59/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_60/ already exists.
Skipping it.
===========
Run 60/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_61/ already exists.
Skipping it.
===========
Run 61/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_62/ already exists.
Skipping it.
===========
Run 62/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_63/ already exists.
Skipping it.
===========
Run 63/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_64/ already exists.
Skipping it.
===========
Run 64/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_65/ already exists.
Skipping it.
===========
Run 65/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_66/ already exists.
Skipping it.
===========
Run 66/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_67/ already exists.
Skipping it.
===========
Run 67/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_68/ already exists.
Skipping it.
===========
Run 68/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_69/ already exists.
Skipping it.
===========
Run 69/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_70/ already exists.
Skipping it.
===========
Run 70/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_71/ already exists.
Skipping it.
===========
Run 71/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_72/ already exists.
Skipping it.
===========
Run 72/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_73/ already exists.
Skipping it.
===========
Run 73/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_74/ already exists.
Skipping it.
===========
Run 74/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_75/ already exists.
Skipping it.
===========
Run 75/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_76/ already exists.
Skipping it.
===========
Run 76/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_77/ already exists.
Skipping it.
===========
Run 77/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_78/ already exists.
Skipping it.
===========
Run 78/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_79/ already exists.
Skipping it.
===========
Run 79/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_80/ already exists.
Skipping it.
===========
Run 80/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_81/ already exists.
Skipping it.
===========
Run 81/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_82/ already exists.
Skipping it.
===========
Run 82/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_83/ already exists.
Skipping it.
===========
Run 83/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_84/ already exists.
Skipping it.
===========
Run 84/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_85/ already exists.
Skipping it.
===========
Run 85/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_86/ already exists.
Skipping it.
===========
Run 86/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_87/ already exists.
Skipping it.
===========
Run 87/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_88/ already exists.
Skipping it.
===========
Run 88/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_89/ already exists.
Skipping it.
===========
Run 89/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_90/ already exists.
Skipping it.
===========
Run 90/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_91/ already exists.
Skipping it.
===========
Run 91/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_92/ already exists.
Skipping it.
===========
Run 92/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_93/ already exists.
Skipping it.
===========
Run 93/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_94/ already exists.
Skipping it.
===========
Run 94/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_95/ already exists.
Skipping it.
===========
Run 95/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_96/ already exists.
Skipping it.
===========
Run 96/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_97/ already exists.
Skipping it.
===========
Run 97/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_98/ already exists.
Skipping it.
===========
Run 98/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_99/ already exists.
Skipping it.
===========
Run 99/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_100/ already exists.
Skipping it.
===========
Run 100/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_101/ already exists.
Skipping it.
===========
Run 101/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_102/ already exists.
Skipping it.
===========
Run 102/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_103/ already exists.
Skipping it.
===========
Run 103/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_104/ already exists.
Skipping it.
===========
Run 104/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_105/ already exists.
Skipping it.
===========
Run 105/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_106/ already exists.
Skipping it.
===========
Run 106/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_107/ already exists.
Skipping it.
===========
Run 107/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_108/ already exists.
Skipping it.
===========
Run 108/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_109/ already exists.
Skipping it.
===========
Run 109/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_110/ already exists.
Skipping it.
===========
Run 110/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_111/ already exists.
Skipping it.
===========
Run 111/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_112/ already exists.
Skipping it.
===========
Run 112/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_113/ already exists.
Skipping it.
===========
Run 113/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_114/ already exists.
Skipping it.
===========
Run 114/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_115/ already exists.
Skipping it.
===========
Run 115/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_116/ already exists.
Skipping it.
===========
Run 116/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_117/ already exists.
Skipping it.
===========
Run 117/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_118/ already exists.
Skipping it.
===========
Run 118/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_119/ already exists.
Skipping it.
===========
Run 119/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_120/ already exists.
Skipping it.
===========
Run 120/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_121/ already exists.
Skipping it.
===========
Run 121/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_122/ already exists.
Skipping it.
===========
Run 122/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_123/ already exists.
Skipping it.
===========
Run 123/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_124/ already exists.
Skipping it.
===========
Run 124/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_125/ already exists.
Skipping it.
===========
Run 125/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_126/ already exists.
Skipping it.
===========
Run 126/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_127/ already exists.
Skipping it.
===========
Run 127/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_128/ already exists.
Skipping it.
===========
Run 128/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_129/ already exists.
Skipping it.
===========
Run 129/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_130/ already exists.
Skipping it.
===========
Run 130/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_131/ already exists.
Skipping it.
===========
Run 131/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_132/ already exists.
Skipping it.
===========
Run 132/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_133/ already exists.
Skipping it.
===========
Run 133/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_134/ already exists.
Skipping it.
===========
Run 134/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_135/ already exists.
Skipping it.
===========
Run 135/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_136/ already exists.
Skipping it.
===========
Run 136/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_137/ already exists.
Skipping it.
===========
Run 137/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_138/ already exists.
Skipping it.
===========
Run 138/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_139/ already exists.
Skipping it.
===========
Run 139/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_140/ already exists.
Skipping it.
===========
Run 140/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_141/ already exists.
Skipping it.
===========
Run 141/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_142/ already exists.
Skipping it.
===========
Run 142/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_143/ already exists.
Skipping it.
===========
Run 143/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_144/ already exists.
Skipping it.
===========
Run 144/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_145/ already exists.
Skipping it.
===========
Run 145/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_146/ already exists.
Skipping it.
===========
Run 146/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_147/ already exists.
Skipping it.
===========
Run 147/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_148/ already exists.
Skipping it.
===========
Run 148/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_149/ already exists.
Skipping it.
===========
Run 149/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_150/ already exists.
Skipping it.
===========
Run 150/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_151/ already exists.
Skipping it.
===========
Run 151/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_152/ already exists.
Skipping it.
===========
Run 152/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_153/ already exists.
Skipping it.
===========
Run 153/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_154/ already exists.
Skipping it.
===========
Run 154/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_155/ already exists.
Skipping it.
===========
Run 155/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_156/ already exists.
Skipping it.
===========
Run 156/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_157/ already exists.
Skipping it.
===========
Run 157/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_158/ already exists.
Skipping it.
===========
Run 158/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_159/ already exists.
Skipping it.
===========
Run 159/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_160/ already exists.
Skipping it.
===========
Run 160/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_161/ already exists.
Skipping it.
===========
Run 161/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_162/ already exists.
Skipping it.
===========
Run 162/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_163/ already exists.
Skipping it.
===========
Run 163/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_164/ already exists.
Skipping it.
===========
Run 164/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_165/ already exists.
Skipping it.
===========
Run 165/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_166/ already exists.
Skipping it.
===========
Run 166/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_167/ already exists.
Skipping it.
===========
Run 167/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_168/ already exists.
Skipping it.
===========
Run 168/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_169/ already exists.
Skipping it.
===========
Run 169/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_170/ already exists.
Skipping it.
===========
Run 170/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_171/ already exists.
Skipping it.
===========
Run 171/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_172/ already exists.
Skipping it.
===========
Run 172/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_173/ already exists.
Skipping it.
===========
Run 173/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_174/ already exists.
Skipping it.
===========
Run 174/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_175/ already exists.
Skipping it.
===========
Run 175/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_176/ already exists.
Skipping it.
===========
Run 176/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_177/ already exists.
Skipping it.
===========
Run 177/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_178/ already exists.
Skipping it.
===========
Run 178/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_179/ already exists.
Skipping it.
===========
Run 179/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_180/ already exists.
Skipping it.
===========
Run 180/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_181/ already exists.
Skipping it.
===========
Run 181/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_182/ already exists.
Skipping it.
===========
Run 182/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_183/ already exists.
Skipping it.
===========
Run 183/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_184/ already exists.
Skipping it.
===========
Run 184/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_185/ already exists.
Skipping it.
===========
Run 185/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_186/ already exists.
Skipping it.
===========
Run 186/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_187/ already exists.
Skipping it.
===========
Run 187/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_188/ already exists.
Skipping it.
===========
Run 188/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_189/ already exists.
Skipping it.
===========
Run 189/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_190/ already exists.
Skipping it.
===========
Run 190/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_191/ already exists.
Skipping it.
===========
Run 191/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_192/ already exists.
Skipping it.
===========
Run 192/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_193/ already exists.
Skipping it.
===========
Run 193/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_194/ already exists.
Skipping it.
===========
Run 194/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_195/ already exists.
Skipping it.
===========
Run 195/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_196/ already exists.
Skipping it.
===========
Run 196/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_197/ already exists.
Skipping it.
===========
Run 197/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_198/ already exists.
Skipping it.
===========
Run 198/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_199/ already exists.
Skipping it.
===========
Run 199/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_200/ already exists.
Skipping it.
===========
Run 200/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_201/ already exists.
Skipping it.
===========
Run 201/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_202/ already exists.
Skipping it.
===========
Run 202/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_203/ already exists.
Skipping it.
===========
Run 203/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_204/ already exists.
Skipping it.
===========
Run 204/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_205/ already exists.
Skipping it.
===========
Run 205/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_206/ already exists.
Skipping it.
===========
Run 206/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_207/ already exists.
Skipping it.
===========
Run 207/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_208/ already exists.
Skipping it.
===========
Run 208/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_209/ already exists.
Skipping it.
===========
Run 209/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_210/ already exists.
Skipping it.
===========
Run 210/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_211/ already exists.
Skipping it.
===========
Run 211/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_212/ already exists.
Skipping it.
===========
Run 212/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_213/ already exists.
Skipping it.
===========
Run 213/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_214/ already exists.
Skipping it.
===========
Run 214/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_215/ already exists.
Skipping it.
===========
Run 215/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_216/ already exists.
Skipping it.
===========
Run 216/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_217/ already exists.
Skipping it.
===========
Run 217/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_218/ already exists.
Skipping it.
===========
Run 218/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_219/ already exists.
Skipping it.
===========
Run 219/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_220/ already exists.
Skipping it.
===========
Run 220/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_221/ already exists.
Skipping it.
===========
Run 221/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_222/ already exists.
Skipping it.
===========
Run 222/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_223/ already exists.
Skipping it.
===========
Run 223/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_224/ already exists.
Skipping it.
===========
Run 224/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_225/ already exists.
Skipping it.
===========
Run 225/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_226/ already exists.
Skipping it.
===========
Run 226/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_227/ already exists.
Skipping it.
===========
Run 227/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_228/ already exists.
Skipping it.
===========
Run 228/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_229/ already exists.
Skipping it.
===========
Run 229/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_230/ already exists.
Skipping it.
===========
Run 230/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_231/ already exists.
Skipping it.
===========
Run 231/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_232/ already exists.
Skipping it.
===========
Run 232/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_233/ already exists.
Skipping it.
===========
Run 233/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_234/ already exists.
Skipping it.
===========
Run 234/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_235/ already exists.
Skipping it.
===========
Run 235/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_236/ already exists.
Skipping it.
===========
Run 236/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_237/ already exists.
Skipping it.
===========
Run 237/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_238/ already exists.
Skipping it.
===========
Run 238/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_239/ already exists.
Skipping it.
===========
Run 239/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_240/ already exists.
Skipping it.
===========
Run 240/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_241/ already exists.
Skipping it.
===========
Run 241/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_242/ already exists.
Skipping it.
===========
Run 242/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_243/ already exists.
Skipping it.
===========
Run 243/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_244/ already exists.
Skipping it.
===========
Run 244/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_245/ already exists.
Skipping it.
===========
Run 245/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_246/ already exists.
Skipping it.
===========
Run 246/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_247/ already exists.
Skipping it.
===========
Run 247/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_248/ already exists.
Skipping it.
===========
Run 248/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_249/ already exists.
Skipping it.
===========
Run 249/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_250/ already exists.
Skipping it.
===========
Run 250/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_251/ already exists.
Skipping it.
===========
Run 251/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_252/ already exists.
Skipping it.
===========
Run 252/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_253/ already exists.
Skipping it.
===========
Run 253/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_254/ already exists.
Skipping it.
===========
Run 254/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_255/ already exists.
Skipping it.
===========
Run 255/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_256/ already exists.
Skipping it.
===========
Run 256/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_257/ already exists.
Skipping it.
===========
Run 257/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_258/ already exists.
Skipping it.
===========
Run 258/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_259/ already exists.
Skipping it.
===========
Run 259/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_260/ already exists.
Skipping it.
===========
Run 260/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_261/ already exists.
Skipping it.
===========
Run 261/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_262/ already exists.
Skipping it.
===========
Run 262/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_263/ already exists.
Skipping it.
===========
Run 263/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_264/ already exists.
Skipping it.
===========
Run 264/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_265/ already exists.
Skipping it.
===========
Run 265/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_266/ already exists.
Skipping it.
===========
Run 266/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_267/ already exists.
Skipping it.
===========
Run 267/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_268/ already exists.
Skipping it.
===========
Run 268/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_269/ already exists.
Skipping it.
===========
Run 269/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_270/ already exists.
Skipping it.
===========
Run 270/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_271/ already exists.
Skipping it.
===========
Run 271/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_272/ already exists.
Skipping it.
===========
Run 272/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_273/ already exists.
Skipping it.
===========
Run 273/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_274/ already exists.
Skipping it.
===========
Run 274/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_275/ already exists.
Skipping it.
===========
Run 275/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_276/ already exists.
Skipping it.
===========
Run 276/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_277/ already exists.
Skipping it.
===========
Run 277/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_278/ already exists.
Skipping it.
===========
Run 278/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_279/ already exists.
Skipping it.
===========
Run 279/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_280/ already exists.
Skipping it.
===========
Run 280/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_281/ already exists.
Skipping it.
===========
Run 281/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_282/ already exists.
Skipping it.
===========
Run 282/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_283/ already exists.
Skipping it.
===========
Run 283/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_284/ already exists.
Skipping it.
===========
Run 284/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_285/ already exists.
Skipping it.
===========
Run 285/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_286/ already exists.
Skipping it.
===========
Run 286/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_287/ already exists.
Skipping it.
===========
Run 287/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_288/ already exists.
Skipping it.
===========
Run 288/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_289/ already exists.
Skipping it.
===========
Run 289/720 already exists. Skipping it.
===========

===========
Generating train data for run 290.
===========
Train data generated in 0.10 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_290/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 721}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_290/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.3570018 ,  5.6703625 ,  0.48762453, ...,  0.76730525,
         6.4537516 ,  1.4289263 ],
       [ 4.8933635 ,  4.954244  , -0.7742728 , ...,  1.033045  ,
         7.5179    ,  1.3446053 ],
       [ 2.6192126 ,  3.6659784 ,  9.3581505 , ...,  6.4096394 ,
         2.8983781 ,  1.8163822 ],
       ...,
       [ 4.4286733 ,  5.582822  , -0.08033401, ...,  1.0755457 ,
         6.482341  ,  1.3259909 ],
       [ 5.2667904 ,  5.78094   ,  0.52473783, ...,  1.3584392 ,
         6.6371064 ,  1.4527634 ],
       [ 1.3711529 ,  3.985786  ,  8.434673  , ...,  6.650886  ,
         2.8764064 ,  2.071397  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_290/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_290
self.data_kwargs: {'seed': 721}
self.x_data: [[ 5.4479074  7.1622143  6.6614265 ...  3.6775346  2.6042771  7.6607866]
 [ 1.558796   3.7643342  7.765504  ...  7.3985186  2.2847533  2.1737251]
 [ 2.023865   3.5498223  9.736043  ...  7.378399   2.9893079  1.7681804]
 ...
 [ 3.157689   3.8140697  9.128405  ...  7.3280826  3.3431854  1.6145309]
 [ 4.9035525  5.63367    1.3061054 ...  1.7336241  7.263643   1.3624198]
 [ 4.873677   6.0271535 -0.4844559 ...  1.4305246  6.6029577  1.4994687]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_5"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_6 (InputLayer)        [(None, 32)]              0         
                                                                 
 log_prob_layer (LogProbLaye  (None,)                  1152560   
 r)                                                              
                                                                 
=================================================================
Total params: 1,152,560
Trainable params: 1,152,560
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer'")
self.model: <keras.engine.functional.Functional object at 0x7fb534093b20>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fb53447d120>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fb53447d120>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fb5369779d0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fb534548e20>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_290/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fb53454afe0>, <keras.callbacks.ModelCheckpoint object at 0x7fb5345498d0>, <keras.callbacks.EarlyStopping object at 0x7fb534549ab0>, <keras.callbacks.ReduceLROnPlateau object at 0x7fb53454be80>, <keras.callbacks.TerminateOnNaN object at 0x7fb53454ac50>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.3570018 ,  5.6703625 ,  0.48762453, ...,  0.76730525,
         6.4537516 ,  1.4289263 ],
       [ 4.8933635 ,  4.954244  , -0.7742728 , ...,  1.033045  ,
         7.5179    ,  1.3446053 ],
       [ 2.6192126 ,  3.6659784 ,  9.3581505 , ...,  6.4096394 ,
         2.8983781 ,  1.8163822 ],
       ...,
       [ 4.4286733 ,  5.582822  , -0.08033401, ...,  1.0755457 ,
         6.482341  ,  1.3259909 ],
       [ 5.2667904 ,  5.78094   ,  0.52473783, ...,  1.3584392 ,
         6.6371064 ,  1.4527634 ],
       [ 1.3711529 ,  3.985786  ,  8.434673  , ...,  6.650886  ,
         2.8764064 ,  2.071397  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_290/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 290/720 with hyperparameters:
timestamp = 2023-09-26 13:23:53.351716
ndims = 32
seed_train = 721
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 1152560
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 5.4479074  7.1622143  6.6614265  5.4466543  4.679343   6.5534453
  4.5869303  8.297877   9.431402   3.191049   9.163281   4.112924
  5.872972  10.008474   0.4502598  1.3920732 -0.6747763  8.487896
  9.224454   7.913118   8.449968   7.6997313  5.9028535  7.439646
  2.3174791  6.975212   1.6619607  9.750696   4.816901   3.6775346
  2.6042771  7.6607866]
Epoch 1/1000
2023-09-26 13:25:11.215 
Epoch 1/1000 
	 loss: 69.6916, MinusLogProbMetric: 69.6916, val_loss: 28.1290, val_MinusLogProbMetric: 28.1290

Epoch 1: val_loss improved from inf to 28.12896, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_290/weights/best_weights.h5
196/196 - 78s - loss: 69.6916 - MinusLogProbMetric: 69.6916 - val_loss: 28.1290 - val_MinusLogProbMetric: 28.1290 - lr: 0.0010 - 78s/epoch - 400ms/step
Epoch 2/1000
2023-09-26 13:25:39.920 
Epoch 2/1000 
	 loss: 25.3497, MinusLogProbMetric: 25.3497, val_loss: 23.4245, val_MinusLogProbMetric: 23.4245

Epoch 2: val_loss improved from 28.12896 to 23.42450, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_290/weights/best_weights.h5
196/196 - 29s - loss: 25.3497 - MinusLogProbMetric: 25.3497 - val_loss: 23.4245 - val_MinusLogProbMetric: 23.4245 - lr: 0.0010 - 29s/epoch - 146ms/step
Epoch 3/1000
2023-09-26 13:26:06.271 
Epoch 3/1000 
	 loss: 23.1270, MinusLogProbMetric: 23.1270, val_loss: 22.4895, val_MinusLogProbMetric: 22.4895

Epoch 3: val_loss improved from 23.42450 to 22.48954, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_290/weights/best_weights.h5
196/196 - 26s - loss: 23.1270 - MinusLogProbMetric: 23.1270 - val_loss: 22.4895 - val_MinusLogProbMetric: 22.4895 - lr: 0.0010 - 26s/epoch - 134ms/step
Epoch 4/1000
2023-09-26 13:26:32.683 
Epoch 4/1000 
	 loss: 22.3903, MinusLogProbMetric: 22.3903, val_loss: 21.6206, val_MinusLogProbMetric: 21.6206

Epoch 4: val_loss improved from 22.48954 to 21.62065, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_290/weights/best_weights.h5
196/196 - 27s - loss: 22.3903 - MinusLogProbMetric: 22.3903 - val_loss: 21.6206 - val_MinusLogProbMetric: 21.6206 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 5/1000
2023-09-26 13:26:59.996 
Epoch 5/1000 
	 loss: 21.4896, MinusLogProbMetric: 21.4896, val_loss: 20.7418, val_MinusLogProbMetric: 20.7418

Epoch 5: val_loss improved from 21.62065 to 20.74176, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_290/weights/best_weights.h5
196/196 - 27s - loss: 21.4896 - MinusLogProbMetric: 21.4896 - val_loss: 20.7418 - val_MinusLogProbMetric: 20.7418 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 6/1000
2023-09-26 13:27:27.637 
Epoch 6/1000 
	 loss: 21.2408, MinusLogProbMetric: 21.2408, val_loss: 20.4326, val_MinusLogProbMetric: 20.4326

Epoch 6: val_loss improved from 20.74176 to 20.43259, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_290/weights/best_weights.h5
196/196 - 28s - loss: 21.2408 - MinusLogProbMetric: 21.2408 - val_loss: 20.4326 - val_MinusLogProbMetric: 20.4326 - lr: 0.0010 - 28s/epoch - 144ms/step
Epoch 7/1000
2023-09-26 13:27:57.443 
Epoch 7/1000 
	 loss: 20.9101, MinusLogProbMetric: 20.9101, val_loss: 20.2652, val_MinusLogProbMetric: 20.2652

Epoch 7: val_loss improved from 20.43259 to 20.26516, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_290/weights/best_weights.h5
196/196 - 29s - loss: 20.9101 - MinusLogProbMetric: 20.9101 - val_loss: 20.2652 - val_MinusLogProbMetric: 20.2652 - lr: 0.0010 - 29s/epoch - 149ms/step
Epoch 8/1000
2023-09-26 13:28:26.164 
Epoch 8/1000 
	 loss: 20.4173, MinusLogProbMetric: 20.4173, val_loss: 20.0252, val_MinusLogProbMetric: 20.0252

Epoch 8: val_loss improved from 20.26516 to 20.02520, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_290/weights/best_weights.h5
196/196 - 29s - loss: 20.4173 - MinusLogProbMetric: 20.4173 - val_loss: 20.0252 - val_MinusLogProbMetric: 20.0252 - lr: 0.0010 - 29s/epoch - 146ms/step
Epoch 9/1000
2023-09-26 13:28:57.759 
Epoch 9/1000 
	 loss: 20.2326, MinusLogProbMetric: 20.2326, val_loss: 20.4835, val_MinusLogProbMetric: 20.4835

Epoch 9: val_loss did not improve from 20.02520
196/196 - 31s - loss: 20.2326 - MinusLogProbMetric: 20.2326 - val_loss: 20.4835 - val_MinusLogProbMetric: 20.4835 - lr: 0.0010 - 31s/epoch - 159ms/step
Epoch 10/1000
2023-09-26 13:29:30.132 
Epoch 10/1000 
	 loss: 20.0472, MinusLogProbMetric: 20.0472, val_loss: 20.1496, val_MinusLogProbMetric: 20.1496

Epoch 10: val_loss did not improve from 20.02520
196/196 - 32s - loss: 20.0472 - MinusLogProbMetric: 20.0472 - val_loss: 20.1496 - val_MinusLogProbMetric: 20.1496 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 11/1000
2023-09-26 13:30:02.612 
Epoch 11/1000 
	 loss: 20.0693, MinusLogProbMetric: 20.0693, val_loss: 19.5309, val_MinusLogProbMetric: 19.5309

Epoch 11: val_loss improved from 20.02520 to 19.53090, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_290/weights/best_weights.h5
196/196 - 33s - loss: 20.0693 - MinusLogProbMetric: 20.0693 - val_loss: 19.5309 - val_MinusLogProbMetric: 19.5309 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 12/1000
2023-09-26 13:30:34.638 
Epoch 12/1000 
	 loss: 19.7297, MinusLogProbMetric: 19.7297, val_loss: 18.9352, val_MinusLogProbMetric: 18.9352

Epoch 12: val_loss improved from 19.53090 to 18.93520, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_290/weights/best_weights.h5
196/196 - 32s - loss: 19.7297 - MinusLogProbMetric: 19.7297 - val_loss: 18.9352 - val_MinusLogProbMetric: 18.9352 - lr: 0.0010 - 32s/epoch - 163ms/step
Epoch 13/1000
2023-09-26 13:31:07.724 
Epoch 13/1000 
	 loss: 19.8086, MinusLogProbMetric: 19.8086, val_loss: 19.7638, val_MinusLogProbMetric: 19.7638

Epoch 13: val_loss did not improve from 18.93520
196/196 - 33s - loss: 19.8086 - MinusLogProbMetric: 19.8086 - val_loss: 19.7638 - val_MinusLogProbMetric: 19.7638 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 14/1000
2023-09-26 13:31:40.307 
Epoch 14/1000 
	 loss: 19.5325, MinusLogProbMetric: 19.5325, val_loss: 19.1339, val_MinusLogProbMetric: 19.1339

Epoch 14: val_loss did not improve from 18.93520
196/196 - 33s - loss: 19.5325 - MinusLogProbMetric: 19.5325 - val_loss: 19.1339 - val_MinusLogProbMetric: 19.1339 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 15/1000
2023-09-26 13:32:12.698 
Epoch 15/1000 
	 loss: 19.3540, MinusLogProbMetric: 19.3540, val_loss: 18.8913, val_MinusLogProbMetric: 18.8913

Epoch 15: val_loss improved from 18.93520 to 18.89133, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_290/weights/best_weights.h5
196/196 - 33s - loss: 19.3540 - MinusLogProbMetric: 19.3540 - val_loss: 18.8913 - val_MinusLogProbMetric: 18.8913 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 16/1000
2023-09-26 13:32:45.667 
Epoch 16/1000 
	 loss: 19.2758, MinusLogProbMetric: 19.2758, val_loss: 19.9351, val_MinusLogProbMetric: 19.9351

Epoch 16: val_loss did not improve from 18.89133
196/196 - 32s - loss: 19.2758 - MinusLogProbMetric: 19.2758 - val_loss: 19.9351 - val_MinusLogProbMetric: 19.9351 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 17/1000
2023-09-26 13:33:17.848 
Epoch 17/1000 
	 loss: 19.2843, MinusLogProbMetric: 19.2843, val_loss: 18.9608, val_MinusLogProbMetric: 18.9608

Epoch 17: val_loss did not improve from 18.89133
196/196 - 32s - loss: 19.2843 - MinusLogProbMetric: 19.2843 - val_loss: 18.9608 - val_MinusLogProbMetric: 18.9608 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 18/1000
2023-09-26 13:33:50.514 
Epoch 18/1000 
	 loss: 19.2154, MinusLogProbMetric: 19.2154, val_loss: 18.7392, val_MinusLogProbMetric: 18.7392

Epoch 18: val_loss improved from 18.89133 to 18.73918, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_290/weights/best_weights.h5
196/196 - 33s - loss: 19.2154 - MinusLogProbMetric: 19.2154 - val_loss: 18.7392 - val_MinusLogProbMetric: 18.7392 - lr: 0.0010 - 33s/epoch - 169ms/step
Epoch 19/1000
2023-09-26 13:34:23.044 
Epoch 19/1000 
	 loss: 19.1563, MinusLogProbMetric: 19.1563, val_loss: 18.9056, val_MinusLogProbMetric: 18.9056

Epoch 19: val_loss did not improve from 18.73918
196/196 - 32s - loss: 19.1563 - MinusLogProbMetric: 19.1563 - val_loss: 18.9056 - val_MinusLogProbMetric: 18.9056 - lr: 0.0010 - 32s/epoch - 163ms/step
Epoch 20/1000
2023-09-26 13:34:55.647 
Epoch 20/1000 
	 loss: 18.9522, MinusLogProbMetric: 18.9522, val_loss: 19.5733, val_MinusLogProbMetric: 19.5733

Epoch 20: val_loss did not improve from 18.73918
196/196 - 33s - loss: 18.9522 - MinusLogProbMetric: 18.9522 - val_loss: 19.5733 - val_MinusLogProbMetric: 19.5733 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 21/1000
2023-09-26 13:35:27.546 
Epoch 21/1000 
	 loss: 18.8088, MinusLogProbMetric: 18.8088, val_loss: 18.4020, val_MinusLogProbMetric: 18.4020

Epoch 21: val_loss improved from 18.73918 to 18.40201, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_290/weights/best_weights.h5
196/196 - 32s - loss: 18.8088 - MinusLogProbMetric: 18.8088 - val_loss: 18.4020 - val_MinusLogProbMetric: 18.4020 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 22/1000
2023-09-26 13:36:00.641 
Epoch 22/1000 
	 loss: 18.8738, MinusLogProbMetric: 18.8738, val_loss: 19.1461, val_MinusLogProbMetric: 19.1461

Epoch 22: val_loss did not improve from 18.40201
196/196 - 33s - loss: 18.8738 - MinusLogProbMetric: 18.8738 - val_loss: 19.1461 - val_MinusLogProbMetric: 19.1461 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 23/1000
2023-09-26 13:36:32.798 
Epoch 23/1000 
	 loss: 19.0383, MinusLogProbMetric: 19.0383, val_loss: 18.6793, val_MinusLogProbMetric: 18.6793

Epoch 23: val_loss did not improve from 18.40201
196/196 - 32s - loss: 19.0383 - MinusLogProbMetric: 19.0383 - val_loss: 18.6793 - val_MinusLogProbMetric: 18.6793 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 24/1000
2023-09-26 13:37:05.260 
Epoch 24/1000 
	 loss: 18.6959, MinusLogProbMetric: 18.6959, val_loss: 18.4507, val_MinusLogProbMetric: 18.4507

Epoch 24: val_loss did not improve from 18.40201
196/196 - 32s - loss: 18.6959 - MinusLogProbMetric: 18.6959 - val_loss: 18.4507 - val_MinusLogProbMetric: 18.4507 - lr: 0.0010 - 32s/epoch - 166ms/step
Epoch 25/1000
2023-09-26 13:37:37.558 
Epoch 25/1000 
	 loss: 18.5310, MinusLogProbMetric: 18.5310, val_loss: 18.4046, val_MinusLogProbMetric: 18.4046

Epoch 25: val_loss did not improve from 18.40201
196/196 - 32s - loss: 18.5310 - MinusLogProbMetric: 18.5310 - val_loss: 18.4046 - val_MinusLogProbMetric: 18.4046 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 26/1000
2023-09-26 13:38:10.375 
Epoch 26/1000 
	 loss: 18.6551, MinusLogProbMetric: 18.6551, val_loss: 18.3775, val_MinusLogProbMetric: 18.3775

Epoch 26: val_loss improved from 18.40201 to 18.37746, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_290/weights/best_weights.h5
196/196 - 33s - loss: 18.6551 - MinusLogProbMetric: 18.6551 - val_loss: 18.3775 - val_MinusLogProbMetric: 18.3775 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 27/1000
2023-09-26 13:38:43.055 
Epoch 27/1000 
	 loss: 18.7954, MinusLogProbMetric: 18.7954, val_loss: 19.3762, val_MinusLogProbMetric: 19.3762

Epoch 27: val_loss did not improve from 18.37746
196/196 - 32s - loss: 18.7954 - MinusLogProbMetric: 18.7954 - val_loss: 19.3762 - val_MinusLogProbMetric: 19.3762 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 28/1000
2023-09-26 13:39:15.292 
Epoch 28/1000 
	 loss: 18.4840, MinusLogProbMetric: 18.4840, val_loss: 18.3336, val_MinusLogProbMetric: 18.3336

Epoch 28: val_loss improved from 18.37746 to 18.33356, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_290/weights/best_weights.h5
196/196 - 33s - loss: 18.4840 - MinusLogProbMetric: 18.4840 - val_loss: 18.3336 - val_MinusLogProbMetric: 18.3336 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 29/1000
2023-09-26 13:39:48.466 
Epoch 29/1000 
	 loss: 18.5257, MinusLogProbMetric: 18.5257, val_loss: 18.4549, val_MinusLogProbMetric: 18.4549

Epoch 29: val_loss did not improve from 18.33356
196/196 - 33s - loss: 18.5257 - MinusLogProbMetric: 18.5257 - val_loss: 18.4549 - val_MinusLogProbMetric: 18.4549 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 30/1000
2023-09-26 13:40:21.220 
Epoch 30/1000 
	 loss: 18.3950, MinusLogProbMetric: 18.3950, val_loss: 18.6078, val_MinusLogProbMetric: 18.6078

Epoch 30: val_loss did not improve from 18.33356
196/196 - 33s - loss: 18.3950 - MinusLogProbMetric: 18.3950 - val_loss: 18.6078 - val_MinusLogProbMetric: 18.6078 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 31/1000
2023-09-26 13:40:54.057 
Epoch 31/1000 
	 loss: 18.4160, MinusLogProbMetric: 18.4160, val_loss: 18.8212, val_MinusLogProbMetric: 18.8212

Epoch 31: val_loss did not improve from 18.33356
196/196 - 33s - loss: 18.4160 - MinusLogProbMetric: 18.4160 - val_loss: 18.8212 - val_MinusLogProbMetric: 18.8212 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 32/1000
2023-09-26 13:41:26.555 
Epoch 32/1000 
	 loss: 18.3982, MinusLogProbMetric: 18.3982, val_loss: 18.1969, val_MinusLogProbMetric: 18.1969

Epoch 32: val_loss improved from 18.33356 to 18.19687, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_290/weights/best_weights.h5
196/196 - 33s - loss: 18.3982 - MinusLogProbMetric: 18.3982 - val_loss: 18.1969 - val_MinusLogProbMetric: 18.1969 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 33/1000
2023-09-26 13:41:59.363 
Epoch 33/1000 
	 loss: 18.3878, MinusLogProbMetric: 18.3878, val_loss: 19.3087, val_MinusLogProbMetric: 19.3087

Epoch 33: val_loss did not improve from 18.19687
196/196 - 32s - loss: 18.3878 - MinusLogProbMetric: 18.3878 - val_loss: 19.3087 - val_MinusLogProbMetric: 19.3087 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 34/1000
2023-09-26 13:42:31.700 
Epoch 34/1000 
	 loss: 18.2301, MinusLogProbMetric: 18.2301, val_loss: 18.2235, val_MinusLogProbMetric: 18.2235

Epoch 34: val_loss did not improve from 18.19687
196/196 - 32s - loss: 18.2301 - MinusLogProbMetric: 18.2301 - val_loss: 18.2235 - val_MinusLogProbMetric: 18.2235 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 35/1000
2023-09-26 13:43:04.076 
Epoch 35/1000 
	 loss: 18.3258, MinusLogProbMetric: 18.3258, val_loss: 18.6294, val_MinusLogProbMetric: 18.6294

Epoch 35: val_loss did not improve from 18.19687
196/196 - 32s - loss: 18.3258 - MinusLogProbMetric: 18.3258 - val_loss: 18.6294 - val_MinusLogProbMetric: 18.6294 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 36/1000
2023-09-26 13:43:36.533 
Epoch 36/1000 
	 loss: 18.2895, MinusLogProbMetric: 18.2895, val_loss: 18.2997, val_MinusLogProbMetric: 18.2997

Epoch 36: val_loss did not improve from 18.19687
196/196 - 32s - loss: 18.2895 - MinusLogProbMetric: 18.2895 - val_loss: 18.2997 - val_MinusLogProbMetric: 18.2997 - lr: 0.0010 - 32s/epoch - 166ms/step
Epoch 37/1000
2023-09-26 13:44:08.895 
Epoch 37/1000 
	 loss: 18.1496, MinusLogProbMetric: 18.1496, val_loss: 18.6078, val_MinusLogProbMetric: 18.6078

Epoch 37: val_loss did not improve from 18.19687
196/196 - 32s - loss: 18.1496 - MinusLogProbMetric: 18.1496 - val_loss: 18.6078 - val_MinusLogProbMetric: 18.6078 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 38/1000
2023-09-26 13:44:41.162 
Epoch 38/1000 
	 loss: 18.2131, MinusLogProbMetric: 18.2131, val_loss: 19.1071, val_MinusLogProbMetric: 19.1071

Epoch 38: val_loss did not improve from 18.19687
196/196 - 32s - loss: 18.2131 - MinusLogProbMetric: 18.2131 - val_loss: 19.1071 - val_MinusLogProbMetric: 19.1071 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 39/1000
2023-09-26 13:45:13.479 
Epoch 39/1000 
	 loss: 18.1671, MinusLogProbMetric: 18.1671, val_loss: 17.9119, val_MinusLogProbMetric: 17.9119

Epoch 39: val_loss improved from 18.19687 to 17.91194, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_290/weights/best_weights.h5
196/196 - 33s - loss: 18.1671 - MinusLogProbMetric: 18.1671 - val_loss: 17.9119 - val_MinusLogProbMetric: 17.9119 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 40/1000
2023-09-26 13:45:46.314 
Epoch 40/1000 
	 loss: 18.1967, MinusLogProbMetric: 18.1967, val_loss: 18.9119, val_MinusLogProbMetric: 18.9119

Epoch 40: val_loss did not improve from 17.91194
196/196 - 32s - loss: 18.1967 - MinusLogProbMetric: 18.1967 - val_loss: 18.9119 - val_MinusLogProbMetric: 18.9119 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 41/1000
2023-09-26 13:46:19.157 
Epoch 41/1000 
	 loss: 18.0098, MinusLogProbMetric: 18.0098, val_loss: 18.2910, val_MinusLogProbMetric: 18.2910

Epoch 41: val_loss did not improve from 17.91194
196/196 - 33s - loss: 18.0098 - MinusLogProbMetric: 18.0098 - val_loss: 18.2910 - val_MinusLogProbMetric: 18.2910 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 42/1000
2023-09-26 13:46:51.745 
Epoch 42/1000 
	 loss: 17.9442, MinusLogProbMetric: 17.9442, val_loss: 18.1688, val_MinusLogProbMetric: 18.1688

Epoch 42: val_loss did not improve from 17.91194
196/196 - 33s - loss: 17.9442 - MinusLogProbMetric: 17.9442 - val_loss: 18.1688 - val_MinusLogProbMetric: 18.1688 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 43/1000
2023-09-26 13:47:24.224 
Epoch 43/1000 
	 loss: 18.1045, MinusLogProbMetric: 18.1045, val_loss: 17.8238, val_MinusLogProbMetric: 17.8238

Epoch 43: val_loss improved from 17.91194 to 17.82377, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_290/weights/best_weights.h5
196/196 - 33s - loss: 18.1045 - MinusLogProbMetric: 18.1045 - val_loss: 17.8238 - val_MinusLogProbMetric: 17.8238 - lr: 0.0010 - 33s/epoch - 169ms/step
Epoch 44/1000
2023-09-26 13:47:57.179 
Epoch 44/1000 
	 loss: 18.0394, MinusLogProbMetric: 18.0394, val_loss: 18.1427, val_MinusLogProbMetric: 18.1427

Epoch 44: val_loss did not improve from 17.82377
196/196 - 32s - loss: 18.0394 - MinusLogProbMetric: 18.0394 - val_loss: 18.1427 - val_MinusLogProbMetric: 18.1427 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 45/1000
2023-09-26 13:48:29.696 
Epoch 45/1000 
	 loss: 17.9847, MinusLogProbMetric: 17.9847, val_loss: 18.2876, val_MinusLogProbMetric: 18.2876

Epoch 45: val_loss did not improve from 17.82377
196/196 - 33s - loss: 17.9847 - MinusLogProbMetric: 17.9847 - val_loss: 18.2876 - val_MinusLogProbMetric: 18.2876 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 46/1000
2023-09-26 13:49:01.787 
Epoch 46/1000 
	 loss: 17.9487, MinusLogProbMetric: 17.9487, val_loss: 18.2482, val_MinusLogProbMetric: 18.2482

Epoch 46: val_loss did not improve from 17.82377
196/196 - 32s - loss: 17.9487 - MinusLogProbMetric: 17.9487 - val_loss: 18.2482 - val_MinusLogProbMetric: 18.2482 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 47/1000
2023-09-26 13:49:34.257 
Epoch 47/1000 
	 loss: 17.7898, MinusLogProbMetric: 17.7898, val_loss: 17.9486, val_MinusLogProbMetric: 17.9486

Epoch 47: val_loss did not improve from 17.82377
196/196 - 32s - loss: 17.7898 - MinusLogProbMetric: 17.7898 - val_loss: 17.9486 - val_MinusLogProbMetric: 17.9486 - lr: 0.0010 - 32s/epoch - 166ms/step
Epoch 48/1000
2023-09-26 13:50:06.475 
Epoch 48/1000 
	 loss: 17.7839, MinusLogProbMetric: 17.7839, val_loss: 18.6826, val_MinusLogProbMetric: 18.6826

Epoch 48: val_loss did not improve from 17.82377
196/196 - 32s - loss: 17.7839 - MinusLogProbMetric: 17.7839 - val_loss: 18.6826 - val_MinusLogProbMetric: 18.6826 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 49/1000
2023-09-26 13:50:38.955 
Epoch 49/1000 
	 loss: 17.9251, MinusLogProbMetric: 17.9251, val_loss: 18.6709, val_MinusLogProbMetric: 18.6709

Epoch 49: val_loss did not improve from 17.82377
196/196 - 32s - loss: 17.9251 - MinusLogProbMetric: 17.9251 - val_loss: 18.6709 - val_MinusLogProbMetric: 18.6709 - lr: 0.0010 - 32s/epoch - 166ms/step
Epoch 50/1000
2023-09-26 13:51:11.221 
Epoch 50/1000 
	 loss: 17.8222, MinusLogProbMetric: 17.8222, val_loss: 19.4647, val_MinusLogProbMetric: 19.4647

Epoch 50: val_loss did not improve from 17.82377
196/196 - 32s - loss: 17.8222 - MinusLogProbMetric: 17.8222 - val_loss: 19.4647 - val_MinusLogProbMetric: 19.4647 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 51/1000
2023-09-26 13:51:44.178 
Epoch 51/1000 
	 loss: 17.8503, MinusLogProbMetric: 17.8503, val_loss: 18.0594, val_MinusLogProbMetric: 18.0594

Epoch 51: val_loss did not improve from 17.82377
196/196 - 33s - loss: 17.8503 - MinusLogProbMetric: 17.8503 - val_loss: 18.0594 - val_MinusLogProbMetric: 18.0594 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 52/1000
2023-09-26 13:52:16.853 
Epoch 52/1000 
	 loss: 17.7970, MinusLogProbMetric: 17.7970, val_loss: 17.8124, val_MinusLogProbMetric: 17.8124

Epoch 52: val_loss improved from 17.82377 to 17.81240, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_290/weights/best_weights.h5
196/196 - 34s - loss: 17.7970 - MinusLogProbMetric: 17.7970 - val_loss: 17.8124 - val_MinusLogProbMetric: 17.8124 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 53/1000
2023-09-26 13:52:50.508 
Epoch 53/1000 
	 loss: 17.6973, MinusLogProbMetric: 17.6973, val_loss: 18.4325, val_MinusLogProbMetric: 18.4325

Epoch 53: val_loss did not improve from 17.81240
196/196 - 33s - loss: 17.6973 - MinusLogProbMetric: 17.6973 - val_loss: 18.4325 - val_MinusLogProbMetric: 18.4325 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 54/1000
2023-09-26 13:53:23.074 
Epoch 54/1000 
	 loss: 17.6573, MinusLogProbMetric: 17.6573, val_loss: 18.1041, val_MinusLogProbMetric: 18.1041

Epoch 54: val_loss did not improve from 17.81240
196/196 - 33s - loss: 17.6573 - MinusLogProbMetric: 17.6573 - val_loss: 18.1041 - val_MinusLogProbMetric: 18.1041 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 55/1000
2023-09-26 13:53:55.359 
Epoch 55/1000 
	 loss: 17.7575, MinusLogProbMetric: 17.7575, val_loss: 18.0511, val_MinusLogProbMetric: 18.0511

Epoch 55: val_loss did not improve from 17.81240
196/196 - 32s - loss: 17.7575 - MinusLogProbMetric: 17.7575 - val_loss: 18.0511 - val_MinusLogProbMetric: 18.0511 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 56/1000
2023-09-26 13:54:28.111 
Epoch 56/1000 
	 loss: 17.6799, MinusLogProbMetric: 17.6799, val_loss: 17.6953, val_MinusLogProbMetric: 17.6953

Epoch 56: val_loss improved from 17.81240 to 17.69526, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_290/weights/best_weights.h5
196/196 - 33s - loss: 17.6799 - MinusLogProbMetric: 17.6799 - val_loss: 17.6953 - val_MinusLogProbMetric: 17.6953 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 57/1000
2023-09-26 13:55:01.341 
Epoch 57/1000 
	 loss: 17.7079, MinusLogProbMetric: 17.7079, val_loss: 17.8616, val_MinusLogProbMetric: 17.8616

Epoch 57: val_loss did not improve from 17.69526
196/196 - 33s - loss: 17.7079 - MinusLogProbMetric: 17.7079 - val_loss: 17.8616 - val_MinusLogProbMetric: 17.8616 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 58/1000
2023-09-26 13:55:34.289 
Epoch 58/1000 
	 loss: 17.7012, MinusLogProbMetric: 17.7012, val_loss: 17.6581, val_MinusLogProbMetric: 17.6581

Epoch 58: val_loss improved from 17.69526 to 17.65808, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_290/weights/best_weights.h5
196/196 - 34s - loss: 17.7012 - MinusLogProbMetric: 17.7012 - val_loss: 17.6581 - val_MinusLogProbMetric: 17.6581 - lr: 0.0010 - 34s/epoch - 171ms/step
Epoch 59/1000
2023-09-26 13:56:07.461 
Epoch 59/1000 
	 loss: 17.5546, MinusLogProbMetric: 17.5546, val_loss: 18.2778, val_MinusLogProbMetric: 18.2778

Epoch 59: val_loss did not improve from 17.65808
196/196 - 33s - loss: 17.5546 - MinusLogProbMetric: 17.5546 - val_loss: 18.2778 - val_MinusLogProbMetric: 18.2778 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 60/1000
2023-09-26 13:56:39.505 
Epoch 60/1000 
	 loss: 17.6692, MinusLogProbMetric: 17.6692, val_loss: 17.8640, val_MinusLogProbMetric: 17.8640

Epoch 60: val_loss did not improve from 17.65808
196/196 - 32s - loss: 17.6692 - MinusLogProbMetric: 17.6692 - val_loss: 17.8640 - val_MinusLogProbMetric: 17.8640 - lr: 0.0010 - 32s/epoch - 163ms/step
Epoch 61/1000
2023-09-26 13:57:11.787 
Epoch 61/1000 
	 loss: 17.7745, MinusLogProbMetric: 17.7745, val_loss: 18.4843, val_MinusLogProbMetric: 18.4843

Epoch 61: val_loss did not improve from 17.65808
196/196 - 32s - loss: 17.7745 - MinusLogProbMetric: 17.7745 - val_loss: 18.4843 - val_MinusLogProbMetric: 18.4843 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 62/1000
2023-09-26 13:57:44.516 
Epoch 62/1000 
	 loss: 17.5265, MinusLogProbMetric: 17.5265, val_loss: 17.9623, val_MinusLogProbMetric: 17.9623

Epoch 62: val_loss did not improve from 17.65808
196/196 - 33s - loss: 17.5265 - MinusLogProbMetric: 17.5265 - val_loss: 17.9623 - val_MinusLogProbMetric: 17.9623 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 63/1000
2023-09-26 13:58:17.025 
Epoch 63/1000 
	 loss: 17.5625, MinusLogProbMetric: 17.5625, val_loss: 17.7284, val_MinusLogProbMetric: 17.7284

Epoch 63: val_loss did not improve from 17.65808
196/196 - 33s - loss: 17.5625 - MinusLogProbMetric: 17.5625 - val_loss: 17.7284 - val_MinusLogProbMetric: 17.7284 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 64/1000
2023-09-26 13:58:50.166 
Epoch 64/1000 
	 loss: 17.5419, MinusLogProbMetric: 17.5419, val_loss: 18.1040, val_MinusLogProbMetric: 18.1040

Epoch 64: val_loss did not improve from 17.65808
196/196 - 33s - loss: 17.5419 - MinusLogProbMetric: 17.5419 - val_loss: 18.1040 - val_MinusLogProbMetric: 18.1040 - lr: 0.0010 - 33s/epoch - 169ms/step
Epoch 65/1000
2023-09-26 13:59:23.435 
Epoch 65/1000 
	 loss: 17.4704, MinusLogProbMetric: 17.4704, val_loss: 17.8315, val_MinusLogProbMetric: 17.8315

Epoch 65: val_loss did not improve from 17.65808
196/196 - 33s - loss: 17.4704 - MinusLogProbMetric: 17.4704 - val_loss: 17.8315 - val_MinusLogProbMetric: 17.8315 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 66/1000
2023-09-26 13:59:56.644 
Epoch 66/1000 
	 loss: 17.4911, MinusLogProbMetric: 17.4911, val_loss: 17.9753, val_MinusLogProbMetric: 17.9753

Epoch 66: val_loss did not improve from 17.65808
196/196 - 33s - loss: 17.4911 - MinusLogProbMetric: 17.4911 - val_loss: 17.9753 - val_MinusLogProbMetric: 17.9753 - lr: 0.0010 - 33s/epoch - 169ms/step
Epoch 67/1000
2023-09-26 14:00:29.911 
Epoch 67/1000 
	 loss: 17.6796, MinusLogProbMetric: 17.6796, val_loss: 17.8804, val_MinusLogProbMetric: 17.8804

Epoch 67: val_loss did not improve from 17.65808
196/196 - 33s - loss: 17.6796 - MinusLogProbMetric: 17.6796 - val_loss: 17.8804 - val_MinusLogProbMetric: 17.8804 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 68/1000
2023-09-26 14:01:02.466 
Epoch 68/1000 
	 loss: 17.4813, MinusLogProbMetric: 17.4813, val_loss: 17.7789, val_MinusLogProbMetric: 17.7789

Epoch 68: val_loss did not improve from 17.65808
196/196 - 33s - loss: 17.4813 - MinusLogProbMetric: 17.4813 - val_loss: 17.7789 - val_MinusLogProbMetric: 17.7789 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 69/1000
2023-09-26 14:01:35.083 
Epoch 69/1000 
	 loss: 17.4334, MinusLogProbMetric: 17.4334, val_loss: 17.6532, val_MinusLogProbMetric: 17.6532

Epoch 69: val_loss improved from 17.65808 to 17.65320, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_290/weights/best_weights.h5
196/196 - 33s - loss: 17.4334 - MinusLogProbMetric: 17.4334 - val_loss: 17.6532 - val_MinusLogProbMetric: 17.6532 - lr: 0.0010 - 33s/epoch - 169ms/step
Epoch 70/1000
2023-09-26 14:02:08.240 
Epoch 70/1000 
	 loss: 17.5276, MinusLogProbMetric: 17.5276, val_loss: 17.7838, val_MinusLogProbMetric: 17.7838

Epoch 70: val_loss did not improve from 17.65320
196/196 - 33s - loss: 17.5276 - MinusLogProbMetric: 17.5276 - val_loss: 17.7838 - val_MinusLogProbMetric: 17.7838 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 71/1000
2023-09-26 14:02:40.790 
Epoch 71/1000 
	 loss: 17.4055, MinusLogProbMetric: 17.4055, val_loss: 17.7307, val_MinusLogProbMetric: 17.7307

Epoch 71: val_loss did not improve from 17.65320
196/196 - 33s - loss: 17.4055 - MinusLogProbMetric: 17.4055 - val_loss: 17.7307 - val_MinusLogProbMetric: 17.7307 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 72/1000
2023-09-26 14:03:13.364 
Epoch 72/1000 
	 loss: 17.3995, MinusLogProbMetric: 17.3995, val_loss: 18.2090, val_MinusLogProbMetric: 18.2090

Epoch 72: val_loss did not improve from 17.65320
196/196 - 33s - loss: 17.3995 - MinusLogProbMetric: 17.3995 - val_loss: 18.2090 - val_MinusLogProbMetric: 18.2090 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 73/1000
2023-09-26 14:03:46.678 
Epoch 73/1000 
	 loss: 17.4111, MinusLogProbMetric: 17.4111, val_loss: 18.1309, val_MinusLogProbMetric: 18.1309

Epoch 73: val_loss did not improve from 17.65320
196/196 - 33s - loss: 17.4111 - MinusLogProbMetric: 17.4111 - val_loss: 18.1309 - val_MinusLogProbMetric: 18.1309 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 74/1000
2023-09-26 14:04:18.999 
Epoch 74/1000 
	 loss: 17.3323, MinusLogProbMetric: 17.3323, val_loss: 18.0307, val_MinusLogProbMetric: 18.0307

Epoch 74: val_loss did not improve from 17.65320
196/196 - 32s - loss: 17.3323 - MinusLogProbMetric: 17.3323 - val_loss: 18.0307 - val_MinusLogProbMetric: 18.0307 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 75/1000
2023-09-26 14:04:51.813 
Epoch 75/1000 
	 loss: 17.3923, MinusLogProbMetric: 17.3923, val_loss: 17.8835, val_MinusLogProbMetric: 17.8835

Epoch 75: val_loss did not improve from 17.65320
196/196 - 33s - loss: 17.3923 - MinusLogProbMetric: 17.3923 - val_loss: 17.8835 - val_MinusLogProbMetric: 17.8835 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 76/1000
2023-09-26 14:05:24.466 
Epoch 76/1000 
	 loss: 17.4605, MinusLogProbMetric: 17.4605, val_loss: 18.0578, val_MinusLogProbMetric: 18.0578

Epoch 76: val_loss did not improve from 17.65320
196/196 - 33s - loss: 17.4605 - MinusLogProbMetric: 17.4605 - val_loss: 18.0578 - val_MinusLogProbMetric: 18.0578 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 77/1000
2023-09-26 14:05:57.001 
Epoch 77/1000 
	 loss: 17.2571, MinusLogProbMetric: 17.2571, val_loss: 17.6564, val_MinusLogProbMetric: 17.6564

Epoch 77: val_loss did not improve from 17.65320
196/196 - 33s - loss: 17.2571 - MinusLogProbMetric: 17.2571 - val_loss: 17.6564 - val_MinusLogProbMetric: 17.6564 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 78/1000
2023-09-26 14:06:29.182 
Epoch 78/1000 
	 loss: 17.3341, MinusLogProbMetric: 17.3341, val_loss: 18.3201, val_MinusLogProbMetric: 18.3201

Epoch 78: val_loss did not improve from 17.65320
196/196 - 32s - loss: 17.3341 - MinusLogProbMetric: 17.3341 - val_loss: 18.3201 - val_MinusLogProbMetric: 18.3201 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 79/1000
2023-09-26 14:07:01.964 
Epoch 79/1000 
	 loss: 17.2658, MinusLogProbMetric: 17.2658, val_loss: 18.5532, val_MinusLogProbMetric: 18.5532

Epoch 79: val_loss did not improve from 17.65320
196/196 - 33s - loss: 17.2658 - MinusLogProbMetric: 17.2658 - val_loss: 18.5532 - val_MinusLogProbMetric: 18.5532 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 80/1000
2023-09-26 14:07:34.543 
Epoch 80/1000 
	 loss: 17.3510, MinusLogProbMetric: 17.3510, val_loss: 18.0253, val_MinusLogProbMetric: 18.0253

Epoch 80: val_loss did not improve from 17.65320
196/196 - 33s - loss: 17.3510 - MinusLogProbMetric: 17.3510 - val_loss: 18.0253 - val_MinusLogProbMetric: 18.0253 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 81/1000
2023-09-26 14:08:07.230 
Epoch 81/1000 
	 loss: 17.2863, MinusLogProbMetric: 17.2863, val_loss: 17.9400, val_MinusLogProbMetric: 17.9400

Epoch 81: val_loss did not improve from 17.65320
196/196 - 33s - loss: 17.2863 - MinusLogProbMetric: 17.2863 - val_loss: 17.9400 - val_MinusLogProbMetric: 17.9400 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 82/1000
2023-09-26 14:08:39.366 
Epoch 82/1000 
	 loss: 17.2160, MinusLogProbMetric: 17.2160, val_loss: 17.6818, val_MinusLogProbMetric: 17.6818

Epoch 82: val_loss did not improve from 17.65320
196/196 - 32s - loss: 17.2160 - MinusLogProbMetric: 17.2160 - val_loss: 17.6818 - val_MinusLogProbMetric: 17.6818 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 83/1000
2023-09-26 14:09:12.343 
Epoch 83/1000 
	 loss: 17.2438, MinusLogProbMetric: 17.2438, val_loss: 18.8548, val_MinusLogProbMetric: 18.8548

Epoch 83: val_loss did not improve from 17.65320
196/196 - 33s - loss: 17.2438 - MinusLogProbMetric: 17.2438 - val_loss: 18.8548 - val_MinusLogProbMetric: 18.8548 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 84/1000
2023-09-26 14:09:45.478 
Epoch 84/1000 
	 loss: 17.3208, MinusLogProbMetric: 17.3208, val_loss: 18.0163, val_MinusLogProbMetric: 18.0163

Epoch 84: val_loss did not improve from 17.65320
196/196 - 33s - loss: 17.3208 - MinusLogProbMetric: 17.3208 - val_loss: 18.0163 - val_MinusLogProbMetric: 18.0163 - lr: 0.0010 - 33s/epoch - 169ms/step
Epoch 85/1000
2023-09-26 14:10:19.828 
Epoch 85/1000 
	 loss: 17.1937, MinusLogProbMetric: 17.1937, val_loss: 18.2983, val_MinusLogProbMetric: 18.2983

Epoch 85: val_loss did not improve from 17.65320
196/196 - 34s - loss: 17.1937 - MinusLogProbMetric: 17.1937 - val_loss: 18.2983 - val_MinusLogProbMetric: 18.2983 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 86/1000
2023-09-26 14:10:53.345 
Epoch 86/1000 
	 loss: 17.3813, MinusLogProbMetric: 17.3813, val_loss: 17.8077, val_MinusLogProbMetric: 17.8077

Epoch 86: val_loss did not improve from 17.65320
196/196 - 34s - loss: 17.3813 - MinusLogProbMetric: 17.3813 - val_loss: 17.8077 - val_MinusLogProbMetric: 17.8077 - lr: 0.0010 - 34s/epoch - 171ms/step
Epoch 87/1000
2023-09-26 14:11:26.977 
Epoch 87/1000 
	 loss: 17.2233, MinusLogProbMetric: 17.2233, val_loss: 17.7832, val_MinusLogProbMetric: 17.7832

Epoch 87: val_loss did not improve from 17.65320
196/196 - 34s - loss: 17.2233 - MinusLogProbMetric: 17.2233 - val_loss: 17.7832 - val_MinusLogProbMetric: 17.7832 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 88/1000
2023-09-26 14:12:00.232 
Epoch 88/1000 
	 loss: 17.2902, MinusLogProbMetric: 17.2902, val_loss: 17.5643, val_MinusLogProbMetric: 17.5643

Epoch 88: val_loss improved from 17.65320 to 17.56432, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_290/weights/best_weights.h5
196/196 - 34s - loss: 17.2902 - MinusLogProbMetric: 17.2902 - val_loss: 17.5643 - val_MinusLogProbMetric: 17.5643 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 89/1000
2023-09-26 14:12:33.188 
Epoch 89/1000 
	 loss: 17.2277, MinusLogProbMetric: 17.2277, val_loss: 17.6647, val_MinusLogProbMetric: 17.6647

Epoch 89: val_loss did not improve from 17.56432
196/196 - 32s - loss: 17.2277 - MinusLogProbMetric: 17.2277 - val_loss: 17.6647 - val_MinusLogProbMetric: 17.6647 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 90/1000
2023-09-26 14:13:05.906 
Epoch 90/1000 
	 loss: 17.1556, MinusLogProbMetric: 17.1556, val_loss: 17.6518, val_MinusLogProbMetric: 17.6518

Epoch 90: val_loss did not improve from 17.56432
196/196 - 33s - loss: 17.1556 - MinusLogProbMetric: 17.1556 - val_loss: 17.6518 - val_MinusLogProbMetric: 17.6518 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 91/1000
2023-09-26 14:13:38.626 
Epoch 91/1000 
	 loss: 17.1157, MinusLogProbMetric: 17.1157, val_loss: 17.6856, val_MinusLogProbMetric: 17.6856

Epoch 91: val_loss did not improve from 17.56432
196/196 - 33s - loss: 17.1157 - MinusLogProbMetric: 17.1157 - val_loss: 17.6856 - val_MinusLogProbMetric: 17.6856 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 92/1000
2023-09-26 14:14:11.383 
Epoch 92/1000 
	 loss: 17.1400, MinusLogProbMetric: 17.1400, val_loss: 18.0025, val_MinusLogProbMetric: 18.0025

Epoch 92: val_loss did not improve from 17.56432
196/196 - 33s - loss: 17.1400 - MinusLogProbMetric: 17.1400 - val_loss: 18.0025 - val_MinusLogProbMetric: 18.0025 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 93/1000
2023-09-26 14:14:44.405 
Epoch 93/1000 
	 loss: 17.1661, MinusLogProbMetric: 17.1661, val_loss: 18.1399, val_MinusLogProbMetric: 18.1399

Epoch 93: val_loss did not improve from 17.56432
196/196 - 33s - loss: 17.1661 - MinusLogProbMetric: 17.1661 - val_loss: 18.1399 - val_MinusLogProbMetric: 18.1399 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 94/1000
2023-09-26 14:15:16.910 
Epoch 94/1000 
	 loss: 17.2520, MinusLogProbMetric: 17.2520, val_loss: 18.0246, val_MinusLogProbMetric: 18.0246

Epoch 94: val_loss did not improve from 17.56432
196/196 - 33s - loss: 17.2520 - MinusLogProbMetric: 17.2520 - val_loss: 18.0246 - val_MinusLogProbMetric: 18.0246 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 95/1000
2023-09-26 14:15:49.958 
Epoch 95/1000 
	 loss: 17.2031, MinusLogProbMetric: 17.2031, val_loss: 17.7313, val_MinusLogProbMetric: 17.7313

Epoch 95: val_loss did not improve from 17.56432
196/196 - 33s - loss: 17.2031 - MinusLogProbMetric: 17.2031 - val_loss: 17.7313 - val_MinusLogProbMetric: 17.7313 - lr: 0.0010 - 33s/epoch - 169ms/step
Epoch 96/1000
2023-09-26 14:16:22.407 
Epoch 96/1000 
	 loss: 17.0727, MinusLogProbMetric: 17.0727, val_loss: 17.6680, val_MinusLogProbMetric: 17.6680

Epoch 96: val_loss did not improve from 17.56432
196/196 - 32s - loss: 17.0727 - MinusLogProbMetric: 17.0727 - val_loss: 17.6680 - val_MinusLogProbMetric: 17.6680 - lr: 0.0010 - 32s/epoch - 166ms/step
Epoch 97/1000
2023-09-26 14:16:54.841 
Epoch 97/1000 
	 loss: 17.1709, MinusLogProbMetric: 17.1709, val_loss: 17.8118, val_MinusLogProbMetric: 17.8118

Epoch 97: val_loss did not improve from 17.56432
196/196 - 32s - loss: 17.1709 - MinusLogProbMetric: 17.1709 - val_loss: 17.8118 - val_MinusLogProbMetric: 17.8118 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 98/1000
2023-09-26 14:17:28.227 
Epoch 98/1000 
	 loss: 17.0641, MinusLogProbMetric: 17.0641, val_loss: 17.9370, val_MinusLogProbMetric: 17.9370

Epoch 98: val_loss did not improve from 17.56432
196/196 - 33s - loss: 17.0641 - MinusLogProbMetric: 17.0641 - val_loss: 17.9370 - val_MinusLogProbMetric: 17.9370 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 99/1000
2023-09-26 14:18:00.961 
Epoch 99/1000 
	 loss: 17.0843, MinusLogProbMetric: 17.0843, val_loss: 17.7697, val_MinusLogProbMetric: 17.7697

Epoch 99: val_loss did not improve from 17.56432
196/196 - 33s - loss: 17.0843 - MinusLogProbMetric: 17.0843 - val_loss: 17.7697 - val_MinusLogProbMetric: 17.7697 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 100/1000
2023-09-26 14:18:34.696 
Epoch 100/1000 
	 loss: 17.0866, MinusLogProbMetric: 17.0866, val_loss: 17.6842, val_MinusLogProbMetric: 17.6842

Epoch 100: val_loss did not improve from 17.56432
196/196 - 34s - loss: 17.0866 - MinusLogProbMetric: 17.0866 - val_loss: 17.6842 - val_MinusLogProbMetric: 17.6842 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 101/1000
2023-09-26 14:19:07.920 
Epoch 101/1000 
	 loss: 17.0333, MinusLogProbMetric: 17.0333, val_loss: 17.5899, val_MinusLogProbMetric: 17.5899

Epoch 101: val_loss did not improve from 17.56432
196/196 - 33s - loss: 17.0333 - MinusLogProbMetric: 17.0333 - val_loss: 17.5899 - val_MinusLogProbMetric: 17.5899 - lr: 0.0010 - 33s/epoch - 169ms/step
Epoch 102/1000
2023-09-26 14:19:40.562 
Epoch 102/1000 
	 loss: 16.9644, MinusLogProbMetric: 16.9644, val_loss: 18.1276, val_MinusLogProbMetric: 18.1276

Epoch 102: val_loss did not improve from 17.56432
196/196 - 33s - loss: 16.9644 - MinusLogProbMetric: 16.9644 - val_loss: 18.1276 - val_MinusLogProbMetric: 18.1276 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 103/1000
2023-09-26 14:20:11.557 
Epoch 103/1000 
	 loss: 17.0419, MinusLogProbMetric: 17.0419, val_loss: 17.7530, val_MinusLogProbMetric: 17.7530

Epoch 103: val_loss did not improve from 17.56432
196/196 - 31s - loss: 17.0419 - MinusLogProbMetric: 17.0419 - val_loss: 17.7530 - val_MinusLogProbMetric: 17.7530 - lr: 0.0010 - 31s/epoch - 158ms/step
Epoch 104/1000
2023-09-26 14:20:43.607 
Epoch 104/1000 
	 loss: 17.0811, MinusLogProbMetric: 17.0811, val_loss: 17.7042, val_MinusLogProbMetric: 17.7042

Epoch 104: val_loss did not improve from 17.56432
196/196 - 32s - loss: 17.0811 - MinusLogProbMetric: 17.0811 - val_loss: 17.7042 - val_MinusLogProbMetric: 17.7042 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 105/1000
2023-09-26 14:21:14.952 
Epoch 105/1000 
	 loss: 16.9967, MinusLogProbMetric: 16.9967, val_loss: 18.1064, val_MinusLogProbMetric: 18.1064

Epoch 105: val_loss did not improve from 17.56432
196/196 - 31s - loss: 16.9967 - MinusLogProbMetric: 16.9967 - val_loss: 18.1064 - val_MinusLogProbMetric: 18.1064 - lr: 0.0010 - 31s/epoch - 160ms/step
Epoch 106/1000
2023-09-26 14:21:46.957 
Epoch 106/1000 
	 loss: 17.0054, MinusLogProbMetric: 17.0054, val_loss: 17.6278, val_MinusLogProbMetric: 17.6278

Epoch 106: val_loss did not improve from 17.56432
196/196 - 32s - loss: 17.0054 - MinusLogProbMetric: 17.0054 - val_loss: 17.6278 - val_MinusLogProbMetric: 17.6278 - lr: 0.0010 - 32s/epoch - 163ms/step
Epoch 107/1000
2023-09-26 14:22:18.865 
Epoch 107/1000 
	 loss: 17.0105, MinusLogProbMetric: 17.0105, val_loss: 17.8498, val_MinusLogProbMetric: 17.8498

Epoch 107: val_loss did not improve from 17.56432
196/196 - 32s - loss: 17.0105 - MinusLogProbMetric: 17.0105 - val_loss: 17.8498 - val_MinusLogProbMetric: 17.8498 - lr: 0.0010 - 32s/epoch - 163ms/step
Epoch 108/1000
2023-09-26 14:22:51.861 
Epoch 108/1000 
	 loss: 17.0861, MinusLogProbMetric: 17.0861, val_loss: 17.7070, val_MinusLogProbMetric: 17.7070

Epoch 108: val_loss did not improve from 17.56432
196/196 - 33s - loss: 17.0861 - MinusLogProbMetric: 17.0861 - val_loss: 17.7070 - val_MinusLogProbMetric: 17.7070 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 109/1000
2023-09-26 14:23:24.378 
Epoch 109/1000 
	 loss: 17.0643, MinusLogProbMetric: 17.0643, val_loss: 19.4067, val_MinusLogProbMetric: 19.4067

Epoch 109: val_loss did not improve from 17.56432
196/196 - 33s - loss: 17.0643 - MinusLogProbMetric: 17.0643 - val_loss: 19.4067 - val_MinusLogProbMetric: 19.4067 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 110/1000
2023-09-26 14:23:57.354 
Epoch 110/1000 
	 loss: 16.9475, MinusLogProbMetric: 16.9475, val_loss: 17.6544, val_MinusLogProbMetric: 17.6544

Epoch 110: val_loss did not improve from 17.56432
196/196 - 33s - loss: 16.9475 - MinusLogProbMetric: 16.9475 - val_loss: 17.6544 - val_MinusLogProbMetric: 17.6544 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 111/1000
2023-09-26 14:24:30.062 
Epoch 111/1000 
	 loss: 16.9419, MinusLogProbMetric: 16.9419, val_loss: 19.5308, val_MinusLogProbMetric: 19.5308

Epoch 111: val_loss did not improve from 17.56432
196/196 - 33s - loss: 16.9419 - MinusLogProbMetric: 16.9419 - val_loss: 19.5308 - val_MinusLogProbMetric: 19.5308 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 112/1000
2023-09-26 14:25:03.285 
Epoch 112/1000 
	 loss: 17.0627, MinusLogProbMetric: 17.0627, val_loss: 17.7228, val_MinusLogProbMetric: 17.7228

Epoch 112: val_loss did not improve from 17.56432
196/196 - 33s - loss: 17.0627 - MinusLogProbMetric: 17.0627 - val_loss: 17.7228 - val_MinusLogProbMetric: 17.7228 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 113/1000
2023-09-26 14:25:34.271 
Epoch 113/1000 
	 loss: 16.9135, MinusLogProbMetric: 16.9135, val_loss: 17.7462, val_MinusLogProbMetric: 17.7462

Epoch 113: val_loss did not improve from 17.56432
196/196 - 31s - loss: 16.9135 - MinusLogProbMetric: 16.9135 - val_loss: 17.7462 - val_MinusLogProbMetric: 17.7462 - lr: 0.0010 - 31s/epoch - 158ms/step
Epoch 114/1000
2023-09-26 14:26:06.376 
Epoch 114/1000 
	 loss: 16.9360, MinusLogProbMetric: 16.9360, val_loss: 18.9182, val_MinusLogProbMetric: 18.9182

Epoch 114: val_loss did not improve from 17.56432
196/196 - 32s - loss: 16.9360 - MinusLogProbMetric: 16.9360 - val_loss: 18.9182 - val_MinusLogProbMetric: 18.9182 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 115/1000
2023-09-26 14:26:38.470 
Epoch 115/1000 
	 loss: 16.8996, MinusLogProbMetric: 16.8996, val_loss: 17.6967, val_MinusLogProbMetric: 17.6967

Epoch 115: val_loss did not improve from 17.56432
196/196 - 32s - loss: 16.8996 - MinusLogProbMetric: 16.8996 - val_loss: 17.6967 - val_MinusLogProbMetric: 17.6967 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 116/1000
2023-09-26 14:27:10.499 
Epoch 116/1000 
	 loss: 16.8826, MinusLogProbMetric: 16.8826, val_loss: 17.6606, val_MinusLogProbMetric: 17.6606

Epoch 116: val_loss did not improve from 17.56432
196/196 - 32s - loss: 16.8826 - MinusLogProbMetric: 16.8826 - val_loss: 17.6606 - val_MinusLogProbMetric: 17.6606 - lr: 0.0010 - 32s/epoch - 163ms/step
Epoch 117/1000
2023-09-26 14:27:43.209 
Epoch 117/1000 
	 loss: 16.8740, MinusLogProbMetric: 16.8740, val_loss: 17.7212, val_MinusLogProbMetric: 17.7212

Epoch 117: val_loss did not improve from 17.56432
196/196 - 33s - loss: 16.8740 - MinusLogProbMetric: 16.8740 - val_loss: 17.7212 - val_MinusLogProbMetric: 17.7212 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 118/1000
2023-09-26 14:28:15.236 
Epoch 118/1000 
	 loss: 16.9191, MinusLogProbMetric: 16.9191, val_loss: 17.7253, val_MinusLogProbMetric: 17.7253

Epoch 118: val_loss did not improve from 17.56432
196/196 - 32s - loss: 16.9191 - MinusLogProbMetric: 16.9191 - val_loss: 17.7253 - val_MinusLogProbMetric: 17.7253 - lr: 0.0010 - 32s/epoch - 163ms/step
Epoch 119/1000
2023-09-26 14:28:46.853 
Epoch 119/1000 
	 loss: 16.9015, MinusLogProbMetric: 16.9015, val_loss: 17.5230, val_MinusLogProbMetric: 17.5230

Epoch 119: val_loss improved from 17.56432 to 17.52302, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_290/weights/best_weights.h5
196/196 - 32s - loss: 16.9015 - MinusLogProbMetric: 16.9015 - val_loss: 17.5230 - val_MinusLogProbMetric: 17.5230 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 120/1000
2023-09-26 14:29:20.370 
Epoch 120/1000 
	 loss: 16.8052, MinusLogProbMetric: 16.8052, val_loss: 17.8182, val_MinusLogProbMetric: 17.8182

Epoch 120: val_loss did not improve from 17.52302
196/196 - 33s - loss: 16.8052 - MinusLogProbMetric: 16.8052 - val_loss: 17.8182 - val_MinusLogProbMetric: 17.8182 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 121/1000
2023-09-26 14:29:53.091 
Epoch 121/1000 
	 loss: 16.7948, MinusLogProbMetric: 16.7948, val_loss: 17.9160, val_MinusLogProbMetric: 17.9160

Epoch 121: val_loss did not improve from 17.52302
196/196 - 33s - loss: 16.7948 - MinusLogProbMetric: 16.7948 - val_loss: 17.9160 - val_MinusLogProbMetric: 17.9160 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 122/1000
2023-09-26 14:30:25.752 
Epoch 122/1000 
	 loss: 16.9360, MinusLogProbMetric: 16.9360, val_loss: 17.6395, val_MinusLogProbMetric: 17.6395

Epoch 122: val_loss did not improve from 17.52302
196/196 - 33s - loss: 16.9360 - MinusLogProbMetric: 16.9360 - val_loss: 17.6395 - val_MinusLogProbMetric: 17.6395 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 123/1000
2023-09-26 14:30:58.094 
Epoch 123/1000 
	 loss: 16.8143, MinusLogProbMetric: 16.8143, val_loss: 18.0383, val_MinusLogProbMetric: 18.0383

Epoch 123: val_loss did not improve from 17.52302
196/196 - 32s - loss: 16.8143 - MinusLogProbMetric: 16.8143 - val_loss: 18.0383 - val_MinusLogProbMetric: 18.0383 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 124/1000
2023-09-26 14:31:31.148 
Epoch 124/1000 
	 loss: 16.8173, MinusLogProbMetric: 16.8173, val_loss: 18.2856, val_MinusLogProbMetric: 18.2856

Epoch 124: val_loss did not improve from 17.52302
196/196 - 33s - loss: 16.8173 - MinusLogProbMetric: 16.8173 - val_loss: 18.2856 - val_MinusLogProbMetric: 18.2856 - lr: 0.0010 - 33s/epoch - 169ms/step
Epoch 125/1000
2023-09-26 14:32:03.090 
Epoch 125/1000 
	 loss: 16.7679, MinusLogProbMetric: 16.7679, val_loss: 18.0262, val_MinusLogProbMetric: 18.0262

Epoch 125: val_loss did not improve from 17.52302
196/196 - 32s - loss: 16.7679 - MinusLogProbMetric: 16.7679 - val_loss: 18.0262 - val_MinusLogProbMetric: 18.0262 - lr: 0.0010 - 32s/epoch - 163ms/step
Epoch 126/1000
2023-09-26 14:32:35.727 
Epoch 126/1000 
	 loss: 16.7937, MinusLogProbMetric: 16.7937, val_loss: 17.5224, val_MinusLogProbMetric: 17.5224

Epoch 126: val_loss improved from 17.52302 to 17.52244, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_290/weights/best_weights.h5
196/196 - 33s - loss: 16.7937 - MinusLogProbMetric: 16.7937 - val_loss: 17.5224 - val_MinusLogProbMetric: 17.5224 - lr: 0.0010 - 33s/epoch - 171ms/step
Epoch 127/1000
2023-09-26 14:33:09.474 
Epoch 127/1000 
	 loss: 16.9408, MinusLogProbMetric: 16.9408, val_loss: 17.8539, val_MinusLogProbMetric: 17.8539

Epoch 127: val_loss did not improve from 17.52244
196/196 - 33s - loss: 16.9408 - MinusLogProbMetric: 16.9408 - val_loss: 17.8539 - val_MinusLogProbMetric: 17.8539 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 128/1000
2023-09-26 14:33:42.377 
Epoch 128/1000 
	 loss: 16.7942, MinusLogProbMetric: 16.7942, val_loss: 18.5844, val_MinusLogProbMetric: 18.5844

Epoch 128: val_loss did not improve from 17.52244
196/196 - 33s - loss: 16.7942 - MinusLogProbMetric: 16.7942 - val_loss: 18.5844 - val_MinusLogProbMetric: 18.5844 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 129/1000
2023-09-26 14:34:15.159 
Epoch 129/1000 
	 loss: 16.7840, MinusLogProbMetric: 16.7840, val_loss: 17.9336, val_MinusLogProbMetric: 17.9336

Epoch 129: val_loss did not improve from 17.52244
196/196 - 33s - loss: 16.7840 - MinusLogProbMetric: 16.7840 - val_loss: 17.9336 - val_MinusLogProbMetric: 17.9336 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 130/1000
2023-09-26 14:34:48.842 
Epoch 130/1000 
	 loss: 16.7903, MinusLogProbMetric: 16.7903, val_loss: 18.2143, val_MinusLogProbMetric: 18.2143

Epoch 130: val_loss did not improve from 17.52244
196/196 - 34s - loss: 16.7903 - MinusLogProbMetric: 16.7903 - val_loss: 18.2143 - val_MinusLogProbMetric: 18.2143 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 131/1000
2023-09-26 14:35:23.061 
Epoch 131/1000 
	 loss: 16.7335, MinusLogProbMetric: 16.7335, val_loss: 17.5841, val_MinusLogProbMetric: 17.5841

Epoch 131: val_loss did not improve from 17.52244
196/196 - 34s - loss: 16.7335 - MinusLogProbMetric: 16.7335 - val_loss: 17.5841 - val_MinusLogProbMetric: 17.5841 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 132/1000
2023-09-26 14:35:56.025 
Epoch 132/1000 
	 loss: 16.7006, MinusLogProbMetric: 16.7006, val_loss: 17.8735, val_MinusLogProbMetric: 17.8735

Epoch 132: val_loss did not improve from 17.52244
196/196 - 33s - loss: 16.7006 - MinusLogProbMetric: 16.7006 - val_loss: 17.8735 - val_MinusLogProbMetric: 17.8735 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 133/1000
2023-09-26 14:36:29.323 
Epoch 133/1000 
	 loss: 16.7384, MinusLogProbMetric: 16.7384, val_loss: 17.6771, val_MinusLogProbMetric: 17.6771

Epoch 133: val_loss did not improve from 17.52244
196/196 - 33s - loss: 16.7384 - MinusLogProbMetric: 16.7384 - val_loss: 17.6771 - val_MinusLogProbMetric: 17.6771 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 134/1000
2023-09-26 14:37:02.902 
Epoch 134/1000 
	 loss: 16.8073, MinusLogProbMetric: 16.8073, val_loss: 17.6273, val_MinusLogProbMetric: 17.6273

Epoch 134: val_loss did not improve from 17.52244
196/196 - 34s - loss: 16.8073 - MinusLogProbMetric: 16.8073 - val_loss: 17.6273 - val_MinusLogProbMetric: 17.6273 - lr: 0.0010 - 34s/epoch - 171ms/step
Epoch 135/1000
2023-09-26 14:37:36.823 
Epoch 135/1000 
	 loss: 16.6712, MinusLogProbMetric: 16.6712, val_loss: 18.1920, val_MinusLogProbMetric: 18.1920

Epoch 135: val_loss did not improve from 17.52244
196/196 - 34s - loss: 16.6712 - MinusLogProbMetric: 16.6712 - val_loss: 18.1920 - val_MinusLogProbMetric: 18.1920 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 136/1000
2023-09-26 14:38:10.497 
Epoch 136/1000 
	 loss: 16.7271, MinusLogProbMetric: 16.7271, val_loss: 17.6327, val_MinusLogProbMetric: 17.6327

Epoch 136: val_loss did not improve from 17.52244
196/196 - 34s - loss: 16.7271 - MinusLogProbMetric: 16.7271 - val_loss: 17.6327 - val_MinusLogProbMetric: 17.6327 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 137/1000
2023-09-26 14:38:43.930 
Epoch 137/1000 
	 loss: 16.7288, MinusLogProbMetric: 16.7288, val_loss: 18.0148, val_MinusLogProbMetric: 18.0148

Epoch 137: val_loss did not improve from 17.52244
196/196 - 33s - loss: 16.7288 - MinusLogProbMetric: 16.7288 - val_loss: 18.0148 - val_MinusLogProbMetric: 18.0148 - lr: 0.0010 - 33s/epoch - 171ms/step
Epoch 138/1000
2023-09-26 14:39:17.613 
Epoch 138/1000 
	 loss: 16.6822, MinusLogProbMetric: 16.6822, val_loss: 17.9393, val_MinusLogProbMetric: 17.9393

Epoch 138: val_loss did not improve from 17.52244
196/196 - 34s - loss: 16.6822 - MinusLogProbMetric: 16.6822 - val_loss: 17.9393 - val_MinusLogProbMetric: 17.9393 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 139/1000
2023-09-26 14:39:50.839 
Epoch 139/1000 
	 loss: 16.7042, MinusLogProbMetric: 16.7042, val_loss: 17.7940, val_MinusLogProbMetric: 17.7940

Epoch 139: val_loss did not improve from 17.52244
196/196 - 33s - loss: 16.7042 - MinusLogProbMetric: 16.7042 - val_loss: 17.7940 - val_MinusLogProbMetric: 17.7940 - lr: 0.0010 - 33s/epoch - 169ms/step
Epoch 140/1000
2023-09-26 14:40:23.956 
Epoch 140/1000 
	 loss: 16.6085, MinusLogProbMetric: 16.6085, val_loss: 17.6253, val_MinusLogProbMetric: 17.6253

Epoch 140: val_loss did not improve from 17.52244
196/196 - 33s - loss: 16.6085 - MinusLogProbMetric: 16.6085 - val_loss: 17.6253 - val_MinusLogProbMetric: 17.6253 - lr: 0.0010 - 33s/epoch - 169ms/step
Epoch 141/1000
2023-09-26 14:40:57.358 
Epoch 141/1000 
	 loss: 16.6579, MinusLogProbMetric: 16.6579, val_loss: 18.5334, val_MinusLogProbMetric: 18.5334

Epoch 141: val_loss did not improve from 17.52244
196/196 - 33s - loss: 16.6579 - MinusLogProbMetric: 16.6579 - val_loss: 18.5334 - val_MinusLogProbMetric: 18.5334 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 142/1000
2023-09-26 14:41:30.693 
Epoch 142/1000 
	 loss: 16.7937, MinusLogProbMetric: 16.7937, val_loss: 17.8640, val_MinusLogProbMetric: 17.8640

Epoch 142: val_loss did not improve from 17.52244
196/196 - 33s - loss: 16.7937 - MinusLogProbMetric: 16.7937 - val_loss: 17.8640 - val_MinusLogProbMetric: 17.8640 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 143/1000
2023-09-26 14:42:04.811 
Epoch 143/1000 
	 loss: 16.5726, MinusLogProbMetric: 16.5726, val_loss: 17.6775, val_MinusLogProbMetric: 17.6775

Epoch 143: val_loss did not improve from 17.52244
196/196 - 34s - loss: 16.5726 - MinusLogProbMetric: 16.5726 - val_loss: 17.6775 - val_MinusLogProbMetric: 17.6775 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 144/1000
2023-09-26 14:42:38.860 
Epoch 144/1000 
	 loss: 16.6872, MinusLogProbMetric: 16.6872, val_loss: 17.7308, val_MinusLogProbMetric: 17.7308

Epoch 144: val_loss did not improve from 17.52244
196/196 - 34s - loss: 16.6872 - MinusLogProbMetric: 16.6872 - val_loss: 17.7308 - val_MinusLogProbMetric: 17.7308 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 145/1000
2023-09-26 14:43:12.414 
Epoch 145/1000 
	 loss: 16.6864, MinusLogProbMetric: 16.6864, val_loss: 17.8687, val_MinusLogProbMetric: 17.8687

Epoch 145: val_loss did not improve from 17.52244
196/196 - 34s - loss: 16.6864 - MinusLogProbMetric: 16.6864 - val_loss: 17.8687 - val_MinusLogProbMetric: 17.8687 - lr: 0.0010 - 34s/epoch - 171ms/step
Epoch 146/1000
2023-09-26 14:43:45.848 
Epoch 146/1000 
	 loss: 16.6387, MinusLogProbMetric: 16.6387, val_loss: 17.9199, val_MinusLogProbMetric: 17.9199

Epoch 146: val_loss did not improve from 17.52244
196/196 - 33s - loss: 16.6387 - MinusLogProbMetric: 16.6387 - val_loss: 17.9199 - val_MinusLogProbMetric: 17.9199 - lr: 0.0010 - 33s/epoch - 171ms/step
Epoch 147/1000
2023-09-26 14:44:20.089 
Epoch 147/1000 
	 loss: 16.6251, MinusLogProbMetric: 16.6251, val_loss: 18.0957, val_MinusLogProbMetric: 18.0957

Epoch 147: val_loss did not improve from 17.52244
196/196 - 34s - loss: 16.6251 - MinusLogProbMetric: 16.6251 - val_loss: 18.0957 - val_MinusLogProbMetric: 18.0957 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 148/1000
2023-09-26 14:44:53.470 
Epoch 148/1000 
	 loss: 16.5237, MinusLogProbMetric: 16.5237, val_loss: 17.8173, val_MinusLogProbMetric: 17.8173

Epoch 148: val_loss did not improve from 17.52244
196/196 - 33s - loss: 16.5237 - MinusLogProbMetric: 16.5237 - val_loss: 17.8173 - val_MinusLogProbMetric: 17.8173 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 149/1000
2023-09-26 14:45:26.612 
Epoch 149/1000 
	 loss: 16.5530, MinusLogProbMetric: 16.5530, val_loss: 17.7524, val_MinusLogProbMetric: 17.7524

Epoch 149: val_loss did not improve from 17.52244
196/196 - 33s - loss: 16.5530 - MinusLogProbMetric: 16.5530 - val_loss: 17.7524 - val_MinusLogProbMetric: 17.7524 - lr: 0.0010 - 33s/epoch - 169ms/step
Epoch 150/1000
2023-09-26 14:46:00.158 
Epoch 150/1000 
	 loss: 16.5721, MinusLogProbMetric: 16.5721, val_loss: 17.9295, val_MinusLogProbMetric: 17.9295

Epoch 150: val_loss did not improve from 17.52244
196/196 - 34s - loss: 16.5721 - MinusLogProbMetric: 16.5721 - val_loss: 17.9295 - val_MinusLogProbMetric: 17.9295 - lr: 0.0010 - 34s/epoch - 171ms/step
Epoch 151/1000
2023-09-26 14:46:33.065 
Epoch 151/1000 
	 loss: 16.5460, MinusLogProbMetric: 16.5460, val_loss: 17.6911, val_MinusLogProbMetric: 17.6911

Epoch 151: val_loss did not improve from 17.52244
196/196 - 33s - loss: 16.5460 - MinusLogProbMetric: 16.5460 - val_loss: 17.6911 - val_MinusLogProbMetric: 17.6911 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 152/1000
2023-09-26 14:47:06.267 
Epoch 152/1000 
	 loss: 16.5582, MinusLogProbMetric: 16.5582, val_loss: 17.6514, val_MinusLogProbMetric: 17.6514

Epoch 152: val_loss did not improve from 17.52244
196/196 - 33s - loss: 16.5582 - MinusLogProbMetric: 16.5582 - val_loss: 17.6514 - val_MinusLogProbMetric: 17.6514 - lr: 0.0010 - 33s/epoch - 169ms/step
Epoch 153/1000
2023-09-26 14:47:39.190 
Epoch 153/1000 
	 loss: 16.5206, MinusLogProbMetric: 16.5206, val_loss: 18.0370, val_MinusLogProbMetric: 18.0370

Epoch 153: val_loss did not improve from 17.52244
196/196 - 33s - loss: 16.5206 - MinusLogProbMetric: 16.5206 - val_loss: 18.0370 - val_MinusLogProbMetric: 18.0370 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 154/1000
2023-09-26 14:48:12.318 
Epoch 154/1000 
	 loss: 16.5414, MinusLogProbMetric: 16.5414, val_loss: 17.9370, val_MinusLogProbMetric: 17.9370

Epoch 154: val_loss did not improve from 17.52244
196/196 - 33s - loss: 16.5414 - MinusLogProbMetric: 16.5414 - val_loss: 17.9370 - val_MinusLogProbMetric: 17.9370 - lr: 0.0010 - 33s/epoch - 169ms/step
Epoch 155/1000
2023-09-26 14:48:46.946 
Epoch 155/1000 
	 loss: 16.4618, MinusLogProbMetric: 16.4618, val_loss: 17.7344, val_MinusLogProbMetric: 17.7344

Epoch 155: val_loss did not improve from 17.52244
196/196 - 35s - loss: 16.4618 - MinusLogProbMetric: 16.4618 - val_loss: 17.7344 - val_MinusLogProbMetric: 17.7344 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 156/1000
2023-09-26 14:49:20.718 
Epoch 156/1000 
	 loss: 16.5319, MinusLogProbMetric: 16.5319, val_loss: 17.7832, val_MinusLogProbMetric: 17.7832

Epoch 156: val_loss did not improve from 17.52244
196/196 - 34s - loss: 16.5319 - MinusLogProbMetric: 16.5319 - val_loss: 17.7832 - val_MinusLogProbMetric: 17.7832 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 157/1000
2023-09-26 14:49:54.986 
Epoch 157/1000 
	 loss: 16.5795, MinusLogProbMetric: 16.5795, val_loss: 18.1103, val_MinusLogProbMetric: 18.1103

Epoch 157: val_loss did not improve from 17.52244
196/196 - 34s - loss: 16.5795 - MinusLogProbMetric: 16.5795 - val_loss: 18.1103 - val_MinusLogProbMetric: 18.1103 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 158/1000
2023-09-26 14:50:29.952 
Epoch 158/1000 
	 loss: 16.4939, MinusLogProbMetric: 16.4939, val_loss: 17.6332, val_MinusLogProbMetric: 17.6332

Epoch 158: val_loss did not improve from 17.52244
196/196 - 35s - loss: 16.4939 - MinusLogProbMetric: 16.4939 - val_loss: 17.6332 - val_MinusLogProbMetric: 17.6332 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 159/1000
2023-09-26 14:51:03.378 
Epoch 159/1000 
	 loss: 16.5107, MinusLogProbMetric: 16.5107, val_loss: 17.8121, val_MinusLogProbMetric: 17.8121

Epoch 159: val_loss did not improve from 17.52244
196/196 - 33s - loss: 16.5107 - MinusLogProbMetric: 16.5107 - val_loss: 17.8121 - val_MinusLogProbMetric: 17.8121 - lr: 0.0010 - 33s/epoch - 171ms/step
Epoch 160/1000
2023-09-26 14:51:37.879 
Epoch 160/1000 
	 loss: 16.4209, MinusLogProbMetric: 16.4209, val_loss: 17.7528, val_MinusLogProbMetric: 17.7528

Epoch 160: val_loss did not improve from 17.52244
196/196 - 35s - loss: 16.4209 - MinusLogProbMetric: 16.4209 - val_loss: 17.7528 - val_MinusLogProbMetric: 17.7528 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 161/1000
2023-09-26 14:52:11.403 
Epoch 161/1000 
	 loss: 16.4861, MinusLogProbMetric: 16.4861, val_loss: 17.5860, val_MinusLogProbMetric: 17.5860

Epoch 161: val_loss did not improve from 17.52244
196/196 - 34s - loss: 16.4861 - MinusLogProbMetric: 16.4861 - val_loss: 17.5860 - val_MinusLogProbMetric: 17.5860 - lr: 0.0010 - 34s/epoch - 171ms/step
Epoch 162/1000
2023-09-26 14:52:45.623 
Epoch 162/1000 
	 loss: 16.4893, MinusLogProbMetric: 16.4893, val_loss: 17.7193, val_MinusLogProbMetric: 17.7193

Epoch 162: val_loss did not improve from 17.52244
196/196 - 34s - loss: 16.4893 - MinusLogProbMetric: 16.4893 - val_loss: 17.7193 - val_MinusLogProbMetric: 17.7193 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 163/1000
2023-09-26 14:53:19.197 
Epoch 163/1000 
	 loss: 16.4123, MinusLogProbMetric: 16.4123, val_loss: 17.7817, val_MinusLogProbMetric: 17.7817

Epoch 163: val_loss did not improve from 17.52244
196/196 - 34s - loss: 16.4123 - MinusLogProbMetric: 16.4123 - val_loss: 17.7817 - val_MinusLogProbMetric: 17.7817 - lr: 0.0010 - 34s/epoch - 171ms/step
Epoch 164/1000
2023-09-26 14:53:52.555 
Epoch 164/1000 
	 loss: 16.3950, MinusLogProbMetric: 16.3950, val_loss: 17.7995, val_MinusLogProbMetric: 17.7995

Epoch 164: val_loss did not improve from 17.52244
196/196 - 33s - loss: 16.3950 - MinusLogProbMetric: 16.3950 - val_loss: 17.7995 - val_MinusLogProbMetric: 17.7995 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 165/1000
2023-09-26 14:54:24.995 
Epoch 165/1000 
	 loss: 16.4420, MinusLogProbMetric: 16.4420, val_loss: 17.7535, val_MinusLogProbMetric: 17.7535

Epoch 165: val_loss did not improve from 17.52244
196/196 - 32s - loss: 16.4420 - MinusLogProbMetric: 16.4420 - val_loss: 17.7535 - val_MinusLogProbMetric: 17.7535 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 166/1000
2023-09-26 14:54:58.275 
Epoch 166/1000 
	 loss: 16.5113, MinusLogProbMetric: 16.5113, val_loss: 17.7695, val_MinusLogProbMetric: 17.7695

Epoch 166: val_loss did not improve from 17.52244
196/196 - 33s - loss: 16.5113 - MinusLogProbMetric: 16.5113 - val_loss: 17.7695 - val_MinusLogProbMetric: 17.7695 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 167/1000
2023-09-26 14:55:31.627 
Epoch 167/1000 
	 loss: 16.4079, MinusLogProbMetric: 16.4079, val_loss: 18.1142, val_MinusLogProbMetric: 18.1142

Epoch 167: val_loss did not improve from 17.52244
196/196 - 33s - loss: 16.4079 - MinusLogProbMetric: 16.4079 - val_loss: 18.1142 - val_MinusLogProbMetric: 18.1142 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 168/1000
2023-09-26 14:56:04.742 
Epoch 168/1000 
	 loss: 16.3939, MinusLogProbMetric: 16.3939, val_loss: 17.9002, val_MinusLogProbMetric: 17.9002

Epoch 168: val_loss did not improve from 17.52244
196/196 - 33s - loss: 16.3939 - MinusLogProbMetric: 16.3939 - val_loss: 17.9002 - val_MinusLogProbMetric: 17.9002 - lr: 0.0010 - 33s/epoch - 169ms/step
Epoch 169/1000
2023-09-26 14:56:36.956 
Epoch 169/1000 
	 loss: 16.3938, MinusLogProbMetric: 16.3938, val_loss: 17.9060, val_MinusLogProbMetric: 17.9060

Epoch 169: val_loss did not improve from 17.52244
196/196 - 32s - loss: 16.3938 - MinusLogProbMetric: 16.3938 - val_loss: 17.9060 - val_MinusLogProbMetric: 17.9060 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 170/1000
2023-09-26 14:57:09.632 
Epoch 170/1000 
	 loss: 16.3541, MinusLogProbMetric: 16.3541, val_loss: 18.0741, val_MinusLogProbMetric: 18.0741

Epoch 170: val_loss did not improve from 17.52244
196/196 - 33s - loss: 16.3541 - MinusLogProbMetric: 16.3541 - val_loss: 18.0741 - val_MinusLogProbMetric: 18.0741 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 171/1000
2023-09-26 14:57:42.466 
Epoch 171/1000 
	 loss: 16.3991, MinusLogProbMetric: 16.3991, val_loss: 17.7219, val_MinusLogProbMetric: 17.7219

Epoch 171: val_loss did not improve from 17.52244
196/196 - 33s - loss: 16.3991 - MinusLogProbMetric: 16.3991 - val_loss: 17.7219 - val_MinusLogProbMetric: 17.7219 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 172/1000
2023-09-26 14:58:14.853 
Epoch 172/1000 
	 loss: 16.3688, MinusLogProbMetric: 16.3688, val_loss: 17.8406, val_MinusLogProbMetric: 17.8406

Epoch 172: val_loss did not improve from 17.52244
196/196 - 32s - loss: 16.3688 - MinusLogProbMetric: 16.3688 - val_loss: 17.8406 - val_MinusLogProbMetric: 17.8406 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 173/1000
2023-09-26 14:58:47.573 
Epoch 173/1000 
	 loss: 16.4073, MinusLogProbMetric: 16.4073, val_loss: 17.7834, val_MinusLogProbMetric: 17.7834

Epoch 173: val_loss did not improve from 17.52244
196/196 - 33s - loss: 16.4073 - MinusLogProbMetric: 16.4073 - val_loss: 17.7834 - val_MinusLogProbMetric: 17.7834 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 174/1000
2023-09-26 14:59:20.254 
Epoch 174/1000 
	 loss: 16.3798, MinusLogProbMetric: 16.3798, val_loss: 18.0369, val_MinusLogProbMetric: 18.0369

Epoch 174: val_loss did not improve from 17.52244
196/196 - 33s - loss: 16.3798 - MinusLogProbMetric: 16.3798 - val_loss: 18.0369 - val_MinusLogProbMetric: 18.0369 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 175/1000
2023-09-26 14:59:53.277 
Epoch 175/1000 
	 loss: 16.3539, MinusLogProbMetric: 16.3539, val_loss: 17.8812, val_MinusLogProbMetric: 17.8812

Epoch 175: val_loss did not improve from 17.52244
196/196 - 33s - loss: 16.3539 - MinusLogProbMetric: 16.3539 - val_loss: 17.8812 - val_MinusLogProbMetric: 17.8812 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 176/1000
2023-09-26 15:00:25.800 
Epoch 176/1000 
	 loss: 16.4074, MinusLogProbMetric: 16.4074, val_loss: 18.4400, val_MinusLogProbMetric: 18.4400

Epoch 176: val_loss did not improve from 17.52244
196/196 - 33s - loss: 16.4074 - MinusLogProbMetric: 16.4074 - val_loss: 18.4400 - val_MinusLogProbMetric: 18.4400 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 177/1000
2023-09-26 15:00:58.712 
Epoch 177/1000 
	 loss: 15.9948, MinusLogProbMetric: 15.9948, val_loss: 17.7478, val_MinusLogProbMetric: 17.7478

Epoch 177: val_loss did not improve from 17.52244
196/196 - 33s - loss: 15.9948 - MinusLogProbMetric: 15.9948 - val_loss: 17.7478 - val_MinusLogProbMetric: 17.7478 - lr: 5.0000e-04 - 33s/epoch - 168ms/step
Epoch 178/1000
2023-09-26 15:01:32.245 
Epoch 178/1000 
	 loss: 15.9287, MinusLogProbMetric: 15.9287, val_loss: 17.9149, val_MinusLogProbMetric: 17.9149

Epoch 178: val_loss did not improve from 17.52244
196/196 - 34s - loss: 15.9287 - MinusLogProbMetric: 15.9287 - val_loss: 17.9149 - val_MinusLogProbMetric: 17.9149 - lr: 5.0000e-04 - 34s/epoch - 171ms/step
Epoch 179/1000
2023-09-26 15:02:05.263 
Epoch 179/1000 
	 loss: 16.0010, MinusLogProbMetric: 16.0010, val_loss: 17.6642, val_MinusLogProbMetric: 17.6642

Epoch 179: val_loss did not improve from 17.52244
196/196 - 33s - loss: 16.0010 - MinusLogProbMetric: 16.0010 - val_loss: 17.6642 - val_MinusLogProbMetric: 17.6642 - lr: 5.0000e-04 - 33s/epoch - 168ms/step
Epoch 180/1000
2023-09-26 15:02:38.568 
Epoch 180/1000 
	 loss: 15.9164, MinusLogProbMetric: 15.9164, val_loss: 17.6207, val_MinusLogProbMetric: 17.6207

Epoch 180: val_loss did not improve from 17.52244
196/196 - 33s - loss: 15.9164 - MinusLogProbMetric: 15.9164 - val_loss: 17.6207 - val_MinusLogProbMetric: 17.6207 - lr: 5.0000e-04 - 33s/epoch - 170ms/step
Epoch 181/1000
2023-09-26 15:03:12.308 
Epoch 181/1000 
	 loss: 15.9469, MinusLogProbMetric: 15.9469, val_loss: 17.6629, val_MinusLogProbMetric: 17.6629

Epoch 181: val_loss did not improve from 17.52244
196/196 - 34s - loss: 15.9469 - MinusLogProbMetric: 15.9469 - val_loss: 17.6629 - val_MinusLogProbMetric: 17.6629 - lr: 5.0000e-04 - 34s/epoch - 172ms/step
Epoch 182/1000
2023-09-26 15:03:46.468 
Epoch 182/1000 
	 loss: 15.9438, MinusLogProbMetric: 15.9438, val_loss: 17.7041, val_MinusLogProbMetric: 17.7041

Epoch 182: val_loss did not improve from 17.52244
196/196 - 34s - loss: 15.9438 - MinusLogProbMetric: 15.9438 - val_loss: 17.7041 - val_MinusLogProbMetric: 17.7041 - lr: 5.0000e-04 - 34s/epoch - 174ms/step
Epoch 183/1000
2023-09-26 15:04:20.199 
Epoch 183/1000 
	 loss: 15.9440, MinusLogProbMetric: 15.9440, val_loss: 17.6767, val_MinusLogProbMetric: 17.6767

Epoch 183: val_loss did not improve from 17.52244
196/196 - 34s - loss: 15.9440 - MinusLogProbMetric: 15.9440 - val_loss: 17.6767 - val_MinusLogProbMetric: 17.6767 - lr: 5.0000e-04 - 34s/epoch - 172ms/step
Epoch 184/1000
2023-09-26 15:04:53.888 
Epoch 184/1000 
	 loss: 15.9378, MinusLogProbMetric: 15.9378, val_loss: 17.6656, val_MinusLogProbMetric: 17.6656

Epoch 184: val_loss did not improve from 17.52244
196/196 - 34s - loss: 15.9378 - MinusLogProbMetric: 15.9378 - val_loss: 17.6656 - val_MinusLogProbMetric: 17.6656 - lr: 5.0000e-04 - 34s/epoch - 172ms/step
Epoch 185/1000
2023-09-26 15:05:27.926 
Epoch 185/1000 
	 loss: 15.8890, MinusLogProbMetric: 15.8890, val_loss: 17.6570, val_MinusLogProbMetric: 17.6570

Epoch 185: val_loss did not improve from 17.52244
196/196 - 34s - loss: 15.8890 - MinusLogProbMetric: 15.8890 - val_loss: 17.6570 - val_MinusLogProbMetric: 17.6570 - lr: 5.0000e-04 - 34s/epoch - 174ms/step
Epoch 186/1000
2023-09-26 15:06:01.789 
Epoch 186/1000 
	 loss: 15.9339, MinusLogProbMetric: 15.9339, val_loss: 17.7260, val_MinusLogProbMetric: 17.7260

Epoch 186: val_loss did not improve from 17.52244
196/196 - 34s - loss: 15.9339 - MinusLogProbMetric: 15.9339 - val_loss: 17.7260 - val_MinusLogProbMetric: 17.7260 - lr: 5.0000e-04 - 34s/epoch - 173ms/step
Epoch 187/1000
2023-09-26 15:06:36.017 
Epoch 187/1000 
	 loss: 15.9085, MinusLogProbMetric: 15.9085, val_loss: 17.8300, val_MinusLogProbMetric: 17.8300

Epoch 187: val_loss did not improve from 17.52244
196/196 - 34s - loss: 15.9085 - MinusLogProbMetric: 15.9085 - val_loss: 17.8300 - val_MinusLogProbMetric: 17.8300 - lr: 5.0000e-04 - 34s/epoch - 175ms/step
Epoch 188/1000
2023-09-26 15:07:09.558 
Epoch 188/1000 
	 loss: 15.9760, MinusLogProbMetric: 15.9760, val_loss: 17.7368, val_MinusLogProbMetric: 17.7368

Epoch 188: val_loss did not improve from 17.52244
196/196 - 34s - loss: 15.9760 - MinusLogProbMetric: 15.9760 - val_loss: 17.7368 - val_MinusLogProbMetric: 17.7368 - lr: 5.0000e-04 - 34s/epoch - 171ms/step
Epoch 189/1000
2023-09-26 15:07:43.483 
Epoch 189/1000 
	 loss: 15.9031, MinusLogProbMetric: 15.9031, val_loss: 18.0713, val_MinusLogProbMetric: 18.0713

Epoch 189: val_loss did not improve from 17.52244
196/196 - 34s - loss: 15.9031 - MinusLogProbMetric: 15.9031 - val_loss: 18.0713 - val_MinusLogProbMetric: 18.0713 - lr: 5.0000e-04 - 34s/epoch - 173ms/step
Epoch 190/1000
2023-09-26 15:08:17.782 
Epoch 190/1000 
	 loss: 15.8821, MinusLogProbMetric: 15.8821, val_loss: 17.6755, val_MinusLogProbMetric: 17.6755

Epoch 190: val_loss did not improve from 17.52244
196/196 - 34s - loss: 15.8821 - MinusLogProbMetric: 15.8821 - val_loss: 17.6755 - val_MinusLogProbMetric: 17.6755 - lr: 5.0000e-04 - 34s/epoch - 175ms/step
Epoch 191/1000
2023-09-26 15:08:52.755 
Epoch 191/1000 
	 loss: 15.9012, MinusLogProbMetric: 15.9012, val_loss: 17.6853, val_MinusLogProbMetric: 17.6853

Epoch 191: val_loss did not improve from 17.52244
196/196 - 35s - loss: 15.9012 - MinusLogProbMetric: 15.9012 - val_loss: 17.6853 - val_MinusLogProbMetric: 17.6853 - lr: 5.0000e-04 - 35s/epoch - 178ms/step
Epoch 192/1000
2023-09-26 15:09:25.978 
Epoch 192/1000 
	 loss: 15.9006, MinusLogProbMetric: 15.9006, val_loss: 17.7003, val_MinusLogProbMetric: 17.7003

Epoch 192: val_loss did not improve from 17.52244
196/196 - 33s - loss: 15.9006 - MinusLogProbMetric: 15.9006 - val_loss: 17.7003 - val_MinusLogProbMetric: 17.7003 - lr: 5.0000e-04 - 33s/epoch - 170ms/step
Epoch 193/1000
2023-09-26 15:09:59.550 
Epoch 193/1000 
	 loss: 15.8893, MinusLogProbMetric: 15.8893, val_loss: 17.7546, val_MinusLogProbMetric: 17.7546

Epoch 193: val_loss did not improve from 17.52244
196/196 - 34s - loss: 15.8893 - MinusLogProbMetric: 15.8893 - val_loss: 17.7546 - val_MinusLogProbMetric: 17.7546 - lr: 5.0000e-04 - 34s/epoch - 171ms/step
Epoch 194/1000
2023-09-26 15:10:33.447 
Epoch 194/1000 
	 loss: 15.8919, MinusLogProbMetric: 15.8919, val_loss: 17.7287, val_MinusLogProbMetric: 17.7287

Epoch 194: val_loss did not improve from 17.52244
196/196 - 34s - loss: 15.8919 - MinusLogProbMetric: 15.8919 - val_loss: 17.7287 - val_MinusLogProbMetric: 17.7287 - lr: 5.0000e-04 - 34s/epoch - 173ms/step
Epoch 195/1000
2023-09-26 15:11:07.094 
Epoch 195/1000 
	 loss: 15.8764, MinusLogProbMetric: 15.8764, val_loss: 17.8578, val_MinusLogProbMetric: 17.8578

Epoch 195: val_loss did not improve from 17.52244
196/196 - 34s - loss: 15.8764 - MinusLogProbMetric: 15.8764 - val_loss: 17.8578 - val_MinusLogProbMetric: 17.8578 - lr: 5.0000e-04 - 34s/epoch - 172ms/step
Epoch 196/1000
2023-09-26 15:11:40.988 
Epoch 196/1000 
	 loss: 15.8956, MinusLogProbMetric: 15.8956, val_loss: 17.7355, val_MinusLogProbMetric: 17.7355

Epoch 196: val_loss did not improve from 17.52244
196/196 - 34s - loss: 15.8956 - MinusLogProbMetric: 15.8956 - val_loss: 17.7355 - val_MinusLogProbMetric: 17.7355 - lr: 5.0000e-04 - 34s/epoch - 173ms/step
Epoch 197/1000
2023-09-26 15:12:14.584 
Epoch 197/1000 
	 loss: 15.9009, MinusLogProbMetric: 15.9009, val_loss: 17.7056, val_MinusLogProbMetric: 17.7056

Epoch 197: val_loss did not improve from 17.52244
196/196 - 34s - loss: 15.9009 - MinusLogProbMetric: 15.9009 - val_loss: 17.7056 - val_MinusLogProbMetric: 17.7056 - lr: 5.0000e-04 - 34s/epoch - 171ms/step
Epoch 198/1000
2023-09-26 15:12:48.082 
Epoch 198/1000 
	 loss: 15.8725, MinusLogProbMetric: 15.8725, val_loss: 17.9805, val_MinusLogProbMetric: 17.9805

Epoch 198: val_loss did not improve from 17.52244
196/196 - 33s - loss: 15.8725 - MinusLogProbMetric: 15.8725 - val_loss: 17.9805 - val_MinusLogProbMetric: 17.9805 - lr: 5.0000e-04 - 33s/epoch - 171ms/step
Epoch 199/1000
2023-09-26 15:13:22.019 
Epoch 199/1000 
	 loss: 15.8764, MinusLogProbMetric: 15.8764, val_loss: 17.8115, val_MinusLogProbMetric: 17.8115

Epoch 199: val_loss did not improve from 17.52244
196/196 - 34s - loss: 15.8764 - MinusLogProbMetric: 15.8764 - val_loss: 17.8115 - val_MinusLogProbMetric: 17.8115 - lr: 5.0000e-04 - 34s/epoch - 173ms/step
Epoch 200/1000
2023-09-26 15:13:55.971 
Epoch 200/1000 
	 loss: 15.8853, MinusLogProbMetric: 15.8853, val_loss: 17.8033, val_MinusLogProbMetric: 17.8033

Epoch 200: val_loss did not improve from 17.52244
196/196 - 34s - loss: 15.8853 - MinusLogProbMetric: 15.8853 - val_loss: 17.8033 - val_MinusLogProbMetric: 17.8033 - lr: 5.0000e-04 - 34s/epoch - 173ms/step
Epoch 201/1000
2023-09-26 15:14:29.591 
Epoch 201/1000 
	 loss: 15.8457, MinusLogProbMetric: 15.8457, val_loss: 17.7589, val_MinusLogProbMetric: 17.7589

Epoch 201: val_loss did not improve from 17.52244
196/196 - 34s - loss: 15.8457 - MinusLogProbMetric: 15.8457 - val_loss: 17.7589 - val_MinusLogProbMetric: 17.7589 - lr: 5.0000e-04 - 34s/epoch - 172ms/step
Epoch 202/1000
2023-09-26 15:15:02.815 
Epoch 202/1000 
	 loss: 15.8693, MinusLogProbMetric: 15.8693, val_loss: 17.7629, val_MinusLogProbMetric: 17.7629

Epoch 202: val_loss did not improve from 17.52244
196/196 - 33s - loss: 15.8693 - MinusLogProbMetric: 15.8693 - val_loss: 17.7629 - val_MinusLogProbMetric: 17.7629 - lr: 5.0000e-04 - 33s/epoch - 169ms/step
Epoch 203/1000
2023-09-26 15:15:36.411 
Epoch 203/1000 
	 loss: 15.8346, MinusLogProbMetric: 15.8346, val_loss: 17.8875, val_MinusLogProbMetric: 17.8875

Epoch 203: val_loss did not improve from 17.52244
196/196 - 34s - loss: 15.8346 - MinusLogProbMetric: 15.8346 - val_loss: 17.8875 - val_MinusLogProbMetric: 17.8875 - lr: 5.0000e-04 - 34s/epoch - 171ms/step
Epoch 204/1000
2023-09-26 15:16:10.563 
Epoch 204/1000 
	 loss: 15.8710, MinusLogProbMetric: 15.8710, val_loss: 17.7864, val_MinusLogProbMetric: 17.7864

Epoch 204: val_loss did not improve from 17.52244
196/196 - 34s - loss: 15.8710 - MinusLogProbMetric: 15.8710 - val_loss: 17.7864 - val_MinusLogProbMetric: 17.7864 - lr: 5.0000e-04 - 34s/epoch - 174ms/step
Epoch 205/1000
2023-09-26 15:16:45.010 
Epoch 205/1000 
	 loss: 15.8430, MinusLogProbMetric: 15.8430, val_loss: 17.8358, val_MinusLogProbMetric: 17.8358

Epoch 205: val_loss did not improve from 17.52244
196/196 - 34s - loss: 15.8430 - MinusLogProbMetric: 15.8430 - val_loss: 17.8358 - val_MinusLogProbMetric: 17.8358 - lr: 5.0000e-04 - 34s/epoch - 176ms/step
Epoch 206/1000
2023-09-26 15:17:18.399 
Epoch 206/1000 
	 loss: 15.8716, MinusLogProbMetric: 15.8716, val_loss: 18.1471, val_MinusLogProbMetric: 18.1471

Epoch 206: val_loss did not improve from 17.52244
196/196 - 33s - loss: 15.8716 - MinusLogProbMetric: 15.8716 - val_loss: 18.1471 - val_MinusLogProbMetric: 18.1471 - lr: 5.0000e-04 - 33s/epoch - 170ms/step
Epoch 207/1000
2023-09-26 15:17:51.887 
Epoch 207/1000 
	 loss: 15.8129, MinusLogProbMetric: 15.8129, val_loss: 17.8996, val_MinusLogProbMetric: 17.8996

Epoch 207: val_loss did not improve from 17.52244
196/196 - 33s - loss: 15.8129 - MinusLogProbMetric: 15.8129 - val_loss: 17.8996 - val_MinusLogProbMetric: 17.8996 - lr: 5.0000e-04 - 33s/epoch - 171ms/step
Epoch 208/1000
2023-09-26 15:18:25.629 
Epoch 208/1000 
	 loss: 15.8717, MinusLogProbMetric: 15.8717, val_loss: 17.9845, val_MinusLogProbMetric: 17.9845

Epoch 208: val_loss did not improve from 17.52244
196/196 - 34s - loss: 15.8717 - MinusLogProbMetric: 15.8717 - val_loss: 17.9845 - val_MinusLogProbMetric: 17.9845 - lr: 5.0000e-04 - 34s/epoch - 172ms/step
Epoch 209/1000
2023-09-26 15:18:58.764 
Epoch 209/1000 
	 loss: 15.8234, MinusLogProbMetric: 15.8234, val_loss: 17.9264, val_MinusLogProbMetric: 17.9264

Epoch 209: val_loss did not improve from 17.52244
196/196 - 33s - loss: 15.8234 - MinusLogProbMetric: 15.8234 - val_loss: 17.9264 - val_MinusLogProbMetric: 17.9264 - lr: 5.0000e-04 - 33s/epoch - 169ms/step
Epoch 210/1000
2023-09-26 15:19:32.297 
Epoch 210/1000 
	 loss: 15.8307, MinusLogProbMetric: 15.8307, val_loss: 17.8713, val_MinusLogProbMetric: 17.8713

Epoch 210: val_loss did not improve from 17.52244
196/196 - 34s - loss: 15.8307 - MinusLogProbMetric: 15.8307 - val_loss: 17.8713 - val_MinusLogProbMetric: 17.8713 - lr: 5.0000e-04 - 34s/epoch - 171ms/step
Epoch 211/1000
2023-09-26 15:20:05.274 
Epoch 211/1000 
	 loss: 15.8023, MinusLogProbMetric: 15.8023, val_loss: 17.8538, val_MinusLogProbMetric: 17.8538

Epoch 211: val_loss did not improve from 17.52244
196/196 - 33s - loss: 15.8023 - MinusLogProbMetric: 15.8023 - val_loss: 17.8538 - val_MinusLogProbMetric: 17.8538 - lr: 5.0000e-04 - 33s/epoch - 168ms/step
Epoch 212/1000
2023-09-26 15:20:37.984 
Epoch 212/1000 
	 loss: 15.7898, MinusLogProbMetric: 15.7898, val_loss: 17.8894, val_MinusLogProbMetric: 17.8894

Epoch 212: val_loss did not improve from 17.52244
196/196 - 33s - loss: 15.7898 - MinusLogProbMetric: 15.7898 - val_loss: 17.8894 - val_MinusLogProbMetric: 17.8894 - lr: 5.0000e-04 - 33s/epoch - 167ms/step
Epoch 213/1000
2023-09-26 15:21:10.133 
Epoch 213/1000 
	 loss: 15.8945, MinusLogProbMetric: 15.8945, val_loss: 18.0323, val_MinusLogProbMetric: 18.0323

Epoch 213: val_loss did not improve from 17.52244
196/196 - 32s - loss: 15.8945 - MinusLogProbMetric: 15.8945 - val_loss: 18.0323 - val_MinusLogProbMetric: 18.0323 - lr: 5.0000e-04 - 32s/epoch - 164ms/step
Epoch 214/1000
2023-09-26 15:21:44.028 
Epoch 214/1000 
	 loss: 15.7753, MinusLogProbMetric: 15.7753, val_loss: 17.9125, val_MinusLogProbMetric: 17.9125

Epoch 214: val_loss did not improve from 17.52244
196/196 - 34s - loss: 15.7753 - MinusLogProbMetric: 15.7753 - val_loss: 17.9125 - val_MinusLogProbMetric: 17.9125 - lr: 5.0000e-04 - 34s/epoch - 173ms/step
Epoch 215/1000
2023-09-26 15:22:17.851 
Epoch 215/1000 
	 loss: 15.8469, MinusLogProbMetric: 15.8469, val_loss: 17.8395, val_MinusLogProbMetric: 17.8395

Epoch 215: val_loss did not improve from 17.52244
196/196 - 34s - loss: 15.8469 - MinusLogProbMetric: 15.8469 - val_loss: 17.8395 - val_MinusLogProbMetric: 17.8395 - lr: 5.0000e-04 - 34s/epoch - 173ms/step
Epoch 216/1000
2023-09-26 15:22:52.354 
Epoch 216/1000 
	 loss: 15.7652, MinusLogProbMetric: 15.7652, val_loss: 17.8686, val_MinusLogProbMetric: 17.8686

Epoch 216: val_loss did not improve from 17.52244
196/196 - 35s - loss: 15.7652 - MinusLogProbMetric: 15.7652 - val_loss: 17.8686 - val_MinusLogProbMetric: 17.8686 - lr: 5.0000e-04 - 35s/epoch - 176ms/step
Epoch 217/1000
2023-09-26 15:23:26.563 
Epoch 217/1000 
	 loss: 15.8294, MinusLogProbMetric: 15.8294, val_loss: 17.8427, val_MinusLogProbMetric: 17.8427

Epoch 217: val_loss did not improve from 17.52244
196/196 - 34s - loss: 15.8294 - MinusLogProbMetric: 15.8294 - val_loss: 17.8427 - val_MinusLogProbMetric: 17.8427 - lr: 5.0000e-04 - 34s/epoch - 174ms/step
Epoch 218/1000
2023-09-26 15:24:00.134 
Epoch 218/1000 
	 loss: 15.8409, MinusLogProbMetric: 15.8409, val_loss: 17.9777, val_MinusLogProbMetric: 17.9777

Epoch 218: val_loss did not improve from 17.52244
196/196 - 34s - loss: 15.8409 - MinusLogProbMetric: 15.8409 - val_loss: 17.9777 - val_MinusLogProbMetric: 17.9777 - lr: 5.0000e-04 - 34s/epoch - 171ms/step
Epoch 219/1000
2023-09-26 15:24:33.718 
Epoch 219/1000 
	 loss: 15.8081, MinusLogProbMetric: 15.8081, val_loss: 17.9468, val_MinusLogProbMetric: 17.9468

Epoch 219: val_loss did not improve from 17.52244
196/196 - 34s - loss: 15.8081 - MinusLogProbMetric: 15.8081 - val_loss: 17.9468 - val_MinusLogProbMetric: 17.9468 - lr: 5.0000e-04 - 34s/epoch - 171ms/step
Epoch 220/1000
2023-09-26 15:25:06.865 
Epoch 220/1000 
	 loss: 15.7838, MinusLogProbMetric: 15.7838, val_loss: 18.1140, val_MinusLogProbMetric: 18.1140

Epoch 220: val_loss did not improve from 17.52244
196/196 - 33s - loss: 15.7838 - MinusLogProbMetric: 15.7838 - val_loss: 18.1140 - val_MinusLogProbMetric: 18.1140 - lr: 5.0000e-04 - 33s/epoch - 169ms/step
Epoch 221/1000
2023-09-26 15:25:40.074 
Epoch 221/1000 
	 loss: 15.7909, MinusLogProbMetric: 15.7909, val_loss: 18.2065, val_MinusLogProbMetric: 18.2065

Epoch 221: val_loss did not improve from 17.52244
196/196 - 33s - loss: 15.7909 - MinusLogProbMetric: 15.7909 - val_loss: 18.2065 - val_MinusLogProbMetric: 18.2065 - lr: 5.0000e-04 - 33s/epoch - 169ms/step
Epoch 222/1000
2023-09-26 15:26:13.567 
Epoch 222/1000 
	 loss: 15.7886, MinusLogProbMetric: 15.7886, val_loss: 17.8791, val_MinusLogProbMetric: 17.8791

Epoch 222: val_loss did not improve from 17.52244
196/196 - 33s - loss: 15.7886 - MinusLogProbMetric: 15.7886 - val_loss: 17.8791 - val_MinusLogProbMetric: 17.8791 - lr: 5.0000e-04 - 33s/epoch - 171ms/step
Epoch 223/1000
2023-09-26 15:26:45.649 
Epoch 223/1000 
	 loss: 15.7757, MinusLogProbMetric: 15.7757, val_loss: 17.9683, val_MinusLogProbMetric: 17.9683

Epoch 223: val_loss did not improve from 17.52244
196/196 - 32s - loss: 15.7757 - MinusLogProbMetric: 15.7757 - val_loss: 17.9683 - val_MinusLogProbMetric: 17.9683 - lr: 5.0000e-04 - 32s/epoch - 164ms/step
Epoch 224/1000
2023-09-26 15:27:15.333 
Epoch 224/1000 
	 loss: 15.7823, MinusLogProbMetric: 15.7823, val_loss: 17.9154, val_MinusLogProbMetric: 17.9154

Epoch 224: val_loss did not improve from 17.52244
196/196 - 30s - loss: 15.7823 - MinusLogProbMetric: 15.7823 - val_loss: 17.9154 - val_MinusLogProbMetric: 17.9154 - lr: 5.0000e-04 - 30s/epoch - 151ms/step
Epoch 225/1000
2023-09-26 15:27:50.609 
Epoch 225/1000 
	 loss: 15.7981, MinusLogProbMetric: 15.7981, val_loss: 18.0502, val_MinusLogProbMetric: 18.0502

Epoch 225: val_loss did not improve from 17.52244
196/196 - 35s - loss: 15.7981 - MinusLogProbMetric: 15.7981 - val_loss: 18.0502 - val_MinusLogProbMetric: 18.0502 - lr: 5.0000e-04 - 35s/epoch - 180ms/step
Epoch 226/1000
2023-09-26 15:28:26.744 
Epoch 226/1000 
	 loss: 15.7848, MinusLogProbMetric: 15.7848, val_loss: 18.0188, val_MinusLogProbMetric: 18.0188

Epoch 226: val_loss did not improve from 17.52244
Restoring model weights from the end of the best epoch: 126.
196/196 - 36s - loss: 15.7848 - MinusLogProbMetric: 15.7848 - val_loss: 18.0188 - val_MinusLogProbMetric: 18.0188 - lr: 5.0000e-04 - 36s/epoch - 186ms/step
Epoch 226: early stopping
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.

------------------------------------------
Starting LR metric calculation...
Running TF LR calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
LR metric calculation completed in 13.115043153986335 seconds.

------------------------------------------
Starting KS tests calculation...
Running TF KS tests...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
KS tests calculation completed in 7.762347123993095 seconds.

------------------------------------------
Starting SWD metric calculation...
Running TF SWD calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
SWD metric calculation completed in 6.751249700959306 seconds.

------------------------------------------
Starting FN metric calculation...
Running TF FN calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
FN metric calculation completed in 7.428610969975125 seconds.
Training succeeded with seed 721.
Model trained in 7473.85 s.

===========
Computing predictions
===========

Computing metrics...
Metrics computed in 36.46 s.
                ===========
                print("===========
Failed to plot
===========
")
                Exception type: ValueError
                Exception message: Number of rows must be a positive integer, not 0
                Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN.py , Line : 469, Func.Name : prediction_function, Message : Plotters.cornerplotter(X_data_test.numpy(),X_data_nf.numpy(),path_to_results,ndims,norm=True) # type: ignore', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/Plotters.py , Line : 156, Func.Name : cornerplotter, Message : figure=corner.corner(target_samples,color=\'red\',bins=n_bins,labels=[r"%s" % s for s in labels],normalize1d=True)', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/corner.py , Line : 235, Func.Name : corner, Message : fig, axes = pl.subplots(K, K, figsize=(dim, dim))', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/pyplot.py , Line : 1502, Func.Name : subplots, Message : axs = fig.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 905, Func.Name : subplots, Message : gs = self.add_gridspec(nrows, ncols, figure=self, **gridspec_kw)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 1527, Func.Name : add_gridspec, Message : gs = GridSpec(nrows=nrows, ncols=ncols, figure=self, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 379, Func.Name : __init__, Message : super().__init__(nrows, ncols,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 49, Func.Name : __init__, Message : raise ValueError(']
                ===========

results.txt saved
results.json saved
Results log saved
Model predictions computed in 36.92 s.
===========
Run 290/720 done in 7513.34 s.
===========

Directory ../../results/CsplineN_new/run_291/ already exists.
Skipping it.
===========
Run 291/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_292/ already exists.
Skipping it.
===========
Run 292/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_293/ already exists.
Skipping it.
===========
Run 293/720 already exists. Skipping it.
===========

===========
Generating train data for run 294.
===========
Train data generated in 0.17 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_294/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 721}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_294/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.3570018 ,  5.6703625 ,  0.48762453, ...,  0.76730525,
         6.4537516 ,  1.4289263 ],
       [ 4.8933635 ,  4.954244  , -0.7742728 , ...,  1.033045  ,
         7.5179    ,  1.3446053 ],
       [ 2.6192126 ,  3.6659784 ,  9.3581505 , ...,  6.4096394 ,
         2.8983781 ,  1.8163822 ],
       ...,
       [ 4.4286733 ,  5.582822  , -0.08033401, ...,  1.0755457 ,
         6.482341  ,  1.3259909 ],
       [ 5.2667904 ,  5.78094   ,  0.52473783, ...,  1.3584392 ,
         6.6371064 ,  1.4527634 ],
       [ 1.3711529 ,  3.985786  ,  8.434673  , ...,  6.650886  ,
         2.8764064 ,  2.071397  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_294/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_294
self.data_kwargs: {'seed': 721}
self.x_data: [[ 5.4479074  7.1622143  6.6614265 ...  3.6775346  2.6042771  7.6607866]
 [ 1.558796   3.7643342  7.765504  ...  7.3985186  2.2847533  2.1737251]
 [ 2.023865   3.5498223  9.736043  ...  7.378399   2.9893079  1.7681804]
 ...
 [ 3.157689   3.8140697  9.128405  ...  7.3280826  3.3431854  1.6145309]
 [ 4.9035525  5.63367    1.3061054 ...  1.7336241  7.263643   1.3624198]
 [ 4.873677   6.0271535 -0.4844559 ...  1.4305246  6.6029577  1.4994687]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_16"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_17 (InputLayer)       [(None, 32)]              0         
                                                                 
 log_prob_layer_1 (LogProbLa  (None,)                  2305120   
 yer)                                                            
                                                                 
=================================================================
Total params: 2,305,120
Trainable params: 2,305,120
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_1/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_1'")
self.model: <keras.engine.functional.Functional object at 0x7fb27066fb80>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fb290323790>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fb290323790>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fb2b818a680>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fb27069dea0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_294/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fb27069e410>, <keras.callbacks.ModelCheckpoint object at 0x7fb27069e4d0>, <keras.callbacks.EarlyStopping object at 0x7fb27069e740>, <keras.callbacks.ReduceLROnPlateau object at 0x7fb27069e770>, <keras.callbacks.TerminateOnNaN object at 0x7fb27069e3b0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.3570018 ,  5.6703625 ,  0.48762453, ...,  0.76730525,
         6.4537516 ,  1.4289263 ],
       [ 4.8933635 ,  4.954244  , -0.7742728 , ...,  1.033045  ,
         7.5179    ,  1.3446053 ],
       [ 2.6192126 ,  3.6659784 ,  9.3581505 , ...,  6.4096394 ,
         2.8983781 ,  1.8163822 ],
       ...,
       [ 4.4286733 ,  5.582822  , -0.08033401, ...,  1.0755457 ,
         6.482341  ,  1.3259909 ],
       [ 5.2667904 ,  5.78094   ,  0.52473783, ...,  1.3584392 ,
         6.6371064 ,  1.4527634 ],
       [ 1.3711529 ,  3.985786  ,  8.434673  , ...,  6.650886  ,
         2.8764064 ,  2.071397  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_294/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 294/720 with hyperparameters:
timestamp = 2023-09-26 15:29:11.416996
ndims = 32
seed_train = 721
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 2305120
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 5.4479074  7.1622143  6.6614265  5.4466543  4.679343   6.5534453
  4.5869303  8.297877   9.431402   3.191049   9.163281   4.112924
  5.872972  10.008474   0.4502598  1.3920732 -0.6747763  8.487896
  9.224454   7.913118   8.449968   7.6997313  5.9028535  7.439646
  2.3174791  6.975212   1.6619607  9.750696   4.816901   3.6775346
  2.6042771  7.6607866]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 29: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-26 15:31:47.016 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 1497.8458, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 155s - loss: nan - MinusLogProbMetric: 1497.8458 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 155s/epoch - 793ms/step
The loss history contains NaN values.
Training failed: trying again with seed 821433 and lr 0.0003333333333333333.
===========
Generating train data for run 294.
===========
Train data generated in 0.24 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_294/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 721}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_294/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.3570018 ,  5.6703625 ,  0.48762453, ...,  0.76730525,
         6.4537516 ,  1.4289263 ],
       [ 4.8933635 ,  4.954244  , -0.7742728 , ...,  1.033045  ,
         7.5179    ,  1.3446053 ],
       [ 2.6192126 ,  3.6659784 ,  9.3581505 , ...,  6.4096394 ,
         2.8983781 ,  1.8163822 ],
       ...,
       [ 4.4286733 ,  5.582822  , -0.08033401, ...,  1.0755457 ,
         6.482341  ,  1.3259909 ],
       [ 5.2667904 ,  5.78094   ,  0.52473783, ...,  1.3584392 ,
         6.6371064 ,  1.4527634 ],
       [ 1.3711529 ,  3.985786  ,  8.434673  , ...,  6.650886  ,
         2.8764064 ,  2.071397  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_294/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_294
self.data_kwargs: {'seed': 721}
self.x_data: [[ 5.4479074  7.1622143  6.6614265 ...  3.6775346  2.6042771  7.6607866]
 [ 1.558796   3.7643342  7.765504  ...  7.3985186  2.2847533  2.1737251]
 [ 2.023865   3.5498223  9.736043  ...  7.378399   2.9893079  1.7681804]
 ...
 [ 3.157689   3.8140697  9.128405  ...  7.3280826  3.3431854  1.6145309]
 [ 4.9035525  5.63367    1.3061054 ...  1.7336241  7.263643   1.3624198]
 [ 4.873677   6.0271535 -0.4844559 ...  1.4305246  6.6029577  1.4994687]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_27"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_28 (InputLayer)       [(None, 32)]              0         
                                                                 
 log_prob_layer_2 (LogProbLa  (None,)                  2305120   
 yer)                                                            
                                                                 
=================================================================
Total params: 2,305,120
Trainable params: 2,305,120
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_2/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_2'")
self.model: <keras.engine.functional.Functional object at 0x7fb216c500d0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fb21680fa90>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fb21680fa90>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fb21648b250>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fb2164dfe80>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_294/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fb216314430>, <keras.callbacks.ModelCheckpoint object at 0x7fb2163144f0>, <keras.callbacks.EarlyStopping object at 0x7fb216314760>, <keras.callbacks.ReduceLROnPlateau object at 0x7fb216314790>, <keras.callbacks.TerminateOnNaN object at 0x7fb2163143d0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.3570018 ,  5.6703625 ,  0.48762453, ...,  0.76730525,
         6.4537516 ,  1.4289263 ],
       [ 4.8933635 ,  4.954244  , -0.7742728 , ...,  1.033045  ,
         7.5179    ,  1.3446053 ],
       [ 2.6192126 ,  3.6659784 ,  9.3581505 , ...,  6.4096394 ,
         2.8983781 ,  1.8163822 ],
       ...,
       [ 4.4286733 ,  5.582822  , -0.08033401, ...,  1.0755457 ,
         6.482341  ,  1.3259909 ],
       [ 5.2667904 ,  5.78094   ,  0.52473783, ...,  1.3584392 ,
         6.6371064 ,  1.4527634 ],
       [ 1.3711529 ,  3.985786  ,  8.434673  , ...,  6.650886  ,
         2.8764064 ,  2.071397  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_294/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 294/720 with hyperparameters:
timestamp = 2023-09-26 15:31:57.212812
ndims = 32
seed_train = 721
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 2305120
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 5.4479074  7.1622143  6.6614265  5.4466543  4.679343   6.5534453
  4.5869303  8.297877   9.431402   3.191049   9.163281   4.112924
  5.872972  10.008474   0.4502598  1.3920732 -0.6747763  8.487896
  9.224454   7.913118   8.449968   7.6997313  5.9028535  7.439646
  2.3174791  6.975212   1.6619607  9.750696   4.816901   3.6775346
  2.6042771  7.6607866]
Epoch 1/1000
2023-09-26 15:35:45.705 
Epoch 1/1000 
	 loss: 271.9011, MinusLogProbMetric: 271.9011, val_loss: 75.1128, val_MinusLogProbMetric: 75.1128

Epoch 1: val_loss improved from inf to 75.11284, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_294/weights/best_weights.h5
196/196 - 229s - loss: 271.9011 - MinusLogProbMetric: 271.9011 - val_loss: 75.1128 - val_MinusLogProbMetric: 75.1128 - lr: 3.3333e-04 - 229s/epoch - 1s/step
Epoch 2/1000
2023-09-26 15:36:51.928 
Epoch 2/1000 
	 loss: 61.3183, MinusLogProbMetric: 61.3183, val_loss: 51.1657, val_MinusLogProbMetric: 51.1657

Epoch 2: val_loss improved from 75.11284 to 51.16574, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_294/weights/best_weights.h5
196/196 - 66s - loss: 61.3183 - MinusLogProbMetric: 61.3183 - val_loss: 51.1657 - val_MinusLogProbMetric: 51.1657 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 3/1000
2023-09-26 15:37:57.458 
Epoch 3/1000 
	 loss: 39.2677, MinusLogProbMetric: 39.2677, val_loss: 33.2167, val_MinusLogProbMetric: 33.2167

Epoch 3: val_loss improved from 51.16574 to 33.21667, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_294/weights/best_weights.h5
196/196 - 66s - loss: 39.2677 - MinusLogProbMetric: 39.2677 - val_loss: 33.2167 - val_MinusLogProbMetric: 33.2167 - lr: 3.3333e-04 - 66s/epoch - 334ms/step
Epoch 4/1000
2023-09-26 15:39:03.702 
Epoch 4/1000 
	 loss: 32.0120, MinusLogProbMetric: 32.0120, val_loss: 30.9310, val_MinusLogProbMetric: 30.9310

Epoch 4: val_loss improved from 33.21667 to 30.93095, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_294/weights/best_weights.h5
196/196 - 66s - loss: 32.0120 - MinusLogProbMetric: 32.0120 - val_loss: 30.9310 - val_MinusLogProbMetric: 30.9310 - lr: 3.3333e-04 - 66s/epoch - 339ms/step
Epoch 5/1000
2023-09-26 15:40:09.956 
Epoch 5/1000 
	 loss: 29.0464, MinusLogProbMetric: 29.0464, val_loss: 28.3289, val_MinusLogProbMetric: 28.3289

Epoch 5: val_loss improved from 30.93095 to 28.32885, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_294/weights/best_weights.h5
196/196 - 66s - loss: 29.0464 - MinusLogProbMetric: 29.0464 - val_loss: 28.3289 - val_MinusLogProbMetric: 28.3289 - lr: 3.3333e-04 - 66s/epoch - 337ms/step
Epoch 6/1000
2023-09-26 15:41:15.561 
Epoch 6/1000 
	 loss: 26.9526, MinusLogProbMetric: 26.9526, val_loss: 26.4985, val_MinusLogProbMetric: 26.4985

Epoch 6: val_loss improved from 28.32885 to 26.49847, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_294/weights/best_weights.h5
196/196 - 66s - loss: 26.9526 - MinusLogProbMetric: 26.9526 - val_loss: 26.4985 - val_MinusLogProbMetric: 26.4985 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 7/1000
2023-09-26 15:42:19.677 
Epoch 7/1000 
	 loss: 25.4583, MinusLogProbMetric: 25.4583, val_loss: 26.8512, val_MinusLogProbMetric: 26.8512

Epoch 7: val_loss did not improve from 26.49847
196/196 - 63s - loss: 25.4583 - MinusLogProbMetric: 25.4583 - val_loss: 26.8512 - val_MinusLogProbMetric: 26.8512 - lr: 3.3333e-04 - 63s/epoch - 321ms/step
Epoch 8/1000
2023-09-26 15:43:23.383 
Epoch 8/1000 
	 loss: 24.6200, MinusLogProbMetric: 24.6200, val_loss: 23.4925, val_MinusLogProbMetric: 23.4925

Epoch 8: val_loss improved from 26.49847 to 23.49247, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_294/weights/best_weights.h5
196/196 - 65s - loss: 24.6200 - MinusLogProbMetric: 24.6200 - val_loss: 23.4925 - val_MinusLogProbMetric: 23.4925 - lr: 3.3333e-04 - 65s/epoch - 331ms/step
Epoch 9/1000
2023-09-26 15:44:28.574 
Epoch 9/1000 
	 loss: 23.5982, MinusLogProbMetric: 23.5982, val_loss: 23.1018, val_MinusLogProbMetric: 23.1018

Epoch 9: val_loss improved from 23.49247 to 23.10179, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_294/weights/best_weights.h5
196/196 - 65s - loss: 23.5982 - MinusLogProbMetric: 23.5982 - val_loss: 23.1018 - val_MinusLogProbMetric: 23.1018 - lr: 3.3333e-04 - 65s/epoch - 332ms/step
Epoch 10/1000
2023-09-26 15:45:34.945 
Epoch 10/1000 
	 loss: 23.0718, MinusLogProbMetric: 23.0718, val_loss: 22.9055, val_MinusLogProbMetric: 22.9055

Epoch 10: val_loss improved from 23.10179 to 22.90549, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_294/weights/best_weights.h5
196/196 - 66s - loss: 23.0718 - MinusLogProbMetric: 23.0718 - val_loss: 22.9055 - val_MinusLogProbMetric: 22.9055 - lr: 3.3333e-04 - 66s/epoch - 339ms/step
Epoch 11/1000
2023-09-26 15:46:41.898 
Epoch 11/1000 
	 loss: 22.7062, MinusLogProbMetric: 22.7062, val_loss: 22.5548, val_MinusLogProbMetric: 22.5548

Epoch 11: val_loss improved from 22.90549 to 22.55479, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_294/weights/best_weights.h5
196/196 - 67s - loss: 22.7062 - MinusLogProbMetric: 22.7062 - val_loss: 22.5548 - val_MinusLogProbMetric: 22.5548 - lr: 3.3333e-04 - 67s/epoch - 340ms/step
Epoch 12/1000
2023-09-26 15:47:47.628 
Epoch 12/1000 
	 loss: 22.1342, MinusLogProbMetric: 22.1342, val_loss: 23.1154, val_MinusLogProbMetric: 23.1154

Epoch 12: val_loss did not improve from 22.55479
196/196 - 65s - loss: 22.1342 - MinusLogProbMetric: 22.1342 - val_loss: 23.1154 - val_MinusLogProbMetric: 23.1154 - lr: 3.3333e-04 - 65s/epoch - 331ms/step
Epoch 13/1000
2023-09-26 15:48:53.131 
Epoch 13/1000 
	 loss: 21.9181, MinusLogProbMetric: 21.9181, val_loss: 21.6753, val_MinusLogProbMetric: 21.6753

Epoch 13: val_loss improved from 22.55479 to 21.67532, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_294/weights/best_weights.h5
196/196 - 67s - loss: 21.9181 - MinusLogProbMetric: 21.9181 - val_loss: 21.6753 - val_MinusLogProbMetric: 21.6753 - lr: 3.3333e-04 - 67s/epoch - 340ms/step
Epoch 14/1000
2023-09-26 15:49:59.674 
Epoch 14/1000 
	 loss: 21.6369, MinusLogProbMetric: 21.6369, val_loss: 21.8865, val_MinusLogProbMetric: 21.8865

Epoch 14: val_loss did not improve from 21.67532
196/196 - 65s - loss: 21.6369 - MinusLogProbMetric: 21.6369 - val_loss: 21.8865 - val_MinusLogProbMetric: 21.8865 - lr: 3.3333e-04 - 65s/epoch - 333ms/step
Epoch 15/1000
2023-09-26 15:51:05.862 
Epoch 15/1000 
	 loss: 21.2256, MinusLogProbMetric: 21.2256, val_loss: 21.6713, val_MinusLogProbMetric: 21.6713

Epoch 15: val_loss improved from 21.67532 to 21.67133, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_294/weights/best_weights.h5
196/196 - 67s - loss: 21.2256 - MinusLogProbMetric: 21.2256 - val_loss: 21.6713 - val_MinusLogProbMetric: 21.6713 - lr: 3.3333e-04 - 67s/epoch - 343ms/step
Epoch 16/1000
2023-09-26 15:52:12.686 
Epoch 16/1000 
	 loss: 21.1250, MinusLogProbMetric: 21.1250, val_loss: 21.2209, val_MinusLogProbMetric: 21.2209

Epoch 16: val_loss improved from 21.67133 to 21.22088, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_294/weights/best_weights.h5
196/196 - 67s - loss: 21.1250 - MinusLogProbMetric: 21.1250 - val_loss: 21.2209 - val_MinusLogProbMetric: 21.2209 - lr: 3.3333e-04 - 67s/epoch - 341ms/step
Epoch 17/1000
2023-09-26 15:53:19.445 
Epoch 17/1000 
	 loss: 20.9016, MinusLogProbMetric: 20.9016, val_loss: 21.0942, val_MinusLogProbMetric: 21.0942

Epoch 17: val_loss improved from 21.22088 to 21.09417, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_294/weights/best_weights.h5
196/196 - 67s - loss: 20.9016 - MinusLogProbMetric: 20.9016 - val_loss: 21.0942 - val_MinusLogProbMetric: 21.0942 - lr: 3.3333e-04 - 67s/epoch - 342ms/step
Epoch 18/1000
2023-09-26 15:54:27.553 
Epoch 18/1000 
	 loss: 20.7188, MinusLogProbMetric: 20.7188, val_loss: 21.0027, val_MinusLogProbMetric: 21.0027

Epoch 18: val_loss improved from 21.09417 to 21.00273, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_294/weights/best_weights.h5
196/196 - 68s - loss: 20.7188 - MinusLogProbMetric: 20.7188 - val_loss: 21.0027 - val_MinusLogProbMetric: 21.0027 - lr: 3.3333e-04 - 68s/epoch - 346ms/step
Epoch 19/1000
2023-09-26 15:55:33.373 
Epoch 19/1000 
	 loss: 20.4970, MinusLogProbMetric: 20.4970, val_loss: 21.1584, val_MinusLogProbMetric: 21.1584

Epoch 19: val_loss did not improve from 21.00273
196/196 - 65s - loss: 20.4970 - MinusLogProbMetric: 20.4970 - val_loss: 21.1584 - val_MinusLogProbMetric: 21.1584 - lr: 3.3333e-04 - 65s/epoch - 331ms/step
Epoch 20/1000
2023-09-26 15:56:37.014 
Epoch 20/1000 
	 loss: 20.4806, MinusLogProbMetric: 20.4806, val_loss: 21.0531, val_MinusLogProbMetric: 21.0531

Epoch 20: val_loss did not improve from 21.00273
196/196 - 64s - loss: 20.4806 - MinusLogProbMetric: 20.4806 - val_loss: 21.0531 - val_MinusLogProbMetric: 21.0531 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 21/1000
2023-09-26 15:57:29.775 
Epoch 21/1000 
	 loss: 20.4045, MinusLogProbMetric: 20.4045, val_loss: 20.2487, val_MinusLogProbMetric: 20.2487

Epoch 21: val_loss improved from 21.00273 to 20.24866, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_294/weights/best_weights.h5
196/196 - 54s - loss: 20.4045 - MinusLogProbMetric: 20.4045 - val_loss: 20.2487 - val_MinusLogProbMetric: 20.2487 - lr: 3.3333e-04 - 54s/epoch - 273ms/step
Epoch 22/1000
2023-09-26 15:58:32.129 
Epoch 22/1000 
	 loss: 20.2307, MinusLogProbMetric: 20.2307, val_loss: 19.8050, val_MinusLogProbMetric: 19.8050

Epoch 22: val_loss improved from 20.24866 to 19.80504, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_294/weights/best_weights.h5
196/196 - 63s - loss: 20.2307 - MinusLogProbMetric: 20.2307 - val_loss: 19.8050 - val_MinusLogProbMetric: 19.8050 - lr: 3.3333e-04 - 63s/epoch - 319ms/step
Epoch 23/1000
2023-09-26 15:59:38.235 
Epoch 23/1000 
	 loss: 20.1164, MinusLogProbMetric: 20.1164, val_loss: 20.4020, val_MinusLogProbMetric: 20.4020

Epoch 23: val_loss did not improve from 19.80504
196/196 - 65s - loss: 20.1164 - MinusLogProbMetric: 20.1164 - val_loss: 20.4020 - val_MinusLogProbMetric: 20.4020 - lr: 3.3333e-04 - 65s/epoch - 332ms/step
Epoch 24/1000
2023-09-26 16:00:44.498 
Epoch 24/1000 
	 loss: 20.1440, MinusLogProbMetric: 20.1440, val_loss: 20.6829, val_MinusLogProbMetric: 20.6829

Epoch 24: val_loss did not improve from 19.80504
196/196 - 66s - loss: 20.1440 - MinusLogProbMetric: 20.1440 - val_loss: 20.6829 - val_MinusLogProbMetric: 20.6829 - lr: 3.3333e-04 - 66s/epoch - 338ms/step
Epoch 25/1000
2023-09-26 16:01:50.763 
Epoch 25/1000 
	 loss: 19.7646, MinusLogProbMetric: 19.7646, val_loss: 20.5429, val_MinusLogProbMetric: 20.5429

Epoch 25: val_loss did not improve from 19.80504
196/196 - 66s - loss: 19.7646 - MinusLogProbMetric: 19.7646 - val_loss: 20.5429 - val_MinusLogProbMetric: 20.5429 - lr: 3.3333e-04 - 66s/epoch - 338ms/step
Epoch 26/1000
2023-09-26 16:02:55.915 
Epoch 26/1000 
	 loss: 19.8325, MinusLogProbMetric: 19.8325, val_loss: 19.2543, val_MinusLogProbMetric: 19.2543

Epoch 26: val_loss improved from 19.80504 to 19.25430, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_294/weights/best_weights.h5
196/196 - 66s - loss: 19.8325 - MinusLogProbMetric: 19.8325 - val_loss: 19.2543 - val_MinusLogProbMetric: 19.2543 - lr: 3.3333e-04 - 66s/epoch - 337ms/step
Epoch 27/1000
2023-09-26 16:04:03.015 
Epoch 27/1000 
	 loss: 19.7912, MinusLogProbMetric: 19.7912, val_loss: 20.3242, val_MinusLogProbMetric: 20.3242

Epoch 27: val_loss did not improve from 19.25430
196/196 - 66s - loss: 19.7912 - MinusLogProbMetric: 19.7912 - val_loss: 20.3242 - val_MinusLogProbMetric: 20.3242 - lr: 3.3333e-04 - 66s/epoch - 338ms/step
Epoch 28/1000
2023-09-26 16:05:08.631 
Epoch 28/1000 
	 loss: 19.7392, MinusLogProbMetric: 19.7392, val_loss: 20.1642, val_MinusLogProbMetric: 20.1642

Epoch 28: val_loss did not improve from 19.25430
196/196 - 66s - loss: 19.7392 - MinusLogProbMetric: 19.7392 - val_loss: 20.1642 - val_MinusLogProbMetric: 20.1642 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 29/1000
2023-09-26 16:06:14.311 
Epoch 29/1000 
	 loss: 19.5942, MinusLogProbMetric: 19.5942, val_loss: 19.5611, val_MinusLogProbMetric: 19.5611

Epoch 29: val_loss did not improve from 19.25430
196/196 - 66s - loss: 19.5942 - MinusLogProbMetric: 19.5942 - val_loss: 19.5611 - val_MinusLogProbMetric: 19.5611 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 30/1000
2023-09-26 16:07:20.863 
Epoch 30/1000 
	 loss: 19.5733, MinusLogProbMetric: 19.5733, val_loss: 19.4714, val_MinusLogProbMetric: 19.4714

Epoch 30: val_loss did not improve from 19.25430
196/196 - 67s - loss: 19.5733 - MinusLogProbMetric: 19.5733 - val_loss: 19.4714 - val_MinusLogProbMetric: 19.4714 - lr: 3.3333e-04 - 67s/epoch - 340ms/step
Epoch 31/1000
2023-09-26 16:08:26.340 
Epoch 31/1000 
	 loss: 19.7991, MinusLogProbMetric: 19.7991, val_loss: 20.1426, val_MinusLogProbMetric: 20.1426

Epoch 31: val_loss did not improve from 19.25430
196/196 - 65s - loss: 19.7991 - MinusLogProbMetric: 19.7991 - val_loss: 20.1426 - val_MinusLogProbMetric: 20.1426 - lr: 3.3333e-04 - 65s/epoch - 334ms/step
Epoch 32/1000
2023-09-26 16:09:31.545 
Epoch 32/1000 
	 loss: 19.3552, MinusLogProbMetric: 19.3552, val_loss: 20.4140, val_MinusLogProbMetric: 20.4140

Epoch 32: val_loss did not improve from 19.25430
196/196 - 65s - loss: 19.3552 - MinusLogProbMetric: 19.3552 - val_loss: 20.4140 - val_MinusLogProbMetric: 20.4140 - lr: 3.3333e-04 - 65s/epoch - 333ms/step
Epoch 33/1000
2023-09-26 16:10:36.410 
Epoch 33/1000 
	 loss: 19.3895, MinusLogProbMetric: 19.3895, val_loss: 19.4651, val_MinusLogProbMetric: 19.4651

Epoch 33: val_loss did not improve from 19.25430
196/196 - 65s - loss: 19.3895 - MinusLogProbMetric: 19.3895 - val_loss: 19.4651 - val_MinusLogProbMetric: 19.4651 - lr: 3.3333e-04 - 65s/epoch - 331ms/step
Epoch 34/1000
2023-09-26 16:11:43.038 
Epoch 34/1000 
	 loss: 19.4512, MinusLogProbMetric: 19.4512, val_loss: 19.5986, val_MinusLogProbMetric: 19.5986

Epoch 34: val_loss did not improve from 19.25430
196/196 - 67s - loss: 19.4512 - MinusLogProbMetric: 19.4512 - val_loss: 19.5986 - val_MinusLogProbMetric: 19.5986 - lr: 3.3333e-04 - 67s/epoch - 340ms/step
Epoch 35/1000
2023-09-26 16:12:49.960 
Epoch 35/1000 
	 loss: 19.2238, MinusLogProbMetric: 19.2238, val_loss: 19.5739, val_MinusLogProbMetric: 19.5739

Epoch 35: val_loss did not improve from 19.25430
196/196 - 67s - loss: 19.2238 - MinusLogProbMetric: 19.2238 - val_loss: 19.5739 - val_MinusLogProbMetric: 19.5739 - lr: 3.3333e-04 - 67s/epoch - 341ms/step
Epoch 36/1000
2023-09-26 16:13:56.045 
Epoch 36/1000 
	 loss: 19.2948, MinusLogProbMetric: 19.2948, val_loss: 21.4937, val_MinusLogProbMetric: 21.4937

Epoch 36: val_loss did not improve from 19.25430
196/196 - 66s - loss: 19.2948 - MinusLogProbMetric: 19.2948 - val_loss: 21.4937 - val_MinusLogProbMetric: 21.4937 - lr: 3.3333e-04 - 66s/epoch - 337ms/step
Epoch 37/1000
2023-09-26 16:15:01.743 
Epoch 37/1000 
	 loss: 19.2892, MinusLogProbMetric: 19.2892, val_loss: 19.4434, val_MinusLogProbMetric: 19.4434

Epoch 37: val_loss did not improve from 19.25430
196/196 - 66s - loss: 19.2892 - MinusLogProbMetric: 19.2892 - val_loss: 19.4434 - val_MinusLogProbMetric: 19.4434 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 38/1000
2023-09-26 16:16:06.026 
Epoch 38/1000 
	 loss: 18.9545, MinusLogProbMetric: 18.9545, val_loss: 19.1931, val_MinusLogProbMetric: 19.1931

Epoch 38: val_loss improved from 19.25430 to 19.19312, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_294/weights/best_weights.h5
196/196 - 65s - loss: 18.9545 - MinusLogProbMetric: 18.9545 - val_loss: 19.1931 - val_MinusLogProbMetric: 19.1931 - lr: 3.3333e-04 - 65s/epoch - 333ms/step
Epoch 39/1000
2023-09-26 16:17:12.470 
Epoch 39/1000 
	 loss: 19.0465, MinusLogProbMetric: 19.0465, val_loss: 19.2028, val_MinusLogProbMetric: 19.2028

Epoch 39: val_loss did not improve from 19.19312
196/196 - 65s - loss: 19.0465 - MinusLogProbMetric: 19.0465 - val_loss: 19.2028 - val_MinusLogProbMetric: 19.2028 - lr: 3.3333e-04 - 65s/epoch - 333ms/step
Epoch 40/1000
2023-09-26 16:18:18.980 
Epoch 40/1000 
	 loss: 19.8235, MinusLogProbMetric: 19.8235, val_loss: 18.9173, val_MinusLogProbMetric: 18.9173

Epoch 40: val_loss improved from 19.19312 to 18.91735, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_294/weights/best_weights.h5
196/196 - 68s - loss: 19.8235 - MinusLogProbMetric: 19.8235 - val_loss: 18.9173 - val_MinusLogProbMetric: 18.9173 - lr: 3.3333e-04 - 68s/epoch - 345ms/step
Epoch 41/1000
2023-09-26 16:19:25.382 
Epoch 41/1000 
	 loss: 19.3029, MinusLogProbMetric: 19.3029, val_loss: 19.0400, val_MinusLogProbMetric: 19.0400

Epoch 41: val_loss did not improve from 18.91735
196/196 - 65s - loss: 19.3029 - MinusLogProbMetric: 19.3029 - val_loss: 19.0400 - val_MinusLogProbMetric: 19.0400 - lr: 3.3333e-04 - 65s/epoch - 333ms/step
Epoch 42/1000
2023-09-26 16:20:31.822 
Epoch 42/1000 
	 loss: 18.8929, MinusLogProbMetric: 18.8929, val_loss: 19.0951, val_MinusLogProbMetric: 19.0951

Epoch 42: val_loss did not improve from 18.91735
196/196 - 66s - loss: 18.8929 - MinusLogProbMetric: 18.8929 - val_loss: 19.0951 - val_MinusLogProbMetric: 19.0951 - lr: 3.3333e-04 - 66s/epoch - 339ms/step
Epoch 43/1000
2023-09-26 16:21:38.212 
Epoch 43/1000 
	 loss: 18.9745, MinusLogProbMetric: 18.9745, val_loss: 20.9021, val_MinusLogProbMetric: 20.9021

Epoch 43: val_loss did not improve from 18.91735
196/196 - 66s - loss: 18.9745 - MinusLogProbMetric: 18.9745 - val_loss: 20.9021 - val_MinusLogProbMetric: 20.9021 - lr: 3.3333e-04 - 66s/epoch - 339ms/step
Epoch 44/1000
2023-09-26 16:22:43.168 
Epoch 44/1000 
	 loss: 18.9672, MinusLogProbMetric: 18.9672, val_loss: 20.1098, val_MinusLogProbMetric: 20.1098

Epoch 44: val_loss did not improve from 18.91735
196/196 - 65s - loss: 18.9672 - MinusLogProbMetric: 18.9672 - val_loss: 20.1098 - val_MinusLogProbMetric: 20.1098 - lr: 3.3333e-04 - 65s/epoch - 331ms/step
Epoch 45/1000
2023-09-26 16:23:47.358 
Epoch 45/1000 
	 loss: 18.9248, MinusLogProbMetric: 18.9248, val_loss: 19.2676, val_MinusLogProbMetric: 19.2676

Epoch 45: val_loss did not improve from 18.91735
196/196 - 64s - loss: 18.9248 - MinusLogProbMetric: 18.9248 - val_loss: 19.2676 - val_MinusLogProbMetric: 19.2676 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 46/1000
2023-09-26 16:24:51.707 
Epoch 46/1000 
	 loss: 18.8751, MinusLogProbMetric: 18.8751, val_loss: 20.1648, val_MinusLogProbMetric: 20.1648

Epoch 46: val_loss did not improve from 18.91735
196/196 - 64s - loss: 18.8751 - MinusLogProbMetric: 18.8751 - val_loss: 20.1648 - val_MinusLogProbMetric: 20.1648 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 47/1000
2023-09-26 16:25:56.418 
Epoch 47/1000 
	 loss: 18.8753, MinusLogProbMetric: 18.8753, val_loss: 20.2004, val_MinusLogProbMetric: 20.2004

Epoch 47: val_loss did not improve from 18.91735
196/196 - 65s - loss: 18.8753 - MinusLogProbMetric: 18.8753 - val_loss: 20.2004 - val_MinusLogProbMetric: 20.2004 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 48/1000
2023-09-26 16:27:01.538 
Epoch 48/1000 
	 loss: 18.8661, MinusLogProbMetric: 18.8661, val_loss: 18.4682, val_MinusLogProbMetric: 18.4682

Epoch 48: val_loss improved from 18.91735 to 18.46823, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_294/weights/best_weights.h5
196/196 - 66s - loss: 18.8661 - MinusLogProbMetric: 18.8661 - val_loss: 18.4682 - val_MinusLogProbMetric: 18.4682 - lr: 3.3333e-04 - 66s/epoch - 337ms/step
Epoch 49/1000
2023-09-26 16:28:07.632 
Epoch 49/1000 
	 loss: 18.7674, MinusLogProbMetric: 18.7674, val_loss: 18.3435, val_MinusLogProbMetric: 18.3435

Epoch 49: val_loss improved from 18.46823 to 18.34349, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_294/weights/best_weights.h5
196/196 - 66s - loss: 18.7674 - MinusLogProbMetric: 18.7674 - val_loss: 18.3435 - val_MinusLogProbMetric: 18.3435 - lr: 3.3333e-04 - 66s/epoch - 337ms/step
Epoch 50/1000
2023-09-26 16:29:14.131 
Epoch 50/1000 
	 loss: 18.8419, MinusLogProbMetric: 18.8419, val_loss: 18.9634, val_MinusLogProbMetric: 18.9634

Epoch 50: val_loss did not improve from 18.34349
196/196 - 66s - loss: 18.8419 - MinusLogProbMetric: 18.8419 - val_loss: 18.9634 - val_MinusLogProbMetric: 18.9634 - lr: 3.3333e-04 - 66s/epoch - 334ms/step
Epoch 51/1000
2023-09-26 16:30:20.020 
Epoch 51/1000 
	 loss: 18.5874, MinusLogProbMetric: 18.5874, val_loss: 18.6833, val_MinusLogProbMetric: 18.6833

Epoch 51: val_loss did not improve from 18.34349
196/196 - 66s - loss: 18.5874 - MinusLogProbMetric: 18.5874 - val_loss: 18.6833 - val_MinusLogProbMetric: 18.6833 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 52/1000
2023-09-26 16:31:24.265 
Epoch 52/1000 
	 loss: 18.6004, MinusLogProbMetric: 18.6004, val_loss: 18.6257, val_MinusLogProbMetric: 18.6257

Epoch 52: val_loss did not improve from 18.34349
196/196 - 64s - loss: 18.6004 - MinusLogProbMetric: 18.6004 - val_loss: 18.6257 - val_MinusLogProbMetric: 18.6257 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 53/1000
2023-09-26 16:32:28.732 
Epoch 53/1000 
	 loss: 18.7634, MinusLogProbMetric: 18.7634, val_loss: 18.5666, val_MinusLogProbMetric: 18.5666

Epoch 53: val_loss did not improve from 18.34349
196/196 - 64s - loss: 18.7634 - MinusLogProbMetric: 18.7634 - val_loss: 18.5666 - val_MinusLogProbMetric: 18.5666 - lr: 3.3333e-04 - 64s/epoch - 329ms/step
Epoch 54/1000
2023-09-26 16:33:32.984 
Epoch 54/1000 
	 loss: 18.6535, MinusLogProbMetric: 18.6535, val_loss: 18.9570, val_MinusLogProbMetric: 18.9570

Epoch 54: val_loss did not improve from 18.34349
196/196 - 64s - loss: 18.6535 - MinusLogProbMetric: 18.6535 - val_loss: 18.9570 - val_MinusLogProbMetric: 18.9570 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 55/1000
2023-09-26 16:34:38.332 
Epoch 55/1000 
	 loss: 18.6409, MinusLogProbMetric: 18.6409, val_loss: 18.4796, val_MinusLogProbMetric: 18.4796

Epoch 55: val_loss did not improve from 18.34349
196/196 - 65s - loss: 18.6409 - MinusLogProbMetric: 18.6409 - val_loss: 18.4796 - val_MinusLogProbMetric: 18.4796 - lr: 3.3333e-04 - 65s/epoch - 333ms/step
Epoch 56/1000
2023-09-26 16:35:43.795 
Epoch 56/1000 
	 loss: 18.6499, MinusLogProbMetric: 18.6499, val_loss: 18.9401, val_MinusLogProbMetric: 18.9401

Epoch 56: val_loss did not improve from 18.34349
196/196 - 65s - loss: 18.6499 - MinusLogProbMetric: 18.6499 - val_loss: 18.9401 - val_MinusLogProbMetric: 18.9401 - lr: 3.3333e-04 - 65s/epoch - 334ms/step
Epoch 57/1000
2023-09-26 16:36:48.713 
Epoch 57/1000 
	 loss: 18.5511, MinusLogProbMetric: 18.5511, val_loss: 18.8276, val_MinusLogProbMetric: 18.8276

Epoch 57: val_loss did not improve from 18.34349
196/196 - 65s - loss: 18.5511 - MinusLogProbMetric: 18.5511 - val_loss: 18.8276 - val_MinusLogProbMetric: 18.8276 - lr: 3.3333e-04 - 65s/epoch - 331ms/step
Epoch 58/1000
2023-09-26 16:37:53.830 
Epoch 58/1000 
	 loss: 18.4194, MinusLogProbMetric: 18.4194, val_loss: 18.6529, val_MinusLogProbMetric: 18.6529

Epoch 58: val_loss did not improve from 18.34349
196/196 - 65s - loss: 18.4194 - MinusLogProbMetric: 18.4194 - val_loss: 18.6529 - val_MinusLogProbMetric: 18.6529 - lr: 3.3333e-04 - 65s/epoch - 332ms/step
Epoch 59/1000
2023-09-26 16:38:59.595 
Epoch 59/1000 
	 loss: 18.4959, MinusLogProbMetric: 18.4959, val_loss: 19.3623, val_MinusLogProbMetric: 19.3623

Epoch 59: val_loss did not improve from 18.34349
196/196 - 66s - loss: 18.4959 - MinusLogProbMetric: 18.4959 - val_loss: 19.3623 - val_MinusLogProbMetric: 19.3623 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 60/1000
2023-09-26 16:40:04.530 
Epoch 60/1000 
	 loss: 18.5044, MinusLogProbMetric: 18.5044, val_loss: 19.2130, val_MinusLogProbMetric: 19.2130

Epoch 60: val_loss did not improve from 18.34349
196/196 - 65s - loss: 18.5044 - MinusLogProbMetric: 18.5044 - val_loss: 19.2130 - val_MinusLogProbMetric: 19.2130 - lr: 3.3333e-04 - 65s/epoch - 331ms/step
Epoch 61/1000
2023-09-26 16:41:10.653 
Epoch 61/1000 
	 loss: 18.5494, MinusLogProbMetric: 18.5494, val_loss: 18.9388, val_MinusLogProbMetric: 18.9388

Epoch 61: val_loss did not improve from 18.34349
196/196 - 66s - loss: 18.5494 - MinusLogProbMetric: 18.5494 - val_loss: 18.9388 - val_MinusLogProbMetric: 18.9388 - lr: 3.3333e-04 - 66s/epoch - 337ms/step
Epoch 62/1000
2023-09-26 16:42:16.920 
Epoch 62/1000 
	 loss: 18.3179, MinusLogProbMetric: 18.3179, val_loss: 18.9952, val_MinusLogProbMetric: 18.9952

Epoch 62: val_loss did not improve from 18.34349
196/196 - 66s - loss: 18.3179 - MinusLogProbMetric: 18.3179 - val_loss: 18.9952 - val_MinusLogProbMetric: 18.9952 - lr: 3.3333e-04 - 66s/epoch - 338ms/step
Epoch 63/1000
2023-09-26 16:43:21.814 
Epoch 63/1000 
	 loss: 18.5542, MinusLogProbMetric: 18.5542, val_loss: 18.3077, val_MinusLogProbMetric: 18.3077

Epoch 63: val_loss improved from 18.34349 to 18.30766, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_294/weights/best_weights.h5
196/196 - 66s - loss: 18.5542 - MinusLogProbMetric: 18.5542 - val_loss: 18.3077 - val_MinusLogProbMetric: 18.3077 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 64/1000
2023-09-26 16:44:27.525 
Epoch 64/1000 
	 loss: 18.3548, MinusLogProbMetric: 18.3548, val_loss: 18.7499, val_MinusLogProbMetric: 18.7499

Epoch 64: val_loss did not improve from 18.30766
196/196 - 65s - loss: 18.3548 - MinusLogProbMetric: 18.3548 - val_loss: 18.7499 - val_MinusLogProbMetric: 18.7499 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 65/1000
2023-09-26 16:45:33.856 
Epoch 65/1000 
	 loss: 18.3925, MinusLogProbMetric: 18.3925, val_loss: 19.1536, val_MinusLogProbMetric: 19.1536

Epoch 65: val_loss did not improve from 18.30766
196/196 - 66s - loss: 18.3925 - MinusLogProbMetric: 18.3925 - val_loss: 19.1536 - val_MinusLogProbMetric: 19.1536 - lr: 3.3333e-04 - 66s/epoch - 338ms/step
Epoch 66/1000
2023-09-26 16:46:39.858 
Epoch 66/1000 
	 loss: 18.3007, MinusLogProbMetric: 18.3007, val_loss: 18.9814, val_MinusLogProbMetric: 18.9814

Epoch 66: val_loss did not improve from 18.30766
196/196 - 66s - loss: 18.3007 - MinusLogProbMetric: 18.3007 - val_loss: 18.9814 - val_MinusLogProbMetric: 18.9814 - lr: 3.3333e-04 - 66s/epoch - 337ms/step
Epoch 67/1000
2023-09-26 16:47:45.673 
Epoch 67/1000 
	 loss: 18.4494, MinusLogProbMetric: 18.4494, val_loss: 19.0836, val_MinusLogProbMetric: 19.0836

Epoch 67: val_loss did not improve from 18.30766
196/196 - 66s - loss: 18.4494 - MinusLogProbMetric: 18.4494 - val_loss: 19.0836 - val_MinusLogProbMetric: 19.0836 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 68/1000
2023-09-26 16:48:50.799 
Epoch 68/1000 
	 loss: 18.3626, MinusLogProbMetric: 18.3626, val_loss: 18.6222, val_MinusLogProbMetric: 18.6222

Epoch 68: val_loss did not improve from 18.30766
196/196 - 65s - loss: 18.3626 - MinusLogProbMetric: 18.3626 - val_loss: 18.6222 - val_MinusLogProbMetric: 18.6222 - lr: 3.3333e-04 - 65s/epoch - 332ms/step
Epoch 69/1000
2023-09-26 16:49:55.784 
Epoch 69/1000 
	 loss: 18.3059, MinusLogProbMetric: 18.3059, val_loss: 18.1782, val_MinusLogProbMetric: 18.1782

Epoch 69: val_loss improved from 18.30766 to 18.17817, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_294/weights/best_weights.h5
196/196 - 66s - loss: 18.3059 - MinusLogProbMetric: 18.3059 - val_loss: 18.1782 - val_MinusLogProbMetric: 18.1782 - lr: 3.3333e-04 - 66s/epoch - 337ms/step
Epoch 70/1000
2023-09-26 16:51:01.308 
Epoch 70/1000 
	 loss: 18.3433, MinusLogProbMetric: 18.3433, val_loss: 18.1512, val_MinusLogProbMetric: 18.1512

Epoch 70: val_loss improved from 18.17817 to 18.15116, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_294/weights/best_weights.h5
196/196 - 65s - loss: 18.3433 - MinusLogProbMetric: 18.3433 - val_loss: 18.1512 - val_MinusLogProbMetric: 18.1512 - lr: 3.3333e-04 - 65s/epoch - 334ms/step
Epoch 71/1000
2023-09-26 16:52:06.891 
Epoch 71/1000 
	 loss: 18.2658, MinusLogProbMetric: 18.2658, val_loss: 18.6164, val_MinusLogProbMetric: 18.6164

Epoch 71: val_loss did not improve from 18.15116
196/196 - 65s - loss: 18.2658 - MinusLogProbMetric: 18.2658 - val_loss: 18.6164 - val_MinusLogProbMetric: 18.6164 - lr: 3.3333e-04 - 65s/epoch - 329ms/step
Epoch 72/1000
2023-09-26 16:53:11.898 
Epoch 72/1000 
	 loss: 18.3282, MinusLogProbMetric: 18.3282, val_loss: 18.6448, val_MinusLogProbMetric: 18.6448

Epoch 72: val_loss did not improve from 18.15116
196/196 - 65s - loss: 18.3282 - MinusLogProbMetric: 18.3282 - val_loss: 18.6448 - val_MinusLogProbMetric: 18.6448 - lr: 3.3333e-04 - 65s/epoch - 332ms/step
Epoch 73/1000
2023-09-26 16:54:16.372 
Epoch 73/1000 
	 loss: 18.2819, MinusLogProbMetric: 18.2819, val_loss: 18.6216, val_MinusLogProbMetric: 18.6216

Epoch 73: val_loss did not improve from 18.15116
196/196 - 64s - loss: 18.2819 - MinusLogProbMetric: 18.2819 - val_loss: 18.6216 - val_MinusLogProbMetric: 18.6216 - lr: 3.3333e-04 - 64s/epoch - 329ms/step
Epoch 74/1000
2023-09-26 16:55:21.535 
Epoch 74/1000 
	 loss: 18.3354, MinusLogProbMetric: 18.3354, val_loss: 18.1226, val_MinusLogProbMetric: 18.1226

Epoch 74: val_loss improved from 18.15116 to 18.12261, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_294/weights/best_weights.h5
196/196 - 66s - loss: 18.3354 - MinusLogProbMetric: 18.3354 - val_loss: 18.1226 - val_MinusLogProbMetric: 18.1226 - lr: 3.3333e-04 - 66s/epoch - 338ms/step
Epoch 75/1000
2023-09-26 16:56:26.420 
Epoch 75/1000 
	 loss: 18.3506, MinusLogProbMetric: 18.3506, val_loss: 18.7550, val_MinusLogProbMetric: 18.7550

Epoch 75: val_loss did not improve from 18.12261
196/196 - 64s - loss: 18.3506 - MinusLogProbMetric: 18.3506 - val_loss: 18.7550 - val_MinusLogProbMetric: 18.7550 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 76/1000
2023-09-26 16:57:31.189 
Epoch 76/1000 
	 loss: 18.1214, MinusLogProbMetric: 18.1214, val_loss: 18.8343, val_MinusLogProbMetric: 18.8343

Epoch 76: val_loss did not improve from 18.12261
196/196 - 65s - loss: 18.1214 - MinusLogProbMetric: 18.1214 - val_loss: 18.8343 - val_MinusLogProbMetric: 18.8343 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 77/1000
2023-09-26 16:58:36.158 
Epoch 77/1000 
	 loss: 18.4640, MinusLogProbMetric: 18.4640, val_loss: 19.0548, val_MinusLogProbMetric: 19.0548

Epoch 77: val_loss did not improve from 18.12261
196/196 - 65s - loss: 18.4640 - MinusLogProbMetric: 18.4640 - val_loss: 19.0548 - val_MinusLogProbMetric: 19.0548 - lr: 3.3333e-04 - 65s/epoch - 331ms/step
Epoch 78/1000
2023-09-26 16:59:40.173 
Epoch 78/1000 
	 loss: 18.0787, MinusLogProbMetric: 18.0787, val_loss: 18.3298, val_MinusLogProbMetric: 18.3298

Epoch 78: val_loss did not improve from 18.12261
196/196 - 64s - loss: 18.0787 - MinusLogProbMetric: 18.0787 - val_loss: 18.3298 - val_MinusLogProbMetric: 18.3298 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 79/1000
2023-09-26 17:00:44.894 
Epoch 79/1000 
	 loss: 18.2292, MinusLogProbMetric: 18.2292, val_loss: 18.5880, val_MinusLogProbMetric: 18.5880

Epoch 79: val_loss did not improve from 18.12261
196/196 - 65s - loss: 18.2292 - MinusLogProbMetric: 18.2292 - val_loss: 18.5880 - val_MinusLogProbMetric: 18.5880 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 80/1000
2023-09-26 17:01:50.198 
Epoch 80/1000 
	 loss: 18.2955, MinusLogProbMetric: 18.2955, val_loss: 19.6731, val_MinusLogProbMetric: 19.6731

Epoch 80: val_loss did not improve from 18.12261
196/196 - 65s - loss: 18.2955 - MinusLogProbMetric: 18.2955 - val_loss: 19.6731 - val_MinusLogProbMetric: 19.6731 - lr: 3.3333e-04 - 65s/epoch - 333ms/step
Epoch 81/1000
2023-09-26 17:02:55.981 
Epoch 81/1000 
	 loss: 18.2935, MinusLogProbMetric: 18.2935, val_loss: 17.9669, val_MinusLogProbMetric: 17.9669

Epoch 81: val_loss improved from 18.12261 to 17.96695, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_294/weights/best_weights.h5
196/196 - 67s - loss: 18.2935 - MinusLogProbMetric: 18.2935 - val_loss: 17.9669 - val_MinusLogProbMetric: 17.9669 - lr: 3.3333e-04 - 67s/epoch - 341ms/step
Epoch 82/1000
2023-09-26 17:04:01.955 
Epoch 82/1000 
	 loss: 18.1099, MinusLogProbMetric: 18.1099, val_loss: 18.1594, val_MinusLogProbMetric: 18.1594

Epoch 82: val_loss did not improve from 17.96695
196/196 - 65s - loss: 18.1099 - MinusLogProbMetric: 18.1099 - val_loss: 18.1594 - val_MinusLogProbMetric: 18.1594 - lr: 3.3333e-04 - 65s/epoch - 331ms/step
Epoch 83/1000
2023-09-26 17:05:07.317 
Epoch 83/1000 
	 loss: 18.2295, MinusLogProbMetric: 18.2295, val_loss: 18.1198, val_MinusLogProbMetric: 18.1198

Epoch 83: val_loss did not improve from 17.96695
196/196 - 65s - loss: 18.2295 - MinusLogProbMetric: 18.2295 - val_loss: 18.1198 - val_MinusLogProbMetric: 18.1198 - lr: 3.3333e-04 - 65s/epoch - 333ms/step
Epoch 84/1000
2023-09-26 17:06:12.341 
Epoch 84/1000 
	 loss: 18.2115, MinusLogProbMetric: 18.2115, val_loss: 18.9802, val_MinusLogProbMetric: 18.9802

Epoch 84: val_loss did not improve from 17.96695
196/196 - 65s - loss: 18.2115 - MinusLogProbMetric: 18.2115 - val_loss: 18.9802 - val_MinusLogProbMetric: 18.9802 - lr: 3.3333e-04 - 65s/epoch - 332ms/step
Epoch 85/1000
2023-09-26 17:07:16.798 
Epoch 85/1000 
	 loss: 18.0105, MinusLogProbMetric: 18.0105, val_loss: 18.1697, val_MinusLogProbMetric: 18.1697

Epoch 85: val_loss did not improve from 17.96695
196/196 - 64s - loss: 18.0105 - MinusLogProbMetric: 18.0105 - val_loss: 18.1697 - val_MinusLogProbMetric: 18.1697 - lr: 3.3333e-04 - 64s/epoch - 329ms/step
Epoch 86/1000
2023-09-26 17:08:20.701 
Epoch 86/1000 
	 loss: 18.0934, MinusLogProbMetric: 18.0934, val_loss: 18.0531, val_MinusLogProbMetric: 18.0531

Epoch 86: val_loss did not improve from 17.96695
196/196 - 64s - loss: 18.0934 - MinusLogProbMetric: 18.0934 - val_loss: 18.0531 - val_MinusLogProbMetric: 18.0531 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 87/1000
2023-09-26 17:09:24.970 
Epoch 87/1000 
	 loss: 18.3307, MinusLogProbMetric: 18.3307, val_loss: 20.7192, val_MinusLogProbMetric: 20.7192

Epoch 87: val_loss did not improve from 17.96695
196/196 - 64s - loss: 18.3307 - MinusLogProbMetric: 18.3307 - val_loss: 20.7192 - val_MinusLogProbMetric: 20.7192 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 88/1000
2023-09-26 17:10:30.240 
Epoch 88/1000 
	 loss: 18.1511, MinusLogProbMetric: 18.1511, val_loss: 19.4477, val_MinusLogProbMetric: 19.4477

Epoch 88: val_loss did not improve from 17.96695
196/196 - 65s - loss: 18.1511 - MinusLogProbMetric: 18.1511 - val_loss: 19.4477 - val_MinusLogProbMetric: 19.4477 - lr: 3.3333e-04 - 65s/epoch - 333ms/step
Epoch 89/1000
2023-09-26 17:11:35.129 
Epoch 89/1000 
	 loss: 17.9773, MinusLogProbMetric: 17.9773, val_loss: 18.2084, val_MinusLogProbMetric: 18.2084

Epoch 89: val_loss did not improve from 17.96695
196/196 - 65s - loss: 17.9773 - MinusLogProbMetric: 17.9773 - val_loss: 18.2084 - val_MinusLogProbMetric: 18.2084 - lr: 3.3333e-04 - 65s/epoch - 331ms/step
Epoch 90/1000
2023-09-26 17:12:40.364 
Epoch 90/1000 
	 loss: 18.1064, MinusLogProbMetric: 18.1064, val_loss: 18.6072, val_MinusLogProbMetric: 18.6072

Epoch 90: val_loss did not improve from 17.96695
196/196 - 65s - loss: 18.1064 - MinusLogProbMetric: 18.1064 - val_loss: 18.6072 - val_MinusLogProbMetric: 18.6072 - lr: 3.3333e-04 - 65s/epoch - 333ms/step
Epoch 91/1000
2023-09-26 17:13:45.941 
Epoch 91/1000 
	 loss: 18.0585, MinusLogProbMetric: 18.0585, val_loss: 18.5067, val_MinusLogProbMetric: 18.5067

Epoch 91: val_loss did not improve from 17.96695
196/196 - 66s - loss: 18.0585 - MinusLogProbMetric: 18.0585 - val_loss: 18.5067 - val_MinusLogProbMetric: 18.5067 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 92/1000
2023-09-26 17:14:51.291 
Epoch 92/1000 
	 loss: 18.1398, MinusLogProbMetric: 18.1398, val_loss: 19.3990, val_MinusLogProbMetric: 19.3990

Epoch 92: val_loss did not improve from 17.96695
196/196 - 65s - loss: 18.1398 - MinusLogProbMetric: 18.1398 - val_loss: 19.3990 - val_MinusLogProbMetric: 19.3990 - lr: 3.3333e-04 - 65s/epoch - 333ms/step
Epoch 93/1000
2023-09-26 17:15:55.775 
Epoch 93/1000 
	 loss: 18.0146, MinusLogProbMetric: 18.0146, val_loss: 18.7696, val_MinusLogProbMetric: 18.7696

Epoch 93: val_loss did not improve from 17.96695
196/196 - 64s - loss: 18.0146 - MinusLogProbMetric: 18.0146 - val_loss: 18.7696 - val_MinusLogProbMetric: 18.7696 - lr: 3.3333e-04 - 64s/epoch - 329ms/step
Epoch 94/1000
2023-09-26 17:17:00.534 
Epoch 94/1000 
	 loss: 17.9562, MinusLogProbMetric: 17.9562, val_loss: 18.3073, val_MinusLogProbMetric: 18.3073

Epoch 94: val_loss did not improve from 17.96695
196/196 - 65s - loss: 17.9562 - MinusLogProbMetric: 17.9562 - val_loss: 18.3073 - val_MinusLogProbMetric: 18.3073 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 95/1000
2023-09-26 17:18:06.461 
Epoch 95/1000 
	 loss: 17.9828, MinusLogProbMetric: 17.9828, val_loss: 17.9260, val_MinusLogProbMetric: 17.9260

Epoch 95: val_loss improved from 17.96695 to 17.92604, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_294/weights/best_weights.h5
196/196 - 67s - loss: 17.9828 - MinusLogProbMetric: 17.9828 - val_loss: 17.9260 - val_MinusLogProbMetric: 17.9260 - lr: 3.3333e-04 - 67s/epoch - 342ms/step
Epoch 96/1000
2023-09-26 17:19:13.823 
Epoch 96/1000 
	 loss: 18.0547, MinusLogProbMetric: 18.0547, val_loss: 18.3569, val_MinusLogProbMetric: 18.3569

Epoch 96: val_loss did not improve from 17.92604
196/196 - 66s - loss: 18.0547 - MinusLogProbMetric: 18.0547 - val_loss: 18.3569 - val_MinusLogProbMetric: 18.3569 - lr: 3.3333e-04 - 66s/epoch - 338ms/step
Epoch 97/1000
2023-09-26 17:20:18.904 
Epoch 97/1000 
	 loss: 17.9500, MinusLogProbMetric: 17.9500, val_loss: 17.8254, val_MinusLogProbMetric: 17.8254

Epoch 97: val_loss improved from 17.92604 to 17.82535, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_294/weights/best_weights.h5
196/196 - 66s - loss: 17.9500 - MinusLogProbMetric: 17.9500 - val_loss: 17.8254 - val_MinusLogProbMetric: 17.8254 - lr: 3.3333e-04 - 66s/epoch - 337ms/step
Epoch 98/1000
2023-09-26 17:21:25.601 
Epoch 98/1000 
	 loss: 18.0219, MinusLogProbMetric: 18.0219, val_loss: 18.3564, val_MinusLogProbMetric: 18.3564

Epoch 98: val_loss did not improve from 17.82535
196/196 - 66s - loss: 18.0219 - MinusLogProbMetric: 18.0219 - val_loss: 18.3564 - val_MinusLogProbMetric: 18.3564 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 99/1000
2023-09-26 17:22:30.873 
Epoch 99/1000 
	 loss: 18.0245, MinusLogProbMetric: 18.0245, val_loss: 17.9881, val_MinusLogProbMetric: 17.9881

Epoch 99: val_loss did not improve from 17.82535
196/196 - 65s - loss: 18.0245 - MinusLogProbMetric: 18.0245 - val_loss: 17.9881 - val_MinusLogProbMetric: 17.9881 - lr: 3.3333e-04 - 65s/epoch - 333ms/step
Epoch 100/1000
2023-09-26 17:23:35.724 
Epoch 100/1000 
	 loss: 17.9076, MinusLogProbMetric: 17.9076, val_loss: 18.0554, val_MinusLogProbMetric: 18.0554

Epoch 100: val_loss did not improve from 17.82535
196/196 - 65s - loss: 17.9076 - MinusLogProbMetric: 17.9076 - val_loss: 18.0554 - val_MinusLogProbMetric: 18.0554 - lr: 3.3333e-04 - 65s/epoch - 331ms/step
Epoch 101/1000
2023-09-26 17:24:41.989 
Epoch 101/1000 
	 loss: 18.0635, MinusLogProbMetric: 18.0635, val_loss: 18.3375, val_MinusLogProbMetric: 18.3375

Epoch 101: val_loss did not improve from 17.82535
196/196 - 66s - loss: 18.0635 - MinusLogProbMetric: 18.0635 - val_loss: 18.3375 - val_MinusLogProbMetric: 18.3375 - lr: 3.3333e-04 - 66s/epoch - 338ms/step
Epoch 102/1000
2023-09-26 17:25:46.113 
Epoch 102/1000 
	 loss: 18.0142, MinusLogProbMetric: 18.0142, val_loss: 18.0674, val_MinusLogProbMetric: 18.0674

Epoch 102: val_loss did not improve from 17.82535
196/196 - 64s - loss: 18.0142 - MinusLogProbMetric: 18.0142 - val_loss: 18.0674 - val_MinusLogProbMetric: 18.0674 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 103/1000
2023-09-26 17:26:51.537 
Epoch 103/1000 
	 loss: 17.8233, MinusLogProbMetric: 17.8233, val_loss: 18.5222, val_MinusLogProbMetric: 18.5222

Epoch 103: val_loss did not improve from 17.82535
196/196 - 65s - loss: 17.8233 - MinusLogProbMetric: 17.8233 - val_loss: 18.5222 - val_MinusLogProbMetric: 18.5222 - lr: 3.3333e-04 - 65s/epoch - 334ms/step
Epoch 104/1000
2023-09-26 17:27:56.971 
Epoch 104/1000 
	 loss: 17.9029, MinusLogProbMetric: 17.9029, val_loss: 18.1140, val_MinusLogProbMetric: 18.1140

Epoch 104: val_loss did not improve from 17.82535
196/196 - 65s - loss: 17.9029 - MinusLogProbMetric: 17.9029 - val_loss: 18.1140 - val_MinusLogProbMetric: 18.1140 - lr: 3.3333e-04 - 65s/epoch - 334ms/step
Epoch 105/1000
2023-09-26 17:29:02.558 
Epoch 105/1000 
	 loss: 17.8011, MinusLogProbMetric: 17.8011, val_loss: 18.3362, val_MinusLogProbMetric: 18.3362

Epoch 105: val_loss did not improve from 17.82535
196/196 - 66s - loss: 17.8011 - MinusLogProbMetric: 17.8011 - val_loss: 18.3362 - val_MinusLogProbMetric: 18.3362 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 106/1000
2023-09-26 17:30:08.449 
Epoch 106/1000 
	 loss: 17.9774, MinusLogProbMetric: 17.9774, val_loss: 18.3417, val_MinusLogProbMetric: 18.3417

Epoch 106: val_loss did not improve from 17.82535
196/196 - 66s - loss: 17.9774 - MinusLogProbMetric: 17.9774 - val_loss: 18.3417 - val_MinusLogProbMetric: 18.3417 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 107/1000
2023-09-26 17:31:14.216 
Epoch 107/1000 
	 loss: 17.9790, MinusLogProbMetric: 17.9790, val_loss: 18.0724, val_MinusLogProbMetric: 18.0724

Epoch 107: val_loss did not improve from 17.82535
196/196 - 66s - loss: 17.9790 - MinusLogProbMetric: 17.9790 - val_loss: 18.0724 - val_MinusLogProbMetric: 18.0724 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 108/1000
2023-09-26 17:32:20.220 
Epoch 108/1000 
	 loss: 17.9076, MinusLogProbMetric: 17.9076, val_loss: 18.0930, val_MinusLogProbMetric: 18.0930

Epoch 108: val_loss did not improve from 17.82535
196/196 - 66s - loss: 17.9076 - MinusLogProbMetric: 17.9076 - val_loss: 18.0930 - val_MinusLogProbMetric: 18.0930 - lr: 3.3333e-04 - 66s/epoch - 337ms/step
Epoch 109/1000
2023-09-26 17:33:26.534 
Epoch 109/1000 
	 loss: 17.8127, MinusLogProbMetric: 17.8127, val_loss: 18.1746, val_MinusLogProbMetric: 18.1746

Epoch 109: val_loss did not improve from 17.82535
196/196 - 66s - loss: 17.8127 - MinusLogProbMetric: 17.8127 - val_loss: 18.1746 - val_MinusLogProbMetric: 18.1746 - lr: 3.3333e-04 - 66s/epoch - 338ms/step
Epoch 110/1000
2023-09-26 17:34:31.627 
Epoch 110/1000 
	 loss: 17.9368, MinusLogProbMetric: 17.9368, val_loss: 17.9857, val_MinusLogProbMetric: 17.9857

Epoch 110: val_loss did not improve from 17.82535
196/196 - 65s - loss: 17.9368 - MinusLogProbMetric: 17.9368 - val_loss: 17.9857 - val_MinusLogProbMetric: 17.9857 - lr: 3.3333e-04 - 65s/epoch - 332ms/step
Epoch 111/1000
2023-09-26 17:35:36.520 
Epoch 111/1000 
	 loss: 17.8044, MinusLogProbMetric: 17.8044, val_loss: 18.4019, val_MinusLogProbMetric: 18.4019

Epoch 111: val_loss did not improve from 17.82535
196/196 - 65s - loss: 17.8044 - MinusLogProbMetric: 17.8044 - val_loss: 18.4019 - val_MinusLogProbMetric: 18.4019 - lr: 3.3333e-04 - 65s/epoch - 331ms/step
Epoch 112/1000
2023-09-26 17:36:42.392 
Epoch 112/1000 
	 loss: 17.9038, MinusLogProbMetric: 17.9038, val_loss: 17.8027, val_MinusLogProbMetric: 17.8027

Epoch 112: val_loss improved from 17.82535 to 17.80270, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_294/weights/best_weights.h5
196/196 - 67s - loss: 17.9038 - MinusLogProbMetric: 17.9038 - val_loss: 17.8027 - val_MinusLogProbMetric: 17.8027 - lr: 3.3333e-04 - 67s/epoch - 341ms/step
Epoch 113/1000
2023-09-26 17:37:47.922 
Epoch 113/1000 
	 loss: 17.8583, MinusLogProbMetric: 17.8583, val_loss: 17.8695, val_MinusLogProbMetric: 17.8695

Epoch 113: val_loss did not improve from 17.80270
196/196 - 65s - loss: 17.8583 - MinusLogProbMetric: 17.8583 - val_loss: 17.8695 - val_MinusLogProbMetric: 17.8695 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 114/1000
2023-09-26 17:38:53.575 
Epoch 114/1000 
	 loss: 17.7706, MinusLogProbMetric: 17.7706, val_loss: 17.6463, val_MinusLogProbMetric: 17.6463

Epoch 114: val_loss improved from 17.80270 to 17.64635, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_294/weights/best_weights.h5
196/196 - 67s - loss: 17.7706 - MinusLogProbMetric: 17.7706 - val_loss: 17.6463 - val_MinusLogProbMetric: 17.6463 - lr: 3.3333e-04 - 67s/epoch - 339ms/step
Epoch 115/1000
2023-09-26 17:39:59.778 
Epoch 115/1000 
	 loss: 17.7740, MinusLogProbMetric: 17.7740, val_loss: 17.9179, val_MinusLogProbMetric: 17.9179

Epoch 115: val_loss did not improve from 17.64635
196/196 - 65s - loss: 17.7740 - MinusLogProbMetric: 17.7740 - val_loss: 17.9179 - val_MinusLogProbMetric: 17.9179 - lr: 3.3333e-04 - 65s/epoch - 333ms/step
Epoch 116/1000
2023-09-26 17:41:05.461 
Epoch 116/1000 
	 loss: 17.8943, MinusLogProbMetric: 17.8943, val_loss: 19.1985, val_MinusLogProbMetric: 19.1985

Epoch 116: val_loss did not improve from 17.64635
196/196 - 66s - loss: 17.8943 - MinusLogProbMetric: 17.8943 - val_loss: 19.1985 - val_MinusLogProbMetric: 19.1985 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 117/1000
2023-09-26 17:42:10.628 
Epoch 117/1000 
	 loss: 17.7655, MinusLogProbMetric: 17.7655, val_loss: 18.1168, val_MinusLogProbMetric: 18.1168

Epoch 117: val_loss did not improve from 17.64635
196/196 - 65s - loss: 17.7655 - MinusLogProbMetric: 17.7655 - val_loss: 18.1168 - val_MinusLogProbMetric: 18.1168 - lr: 3.3333e-04 - 65s/epoch - 332ms/step
Epoch 118/1000
2023-09-26 17:43:15.448 
Epoch 118/1000 
	 loss: 17.7453, MinusLogProbMetric: 17.7453, val_loss: 18.0404, val_MinusLogProbMetric: 18.0404

Epoch 118: val_loss did not improve from 17.64635
196/196 - 65s - loss: 17.7453 - MinusLogProbMetric: 17.7453 - val_loss: 18.0404 - val_MinusLogProbMetric: 18.0404 - lr: 3.3333e-04 - 65s/epoch - 331ms/step
Epoch 119/1000
2023-09-26 17:44:21.421 
Epoch 119/1000 
	 loss: 17.7343, MinusLogProbMetric: 17.7343, val_loss: 17.7689, val_MinusLogProbMetric: 17.7689

Epoch 119: val_loss did not improve from 17.64635
196/196 - 66s - loss: 17.7343 - MinusLogProbMetric: 17.7343 - val_loss: 17.7689 - val_MinusLogProbMetric: 17.7689 - lr: 3.3333e-04 - 66s/epoch - 337ms/step
Epoch 120/1000
2023-09-26 17:45:27.207 
Epoch 120/1000 
	 loss: 17.7246, MinusLogProbMetric: 17.7246, val_loss: 18.4572, val_MinusLogProbMetric: 18.4572

Epoch 120: val_loss did not improve from 17.64635
196/196 - 66s - loss: 17.7246 - MinusLogProbMetric: 17.7246 - val_loss: 18.4572 - val_MinusLogProbMetric: 18.4572 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 121/1000
2023-09-26 17:46:32.736 
Epoch 121/1000 
	 loss: 17.9044, MinusLogProbMetric: 17.9044, val_loss: 17.9239, val_MinusLogProbMetric: 17.9239

Epoch 121: val_loss did not improve from 17.64635
196/196 - 66s - loss: 17.9044 - MinusLogProbMetric: 17.9044 - val_loss: 17.9239 - val_MinusLogProbMetric: 17.9239 - lr: 3.3333e-04 - 66s/epoch - 334ms/step
Epoch 122/1000
2023-09-26 17:47:38.026 
Epoch 122/1000 
	 loss: 17.6792, MinusLogProbMetric: 17.6792, val_loss: 18.4373, val_MinusLogProbMetric: 18.4373

Epoch 122: val_loss did not improve from 17.64635
196/196 - 65s - loss: 17.6792 - MinusLogProbMetric: 17.6792 - val_loss: 18.4373 - val_MinusLogProbMetric: 18.4373 - lr: 3.3333e-04 - 65s/epoch - 333ms/step
Epoch 123/1000
2023-09-26 17:48:43.653 
Epoch 123/1000 
	 loss: 17.7392, MinusLogProbMetric: 17.7392, val_loss: 17.9376, val_MinusLogProbMetric: 17.9376

Epoch 123: val_loss did not improve from 17.64635
196/196 - 66s - loss: 17.7392 - MinusLogProbMetric: 17.7392 - val_loss: 17.9376 - val_MinusLogProbMetric: 17.9376 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 124/1000
2023-09-26 17:49:49.056 
Epoch 124/1000 
	 loss: 17.6991, MinusLogProbMetric: 17.6991, val_loss: 18.1896, val_MinusLogProbMetric: 18.1896

Epoch 124: val_loss did not improve from 17.64635
196/196 - 65s - loss: 17.6991 - MinusLogProbMetric: 17.6991 - val_loss: 18.1896 - val_MinusLogProbMetric: 18.1896 - lr: 3.3333e-04 - 65s/epoch - 334ms/step
Epoch 125/1000
2023-09-26 17:50:54.233 
Epoch 125/1000 
	 loss: 17.7284, MinusLogProbMetric: 17.7284, val_loss: 17.7220, val_MinusLogProbMetric: 17.7220

Epoch 125: val_loss did not improve from 17.64635
196/196 - 65s - loss: 17.7284 - MinusLogProbMetric: 17.7284 - val_loss: 17.7220 - val_MinusLogProbMetric: 17.7220 - lr: 3.3333e-04 - 65s/epoch - 333ms/step
Epoch 126/1000
2023-09-26 17:52:00.485 
Epoch 126/1000 
	 loss: 17.7004, MinusLogProbMetric: 17.7004, val_loss: 18.0096, val_MinusLogProbMetric: 18.0096

Epoch 126: val_loss did not improve from 17.64635
196/196 - 66s - loss: 17.7004 - MinusLogProbMetric: 17.7004 - val_loss: 18.0096 - val_MinusLogProbMetric: 18.0096 - lr: 3.3333e-04 - 66s/epoch - 338ms/step
Epoch 127/1000
2023-09-26 17:53:05.837 
Epoch 127/1000 
	 loss: 17.7480, MinusLogProbMetric: 17.7480, val_loss: 18.0014, val_MinusLogProbMetric: 18.0014

Epoch 127: val_loss did not improve from 17.64635
196/196 - 65s - loss: 17.7480 - MinusLogProbMetric: 17.7480 - val_loss: 18.0014 - val_MinusLogProbMetric: 18.0014 - lr: 3.3333e-04 - 65s/epoch - 333ms/step
Epoch 128/1000
2023-09-26 17:54:12.894 
Epoch 128/1000 
	 loss: 17.7180, MinusLogProbMetric: 17.7180, val_loss: 17.9829, val_MinusLogProbMetric: 17.9829

Epoch 128: val_loss did not improve from 17.64635
196/196 - 67s - loss: 17.7180 - MinusLogProbMetric: 17.7180 - val_loss: 17.9829 - val_MinusLogProbMetric: 17.9829 - lr: 3.3333e-04 - 67s/epoch - 342ms/step
Epoch 129/1000
2023-09-26 17:55:18.103 
Epoch 129/1000 
	 loss: 17.7405, MinusLogProbMetric: 17.7405, val_loss: 18.4184, val_MinusLogProbMetric: 18.4184

Epoch 129: val_loss did not improve from 17.64635
196/196 - 65s - loss: 17.7405 - MinusLogProbMetric: 17.7405 - val_loss: 18.4184 - val_MinusLogProbMetric: 18.4184 - lr: 3.3333e-04 - 65s/epoch - 333ms/step
Epoch 130/1000
2023-09-26 17:56:23.187 
Epoch 130/1000 
	 loss: 17.6955, MinusLogProbMetric: 17.6955, val_loss: 17.8123, val_MinusLogProbMetric: 17.8123

Epoch 130: val_loss did not improve from 17.64635
196/196 - 65s - loss: 17.6955 - MinusLogProbMetric: 17.6955 - val_loss: 17.8123 - val_MinusLogProbMetric: 17.8123 - lr: 3.3333e-04 - 65s/epoch - 332ms/step
Epoch 131/1000
2023-09-26 17:57:29.250 
Epoch 131/1000 
	 loss: 17.7213, MinusLogProbMetric: 17.7213, val_loss: 18.2332, val_MinusLogProbMetric: 18.2332

Epoch 131: val_loss did not improve from 17.64635
196/196 - 66s - loss: 17.7213 - MinusLogProbMetric: 17.7213 - val_loss: 18.2332 - val_MinusLogProbMetric: 18.2332 - lr: 3.3333e-04 - 66s/epoch - 337ms/step
Epoch 132/1000
2023-09-26 17:58:36.039 
Epoch 132/1000 
	 loss: 17.7033, MinusLogProbMetric: 17.7033, val_loss: 18.0658, val_MinusLogProbMetric: 18.0658

Epoch 132: val_loss did not improve from 17.64635
196/196 - 67s - loss: 17.7033 - MinusLogProbMetric: 17.7033 - val_loss: 18.0658 - val_MinusLogProbMetric: 18.0658 - lr: 3.3333e-04 - 67s/epoch - 341ms/step
Epoch 133/1000
2023-09-26 17:59:41.476 
Epoch 133/1000 
	 loss: 17.6428, MinusLogProbMetric: 17.6428, val_loss: 17.6605, val_MinusLogProbMetric: 17.6605

Epoch 133: val_loss did not improve from 17.64635
196/196 - 65s - loss: 17.6428 - MinusLogProbMetric: 17.6428 - val_loss: 17.6605 - val_MinusLogProbMetric: 17.6605 - lr: 3.3333e-04 - 65s/epoch - 334ms/step
Epoch 134/1000
2023-09-26 18:00:47.634 
Epoch 134/1000 
	 loss: 17.6408, MinusLogProbMetric: 17.6408, val_loss: 18.2476, val_MinusLogProbMetric: 18.2476

Epoch 134: val_loss did not improve from 17.64635
196/196 - 66s - loss: 17.6408 - MinusLogProbMetric: 17.6408 - val_loss: 18.2476 - val_MinusLogProbMetric: 18.2476 - lr: 3.3333e-04 - 66s/epoch - 338ms/step
Epoch 135/1000
2023-09-26 18:01:55.756 
Epoch 135/1000 
	 loss: 17.6669, MinusLogProbMetric: 17.6669, val_loss: 18.2144, val_MinusLogProbMetric: 18.2144

Epoch 135: val_loss did not improve from 17.64635
196/196 - 68s - loss: 17.6669 - MinusLogProbMetric: 17.6669 - val_loss: 18.2144 - val_MinusLogProbMetric: 18.2144 - lr: 3.3333e-04 - 68s/epoch - 348ms/step
Epoch 136/1000
2023-09-26 18:03:01.427 
Epoch 136/1000 
	 loss: 17.6422, MinusLogProbMetric: 17.6422, val_loss: 17.9038, val_MinusLogProbMetric: 17.9038

Epoch 136: val_loss did not improve from 17.64635
196/196 - 66s - loss: 17.6422 - MinusLogProbMetric: 17.6422 - val_loss: 17.9038 - val_MinusLogProbMetric: 17.9038 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 137/1000
2023-09-26 18:04:06.629 
Epoch 137/1000 
	 loss: 17.6691, MinusLogProbMetric: 17.6691, val_loss: 18.7182, val_MinusLogProbMetric: 18.7182

Epoch 137: val_loss did not improve from 17.64635
196/196 - 65s - loss: 17.6691 - MinusLogProbMetric: 17.6691 - val_loss: 18.7182 - val_MinusLogProbMetric: 18.7182 - lr: 3.3333e-04 - 65s/epoch - 333ms/step
Epoch 138/1000
2023-09-26 18:05:12.070 
Epoch 138/1000 
	 loss: 17.7157, MinusLogProbMetric: 17.7157, val_loss: 18.1172, val_MinusLogProbMetric: 18.1172

Epoch 138: val_loss did not improve from 17.64635
196/196 - 65s - loss: 17.7157 - MinusLogProbMetric: 17.7157 - val_loss: 18.1172 - val_MinusLogProbMetric: 18.1172 - lr: 3.3333e-04 - 65s/epoch - 334ms/step
Epoch 139/1000
2023-09-26 18:06:18.257 
Epoch 139/1000 
	 loss: 17.6335, MinusLogProbMetric: 17.6335, val_loss: 18.2151, val_MinusLogProbMetric: 18.2151

Epoch 139: val_loss did not improve from 17.64635
196/196 - 66s - loss: 17.6335 - MinusLogProbMetric: 17.6335 - val_loss: 18.2151 - val_MinusLogProbMetric: 18.2151 - lr: 3.3333e-04 - 66s/epoch - 338ms/step
Epoch 140/1000
2023-09-26 18:07:24.063 
Epoch 140/1000 
	 loss: 17.6199, MinusLogProbMetric: 17.6199, val_loss: 20.0549, val_MinusLogProbMetric: 20.0549

Epoch 140: val_loss did not improve from 17.64635
196/196 - 66s - loss: 17.6199 - MinusLogProbMetric: 17.6199 - val_loss: 20.0549 - val_MinusLogProbMetric: 20.0549 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 141/1000
2023-09-26 18:08:29.177 
Epoch 141/1000 
	 loss: 17.6865, MinusLogProbMetric: 17.6865, val_loss: 17.8007, val_MinusLogProbMetric: 17.8007

Epoch 141: val_loss did not improve from 17.64635
196/196 - 65s - loss: 17.6865 - MinusLogProbMetric: 17.6865 - val_loss: 17.8007 - val_MinusLogProbMetric: 17.8007 - lr: 3.3333e-04 - 65s/epoch - 332ms/step
Epoch 142/1000
2023-09-26 18:09:35.584 
Epoch 142/1000 
	 loss: 17.5959, MinusLogProbMetric: 17.5959, val_loss: 17.7104, val_MinusLogProbMetric: 17.7104

Epoch 142: val_loss did not improve from 17.64635
196/196 - 66s - loss: 17.5959 - MinusLogProbMetric: 17.5959 - val_loss: 17.7104 - val_MinusLogProbMetric: 17.7104 - lr: 3.3333e-04 - 66s/epoch - 339ms/step
Epoch 143/1000
2023-09-26 18:10:41.337 
Epoch 143/1000 
	 loss: 17.6503, MinusLogProbMetric: 17.6503, val_loss: 18.0966, val_MinusLogProbMetric: 18.0966

Epoch 143: val_loss did not improve from 17.64635
196/196 - 66s - loss: 17.6503 - MinusLogProbMetric: 17.6503 - val_loss: 18.0966 - val_MinusLogProbMetric: 18.0966 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 144/1000
2023-09-26 18:11:48.358 
Epoch 144/1000 
	 loss: 17.5661, MinusLogProbMetric: 17.5661, val_loss: 18.2419, val_MinusLogProbMetric: 18.2419

Epoch 144: val_loss did not improve from 17.64635
196/196 - 67s - loss: 17.5661 - MinusLogProbMetric: 17.5661 - val_loss: 18.2419 - val_MinusLogProbMetric: 18.2419 - lr: 3.3333e-04 - 67s/epoch - 342ms/step
Epoch 145/1000
2023-09-26 18:12:53.562 
Epoch 145/1000 
	 loss: 17.6816, MinusLogProbMetric: 17.6816, val_loss: 17.8687, val_MinusLogProbMetric: 17.8687

Epoch 145: val_loss did not improve from 17.64635
196/196 - 65s - loss: 17.6816 - MinusLogProbMetric: 17.6816 - val_loss: 17.8687 - val_MinusLogProbMetric: 17.8687 - lr: 3.3333e-04 - 65s/epoch - 333ms/step
Epoch 146/1000
2023-09-26 18:14:00.664 
Epoch 146/1000 
	 loss: 17.5063, MinusLogProbMetric: 17.5063, val_loss: 17.8246, val_MinusLogProbMetric: 17.8246

Epoch 146: val_loss did not improve from 17.64635
196/196 - 67s - loss: 17.5063 - MinusLogProbMetric: 17.5063 - val_loss: 17.8246 - val_MinusLogProbMetric: 17.8246 - lr: 3.3333e-04 - 67s/epoch - 342ms/step
Epoch 147/1000
2023-09-26 18:15:06.696 
Epoch 147/1000 
	 loss: 17.6151, MinusLogProbMetric: 17.6151, val_loss: 17.9450, val_MinusLogProbMetric: 17.9450

Epoch 147: val_loss did not improve from 17.64635
196/196 - 66s - loss: 17.6151 - MinusLogProbMetric: 17.6151 - val_loss: 17.9450 - val_MinusLogProbMetric: 17.9450 - lr: 3.3333e-04 - 66s/epoch - 337ms/step
Epoch 148/1000
2023-09-26 18:16:13.115 
Epoch 148/1000 
	 loss: 17.5230, MinusLogProbMetric: 17.5230, val_loss: 18.0126, val_MinusLogProbMetric: 18.0126

Epoch 148: val_loss did not improve from 17.64635
196/196 - 66s - loss: 17.5230 - MinusLogProbMetric: 17.5230 - val_loss: 18.0126 - val_MinusLogProbMetric: 18.0126 - lr: 3.3333e-04 - 66s/epoch - 339ms/step
Epoch 149/1000
2023-09-26 18:17:19.455 
Epoch 149/1000 
	 loss: 17.5643, MinusLogProbMetric: 17.5643, val_loss: 17.9131, val_MinusLogProbMetric: 17.9131

Epoch 149: val_loss did not improve from 17.64635
196/196 - 66s - loss: 17.5643 - MinusLogProbMetric: 17.5643 - val_loss: 17.9131 - val_MinusLogProbMetric: 17.9131 - lr: 3.3333e-04 - 66s/epoch - 338ms/step
Epoch 150/1000
2023-09-26 18:18:24.804 
Epoch 150/1000 
	 loss: 17.5284, MinusLogProbMetric: 17.5284, val_loss: 17.5643, val_MinusLogProbMetric: 17.5643

Epoch 150: val_loss improved from 17.64635 to 17.56425, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_294/weights/best_weights.h5
196/196 - 66s - loss: 17.5284 - MinusLogProbMetric: 17.5284 - val_loss: 17.5643 - val_MinusLogProbMetric: 17.5643 - lr: 3.3333e-04 - 66s/epoch - 339ms/step
Epoch 151/1000
2023-09-26 18:19:31.183 
Epoch 151/1000 
	 loss: 17.5214, MinusLogProbMetric: 17.5214, val_loss: 17.9390, val_MinusLogProbMetric: 17.9390

Epoch 151: val_loss did not improve from 17.56425
196/196 - 65s - loss: 17.5214 - MinusLogProbMetric: 17.5214 - val_loss: 17.9390 - val_MinusLogProbMetric: 17.9390 - lr: 3.3333e-04 - 65s/epoch - 333ms/step
Epoch 152/1000
2023-09-26 18:20:37.363 
Epoch 152/1000 
	 loss: 17.5940, MinusLogProbMetric: 17.5940, val_loss: 17.8596, val_MinusLogProbMetric: 17.8596

Epoch 152: val_loss did not improve from 17.56425
196/196 - 66s - loss: 17.5940 - MinusLogProbMetric: 17.5940 - val_loss: 17.8596 - val_MinusLogProbMetric: 17.8596 - lr: 3.3333e-04 - 66s/epoch - 338ms/step
Epoch 153/1000
2023-09-26 18:21:42.529 
Epoch 153/1000 
	 loss: 17.5199, MinusLogProbMetric: 17.5199, val_loss: 18.3309, val_MinusLogProbMetric: 18.3309

Epoch 153: val_loss did not improve from 17.56425
196/196 - 65s - loss: 17.5199 - MinusLogProbMetric: 17.5199 - val_loss: 18.3309 - val_MinusLogProbMetric: 18.3309 - lr: 3.3333e-04 - 65s/epoch - 332ms/step
Epoch 154/1000
2023-09-26 18:22:49.058 
Epoch 154/1000 
	 loss: 17.5548, MinusLogProbMetric: 17.5548, val_loss: 18.2095, val_MinusLogProbMetric: 18.2095

Epoch 154: val_loss did not improve from 17.56425
196/196 - 67s - loss: 17.5548 - MinusLogProbMetric: 17.5548 - val_loss: 18.2095 - val_MinusLogProbMetric: 18.2095 - lr: 3.3333e-04 - 67s/epoch - 339ms/step
Epoch 155/1000
2023-09-26 18:23:55.578 
Epoch 155/1000 
	 loss: 17.6157, MinusLogProbMetric: 17.6157, val_loss: 18.0852, val_MinusLogProbMetric: 18.0852

Epoch 155: val_loss did not improve from 17.56425
196/196 - 67s - loss: 17.6157 - MinusLogProbMetric: 17.6157 - val_loss: 18.0852 - val_MinusLogProbMetric: 18.0852 - lr: 3.3333e-04 - 67s/epoch - 339ms/step
Epoch 156/1000
2023-09-26 18:25:00.446 
Epoch 156/1000 
	 loss: 17.5766, MinusLogProbMetric: 17.5766, val_loss: 18.1588, val_MinusLogProbMetric: 18.1588

Epoch 156: val_loss did not improve from 17.56425
196/196 - 65s - loss: 17.5766 - MinusLogProbMetric: 17.5766 - val_loss: 18.1588 - val_MinusLogProbMetric: 18.1588 - lr: 3.3333e-04 - 65s/epoch - 331ms/step
Epoch 157/1000
2023-09-26 18:26:05.036 
Epoch 157/1000 
	 loss: 17.5085, MinusLogProbMetric: 17.5085, val_loss: 18.1062, val_MinusLogProbMetric: 18.1062

Epoch 157: val_loss did not improve from 17.56425
196/196 - 65s - loss: 17.5085 - MinusLogProbMetric: 17.5085 - val_loss: 18.1062 - val_MinusLogProbMetric: 18.1062 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 158/1000
2023-09-26 18:27:11.058 
Epoch 158/1000 
	 loss: 17.5425, MinusLogProbMetric: 17.5425, val_loss: 17.6662, val_MinusLogProbMetric: 17.6662

Epoch 158: val_loss did not improve from 17.56425
196/196 - 66s - loss: 17.5425 - MinusLogProbMetric: 17.5425 - val_loss: 17.6662 - val_MinusLogProbMetric: 17.6662 - lr: 3.3333e-04 - 66s/epoch - 337ms/step
Epoch 159/1000
2023-09-26 18:28:15.399 
Epoch 159/1000 
	 loss: 17.5376, MinusLogProbMetric: 17.5376, val_loss: 17.6756, val_MinusLogProbMetric: 17.6756

Epoch 159: val_loss did not improve from 17.56425
196/196 - 64s - loss: 17.5376 - MinusLogProbMetric: 17.5376 - val_loss: 17.6756 - val_MinusLogProbMetric: 17.6756 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 160/1000
2023-09-26 18:29:20.008 
Epoch 160/1000 
	 loss: 17.5766, MinusLogProbMetric: 17.5766, val_loss: 17.8650, val_MinusLogProbMetric: 17.8650

Epoch 160: val_loss did not improve from 17.56425
196/196 - 65s - loss: 17.5766 - MinusLogProbMetric: 17.5766 - val_loss: 17.8650 - val_MinusLogProbMetric: 17.8650 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 161/1000
2023-09-26 18:30:25.982 
Epoch 161/1000 
	 loss: 17.4843, MinusLogProbMetric: 17.4843, val_loss: 18.0457, val_MinusLogProbMetric: 18.0457

Epoch 161: val_loss did not improve from 17.56425
196/196 - 66s - loss: 17.4843 - MinusLogProbMetric: 17.4843 - val_loss: 18.0457 - val_MinusLogProbMetric: 18.0457 - lr: 3.3333e-04 - 66s/epoch - 337ms/step
Epoch 162/1000
2023-09-26 18:31:32.855 
Epoch 162/1000 
	 loss: 17.4891, MinusLogProbMetric: 17.4891, val_loss: 18.2622, val_MinusLogProbMetric: 18.2622

Epoch 162: val_loss did not improve from 17.56425
196/196 - 67s - loss: 17.4891 - MinusLogProbMetric: 17.4891 - val_loss: 18.2622 - val_MinusLogProbMetric: 18.2622 - lr: 3.3333e-04 - 67s/epoch - 341ms/step
Epoch 163/1000
2023-09-26 18:32:38.600 
Epoch 163/1000 
	 loss: 17.5530, MinusLogProbMetric: 17.5530, val_loss: 19.6859, val_MinusLogProbMetric: 19.6859

Epoch 163: val_loss did not improve from 17.56425
196/196 - 66s - loss: 17.5530 - MinusLogProbMetric: 17.5530 - val_loss: 19.6859 - val_MinusLogProbMetric: 19.6859 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 164/1000
2023-09-26 18:33:45.232 
Epoch 164/1000 
	 loss: 17.5651, MinusLogProbMetric: 17.5651, val_loss: 18.3419, val_MinusLogProbMetric: 18.3419

Epoch 164: val_loss did not improve from 17.56425
196/196 - 67s - loss: 17.5651 - MinusLogProbMetric: 17.5651 - val_loss: 18.3419 - val_MinusLogProbMetric: 18.3419 - lr: 3.3333e-04 - 67s/epoch - 340ms/step
Epoch 165/1000
2023-09-26 18:34:50.366 
Epoch 165/1000 
	 loss: 17.5215, MinusLogProbMetric: 17.5215, val_loss: 17.7386, val_MinusLogProbMetric: 17.7386

Epoch 165: val_loss did not improve from 17.56425
196/196 - 65s - loss: 17.5215 - MinusLogProbMetric: 17.5215 - val_loss: 17.7386 - val_MinusLogProbMetric: 17.7386 - lr: 3.3333e-04 - 65s/epoch - 332ms/step
Epoch 166/1000
2023-09-26 18:35:55.095 
Epoch 166/1000 
	 loss: 17.5101, MinusLogProbMetric: 17.5101, val_loss: 18.1753, val_MinusLogProbMetric: 18.1753

Epoch 166: val_loss did not improve from 17.56425
196/196 - 65s - loss: 17.5101 - MinusLogProbMetric: 17.5101 - val_loss: 18.1753 - val_MinusLogProbMetric: 18.1753 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 167/1000
2023-09-26 18:37:00.845 
Epoch 167/1000 
	 loss: 17.4726, MinusLogProbMetric: 17.4726, val_loss: 17.8783, val_MinusLogProbMetric: 17.8783

Epoch 167: val_loss did not improve from 17.56425
196/196 - 66s - loss: 17.4726 - MinusLogProbMetric: 17.4726 - val_loss: 17.8783 - val_MinusLogProbMetric: 17.8783 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 168/1000
2023-09-26 18:38:06.178 
Epoch 168/1000 
	 loss: 17.5797, MinusLogProbMetric: 17.5797, val_loss: 17.5098, val_MinusLogProbMetric: 17.5098

Epoch 168: val_loss improved from 17.56425 to 17.50977, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_294/weights/best_weights.h5
196/196 - 66s - loss: 17.5797 - MinusLogProbMetric: 17.5797 - val_loss: 17.5098 - val_MinusLogProbMetric: 17.5098 - lr: 3.3333e-04 - 66s/epoch - 339ms/step
Epoch 169/1000
2023-09-26 18:39:13.110 
Epoch 169/1000 
	 loss: 17.4175, MinusLogProbMetric: 17.4175, val_loss: 17.7741, val_MinusLogProbMetric: 17.7741

Epoch 169: val_loss did not improve from 17.50977
196/196 - 66s - loss: 17.4175 - MinusLogProbMetric: 17.4175 - val_loss: 17.7741 - val_MinusLogProbMetric: 17.7741 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 170/1000
2023-09-26 18:40:17.265 
Epoch 170/1000 
	 loss: 17.5305, MinusLogProbMetric: 17.5305, val_loss: 17.7246, val_MinusLogProbMetric: 17.7246

Epoch 170: val_loss did not improve from 17.50977
196/196 - 64s - loss: 17.5305 - MinusLogProbMetric: 17.5305 - val_loss: 17.7246 - val_MinusLogProbMetric: 17.7246 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 171/1000
2023-09-26 18:41:21.643 
Epoch 171/1000 
	 loss: 17.4429, MinusLogProbMetric: 17.4429, val_loss: 17.6375, val_MinusLogProbMetric: 17.6375

Epoch 171: val_loss did not improve from 17.50977
196/196 - 64s - loss: 17.4429 - MinusLogProbMetric: 17.4429 - val_loss: 17.6375 - val_MinusLogProbMetric: 17.6375 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 172/1000
2023-09-26 18:42:26.873 
Epoch 172/1000 
	 loss: 17.4542, MinusLogProbMetric: 17.4542, val_loss: 18.4567, val_MinusLogProbMetric: 18.4567

Epoch 172: val_loss did not improve from 17.50977
196/196 - 65s - loss: 17.4542 - MinusLogProbMetric: 17.4542 - val_loss: 18.4567 - val_MinusLogProbMetric: 18.4567 - lr: 3.3333e-04 - 65s/epoch - 333ms/step
Epoch 173/1000
2023-09-26 18:43:32.458 
Epoch 173/1000 
	 loss: 17.4701, MinusLogProbMetric: 17.4701, val_loss: 17.9716, val_MinusLogProbMetric: 17.9716

Epoch 173: val_loss did not improve from 17.50977
196/196 - 66s - loss: 17.4701 - MinusLogProbMetric: 17.4701 - val_loss: 17.9716 - val_MinusLogProbMetric: 17.9716 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 174/1000
2023-09-26 18:44:37.567 
Epoch 174/1000 
	 loss: 17.4149, MinusLogProbMetric: 17.4149, val_loss: 18.4622, val_MinusLogProbMetric: 18.4622

Epoch 174: val_loss did not improve from 17.50977
196/196 - 65s - loss: 17.4149 - MinusLogProbMetric: 17.4149 - val_loss: 18.4622 - val_MinusLogProbMetric: 18.4622 - lr: 3.3333e-04 - 65s/epoch - 332ms/step
Epoch 175/1000
2023-09-26 18:45:41.691 
Epoch 175/1000 
	 loss: 17.4740, MinusLogProbMetric: 17.4740, val_loss: 17.5229, val_MinusLogProbMetric: 17.5229

Epoch 175: val_loss did not improve from 17.50977
196/196 - 64s - loss: 17.4740 - MinusLogProbMetric: 17.4740 - val_loss: 17.5229 - val_MinusLogProbMetric: 17.5229 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 176/1000
2023-09-26 18:46:47.360 
Epoch 176/1000 
	 loss: 17.4339, MinusLogProbMetric: 17.4339, val_loss: 18.3405, val_MinusLogProbMetric: 18.3405

Epoch 176: val_loss did not improve from 17.50977
196/196 - 66s - loss: 17.4339 - MinusLogProbMetric: 17.4339 - val_loss: 18.3405 - val_MinusLogProbMetric: 18.3405 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 177/1000
2023-09-26 18:47:52.255 
Epoch 177/1000 
	 loss: 17.4705, MinusLogProbMetric: 17.4705, val_loss: 17.9777, val_MinusLogProbMetric: 17.9777

Epoch 177: val_loss did not improve from 17.50977
196/196 - 65s - loss: 17.4705 - MinusLogProbMetric: 17.4705 - val_loss: 17.9777 - val_MinusLogProbMetric: 17.9777 - lr: 3.3333e-04 - 65s/epoch - 331ms/step
Epoch 178/1000
2023-09-26 18:48:58.292 
Epoch 178/1000 
	 loss: 17.3909, MinusLogProbMetric: 17.3909, val_loss: 18.0199, val_MinusLogProbMetric: 18.0199

Epoch 178: val_loss did not improve from 17.50977
196/196 - 66s - loss: 17.3909 - MinusLogProbMetric: 17.3909 - val_loss: 18.0199 - val_MinusLogProbMetric: 18.0199 - lr: 3.3333e-04 - 66s/epoch - 337ms/step
Epoch 179/1000
2023-09-26 18:50:05.129 
Epoch 179/1000 
	 loss: 17.4581, MinusLogProbMetric: 17.4581, val_loss: 17.7427, val_MinusLogProbMetric: 17.7427

Epoch 179: val_loss did not improve from 17.50977
196/196 - 67s - loss: 17.4581 - MinusLogProbMetric: 17.4581 - val_loss: 17.7427 - val_MinusLogProbMetric: 17.7427 - lr: 3.3333e-04 - 67s/epoch - 341ms/step
Epoch 180/1000
2023-09-26 18:51:08.555 
Epoch 180/1000 
	 loss: 17.4424, MinusLogProbMetric: 17.4424, val_loss: 17.6553, val_MinusLogProbMetric: 17.6553

Epoch 180: val_loss did not improve from 17.50977
196/196 - 63s - loss: 17.4424 - MinusLogProbMetric: 17.4424 - val_loss: 17.6553 - val_MinusLogProbMetric: 17.6553 - lr: 3.3333e-04 - 63s/epoch - 324ms/step
Epoch 181/1000
2023-09-26 18:52:13.203 
Epoch 181/1000 
	 loss: 17.4247, MinusLogProbMetric: 17.4247, val_loss: 17.5671, val_MinusLogProbMetric: 17.5671

Epoch 181: val_loss did not improve from 17.50977
196/196 - 65s - loss: 17.4247 - MinusLogProbMetric: 17.4247 - val_loss: 17.5671 - val_MinusLogProbMetric: 17.5671 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 182/1000
2023-09-26 18:53:17.890 
Epoch 182/1000 
	 loss: 17.3058, MinusLogProbMetric: 17.3058, val_loss: 17.5560, val_MinusLogProbMetric: 17.5560

Epoch 182: val_loss did not improve from 17.50977
196/196 - 65s - loss: 17.3058 - MinusLogProbMetric: 17.3058 - val_loss: 17.5560 - val_MinusLogProbMetric: 17.5560 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 183/1000
2023-09-26 18:54:23.531 
Epoch 183/1000 
	 loss: 17.4106, MinusLogProbMetric: 17.4106, val_loss: 17.6424, val_MinusLogProbMetric: 17.6424

Epoch 183: val_loss did not improve from 17.50977
196/196 - 66s - loss: 17.4106 - MinusLogProbMetric: 17.4106 - val_loss: 17.6424 - val_MinusLogProbMetric: 17.6424 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 184/1000
2023-09-26 18:55:28.724 
Epoch 184/1000 
	 loss: 17.3982, MinusLogProbMetric: 17.3982, val_loss: 17.7643, val_MinusLogProbMetric: 17.7643

Epoch 184: val_loss did not improve from 17.50977
196/196 - 65s - loss: 17.3982 - MinusLogProbMetric: 17.3982 - val_loss: 17.7643 - val_MinusLogProbMetric: 17.7643 - lr: 3.3333e-04 - 65s/epoch - 333ms/step
Epoch 185/1000
2023-09-26 18:56:33.390 
Epoch 185/1000 
	 loss: 17.4060, MinusLogProbMetric: 17.4060, val_loss: 17.8261, val_MinusLogProbMetric: 17.8261

Epoch 185: val_loss did not improve from 17.50977
196/196 - 65s - loss: 17.4060 - MinusLogProbMetric: 17.4060 - val_loss: 17.8261 - val_MinusLogProbMetric: 17.8261 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 186/1000
2023-09-26 18:57:38.386 
Epoch 186/1000 
	 loss: 17.3986, MinusLogProbMetric: 17.3986, val_loss: 17.7105, val_MinusLogProbMetric: 17.7105

Epoch 186: val_loss did not improve from 17.50977
196/196 - 65s - loss: 17.3986 - MinusLogProbMetric: 17.3986 - val_loss: 17.7105 - val_MinusLogProbMetric: 17.7105 - lr: 3.3333e-04 - 65s/epoch - 332ms/step
Epoch 187/1000
2023-09-26 18:58:43.595 
Epoch 187/1000 
	 loss: 17.3521, MinusLogProbMetric: 17.3521, val_loss: 17.6182, val_MinusLogProbMetric: 17.6182

Epoch 187: val_loss did not improve from 17.50977
196/196 - 65s - loss: 17.3521 - MinusLogProbMetric: 17.3521 - val_loss: 17.6182 - val_MinusLogProbMetric: 17.6182 - lr: 3.3333e-04 - 65s/epoch - 333ms/step
Epoch 188/1000
2023-09-26 18:59:49.248 
Epoch 188/1000 
	 loss: 17.4134, MinusLogProbMetric: 17.4134, val_loss: 17.4985, val_MinusLogProbMetric: 17.4985

Epoch 188: val_loss improved from 17.50977 to 17.49846, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_294/weights/best_weights.h5
196/196 - 67s - loss: 17.4134 - MinusLogProbMetric: 17.4134 - val_loss: 17.4985 - val_MinusLogProbMetric: 17.4985 - lr: 3.3333e-04 - 67s/epoch - 340ms/step
Epoch 189/1000
2023-09-26 19:00:56.871 
Epoch 189/1000 
	 loss: 17.3129, MinusLogProbMetric: 17.3129, val_loss: 18.0033, val_MinusLogProbMetric: 18.0033

Epoch 189: val_loss did not improve from 17.49846
196/196 - 67s - loss: 17.3129 - MinusLogProbMetric: 17.3129 - val_loss: 18.0033 - val_MinusLogProbMetric: 18.0033 - lr: 3.3333e-04 - 67s/epoch - 340ms/step
Epoch 190/1000
2023-09-26 19:02:01.524 
Epoch 190/1000 
	 loss: 17.4291, MinusLogProbMetric: 17.4291, val_loss: 17.7058, val_MinusLogProbMetric: 17.7058

Epoch 190: val_loss did not improve from 17.49846
196/196 - 65s - loss: 17.4291 - MinusLogProbMetric: 17.4291 - val_loss: 17.7058 - val_MinusLogProbMetric: 17.7058 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 191/1000
2023-09-26 19:03:07.733 
Epoch 191/1000 
	 loss: 17.4187, MinusLogProbMetric: 17.4187, val_loss: 17.7772, val_MinusLogProbMetric: 17.7772

Epoch 191: val_loss did not improve from 17.49846
196/196 - 66s - loss: 17.4187 - MinusLogProbMetric: 17.4187 - val_loss: 17.7772 - val_MinusLogProbMetric: 17.7772 - lr: 3.3333e-04 - 66s/epoch - 338ms/step
Epoch 192/1000
2023-09-26 19:04:13.299 
Epoch 192/1000 
	 loss: 17.3405, MinusLogProbMetric: 17.3405, val_loss: 18.0937, val_MinusLogProbMetric: 18.0937

Epoch 192: val_loss did not improve from 17.49846
196/196 - 66s - loss: 17.3405 - MinusLogProbMetric: 17.3405 - val_loss: 18.0937 - val_MinusLogProbMetric: 18.0937 - lr: 3.3333e-04 - 66s/epoch - 334ms/step
Epoch 193/1000
2023-09-26 19:05:18.973 
Epoch 193/1000 
	 loss: 17.4234, MinusLogProbMetric: 17.4234, val_loss: 18.1059, val_MinusLogProbMetric: 18.1059

Epoch 193: val_loss did not improve from 17.49846
196/196 - 66s - loss: 17.4234 - MinusLogProbMetric: 17.4234 - val_loss: 18.1059 - val_MinusLogProbMetric: 18.1059 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 194/1000
2023-09-26 19:06:24.504 
Epoch 194/1000 
	 loss: 17.3847, MinusLogProbMetric: 17.3847, val_loss: 17.6486, val_MinusLogProbMetric: 17.6486

Epoch 194: val_loss did not improve from 17.49846
196/196 - 66s - loss: 17.3847 - MinusLogProbMetric: 17.3847 - val_loss: 17.6486 - val_MinusLogProbMetric: 17.6486 - lr: 3.3333e-04 - 66s/epoch - 334ms/step
Epoch 195/1000
2023-09-26 19:07:29.660 
Epoch 195/1000 
	 loss: 17.3705, MinusLogProbMetric: 17.3705, val_loss: 17.7363, val_MinusLogProbMetric: 17.7363

Epoch 195: val_loss did not improve from 17.49846
196/196 - 65s - loss: 17.3705 - MinusLogProbMetric: 17.3705 - val_loss: 17.7363 - val_MinusLogProbMetric: 17.7363 - lr: 3.3333e-04 - 65s/epoch - 332ms/step
Epoch 196/1000
2023-09-26 19:08:34.874 
Epoch 196/1000 
	 loss: 17.3562, MinusLogProbMetric: 17.3562, val_loss: 17.7218, val_MinusLogProbMetric: 17.7218

Epoch 196: val_loss did not improve from 17.49846
196/196 - 65s - loss: 17.3562 - MinusLogProbMetric: 17.3562 - val_loss: 17.7218 - val_MinusLogProbMetric: 17.7218 - lr: 3.3333e-04 - 65s/epoch - 333ms/step
Epoch 197/1000
2023-09-26 19:09:39.564 
Epoch 197/1000 
	 loss: 17.4094, MinusLogProbMetric: 17.4094, val_loss: 17.8447, val_MinusLogProbMetric: 17.8447

Epoch 197: val_loss did not improve from 17.49846
196/196 - 65s - loss: 17.4094 - MinusLogProbMetric: 17.4094 - val_loss: 17.8447 - val_MinusLogProbMetric: 17.8447 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 198/1000
2023-09-26 19:10:45.157 
Epoch 198/1000 
	 loss: 17.3945, MinusLogProbMetric: 17.3945, val_loss: 17.5304, val_MinusLogProbMetric: 17.5304

Epoch 198: val_loss did not improve from 17.49846
196/196 - 66s - loss: 17.3945 - MinusLogProbMetric: 17.3945 - val_loss: 17.5304 - val_MinusLogProbMetric: 17.5304 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 199/1000
2023-09-26 19:11:51.923 
Epoch 199/1000 
	 loss: 17.3110, MinusLogProbMetric: 17.3110, val_loss: 17.7898, val_MinusLogProbMetric: 17.7898

Epoch 199: val_loss did not improve from 17.49846
196/196 - 67s - loss: 17.3110 - MinusLogProbMetric: 17.3110 - val_loss: 17.7898 - val_MinusLogProbMetric: 17.7898 - lr: 3.3333e-04 - 67s/epoch - 341ms/step
Epoch 200/1000
2023-09-26 19:12:57.671 
Epoch 200/1000 
	 loss: 17.3320, MinusLogProbMetric: 17.3320, val_loss: 17.8976, val_MinusLogProbMetric: 17.8976

Epoch 200: val_loss did not improve from 17.49846
196/196 - 66s - loss: 17.3320 - MinusLogProbMetric: 17.3320 - val_loss: 17.8976 - val_MinusLogProbMetric: 17.8976 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 201/1000
2023-09-26 19:14:02.952 
Epoch 201/1000 
	 loss: 17.3510, MinusLogProbMetric: 17.3510, val_loss: 17.5706, val_MinusLogProbMetric: 17.5706

Epoch 201: val_loss did not improve from 17.49846
196/196 - 65s - loss: 17.3510 - MinusLogProbMetric: 17.3510 - val_loss: 17.5706 - val_MinusLogProbMetric: 17.5706 - lr: 3.3333e-04 - 65s/epoch - 333ms/step
Epoch 202/1000
2023-09-26 19:15:08.039 
Epoch 202/1000 
	 loss: 17.3137, MinusLogProbMetric: 17.3137, val_loss: 17.7456, val_MinusLogProbMetric: 17.7456

Epoch 202: val_loss did not improve from 17.49846
196/196 - 65s - loss: 17.3137 - MinusLogProbMetric: 17.3137 - val_loss: 17.7456 - val_MinusLogProbMetric: 17.7456 - lr: 3.3333e-04 - 65s/epoch - 332ms/step
Epoch 203/1000
2023-09-26 19:16:13.193 
Epoch 203/1000 
	 loss: 17.3570, MinusLogProbMetric: 17.3570, val_loss: 17.6680, val_MinusLogProbMetric: 17.6680

Epoch 203: val_loss did not improve from 17.49846
196/196 - 65s - loss: 17.3570 - MinusLogProbMetric: 17.3570 - val_loss: 17.6680 - val_MinusLogProbMetric: 17.6680 - lr: 3.3333e-04 - 65s/epoch - 332ms/step
Epoch 204/1000
2023-09-26 19:17:18.081 
Epoch 204/1000 
	 loss: 17.3146, MinusLogProbMetric: 17.3146, val_loss: 17.3856, val_MinusLogProbMetric: 17.3856

Epoch 204: val_loss improved from 17.49846 to 17.38556, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_294/weights/best_weights.h5
196/196 - 66s - loss: 17.3146 - MinusLogProbMetric: 17.3146 - val_loss: 17.3856 - val_MinusLogProbMetric: 17.3856 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 205/1000
2023-09-26 19:18:25.144 
Epoch 205/1000 
	 loss: 17.3173, MinusLogProbMetric: 17.3173, val_loss: 17.8493, val_MinusLogProbMetric: 17.8493

Epoch 205: val_loss did not improve from 17.38556
196/196 - 66s - loss: 17.3173 - MinusLogProbMetric: 17.3173 - val_loss: 17.8493 - val_MinusLogProbMetric: 17.8493 - lr: 3.3333e-04 - 66s/epoch - 337ms/step
Epoch 206/1000
2023-09-26 19:19:24.992 
Epoch 206/1000 
	 loss: 17.3491, MinusLogProbMetric: 17.3491, val_loss: 17.5635, val_MinusLogProbMetric: 17.5635

Epoch 206: val_loss did not improve from 17.38556
196/196 - 60s - loss: 17.3491 - MinusLogProbMetric: 17.3491 - val_loss: 17.5635 - val_MinusLogProbMetric: 17.5635 - lr: 3.3333e-04 - 60s/epoch - 305ms/step
Epoch 207/1000
2023-09-26 19:20:24.936 
Epoch 207/1000 
	 loss: 17.3540, MinusLogProbMetric: 17.3540, val_loss: 17.8018, val_MinusLogProbMetric: 17.8018

Epoch 207: val_loss did not improve from 17.38556
196/196 - 60s - loss: 17.3540 - MinusLogProbMetric: 17.3540 - val_loss: 17.8018 - val_MinusLogProbMetric: 17.8018 - lr: 3.3333e-04 - 60s/epoch - 306ms/step
Epoch 208/1000
2023-09-26 19:21:33.522 
Epoch 208/1000 
	 loss: 17.3251, MinusLogProbMetric: 17.3251, val_loss: 17.6154, val_MinusLogProbMetric: 17.6154

Epoch 208: val_loss did not improve from 17.38556
196/196 - 69s - loss: 17.3251 - MinusLogProbMetric: 17.3251 - val_loss: 17.6154 - val_MinusLogProbMetric: 17.6154 - lr: 3.3333e-04 - 69s/epoch - 350ms/step
Epoch 209/1000
2023-09-26 19:22:33.300 
Epoch 209/1000 
	 loss: 17.3408, MinusLogProbMetric: 17.3408, val_loss: 17.7636, val_MinusLogProbMetric: 17.7636

Epoch 209: val_loss did not improve from 17.38556
196/196 - 60s - loss: 17.3408 - MinusLogProbMetric: 17.3408 - val_loss: 17.7636 - val_MinusLogProbMetric: 17.7636 - lr: 3.3333e-04 - 60s/epoch - 305ms/step
Epoch 210/1000
2023-09-26 19:23:31.750 
Epoch 210/1000 
	 loss: 17.3027, MinusLogProbMetric: 17.3027, val_loss: 17.5786, val_MinusLogProbMetric: 17.5786

Epoch 210: val_loss did not improve from 17.38556
196/196 - 58s - loss: 17.3027 - MinusLogProbMetric: 17.3027 - val_loss: 17.5786 - val_MinusLogProbMetric: 17.5786 - lr: 3.3333e-04 - 58s/epoch - 298ms/step
Epoch 211/1000
2023-09-26 19:24:40.362 
Epoch 211/1000 
	 loss: 17.3083, MinusLogProbMetric: 17.3083, val_loss: 17.9041, val_MinusLogProbMetric: 17.9041

Epoch 211: val_loss did not improve from 17.38556
196/196 - 69s - loss: 17.3083 - MinusLogProbMetric: 17.3083 - val_loss: 17.9041 - val_MinusLogProbMetric: 17.9041 - lr: 3.3333e-04 - 69s/epoch - 350ms/step
Epoch 212/1000
2023-09-26 19:25:50.079 
Epoch 212/1000 
	 loss: 17.3317, MinusLogProbMetric: 17.3317, val_loss: 17.8201, val_MinusLogProbMetric: 17.8201

Epoch 212: val_loss did not improve from 17.38556
196/196 - 70s - loss: 17.3317 - MinusLogProbMetric: 17.3317 - val_loss: 17.8201 - val_MinusLogProbMetric: 17.8201 - lr: 3.3333e-04 - 70s/epoch - 356ms/step
Epoch 213/1000
2023-09-26 19:26:59.831 
Epoch 213/1000 
	 loss: 17.3560, MinusLogProbMetric: 17.3560, val_loss: 18.0534, val_MinusLogProbMetric: 18.0534

Epoch 213: val_loss did not improve from 17.38556
196/196 - 70s - loss: 17.3560 - MinusLogProbMetric: 17.3560 - val_loss: 18.0534 - val_MinusLogProbMetric: 18.0534 - lr: 3.3333e-04 - 70s/epoch - 356ms/step
Epoch 214/1000
2023-09-26 19:28:07.662 
Epoch 214/1000 
	 loss: 17.3076, MinusLogProbMetric: 17.3076, val_loss: 17.8058, val_MinusLogProbMetric: 17.8058

Epoch 214: val_loss did not improve from 17.38556
196/196 - 68s - loss: 17.3076 - MinusLogProbMetric: 17.3076 - val_loss: 17.8058 - val_MinusLogProbMetric: 17.8058 - lr: 3.3333e-04 - 68s/epoch - 346ms/step
Epoch 215/1000
2023-09-26 19:29:05.492 
Epoch 215/1000 
	 loss: 17.2476, MinusLogProbMetric: 17.2476, val_loss: 17.7816, val_MinusLogProbMetric: 17.7816

Epoch 215: val_loss did not improve from 17.38556
196/196 - 58s - loss: 17.2476 - MinusLogProbMetric: 17.2476 - val_loss: 17.7816 - val_MinusLogProbMetric: 17.7816 - lr: 3.3333e-04 - 58s/epoch - 295ms/step
Epoch 216/1000
2023-09-26 19:30:08.885 
Epoch 216/1000 
	 loss: 17.2632, MinusLogProbMetric: 17.2632, val_loss: 17.7961, val_MinusLogProbMetric: 17.7961

Epoch 216: val_loss did not improve from 17.38556
196/196 - 63s - loss: 17.2632 - MinusLogProbMetric: 17.2632 - val_loss: 17.7961 - val_MinusLogProbMetric: 17.7961 - lr: 3.3333e-04 - 63s/epoch - 323ms/step
Epoch 217/1000
2023-09-26 19:31:19.278 
Epoch 217/1000 
	 loss: 17.2868, MinusLogProbMetric: 17.2868, val_loss: 17.6030, val_MinusLogProbMetric: 17.6030

Epoch 217: val_loss did not improve from 17.38556
196/196 - 70s - loss: 17.2868 - MinusLogProbMetric: 17.2868 - val_loss: 17.6030 - val_MinusLogProbMetric: 17.6030 - lr: 3.3333e-04 - 70s/epoch - 359ms/step
Epoch 218/1000
2023-09-26 19:32:27.725 
Epoch 218/1000 
	 loss: 17.3094, MinusLogProbMetric: 17.3094, val_loss: 17.9732, val_MinusLogProbMetric: 17.9732

Epoch 218: val_loss did not improve from 17.38556
196/196 - 68s - loss: 17.3094 - MinusLogProbMetric: 17.3094 - val_loss: 17.9732 - val_MinusLogProbMetric: 17.9732 - lr: 3.3333e-04 - 68s/epoch - 349ms/step
Epoch 219/1000
2023-09-26 19:33:36.880 
Epoch 219/1000 
	 loss: 17.3120, MinusLogProbMetric: 17.3120, val_loss: 17.3932, val_MinusLogProbMetric: 17.3932

Epoch 219: val_loss did not improve from 17.38556
196/196 - 69s - loss: 17.3120 - MinusLogProbMetric: 17.3120 - val_loss: 17.3932 - val_MinusLogProbMetric: 17.3932 - lr: 3.3333e-04 - 69s/epoch - 353ms/step
Epoch 220/1000
2023-09-26 19:34:47.101 
Epoch 220/1000 
	 loss: 17.2658, MinusLogProbMetric: 17.2658, val_loss: 17.5234, val_MinusLogProbMetric: 17.5234

Epoch 220: val_loss did not improve from 17.38556
196/196 - 70s - loss: 17.2658 - MinusLogProbMetric: 17.2658 - val_loss: 17.5234 - val_MinusLogProbMetric: 17.5234 - lr: 3.3333e-04 - 70s/epoch - 358ms/step
Epoch 221/1000
2023-09-26 19:35:56.770 
Epoch 221/1000 
	 loss: 17.3071, MinusLogProbMetric: 17.3071, val_loss: 17.9792, val_MinusLogProbMetric: 17.9792

Epoch 221: val_loss did not improve from 17.38556
196/196 - 70s - loss: 17.3071 - MinusLogProbMetric: 17.3071 - val_loss: 17.9792 - val_MinusLogProbMetric: 17.9792 - lr: 3.3333e-04 - 70s/epoch - 355ms/step
Epoch 222/1000
2023-09-26 19:37:06.435 
Epoch 222/1000 
	 loss: 17.2746, MinusLogProbMetric: 17.2746, val_loss: 17.5070, val_MinusLogProbMetric: 17.5070

Epoch 222: val_loss did not improve from 17.38556
196/196 - 70s - loss: 17.2746 - MinusLogProbMetric: 17.2746 - val_loss: 17.5070 - val_MinusLogProbMetric: 17.5070 - lr: 3.3333e-04 - 70s/epoch - 355ms/step
Epoch 223/1000
2023-09-26 19:38:16.257 
Epoch 223/1000 
	 loss: 17.2232, MinusLogProbMetric: 17.2232, val_loss: 17.4992, val_MinusLogProbMetric: 17.4992

Epoch 223: val_loss did not improve from 17.38556
196/196 - 70s - loss: 17.2232 - MinusLogProbMetric: 17.2232 - val_loss: 17.4992 - val_MinusLogProbMetric: 17.4992 - lr: 3.3333e-04 - 70s/epoch - 356ms/step
Epoch 224/1000
2023-09-26 19:39:25.077 
Epoch 224/1000 
	 loss: 17.2290, MinusLogProbMetric: 17.2290, val_loss: 17.5518, val_MinusLogProbMetric: 17.5518

Epoch 224: val_loss did not improve from 17.38556
196/196 - 69s - loss: 17.2290 - MinusLogProbMetric: 17.2290 - val_loss: 17.5518 - val_MinusLogProbMetric: 17.5518 - lr: 3.3333e-04 - 69s/epoch - 351ms/step
Epoch 225/1000
2023-09-26 19:40:32.669 
Epoch 225/1000 
	 loss: 17.3214, MinusLogProbMetric: 17.3214, val_loss: 17.9040, val_MinusLogProbMetric: 17.9040

Epoch 225: val_loss did not improve from 17.38556
196/196 - 68s - loss: 17.3214 - MinusLogProbMetric: 17.3214 - val_loss: 17.9040 - val_MinusLogProbMetric: 17.9040 - lr: 3.3333e-04 - 68s/epoch - 345ms/step
Epoch 226/1000
2023-09-26 19:41:38.873 
Epoch 226/1000 
	 loss: 17.2363, MinusLogProbMetric: 17.2363, val_loss: 17.5520, val_MinusLogProbMetric: 17.5520

Epoch 226: val_loss did not improve from 17.38556
196/196 - 66s - loss: 17.2363 - MinusLogProbMetric: 17.2363 - val_loss: 17.5520 - val_MinusLogProbMetric: 17.5520 - lr: 3.3333e-04 - 66s/epoch - 338ms/step
Epoch 227/1000
2023-09-26 19:42:44.086 
Epoch 227/1000 
	 loss: 17.2896, MinusLogProbMetric: 17.2896, val_loss: 17.8141, val_MinusLogProbMetric: 17.8141

Epoch 227: val_loss did not improve from 17.38556
196/196 - 65s - loss: 17.2896 - MinusLogProbMetric: 17.2896 - val_loss: 17.8141 - val_MinusLogProbMetric: 17.8141 - lr: 3.3333e-04 - 65s/epoch - 333ms/step
Epoch 228/1000
2023-09-26 19:43:49.943 
Epoch 228/1000 
	 loss: 17.3023, MinusLogProbMetric: 17.3023, val_loss: 17.7970, val_MinusLogProbMetric: 17.7970

Epoch 228: val_loss did not improve from 17.38556
196/196 - 66s - loss: 17.3023 - MinusLogProbMetric: 17.3023 - val_loss: 17.7970 - val_MinusLogProbMetric: 17.7970 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 229/1000
2023-09-26 19:44:55.616 
Epoch 229/1000 
	 loss: 17.2242, MinusLogProbMetric: 17.2242, val_loss: 17.3859, val_MinusLogProbMetric: 17.3859

Epoch 229: val_loss did not improve from 17.38556
196/196 - 66s - loss: 17.2242 - MinusLogProbMetric: 17.2242 - val_loss: 17.3859 - val_MinusLogProbMetric: 17.3859 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 230/1000
2023-09-26 19:46:02.359 
Epoch 230/1000 
	 loss: 17.2420, MinusLogProbMetric: 17.2420, val_loss: 17.3958, val_MinusLogProbMetric: 17.3958

Epoch 230: val_loss did not improve from 17.38556
196/196 - 67s - loss: 17.2420 - MinusLogProbMetric: 17.2420 - val_loss: 17.3958 - val_MinusLogProbMetric: 17.3958 - lr: 3.3333e-04 - 67s/epoch - 341ms/step
Epoch 231/1000
2023-09-26 19:47:10.711 
Epoch 231/1000 
	 loss: 17.2792, MinusLogProbMetric: 17.2792, val_loss: 17.6875, val_MinusLogProbMetric: 17.6875

Epoch 231: val_loss did not improve from 17.38556
196/196 - 68s - loss: 17.2792 - MinusLogProbMetric: 17.2792 - val_loss: 17.6875 - val_MinusLogProbMetric: 17.6875 - lr: 3.3333e-04 - 68s/epoch - 349ms/step
Epoch 232/1000
2023-09-26 19:48:16.552 
Epoch 232/1000 
	 loss: 17.2316, MinusLogProbMetric: 17.2316, val_loss: 17.7200, val_MinusLogProbMetric: 17.7200

Epoch 232: val_loss did not improve from 17.38556
196/196 - 66s - loss: 17.2316 - MinusLogProbMetric: 17.2316 - val_loss: 17.7200 - val_MinusLogProbMetric: 17.7200 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 233/1000
2023-09-26 19:49:22.165 
Epoch 233/1000 
	 loss: 17.2038, MinusLogProbMetric: 17.2038, val_loss: 17.4851, val_MinusLogProbMetric: 17.4851

Epoch 233: val_loss did not improve from 17.38556
196/196 - 66s - loss: 17.2038 - MinusLogProbMetric: 17.2038 - val_loss: 17.4851 - val_MinusLogProbMetric: 17.4851 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 234/1000
2023-09-26 19:50:27.952 
Epoch 234/1000 
	 loss: 17.2374, MinusLogProbMetric: 17.2374, val_loss: 18.5238, val_MinusLogProbMetric: 18.5238

Epoch 234: val_loss did not improve from 17.38556
196/196 - 66s - loss: 17.2374 - MinusLogProbMetric: 17.2374 - val_loss: 18.5238 - val_MinusLogProbMetric: 18.5238 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 235/1000
2023-09-26 19:51:33.726 
Epoch 235/1000 
	 loss: 17.3503, MinusLogProbMetric: 17.3503, val_loss: 17.9064, val_MinusLogProbMetric: 17.9064

Epoch 235: val_loss did not improve from 17.38556
196/196 - 66s - loss: 17.3503 - MinusLogProbMetric: 17.3503 - val_loss: 17.9064 - val_MinusLogProbMetric: 17.9064 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 236/1000
2023-09-26 19:52:39.430 
Epoch 236/1000 
	 loss: 17.2740, MinusLogProbMetric: 17.2740, val_loss: 17.5360, val_MinusLogProbMetric: 17.5360

Epoch 236: val_loss did not improve from 17.38556
196/196 - 66s - loss: 17.2740 - MinusLogProbMetric: 17.2740 - val_loss: 17.5360 - val_MinusLogProbMetric: 17.5360 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 237/1000
2023-09-26 19:53:45.716 
Epoch 237/1000 
	 loss: 17.2651, MinusLogProbMetric: 17.2651, val_loss: 17.5659, val_MinusLogProbMetric: 17.5659

Epoch 237: val_loss did not improve from 17.38556
196/196 - 66s - loss: 17.2651 - MinusLogProbMetric: 17.2651 - val_loss: 17.5659 - val_MinusLogProbMetric: 17.5659 - lr: 3.3333e-04 - 66s/epoch - 338ms/step
Epoch 238/1000
2023-09-26 19:54:51.130 
Epoch 238/1000 
	 loss: 17.2126, MinusLogProbMetric: 17.2126, val_loss: 17.4168, val_MinusLogProbMetric: 17.4168

Epoch 238: val_loss did not improve from 17.38556
196/196 - 65s - loss: 17.2126 - MinusLogProbMetric: 17.2126 - val_loss: 17.4168 - val_MinusLogProbMetric: 17.4168 - lr: 3.3333e-04 - 65s/epoch - 334ms/step
Epoch 239/1000
2023-09-26 19:55:57.929 
Epoch 239/1000 
	 loss: 17.2243, MinusLogProbMetric: 17.2243, val_loss: 17.8960, val_MinusLogProbMetric: 17.8960

Epoch 239: val_loss did not improve from 17.38556
196/196 - 67s - loss: 17.2243 - MinusLogProbMetric: 17.2243 - val_loss: 17.8960 - val_MinusLogProbMetric: 17.8960 - lr: 3.3333e-04 - 67s/epoch - 341ms/step
Epoch 240/1000
2023-09-26 19:57:04.431 
Epoch 240/1000 
	 loss: 17.2831, MinusLogProbMetric: 17.2831, val_loss: 18.0262, val_MinusLogProbMetric: 18.0262

Epoch 240: val_loss did not improve from 17.38556
196/196 - 66s - loss: 17.2831 - MinusLogProbMetric: 17.2831 - val_loss: 18.0262 - val_MinusLogProbMetric: 18.0262 - lr: 3.3333e-04 - 66s/epoch - 339ms/step
Epoch 241/1000
2023-09-26 19:58:09.527 
Epoch 241/1000 
	 loss: 17.2508, MinusLogProbMetric: 17.2508, val_loss: 17.7330, val_MinusLogProbMetric: 17.7330

Epoch 241: val_loss did not improve from 17.38556
196/196 - 65s - loss: 17.2508 - MinusLogProbMetric: 17.2508 - val_loss: 17.7330 - val_MinusLogProbMetric: 17.7330 - lr: 3.3333e-04 - 65s/epoch - 332ms/step
Epoch 242/1000
2023-09-26 19:59:14.841 
Epoch 242/1000 
	 loss: 17.2270, MinusLogProbMetric: 17.2270, val_loss: 17.7035, val_MinusLogProbMetric: 17.7035

Epoch 242: val_loss did not improve from 17.38556
196/196 - 65s - loss: 17.2270 - MinusLogProbMetric: 17.2270 - val_loss: 17.7035 - val_MinusLogProbMetric: 17.7035 - lr: 3.3333e-04 - 65s/epoch - 333ms/step
Epoch 243/1000
2023-09-26 20:00:21.490 
Epoch 243/1000 
	 loss: 17.2831, MinusLogProbMetric: 17.2831, val_loss: 17.7846, val_MinusLogProbMetric: 17.7846

Epoch 243: val_loss did not improve from 17.38556
196/196 - 67s - loss: 17.2831 - MinusLogProbMetric: 17.2831 - val_loss: 17.7846 - val_MinusLogProbMetric: 17.7846 - lr: 3.3333e-04 - 67s/epoch - 340ms/step
Epoch 244/1000
2023-09-26 20:01:27.490 
Epoch 244/1000 
	 loss: 17.2039, MinusLogProbMetric: 17.2039, val_loss: 18.1172, val_MinusLogProbMetric: 18.1172

Epoch 244: val_loss did not improve from 17.38556
196/196 - 66s - loss: 17.2039 - MinusLogProbMetric: 17.2039 - val_loss: 18.1172 - val_MinusLogProbMetric: 18.1172 - lr: 3.3333e-04 - 66s/epoch - 337ms/step
Epoch 245/1000
2023-09-26 20:02:31.293 
Epoch 245/1000 
	 loss: 17.1733, MinusLogProbMetric: 17.1733, val_loss: 17.8626, val_MinusLogProbMetric: 17.8626

Epoch 245: val_loss did not improve from 17.38556
196/196 - 64s - loss: 17.1733 - MinusLogProbMetric: 17.1733 - val_loss: 17.8626 - val_MinusLogProbMetric: 17.8626 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 246/1000
2023-09-26 20:03:37.469 
Epoch 246/1000 
	 loss: 17.1770, MinusLogProbMetric: 17.1770, val_loss: 17.5000, val_MinusLogProbMetric: 17.5000

Epoch 246: val_loss did not improve from 17.38556
196/196 - 66s - loss: 17.1770 - MinusLogProbMetric: 17.1770 - val_loss: 17.5000 - val_MinusLogProbMetric: 17.5000 - lr: 3.3333e-04 - 66s/epoch - 338ms/step
Epoch 247/1000
2023-09-26 20:04:42.766 
Epoch 247/1000 
	 loss: 17.1820, MinusLogProbMetric: 17.1820, val_loss: 18.4564, val_MinusLogProbMetric: 18.4564

Epoch 247: val_loss did not improve from 17.38556
196/196 - 65s - loss: 17.1820 - MinusLogProbMetric: 17.1820 - val_loss: 18.4564 - val_MinusLogProbMetric: 18.4564 - lr: 3.3333e-04 - 65s/epoch - 333ms/step
Epoch 248/1000
2023-09-26 20:05:48.637 
Epoch 248/1000 
	 loss: 17.2044, MinusLogProbMetric: 17.2044, val_loss: 17.6680, val_MinusLogProbMetric: 17.6680

Epoch 248: val_loss did not improve from 17.38556
196/196 - 66s - loss: 17.2044 - MinusLogProbMetric: 17.2044 - val_loss: 17.6680 - val_MinusLogProbMetric: 17.6680 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 249/1000
2023-09-26 20:06:56.045 
Epoch 249/1000 
	 loss: 17.2453, MinusLogProbMetric: 17.2453, val_loss: 17.8246, val_MinusLogProbMetric: 17.8246

Epoch 249: val_loss did not improve from 17.38556
196/196 - 67s - loss: 17.2453 - MinusLogProbMetric: 17.2453 - val_loss: 17.8246 - val_MinusLogProbMetric: 17.8246 - lr: 3.3333e-04 - 67s/epoch - 344ms/step
Epoch 250/1000
2023-09-26 20:08:03.198 
Epoch 250/1000 
	 loss: 17.2372, MinusLogProbMetric: 17.2372, val_loss: 17.5307, val_MinusLogProbMetric: 17.5307

Epoch 250: val_loss did not improve from 17.38556
196/196 - 67s - loss: 17.2372 - MinusLogProbMetric: 17.2372 - val_loss: 17.5307 - val_MinusLogProbMetric: 17.5307 - lr: 3.3333e-04 - 67s/epoch - 343ms/step
Epoch 251/1000
2023-09-26 20:09:08.499 
Epoch 251/1000 
	 loss: 17.1761, MinusLogProbMetric: 17.1761, val_loss: 17.5903, val_MinusLogProbMetric: 17.5903

Epoch 251: val_loss did not improve from 17.38556
196/196 - 65s - loss: 17.1761 - MinusLogProbMetric: 17.1761 - val_loss: 17.5903 - val_MinusLogProbMetric: 17.5903 - lr: 3.3333e-04 - 65s/epoch - 333ms/step
Epoch 252/1000
2023-09-26 20:10:14.062 
Epoch 252/1000 
	 loss: 17.1713, MinusLogProbMetric: 17.1713, val_loss: 17.6510, val_MinusLogProbMetric: 17.6510

Epoch 252: val_loss did not improve from 17.38556
196/196 - 66s - loss: 17.1713 - MinusLogProbMetric: 17.1713 - val_loss: 17.6510 - val_MinusLogProbMetric: 17.6510 - lr: 3.3333e-04 - 66s/epoch - 334ms/step
Epoch 253/1000
2023-09-26 20:11:19.308 
Epoch 253/1000 
	 loss: 17.2451, MinusLogProbMetric: 17.2451, val_loss: 17.5039, val_MinusLogProbMetric: 17.5039

Epoch 253: val_loss did not improve from 17.38556
196/196 - 65s - loss: 17.2451 - MinusLogProbMetric: 17.2451 - val_loss: 17.5039 - val_MinusLogProbMetric: 17.5039 - lr: 3.3333e-04 - 65s/epoch - 333ms/step
Epoch 254/1000
2023-09-26 20:12:25.157 
Epoch 254/1000 
	 loss: 17.1486, MinusLogProbMetric: 17.1486, val_loss: 17.4423, val_MinusLogProbMetric: 17.4423

Epoch 254: val_loss did not improve from 17.38556
196/196 - 66s - loss: 17.1486 - MinusLogProbMetric: 17.1486 - val_loss: 17.4423 - val_MinusLogProbMetric: 17.4423 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 255/1000
2023-09-26 20:13:31.557 
Epoch 255/1000 
	 loss: 16.8239, MinusLogProbMetric: 16.8239, val_loss: 17.4196, val_MinusLogProbMetric: 17.4196

Epoch 255: val_loss did not improve from 17.38556
196/196 - 66s - loss: 16.8239 - MinusLogProbMetric: 16.8239 - val_loss: 17.4196 - val_MinusLogProbMetric: 17.4196 - lr: 1.6667e-04 - 66s/epoch - 339ms/step
Epoch 256/1000
2023-09-26 20:14:38.098 
Epoch 256/1000 
	 loss: 16.8208, MinusLogProbMetric: 16.8208, val_loss: 17.1960, val_MinusLogProbMetric: 17.1960

Epoch 256: val_loss improved from 17.38556 to 17.19603, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_294/weights/best_weights.h5
196/196 - 68s - loss: 16.8208 - MinusLogProbMetric: 16.8208 - val_loss: 17.1960 - val_MinusLogProbMetric: 17.1960 - lr: 1.6667e-04 - 68s/epoch - 345ms/step
Epoch 257/1000
2023-09-26 20:15:45.649 
Epoch 257/1000 
	 loss: 16.7979, MinusLogProbMetric: 16.7979, val_loss: 17.1772, val_MinusLogProbMetric: 17.1772

Epoch 257: val_loss improved from 17.19603 to 17.17724, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_294/weights/best_weights.h5
196/196 - 67s - loss: 16.7979 - MinusLogProbMetric: 16.7979 - val_loss: 17.1772 - val_MinusLogProbMetric: 17.1772 - lr: 1.6667e-04 - 67s/epoch - 344ms/step
Epoch 258/1000
2023-09-26 20:16:53.825 
Epoch 258/1000 
	 loss: 16.7961, MinusLogProbMetric: 16.7961, val_loss: 17.3329, val_MinusLogProbMetric: 17.3329

Epoch 258: val_loss did not improve from 17.17724
196/196 - 67s - loss: 16.7961 - MinusLogProbMetric: 16.7961 - val_loss: 17.3329 - val_MinusLogProbMetric: 17.3329 - lr: 1.6667e-04 - 67s/epoch - 343ms/step
Epoch 259/1000
2023-09-26 20:17:59.512 
Epoch 259/1000 
	 loss: 16.8146, MinusLogProbMetric: 16.8146, val_loss: 17.1889, val_MinusLogProbMetric: 17.1889

Epoch 259: val_loss did not improve from 17.17724
196/196 - 66s - loss: 16.8146 - MinusLogProbMetric: 16.8146 - val_loss: 17.1889 - val_MinusLogProbMetric: 17.1889 - lr: 1.6667e-04 - 66s/epoch - 335ms/step
Epoch 260/1000
2023-09-26 20:19:06.134 
Epoch 260/1000 
	 loss: 16.8121, MinusLogProbMetric: 16.8121, val_loss: 17.2669, val_MinusLogProbMetric: 17.2669

Epoch 260: val_loss did not improve from 17.17724
196/196 - 67s - loss: 16.8121 - MinusLogProbMetric: 16.8121 - val_loss: 17.2669 - val_MinusLogProbMetric: 17.2669 - lr: 1.6667e-04 - 67s/epoch - 340ms/step
Epoch 261/1000
2023-09-26 20:20:11.672 
Epoch 261/1000 
	 loss: 16.8011, MinusLogProbMetric: 16.8011, val_loss: 17.1975, val_MinusLogProbMetric: 17.1975

Epoch 261: val_loss did not improve from 17.17724
196/196 - 66s - loss: 16.8011 - MinusLogProbMetric: 16.8011 - val_loss: 17.1975 - val_MinusLogProbMetric: 17.1975 - lr: 1.6667e-04 - 66s/epoch - 334ms/step
Epoch 262/1000
2023-09-26 20:21:16.407 
Epoch 262/1000 
	 loss: 16.8256, MinusLogProbMetric: 16.8256, val_loss: 17.2387, val_MinusLogProbMetric: 17.2387

Epoch 262: val_loss did not improve from 17.17724
196/196 - 65s - loss: 16.8256 - MinusLogProbMetric: 16.8256 - val_loss: 17.2387 - val_MinusLogProbMetric: 17.2387 - lr: 1.6667e-04 - 65s/epoch - 330ms/step
Epoch 263/1000
2023-09-26 20:22:21.758 
Epoch 263/1000 
	 loss: 16.8274, MinusLogProbMetric: 16.8274, val_loss: 17.2648, val_MinusLogProbMetric: 17.2648

Epoch 263: val_loss did not improve from 17.17724
196/196 - 65s - loss: 16.8274 - MinusLogProbMetric: 16.8274 - val_loss: 17.2648 - val_MinusLogProbMetric: 17.2648 - lr: 1.6667e-04 - 65s/epoch - 333ms/step
Epoch 264/1000
2023-09-26 20:23:26.885 
Epoch 264/1000 
	 loss: 16.7953, MinusLogProbMetric: 16.7953, val_loss: 17.5306, val_MinusLogProbMetric: 17.5306

Epoch 264: val_loss did not improve from 17.17724
196/196 - 65s - loss: 16.7953 - MinusLogProbMetric: 16.7953 - val_loss: 17.5306 - val_MinusLogProbMetric: 17.5306 - lr: 1.6667e-04 - 65s/epoch - 332ms/step
Epoch 265/1000
2023-09-26 20:24:32.901 
Epoch 265/1000 
	 loss: 16.8093, MinusLogProbMetric: 16.8093, val_loss: 17.2190, val_MinusLogProbMetric: 17.2190

Epoch 265: val_loss did not improve from 17.17724
196/196 - 66s - loss: 16.8093 - MinusLogProbMetric: 16.8093 - val_loss: 17.2190 - val_MinusLogProbMetric: 17.2190 - lr: 1.6667e-04 - 66s/epoch - 337ms/step
Epoch 266/1000
2023-09-26 20:25:39.303 
Epoch 266/1000 
	 loss: 16.8134, MinusLogProbMetric: 16.8134, val_loss: 17.2731, val_MinusLogProbMetric: 17.2731

Epoch 266: val_loss did not improve from 17.17724
196/196 - 66s - loss: 16.8134 - MinusLogProbMetric: 16.8134 - val_loss: 17.2731 - val_MinusLogProbMetric: 17.2731 - lr: 1.6667e-04 - 66s/epoch - 339ms/step
Epoch 267/1000
2023-09-26 20:26:45.454 
Epoch 267/1000 
	 loss: 16.7822, MinusLogProbMetric: 16.7822, val_loss: 17.2066, val_MinusLogProbMetric: 17.2066

Epoch 267: val_loss did not improve from 17.17724
196/196 - 66s - loss: 16.7822 - MinusLogProbMetric: 16.7822 - val_loss: 17.2066 - val_MinusLogProbMetric: 17.2066 - lr: 1.6667e-04 - 66s/epoch - 337ms/step
Epoch 268/1000
2023-09-26 20:27:51.278 
Epoch 268/1000 
	 loss: 16.8101, MinusLogProbMetric: 16.8101, val_loss: 17.6129, val_MinusLogProbMetric: 17.6129

Epoch 268: val_loss did not improve from 17.17724
196/196 - 66s - loss: 16.8101 - MinusLogProbMetric: 16.8101 - val_loss: 17.6129 - val_MinusLogProbMetric: 17.6129 - lr: 1.6667e-04 - 66s/epoch - 336ms/step
Epoch 269/1000
2023-09-26 20:28:57.280 
Epoch 269/1000 
	 loss: 16.8039, MinusLogProbMetric: 16.8039, val_loss: 17.2390, val_MinusLogProbMetric: 17.2390

Epoch 269: val_loss did not improve from 17.17724
196/196 - 66s - loss: 16.8039 - MinusLogProbMetric: 16.8039 - val_loss: 17.2390 - val_MinusLogProbMetric: 17.2390 - lr: 1.6667e-04 - 66s/epoch - 337ms/step
Epoch 270/1000
2023-09-26 20:30:02.689 
Epoch 270/1000 
	 loss: 16.7922, MinusLogProbMetric: 16.7922, val_loss: 17.2818, val_MinusLogProbMetric: 17.2818

Epoch 270: val_loss did not improve from 17.17724
196/196 - 65s - loss: 16.7922 - MinusLogProbMetric: 16.7922 - val_loss: 17.2818 - val_MinusLogProbMetric: 17.2818 - lr: 1.6667e-04 - 65s/epoch - 334ms/step
Epoch 271/1000
2023-09-26 20:31:08.635 
Epoch 271/1000 
	 loss: 16.8107, MinusLogProbMetric: 16.8107, val_loss: 17.2463, val_MinusLogProbMetric: 17.2463

Epoch 271: val_loss did not improve from 17.17724
196/196 - 66s - loss: 16.8107 - MinusLogProbMetric: 16.8107 - val_loss: 17.2463 - val_MinusLogProbMetric: 17.2463 - lr: 1.6667e-04 - 66s/epoch - 336ms/step
Epoch 272/1000
2023-09-26 20:32:14.426 
Epoch 272/1000 
	 loss: 16.8043, MinusLogProbMetric: 16.8043, val_loss: 17.1936, val_MinusLogProbMetric: 17.1936

Epoch 272: val_loss did not improve from 17.17724
196/196 - 66s - loss: 16.8043 - MinusLogProbMetric: 16.8043 - val_loss: 17.1936 - val_MinusLogProbMetric: 17.1936 - lr: 1.6667e-04 - 66s/epoch - 336ms/step
Epoch 273/1000
2023-09-26 20:33:21.435 
Epoch 273/1000 
	 loss: 16.8338, MinusLogProbMetric: 16.8338, val_loss: 17.2283, val_MinusLogProbMetric: 17.2283

Epoch 273: val_loss did not improve from 17.17724
196/196 - 67s - loss: 16.8338 - MinusLogProbMetric: 16.8338 - val_loss: 17.2283 - val_MinusLogProbMetric: 17.2283 - lr: 1.6667e-04 - 67s/epoch - 342ms/step
Epoch 274/1000
2023-09-26 20:34:26.718 
Epoch 274/1000 
	 loss: 16.7837, MinusLogProbMetric: 16.7837, val_loss: 17.3144, val_MinusLogProbMetric: 17.3144

Epoch 274: val_loss did not improve from 17.17724
196/196 - 65s - loss: 16.7837 - MinusLogProbMetric: 16.7837 - val_loss: 17.3144 - val_MinusLogProbMetric: 17.3144 - lr: 1.6667e-04 - 65s/epoch - 333ms/step
Epoch 275/1000
2023-09-26 20:35:32.611 
Epoch 275/1000 
	 loss: 16.7931, MinusLogProbMetric: 16.7931, val_loss: 17.1161, val_MinusLogProbMetric: 17.1161

Epoch 275: val_loss improved from 17.17724 to 17.11608, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_294/weights/best_weights.h5
196/196 - 67s - loss: 16.7931 - MinusLogProbMetric: 16.7931 - val_loss: 17.1161 - val_MinusLogProbMetric: 17.1161 - lr: 1.6667e-04 - 67s/epoch - 342ms/step
Epoch 276/1000
2023-09-26 20:36:38.944 
Epoch 276/1000 
	 loss: 16.8257, MinusLogProbMetric: 16.8257, val_loss: 17.3317, val_MinusLogProbMetric: 17.3317

Epoch 276: val_loss did not improve from 17.11608
196/196 - 65s - loss: 16.8257 - MinusLogProbMetric: 16.8257 - val_loss: 17.3317 - val_MinusLogProbMetric: 17.3317 - lr: 1.6667e-04 - 65s/epoch - 333ms/step
Epoch 277/1000
2023-09-26 20:37:44.983 
Epoch 277/1000 
	 loss: 16.8037, MinusLogProbMetric: 16.8037, val_loss: 17.4263, val_MinusLogProbMetric: 17.4263

Epoch 277: val_loss did not improve from 17.11608
196/196 - 66s - loss: 16.8037 - MinusLogProbMetric: 16.8037 - val_loss: 17.4263 - val_MinusLogProbMetric: 17.4263 - lr: 1.6667e-04 - 66s/epoch - 337ms/step
Epoch 278/1000
2023-09-26 20:38:52.651 
Epoch 278/1000 
	 loss: 16.7656, MinusLogProbMetric: 16.7656, val_loss: 17.1430, val_MinusLogProbMetric: 17.1430

Epoch 278: val_loss did not improve from 17.11608
196/196 - 68s - loss: 16.7656 - MinusLogProbMetric: 16.7656 - val_loss: 17.1430 - val_MinusLogProbMetric: 17.1430 - lr: 1.6667e-04 - 68s/epoch - 345ms/step
Epoch 279/1000
2023-09-26 20:39:59.721 
Epoch 279/1000 
	 loss: 16.7809, MinusLogProbMetric: 16.7809, val_loss: 17.2430, val_MinusLogProbMetric: 17.2430

Epoch 279: val_loss did not improve from 17.11608
196/196 - 67s - loss: 16.7809 - MinusLogProbMetric: 16.7809 - val_loss: 17.2430 - val_MinusLogProbMetric: 17.2430 - lr: 1.6667e-04 - 67s/epoch - 342ms/step
Epoch 280/1000
2023-09-26 20:41:06.002 
Epoch 280/1000 
	 loss: 16.8007, MinusLogProbMetric: 16.8007, val_loss: 17.1668, val_MinusLogProbMetric: 17.1668

Epoch 280: val_loss did not improve from 17.11608
196/196 - 66s - loss: 16.8007 - MinusLogProbMetric: 16.8007 - val_loss: 17.1668 - val_MinusLogProbMetric: 17.1668 - lr: 1.6667e-04 - 66s/epoch - 338ms/step
Epoch 281/1000
2023-09-26 20:42:10.229 
Epoch 281/1000 
	 loss: 16.8002, MinusLogProbMetric: 16.8002, val_loss: 17.2226, val_MinusLogProbMetric: 17.2226

Epoch 281: val_loss did not improve from 17.11608
196/196 - 64s - loss: 16.8002 - MinusLogProbMetric: 16.8002 - val_loss: 17.2226 - val_MinusLogProbMetric: 17.2226 - lr: 1.6667e-04 - 64s/epoch - 328ms/step
Epoch 282/1000
2023-09-26 20:43:15.244 
Epoch 282/1000 
	 loss: 16.7948, MinusLogProbMetric: 16.7948, val_loss: 17.1964, val_MinusLogProbMetric: 17.1964

Epoch 282: val_loss did not improve from 17.11608
196/196 - 65s - loss: 16.7948 - MinusLogProbMetric: 16.7948 - val_loss: 17.1964 - val_MinusLogProbMetric: 17.1964 - lr: 1.6667e-04 - 65s/epoch - 332ms/step
Epoch 283/1000
2023-09-26 20:44:21.379 
Epoch 283/1000 
	 loss: 16.8214, MinusLogProbMetric: 16.8214, val_loss: 17.4815, val_MinusLogProbMetric: 17.4815

Epoch 283: val_loss did not improve from 17.11608
196/196 - 66s - loss: 16.8214 - MinusLogProbMetric: 16.8214 - val_loss: 17.4815 - val_MinusLogProbMetric: 17.4815 - lr: 1.6667e-04 - 66s/epoch - 337ms/step
Epoch 284/1000
2023-09-26 20:45:27.211 
Epoch 284/1000 
	 loss: 16.8088, MinusLogProbMetric: 16.8088, val_loss: 17.2933, val_MinusLogProbMetric: 17.2933

Epoch 284: val_loss did not improve from 17.11608
196/196 - 66s - loss: 16.8088 - MinusLogProbMetric: 16.8088 - val_loss: 17.2933 - val_MinusLogProbMetric: 17.2933 - lr: 1.6667e-04 - 66s/epoch - 336ms/step
Epoch 285/1000
2023-09-26 20:46:33.402 
Epoch 285/1000 
	 loss: 16.7864, MinusLogProbMetric: 16.7864, val_loss: 17.2253, val_MinusLogProbMetric: 17.2253

Epoch 285: val_loss did not improve from 17.11608
196/196 - 66s - loss: 16.7864 - MinusLogProbMetric: 16.7864 - val_loss: 17.2253 - val_MinusLogProbMetric: 17.2253 - lr: 1.6667e-04 - 66s/epoch - 338ms/step
Epoch 286/1000
2023-09-26 20:47:39.773 
Epoch 286/1000 
	 loss: 16.7831, MinusLogProbMetric: 16.7831, val_loss: 17.1868, val_MinusLogProbMetric: 17.1868

Epoch 286: val_loss did not improve from 17.11608
196/196 - 66s - loss: 16.7831 - MinusLogProbMetric: 16.7831 - val_loss: 17.1868 - val_MinusLogProbMetric: 17.1868 - lr: 1.6667e-04 - 66s/epoch - 339ms/step
Epoch 287/1000
2023-09-26 20:48:46.939 
Epoch 287/1000 
	 loss: 16.7712, MinusLogProbMetric: 16.7712, val_loss: 17.1802, val_MinusLogProbMetric: 17.1802

Epoch 287: val_loss did not improve from 17.11608
196/196 - 67s - loss: 16.7712 - MinusLogProbMetric: 16.7712 - val_loss: 17.1802 - val_MinusLogProbMetric: 17.1802 - lr: 1.6667e-04 - 67s/epoch - 343ms/step
Epoch 288/1000
2023-09-26 20:49:54.281 
Epoch 288/1000 
	 loss: 16.7797, MinusLogProbMetric: 16.7797, val_loss: 17.2251, val_MinusLogProbMetric: 17.2251

Epoch 288: val_loss did not improve from 17.11608
196/196 - 67s - loss: 16.7797 - MinusLogProbMetric: 16.7797 - val_loss: 17.2251 - val_MinusLogProbMetric: 17.2251 - lr: 1.6667e-04 - 67s/epoch - 344ms/step
Epoch 289/1000
2023-09-26 20:51:00.041 
Epoch 289/1000 
	 loss: 16.7716, MinusLogProbMetric: 16.7716, val_loss: 17.1566, val_MinusLogProbMetric: 17.1566

Epoch 289: val_loss did not improve from 17.11608
196/196 - 66s - loss: 16.7716 - MinusLogProbMetric: 16.7716 - val_loss: 17.1566 - val_MinusLogProbMetric: 17.1566 - lr: 1.6667e-04 - 66s/epoch - 335ms/step
Epoch 290/1000
2023-09-26 20:52:06.048 
Epoch 290/1000 
	 loss: 16.7795, MinusLogProbMetric: 16.7795, val_loss: 17.2493, val_MinusLogProbMetric: 17.2493

Epoch 290: val_loss did not improve from 17.11608
196/196 - 66s - loss: 16.7795 - MinusLogProbMetric: 16.7795 - val_loss: 17.2493 - val_MinusLogProbMetric: 17.2493 - lr: 1.6667e-04 - 66s/epoch - 337ms/step
Epoch 291/1000
2023-09-26 20:53:11.853 
Epoch 291/1000 
	 loss: 16.7787, MinusLogProbMetric: 16.7787, val_loss: 17.2769, val_MinusLogProbMetric: 17.2769

Epoch 291: val_loss did not improve from 17.11608
196/196 - 66s - loss: 16.7787 - MinusLogProbMetric: 16.7787 - val_loss: 17.2769 - val_MinusLogProbMetric: 17.2769 - lr: 1.6667e-04 - 66s/epoch - 336ms/step
Epoch 292/1000
2023-09-26 20:54:17.509 
Epoch 292/1000 
	 loss: 16.7647, MinusLogProbMetric: 16.7647, val_loss: 17.2063, val_MinusLogProbMetric: 17.2063

Epoch 292: val_loss did not improve from 17.11608
196/196 - 66s - loss: 16.7647 - MinusLogProbMetric: 16.7647 - val_loss: 17.2063 - val_MinusLogProbMetric: 17.2063 - lr: 1.6667e-04 - 66s/epoch - 335ms/step
Epoch 293/1000
2023-09-26 20:55:23.043 
Epoch 293/1000 
	 loss: 16.7553, MinusLogProbMetric: 16.7553, val_loss: 17.1340, val_MinusLogProbMetric: 17.1340

Epoch 293: val_loss did not improve from 17.11608
196/196 - 66s - loss: 16.7553 - MinusLogProbMetric: 16.7553 - val_loss: 17.1340 - val_MinusLogProbMetric: 17.1340 - lr: 1.6667e-04 - 66s/epoch - 334ms/step
Epoch 294/1000
2023-09-26 20:56:28.880 
Epoch 294/1000 
	 loss: 16.7817, MinusLogProbMetric: 16.7817, val_loss: 17.2383, val_MinusLogProbMetric: 17.2383

Epoch 294: val_loss did not improve from 17.11608
196/196 - 66s - loss: 16.7817 - MinusLogProbMetric: 16.7817 - val_loss: 17.2383 - val_MinusLogProbMetric: 17.2383 - lr: 1.6667e-04 - 66s/epoch - 336ms/step
Epoch 295/1000
2023-09-26 20:57:34.693 
Epoch 295/1000 
	 loss: 16.7974, MinusLogProbMetric: 16.7974, val_loss: 17.2243, val_MinusLogProbMetric: 17.2243

Epoch 295: val_loss did not improve from 17.11608
196/196 - 66s - loss: 16.7974 - MinusLogProbMetric: 16.7974 - val_loss: 17.2243 - val_MinusLogProbMetric: 17.2243 - lr: 1.6667e-04 - 66s/epoch - 336ms/step
Epoch 296/1000
2023-09-26 20:58:40.330 
Epoch 296/1000 
	 loss: 16.7851, MinusLogProbMetric: 16.7851, val_loss: 17.2588, val_MinusLogProbMetric: 17.2588

Epoch 296: val_loss did not improve from 17.11608
196/196 - 66s - loss: 16.7851 - MinusLogProbMetric: 16.7851 - val_loss: 17.2588 - val_MinusLogProbMetric: 17.2588 - lr: 1.6667e-04 - 66s/epoch - 335ms/step
Epoch 297/1000
2023-09-26 20:59:46.799 
Epoch 297/1000 
	 loss: 16.7683, MinusLogProbMetric: 16.7683, val_loss: 17.1736, val_MinusLogProbMetric: 17.1736

Epoch 297: val_loss did not improve from 17.11608
196/196 - 66s - loss: 16.7683 - MinusLogProbMetric: 16.7683 - val_loss: 17.1736 - val_MinusLogProbMetric: 17.1736 - lr: 1.6667e-04 - 66s/epoch - 339ms/step
Epoch 298/1000
2023-09-26 21:00:52.790 
Epoch 298/1000 
	 loss: 16.7804, MinusLogProbMetric: 16.7804, val_loss: 17.1852, val_MinusLogProbMetric: 17.1852

Epoch 298: val_loss did not improve from 17.11608
196/196 - 66s - loss: 16.7804 - MinusLogProbMetric: 16.7804 - val_loss: 17.1852 - val_MinusLogProbMetric: 17.1852 - lr: 1.6667e-04 - 66s/epoch - 337ms/step
Epoch 299/1000
2023-09-26 21:01:58.507 
Epoch 299/1000 
	 loss: 16.7627, MinusLogProbMetric: 16.7627, val_loss: 17.2108, val_MinusLogProbMetric: 17.2108

Epoch 299: val_loss did not improve from 17.11608
196/196 - 66s - loss: 16.7627 - MinusLogProbMetric: 16.7627 - val_loss: 17.2108 - val_MinusLogProbMetric: 17.2108 - lr: 1.6667e-04 - 66s/epoch - 335ms/step
Epoch 300/1000
2023-09-26 21:03:04.465 
Epoch 300/1000 
	 loss: 16.8014, MinusLogProbMetric: 16.8014, val_loss: 17.1688, val_MinusLogProbMetric: 17.1688

Epoch 300: val_loss did not improve from 17.11608
196/196 - 66s - loss: 16.8014 - MinusLogProbMetric: 16.8014 - val_loss: 17.1688 - val_MinusLogProbMetric: 17.1688 - lr: 1.6667e-04 - 66s/epoch - 336ms/step
Epoch 301/1000
2023-09-26 21:04:10.650 
Epoch 301/1000 
	 loss: 16.7697, MinusLogProbMetric: 16.7697, val_loss: 17.1605, val_MinusLogProbMetric: 17.1605

Epoch 301: val_loss did not improve from 17.11608
196/196 - 66s - loss: 16.7697 - MinusLogProbMetric: 16.7697 - val_loss: 17.1605 - val_MinusLogProbMetric: 17.1605 - lr: 1.6667e-04 - 66s/epoch - 338ms/step
Epoch 302/1000
2023-09-26 21:05:16.919 
Epoch 302/1000 
	 loss: 16.7846, MinusLogProbMetric: 16.7846, val_loss: 17.2224, val_MinusLogProbMetric: 17.2224

Epoch 302: val_loss did not improve from 17.11608
196/196 - 66s - loss: 16.7846 - MinusLogProbMetric: 16.7846 - val_loss: 17.2224 - val_MinusLogProbMetric: 17.2224 - lr: 1.6667e-04 - 66s/epoch - 338ms/step
Epoch 303/1000
2023-09-26 21:06:22.839 
Epoch 303/1000 
	 loss: 16.7806, MinusLogProbMetric: 16.7806, val_loss: 17.2009, val_MinusLogProbMetric: 17.2009

Epoch 303: val_loss did not improve from 17.11608
196/196 - 66s - loss: 16.7806 - MinusLogProbMetric: 16.7806 - val_loss: 17.2009 - val_MinusLogProbMetric: 17.2009 - lr: 1.6667e-04 - 66s/epoch - 336ms/step
Epoch 304/1000
2023-09-26 21:07:29.335 
Epoch 304/1000 
	 loss: 16.7860, MinusLogProbMetric: 16.7860, val_loss: 17.1836, val_MinusLogProbMetric: 17.1836

Epoch 304: val_loss did not improve from 17.11608
196/196 - 66s - loss: 16.7860 - MinusLogProbMetric: 16.7860 - val_loss: 17.1836 - val_MinusLogProbMetric: 17.1836 - lr: 1.6667e-04 - 66s/epoch - 339ms/step
Epoch 305/1000
2023-09-26 21:08:35.856 
Epoch 305/1000 
	 loss: 16.7622, MinusLogProbMetric: 16.7622, val_loss: 17.1805, val_MinusLogProbMetric: 17.1805

Epoch 305: val_loss did not improve from 17.11608
196/196 - 67s - loss: 16.7622 - MinusLogProbMetric: 16.7622 - val_loss: 17.1805 - val_MinusLogProbMetric: 17.1805 - lr: 1.6667e-04 - 67s/epoch - 339ms/step
Epoch 306/1000
2023-09-26 21:09:42.535 
Epoch 306/1000 
	 loss: 16.7827, MinusLogProbMetric: 16.7827, val_loss: 17.3059, val_MinusLogProbMetric: 17.3059

Epoch 306: val_loss did not improve from 17.11608
196/196 - 67s - loss: 16.7827 - MinusLogProbMetric: 16.7827 - val_loss: 17.3059 - val_MinusLogProbMetric: 17.3059 - lr: 1.6667e-04 - 67s/epoch - 340ms/step
Epoch 307/1000
2023-09-26 21:10:48.476 
Epoch 307/1000 
	 loss: 16.7530, MinusLogProbMetric: 16.7530, val_loss: 17.1421, val_MinusLogProbMetric: 17.1421

Epoch 307: val_loss did not improve from 17.11608
196/196 - 66s - loss: 16.7530 - MinusLogProbMetric: 16.7530 - val_loss: 17.1421 - val_MinusLogProbMetric: 17.1421 - lr: 1.6667e-04 - 66s/epoch - 336ms/step
Epoch 308/1000
2023-09-26 21:11:54.862 
Epoch 308/1000 
	 loss: 16.7647, MinusLogProbMetric: 16.7647, val_loss: 17.2575, val_MinusLogProbMetric: 17.2575

Epoch 308: val_loss did not improve from 17.11608
196/196 - 66s - loss: 16.7647 - MinusLogProbMetric: 16.7647 - val_loss: 17.2575 - val_MinusLogProbMetric: 17.2575 - lr: 1.6667e-04 - 66s/epoch - 339ms/step
Epoch 309/1000
2023-09-26 21:13:01.822 
Epoch 309/1000 
	 loss: 16.7616, MinusLogProbMetric: 16.7616, val_loss: 17.2073, val_MinusLogProbMetric: 17.2073

Epoch 309: val_loss did not improve from 17.11608
196/196 - 67s - loss: 16.7616 - MinusLogProbMetric: 16.7616 - val_loss: 17.2073 - val_MinusLogProbMetric: 17.2073 - lr: 1.6667e-04 - 67s/epoch - 342ms/step
Epoch 310/1000
2023-09-26 21:14:08.821 
Epoch 310/1000 
	 loss: 16.7958, MinusLogProbMetric: 16.7958, val_loss: 17.2678, val_MinusLogProbMetric: 17.2678

Epoch 310: val_loss did not improve from 17.11608
196/196 - 67s - loss: 16.7958 - MinusLogProbMetric: 16.7958 - val_loss: 17.2678 - val_MinusLogProbMetric: 17.2678 - lr: 1.6667e-04 - 67s/epoch - 342ms/step
Epoch 311/1000
2023-09-26 21:15:15.613 
Epoch 311/1000 
	 loss: 16.7875, MinusLogProbMetric: 16.7875, val_loss: 17.3478, val_MinusLogProbMetric: 17.3478

Epoch 311: val_loss did not improve from 17.11608
196/196 - 67s - loss: 16.7875 - MinusLogProbMetric: 16.7875 - val_loss: 17.3478 - val_MinusLogProbMetric: 17.3478 - lr: 1.6667e-04 - 67s/epoch - 341ms/step
Epoch 312/1000
2023-09-26 21:16:20.700 
Epoch 312/1000 
	 loss: 16.7865, MinusLogProbMetric: 16.7865, val_loss: 17.2207, val_MinusLogProbMetric: 17.2207

Epoch 312: val_loss did not improve from 17.11608
196/196 - 65s - loss: 16.7865 - MinusLogProbMetric: 16.7865 - val_loss: 17.2207 - val_MinusLogProbMetric: 17.2207 - lr: 1.6667e-04 - 65s/epoch - 332ms/step
Epoch 313/1000
2023-09-26 21:17:27.106 
Epoch 313/1000 
	 loss: 16.7567, MinusLogProbMetric: 16.7567, val_loss: 17.1845, val_MinusLogProbMetric: 17.1845

Epoch 313: val_loss did not improve from 17.11608
196/196 - 66s - loss: 16.7567 - MinusLogProbMetric: 16.7567 - val_loss: 17.1845 - val_MinusLogProbMetric: 17.1845 - lr: 1.6667e-04 - 66s/epoch - 339ms/step
Epoch 314/1000
2023-09-26 21:18:33.716 
Epoch 314/1000 
	 loss: 16.7405, MinusLogProbMetric: 16.7405, val_loss: 17.2784, val_MinusLogProbMetric: 17.2784

Epoch 314: val_loss did not improve from 17.11608
196/196 - 67s - loss: 16.7405 - MinusLogProbMetric: 16.7405 - val_loss: 17.2784 - val_MinusLogProbMetric: 17.2784 - lr: 1.6667e-04 - 67s/epoch - 340ms/step
Epoch 315/1000
2023-09-26 21:19:39.729 
Epoch 315/1000 
	 loss: 16.7782, MinusLogProbMetric: 16.7782, val_loss: 17.2688, val_MinusLogProbMetric: 17.2688

Epoch 315: val_loss did not improve from 17.11608
196/196 - 66s - loss: 16.7782 - MinusLogProbMetric: 16.7782 - val_loss: 17.2688 - val_MinusLogProbMetric: 17.2688 - lr: 1.6667e-04 - 66s/epoch - 337ms/step
Epoch 316/1000
2023-09-26 21:20:46.569 
Epoch 316/1000 
	 loss: 16.7754, MinusLogProbMetric: 16.7754, val_loss: 17.1673, val_MinusLogProbMetric: 17.1673

Epoch 316: val_loss did not improve from 17.11608
196/196 - 67s - loss: 16.7754 - MinusLogProbMetric: 16.7754 - val_loss: 17.1673 - val_MinusLogProbMetric: 17.1673 - lr: 1.6667e-04 - 67s/epoch - 341ms/step
Epoch 317/1000
2023-09-26 21:21:52.888 
Epoch 317/1000 
	 loss: 16.7674, MinusLogProbMetric: 16.7674, val_loss: 17.1696, val_MinusLogProbMetric: 17.1696

Epoch 317: val_loss did not improve from 17.11608
196/196 - 66s - loss: 16.7674 - MinusLogProbMetric: 16.7674 - val_loss: 17.1696 - val_MinusLogProbMetric: 17.1696 - lr: 1.6667e-04 - 66s/epoch - 338ms/step
Epoch 318/1000
2023-09-26 21:22:58.953 
Epoch 318/1000 
	 loss: 16.7757, MinusLogProbMetric: 16.7757, val_loss: 17.2091, val_MinusLogProbMetric: 17.2091

Epoch 318: val_loss did not improve from 17.11608
196/196 - 66s - loss: 16.7757 - MinusLogProbMetric: 16.7757 - val_loss: 17.2091 - val_MinusLogProbMetric: 17.2091 - lr: 1.6667e-04 - 66s/epoch - 337ms/step
Epoch 319/1000
2023-09-26 21:24:04.934 
Epoch 319/1000 
	 loss: 16.7750, MinusLogProbMetric: 16.7750, val_loss: 17.2312, val_MinusLogProbMetric: 17.2312

Epoch 319: val_loss did not improve from 17.11608
196/196 - 66s - loss: 16.7750 - MinusLogProbMetric: 16.7750 - val_loss: 17.2312 - val_MinusLogProbMetric: 17.2312 - lr: 1.6667e-04 - 66s/epoch - 337ms/step
Epoch 320/1000
2023-09-26 21:25:10.156 
Epoch 320/1000 
	 loss: 16.7583, MinusLogProbMetric: 16.7583, val_loss: 17.2379, val_MinusLogProbMetric: 17.2379

Epoch 320: val_loss did not improve from 17.11608
196/196 - 65s - loss: 16.7583 - MinusLogProbMetric: 16.7583 - val_loss: 17.2379 - val_MinusLogProbMetric: 17.2379 - lr: 1.6667e-04 - 65s/epoch - 333ms/step
Epoch 321/1000
2023-09-26 21:26:16.120 
Epoch 321/1000 
	 loss: 16.7827, MinusLogProbMetric: 16.7827, val_loss: 17.2091, val_MinusLogProbMetric: 17.2091

Epoch 321: val_loss did not improve from 17.11608
196/196 - 66s - loss: 16.7827 - MinusLogProbMetric: 16.7827 - val_loss: 17.2091 - val_MinusLogProbMetric: 17.2091 - lr: 1.6667e-04 - 66s/epoch - 337ms/step
Epoch 322/1000
2023-09-26 21:27:21.307 
Epoch 322/1000 
	 loss: 16.7735, MinusLogProbMetric: 16.7735, val_loss: 17.2445, val_MinusLogProbMetric: 17.2445

Epoch 322: val_loss did not improve from 17.11608
196/196 - 65s - loss: 16.7735 - MinusLogProbMetric: 16.7735 - val_loss: 17.2445 - val_MinusLogProbMetric: 17.2445 - lr: 1.6667e-04 - 65s/epoch - 333ms/step
Epoch 323/1000
2023-09-26 21:28:26.783 
Epoch 323/1000 
	 loss: 16.7668, MinusLogProbMetric: 16.7668, val_loss: 17.1481, val_MinusLogProbMetric: 17.1481

Epoch 323: val_loss did not improve from 17.11608
196/196 - 65s - loss: 16.7668 - MinusLogProbMetric: 16.7668 - val_loss: 17.1481 - val_MinusLogProbMetric: 17.1481 - lr: 1.6667e-04 - 65s/epoch - 334ms/step
Epoch 324/1000
2023-09-26 21:29:32.696 
Epoch 324/1000 
	 loss: 16.7478, MinusLogProbMetric: 16.7478, val_loss: 17.1758, val_MinusLogProbMetric: 17.1758

Epoch 324: val_loss did not improve from 17.11608
196/196 - 66s - loss: 16.7478 - MinusLogProbMetric: 16.7478 - val_loss: 17.1758 - val_MinusLogProbMetric: 17.1758 - lr: 1.6667e-04 - 66s/epoch - 336ms/step
Epoch 325/1000
2023-09-26 21:30:37.339 
Epoch 325/1000 
	 loss: 16.7683, MinusLogProbMetric: 16.7683, val_loss: 17.1770, val_MinusLogProbMetric: 17.1770

Epoch 325: val_loss did not improve from 17.11608
196/196 - 65s - loss: 16.7683 - MinusLogProbMetric: 16.7683 - val_loss: 17.1770 - val_MinusLogProbMetric: 17.1770 - lr: 1.6667e-04 - 65s/epoch - 330ms/step
Epoch 326/1000
2023-09-26 21:31:42.952 
Epoch 326/1000 
	 loss: 16.6158, MinusLogProbMetric: 16.6158, val_loss: 17.0554, val_MinusLogProbMetric: 17.0554

Epoch 326: val_loss improved from 17.11608 to 17.05539, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_294/weights/best_weights.h5
196/196 - 67s - loss: 16.6158 - MinusLogProbMetric: 16.6158 - val_loss: 17.0554 - val_MinusLogProbMetric: 17.0554 - lr: 8.3333e-05 - 67s/epoch - 340ms/step
Epoch 327/1000
2023-09-26 21:32:51.924 
Epoch 327/1000 
	 loss: 16.6239, MinusLogProbMetric: 16.6239, val_loss: 17.0819, val_MinusLogProbMetric: 17.0819

Epoch 327: val_loss did not improve from 17.05539
196/196 - 68s - loss: 16.6239 - MinusLogProbMetric: 16.6239 - val_loss: 17.0819 - val_MinusLogProbMetric: 17.0819 - lr: 8.3333e-05 - 68s/epoch - 347ms/step
Epoch 328/1000
2023-09-26 21:33:58.259 
Epoch 328/1000 
	 loss: 16.6198, MinusLogProbMetric: 16.6198, val_loss: 17.0620, val_MinusLogProbMetric: 17.0620

Epoch 328: val_loss did not improve from 17.05539
196/196 - 66s - loss: 16.6198 - MinusLogProbMetric: 16.6198 - val_loss: 17.0620 - val_MinusLogProbMetric: 17.0620 - lr: 8.3333e-05 - 66s/epoch - 338ms/step
Epoch 329/1000
2023-09-26 21:35:04.679 
Epoch 329/1000 
	 loss: 16.6196, MinusLogProbMetric: 16.6196, val_loss: 17.0453, val_MinusLogProbMetric: 17.0453

Epoch 329: val_loss improved from 17.05539 to 17.04527, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_294/weights/best_weights.h5
196/196 - 67s - loss: 16.6196 - MinusLogProbMetric: 16.6196 - val_loss: 17.0453 - val_MinusLogProbMetric: 17.0453 - lr: 8.3333e-05 - 67s/epoch - 343ms/step
Epoch 330/1000
2023-09-26 21:36:09.740 
Epoch 330/1000 
	 loss: 16.6175, MinusLogProbMetric: 16.6175, val_loss: 17.1182, val_MinusLogProbMetric: 17.1182

Epoch 330: val_loss did not improve from 17.04527
196/196 - 64s - loss: 16.6175 - MinusLogProbMetric: 16.6175 - val_loss: 17.1182 - val_MinusLogProbMetric: 17.1182 - lr: 8.3333e-05 - 64s/epoch - 327ms/step
Epoch 331/1000
2023-09-26 21:37:15.016 
Epoch 331/1000 
	 loss: 16.6076, MinusLogProbMetric: 16.6076, val_loss: 17.1888, val_MinusLogProbMetric: 17.1888

Epoch 331: val_loss did not improve from 17.04527
196/196 - 65s - loss: 16.6076 - MinusLogProbMetric: 16.6076 - val_loss: 17.1888 - val_MinusLogProbMetric: 17.1888 - lr: 8.3333e-05 - 65s/epoch - 333ms/step
Epoch 332/1000
2023-09-26 21:38:20.887 
Epoch 332/1000 
	 loss: 16.6224, MinusLogProbMetric: 16.6224, val_loss: 17.0158, val_MinusLogProbMetric: 17.0158

Epoch 332: val_loss improved from 17.04527 to 17.01579, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_294/weights/best_weights.h5
196/196 - 67s - loss: 16.6224 - MinusLogProbMetric: 16.6224 - val_loss: 17.0158 - val_MinusLogProbMetric: 17.0158 - lr: 8.3333e-05 - 67s/epoch - 341ms/step
Epoch 333/1000
2023-09-26 21:39:26.283 
Epoch 333/1000 
	 loss: 16.6185, MinusLogProbMetric: 16.6185, val_loss: 17.0836, val_MinusLogProbMetric: 17.0836

Epoch 333: val_loss did not improve from 17.01579
196/196 - 65s - loss: 16.6185 - MinusLogProbMetric: 16.6185 - val_loss: 17.0836 - val_MinusLogProbMetric: 17.0836 - lr: 8.3333e-05 - 65s/epoch - 329ms/step
Epoch 334/1000
2023-09-26 21:40:32.690 
Epoch 334/1000 
	 loss: 16.6088, MinusLogProbMetric: 16.6088, val_loss: 17.0642, val_MinusLogProbMetric: 17.0642

Epoch 334: val_loss did not improve from 17.01579
196/196 - 66s - loss: 16.6088 - MinusLogProbMetric: 16.6088 - val_loss: 17.0642 - val_MinusLogProbMetric: 17.0642 - lr: 8.3333e-05 - 66s/epoch - 339ms/step
Epoch 335/1000
2023-09-26 21:41:38.943 
Epoch 335/1000 
	 loss: 16.6293, MinusLogProbMetric: 16.6293, val_loss: 17.0917, val_MinusLogProbMetric: 17.0917

Epoch 335: val_loss did not improve from 17.01579
196/196 - 66s - loss: 16.6293 - MinusLogProbMetric: 16.6293 - val_loss: 17.0917 - val_MinusLogProbMetric: 17.0917 - lr: 8.3333e-05 - 66s/epoch - 338ms/step
Epoch 336/1000
2023-09-26 21:42:45.667 
Epoch 336/1000 
	 loss: 16.6093, MinusLogProbMetric: 16.6093, val_loss: 17.0353, val_MinusLogProbMetric: 17.0353

Epoch 336: val_loss did not improve from 17.01579
196/196 - 67s - loss: 16.6093 - MinusLogProbMetric: 16.6093 - val_loss: 17.0353 - val_MinusLogProbMetric: 17.0353 - lr: 8.3333e-05 - 67s/epoch - 340ms/step
Epoch 337/1000
2023-09-26 21:43:51.524 
Epoch 337/1000 
	 loss: 16.6087, MinusLogProbMetric: 16.6087, val_loss: 17.0524, val_MinusLogProbMetric: 17.0524

Epoch 337: val_loss did not improve from 17.01579
196/196 - 66s - loss: 16.6087 - MinusLogProbMetric: 16.6087 - val_loss: 17.0524 - val_MinusLogProbMetric: 17.0524 - lr: 8.3333e-05 - 66s/epoch - 336ms/step
Epoch 338/1000
2023-09-26 21:44:58.404 
Epoch 338/1000 
	 loss: 16.5966, MinusLogProbMetric: 16.5966, val_loss: 17.1411, val_MinusLogProbMetric: 17.1411

Epoch 338: val_loss did not improve from 17.01579
196/196 - 67s - loss: 16.5966 - MinusLogProbMetric: 16.5966 - val_loss: 17.1411 - val_MinusLogProbMetric: 17.1411 - lr: 8.3333e-05 - 67s/epoch - 341ms/step
Epoch 339/1000
2023-09-26 21:46:04.579 
Epoch 339/1000 
	 loss: 16.6100, MinusLogProbMetric: 16.6100, val_loss: 17.1018, val_MinusLogProbMetric: 17.1018

Epoch 339: val_loss did not improve from 17.01579
196/196 - 66s - loss: 16.6100 - MinusLogProbMetric: 16.6100 - val_loss: 17.1018 - val_MinusLogProbMetric: 17.1018 - lr: 8.3333e-05 - 66s/epoch - 338ms/step
Epoch 340/1000
2023-09-26 21:47:09.971 
Epoch 340/1000 
	 loss: 16.6366, MinusLogProbMetric: 16.6366, val_loss: 17.1395, val_MinusLogProbMetric: 17.1395

Epoch 340: val_loss did not improve from 17.01579
196/196 - 65s - loss: 16.6366 - MinusLogProbMetric: 16.6366 - val_loss: 17.1395 - val_MinusLogProbMetric: 17.1395 - lr: 8.3333e-05 - 65s/epoch - 334ms/step
Epoch 341/1000
2023-09-26 21:48:15.095 
Epoch 341/1000 
	 loss: 16.6009, MinusLogProbMetric: 16.6009, val_loss: 17.0406, val_MinusLogProbMetric: 17.0406

Epoch 341: val_loss did not improve from 17.01579
196/196 - 65s - loss: 16.6009 - MinusLogProbMetric: 16.6009 - val_loss: 17.0406 - val_MinusLogProbMetric: 17.0406 - lr: 8.3333e-05 - 65s/epoch - 332ms/step
Epoch 342/1000
2023-09-26 21:49:21.732 
Epoch 342/1000 
	 loss: 16.6020, MinusLogProbMetric: 16.6020, val_loss: 17.0614, val_MinusLogProbMetric: 17.0614

Epoch 342: val_loss did not improve from 17.01579
196/196 - 67s - loss: 16.6020 - MinusLogProbMetric: 16.6020 - val_loss: 17.0614 - val_MinusLogProbMetric: 17.0614 - lr: 8.3333e-05 - 67s/epoch - 340ms/step
Epoch 343/1000
2023-09-26 21:50:28.276 
Epoch 343/1000 
	 loss: 16.6176, MinusLogProbMetric: 16.6176, val_loss: 17.0233, val_MinusLogProbMetric: 17.0233

Epoch 343: val_loss did not improve from 17.01579
196/196 - 67s - loss: 16.6176 - MinusLogProbMetric: 16.6176 - val_loss: 17.0233 - val_MinusLogProbMetric: 17.0233 - lr: 8.3333e-05 - 67s/epoch - 339ms/step
Epoch 344/1000
2023-09-26 21:51:34.259 
Epoch 344/1000 
	 loss: 16.6052, MinusLogProbMetric: 16.6052, val_loss: 17.0886, val_MinusLogProbMetric: 17.0886

Epoch 344: val_loss did not improve from 17.01579
196/196 - 66s - loss: 16.6052 - MinusLogProbMetric: 16.6052 - val_loss: 17.0886 - val_MinusLogProbMetric: 17.0886 - lr: 8.3333e-05 - 66s/epoch - 337ms/step
Epoch 345/1000
2023-09-26 21:52:39.936 
Epoch 345/1000 
	 loss: 16.6084, MinusLogProbMetric: 16.6084, val_loss: 17.0813, val_MinusLogProbMetric: 17.0813

Epoch 345: val_loss did not improve from 17.01579
196/196 - 66s - loss: 16.6084 - MinusLogProbMetric: 16.6084 - val_loss: 17.0813 - val_MinusLogProbMetric: 17.0813 - lr: 8.3333e-05 - 66s/epoch - 335ms/step
Epoch 346/1000
2023-09-26 21:53:46.656 
Epoch 346/1000 
	 loss: 16.6043, MinusLogProbMetric: 16.6043, val_loss: 17.1161, val_MinusLogProbMetric: 17.1161

Epoch 346: val_loss did not improve from 17.01579
196/196 - 67s - loss: 16.6043 - MinusLogProbMetric: 16.6043 - val_loss: 17.1161 - val_MinusLogProbMetric: 17.1161 - lr: 8.3333e-05 - 67s/epoch - 340ms/step
Epoch 347/1000
2023-09-26 21:54:53.745 
Epoch 347/1000 
	 loss: 16.5983, MinusLogProbMetric: 16.5983, val_loss: 17.0986, val_MinusLogProbMetric: 17.0986

Epoch 347: val_loss did not improve from 17.01579
196/196 - 67s - loss: 16.5983 - MinusLogProbMetric: 16.5983 - val_loss: 17.0986 - val_MinusLogProbMetric: 17.0986 - lr: 8.3333e-05 - 67s/epoch - 342ms/step
Epoch 348/1000
2023-09-26 21:56:00.887 
Epoch 348/1000 
	 loss: 16.6103, MinusLogProbMetric: 16.6103, val_loss: 17.0580, val_MinusLogProbMetric: 17.0580

Epoch 348: val_loss did not improve from 17.01579
196/196 - 67s - loss: 16.6103 - MinusLogProbMetric: 16.6103 - val_loss: 17.0580 - val_MinusLogProbMetric: 17.0580 - lr: 8.3333e-05 - 67s/epoch - 343ms/step
Epoch 349/1000
2023-09-26 21:57:07.242 
Epoch 349/1000 
	 loss: 16.5937, MinusLogProbMetric: 16.5937, val_loss: 17.0241, val_MinusLogProbMetric: 17.0241

Epoch 349: val_loss did not improve from 17.01579
196/196 - 66s - loss: 16.5937 - MinusLogProbMetric: 16.5937 - val_loss: 17.0241 - val_MinusLogProbMetric: 17.0241 - lr: 8.3333e-05 - 66s/epoch - 339ms/step
Epoch 350/1000
2023-09-26 21:58:13.421 
Epoch 350/1000 
	 loss: 16.6213, MinusLogProbMetric: 16.6213, val_loss: 17.0616, val_MinusLogProbMetric: 17.0616

Epoch 350: val_loss did not improve from 17.01579
196/196 - 66s - loss: 16.6213 - MinusLogProbMetric: 16.6213 - val_loss: 17.0616 - val_MinusLogProbMetric: 17.0616 - lr: 8.3333e-05 - 66s/epoch - 338ms/step
Epoch 351/1000
2023-09-26 21:59:19.541 
Epoch 351/1000 
	 loss: 16.6135, MinusLogProbMetric: 16.6135, val_loss: 17.0675, val_MinusLogProbMetric: 17.0675

Epoch 351: val_loss did not improve from 17.01579
196/196 - 66s - loss: 16.6135 - MinusLogProbMetric: 16.6135 - val_loss: 17.0675 - val_MinusLogProbMetric: 17.0675 - lr: 8.3333e-05 - 66s/epoch - 337ms/step
Epoch 352/1000
2023-09-26 22:00:24.087 
Epoch 352/1000 
	 loss: 16.6052, MinusLogProbMetric: 16.6052, val_loss: 17.0208, val_MinusLogProbMetric: 17.0208

Epoch 352: val_loss did not improve from 17.01579
196/196 - 65s - loss: 16.6052 - MinusLogProbMetric: 16.6052 - val_loss: 17.0208 - val_MinusLogProbMetric: 17.0208 - lr: 8.3333e-05 - 65s/epoch - 329ms/step
Epoch 353/1000
2023-09-26 22:01:30.048 
Epoch 353/1000 
	 loss: 16.6091, MinusLogProbMetric: 16.6091, val_loss: 17.0599, val_MinusLogProbMetric: 17.0599

Epoch 353: val_loss did not improve from 17.01579
196/196 - 66s - loss: 16.6091 - MinusLogProbMetric: 16.6091 - val_loss: 17.0599 - val_MinusLogProbMetric: 17.0599 - lr: 8.3333e-05 - 66s/epoch - 337ms/step
Epoch 354/1000
2023-09-26 22:02:36.162 
Epoch 354/1000 
	 loss: 16.6216, MinusLogProbMetric: 16.6216, val_loss: 17.0464, val_MinusLogProbMetric: 17.0464

Epoch 354: val_loss did not improve from 17.01579
196/196 - 66s - loss: 16.6216 - MinusLogProbMetric: 16.6216 - val_loss: 17.0464 - val_MinusLogProbMetric: 17.0464 - lr: 8.3333e-05 - 66s/epoch - 337ms/step
Epoch 355/1000
2023-09-26 22:03:42.356 
Epoch 355/1000 
	 loss: 16.6318, MinusLogProbMetric: 16.6318, val_loss: 17.0671, val_MinusLogProbMetric: 17.0671

Epoch 355: val_loss did not improve from 17.01579
196/196 - 66s - loss: 16.6318 - MinusLogProbMetric: 16.6318 - val_loss: 17.0671 - val_MinusLogProbMetric: 17.0671 - lr: 8.3333e-05 - 66s/epoch - 338ms/step
Epoch 356/1000
2023-09-26 22:04:48.161 
Epoch 356/1000 
	 loss: 16.6065, MinusLogProbMetric: 16.6065, val_loss: 17.0645, val_MinusLogProbMetric: 17.0645

Epoch 356: val_loss did not improve from 17.01579
196/196 - 66s - loss: 16.6065 - MinusLogProbMetric: 16.6065 - val_loss: 17.0645 - val_MinusLogProbMetric: 17.0645 - lr: 8.3333e-05 - 66s/epoch - 336ms/step
Epoch 357/1000
2023-09-26 22:05:55.709 
Epoch 357/1000 
	 loss: 16.6172, MinusLogProbMetric: 16.6172, val_loss: 17.0135, val_MinusLogProbMetric: 17.0135

Epoch 357: val_loss improved from 17.01579 to 17.01348, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_294/weights/best_weights.h5
196/196 - 69s - loss: 16.6172 - MinusLogProbMetric: 16.6172 - val_loss: 17.0135 - val_MinusLogProbMetric: 17.0135 - lr: 8.3333e-05 - 69s/epoch - 350ms/step
Epoch 358/1000
2023-09-26 22:07:01.980 
Epoch 358/1000 
	 loss: 16.6101, MinusLogProbMetric: 16.6101, val_loss: 17.0944, val_MinusLogProbMetric: 17.0944

Epoch 358: val_loss did not improve from 17.01348
196/196 - 65s - loss: 16.6101 - MinusLogProbMetric: 16.6101 - val_loss: 17.0944 - val_MinusLogProbMetric: 17.0944 - lr: 8.3333e-05 - 65s/epoch - 333ms/step
Epoch 359/1000
2023-09-26 22:08:08.431 
Epoch 359/1000 
	 loss: 16.6162, MinusLogProbMetric: 16.6162, val_loss: 17.0239, val_MinusLogProbMetric: 17.0239

Epoch 359: val_loss did not improve from 17.01348
196/196 - 66s - loss: 16.6162 - MinusLogProbMetric: 16.6162 - val_loss: 17.0239 - val_MinusLogProbMetric: 17.0239 - lr: 8.3333e-05 - 66s/epoch - 339ms/step
Epoch 360/1000
2023-09-26 22:09:14.298 
Epoch 360/1000 
	 loss: 16.6099, MinusLogProbMetric: 16.6099, val_loss: 17.0577, val_MinusLogProbMetric: 17.0577

Epoch 360: val_loss did not improve from 17.01348
196/196 - 66s - loss: 16.6099 - MinusLogProbMetric: 16.6099 - val_loss: 17.0577 - val_MinusLogProbMetric: 17.0577 - lr: 8.3333e-05 - 66s/epoch - 336ms/step
Epoch 361/1000
2023-09-26 22:10:19.736 
Epoch 361/1000 
	 loss: 16.5982, MinusLogProbMetric: 16.5982, val_loss: 17.0876, val_MinusLogProbMetric: 17.0876

Epoch 361: val_loss did not improve from 17.01348
196/196 - 65s - loss: 16.5982 - MinusLogProbMetric: 16.5982 - val_loss: 17.0876 - val_MinusLogProbMetric: 17.0876 - lr: 8.3333e-05 - 65s/epoch - 334ms/step
Epoch 362/1000
2023-09-26 22:11:27.498 
Epoch 362/1000 
	 loss: 16.6414, MinusLogProbMetric: 16.6414, val_loss: 17.0777, val_MinusLogProbMetric: 17.0777

Epoch 362: val_loss did not improve from 17.01348
196/196 - 68s - loss: 16.6414 - MinusLogProbMetric: 16.6414 - val_loss: 17.0777 - val_MinusLogProbMetric: 17.0777 - lr: 8.3333e-05 - 68s/epoch - 346ms/step
Epoch 363/1000
2023-09-26 22:12:33.854 
Epoch 363/1000 
	 loss: 16.6012, MinusLogProbMetric: 16.6012, val_loss: 17.0335, val_MinusLogProbMetric: 17.0335

Epoch 363: val_loss did not improve from 17.01348
196/196 - 66s - loss: 16.6012 - MinusLogProbMetric: 16.6012 - val_loss: 17.0335 - val_MinusLogProbMetric: 17.0335 - lr: 8.3333e-05 - 66s/epoch - 339ms/step
Epoch 364/1000
2023-09-26 22:13:39.924 
Epoch 364/1000 
	 loss: 16.6072, MinusLogProbMetric: 16.6072, val_loss: 17.1504, val_MinusLogProbMetric: 17.1504

Epoch 364: val_loss did not improve from 17.01348
196/196 - 66s - loss: 16.6072 - MinusLogProbMetric: 16.6072 - val_loss: 17.1504 - val_MinusLogProbMetric: 17.1504 - lr: 8.3333e-05 - 66s/epoch - 337ms/step
Epoch 365/1000
2023-09-26 22:14:46.803 
Epoch 365/1000 
	 loss: 16.5963, MinusLogProbMetric: 16.5963, val_loss: 17.1585, val_MinusLogProbMetric: 17.1585

Epoch 365: val_loss did not improve from 17.01348
196/196 - 67s - loss: 16.5963 - MinusLogProbMetric: 16.5963 - val_loss: 17.1585 - val_MinusLogProbMetric: 17.1585 - lr: 8.3333e-05 - 67s/epoch - 341ms/step
Epoch 366/1000
2023-09-26 22:15:51.376 
Epoch 366/1000 
	 loss: 16.6177, MinusLogProbMetric: 16.6177, val_loss: 17.0507, val_MinusLogProbMetric: 17.0507

Epoch 366: val_loss did not improve from 17.01348
196/196 - 65s - loss: 16.6177 - MinusLogProbMetric: 16.6177 - val_loss: 17.0507 - val_MinusLogProbMetric: 17.0507 - lr: 8.3333e-05 - 65s/epoch - 329ms/step
Epoch 367/1000
2023-09-26 22:16:56.911 
Epoch 367/1000 
	 loss: 16.5879, MinusLogProbMetric: 16.5879, val_loss: 17.0287, val_MinusLogProbMetric: 17.0287

Epoch 367: val_loss did not improve from 17.01348
196/196 - 66s - loss: 16.5879 - MinusLogProbMetric: 16.5879 - val_loss: 17.0287 - val_MinusLogProbMetric: 17.0287 - lr: 8.3333e-05 - 66s/epoch - 334ms/step
Epoch 368/1000
2023-09-26 22:18:02.582 
Epoch 368/1000 
	 loss: 16.6117, MinusLogProbMetric: 16.6117, val_loss: 17.0334, val_MinusLogProbMetric: 17.0334

Epoch 368: val_loss did not improve from 17.01348
196/196 - 66s - loss: 16.6117 - MinusLogProbMetric: 16.6117 - val_loss: 17.0334 - val_MinusLogProbMetric: 17.0334 - lr: 8.3333e-05 - 66s/epoch - 335ms/step
Epoch 369/1000
2023-09-26 22:19:08.565 
Epoch 369/1000 
	 loss: 16.5964, MinusLogProbMetric: 16.5964, val_loss: 17.0620, val_MinusLogProbMetric: 17.0620

Epoch 369: val_loss did not improve from 17.01348
196/196 - 66s - loss: 16.5964 - MinusLogProbMetric: 16.5964 - val_loss: 17.0620 - val_MinusLogProbMetric: 17.0620 - lr: 8.3333e-05 - 66s/epoch - 337ms/step
Epoch 370/1000
2023-09-26 22:20:13.657 
Epoch 370/1000 
	 loss: 16.5921, MinusLogProbMetric: 16.5921, val_loss: 17.1011, val_MinusLogProbMetric: 17.1011

Epoch 370: val_loss did not improve from 17.01348
196/196 - 65s - loss: 16.5921 - MinusLogProbMetric: 16.5921 - val_loss: 17.1011 - val_MinusLogProbMetric: 17.1011 - lr: 8.3333e-05 - 65s/epoch - 332ms/step
Epoch 371/1000
2023-09-26 22:21:20.529 
Epoch 371/1000 
	 loss: 16.6051, MinusLogProbMetric: 16.6051, val_loss: 17.2582, val_MinusLogProbMetric: 17.2582

Epoch 371: val_loss did not improve from 17.01348
196/196 - 67s - loss: 16.6051 - MinusLogProbMetric: 16.6051 - val_loss: 17.2582 - val_MinusLogProbMetric: 17.2582 - lr: 8.3333e-05 - 67s/epoch - 341ms/step
Epoch 372/1000
2023-09-26 22:22:27.288 
Epoch 372/1000 
	 loss: 16.6067, MinusLogProbMetric: 16.6067, val_loss: 17.0292, val_MinusLogProbMetric: 17.0292

Epoch 372: val_loss did not improve from 17.01348
196/196 - 67s - loss: 16.6067 - MinusLogProbMetric: 16.6067 - val_loss: 17.0292 - val_MinusLogProbMetric: 17.0292 - lr: 8.3333e-05 - 67s/epoch - 341ms/step
Epoch 373/1000
2023-09-26 22:23:33.067 
Epoch 373/1000 
	 loss: 16.5949, MinusLogProbMetric: 16.5949, val_loss: 17.1008, val_MinusLogProbMetric: 17.1008

Epoch 373: val_loss did not improve from 17.01348
196/196 - 66s - loss: 16.5949 - MinusLogProbMetric: 16.5949 - val_loss: 17.1008 - val_MinusLogProbMetric: 17.1008 - lr: 8.3333e-05 - 66s/epoch - 336ms/step
Epoch 374/1000
2023-09-26 22:24:39.364 
Epoch 374/1000 
	 loss: 16.5871, MinusLogProbMetric: 16.5871, val_loss: 17.2028, val_MinusLogProbMetric: 17.2028

Epoch 374: val_loss did not improve from 17.01348
196/196 - 66s - loss: 16.5871 - MinusLogProbMetric: 16.5871 - val_loss: 17.2028 - val_MinusLogProbMetric: 17.2028 - lr: 8.3333e-05 - 66s/epoch - 338ms/step
Epoch 375/1000
2023-09-26 22:25:46.011 
Epoch 375/1000 
	 loss: 16.6216, MinusLogProbMetric: 16.6216, val_loss: 17.0736, val_MinusLogProbMetric: 17.0736

Epoch 375: val_loss did not improve from 17.01348
196/196 - 67s - loss: 16.6216 - MinusLogProbMetric: 16.6216 - val_loss: 17.0736 - val_MinusLogProbMetric: 17.0736 - lr: 8.3333e-05 - 67s/epoch - 340ms/step
Epoch 376/1000
2023-09-26 22:26:52.000 
Epoch 376/1000 
	 loss: 16.5893, MinusLogProbMetric: 16.5893, val_loss: 17.0886, val_MinusLogProbMetric: 17.0886

Epoch 376: val_loss did not improve from 17.01348
196/196 - 66s - loss: 16.5893 - MinusLogProbMetric: 16.5893 - val_loss: 17.0886 - val_MinusLogProbMetric: 17.0886 - lr: 8.3333e-05 - 66s/epoch - 337ms/step
Epoch 377/1000
2023-09-26 22:27:56.754 
Epoch 377/1000 
	 loss: 16.5994, MinusLogProbMetric: 16.5994, val_loss: 17.2268, val_MinusLogProbMetric: 17.2268

Epoch 377: val_loss did not improve from 17.01348
196/196 - 65s - loss: 16.5994 - MinusLogProbMetric: 16.5994 - val_loss: 17.2268 - val_MinusLogProbMetric: 17.2268 - lr: 8.3333e-05 - 65s/epoch - 330ms/step
Epoch 378/1000
2023-09-26 22:29:02.925 
Epoch 378/1000 
	 loss: 16.6018, MinusLogProbMetric: 16.6018, val_loss: 17.1682, val_MinusLogProbMetric: 17.1682

Epoch 378: val_loss did not improve from 17.01348
196/196 - 66s - loss: 16.6018 - MinusLogProbMetric: 16.6018 - val_loss: 17.1682 - val_MinusLogProbMetric: 17.1682 - lr: 8.3333e-05 - 66s/epoch - 338ms/step
Epoch 379/1000
2023-09-26 22:30:10.118 
Epoch 379/1000 
	 loss: 16.6181, MinusLogProbMetric: 16.6181, val_loss: 17.0243, val_MinusLogProbMetric: 17.0243

Epoch 379: val_loss did not improve from 17.01348
196/196 - 67s - loss: 16.6181 - MinusLogProbMetric: 16.6181 - val_loss: 17.0243 - val_MinusLogProbMetric: 17.0243 - lr: 8.3333e-05 - 67s/epoch - 343ms/step
Epoch 380/1000
2023-09-26 22:31:15.917 
Epoch 380/1000 
	 loss: 16.5946, MinusLogProbMetric: 16.5946, val_loss: 17.0314, val_MinusLogProbMetric: 17.0314

Epoch 380: val_loss did not improve from 17.01348
196/196 - 66s - loss: 16.5946 - MinusLogProbMetric: 16.5946 - val_loss: 17.0314 - val_MinusLogProbMetric: 17.0314 - lr: 8.3333e-05 - 66s/epoch - 336ms/step
Epoch 381/1000
2023-09-26 22:32:17.060 
Epoch 381/1000 
	 loss: 16.5940, MinusLogProbMetric: 16.5940, val_loss: 17.0508, val_MinusLogProbMetric: 17.0508

Epoch 381: val_loss did not improve from 17.01348
196/196 - 61s - loss: 16.5940 - MinusLogProbMetric: 16.5940 - val_loss: 17.0508 - val_MinusLogProbMetric: 17.0508 - lr: 8.3333e-05 - 61s/epoch - 312ms/step
Epoch 382/1000
2023-09-26 22:33:12.635 
Epoch 382/1000 
	 loss: 16.5952, MinusLogProbMetric: 16.5952, val_loss: 17.0025, val_MinusLogProbMetric: 17.0025

Epoch 382: val_loss improved from 17.01348 to 17.00253, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_294/weights/best_weights.h5
196/196 - 57s - loss: 16.5952 - MinusLogProbMetric: 16.5952 - val_loss: 17.0025 - val_MinusLogProbMetric: 17.0025 - lr: 8.3333e-05 - 57s/epoch - 288ms/step
Epoch 383/1000
2023-09-26 22:34:07.080 
Epoch 383/1000 
	 loss: 16.6125, MinusLogProbMetric: 16.6125, val_loss: 17.1521, val_MinusLogProbMetric: 17.1521

Epoch 383: val_loss did not improve from 17.00253
196/196 - 54s - loss: 16.6125 - MinusLogProbMetric: 16.6125 - val_loss: 17.1521 - val_MinusLogProbMetric: 17.1521 - lr: 8.3333e-05 - 54s/epoch - 273ms/step
Epoch 384/1000
2023-09-26 22:35:06.792 
Epoch 384/1000 
	 loss: 16.5867, MinusLogProbMetric: 16.5867, val_loss: 17.0226, val_MinusLogProbMetric: 17.0226

Epoch 384: val_loss did not improve from 17.00253
196/196 - 60s - loss: 16.5867 - MinusLogProbMetric: 16.5867 - val_loss: 17.0226 - val_MinusLogProbMetric: 17.0226 - lr: 8.3333e-05 - 60s/epoch - 305ms/step
Epoch 385/1000
2023-09-26 22:36:05.824 
Epoch 385/1000 
	 loss: 16.6012, MinusLogProbMetric: 16.6012, val_loss: 17.2819, val_MinusLogProbMetric: 17.2819

Epoch 385: val_loss did not improve from 17.00253
196/196 - 59s - loss: 16.6012 - MinusLogProbMetric: 16.6012 - val_loss: 17.2819 - val_MinusLogProbMetric: 17.2819 - lr: 8.3333e-05 - 59s/epoch - 301ms/step
Epoch 386/1000
2023-09-26 22:36:58.247 
Epoch 386/1000 
	 loss: 16.6038, MinusLogProbMetric: 16.6038, val_loss: 17.0509, val_MinusLogProbMetric: 17.0509

Epoch 386: val_loss did not improve from 17.00253
196/196 - 52s - loss: 16.6038 - MinusLogProbMetric: 16.6038 - val_loss: 17.0509 - val_MinusLogProbMetric: 17.0509 - lr: 8.3333e-05 - 52s/epoch - 267ms/step
Epoch 387/1000
2023-09-26 22:37:54.548 
Epoch 387/1000 
	 loss: 16.5812, MinusLogProbMetric: 16.5812, val_loss: 17.0190, val_MinusLogProbMetric: 17.0190

Epoch 387: val_loss did not improve from 17.00253
196/196 - 56s - loss: 16.5812 - MinusLogProbMetric: 16.5812 - val_loss: 17.0190 - val_MinusLogProbMetric: 17.0190 - lr: 8.3333e-05 - 56s/epoch - 287ms/step
Epoch 388/1000
2023-09-26 22:38:58.179 
Epoch 388/1000 
	 loss: 16.6072, MinusLogProbMetric: 16.6072, val_loss: 17.0263, val_MinusLogProbMetric: 17.0263

Epoch 388: val_loss did not improve from 17.00253
196/196 - 64s - loss: 16.6072 - MinusLogProbMetric: 16.6072 - val_loss: 17.0263 - val_MinusLogProbMetric: 17.0263 - lr: 8.3333e-05 - 64s/epoch - 325ms/step
Epoch 389/1000
2023-09-26 22:40:02.621 
Epoch 389/1000 
	 loss: 16.5792, MinusLogProbMetric: 16.5792, val_loss: 17.1620, val_MinusLogProbMetric: 17.1620

Epoch 389: val_loss did not improve from 17.00253
196/196 - 64s - loss: 16.5792 - MinusLogProbMetric: 16.5792 - val_loss: 17.1620 - val_MinusLogProbMetric: 17.1620 - lr: 8.3333e-05 - 64s/epoch - 329ms/step
Epoch 390/1000
2023-09-26 22:41:08.209 
Epoch 390/1000 
	 loss: 16.6050, MinusLogProbMetric: 16.6050, val_loss: 17.0852, val_MinusLogProbMetric: 17.0852

Epoch 390: val_loss did not improve from 17.00253
196/196 - 66s - loss: 16.6050 - MinusLogProbMetric: 16.6050 - val_loss: 17.0852 - val_MinusLogProbMetric: 17.0852 - lr: 8.3333e-05 - 66s/epoch - 335ms/step
Epoch 391/1000
2023-09-26 22:42:13.391 
Epoch 391/1000 
	 loss: 16.6153, MinusLogProbMetric: 16.6153, val_loss: 17.1365, val_MinusLogProbMetric: 17.1365

Epoch 391: val_loss did not improve from 17.00253
196/196 - 65s - loss: 16.6153 - MinusLogProbMetric: 16.6153 - val_loss: 17.1365 - val_MinusLogProbMetric: 17.1365 - lr: 8.3333e-05 - 65s/epoch - 333ms/step
Epoch 392/1000
2023-09-26 22:43:19.255 
Epoch 392/1000 
	 loss: 16.5990, MinusLogProbMetric: 16.5990, val_loss: 17.0990, val_MinusLogProbMetric: 17.0990

Epoch 392: val_loss did not improve from 17.00253
196/196 - 66s - loss: 16.5990 - MinusLogProbMetric: 16.5990 - val_loss: 17.0990 - val_MinusLogProbMetric: 17.0990 - lr: 8.3333e-05 - 66s/epoch - 336ms/step
Epoch 393/1000
2023-09-26 22:44:24.646 
Epoch 393/1000 
	 loss: 16.5973, MinusLogProbMetric: 16.5973, val_loss: 17.0312, val_MinusLogProbMetric: 17.0312

Epoch 393: val_loss did not improve from 17.00253
196/196 - 65s - loss: 16.5973 - MinusLogProbMetric: 16.5973 - val_loss: 17.0312 - val_MinusLogProbMetric: 17.0312 - lr: 8.3333e-05 - 65s/epoch - 334ms/step
Epoch 394/1000
2023-09-26 22:45:31.070 
Epoch 394/1000 
	 loss: 16.5852, MinusLogProbMetric: 16.5852, val_loss: 17.0376, val_MinusLogProbMetric: 17.0376

Epoch 394: val_loss did not improve from 17.00253
196/196 - 66s - loss: 16.5852 - MinusLogProbMetric: 16.5852 - val_loss: 17.0376 - val_MinusLogProbMetric: 17.0376 - lr: 8.3333e-05 - 66s/epoch - 339ms/step
Epoch 395/1000
2023-09-26 22:46:36.850 
Epoch 395/1000 
	 loss: 16.6111, MinusLogProbMetric: 16.6111, val_loss: 17.1090, val_MinusLogProbMetric: 17.1090

Epoch 395: val_loss did not improve from 17.00253
196/196 - 66s - loss: 16.6111 - MinusLogProbMetric: 16.6111 - val_loss: 17.1090 - val_MinusLogProbMetric: 17.1090 - lr: 8.3333e-05 - 66s/epoch - 336ms/step
Epoch 396/1000
2023-09-26 22:47:42.992 
Epoch 396/1000 
	 loss: 16.5791, MinusLogProbMetric: 16.5791, val_loss: 17.0224, val_MinusLogProbMetric: 17.0224

Epoch 396: val_loss did not improve from 17.00253
196/196 - 66s - loss: 16.5791 - MinusLogProbMetric: 16.5791 - val_loss: 17.0224 - val_MinusLogProbMetric: 17.0224 - lr: 8.3333e-05 - 66s/epoch - 337ms/step
Epoch 397/1000
2023-09-26 22:48:48.438 
Epoch 397/1000 
	 loss: 16.5798, MinusLogProbMetric: 16.5798, val_loss: 17.1213, val_MinusLogProbMetric: 17.1213

Epoch 397: val_loss did not improve from 17.00253
196/196 - 65s - loss: 16.5798 - MinusLogProbMetric: 16.5798 - val_loss: 17.1213 - val_MinusLogProbMetric: 17.1213 - lr: 8.3333e-05 - 65s/epoch - 334ms/step
Epoch 398/1000
2023-09-26 22:49:54.294 
Epoch 398/1000 
	 loss: 16.6022, MinusLogProbMetric: 16.6022, val_loss: 17.1027, val_MinusLogProbMetric: 17.1027

Epoch 398: val_loss did not improve from 17.00253
196/196 - 66s - loss: 16.6022 - MinusLogProbMetric: 16.6022 - val_loss: 17.1027 - val_MinusLogProbMetric: 17.1027 - lr: 8.3333e-05 - 66s/epoch - 336ms/step
Epoch 399/1000
2023-09-26 22:50:58.829 
Epoch 399/1000 
	 loss: 16.6285, MinusLogProbMetric: 16.6285, val_loss: 17.1141, val_MinusLogProbMetric: 17.1141

Epoch 399: val_loss did not improve from 17.00253
196/196 - 65s - loss: 16.6285 - MinusLogProbMetric: 16.6285 - val_loss: 17.1141 - val_MinusLogProbMetric: 17.1141 - lr: 8.3333e-05 - 65s/epoch - 329ms/step
Epoch 400/1000
2023-09-26 22:52:02.980 
Epoch 400/1000 
	 loss: 16.6054, MinusLogProbMetric: 16.6054, val_loss: 17.1598, val_MinusLogProbMetric: 17.1598

Epoch 400: val_loss did not improve from 17.00253
196/196 - 64s - loss: 16.6054 - MinusLogProbMetric: 16.6054 - val_loss: 17.1598 - val_MinusLogProbMetric: 17.1598 - lr: 8.3333e-05 - 64s/epoch - 327ms/step
Epoch 401/1000
2023-09-26 22:53:08.268 
Epoch 401/1000 
	 loss: 16.5931, MinusLogProbMetric: 16.5931, val_loss: 17.0240, val_MinusLogProbMetric: 17.0240

Epoch 401: val_loss did not improve from 17.00253
196/196 - 65s - loss: 16.5931 - MinusLogProbMetric: 16.5931 - val_loss: 17.0240 - val_MinusLogProbMetric: 17.0240 - lr: 8.3333e-05 - 65s/epoch - 333ms/step
Epoch 402/1000
2023-09-26 22:54:14.343 
Epoch 402/1000 
	 loss: 16.5935, MinusLogProbMetric: 16.5935, val_loss: 17.0479, val_MinusLogProbMetric: 17.0479

Epoch 402: val_loss did not improve from 17.00253
196/196 - 66s - loss: 16.5935 - MinusLogProbMetric: 16.5935 - val_loss: 17.0479 - val_MinusLogProbMetric: 17.0479 - lr: 8.3333e-05 - 66s/epoch - 337ms/step
Epoch 403/1000
2023-09-26 22:55:18.767 
Epoch 403/1000 
	 loss: 16.5792, MinusLogProbMetric: 16.5792, val_loss: 17.1174, val_MinusLogProbMetric: 17.1174

Epoch 403: val_loss did not improve from 17.00253
196/196 - 64s - loss: 16.5792 - MinusLogProbMetric: 16.5792 - val_loss: 17.1174 - val_MinusLogProbMetric: 17.1174 - lr: 8.3333e-05 - 64s/epoch - 329ms/step
Epoch 404/1000
2023-09-26 22:56:23.330 
Epoch 404/1000 
	 loss: 16.5834, MinusLogProbMetric: 16.5834, val_loss: 17.0597, val_MinusLogProbMetric: 17.0597

Epoch 404: val_loss did not improve from 17.00253
196/196 - 65s - loss: 16.5834 - MinusLogProbMetric: 16.5834 - val_loss: 17.0597 - val_MinusLogProbMetric: 17.0597 - lr: 8.3333e-05 - 65s/epoch - 329ms/step
Epoch 405/1000
2023-09-26 22:57:28.250 
Epoch 405/1000 
	 loss: 16.5863, MinusLogProbMetric: 16.5863, val_loss: 17.0658, val_MinusLogProbMetric: 17.0658

Epoch 405: val_loss did not improve from 17.00253
196/196 - 65s - loss: 16.5863 - MinusLogProbMetric: 16.5863 - val_loss: 17.0658 - val_MinusLogProbMetric: 17.0658 - lr: 8.3333e-05 - 65s/epoch - 331ms/step
Epoch 406/1000
2023-09-26 22:58:33.249 
Epoch 406/1000 
	 loss: 16.6024, MinusLogProbMetric: 16.6024, val_loss: 17.0934, val_MinusLogProbMetric: 17.0934

Epoch 406: val_loss did not improve from 17.00253
196/196 - 65s - loss: 16.6024 - MinusLogProbMetric: 16.6024 - val_loss: 17.0934 - val_MinusLogProbMetric: 17.0934 - lr: 8.3333e-05 - 65s/epoch - 332ms/step
Epoch 407/1000
2023-09-26 22:59:38.753 
Epoch 407/1000 
	 loss: 16.5974, MinusLogProbMetric: 16.5974, val_loss: 17.1583, val_MinusLogProbMetric: 17.1583

Epoch 407: val_loss did not improve from 17.00253
196/196 - 65s - loss: 16.5974 - MinusLogProbMetric: 16.5974 - val_loss: 17.1583 - val_MinusLogProbMetric: 17.1583 - lr: 8.3333e-05 - 65s/epoch - 334ms/step
Epoch 408/1000
2023-09-26 23:00:44.991 
Epoch 408/1000 
	 loss: 16.5920, MinusLogProbMetric: 16.5920, val_loss: 17.0847, val_MinusLogProbMetric: 17.0847

Epoch 408: val_loss did not improve from 17.00253
196/196 - 66s - loss: 16.5920 - MinusLogProbMetric: 16.5920 - val_loss: 17.0847 - val_MinusLogProbMetric: 17.0847 - lr: 8.3333e-05 - 66s/epoch - 338ms/step
Epoch 409/1000
2023-09-26 23:01:50.252 
Epoch 409/1000 
	 loss: 16.5857, MinusLogProbMetric: 16.5857, val_loss: 17.0362, val_MinusLogProbMetric: 17.0362

Epoch 409: val_loss did not improve from 17.00253
196/196 - 65s - loss: 16.5857 - MinusLogProbMetric: 16.5857 - val_loss: 17.0362 - val_MinusLogProbMetric: 17.0362 - lr: 8.3333e-05 - 65s/epoch - 333ms/step
Epoch 410/1000
2023-09-26 23:02:57.345 
Epoch 410/1000 
	 loss: 16.5838, MinusLogProbMetric: 16.5838, val_loss: 17.0165, val_MinusLogProbMetric: 17.0165

Epoch 410: val_loss did not improve from 17.00253
196/196 - 67s - loss: 16.5838 - MinusLogProbMetric: 16.5838 - val_loss: 17.0165 - val_MinusLogProbMetric: 17.0165 - lr: 8.3333e-05 - 67s/epoch - 342ms/step
Epoch 411/1000
2023-09-26 23:04:03.085 
Epoch 411/1000 
	 loss: 16.5707, MinusLogProbMetric: 16.5707, val_loss: 17.0381, val_MinusLogProbMetric: 17.0381

Epoch 411: val_loss did not improve from 17.00253
196/196 - 66s - loss: 16.5707 - MinusLogProbMetric: 16.5707 - val_loss: 17.0381 - val_MinusLogProbMetric: 17.0381 - lr: 8.3333e-05 - 66s/epoch - 335ms/step
Epoch 412/1000
2023-09-26 23:05:08.624 
Epoch 412/1000 
	 loss: 16.5816, MinusLogProbMetric: 16.5816, val_loss: 17.0687, val_MinusLogProbMetric: 17.0687

Epoch 412: val_loss did not improve from 17.00253
196/196 - 66s - loss: 16.5816 - MinusLogProbMetric: 16.5816 - val_loss: 17.0687 - val_MinusLogProbMetric: 17.0687 - lr: 8.3333e-05 - 66s/epoch - 334ms/step
Epoch 413/1000
2023-09-26 23:06:15.249 
Epoch 413/1000 
	 loss: 16.5915, MinusLogProbMetric: 16.5915, val_loss: 17.1323, val_MinusLogProbMetric: 17.1323

Epoch 413: val_loss did not improve from 17.00253
196/196 - 67s - loss: 16.5915 - MinusLogProbMetric: 16.5915 - val_loss: 17.1323 - val_MinusLogProbMetric: 17.1323 - lr: 8.3333e-05 - 67s/epoch - 340ms/step
Epoch 414/1000
2023-09-26 23:07:22.759 
Epoch 414/1000 
	 loss: 16.5926, MinusLogProbMetric: 16.5926, val_loss: 17.0492, val_MinusLogProbMetric: 17.0492

Epoch 414: val_loss did not improve from 17.00253
196/196 - 68s - loss: 16.5926 - MinusLogProbMetric: 16.5926 - val_loss: 17.0492 - val_MinusLogProbMetric: 17.0492 - lr: 8.3333e-05 - 68s/epoch - 344ms/step
Epoch 415/1000
2023-09-26 23:08:29.338 
Epoch 415/1000 
	 loss: 16.6018, MinusLogProbMetric: 16.6018, val_loss: 17.0317, val_MinusLogProbMetric: 17.0317

Epoch 415: val_loss did not improve from 17.00253
196/196 - 67s - loss: 16.6018 - MinusLogProbMetric: 16.6018 - val_loss: 17.0317 - val_MinusLogProbMetric: 17.0317 - lr: 8.3333e-05 - 67s/epoch - 340ms/step
Epoch 416/1000
2023-09-26 23:09:35.614 
Epoch 416/1000 
	 loss: 16.5764, MinusLogProbMetric: 16.5764, val_loss: 17.0315, val_MinusLogProbMetric: 17.0315

Epoch 416: val_loss did not improve from 17.00253
196/196 - 66s - loss: 16.5764 - MinusLogProbMetric: 16.5764 - val_loss: 17.0315 - val_MinusLogProbMetric: 17.0315 - lr: 8.3333e-05 - 66s/epoch - 338ms/step
Epoch 417/1000
2023-09-26 23:10:40.953 
Epoch 417/1000 
	 loss: 16.5830, MinusLogProbMetric: 16.5830, val_loss: 17.2243, val_MinusLogProbMetric: 17.2243

Epoch 417: val_loss did not improve from 17.00253
196/196 - 65s - loss: 16.5830 - MinusLogProbMetric: 16.5830 - val_loss: 17.2243 - val_MinusLogProbMetric: 17.2243 - lr: 8.3333e-05 - 65s/epoch - 333ms/step
Epoch 418/1000
2023-09-26 23:11:46.556 
Epoch 418/1000 
	 loss: 16.5868, MinusLogProbMetric: 16.5868, val_loss: 17.0809, val_MinusLogProbMetric: 17.0809

Epoch 418: val_loss did not improve from 17.00253
196/196 - 66s - loss: 16.5868 - MinusLogProbMetric: 16.5868 - val_loss: 17.0809 - val_MinusLogProbMetric: 17.0809 - lr: 8.3333e-05 - 66s/epoch - 335ms/step
Epoch 419/1000
2023-09-26 23:12:51.482 
Epoch 419/1000 
	 loss: 16.5693, MinusLogProbMetric: 16.5693, val_loss: 17.0159, val_MinusLogProbMetric: 17.0159

Epoch 419: val_loss did not improve from 17.00253
196/196 - 65s - loss: 16.5693 - MinusLogProbMetric: 16.5693 - val_loss: 17.0159 - val_MinusLogProbMetric: 17.0159 - lr: 8.3333e-05 - 65s/epoch - 331ms/step
Epoch 420/1000
2023-09-26 23:13:57.082 
Epoch 420/1000 
	 loss: 16.6103, MinusLogProbMetric: 16.6103, val_loss: 17.0528, val_MinusLogProbMetric: 17.0528

Epoch 420: val_loss did not improve from 17.00253
196/196 - 66s - loss: 16.6103 - MinusLogProbMetric: 16.6103 - val_loss: 17.0528 - val_MinusLogProbMetric: 17.0528 - lr: 8.3333e-05 - 66s/epoch - 335ms/step
Epoch 421/1000
2023-09-26 23:15:01.829 
Epoch 421/1000 
	 loss: 16.5886, MinusLogProbMetric: 16.5886, val_loss: 17.0597, val_MinusLogProbMetric: 17.0597

Epoch 421: val_loss did not improve from 17.00253
196/196 - 65s - loss: 16.5886 - MinusLogProbMetric: 16.5886 - val_loss: 17.0597 - val_MinusLogProbMetric: 17.0597 - lr: 8.3333e-05 - 65s/epoch - 330ms/step
Epoch 422/1000
2023-09-26 23:16:06.753 
Epoch 422/1000 
	 loss: 16.5709, MinusLogProbMetric: 16.5709, val_loss: 17.0061, val_MinusLogProbMetric: 17.0061

Epoch 422: val_loss did not improve from 17.00253
196/196 - 65s - loss: 16.5709 - MinusLogProbMetric: 16.5709 - val_loss: 17.0061 - val_MinusLogProbMetric: 17.0061 - lr: 8.3333e-05 - 65s/epoch - 331ms/step
Epoch 423/1000
2023-09-26 23:17:11.411 
Epoch 423/1000 
	 loss: 16.5866, MinusLogProbMetric: 16.5866, val_loss: 17.0669, val_MinusLogProbMetric: 17.0669

Epoch 423: val_loss did not improve from 17.00253
196/196 - 65s - loss: 16.5866 - MinusLogProbMetric: 16.5866 - val_loss: 17.0669 - val_MinusLogProbMetric: 17.0669 - lr: 8.3333e-05 - 65s/epoch - 330ms/step
Epoch 424/1000
2023-09-26 23:18:16.659 
Epoch 424/1000 
	 loss: 16.5646, MinusLogProbMetric: 16.5646, val_loss: 17.0341, val_MinusLogProbMetric: 17.0341

Epoch 424: val_loss did not improve from 17.00253
196/196 - 65s - loss: 16.5646 - MinusLogProbMetric: 16.5646 - val_loss: 17.0341 - val_MinusLogProbMetric: 17.0341 - lr: 8.3333e-05 - 65s/epoch - 333ms/step
Epoch 425/1000
2023-09-26 23:19:21.099 
Epoch 425/1000 
	 loss: 16.5864, MinusLogProbMetric: 16.5864, val_loss: 17.0879, val_MinusLogProbMetric: 17.0879

Epoch 425: val_loss did not improve from 17.00253
196/196 - 64s - loss: 16.5864 - MinusLogProbMetric: 16.5864 - val_loss: 17.0879 - val_MinusLogProbMetric: 17.0879 - lr: 8.3333e-05 - 64s/epoch - 329ms/step
Epoch 426/1000
2023-09-26 23:20:27.338 
Epoch 426/1000 
	 loss: 16.5756, MinusLogProbMetric: 16.5756, val_loss: 17.0578, val_MinusLogProbMetric: 17.0578

Epoch 426: val_loss did not improve from 17.00253
196/196 - 66s - loss: 16.5756 - MinusLogProbMetric: 16.5756 - val_loss: 17.0578 - val_MinusLogProbMetric: 17.0578 - lr: 8.3333e-05 - 66s/epoch - 338ms/step
Epoch 427/1000
2023-09-26 23:21:33.197 
Epoch 427/1000 
	 loss: 16.6002, MinusLogProbMetric: 16.6002, val_loss: 17.0743, val_MinusLogProbMetric: 17.0743

Epoch 427: val_loss did not improve from 17.00253
196/196 - 66s - loss: 16.6002 - MinusLogProbMetric: 16.6002 - val_loss: 17.0743 - val_MinusLogProbMetric: 17.0743 - lr: 8.3333e-05 - 66s/epoch - 336ms/step
Epoch 428/1000
2023-09-26 23:22:37.864 
Epoch 428/1000 
	 loss: 16.5742, MinusLogProbMetric: 16.5742, val_loss: 17.0861, val_MinusLogProbMetric: 17.0861

Epoch 428: val_loss did not improve from 17.00253
196/196 - 65s - loss: 16.5742 - MinusLogProbMetric: 16.5742 - val_loss: 17.0861 - val_MinusLogProbMetric: 17.0861 - lr: 8.3333e-05 - 65s/epoch - 330ms/step
Epoch 429/1000
2023-09-26 23:23:42.814 
Epoch 429/1000 
	 loss: 16.6099, MinusLogProbMetric: 16.6099, val_loss: 17.2500, val_MinusLogProbMetric: 17.2500

Epoch 429: val_loss did not improve from 17.00253
196/196 - 65s - loss: 16.6099 - MinusLogProbMetric: 16.6099 - val_loss: 17.2500 - val_MinusLogProbMetric: 17.2500 - lr: 8.3333e-05 - 65s/epoch - 331ms/step
Epoch 430/1000
2023-09-26 23:24:48.236 
Epoch 430/1000 
	 loss: 16.5873, MinusLogProbMetric: 16.5873, val_loss: 17.1000, val_MinusLogProbMetric: 17.1000

Epoch 430: val_loss did not improve from 17.00253
196/196 - 65s - loss: 16.5873 - MinusLogProbMetric: 16.5873 - val_loss: 17.1000 - val_MinusLogProbMetric: 17.1000 - lr: 8.3333e-05 - 65s/epoch - 334ms/step
Epoch 431/1000
2023-09-26 23:25:54.360 
Epoch 431/1000 
	 loss: 16.5711, MinusLogProbMetric: 16.5711, val_loss: 17.1273, val_MinusLogProbMetric: 17.1273

Epoch 431: val_loss did not improve from 17.00253
196/196 - 66s - loss: 16.5711 - MinusLogProbMetric: 16.5711 - val_loss: 17.1273 - val_MinusLogProbMetric: 17.1273 - lr: 8.3333e-05 - 66s/epoch - 337ms/step
Epoch 432/1000
2023-09-26 23:26:59.843 
Epoch 432/1000 
	 loss: 16.5945, MinusLogProbMetric: 16.5945, val_loss: 17.0685, val_MinusLogProbMetric: 17.0685

Epoch 432: val_loss did not improve from 17.00253
196/196 - 65s - loss: 16.5945 - MinusLogProbMetric: 16.5945 - val_loss: 17.0685 - val_MinusLogProbMetric: 17.0685 - lr: 8.3333e-05 - 65s/epoch - 334ms/step
Epoch 433/1000
2023-09-26 23:28:06.530 
Epoch 433/1000 
	 loss: 16.5200, MinusLogProbMetric: 16.5200, val_loss: 16.9772, val_MinusLogProbMetric: 16.9772

Epoch 433: val_loss improved from 17.00253 to 16.97721, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_294/weights/best_weights.h5
196/196 - 68s - loss: 16.5200 - MinusLogProbMetric: 16.5200 - val_loss: 16.9772 - val_MinusLogProbMetric: 16.9772 - lr: 4.1667e-05 - 68s/epoch - 345ms/step
Epoch 434/1000
2023-09-26 23:29:13.072 
Epoch 434/1000 
	 loss: 16.5109, MinusLogProbMetric: 16.5109, val_loss: 16.9969, val_MinusLogProbMetric: 16.9969

Epoch 434: val_loss did not improve from 16.97721
196/196 - 66s - loss: 16.5109 - MinusLogProbMetric: 16.5109 - val_loss: 16.9969 - val_MinusLogProbMetric: 16.9969 - lr: 4.1667e-05 - 66s/epoch - 335ms/step
Epoch 435/1000
2023-09-26 23:30:17.881 
Epoch 435/1000 
	 loss: 16.5071, MinusLogProbMetric: 16.5071, val_loss: 16.9754, val_MinusLogProbMetric: 16.9754

Epoch 435: val_loss improved from 16.97721 to 16.97543, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_294/weights/best_weights.h5
196/196 - 66s - loss: 16.5071 - MinusLogProbMetric: 16.5071 - val_loss: 16.9754 - val_MinusLogProbMetric: 16.9754 - lr: 4.1667e-05 - 66s/epoch - 335ms/step
Epoch 436/1000
2023-09-26 23:31:24.158 
Epoch 436/1000 
	 loss: 16.5084, MinusLogProbMetric: 16.5084, val_loss: 17.0242, val_MinusLogProbMetric: 17.0242

Epoch 436: val_loss did not improve from 16.97543
196/196 - 65s - loss: 16.5084 - MinusLogProbMetric: 16.5084 - val_loss: 17.0242 - val_MinusLogProbMetric: 17.0242 - lr: 4.1667e-05 - 65s/epoch - 333ms/step
Epoch 437/1000
2023-09-26 23:32:30.112 
Epoch 437/1000 
	 loss: 16.5102, MinusLogProbMetric: 16.5102, val_loss: 17.0190, val_MinusLogProbMetric: 17.0190

Epoch 437: val_loss did not improve from 16.97543
196/196 - 66s - loss: 16.5102 - MinusLogProbMetric: 16.5102 - val_loss: 17.0190 - val_MinusLogProbMetric: 17.0190 - lr: 4.1667e-05 - 66s/epoch - 336ms/step
Epoch 438/1000
2023-09-26 23:33:34.557 
Epoch 438/1000 
	 loss: 16.5104, MinusLogProbMetric: 16.5104, val_loss: 16.9960, val_MinusLogProbMetric: 16.9960

Epoch 438: val_loss did not improve from 16.97543
196/196 - 64s - loss: 16.5104 - MinusLogProbMetric: 16.5104 - val_loss: 16.9960 - val_MinusLogProbMetric: 16.9960 - lr: 4.1667e-05 - 64s/epoch - 329ms/step
Epoch 439/1000
2023-09-26 23:34:39.321 
Epoch 439/1000 
	 loss: 16.5071, MinusLogProbMetric: 16.5071, val_loss: 16.9947, val_MinusLogProbMetric: 16.9947

Epoch 439: val_loss did not improve from 16.97543
196/196 - 65s - loss: 16.5071 - MinusLogProbMetric: 16.5071 - val_loss: 16.9947 - val_MinusLogProbMetric: 16.9947 - lr: 4.1667e-05 - 65s/epoch - 330ms/step
Epoch 440/1000
2023-09-26 23:35:44.398 
Epoch 440/1000 
	 loss: 16.5064, MinusLogProbMetric: 16.5064, val_loss: 16.9799, val_MinusLogProbMetric: 16.9799

Epoch 440: val_loss did not improve from 16.97543
196/196 - 65s - loss: 16.5064 - MinusLogProbMetric: 16.5064 - val_loss: 16.9799 - val_MinusLogProbMetric: 16.9799 - lr: 4.1667e-05 - 65s/epoch - 332ms/step
Epoch 441/1000
2023-09-26 23:36:49.223 
Epoch 441/1000 
	 loss: 16.5071, MinusLogProbMetric: 16.5071, val_loss: 16.9764, val_MinusLogProbMetric: 16.9764

Epoch 441: val_loss did not improve from 16.97543
196/196 - 65s - loss: 16.5071 - MinusLogProbMetric: 16.5071 - val_loss: 16.9764 - val_MinusLogProbMetric: 16.9764 - lr: 4.1667e-05 - 65s/epoch - 331ms/step
Epoch 442/1000
2023-09-26 23:37:55.250 
Epoch 442/1000 
	 loss: 16.5102, MinusLogProbMetric: 16.5102, val_loss: 16.9976, val_MinusLogProbMetric: 16.9976

Epoch 442: val_loss did not improve from 16.97543
196/196 - 66s - loss: 16.5102 - MinusLogProbMetric: 16.5102 - val_loss: 16.9976 - val_MinusLogProbMetric: 16.9976 - lr: 4.1667e-05 - 66s/epoch - 337ms/step
Epoch 443/1000
2023-09-26 23:39:00.530 
Epoch 443/1000 
	 loss: 16.5089, MinusLogProbMetric: 16.5089, val_loss: 16.9862, val_MinusLogProbMetric: 16.9862

Epoch 443: val_loss did not improve from 16.97543
196/196 - 65s - loss: 16.5089 - MinusLogProbMetric: 16.5089 - val_loss: 16.9862 - val_MinusLogProbMetric: 16.9862 - lr: 4.1667e-05 - 65s/epoch - 333ms/step
Epoch 444/1000
2023-09-26 23:40:05.894 
Epoch 444/1000 
	 loss: 16.5110, MinusLogProbMetric: 16.5110, val_loss: 17.0366, val_MinusLogProbMetric: 17.0366

Epoch 444: val_loss did not improve from 16.97543
196/196 - 65s - loss: 16.5110 - MinusLogProbMetric: 16.5110 - val_loss: 17.0366 - val_MinusLogProbMetric: 17.0366 - lr: 4.1667e-05 - 65s/epoch - 333ms/step
Epoch 445/1000
2023-09-26 23:41:09.901 
Epoch 445/1000 
	 loss: 16.5070, MinusLogProbMetric: 16.5070, val_loss: 17.0335, val_MinusLogProbMetric: 17.0335

Epoch 445: val_loss did not improve from 16.97543
196/196 - 64s - loss: 16.5070 - MinusLogProbMetric: 16.5070 - val_loss: 17.0335 - val_MinusLogProbMetric: 17.0335 - lr: 4.1667e-05 - 64s/epoch - 327ms/step
Epoch 446/1000
2023-09-26 23:42:15.362 
Epoch 446/1000 
	 loss: 16.5167, MinusLogProbMetric: 16.5167, val_loss: 17.0220, val_MinusLogProbMetric: 17.0220

Epoch 446: val_loss did not improve from 16.97543
196/196 - 65s - loss: 16.5167 - MinusLogProbMetric: 16.5167 - val_loss: 17.0220 - val_MinusLogProbMetric: 17.0220 - lr: 4.1667e-05 - 65s/epoch - 334ms/step
Epoch 447/1000
2023-09-26 23:43:21.659 
Epoch 447/1000 
	 loss: 16.5065, MinusLogProbMetric: 16.5065, val_loss: 16.9983, val_MinusLogProbMetric: 16.9983

Epoch 447: val_loss did not improve from 16.97543
196/196 - 66s - loss: 16.5065 - MinusLogProbMetric: 16.5065 - val_loss: 16.9983 - val_MinusLogProbMetric: 16.9983 - lr: 4.1667e-05 - 66s/epoch - 338ms/step
Epoch 448/1000
2023-09-26 23:44:27.328 
Epoch 448/1000 
	 loss: 16.5085, MinusLogProbMetric: 16.5085, val_loss: 17.0461, val_MinusLogProbMetric: 17.0461

Epoch 448: val_loss did not improve from 16.97543
196/196 - 66s - loss: 16.5085 - MinusLogProbMetric: 16.5085 - val_loss: 17.0461 - val_MinusLogProbMetric: 17.0461 - lr: 4.1667e-05 - 66s/epoch - 335ms/step
Epoch 449/1000
2023-09-26 23:45:32.296 
Epoch 449/1000 
	 loss: 16.5045, MinusLogProbMetric: 16.5045, val_loss: 16.9781, val_MinusLogProbMetric: 16.9781

Epoch 449: val_loss did not improve from 16.97543
196/196 - 65s - loss: 16.5045 - MinusLogProbMetric: 16.5045 - val_loss: 16.9781 - val_MinusLogProbMetric: 16.9781 - lr: 4.1667e-05 - 65s/epoch - 331ms/step
Epoch 450/1000
2023-09-26 23:46:37.236 
Epoch 450/1000 
	 loss: 16.5069, MinusLogProbMetric: 16.5069, val_loss: 16.9948, val_MinusLogProbMetric: 16.9948

Epoch 450: val_loss did not improve from 16.97543
196/196 - 65s - loss: 16.5069 - MinusLogProbMetric: 16.5069 - val_loss: 16.9948 - val_MinusLogProbMetric: 16.9948 - lr: 4.1667e-05 - 65s/epoch - 331ms/step
Epoch 451/1000
2023-09-26 23:47:42.855 
Epoch 451/1000 
	 loss: 16.5047, MinusLogProbMetric: 16.5047, val_loss: 16.9883, val_MinusLogProbMetric: 16.9883

Epoch 451: val_loss did not improve from 16.97543
196/196 - 66s - loss: 16.5047 - MinusLogProbMetric: 16.5047 - val_loss: 16.9883 - val_MinusLogProbMetric: 16.9883 - lr: 4.1667e-05 - 66s/epoch - 335ms/step
Epoch 452/1000
2023-09-26 23:48:47.120 
Epoch 452/1000 
	 loss: 16.5040, MinusLogProbMetric: 16.5040, val_loss: 16.9900, val_MinusLogProbMetric: 16.9900

Epoch 452: val_loss did not improve from 16.97543
196/196 - 64s - loss: 16.5040 - MinusLogProbMetric: 16.5040 - val_loss: 16.9900 - val_MinusLogProbMetric: 16.9900 - lr: 4.1667e-05 - 64s/epoch - 328ms/step
Epoch 453/1000
2023-09-26 23:49:52.976 
Epoch 453/1000 
	 loss: 16.5050, MinusLogProbMetric: 16.5050, val_loss: 16.9928, val_MinusLogProbMetric: 16.9928

Epoch 453: val_loss did not improve from 16.97543
196/196 - 66s - loss: 16.5050 - MinusLogProbMetric: 16.5050 - val_loss: 16.9928 - val_MinusLogProbMetric: 16.9928 - lr: 4.1667e-05 - 66s/epoch - 336ms/step
Epoch 454/1000
2023-09-26 23:50:59.469 
Epoch 454/1000 
	 loss: 16.5120, MinusLogProbMetric: 16.5120, val_loss: 17.0428, val_MinusLogProbMetric: 17.0428

Epoch 454: val_loss did not improve from 16.97543
196/196 - 66s - loss: 16.5120 - MinusLogProbMetric: 16.5120 - val_loss: 17.0428 - val_MinusLogProbMetric: 17.0428 - lr: 4.1667e-05 - 66s/epoch - 339ms/step
Epoch 455/1000
2023-09-26 23:52:05.798 
Epoch 455/1000 
	 loss: 16.5155, MinusLogProbMetric: 16.5155, val_loss: 16.9973, val_MinusLogProbMetric: 16.9973

Epoch 455: val_loss did not improve from 16.97543
196/196 - 66s - loss: 16.5155 - MinusLogProbMetric: 16.5155 - val_loss: 16.9973 - val_MinusLogProbMetric: 16.9973 - lr: 4.1667e-05 - 66s/epoch - 338ms/step
Epoch 456/1000
2023-09-26 23:53:13.073 
Epoch 456/1000 
	 loss: 16.5058, MinusLogProbMetric: 16.5058, val_loss: 17.0012, val_MinusLogProbMetric: 17.0012

Epoch 456: val_loss did not improve from 16.97543
196/196 - 67s - loss: 16.5058 - MinusLogProbMetric: 16.5058 - val_loss: 17.0012 - val_MinusLogProbMetric: 17.0012 - lr: 4.1667e-05 - 67s/epoch - 343ms/step
Epoch 457/1000
2023-09-26 23:54:19.455 
Epoch 457/1000 
	 loss: 16.5096, MinusLogProbMetric: 16.5096, val_loss: 17.0104, val_MinusLogProbMetric: 17.0104

Epoch 457: val_loss did not improve from 16.97543
196/196 - 66s - loss: 16.5096 - MinusLogProbMetric: 16.5096 - val_loss: 17.0104 - val_MinusLogProbMetric: 17.0104 - lr: 4.1667e-05 - 66s/epoch - 339ms/step
Epoch 458/1000
2023-09-26 23:55:24.502 
Epoch 458/1000 
	 loss: 16.5046, MinusLogProbMetric: 16.5046, val_loss: 16.9820, val_MinusLogProbMetric: 16.9820

Epoch 458: val_loss did not improve from 16.97543
196/196 - 65s - loss: 16.5046 - MinusLogProbMetric: 16.5046 - val_loss: 16.9820 - val_MinusLogProbMetric: 16.9820 - lr: 4.1667e-05 - 65s/epoch - 332ms/step
Epoch 459/1000
2023-09-26 23:56:29.756 
Epoch 459/1000 
	 loss: 16.5154, MinusLogProbMetric: 16.5154, val_loss: 16.9945, val_MinusLogProbMetric: 16.9945

Epoch 459: val_loss did not improve from 16.97543
196/196 - 65s - loss: 16.5154 - MinusLogProbMetric: 16.5154 - val_loss: 16.9945 - val_MinusLogProbMetric: 16.9945 - lr: 4.1667e-05 - 65s/epoch - 333ms/step
Epoch 460/1000
2023-09-26 23:57:34.888 
Epoch 460/1000 
	 loss: 16.5027, MinusLogProbMetric: 16.5027, val_loss: 16.9720, val_MinusLogProbMetric: 16.9720

Epoch 460: val_loss improved from 16.97543 to 16.97199, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_294/weights/best_weights.h5
196/196 - 66s - loss: 16.5027 - MinusLogProbMetric: 16.5027 - val_loss: 16.9720 - val_MinusLogProbMetric: 16.9720 - lr: 4.1667e-05 - 66s/epoch - 337ms/step
Epoch 461/1000
2023-09-26 23:58:41.868 
Epoch 461/1000 
	 loss: 16.5044, MinusLogProbMetric: 16.5044, val_loss: 16.9956, val_MinusLogProbMetric: 16.9956

Epoch 461: val_loss did not improve from 16.97199
196/196 - 66s - loss: 16.5044 - MinusLogProbMetric: 16.5044 - val_loss: 16.9956 - val_MinusLogProbMetric: 16.9956 - lr: 4.1667e-05 - 66s/epoch - 337ms/step
Epoch 462/1000
2023-09-26 23:59:48.380 
Epoch 462/1000 
	 loss: 16.5010, MinusLogProbMetric: 16.5010, val_loss: 17.0057, val_MinusLogProbMetric: 17.0057

Epoch 462: val_loss did not improve from 16.97199
196/196 - 67s - loss: 16.5010 - MinusLogProbMetric: 16.5010 - val_loss: 17.0057 - val_MinusLogProbMetric: 17.0057 - lr: 4.1667e-05 - 67s/epoch - 339ms/step
Epoch 463/1000
2023-09-27 00:00:53.867 
Epoch 463/1000 
	 loss: 16.5031, MinusLogProbMetric: 16.5031, val_loss: 16.9998, val_MinusLogProbMetric: 16.9998

Epoch 463: val_loss did not improve from 16.97199
196/196 - 65s - loss: 16.5031 - MinusLogProbMetric: 16.5031 - val_loss: 16.9998 - val_MinusLogProbMetric: 16.9998 - lr: 4.1667e-05 - 65s/epoch - 334ms/step
Epoch 464/1000
2023-09-27 00:01:58.180 
Epoch 464/1000 
	 loss: 16.5014, MinusLogProbMetric: 16.5014, val_loss: 16.9984, val_MinusLogProbMetric: 16.9984

Epoch 464: val_loss did not improve from 16.97199
196/196 - 64s - loss: 16.5014 - MinusLogProbMetric: 16.5014 - val_loss: 16.9984 - val_MinusLogProbMetric: 16.9984 - lr: 4.1667e-05 - 64s/epoch - 328ms/step
Epoch 465/1000
2023-09-27 00:03:03.528 
Epoch 465/1000 
	 loss: 16.5042, MinusLogProbMetric: 16.5042, val_loss: 16.9985, val_MinusLogProbMetric: 16.9985

Epoch 465: val_loss did not improve from 16.97199
196/196 - 65s - loss: 16.5042 - MinusLogProbMetric: 16.5042 - val_loss: 16.9985 - val_MinusLogProbMetric: 16.9985 - lr: 4.1667e-05 - 65s/epoch - 333ms/step
Epoch 466/1000
2023-09-27 00:04:10.092 
Epoch 466/1000 
	 loss: 16.5054, MinusLogProbMetric: 16.5054, val_loss: 17.0002, val_MinusLogProbMetric: 17.0002

Epoch 466: val_loss did not improve from 16.97199
196/196 - 67s - loss: 16.5054 - MinusLogProbMetric: 16.5054 - val_loss: 17.0002 - val_MinusLogProbMetric: 17.0002 - lr: 4.1667e-05 - 67s/epoch - 340ms/step
Epoch 467/1000
2023-09-27 00:05:16.056 
Epoch 467/1000 
	 loss: 16.5100, MinusLogProbMetric: 16.5100, val_loss: 16.9958, val_MinusLogProbMetric: 16.9958

Epoch 467: val_loss did not improve from 16.97199
196/196 - 66s - loss: 16.5100 - MinusLogProbMetric: 16.5100 - val_loss: 16.9958 - val_MinusLogProbMetric: 16.9958 - lr: 4.1667e-05 - 66s/epoch - 337ms/step
Epoch 468/1000
2023-09-27 00:06:22.454 
Epoch 468/1000 
	 loss: 16.5230, MinusLogProbMetric: 16.5230, val_loss: 17.0010, val_MinusLogProbMetric: 17.0010

Epoch 468: val_loss did not improve from 16.97199
196/196 - 66s - loss: 16.5230 - MinusLogProbMetric: 16.5230 - val_loss: 17.0010 - val_MinusLogProbMetric: 17.0010 - lr: 4.1667e-05 - 66s/epoch - 339ms/step
Epoch 469/1000
2023-09-27 00:07:27.837 
Epoch 469/1000 
	 loss: 16.4999, MinusLogProbMetric: 16.4999, val_loss: 17.0303, val_MinusLogProbMetric: 17.0303

Epoch 469: val_loss did not improve from 16.97199
196/196 - 65s - loss: 16.4999 - MinusLogProbMetric: 16.4999 - val_loss: 17.0303 - val_MinusLogProbMetric: 17.0303 - lr: 4.1667e-05 - 65s/epoch - 334ms/step
Epoch 470/1000
2023-09-27 00:08:34.289 
Epoch 470/1000 
	 loss: 16.5026, MinusLogProbMetric: 16.5026, val_loss: 16.9891, val_MinusLogProbMetric: 16.9891

Epoch 470: val_loss did not improve from 16.97199
196/196 - 66s - loss: 16.5026 - MinusLogProbMetric: 16.5026 - val_loss: 16.9891 - val_MinusLogProbMetric: 16.9891 - lr: 4.1667e-05 - 66s/epoch - 339ms/step
Epoch 471/1000
2023-09-27 00:09:39.854 
Epoch 471/1000 
	 loss: 16.5004, MinusLogProbMetric: 16.5004, val_loss: 17.0067, val_MinusLogProbMetric: 17.0067

Epoch 471: val_loss did not improve from 16.97199
196/196 - 66s - loss: 16.5004 - MinusLogProbMetric: 16.5004 - val_loss: 17.0067 - val_MinusLogProbMetric: 17.0067 - lr: 4.1667e-05 - 66s/epoch - 334ms/step
Epoch 472/1000
2023-09-27 00:10:44.785 
Epoch 472/1000 
	 loss: 16.4974, MinusLogProbMetric: 16.4974, val_loss: 16.9985, val_MinusLogProbMetric: 16.9985

Epoch 472: val_loss did not improve from 16.97199
196/196 - 65s - loss: 16.4974 - MinusLogProbMetric: 16.4974 - val_loss: 16.9985 - val_MinusLogProbMetric: 16.9985 - lr: 4.1667e-05 - 65s/epoch - 331ms/step
Epoch 473/1000
2023-09-27 00:11:49.218 
Epoch 473/1000 
	 loss: 16.5027, MinusLogProbMetric: 16.5027, val_loss: 17.0078, val_MinusLogProbMetric: 17.0078

Epoch 473: val_loss did not improve from 16.97199
196/196 - 64s - loss: 16.5027 - MinusLogProbMetric: 16.5027 - val_loss: 17.0078 - val_MinusLogProbMetric: 17.0078 - lr: 4.1667e-05 - 64s/epoch - 329ms/step
Epoch 474/1000
2023-09-27 00:12:54.125 
Epoch 474/1000 
	 loss: 16.5006, MinusLogProbMetric: 16.5006, val_loss: 16.9899, val_MinusLogProbMetric: 16.9899

Epoch 474: val_loss did not improve from 16.97199
196/196 - 65s - loss: 16.5006 - MinusLogProbMetric: 16.5006 - val_loss: 16.9899 - val_MinusLogProbMetric: 16.9899 - lr: 4.1667e-05 - 65s/epoch - 331ms/step
Epoch 475/1000
2023-09-27 00:13:59.134 
Epoch 475/1000 
	 loss: 16.5223, MinusLogProbMetric: 16.5223, val_loss: 16.9896, val_MinusLogProbMetric: 16.9896

Epoch 475: val_loss did not improve from 16.97199
196/196 - 65s - loss: 16.5223 - MinusLogProbMetric: 16.5223 - val_loss: 16.9896 - val_MinusLogProbMetric: 16.9896 - lr: 4.1667e-05 - 65s/epoch - 332ms/step
Epoch 476/1000
2023-09-27 00:15:04.850 
Epoch 476/1000 
	 loss: 16.5018, MinusLogProbMetric: 16.5018, val_loss: 17.0445, val_MinusLogProbMetric: 17.0445

Epoch 476: val_loss did not improve from 16.97199
196/196 - 66s - loss: 16.5018 - MinusLogProbMetric: 16.5018 - val_loss: 17.0445 - val_MinusLogProbMetric: 17.0445 - lr: 4.1667e-05 - 66s/epoch - 335ms/step
Epoch 477/1000
2023-09-27 00:16:09.333 
Epoch 477/1000 
	 loss: 16.4996, MinusLogProbMetric: 16.4996, val_loss: 16.9811, val_MinusLogProbMetric: 16.9811

Epoch 477: val_loss did not improve from 16.97199
196/196 - 64s - loss: 16.4996 - MinusLogProbMetric: 16.4996 - val_loss: 16.9811 - val_MinusLogProbMetric: 16.9811 - lr: 4.1667e-05 - 64s/epoch - 329ms/step
Epoch 478/1000
2023-09-27 00:17:14.418 
Epoch 478/1000 
	 loss: 16.4984, MinusLogProbMetric: 16.4984, val_loss: 16.9891, val_MinusLogProbMetric: 16.9891

Epoch 478: val_loss did not improve from 16.97199
196/196 - 65s - loss: 16.4984 - MinusLogProbMetric: 16.4984 - val_loss: 16.9891 - val_MinusLogProbMetric: 16.9891 - lr: 4.1667e-05 - 65s/epoch - 332ms/step
Epoch 479/1000
2023-09-27 00:18:20.221 
Epoch 479/1000 
	 loss: 16.4939, MinusLogProbMetric: 16.4939, val_loss: 16.9951, val_MinusLogProbMetric: 16.9951

Epoch 479: val_loss did not improve from 16.97199
196/196 - 66s - loss: 16.4939 - MinusLogProbMetric: 16.4939 - val_loss: 16.9951 - val_MinusLogProbMetric: 16.9951 - lr: 4.1667e-05 - 66s/epoch - 336ms/step
Epoch 480/1000
2023-09-27 00:19:25.466 
Epoch 480/1000 
	 loss: 16.5013, MinusLogProbMetric: 16.5013, val_loss: 16.9858, val_MinusLogProbMetric: 16.9858

Epoch 480: val_loss did not improve from 16.97199
196/196 - 65s - loss: 16.5013 - MinusLogProbMetric: 16.5013 - val_loss: 16.9858 - val_MinusLogProbMetric: 16.9858 - lr: 4.1667e-05 - 65s/epoch - 333ms/step
Epoch 481/1000
2023-09-27 00:20:30.667 
Epoch 481/1000 
	 loss: 16.4994, MinusLogProbMetric: 16.4994, val_loss: 17.0105, val_MinusLogProbMetric: 17.0105

Epoch 481: val_loss did not improve from 16.97199
196/196 - 65s - loss: 16.4994 - MinusLogProbMetric: 16.4994 - val_loss: 17.0105 - val_MinusLogProbMetric: 17.0105 - lr: 4.1667e-05 - 65s/epoch - 333ms/step
Epoch 482/1000
2023-09-27 00:21:35.547 
Epoch 482/1000 
	 loss: 16.5002, MinusLogProbMetric: 16.5002, val_loss: 17.1850, val_MinusLogProbMetric: 17.1850

Epoch 482: val_loss did not improve from 16.97199
196/196 - 65s - loss: 16.5002 - MinusLogProbMetric: 16.5002 - val_loss: 17.1850 - val_MinusLogProbMetric: 17.1850 - lr: 4.1667e-05 - 65s/epoch - 331ms/step
Epoch 483/1000
2023-09-27 00:22:40.607 
Epoch 483/1000 
	 loss: 16.5117, MinusLogProbMetric: 16.5117, val_loss: 17.0028, val_MinusLogProbMetric: 17.0028

Epoch 483: val_loss did not improve from 16.97199
196/196 - 65s - loss: 16.5117 - MinusLogProbMetric: 16.5117 - val_loss: 17.0028 - val_MinusLogProbMetric: 17.0028 - lr: 4.1667e-05 - 65s/epoch - 332ms/step
Epoch 484/1000
2023-09-27 00:23:45.166 
Epoch 484/1000 
	 loss: 16.5092, MinusLogProbMetric: 16.5092, val_loss: 17.0030, val_MinusLogProbMetric: 17.0030

Epoch 484: val_loss did not improve from 16.97199
196/196 - 65s - loss: 16.5092 - MinusLogProbMetric: 16.5092 - val_loss: 17.0030 - val_MinusLogProbMetric: 17.0030 - lr: 4.1667e-05 - 65s/epoch - 329ms/step
Epoch 485/1000
2023-09-27 00:24:51.548 
Epoch 485/1000 
	 loss: 16.4961, MinusLogProbMetric: 16.4961, val_loss: 17.0012, val_MinusLogProbMetric: 17.0012

Epoch 485: val_loss did not improve from 16.97199
196/196 - 66s - loss: 16.4961 - MinusLogProbMetric: 16.4961 - val_loss: 17.0012 - val_MinusLogProbMetric: 17.0012 - lr: 4.1667e-05 - 66s/epoch - 339ms/step
Epoch 486/1000
2023-09-27 00:25:55.453 
Epoch 486/1000 
	 loss: 16.5059, MinusLogProbMetric: 16.5059, val_loss: 17.0021, val_MinusLogProbMetric: 17.0021

Epoch 486: val_loss did not improve from 16.97199
196/196 - 64s - loss: 16.5059 - MinusLogProbMetric: 16.5059 - val_loss: 17.0021 - val_MinusLogProbMetric: 17.0021 - lr: 4.1667e-05 - 64s/epoch - 326ms/step
Epoch 487/1000
2023-09-27 00:27:06.852 
Epoch 487/1000 
	 loss: 16.5047, MinusLogProbMetric: 16.5047, val_loss: 16.9999, val_MinusLogProbMetric: 16.9999

Epoch 487: val_loss did not improve from 16.97199
196/196 - 71s - loss: 16.5047 - MinusLogProbMetric: 16.5047 - val_loss: 16.9999 - val_MinusLogProbMetric: 16.9999 - lr: 4.1667e-05 - 71s/epoch - 364ms/step
Epoch 488/1000
2023-09-27 00:28:17.576 
Epoch 488/1000 
	 loss: 16.4975, MinusLogProbMetric: 16.4975, val_loss: 17.0115, val_MinusLogProbMetric: 17.0115

Epoch 488: val_loss did not improve from 16.97199
196/196 - 71s - loss: 16.4975 - MinusLogProbMetric: 16.4975 - val_loss: 17.0115 - val_MinusLogProbMetric: 17.0115 - lr: 4.1667e-05 - 71s/epoch - 361ms/step
Epoch 489/1000
2023-09-27 00:29:28.281 
Epoch 489/1000 
	 loss: 16.4906, MinusLogProbMetric: 16.4906, val_loss: 17.0419, val_MinusLogProbMetric: 17.0419

Epoch 489: val_loss did not improve from 16.97199
196/196 - 71s - loss: 16.4906 - MinusLogProbMetric: 16.4906 - val_loss: 17.0419 - val_MinusLogProbMetric: 17.0419 - lr: 4.1667e-05 - 71s/epoch - 361ms/step
Epoch 490/1000
2023-09-27 00:30:39.259 
Epoch 490/1000 
	 loss: 16.5009, MinusLogProbMetric: 16.5009, val_loss: 16.9783, val_MinusLogProbMetric: 16.9783

Epoch 490: val_loss did not improve from 16.97199
196/196 - 71s - loss: 16.5009 - MinusLogProbMetric: 16.5009 - val_loss: 16.9783 - val_MinusLogProbMetric: 16.9783 - lr: 4.1667e-05 - 71s/epoch - 362ms/step
Epoch 491/1000
2023-09-27 00:31:50.207 
Epoch 491/1000 
	 loss: 16.4996, MinusLogProbMetric: 16.4996, val_loss: 17.0157, val_MinusLogProbMetric: 17.0157

Epoch 491: val_loss did not improve from 16.97199
196/196 - 71s - loss: 16.4996 - MinusLogProbMetric: 16.4996 - val_loss: 17.0157 - val_MinusLogProbMetric: 17.0157 - lr: 4.1667e-05 - 71s/epoch - 362ms/step
Epoch 492/1000
2023-09-27 00:33:00.816 
Epoch 492/1000 
	 loss: 16.5006, MinusLogProbMetric: 16.5006, val_loss: 16.9805, val_MinusLogProbMetric: 16.9805

Epoch 492: val_loss did not improve from 16.97199
196/196 - 71s - loss: 16.5006 - MinusLogProbMetric: 16.5006 - val_loss: 16.9805 - val_MinusLogProbMetric: 16.9805 - lr: 4.1667e-05 - 71s/epoch - 360ms/step
Epoch 493/1000
2023-09-27 00:34:11.586 
Epoch 493/1000 
	 loss: 16.5004, MinusLogProbMetric: 16.5004, val_loss: 17.0281, val_MinusLogProbMetric: 17.0281

Epoch 493: val_loss did not improve from 16.97199
196/196 - 71s - loss: 16.5004 - MinusLogProbMetric: 16.5004 - val_loss: 17.0281 - val_MinusLogProbMetric: 17.0281 - lr: 4.1667e-05 - 71s/epoch - 361ms/step
Epoch 494/1000
2023-09-27 00:35:22.428 
Epoch 494/1000 
	 loss: 16.4949, MinusLogProbMetric: 16.4949, val_loss: 17.0216, val_MinusLogProbMetric: 17.0216

Epoch 494: val_loss did not improve from 16.97199
196/196 - 71s - loss: 16.4949 - MinusLogProbMetric: 16.4949 - val_loss: 17.0216 - val_MinusLogProbMetric: 17.0216 - lr: 4.1667e-05 - 71s/epoch - 361ms/step
Epoch 495/1000
2023-09-27 00:36:33.625 
Epoch 495/1000 
	 loss: 16.5010, MinusLogProbMetric: 16.5010, val_loss: 16.9823, val_MinusLogProbMetric: 16.9823

Epoch 495: val_loss did not improve from 16.97199
196/196 - 71s - loss: 16.5010 - MinusLogProbMetric: 16.5010 - val_loss: 16.9823 - val_MinusLogProbMetric: 16.9823 - lr: 4.1667e-05 - 71s/epoch - 363ms/step
Epoch 496/1000
2023-09-27 00:37:44.307 
Epoch 496/1000 
	 loss: 16.5005, MinusLogProbMetric: 16.5005, val_loss: 17.0224, val_MinusLogProbMetric: 17.0224

Epoch 496: val_loss did not improve from 16.97199
196/196 - 71s - loss: 16.5005 - MinusLogProbMetric: 16.5005 - val_loss: 17.0224 - val_MinusLogProbMetric: 17.0224 - lr: 4.1667e-05 - 71s/epoch - 361ms/step
Epoch 497/1000
2023-09-27 00:38:54.791 
Epoch 497/1000 
	 loss: 16.4912, MinusLogProbMetric: 16.4912, val_loss: 17.0000, val_MinusLogProbMetric: 17.0000

Epoch 497: val_loss did not improve from 16.97199
196/196 - 70s - loss: 16.4912 - MinusLogProbMetric: 16.4912 - val_loss: 17.0000 - val_MinusLogProbMetric: 17.0000 - lr: 4.1667e-05 - 70s/epoch - 360ms/step
Epoch 498/1000
2023-09-27 00:40:05.589 
Epoch 498/1000 
	 loss: 16.4913, MinusLogProbMetric: 16.4913, val_loss: 16.9934, val_MinusLogProbMetric: 16.9934

Epoch 498: val_loss did not improve from 16.97199
196/196 - 71s - loss: 16.4913 - MinusLogProbMetric: 16.4913 - val_loss: 16.9934 - val_MinusLogProbMetric: 16.9934 - lr: 4.1667e-05 - 71s/epoch - 361ms/step
Epoch 499/1000
2023-09-27 00:41:15.426 
Epoch 499/1000 
	 loss: 16.4927, MinusLogProbMetric: 16.4927, val_loss: 17.0354, val_MinusLogProbMetric: 17.0354

Epoch 499: val_loss did not improve from 16.97199
196/196 - 70s - loss: 16.4927 - MinusLogProbMetric: 16.4927 - val_loss: 17.0354 - val_MinusLogProbMetric: 17.0354 - lr: 4.1667e-05 - 70s/epoch - 356ms/step
Epoch 500/1000
2023-09-27 00:42:26.151 
Epoch 500/1000 
	 loss: 16.4984, MinusLogProbMetric: 16.4984, val_loss: 16.9951, val_MinusLogProbMetric: 16.9951

Epoch 500: val_loss did not improve from 16.97199
196/196 - 71s - loss: 16.4984 - MinusLogProbMetric: 16.4984 - val_loss: 16.9951 - val_MinusLogProbMetric: 16.9951 - lr: 4.1667e-05 - 71s/epoch - 361ms/step
Epoch 501/1000
2023-09-27 00:43:37.238 
Epoch 501/1000 
	 loss: 16.5021, MinusLogProbMetric: 16.5021, val_loss: 17.0158, val_MinusLogProbMetric: 17.0158

Epoch 501: val_loss did not improve from 16.97199
196/196 - 71s - loss: 16.5021 - MinusLogProbMetric: 16.5021 - val_loss: 17.0158 - val_MinusLogProbMetric: 17.0158 - lr: 4.1667e-05 - 71s/epoch - 363ms/step
Epoch 502/1000
2023-09-27 00:44:48.539 
Epoch 502/1000 
	 loss: 16.4890, MinusLogProbMetric: 16.4890, val_loss: 17.0677, val_MinusLogProbMetric: 17.0677

Epoch 502: val_loss did not improve from 16.97199
196/196 - 71s - loss: 16.4890 - MinusLogProbMetric: 16.4890 - val_loss: 17.0677 - val_MinusLogProbMetric: 17.0677 - lr: 4.1667e-05 - 71s/epoch - 364ms/step
Epoch 503/1000
2023-09-27 00:45:59.560 
Epoch 503/1000 
	 loss: 16.5147, MinusLogProbMetric: 16.5147, val_loss: 17.0164, val_MinusLogProbMetric: 17.0164

Epoch 503: val_loss did not improve from 16.97199
196/196 - 71s - loss: 16.5147 - MinusLogProbMetric: 16.5147 - val_loss: 17.0164 - val_MinusLogProbMetric: 17.0164 - lr: 4.1667e-05 - 71s/epoch - 362ms/step
Epoch 504/1000
2023-09-27 00:47:10.333 
Epoch 504/1000 
	 loss: 16.4993, MinusLogProbMetric: 16.4993, val_loss: 16.9807, val_MinusLogProbMetric: 16.9807

Epoch 504: val_loss did not improve from 16.97199
196/196 - 71s - loss: 16.4993 - MinusLogProbMetric: 16.4993 - val_loss: 16.9807 - val_MinusLogProbMetric: 16.9807 - lr: 4.1667e-05 - 71s/epoch - 361ms/step
Epoch 505/1000
2023-09-27 00:48:21.092 
Epoch 505/1000 
	 loss: 16.4966, MinusLogProbMetric: 16.4966, val_loss: 16.9956, val_MinusLogProbMetric: 16.9956

Epoch 505: val_loss did not improve from 16.97199
196/196 - 71s - loss: 16.4966 - MinusLogProbMetric: 16.4966 - val_loss: 16.9956 - val_MinusLogProbMetric: 16.9956 - lr: 4.1667e-05 - 71s/epoch - 361ms/step
Epoch 506/1000
2023-09-27 00:49:31.654 
Epoch 506/1000 
	 loss: 16.4965, MinusLogProbMetric: 16.4965, val_loss: 16.9872, val_MinusLogProbMetric: 16.9872

Epoch 506: val_loss did not improve from 16.97199
196/196 - 71s - loss: 16.4965 - MinusLogProbMetric: 16.4965 - val_loss: 16.9872 - val_MinusLogProbMetric: 16.9872 - lr: 4.1667e-05 - 71s/epoch - 360ms/step
Epoch 507/1000
2023-09-27 00:50:42.645 
Epoch 507/1000 
	 loss: 16.4957, MinusLogProbMetric: 16.4957, val_loss: 16.9775, val_MinusLogProbMetric: 16.9775

Epoch 507: val_loss did not improve from 16.97199
196/196 - 71s - loss: 16.4957 - MinusLogProbMetric: 16.4957 - val_loss: 16.9775 - val_MinusLogProbMetric: 16.9775 - lr: 4.1667e-05 - 71s/epoch - 362ms/step
Epoch 508/1000
2023-09-27 00:51:53.180 
Epoch 508/1000 
	 loss: 16.5017, MinusLogProbMetric: 16.5017, val_loss: 17.0368, val_MinusLogProbMetric: 17.0368

Epoch 508: val_loss did not improve from 16.97199
196/196 - 71s - loss: 16.5017 - MinusLogProbMetric: 16.5017 - val_loss: 17.0368 - val_MinusLogProbMetric: 17.0368 - lr: 4.1667e-05 - 71s/epoch - 360ms/step
Epoch 509/1000
2023-09-27 00:53:01.871 
Epoch 509/1000 
	 loss: 16.4981, MinusLogProbMetric: 16.4981, val_loss: 16.9687, val_MinusLogProbMetric: 16.9687

Epoch 509: val_loss improved from 16.97199 to 16.96868, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_294/weights/best_weights.h5
196/196 - 70s - loss: 16.4981 - MinusLogProbMetric: 16.4981 - val_loss: 16.9687 - val_MinusLogProbMetric: 16.9687 - lr: 4.1667e-05 - 70s/epoch - 356ms/step
Epoch 510/1000
2023-09-27 00:54:10.782 
Epoch 510/1000 
	 loss: 16.4967, MinusLogProbMetric: 16.4967, val_loss: 17.0187, val_MinusLogProbMetric: 17.0187

Epoch 510: val_loss did not improve from 16.96868
196/196 - 68s - loss: 16.4967 - MinusLogProbMetric: 16.4967 - val_loss: 17.0187 - val_MinusLogProbMetric: 17.0187 - lr: 4.1667e-05 - 68s/epoch - 346ms/step
Epoch 511/1000
2023-09-27 00:55:17.670 
Epoch 511/1000 
	 loss: 16.4926, MinusLogProbMetric: 16.4926, val_loss: 17.0511, val_MinusLogProbMetric: 17.0511

Epoch 511: val_loss did not improve from 16.96868
196/196 - 67s - loss: 16.4926 - MinusLogProbMetric: 16.4926 - val_loss: 17.0511 - val_MinusLogProbMetric: 17.0511 - lr: 4.1667e-05 - 67s/epoch - 341ms/step
Epoch 512/1000
2023-09-27 00:56:23.348 
Epoch 512/1000 
	 loss: 16.5030, MinusLogProbMetric: 16.5030, val_loss: 16.9867, val_MinusLogProbMetric: 16.9867

Epoch 512: val_loss did not improve from 16.96868
196/196 - 66s - loss: 16.5030 - MinusLogProbMetric: 16.5030 - val_loss: 16.9867 - val_MinusLogProbMetric: 16.9867 - lr: 4.1667e-05 - 66s/epoch - 335ms/step
Epoch 513/1000
2023-09-27 00:57:28.105 
Epoch 513/1000 
	 loss: 16.4940, MinusLogProbMetric: 16.4940, val_loss: 17.0223, val_MinusLogProbMetric: 17.0223

Epoch 513: val_loss did not improve from 16.96868
196/196 - 65s - loss: 16.4940 - MinusLogProbMetric: 16.4940 - val_loss: 17.0223 - val_MinusLogProbMetric: 17.0223 - lr: 4.1667e-05 - 65s/epoch - 330ms/step
Epoch 514/1000
2023-09-27 00:58:31.973 
Epoch 514/1000 
	 loss: 16.4992, MinusLogProbMetric: 16.4992, val_loss: 16.9863, val_MinusLogProbMetric: 16.9863

Epoch 514: val_loss did not improve from 16.96868
196/196 - 64s - loss: 16.4992 - MinusLogProbMetric: 16.4992 - val_loss: 16.9863 - val_MinusLogProbMetric: 16.9863 - lr: 4.1667e-05 - 64s/epoch - 326ms/step
Epoch 515/1000
2023-09-27 00:59:36.772 
Epoch 515/1000 
	 loss: 16.4944, MinusLogProbMetric: 16.4944, val_loss: 17.0194, val_MinusLogProbMetric: 17.0194

Epoch 515: val_loss did not improve from 16.96868
196/196 - 65s - loss: 16.4944 - MinusLogProbMetric: 16.4944 - val_loss: 17.0194 - val_MinusLogProbMetric: 17.0194 - lr: 4.1667e-05 - 65s/epoch - 331ms/step
Epoch 516/1000
2023-09-27 01:00:40.926 
Epoch 516/1000 
	 loss: 16.5001, MinusLogProbMetric: 16.5001, val_loss: 17.0065, val_MinusLogProbMetric: 17.0065

Epoch 516: val_loss did not improve from 16.96868
196/196 - 64s - loss: 16.5001 - MinusLogProbMetric: 16.5001 - val_loss: 17.0065 - val_MinusLogProbMetric: 17.0065 - lr: 4.1667e-05 - 64s/epoch - 327ms/step
Epoch 517/1000
2023-09-27 01:01:46.538 
Epoch 517/1000 
	 loss: 16.4978, MinusLogProbMetric: 16.4978, val_loss: 17.0385, val_MinusLogProbMetric: 17.0385

Epoch 517: val_loss did not improve from 16.96868
196/196 - 66s - loss: 16.4978 - MinusLogProbMetric: 16.4978 - val_loss: 17.0385 - val_MinusLogProbMetric: 17.0385 - lr: 4.1667e-05 - 66s/epoch - 335ms/step
Epoch 518/1000
2023-09-27 01:02:51.588 
Epoch 518/1000 
	 loss: 16.4900, MinusLogProbMetric: 16.4900, val_loss: 17.0023, val_MinusLogProbMetric: 17.0023

Epoch 518: val_loss did not improve from 16.96868
196/196 - 65s - loss: 16.4900 - MinusLogProbMetric: 16.4900 - val_loss: 17.0023 - val_MinusLogProbMetric: 17.0023 - lr: 4.1667e-05 - 65s/epoch - 332ms/step
Epoch 519/1000
2023-09-27 01:03:56.064 
Epoch 519/1000 
	 loss: 16.5019, MinusLogProbMetric: 16.5019, val_loss: 17.0519, val_MinusLogProbMetric: 17.0519

Epoch 519: val_loss did not improve from 16.96868
196/196 - 64s - loss: 16.5019 - MinusLogProbMetric: 16.5019 - val_loss: 17.0519 - val_MinusLogProbMetric: 17.0519 - lr: 4.1667e-05 - 64s/epoch - 329ms/step
Epoch 520/1000
2023-09-27 01:04:59.123 
Epoch 520/1000 
	 loss: 16.5030, MinusLogProbMetric: 16.5030, val_loss: 17.0218, val_MinusLogProbMetric: 17.0218

Epoch 520: val_loss did not improve from 16.96868
196/196 - 63s - loss: 16.5030 - MinusLogProbMetric: 16.5030 - val_loss: 17.0218 - val_MinusLogProbMetric: 17.0218 - lr: 4.1667e-05 - 63s/epoch - 322ms/step
Epoch 521/1000
2023-09-27 01:06:02.750 
Epoch 521/1000 
	 loss: 16.4943, MinusLogProbMetric: 16.4943, val_loss: 16.9779, val_MinusLogProbMetric: 16.9779

Epoch 521: val_loss did not improve from 16.96868
196/196 - 64s - loss: 16.4943 - MinusLogProbMetric: 16.4943 - val_loss: 16.9779 - val_MinusLogProbMetric: 16.9779 - lr: 4.1667e-05 - 64s/epoch - 325ms/step
Epoch 522/1000
2023-09-27 01:07:06.596 
Epoch 522/1000 
	 loss: 16.4864, MinusLogProbMetric: 16.4864, val_loss: 16.9748, val_MinusLogProbMetric: 16.9748

Epoch 522: val_loss did not improve from 16.96868
196/196 - 64s - loss: 16.4864 - MinusLogProbMetric: 16.4864 - val_loss: 16.9748 - val_MinusLogProbMetric: 16.9748 - lr: 4.1667e-05 - 64s/epoch - 326ms/step
Epoch 523/1000
2023-09-27 01:08:10.510 
Epoch 523/1000 
	 loss: 16.5005, MinusLogProbMetric: 16.5005, val_loss: 17.0446, val_MinusLogProbMetric: 17.0446

Epoch 523: val_loss did not improve from 16.96868
196/196 - 64s - loss: 16.5005 - MinusLogProbMetric: 16.5005 - val_loss: 17.0446 - val_MinusLogProbMetric: 17.0446 - lr: 4.1667e-05 - 64s/epoch - 326ms/step
Epoch 524/1000
2023-09-27 01:09:14.785 
Epoch 524/1000 
	 loss: 16.4928, MinusLogProbMetric: 16.4928, val_loss: 16.9888, val_MinusLogProbMetric: 16.9888

Epoch 524: val_loss did not improve from 16.96868
196/196 - 64s - loss: 16.4928 - MinusLogProbMetric: 16.4928 - val_loss: 16.9888 - val_MinusLogProbMetric: 16.9888 - lr: 4.1667e-05 - 64s/epoch - 328ms/step
Epoch 525/1000
2023-09-27 01:10:19.135 
Epoch 525/1000 
	 loss: 16.4854, MinusLogProbMetric: 16.4854, val_loss: 17.0009, val_MinusLogProbMetric: 17.0009

Epoch 525: val_loss did not improve from 16.96868
196/196 - 64s - loss: 16.4854 - MinusLogProbMetric: 16.4854 - val_loss: 17.0009 - val_MinusLogProbMetric: 17.0009 - lr: 4.1667e-05 - 64s/epoch - 328ms/step
Epoch 526/1000
2023-09-27 01:11:23.506 
Epoch 526/1000 
	 loss: 16.4958, MinusLogProbMetric: 16.4958, val_loss: 17.0061, val_MinusLogProbMetric: 17.0061

Epoch 526: val_loss did not improve from 16.96868
196/196 - 64s - loss: 16.4958 - MinusLogProbMetric: 16.4958 - val_loss: 17.0061 - val_MinusLogProbMetric: 17.0061 - lr: 4.1667e-05 - 64s/epoch - 328ms/step
Epoch 527/1000
2023-09-27 01:12:26.219 
Epoch 527/1000 
	 loss: 16.4958, MinusLogProbMetric: 16.4958, val_loss: 17.0544, val_MinusLogProbMetric: 17.0544

Epoch 527: val_loss did not improve from 16.96868
196/196 - 63s - loss: 16.4958 - MinusLogProbMetric: 16.4958 - val_loss: 17.0544 - val_MinusLogProbMetric: 17.0544 - lr: 4.1667e-05 - 63s/epoch - 320ms/step
Epoch 528/1000
2023-09-27 01:13:28.951 
Epoch 528/1000 
	 loss: 16.4991, MinusLogProbMetric: 16.4991, val_loss: 17.0936, val_MinusLogProbMetric: 17.0936

Epoch 528: val_loss did not improve from 16.96868
196/196 - 63s - loss: 16.4991 - MinusLogProbMetric: 16.4991 - val_loss: 17.0936 - val_MinusLogProbMetric: 17.0936 - lr: 4.1667e-05 - 63s/epoch - 320ms/step
Epoch 529/1000
2023-09-27 01:14:32.647 
Epoch 529/1000 
	 loss: 16.4961, MinusLogProbMetric: 16.4961, val_loss: 16.9922, val_MinusLogProbMetric: 16.9922

Epoch 529: val_loss did not improve from 16.96868
196/196 - 64s - loss: 16.4961 - MinusLogProbMetric: 16.4961 - val_loss: 16.9922 - val_MinusLogProbMetric: 16.9922 - lr: 4.1667e-05 - 64s/epoch - 325ms/step
Epoch 530/1000
2023-09-27 01:15:35.899 
Epoch 530/1000 
	 loss: 16.4906, MinusLogProbMetric: 16.4906, val_loss: 16.9790, val_MinusLogProbMetric: 16.9790

Epoch 530: val_loss did not improve from 16.96868
196/196 - 63s - loss: 16.4906 - MinusLogProbMetric: 16.4906 - val_loss: 16.9790 - val_MinusLogProbMetric: 16.9790 - lr: 4.1667e-05 - 63s/epoch - 323ms/step
Epoch 531/1000
2023-09-27 01:16:39.109 
Epoch 531/1000 
	 loss: 16.4883, MinusLogProbMetric: 16.4883, val_loss: 17.0184, val_MinusLogProbMetric: 17.0184

Epoch 531: val_loss did not improve from 16.96868
196/196 - 63s - loss: 16.4883 - MinusLogProbMetric: 16.4883 - val_loss: 17.0184 - val_MinusLogProbMetric: 17.0184 - lr: 4.1667e-05 - 63s/epoch - 322ms/step
Epoch 532/1000
2023-09-27 01:17:42.425 
Epoch 532/1000 
	 loss: 16.4957, MinusLogProbMetric: 16.4957, val_loss: 16.9977, val_MinusLogProbMetric: 16.9977

Epoch 532: val_loss did not improve from 16.96868
196/196 - 63s - loss: 16.4957 - MinusLogProbMetric: 16.4957 - val_loss: 16.9977 - val_MinusLogProbMetric: 16.9977 - lr: 4.1667e-05 - 63s/epoch - 323ms/step
Epoch 533/1000
2023-09-27 01:18:45.643 
Epoch 533/1000 
	 loss: 16.4938, MinusLogProbMetric: 16.4938, val_loss: 17.0101, val_MinusLogProbMetric: 17.0101

Epoch 533: val_loss did not improve from 16.96868
196/196 - 63s - loss: 16.4938 - MinusLogProbMetric: 16.4938 - val_loss: 17.0101 - val_MinusLogProbMetric: 17.0101 - lr: 4.1667e-05 - 63s/epoch - 323ms/step
Epoch 534/1000
2023-09-27 01:19:47.802 
Epoch 534/1000 
	 loss: 16.4915, MinusLogProbMetric: 16.4915, val_loss: 17.0355, val_MinusLogProbMetric: 17.0355

Epoch 534: val_loss did not improve from 16.96868
196/196 - 62s - loss: 16.4915 - MinusLogProbMetric: 16.4915 - val_loss: 17.0355 - val_MinusLogProbMetric: 17.0355 - lr: 4.1667e-05 - 62s/epoch - 317ms/step
Epoch 535/1000
2023-09-27 01:20:50.122 
Epoch 535/1000 
	 loss: 16.4985, MinusLogProbMetric: 16.4985, val_loss: 17.0006, val_MinusLogProbMetric: 17.0006

Epoch 535: val_loss did not improve from 16.96868
196/196 - 62s - loss: 16.4985 - MinusLogProbMetric: 16.4985 - val_loss: 17.0006 - val_MinusLogProbMetric: 17.0006 - lr: 4.1667e-05 - 62s/epoch - 318ms/step
Epoch 536/1000
2023-09-27 01:21:52.979 
Epoch 536/1000 
	 loss: 16.4837, MinusLogProbMetric: 16.4837, val_loss: 16.9811, val_MinusLogProbMetric: 16.9811

Epoch 536: val_loss did not improve from 16.96868
196/196 - 63s - loss: 16.4837 - MinusLogProbMetric: 16.4837 - val_loss: 16.9811 - val_MinusLogProbMetric: 16.9811 - lr: 4.1667e-05 - 63s/epoch - 321ms/step
Epoch 537/1000
2023-09-27 01:22:56.448 
Epoch 537/1000 
	 loss: 16.4933, MinusLogProbMetric: 16.4933, val_loss: 16.9803, val_MinusLogProbMetric: 16.9803

Epoch 537: val_loss did not improve from 16.96868
196/196 - 63s - loss: 16.4933 - MinusLogProbMetric: 16.4933 - val_loss: 16.9803 - val_MinusLogProbMetric: 16.9803 - lr: 4.1667e-05 - 63s/epoch - 324ms/step
Epoch 538/1000
2023-09-27 01:23:58.515 
Epoch 538/1000 
	 loss: 16.4984, MinusLogProbMetric: 16.4984, val_loss: 16.9843, val_MinusLogProbMetric: 16.9843

Epoch 538: val_loss did not improve from 16.96868
196/196 - 62s - loss: 16.4984 - MinusLogProbMetric: 16.4984 - val_loss: 16.9843 - val_MinusLogProbMetric: 16.9843 - lr: 4.1667e-05 - 62s/epoch - 317ms/step
Epoch 539/1000
2023-09-27 01:25:00.259 
Epoch 539/1000 
	 loss: 16.4797, MinusLogProbMetric: 16.4797, val_loss: 16.9884, val_MinusLogProbMetric: 16.9884

Epoch 539: val_loss did not improve from 16.96868
196/196 - 62s - loss: 16.4797 - MinusLogProbMetric: 16.4797 - val_loss: 16.9884 - val_MinusLogProbMetric: 16.9884 - lr: 4.1667e-05 - 62s/epoch - 315ms/step
Epoch 540/1000
2023-09-27 01:26:02.254 
Epoch 540/1000 
	 loss: 16.4921, MinusLogProbMetric: 16.4921, val_loss: 17.0496, val_MinusLogProbMetric: 17.0496

Epoch 540: val_loss did not improve from 16.96868
196/196 - 62s - loss: 16.4921 - MinusLogProbMetric: 16.4921 - val_loss: 17.0496 - val_MinusLogProbMetric: 17.0496 - lr: 4.1667e-05 - 62s/epoch - 316ms/step
Epoch 541/1000
2023-09-27 01:27:04.618 
Epoch 541/1000 
	 loss: 16.4979, MinusLogProbMetric: 16.4979, val_loss: 17.0124, val_MinusLogProbMetric: 17.0124

Epoch 541: val_loss did not improve from 16.96868
196/196 - 62s - loss: 16.4979 - MinusLogProbMetric: 16.4979 - val_loss: 17.0124 - val_MinusLogProbMetric: 17.0124 - lr: 4.1667e-05 - 62s/epoch - 318ms/step
Epoch 542/1000
2023-09-27 01:28:07.629 
Epoch 542/1000 
	 loss: 16.4868, MinusLogProbMetric: 16.4868, val_loss: 16.9919, val_MinusLogProbMetric: 16.9919

Epoch 542: val_loss did not improve from 16.96868
196/196 - 63s - loss: 16.4868 - MinusLogProbMetric: 16.4868 - val_loss: 16.9919 - val_MinusLogProbMetric: 16.9919 - lr: 4.1667e-05 - 63s/epoch - 321ms/step
Epoch 543/1000
2023-09-27 01:29:11.335 
Epoch 543/1000 
	 loss: 16.4843, MinusLogProbMetric: 16.4843, val_loss: 17.0364, val_MinusLogProbMetric: 17.0364

Epoch 543: val_loss did not improve from 16.96868
196/196 - 64s - loss: 16.4843 - MinusLogProbMetric: 16.4843 - val_loss: 17.0364 - val_MinusLogProbMetric: 17.0364 - lr: 4.1667e-05 - 64s/epoch - 325ms/step
Epoch 544/1000
2023-09-27 01:30:14.613 
Epoch 544/1000 
	 loss: 16.4959, MinusLogProbMetric: 16.4959, val_loss: 16.9952, val_MinusLogProbMetric: 16.9952

Epoch 544: val_loss did not improve from 16.96868
196/196 - 63s - loss: 16.4959 - MinusLogProbMetric: 16.4959 - val_loss: 16.9952 - val_MinusLogProbMetric: 16.9952 - lr: 4.1667e-05 - 63s/epoch - 323ms/step
Epoch 545/1000
2023-09-27 01:31:17.190 
Epoch 545/1000 
	 loss: 16.4847, MinusLogProbMetric: 16.4847, val_loss: 16.9741, val_MinusLogProbMetric: 16.9741

Epoch 545: val_loss did not improve from 16.96868
196/196 - 63s - loss: 16.4847 - MinusLogProbMetric: 16.4847 - val_loss: 16.9741 - val_MinusLogProbMetric: 16.9741 - lr: 4.1667e-05 - 63s/epoch - 319ms/step
Epoch 546/1000
2023-09-27 01:32:19.767 
Epoch 546/1000 
	 loss: 16.4951, MinusLogProbMetric: 16.4951, val_loss: 16.9835, val_MinusLogProbMetric: 16.9835

Epoch 546: val_loss did not improve from 16.96868
196/196 - 63s - loss: 16.4951 - MinusLogProbMetric: 16.4951 - val_loss: 16.9835 - val_MinusLogProbMetric: 16.9835 - lr: 4.1667e-05 - 63s/epoch - 319ms/step
Epoch 547/1000
2023-09-27 01:33:22.486 
Epoch 547/1000 
	 loss: 16.4986, MinusLogProbMetric: 16.4986, val_loss: 17.0362, val_MinusLogProbMetric: 17.0362

Epoch 547: val_loss did not improve from 16.96868
196/196 - 63s - loss: 16.4986 - MinusLogProbMetric: 16.4986 - val_loss: 17.0362 - val_MinusLogProbMetric: 17.0362 - lr: 4.1667e-05 - 63s/epoch - 320ms/step
Epoch 548/1000
2023-09-27 01:34:24.227 
Epoch 548/1000 
	 loss: 16.5011, MinusLogProbMetric: 16.5011, val_loss: 16.9703, val_MinusLogProbMetric: 16.9703

Epoch 548: val_loss did not improve from 16.96868
196/196 - 62s - loss: 16.5011 - MinusLogProbMetric: 16.5011 - val_loss: 16.9703 - val_MinusLogProbMetric: 16.9703 - lr: 4.1667e-05 - 62s/epoch - 315ms/step
Epoch 549/1000
2023-09-27 01:35:26.302 
Epoch 549/1000 
	 loss: 16.4887, MinusLogProbMetric: 16.4887, val_loss: 16.9812, val_MinusLogProbMetric: 16.9812

Epoch 549: val_loss did not improve from 16.96868
196/196 - 62s - loss: 16.4887 - MinusLogProbMetric: 16.4887 - val_loss: 16.9812 - val_MinusLogProbMetric: 16.9812 - lr: 4.1667e-05 - 62s/epoch - 317ms/step
Epoch 550/1000
2023-09-27 01:36:28.166 
Epoch 550/1000 
	 loss: 16.4979, MinusLogProbMetric: 16.4979, val_loss: 17.0596, val_MinusLogProbMetric: 17.0596

Epoch 550: val_loss did not improve from 16.96868
196/196 - 62s - loss: 16.4979 - MinusLogProbMetric: 16.4979 - val_loss: 17.0596 - val_MinusLogProbMetric: 17.0596 - lr: 4.1667e-05 - 62s/epoch - 316ms/step
Epoch 551/1000
2023-09-27 01:37:30.328 
Epoch 551/1000 
	 loss: 16.5008, MinusLogProbMetric: 16.5008, val_loss: 16.9864, val_MinusLogProbMetric: 16.9864

Epoch 551: val_loss did not improve from 16.96868
196/196 - 62s - loss: 16.5008 - MinusLogProbMetric: 16.5008 - val_loss: 16.9864 - val_MinusLogProbMetric: 16.9864 - lr: 4.1667e-05 - 62s/epoch - 317ms/step
Epoch 552/1000
2023-09-27 01:38:33.308 
Epoch 552/1000 
	 loss: 16.4770, MinusLogProbMetric: 16.4770, val_loss: 17.0005, val_MinusLogProbMetric: 17.0005

Epoch 552: val_loss did not improve from 16.96868
196/196 - 63s - loss: 16.4770 - MinusLogProbMetric: 16.4770 - val_loss: 17.0005 - val_MinusLogProbMetric: 17.0005 - lr: 4.1667e-05 - 63s/epoch - 321ms/step
Epoch 553/1000
2023-09-27 01:39:36.164 
Epoch 553/1000 
	 loss: 16.4847, MinusLogProbMetric: 16.4847, val_loss: 16.9973, val_MinusLogProbMetric: 16.9973

Epoch 553: val_loss did not improve from 16.96868
196/196 - 63s - loss: 16.4847 - MinusLogProbMetric: 16.4847 - val_loss: 16.9973 - val_MinusLogProbMetric: 16.9973 - lr: 4.1667e-05 - 63s/epoch - 321ms/step
Epoch 554/1000
2023-09-27 01:40:40.134 
Epoch 554/1000 
	 loss: 16.4983, MinusLogProbMetric: 16.4983, val_loss: 17.0107, val_MinusLogProbMetric: 17.0107

Epoch 554: val_loss did not improve from 16.96868
196/196 - 64s - loss: 16.4983 - MinusLogProbMetric: 16.4983 - val_loss: 17.0107 - val_MinusLogProbMetric: 17.0107 - lr: 4.1667e-05 - 64s/epoch - 326ms/step
Epoch 555/1000
2023-09-27 01:41:42.034 
Epoch 555/1000 
	 loss: 16.4857, MinusLogProbMetric: 16.4857, val_loss: 16.9835, val_MinusLogProbMetric: 16.9835

Epoch 555: val_loss did not improve from 16.96868
196/196 - 62s - loss: 16.4857 - MinusLogProbMetric: 16.4857 - val_loss: 16.9835 - val_MinusLogProbMetric: 16.9835 - lr: 4.1667e-05 - 62s/epoch - 316ms/step
Epoch 556/1000
2023-09-27 01:42:44.812 
Epoch 556/1000 
	 loss: 16.4894, MinusLogProbMetric: 16.4894, val_loss: 16.9778, val_MinusLogProbMetric: 16.9778

Epoch 556: val_loss did not improve from 16.96868
196/196 - 63s - loss: 16.4894 - MinusLogProbMetric: 16.4894 - val_loss: 16.9778 - val_MinusLogProbMetric: 16.9778 - lr: 4.1667e-05 - 63s/epoch - 320ms/step
Epoch 557/1000
2023-09-27 01:43:47.364 
Epoch 557/1000 
	 loss: 16.4836, MinusLogProbMetric: 16.4836, val_loss: 16.9731, val_MinusLogProbMetric: 16.9731

Epoch 557: val_loss did not improve from 16.96868
196/196 - 63s - loss: 16.4836 - MinusLogProbMetric: 16.4836 - val_loss: 16.9731 - val_MinusLogProbMetric: 16.9731 - lr: 4.1667e-05 - 63s/epoch - 319ms/step
Epoch 558/1000
2023-09-27 01:44:49.735 
Epoch 558/1000 
	 loss: 16.5040, MinusLogProbMetric: 16.5040, val_loss: 16.9996, val_MinusLogProbMetric: 16.9996

Epoch 558: val_loss did not improve from 16.96868
196/196 - 62s - loss: 16.5040 - MinusLogProbMetric: 16.5040 - val_loss: 16.9996 - val_MinusLogProbMetric: 16.9996 - lr: 4.1667e-05 - 62s/epoch - 318ms/step
Epoch 559/1000
2023-09-27 01:45:52.419 
Epoch 559/1000 
	 loss: 16.4854, MinusLogProbMetric: 16.4854, val_loss: 16.9802, val_MinusLogProbMetric: 16.9802

Epoch 559: val_loss did not improve from 16.96868
196/196 - 63s - loss: 16.4854 - MinusLogProbMetric: 16.4854 - val_loss: 16.9802 - val_MinusLogProbMetric: 16.9802 - lr: 4.1667e-05 - 63s/epoch - 320ms/step
Epoch 560/1000
2023-09-27 01:46:55.697 
Epoch 560/1000 
	 loss: 16.4556, MinusLogProbMetric: 16.4556, val_loss: 16.9958, val_MinusLogProbMetric: 16.9958

Epoch 560: val_loss did not improve from 16.96868
196/196 - 63s - loss: 16.4556 - MinusLogProbMetric: 16.4556 - val_loss: 16.9958 - val_MinusLogProbMetric: 16.9958 - lr: 2.0833e-05 - 63s/epoch - 323ms/step
Epoch 561/1000
2023-09-27 01:47:58.532 
Epoch 561/1000 
	 loss: 16.4582, MinusLogProbMetric: 16.4582, val_loss: 16.9667, val_MinusLogProbMetric: 16.9667

Epoch 561: val_loss improved from 16.96868 to 16.96671, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_294/weights/best_weights.h5
196/196 - 64s - loss: 16.4582 - MinusLogProbMetric: 16.4582 - val_loss: 16.9667 - val_MinusLogProbMetric: 16.9667 - lr: 2.0833e-05 - 64s/epoch - 326ms/step
Epoch 562/1000
2023-09-27 01:49:02.841 
Epoch 562/1000 
	 loss: 16.4554, MinusLogProbMetric: 16.4554, val_loss: 16.9819, val_MinusLogProbMetric: 16.9819

Epoch 562: val_loss did not improve from 16.96671
196/196 - 63s - loss: 16.4554 - MinusLogProbMetric: 16.4554 - val_loss: 16.9819 - val_MinusLogProbMetric: 16.9819 - lr: 2.0833e-05 - 63s/epoch - 322ms/step
Epoch 563/1000
2023-09-27 01:50:05.294 
Epoch 563/1000 
	 loss: 16.4535, MinusLogProbMetric: 16.4535, val_loss: 16.9668, val_MinusLogProbMetric: 16.9668

Epoch 563: val_loss did not improve from 16.96671
196/196 - 62s - loss: 16.4535 - MinusLogProbMetric: 16.4535 - val_loss: 16.9668 - val_MinusLogProbMetric: 16.9668 - lr: 2.0833e-05 - 62s/epoch - 319ms/step
Epoch 564/1000
2023-09-27 01:51:08.386 
Epoch 564/1000 
	 loss: 16.4561, MinusLogProbMetric: 16.4561, val_loss: 16.9607, val_MinusLogProbMetric: 16.9607

Epoch 564: val_loss improved from 16.96671 to 16.96074, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_294/weights/best_weights.h5
196/196 - 64s - loss: 16.4561 - MinusLogProbMetric: 16.4561 - val_loss: 16.9607 - val_MinusLogProbMetric: 16.9607 - lr: 2.0833e-05 - 64s/epoch - 328ms/step
Epoch 565/1000
2023-09-27 01:52:11.514 
Epoch 565/1000 
	 loss: 16.4530, MinusLogProbMetric: 16.4530, val_loss: 17.1182, val_MinusLogProbMetric: 17.1182

Epoch 565: val_loss did not improve from 16.96074
196/196 - 62s - loss: 16.4530 - MinusLogProbMetric: 16.4530 - val_loss: 17.1182 - val_MinusLogProbMetric: 17.1182 - lr: 2.0833e-05 - 62s/epoch - 316ms/step
Epoch 566/1000
2023-09-27 01:53:14.655 
Epoch 566/1000 
	 loss: 16.4648, MinusLogProbMetric: 16.4648, val_loss: 16.9676, val_MinusLogProbMetric: 16.9676

Epoch 566: val_loss did not improve from 16.96074
196/196 - 63s - loss: 16.4648 - MinusLogProbMetric: 16.4648 - val_loss: 16.9676 - val_MinusLogProbMetric: 16.9676 - lr: 2.0833e-05 - 63s/epoch - 322ms/step
Epoch 567/1000
2023-09-27 01:54:17.746 
Epoch 567/1000 
	 loss: 16.4587, MinusLogProbMetric: 16.4587, val_loss: 17.0001, val_MinusLogProbMetric: 17.0001

Epoch 567: val_loss did not improve from 16.96074
196/196 - 63s - loss: 16.4587 - MinusLogProbMetric: 16.4587 - val_loss: 17.0001 - val_MinusLogProbMetric: 17.0001 - lr: 2.0833e-05 - 63s/epoch - 322ms/step
Epoch 568/1000
2023-09-27 01:55:20.702 
Epoch 568/1000 
	 loss: 16.4551, MinusLogProbMetric: 16.4551, val_loss: 16.9624, val_MinusLogProbMetric: 16.9624

Epoch 568: val_loss did not improve from 16.96074
196/196 - 63s - loss: 16.4551 - MinusLogProbMetric: 16.4551 - val_loss: 16.9624 - val_MinusLogProbMetric: 16.9624 - lr: 2.0833e-05 - 63s/epoch - 321ms/step
Epoch 569/1000
2023-09-27 01:56:22.319 
Epoch 569/1000 
	 loss: 16.4539, MinusLogProbMetric: 16.4539, val_loss: 16.9618, val_MinusLogProbMetric: 16.9618

Epoch 569: val_loss did not improve from 16.96074
196/196 - 62s - loss: 16.4539 - MinusLogProbMetric: 16.4539 - val_loss: 16.9618 - val_MinusLogProbMetric: 16.9618 - lr: 2.0833e-05 - 62s/epoch - 314ms/step
Epoch 570/1000
2023-09-27 01:57:25.341 
Epoch 570/1000 
	 loss: 16.4526, MinusLogProbMetric: 16.4526, val_loss: 16.9724, val_MinusLogProbMetric: 16.9724

Epoch 570: val_loss did not improve from 16.96074
196/196 - 63s - loss: 16.4526 - MinusLogProbMetric: 16.4526 - val_loss: 16.9724 - val_MinusLogProbMetric: 16.9724 - lr: 2.0833e-05 - 63s/epoch - 322ms/step
Epoch 571/1000
2023-09-27 01:58:29.352 
Epoch 571/1000 
	 loss: 16.4538, MinusLogProbMetric: 16.4538, val_loss: 16.9886, val_MinusLogProbMetric: 16.9886

Epoch 571: val_loss did not improve from 16.96074
196/196 - 64s - loss: 16.4538 - MinusLogProbMetric: 16.4538 - val_loss: 16.9886 - val_MinusLogProbMetric: 16.9886 - lr: 2.0833e-05 - 64s/epoch - 327ms/step
Epoch 572/1000
2023-09-27 01:59:31.180 
Epoch 572/1000 
	 loss: 16.4512, MinusLogProbMetric: 16.4512, val_loss: 16.9635, val_MinusLogProbMetric: 16.9635

Epoch 572: val_loss did not improve from 16.96074
196/196 - 62s - loss: 16.4512 - MinusLogProbMetric: 16.4512 - val_loss: 16.9635 - val_MinusLogProbMetric: 16.9635 - lr: 2.0833e-05 - 62s/epoch - 315ms/step
Epoch 573/1000
2023-09-27 02:00:34.279 
Epoch 573/1000 
	 loss: 16.4504, MinusLogProbMetric: 16.4504, val_loss: 16.9702, val_MinusLogProbMetric: 16.9702

Epoch 573: val_loss did not improve from 16.96074
196/196 - 63s - loss: 16.4504 - MinusLogProbMetric: 16.4504 - val_loss: 16.9702 - val_MinusLogProbMetric: 16.9702 - lr: 2.0833e-05 - 63s/epoch - 322ms/step
Epoch 574/1000
2023-09-27 02:01:37.165 
Epoch 574/1000 
	 loss: 16.4558, MinusLogProbMetric: 16.4558, val_loss: 16.9696, val_MinusLogProbMetric: 16.9696

Epoch 574: val_loss did not improve from 16.96074
196/196 - 63s - loss: 16.4558 - MinusLogProbMetric: 16.4558 - val_loss: 16.9696 - val_MinusLogProbMetric: 16.9696 - lr: 2.0833e-05 - 63s/epoch - 321ms/step
Epoch 575/1000
2023-09-27 02:02:38.658 
Epoch 575/1000 
	 loss: 16.4530, MinusLogProbMetric: 16.4530, val_loss: 16.9628, val_MinusLogProbMetric: 16.9628

Epoch 575: val_loss did not improve from 16.96074
196/196 - 61s - loss: 16.4530 - MinusLogProbMetric: 16.4530 - val_loss: 16.9628 - val_MinusLogProbMetric: 16.9628 - lr: 2.0833e-05 - 61s/epoch - 314ms/step
Epoch 576/1000
2023-09-27 02:03:41.979 
Epoch 576/1000 
	 loss: 16.4539, MinusLogProbMetric: 16.4539, val_loss: 16.9692, val_MinusLogProbMetric: 16.9692

Epoch 576: val_loss did not improve from 16.96074
196/196 - 63s - loss: 16.4539 - MinusLogProbMetric: 16.4539 - val_loss: 16.9692 - val_MinusLogProbMetric: 16.9692 - lr: 2.0833e-05 - 63s/epoch - 323ms/step
Epoch 577/1000
2023-09-27 02:04:45.582 
Epoch 577/1000 
	 loss: 16.4551, MinusLogProbMetric: 16.4551, val_loss: 16.9758, val_MinusLogProbMetric: 16.9758

Epoch 577: val_loss did not improve from 16.96074
196/196 - 64s - loss: 16.4551 - MinusLogProbMetric: 16.4551 - val_loss: 16.9758 - val_MinusLogProbMetric: 16.9758 - lr: 2.0833e-05 - 64s/epoch - 324ms/step
Epoch 578/1000
2023-09-27 02:05:47.459 
Epoch 578/1000 
	 loss: 16.4532, MinusLogProbMetric: 16.4532, val_loss: 16.9572, val_MinusLogProbMetric: 16.9572

Epoch 578: val_loss improved from 16.96074 to 16.95719, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_294/weights/best_weights.h5
196/196 - 63s - loss: 16.4532 - MinusLogProbMetric: 16.4532 - val_loss: 16.9572 - val_MinusLogProbMetric: 16.9572 - lr: 2.0833e-05 - 63s/epoch - 323ms/step
Epoch 579/1000
2023-09-27 02:06:51.569 
Epoch 579/1000 
	 loss: 16.4542, MinusLogProbMetric: 16.4542, val_loss: 16.9957, val_MinusLogProbMetric: 16.9957

Epoch 579: val_loss did not improve from 16.95719
196/196 - 63s - loss: 16.4542 - MinusLogProbMetric: 16.4542 - val_loss: 16.9957 - val_MinusLogProbMetric: 16.9957 - lr: 2.0833e-05 - 63s/epoch - 320ms/step
Epoch 580/1000
2023-09-27 02:07:55.164 
Epoch 580/1000 
	 loss: 16.4523, MinusLogProbMetric: 16.4523, val_loss: 16.9715, val_MinusLogProbMetric: 16.9715

Epoch 580: val_loss did not improve from 16.95719
196/196 - 64s - loss: 16.4523 - MinusLogProbMetric: 16.4523 - val_loss: 16.9715 - val_MinusLogProbMetric: 16.9715 - lr: 2.0833e-05 - 64s/epoch - 324ms/step
Epoch 581/1000
2023-09-27 02:08:57.185 
Epoch 581/1000 
	 loss: 16.4519, MinusLogProbMetric: 16.4519, val_loss: 16.9551, val_MinusLogProbMetric: 16.9551

Epoch 581: val_loss improved from 16.95719 to 16.95506, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_294/weights/best_weights.h5
196/196 - 63s - loss: 16.4519 - MinusLogProbMetric: 16.4519 - val_loss: 16.9551 - val_MinusLogProbMetric: 16.9551 - lr: 2.0833e-05 - 63s/epoch - 321ms/step
Epoch 582/1000
2023-09-27 02:10:00.437 
Epoch 582/1000 
	 loss: 16.4537, MinusLogProbMetric: 16.4537, val_loss: 16.9550, val_MinusLogProbMetric: 16.9550

Epoch 582: val_loss improved from 16.95506 to 16.95505, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_294/weights/best_weights.h5
196/196 - 63s - loss: 16.4537 - MinusLogProbMetric: 16.4537 - val_loss: 16.9550 - val_MinusLogProbMetric: 16.9550 - lr: 2.0833e-05 - 63s/epoch - 323ms/step
Epoch 583/1000
2023-09-27 02:11:03.533 
Epoch 583/1000 
	 loss: 16.4510, MinusLogProbMetric: 16.4510, val_loss: 16.9670, val_MinusLogProbMetric: 16.9670

Epoch 583: val_loss did not improve from 16.95505
196/196 - 62s - loss: 16.4510 - MinusLogProbMetric: 16.4510 - val_loss: 16.9670 - val_MinusLogProbMetric: 16.9670 - lr: 2.0833e-05 - 62s/epoch - 317ms/step
Epoch 584/1000
2023-09-27 02:12:06.528 
Epoch 584/1000 
	 loss: 16.4559, MinusLogProbMetric: 16.4559, val_loss: 16.9639, val_MinusLogProbMetric: 16.9639

Epoch 584: val_loss did not improve from 16.95505
196/196 - 63s - loss: 16.4559 - MinusLogProbMetric: 16.4559 - val_loss: 16.9639 - val_MinusLogProbMetric: 16.9639 - lr: 2.0833e-05 - 63s/epoch - 321ms/step
Epoch 585/1000
2023-09-27 02:13:09.677 
Epoch 585/1000 
	 loss: 16.4497, MinusLogProbMetric: 16.4497, val_loss: 16.9813, val_MinusLogProbMetric: 16.9813

Epoch 585: val_loss did not improve from 16.95505
196/196 - 63s - loss: 16.4497 - MinusLogProbMetric: 16.4497 - val_loss: 16.9813 - val_MinusLogProbMetric: 16.9813 - lr: 2.0833e-05 - 63s/epoch - 322ms/step
Epoch 586/1000
2023-09-27 02:14:12.862 
Epoch 586/1000 
	 loss: 16.4519, MinusLogProbMetric: 16.4519, val_loss: 16.9743, val_MinusLogProbMetric: 16.9743

Epoch 586: val_loss did not improve from 16.95505
196/196 - 63s - loss: 16.4519 - MinusLogProbMetric: 16.4519 - val_loss: 16.9743 - val_MinusLogProbMetric: 16.9743 - lr: 2.0833e-05 - 63s/epoch - 322ms/step
Epoch 587/1000
2023-09-27 02:15:15.617 
Epoch 587/1000 
	 loss: 16.4605, MinusLogProbMetric: 16.4605, val_loss: 16.9675, val_MinusLogProbMetric: 16.9675

Epoch 587: val_loss did not improve from 16.95505
196/196 - 63s - loss: 16.4605 - MinusLogProbMetric: 16.4605 - val_loss: 16.9675 - val_MinusLogProbMetric: 16.9675 - lr: 2.0833e-05 - 63s/epoch - 320ms/step
Epoch 588/1000
2023-09-27 02:16:17.756 
Epoch 588/1000 
	 loss: 16.4486, MinusLogProbMetric: 16.4486, val_loss: 16.9710, val_MinusLogProbMetric: 16.9710

Epoch 588: val_loss did not improve from 16.95505
196/196 - 62s - loss: 16.4486 - MinusLogProbMetric: 16.4486 - val_loss: 16.9710 - val_MinusLogProbMetric: 16.9710 - lr: 2.0833e-05 - 62s/epoch - 317ms/step
Epoch 589/1000
2023-09-27 02:17:19.009 
Epoch 589/1000 
	 loss: 16.4509, MinusLogProbMetric: 16.4509, val_loss: 16.9716, val_MinusLogProbMetric: 16.9716

Epoch 589: val_loss did not improve from 16.95505
196/196 - 61s - loss: 16.4509 - MinusLogProbMetric: 16.4509 - val_loss: 16.9716 - val_MinusLogProbMetric: 16.9716 - lr: 2.0833e-05 - 61s/epoch - 312ms/step
Epoch 590/1000
2023-09-27 02:18:21.585 
Epoch 590/1000 
	 loss: 16.4487, MinusLogProbMetric: 16.4487, val_loss: 16.9655, val_MinusLogProbMetric: 16.9655

Epoch 590: val_loss did not improve from 16.95505
196/196 - 63s - loss: 16.4487 - MinusLogProbMetric: 16.4487 - val_loss: 16.9655 - val_MinusLogProbMetric: 16.9655 - lr: 2.0833e-05 - 63s/epoch - 319ms/step
Epoch 591/1000
2023-09-27 02:19:25.186 
Epoch 591/1000 
	 loss: 16.4546, MinusLogProbMetric: 16.4546, val_loss: 16.9671, val_MinusLogProbMetric: 16.9671

Epoch 591: val_loss did not improve from 16.95505
196/196 - 64s - loss: 16.4546 - MinusLogProbMetric: 16.4546 - val_loss: 16.9671 - val_MinusLogProbMetric: 16.9671 - lr: 2.0833e-05 - 64s/epoch - 324ms/step
Epoch 592/1000
2023-09-27 02:20:28.274 
Epoch 592/1000 
	 loss: 16.4493, MinusLogProbMetric: 16.4493, val_loss: 16.9797, val_MinusLogProbMetric: 16.9797

Epoch 592: val_loss did not improve from 16.95505
196/196 - 63s - loss: 16.4493 - MinusLogProbMetric: 16.4493 - val_loss: 16.9797 - val_MinusLogProbMetric: 16.9797 - lr: 2.0833e-05 - 63s/epoch - 322ms/step
Epoch 593/1000
2023-09-27 02:21:31.163 
Epoch 593/1000 
	 loss: 16.4511, MinusLogProbMetric: 16.4511, val_loss: 16.9630, val_MinusLogProbMetric: 16.9630

Epoch 593: val_loss did not improve from 16.95505
196/196 - 63s - loss: 16.4511 - MinusLogProbMetric: 16.4511 - val_loss: 16.9630 - val_MinusLogProbMetric: 16.9630 - lr: 2.0833e-05 - 63s/epoch - 321ms/step
Epoch 594/1000
2023-09-27 02:22:34.157 
Epoch 594/1000 
	 loss: 16.4549, MinusLogProbMetric: 16.4549, val_loss: 16.9646, val_MinusLogProbMetric: 16.9646

Epoch 594: val_loss did not improve from 16.95505
196/196 - 63s - loss: 16.4549 - MinusLogProbMetric: 16.4549 - val_loss: 16.9646 - val_MinusLogProbMetric: 16.9646 - lr: 2.0833e-05 - 63s/epoch - 321ms/step
Epoch 595/1000
2023-09-27 02:23:36.265 
Epoch 595/1000 
	 loss: 16.4526, MinusLogProbMetric: 16.4526, val_loss: 16.9663, val_MinusLogProbMetric: 16.9663

Epoch 595: val_loss did not improve from 16.95505
196/196 - 62s - loss: 16.4526 - MinusLogProbMetric: 16.4526 - val_loss: 16.9663 - val_MinusLogProbMetric: 16.9663 - lr: 2.0833e-05 - 62s/epoch - 317ms/step
Epoch 596/1000
2023-09-27 02:24:39.516 
Epoch 596/1000 
	 loss: 16.4498, MinusLogProbMetric: 16.4498, val_loss: 16.9707, val_MinusLogProbMetric: 16.9707

Epoch 596: val_loss did not improve from 16.95505
196/196 - 63s - loss: 16.4498 - MinusLogProbMetric: 16.4498 - val_loss: 16.9707 - val_MinusLogProbMetric: 16.9707 - lr: 2.0833e-05 - 63s/epoch - 323ms/step
Epoch 597/1000
2023-09-27 02:25:41.715 
Epoch 597/1000 
	 loss: 16.4542, MinusLogProbMetric: 16.4542, val_loss: 16.9725, val_MinusLogProbMetric: 16.9725

Epoch 597: val_loss did not improve from 16.95505
196/196 - 62s - loss: 16.4542 - MinusLogProbMetric: 16.4542 - val_loss: 16.9725 - val_MinusLogProbMetric: 16.9725 - lr: 2.0833e-05 - 62s/epoch - 317ms/step
Epoch 598/1000
2023-09-27 02:26:44.714 
Epoch 598/1000 
	 loss: 16.4469, MinusLogProbMetric: 16.4469, val_loss: 17.0534, val_MinusLogProbMetric: 17.0534

Epoch 598: val_loss did not improve from 16.95505
196/196 - 63s - loss: 16.4469 - MinusLogProbMetric: 16.4469 - val_loss: 17.0534 - val_MinusLogProbMetric: 17.0534 - lr: 2.0833e-05 - 63s/epoch - 321ms/step
Epoch 599/1000
2023-09-27 02:27:46.690 
Epoch 599/1000 
	 loss: 16.4496, MinusLogProbMetric: 16.4496, val_loss: 16.9559, val_MinusLogProbMetric: 16.9559

Epoch 599: val_loss did not improve from 16.95505
196/196 - 62s - loss: 16.4496 - MinusLogProbMetric: 16.4496 - val_loss: 16.9559 - val_MinusLogProbMetric: 16.9559 - lr: 2.0833e-05 - 62s/epoch - 316ms/step
Epoch 600/1000
2023-09-27 02:28:49.689 
Epoch 600/1000 
	 loss: 16.4488, MinusLogProbMetric: 16.4488, val_loss: 16.9599, val_MinusLogProbMetric: 16.9599

Epoch 600: val_loss did not improve from 16.95505
196/196 - 63s - loss: 16.4488 - MinusLogProbMetric: 16.4488 - val_loss: 16.9599 - val_MinusLogProbMetric: 16.9599 - lr: 2.0833e-05 - 63s/epoch - 321ms/step
Epoch 601/1000
2023-09-27 02:29:52.125 
Epoch 601/1000 
	 loss: 16.4505, MinusLogProbMetric: 16.4505, val_loss: 16.9703, val_MinusLogProbMetric: 16.9703

Epoch 601: val_loss did not improve from 16.95505
196/196 - 62s - loss: 16.4505 - MinusLogProbMetric: 16.4505 - val_loss: 16.9703 - val_MinusLogProbMetric: 16.9703 - lr: 2.0833e-05 - 62s/epoch - 319ms/step
Epoch 602/1000
2023-09-27 02:30:53.753 
Epoch 602/1000 
	 loss: 16.4502, MinusLogProbMetric: 16.4502, val_loss: 16.9599, val_MinusLogProbMetric: 16.9599

Epoch 602: val_loss did not improve from 16.95505
196/196 - 62s - loss: 16.4502 - MinusLogProbMetric: 16.4502 - val_loss: 16.9599 - val_MinusLogProbMetric: 16.9599 - lr: 2.0833e-05 - 62s/epoch - 314ms/step
Epoch 603/1000
2023-09-27 02:31:49.225 
Epoch 603/1000 
	 loss: 16.4462, MinusLogProbMetric: 16.4462, val_loss: 16.9607, val_MinusLogProbMetric: 16.9607

Epoch 603: val_loss did not improve from 16.95505
196/196 - 55s - loss: 16.4462 - MinusLogProbMetric: 16.4462 - val_loss: 16.9607 - val_MinusLogProbMetric: 16.9607 - lr: 2.0833e-05 - 55s/epoch - 283ms/step
Epoch 604/1000
2023-09-27 02:32:42.242 
Epoch 604/1000 
	 loss: 16.4472, MinusLogProbMetric: 16.4472, val_loss: 16.9648, val_MinusLogProbMetric: 16.9648

Epoch 604: val_loss did not improve from 16.95505
196/196 - 53s - loss: 16.4472 - MinusLogProbMetric: 16.4472 - val_loss: 16.9648 - val_MinusLogProbMetric: 16.9648 - lr: 2.0833e-05 - 53s/epoch - 270ms/step
Epoch 605/1000
2023-09-27 02:33:43.648 
Epoch 605/1000 
	 loss: 16.4502, MinusLogProbMetric: 16.4502, val_loss: 16.9650, val_MinusLogProbMetric: 16.9650

Epoch 605: val_loss did not improve from 16.95505
196/196 - 61s - loss: 16.4502 - MinusLogProbMetric: 16.4502 - val_loss: 16.9650 - val_MinusLogProbMetric: 16.9650 - lr: 2.0833e-05 - 61s/epoch - 313ms/step
Epoch 606/1000
2023-09-27 02:34:47.559 
Epoch 606/1000 
	 loss: 16.4511, MinusLogProbMetric: 16.4511, val_loss: 17.0080, val_MinusLogProbMetric: 17.0080

Epoch 606: val_loss did not improve from 16.95505
196/196 - 64s - loss: 16.4511 - MinusLogProbMetric: 16.4511 - val_loss: 17.0080 - val_MinusLogProbMetric: 17.0080 - lr: 2.0833e-05 - 64s/epoch - 326ms/step
Epoch 607/1000
2023-09-27 02:35:50.710 
Epoch 607/1000 
	 loss: 16.4554, MinusLogProbMetric: 16.4554, val_loss: 16.9882, val_MinusLogProbMetric: 16.9882

Epoch 607: val_loss did not improve from 16.95505
196/196 - 63s - loss: 16.4554 - MinusLogProbMetric: 16.4554 - val_loss: 16.9882 - val_MinusLogProbMetric: 16.9882 - lr: 2.0833e-05 - 63s/epoch - 322ms/step
Epoch 608/1000
2023-09-27 02:36:54.047 
Epoch 608/1000 
	 loss: 16.4521, MinusLogProbMetric: 16.4521, val_loss: 16.9671, val_MinusLogProbMetric: 16.9671

Epoch 608: val_loss did not improve from 16.95505
196/196 - 63s - loss: 16.4521 - MinusLogProbMetric: 16.4521 - val_loss: 16.9671 - val_MinusLogProbMetric: 16.9671 - lr: 2.0833e-05 - 63s/epoch - 323ms/step
Epoch 609/1000
2023-09-27 02:37:57.251 
Epoch 609/1000 
	 loss: 16.4508, MinusLogProbMetric: 16.4508, val_loss: 16.9672, val_MinusLogProbMetric: 16.9672

Epoch 609: val_loss did not improve from 16.95505
196/196 - 63s - loss: 16.4508 - MinusLogProbMetric: 16.4508 - val_loss: 16.9672 - val_MinusLogProbMetric: 16.9672 - lr: 2.0833e-05 - 63s/epoch - 322ms/step
Epoch 610/1000
2023-09-27 02:38:59.190 
Epoch 610/1000 
	 loss: 16.4515, MinusLogProbMetric: 16.4515, val_loss: 16.9661, val_MinusLogProbMetric: 16.9661

Epoch 610: val_loss did not improve from 16.95505
196/196 - 62s - loss: 16.4515 - MinusLogProbMetric: 16.4515 - val_loss: 16.9661 - val_MinusLogProbMetric: 16.9661 - lr: 2.0833e-05 - 62s/epoch - 316ms/step
Epoch 611/1000
2023-09-27 02:40:01.281 
Epoch 611/1000 
	 loss: 16.4533, MinusLogProbMetric: 16.4533, val_loss: 16.9607, val_MinusLogProbMetric: 16.9607

Epoch 611: val_loss did not improve from 16.95505
196/196 - 62s - loss: 16.4533 - MinusLogProbMetric: 16.4533 - val_loss: 16.9607 - val_MinusLogProbMetric: 16.9607 - lr: 2.0833e-05 - 62s/epoch - 317ms/step
Epoch 612/1000
2023-09-27 02:41:04.160 
Epoch 612/1000 
	 loss: 16.4543, MinusLogProbMetric: 16.4543, val_loss: 16.9685, val_MinusLogProbMetric: 16.9685

Epoch 612: val_loss did not improve from 16.95505
196/196 - 63s - loss: 16.4543 - MinusLogProbMetric: 16.4543 - val_loss: 16.9685 - val_MinusLogProbMetric: 16.9685 - lr: 2.0833e-05 - 63s/epoch - 321ms/step
Epoch 613/1000
2023-09-27 02:42:07.946 
Epoch 613/1000 
	 loss: 16.4487, MinusLogProbMetric: 16.4487, val_loss: 16.9623, val_MinusLogProbMetric: 16.9623

Epoch 613: val_loss did not improve from 16.95505
196/196 - 64s - loss: 16.4487 - MinusLogProbMetric: 16.4487 - val_loss: 16.9623 - val_MinusLogProbMetric: 16.9623 - lr: 2.0833e-05 - 64s/epoch - 325ms/step
Epoch 614/1000
2023-09-27 02:43:11.512 
Epoch 614/1000 
	 loss: 16.4511, MinusLogProbMetric: 16.4511, val_loss: 17.0469, val_MinusLogProbMetric: 17.0469

Epoch 614: val_loss did not improve from 16.95505
196/196 - 64s - loss: 16.4511 - MinusLogProbMetric: 16.4511 - val_loss: 17.0469 - val_MinusLogProbMetric: 17.0469 - lr: 2.0833e-05 - 64s/epoch - 324ms/step
Epoch 615/1000
2023-09-27 02:44:14.838 
Epoch 615/1000 
	 loss: 16.4551, MinusLogProbMetric: 16.4551, val_loss: 16.9588, val_MinusLogProbMetric: 16.9588

Epoch 615: val_loss did not improve from 16.95505
196/196 - 63s - loss: 16.4551 - MinusLogProbMetric: 16.4551 - val_loss: 16.9588 - val_MinusLogProbMetric: 16.9588 - lr: 2.0833e-05 - 63s/epoch - 323ms/step
Epoch 616/1000
2023-09-27 02:45:17.631 
Epoch 616/1000 
	 loss: 16.4478, MinusLogProbMetric: 16.4478, val_loss: 16.9670, val_MinusLogProbMetric: 16.9670

Epoch 616: val_loss did not improve from 16.95505
196/196 - 63s - loss: 16.4478 - MinusLogProbMetric: 16.4478 - val_loss: 16.9670 - val_MinusLogProbMetric: 16.9670 - lr: 2.0833e-05 - 63s/epoch - 320ms/step
Epoch 617/1000
2023-09-27 02:46:20.465 
Epoch 617/1000 
	 loss: 16.4510, MinusLogProbMetric: 16.4510, val_loss: 16.9717, val_MinusLogProbMetric: 16.9717

Epoch 617: val_loss did not improve from 16.95505
196/196 - 63s - loss: 16.4510 - MinusLogProbMetric: 16.4510 - val_loss: 16.9717 - val_MinusLogProbMetric: 16.9717 - lr: 2.0833e-05 - 63s/epoch - 321ms/step
Epoch 618/1000
2023-09-27 02:47:22.552 
Epoch 618/1000 
	 loss: 16.4510, MinusLogProbMetric: 16.4510, val_loss: 16.9733, val_MinusLogProbMetric: 16.9733

Epoch 618: val_loss did not improve from 16.95505
196/196 - 62s - loss: 16.4510 - MinusLogProbMetric: 16.4510 - val_loss: 16.9733 - val_MinusLogProbMetric: 16.9733 - lr: 2.0833e-05 - 62s/epoch - 317ms/step
Epoch 619/1000
2023-09-27 02:48:24.519 
Epoch 619/1000 
	 loss: 16.4489, MinusLogProbMetric: 16.4489, val_loss: 16.9616, val_MinusLogProbMetric: 16.9616

Epoch 619: val_loss did not improve from 16.95505
196/196 - 62s - loss: 16.4489 - MinusLogProbMetric: 16.4489 - val_loss: 16.9616 - val_MinusLogProbMetric: 16.9616 - lr: 2.0833e-05 - 62s/epoch - 316ms/step
Epoch 620/1000
2023-09-27 02:49:28.129 
Epoch 620/1000 
	 loss: 16.4536, MinusLogProbMetric: 16.4536, val_loss: 16.9657, val_MinusLogProbMetric: 16.9657

Epoch 620: val_loss did not improve from 16.95505
196/196 - 64s - loss: 16.4536 - MinusLogProbMetric: 16.4536 - val_loss: 16.9657 - val_MinusLogProbMetric: 16.9657 - lr: 2.0833e-05 - 64s/epoch - 325ms/step
Epoch 621/1000
2023-09-27 02:50:31.938 
Epoch 621/1000 
	 loss: 16.4493, MinusLogProbMetric: 16.4493, val_loss: 16.9622, val_MinusLogProbMetric: 16.9622

Epoch 621: val_loss did not improve from 16.95505
196/196 - 64s - loss: 16.4493 - MinusLogProbMetric: 16.4493 - val_loss: 16.9622 - val_MinusLogProbMetric: 16.9622 - lr: 2.0833e-05 - 64s/epoch - 326ms/step
Epoch 622/1000
2023-09-27 02:51:35.088 
Epoch 622/1000 
	 loss: 16.4499, MinusLogProbMetric: 16.4499, val_loss: 16.9702, val_MinusLogProbMetric: 16.9702

Epoch 622: val_loss did not improve from 16.95505
196/196 - 63s - loss: 16.4499 - MinusLogProbMetric: 16.4499 - val_loss: 16.9702 - val_MinusLogProbMetric: 16.9702 - lr: 2.0833e-05 - 63s/epoch - 322ms/step
Epoch 623/1000
2023-09-27 02:52:38.110 
Epoch 623/1000 
	 loss: 16.4486, MinusLogProbMetric: 16.4486, val_loss: 16.9573, val_MinusLogProbMetric: 16.9573

Epoch 623: val_loss did not improve from 16.95505
196/196 - 63s - loss: 16.4486 - MinusLogProbMetric: 16.4486 - val_loss: 16.9573 - val_MinusLogProbMetric: 16.9573 - lr: 2.0833e-05 - 63s/epoch - 322ms/step
Epoch 624/1000
2023-09-27 02:53:40.826 
Epoch 624/1000 
	 loss: 16.4490, MinusLogProbMetric: 16.4490, val_loss: 16.9642, val_MinusLogProbMetric: 16.9642

Epoch 624: val_loss did not improve from 16.95505
196/196 - 63s - loss: 16.4490 - MinusLogProbMetric: 16.4490 - val_loss: 16.9642 - val_MinusLogProbMetric: 16.9642 - lr: 2.0833e-05 - 63s/epoch - 320ms/step
Epoch 625/1000
2023-09-27 02:54:43.054 
Epoch 625/1000 
	 loss: 16.4494, MinusLogProbMetric: 16.4494, val_loss: 16.9802, val_MinusLogProbMetric: 16.9802

Epoch 625: val_loss did not improve from 16.95505
196/196 - 62s - loss: 16.4494 - MinusLogProbMetric: 16.4494 - val_loss: 16.9802 - val_MinusLogProbMetric: 16.9802 - lr: 2.0833e-05 - 62s/epoch - 317ms/step
Epoch 626/1000
2023-09-27 02:55:46.596 
Epoch 626/1000 
	 loss: 16.4494, MinusLogProbMetric: 16.4494, val_loss: 16.9834, val_MinusLogProbMetric: 16.9834

Epoch 626: val_loss did not improve from 16.95505
196/196 - 64s - loss: 16.4494 - MinusLogProbMetric: 16.4494 - val_loss: 16.9834 - val_MinusLogProbMetric: 16.9834 - lr: 2.0833e-05 - 64s/epoch - 324ms/step
Epoch 627/1000
2023-09-27 02:56:49.470 
Epoch 627/1000 
	 loss: 16.4507, MinusLogProbMetric: 16.4507, val_loss: 16.9706, val_MinusLogProbMetric: 16.9706

Epoch 627: val_loss did not improve from 16.95505
196/196 - 63s - loss: 16.4507 - MinusLogProbMetric: 16.4507 - val_loss: 16.9706 - val_MinusLogProbMetric: 16.9706 - lr: 2.0833e-05 - 63s/epoch - 321ms/step
Epoch 628/1000
2023-09-27 02:57:52.316 
Epoch 628/1000 
	 loss: 16.4470, MinusLogProbMetric: 16.4470, val_loss: 16.9691, val_MinusLogProbMetric: 16.9691

Epoch 628: val_loss did not improve from 16.95505
196/196 - 63s - loss: 16.4470 - MinusLogProbMetric: 16.4470 - val_loss: 16.9691 - val_MinusLogProbMetric: 16.9691 - lr: 2.0833e-05 - 63s/epoch - 321ms/step
Epoch 629/1000
2023-09-27 02:58:56.450 
Epoch 629/1000 
	 loss: 16.4452, MinusLogProbMetric: 16.4452, val_loss: 16.9703, val_MinusLogProbMetric: 16.9703

Epoch 629: val_loss did not improve from 16.95505
196/196 - 64s - loss: 16.4452 - MinusLogProbMetric: 16.4452 - val_loss: 16.9703 - val_MinusLogProbMetric: 16.9703 - lr: 2.0833e-05 - 64s/epoch - 327ms/step
Epoch 630/1000
2023-09-27 02:59:59.334 
Epoch 630/1000 
	 loss: 16.4435, MinusLogProbMetric: 16.4435, val_loss: 16.9603, val_MinusLogProbMetric: 16.9603

Epoch 630: val_loss did not improve from 16.95505
196/196 - 63s - loss: 16.4435 - MinusLogProbMetric: 16.4435 - val_loss: 16.9603 - val_MinusLogProbMetric: 16.9603 - lr: 2.0833e-05 - 63s/epoch - 321ms/step
Epoch 631/1000
2023-09-27 03:01:02.886 
Epoch 631/1000 
	 loss: 16.4494, MinusLogProbMetric: 16.4494, val_loss: 16.9663, val_MinusLogProbMetric: 16.9663

Epoch 631: val_loss did not improve from 16.95505
196/196 - 64s - loss: 16.4494 - MinusLogProbMetric: 16.4494 - val_loss: 16.9663 - val_MinusLogProbMetric: 16.9663 - lr: 2.0833e-05 - 64s/epoch - 324ms/step
Epoch 632/1000
2023-09-27 03:02:04.148 
Epoch 632/1000 
	 loss: 16.4306, MinusLogProbMetric: 16.4306, val_loss: 16.9582, val_MinusLogProbMetric: 16.9582

Epoch 632: val_loss did not improve from 16.95505
196/196 - 61s - loss: 16.4306 - MinusLogProbMetric: 16.4306 - val_loss: 16.9582 - val_MinusLogProbMetric: 16.9582 - lr: 1.0417e-05 - 61s/epoch - 313ms/step
Epoch 633/1000
2023-09-27 03:03:07.359 
Epoch 633/1000 
	 loss: 16.4320, MinusLogProbMetric: 16.4320, val_loss: 16.9540, val_MinusLogProbMetric: 16.9540

Epoch 633: val_loss improved from 16.95505 to 16.95397, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_294/weights/best_weights.h5
196/196 - 64s - loss: 16.4320 - MinusLogProbMetric: 16.4320 - val_loss: 16.9540 - val_MinusLogProbMetric: 16.9540 - lr: 1.0417e-05 - 64s/epoch - 329ms/step
Epoch 634/1000
2023-09-27 03:04:10.632 
Epoch 634/1000 
	 loss: 16.4300, MinusLogProbMetric: 16.4300, val_loss: 16.9573, val_MinusLogProbMetric: 16.9573

Epoch 634: val_loss did not improve from 16.95397
196/196 - 62s - loss: 16.4300 - MinusLogProbMetric: 16.4300 - val_loss: 16.9573 - val_MinusLogProbMetric: 16.9573 - lr: 1.0417e-05 - 62s/epoch - 317ms/step
Epoch 635/1000
2023-09-27 03:05:12.588 
Epoch 635/1000 
	 loss: 16.4333, MinusLogProbMetric: 16.4333, val_loss: 16.9605, val_MinusLogProbMetric: 16.9605

Epoch 635: val_loss did not improve from 16.95397
196/196 - 62s - loss: 16.4333 - MinusLogProbMetric: 16.4333 - val_loss: 16.9605 - val_MinusLogProbMetric: 16.9605 - lr: 1.0417e-05 - 62s/epoch - 316ms/step
Epoch 636/1000
2023-09-27 03:06:15.569 
Epoch 636/1000 
	 loss: 16.4318, MinusLogProbMetric: 16.4318, val_loss: 16.9755, val_MinusLogProbMetric: 16.9755

Epoch 636: val_loss did not improve from 16.95397
196/196 - 63s - loss: 16.4318 - MinusLogProbMetric: 16.4318 - val_loss: 16.9755 - val_MinusLogProbMetric: 16.9755 - lr: 1.0417e-05 - 63s/epoch - 321ms/step
Epoch 637/1000
2023-09-27 03:07:18.721 
Epoch 637/1000 
	 loss: 16.4325, MinusLogProbMetric: 16.4325, val_loss: 16.9516, val_MinusLogProbMetric: 16.9516

Epoch 637: val_loss improved from 16.95397 to 16.95159, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_294/weights/best_weights.h5
196/196 - 64s - loss: 16.4325 - MinusLogProbMetric: 16.4325 - val_loss: 16.9516 - val_MinusLogProbMetric: 16.9516 - lr: 1.0417e-05 - 64s/epoch - 329ms/step
Epoch 638/1000
2023-09-27 03:08:24.101 
Epoch 638/1000 
	 loss: 16.4313, MinusLogProbMetric: 16.4313, val_loss: 16.9589, val_MinusLogProbMetric: 16.9589

Epoch 638: val_loss did not improve from 16.95159
196/196 - 64s - loss: 16.4313 - MinusLogProbMetric: 16.4313 - val_loss: 16.9589 - val_MinusLogProbMetric: 16.9589 - lr: 1.0417e-05 - 64s/epoch - 327ms/step
Epoch 639/1000
2023-09-27 03:09:27.434 
Epoch 639/1000 
	 loss: 16.4323, MinusLogProbMetric: 16.4323, val_loss: 16.9530, val_MinusLogProbMetric: 16.9530

Epoch 639: val_loss did not improve from 16.95159
196/196 - 63s - loss: 16.4323 - MinusLogProbMetric: 16.4323 - val_loss: 16.9530 - val_MinusLogProbMetric: 16.9530 - lr: 1.0417e-05 - 63s/epoch - 323ms/step
Epoch 640/1000
2023-09-27 03:10:30.013 
Epoch 640/1000 
	 loss: 16.4306, MinusLogProbMetric: 16.4306, val_loss: 16.9541, val_MinusLogProbMetric: 16.9541

Epoch 640: val_loss did not improve from 16.95159
196/196 - 63s - loss: 16.4306 - MinusLogProbMetric: 16.4306 - val_loss: 16.9541 - val_MinusLogProbMetric: 16.9541 - lr: 1.0417e-05 - 63s/epoch - 319ms/step
Epoch 641/1000
2023-09-27 03:11:32.817 
Epoch 641/1000 
	 loss: 16.4326, MinusLogProbMetric: 16.4326, val_loss: 16.9574, val_MinusLogProbMetric: 16.9574

Epoch 641: val_loss did not improve from 16.95159
196/196 - 63s - loss: 16.4326 - MinusLogProbMetric: 16.4326 - val_loss: 16.9574 - val_MinusLogProbMetric: 16.9574 - lr: 1.0417e-05 - 63s/epoch - 320ms/step
Epoch 642/1000
2023-09-27 03:12:35.536 
Epoch 642/1000 
	 loss: 16.4315, MinusLogProbMetric: 16.4315, val_loss: 16.9582, val_MinusLogProbMetric: 16.9582

Epoch 642: val_loss did not improve from 16.95159
196/196 - 63s - loss: 16.4315 - MinusLogProbMetric: 16.4315 - val_loss: 16.9582 - val_MinusLogProbMetric: 16.9582 - lr: 1.0417e-05 - 63s/epoch - 320ms/step
Epoch 643/1000
2023-09-27 03:13:38.427 
Epoch 643/1000 
	 loss: 16.4317, MinusLogProbMetric: 16.4317, val_loss: 16.9517, val_MinusLogProbMetric: 16.9517

Epoch 643: val_loss did not improve from 16.95159
196/196 - 63s - loss: 16.4317 - MinusLogProbMetric: 16.4317 - val_loss: 16.9517 - val_MinusLogProbMetric: 16.9517 - lr: 1.0417e-05 - 63s/epoch - 321ms/step
Epoch 644/1000
2023-09-27 03:14:41.127 
Epoch 644/1000 
	 loss: 16.4325, MinusLogProbMetric: 16.4325, val_loss: 16.9549, val_MinusLogProbMetric: 16.9549

Epoch 644: val_loss did not improve from 16.95159
196/196 - 63s - loss: 16.4325 - MinusLogProbMetric: 16.4325 - val_loss: 16.9549 - val_MinusLogProbMetric: 16.9549 - lr: 1.0417e-05 - 63s/epoch - 320ms/step
Epoch 645/1000
2023-09-27 03:15:44.336 
Epoch 645/1000 
	 loss: 16.4300, MinusLogProbMetric: 16.4300, val_loss: 16.9552, val_MinusLogProbMetric: 16.9552

Epoch 645: val_loss did not improve from 16.95159
196/196 - 63s - loss: 16.4300 - MinusLogProbMetric: 16.4300 - val_loss: 16.9552 - val_MinusLogProbMetric: 16.9552 - lr: 1.0417e-05 - 63s/epoch - 322ms/step
Epoch 646/1000
2023-09-27 03:16:47.447 
Epoch 646/1000 
	 loss: 16.4306, MinusLogProbMetric: 16.4306, val_loss: 16.9571, val_MinusLogProbMetric: 16.9571

Epoch 646: val_loss did not improve from 16.95159
196/196 - 63s - loss: 16.4306 - MinusLogProbMetric: 16.4306 - val_loss: 16.9571 - val_MinusLogProbMetric: 16.9571 - lr: 1.0417e-05 - 63s/epoch - 322ms/step
Epoch 647/1000
2023-09-27 03:17:50.242 
Epoch 647/1000 
	 loss: 16.4333, MinusLogProbMetric: 16.4333, val_loss: 16.9756, val_MinusLogProbMetric: 16.9756

Epoch 647: val_loss did not improve from 16.95159
196/196 - 63s - loss: 16.4333 - MinusLogProbMetric: 16.4333 - val_loss: 16.9756 - val_MinusLogProbMetric: 16.9756 - lr: 1.0417e-05 - 63s/epoch - 320ms/step
Epoch 648/1000
2023-09-27 03:18:53.453 
Epoch 648/1000 
	 loss: 16.4353, MinusLogProbMetric: 16.4353, val_loss: 16.9658, val_MinusLogProbMetric: 16.9658

Epoch 648: val_loss did not improve from 16.95159
196/196 - 63s - loss: 16.4353 - MinusLogProbMetric: 16.4353 - val_loss: 16.9658 - val_MinusLogProbMetric: 16.9658 - lr: 1.0417e-05 - 63s/epoch - 322ms/step
Epoch 649/1000
2023-09-27 03:19:55.643 
Epoch 649/1000 
	 loss: 16.4321, MinusLogProbMetric: 16.4321, val_loss: 16.9584, val_MinusLogProbMetric: 16.9584

Epoch 649: val_loss did not improve from 16.95159
196/196 - 62s - loss: 16.4321 - MinusLogProbMetric: 16.4321 - val_loss: 16.9584 - val_MinusLogProbMetric: 16.9584 - lr: 1.0417e-05 - 62s/epoch - 317ms/step
Epoch 650/1000
2023-09-27 03:20:57.986 
Epoch 650/1000 
	 loss: 16.4305, MinusLogProbMetric: 16.4305, val_loss: 16.9580, val_MinusLogProbMetric: 16.9580

Epoch 650: val_loss did not improve from 16.95159
196/196 - 62s - loss: 16.4305 - MinusLogProbMetric: 16.4305 - val_loss: 16.9580 - val_MinusLogProbMetric: 16.9580 - lr: 1.0417e-05 - 62s/epoch - 318ms/step
Epoch 651/1000
2023-09-27 03:22:00.592 
Epoch 651/1000 
	 loss: 16.4286, MinusLogProbMetric: 16.4286, val_loss: 16.9582, val_MinusLogProbMetric: 16.9582

Epoch 651: val_loss did not improve from 16.95159
196/196 - 63s - loss: 16.4286 - MinusLogProbMetric: 16.4286 - val_loss: 16.9582 - val_MinusLogProbMetric: 16.9582 - lr: 1.0417e-05 - 63s/epoch - 319ms/step
Epoch 652/1000
2023-09-27 03:23:03.564 
Epoch 652/1000 
	 loss: 16.4310, MinusLogProbMetric: 16.4310, val_loss: 16.9568, val_MinusLogProbMetric: 16.9568

Epoch 652: val_loss did not improve from 16.95159
196/196 - 63s - loss: 16.4310 - MinusLogProbMetric: 16.4310 - val_loss: 16.9568 - val_MinusLogProbMetric: 16.9568 - lr: 1.0417e-05 - 63s/epoch - 321ms/step
Epoch 653/1000
2023-09-27 03:24:06.338 
Epoch 653/1000 
	 loss: 16.4299, MinusLogProbMetric: 16.4299, val_loss: 16.9618, val_MinusLogProbMetric: 16.9618

Epoch 653: val_loss did not improve from 16.95159
196/196 - 63s - loss: 16.4299 - MinusLogProbMetric: 16.4299 - val_loss: 16.9618 - val_MinusLogProbMetric: 16.9618 - lr: 1.0417e-05 - 63s/epoch - 320ms/step
Epoch 654/1000
2023-09-27 03:25:08.944 
Epoch 654/1000 
	 loss: 16.4288, MinusLogProbMetric: 16.4288, val_loss: 16.9521, val_MinusLogProbMetric: 16.9521

Epoch 654: val_loss did not improve from 16.95159
196/196 - 63s - loss: 16.4288 - MinusLogProbMetric: 16.4288 - val_loss: 16.9521 - val_MinusLogProbMetric: 16.9521 - lr: 1.0417e-05 - 63s/epoch - 319ms/step
Epoch 655/1000
2023-09-27 03:26:12.322 
Epoch 655/1000 
	 loss: 16.4294, MinusLogProbMetric: 16.4294, val_loss: 16.9513, val_MinusLogProbMetric: 16.9513

Epoch 655: val_loss improved from 16.95159 to 16.95128, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_294/weights/best_weights.h5
196/196 - 65s - loss: 16.4294 - MinusLogProbMetric: 16.4294 - val_loss: 16.9513 - val_MinusLogProbMetric: 16.9513 - lr: 1.0417e-05 - 65s/epoch - 330ms/step
Epoch 656/1000
2023-09-27 03:27:16.507 
Epoch 656/1000 
	 loss: 16.4327, MinusLogProbMetric: 16.4327, val_loss: 16.9550, val_MinusLogProbMetric: 16.9550

Epoch 656: val_loss did not improve from 16.95128
196/196 - 63s - loss: 16.4327 - MinusLogProbMetric: 16.4327 - val_loss: 16.9550 - val_MinusLogProbMetric: 16.9550 - lr: 1.0417e-05 - 63s/epoch - 321ms/step
Epoch 657/1000
2023-09-27 03:28:19.979 
Epoch 657/1000 
	 loss: 16.4325, MinusLogProbMetric: 16.4325, val_loss: 16.9507, val_MinusLogProbMetric: 16.9507

Epoch 657: val_loss improved from 16.95128 to 16.95074, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_294/weights/best_weights.h5
196/196 - 65s - loss: 16.4325 - MinusLogProbMetric: 16.4325 - val_loss: 16.9507 - val_MinusLogProbMetric: 16.9507 - lr: 1.0417e-05 - 65s/epoch - 330ms/step
Epoch 658/1000
2023-09-27 03:29:23.828 
Epoch 658/1000 
	 loss: 16.4291, MinusLogProbMetric: 16.4291, val_loss: 16.9574, val_MinusLogProbMetric: 16.9574

Epoch 658: val_loss did not improve from 16.95074
196/196 - 63s - loss: 16.4291 - MinusLogProbMetric: 16.4291 - val_loss: 16.9574 - val_MinusLogProbMetric: 16.9574 - lr: 1.0417e-05 - 63s/epoch - 320ms/step
Epoch 659/1000
2023-09-27 03:30:27.378 
Epoch 659/1000 
	 loss: 16.4296, MinusLogProbMetric: 16.4296, val_loss: 16.9543, val_MinusLogProbMetric: 16.9543

Epoch 659: val_loss did not improve from 16.95074
196/196 - 64s - loss: 16.4296 - MinusLogProbMetric: 16.4296 - val_loss: 16.9543 - val_MinusLogProbMetric: 16.9543 - lr: 1.0417e-05 - 64s/epoch - 324ms/step
Epoch 660/1000
2023-09-27 03:31:30.534 
Epoch 660/1000 
	 loss: 16.4293, MinusLogProbMetric: 16.4293, val_loss: 16.9538, val_MinusLogProbMetric: 16.9538

Epoch 660: val_loss did not improve from 16.95074
196/196 - 63s - loss: 16.4293 - MinusLogProbMetric: 16.4293 - val_loss: 16.9538 - val_MinusLogProbMetric: 16.9538 - lr: 1.0417e-05 - 63s/epoch - 322ms/step
Epoch 661/1000
2023-09-27 03:32:32.991 
Epoch 661/1000 
	 loss: 16.4304, MinusLogProbMetric: 16.4304, val_loss: 16.9540, val_MinusLogProbMetric: 16.9540

Epoch 661: val_loss did not improve from 16.95074
196/196 - 62s - loss: 16.4304 - MinusLogProbMetric: 16.4304 - val_loss: 16.9540 - val_MinusLogProbMetric: 16.9540 - lr: 1.0417e-05 - 62s/epoch - 319ms/step
Epoch 662/1000
2023-09-27 03:33:36.646 
Epoch 662/1000 
	 loss: 16.4297, MinusLogProbMetric: 16.4297, val_loss: 16.9607, val_MinusLogProbMetric: 16.9607

Epoch 662: val_loss did not improve from 16.95074
196/196 - 64s - loss: 16.4297 - MinusLogProbMetric: 16.4297 - val_loss: 16.9607 - val_MinusLogProbMetric: 16.9607 - lr: 1.0417e-05 - 64s/epoch - 325ms/step
Epoch 663/1000
2023-09-27 03:34:40.484 
Epoch 663/1000 
	 loss: 16.4305, MinusLogProbMetric: 16.4305, val_loss: 16.9598, val_MinusLogProbMetric: 16.9598

Epoch 663: val_loss did not improve from 16.95074
196/196 - 64s - loss: 16.4305 - MinusLogProbMetric: 16.4305 - val_loss: 16.9598 - val_MinusLogProbMetric: 16.9598 - lr: 1.0417e-05 - 64s/epoch - 326ms/step
Epoch 664/1000
2023-09-27 03:35:44.233 
Epoch 664/1000 
	 loss: 16.4309, MinusLogProbMetric: 16.4309, val_loss: 16.9537, val_MinusLogProbMetric: 16.9537

Epoch 664: val_loss did not improve from 16.95074
196/196 - 64s - loss: 16.4309 - MinusLogProbMetric: 16.4309 - val_loss: 16.9537 - val_MinusLogProbMetric: 16.9537 - lr: 1.0417e-05 - 64s/epoch - 325ms/step
Epoch 665/1000
2023-09-27 03:36:47.363 
Epoch 665/1000 
	 loss: 16.4302, MinusLogProbMetric: 16.4302, val_loss: 16.9541, val_MinusLogProbMetric: 16.9541

Epoch 665: val_loss did not improve from 16.95074
196/196 - 63s - loss: 16.4302 - MinusLogProbMetric: 16.4302 - val_loss: 16.9541 - val_MinusLogProbMetric: 16.9541 - lr: 1.0417e-05 - 63s/epoch - 322ms/step
Epoch 666/1000
2023-09-27 03:37:50.152 
Epoch 666/1000 
	 loss: 16.4297, MinusLogProbMetric: 16.4297, val_loss: 16.9625, val_MinusLogProbMetric: 16.9625

Epoch 666: val_loss did not improve from 16.95074
196/196 - 63s - loss: 16.4297 - MinusLogProbMetric: 16.4297 - val_loss: 16.9625 - val_MinusLogProbMetric: 16.9625 - lr: 1.0417e-05 - 63s/epoch - 320ms/step
Epoch 667/1000
2023-09-27 03:38:53.984 
Epoch 667/1000 
	 loss: 16.4321, MinusLogProbMetric: 16.4321, val_loss: 16.9583, val_MinusLogProbMetric: 16.9583

Epoch 667: val_loss did not improve from 16.95074
196/196 - 64s - loss: 16.4321 - MinusLogProbMetric: 16.4321 - val_loss: 16.9583 - val_MinusLogProbMetric: 16.9583 - lr: 1.0417e-05 - 64s/epoch - 326ms/step
Epoch 668/1000
2023-09-27 03:39:57.044 
Epoch 668/1000 
	 loss: 16.4302, MinusLogProbMetric: 16.4302, val_loss: 16.9648, val_MinusLogProbMetric: 16.9648

Epoch 668: val_loss did not improve from 16.95074
196/196 - 63s - loss: 16.4302 - MinusLogProbMetric: 16.4302 - val_loss: 16.9648 - val_MinusLogProbMetric: 16.9648 - lr: 1.0417e-05 - 63s/epoch - 322ms/step
Epoch 669/1000
2023-09-27 03:40:59.627 
Epoch 669/1000 
	 loss: 16.4290, MinusLogProbMetric: 16.4290, val_loss: 16.9510, val_MinusLogProbMetric: 16.9510

Epoch 669: val_loss did not improve from 16.95074
196/196 - 63s - loss: 16.4290 - MinusLogProbMetric: 16.4290 - val_loss: 16.9510 - val_MinusLogProbMetric: 16.9510 - lr: 1.0417e-05 - 63s/epoch - 319ms/step
Epoch 670/1000
2023-09-27 03:42:02.232 
Epoch 670/1000 
	 loss: 16.4300, MinusLogProbMetric: 16.4300, val_loss: 16.9552, val_MinusLogProbMetric: 16.9552

Epoch 670: val_loss did not improve from 16.95074
196/196 - 63s - loss: 16.4300 - MinusLogProbMetric: 16.4300 - val_loss: 16.9552 - val_MinusLogProbMetric: 16.9552 - lr: 1.0417e-05 - 63s/epoch - 319ms/step
Epoch 671/1000
2023-09-27 03:43:05.577 
Epoch 671/1000 
	 loss: 16.4301, MinusLogProbMetric: 16.4301, val_loss: 16.9581, val_MinusLogProbMetric: 16.9581

Epoch 671: val_loss did not improve from 16.95074
196/196 - 63s - loss: 16.4301 - MinusLogProbMetric: 16.4301 - val_loss: 16.9581 - val_MinusLogProbMetric: 16.9581 - lr: 1.0417e-05 - 63s/epoch - 323ms/step
Epoch 672/1000
2023-09-27 03:44:08.142 
Epoch 672/1000 
	 loss: 16.4330, MinusLogProbMetric: 16.4330, val_loss: 16.9510, val_MinusLogProbMetric: 16.9510

Epoch 672: val_loss did not improve from 16.95074
196/196 - 63s - loss: 16.4330 - MinusLogProbMetric: 16.4330 - val_loss: 16.9510 - val_MinusLogProbMetric: 16.9510 - lr: 1.0417e-05 - 63s/epoch - 319ms/step
Epoch 673/1000
2023-09-27 03:45:11.704 
Epoch 673/1000 
	 loss: 16.4302, MinusLogProbMetric: 16.4302, val_loss: 16.9586, val_MinusLogProbMetric: 16.9586

Epoch 673: val_loss did not improve from 16.95074
196/196 - 64s - loss: 16.4302 - MinusLogProbMetric: 16.4302 - val_loss: 16.9586 - val_MinusLogProbMetric: 16.9586 - lr: 1.0417e-05 - 64s/epoch - 324ms/step
Epoch 674/1000
2023-09-27 03:46:15.289 
Epoch 674/1000 
	 loss: 16.4303, MinusLogProbMetric: 16.4303, val_loss: 16.9507, val_MinusLogProbMetric: 16.9507

Epoch 674: val_loss improved from 16.95074 to 16.95066, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_294/weights/best_weights.h5
196/196 - 64s - loss: 16.4303 - MinusLogProbMetric: 16.4303 - val_loss: 16.9507 - val_MinusLogProbMetric: 16.9507 - lr: 1.0417e-05 - 64s/epoch - 328ms/step
Epoch 675/1000
2023-09-27 03:47:18.286 
Epoch 675/1000 
	 loss: 16.4271, MinusLogProbMetric: 16.4271, val_loss: 16.9554, val_MinusLogProbMetric: 16.9554

Epoch 675: val_loss did not improve from 16.95066
196/196 - 62s - loss: 16.4271 - MinusLogProbMetric: 16.4271 - val_loss: 16.9554 - val_MinusLogProbMetric: 16.9554 - lr: 1.0417e-05 - 62s/epoch - 317ms/step
Epoch 676/1000
2023-09-27 03:48:21.739 
Epoch 676/1000 
	 loss: 16.4310, MinusLogProbMetric: 16.4310, val_loss: 16.9539, val_MinusLogProbMetric: 16.9539

Epoch 676: val_loss did not improve from 16.95066
196/196 - 63s - loss: 16.4310 - MinusLogProbMetric: 16.4310 - val_loss: 16.9539 - val_MinusLogProbMetric: 16.9539 - lr: 1.0417e-05 - 63s/epoch - 324ms/step
Epoch 677/1000
2023-09-27 03:49:24.141 
Epoch 677/1000 
	 loss: 16.4289, MinusLogProbMetric: 16.4289, val_loss: 16.9557, val_MinusLogProbMetric: 16.9557

Epoch 677: val_loss did not improve from 16.95066
196/196 - 62s - loss: 16.4289 - MinusLogProbMetric: 16.4289 - val_loss: 16.9557 - val_MinusLogProbMetric: 16.9557 - lr: 1.0417e-05 - 62s/epoch - 318ms/step
Epoch 678/1000
2023-09-27 03:50:27.329 
Epoch 678/1000 
	 loss: 16.4297, MinusLogProbMetric: 16.4297, val_loss: 16.9674, val_MinusLogProbMetric: 16.9674

Epoch 678: val_loss did not improve from 16.95066
196/196 - 63s - loss: 16.4297 - MinusLogProbMetric: 16.4297 - val_loss: 16.9674 - val_MinusLogProbMetric: 16.9674 - lr: 1.0417e-05 - 63s/epoch - 322ms/step
Epoch 679/1000
2023-09-27 03:51:30.689 
Epoch 679/1000 
	 loss: 16.4316, MinusLogProbMetric: 16.4316, val_loss: 16.9560, val_MinusLogProbMetric: 16.9560

Epoch 679: val_loss did not improve from 16.95066
196/196 - 63s - loss: 16.4316 - MinusLogProbMetric: 16.4316 - val_loss: 16.9560 - val_MinusLogProbMetric: 16.9560 - lr: 1.0417e-05 - 63s/epoch - 323ms/step
Epoch 680/1000
2023-09-27 03:52:34.076 
Epoch 680/1000 
	 loss: 16.4291, MinusLogProbMetric: 16.4291, val_loss: 16.9549, val_MinusLogProbMetric: 16.9549

Epoch 680: val_loss did not improve from 16.95066
196/196 - 63s - loss: 16.4291 - MinusLogProbMetric: 16.4291 - val_loss: 16.9549 - val_MinusLogProbMetric: 16.9549 - lr: 1.0417e-05 - 63s/epoch - 323ms/step
Epoch 681/1000
2023-09-27 03:53:36.754 
Epoch 681/1000 
	 loss: 16.4291, MinusLogProbMetric: 16.4291, val_loss: 16.9770, val_MinusLogProbMetric: 16.9770

Epoch 681: val_loss did not improve from 16.95066
196/196 - 63s - loss: 16.4291 - MinusLogProbMetric: 16.4291 - val_loss: 16.9770 - val_MinusLogProbMetric: 16.9770 - lr: 1.0417e-05 - 63s/epoch - 320ms/step
Epoch 682/1000
2023-09-27 03:54:38.412 
Epoch 682/1000 
	 loss: 16.4281, MinusLogProbMetric: 16.4281, val_loss: 16.9505, val_MinusLogProbMetric: 16.9505

Epoch 682: val_loss improved from 16.95066 to 16.95048, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_294/weights/best_weights.h5
196/196 - 63s - loss: 16.4281 - MinusLogProbMetric: 16.4281 - val_loss: 16.9505 - val_MinusLogProbMetric: 16.9505 - lr: 1.0417e-05 - 63s/epoch - 319ms/step
Epoch 683/1000
2023-09-27 03:55:41.773 
Epoch 683/1000 
	 loss: 16.4294, MinusLogProbMetric: 16.4294, val_loss: 16.9545, val_MinusLogProbMetric: 16.9545

Epoch 683: val_loss did not improve from 16.95048
196/196 - 62s - loss: 16.4294 - MinusLogProbMetric: 16.4294 - val_loss: 16.9545 - val_MinusLogProbMetric: 16.9545 - lr: 1.0417e-05 - 62s/epoch - 318ms/step
Epoch 684/1000
2023-09-27 03:56:43.432 
Epoch 684/1000 
	 loss: 16.4294, MinusLogProbMetric: 16.4294, val_loss: 16.9573, val_MinusLogProbMetric: 16.9573

Epoch 684: val_loss did not improve from 16.95048
196/196 - 62s - loss: 16.4294 - MinusLogProbMetric: 16.4294 - val_loss: 16.9573 - val_MinusLogProbMetric: 16.9573 - lr: 1.0417e-05 - 62s/epoch - 315ms/step
Epoch 685/1000
2023-09-27 03:57:46.398 
Epoch 685/1000 
	 loss: 16.4279, MinusLogProbMetric: 16.4279, val_loss: 16.9645, val_MinusLogProbMetric: 16.9645

Epoch 685: val_loss did not improve from 16.95048
196/196 - 63s - loss: 16.4279 - MinusLogProbMetric: 16.4279 - val_loss: 16.9645 - val_MinusLogProbMetric: 16.9645 - lr: 1.0417e-05 - 63s/epoch - 321ms/step
Epoch 686/1000
2023-09-27 03:58:51.019 
Epoch 686/1000 
	 loss: 16.4283, MinusLogProbMetric: 16.4283, val_loss: 16.9528, val_MinusLogProbMetric: 16.9528

Epoch 686: val_loss did not improve from 16.95048
196/196 - 65s - loss: 16.4283 - MinusLogProbMetric: 16.4283 - val_loss: 16.9528 - val_MinusLogProbMetric: 16.9528 - lr: 1.0417e-05 - 65s/epoch - 330ms/step
Epoch 687/1000
2023-09-27 03:59:53.125 
Epoch 687/1000 
	 loss: 16.4273, MinusLogProbMetric: 16.4273, val_loss: 16.9544, val_MinusLogProbMetric: 16.9544

Epoch 687: val_loss did not improve from 16.95048
196/196 - 62s - loss: 16.4273 - MinusLogProbMetric: 16.4273 - val_loss: 16.9544 - val_MinusLogProbMetric: 16.9544 - lr: 1.0417e-05 - 62s/epoch - 317ms/step
Epoch 688/1000
2023-09-27 04:00:56.057 
Epoch 688/1000 
	 loss: 16.4274, MinusLogProbMetric: 16.4274, val_loss: 16.9551, val_MinusLogProbMetric: 16.9551

Epoch 688: val_loss did not improve from 16.95048
196/196 - 63s - loss: 16.4274 - MinusLogProbMetric: 16.4274 - val_loss: 16.9551 - val_MinusLogProbMetric: 16.9551 - lr: 1.0417e-05 - 63s/epoch - 321ms/step
Epoch 689/1000
2023-09-27 04:01:58.268 
Epoch 689/1000 
	 loss: 16.4281, MinusLogProbMetric: 16.4281, val_loss: 16.9545, val_MinusLogProbMetric: 16.9545

Epoch 689: val_loss did not improve from 16.95048
196/196 - 62s - loss: 16.4281 - MinusLogProbMetric: 16.4281 - val_loss: 16.9545 - val_MinusLogProbMetric: 16.9545 - lr: 1.0417e-05 - 62s/epoch - 317ms/step
Epoch 690/1000
2023-09-27 04:03:00.457 
Epoch 690/1000 
	 loss: 16.4287, MinusLogProbMetric: 16.4287, val_loss: 16.9525, val_MinusLogProbMetric: 16.9525

Epoch 690: val_loss did not improve from 16.95048
196/196 - 62s - loss: 16.4287 - MinusLogProbMetric: 16.4287 - val_loss: 16.9525 - val_MinusLogProbMetric: 16.9525 - lr: 1.0417e-05 - 62s/epoch - 317ms/step
Epoch 691/1000
2023-09-27 04:04:03.559 
Epoch 691/1000 
	 loss: 16.4281, MinusLogProbMetric: 16.4281, val_loss: 16.9509, val_MinusLogProbMetric: 16.9509

Epoch 691: val_loss did not improve from 16.95048
196/196 - 63s - loss: 16.4281 - MinusLogProbMetric: 16.4281 - val_loss: 16.9509 - val_MinusLogProbMetric: 16.9509 - lr: 1.0417e-05 - 63s/epoch - 322ms/step
Epoch 692/1000
2023-09-27 04:05:06.002 
Epoch 692/1000 
	 loss: 16.4279, MinusLogProbMetric: 16.4279, val_loss: 16.9541, val_MinusLogProbMetric: 16.9541

Epoch 692: val_loss did not improve from 16.95048
196/196 - 62s - loss: 16.4279 - MinusLogProbMetric: 16.4279 - val_loss: 16.9541 - val_MinusLogProbMetric: 16.9541 - lr: 1.0417e-05 - 62s/epoch - 319ms/step
Epoch 693/1000
2023-09-27 04:06:08.916 
Epoch 693/1000 
	 loss: 16.4276, MinusLogProbMetric: 16.4276, val_loss: 16.9487, val_MinusLogProbMetric: 16.9487

Epoch 693: val_loss improved from 16.95048 to 16.94873, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_294/weights/best_weights.h5
196/196 - 64s - loss: 16.4276 - MinusLogProbMetric: 16.4276 - val_loss: 16.9487 - val_MinusLogProbMetric: 16.9487 - lr: 1.0417e-05 - 64s/epoch - 327ms/step
Epoch 694/1000
2023-09-27 04:07:11.767 
Epoch 694/1000 
	 loss: 16.4286, MinusLogProbMetric: 16.4286, val_loss: 16.9601, val_MinusLogProbMetric: 16.9601

Epoch 694: val_loss did not improve from 16.94873
196/196 - 62s - loss: 16.4286 - MinusLogProbMetric: 16.4286 - val_loss: 16.9601 - val_MinusLogProbMetric: 16.9601 - lr: 1.0417e-05 - 62s/epoch - 315ms/step
Epoch 695/1000
2023-09-27 04:08:15.263 
Epoch 695/1000 
	 loss: 16.4288, MinusLogProbMetric: 16.4288, val_loss: 16.9547, val_MinusLogProbMetric: 16.9547

Epoch 695: val_loss did not improve from 16.94873
196/196 - 63s - loss: 16.4288 - MinusLogProbMetric: 16.4288 - val_loss: 16.9547 - val_MinusLogProbMetric: 16.9547 - lr: 1.0417e-05 - 63s/epoch - 324ms/step
Epoch 696/1000
2023-09-27 04:09:17.902 
Epoch 696/1000 
	 loss: 16.4298, MinusLogProbMetric: 16.4298, val_loss: 16.9584, val_MinusLogProbMetric: 16.9584

Epoch 696: val_loss did not improve from 16.94873
196/196 - 63s - loss: 16.4298 - MinusLogProbMetric: 16.4298 - val_loss: 16.9584 - val_MinusLogProbMetric: 16.9584 - lr: 1.0417e-05 - 63s/epoch - 320ms/step
Epoch 697/1000
2023-09-27 04:10:20.593 
Epoch 697/1000 
	 loss: 16.4281, MinusLogProbMetric: 16.4281, val_loss: 16.9483, val_MinusLogProbMetric: 16.9483

Epoch 697: val_loss improved from 16.94873 to 16.94832, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_294/weights/best_weights.h5
196/196 - 64s - loss: 16.4281 - MinusLogProbMetric: 16.4281 - val_loss: 16.9483 - val_MinusLogProbMetric: 16.9483 - lr: 1.0417e-05 - 64s/epoch - 324ms/step
Epoch 698/1000
2023-09-27 04:11:23.714 
Epoch 698/1000 
	 loss: 16.4313, MinusLogProbMetric: 16.4313, val_loss: 16.9567, val_MinusLogProbMetric: 16.9567

Epoch 698: val_loss did not improve from 16.94832
196/196 - 62s - loss: 16.4313 - MinusLogProbMetric: 16.4313 - val_loss: 16.9567 - val_MinusLogProbMetric: 16.9567 - lr: 1.0417e-05 - 62s/epoch - 317ms/step
Epoch 699/1000
2023-09-27 04:12:26.345 
Epoch 699/1000 
	 loss: 16.4294, MinusLogProbMetric: 16.4294, val_loss: 16.9534, val_MinusLogProbMetric: 16.9534

Epoch 699: val_loss did not improve from 16.94832
196/196 - 63s - loss: 16.4294 - MinusLogProbMetric: 16.4294 - val_loss: 16.9534 - val_MinusLogProbMetric: 16.9534 - lr: 1.0417e-05 - 63s/epoch - 320ms/step
Epoch 700/1000
2023-09-27 04:13:29.413 
Epoch 700/1000 
	 loss: 16.4287, MinusLogProbMetric: 16.4287, val_loss: 16.9512, val_MinusLogProbMetric: 16.9512

Epoch 700: val_loss did not improve from 16.94832
196/196 - 63s - loss: 16.4287 - MinusLogProbMetric: 16.4287 - val_loss: 16.9512 - val_MinusLogProbMetric: 16.9512 - lr: 1.0417e-05 - 63s/epoch - 322ms/step
Epoch 701/1000
2023-09-27 04:14:31.556 
Epoch 701/1000 
	 loss: 16.4287, MinusLogProbMetric: 16.4287, val_loss: 16.9527, val_MinusLogProbMetric: 16.9527

Epoch 701: val_loss did not improve from 16.94832
196/196 - 62s - loss: 16.4287 - MinusLogProbMetric: 16.4287 - val_loss: 16.9527 - val_MinusLogProbMetric: 16.9527 - lr: 1.0417e-05 - 62s/epoch - 317ms/step
Epoch 702/1000
2023-09-27 04:15:33.850 
Epoch 702/1000 
	 loss: 16.4293, MinusLogProbMetric: 16.4293, val_loss: 16.9585, val_MinusLogProbMetric: 16.9585

Epoch 702: val_loss did not improve from 16.94832
196/196 - 62s - loss: 16.4293 - MinusLogProbMetric: 16.4293 - val_loss: 16.9585 - val_MinusLogProbMetric: 16.9585 - lr: 1.0417e-05 - 62s/epoch - 318ms/step
Epoch 703/1000
2023-09-27 04:16:36.386 
Epoch 703/1000 
	 loss: 16.4284, MinusLogProbMetric: 16.4284, val_loss: 16.9513, val_MinusLogProbMetric: 16.9513

Epoch 703: val_loss did not improve from 16.94832
196/196 - 63s - loss: 16.4284 - MinusLogProbMetric: 16.4284 - val_loss: 16.9513 - val_MinusLogProbMetric: 16.9513 - lr: 1.0417e-05 - 63s/epoch - 319ms/step
Epoch 704/1000
2023-09-27 04:17:39.245 
Epoch 704/1000 
	 loss: 16.4281, MinusLogProbMetric: 16.4281, val_loss: 16.9542, val_MinusLogProbMetric: 16.9542

Epoch 704: val_loss did not improve from 16.94832
196/196 - 63s - loss: 16.4281 - MinusLogProbMetric: 16.4281 - val_loss: 16.9542 - val_MinusLogProbMetric: 16.9542 - lr: 1.0417e-05 - 63s/epoch - 321ms/step
Epoch 705/1000
2023-09-27 04:18:41.948 
Epoch 705/1000 
	 loss: 16.4290, MinusLogProbMetric: 16.4290, val_loss: 16.9606, val_MinusLogProbMetric: 16.9606

Epoch 705: val_loss did not improve from 16.94832
196/196 - 63s - loss: 16.4290 - MinusLogProbMetric: 16.4290 - val_loss: 16.9606 - val_MinusLogProbMetric: 16.9606 - lr: 1.0417e-05 - 63s/epoch - 320ms/step
Epoch 706/1000
2023-09-27 04:19:44.762 
Epoch 706/1000 
	 loss: 16.4303, MinusLogProbMetric: 16.4303, val_loss: 16.9509, val_MinusLogProbMetric: 16.9509

Epoch 706: val_loss did not improve from 16.94832
196/196 - 63s - loss: 16.4303 - MinusLogProbMetric: 16.4303 - val_loss: 16.9509 - val_MinusLogProbMetric: 16.9509 - lr: 1.0417e-05 - 63s/epoch - 320ms/step
Epoch 707/1000
2023-09-27 04:20:47.467 
Epoch 707/1000 
	 loss: 16.4278, MinusLogProbMetric: 16.4278, val_loss: 16.9560, val_MinusLogProbMetric: 16.9560

Epoch 707: val_loss did not improve from 16.94832
196/196 - 63s - loss: 16.4278 - MinusLogProbMetric: 16.4278 - val_loss: 16.9560 - val_MinusLogProbMetric: 16.9560 - lr: 1.0417e-05 - 63s/epoch - 320ms/step
Epoch 708/1000
2023-09-27 04:21:50.176 
Epoch 708/1000 
	 loss: 16.4280, MinusLogProbMetric: 16.4280, val_loss: 16.9549, val_MinusLogProbMetric: 16.9549

Epoch 708: val_loss did not improve from 16.94832
196/196 - 63s - loss: 16.4280 - MinusLogProbMetric: 16.4280 - val_loss: 16.9549 - val_MinusLogProbMetric: 16.9549 - lr: 1.0417e-05 - 63s/epoch - 320ms/step
Epoch 709/1000
2023-09-27 04:22:53.507 
Epoch 709/1000 
	 loss: 16.4275, MinusLogProbMetric: 16.4275, val_loss: 16.9617, val_MinusLogProbMetric: 16.9617

Epoch 709: val_loss did not improve from 16.94832
196/196 - 63s - loss: 16.4275 - MinusLogProbMetric: 16.4275 - val_loss: 16.9617 - val_MinusLogProbMetric: 16.9617 - lr: 1.0417e-05 - 63s/epoch - 323ms/step
Epoch 710/1000
2023-09-27 04:23:56.272 
Epoch 710/1000 
	 loss: 16.4295, MinusLogProbMetric: 16.4295, val_loss: 16.9644, val_MinusLogProbMetric: 16.9644

Epoch 710: val_loss did not improve from 16.94832
196/196 - 63s - loss: 16.4295 - MinusLogProbMetric: 16.4295 - val_loss: 16.9644 - val_MinusLogProbMetric: 16.9644 - lr: 1.0417e-05 - 63s/epoch - 320ms/step
Epoch 711/1000
2023-09-27 04:24:59.898 
Epoch 711/1000 
	 loss: 16.4283, MinusLogProbMetric: 16.4283, val_loss: 16.9530, val_MinusLogProbMetric: 16.9530

Epoch 711: val_loss did not improve from 16.94832
196/196 - 64s - loss: 16.4283 - MinusLogProbMetric: 16.4283 - val_loss: 16.9530 - val_MinusLogProbMetric: 16.9530 - lr: 1.0417e-05 - 64s/epoch - 325ms/step
Epoch 712/1000
2023-09-27 04:26:03.384 
Epoch 712/1000 
	 loss: 16.4271, MinusLogProbMetric: 16.4271, val_loss: 16.9534, val_MinusLogProbMetric: 16.9534

Epoch 712: val_loss did not improve from 16.94832
196/196 - 63s - loss: 16.4271 - MinusLogProbMetric: 16.4271 - val_loss: 16.9534 - val_MinusLogProbMetric: 16.9534 - lr: 1.0417e-05 - 63s/epoch - 324ms/step
Epoch 713/1000
2023-09-27 04:27:05.910 
Epoch 713/1000 
	 loss: 16.4261, MinusLogProbMetric: 16.4261, val_loss: 16.9519, val_MinusLogProbMetric: 16.9519

Epoch 713: val_loss did not improve from 16.94832
196/196 - 63s - loss: 16.4261 - MinusLogProbMetric: 16.4261 - val_loss: 16.9519 - val_MinusLogProbMetric: 16.9519 - lr: 1.0417e-05 - 63s/epoch - 319ms/step
Epoch 714/1000
2023-09-27 04:28:07.866 
Epoch 714/1000 
	 loss: 16.4273, MinusLogProbMetric: 16.4273, val_loss: 16.9646, val_MinusLogProbMetric: 16.9646

Epoch 714: val_loss did not improve from 16.94832
196/196 - 62s - loss: 16.4273 - MinusLogProbMetric: 16.4273 - val_loss: 16.9646 - val_MinusLogProbMetric: 16.9646 - lr: 1.0417e-05 - 62s/epoch - 316ms/step
Epoch 715/1000
2023-09-27 04:29:10.274 
Epoch 715/1000 
	 loss: 16.4270, MinusLogProbMetric: 16.4270, val_loss: 16.9504, val_MinusLogProbMetric: 16.9504

Epoch 715: val_loss did not improve from 16.94832
196/196 - 62s - loss: 16.4270 - MinusLogProbMetric: 16.4270 - val_loss: 16.9504 - val_MinusLogProbMetric: 16.9504 - lr: 1.0417e-05 - 62s/epoch - 318ms/step
Epoch 716/1000
2023-09-27 04:30:12.705 
Epoch 716/1000 
	 loss: 16.4303, MinusLogProbMetric: 16.4303, val_loss: 16.9509, val_MinusLogProbMetric: 16.9509

Epoch 716: val_loss did not improve from 16.94832
196/196 - 62s - loss: 16.4303 - MinusLogProbMetric: 16.4303 - val_loss: 16.9509 - val_MinusLogProbMetric: 16.9509 - lr: 1.0417e-05 - 62s/epoch - 319ms/step
Epoch 717/1000
2023-09-27 04:31:15.604 
Epoch 717/1000 
	 loss: 16.4292, MinusLogProbMetric: 16.4292, val_loss: 16.9574, val_MinusLogProbMetric: 16.9574

Epoch 717: val_loss did not improve from 16.94832
196/196 - 63s - loss: 16.4292 - MinusLogProbMetric: 16.4292 - val_loss: 16.9574 - val_MinusLogProbMetric: 16.9574 - lr: 1.0417e-05 - 63s/epoch - 321ms/step
Epoch 718/1000
2023-09-27 04:32:17.698 
Epoch 718/1000 
	 loss: 16.4287, MinusLogProbMetric: 16.4287, val_loss: 16.9489, val_MinusLogProbMetric: 16.9489

Epoch 718: val_loss did not improve from 16.94832
196/196 - 62s - loss: 16.4287 - MinusLogProbMetric: 16.4287 - val_loss: 16.9489 - val_MinusLogProbMetric: 16.9489 - lr: 1.0417e-05 - 62s/epoch - 317ms/step
Epoch 719/1000
2023-09-27 04:33:19.773 
Epoch 719/1000 
	 loss: 16.4271, MinusLogProbMetric: 16.4271, val_loss: 16.9567, val_MinusLogProbMetric: 16.9567

Epoch 719: val_loss did not improve from 16.94832
196/196 - 62s - loss: 16.4271 - MinusLogProbMetric: 16.4271 - val_loss: 16.9567 - val_MinusLogProbMetric: 16.9567 - lr: 1.0417e-05 - 62s/epoch - 317ms/step
Epoch 720/1000
2023-09-27 04:34:22.013 
Epoch 720/1000 
	 loss: 16.4269, MinusLogProbMetric: 16.4269, val_loss: 16.9645, val_MinusLogProbMetric: 16.9645

Epoch 720: val_loss did not improve from 16.94832
196/196 - 62s - loss: 16.4269 - MinusLogProbMetric: 16.4269 - val_loss: 16.9645 - val_MinusLogProbMetric: 16.9645 - lr: 1.0417e-05 - 62s/epoch - 318ms/step
Epoch 721/1000
2023-09-27 04:35:25.276 
Epoch 721/1000 
	 loss: 16.4293, MinusLogProbMetric: 16.4293, val_loss: 16.9662, val_MinusLogProbMetric: 16.9662

Epoch 721: val_loss did not improve from 16.94832
196/196 - 63s - loss: 16.4293 - MinusLogProbMetric: 16.4293 - val_loss: 16.9662 - val_MinusLogProbMetric: 16.9662 - lr: 1.0417e-05 - 63s/epoch - 323ms/step
Epoch 722/1000
2023-09-27 04:36:28.086 
Epoch 722/1000 
	 loss: 16.4305, MinusLogProbMetric: 16.4305, val_loss: 16.9561, val_MinusLogProbMetric: 16.9561

Epoch 722: val_loss did not improve from 16.94832
196/196 - 63s - loss: 16.4305 - MinusLogProbMetric: 16.4305 - val_loss: 16.9561 - val_MinusLogProbMetric: 16.9561 - lr: 1.0417e-05 - 63s/epoch - 320ms/step
Epoch 723/1000
2023-09-27 04:37:30.517 
Epoch 723/1000 
	 loss: 16.4283, MinusLogProbMetric: 16.4283, val_loss: 16.9587, val_MinusLogProbMetric: 16.9587

Epoch 723: val_loss did not improve from 16.94832
196/196 - 62s - loss: 16.4283 - MinusLogProbMetric: 16.4283 - val_loss: 16.9587 - val_MinusLogProbMetric: 16.9587 - lr: 1.0417e-05 - 62s/epoch - 319ms/step
Epoch 724/1000
2023-09-27 04:38:32.385 
Epoch 724/1000 
	 loss: 16.4267, MinusLogProbMetric: 16.4267, val_loss: 16.9524, val_MinusLogProbMetric: 16.9524

Epoch 724: val_loss did not improve from 16.94832
196/196 - 62s - loss: 16.4267 - MinusLogProbMetric: 16.4267 - val_loss: 16.9524 - val_MinusLogProbMetric: 16.9524 - lr: 1.0417e-05 - 62s/epoch - 316ms/step
Epoch 725/1000
2023-09-27 04:39:34.566 
Epoch 725/1000 
	 loss: 16.4281, MinusLogProbMetric: 16.4281, val_loss: 16.9779, val_MinusLogProbMetric: 16.9779

Epoch 725: val_loss did not improve from 16.94832
196/196 - 62s - loss: 16.4281 - MinusLogProbMetric: 16.4281 - val_loss: 16.9779 - val_MinusLogProbMetric: 16.9779 - lr: 1.0417e-05 - 62s/epoch - 317ms/step
Epoch 726/1000
2023-09-27 04:40:37.387 
Epoch 726/1000 
	 loss: 16.4309, MinusLogProbMetric: 16.4309, val_loss: 16.9568, val_MinusLogProbMetric: 16.9568

Epoch 726: val_loss did not improve from 16.94832
196/196 - 63s - loss: 16.4309 - MinusLogProbMetric: 16.4309 - val_loss: 16.9568 - val_MinusLogProbMetric: 16.9568 - lr: 1.0417e-05 - 63s/epoch - 320ms/step
Epoch 727/1000
2023-09-27 04:41:40.110 
Epoch 727/1000 
	 loss: 16.4272, MinusLogProbMetric: 16.4272, val_loss: 16.9561, val_MinusLogProbMetric: 16.9561

Epoch 727: val_loss did not improve from 16.94832
196/196 - 63s - loss: 16.4272 - MinusLogProbMetric: 16.4272 - val_loss: 16.9561 - val_MinusLogProbMetric: 16.9561 - lr: 1.0417e-05 - 63s/epoch - 320ms/step
Epoch 728/1000
2023-09-27 04:42:43.662 
Epoch 728/1000 
	 loss: 16.4254, MinusLogProbMetric: 16.4254, val_loss: 16.9560, val_MinusLogProbMetric: 16.9560

Epoch 728: val_loss did not improve from 16.94832
196/196 - 64s - loss: 16.4254 - MinusLogProbMetric: 16.4254 - val_loss: 16.9560 - val_MinusLogProbMetric: 16.9560 - lr: 1.0417e-05 - 64s/epoch - 324ms/step
Epoch 729/1000
2023-09-27 04:43:46.693 
Epoch 729/1000 
	 loss: 16.4272, MinusLogProbMetric: 16.4272, val_loss: 16.9710, val_MinusLogProbMetric: 16.9710

Epoch 729: val_loss did not improve from 16.94832
196/196 - 63s - loss: 16.4272 - MinusLogProbMetric: 16.4272 - val_loss: 16.9710 - val_MinusLogProbMetric: 16.9710 - lr: 1.0417e-05 - 63s/epoch - 322ms/step
Epoch 730/1000
2023-09-27 04:44:49.315 
Epoch 730/1000 
	 loss: 16.4271, MinusLogProbMetric: 16.4271, val_loss: 16.9510, val_MinusLogProbMetric: 16.9510

Epoch 730: val_loss did not improve from 16.94832
196/196 - 63s - loss: 16.4271 - MinusLogProbMetric: 16.4271 - val_loss: 16.9510 - val_MinusLogProbMetric: 16.9510 - lr: 1.0417e-05 - 63s/epoch - 319ms/step
Epoch 731/1000
2023-09-27 04:45:51.715 
Epoch 731/1000 
	 loss: 16.4257, MinusLogProbMetric: 16.4257, val_loss: 16.9659, val_MinusLogProbMetric: 16.9659

Epoch 731: val_loss did not improve from 16.94832
196/196 - 62s - loss: 16.4257 - MinusLogProbMetric: 16.4257 - val_loss: 16.9659 - val_MinusLogProbMetric: 16.9659 - lr: 1.0417e-05 - 62s/epoch - 318ms/step
Epoch 732/1000
2023-09-27 04:46:54.529 
Epoch 732/1000 
	 loss: 16.4265, MinusLogProbMetric: 16.4265, val_loss: 16.9501, val_MinusLogProbMetric: 16.9501

Epoch 732: val_loss did not improve from 16.94832
196/196 - 63s - loss: 16.4265 - MinusLogProbMetric: 16.4265 - val_loss: 16.9501 - val_MinusLogProbMetric: 16.9501 - lr: 1.0417e-05 - 63s/epoch - 320ms/step
Epoch 733/1000
2023-09-27 04:47:57.813 
Epoch 733/1000 
	 loss: 16.4275, MinusLogProbMetric: 16.4275, val_loss: 16.9772, val_MinusLogProbMetric: 16.9772

Epoch 733: val_loss did not improve from 16.94832
196/196 - 63s - loss: 16.4275 - MinusLogProbMetric: 16.4275 - val_loss: 16.9772 - val_MinusLogProbMetric: 16.9772 - lr: 1.0417e-05 - 63s/epoch - 323ms/step
Epoch 734/1000
2023-09-27 04:49:01.509 
Epoch 734/1000 
	 loss: 16.4273, MinusLogProbMetric: 16.4273, val_loss: 16.9681, val_MinusLogProbMetric: 16.9681

Epoch 734: val_loss did not improve from 16.94832
196/196 - 64s - loss: 16.4273 - MinusLogProbMetric: 16.4273 - val_loss: 16.9681 - val_MinusLogProbMetric: 16.9681 - lr: 1.0417e-05 - 64s/epoch - 325ms/step
Epoch 735/1000
2023-09-27 04:50:04.868 
Epoch 735/1000 
	 loss: 16.4266, MinusLogProbMetric: 16.4266, val_loss: 16.9561, val_MinusLogProbMetric: 16.9561

Epoch 735: val_loss did not improve from 16.94832
196/196 - 63s - loss: 16.4266 - MinusLogProbMetric: 16.4266 - val_loss: 16.9561 - val_MinusLogProbMetric: 16.9561 - lr: 1.0417e-05 - 63s/epoch - 323ms/step
Epoch 736/1000
2023-09-27 04:51:07.253 
Epoch 736/1000 
	 loss: 16.4286, MinusLogProbMetric: 16.4286, val_loss: 16.9534, val_MinusLogProbMetric: 16.9534

Epoch 736: val_loss did not improve from 16.94832
196/196 - 62s - loss: 16.4286 - MinusLogProbMetric: 16.4286 - val_loss: 16.9534 - val_MinusLogProbMetric: 16.9534 - lr: 1.0417e-05 - 62s/epoch - 318ms/step
Epoch 737/1000
2023-09-27 04:52:10.145 
Epoch 737/1000 
	 loss: 16.4290, MinusLogProbMetric: 16.4290, val_loss: 16.9511, val_MinusLogProbMetric: 16.9511

Epoch 737: val_loss did not improve from 16.94832
196/196 - 63s - loss: 16.4290 - MinusLogProbMetric: 16.4290 - val_loss: 16.9511 - val_MinusLogProbMetric: 16.9511 - lr: 1.0417e-05 - 63s/epoch - 321ms/step
Epoch 738/1000
2023-09-27 04:53:11.936 
Epoch 738/1000 
	 loss: 16.4260, MinusLogProbMetric: 16.4260, val_loss: 16.9635, val_MinusLogProbMetric: 16.9635

Epoch 738: val_loss did not improve from 16.94832
196/196 - 62s - loss: 16.4260 - MinusLogProbMetric: 16.4260 - val_loss: 16.9635 - val_MinusLogProbMetric: 16.9635 - lr: 1.0417e-05 - 62s/epoch - 315ms/step
Epoch 739/1000
2023-09-27 04:54:16.404 
Epoch 739/1000 
	 loss: 16.4256, MinusLogProbMetric: 16.4256, val_loss: 16.9529, val_MinusLogProbMetric: 16.9529

Epoch 739: val_loss did not improve from 16.94832
196/196 - 64s - loss: 16.4256 - MinusLogProbMetric: 16.4256 - val_loss: 16.9529 - val_MinusLogProbMetric: 16.9529 - lr: 1.0417e-05 - 64s/epoch - 329ms/step
Epoch 740/1000
2023-09-27 04:55:18.644 
Epoch 740/1000 
	 loss: 16.4263, MinusLogProbMetric: 16.4263, val_loss: 16.9597, val_MinusLogProbMetric: 16.9597

Epoch 740: val_loss did not improve from 16.94832
196/196 - 62s - loss: 16.4263 - MinusLogProbMetric: 16.4263 - val_loss: 16.9597 - val_MinusLogProbMetric: 16.9597 - lr: 1.0417e-05 - 62s/epoch - 318ms/step
Epoch 741/1000
2023-09-27 04:56:21.743 
Epoch 741/1000 
	 loss: 16.4268, MinusLogProbMetric: 16.4268, val_loss: 16.9584, val_MinusLogProbMetric: 16.9584

Epoch 741: val_loss did not improve from 16.94832
196/196 - 63s - loss: 16.4268 - MinusLogProbMetric: 16.4268 - val_loss: 16.9584 - val_MinusLogProbMetric: 16.9584 - lr: 1.0417e-05 - 63s/epoch - 322ms/step
Epoch 742/1000
2023-09-27 04:57:25.184 
Epoch 742/1000 
	 loss: 16.4280, MinusLogProbMetric: 16.4280, val_loss: 16.9549, val_MinusLogProbMetric: 16.9549

Epoch 742: val_loss did not improve from 16.94832
196/196 - 63s - loss: 16.4280 - MinusLogProbMetric: 16.4280 - val_loss: 16.9549 - val_MinusLogProbMetric: 16.9549 - lr: 1.0417e-05 - 63s/epoch - 324ms/step
Epoch 743/1000
2023-09-27 04:58:24.842 
Epoch 743/1000 
	 loss: 16.4245, MinusLogProbMetric: 16.4245, val_loss: 16.9593, val_MinusLogProbMetric: 16.9593

Epoch 743: val_loss did not improve from 16.94832
196/196 - 60s - loss: 16.4245 - MinusLogProbMetric: 16.4245 - val_loss: 16.9593 - val_MinusLogProbMetric: 16.9593 - lr: 1.0417e-05 - 60s/epoch - 304ms/step
Epoch 744/1000
2023-09-27 04:59:29.473 
Epoch 744/1000 
	 loss: 16.4252, MinusLogProbMetric: 16.4252, val_loss: 16.9812, val_MinusLogProbMetric: 16.9812

Epoch 744: val_loss did not improve from 16.94832
196/196 - 65s - loss: 16.4252 - MinusLogProbMetric: 16.4252 - val_loss: 16.9812 - val_MinusLogProbMetric: 16.9812 - lr: 1.0417e-05 - 65s/epoch - 330ms/step
Epoch 745/1000
2023-09-27 05:00:31.848 
Epoch 745/1000 
	 loss: 16.4260, MinusLogProbMetric: 16.4260, val_loss: 16.9562, val_MinusLogProbMetric: 16.9562

Epoch 745: val_loss did not improve from 16.94832
196/196 - 62s - loss: 16.4260 - MinusLogProbMetric: 16.4260 - val_loss: 16.9562 - val_MinusLogProbMetric: 16.9562 - lr: 1.0417e-05 - 62s/epoch - 318ms/step
Epoch 746/1000
2023-09-27 05:01:34.767 
Epoch 746/1000 
	 loss: 16.4247, MinusLogProbMetric: 16.4247, val_loss: 16.9539, val_MinusLogProbMetric: 16.9539

Epoch 746: val_loss did not improve from 16.94832
196/196 - 63s - loss: 16.4247 - MinusLogProbMetric: 16.4247 - val_loss: 16.9539 - val_MinusLogProbMetric: 16.9539 - lr: 1.0417e-05 - 63s/epoch - 321ms/step
Epoch 747/1000
2023-09-27 05:02:37.684 
Epoch 747/1000 
	 loss: 16.4258, MinusLogProbMetric: 16.4258, val_loss: 16.9564, val_MinusLogProbMetric: 16.9564

Epoch 747: val_loss did not improve from 16.94832
196/196 - 63s - loss: 16.4258 - MinusLogProbMetric: 16.4258 - val_loss: 16.9564 - val_MinusLogProbMetric: 16.9564 - lr: 1.0417e-05 - 63s/epoch - 321ms/step
Epoch 748/1000
2023-09-27 05:03:40.336 
Epoch 748/1000 
	 loss: 16.4185, MinusLogProbMetric: 16.4185, val_loss: 16.9527, val_MinusLogProbMetric: 16.9527

Epoch 748: val_loss did not improve from 16.94832
196/196 - 63s - loss: 16.4185 - MinusLogProbMetric: 16.4185 - val_loss: 16.9527 - val_MinusLogProbMetric: 16.9527 - lr: 5.2083e-06 - 63s/epoch - 320ms/step
Epoch 749/1000
2023-09-27 05:04:42.671 
Epoch 749/1000 
	 loss: 16.4190, MinusLogProbMetric: 16.4190, val_loss: 16.9490, val_MinusLogProbMetric: 16.9490

Epoch 749: val_loss did not improve from 16.94832
196/196 - 62s - loss: 16.4190 - MinusLogProbMetric: 16.4190 - val_loss: 16.9490 - val_MinusLogProbMetric: 16.9490 - lr: 5.2083e-06 - 62s/epoch - 318ms/step
Epoch 750/1000
2023-09-27 05:05:45.652 
Epoch 750/1000 
	 loss: 16.4192, MinusLogProbMetric: 16.4192, val_loss: 16.9476, val_MinusLogProbMetric: 16.9476

Epoch 750: val_loss improved from 16.94832 to 16.94761, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_294/weights/best_weights.h5
196/196 - 64s - loss: 16.4192 - MinusLogProbMetric: 16.4192 - val_loss: 16.9476 - val_MinusLogProbMetric: 16.9476 - lr: 5.2083e-06 - 64s/epoch - 328ms/step
Epoch 751/1000
2023-09-27 05:06:49.590 
Epoch 751/1000 
	 loss: 16.4181, MinusLogProbMetric: 16.4181, val_loss: 16.9490, val_MinusLogProbMetric: 16.9490

Epoch 751: val_loss did not improve from 16.94761
196/196 - 63s - loss: 16.4181 - MinusLogProbMetric: 16.4181 - val_loss: 16.9490 - val_MinusLogProbMetric: 16.9490 - lr: 5.2083e-06 - 63s/epoch - 320ms/step
Epoch 752/1000
2023-09-27 05:07:52.527 
Epoch 752/1000 
	 loss: 16.4173, MinusLogProbMetric: 16.4173, val_loss: 16.9517, val_MinusLogProbMetric: 16.9517

Epoch 752: val_loss did not improve from 16.94761
196/196 - 63s - loss: 16.4173 - MinusLogProbMetric: 16.4173 - val_loss: 16.9517 - val_MinusLogProbMetric: 16.9517 - lr: 5.2083e-06 - 63s/epoch - 321ms/step
Epoch 753/1000
2023-09-27 05:08:55.247 
Epoch 753/1000 
	 loss: 16.4181, MinusLogProbMetric: 16.4181, val_loss: 16.9506, val_MinusLogProbMetric: 16.9506

Epoch 753: val_loss did not improve from 16.94761
196/196 - 63s - loss: 16.4181 - MinusLogProbMetric: 16.4181 - val_loss: 16.9506 - val_MinusLogProbMetric: 16.9506 - lr: 5.2083e-06 - 63s/epoch - 320ms/step
Epoch 754/1000
2023-09-27 05:09:58.583 
Epoch 754/1000 
	 loss: 16.4176, MinusLogProbMetric: 16.4176, val_loss: 16.9598, val_MinusLogProbMetric: 16.9598

Epoch 754: val_loss did not improve from 16.94761
196/196 - 63s - loss: 16.4176 - MinusLogProbMetric: 16.4176 - val_loss: 16.9598 - val_MinusLogProbMetric: 16.9598 - lr: 5.2083e-06 - 63s/epoch - 323ms/step
Epoch 755/1000
2023-09-27 05:11:01.072 
Epoch 755/1000 
	 loss: 16.4193, MinusLogProbMetric: 16.4193, val_loss: 16.9497, val_MinusLogProbMetric: 16.9497

Epoch 755: val_loss did not improve from 16.94761
196/196 - 62s - loss: 16.4193 - MinusLogProbMetric: 16.4193 - val_loss: 16.9497 - val_MinusLogProbMetric: 16.9497 - lr: 5.2083e-06 - 62s/epoch - 319ms/step
Epoch 756/1000
2023-09-27 05:12:03.334 
Epoch 756/1000 
	 loss: 16.4189, MinusLogProbMetric: 16.4189, val_loss: 16.9494, val_MinusLogProbMetric: 16.9494

Epoch 756: val_loss did not improve from 16.94761
196/196 - 62s - loss: 16.4189 - MinusLogProbMetric: 16.4189 - val_loss: 16.9494 - val_MinusLogProbMetric: 16.9494 - lr: 5.2083e-06 - 62s/epoch - 318ms/step
Epoch 757/1000
2023-09-27 05:13:06.500 
Epoch 757/1000 
	 loss: 16.4183, MinusLogProbMetric: 16.4183, val_loss: 16.9618, val_MinusLogProbMetric: 16.9618

Epoch 757: val_loss did not improve from 16.94761
196/196 - 63s - loss: 16.4183 - MinusLogProbMetric: 16.4183 - val_loss: 16.9618 - val_MinusLogProbMetric: 16.9618 - lr: 5.2083e-06 - 63s/epoch - 322ms/step
Epoch 758/1000
2023-09-27 05:14:09.819 
Epoch 758/1000 
	 loss: 16.4181, MinusLogProbMetric: 16.4181, val_loss: 16.9469, val_MinusLogProbMetric: 16.9469

Epoch 758: val_loss improved from 16.94761 to 16.94687, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_294/weights/best_weights.h5
196/196 - 65s - loss: 16.4181 - MinusLogProbMetric: 16.4181 - val_loss: 16.9469 - val_MinusLogProbMetric: 16.9469 - lr: 5.2083e-06 - 65s/epoch - 329ms/step
Epoch 759/1000
2023-09-27 05:15:13.969 
Epoch 759/1000 
	 loss: 16.4184, MinusLogProbMetric: 16.4184, val_loss: 16.9594, val_MinusLogProbMetric: 16.9594

Epoch 759: val_loss did not improve from 16.94687
196/196 - 63s - loss: 16.4184 - MinusLogProbMetric: 16.4184 - val_loss: 16.9594 - val_MinusLogProbMetric: 16.9594 - lr: 5.2083e-06 - 63s/epoch - 321ms/step
Epoch 760/1000
2023-09-27 05:16:17.170 
Epoch 760/1000 
	 loss: 16.4190, MinusLogProbMetric: 16.4190, val_loss: 16.9533, val_MinusLogProbMetric: 16.9533

Epoch 760: val_loss did not improve from 16.94687
196/196 - 63s - loss: 16.4190 - MinusLogProbMetric: 16.4190 - val_loss: 16.9533 - val_MinusLogProbMetric: 16.9533 - lr: 5.2083e-06 - 63s/epoch - 322ms/step
Epoch 761/1000
2023-09-27 05:17:19.630 
Epoch 761/1000 
	 loss: 16.4192, MinusLogProbMetric: 16.4192, val_loss: 16.9476, val_MinusLogProbMetric: 16.9476

Epoch 761: val_loss did not improve from 16.94687
196/196 - 62s - loss: 16.4192 - MinusLogProbMetric: 16.4192 - val_loss: 16.9476 - val_MinusLogProbMetric: 16.9476 - lr: 5.2083e-06 - 62s/epoch - 319ms/step
Epoch 762/1000
2023-09-27 05:18:22.264 
Epoch 762/1000 
	 loss: 16.4175, MinusLogProbMetric: 16.4175, val_loss: 16.9498, val_MinusLogProbMetric: 16.9498

Epoch 762: val_loss did not improve from 16.94687
196/196 - 63s - loss: 16.4175 - MinusLogProbMetric: 16.4175 - val_loss: 16.9498 - val_MinusLogProbMetric: 16.9498 - lr: 5.2083e-06 - 63s/epoch - 320ms/step
Epoch 763/1000
2023-09-27 05:19:25.912 
Epoch 763/1000 
	 loss: 16.4177, MinusLogProbMetric: 16.4177, val_loss: 16.9485, val_MinusLogProbMetric: 16.9485

Epoch 763: val_loss did not improve from 16.94687
196/196 - 64s - loss: 16.4177 - MinusLogProbMetric: 16.4177 - val_loss: 16.9485 - val_MinusLogProbMetric: 16.9485 - lr: 5.2083e-06 - 64s/epoch - 325ms/step
Epoch 764/1000
2023-09-27 05:20:29.036 
Epoch 764/1000 
	 loss: 16.4177, MinusLogProbMetric: 16.4177, val_loss: 16.9488, val_MinusLogProbMetric: 16.9488

Epoch 764: val_loss did not improve from 16.94687
196/196 - 63s - loss: 16.4177 - MinusLogProbMetric: 16.4177 - val_loss: 16.9488 - val_MinusLogProbMetric: 16.9488 - lr: 5.2083e-06 - 63s/epoch - 322ms/step
Epoch 765/1000
2023-09-27 05:21:30.508 
Epoch 765/1000 
	 loss: 16.4186, MinusLogProbMetric: 16.4186, val_loss: 16.9501, val_MinusLogProbMetric: 16.9501

Epoch 765: val_loss did not improve from 16.94687
196/196 - 61s - loss: 16.4186 - MinusLogProbMetric: 16.4186 - val_loss: 16.9501 - val_MinusLogProbMetric: 16.9501 - lr: 5.2083e-06 - 61s/epoch - 314ms/step
Epoch 766/1000
2023-09-27 05:22:29.981 
Epoch 766/1000 
	 loss: 16.4180, MinusLogProbMetric: 16.4180, val_loss: 16.9467, val_MinusLogProbMetric: 16.9467

Epoch 766: val_loss improved from 16.94687 to 16.94666, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_294/weights/best_weights.h5
196/196 - 60s - loss: 16.4180 - MinusLogProbMetric: 16.4180 - val_loss: 16.9467 - val_MinusLogProbMetric: 16.9467 - lr: 5.2083e-06 - 60s/epoch - 308ms/step
Epoch 767/1000
2023-09-27 05:23:24.488 
Epoch 767/1000 
	 loss: 16.4184, MinusLogProbMetric: 16.4184, val_loss: 16.9476, val_MinusLogProbMetric: 16.9476

Epoch 767: val_loss did not improve from 16.94666
196/196 - 54s - loss: 16.4184 - MinusLogProbMetric: 16.4184 - val_loss: 16.9476 - val_MinusLogProbMetric: 16.9476 - lr: 5.2083e-06 - 54s/epoch - 274ms/step
Epoch 768/1000
2023-09-27 05:24:27.120 
Epoch 768/1000 
	 loss: 16.4183, MinusLogProbMetric: 16.4183, val_loss: 16.9546, val_MinusLogProbMetric: 16.9546

Epoch 768: val_loss did not improve from 16.94666
196/196 - 63s - loss: 16.4183 - MinusLogProbMetric: 16.4183 - val_loss: 16.9546 - val_MinusLogProbMetric: 16.9546 - lr: 5.2083e-06 - 63s/epoch - 320ms/step
Epoch 769/1000
2023-09-27 05:25:30.601 
Epoch 769/1000 
	 loss: 16.4184, MinusLogProbMetric: 16.4184, val_loss: 16.9538, val_MinusLogProbMetric: 16.9538

Epoch 769: val_loss did not improve from 16.94666
196/196 - 63s - loss: 16.4184 - MinusLogProbMetric: 16.4184 - val_loss: 16.9538 - val_MinusLogProbMetric: 16.9538 - lr: 5.2083e-06 - 63s/epoch - 324ms/step
Epoch 770/1000
2023-09-27 05:26:34.046 
Epoch 770/1000 
	 loss: 16.4177, MinusLogProbMetric: 16.4177, val_loss: 16.9500, val_MinusLogProbMetric: 16.9500

Epoch 770: val_loss did not improve from 16.94666
196/196 - 63s - loss: 16.4177 - MinusLogProbMetric: 16.4177 - val_loss: 16.9500 - val_MinusLogProbMetric: 16.9500 - lr: 5.2083e-06 - 63s/epoch - 324ms/step
Epoch 771/1000
2023-09-27 05:27:38.515 
Epoch 771/1000 
	 loss: 16.4190, MinusLogProbMetric: 16.4190, val_loss: 16.9481, val_MinusLogProbMetric: 16.9481

Epoch 771: val_loss did not improve from 16.94666
196/196 - 64s - loss: 16.4190 - MinusLogProbMetric: 16.4190 - val_loss: 16.9481 - val_MinusLogProbMetric: 16.9481 - lr: 5.2083e-06 - 64s/epoch - 329ms/step
Epoch 772/1000
2023-09-27 05:28:42.537 
Epoch 772/1000 
	 loss: 16.4170, MinusLogProbMetric: 16.4170, val_loss: 16.9498, val_MinusLogProbMetric: 16.9498

Epoch 772: val_loss did not improve from 16.94666
196/196 - 64s - loss: 16.4170 - MinusLogProbMetric: 16.4170 - val_loss: 16.9498 - val_MinusLogProbMetric: 16.9498 - lr: 5.2083e-06 - 64s/epoch - 327ms/step
Epoch 773/1000
2023-09-27 05:29:46.254 
Epoch 773/1000 
	 loss: 16.4185, MinusLogProbMetric: 16.4185, val_loss: 16.9542, val_MinusLogProbMetric: 16.9542

Epoch 773: val_loss did not improve from 16.94666
196/196 - 64s - loss: 16.4185 - MinusLogProbMetric: 16.4185 - val_loss: 16.9542 - val_MinusLogProbMetric: 16.9542 - lr: 5.2083e-06 - 64s/epoch - 325ms/step
Epoch 774/1000
2023-09-27 05:30:49.947 
Epoch 774/1000 
	 loss: 16.4167, MinusLogProbMetric: 16.4167, val_loss: 16.9513, val_MinusLogProbMetric: 16.9513

Epoch 774: val_loss did not improve from 16.94666
196/196 - 64s - loss: 16.4167 - MinusLogProbMetric: 16.4167 - val_loss: 16.9513 - val_MinusLogProbMetric: 16.9513 - lr: 5.2083e-06 - 64s/epoch - 325ms/step
Epoch 775/1000
2023-09-27 05:31:51.024 
Epoch 775/1000 
	 loss: 16.4174, MinusLogProbMetric: 16.4174, val_loss: 16.9488, val_MinusLogProbMetric: 16.9488

Epoch 775: val_loss did not improve from 16.94666
196/196 - 61s - loss: 16.4174 - MinusLogProbMetric: 16.4174 - val_loss: 16.9488 - val_MinusLogProbMetric: 16.9488 - lr: 5.2083e-06 - 61s/epoch - 312ms/step
Epoch 776/1000
2023-09-27 05:32:48.732 
Epoch 776/1000 
	 loss: 16.4171, MinusLogProbMetric: 16.4171, val_loss: 16.9647, val_MinusLogProbMetric: 16.9647

Epoch 776: val_loss did not improve from 16.94666
196/196 - 58s - loss: 16.4171 - MinusLogProbMetric: 16.4171 - val_loss: 16.9647 - val_MinusLogProbMetric: 16.9647 - lr: 5.2083e-06 - 58s/epoch - 294ms/step
Epoch 777/1000
2023-09-27 05:33:40.280 
Epoch 777/1000 
	 loss: 16.4178, MinusLogProbMetric: 16.4178, val_loss: 16.9509, val_MinusLogProbMetric: 16.9509

Epoch 777: val_loss did not improve from 16.94666
196/196 - 52s - loss: 16.4178 - MinusLogProbMetric: 16.4178 - val_loss: 16.9509 - val_MinusLogProbMetric: 16.9509 - lr: 5.2083e-06 - 52s/epoch - 263ms/step
Epoch 778/1000
2023-09-27 05:34:38.168 
Epoch 778/1000 
	 loss: 16.4178, MinusLogProbMetric: 16.4178, val_loss: 16.9472, val_MinusLogProbMetric: 16.9472

Epoch 778: val_loss did not improve from 16.94666
196/196 - 58s - loss: 16.4178 - MinusLogProbMetric: 16.4178 - val_loss: 16.9472 - val_MinusLogProbMetric: 16.9472 - lr: 5.2083e-06 - 58s/epoch - 295ms/step
Epoch 779/1000
2023-09-27 05:35:33.191 
Epoch 779/1000 
	 loss: 16.4183, MinusLogProbMetric: 16.4183, val_loss: 16.9505, val_MinusLogProbMetric: 16.9505

Epoch 779: val_loss did not improve from 16.94666
196/196 - 55s - loss: 16.4183 - MinusLogProbMetric: 16.4183 - val_loss: 16.9505 - val_MinusLogProbMetric: 16.9505 - lr: 5.2083e-06 - 55s/epoch - 281ms/step
Epoch 780/1000
2023-09-27 05:36:24.858 
Epoch 780/1000 
	 loss: 16.4176, MinusLogProbMetric: 16.4176, val_loss: 16.9555, val_MinusLogProbMetric: 16.9555

Epoch 780: val_loss did not improve from 16.94666
196/196 - 52s - loss: 16.4176 - MinusLogProbMetric: 16.4176 - val_loss: 16.9555 - val_MinusLogProbMetric: 16.9555 - lr: 5.2083e-06 - 52s/epoch - 264ms/step
Epoch 781/1000
2023-09-27 05:37:24.798 
Epoch 781/1000 
	 loss: 16.4186, MinusLogProbMetric: 16.4186, val_loss: 16.9498, val_MinusLogProbMetric: 16.9498

Epoch 781: val_loss did not improve from 16.94666
196/196 - 60s - loss: 16.4186 - MinusLogProbMetric: 16.4186 - val_loss: 16.9498 - val_MinusLogProbMetric: 16.9498 - lr: 5.2083e-06 - 60s/epoch - 306ms/step
Epoch 782/1000
2023-09-27 05:38:24.927 
Epoch 782/1000 
	 loss: 16.4181, MinusLogProbMetric: 16.4181, val_loss: 16.9543, val_MinusLogProbMetric: 16.9543

Epoch 782: val_loss did not improve from 16.94666
196/196 - 60s - loss: 16.4181 - MinusLogProbMetric: 16.4181 - val_loss: 16.9543 - val_MinusLogProbMetric: 16.9543 - lr: 5.2083e-06 - 60s/epoch - 307ms/step
Epoch 783/1000
2023-09-27 05:39:26.825 
Epoch 783/1000 
	 loss: 16.4180, MinusLogProbMetric: 16.4180, val_loss: 16.9469, val_MinusLogProbMetric: 16.9469

Epoch 783: val_loss did not improve from 16.94666
196/196 - 62s - loss: 16.4180 - MinusLogProbMetric: 16.4180 - val_loss: 16.9469 - val_MinusLogProbMetric: 16.9469 - lr: 5.2083e-06 - 62s/epoch - 316ms/step
Epoch 784/1000
2023-09-27 05:40:28.701 
Epoch 784/1000 
	 loss: 16.4176, MinusLogProbMetric: 16.4176, val_loss: 16.9512, val_MinusLogProbMetric: 16.9512

Epoch 784: val_loss did not improve from 16.94666
196/196 - 62s - loss: 16.4176 - MinusLogProbMetric: 16.4176 - val_loss: 16.9512 - val_MinusLogProbMetric: 16.9512 - lr: 5.2083e-06 - 62s/epoch - 316ms/step
Epoch 785/1000
2023-09-27 05:41:30.540 
Epoch 785/1000 
	 loss: 16.4171, MinusLogProbMetric: 16.4171, val_loss: 16.9497, val_MinusLogProbMetric: 16.9497

Epoch 785: val_loss did not improve from 16.94666
196/196 - 62s - loss: 16.4171 - MinusLogProbMetric: 16.4171 - val_loss: 16.9497 - val_MinusLogProbMetric: 16.9497 - lr: 5.2083e-06 - 62s/epoch - 315ms/step
Epoch 786/1000
2023-09-27 05:42:34.069 
Epoch 786/1000 
	 loss: 16.4166, MinusLogProbMetric: 16.4166, val_loss: 16.9499, val_MinusLogProbMetric: 16.9499

Epoch 786: val_loss did not improve from 16.94666
196/196 - 64s - loss: 16.4166 - MinusLogProbMetric: 16.4166 - val_loss: 16.9499 - val_MinusLogProbMetric: 16.9499 - lr: 5.2083e-06 - 64s/epoch - 324ms/step
Epoch 787/1000
2023-09-27 05:43:36.141 
Epoch 787/1000 
	 loss: 16.4181, MinusLogProbMetric: 16.4181, val_loss: 16.9474, val_MinusLogProbMetric: 16.9474

Epoch 787: val_loss did not improve from 16.94666
196/196 - 62s - loss: 16.4181 - MinusLogProbMetric: 16.4181 - val_loss: 16.9474 - val_MinusLogProbMetric: 16.9474 - lr: 5.2083e-06 - 62s/epoch - 317ms/step
Epoch 788/1000
2023-09-27 05:44:38.667 
Epoch 788/1000 
	 loss: 16.4173, MinusLogProbMetric: 16.4173, val_loss: 16.9516, val_MinusLogProbMetric: 16.9516

Epoch 788: val_loss did not improve from 16.94666
196/196 - 63s - loss: 16.4173 - MinusLogProbMetric: 16.4173 - val_loss: 16.9516 - val_MinusLogProbMetric: 16.9516 - lr: 5.2083e-06 - 63s/epoch - 319ms/step
Epoch 789/1000
2023-09-27 05:45:41.135 
Epoch 789/1000 
	 loss: 16.4168, MinusLogProbMetric: 16.4168, val_loss: 16.9460, val_MinusLogProbMetric: 16.9460

Epoch 789: val_loss improved from 16.94666 to 16.94601, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_294/weights/best_weights.h5
196/196 - 64s - loss: 16.4168 - MinusLogProbMetric: 16.4168 - val_loss: 16.9460 - val_MinusLogProbMetric: 16.9460 - lr: 5.2083e-06 - 64s/epoch - 324ms/step
Epoch 790/1000
2023-09-27 05:46:44.392 
Epoch 790/1000 
	 loss: 16.4162, MinusLogProbMetric: 16.4162, val_loss: 16.9629, val_MinusLogProbMetric: 16.9629

Epoch 790: val_loss did not improve from 16.94601
196/196 - 62s - loss: 16.4162 - MinusLogProbMetric: 16.4162 - val_loss: 16.9629 - val_MinusLogProbMetric: 16.9629 - lr: 5.2083e-06 - 62s/epoch - 317ms/step
Epoch 791/1000
2023-09-27 05:47:47.383 
Epoch 791/1000 
	 loss: 16.4181, MinusLogProbMetric: 16.4181, val_loss: 16.9485, val_MinusLogProbMetric: 16.9485

Epoch 791: val_loss did not improve from 16.94601
196/196 - 63s - loss: 16.4181 - MinusLogProbMetric: 16.4181 - val_loss: 16.9485 - val_MinusLogProbMetric: 16.9485 - lr: 5.2083e-06 - 63s/epoch - 321ms/step
Epoch 792/1000
2023-09-27 05:48:50.655 
Epoch 792/1000 
	 loss: 16.4164, MinusLogProbMetric: 16.4164, val_loss: 16.9520, val_MinusLogProbMetric: 16.9520

Epoch 792: val_loss did not improve from 16.94601
196/196 - 63s - loss: 16.4164 - MinusLogProbMetric: 16.4164 - val_loss: 16.9520 - val_MinusLogProbMetric: 16.9520 - lr: 5.2083e-06 - 63s/epoch - 323ms/step
Epoch 793/1000
2023-09-27 05:49:52.652 
Epoch 793/1000 
	 loss: 16.4178, MinusLogProbMetric: 16.4178, val_loss: 16.9499, val_MinusLogProbMetric: 16.9499

Epoch 793: val_loss did not improve from 16.94601
196/196 - 62s - loss: 16.4178 - MinusLogProbMetric: 16.4178 - val_loss: 16.9499 - val_MinusLogProbMetric: 16.9499 - lr: 5.2083e-06 - 62s/epoch - 316ms/step
Epoch 794/1000
2023-09-27 05:50:54.845 
Epoch 794/1000 
	 loss: 16.4155, MinusLogProbMetric: 16.4155, val_loss: 16.9519, val_MinusLogProbMetric: 16.9519

Epoch 794: val_loss did not improve from 16.94601
196/196 - 62s - loss: 16.4155 - MinusLogProbMetric: 16.4155 - val_loss: 16.9519 - val_MinusLogProbMetric: 16.9519 - lr: 5.2083e-06 - 62s/epoch - 317ms/step
Epoch 795/1000
2023-09-27 05:51:57.245 
Epoch 795/1000 
	 loss: 16.4178, MinusLogProbMetric: 16.4178, val_loss: 16.9490, val_MinusLogProbMetric: 16.9490

Epoch 795: val_loss did not improve from 16.94601
196/196 - 62s - loss: 16.4178 - MinusLogProbMetric: 16.4178 - val_loss: 16.9490 - val_MinusLogProbMetric: 16.9490 - lr: 5.2083e-06 - 62s/epoch - 318ms/step
Epoch 796/1000
2023-09-27 05:52:59.711 
Epoch 796/1000 
	 loss: 16.4188, MinusLogProbMetric: 16.4188, val_loss: 16.9482, val_MinusLogProbMetric: 16.9482

Epoch 796: val_loss did not improve from 16.94601
196/196 - 62s - loss: 16.4188 - MinusLogProbMetric: 16.4188 - val_loss: 16.9482 - val_MinusLogProbMetric: 16.9482 - lr: 5.2083e-06 - 62s/epoch - 319ms/step
Epoch 797/1000
2023-09-27 05:54:02.844 
Epoch 797/1000 
	 loss: 16.4169, MinusLogProbMetric: 16.4169, val_loss: 16.9478, val_MinusLogProbMetric: 16.9478

Epoch 797: val_loss did not improve from 16.94601
196/196 - 63s - loss: 16.4169 - MinusLogProbMetric: 16.4169 - val_loss: 16.9478 - val_MinusLogProbMetric: 16.9478 - lr: 5.2083e-06 - 63s/epoch - 322ms/step
Epoch 798/1000
2023-09-27 05:55:05.625 
Epoch 798/1000 
	 loss: 16.4189, MinusLogProbMetric: 16.4189, val_loss: 16.9520, val_MinusLogProbMetric: 16.9520

Epoch 798: val_loss did not improve from 16.94601
196/196 - 63s - loss: 16.4189 - MinusLogProbMetric: 16.4189 - val_loss: 16.9520 - val_MinusLogProbMetric: 16.9520 - lr: 5.2083e-06 - 63s/epoch - 320ms/step
Epoch 799/1000
2023-09-27 05:56:08.041 
Epoch 799/1000 
	 loss: 16.4171, MinusLogProbMetric: 16.4171, val_loss: 16.9483, val_MinusLogProbMetric: 16.9483

Epoch 799: val_loss did not improve from 16.94601
196/196 - 62s - loss: 16.4171 - MinusLogProbMetric: 16.4171 - val_loss: 16.9483 - val_MinusLogProbMetric: 16.9483 - lr: 5.2083e-06 - 62s/epoch - 318ms/step
Epoch 800/1000
2023-09-27 05:57:09.901 
Epoch 800/1000 
	 loss: 16.4178, MinusLogProbMetric: 16.4178, val_loss: 16.9474, val_MinusLogProbMetric: 16.9474

Epoch 800: val_loss did not improve from 16.94601
196/196 - 62s - loss: 16.4178 - MinusLogProbMetric: 16.4178 - val_loss: 16.9474 - val_MinusLogProbMetric: 16.9474 - lr: 5.2083e-06 - 62s/epoch - 316ms/step
Epoch 801/1000
2023-09-27 05:58:12.105 
Epoch 801/1000 
	 loss: 16.4163, MinusLogProbMetric: 16.4163, val_loss: 16.9520, val_MinusLogProbMetric: 16.9520

Epoch 801: val_loss did not improve from 16.94601
196/196 - 62s - loss: 16.4163 - MinusLogProbMetric: 16.4163 - val_loss: 16.9520 - val_MinusLogProbMetric: 16.9520 - lr: 5.2083e-06 - 62s/epoch - 317ms/step
Epoch 802/1000
2023-09-27 05:59:14.201 
Epoch 802/1000 
	 loss: 16.4167, MinusLogProbMetric: 16.4167, val_loss: 16.9514, val_MinusLogProbMetric: 16.9514

Epoch 802: val_loss did not improve from 16.94601
196/196 - 62s - loss: 16.4167 - MinusLogProbMetric: 16.4167 - val_loss: 16.9514 - val_MinusLogProbMetric: 16.9514 - lr: 5.2083e-06 - 62s/epoch - 317ms/step
Epoch 803/1000
2023-09-27 06:00:15.632 
Epoch 803/1000 
	 loss: 16.4177, MinusLogProbMetric: 16.4177, val_loss: 16.9529, val_MinusLogProbMetric: 16.9529

Epoch 803: val_loss did not improve from 16.94601
196/196 - 61s - loss: 16.4177 - MinusLogProbMetric: 16.4177 - val_loss: 16.9529 - val_MinusLogProbMetric: 16.9529 - lr: 5.2083e-06 - 61s/epoch - 313ms/step
Epoch 804/1000
2023-09-27 06:01:18.218 
Epoch 804/1000 
	 loss: 16.4161, MinusLogProbMetric: 16.4161, val_loss: 16.9508, val_MinusLogProbMetric: 16.9508

Epoch 804: val_loss did not improve from 16.94601
196/196 - 63s - loss: 16.4161 - MinusLogProbMetric: 16.4161 - val_loss: 16.9508 - val_MinusLogProbMetric: 16.9508 - lr: 5.2083e-06 - 63s/epoch - 319ms/step
Epoch 805/1000
2023-09-27 06:02:21.642 
Epoch 805/1000 
	 loss: 16.4176, MinusLogProbMetric: 16.4176, val_loss: 16.9500, val_MinusLogProbMetric: 16.9500

Epoch 805: val_loss did not improve from 16.94601
196/196 - 63s - loss: 16.4176 - MinusLogProbMetric: 16.4176 - val_loss: 16.9500 - val_MinusLogProbMetric: 16.9500 - lr: 5.2083e-06 - 63s/epoch - 324ms/step
Epoch 806/1000
2023-09-27 06:03:24.131 
Epoch 806/1000 
	 loss: 16.4168, MinusLogProbMetric: 16.4168, val_loss: 16.9503, val_MinusLogProbMetric: 16.9503

Epoch 806: val_loss did not improve from 16.94601
196/196 - 62s - loss: 16.4168 - MinusLogProbMetric: 16.4168 - val_loss: 16.9503 - val_MinusLogProbMetric: 16.9503 - lr: 5.2083e-06 - 62s/epoch - 319ms/step
Epoch 807/1000
2023-09-27 06:04:26.516 
Epoch 807/1000 
	 loss: 16.4178, MinusLogProbMetric: 16.4178, val_loss: 16.9477, val_MinusLogProbMetric: 16.9477

Epoch 807: val_loss did not improve from 16.94601
196/196 - 62s - loss: 16.4178 - MinusLogProbMetric: 16.4178 - val_loss: 16.9477 - val_MinusLogProbMetric: 16.9477 - lr: 5.2083e-06 - 62s/epoch - 318ms/step
Epoch 808/1000
2023-09-27 06:05:28.990 
Epoch 808/1000 
	 loss: 16.4168, MinusLogProbMetric: 16.4168, val_loss: 16.9490, val_MinusLogProbMetric: 16.9490

Epoch 808: val_loss did not improve from 16.94601
196/196 - 62s - loss: 16.4168 - MinusLogProbMetric: 16.4168 - val_loss: 16.9490 - val_MinusLogProbMetric: 16.9490 - lr: 5.2083e-06 - 62s/epoch - 319ms/step
Epoch 809/1000
2023-09-27 06:06:31.545 
Epoch 809/1000 
	 loss: 16.4171, MinusLogProbMetric: 16.4171, val_loss: 16.9528, val_MinusLogProbMetric: 16.9528

Epoch 809: val_loss did not improve from 16.94601
196/196 - 63s - loss: 16.4171 - MinusLogProbMetric: 16.4171 - val_loss: 16.9528 - val_MinusLogProbMetric: 16.9528 - lr: 5.2083e-06 - 63s/epoch - 319ms/step
Epoch 810/1000
2023-09-27 06:07:33.867 
Epoch 810/1000 
	 loss: 16.4166, MinusLogProbMetric: 16.4166, val_loss: 16.9488, val_MinusLogProbMetric: 16.9488

Epoch 810: val_loss did not improve from 16.94601
196/196 - 62s - loss: 16.4166 - MinusLogProbMetric: 16.4166 - val_loss: 16.9488 - val_MinusLogProbMetric: 16.9488 - lr: 5.2083e-06 - 62s/epoch - 318ms/step
Epoch 811/1000
2023-09-27 06:08:35.744 
Epoch 811/1000 
	 loss: 16.4157, MinusLogProbMetric: 16.4157, val_loss: 16.9496, val_MinusLogProbMetric: 16.9496

Epoch 811: val_loss did not improve from 16.94601
196/196 - 62s - loss: 16.4157 - MinusLogProbMetric: 16.4157 - val_loss: 16.9496 - val_MinusLogProbMetric: 16.9496 - lr: 5.2083e-06 - 62s/epoch - 316ms/step
Epoch 812/1000
2023-09-27 06:09:38.518 
Epoch 812/1000 
	 loss: 16.4167, MinusLogProbMetric: 16.4167, val_loss: 16.9458, val_MinusLogProbMetric: 16.9458

Epoch 812: val_loss improved from 16.94601 to 16.94576, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_294/weights/best_weights.h5
196/196 - 64s - loss: 16.4167 - MinusLogProbMetric: 16.4167 - val_loss: 16.9458 - val_MinusLogProbMetric: 16.9458 - lr: 5.2083e-06 - 64s/epoch - 327ms/step
Epoch 813/1000
2023-09-27 06:10:41.992 
Epoch 813/1000 
	 loss: 16.4164, MinusLogProbMetric: 16.4164, val_loss: 16.9504, val_MinusLogProbMetric: 16.9504

Epoch 813: val_loss did not improve from 16.94576
196/196 - 62s - loss: 16.4164 - MinusLogProbMetric: 16.4164 - val_loss: 16.9504 - val_MinusLogProbMetric: 16.9504 - lr: 5.2083e-06 - 62s/epoch - 317ms/step
Epoch 814/1000
2023-09-27 06:11:44.417 
Epoch 814/1000 
	 loss: 16.4163, MinusLogProbMetric: 16.4163, val_loss: 16.9483, val_MinusLogProbMetric: 16.9483

Epoch 814: val_loss did not improve from 16.94576
196/196 - 62s - loss: 16.4163 - MinusLogProbMetric: 16.4163 - val_loss: 16.9483 - val_MinusLogProbMetric: 16.9483 - lr: 5.2083e-06 - 62s/epoch - 318ms/step
Epoch 815/1000
2023-09-27 06:12:47.483 
Epoch 815/1000 
	 loss: 16.4180, MinusLogProbMetric: 16.4180, val_loss: 16.9531, val_MinusLogProbMetric: 16.9531

Epoch 815: val_loss did not improve from 16.94576
196/196 - 63s - loss: 16.4180 - MinusLogProbMetric: 16.4180 - val_loss: 16.9531 - val_MinusLogProbMetric: 16.9531 - lr: 5.2083e-06 - 63s/epoch - 322ms/step
Epoch 816/1000
2023-09-27 06:13:51.145 
Epoch 816/1000 
	 loss: 16.4170, MinusLogProbMetric: 16.4170, val_loss: 16.9471, val_MinusLogProbMetric: 16.9471

Epoch 816: val_loss did not improve from 16.94576
196/196 - 64s - loss: 16.4170 - MinusLogProbMetric: 16.4170 - val_loss: 16.9471 - val_MinusLogProbMetric: 16.9471 - lr: 5.2083e-06 - 64s/epoch - 325ms/step
Epoch 817/1000
2023-09-27 06:14:53.719 
Epoch 817/1000 
	 loss: 16.4161, MinusLogProbMetric: 16.4161, val_loss: 16.9536, val_MinusLogProbMetric: 16.9536

Epoch 817: val_loss did not improve from 16.94576
196/196 - 63s - loss: 16.4161 - MinusLogProbMetric: 16.4161 - val_loss: 16.9536 - val_MinusLogProbMetric: 16.9536 - lr: 5.2083e-06 - 63s/epoch - 319ms/step
Epoch 818/1000
2023-09-27 06:15:56.944 
Epoch 818/1000 
	 loss: 16.4164, MinusLogProbMetric: 16.4164, val_loss: 16.9480, val_MinusLogProbMetric: 16.9480

Epoch 818: val_loss did not improve from 16.94576
196/196 - 63s - loss: 16.4164 - MinusLogProbMetric: 16.4164 - val_loss: 16.9480 - val_MinusLogProbMetric: 16.9480 - lr: 5.2083e-06 - 63s/epoch - 323ms/step
Epoch 819/1000
2023-09-27 06:17:00.054 
Epoch 819/1000 
	 loss: 16.4159, MinusLogProbMetric: 16.4159, val_loss: 16.9490, val_MinusLogProbMetric: 16.9490

Epoch 819: val_loss did not improve from 16.94576
196/196 - 63s - loss: 16.4159 - MinusLogProbMetric: 16.4159 - val_loss: 16.9490 - val_MinusLogProbMetric: 16.9490 - lr: 5.2083e-06 - 63s/epoch - 322ms/step
Epoch 820/1000
2023-09-27 06:18:03.141 
Epoch 820/1000 
	 loss: 16.4159, MinusLogProbMetric: 16.4159, val_loss: 16.9480, val_MinusLogProbMetric: 16.9480

Epoch 820: val_loss did not improve from 16.94576
196/196 - 63s - loss: 16.4159 - MinusLogProbMetric: 16.4159 - val_loss: 16.9480 - val_MinusLogProbMetric: 16.9480 - lr: 5.2083e-06 - 63s/epoch - 322ms/step
Epoch 821/1000
2023-09-27 06:19:05.506 
Epoch 821/1000 
	 loss: 16.4163, MinusLogProbMetric: 16.4163, val_loss: 16.9532, val_MinusLogProbMetric: 16.9532

Epoch 821: val_loss did not improve from 16.94576
196/196 - 62s - loss: 16.4163 - MinusLogProbMetric: 16.4163 - val_loss: 16.9532 - val_MinusLogProbMetric: 16.9532 - lr: 5.2083e-06 - 62s/epoch - 318ms/step
Epoch 822/1000
2023-09-27 06:20:08.642 
Epoch 822/1000 
	 loss: 16.4167, MinusLogProbMetric: 16.4167, val_loss: 16.9492, val_MinusLogProbMetric: 16.9492

Epoch 822: val_loss did not improve from 16.94576
196/196 - 63s - loss: 16.4167 - MinusLogProbMetric: 16.4167 - val_loss: 16.9492 - val_MinusLogProbMetric: 16.9492 - lr: 5.2083e-06 - 63s/epoch - 322ms/step
Epoch 823/1000
2023-09-27 06:21:10.414 
Epoch 823/1000 
	 loss: 16.4164, MinusLogProbMetric: 16.4164, val_loss: 16.9478, val_MinusLogProbMetric: 16.9478

Epoch 823: val_loss did not improve from 16.94576
196/196 - 62s - loss: 16.4164 - MinusLogProbMetric: 16.4164 - val_loss: 16.9478 - val_MinusLogProbMetric: 16.9478 - lr: 5.2083e-06 - 62s/epoch - 315ms/step
Epoch 824/1000
2023-09-27 06:22:13.410 
Epoch 824/1000 
	 loss: 16.4166, MinusLogProbMetric: 16.4166, val_loss: 16.9504, val_MinusLogProbMetric: 16.9504

Epoch 824: val_loss did not improve from 16.94576
196/196 - 63s - loss: 16.4166 - MinusLogProbMetric: 16.4166 - val_loss: 16.9504 - val_MinusLogProbMetric: 16.9504 - lr: 5.2083e-06 - 63s/epoch - 321ms/step
Epoch 825/1000
2023-09-27 06:23:16.796 
Epoch 825/1000 
	 loss: 16.4158, MinusLogProbMetric: 16.4158, val_loss: 16.9535, val_MinusLogProbMetric: 16.9535

Epoch 825: val_loss did not improve from 16.94576
196/196 - 63s - loss: 16.4158 - MinusLogProbMetric: 16.4158 - val_loss: 16.9535 - val_MinusLogProbMetric: 16.9535 - lr: 5.2083e-06 - 63s/epoch - 323ms/step
Epoch 826/1000
2023-09-27 06:24:18.679 
Epoch 826/1000 
	 loss: 16.4156, MinusLogProbMetric: 16.4156, val_loss: 16.9484, val_MinusLogProbMetric: 16.9484

Epoch 826: val_loss did not improve from 16.94576
196/196 - 62s - loss: 16.4156 - MinusLogProbMetric: 16.4156 - val_loss: 16.9484 - val_MinusLogProbMetric: 16.9484 - lr: 5.2083e-06 - 62s/epoch - 316ms/step
Epoch 827/1000
2023-09-27 06:25:20.718 
Epoch 827/1000 
	 loss: 16.4155, MinusLogProbMetric: 16.4155, val_loss: 16.9498, val_MinusLogProbMetric: 16.9498

Epoch 827: val_loss did not improve from 16.94576
196/196 - 62s - loss: 16.4155 - MinusLogProbMetric: 16.4155 - val_loss: 16.9498 - val_MinusLogProbMetric: 16.9498 - lr: 5.2083e-06 - 62s/epoch - 317ms/step
Epoch 828/1000
2023-09-27 06:26:22.790 
Epoch 828/1000 
	 loss: 16.4169, MinusLogProbMetric: 16.4169, val_loss: 16.9486, val_MinusLogProbMetric: 16.9486

Epoch 828: val_loss did not improve from 16.94576
196/196 - 62s - loss: 16.4169 - MinusLogProbMetric: 16.4169 - val_loss: 16.9486 - val_MinusLogProbMetric: 16.9486 - lr: 5.2083e-06 - 62s/epoch - 317ms/step
Epoch 829/1000
2023-09-27 06:27:24.329 
Epoch 829/1000 
	 loss: 16.4166, MinusLogProbMetric: 16.4166, val_loss: 16.9535, val_MinusLogProbMetric: 16.9535

Epoch 829: val_loss did not improve from 16.94576
196/196 - 62s - loss: 16.4166 - MinusLogProbMetric: 16.4166 - val_loss: 16.9535 - val_MinusLogProbMetric: 16.9535 - lr: 5.2083e-06 - 62s/epoch - 314ms/step
Epoch 830/1000
2023-09-27 06:28:26.309 
Epoch 830/1000 
	 loss: 16.4163, MinusLogProbMetric: 16.4163, val_loss: 16.9480, val_MinusLogProbMetric: 16.9480

Epoch 830: val_loss did not improve from 16.94576
196/196 - 62s - loss: 16.4163 - MinusLogProbMetric: 16.4163 - val_loss: 16.9480 - val_MinusLogProbMetric: 16.9480 - lr: 5.2083e-06 - 62s/epoch - 316ms/step
Epoch 831/1000
2023-09-27 06:29:28.638 
Epoch 831/1000 
	 loss: 16.4170, MinusLogProbMetric: 16.4170, val_loss: 16.9482, val_MinusLogProbMetric: 16.9482

Epoch 831: val_loss did not improve from 16.94576
196/196 - 62s - loss: 16.4170 - MinusLogProbMetric: 16.4170 - val_loss: 16.9482 - val_MinusLogProbMetric: 16.9482 - lr: 5.2083e-06 - 62s/epoch - 318ms/step
Epoch 832/1000
2023-09-27 06:30:30.410 
Epoch 832/1000 
	 loss: 16.4154, MinusLogProbMetric: 16.4154, val_loss: 16.9488, val_MinusLogProbMetric: 16.9488

Epoch 832: val_loss did not improve from 16.94576
196/196 - 62s - loss: 16.4154 - MinusLogProbMetric: 16.4154 - val_loss: 16.9488 - val_MinusLogProbMetric: 16.9488 - lr: 5.2083e-06 - 62s/epoch - 315ms/step
Epoch 833/1000
2023-09-27 06:31:32.191 
Epoch 833/1000 
	 loss: 16.4155, MinusLogProbMetric: 16.4155, val_loss: 16.9491, val_MinusLogProbMetric: 16.9491

Epoch 833: val_loss did not improve from 16.94576
196/196 - 62s - loss: 16.4155 - MinusLogProbMetric: 16.4155 - val_loss: 16.9491 - val_MinusLogProbMetric: 16.9491 - lr: 5.2083e-06 - 62s/epoch - 315ms/step
Epoch 834/1000
2023-09-27 06:32:34.254 
Epoch 834/1000 
	 loss: 16.4158, MinusLogProbMetric: 16.4158, val_loss: 16.9592, val_MinusLogProbMetric: 16.9592

Epoch 834: val_loss did not improve from 16.94576
196/196 - 62s - loss: 16.4158 - MinusLogProbMetric: 16.4158 - val_loss: 16.9592 - val_MinusLogProbMetric: 16.9592 - lr: 5.2083e-06 - 62s/epoch - 317ms/step
Epoch 835/1000
2023-09-27 06:33:35.553 
Epoch 835/1000 
	 loss: 16.4154, MinusLogProbMetric: 16.4154, val_loss: 16.9504, val_MinusLogProbMetric: 16.9504

Epoch 835: val_loss did not improve from 16.94576
196/196 - 61s - loss: 16.4154 - MinusLogProbMetric: 16.4154 - val_loss: 16.9504 - val_MinusLogProbMetric: 16.9504 - lr: 5.2083e-06 - 61s/epoch - 313ms/step
Epoch 836/1000
2023-09-27 06:34:37.481 
Epoch 836/1000 
	 loss: 16.4167, MinusLogProbMetric: 16.4167, val_loss: 16.9500, val_MinusLogProbMetric: 16.9500

Epoch 836: val_loss did not improve from 16.94576
196/196 - 62s - loss: 16.4167 - MinusLogProbMetric: 16.4167 - val_loss: 16.9500 - val_MinusLogProbMetric: 16.9500 - lr: 5.2083e-06 - 62s/epoch - 316ms/step
Epoch 837/1000
2023-09-27 06:35:39.162 
Epoch 837/1000 
	 loss: 16.4173, MinusLogProbMetric: 16.4173, val_loss: 16.9513, val_MinusLogProbMetric: 16.9513

Epoch 837: val_loss did not improve from 16.94576
196/196 - 62s - loss: 16.4173 - MinusLogProbMetric: 16.4173 - val_loss: 16.9513 - val_MinusLogProbMetric: 16.9513 - lr: 5.2083e-06 - 62s/epoch - 315ms/step
Epoch 838/1000
2023-09-27 06:36:41.512 
Epoch 838/1000 
	 loss: 16.4165, MinusLogProbMetric: 16.4165, val_loss: 16.9489, val_MinusLogProbMetric: 16.9489

Epoch 838: val_loss did not improve from 16.94576
196/196 - 62s - loss: 16.4165 - MinusLogProbMetric: 16.4165 - val_loss: 16.9489 - val_MinusLogProbMetric: 16.9489 - lr: 5.2083e-06 - 62s/epoch - 318ms/step
Epoch 839/1000
2023-09-27 06:37:44.246 
Epoch 839/1000 
	 loss: 16.4161, MinusLogProbMetric: 16.4161, val_loss: 16.9484, val_MinusLogProbMetric: 16.9484

Epoch 839: val_loss did not improve from 16.94576
196/196 - 63s - loss: 16.4161 - MinusLogProbMetric: 16.4161 - val_loss: 16.9484 - val_MinusLogProbMetric: 16.9484 - lr: 5.2083e-06 - 63s/epoch - 320ms/step
Epoch 840/1000
2023-09-27 06:38:46.584 
Epoch 840/1000 
	 loss: 16.4155, MinusLogProbMetric: 16.4155, val_loss: 16.9522, val_MinusLogProbMetric: 16.9522

Epoch 840: val_loss did not improve from 16.94576
196/196 - 62s - loss: 16.4155 - MinusLogProbMetric: 16.4155 - val_loss: 16.9522 - val_MinusLogProbMetric: 16.9522 - lr: 5.2083e-06 - 62s/epoch - 318ms/step
Epoch 841/1000
2023-09-27 06:39:48.747 
Epoch 841/1000 
	 loss: 16.4164, MinusLogProbMetric: 16.4164, val_loss: 16.9490, val_MinusLogProbMetric: 16.9490

Epoch 841: val_loss did not improve from 16.94576
196/196 - 62s - loss: 16.4164 - MinusLogProbMetric: 16.4164 - val_loss: 16.9490 - val_MinusLogProbMetric: 16.9490 - lr: 5.2083e-06 - 62s/epoch - 317ms/step
Epoch 842/1000
2023-09-27 06:40:52.297 
Epoch 842/1000 
	 loss: 16.4161, MinusLogProbMetric: 16.4161, val_loss: 16.9467, val_MinusLogProbMetric: 16.9467

Epoch 842: val_loss did not improve from 16.94576
196/196 - 64s - loss: 16.4161 - MinusLogProbMetric: 16.4161 - val_loss: 16.9467 - val_MinusLogProbMetric: 16.9467 - lr: 5.2083e-06 - 64s/epoch - 324ms/step
Epoch 843/1000
2023-09-27 06:41:55.321 
Epoch 843/1000 
	 loss: 16.4151, MinusLogProbMetric: 16.4151, val_loss: 16.9494, val_MinusLogProbMetric: 16.9494

Epoch 843: val_loss did not improve from 16.94576
196/196 - 63s - loss: 16.4151 - MinusLogProbMetric: 16.4151 - val_loss: 16.9494 - val_MinusLogProbMetric: 16.9494 - lr: 5.2083e-06 - 63s/epoch - 322ms/step
Epoch 844/1000
2023-09-27 06:42:58.649 
Epoch 844/1000 
	 loss: 16.4166, MinusLogProbMetric: 16.4166, val_loss: 16.9512, val_MinusLogProbMetric: 16.9512

Epoch 844: val_loss did not improve from 16.94576
196/196 - 63s - loss: 16.4166 - MinusLogProbMetric: 16.4166 - val_loss: 16.9512 - val_MinusLogProbMetric: 16.9512 - lr: 5.2083e-06 - 63s/epoch - 323ms/step
Epoch 845/1000
2023-09-27 06:44:00.915 
Epoch 845/1000 
	 loss: 16.4154, MinusLogProbMetric: 16.4154, val_loss: 16.9480, val_MinusLogProbMetric: 16.9480

Epoch 845: val_loss did not improve from 16.94576
196/196 - 62s - loss: 16.4154 - MinusLogProbMetric: 16.4154 - val_loss: 16.9480 - val_MinusLogProbMetric: 16.9480 - lr: 5.2083e-06 - 62s/epoch - 318ms/step
Epoch 846/1000
2023-09-27 06:45:03.604 
Epoch 846/1000 
	 loss: 16.4146, MinusLogProbMetric: 16.4146, val_loss: 16.9475, val_MinusLogProbMetric: 16.9475

Epoch 846: val_loss did not improve from 16.94576
196/196 - 63s - loss: 16.4146 - MinusLogProbMetric: 16.4146 - val_loss: 16.9475 - val_MinusLogProbMetric: 16.9475 - lr: 5.2083e-06 - 63s/epoch - 320ms/step
Epoch 847/1000
2023-09-27 06:46:06.504 
Epoch 847/1000 
	 loss: 16.4165, MinusLogProbMetric: 16.4165, val_loss: 16.9545, val_MinusLogProbMetric: 16.9545

Epoch 847: val_loss did not improve from 16.94576
196/196 - 63s - loss: 16.4165 - MinusLogProbMetric: 16.4165 - val_loss: 16.9545 - val_MinusLogProbMetric: 16.9545 - lr: 5.2083e-06 - 63s/epoch - 321ms/step
Epoch 848/1000
2023-09-27 06:47:08.472 
Epoch 848/1000 
	 loss: 16.4167, MinusLogProbMetric: 16.4167, val_loss: 16.9477, val_MinusLogProbMetric: 16.9477

Epoch 848: val_loss did not improve from 16.94576
196/196 - 62s - loss: 16.4167 - MinusLogProbMetric: 16.4167 - val_loss: 16.9477 - val_MinusLogProbMetric: 16.9477 - lr: 5.2083e-06 - 62s/epoch - 316ms/step
Epoch 849/1000
2023-09-27 06:48:11.101 
Epoch 849/1000 
	 loss: 16.4150, MinusLogProbMetric: 16.4150, val_loss: 16.9517, val_MinusLogProbMetric: 16.9517

Epoch 849: val_loss did not improve from 16.94576
196/196 - 63s - loss: 16.4150 - MinusLogProbMetric: 16.4150 - val_loss: 16.9517 - val_MinusLogProbMetric: 16.9517 - lr: 5.2083e-06 - 63s/epoch - 320ms/step
Epoch 850/1000
2023-09-27 06:49:12.776 
Epoch 850/1000 
	 loss: 16.4155, MinusLogProbMetric: 16.4155, val_loss: 16.9480, val_MinusLogProbMetric: 16.9480

Epoch 850: val_loss did not improve from 16.94576
196/196 - 62s - loss: 16.4155 - MinusLogProbMetric: 16.4155 - val_loss: 16.9480 - val_MinusLogProbMetric: 16.9480 - lr: 5.2083e-06 - 62s/epoch - 315ms/step
Epoch 851/1000
2023-09-27 06:50:14.922 
Epoch 851/1000 
	 loss: 16.4142, MinusLogProbMetric: 16.4142, val_loss: 16.9486, val_MinusLogProbMetric: 16.9486

Epoch 851: val_loss did not improve from 16.94576
196/196 - 62s - loss: 16.4142 - MinusLogProbMetric: 16.4142 - val_loss: 16.9486 - val_MinusLogProbMetric: 16.9486 - lr: 5.2083e-06 - 62s/epoch - 317ms/step
Epoch 852/1000
2023-09-27 06:51:17.176 
Epoch 852/1000 
	 loss: 16.4153, MinusLogProbMetric: 16.4153, val_loss: 16.9489, val_MinusLogProbMetric: 16.9489

Epoch 852: val_loss did not improve from 16.94576
196/196 - 62s - loss: 16.4153 - MinusLogProbMetric: 16.4153 - val_loss: 16.9489 - val_MinusLogProbMetric: 16.9489 - lr: 5.2083e-06 - 62s/epoch - 318ms/step
Epoch 853/1000
2023-09-27 06:52:19.252 
Epoch 853/1000 
	 loss: 16.4158, MinusLogProbMetric: 16.4158, val_loss: 16.9516, val_MinusLogProbMetric: 16.9516

Epoch 853: val_loss did not improve from 16.94576
196/196 - 62s - loss: 16.4158 - MinusLogProbMetric: 16.4158 - val_loss: 16.9516 - val_MinusLogProbMetric: 16.9516 - lr: 5.2083e-06 - 62s/epoch - 317ms/step
Epoch 854/1000
2023-09-27 06:53:22.352 
Epoch 854/1000 
	 loss: 16.4143, MinusLogProbMetric: 16.4143, val_loss: 16.9525, val_MinusLogProbMetric: 16.9525

Epoch 854: val_loss did not improve from 16.94576
196/196 - 63s - loss: 16.4143 - MinusLogProbMetric: 16.4143 - val_loss: 16.9525 - val_MinusLogProbMetric: 16.9525 - lr: 5.2083e-06 - 63s/epoch - 322ms/step
Epoch 855/1000
2023-09-27 06:54:24.819 
Epoch 855/1000 
	 loss: 16.4152, MinusLogProbMetric: 16.4152, val_loss: 16.9514, val_MinusLogProbMetric: 16.9514

Epoch 855: val_loss did not improve from 16.94576
196/196 - 62s - loss: 16.4152 - MinusLogProbMetric: 16.4152 - val_loss: 16.9514 - val_MinusLogProbMetric: 16.9514 - lr: 5.2083e-06 - 62s/epoch - 319ms/step
Epoch 856/1000
2023-09-27 06:55:27.740 
Epoch 856/1000 
	 loss: 16.4160, MinusLogProbMetric: 16.4160, val_loss: 16.9509, val_MinusLogProbMetric: 16.9509

Epoch 856: val_loss did not improve from 16.94576
196/196 - 63s - loss: 16.4160 - MinusLogProbMetric: 16.4160 - val_loss: 16.9509 - val_MinusLogProbMetric: 16.9509 - lr: 5.2083e-06 - 63s/epoch - 321ms/step
Epoch 857/1000
2023-09-27 06:56:29.795 
Epoch 857/1000 
	 loss: 16.4156, MinusLogProbMetric: 16.4156, val_loss: 16.9482, val_MinusLogProbMetric: 16.9482

Epoch 857: val_loss did not improve from 16.94576
196/196 - 62s - loss: 16.4156 - MinusLogProbMetric: 16.4156 - val_loss: 16.9482 - val_MinusLogProbMetric: 16.9482 - lr: 5.2083e-06 - 62s/epoch - 317ms/step
Epoch 858/1000
2023-09-27 06:57:32.258 
Epoch 858/1000 
	 loss: 16.4152, MinusLogProbMetric: 16.4152, val_loss: 16.9479, val_MinusLogProbMetric: 16.9479

Epoch 858: val_loss did not improve from 16.94576
196/196 - 62s - loss: 16.4152 - MinusLogProbMetric: 16.4152 - val_loss: 16.9479 - val_MinusLogProbMetric: 16.9479 - lr: 5.2083e-06 - 62s/epoch - 319ms/step
Epoch 859/1000
2023-09-27 06:58:34.675 
Epoch 859/1000 
	 loss: 16.4141, MinusLogProbMetric: 16.4141, val_loss: 16.9509, val_MinusLogProbMetric: 16.9509

Epoch 859: val_loss did not improve from 16.94576
196/196 - 62s - loss: 16.4141 - MinusLogProbMetric: 16.4141 - val_loss: 16.9509 - val_MinusLogProbMetric: 16.9509 - lr: 5.2083e-06 - 62s/epoch - 318ms/step
Epoch 860/1000
2023-09-27 06:59:37.481 
Epoch 860/1000 
	 loss: 16.4151, MinusLogProbMetric: 16.4151, val_loss: 16.9486, val_MinusLogProbMetric: 16.9486

Epoch 860: val_loss did not improve from 16.94576
196/196 - 63s - loss: 16.4151 - MinusLogProbMetric: 16.4151 - val_loss: 16.9486 - val_MinusLogProbMetric: 16.9486 - lr: 5.2083e-06 - 63s/epoch - 320ms/step
Epoch 861/1000
2023-09-27 07:00:39.647 
Epoch 861/1000 
	 loss: 16.4151, MinusLogProbMetric: 16.4151, val_loss: 16.9518, val_MinusLogProbMetric: 16.9518

Epoch 861: val_loss did not improve from 16.94576
196/196 - 62s - loss: 16.4151 - MinusLogProbMetric: 16.4151 - val_loss: 16.9518 - val_MinusLogProbMetric: 16.9518 - lr: 5.2083e-06 - 62s/epoch - 317ms/step
Epoch 862/1000
2023-09-27 07:01:42.086 
Epoch 862/1000 
	 loss: 16.4148, MinusLogProbMetric: 16.4148, val_loss: 16.9472, val_MinusLogProbMetric: 16.9472

Epoch 862: val_loss did not improve from 16.94576
196/196 - 62s - loss: 16.4148 - MinusLogProbMetric: 16.4148 - val_loss: 16.9472 - val_MinusLogProbMetric: 16.9472 - lr: 5.2083e-06 - 62s/epoch - 319ms/step
Epoch 863/1000
2023-09-27 07:02:44.006 
Epoch 863/1000 
	 loss: 16.4112, MinusLogProbMetric: 16.4112, val_loss: 16.9474, val_MinusLogProbMetric: 16.9474

Epoch 863: val_loss did not improve from 16.94576
196/196 - 62s - loss: 16.4112 - MinusLogProbMetric: 16.4112 - val_loss: 16.9474 - val_MinusLogProbMetric: 16.9474 - lr: 2.6042e-06 - 62s/epoch - 316ms/step
Epoch 864/1000
2023-09-27 07:03:46.858 
Epoch 864/1000 
	 loss: 16.4106, MinusLogProbMetric: 16.4106, val_loss: 16.9463, val_MinusLogProbMetric: 16.9463

Epoch 864: val_loss did not improve from 16.94576
196/196 - 63s - loss: 16.4106 - MinusLogProbMetric: 16.4106 - val_loss: 16.9463 - val_MinusLogProbMetric: 16.9463 - lr: 2.6042e-06 - 63s/epoch - 321ms/step
Epoch 865/1000
2023-09-27 07:04:49.278 
Epoch 865/1000 
	 loss: 16.4114, MinusLogProbMetric: 16.4114, val_loss: 16.9466, val_MinusLogProbMetric: 16.9466

Epoch 865: val_loss did not improve from 16.94576
196/196 - 62s - loss: 16.4114 - MinusLogProbMetric: 16.4114 - val_loss: 16.9466 - val_MinusLogProbMetric: 16.9466 - lr: 2.6042e-06 - 62s/epoch - 318ms/step
Epoch 866/1000
2023-09-27 07:05:51.664 
Epoch 866/1000 
	 loss: 16.4109, MinusLogProbMetric: 16.4109, val_loss: 16.9467, val_MinusLogProbMetric: 16.9467

Epoch 866: val_loss did not improve from 16.94576
196/196 - 62s - loss: 16.4109 - MinusLogProbMetric: 16.4109 - val_loss: 16.9467 - val_MinusLogProbMetric: 16.9467 - lr: 2.6042e-06 - 62s/epoch - 318ms/step
Epoch 867/1000
2023-09-27 07:06:53.834 
Epoch 867/1000 
	 loss: 16.4116, MinusLogProbMetric: 16.4116, val_loss: 16.9458, val_MinusLogProbMetric: 16.9458

Epoch 867: val_loss did not improve from 16.94576
196/196 - 62s - loss: 16.4116 - MinusLogProbMetric: 16.4116 - val_loss: 16.9458 - val_MinusLogProbMetric: 16.9458 - lr: 2.6042e-06 - 62s/epoch - 317ms/step
Epoch 868/1000
2023-09-27 07:07:56.547 
Epoch 868/1000 
	 loss: 16.4113, MinusLogProbMetric: 16.4113, val_loss: 16.9457, val_MinusLogProbMetric: 16.9457

Epoch 868: val_loss improved from 16.94576 to 16.94571, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_294/weights/best_weights.h5
196/196 - 64s - loss: 16.4113 - MinusLogProbMetric: 16.4113 - val_loss: 16.9457 - val_MinusLogProbMetric: 16.9457 - lr: 2.6042e-06 - 64s/epoch - 324ms/step
Epoch 869/1000
2023-09-27 07:08:59.855 
Epoch 869/1000 
	 loss: 16.4110, MinusLogProbMetric: 16.4110, val_loss: 16.9474, val_MinusLogProbMetric: 16.9474

Epoch 869: val_loss did not improve from 16.94571
196/196 - 62s - loss: 16.4110 - MinusLogProbMetric: 16.4110 - val_loss: 16.9474 - val_MinusLogProbMetric: 16.9474 - lr: 2.6042e-06 - 62s/epoch - 319ms/step
Epoch 870/1000
2023-09-27 07:10:02.647 
Epoch 870/1000 
	 loss: 16.4110, MinusLogProbMetric: 16.4110, val_loss: 16.9471, val_MinusLogProbMetric: 16.9471

Epoch 870: val_loss did not improve from 16.94571
196/196 - 63s - loss: 16.4110 - MinusLogProbMetric: 16.4110 - val_loss: 16.9471 - val_MinusLogProbMetric: 16.9471 - lr: 2.6042e-06 - 63s/epoch - 320ms/step
Epoch 871/1000
2023-09-27 07:11:04.912 
Epoch 871/1000 
	 loss: 16.4113, MinusLogProbMetric: 16.4113, val_loss: 16.9451, val_MinusLogProbMetric: 16.9451

Epoch 871: val_loss improved from 16.94571 to 16.94511, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_294/weights/best_weights.h5
196/196 - 63s - loss: 16.4113 - MinusLogProbMetric: 16.4113 - val_loss: 16.9451 - val_MinusLogProbMetric: 16.9451 - lr: 2.6042e-06 - 63s/epoch - 323ms/step
Epoch 872/1000
2023-09-27 07:12:08.303 
Epoch 872/1000 
	 loss: 16.4116, MinusLogProbMetric: 16.4116, val_loss: 16.9503, val_MinusLogProbMetric: 16.9503

Epoch 872: val_loss did not improve from 16.94511
196/196 - 62s - loss: 16.4116 - MinusLogProbMetric: 16.4116 - val_loss: 16.9503 - val_MinusLogProbMetric: 16.9503 - lr: 2.6042e-06 - 62s/epoch - 318ms/step
Epoch 873/1000
2023-09-27 07:13:11.028 
Epoch 873/1000 
	 loss: 16.4113, MinusLogProbMetric: 16.4113, val_loss: 16.9457, val_MinusLogProbMetric: 16.9457

Epoch 873: val_loss did not improve from 16.94511
196/196 - 63s - loss: 16.4113 - MinusLogProbMetric: 16.4113 - val_loss: 16.9457 - val_MinusLogProbMetric: 16.9457 - lr: 2.6042e-06 - 63s/epoch - 320ms/step
Epoch 874/1000
2023-09-27 07:14:13.569 
Epoch 874/1000 
	 loss: 16.4104, MinusLogProbMetric: 16.4104, val_loss: 16.9482, val_MinusLogProbMetric: 16.9482

Epoch 874: val_loss did not improve from 16.94511
196/196 - 63s - loss: 16.4104 - MinusLogProbMetric: 16.4104 - val_loss: 16.9482 - val_MinusLogProbMetric: 16.9482 - lr: 2.6042e-06 - 63s/epoch - 319ms/step
Epoch 875/1000
2023-09-27 07:15:16.132 
Epoch 875/1000 
	 loss: 16.4115, MinusLogProbMetric: 16.4115, val_loss: 16.9467, val_MinusLogProbMetric: 16.9467

Epoch 875: val_loss did not improve from 16.94511
196/196 - 63s - loss: 16.4115 - MinusLogProbMetric: 16.4115 - val_loss: 16.9467 - val_MinusLogProbMetric: 16.9467 - lr: 2.6042e-06 - 63s/epoch - 319ms/step
Epoch 876/1000
2023-09-27 07:16:18.407 
Epoch 876/1000 
	 loss: 16.4104, MinusLogProbMetric: 16.4104, val_loss: 16.9469, val_MinusLogProbMetric: 16.9469

Epoch 876: val_loss did not improve from 16.94511
196/196 - 62s - loss: 16.4104 - MinusLogProbMetric: 16.4104 - val_loss: 16.9469 - val_MinusLogProbMetric: 16.9469 - lr: 2.6042e-06 - 62s/epoch - 318ms/step
Epoch 877/1000
2023-09-27 07:17:20.213 
Epoch 877/1000 
	 loss: 16.4112, MinusLogProbMetric: 16.4112, val_loss: 16.9487, val_MinusLogProbMetric: 16.9487

Epoch 877: val_loss did not improve from 16.94511
196/196 - 62s - loss: 16.4112 - MinusLogProbMetric: 16.4112 - val_loss: 16.9487 - val_MinusLogProbMetric: 16.9487 - lr: 2.6042e-06 - 62s/epoch - 315ms/step
Epoch 878/1000
2023-09-27 07:18:15.438 
Epoch 878/1000 
	 loss: 16.4113, MinusLogProbMetric: 16.4113, val_loss: 16.9480, val_MinusLogProbMetric: 16.9480

Epoch 878: val_loss did not improve from 16.94511
196/196 - 55s - loss: 16.4113 - MinusLogProbMetric: 16.4113 - val_loss: 16.9480 - val_MinusLogProbMetric: 16.9480 - lr: 2.6042e-06 - 55s/epoch - 282ms/step
Epoch 879/1000
2023-09-27 07:19:05.998 
Epoch 879/1000 
	 loss: 16.4117, MinusLogProbMetric: 16.4117, val_loss: 16.9466, val_MinusLogProbMetric: 16.9466

Epoch 879: val_loss did not improve from 16.94511
196/196 - 51s - loss: 16.4117 - MinusLogProbMetric: 16.4117 - val_loss: 16.9466 - val_MinusLogProbMetric: 16.9466 - lr: 2.6042e-06 - 51s/epoch - 258ms/step
Epoch 880/1000
2023-09-27 07:20:06.678 
Epoch 880/1000 
	 loss: 16.4115, MinusLogProbMetric: 16.4115, val_loss: 16.9477, val_MinusLogProbMetric: 16.9477

Epoch 880: val_loss did not improve from 16.94511
196/196 - 61s - loss: 16.4115 - MinusLogProbMetric: 16.4115 - val_loss: 16.9477 - val_MinusLogProbMetric: 16.9477 - lr: 2.6042e-06 - 61s/epoch - 310ms/step
Epoch 881/1000
2023-09-27 07:21:01.660 
Epoch 881/1000 
	 loss: 16.4109, MinusLogProbMetric: 16.4109, val_loss: 16.9460, val_MinusLogProbMetric: 16.9460

Epoch 881: val_loss did not improve from 16.94511
196/196 - 55s - loss: 16.4109 - MinusLogProbMetric: 16.4109 - val_loss: 16.9460 - val_MinusLogProbMetric: 16.9460 - lr: 2.6042e-06 - 55s/epoch - 281ms/step
Epoch 882/1000
2023-09-27 07:21:53.222 
Epoch 882/1000 
	 loss: 16.4115, MinusLogProbMetric: 16.4115, val_loss: 16.9467, val_MinusLogProbMetric: 16.9467

Epoch 882: val_loss did not improve from 16.94511
196/196 - 52s - loss: 16.4115 - MinusLogProbMetric: 16.4115 - val_loss: 16.9467 - val_MinusLogProbMetric: 16.9467 - lr: 2.6042e-06 - 52s/epoch - 263ms/step
Epoch 883/1000
2023-09-27 07:22:54.902 
Epoch 883/1000 
	 loss: 16.4111, MinusLogProbMetric: 16.4111, val_loss: 16.9487, val_MinusLogProbMetric: 16.9487

Epoch 883: val_loss did not improve from 16.94511
196/196 - 62s - loss: 16.4111 - MinusLogProbMetric: 16.4111 - val_loss: 16.9487 - val_MinusLogProbMetric: 16.9487 - lr: 2.6042e-06 - 62s/epoch - 315ms/step
Epoch 884/1000
2023-09-27 07:23:57.048 
Epoch 884/1000 
	 loss: 16.4123, MinusLogProbMetric: 16.4123, val_loss: 16.9452, val_MinusLogProbMetric: 16.9452

Epoch 884: val_loss did not improve from 16.94511
196/196 - 62s - loss: 16.4123 - MinusLogProbMetric: 16.4123 - val_loss: 16.9452 - val_MinusLogProbMetric: 16.9452 - lr: 2.6042e-06 - 62s/epoch - 317ms/step
Epoch 885/1000
2023-09-27 07:25:01.612 
Epoch 885/1000 
	 loss: 16.4109, MinusLogProbMetric: 16.4109, val_loss: 16.9481, val_MinusLogProbMetric: 16.9481

Epoch 885: val_loss did not improve from 16.94511
196/196 - 65s - loss: 16.4109 - MinusLogProbMetric: 16.4109 - val_loss: 16.9481 - val_MinusLogProbMetric: 16.9481 - lr: 2.6042e-06 - 65s/epoch - 329ms/step
Epoch 886/1000
2023-09-27 07:26:05.216 
Epoch 886/1000 
	 loss: 16.4114, MinusLogProbMetric: 16.4114, val_loss: 16.9478, val_MinusLogProbMetric: 16.9478

Epoch 886: val_loss did not improve from 16.94511
196/196 - 64s - loss: 16.4114 - MinusLogProbMetric: 16.4114 - val_loss: 16.9478 - val_MinusLogProbMetric: 16.9478 - lr: 2.6042e-06 - 64s/epoch - 324ms/step
Epoch 887/1000
2023-09-27 07:27:08.065 
Epoch 887/1000 
	 loss: 16.4111, MinusLogProbMetric: 16.4111, val_loss: 16.9475, val_MinusLogProbMetric: 16.9475

Epoch 887: val_loss did not improve from 16.94511
196/196 - 63s - loss: 16.4111 - MinusLogProbMetric: 16.4111 - val_loss: 16.9475 - val_MinusLogProbMetric: 16.9475 - lr: 2.6042e-06 - 63s/epoch - 321ms/step
Epoch 888/1000
2023-09-27 07:28:10.051 
Epoch 888/1000 
	 loss: 16.4105, MinusLogProbMetric: 16.4105, val_loss: 16.9485, val_MinusLogProbMetric: 16.9485

Epoch 888: val_loss did not improve from 16.94511
196/196 - 62s - loss: 16.4105 - MinusLogProbMetric: 16.4105 - val_loss: 16.9485 - val_MinusLogProbMetric: 16.9485 - lr: 2.6042e-06 - 62s/epoch - 316ms/step
Epoch 889/1000
2023-09-27 07:29:13.567 
Epoch 889/1000 
	 loss: 16.4105, MinusLogProbMetric: 16.4105, val_loss: 16.9475, val_MinusLogProbMetric: 16.9475

Epoch 889: val_loss did not improve from 16.94511
196/196 - 64s - loss: 16.4105 - MinusLogProbMetric: 16.4105 - val_loss: 16.9475 - val_MinusLogProbMetric: 16.9475 - lr: 2.6042e-06 - 64s/epoch - 324ms/step
Epoch 890/1000
2023-09-27 07:30:16.725 
Epoch 890/1000 
	 loss: 16.4111, MinusLogProbMetric: 16.4111, val_loss: 16.9465, val_MinusLogProbMetric: 16.9465

Epoch 890: val_loss did not improve from 16.94511
196/196 - 63s - loss: 16.4111 - MinusLogProbMetric: 16.4111 - val_loss: 16.9465 - val_MinusLogProbMetric: 16.9465 - lr: 2.6042e-06 - 63s/epoch - 322ms/step
Epoch 891/1000
2023-09-27 07:31:20.161 
Epoch 891/1000 
	 loss: 16.4112, MinusLogProbMetric: 16.4112, val_loss: 16.9472, val_MinusLogProbMetric: 16.9472

Epoch 891: val_loss did not improve from 16.94511
196/196 - 63s - loss: 16.4112 - MinusLogProbMetric: 16.4112 - val_loss: 16.9472 - val_MinusLogProbMetric: 16.9472 - lr: 2.6042e-06 - 63s/epoch - 324ms/step
Epoch 892/1000
2023-09-27 07:32:22.529 
Epoch 892/1000 
	 loss: 16.4109, MinusLogProbMetric: 16.4109, val_loss: 16.9489, val_MinusLogProbMetric: 16.9489

Epoch 892: val_loss did not improve from 16.94511
196/196 - 62s - loss: 16.4109 - MinusLogProbMetric: 16.4109 - val_loss: 16.9489 - val_MinusLogProbMetric: 16.9489 - lr: 2.6042e-06 - 62s/epoch - 318ms/step
Epoch 893/1000
2023-09-27 07:33:25.213 
Epoch 893/1000 
	 loss: 16.4108, MinusLogProbMetric: 16.4108, val_loss: 16.9457, val_MinusLogProbMetric: 16.9457

Epoch 893: val_loss did not improve from 16.94511
196/196 - 63s - loss: 16.4108 - MinusLogProbMetric: 16.4108 - val_loss: 16.9457 - val_MinusLogProbMetric: 16.9457 - lr: 2.6042e-06 - 63s/epoch - 320ms/step
Epoch 894/1000
2023-09-27 07:34:28.081 
Epoch 894/1000 
	 loss: 16.4104, MinusLogProbMetric: 16.4104, val_loss: 16.9470, val_MinusLogProbMetric: 16.9470

Epoch 894: val_loss did not improve from 16.94511
196/196 - 63s - loss: 16.4104 - MinusLogProbMetric: 16.4104 - val_loss: 16.9470 - val_MinusLogProbMetric: 16.9470 - lr: 2.6042e-06 - 63s/epoch - 321ms/step
Epoch 895/1000
2023-09-27 07:35:31.378 
Epoch 895/1000 
	 loss: 16.4112, MinusLogProbMetric: 16.4112, val_loss: 16.9459, val_MinusLogProbMetric: 16.9459

Epoch 895: val_loss did not improve from 16.94511
196/196 - 63s - loss: 16.4112 - MinusLogProbMetric: 16.4112 - val_loss: 16.9459 - val_MinusLogProbMetric: 16.9459 - lr: 2.6042e-06 - 63s/epoch - 323ms/step
Epoch 896/1000
2023-09-27 07:36:33.603 
Epoch 896/1000 
	 loss: 16.4110, MinusLogProbMetric: 16.4110, val_loss: 16.9487, val_MinusLogProbMetric: 16.9487

Epoch 896: val_loss did not improve from 16.94511
196/196 - 62s - loss: 16.4110 - MinusLogProbMetric: 16.4110 - val_loss: 16.9487 - val_MinusLogProbMetric: 16.9487 - lr: 2.6042e-06 - 62s/epoch - 317ms/step
Epoch 897/1000
2023-09-27 07:37:35.348 
Epoch 897/1000 
	 loss: 16.4107, MinusLogProbMetric: 16.4107, val_loss: 16.9470, val_MinusLogProbMetric: 16.9470

Epoch 897: val_loss did not improve from 16.94511
196/196 - 62s - loss: 16.4107 - MinusLogProbMetric: 16.4107 - val_loss: 16.9470 - val_MinusLogProbMetric: 16.9470 - lr: 2.6042e-06 - 62s/epoch - 315ms/step
Epoch 898/1000
2023-09-27 07:38:38.634 
Epoch 898/1000 
	 loss: 16.4111, MinusLogProbMetric: 16.4111, val_loss: 16.9467, val_MinusLogProbMetric: 16.9467

Epoch 898: val_loss did not improve from 16.94511
196/196 - 63s - loss: 16.4111 - MinusLogProbMetric: 16.4111 - val_loss: 16.9467 - val_MinusLogProbMetric: 16.9467 - lr: 2.6042e-06 - 63s/epoch - 323ms/step
Epoch 899/1000
2023-09-27 07:39:41.348 
Epoch 899/1000 
	 loss: 16.4107, MinusLogProbMetric: 16.4107, val_loss: 16.9473, val_MinusLogProbMetric: 16.9473

Epoch 899: val_loss did not improve from 16.94511
196/196 - 63s - loss: 16.4107 - MinusLogProbMetric: 16.4107 - val_loss: 16.9473 - val_MinusLogProbMetric: 16.9473 - lr: 2.6042e-06 - 63s/epoch - 320ms/step
Epoch 900/1000
2023-09-27 07:40:43.559 
Epoch 900/1000 
	 loss: 16.4108, MinusLogProbMetric: 16.4108, val_loss: 16.9477, val_MinusLogProbMetric: 16.9477

Epoch 900: val_loss did not improve from 16.94511
196/196 - 62s - loss: 16.4108 - MinusLogProbMetric: 16.4108 - val_loss: 16.9477 - val_MinusLogProbMetric: 16.9477 - lr: 2.6042e-06 - 62s/epoch - 317ms/step
Epoch 901/1000
2023-09-27 07:41:45.687 
Epoch 901/1000 
	 loss: 16.4108, MinusLogProbMetric: 16.4108, val_loss: 16.9502, val_MinusLogProbMetric: 16.9502

Epoch 901: val_loss did not improve from 16.94511
196/196 - 62s - loss: 16.4108 - MinusLogProbMetric: 16.4108 - val_loss: 16.9502 - val_MinusLogProbMetric: 16.9502 - lr: 2.6042e-06 - 62s/epoch - 317ms/step
Epoch 902/1000
2023-09-27 07:42:48.590 
Epoch 902/1000 
	 loss: 16.4106, MinusLogProbMetric: 16.4106, val_loss: 16.9454, val_MinusLogProbMetric: 16.9454

Epoch 902: val_loss did not improve from 16.94511
196/196 - 63s - loss: 16.4106 - MinusLogProbMetric: 16.4106 - val_loss: 16.9454 - val_MinusLogProbMetric: 16.9454 - lr: 2.6042e-06 - 63s/epoch - 321ms/step
Epoch 903/1000
2023-09-27 07:43:50.808 
Epoch 903/1000 
	 loss: 16.4104, MinusLogProbMetric: 16.4104, val_loss: 16.9481, val_MinusLogProbMetric: 16.9481

Epoch 903: val_loss did not improve from 16.94511
196/196 - 62s - loss: 16.4104 - MinusLogProbMetric: 16.4104 - val_loss: 16.9481 - val_MinusLogProbMetric: 16.9481 - lr: 2.6042e-06 - 62s/epoch - 317ms/step
Epoch 904/1000
2023-09-27 07:44:52.951 
Epoch 904/1000 
	 loss: 16.4105, MinusLogProbMetric: 16.4105, val_loss: 16.9464, val_MinusLogProbMetric: 16.9464

Epoch 904: val_loss did not improve from 16.94511
196/196 - 62s - loss: 16.4105 - MinusLogProbMetric: 16.4105 - val_loss: 16.9464 - val_MinusLogProbMetric: 16.9464 - lr: 2.6042e-06 - 62s/epoch - 317ms/step
Epoch 905/1000
2023-09-27 07:45:56.322 
Epoch 905/1000 
	 loss: 16.4100, MinusLogProbMetric: 16.4100, val_loss: 16.9468, val_MinusLogProbMetric: 16.9468

Epoch 905: val_loss did not improve from 16.94511
196/196 - 63s - loss: 16.4100 - MinusLogProbMetric: 16.4100 - val_loss: 16.9468 - val_MinusLogProbMetric: 16.9468 - lr: 2.6042e-06 - 63s/epoch - 323ms/step
Epoch 906/1000
2023-09-27 07:46:58.099 
Epoch 906/1000 
	 loss: 16.4104, MinusLogProbMetric: 16.4104, val_loss: 16.9472, val_MinusLogProbMetric: 16.9472

Epoch 906: val_loss did not improve from 16.94511
196/196 - 62s - loss: 16.4104 - MinusLogProbMetric: 16.4104 - val_loss: 16.9472 - val_MinusLogProbMetric: 16.9472 - lr: 2.6042e-06 - 62s/epoch - 315ms/step
Epoch 907/1000
2023-09-27 07:48:00.096 
Epoch 907/1000 
	 loss: 16.4101, MinusLogProbMetric: 16.4101, val_loss: 16.9471, val_MinusLogProbMetric: 16.9471

Epoch 907: val_loss did not improve from 16.94511
196/196 - 62s - loss: 16.4101 - MinusLogProbMetric: 16.4101 - val_loss: 16.9471 - val_MinusLogProbMetric: 16.9471 - lr: 2.6042e-06 - 62s/epoch - 316ms/step
Epoch 908/1000
2023-09-27 07:49:02.083 
Epoch 908/1000 
	 loss: 16.4110, MinusLogProbMetric: 16.4110, val_loss: 16.9482, val_MinusLogProbMetric: 16.9482

Epoch 908: val_loss did not improve from 16.94511
196/196 - 62s - loss: 16.4110 - MinusLogProbMetric: 16.4110 - val_loss: 16.9482 - val_MinusLogProbMetric: 16.9482 - lr: 2.6042e-06 - 62s/epoch - 316ms/step
Epoch 909/1000
2023-09-27 07:50:03.582 
Epoch 909/1000 
	 loss: 16.4103, MinusLogProbMetric: 16.4103, val_loss: 16.9467, val_MinusLogProbMetric: 16.9467

Epoch 909: val_loss did not improve from 16.94511
196/196 - 61s - loss: 16.4103 - MinusLogProbMetric: 16.4103 - val_loss: 16.9467 - val_MinusLogProbMetric: 16.9467 - lr: 2.6042e-06 - 61s/epoch - 314ms/step
Epoch 910/1000
2023-09-27 07:51:06.400 
Epoch 910/1000 
	 loss: 16.4100, MinusLogProbMetric: 16.4100, val_loss: 16.9473, val_MinusLogProbMetric: 16.9473

Epoch 910: val_loss did not improve from 16.94511
196/196 - 63s - loss: 16.4100 - MinusLogProbMetric: 16.4100 - val_loss: 16.9473 - val_MinusLogProbMetric: 16.9473 - lr: 2.6042e-06 - 63s/epoch - 320ms/step
Epoch 911/1000
2023-09-27 07:52:08.135 
Epoch 911/1000 
	 loss: 16.4116, MinusLogProbMetric: 16.4116, val_loss: 16.9475, val_MinusLogProbMetric: 16.9475

Epoch 911: val_loss did not improve from 16.94511
196/196 - 62s - loss: 16.4116 - MinusLogProbMetric: 16.4116 - val_loss: 16.9475 - val_MinusLogProbMetric: 16.9475 - lr: 2.6042e-06 - 62s/epoch - 315ms/step
Epoch 912/1000
2023-09-27 07:53:10.476 
Epoch 912/1000 
	 loss: 16.4101, MinusLogProbMetric: 16.4101, val_loss: 16.9477, val_MinusLogProbMetric: 16.9477

Epoch 912: val_loss did not improve from 16.94511
196/196 - 62s - loss: 16.4101 - MinusLogProbMetric: 16.4101 - val_loss: 16.9477 - val_MinusLogProbMetric: 16.9477 - lr: 2.6042e-06 - 62s/epoch - 318ms/step
Epoch 913/1000
2023-09-27 07:54:13.098 
Epoch 913/1000 
	 loss: 16.4107, MinusLogProbMetric: 16.4107, val_loss: 16.9463, val_MinusLogProbMetric: 16.9463

Epoch 913: val_loss did not improve from 16.94511
196/196 - 63s - loss: 16.4107 - MinusLogProbMetric: 16.4107 - val_loss: 16.9463 - val_MinusLogProbMetric: 16.9463 - lr: 2.6042e-06 - 63s/epoch - 319ms/step
Epoch 914/1000
2023-09-27 07:55:14.925 
Epoch 914/1000 
	 loss: 16.4105, MinusLogProbMetric: 16.4105, val_loss: 16.9459, val_MinusLogProbMetric: 16.9459

Epoch 914: val_loss did not improve from 16.94511
196/196 - 62s - loss: 16.4105 - MinusLogProbMetric: 16.4105 - val_loss: 16.9459 - val_MinusLogProbMetric: 16.9459 - lr: 2.6042e-06 - 62s/epoch - 315ms/step
Epoch 915/1000
2023-09-27 07:56:17.218 
Epoch 915/1000 
	 loss: 16.4098, MinusLogProbMetric: 16.4098, val_loss: 16.9456, val_MinusLogProbMetric: 16.9456

Epoch 915: val_loss did not improve from 16.94511
196/196 - 62s - loss: 16.4098 - MinusLogProbMetric: 16.4098 - val_loss: 16.9456 - val_MinusLogProbMetric: 16.9456 - lr: 2.6042e-06 - 62s/epoch - 318ms/step
Epoch 916/1000
2023-09-27 07:57:19.725 
Epoch 916/1000 
	 loss: 16.4109, MinusLogProbMetric: 16.4109, val_loss: 16.9464, val_MinusLogProbMetric: 16.9464

Epoch 916: val_loss did not improve from 16.94511
196/196 - 63s - loss: 16.4109 - MinusLogProbMetric: 16.4109 - val_loss: 16.9464 - val_MinusLogProbMetric: 16.9464 - lr: 2.6042e-06 - 63s/epoch - 319ms/step
Epoch 917/1000
2023-09-27 07:58:22.001 
Epoch 917/1000 
	 loss: 16.4103, MinusLogProbMetric: 16.4103, val_loss: 16.9474, val_MinusLogProbMetric: 16.9474

Epoch 917: val_loss did not improve from 16.94511
196/196 - 62s - loss: 16.4103 - MinusLogProbMetric: 16.4103 - val_loss: 16.9474 - val_MinusLogProbMetric: 16.9474 - lr: 2.6042e-06 - 62s/epoch - 318ms/step
Epoch 918/1000
2023-09-27 07:59:24.256 
Epoch 918/1000 
	 loss: 16.4096, MinusLogProbMetric: 16.4096, val_loss: 16.9488, val_MinusLogProbMetric: 16.9488

Epoch 918: val_loss did not improve from 16.94511
196/196 - 62s - loss: 16.4096 - MinusLogProbMetric: 16.4096 - val_loss: 16.9488 - val_MinusLogProbMetric: 16.9488 - lr: 2.6042e-06 - 62s/epoch - 318ms/step
Epoch 919/1000
2023-09-27 08:00:26.822 
Epoch 919/1000 
	 loss: 16.4102, MinusLogProbMetric: 16.4102, val_loss: 16.9459, val_MinusLogProbMetric: 16.9459

Epoch 919: val_loss did not improve from 16.94511
196/196 - 63s - loss: 16.4102 - MinusLogProbMetric: 16.4102 - val_loss: 16.9459 - val_MinusLogProbMetric: 16.9459 - lr: 2.6042e-06 - 63s/epoch - 319ms/step
Epoch 920/1000
2023-09-27 08:01:29.569 
Epoch 920/1000 
	 loss: 16.4115, MinusLogProbMetric: 16.4115, val_loss: 16.9496, val_MinusLogProbMetric: 16.9496

Epoch 920: val_loss did not improve from 16.94511
196/196 - 63s - loss: 16.4115 - MinusLogProbMetric: 16.4115 - val_loss: 16.9496 - val_MinusLogProbMetric: 16.9496 - lr: 2.6042e-06 - 63s/epoch - 320ms/step
Epoch 921/1000
2023-09-27 08:02:31.811 
Epoch 921/1000 
	 loss: 16.4111, MinusLogProbMetric: 16.4111, val_loss: 16.9474, val_MinusLogProbMetric: 16.9474

Epoch 921: val_loss did not improve from 16.94511
196/196 - 62s - loss: 16.4111 - MinusLogProbMetric: 16.4111 - val_loss: 16.9474 - val_MinusLogProbMetric: 16.9474 - lr: 2.6042e-06 - 62s/epoch - 318ms/step
Epoch 922/1000
2023-09-27 08:03:33.554 
Epoch 922/1000 
	 loss: 16.4085, MinusLogProbMetric: 16.4085, val_loss: 16.9459, val_MinusLogProbMetric: 16.9459

Epoch 922: val_loss did not improve from 16.94511
196/196 - 62s - loss: 16.4085 - MinusLogProbMetric: 16.4085 - val_loss: 16.9459 - val_MinusLogProbMetric: 16.9459 - lr: 1.3021e-06 - 62s/epoch - 315ms/step
Epoch 923/1000
2023-09-27 08:04:36.797 
Epoch 923/1000 
	 loss: 16.4086, MinusLogProbMetric: 16.4086, val_loss: 16.9461, val_MinusLogProbMetric: 16.9461

Epoch 923: val_loss did not improve from 16.94511
196/196 - 63s - loss: 16.4086 - MinusLogProbMetric: 16.4086 - val_loss: 16.9461 - val_MinusLogProbMetric: 16.9461 - lr: 1.3021e-06 - 63s/epoch - 323ms/step
Epoch 924/1000
2023-09-27 08:05:39.115 
Epoch 924/1000 
	 loss: 16.4082, MinusLogProbMetric: 16.4082, val_loss: 16.9460, val_MinusLogProbMetric: 16.9460

Epoch 924: val_loss did not improve from 16.94511
196/196 - 62s - loss: 16.4082 - MinusLogProbMetric: 16.4082 - val_loss: 16.9460 - val_MinusLogProbMetric: 16.9460 - lr: 1.3021e-06 - 62s/epoch - 318ms/step
Epoch 925/1000
2023-09-27 08:06:40.990 
Epoch 925/1000 
	 loss: 16.4085, MinusLogProbMetric: 16.4085, val_loss: 16.9464, val_MinusLogProbMetric: 16.9464

Epoch 925: val_loss did not improve from 16.94511
196/196 - 62s - loss: 16.4085 - MinusLogProbMetric: 16.4085 - val_loss: 16.9464 - val_MinusLogProbMetric: 16.9464 - lr: 1.3021e-06 - 62s/epoch - 316ms/step
Epoch 926/1000
2023-09-27 08:07:43.648 
Epoch 926/1000 
	 loss: 16.4084, MinusLogProbMetric: 16.4084, val_loss: 16.9455, val_MinusLogProbMetric: 16.9455

Epoch 926: val_loss did not improve from 16.94511
196/196 - 63s - loss: 16.4084 - MinusLogProbMetric: 16.4084 - val_loss: 16.9455 - val_MinusLogProbMetric: 16.9455 - lr: 1.3021e-06 - 63s/epoch - 320ms/step
Epoch 927/1000
2023-09-27 08:08:46.058 
Epoch 927/1000 
	 loss: 16.4082, MinusLogProbMetric: 16.4082, val_loss: 16.9473, val_MinusLogProbMetric: 16.9473

Epoch 927: val_loss did not improve from 16.94511
196/196 - 62s - loss: 16.4082 - MinusLogProbMetric: 16.4082 - val_loss: 16.9473 - val_MinusLogProbMetric: 16.9473 - lr: 1.3021e-06 - 62s/epoch - 318ms/step
Epoch 928/1000
2023-09-27 08:09:49.125 
Epoch 928/1000 
	 loss: 16.4085, MinusLogProbMetric: 16.4085, val_loss: 16.9450, val_MinusLogProbMetric: 16.9450

Epoch 928: val_loss improved from 16.94511 to 16.94503, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_294/weights/best_weights.h5
196/196 - 64s - loss: 16.4085 - MinusLogProbMetric: 16.4085 - val_loss: 16.9450 - val_MinusLogProbMetric: 16.9450 - lr: 1.3021e-06 - 64s/epoch - 326ms/step
Epoch 929/1000
2023-09-27 08:10:51.971 
Epoch 929/1000 
	 loss: 16.4089, MinusLogProbMetric: 16.4089, val_loss: 16.9464, val_MinusLogProbMetric: 16.9464

Epoch 929: val_loss did not improve from 16.94503
196/196 - 62s - loss: 16.4089 - MinusLogProbMetric: 16.4089 - val_loss: 16.9464 - val_MinusLogProbMetric: 16.9464 - lr: 1.3021e-06 - 62s/epoch - 317ms/step
Epoch 930/1000
2023-09-27 08:11:53.715 
Epoch 930/1000 
	 loss: 16.4085, MinusLogProbMetric: 16.4085, val_loss: 16.9458, val_MinusLogProbMetric: 16.9458

Epoch 930: val_loss did not improve from 16.94503
196/196 - 62s - loss: 16.4085 - MinusLogProbMetric: 16.4085 - val_loss: 16.9458 - val_MinusLogProbMetric: 16.9458 - lr: 1.3021e-06 - 62s/epoch - 315ms/step
Epoch 931/1000
2023-09-27 08:12:55.441 
Epoch 931/1000 
	 loss: 16.4084, MinusLogProbMetric: 16.4084, val_loss: 16.9469, val_MinusLogProbMetric: 16.9469

Epoch 931: val_loss did not improve from 16.94503
196/196 - 62s - loss: 16.4084 - MinusLogProbMetric: 16.4084 - val_loss: 16.9469 - val_MinusLogProbMetric: 16.9469 - lr: 1.3021e-06 - 62s/epoch - 315ms/step
Epoch 932/1000
2023-09-27 08:13:57.625 
Epoch 932/1000 
	 loss: 16.4080, MinusLogProbMetric: 16.4080, val_loss: 16.9457, val_MinusLogProbMetric: 16.9457

Epoch 932: val_loss did not improve from 16.94503
196/196 - 62s - loss: 16.4080 - MinusLogProbMetric: 16.4080 - val_loss: 16.9457 - val_MinusLogProbMetric: 16.9457 - lr: 1.3021e-06 - 62s/epoch - 317ms/step
Epoch 933/1000
2023-09-27 08:14:59.662 
Epoch 933/1000 
	 loss: 16.4084, MinusLogProbMetric: 16.4084, val_loss: 16.9470, val_MinusLogProbMetric: 16.9470

Epoch 933: val_loss did not improve from 16.94503
196/196 - 62s - loss: 16.4084 - MinusLogProbMetric: 16.4084 - val_loss: 16.9470 - val_MinusLogProbMetric: 16.9470 - lr: 1.3021e-06 - 62s/epoch - 316ms/step
Epoch 934/1000
2023-09-27 08:16:02.517 
Epoch 934/1000 
	 loss: 16.4087, MinusLogProbMetric: 16.4087, val_loss: 16.9451, val_MinusLogProbMetric: 16.9451

Epoch 934: val_loss did not improve from 16.94503
196/196 - 63s - loss: 16.4087 - MinusLogProbMetric: 16.4087 - val_loss: 16.9451 - val_MinusLogProbMetric: 16.9451 - lr: 1.3021e-06 - 63s/epoch - 321ms/step
Epoch 935/1000
2023-09-27 08:17:05.457 
Epoch 935/1000 
	 loss: 16.4082, MinusLogProbMetric: 16.4082, val_loss: 16.9463, val_MinusLogProbMetric: 16.9463

Epoch 935: val_loss did not improve from 16.94503
196/196 - 63s - loss: 16.4082 - MinusLogProbMetric: 16.4082 - val_loss: 16.9463 - val_MinusLogProbMetric: 16.9463 - lr: 1.3021e-06 - 63s/epoch - 321ms/step
Epoch 936/1000
2023-09-27 08:18:07.676 
Epoch 936/1000 
	 loss: 16.4083, MinusLogProbMetric: 16.4083, val_loss: 16.9464, val_MinusLogProbMetric: 16.9464

Epoch 936: val_loss did not improve from 16.94503
196/196 - 62s - loss: 16.4083 - MinusLogProbMetric: 16.4083 - val_loss: 16.9464 - val_MinusLogProbMetric: 16.9464 - lr: 1.3021e-06 - 62s/epoch - 317ms/step
Epoch 937/1000
2023-09-27 08:19:10.554 
Epoch 937/1000 
	 loss: 16.4083, MinusLogProbMetric: 16.4083, val_loss: 16.9475, val_MinusLogProbMetric: 16.9475

Epoch 937: val_loss did not improve from 16.94503
196/196 - 63s - loss: 16.4083 - MinusLogProbMetric: 16.4083 - val_loss: 16.9475 - val_MinusLogProbMetric: 16.9475 - lr: 1.3021e-06 - 63s/epoch - 321ms/step
Epoch 938/1000
2023-09-27 08:20:13.056 
Epoch 938/1000 
	 loss: 16.4084, MinusLogProbMetric: 16.4084, val_loss: 16.9461, val_MinusLogProbMetric: 16.9461

Epoch 938: val_loss did not improve from 16.94503
196/196 - 62s - loss: 16.4084 - MinusLogProbMetric: 16.4084 - val_loss: 16.9461 - val_MinusLogProbMetric: 16.9461 - lr: 1.3021e-06 - 62s/epoch - 319ms/step
Epoch 939/1000
2023-09-27 08:21:16.420 
Epoch 939/1000 
	 loss: 16.4085, MinusLogProbMetric: 16.4085, val_loss: 16.9458, val_MinusLogProbMetric: 16.9458

Epoch 939: val_loss did not improve from 16.94503
196/196 - 63s - loss: 16.4085 - MinusLogProbMetric: 16.4085 - val_loss: 16.9458 - val_MinusLogProbMetric: 16.9458 - lr: 1.3021e-06 - 63s/epoch - 323ms/step
Epoch 940/1000
2023-09-27 08:22:20.329 
Epoch 940/1000 
	 loss: 16.4087, MinusLogProbMetric: 16.4087, val_loss: 16.9475, val_MinusLogProbMetric: 16.9475

Epoch 940: val_loss did not improve from 16.94503
196/196 - 64s - loss: 16.4087 - MinusLogProbMetric: 16.4087 - val_loss: 16.9475 - val_MinusLogProbMetric: 16.9475 - lr: 1.3021e-06 - 64s/epoch - 326ms/step
Epoch 941/1000
2023-09-27 08:23:22.894 
Epoch 941/1000 
	 loss: 16.4089, MinusLogProbMetric: 16.4089, val_loss: 16.9454, val_MinusLogProbMetric: 16.9454

Epoch 941: val_loss did not improve from 16.94503
196/196 - 63s - loss: 16.4089 - MinusLogProbMetric: 16.4089 - val_loss: 16.9454 - val_MinusLogProbMetric: 16.9454 - lr: 1.3021e-06 - 63s/epoch - 319ms/step
Epoch 942/1000
2023-09-27 08:24:24.788 
Epoch 942/1000 
	 loss: 16.4082, MinusLogProbMetric: 16.4082, val_loss: 16.9473, val_MinusLogProbMetric: 16.9473

Epoch 942: val_loss did not improve from 16.94503
196/196 - 62s - loss: 16.4082 - MinusLogProbMetric: 16.4082 - val_loss: 16.9473 - val_MinusLogProbMetric: 16.9473 - lr: 1.3021e-06 - 62s/epoch - 316ms/step
Epoch 943/1000
2023-09-27 08:25:27.032 
Epoch 943/1000 
	 loss: 16.4084, MinusLogProbMetric: 16.4084, val_loss: 16.9457, val_MinusLogProbMetric: 16.9457

Epoch 943: val_loss did not improve from 16.94503
196/196 - 62s - loss: 16.4084 - MinusLogProbMetric: 16.4084 - val_loss: 16.9457 - val_MinusLogProbMetric: 16.9457 - lr: 1.3021e-06 - 62s/epoch - 318ms/step
Epoch 944/1000
2023-09-27 08:26:29.463 
Epoch 944/1000 
	 loss: 16.4082, MinusLogProbMetric: 16.4082, val_loss: 16.9462, val_MinusLogProbMetric: 16.9462

Epoch 944: val_loss did not improve from 16.94503
196/196 - 62s - loss: 16.4082 - MinusLogProbMetric: 16.4082 - val_loss: 16.9462 - val_MinusLogProbMetric: 16.9462 - lr: 1.3021e-06 - 62s/epoch - 319ms/step
Epoch 945/1000
2023-09-27 08:27:32.949 
Epoch 945/1000 
	 loss: 16.4082, MinusLogProbMetric: 16.4082, val_loss: 16.9464, val_MinusLogProbMetric: 16.9464

Epoch 945: val_loss did not improve from 16.94503
196/196 - 63s - loss: 16.4082 - MinusLogProbMetric: 16.4082 - val_loss: 16.9464 - val_MinusLogProbMetric: 16.9464 - lr: 1.3021e-06 - 63s/epoch - 324ms/step
Epoch 946/1000
2023-09-27 08:28:35.108 
Epoch 946/1000 
	 loss: 16.4080, MinusLogProbMetric: 16.4080, val_loss: 16.9464, val_MinusLogProbMetric: 16.9464

Epoch 946: val_loss did not improve from 16.94503
196/196 - 62s - loss: 16.4080 - MinusLogProbMetric: 16.4080 - val_loss: 16.9464 - val_MinusLogProbMetric: 16.9464 - lr: 1.3021e-06 - 62s/epoch - 317ms/step
Epoch 947/1000
2023-09-27 08:29:37.519 
Epoch 947/1000 
	 loss: 16.4085, MinusLogProbMetric: 16.4085, val_loss: 16.9459, val_MinusLogProbMetric: 16.9459

Epoch 947: val_loss did not improve from 16.94503
196/196 - 62s - loss: 16.4085 - MinusLogProbMetric: 16.4085 - val_loss: 16.9459 - val_MinusLogProbMetric: 16.9459 - lr: 1.3021e-06 - 62s/epoch - 318ms/step
Epoch 948/1000
2023-09-27 08:30:40.234 
Epoch 948/1000 
	 loss: 16.4083, MinusLogProbMetric: 16.4083, val_loss: 16.9451, val_MinusLogProbMetric: 16.9451

Epoch 948: val_loss did not improve from 16.94503
196/196 - 63s - loss: 16.4083 - MinusLogProbMetric: 16.4083 - val_loss: 16.9451 - val_MinusLogProbMetric: 16.9451 - lr: 1.3021e-06 - 63s/epoch - 320ms/step
Epoch 949/1000
2023-09-27 08:31:42.305 
Epoch 949/1000 
	 loss: 16.4086, MinusLogProbMetric: 16.4086, val_loss: 16.9461, val_MinusLogProbMetric: 16.9461

Epoch 949: val_loss did not improve from 16.94503
196/196 - 62s - loss: 16.4086 - MinusLogProbMetric: 16.4086 - val_loss: 16.9461 - val_MinusLogProbMetric: 16.9461 - lr: 1.3021e-06 - 62s/epoch - 317ms/step
Epoch 950/1000
2023-09-27 08:32:44.317 
Epoch 950/1000 
	 loss: 16.4081, MinusLogProbMetric: 16.4081, val_loss: 16.9472, val_MinusLogProbMetric: 16.9472

Epoch 950: val_loss did not improve from 16.94503
196/196 - 62s - loss: 16.4081 - MinusLogProbMetric: 16.4081 - val_loss: 16.9472 - val_MinusLogProbMetric: 16.9472 - lr: 1.3021e-06 - 62s/epoch - 316ms/step
Epoch 951/1000
2023-09-27 08:33:46.204 
Epoch 951/1000 
	 loss: 16.4089, MinusLogProbMetric: 16.4089, val_loss: 16.9451, val_MinusLogProbMetric: 16.9451

Epoch 951: val_loss did not improve from 16.94503
196/196 - 62s - loss: 16.4089 - MinusLogProbMetric: 16.4089 - val_loss: 16.9451 - val_MinusLogProbMetric: 16.9451 - lr: 1.3021e-06 - 62s/epoch - 316ms/step
Epoch 952/1000
2023-09-27 08:34:48.958 
Epoch 952/1000 
	 loss: 16.4081, MinusLogProbMetric: 16.4081, val_loss: 16.9456, val_MinusLogProbMetric: 16.9456

Epoch 952: val_loss did not improve from 16.94503
196/196 - 63s - loss: 16.4081 - MinusLogProbMetric: 16.4081 - val_loss: 16.9456 - val_MinusLogProbMetric: 16.9456 - lr: 1.3021e-06 - 63s/epoch - 320ms/step
Epoch 953/1000
2023-09-27 08:35:50.756 
Epoch 953/1000 
	 loss: 16.4084, MinusLogProbMetric: 16.4084, val_loss: 16.9460, val_MinusLogProbMetric: 16.9460

Epoch 953: val_loss did not improve from 16.94503
196/196 - 62s - loss: 16.4084 - MinusLogProbMetric: 16.4084 - val_loss: 16.9460 - val_MinusLogProbMetric: 16.9460 - lr: 1.3021e-06 - 62s/epoch - 315ms/step
Epoch 954/1000
2023-09-27 08:36:53.744 
Epoch 954/1000 
	 loss: 16.4080, MinusLogProbMetric: 16.4080, val_loss: 16.9466, val_MinusLogProbMetric: 16.9466

Epoch 954: val_loss did not improve from 16.94503
196/196 - 63s - loss: 16.4080 - MinusLogProbMetric: 16.4080 - val_loss: 16.9466 - val_MinusLogProbMetric: 16.9466 - lr: 1.3021e-06 - 63s/epoch - 321ms/step
Epoch 955/1000
2023-09-27 08:37:55.934 
Epoch 955/1000 
	 loss: 16.4082, MinusLogProbMetric: 16.4082, val_loss: 16.9455, val_MinusLogProbMetric: 16.9455

Epoch 955: val_loss did not improve from 16.94503
196/196 - 62s - loss: 16.4082 - MinusLogProbMetric: 16.4082 - val_loss: 16.9455 - val_MinusLogProbMetric: 16.9455 - lr: 1.3021e-06 - 62s/epoch - 317ms/step
Epoch 956/1000
2023-09-27 08:38:58.222 
Epoch 956/1000 
	 loss: 16.4085, MinusLogProbMetric: 16.4085, val_loss: 16.9459, val_MinusLogProbMetric: 16.9459

Epoch 956: val_loss did not improve from 16.94503
196/196 - 62s - loss: 16.4085 - MinusLogProbMetric: 16.4085 - val_loss: 16.9459 - val_MinusLogProbMetric: 16.9459 - lr: 1.3021e-06 - 62s/epoch - 318ms/step
Epoch 957/1000
2023-09-27 08:39:59.802 
Epoch 957/1000 
	 loss: 16.4085, MinusLogProbMetric: 16.4085, val_loss: 16.9482, val_MinusLogProbMetric: 16.9482

Epoch 957: val_loss did not improve from 16.94503
196/196 - 62s - loss: 16.4085 - MinusLogProbMetric: 16.4085 - val_loss: 16.9482 - val_MinusLogProbMetric: 16.9482 - lr: 1.3021e-06 - 62s/epoch - 314ms/step
Epoch 958/1000
2023-09-27 08:41:02.731 
Epoch 958/1000 
	 loss: 16.4088, MinusLogProbMetric: 16.4088, val_loss: 16.9457, val_MinusLogProbMetric: 16.9457

Epoch 958: val_loss did not improve from 16.94503
196/196 - 63s - loss: 16.4088 - MinusLogProbMetric: 16.4088 - val_loss: 16.9457 - val_MinusLogProbMetric: 16.9457 - lr: 1.3021e-06 - 63s/epoch - 321ms/step
Epoch 959/1000
2023-09-27 08:42:05.488 
Epoch 959/1000 
	 loss: 16.4086, MinusLogProbMetric: 16.4086, val_loss: 16.9469, val_MinusLogProbMetric: 16.9469

Epoch 959: val_loss did not improve from 16.94503
196/196 - 63s - loss: 16.4086 - MinusLogProbMetric: 16.4086 - val_loss: 16.9469 - val_MinusLogProbMetric: 16.9469 - lr: 1.3021e-06 - 63s/epoch - 320ms/step
Epoch 960/1000
2023-09-27 08:43:07.847 
Epoch 960/1000 
	 loss: 16.4084, MinusLogProbMetric: 16.4084, val_loss: 16.9453, val_MinusLogProbMetric: 16.9453

Epoch 960: val_loss did not improve from 16.94503
196/196 - 62s - loss: 16.4084 - MinusLogProbMetric: 16.4084 - val_loss: 16.9453 - val_MinusLogProbMetric: 16.9453 - lr: 1.3021e-06 - 62s/epoch - 318ms/step
Epoch 961/1000
2023-09-27 08:44:10.599 
Epoch 961/1000 
	 loss: 16.4080, MinusLogProbMetric: 16.4080, val_loss: 16.9472, val_MinusLogProbMetric: 16.9472

Epoch 961: val_loss did not improve from 16.94503
196/196 - 63s - loss: 16.4080 - MinusLogProbMetric: 16.4080 - val_loss: 16.9472 - val_MinusLogProbMetric: 16.9472 - lr: 1.3021e-06 - 63s/epoch - 320ms/step
Epoch 962/1000
2023-09-27 08:45:12.800 
Epoch 962/1000 
	 loss: 16.4084, MinusLogProbMetric: 16.4084, val_loss: 16.9456, val_MinusLogProbMetric: 16.9456

Epoch 962: val_loss did not improve from 16.94503
196/196 - 62s - loss: 16.4084 - MinusLogProbMetric: 16.4084 - val_loss: 16.9456 - val_MinusLogProbMetric: 16.9456 - lr: 1.3021e-06 - 62s/epoch - 317ms/step
Epoch 963/1000
2023-09-27 08:46:14.600 
Epoch 963/1000 
	 loss: 16.4082, MinusLogProbMetric: 16.4082, val_loss: 16.9471, val_MinusLogProbMetric: 16.9471

Epoch 963: val_loss did not improve from 16.94503
196/196 - 62s - loss: 16.4082 - MinusLogProbMetric: 16.4082 - val_loss: 16.9471 - val_MinusLogProbMetric: 16.9471 - lr: 1.3021e-06 - 62s/epoch - 315ms/step
Epoch 964/1000
2023-09-27 08:47:17.585 
Epoch 964/1000 
	 loss: 16.4083, MinusLogProbMetric: 16.4083, val_loss: 16.9457, val_MinusLogProbMetric: 16.9457

Epoch 964: val_loss did not improve from 16.94503
196/196 - 63s - loss: 16.4083 - MinusLogProbMetric: 16.4083 - val_loss: 16.9457 - val_MinusLogProbMetric: 16.9457 - lr: 1.3021e-06 - 63s/epoch - 321ms/step
Epoch 965/1000
2023-09-27 08:48:19.500 
Epoch 965/1000 
	 loss: 16.4080, MinusLogProbMetric: 16.4080, val_loss: 16.9465, val_MinusLogProbMetric: 16.9465

Epoch 965: val_loss did not improve from 16.94503
196/196 - 62s - loss: 16.4080 - MinusLogProbMetric: 16.4080 - val_loss: 16.9465 - val_MinusLogProbMetric: 16.9465 - lr: 1.3021e-06 - 62s/epoch - 316ms/step
Epoch 966/1000
2023-09-27 08:49:20.117 
Epoch 966/1000 
	 loss: 16.4080, MinusLogProbMetric: 16.4080, val_loss: 16.9472, val_MinusLogProbMetric: 16.9472

Epoch 966: val_loss did not improve from 16.94503
196/196 - 61s - loss: 16.4080 - MinusLogProbMetric: 16.4080 - val_loss: 16.9472 - val_MinusLogProbMetric: 16.9472 - lr: 1.3021e-06 - 61s/epoch - 309ms/step
Epoch 967/1000
2023-09-27 08:50:22.678 
Epoch 967/1000 
	 loss: 16.4082, MinusLogProbMetric: 16.4082, val_loss: 16.9458, val_MinusLogProbMetric: 16.9458

Epoch 967: val_loss did not improve from 16.94503
196/196 - 63s - loss: 16.4082 - MinusLogProbMetric: 16.4082 - val_loss: 16.9458 - val_MinusLogProbMetric: 16.9458 - lr: 1.3021e-06 - 63s/epoch - 319ms/step
Epoch 968/1000
2023-09-27 08:51:25.444 
Epoch 968/1000 
	 loss: 16.4082, MinusLogProbMetric: 16.4082, val_loss: 16.9452, val_MinusLogProbMetric: 16.9452

Epoch 968: val_loss did not improve from 16.94503
196/196 - 63s - loss: 16.4082 - MinusLogProbMetric: 16.4082 - val_loss: 16.9452 - val_MinusLogProbMetric: 16.9452 - lr: 1.3021e-06 - 63s/epoch - 320ms/step
Epoch 969/1000
2023-09-27 08:52:27.728 
Epoch 969/1000 
	 loss: 16.4080, MinusLogProbMetric: 16.4080, val_loss: 16.9469, val_MinusLogProbMetric: 16.9469

Epoch 969: val_loss did not improve from 16.94503
196/196 - 62s - loss: 16.4080 - MinusLogProbMetric: 16.4080 - val_loss: 16.9469 - val_MinusLogProbMetric: 16.9469 - lr: 1.3021e-06 - 62s/epoch - 318ms/step
Epoch 970/1000
2023-09-27 08:53:30.045 
Epoch 970/1000 
	 loss: 16.4082, MinusLogProbMetric: 16.4082, val_loss: 16.9453, val_MinusLogProbMetric: 16.9453

Epoch 970: val_loss did not improve from 16.94503
196/196 - 62s - loss: 16.4082 - MinusLogProbMetric: 16.4082 - val_loss: 16.9453 - val_MinusLogProbMetric: 16.9453 - lr: 1.3021e-06 - 62s/epoch - 318ms/step
Epoch 971/1000
2023-09-27 08:54:33.050 
Epoch 971/1000 
	 loss: 16.4083, MinusLogProbMetric: 16.4083, val_loss: 16.9452, val_MinusLogProbMetric: 16.9452

Epoch 971: val_loss did not improve from 16.94503
Restoring model weights from the end of the best epoch: 871.
196/196 - 64s - loss: 16.4083 - MinusLogProbMetric: 16.4083 - val_loss: 16.9452 - val_MinusLogProbMetric: 16.9452 - lr: 1.3021e-06 - 64s/epoch - 325ms/step
Epoch 971: early stopping
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.

------------------------------------------
Starting LR metric calculation...
Running TF LR calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
LR metric calculation completed in 29.041474058001768 seconds.

------------------------------------------
Starting KS tests calculation...
Running TF KS tests...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
KS tests calculation completed in 14.099685689026956 seconds.

------------------------------------------
Starting SWD metric calculation...
Running TF SWD calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
SWD metric calculation completed in 10.994717641966417 seconds.

------------------------------------------
Starting FN metric calculation...
Running TF FN calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
FN metric calculation completed in 11.132849710003939 seconds.
Training succeeded with seed 721.
Model trained in 62556.55 s.

===========
Computing predictions
===========

Computing metrics...
Metrics computed in 67.00 s.
                ===========
                print("===========
Failed to plot
===========
")
                Exception type: ValueError
                Exception message: Number of rows must be a positive integer, not 0
                Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN.py , Line : 469, Func.Name : prediction_function, Message : Plotters.cornerplotter(X_data_test.numpy(),X_data_nf.numpy(),path_to_results,ndims,norm=True) # type: ignore', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/Plotters.py , Line : 156, Func.Name : cornerplotter, Message : figure=corner.corner(target_samples,color=\'red\',bins=n_bins,labels=[r"%s" % s for s in labels],normalize1d=True)', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/corner.py , Line : 235, Func.Name : corner, Message : fig, axes = pl.subplots(K, K, figsize=(dim, dim))', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/pyplot.py , Line : 1502, Func.Name : subplots, Message : axs = fig.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 905, Func.Name : subplots, Message : gs = self.add_gridspec(nrows, ncols, figure=self, **gridspec_kw)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 1527, Func.Name : add_gridspec, Message : gs = GridSpec(nrows=nrows, ncols=ncols, figure=self, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 379, Func.Name : __init__, Message : super().__init__(nrows, ncols,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 49, Func.Name : __init__, Message : raise ValueError(']
                ===========

results.txt saved
results.json saved
Results log saved
Model predictions computed in 67.21 s.
===========
Run 294/720 done in 62796.92 s.
===========

Directory ../../results/CsplineN_new/run_295/ already exists.
Skipping it.
===========
Run 295/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_296/ already exists.
Skipping it.
===========
Run 296/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_297/ already exists.
Skipping it.
===========
Run 297/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_298/ already exists.
Skipping it.
===========
Run 298/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_299/ already exists.
Skipping it.
===========
Run 299/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_300/ already exists.
Skipping it.
===========
Run 300/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_301/ already exists.
Skipping it.
===========
Run 301/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_302/ already exists.
Skipping it.
===========
Run 302/720 already exists. Skipping it.
===========

===========
Generating train data for run 303.
===========
Train data generated in 0.23 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_303/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_303/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.7724807 ,  3.5952213 ,  8.442487  , ...,  7.3644695 ,
         3.6588435 ,  2.0248146 ],
       [ 6.6242166 ,  7.1665483 ,  6.4277024 , ...,  3.1194153 ,
         2.6248946 ,  8.037665  ],
       [ 4.5211086 ,  5.2663136 , -0.6501303 , ...,  0.17722613,
         6.462096  ,  1.3170475 ],
       ...,
       [ 5.2085795 ,  5.348097  , -0.4658583 , ..., -0.03725696,
         5.9864964 ,  1.2388526 ],
       [ 3.9188156 ,  5.4859467 , -0.29968688, ...,  0.4149692 ,
         6.0867124 ,  1.4174792 ],
       [ 2.283803  ,  2.9701724 ,  8.729265  , ...,  6.0229187 ,
         3.2842712 ,  2.1182036 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_303/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_303
self.data_kwargs: {'seed': 869}
self.x_data: [[ 1.7417877   3.5044646   9.194398   ...  7.377848    2.8874114
   1.7757134 ]
 [ 3.8693519   4.134066    8.323213   ...  7.5259514   3.079576
   1.8540689 ]
 [ 2.7888105   3.5250566   6.8470335  ...  7.0512147   2.9893377
   1.7142482 ]
 ...
 [ 3.8914979   5.948059   -0.25578696 ...  1.7629418   6.8860593
   1.464444  ]
 [ 3.0775785   3.9001913   8.887795   ...  7.2041593   3.5200696
   1.5176823 ]
 [ 1.0419946   3.6002321   7.1163735  ...  7.3799496   3.218665
   1.6207042 ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_38"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_39 (InputLayer)       [(None, 32)]              0         
                                                                 
 log_prob_layer_3 (LogProbLa  (None,)                  1074400   
 yer)                                                            
                                                                 
=================================================================
Total params: 1,074,400
Trainable params: 1,074,400
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_3/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_3'")
self.model: <keras.engine.functional.Functional object at 0x7fb1cc3c3bb0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fb1cc3d50f0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fb1cc3d50f0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fb1cc39f1c0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fb1cc3bbc40>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fb1cc3bafe0>, <keras.callbacks.ModelCheckpoint object at 0x7fb1cc3bb490>, <keras.callbacks.EarlyStopping object at 0x7fb1cc3ba740>, <keras.callbacks.ReduceLROnPlateau object at 0x7fb1cc3ba7d0>, <keras.callbacks.TerminateOnNaN object at 0x7fb1cc3bb1c0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.7724807 ,  3.5952213 ,  8.442487  , ...,  7.3644695 ,
         3.6588435 ,  2.0248146 ],
       [ 6.6242166 ,  7.1665483 ,  6.4277024 , ...,  3.1194153 ,
         2.6248946 ,  8.037665  ],
       [ 4.5211086 ,  5.2663136 , -0.6501303 , ...,  0.17722613,
         6.462096  ,  1.3170475 ],
       ...,
       [ 5.2085795 ,  5.348097  , -0.4658583 , ..., -0.03725696,
         5.9864964 ,  1.2388526 ],
       [ 3.9188156 ,  5.4859467 , -0.29968688, ...,  0.4149692 ,
         6.0867124 ,  1.4174792 ],
       [ 2.283803  ,  2.9701724 ,  8.729265  , ...,  6.0229187 ,
         3.2842712 ,  2.1182036 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_303/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 303/720 with hyperparameters:
timestamp = 2023-09-27 08:55:56.536940
ndims = 32
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1074400
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 1.7417877   3.5044646   9.194398    1.0621872   8.910313    1.2448134
  9.752992    4.795541   10.214517    5.8200784   7.3571367   0.42017213
  3.0764782   1.1773593   3.1572356   1.3086243   2.6951017   5.712275
  0.4469142   6.9349284   5.5393224   1.8138231   6.1143165   1.1455625
  7.183009    9.098161    3.4021838   5.9704523   0.79392874  7.377848
  2.8874114   1.7757134 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 2: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-27 08:59:21.856 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 3399.5613, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 205s - loss: nan - MinusLogProbMetric: 3399.5613 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 205s/epoch - 1s/step
The loss history contains NaN values.
Training failed: trying again with seed 235536 and lr 0.0003333333333333333.
===========
Generating train data for run 303.
===========
Train data generated in 0.28 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_303/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_303/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.7724807 ,  3.5952213 ,  8.442487  , ...,  7.3644695 ,
         3.6588435 ,  2.0248146 ],
       [ 6.6242166 ,  7.1665483 ,  6.4277024 , ...,  3.1194153 ,
         2.6248946 ,  8.037665  ],
       [ 4.5211086 ,  5.2663136 , -0.6501303 , ...,  0.17722613,
         6.462096  ,  1.3170475 ],
       ...,
       [ 5.2085795 ,  5.348097  , -0.4658583 , ..., -0.03725696,
         5.9864964 ,  1.2388526 ],
       [ 3.9188156 ,  5.4859467 , -0.29968688, ...,  0.4149692 ,
         6.0867124 ,  1.4174792 ],
       [ 2.283803  ,  2.9701724 ,  8.729265  , ...,  6.0229187 ,
         3.2842712 ,  2.1182036 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_303/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_303
self.data_kwargs: {'seed': 869}
self.x_data: [[ 1.7417877   3.5044646   9.194398   ...  7.377848    2.8874114
   1.7757134 ]
 [ 3.8693519   4.134066    8.323213   ...  7.5259514   3.079576
   1.8540689 ]
 [ 2.7888105   3.5250566   6.8470335  ...  7.0512147   2.9893377
   1.7142482 ]
 ...
 [ 3.8914979   5.948059   -0.25578696 ...  1.7629418   6.8860593
   1.464444  ]
 [ 3.0775785   3.9001913   8.887795   ...  7.2041593   3.5200696
   1.5176823 ]
 [ 1.0419946   3.6002321   7.1163735  ...  7.3799496   3.218665
   1.6207042 ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_49"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_50 (InputLayer)       [(None, 32)]              0         
                                                                 
 log_prob_layer_4 (LogProbLa  (None,)                  1074400   
 yer)                                                            
                                                                 
=================================================================
Total params: 1,074,400
Trainable params: 1,074,400
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_4/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_4'")
self.model: <keras.engine.functional.Functional object at 0x7fb8a97ce5c0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fb8b1a13250>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fb8b1a13250>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fb8a983d660>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fb8a925be50>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fb8a928c400>, <keras.callbacks.ModelCheckpoint object at 0x7fb8a928c4c0>, <keras.callbacks.EarlyStopping object at 0x7fb8a928c730>, <keras.callbacks.ReduceLROnPlateau object at 0x7fb8a928c760>, <keras.callbacks.TerminateOnNaN object at 0x7fb8a928c3a0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.7724807 ,  3.5952213 ,  8.442487  , ...,  7.3644695 ,
         3.6588435 ,  2.0248146 ],
       [ 6.6242166 ,  7.1665483 ,  6.4277024 , ...,  3.1194153 ,
         2.6248946 ,  8.037665  ],
       [ 4.5211086 ,  5.2663136 , -0.6501303 , ...,  0.17722613,
         6.462096  ,  1.3170475 ],
       ...,
       [ 5.2085795 ,  5.348097  , -0.4658583 , ..., -0.03725696,
         5.9864964 ,  1.2388526 ],
       [ 3.9188156 ,  5.4859467 , -0.29968688, ...,  0.4149692 ,
         6.0867124 ,  1.4174792 ],
       [ 2.283803  ,  2.9701724 ,  8.729265  , ...,  6.0229187 ,
         3.2842712 ,  2.1182036 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_303/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 303/720 with hyperparameters:
timestamp = 2023-09-27 08:59:40.451045
ndims = 32
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1074400
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 1.7417877   3.5044646   9.194398    1.0621872   8.910313    1.2448134
  9.752992    4.795541   10.214517    5.8200784   7.3571367   0.42017213
  3.0764782   1.1773593   3.1572356   1.3086243   2.6951017   5.712275
  0.4469142   6.9349284   5.5393224   1.8138231   6.1143165   1.1455625
  7.183009    9.098161    3.4021838   5.9704523   0.79392874  7.377848
  2.8874114   1.7757134 ]
Epoch 1/1000
2023-09-27 09:04:22.022 
Epoch 1/1000 
	 loss: 1458.4661, MinusLogProbMetric: 1458.4661, val_loss: 620.8931, val_MinusLogProbMetric: 620.8931

Epoch 1: val_loss improved from inf to 620.89313, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 282s - loss: 1458.4661 - MinusLogProbMetric: 1458.4661 - val_loss: 620.8931 - val_MinusLogProbMetric: 620.8931 - lr: 3.3333e-04 - 282s/epoch - 1s/step
Epoch 2/1000
2023-09-27 09:05:39.641 
Epoch 2/1000 
	 loss: 455.8180, MinusLogProbMetric: 455.8180, val_loss: 395.9218, val_MinusLogProbMetric: 395.9218

Epoch 2: val_loss improved from 620.89313 to 395.92175, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 77s - loss: 455.8180 - MinusLogProbMetric: 455.8180 - val_loss: 395.9218 - val_MinusLogProbMetric: 395.9218 - lr: 3.3333e-04 - 77s/epoch - 394ms/step
Epoch 3/1000
2023-09-27 09:06:57.380 
Epoch 3/1000 
	 loss: 397.5887, MinusLogProbMetric: 397.5887, val_loss: 362.0425, val_MinusLogProbMetric: 362.0425

Epoch 3: val_loss improved from 395.92175 to 362.04254, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 78s - loss: 397.5887 - MinusLogProbMetric: 397.5887 - val_loss: 362.0425 - val_MinusLogProbMetric: 362.0425 - lr: 3.3333e-04 - 78s/epoch - 397ms/step
Epoch 4/1000
2023-09-27 09:08:15.774 
Epoch 4/1000 
	 loss: 328.5315, MinusLogProbMetric: 328.5315, val_loss: 213.5175, val_MinusLogProbMetric: 213.5175

Epoch 4: val_loss improved from 362.04254 to 213.51753, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 78s - loss: 328.5315 - MinusLogProbMetric: 328.5315 - val_loss: 213.5175 - val_MinusLogProbMetric: 213.5175 - lr: 3.3333e-04 - 78s/epoch - 399ms/step
Epoch 5/1000
2023-09-27 09:09:34.623 
Epoch 5/1000 
	 loss: 410.5398, MinusLogProbMetric: 410.5398, val_loss: 366.7348, val_MinusLogProbMetric: 366.7348

Epoch 5: val_loss did not improve from 213.51753
196/196 - 78s - loss: 410.5398 - MinusLogProbMetric: 410.5398 - val_loss: 366.7348 - val_MinusLogProbMetric: 366.7348 - lr: 3.3333e-04 - 78s/epoch - 396ms/step
Epoch 6/1000
2023-09-27 09:10:51.702 
Epoch 6/1000 
	 loss: 297.8697, MinusLogProbMetric: 297.8697, val_loss: 273.6456, val_MinusLogProbMetric: 273.6456

Epoch 6: val_loss did not improve from 213.51753
196/196 - 77s - loss: 297.8697 - MinusLogProbMetric: 297.8697 - val_loss: 273.6456 - val_MinusLogProbMetric: 273.6456 - lr: 3.3333e-04 - 77s/epoch - 393ms/step
Epoch 7/1000
2023-09-27 09:12:08.907 
Epoch 7/1000 
	 loss: 234.5224, MinusLogProbMetric: 234.5224, val_loss: 209.6550, val_MinusLogProbMetric: 209.6550

Epoch 7: val_loss improved from 213.51753 to 209.65503, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 78s - loss: 234.5224 - MinusLogProbMetric: 234.5224 - val_loss: 209.6550 - val_MinusLogProbMetric: 209.6550 - lr: 3.3333e-04 - 78s/epoch - 400ms/step
Epoch 8/1000
2023-09-27 09:13:27.776 
Epoch 8/1000 
	 loss: 227.6421, MinusLogProbMetric: 227.6421, val_loss: 199.4579, val_MinusLogProbMetric: 199.4579

Epoch 8: val_loss improved from 209.65503 to 199.45787, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 79s - loss: 227.6421 - MinusLogProbMetric: 227.6421 - val_loss: 199.4579 - val_MinusLogProbMetric: 199.4579 - lr: 3.3333e-04 - 79s/epoch - 403ms/step
Epoch 9/1000
2023-09-27 09:14:46.809 
Epoch 9/1000 
	 loss: 182.0825, MinusLogProbMetric: 182.0825, val_loss: 167.6141, val_MinusLogProbMetric: 167.6141

Epoch 9: val_loss improved from 199.45787 to 167.61411, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 79s - loss: 182.0825 - MinusLogProbMetric: 182.0825 - val_loss: 167.6141 - val_MinusLogProbMetric: 167.6141 - lr: 3.3333e-04 - 79s/epoch - 404ms/step
Epoch 10/1000
2023-09-27 09:16:05.868 
Epoch 10/1000 
	 loss: 168.3805, MinusLogProbMetric: 168.3805, val_loss: 191.7699, val_MinusLogProbMetric: 191.7699

Epoch 10: val_loss did not improve from 167.61411
196/196 - 78s - loss: 168.3805 - MinusLogProbMetric: 168.3805 - val_loss: 191.7699 - val_MinusLogProbMetric: 191.7699 - lr: 3.3333e-04 - 78s/epoch - 397ms/step
Epoch 11/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 18: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-27 09:16:17.660 
Epoch 11/1000 
	 loss: nan, MinusLogProbMetric: 223.2670, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 11: val_loss did not improve from 167.61411
196/196 - 12s - loss: nan - MinusLogProbMetric: 223.2670 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 12s/epoch - 60ms/step
The loss history contains NaN values.
Training failed: trying again with seed 235536 and lr 0.0001111111111111111.
===========
Generating train data for run 303.
===========
Train data generated in 0.31 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_303/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_303/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.7724807 ,  3.5952213 ,  8.442487  , ...,  7.3644695 ,
         3.6588435 ,  2.0248146 ],
       [ 6.6242166 ,  7.1665483 ,  6.4277024 , ...,  3.1194153 ,
         2.6248946 ,  8.037665  ],
       [ 4.5211086 ,  5.2663136 , -0.6501303 , ...,  0.17722613,
         6.462096  ,  1.3170475 ],
       ...,
       [ 5.2085795 ,  5.348097  , -0.4658583 , ..., -0.03725696,
         5.9864964 ,  1.2388526 ],
       [ 3.9188156 ,  5.4859467 , -0.29968688, ...,  0.4149692 ,
         6.0867124 ,  1.4174792 ],
       [ 2.283803  ,  2.9701724 ,  8.729265  , ...,  6.0229187 ,
         3.2842712 ,  2.1182036 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_303/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_303
self.data_kwargs: {'seed': 869}
self.x_data: [[ 1.7417877   3.5044646   9.194398   ...  7.377848    2.8874114
   1.7757134 ]
 [ 3.8693519   4.134066    8.323213   ...  7.5259514   3.079576
   1.8540689 ]
 [ 2.7888105   3.5250566   6.8470335  ...  7.0512147   2.9893377
   1.7142482 ]
 ...
 [ 3.8914979   5.948059   -0.25578696 ...  1.7629418   6.8860593
   1.464444  ]
 [ 3.0775785   3.9001913   8.887795   ...  7.2041593   3.5200696
   1.5176823 ]
 [ 1.0419946   3.6002321   7.1163735  ...  7.3799496   3.218665
   1.6207042 ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_60"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_61 (InputLayer)       [(None, 32)]              0         
                                                                 
 log_prob_layer_5 (LogProbLa  (None,)                  1074400   
 yer)                                                            
                                                                 
=================================================================
Total params: 1,074,400
Trainable params: 1,074,400
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_5/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_5'")
self.model: <keras.engine.functional.Functional object at 0x7fb21c7b53f0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fb38c2b7670>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fb38c2b7670>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fb488500400>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fb8e5cd0c10>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fb8e5cd1180>, <keras.callbacks.ModelCheckpoint object at 0x7fb8e5cd1240>, <keras.callbacks.EarlyStopping object at 0x7fb8e5cd14b0>, <keras.callbacks.ReduceLROnPlateau object at 0x7fb8e5cd14e0>, <keras.callbacks.TerminateOnNaN object at 0x7fb8e5cd1120>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.7724807 ,  3.5952213 ,  8.442487  , ...,  7.3644695 ,
         3.6588435 ,  2.0248146 ],
       [ 6.6242166 ,  7.1665483 ,  6.4277024 , ...,  3.1194153 ,
         2.6248946 ,  8.037665  ],
       [ 4.5211086 ,  5.2663136 , -0.6501303 , ...,  0.17722613,
         6.462096  ,  1.3170475 ],
       ...,
       [ 5.2085795 ,  5.348097  , -0.4658583 , ..., -0.03725696,
         5.9864964 ,  1.2388526 ],
       [ 3.9188156 ,  5.4859467 , -0.29968688, ...,  0.4149692 ,
         6.0867124 ,  1.4174792 ],
       [ 2.283803  ,  2.9701724 ,  8.729265  , ...,  6.0229187 ,
         3.2842712 ,  2.1182036 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 303/720 with hyperparameters:
timestamp = 2023-09-27 09:16:31.810069
ndims = 32
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1074400
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 1.7417877   3.5044646   9.194398    1.0621872   8.910313    1.2448134
  9.752992    4.795541   10.214517    5.8200784   7.3571367   0.42017213
  3.0764782   1.1773593   3.1572356   1.3086243   2.6951017   5.712275
  0.4469142   6.9349284   5.5393224   1.8138231   6.1143165   1.1455625
  7.183009    9.098161    3.4021838   5.9704523   0.79392874  7.377848
  2.8874114   1.7757134 ]
Epoch 1/1000
2023-09-27 09:21:14.400 
Epoch 1/1000 
	 loss: 159.2656, MinusLogProbMetric: 159.2656, val_loss: 133.2414, val_MinusLogProbMetric: 133.2414

Epoch 1: val_loss improved from inf to 133.24144, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 284s - loss: 159.2656 - MinusLogProbMetric: 159.2656 - val_loss: 133.2414 - val_MinusLogProbMetric: 133.2414 - lr: 1.1111e-04 - 284s/epoch - 1s/step
Epoch 2/1000
2023-09-27 09:22:29.585 
Epoch 2/1000 
	 loss: 278.1949, MinusLogProbMetric: 278.1949, val_loss: 188.7014, val_MinusLogProbMetric: 188.7014

Epoch 2: val_loss did not improve from 133.24144
196/196 - 73s - loss: 278.1949 - MinusLogProbMetric: 278.1949 - val_loss: 188.7014 - val_MinusLogProbMetric: 188.7014 - lr: 1.1111e-04 - 73s/epoch - 373ms/step
Epoch 3/1000
2023-09-27 09:23:40.121 
Epoch 3/1000 
	 loss: 158.4761, MinusLogProbMetric: 158.4761, val_loss: 139.7460, val_MinusLogProbMetric: 139.7460

Epoch 3: val_loss did not improve from 133.24144
196/196 - 71s - loss: 158.4761 - MinusLogProbMetric: 158.4761 - val_loss: 139.7460 - val_MinusLogProbMetric: 139.7460 - lr: 1.1111e-04 - 71s/epoch - 360ms/step
Epoch 4/1000
2023-09-27 09:24:49.169 
Epoch 4/1000 
	 loss: 149.3029, MinusLogProbMetric: 149.3029, val_loss: 123.5835, val_MinusLogProbMetric: 123.5835

Epoch 4: val_loss improved from 133.24144 to 123.58349, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 70s - loss: 149.3029 - MinusLogProbMetric: 149.3029 - val_loss: 123.5835 - val_MinusLogProbMetric: 123.5835 - lr: 1.1111e-04 - 70s/epoch - 359ms/step
Epoch 5/1000
2023-09-27 09:26:03.925 
Epoch 5/1000 
	 loss: 206.0567, MinusLogProbMetric: 206.0567, val_loss: 324.1090, val_MinusLogProbMetric: 324.1090

Epoch 5: val_loss did not improve from 123.58349
196/196 - 73s - loss: 206.0567 - MinusLogProbMetric: 206.0567 - val_loss: 324.1090 - val_MinusLogProbMetric: 324.1090 - lr: 1.1111e-04 - 73s/epoch - 374ms/step
Epoch 6/1000
2023-09-27 09:27:12.147 
Epoch 6/1000 
	 loss: 232.2507, MinusLogProbMetric: 232.2507, val_loss: 188.3406, val_MinusLogProbMetric: 188.3406

Epoch 6: val_loss did not improve from 123.58349
196/196 - 68s - loss: 232.2507 - MinusLogProbMetric: 232.2507 - val_loss: 188.3406 - val_MinusLogProbMetric: 188.3406 - lr: 1.1111e-04 - 68s/epoch - 348ms/step
Epoch 7/1000
2023-09-27 09:28:26.033 
Epoch 7/1000 
	 loss: 191.5578, MinusLogProbMetric: 191.5578, val_loss: 154.6570, val_MinusLogProbMetric: 154.6570

Epoch 7: val_loss did not improve from 123.58349
196/196 - 74s - loss: 191.5578 - MinusLogProbMetric: 191.5578 - val_loss: 154.6570 - val_MinusLogProbMetric: 154.6570 - lr: 1.1111e-04 - 74s/epoch - 377ms/step
Epoch 8/1000
2023-09-27 09:29:42.757 
Epoch 8/1000 
	 loss: 142.4355, MinusLogProbMetric: 142.4355, val_loss: 124.4979, val_MinusLogProbMetric: 124.4979

Epoch 8: val_loss did not improve from 123.58349
196/196 - 77s - loss: 142.4355 - MinusLogProbMetric: 142.4355 - val_loss: 124.4979 - val_MinusLogProbMetric: 124.4979 - lr: 1.1111e-04 - 77s/epoch - 391ms/step
Epoch 9/1000
2023-09-27 09:30:58.579 
Epoch 9/1000 
	 loss: 115.4331, MinusLogProbMetric: 115.4331, val_loss: 104.3615, val_MinusLogProbMetric: 104.3615

Epoch 9: val_loss improved from 123.58349 to 104.36149, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 77s - loss: 115.4331 - MinusLogProbMetric: 115.4331 - val_loss: 104.3615 - val_MinusLogProbMetric: 104.3615 - lr: 1.1111e-04 - 77s/epoch - 395ms/step
Epoch 10/1000
2023-09-27 09:32:16.727 
Epoch 10/1000 
	 loss: 96.9399, MinusLogProbMetric: 96.9399, val_loss: 90.8583, val_MinusLogProbMetric: 90.8583

Epoch 10: val_loss improved from 104.36149 to 90.85825, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 78s - loss: 96.9399 - MinusLogProbMetric: 96.9399 - val_loss: 90.8583 - val_MinusLogProbMetric: 90.8583 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 11/1000
2023-09-27 09:33:33.372 
Epoch 11/1000 
	 loss: 86.6003, MinusLogProbMetric: 86.6003, val_loss: 81.6955, val_MinusLogProbMetric: 81.6955

Epoch 11: val_loss improved from 90.85825 to 81.69550, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 76s - loss: 86.6003 - MinusLogProbMetric: 86.6003 - val_loss: 81.6955 - val_MinusLogProbMetric: 81.6955 - lr: 1.1111e-04 - 76s/epoch - 390ms/step
Epoch 12/1000
2023-09-27 09:34:49.425 
Epoch 12/1000 
	 loss: 79.2312, MinusLogProbMetric: 79.2312, val_loss: 75.6794, val_MinusLogProbMetric: 75.6794

Epoch 12: val_loss improved from 81.69550 to 75.67937, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 76s - loss: 79.2312 - MinusLogProbMetric: 79.2312 - val_loss: 75.6794 - val_MinusLogProbMetric: 75.6794 - lr: 1.1111e-04 - 76s/epoch - 389ms/step
Epoch 13/1000
2023-09-27 09:36:07.113 
Epoch 13/1000 
	 loss: 77.0088, MinusLogProbMetric: 77.0088, val_loss: 121.1237, val_MinusLogProbMetric: 121.1237

Epoch 13: val_loss did not improve from 75.67937
196/196 - 76s - loss: 77.0088 - MinusLogProbMetric: 77.0088 - val_loss: 121.1237 - val_MinusLogProbMetric: 121.1237 - lr: 1.1111e-04 - 76s/epoch - 389ms/step
Epoch 14/1000
2023-09-27 09:37:23.508 
Epoch 14/1000 
	 loss: 85.5992, MinusLogProbMetric: 85.5992, val_loss: 78.1704, val_MinusLogProbMetric: 78.1704

Epoch 14: val_loss did not improve from 75.67937
196/196 - 76s - loss: 85.5992 - MinusLogProbMetric: 85.5992 - val_loss: 78.1704 - val_MinusLogProbMetric: 78.1704 - lr: 1.1111e-04 - 76s/epoch - 390ms/step
Epoch 15/1000
2023-09-27 09:38:40.130 
Epoch 15/1000 
	 loss: 68.8380, MinusLogProbMetric: 68.8380, val_loss: 65.2552, val_MinusLogProbMetric: 65.2552

Epoch 15: val_loss improved from 75.67937 to 65.25520, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 78s - loss: 68.8380 - MinusLogProbMetric: 68.8380 - val_loss: 65.2552 - val_MinusLogProbMetric: 65.2552 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 16/1000
2023-09-27 09:39:57.675 
Epoch 16/1000 
	 loss: 63.1868, MinusLogProbMetric: 63.1868, val_loss: 60.9859, val_MinusLogProbMetric: 60.9859

Epoch 16: val_loss improved from 65.25520 to 60.98592, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 77s - loss: 63.1868 - MinusLogProbMetric: 63.1868 - val_loss: 60.9859 - val_MinusLogProbMetric: 60.9859 - lr: 1.1111e-04 - 77s/epoch - 394ms/step
Epoch 17/1000
2023-09-27 09:41:14.018 
Epoch 17/1000 
	 loss: 65.6041, MinusLogProbMetric: 65.6041, val_loss: 59.5812, val_MinusLogProbMetric: 59.5812

Epoch 17: val_loss improved from 60.98592 to 59.58117, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 77s - loss: 65.6041 - MinusLogProbMetric: 65.6041 - val_loss: 59.5812 - val_MinusLogProbMetric: 59.5812 - lr: 1.1111e-04 - 77s/epoch - 391ms/step
Epoch 18/1000
2023-09-27 09:42:31.165 
Epoch 18/1000 
	 loss: 57.4862, MinusLogProbMetric: 57.4862, val_loss: 56.2203, val_MinusLogProbMetric: 56.2203

Epoch 18: val_loss improved from 59.58117 to 56.22027, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 77s - loss: 57.4862 - MinusLogProbMetric: 57.4862 - val_loss: 56.2203 - val_MinusLogProbMetric: 56.2203 - lr: 1.1111e-04 - 77s/epoch - 392ms/step
Epoch 19/1000
2023-09-27 09:43:46.737 
Epoch 19/1000 
	 loss: 54.7584, MinusLogProbMetric: 54.7584, val_loss: 53.3530, val_MinusLogProbMetric: 53.3530

Epoch 19: val_loss improved from 56.22027 to 53.35300, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 76s - loss: 54.7584 - MinusLogProbMetric: 54.7584 - val_loss: 53.3530 - val_MinusLogProbMetric: 53.3530 - lr: 1.1111e-04 - 76s/epoch - 388ms/step
Epoch 20/1000
2023-09-27 09:45:04.895 
Epoch 20/1000 
	 loss: 52.7356, MinusLogProbMetric: 52.7356, val_loss: 51.7118, val_MinusLogProbMetric: 51.7118

Epoch 20: val_loss improved from 53.35300 to 51.71180, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 78s - loss: 52.7356 - MinusLogProbMetric: 52.7356 - val_loss: 51.7118 - val_MinusLogProbMetric: 51.7118 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 21/1000
2023-09-27 09:46:21.781 
Epoch 21/1000 
	 loss: 50.6293, MinusLogProbMetric: 50.6293, val_loss: 49.9819, val_MinusLogProbMetric: 49.9819

Epoch 21: val_loss improved from 51.71180 to 49.98193, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 77s - loss: 50.6293 - MinusLogProbMetric: 50.6293 - val_loss: 49.9819 - val_MinusLogProbMetric: 49.9819 - lr: 1.1111e-04 - 77s/epoch - 393ms/step
Epoch 22/1000
2023-09-27 09:47:39.310 
Epoch 22/1000 
	 loss: 49.7825, MinusLogProbMetric: 49.7825, val_loss: 48.4027, val_MinusLogProbMetric: 48.4027

Epoch 22: val_loss improved from 49.98193 to 48.40274, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 78s - loss: 49.7825 - MinusLogProbMetric: 49.7825 - val_loss: 48.4027 - val_MinusLogProbMetric: 48.4027 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 23/1000
2023-09-27 09:48:57.090 
Epoch 23/1000 
	 loss: 47.6896, MinusLogProbMetric: 47.6896, val_loss: 46.4105, val_MinusLogProbMetric: 46.4105

Epoch 23: val_loss improved from 48.40274 to 46.41055, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 78s - loss: 47.6896 - MinusLogProbMetric: 47.6896 - val_loss: 46.4105 - val_MinusLogProbMetric: 46.4105 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 24/1000
2023-09-27 09:50:14.394 
Epoch 24/1000 
	 loss: 45.8362, MinusLogProbMetric: 45.8362, val_loss: 45.0443, val_MinusLogProbMetric: 45.0443

Epoch 24: val_loss improved from 46.41055 to 45.04434, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 77s - loss: 45.8362 - MinusLogProbMetric: 45.8362 - val_loss: 45.0443 - val_MinusLogProbMetric: 45.0443 - lr: 1.1111e-04 - 77s/epoch - 393ms/step
Epoch 25/1000
2023-09-27 09:51:31.350 
Epoch 25/1000 
	 loss: 44.7118, MinusLogProbMetric: 44.7118, val_loss: 43.8180, val_MinusLogProbMetric: 43.8180

Epoch 25: val_loss improved from 45.04434 to 43.81797, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 77s - loss: 44.7118 - MinusLogProbMetric: 44.7118 - val_loss: 43.8180 - val_MinusLogProbMetric: 43.8180 - lr: 1.1111e-04 - 77s/epoch - 394ms/step
Epoch 26/1000
2023-09-27 09:52:48.678 
Epoch 26/1000 
	 loss: 43.2458, MinusLogProbMetric: 43.2458, val_loss: 42.4269, val_MinusLogProbMetric: 42.4269

Epoch 26: val_loss improved from 43.81797 to 42.42692, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 77s - loss: 43.2458 - MinusLogProbMetric: 43.2458 - val_loss: 42.4269 - val_MinusLogProbMetric: 42.4269 - lr: 1.1111e-04 - 77s/epoch - 394ms/step
Epoch 27/1000
2023-09-27 09:54:07.922 
Epoch 27/1000 
	 loss: 43.3696, MinusLogProbMetric: 43.3696, val_loss: 42.2256, val_MinusLogProbMetric: 42.2256

Epoch 27: val_loss improved from 42.42692 to 42.22555, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 79s - loss: 43.3696 - MinusLogProbMetric: 43.3696 - val_loss: 42.2256 - val_MinusLogProbMetric: 42.2256 - lr: 1.1111e-04 - 79s/epoch - 404ms/step
Epoch 28/1000
2023-09-27 09:55:26.270 
Epoch 28/1000 
	 loss: 41.0282, MinusLogProbMetric: 41.0282, val_loss: 40.9192, val_MinusLogProbMetric: 40.9192

Epoch 28: val_loss improved from 42.22555 to 40.91924, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 79s - loss: 41.0282 - MinusLogProbMetric: 41.0282 - val_loss: 40.9192 - val_MinusLogProbMetric: 40.9192 - lr: 1.1111e-04 - 79s/epoch - 402ms/step
Epoch 29/1000
2023-09-27 09:56:44.251 
Epoch 29/1000 
	 loss: 40.0290, MinusLogProbMetric: 40.0290, val_loss: 39.9408, val_MinusLogProbMetric: 39.9408

Epoch 29: val_loss improved from 40.91924 to 39.94079, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 77s - loss: 40.0290 - MinusLogProbMetric: 40.0290 - val_loss: 39.9408 - val_MinusLogProbMetric: 39.9408 - lr: 1.1111e-04 - 77s/epoch - 395ms/step
Epoch 30/1000
2023-09-27 09:58:02.421 
Epoch 30/1000 
	 loss: 39.0317, MinusLogProbMetric: 39.0317, val_loss: 38.7660, val_MinusLogProbMetric: 38.7660

Epoch 30: val_loss improved from 39.94079 to 38.76596, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 78s - loss: 39.0317 - MinusLogProbMetric: 39.0317 - val_loss: 38.7660 - val_MinusLogProbMetric: 38.7660 - lr: 1.1111e-04 - 78s/epoch - 400ms/step
Epoch 31/1000
2023-09-27 09:59:20.159 
Epoch 31/1000 
	 loss: 38.4025, MinusLogProbMetric: 38.4025, val_loss: 38.3516, val_MinusLogProbMetric: 38.3516

Epoch 31: val_loss improved from 38.76596 to 38.35158, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 78s - loss: 38.4025 - MinusLogProbMetric: 38.4025 - val_loss: 38.3516 - val_MinusLogProbMetric: 38.3516 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 32/1000
2023-09-27 10:00:39.573 
Epoch 32/1000 
	 loss: 37.7505, MinusLogProbMetric: 37.7505, val_loss: 37.8410, val_MinusLogProbMetric: 37.8410

Epoch 32: val_loss improved from 38.35158 to 37.84096, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 79s - loss: 37.7505 - MinusLogProbMetric: 37.7505 - val_loss: 37.8410 - val_MinusLogProbMetric: 37.8410 - lr: 1.1111e-04 - 79s/epoch - 405ms/step
Epoch 33/1000
2023-09-27 10:01:57.094 
Epoch 33/1000 
	 loss: 37.6254, MinusLogProbMetric: 37.6254, val_loss: 36.3525, val_MinusLogProbMetric: 36.3525

Epoch 33: val_loss improved from 37.84096 to 36.35247, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 78s - loss: 37.6254 - MinusLogProbMetric: 37.6254 - val_loss: 36.3525 - val_MinusLogProbMetric: 36.3525 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 34/1000
2023-09-27 10:03:15.425 
Epoch 34/1000 
	 loss: 36.2256, MinusLogProbMetric: 36.2256, val_loss: 35.9183, val_MinusLogProbMetric: 35.9183

Epoch 34: val_loss improved from 36.35247 to 35.91826, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 78s - loss: 36.2256 - MinusLogProbMetric: 36.2256 - val_loss: 35.9183 - val_MinusLogProbMetric: 35.9183 - lr: 1.1111e-04 - 78s/epoch - 400ms/step
Epoch 35/1000
2023-09-27 10:04:33.057 
Epoch 35/1000 
	 loss: 36.4557, MinusLogProbMetric: 36.4557, val_loss: 35.5047, val_MinusLogProbMetric: 35.5047

Epoch 35: val_loss improved from 35.91826 to 35.50468, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 77s - loss: 36.4557 - MinusLogProbMetric: 36.4557 - val_loss: 35.5047 - val_MinusLogProbMetric: 35.5047 - lr: 1.1111e-04 - 77s/epoch - 394ms/step
Epoch 36/1000
2023-09-27 10:05:49.652 
Epoch 36/1000 
	 loss: 35.2978, MinusLogProbMetric: 35.2978, val_loss: 35.3315, val_MinusLogProbMetric: 35.3315

Epoch 36: val_loss improved from 35.50468 to 35.33151, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 77s - loss: 35.2978 - MinusLogProbMetric: 35.2978 - val_loss: 35.3315 - val_MinusLogProbMetric: 35.3315 - lr: 1.1111e-04 - 77s/epoch - 392ms/step
Epoch 37/1000
2023-09-27 10:07:05.277 
Epoch 37/1000 
	 loss: 34.7207, MinusLogProbMetric: 34.7207, val_loss: 35.1451, val_MinusLogProbMetric: 35.1451

Epoch 37: val_loss improved from 35.33151 to 35.14506, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 75s - loss: 34.7207 - MinusLogProbMetric: 34.7207 - val_loss: 35.1451 - val_MinusLogProbMetric: 35.1451 - lr: 1.1111e-04 - 75s/epoch - 385ms/step
Epoch 38/1000
2023-09-27 10:08:19.468 
Epoch 38/1000 
	 loss: 34.8898, MinusLogProbMetric: 34.8898, val_loss: 34.0649, val_MinusLogProbMetric: 34.0649

Epoch 38: val_loss improved from 35.14506 to 34.06489, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 75s - loss: 34.8898 - MinusLogProbMetric: 34.8898 - val_loss: 34.0649 - val_MinusLogProbMetric: 34.0649 - lr: 1.1111e-04 - 75s/epoch - 380ms/step
Epoch 39/1000
2023-09-27 10:09:39.658 
Epoch 39/1000 
	 loss: 33.7649, MinusLogProbMetric: 33.7649, val_loss: 33.4453, val_MinusLogProbMetric: 33.4453

Epoch 39: val_loss improved from 34.06489 to 33.44535, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 80s - loss: 33.7649 - MinusLogProbMetric: 33.7649 - val_loss: 33.4453 - val_MinusLogProbMetric: 33.4453 - lr: 1.1111e-04 - 80s/epoch - 409ms/step
Epoch 40/1000
2023-09-27 10:10:56.671 
Epoch 40/1000 
	 loss: 33.3765, MinusLogProbMetric: 33.3765, val_loss: 33.1899, val_MinusLogProbMetric: 33.1899

Epoch 40: val_loss improved from 33.44535 to 33.18993, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 77s - loss: 33.3765 - MinusLogProbMetric: 33.3765 - val_loss: 33.1899 - val_MinusLogProbMetric: 33.1899 - lr: 1.1111e-04 - 77s/epoch - 393ms/step
Epoch 41/1000
2023-09-27 10:12:14.065 
Epoch 41/1000 
	 loss: 32.9197, MinusLogProbMetric: 32.9197, val_loss: 32.8516, val_MinusLogProbMetric: 32.8516

Epoch 41: val_loss improved from 33.18993 to 32.85157, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 77s - loss: 32.9197 - MinusLogProbMetric: 32.9197 - val_loss: 32.8516 - val_MinusLogProbMetric: 32.8516 - lr: 1.1111e-04 - 77s/epoch - 394ms/step
Epoch 42/1000
2023-09-27 10:13:30.755 
Epoch 42/1000 
	 loss: 32.3900, MinusLogProbMetric: 32.3900, val_loss: 32.7911, val_MinusLogProbMetric: 32.7911

Epoch 42: val_loss improved from 32.85157 to 32.79112, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 77s - loss: 32.3900 - MinusLogProbMetric: 32.3900 - val_loss: 32.7911 - val_MinusLogProbMetric: 32.7911 - lr: 1.1111e-04 - 77s/epoch - 393ms/step
Epoch 43/1000
2023-09-27 10:14:48.268 
Epoch 43/1000 
	 loss: 32.0699, MinusLogProbMetric: 32.0699, val_loss: 32.2078, val_MinusLogProbMetric: 32.2078

Epoch 43: val_loss improved from 32.79112 to 32.20779, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 77s - loss: 32.0699 - MinusLogProbMetric: 32.0699 - val_loss: 32.2078 - val_MinusLogProbMetric: 32.2078 - lr: 1.1111e-04 - 77s/epoch - 395ms/step
Epoch 44/1000
2023-09-27 10:16:05.329 
Epoch 44/1000 
	 loss: 31.6941, MinusLogProbMetric: 31.6941, val_loss: 31.3886, val_MinusLogProbMetric: 31.3886

Epoch 44: val_loss improved from 32.20779 to 31.38858, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 77s - loss: 31.6941 - MinusLogProbMetric: 31.6941 - val_loss: 31.3886 - val_MinusLogProbMetric: 31.3886 - lr: 1.1111e-04 - 77s/epoch - 392ms/step
Epoch 45/1000
2023-09-27 10:17:21.814 
Epoch 45/1000 
	 loss: 31.5348, MinusLogProbMetric: 31.5348, val_loss: 31.8336, val_MinusLogProbMetric: 31.8336

Epoch 45: val_loss did not improve from 31.38858
196/196 - 75s - loss: 31.5348 - MinusLogProbMetric: 31.5348 - val_loss: 31.8336 - val_MinusLogProbMetric: 31.8336 - lr: 1.1111e-04 - 75s/epoch - 384ms/step
Epoch 46/1000
2023-09-27 10:18:37.230 
Epoch 46/1000 
	 loss: 31.0969, MinusLogProbMetric: 31.0969, val_loss: 31.1484, val_MinusLogProbMetric: 31.1484

Epoch 46: val_loss improved from 31.38858 to 31.14838, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 77s - loss: 31.0969 - MinusLogProbMetric: 31.0969 - val_loss: 31.1484 - val_MinusLogProbMetric: 31.1484 - lr: 1.1111e-04 - 77s/epoch - 391ms/step
Epoch 47/1000
2023-09-27 10:19:54.771 
Epoch 47/1000 
	 loss: 30.8424, MinusLogProbMetric: 30.8424, val_loss: 31.4583, val_MinusLogProbMetric: 31.4583

Epoch 47: val_loss did not improve from 31.14838
196/196 - 76s - loss: 30.8424 - MinusLogProbMetric: 30.8424 - val_loss: 31.4583 - val_MinusLogProbMetric: 31.4583 - lr: 1.1111e-04 - 76s/epoch - 389ms/step
Epoch 48/1000
2023-09-27 10:21:10.670 
Epoch 48/1000 
	 loss: 31.0655, MinusLogProbMetric: 31.0655, val_loss: 30.4569, val_MinusLogProbMetric: 30.4569

Epoch 48: val_loss improved from 31.14838 to 30.45691, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 77s - loss: 31.0655 - MinusLogProbMetric: 31.0655 - val_loss: 30.4569 - val_MinusLogProbMetric: 30.4569 - lr: 1.1111e-04 - 77s/epoch - 394ms/step
Epoch 49/1000
2023-09-27 10:22:27.765 
Epoch 49/1000 
	 loss: 30.5990, MinusLogProbMetric: 30.5990, val_loss: 30.3924, val_MinusLogProbMetric: 30.3924

Epoch 49: val_loss improved from 30.45691 to 30.39241, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 77s - loss: 30.5990 - MinusLogProbMetric: 30.5990 - val_loss: 30.3924 - val_MinusLogProbMetric: 30.3924 - lr: 1.1111e-04 - 77s/epoch - 394ms/step
Epoch 50/1000
2023-09-27 10:23:44.174 
Epoch 50/1000 
	 loss: 30.3156, MinusLogProbMetric: 30.3156, val_loss: 29.9364, val_MinusLogProbMetric: 29.9364

Epoch 50: val_loss improved from 30.39241 to 29.93636, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 76s - loss: 30.3156 - MinusLogProbMetric: 30.3156 - val_loss: 29.9364 - val_MinusLogProbMetric: 29.9364 - lr: 1.1111e-04 - 76s/epoch - 390ms/step
Epoch 51/1000
2023-09-27 10:25:01.427 
Epoch 51/1000 
	 loss: 29.6907, MinusLogProbMetric: 29.6907, val_loss: 29.8090, val_MinusLogProbMetric: 29.8090

Epoch 51: val_loss improved from 29.93636 to 29.80901, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 77s - loss: 29.6907 - MinusLogProbMetric: 29.6907 - val_loss: 29.8090 - val_MinusLogProbMetric: 29.8090 - lr: 1.1111e-04 - 77s/epoch - 394ms/step
Epoch 52/1000
2023-09-27 10:26:19.718 
Epoch 52/1000 
	 loss: 29.5929, MinusLogProbMetric: 29.5929, val_loss: 29.2871, val_MinusLogProbMetric: 29.2871

Epoch 52: val_loss improved from 29.80901 to 29.28708, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 78s - loss: 29.5929 - MinusLogProbMetric: 29.5929 - val_loss: 29.2871 - val_MinusLogProbMetric: 29.2871 - lr: 1.1111e-04 - 78s/epoch - 399ms/step
Epoch 53/1000
2023-09-27 10:27:36.604 
Epoch 53/1000 
	 loss: 29.2557, MinusLogProbMetric: 29.2557, val_loss: 28.9438, val_MinusLogProbMetric: 28.9438

Epoch 53: val_loss improved from 29.28708 to 28.94375, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 77s - loss: 29.2557 - MinusLogProbMetric: 29.2557 - val_loss: 28.9438 - val_MinusLogProbMetric: 28.9438 - lr: 1.1111e-04 - 77s/epoch - 392ms/step
Epoch 54/1000
2023-09-27 10:28:53.599 
Epoch 54/1000 
	 loss: 28.9976, MinusLogProbMetric: 28.9976, val_loss: 30.3117, val_MinusLogProbMetric: 30.3117

Epoch 54: val_loss did not improve from 28.94375
196/196 - 76s - loss: 28.9976 - MinusLogProbMetric: 28.9976 - val_loss: 30.3117 - val_MinusLogProbMetric: 30.3117 - lr: 1.1111e-04 - 76s/epoch - 387ms/step
Epoch 55/1000
2023-09-27 10:30:09.502 
Epoch 55/1000 
	 loss: 29.2794, MinusLogProbMetric: 29.2794, val_loss: 49.6594, val_MinusLogProbMetric: 49.6594

Epoch 55: val_loss did not improve from 28.94375
196/196 - 76s - loss: 29.2794 - MinusLogProbMetric: 29.2794 - val_loss: 49.6594 - val_MinusLogProbMetric: 49.6594 - lr: 1.1111e-04 - 76s/epoch - 387ms/step
Epoch 56/1000
2023-09-27 10:31:24.527 
Epoch 56/1000 
	 loss: 31.3176, MinusLogProbMetric: 31.3176, val_loss: 28.6699, val_MinusLogProbMetric: 28.6699

Epoch 56: val_loss improved from 28.94375 to 28.66988, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 76s - loss: 31.3176 - MinusLogProbMetric: 31.3176 - val_loss: 28.6699 - val_MinusLogProbMetric: 28.6699 - lr: 1.1111e-04 - 76s/epoch - 389ms/step
Epoch 57/1000
2023-09-27 10:32:42.048 
Epoch 57/1000 
	 loss: 28.4180, MinusLogProbMetric: 28.4180, val_loss: 28.4589, val_MinusLogProbMetric: 28.4589

Epoch 57: val_loss improved from 28.66988 to 28.45888, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 78s - loss: 28.4180 - MinusLogProbMetric: 28.4180 - val_loss: 28.4589 - val_MinusLogProbMetric: 28.4589 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 58/1000
2023-09-27 10:33:59.432 
Epoch 58/1000 
	 loss: 28.2079, MinusLogProbMetric: 28.2079, val_loss: 28.4461, val_MinusLogProbMetric: 28.4461

Epoch 58: val_loss improved from 28.45888 to 28.44612, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 77s - loss: 28.2079 - MinusLogProbMetric: 28.2079 - val_loss: 28.4461 - val_MinusLogProbMetric: 28.4461 - lr: 1.1111e-04 - 77s/epoch - 395ms/step
Epoch 59/1000
2023-09-27 10:35:17.124 
Epoch 59/1000 
	 loss: 28.0553, MinusLogProbMetric: 28.0553, val_loss: 28.4812, val_MinusLogProbMetric: 28.4812

Epoch 59: val_loss did not improve from 28.44612
196/196 - 76s - loss: 28.0553 - MinusLogProbMetric: 28.0553 - val_loss: 28.4812 - val_MinusLogProbMetric: 28.4812 - lr: 1.1111e-04 - 76s/epoch - 389ms/step
Epoch 60/1000
2023-09-27 10:36:32.830 
Epoch 60/1000 
	 loss: 54.6538, MinusLogProbMetric: 54.6538, val_loss: 36.2211, val_MinusLogProbMetric: 36.2211

Epoch 60: val_loss did not improve from 28.44612
196/196 - 76s - loss: 54.6538 - MinusLogProbMetric: 54.6538 - val_loss: 36.2211 - val_MinusLogProbMetric: 36.2211 - lr: 1.1111e-04 - 76s/epoch - 386ms/step
Epoch 61/1000
2023-09-27 10:37:47.730 
Epoch 61/1000 
	 loss: 32.8625, MinusLogProbMetric: 32.8625, val_loss: 30.9535, val_MinusLogProbMetric: 30.9535

Epoch 61: val_loss did not improve from 28.44612
196/196 - 75s - loss: 32.8625 - MinusLogProbMetric: 32.8625 - val_loss: 30.9535 - val_MinusLogProbMetric: 30.9535 - lr: 1.1111e-04 - 75s/epoch - 382ms/step
Epoch 62/1000
2023-09-27 10:39:03.845 
Epoch 62/1000 
	 loss: 30.0936, MinusLogProbMetric: 30.0936, val_loss: 29.1389, val_MinusLogProbMetric: 29.1389

Epoch 62: val_loss did not improve from 28.44612
196/196 - 76s - loss: 30.0936 - MinusLogProbMetric: 30.0936 - val_loss: 29.1389 - val_MinusLogProbMetric: 29.1389 - lr: 1.1111e-04 - 76s/epoch - 388ms/step
Epoch 63/1000
2023-09-27 10:40:20.637 
Epoch 63/1000 
	 loss: 28.5304, MinusLogProbMetric: 28.5304, val_loss: 28.1677, val_MinusLogProbMetric: 28.1677

Epoch 63: val_loss improved from 28.44612 to 28.16774, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 78s - loss: 28.5304 - MinusLogProbMetric: 28.5304 - val_loss: 28.1677 - val_MinusLogProbMetric: 28.1677 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 64/1000
2023-09-27 10:41:37.597 
Epoch 64/1000 
	 loss: 28.1105, MinusLogProbMetric: 28.1105, val_loss: 27.8835, val_MinusLogProbMetric: 27.8835

Epoch 64: val_loss improved from 28.16774 to 27.88354, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 77s - loss: 28.1105 - MinusLogProbMetric: 28.1105 - val_loss: 27.8835 - val_MinusLogProbMetric: 27.8835 - lr: 1.1111e-04 - 77s/epoch - 393ms/step
Epoch 65/1000
2023-09-27 10:42:54.130 
Epoch 65/1000 
	 loss: 27.8112, MinusLogProbMetric: 27.8112, val_loss: 27.5565, val_MinusLogProbMetric: 27.5565

Epoch 65: val_loss improved from 27.88354 to 27.55648, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 77s - loss: 27.8112 - MinusLogProbMetric: 27.8112 - val_loss: 27.5565 - val_MinusLogProbMetric: 27.5565 - lr: 1.1111e-04 - 77s/epoch - 391ms/step
Epoch 66/1000
2023-09-27 10:44:11.440 
Epoch 66/1000 
	 loss: 129.5495, MinusLogProbMetric: 129.5495, val_loss: 95.6279, val_MinusLogProbMetric: 95.6279

Epoch 66: val_loss did not improve from 27.55648
196/196 - 76s - loss: 129.5495 - MinusLogProbMetric: 129.5495 - val_loss: 95.6279 - val_MinusLogProbMetric: 95.6279 - lr: 1.1111e-04 - 76s/epoch - 387ms/step
Epoch 67/1000
2023-09-27 10:45:27.110 
Epoch 67/1000 
	 loss: 79.4946, MinusLogProbMetric: 79.4946, val_loss: 68.6365, val_MinusLogProbMetric: 68.6365

Epoch 67: val_loss did not improve from 27.55648
196/196 - 76s - loss: 79.4946 - MinusLogProbMetric: 79.4946 - val_loss: 68.6365 - val_MinusLogProbMetric: 68.6365 - lr: 1.1111e-04 - 76s/epoch - 386ms/step
Epoch 68/1000
2023-09-27 10:46:43.588 
Epoch 68/1000 
	 loss: 62.6339, MinusLogProbMetric: 62.6339, val_loss: 57.2392, val_MinusLogProbMetric: 57.2392

Epoch 68: val_loss did not improve from 27.55648
196/196 - 76s - loss: 62.6339 - MinusLogProbMetric: 62.6339 - val_loss: 57.2392 - val_MinusLogProbMetric: 57.2392 - lr: 1.1111e-04 - 76s/epoch - 390ms/step
Epoch 69/1000
2023-09-27 10:47:59.317 
Epoch 69/1000 
	 loss: 54.2677, MinusLogProbMetric: 54.2677, val_loss: 51.6004, val_MinusLogProbMetric: 51.6004

Epoch 69: val_loss did not improve from 27.55648
196/196 - 76s - loss: 54.2677 - MinusLogProbMetric: 54.2677 - val_loss: 51.6004 - val_MinusLogProbMetric: 51.6004 - lr: 1.1111e-04 - 76s/epoch - 386ms/step
Epoch 70/1000
2023-09-27 10:49:16.632 
Epoch 70/1000 
	 loss: 49.5168, MinusLogProbMetric: 49.5168, val_loss: 47.8462, val_MinusLogProbMetric: 47.8462

Epoch 70: val_loss did not improve from 27.55648
196/196 - 77s - loss: 49.5168 - MinusLogProbMetric: 49.5168 - val_loss: 47.8462 - val_MinusLogProbMetric: 47.8462 - lr: 1.1111e-04 - 77s/epoch - 394ms/step
Epoch 71/1000
2023-09-27 10:50:32.788 
Epoch 71/1000 
	 loss: 46.1899, MinusLogProbMetric: 46.1899, val_loss: 44.7067, val_MinusLogProbMetric: 44.7067

Epoch 71: val_loss did not improve from 27.55648
196/196 - 76s - loss: 46.1899 - MinusLogProbMetric: 46.1899 - val_loss: 44.7067 - val_MinusLogProbMetric: 44.7067 - lr: 1.1111e-04 - 76s/epoch - 389ms/step
Epoch 72/1000
2023-09-27 10:51:49.639 
Epoch 72/1000 
	 loss: 43.0049, MinusLogProbMetric: 43.0049, val_loss: 41.6605, val_MinusLogProbMetric: 41.6605

Epoch 72: val_loss did not improve from 27.55648
196/196 - 77s - loss: 43.0049 - MinusLogProbMetric: 43.0049 - val_loss: 41.6605 - val_MinusLogProbMetric: 41.6605 - lr: 1.1111e-04 - 77s/epoch - 392ms/step
Epoch 73/1000
2023-09-27 10:53:05.391 
Epoch 73/1000 
	 loss: 40.8688, MinusLogProbMetric: 40.8688, val_loss: 40.3431, val_MinusLogProbMetric: 40.3431

Epoch 73: val_loss did not improve from 27.55648
196/196 - 76s - loss: 40.8688 - MinusLogProbMetric: 40.8688 - val_loss: 40.3431 - val_MinusLogProbMetric: 40.3431 - lr: 1.1111e-04 - 76s/epoch - 386ms/step
Epoch 74/1000
2023-09-27 10:54:21.847 
Epoch 74/1000 
	 loss: 39.1589, MinusLogProbMetric: 39.1589, val_loss: 38.7793, val_MinusLogProbMetric: 38.7793

Epoch 74: val_loss did not improve from 27.55648
196/196 - 76s - loss: 39.1589 - MinusLogProbMetric: 39.1589 - val_loss: 38.7793 - val_MinusLogProbMetric: 38.7793 - lr: 1.1111e-04 - 76s/epoch - 390ms/step
Epoch 75/1000
2023-09-27 10:55:38.113 
Epoch 75/1000 
	 loss: 37.7395, MinusLogProbMetric: 37.7395, val_loss: 37.0817, val_MinusLogProbMetric: 37.0817

Epoch 75: val_loss did not improve from 27.55648
196/196 - 76s - loss: 37.7395 - MinusLogProbMetric: 37.7395 - val_loss: 37.0817 - val_MinusLogProbMetric: 37.0817 - lr: 1.1111e-04 - 76s/epoch - 389ms/step
Epoch 76/1000
2023-09-27 10:56:54.636 
Epoch 76/1000 
	 loss: 36.3618, MinusLogProbMetric: 36.3618, val_loss: 35.6795, val_MinusLogProbMetric: 35.6795

Epoch 76: val_loss did not improve from 27.55648
196/196 - 77s - loss: 36.3618 - MinusLogProbMetric: 36.3618 - val_loss: 35.6795 - val_MinusLogProbMetric: 35.6795 - lr: 1.1111e-04 - 77s/epoch - 390ms/step
Epoch 77/1000
2023-09-27 10:58:11.137 
Epoch 77/1000 
	 loss: 35.1462, MinusLogProbMetric: 35.1462, val_loss: 34.3908, val_MinusLogProbMetric: 34.3908

Epoch 77: val_loss did not improve from 27.55648
196/196 - 76s - loss: 35.1462 - MinusLogProbMetric: 35.1462 - val_loss: 34.3908 - val_MinusLogProbMetric: 34.3908 - lr: 1.1111e-04 - 76s/epoch - 390ms/step
Epoch 78/1000
2023-09-27 10:59:26.606 
Epoch 78/1000 
	 loss: 34.2029, MinusLogProbMetric: 34.2029, val_loss: 34.0202, val_MinusLogProbMetric: 34.0202

Epoch 78: val_loss did not improve from 27.55648
196/196 - 75s - loss: 34.2029 - MinusLogProbMetric: 34.2029 - val_loss: 34.0202 - val_MinusLogProbMetric: 34.0202 - lr: 1.1111e-04 - 75s/epoch - 385ms/step
Epoch 79/1000
2023-09-27 11:00:42.692 
Epoch 79/1000 
	 loss: 33.2898, MinusLogProbMetric: 33.2898, val_loss: 32.8262, val_MinusLogProbMetric: 32.8262

Epoch 79: val_loss did not improve from 27.55648
196/196 - 76s - loss: 33.2898 - MinusLogProbMetric: 33.2898 - val_loss: 32.8262 - val_MinusLogProbMetric: 32.8262 - lr: 1.1111e-04 - 76s/epoch - 388ms/step
Epoch 80/1000
2023-09-27 11:01:58.589 
Epoch 80/1000 
	 loss: 32.6612, MinusLogProbMetric: 32.6612, val_loss: 32.9653, val_MinusLogProbMetric: 32.9653

Epoch 80: val_loss did not improve from 27.55648
196/196 - 76s - loss: 32.6612 - MinusLogProbMetric: 32.6612 - val_loss: 32.9653 - val_MinusLogProbMetric: 32.9653 - lr: 1.1111e-04 - 76s/epoch - 387ms/step
Epoch 81/1000
2023-09-27 11:03:14.879 
Epoch 81/1000 
	 loss: 31.9937, MinusLogProbMetric: 31.9937, val_loss: 31.6886, val_MinusLogProbMetric: 31.6886

Epoch 81: val_loss did not improve from 27.55648
196/196 - 76s - loss: 31.9937 - MinusLogProbMetric: 31.9937 - val_loss: 31.6886 - val_MinusLogProbMetric: 31.6886 - lr: 1.1111e-04 - 76s/epoch - 389ms/step
Epoch 82/1000
2023-09-27 11:04:31.419 
Epoch 82/1000 
	 loss: 31.4555, MinusLogProbMetric: 31.4555, val_loss: 31.1524, val_MinusLogProbMetric: 31.1524

Epoch 82: val_loss did not improve from 27.55648
196/196 - 77s - loss: 31.4555 - MinusLogProbMetric: 31.4555 - val_loss: 31.1524 - val_MinusLogProbMetric: 31.1524 - lr: 1.1111e-04 - 77s/epoch - 391ms/step
Epoch 83/1000
2023-09-27 11:05:47.129 
Epoch 83/1000 
	 loss: 31.0443, MinusLogProbMetric: 31.0443, val_loss: 30.5601, val_MinusLogProbMetric: 30.5601

Epoch 83: val_loss did not improve from 27.55648
196/196 - 76s - loss: 31.0443 - MinusLogProbMetric: 31.0443 - val_loss: 30.5601 - val_MinusLogProbMetric: 30.5601 - lr: 1.1111e-04 - 76s/epoch - 386ms/step
Epoch 84/1000
2023-09-27 11:07:04.006 
Epoch 84/1000 
	 loss: 30.4949, MinusLogProbMetric: 30.4949, val_loss: 30.3952, val_MinusLogProbMetric: 30.3952

Epoch 84: val_loss did not improve from 27.55648
196/196 - 77s - loss: 30.4949 - MinusLogProbMetric: 30.4949 - val_loss: 30.3952 - val_MinusLogProbMetric: 30.3952 - lr: 1.1111e-04 - 77s/epoch - 392ms/step
Epoch 85/1000
2023-09-27 11:08:19.385 
Epoch 85/1000 
	 loss: 30.1442, MinusLogProbMetric: 30.1442, val_loss: 30.1499, val_MinusLogProbMetric: 30.1499

Epoch 85: val_loss did not improve from 27.55648
196/196 - 75s - loss: 30.1442 - MinusLogProbMetric: 30.1442 - val_loss: 30.1499 - val_MinusLogProbMetric: 30.1499 - lr: 1.1111e-04 - 75s/epoch - 385ms/step
Epoch 86/1000
2023-09-27 11:09:35.156 
Epoch 86/1000 
	 loss: 29.6862, MinusLogProbMetric: 29.6862, val_loss: 30.3912, val_MinusLogProbMetric: 30.3912

Epoch 86: val_loss did not improve from 27.55648
196/196 - 76s - loss: 29.6862 - MinusLogProbMetric: 29.6862 - val_loss: 30.3912 - val_MinusLogProbMetric: 30.3912 - lr: 1.1111e-04 - 76s/epoch - 387ms/step
Epoch 87/1000
2023-09-27 11:10:55.306 
Epoch 87/1000 
	 loss: 29.2738, MinusLogProbMetric: 29.2738, val_loss: 29.1740, val_MinusLogProbMetric: 29.1740

Epoch 87: val_loss did not improve from 27.55648
196/196 - 80s - loss: 29.2738 - MinusLogProbMetric: 29.2738 - val_loss: 29.1740 - val_MinusLogProbMetric: 29.1740 - lr: 1.1111e-04 - 80s/epoch - 409ms/step
Epoch 88/1000
2023-09-27 11:12:11.191 
Epoch 88/1000 
	 loss: 28.9846, MinusLogProbMetric: 28.9846, val_loss: 29.1835, val_MinusLogProbMetric: 29.1835

Epoch 88: val_loss did not improve from 27.55648
196/196 - 76s - loss: 28.9846 - MinusLogProbMetric: 28.9846 - val_loss: 29.1835 - val_MinusLogProbMetric: 29.1835 - lr: 1.1111e-04 - 76s/epoch - 387ms/step
Epoch 89/1000
2023-09-27 11:13:26.565 
Epoch 89/1000 
	 loss: 28.7070, MinusLogProbMetric: 28.7070, val_loss: 29.2522, val_MinusLogProbMetric: 29.2522

Epoch 89: val_loss did not improve from 27.55648
196/196 - 75s - loss: 28.7070 - MinusLogProbMetric: 28.7070 - val_loss: 29.2522 - val_MinusLogProbMetric: 29.2522 - lr: 1.1111e-04 - 75s/epoch - 385ms/step
Epoch 90/1000
2023-09-27 11:14:42.441 
Epoch 90/1000 
	 loss: 28.3635, MinusLogProbMetric: 28.3635, val_loss: 28.2840, val_MinusLogProbMetric: 28.2840

Epoch 90: val_loss did not improve from 27.55648
196/196 - 76s - loss: 28.3635 - MinusLogProbMetric: 28.3635 - val_loss: 28.2840 - val_MinusLogProbMetric: 28.2840 - lr: 1.1111e-04 - 76s/epoch - 387ms/step
Epoch 91/1000
2023-09-27 11:15:59.458 
Epoch 91/1000 
	 loss: 28.0983, MinusLogProbMetric: 28.0983, val_loss: 28.1040, val_MinusLogProbMetric: 28.1040

Epoch 91: val_loss did not improve from 27.55648
196/196 - 77s - loss: 28.0983 - MinusLogProbMetric: 28.0983 - val_loss: 28.1040 - val_MinusLogProbMetric: 28.1040 - lr: 1.1111e-04 - 77s/epoch - 393ms/step
Epoch 92/1000
2023-09-27 11:17:16.886 
Epoch 92/1000 
	 loss: 27.9072, MinusLogProbMetric: 27.9072, val_loss: 27.7900, val_MinusLogProbMetric: 27.7900

Epoch 92: val_loss did not improve from 27.55648
196/196 - 77s - loss: 27.9072 - MinusLogProbMetric: 27.9072 - val_loss: 27.7900 - val_MinusLogProbMetric: 27.7900 - lr: 1.1111e-04 - 77s/epoch - 395ms/step
Epoch 93/1000
2023-09-27 11:18:32.858 
Epoch 93/1000 
	 loss: 27.6115, MinusLogProbMetric: 27.6115, val_loss: 27.6519, val_MinusLogProbMetric: 27.6519

Epoch 93: val_loss did not improve from 27.55648
196/196 - 76s - loss: 27.6115 - MinusLogProbMetric: 27.6115 - val_loss: 27.6519 - val_MinusLogProbMetric: 27.6519 - lr: 1.1111e-04 - 76s/epoch - 388ms/step
Epoch 94/1000
2023-09-27 11:19:49.005 
Epoch 94/1000 
	 loss: 27.3667, MinusLogProbMetric: 27.3667, val_loss: 27.7377, val_MinusLogProbMetric: 27.7377

Epoch 94: val_loss did not improve from 27.55648
196/196 - 76s - loss: 27.3667 - MinusLogProbMetric: 27.3667 - val_loss: 27.7377 - val_MinusLogProbMetric: 27.7377 - lr: 1.1111e-04 - 76s/epoch - 388ms/step
Epoch 95/1000
2023-09-27 11:21:07.003 
Epoch 95/1000 
	 loss: 27.1572, MinusLogProbMetric: 27.1572, val_loss: 26.9868, val_MinusLogProbMetric: 26.9868

Epoch 95: val_loss improved from 27.55648 to 26.98676, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 80s - loss: 27.1572 - MinusLogProbMetric: 27.1572 - val_loss: 26.9868 - val_MinusLogProbMetric: 26.9868 - lr: 1.1111e-04 - 80s/epoch - 406ms/step
Epoch 96/1000
2023-09-27 11:22:24.542 
Epoch 96/1000 
	 loss: 27.0289, MinusLogProbMetric: 27.0289, val_loss: 26.8128, val_MinusLogProbMetric: 26.8128

Epoch 96: val_loss improved from 26.98676 to 26.81284, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 77s - loss: 27.0289 - MinusLogProbMetric: 27.0289 - val_loss: 26.8128 - val_MinusLogProbMetric: 26.8128 - lr: 1.1111e-04 - 77s/epoch - 395ms/step
Epoch 97/1000
2023-09-27 11:23:41.033 
Epoch 97/1000 
	 loss: 26.7225, MinusLogProbMetric: 26.7225, val_loss: 26.6419, val_MinusLogProbMetric: 26.6419

Epoch 97: val_loss improved from 26.81284 to 26.64187, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 77s - loss: 26.7225 - MinusLogProbMetric: 26.7225 - val_loss: 26.6419 - val_MinusLogProbMetric: 26.6419 - lr: 1.1111e-04 - 77s/epoch - 391ms/step
Epoch 98/1000
2023-09-27 11:24:59.043 
Epoch 98/1000 
	 loss: 26.4932, MinusLogProbMetric: 26.4932, val_loss: 26.7134, val_MinusLogProbMetric: 26.7134

Epoch 98: val_loss did not improve from 26.64187
196/196 - 77s - loss: 26.4932 - MinusLogProbMetric: 26.4932 - val_loss: 26.7134 - val_MinusLogProbMetric: 26.7134 - lr: 1.1111e-04 - 77s/epoch - 391ms/step
Epoch 99/1000
2023-09-27 11:26:15.747 
Epoch 99/1000 
	 loss: 26.2817, MinusLogProbMetric: 26.2817, val_loss: 26.1084, val_MinusLogProbMetric: 26.1084

Epoch 99: val_loss improved from 26.64187 to 26.10840, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 78s - loss: 26.2817 - MinusLogProbMetric: 26.2817 - val_loss: 26.1084 - val_MinusLogProbMetric: 26.1084 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 100/1000
2023-09-27 11:27:32.081 
Epoch 100/1000 
	 loss: 26.1892, MinusLogProbMetric: 26.1892, val_loss: 26.1191, val_MinusLogProbMetric: 26.1191

Epoch 100: val_loss did not improve from 26.10840
196/196 - 75s - loss: 26.1892 - MinusLogProbMetric: 26.1892 - val_loss: 26.1191 - val_MinusLogProbMetric: 26.1191 - lr: 1.1111e-04 - 75s/epoch - 382ms/step
Epoch 101/1000
2023-09-27 11:28:48.044 
Epoch 101/1000 
	 loss: 26.0150, MinusLogProbMetric: 26.0150, val_loss: 26.0517, val_MinusLogProbMetric: 26.0517

Epoch 101: val_loss improved from 26.10840 to 26.05168, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 77s - loss: 26.0150 - MinusLogProbMetric: 26.0150 - val_loss: 26.0517 - val_MinusLogProbMetric: 26.0517 - lr: 1.1111e-04 - 77s/epoch - 395ms/step
Epoch 102/1000
2023-09-27 11:30:05.043 
Epoch 102/1000 
	 loss: 25.8558, MinusLogProbMetric: 25.8558, val_loss: 25.7092, val_MinusLogProbMetric: 25.7092

Epoch 102: val_loss improved from 26.05168 to 25.70924, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 77s - loss: 25.8558 - MinusLogProbMetric: 25.8558 - val_loss: 25.7092 - val_MinusLogProbMetric: 25.7092 - lr: 1.1111e-04 - 77s/epoch - 392ms/step
Epoch 103/1000
2023-09-27 11:31:21.691 
Epoch 103/1000 
	 loss: 25.7165, MinusLogProbMetric: 25.7165, val_loss: 25.8418, val_MinusLogProbMetric: 25.8418

Epoch 103: val_loss did not improve from 25.70924
196/196 - 75s - loss: 25.7165 - MinusLogProbMetric: 25.7165 - val_loss: 25.8418 - val_MinusLogProbMetric: 25.8418 - lr: 1.1111e-04 - 75s/epoch - 384ms/step
Epoch 104/1000
2023-09-27 11:32:37.586 
Epoch 104/1000 
	 loss: 25.5894, MinusLogProbMetric: 25.5894, val_loss: 25.5682, val_MinusLogProbMetric: 25.5682

Epoch 104: val_loss improved from 25.70924 to 25.56824, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 77s - loss: 25.5894 - MinusLogProbMetric: 25.5894 - val_loss: 25.5682 - val_MinusLogProbMetric: 25.5682 - lr: 1.1111e-04 - 77s/epoch - 393ms/step
Epoch 105/1000
2023-09-27 11:33:54.722 
Epoch 105/1000 
	 loss: 25.3673, MinusLogProbMetric: 25.3673, val_loss: 25.4965, val_MinusLogProbMetric: 25.4965

Epoch 105: val_loss improved from 25.56824 to 25.49648, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 77s - loss: 25.3673 - MinusLogProbMetric: 25.3673 - val_loss: 25.4965 - val_MinusLogProbMetric: 25.4965 - lr: 1.1111e-04 - 77s/epoch - 395ms/step
Epoch 106/1000
2023-09-27 11:35:12.589 
Epoch 106/1000 
	 loss: 25.2614, MinusLogProbMetric: 25.2614, val_loss: 25.2872, val_MinusLogProbMetric: 25.2872

Epoch 106: val_loss improved from 25.49648 to 25.28720, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 78s - loss: 25.2614 - MinusLogProbMetric: 25.2614 - val_loss: 25.2872 - val_MinusLogProbMetric: 25.2872 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 107/1000
2023-09-27 11:36:30.858 
Epoch 107/1000 
	 loss: 25.0761, MinusLogProbMetric: 25.0761, val_loss: 25.3520, val_MinusLogProbMetric: 25.3520

Epoch 107: val_loss did not improve from 25.28720
196/196 - 77s - loss: 25.0761 - MinusLogProbMetric: 25.0761 - val_loss: 25.3520 - val_MinusLogProbMetric: 25.3520 - lr: 1.1111e-04 - 77s/epoch - 392ms/step
Epoch 108/1000
2023-09-27 11:37:46.856 
Epoch 108/1000 
	 loss: 24.9624, MinusLogProbMetric: 24.9624, val_loss: 24.8762, val_MinusLogProbMetric: 24.8762

Epoch 108: val_loss improved from 25.28720 to 24.87624, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 77s - loss: 24.9624 - MinusLogProbMetric: 24.9624 - val_loss: 24.8762 - val_MinusLogProbMetric: 24.8762 - lr: 1.1111e-04 - 77s/epoch - 394ms/step
Epoch 109/1000
2023-09-27 11:39:04.161 
Epoch 109/1000 
	 loss: 24.8450, MinusLogProbMetric: 24.8450, val_loss: 24.7485, val_MinusLogProbMetric: 24.7485

Epoch 109: val_loss improved from 24.87624 to 24.74854, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 77s - loss: 24.8450 - MinusLogProbMetric: 24.8450 - val_loss: 24.7485 - val_MinusLogProbMetric: 24.7485 - lr: 1.1111e-04 - 77s/epoch - 395ms/step
Epoch 110/1000
2023-09-27 11:40:21.460 
Epoch 110/1000 
	 loss: 24.7048, MinusLogProbMetric: 24.7048, val_loss: 24.7094, val_MinusLogProbMetric: 24.7094

Epoch 110: val_loss improved from 24.74854 to 24.70940, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 77s - loss: 24.7048 - MinusLogProbMetric: 24.7048 - val_loss: 24.7094 - val_MinusLogProbMetric: 24.7094 - lr: 1.1111e-04 - 77s/epoch - 394ms/step
Epoch 111/1000
2023-09-27 11:41:39.818 
Epoch 111/1000 
	 loss: 24.5959, MinusLogProbMetric: 24.5959, val_loss: 24.9725, val_MinusLogProbMetric: 24.9725

Epoch 111: val_loss did not improve from 24.70940
196/196 - 77s - loss: 24.5959 - MinusLogProbMetric: 24.5959 - val_loss: 24.9725 - val_MinusLogProbMetric: 24.9725 - lr: 1.1111e-04 - 77s/epoch - 393ms/step
Epoch 112/1000
2023-09-27 11:42:56.331 
Epoch 112/1000 
	 loss: 24.4934, MinusLogProbMetric: 24.4934, val_loss: 24.7812, val_MinusLogProbMetric: 24.7812

Epoch 112: val_loss did not improve from 24.70940
196/196 - 77s - loss: 24.4934 - MinusLogProbMetric: 24.4934 - val_loss: 24.7812 - val_MinusLogProbMetric: 24.7812 - lr: 1.1111e-04 - 77s/epoch - 390ms/step
Epoch 113/1000
2023-09-27 11:44:12.604 
Epoch 113/1000 
	 loss: 24.4365, MinusLogProbMetric: 24.4365, val_loss: 24.2740, val_MinusLogProbMetric: 24.2740

Epoch 113: val_loss improved from 24.70940 to 24.27404, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 78s - loss: 24.4365 - MinusLogProbMetric: 24.4365 - val_loss: 24.2740 - val_MinusLogProbMetric: 24.2740 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 114/1000
2023-09-27 11:45:30.596 
Epoch 114/1000 
	 loss: 24.3021, MinusLogProbMetric: 24.3021, val_loss: 24.2997, val_MinusLogProbMetric: 24.2997

Epoch 114: val_loss did not improve from 24.27404
196/196 - 77s - loss: 24.3021 - MinusLogProbMetric: 24.3021 - val_loss: 24.2997 - val_MinusLogProbMetric: 24.2997 - lr: 1.1111e-04 - 77s/epoch - 391ms/step
Epoch 115/1000
2023-09-27 11:46:45.855 
Epoch 115/1000 
	 loss: 24.1829, MinusLogProbMetric: 24.1829, val_loss: 24.1388, val_MinusLogProbMetric: 24.1388

Epoch 115: val_loss improved from 24.27404 to 24.13879, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 77s - loss: 24.1829 - MinusLogProbMetric: 24.1829 - val_loss: 24.1388 - val_MinusLogProbMetric: 24.1388 - lr: 1.1111e-04 - 77s/epoch - 391ms/step
Epoch 116/1000
2023-09-27 11:48:03.806 
Epoch 116/1000 
	 loss: 24.1406, MinusLogProbMetric: 24.1406, val_loss: 24.4221, val_MinusLogProbMetric: 24.4221

Epoch 116: val_loss did not improve from 24.13879
196/196 - 76s - loss: 24.1406 - MinusLogProbMetric: 24.1406 - val_loss: 24.4221 - val_MinusLogProbMetric: 24.4221 - lr: 1.1111e-04 - 76s/epoch - 390ms/step
Epoch 117/1000
2023-09-27 11:49:20.337 
Epoch 117/1000 
	 loss: 24.0295, MinusLogProbMetric: 24.0295, val_loss: 24.0884, val_MinusLogProbMetric: 24.0884

Epoch 117: val_loss improved from 24.13879 to 24.08837, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 78s - loss: 24.0295 - MinusLogProbMetric: 24.0295 - val_loss: 24.0884 - val_MinusLogProbMetric: 24.0884 - lr: 1.1111e-04 - 78s/epoch - 399ms/step
Epoch 118/1000
2023-09-27 11:50:37.733 
Epoch 118/1000 
	 loss: 23.9582, MinusLogProbMetric: 23.9582, val_loss: 24.0197, val_MinusLogProbMetric: 24.0197

Epoch 118: val_loss improved from 24.08837 to 24.01967, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 77s - loss: 23.9582 - MinusLogProbMetric: 23.9582 - val_loss: 24.0197 - val_MinusLogProbMetric: 24.0197 - lr: 1.1111e-04 - 77s/epoch - 394ms/step
Epoch 119/1000
2023-09-27 11:51:55.555 
Epoch 119/1000 
	 loss: 23.8887, MinusLogProbMetric: 23.8887, val_loss: 23.9812, val_MinusLogProbMetric: 23.9812

Epoch 119: val_loss improved from 24.01967 to 23.98118, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 78s - loss: 23.8887 - MinusLogProbMetric: 23.8887 - val_loss: 23.9812 - val_MinusLogProbMetric: 23.9812 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 120/1000
2023-09-27 11:53:12.991 
Epoch 120/1000 
	 loss: 24.3414, MinusLogProbMetric: 24.3414, val_loss: 25.6151, val_MinusLogProbMetric: 25.6151

Epoch 120: val_loss did not improve from 23.98118
196/196 - 76s - loss: 24.3414 - MinusLogProbMetric: 24.3414 - val_loss: 25.6151 - val_MinusLogProbMetric: 25.6151 - lr: 1.1111e-04 - 76s/epoch - 387ms/step
Epoch 121/1000
2023-09-27 11:54:28.640 
Epoch 121/1000 
	 loss: 24.3143, MinusLogProbMetric: 24.3143, val_loss: 23.8526, val_MinusLogProbMetric: 23.8526

Epoch 121: val_loss improved from 23.98118 to 23.85259, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 77s - loss: 24.3143 - MinusLogProbMetric: 24.3143 - val_loss: 23.8526 - val_MinusLogProbMetric: 23.8526 - lr: 1.1111e-04 - 77s/epoch - 394ms/step
Epoch 122/1000
2023-09-27 11:55:46.167 
Epoch 122/1000 
	 loss: 23.5863, MinusLogProbMetric: 23.5863, val_loss: 23.7319, val_MinusLogProbMetric: 23.7319

Epoch 122: val_loss improved from 23.85259 to 23.73192, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 77s - loss: 23.5863 - MinusLogProbMetric: 23.5863 - val_loss: 23.7319 - val_MinusLogProbMetric: 23.7319 - lr: 1.1111e-04 - 77s/epoch - 394ms/step
Epoch 123/1000
2023-09-27 11:57:03.199 
Epoch 123/1000 
	 loss: 23.6014, MinusLogProbMetric: 23.6014, val_loss: 23.6326, val_MinusLogProbMetric: 23.6326

Epoch 123: val_loss improved from 23.73192 to 23.63264, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 77s - loss: 23.6014 - MinusLogProbMetric: 23.6014 - val_loss: 23.6326 - val_MinusLogProbMetric: 23.6326 - lr: 1.1111e-04 - 77s/epoch - 394ms/step
Epoch 124/1000
2023-09-27 11:58:20.427 
Epoch 124/1000 
	 loss: 23.4844, MinusLogProbMetric: 23.4844, val_loss: 23.7400, val_MinusLogProbMetric: 23.7400

Epoch 124: val_loss did not improve from 23.63264
196/196 - 76s - loss: 23.4844 - MinusLogProbMetric: 23.4844 - val_loss: 23.7400 - val_MinusLogProbMetric: 23.7400 - lr: 1.1111e-04 - 76s/epoch - 387ms/step
Epoch 125/1000
2023-09-27 11:59:36.360 
Epoch 125/1000 
	 loss: 23.4103, MinusLogProbMetric: 23.4103, val_loss: 23.5585, val_MinusLogProbMetric: 23.5585

Epoch 125: val_loss improved from 23.63264 to 23.55846, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 77s - loss: 23.4103 - MinusLogProbMetric: 23.4103 - val_loss: 23.5585 - val_MinusLogProbMetric: 23.5585 - lr: 1.1111e-04 - 77s/epoch - 394ms/step
Epoch 126/1000
2023-09-27 12:00:53.585 
Epoch 126/1000 
	 loss: 23.3429, MinusLogProbMetric: 23.3429, val_loss: 23.6587, val_MinusLogProbMetric: 23.6587

Epoch 126: val_loss did not improve from 23.55846
196/196 - 76s - loss: 23.3429 - MinusLogProbMetric: 23.3429 - val_loss: 23.6587 - val_MinusLogProbMetric: 23.6587 - lr: 1.1111e-04 - 76s/epoch - 387ms/step
Epoch 127/1000
2023-09-27 12:02:08.981 
Epoch 127/1000 
	 loss: 87.2683, MinusLogProbMetric: 87.2683, val_loss: 69.4623, val_MinusLogProbMetric: 69.4623

Epoch 127: val_loss did not improve from 23.55846
196/196 - 75s - loss: 87.2683 - MinusLogProbMetric: 87.2683 - val_loss: 69.4623 - val_MinusLogProbMetric: 69.4623 - lr: 1.1111e-04 - 75s/epoch - 385ms/step
Epoch 128/1000
2023-09-27 12:03:26.317 
Epoch 128/1000 
	 loss: 57.9139, MinusLogProbMetric: 57.9139, val_loss: 49.5627, val_MinusLogProbMetric: 49.5627

Epoch 128: val_loss did not improve from 23.55846
196/196 - 77s - loss: 57.9139 - MinusLogProbMetric: 57.9139 - val_loss: 49.5627 - val_MinusLogProbMetric: 49.5627 - lr: 1.1111e-04 - 77s/epoch - 395ms/step
Epoch 129/1000
2023-09-27 12:04:42.958 
Epoch 129/1000 
	 loss: 45.1953, MinusLogProbMetric: 45.1953, val_loss: 41.8732, val_MinusLogProbMetric: 41.8732

Epoch 129: val_loss did not improve from 23.55846
196/196 - 77s - loss: 45.1953 - MinusLogProbMetric: 45.1953 - val_loss: 41.8732 - val_MinusLogProbMetric: 41.8732 - lr: 1.1111e-04 - 77s/epoch - 391ms/step
Epoch 130/1000
2023-09-27 12:05:58.922 
Epoch 130/1000 
	 loss: 39.4116, MinusLogProbMetric: 39.4116, val_loss: 37.4531, val_MinusLogProbMetric: 37.4531

Epoch 130: val_loss did not improve from 23.55846
196/196 - 76s - loss: 39.4116 - MinusLogProbMetric: 39.4116 - val_loss: 37.4531 - val_MinusLogProbMetric: 37.4531 - lr: 1.1111e-04 - 76s/epoch - 388ms/step
Epoch 131/1000
2023-09-27 12:07:15.402 
Epoch 131/1000 
	 loss: 35.8943, MinusLogProbMetric: 35.8943, val_loss: 34.7235, val_MinusLogProbMetric: 34.7235

Epoch 131: val_loss did not improve from 23.55846
196/196 - 76s - loss: 35.8943 - MinusLogProbMetric: 35.8943 - val_loss: 34.7235 - val_MinusLogProbMetric: 34.7235 - lr: 1.1111e-04 - 76s/epoch - 390ms/step
Epoch 132/1000
2023-09-27 12:08:30.644 
Epoch 132/1000 
	 loss: 33.6809, MinusLogProbMetric: 33.6809, val_loss: 32.8091, val_MinusLogProbMetric: 32.8091

Epoch 132: val_loss did not improve from 23.55846
196/196 - 75s - loss: 33.6809 - MinusLogProbMetric: 33.6809 - val_loss: 32.8091 - val_MinusLogProbMetric: 32.8091 - lr: 1.1111e-04 - 75s/epoch - 384ms/step
Epoch 133/1000
2023-09-27 12:09:47.387 
Epoch 133/1000 
	 loss: 32.1308, MinusLogProbMetric: 32.1308, val_loss: 31.6269, val_MinusLogProbMetric: 31.6269

Epoch 133: val_loss did not improve from 23.55846
196/196 - 77s - loss: 32.1308 - MinusLogProbMetric: 32.1308 - val_loss: 31.6269 - val_MinusLogProbMetric: 31.6269 - lr: 1.1111e-04 - 77s/epoch - 392ms/step
Epoch 134/1000
2023-09-27 12:11:03.902 
Epoch 134/1000 
	 loss: 30.9892, MinusLogProbMetric: 30.9892, val_loss: 30.5539, val_MinusLogProbMetric: 30.5539

Epoch 134: val_loss did not improve from 23.55846
196/196 - 77s - loss: 30.9892 - MinusLogProbMetric: 30.9892 - val_loss: 30.5539 - val_MinusLogProbMetric: 30.5539 - lr: 1.1111e-04 - 77s/epoch - 390ms/step
Epoch 135/1000
2023-09-27 12:12:20.948 
Epoch 135/1000 
	 loss: 29.9871, MinusLogProbMetric: 29.9871, val_loss: 29.6063, val_MinusLogProbMetric: 29.6063

Epoch 135: val_loss did not improve from 23.55846
196/196 - 77s - loss: 29.9871 - MinusLogProbMetric: 29.9871 - val_loss: 29.6063 - val_MinusLogProbMetric: 29.6063 - lr: 1.1111e-04 - 77s/epoch - 393ms/step
Epoch 136/1000
2023-09-27 12:13:37.293 
Epoch 136/1000 
	 loss: 29.1134, MinusLogProbMetric: 29.1134, val_loss: 28.8140, val_MinusLogProbMetric: 28.8140

Epoch 136: val_loss did not improve from 23.55846
196/196 - 76s - loss: 29.1134 - MinusLogProbMetric: 29.1134 - val_loss: 28.8140 - val_MinusLogProbMetric: 28.8140 - lr: 1.1111e-04 - 76s/epoch - 389ms/step
Epoch 137/1000
2023-09-27 12:14:53.500 
Epoch 137/1000 
	 loss: 28.3249, MinusLogProbMetric: 28.3249, val_loss: 28.3836, val_MinusLogProbMetric: 28.3836

Epoch 137: val_loss did not improve from 23.55846
196/196 - 76s - loss: 28.3249 - MinusLogProbMetric: 28.3249 - val_loss: 28.3836 - val_MinusLogProbMetric: 28.3836 - lr: 1.1111e-04 - 76s/epoch - 389ms/step
Epoch 138/1000
2023-09-27 12:16:09.592 
Epoch 138/1000 
	 loss: 27.7182, MinusLogProbMetric: 27.7182, val_loss: 27.6853, val_MinusLogProbMetric: 27.6853

Epoch 138: val_loss did not improve from 23.55846
196/196 - 76s - loss: 27.7182 - MinusLogProbMetric: 27.7182 - val_loss: 27.6853 - val_MinusLogProbMetric: 27.6853 - lr: 1.1111e-04 - 76s/epoch - 388ms/step
Epoch 139/1000
2023-09-27 12:17:25.306 
Epoch 139/1000 
	 loss: 27.1445, MinusLogProbMetric: 27.1445, val_loss: 26.9604, val_MinusLogProbMetric: 26.9604

Epoch 139: val_loss did not improve from 23.55846
196/196 - 76s - loss: 27.1445 - MinusLogProbMetric: 27.1445 - val_loss: 26.9604 - val_MinusLogProbMetric: 26.9604 - lr: 1.1111e-04 - 76s/epoch - 386ms/step
Epoch 140/1000
2023-09-27 12:18:40.846 
Epoch 140/1000 
	 loss: 26.7038, MinusLogProbMetric: 26.7038, val_loss: 26.5567, val_MinusLogProbMetric: 26.5567

Epoch 140: val_loss did not improve from 23.55846
196/196 - 76s - loss: 26.7038 - MinusLogProbMetric: 26.7038 - val_loss: 26.5567 - val_MinusLogProbMetric: 26.5567 - lr: 1.1111e-04 - 76s/epoch - 385ms/step
Epoch 141/1000
2023-09-27 12:19:57.721 
Epoch 141/1000 
	 loss: 26.2805, MinusLogProbMetric: 26.2805, val_loss: 26.2142, val_MinusLogProbMetric: 26.2142

Epoch 141: val_loss did not improve from 23.55846
196/196 - 77s - loss: 26.2805 - MinusLogProbMetric: 26.2805 - val_loss: 26.2142 - val_MinusLogProbMetric: 26.2142 - lr: 1.1111e-04 - 77s/epoch - 392ms/step
Epoch 142/1000
2023-09-27 12:21:13.661 
Epoch 142/1000 
	 loss: 25.9530, MinusLogProbMetric: 25.9530, val_loss: 26.1991, val_MinusLogProbMetric: 26.1991

Epoch 142: val_loss did not improve from 23.55846
196/196 - 76s - loss: 25.9530 - MinusLogProbMetric: 25.9530 - val_loss: 26.1991 - val_MinusLogProbMetric: 26.1991 - lr: 1.1111e-04 - 76s/epoch - 387ms/step
Epoch 143/1000
2023-09-27 12:22:29.264 
Epoch 143/1000 
	 loss: 25.6437, MinusLogProbMetric: 25.6437, val_loss: 25.5013, val_MinusLogProbMetric: 25.5013

Epoch 143: val_loss did not improve from 23.55846
196/196 - 76s - loss: 25.6437 - MinusLogProbMetric: 25.6437 - val_loss: 25.5013 - val_MinusLogProbMetric: 25.5013 - lr: 1.1111e-04 - 76s/epoch - 386ms/step
Epoch 144/1000
2023-09-27 12:23:44.752 
Epoch 144/1000 
	 loss: 25.3815, MinusLogProbMetric: 25.3815, val_loss: 25.4973, val_MinusLogProbMetric: 25.4973

Epoch 144: val_loss did not improve from 23.55846
196/196 - 75s - loss: 25.3815 - MinusLogProbMetric: 25.3815 - val_loss: 25.4973 - val_MinusLogProbMetric: 25.4973 - lr: 1.1111e-04 - 75s/epoch - 385ms/step
Epoch 145/1000
2023-09-27 12:25:00.062 
Epoch 145/1000 
	 loss: 25.1017, MinusLogProbMetric: 25.1017, val_loss: 25.1494, val_MinusLogProbMetric: 25.1494

Epoch 145: val_loss did not improve from 23.55846
196/196 - 75s - loss: 25.1017 - MinusLogProbMetric: 25.1017 - val_loss: 25.1494 - val_MinusLogProbMetric: 25.1494 - lr: 1.1111e-04 - 75s/epoch - 384ms/step
Epoch 146/1000
2023-09-27 12:26:16.446 
Epoch 146/1000 
	 loss: 24.8928, MinusLogProbMetric: 24.8928, val_loss: 25.0450, val_MinusLogProbMetric: 25.0450

Epoch 146: val_loss did not improve from 23.55846
196/196 - 76s - loss: 24.8928 - MinusLogProbMetric: 24.8928 - val_loss: 25.0450 - val_MinusLogProbMetric: 25.0450 - lr: 1.1111e-04 - 76s/epoch - 390ms/step
Epoch 147/1000
2023-09-27 12:27:32.559 
Epoch 147/1000 
	 loss: 24.6927, MinusLogProbMetric: 24.6927, val_loss: 24.6191, val_MinusLogProbMetric: 24.6191

Epoch 147: val_loss did not improve from 23.55846
196/196 - 76s - loss: 24.6927 - MinusLogProbMetric: 24.6927 - val_loss: 24.6191 - val_MinusLogProbMetric: 24.6191 - lr: 1.1111e-04 - 76s/epoch - 388ms/step
Epoch 148/1000
2023-09-27 12:28:49.004 
Epoch 148/1000 
	 loss: 24.4675, MinusLogProbMetric: 24.4675, val_loss: 24.7482, val_MinusLogProbMetric: 24.7482

Epoch 148: val_loss did not improve from 23.55846
196/196 - 76s - loss: 24.4675 - MinusLogProbMetric: 24.4675 - val_loss: 24.7482 - val_MinusLogProbMetric: 24.7482 - lr: 1.1111e-04 - 76s/epoch - 390ms/step
Epoch 149/1000
2023-09-27 12:30:05.682 
Epoch 149/1000 
	 loss: 24.3081, MinusLogProbMetric: 24.3081, val_loss: 24.3001, val_MinusLogProbMetric: 24.3001

Epoch 149: val_loss did not improve from 23.55846
196/196 - 77s - loss: 24.3081 - MinusLogProbMetric: 24.3081 - val_loss: 24.3001 - val_MinusLogProbMetric: 24.3001 - lr: 1.1111e-04 - 77s/epoch - 391ms/step
Epoch 150/1000
2023-09-27 12:31:22.166 
Epoch 150/1000 
	 loss: 24.1508, MinusLogProbMetric: 24.1508, val_loss: 24.2194, val_MinusLogProbMetric: 24.2194

Epoch 150: val_loss did not improve from 23.55846
196/196 - 76s - loss: 24.1508 - MinusLogProbMetric: 24.1508 - val_loss: 24.2194 - val_MinusLogProbMetric: 24.2194 - lr: 1.1111e-04 - 76s/epoch - 390ms/step
Epoch 151/1000
2023-09-27 12:32:38.591 
Epoch 151/1000 
	 loss: 24.0299, MinusLogProbMetric: 24.0299, val_loss: 24.0768, val_MinusLogProbMetric: 24.0768

Epoch 151: val_loss did not improve from 23.55846
196/196 - 76s - loss: 24.0299 - MinusLogProbMetric: 24.0299 - val_loss: 24.0768 - val_MinusLogProbMetric: 24.0768 - lr: 1.1111e-04 - 76s/epoch - 390ms/step
Epoch 152/1000
2023-09-27 12:33:55.055 
Epoch 152/1000 
	 loss: 23.8962, MinusLogProbMetric: 23.8962, val_loss: 24.2893, val_MinusLogProbMetric: 24.2893

Epoch 152: val_loss did not improve from 23.55846
196/196 - 76s - loss: 23.8962 - MinusLogProbMetric: 23.8962 - val_loss: 24.2893 - val_MinusLogProbMetric: 24.2893 - lr: 1.1111e-04 - 76s/epoch - 390ms/step
Epoch 153/1000
2023-09-27 12:35:11.160 
Epoch 153/1000 
	 loss: 23.7186, MinusLogProbMetric: 23.7186, val_loss: 23.7245, val_MinusLogProbMetric: 23.7245

Epoch 153: val_loss did not improve from 23.55846
196/196 - 76s - loss: 23.7186 - MinusLogProbMetric: 23.7186 - val_loss: 23.7245 - val_MinusLogProbMetric: 23.7245 - lr: 1.1111e-04 - 76s/epoch - 388ms/step
Epoch 154/1000
2023-09-27 12:36:27.212 
Epoch 154/1000 
	 loss: 23.5666, MinusLogProbMetric: 23.5666, val_loss: 23.6052, val_MinusLogProbMetric: 23.6052

Epoch 154: val_loss did not improve from 23.55846
196/196 - 76s - loss: 23.5666 - MinusLogProbMetric: 23.5666 - val_loss: 23.6052 - val_MinusLogProbMetric: 23.6052 - lr: 1.1111e-04 - 76s/epoch - 388ms/step
Epoch 155/1000
2023-09-27 12:37:44.222 
Epoch 155/1000 
	 loss: 23.4850, MinusLogProbMetric: 23.4850, val_loss: 23.4725, val_MinusLogProbMetric: 23.4725

Epoch 155: val_loss improved from 23.55846 to 23.47249, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 79s - loss: 23.4850 - MinusLogProbMetric: 23.4850 - val_loss: 23.4725 - val_MinusLogProbMetric: 23.4725 - lr: 1.1111e-04 - 79s/epoch - 403ms/step
Epoch 156/1000
2023-09-27 12:39:03.531 
Epoch 156/1000 
	 loss: 23.3944, MinusLogProbMetric: 23.3944, val_loss: 23.3525, val_MinusLogProbMetric: 23.3525

Epoch 156: val_loss improved from 23.47249 to 23.35251, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 79s - loss: 23.3944 - MinusLogProbMetric: 23.3944 - val_loss: 23.3525 - val_MinusLogProbMetric: 23.3525 - lr: 1.1111e-04 - 79s/epoch - 401ms/step
Epoch 157/1000
2023-09-27 12:40:19.869 
Epoch 157/1000 
	 loss: 23.2673, MinusLogProbMetric: 23.2673, val_loss: 23.2603, val_MinusLogProbMetric: 23.2603

Epoch 157: val_loss improved from 23.35251 to 23.26033, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 77s - loss: 23.2673 - MinusLogProbMetric: 23.2673 - val_loss: 23.2603 - val_MinusLogProbMetric: 23.2603 - lr: 1.1111e-04 - 77s/epoch - 391ms/step
Epoch 158/1000
2023-09-27 12:41:36.903 
Epoch 158/1000 
	 loss: 23.1392, MinusLogProbMetric: 23.1392, val_loss: 23.1825, val_MinusLogProbMetric: 23.1825

Epoch 158: val_loss improved from 23.26033 to 23.18246, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 77s - loss: 23.1392 - MinusLogProbMetric: 23.1392 - val_loss: 23.1825 - val_MinusLogProbMetric: 23.1825 - lr: 1.1111e-04 - 77s/epoch - 393ms/step
Epoch 159/1000
2023-09-27 12:42:54.769 
Epoch 159/1000 
	 loss: 23.0561, MinusLogProbMetric: 23.0561, val_loss: 23.1813, val_MinusLogProbMetric: 23.1813

Epoch 159: val_loss improved from 23.18246 to 23.18128, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 78s - loss: 23.0561 - MinusLogProbMetric: 23.0561 - val_loss: 23.1813 - val_MinusLogProbMetric: 23.1813 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 160/1000
2023-09-27 12:44:13.027 
Epoch 160/1000 
	 loss: 23.0089, MinusLogProbMetric: 23.0089, val_loss: 23.0823, val_MinusLogProbMetric: 23.0823

Epoch 160: val_loss improved from 23.18128 to 23.08234, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 78s - loss: 23.0089 - MinusLogProbMetric: 23.0089 - val_loss: 23.0823 - val_MinusLogProbMetric: 23.0823 - lr: 1.1111e-04 - 78s/epoch - 399ms/step
Epoch 161/1000
2023-09-27 12:45:30.198 
Epoch 161/1000 
	 loss: 22.8694, MinusLogProbMetric: 22.8694, val_loss: 22.8878, val_MinusLogProbMetric: 22.8878

Epoch 161: val_loss improved from 23.08234 to 22.88779, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 77s - loss: 22.8694 - MinusLogProbMetric: 22.8694 - val_loss: 22.8878 - val_MinusLogProbMetric: 22.8878 - lr: 1.1111e-04 - 77s/epoch - 393ms/step
Epoch 162/1000
2023-09-27 12:46:47.321 
Epoch 162/1000 
	 loss: 22.8096, MinusLogProbMetric: 22.8096, val_loss: 23.0237, val_MinusLogProbMetric: 23.0237

Epoch 162: val_loss did not improve from 22.88779
196/196 - 76s - loss: 22.8096 - MinusLogProbMetric: 22.8096 - val_loss: 23.0237 - val_MinusLogProbMetric: 23.0237 - lr: 1.1111e-04 - 76s/epoch - 387ms/step
Epoch 163/1000
2023-09-27 12:48:03.957 
Epoch 163/1000 
	 loss: 22.6987, MinusLogProbMetric: 22.6987, val_loss: 22.7674, val_MinusLogProbMetric: 22.7674

Epoch 163: val_loss improved from 22.88779 to 22.76744, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 78s - loss: 22.6987 - MinusLogProbMetric: 22.6987 - val_loss: 22.7674 - val_MinusLogProbMetric: 22.7674 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 164/1000
2023-09-27 12:49:21.669 
Epoch 164/1000 
	 loss: 22.6126, MinusLogProbMetric: 22.6126, val_loss: 22.7865, val_MinusLogProbMetric: 22.7865

Epoch 164: val_loss did not improve from 22.76744
196/196 - 76s - loss: 22.6126 - MinusLogProbMetric: 22.6126 - val_loss: 22.7865 - val_MinusLogProbMetric: 22.7865 - lr: 1.1111e-04 - 76s/epoch - 390ms/step
Epoch 165/1000
2023-09-27 12:50:38.775 
Epoch 165/1000 
	 loss: 22.5707, MinusLogProbMetric: 22.5707, val_loss: 22.5864, val_MinusLogProbMetric: 22.5864

Epoch 165: val_loss improved from 22.76744 to 22.58643, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 79s - loss: 22.5707 - MinusLogProbMetric: 22.5707 - val_loss: 22.5864 - val_MinusLogProbMetric: 22.5864 - lr: 1.1111e-04 - 79s/epoch - 403ms/step
Epoch 166/1000
2023-09-27 12:51:56.331 
Epoch 166/1000 
	 loss: 22.4746, MinusLogProbMetric: 22.4746, val_loss: 22.5678, val_MinusLogProbMetric: 22.5678

Epoch 166: val_loss improved from 22.58643 to 22.56782, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 77s - loss: 22.4746 - MinusLogProbMetric: 22.4746 - val_loss: 22.5678 - val_MinusLogProbMetric: 22.5678 - lr: 1.1111e-04 - 77s/epoch - 394ms/step
Epoch 167/1000
2023-09-27 12:53:15.725 
Epoch 167/1000 
	 loss: 22.4329, MinusLogProbMetric: 22.4329, val_loss: 22.4515, val_MinusLogProbMetric: 22.4515

Epoch 167: val_loss improved from 22.56782 to 22.45145, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 80s - loss: 22.4329 - MinusLogProbMetric: 22.4329 - val_loss: 22.4515 - val_MinusLogProbMetric: 22.4515 - lr: 1.1111e-04 - 80s/epoch - 410ms/step
Epoch 168/1000
2023-09-27 12:54:34.223 
Epoch 168/1000 
	 loss: 22.3598, MinusLogProbMetric: 22.3598, val_loss: 22.4880, val_MinusLogProbMetric: 22.4880

Epoch 168: val_loss did not improve from 22.45145
196/196 - 76s - loss: 22.3598 - MinusLogProbMetric: 22.3598 - val_loss: 22.4880 - val_MinusLogProbMetric: 22.4880 - lr: 1.1111e-04 - 76s/epoch - 388ms/step
Epoch 169/1000
2023-09-27 12:55:50.558 
Epoch 169/1000 
	 loss: 22.2534, MinusLogProbMetric: 22.2534, val_loss: 22.2871, val_MinusLogProbMetric: 22.2871

Epoch 169: val_loss improved from 22.45145 to 22.28706, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 78s - loss: 22.2534 - MinusLogProbMetric: 22.2534 - val_loss: 22.2871 - val_MinusLogProbMetric: 22.2871 - lr: 1.1111e-04 - 78s/epoch - 400ms/step
Epoch 170/1000
2023-09-27 12:57:08.767 
Epoch 170/1000 
	 loss: 22.2452, MinusLogProbMetric: 22.2452, val_loss: 22.3591, val_MinusLogProbMetric: 22.3591

Epoch 170: val_loss did not improve from 22.28706
196/196 - 76s - loss: 22.2452 - MinusLogProbMetric: 22.2452 - val_loss: 22.3591 - val_MinusLogProbMetric: 22.3591 - lr: 1.1111e-04 - 76s/epoch - 388ms/step
Epoch 171/1000
2023-09-27 12:58:25.733 
Epoch 171/1000 
	 loss: 22.1489, MinusLogProbMetric: 22.1489, val_loss: 22.6275, val_MinusLogProbMetric: 22.6275

Epoch 171: val_loss did not improve from 22.28706
196/196 - 77s - loss: 22.1489 - MinusLogProbMetric: 22.1489 - val_loss: 22.6275 - val_MinusLogProbMetric: 22.6275 - lr: 1.1111e-04 - 77s/epoch - 393ms/step
Epoch 172/1000
2023-09-27 12:59:43.141 
Epoch 172/1000 
	 loss: 22.1335, MinusLogProbMetric: 22.1335, val_loss: 22.0903, val_MinusLogProbMetric: 22.0903

Epoch 172: val_loss improved from 22.28706 to 22.09030, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 79s - loss: 22.1335 - MinusLogProbMetric: 22.1335 - val_loss: 22.0903 - val_MinusLogProbMetric: 22.0903 - lr: 1.1111e-04 - 79s/epoch - 401ms/step
Epoch 173/1000
2023-09-27 13:01:00.252 
Epoch 173/1000 
	 loss: 22.0253, MinusLogProbMetric: 22.0253, val_loss: 21.9808, val_MinusLogProbMetric: 21.9808

Epoch 173: val_loss improved from 22.09030 to 21.98080, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 77s - loss: 22.0253 - MinusLogProbMetric: 22.0253 - val_loss: 21.9808 - val_MinusLogProbMetric: 21.9808 - lr: 1.1111e-04 - 77s/epoch - 394ms/step
Epoch 174/1000
2023-09-27 13:02:16.761 
Epoch 174/1000 
	 loss: 22.0152, MinusLogProbMetric: 22.0152, val_loss: 22.0528, val_MinusLogProbMetric: 22.0528

Epoch 174: val_loss did not improve from 21.98080
196/196 - 75s - loss: 22.0152 - MinusLogProbMetric: 22.0152 - val_loss: 22.0528 - val_MinusLogProbMetric: 22.0528 - lr: 1.1111e-04 - 75s/epoch - 383ms/step
Epoch 175/1000
2023-09-27 13:03:32.286 
Epoch 175/1000 
	 loss: 21.9403, MinusLogProbMetric: 21.9403, val_loss: 22.0564, val_MinusLogProbMetric: 22.0564

Epoch 175: val_loss did not improve from 21.98080
196/196 - 76s - loss: 21.9403 - MinusLogProbMetric: 21.9403 - val_loss: 22.0564 - val_MinusLogProbMetric: 22.0564 - lr: 1.1111e-04 - 76s/epoch - 385ms/step
Epoch 176/1000
2023-09-27 13:04:49.195 
Epoch 176/1000 
	 loss: 21.8725, MinusLogProbMetric: 21.8725, val_loss: 21.9562, val_MinusLogProbMetric: 21.9562

Epoch 176: val_loss improved from 21.98080 to 21.95624, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 78s - loss: 21.8725 - MinusLogProbMetric: 21.8725 - val_loss: 21.9562 - val_MinusLogProbMetric: 21.9562 - lr: 1.1111e-04 - 78s/epoch - 400ms/step
Epoch 177/1000
2023-09-27 13:06:07.825 
Epoch 177/1000 
	 loss: 21.8273, MinusLogProbMetric: 21.8273, val_loss: 21.9848, val_MinusLogProbMetric: 21.9848

Epoch 177: val_loss did not improve from 21.95624
196/196 - 77s - loss: 21.8273 - MinusLogProbMetric: 21.8273 - val_loss: 21.9848 - val_MinusLogProbMetric: 21.9848 - lr: 1.1111e-04 - 77s/epoch - 393ms/step
Epoch 178/1000
2023-09-27 13:07:23.146 
Epoch 178/1000 
	 loss: 21.8075, MinusLogProbMetric: 21.8075, val_loss: 21.7491, val_MinusLogProbMetric: 21.7491

Epoch 178: val_loss improved from 21.95624 to 21.74909, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 77s - loss: 21.8075 - MinusLogProbMetric: 21.8075 - val_loss: 21.7491 - val_MinusLogProbMetric: 21.7491 - lr: 1.1111e-04 - 77s/epoch - 391ms/step
Epoch 179/1000
2023-09-27 13:08:41.307 
Epoch 179/1000 
	 loss: 21.7150, MinusLogProbMetric: 21.7150, val_loss: 21.7899, val_MinusLogProbMetric: 21.7899

Epoch 179: val_loss did not improve from 21.74909
196/196 - 77s - loss: 21.7150 - MinusLogProbMetric: 21.7150 - val_loss: 21.7899 - val_MinusLogProbMetric: 21.7899 - lr: 1.1111e-04 - 77s/epoch - 392ms/step
Epoch 180/1000
2023-09-27 13:09:58.496 
Epoch 180/1000 
	 loss: 21.7000, MinusLogProbMetric: 21.7000, val_loss: 21.8117, val_MinusLogProbMetric: 21.8117

Epoch 180: val_loss did not improve from 21.74909
196/196 - 77s - loss: 21.7000 - MinusLogProbMetric: 21.7000 - val_loss: 21.8117 - val_MinusLogProbMetric: 21.8117 - lr: 1.1111e-04 - 77s/epoch - 394ms/step
Epoch 181/1000
2023-09-27 13:11:13.665 
Epoch 181/1000 
	 loss: 21.6488, MinusLogProbMetric: 21.6488, val_loss: 21.7369, val_MinusLogProbMetric: 21.7369

Epoch 181: val_loss improved from 21.74909 to 21.73691, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 77s - loss: 21.6488 - MinusLogProbMetric: 21.6488 - val_loss: 21.7369 - val_MinusLogProbMetric: 21.7369 - lr: 1.1111e-04 - 77s/epoch - 392ms/step
Epoch 182/1000
2023-09-27 13:12:33.029 
Epoch 182/1000 
	 loss: 21.5992, MinusLogProbMetric: 21.5992, val_loss: 21.7235, val_MinusLogProbMetric: 21.7235

Epoch 182: val_loss improved from 21.73691 to 21.72352, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 79s - loss: 21.5992 - MinusLogProbMetric: 21.5992 - val_loss: 21.7235 - val_MinusLogProbMetric: 21.7235 - lr: 1.1111e-04 - 79s/epoch - 404ms/step
Epoch 183/1000
2023-09-27 13:13:50.626 
Epoch 183/1000 
	 loss: 21.5515, MinusLogProbMetric: 21.5515, val_loss: 21.6205, val_MinusLogProbMetric: 21.6205

Epoch 183: val_loss improved from 21.72352 to 21.62051, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 77s - loss: 21.5515 - MinusLogProbMetric: 21.5515 - val_loss: 21.6205 - val_MinusLogProbMetric: 21.6205 - lr: 1.1111e-04 - 77s/epoch - 394ms/step
Epoch 184/1000
2023-09-27 13:15:08.107 
Epoch 184/1000 
	 loss: 21.4725, MinusLogProbMetric: 21.4725, val_loss: 21.5489, val_MinusLogProbMetric: 21.5489

Epoch 184: val_loss improved from 21.62051 to 21.54886, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 78s - loss: 21.4725 - MinusLogProbMetric: 21.4725 - val_loss: 21.5489 - val_MinusLogProbMetric: 21.5489 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 185/1000
2023-09-27 13:16:25.382 
Epoch 185/1000 
	 loss: 21.4625, MinusLogProbMetric: 21.4625, val_loss: 21.4853, val_MinusLogProbMetric: 21.4853

Epoch 185: val_loss improved from 21.54886 to 21.48529, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 77s - loss: 21.4625 - MinusLogProbMetric: 21.4625 - val_loss: 21.4853 - val_MinusLogProbMetric: 21.4853 - lr: 1.1111e-04 - 77s/epoch - 393ms/step
Epoch 186/1000
2023-09-27 13:17:44.604 
Epoch 186/1000 
	 loss: 21.4394, MinusLogProbMetric: 21.4394, val_loss: 21.4635, val_MinusLogProbMetric: 21.4635

Epoch 186: val_loss improved from 21.48529 to 21.46347, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 80s - loss: 21.4394 - MinusLogProbMetric: 21.4394 - val_loss: 21.4635 - val_MinusLogProbMetric: 21.4635 - lr: 1.1111e-04 - 80s/epoch - 406ms/step
Epoch 187/1000
2023-09-27 13:19:03.017 
Epoch 187/1000 
	 loss: 21.3939, MinusLogProbMetric: 21.3939, val_loss: 21.3876, val_MinusLogProbMetric: 21.3876

Epoch 187: val_loss improved from 21.46347 to 21.38761, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 78s - loss: 21.3939 - MinusLogProbMetric: 21.3939 - val_loss: 21.3876 - val_MinusLogProbMetric: 21.3876 - lr: 1.1111e-04 - 78s/epoch - 399ms/step
Epoch 188/1000
2023-09-27 13:20:20.520 
Epoch 188/1000 
	 loss: 21.3556, MinusLogProbMetric: 21.3556, val_loss: 21.5065, val_MinusLogProbMetric: 21.5065

Epoch 188: val_loss did not improve from 21.38761
196/196 - 76s - loss: 21.3556 - MinusLogProbMetric: 21.3556 - val_loss: 21.5065 - val_MinusLogProbMetric: 21.5065 - lr: 1.1111e-04 - 76s/epoch - 388ms/step
Epoch 189/1000
2023-09-27 13:21:36.508 
Epoch 189/1000 
	 loss: 21.2882, MinusLogProbMetric: 21.2882, val_loss: 21.3782, val_MinusLogProbMetric: 21.3782

Epoch 189: val_loss improved from 21.38761 to 21.37825, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 78s - loss: 21.2882 - MinusLogProbMetric: 21.2882 - val_loss: 21.3782 - val_MinusLogProbMetric: 21.3782 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 190/1000
2023-09-27 13:22:53.216 
Epoch 190/1000 
	 loss: 21.2709, MinusLogProbMetric: 21.2709, val_loss: 21.2790, val_MinusLogProbMetric: 21.2790

Epoch 190: val_loss improved from 21.37825 to 21.27899, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 76s - loss: 21.2709 - MinusLogProbMetric: 21.2709 - val_loss: 21.2790 - val_MinusLogProbMetric: 21.2790 - lr: 1.1111e-04 - 76s/epoch - 390ms/step
Epoch 191/1000
2023-09-27 13:24:10.822 
Epoch 191/1000 
	 loss: 21.2596, MinusLogProbMetric: 21.2596, val_loss: 21.2510, val_MinusLogProbMetric: 21.2510

Epoch 191: val_loss improved from 21.27899 to 21.25103, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 78s - loss: 21.2596 - MinusLogProbMetric: 21.2596 - val_loss: 21.2510 - val_MinusLogProbMetric: 21.2510 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 192/1000
2023-09-27 13:25:29.007 
Epoch 192/1000 
	 loss: 21.1689, MinusLogProbMetric: 21.1689, val_loss: 21.2241, val_MinusLogProbMetric: 21.2241

Epoch 192: val_loss improved from 21.25103 to 21.22405, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 78s - loss: 21.1689 - MinusLogProbMetric: 21.1689 - val_loss: 21.2241 - val_MinusLogProbMetric: 21.2241 - lr: 1.1111e-04 - 78s/epoch - 400ms/step
Epoch 193/1000
2023-09-27 13:26:46.610 
Epoch 193/1000 
	 loss: 21.1580, MinusLogProbMetric: 21.1580, val_loss: 21.3231, val_MinusLogProbMetric: 21.3231

Epoch 193: val_loss did not improve from 21.22405
196/196 - 76s - loss: 21.1580 - MinusLogProbMetric: 21.1580 - val_loss: 21.3231 - val_MinusLogProbMetric: 21.3231 - lr: 1.1111e-04 - 76s/epoch - 388ms/step
Epoch 194/1000
2023-09-27 13:28:03.240 
Epoch 194/1000 
	 loss: 21.1544, MinusLogProbMetric: 21.1544, val_loss: 21.4106, val_MinusLogProbMetric: 21.4106

Epoch 194: val_loss did not improve from 21.22405
196/196 - 77s - loss: 21.1544 - MinusLogProbMetric: 21.1544 - val_loss: 21.4106 - val_MinusLogProbMetric: 21.4106 - lr: 1.1111e-04 - 77s/epoch - 391ms/step
Epoch 195/1000
2023-09-27 13:29:21.020 
Epoch 195/1000 
	 loss: 21.0607, MinusLogProbMetric: 21.0607, val_loss: 21.3941, val_MinusLogProbMetric: 21.3941

Epoch 195: val_loss did not improve from 21.22405
196/196 - 78s - loss: 21.0607 - MinusLogProbMetric: 21.0607 - val_loss: 21.3941 - val_MinusLogProbMetric: 21.3941 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 196/1000
2023-09-27 13:30:37.056 
Epoch 196/1000 
	 loss: 21.0456, MinusLogProbMetric: 21.0456, val_loss: 21.2256, val_MinusLogProbMetric: 21.2256

Epoch 196: val_loss did not improve from 21.22405
196/196 - 76s - loss: 21.0456 - MinusLogProbMetric: 21.0456 - val_loss: 21.2256 - val_MinusLogProbMetric: 21.2256 - lr: 1.1111e-04 - 76s/epoch - 388ms/step
Epoch 197/1000
2023-09-27 13:31:53.421 
Epoch 197/1000 
	 loss: 21.0109, MinusLogProbMetric: 21.0109, val_loss: 21.2303, val_MinusLogProbMetric: 21.2303

Epoch 197: val_loss did not improve from 21.22405
196/196 - 76s - loss: 21.0109 - MinusLogProbMetric: 21.0109 - val_loss: 21.2303 - val_MinusLogProbMetric: 21.2303 - lr: 1.1111e-04 - 76s/epoch - 390ms/step
Epoch 198/1000
2023-09-27 13:33:10.413 
Epoch 198/1000 
	 loss: 21.0126, MinusLogProbMetric: 21.0126, val_loss: 21.0596, val_MinusLogProbMetric: 21.0596

Epoch 198: val_loss improved from 21.22405 to 21.05957, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 78s - loss: 21.0126 - MinusLogProbMetric: 21.0126 - val_loss: 21.0596 - val_MinusLogProbMetric: 21.0596 - lr: 1.1111e-04 - 78s/epoch - 400ms/step
Epoch 199/1000
2023-09-27 13:34:28.795 
Epoch 199/1000 
	 loss: 23.8969, MinusLogProbMetric: 23.8969, val_loss: 23.1244, val_MinusLogProbMetric: 23.1244

Epoch 199: val_loss did not improve from 21.05957
196/196 - 77s - loss: 23.8969 - MinusLogProbMetric: 23.8969 - val_loss: 23.1244 - val_MinusLogProbMetric: 23.1244 - lr: 1.1111e-04 - 77s/epoch - 393ms/step
Epoch 200/1000
2023-09-27 13:35:44.565 
Epoch 200/1000 
	 loss: 21.8335, MinusLogProbMetric: 21.8335, val_loss: 21.3196, val_MinusLogProbMetric: 21.3196

Epoch 200: val_loss did not improve from 21.05957
196/196 - 76s - loss: 21.8335 - MinusLogProbMetric: 21.8335 - val_loss: 21.3196 - val_MinusLogProbMetric: 21.3196 - lr: 1.1111e-04 - 76s/epoch - 387ms/step
Epoch 201/1000
2023-09-27 13:37:00.613 
Epoch 201/1000 
	 loss: 21.0578, MinusLogProbMetric: 21.0578, val_loss: 21.0204, val_MinusLogProbMetric: 21.0204

Epoch 201: val_loss improved from 21.05957 to 21.02043, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 77s - loss: 21.0578 - MinusLogProbMetric: 21.0578 - val_loss: 21.0204 - val_MinusLogProbMetric: 21.0204 - lr: 1.1111e-04 - 77s/epoch - 395ms/step
Epoch 202/1000
2023-09-27 13:38:19.137 
Epoch 202/1000 
	 loss: 20.9867, MinusLogProbMetric: 20.9867, val_loss: 20.9781, val_MinusLogProbMetric: 20.9781

Epoch 202: val_loss improved from 21.02043 to 20.97814, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 79s - loss: 20.9867 - MinusLogProbMetric: 20.9867 - val_loss: 20.9781 - val_MinusLogProbMetric: 20.9781 - lr: 1.1111e-04 - 79s/epoch - 401ms/step
Epoch 203/1000
2023-09-27 13:39:36.640 
Epoch 203/1000 
	 loss: 20.9198, MinusLogProbMetric: 20.9198, val_loss: 20.9170, val_MinusLogProbMetric: 20.9170

Epoch 203: val_loss improved from 20.97814 to 20.91695, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 77s - loss: 20.9198 - MinusLogProbMetric: 20.9198 - val_loss: 20.9170 - val_MinusLogProbMetric: 20.9170 - lr: 1.1111e-04 - 77s/epoch - 395ms/step
Epoch 204/1000
2023-09-27 13:40:54.147 
Epoch 204/1000 
	 loss: 20.8577, MinusLogProbMetric: 20.8577, val_loss: 20.8595, val_MinusLogProbMetric: 20.8595

Epoch 204: val_loss improved from 20.91695 to 20.85951, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 77s - loss: 20.8577 - MinusLogProbMetric: 20.8577 - val_loss: 20.8595 - val_MinusLogProbMetric: 20.8595 - lr: 1.1111e-04 - 77s/epoch - 395ms/step
Epoch 205/1000
2023-09-27 13:42:12.326 
Epoch 205/1000 
	 loss: 20.7906, MinusLogProbMetric: 20.7906, val_loss: 20.8967, val_MinusLogProbMetric: 20.8967

Epoch 205: val_loss did not improve from 20.85951
196/196 - 77s - loss: 20.7906 - MinusLogProbMetric: 20.7906 - val_loss: 20.8967 - val_MinusLogProbMetric: 20.8967 - lr: 1.1111e-04 - 77s/epoch - 392ms/step
Epoch 206/1000
2023-09-27 13:43:28.459 
Epoch 206/1000 
	 loss: 20.7703, MinusLogProbMetric: 20.7703, val_loss: 21.0540, val_MinusLogProbMetric: 21.0540

Epoch 206: val_loss did not improve from 20.85951
196/196 - 76s - loss: 20.7703 - MinusLogProbMetric: 20.7703 - val_loss: 21.0540 - val_MinusLogProbMetric: 21.0540 - lr: 1.1111e-04 - 76s/epoch - 388ms/step
Epoch 207/1000
2023-09-27 13:44:44.804 
Epoch 207/1000 
	 loss: 20.7465, MinusLogProbMetric: 20.7465, val_loss: 20.7584, val_MinusLogProbMetric: 20.7584

Epoch 207: val_loss improved from 20.85951 to 20.75840, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 78s - loss: 20.7465 - MinusLogProbMetric: 20.7465 - val_loss: 20.7584 - val_MinusLogProbMetric: 20.7584 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 208/1000
2023-09-27 13:45:57.338 
Epoch 208/1000 
	 loss: 20.7347, MinusLogProbMetric: 20.7347, val_loss: 20.7860, val_MinusLogProbMetric: 20.7860

Epoch 208: val_loss did not improve from 20.75840
196/196 - 71s - loss: 20.7347 - MinusLogProbMetric: 20.7347 - val_loss: 20.7860 - val_MinusLogProbMetric: 20.7860 - lr: 1.1111e-04 - 71s/epoch - 363ms/step
Epoch 209/1000
2023-09-27 13:47:14.196 
Epoch 209/1000 
	 loss: 20.7079, MinusLogProbMetric: 20.7079, val_loss: 20.6683, val_MinusLogProbMetric: 20.6683

Epoch 209: val_loss improved from 20.75840 to 20.66829, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 78s - loss: 20.7079 - MinusLogProbMetric: 20.7079 - val_loss: 20.6683 - val_MinusLogProbMetric: 20.6683 - lr: 1.1111e-04 - 78s/epoch - 399ms/step
Epoch 210/1000
2023-09-27 13:48:32.027 
Epoch 210/1000 
	 loss: 20.6719, MinusLogProbMetric: 20.6719, val_loss: 20.9010, val_MinusLogProbMetric: 20.9010

Epoch 210: val_loss did not improve from 20.66829
196/196 - 76s - loss: 20.6719 - MinusLogProbMetric: 20.6719 - val_loss: 20.9010 - val_MinusLogProbMetric: 20.9010 - lr: 1.1111e-04 - 76s/epoch - 390ms/step
Epoch 211/1000
2023-09-27 13:49:47.605 
Epoch 211/1000 
	 loss: 20.6215, MinusLogProbMetric: 20.6215, val_loss: 20.8187, val_MinusLogProbMetric: 20.8187

Epoch 211: val_loss did not improve from 20.66829
196/196 - 76s - loss: 20.6215 - MinusLogProbMetric: 20.6215 - val_loss: 20.8187 - val_MinusLogProbMetric: 20.8187 - lr: 1.1111e-04 - 76s/epoch - 386ms/step
Epoch 212/1000
2023-09-27 13:51:03.279 
Epoch 212/1000 
	 loss: 20.6056, MinusLogProbMetric: 20.6056, val_loss: 20.6223, val_MinusLogProbMetric: 20.6223

Epoch 212: val_loss improved from 20.66829 to 20.62232, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 77s - loss: 20.6056 - MinusLogProbMetric: 20.6056 - val_loss: 20.6223 - val_MinusLogProbMetric: 20.6223 - lr: 1.1111e-04 - 77s/epoch - 395ms/step
Epoch 213/1000
2023-09-27 13:52:19.080 
Epoch 213/1000 
	 loss: 20.5700, MinusLogProbMetric: 20.5700, val_loss: 20.7673, val_MinusLogProbMetric: 20.7673

Epoch 213: val_loss did not improve from 20.62232
196/196 - 74s - loss: 20.5700 - MinusLogProbMetric: 20.5700 - val_loss: 20.7673 - val_MinusLogProbMetric: 20.7673 - lr: 1.1111e-04 - 74s/epoch - 378ms/step
Epoch 214/1000
2023-09-27 13:53:35.875 
Epoch 214/1000 
	 loss: 20.5537, MinusLogProbMetric: 20.5537, val_loss: 20.7141, val_MinusLogProbMetric: 20.7141

Epoch 214: val_loss did not improve from 20.62232
196/196 - 77s - loss: 20.5537 - MinusLogProbMetric: 20.5537 - val_loss: 20.7141 - val_MinusLogProbMetric: 20.7141 - lr: 1.1111e-04 - 77s/epoch - 392ms/step
Epoch 215/1000
2023-09-27 13:54:51.654 
Epoch 215/1000 
	 loss: 20.5343, MinusLogProbMetric: 20.5343, val_loss: 20.4738, val_MinusLogProbMetric: 20.4738

Epoch 215: val_loss improved from 20.62232 to 20.47378, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 77s - loss: 20.5343 - MinusLogProbMetric: 20.5343 - val_loss: 20.4738 - val_MinusLogProbMetric: 20.4738 - lr: 1.1111e-04 - 77s/epoch - 394ms/step
Epoch 216/1000
2023-09-27 13:56:09.166 
Epoch 216/1000 
	 loss: 20.5219, MinusLogProbMetric: 20.5219, val_loss: 20.5063, val_MinusLogProbMetric: 20.5063

Epoch 216: val_loss did not improve from 20.47378
196/196 - 76s - loss: 20.5219 - MinusLogProbMetric: 20.5219 - val_loss: 20.5063 - val_MinusLogProbMetric: 20.5063 - lr: 1.1111e-04 - 76s/epoch - 388ms/step
Epoch 217/1000
2023-09-27 13:57:24.090 
Epoch 217/1000 
	 loss: 20.4890, MinusLogProbMetric: 20.4890, val_loss: 20.7233, val_MinusLogProbMetric: 20.7233

Epoch 217: val_loss did not improve from 20.47378
196/196 - 75s - loss: 20.4890 - MinusLogProbMetric: 20.4890 - val_loss: 20.7233 - val_MinusLogProbMetric: 20.7233 - lr: 1.1111e-04 - 75s/epoch - 382ms/step
Epoch 218/1000
2023-09-27 13:58:39.505 
Epoch 218/1000 
	 loss: 20.4445, MinusLogProbMetric: 20.4445, val_loss: 20.5439, val_MinusLogProbMetric: 20.5439

Epoch 218: val_loss did not improve from 20.47378
196/196 - 75s - loss: 20.4445 - MinusLogProbMetric: 20.4445 - val_loss: 20.5439 - val_MinusLogProbMetric: 20.5439 - lr: 1.1111e-04 - 75s/epoch - 385ms/step
Epoch 219/1000
2023-09-27 13:59:56.022 
Epoch 219/1000 
	 loss: 20.4372, MinusLogProbMetric: 20.4372, val_loss: 20.5138, val_MinusLogProbMetric: 20.5138

Epoch 219: val_loss did not improve from 20.47378
196/196 - 77s - loss: 20.4372 - MinusLogProbMetric: 20.4372 - val_loss: 20.5138 - val_MinusLogProbMetric: 20.5138 - lr: 1.1111e-04 - 77s/epoch - 390ms/step
Epoch 220/1000
2023-09-27 14:01:11.549 
Epoch 220/1000 
	 loss: 20.3914, MinusLogProbMetric: 20.3914, val_loss: 20.5264, val_MinusLogProbMetric: 20.5264

Epoch 220: val_loss did not improve from 20.47378
196/196 - 76s - loss: 20.3914 - MinusLogProbMetric: 20.3914 - val_loss: 20.5264 - val_MinusLogProbMetric: 20.5264 - lr: 1.1111e-04 - 76s/epoch - 385ms/step
Epoch 221/1000
2023-09-27 14:02:28.574 
Epoch 221/1000 
	 loss: 20.3886, MinusLogProbMetric: 20.3886, val_loss: 20.6773, val_MinusLogProbMetric: 20.6773

Epoch 221: val_loss did not improve from 20.47378
196/196 - 77s - loss: 20.3886 - MinusLogProbMetric: 20.3886 - val_loss: 20.6773 - val_MinusLogProbMetric: 20.6773 - lr: 1.1111e-04 - 77s/epoch - 393ms/step
Epoch 222/1000
2023-09-27 14:03:45.530 
Epoch 222/1000 
	 loss: 20.3552, MinusLogProbMetric: 20.3552, val_loss: 20.4121, val_MinusLogProbMetric: 20.4121

Epoch 222: val_loss improved from 20.47378 to 20.41211, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 78s - loss: 20.3552 - MinusLogProbMetric: 20.3552 - val_loss: 20.4121 - val_MinusLogProbMetric: 20.4121 - lr: 1.1111e-04 - 78s/epoch - 400ms/step
Epoch 223/1000
2023-09-27 14:05:02.519 
Epoch 223/1000 
	 loss: 20.3760, MinusLogProbMetric: 20.3760, val_loss: 20.4794, val_MinusLogProbMetric: 20.4794

Epoch 223: val_loss did not improve from 20.41211
196/196 - 76s - loss: 20.3760 - MinusLogProbMetric: 20.3760 - val_loss: 20.4794 - val_MinusLogProbMetric: 20.4794 - lr: 1.1111e-04 - 76s/epoch - 386ms/step
Epoch 224/1000
2023-09-27 14:06:19.540 
Epoch 224/1000 
	 loss: 20.3545, MinusLogProbMetric: 20.3545, val_loss: 20.4940, val_MinusLogProbMetric: 20.4940

Epoch 224: val_loss did not improve from 20.41211
196/196 - 77s - loss: 20.3545 - MinusLogProbMetric: 20.3545 - val_loss: 20.4940 - val_MinusLogProbMetric: 20.4940 - lr: 1.1111e-04 - 77s/epoch - 393ms/step
Epoch 225/1000
2023-09-27 14:07:34.898 
Epoch 225/1000 
	 loss: 20.3313, MinusLogProbMetric: 20.3313, val_loss: 20.3491, val_MinusLogProbMetric: 20.3491

Epoch 225: val_loss improved from 20.41211 to 20.34910, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 77s - loss: 20.3313 - MinusLogProbMetric: 20.3313 - val_loss: 20.3491 - val_MinusLogProbMetric: 20.3491 - lr: 1.1111e-04 - 77s/epoch - 392ms/step
Epoch 226/1000
2023-09-27 14:08:53.559 
Epoch 226/1000 
	 loss: 20.2694, MinusLogProbMetric: 20.2694, val_loss: 20.6014, val_MinusLogProbMetric: 20.6014

Epoch 226: val_loss did not improve from 20.34910
196/196 - 77s - loss: 20.2694 - MinusLogProbMetric: 20.2694 - val_loss: 20.6014 - val_MinusLogProbMetric: 20.6014 - lr: 1.1111e-04 - 77s/epoch - 394ms/step
Epoch 227/1000
2023-09-27 14:10:10.752 
Epoch 227/1000 
	 loss: 20.2648, MinusLogProbMetric: 20.2648, val_loss: 20.3290, val_MinusLogProbMetric: 20.3290

Epoch 227: val_loss improved from 20.34910 to 20.32899, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 79s - loss: 20.2648 - MinusLogProbMetric: 20.2648 - val_loss: 20.3290 - val_MinusLogProbMetric: 20.3290 - lr: 1.1111e-04 - 79s/epoch - 402ms/step
Epoch 228/1000
2023-09-27 14:11:29.047 
Epoch 228/1000 
	 loss: 20.2650, MinusLogProbMetric: 20.2650, val_loss: 20.3313, val_MinusLogProbMetric: 20.3313

Epoch 228: val_loss did not improve from 20.32899
196/196 - 77s - loss: 20.2650 - MinusLogProbMetric: 20.2650 - val_loss: 20.3313 - val_MinusLogProbMetric: 20.3313 - lr: 1.1111e-04 - 77s/epoch - 392ms/step
Epoch 229/1000
2023-09-27 14:12:46.429 
Epoch 229/1000 
	 loss: 20.1976, MinusLogProbMetric: 20.1976, val_loss: 20.2853, val_MinusLogProbMetric: 20.2853

Epoch 229: val_loss improved from 20.32899 to 20.28532, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 79s - loss: 20.1976 - MinusLogProbMetric: 20.1976 - val_loss: 20.2853 - val_MinusLogProbMetric: 20.2853 - lr: 1.1111e-04 - 79s/epoch - 403ms/step
Epoch 230/1000
2023-09-27 14:14:03.909 
Epoch 230/1000 
	 loss: 20.1971, MinusLogProbMetric: 20.1971, val_loss: 20.2374, val_MinusLogProbMetric: 20.2374

Epoch 230: val_loss improved from 20.28532 to 20.23738, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 78s - loss: 20.1971 - MinusLogProbMetric: 20.1971 - val_loss: 20.2374 - val_MinusLogProbMetric: 20.2374 - lr: 1.1111e-04 - 78s/epoch - 395ms/step
Epoch 231/1000
2023-09-27 14:15:21.687 
Epoch 231/1000 
	 loss: 20.2023, MinusLogProbMetric: 20.2023, val_loss: 20.4235, val_MinusLogProbMetric: 20.4235

Epoch 231: val_loss did not improve from 20.23738
196/196 - 76s - loss: 20.2023 - MinusLogProbMetric: 20.2023 - val_loss: 20.4235 - val_MinusLogProbMetric: 20.4235 - lr: 1.1111e-04 - 76s/epoch - 389ms/step
Epoch 232/1000
2023-09-27 14:16:37.153 
Epoch 232/1000 
	 loss: 20.1585, MinusLogProbMetric: 20.1585, val_loss: 20.2684, val_MinusLogProbMetric: 20.2684

Epoch 232: val_loss did not improve from 20.23738
196/196 - 75s - loss: 20.1585 - MinusLogProbMetric: 20.1585 - val_loss: 20.2684 - val_MinusLogProbMetric: 20.2684 - lr: 1.1111e-04 - 75s/epoch - 385ms/step
Epoch 233/1000
2023-09-27 14:17:53.366 
Epoch 233/1000 
	 loss: 20.1499, MinusLogProbMetric: 20.1499, val_loss: 20.3222, val_MinusLogProbMetric: 20.3222

Epoch 233: val_loss did not improve from 20.23738
196/196 - 76s - loss: 20.1499 - MinusLogProbMetric: 20.1499 - val_loss: 20.3222 - val_MinusLogProbMetric: 20.3222 - lr: 1.1111e-04 - 76s/epoch - 389ms/step
Epoch 234/1000
2023-09-27 14:19:09.535 
Epoch 234/1000 
	 loss: 20.1531, MinusLogProbMetric: 20.1531, val_loss: 20.2040, val_MinusLogProbMetric: 20.2040

Epoch 234: val_loss improved from 20.23738 to 20.20395, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 78s - loss: 20.1531 - MinusLogProbMetric: 20.1531 - val_loss: 20.2040 - val_MinusLogProbMetric: 20.2040 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 235/1000
2023-09-27 14:20:27.512 
Epoch 235/1000 
	 loss: 20.1087, MinusLogProbMetric: 20.1087, val_loss: 20.3554, val_MinusLogProbMetric: 20.3554

Epoch 235: val_loss did not improve from 20.20395
196/196 - 76s - loss: 20.1087 - MinusLogProbMetric: 20.1087 - val_loss: 20.3554 - val_MinusLogProbMetric: 20.3554 - lr: 1.1111e-04 - 76s/epoch - 390ms/step
Epoch 236/1000
2023-09-27 14:21:43.211 
Epoch 236/1000 
	 loss: 20.1285, MinusLogProbMetric: 20.1285, val_loss: 20.3167, val_MinusLogProbMetric: 20.3167

Epoch 236: val_loss did not improve from 20.20395
196/196 - 76s - loss: 20.1285 - MinusLogProbMetric: 20.1285 - val_loss: 20.3167 - val_MinusLogProbMetric: 20.3167 - lr: 1.1111e-04 - 76s/epoch - 386ms/step
Epoch 237/1000
2023-09-27 14:23:00.336 
Epoch 237/1000 
	 loss: 20.0946, MinusLogProbMetric: 20.0946, val_loss: 20.3227, val_MinusLogProbMetric: 20.3227

Epoch 237: val_loss did not improve from 20.20395
196/196 - 77s - loss: 20.0946 - MinusLogProbMetric: 20.0946 - val_loss: 20.3227 - val_MinusLogProbMetric: 20.3227 - lr: 1.1111e-04 - 77s/epoch - 393ms/step
Epoch 238/1000
2023-09-27 14:24:16.370 
Epoch 238/1000 
	 loss: 20.0693, MinusLogProbMetric: 20.0693, val_loss: 20.1962, val_MinusLogProbMetric: 20.1962

Epoch 238: val_loss improved from 20.20395 to 20.19615, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 77s - loss: 20.0693 - MinusLogProbMetric: 20.0693 - val_loss: 20.1962 - val_MinusLogProbMetric: 20.1962 - lr: 1.1111e-04 - 77s/epoch - 395ms/step
Epoch 239/1000
2023-09-27 14:25:34.271 
Epoch 239/1000 
	 loss: 20.0448, MinusLogProbMetric: 20.0448, val_loss: 20.0301, val_MinusLogProbMetric: 20.0301

Epoch 239: val_loss improved from 20.19615 to 20.03013, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 78s - loss: 20.0448 - MinusLogProbMetric: 20.0448 - val_loss: 20.0301 - val_MinusLogProbMetric: 20.0301 - lr: 1.1111e-04 - 78s/epoch - 399ms/step
Epoch 240/1000
2023-09-27 14:26:53.324 
Epoch 240/1000 
	 loss: 20.0363, MinusLogProbMetric: 20.0363, val_loss: 20.2374, val_MinusLogProbMetric: 20.2374

Epoch 240: val_loss did not improve from 20.03013
196/196 - 77s - loss: 20.0363 - MinusLogProbMetric: 20.0363 - val_loss: 20.2374 - val_MinusLogProbMetric: 20.2374 - lr: 1.1111e-04 - 77s/epoch - 395ms/step
Epoch 241/1000
2023-09-27 14:28:09.062 
Epoch 241/1000 
	 loss: 20.0041, MinusLogProbMetric: 20.0041, val_loss: 19.9701, val_MinusLogProbMetric: 19.9701

Epoch 241: val_loss improved from 20.03013 to 19.97013, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 77s - loss: 20.0041 - MinusLogProbMetric: 20.0041 - val_loss: 19.9701 - val_MinusLogProbMetric: 19.9701 - lr: 1.1111e-04 - 77s/epoch - 393ms/step
Epoch 242/1000
2023-09-27 14:29:26.242 
Epoch 242/1000 
	 loss: 19.9642, MinusLogProbMetric: 19.9642, val_loss: 20.1697, val_MinusLogProbMetric: 20.1697

Epoch 242: val_loss did not improve from 19.97013
196/196 - 76s - loss: 19.9642 - MinusLogProbMetric: 19.9642 - val_loss: 20.1697 - val_MinusLogProbMetric: 20.1697 - lr: 1.1111e-04 - 76s/epoch - 387ms/step
Epoch 243/1000
2023-09-27 14:30:42.314 
Epoch 243/1000 
	 loss: 19.9688, MinusLogProbMetric: 19.9688, val_loss: 20.0334, val_MinusLogProbMetric: 20.0334

Epoch 243: val_loss did not improve from 19.97013
196/196 - 76s - loss: 19.9688 - MinusLogProbMetric: 19.9688 - val_loss: 20.0334 - val_MinusLogProbMetric: 20.0334 - lr: 1.1111e-04 - 76s/epoch - 388ms/step
Epoch 244/1000
2023-09-27 14:31:58.576 
Epoch 244/1000 
	 loss: 19.9476, MinusLogProbMetric: 19.9476, val_loss: 20.0761, val_MinusLogProbMetric: 20.0761

Epoch 244: val_loss did not improve from 19.97013
196/196 - 76s - loss: 19.9476 - MinusLogProbMetric: 19.9476 - val_loss: 20.0761 - val_MinusLogProbMetric: 20.0761 - lr: 1.1111e-04 - 76s/epoch - 389ms/step
Epoch 245/1000
2023-09-27 14:33:12.985 
Epoch 245/1000 
	 loss: 19.9342, MinusLogProbMetric: 19.9342, val_loss: 20.0426, val_MinusLogProbMetric: 20.0426

Epoch 245: val_loss did not improve from 19.97013
196/196 - 74s - loss: 19.9342 - MinusLogProbMetric: 19.9342 - val_loss: 20.0426 - val_MinusLogProbMetric: 20.0426 - lr: 1.1111e-04 - 74s/epoch - 380ms/step
Epoch 246/1000
2023-09-27 14:34:28.306 
Epoch 246/1000 
	 loss: 19.9034, MinusLogProbMetric: 19.9034, val_loss: 20.1250, val_MinusLogProbMetric: 20.1250

Epoch 246: val_loss did not improve from 19.97013
196/196 - 75s - loss: 19.9034 - MinusLogProbMetric: 19.9034 - val_loss: 20.1250 - val_MinusLogProbMetric: 20.1250 - lr: 1.1111e-04 - 75s/epoch - 384ms/step
Epoch 247/1000
2023-09-27 14:35:43.919 
Epoch 247/1000 
	 loss: 19.9114, MinusLogProbMetric: 19.9114, val_loss: 19.9127, val_MinusLogProbMetric: 19.9127

Epoch 247: val_loss improved from 19.97013 to 19.91273, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 77s - loss: 19.9114 - MinusLogProbMetric: 19.9114 - val_loss: 19.9127 - val_MinusLogProbMetric: 19.9127 - lr: 1.1111e-04 - 77s/epoch - 392ms/step
Epoch 248/1000
2023-09-27 14:37:01.744 
Epoch 248/1000 
	 loss: 19.9056, MinusLogProbMetric: 19.9056, val_loss: 19.9489, val_MinusLogProbMetric: 19.9489

Epoch 248: val_loss did not improve from 19.91273
196/196 - 77s - loss: 19.9056 - MinusLogProbMetric: 19.9056 - val_loss: 19.9489 - val_MinusLogProbMetric: 19.9489 - lr: 1.1111e-04 - 77s/epoch - 391ms/step
Epoch 249/1000
2023-09-27 14:38:18.766 
Epoch 249/1000 
	 loss: 19.9124, MinusLogProbMetric: 19.9124, val_loss: 19.9634, val_MinusLogProbMetric: 19.9634

Epoch 249: val_loss did not improve from 19.91273
196/196 - 77s - loss: 19.9124 - MinusLogProbMetric: 19.9124 - val_loss: 19.9634 - val_MinusLogProbMetric: 19.9634 - lr: 1.1111e-04 - 77s/epoch - 393ms/step
Epoch 250/1000
2023-09-27 14:39:35.293 
Epoch 250/1000 
	 loss: 19.8708, MinusLogProbMetric: 19.8708, val_loss: 19.9504, val_MinusLogProbMetric: 19.9504

Epoch 250: val_loss did not improve from 19.91273
196/196 - 77s - loss: 19.8708 - MinusLogProbMetric: 19.8708 - val_loss: 19.9504 - val_MinusLogProbMetric: 19.9504 - lr: 1.1111e-04 - 77s/epoch - 390ms/step
Epoch 251/1000
2023-09-27 14:40:51.925 
Epoch 251/1000 
	 loss: 19.8152, MinusLogProbMetric: 19.8152, val_loss: 19.9670, val_MinusLogProbMetric: 19.9670

Epoch 251: val_loss did not improve from 19.91273
196/196 - 77s - loss: 19.8152 - MinusLogProbMetric: 19.8152 - val_loss: 19.9670 - val_MinusLogProbMetric: 19.9670 - lr: 1.1111e-04 - 77s/epoch - 391ms/step
Epoch 252/1000
2023-09-27 14:42:08.441 
Epoch 252/1000 
	 loss: 19.8439, MinusLogProbMetric: 19.8439, val_loss: 20.0096, val_MinusLogProbMetric: 20.0096

Epoch 252: val_loss did not improve from 19.91273
196/196 - 77s - loss: 19.8439 - MinusLogProbMetric: 19.8439 - val_loss: 20.0096 - val_MinusLogProbMetric: 20.0096 - lr: 1.1111e-04 - 77s/epoch - 390ms/step
Epoch 253/1000
2023-09-27 14:43:24.493 
Epoch 253/1000 
	 loss: 19.8387, MinusLogProbMetric: 19.8387, val_loss: 19.8919, val_MinusLogProbMetric: 19.8919

Epoch 253: val_loss improved from 19.91273 to 19.89186, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 78s - loss: 19.8387 - MinusLogProbMetric: 19.8387 - val_loss: 19.8919 - val_MinusLogProbMetric: 19.8919 - lr: 1.1111e-04 - 78s/epoch - 395ms/step
Epoch 254/1000
2023-09-27 14:44:41.041 
Epoch 254/1000 
	 loss: 19.7974, MinusLogProbMetric: 19.7974, val_loss: 19.7981, val_MinusLogProbMetric: 19.7981

Epoch 254: val_loss improved from 19.89186 to 19.79806, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 77s - loss: 19.7974 - MinusLogProbMetric: 19.7974 - val_loss: 19.7981 - val_MinusLogProbMetric: 19.7981 - lr: 1.1111e-04 - 77s/epoch - 391ms/step
Epoch 255/1000
2023-09-27 14:45:58.321 
Epoch 255/1000 
	 loss: 19.8111, MinusLogProbMetric: 19.8111, val_loss: 19.9105, val_MinusLogProbMetric: 19.9105

Epoch 255: val_loss did not improve from 19.79806
196/196 - 76s - loss: 19.8111 - MinusLogProbMetric: 19.8111 - val_loss: 19.9105 - val_MinusLogProbMetric: 19.9105 - lr: 1.1111e-04 - 76s/epoch - 386ms/step
Epoch 256/1000
2023-09-27 14:47:13.649 
Epoch 256/1000 
	 loss: 19.7913, MinusLogProbMetric: 19.7913, val_loss: 19.8297, val_MinusLogProbMetric: 19.8297

Epoch 256: val_loss did not improve from 19.79806
196/196 - 75s - loss: 19.7913 - MinusLogProbMetric: 19.7913 - val_loss: 19.8297 - val_MinusLogProbMetric: 19.8297 - lr: 1.1111e-04 - 75s/epoch - 384ms/step
Epoch 257/1000
2023-09-27 14:48:28.799 
Epoch 257/1000 
	 loss: 19.7727, MinusLogProbMetric: 19.7727, val_loss: 19.9291, val_MinusLogProbMetric: 19.9291

Epoch 257: val_loss did not improve from 19.79806
196/196 - 75s - loss: 19.7727 - MinusLogProbMetric: 19.7727 - val_loss: 19.9291 - val_MinusLogProbMetric: 19.9291 - lr: 1.1111e-04 - 75s/epoch - 383ms/step
Epoch 258/1000
2023-09-27 14:49:44.865 
Epoch 258/1000 
	 loss: 19.7272, MinusLogProbMetric: 19.7272, val_loss: 19.8919, val_MinusLogProbMetric: 19.8919

Epoch 258: val_loss did not improve from 19.79806
196/196 - 76s - loss: 19.7272 - MinusLogProbMetric: 19.7272 - val_loss: 19.8919 - val_MinusLogProbMetric: 19.8919 - lr: 1.1111e-04 - 76s/epoch - 388ms/step
Epoch 259/1000
2023-09-27 14:51:01.156 
Epoch 259/1000 
	 loss: 19.7251, MinusLogProbMetric: 19.7251, val_loss: 19.7821, val_MinusLogProbMetric: 19.7821

Epoch 259: val_loss improved from 19.79806 to 19.78207, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 78s - loss: 19.7251 - MinusLogProbMetric: 19.7251 - val_loss: 19.7821 - val_MinusLogProbMetric: 19.7821 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 260/1000
2023-09-27 14:52:18.343 
Epoch 260/1000 
	 loss: 19.7142, MinusLogProbMetric: 19.7142, val_loss: 19.9470, val_MinusLogProbMetric: 19.9470

Epoch 260: val_loss did not improve from 19.78207
196/196 - 76s - loss: 19.7142 - MinusLogProbMetric: 19.7142 - val_loss: 19.9470 - val_MinusLogProbMetric: 19.9470 - lr: 1.1111e-04 - 76s/epoch - 387ms/step
Epoch 261/1000
2023-09-27 14:53:34.131 
Epoch 261/1000 
	 loss: 19.7241, MinusLogProbMetric: 19.7241, val_loss: 19.9717, val_MinusLogProbMetric: 19.9717

Epoch 261: val_loss did not improve from 19.78207
196/196 - 76s - loss: 19.7241 - MinusLogProbMetric: 19.7241 - val_loss: 19.9717 - val_MinusLogProbMetric: 19.9717 - lr: 1.1111e-04 - 76s/epoch - 387ms/step
Epoch 262/1000
2023-09-27 14:54:50.446 
Epoch 262/1000 
	 loss: 19.6974, MinusLogProbMetric: 19.6974, val_loss: 19.8157, val_MinusLogProbMetric: 19.8157

Epoch 262: val_loss did not improve from 19.78207
196/196 - 76s - loss: 19.6974 - MinusLogProbMetric: 19.6974 - val_loss: 19.8157 - val_MinusLogProbMetric: 19.8157 - lr: 1.1111e-04 - 76s/epoch - 389ms/step
Epoch 263/1000
2023-09-27 14:56:07.033 
Epoch 263/1000 
	 loss: 19.6892, MinusLogProbMetric: 19.6892, val_loss: 19.9360, val_MinusLogProbMetric: 19.9360

Epoch 263: val_loss did not improve from 19.78207
196/196 - 77s - loss: 19.6892 - MinusLogProbMetric: 19.6892 - val_loss: 19.9360 - val_MinusLogProbMetric: 19.9360 - lr: 1.1111e-04 - 77s/epoch - 391ms/step
Epoch 264/1000
2023-09-27 14:57:23.040 
Epoch 264/1000 
	 loss: 19.7051, MinusLogProbMetric: 19.7051, val_loss: 19.7499, val_MinusLogProbMetric: 19.7499

Epoch 264: val_loss improved from 19.78207 to 19.74992, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 77s - loss: 19.7051 - MinusLogProbMetric: 19.7051 - val_loss: 19.7499 - val_MinusLogProbMetric: 19.7499 - lr: 1.1111e-04 - 77s/epoch - 395ms/step
Epoch 265/1000
2023-09-27 14:58:40.019 
Epoch 265/1000 
	 loss: 19.6296, MinusLogProbMetric: 19.6296, val_loss: 20.2310, val_MinusLogProbMetric: 20.2310

Epoch 265: val_loss did not improve from 19.74992
196/196 - 76s - loss: 19.6296 - MinusLogProbMetric: 19.6296 - val_loss: 20.2310 - val_MinusLogProbMetric: 20.2310 - lr: 1.1111e-04 - 76s/epoch - 386ms/step
Epoch 266/1000
2023-09-27 14:59:54.964 
Epoch 266/1000 
	 loss: 19.6590, MinusLogProbMetric: 19.6590, val_loss: 19.8194, val_MinusLogProbMetric: 19.8194

Epoch 266: val_loss did not improve from 19.74992
196/196 - 75s - loss: 19.6590 - MinusLogProbMetric: 19.6590 - val_loss: 19.8194 - val_MinusLogProbMetric: 19.8194 - lr: 1.1111e-04 - 75s/epoch - 382ms/step
Epoch 267/1000
2023-09-27 15:01:12.288 
Epoch 267/1000 
	 loss: 19.6238, MinusLogProbMetric: 19.6238, val_loss: 19.7493, val_MinusLogProbMetric: 19.7493

Epoch 267: val_loss improved from 19.74992 to 19.74926, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 78s - loss: 19.6238 - MinusLogProbMetric: 19.6238 - val_loss: 19.7493 - val_MinusLogProbMetric: 19.7493 - lr: 1.1111e-04 - 78s/epoch - 400ms/step
Epoch 268/1000
2023-09-27 15:02:29.731 
Epoch 268/1000 
	 loss: 19.6132, MinusLogProbMetric: 19.6132, val_loss: 19.6558, val_MinusLogProbMetric: 19.6558

Epoch 268: val_loss improved from 19.74926 to 19.65575, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 78s - loss: 19.6132 - MinusLogProbMetric: 19.6132 - val_loss: 19.6558 - val_MinusLogProbMetric: 19.6558 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 269/1000
2023-09-27 15:03:47.882 
Epoch 269/1000 
	 loss: 19.6419, MinusLogProbMetric: 19.6419, val_loss: 19.6700, val_MinusLogProbMetric: 19.6700

Epoch 269: val_loss did not improve from 19.65575
196/196 - 77s - loss: 19.6419 - MinusLogProbMetric: 19.6419 - val_loss: 19.6700 - val_MinusLogProbMetric: 19.6700 - lr: 1.1111e-04 - 77s/epoch - 392ms/step
Epoch 270/1000
2023-09-27 15:05:04.248 
Epoch 270/1000 
	 loss: 19.5903, MinusLogProbMetric: 19.5903, val_loss: 19.6496, val_MinusLogProbMetric: 19.6496

Epoch 270: val_loss improved from 19.65575 to 19.64961, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 78s - loss: 19.5903 - MinusLogProbMetric: 19.5903 - val_loss: 19.6496 - val_MinusLogProbMetric: 19.6496 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 271/1000
2023-09-27 15:06:22.661 
Epoch 271/1000 
	 loss: 19.5762, MinusLogProbMetric: 19.5762, val_loss: 19.6790, val_MinusLogProbMetric: 19.6790

Epoch 271: val_loss did not improve from 19.64961
196/196 - 77s - loss: 19.5762 - MinusLogProbMetric: 19.5762 - val_loss: 19.6790 - val_MinusLogProbMetric: 19.6790 - lr: 1.1111e-04 - 77s/epoch - 394ms/step
Epoch 272/1000
2023-09-27 15:07:40.074 
Epoch 272/1000 
	 loss: 19.5479, MinusLogProbMetric: 19.5479, val_loss: 19.7978, val_MinusLogProbMetric: 19.7978

Epoch 272: val_loss did not improve from 19.64961
196/196 - 77s - loss: 19.5479 - MinusLogProbMetric: 19.5479 - val_loss: 19.7978 - val_MinusLogProbMetric: 19.7978 - lr: 1.1111e-04 - 77s/epoch - 395ms/step
Epoch 273/1000
2023-09-27 15:08:56.235 
Epoch 273/1000 
	 loss: 19.5427, MinusLogProbMetric: 19.5427, val_loss: 19.6257, val_MinusLogProbMetric: 19.6257

Epoch 273: val_loss improved from 19.64961 to 19.62568, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 78s - loss: 19.5427 - MinusLogProbMetric: 19.5427 - val_loss: 19.6257 - val_MinusLogProbMetric: 19.6257 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 274/1000
2023-09-27 15:10:13.424 
Epoch 274/1000 
	 loss: 19.5592, MinusLogProbMetric: 19.5592, val_loss: 19.6395, val_MinusLogProbMetric: 19.6395

Epoch 274: val_loss did not improve from 19.62568
196/196 - 76s - loss: 19.5592 - MinusLogProbMetric: 19.5592 - val_loss: 19.6395 - val_MinusLogProbMetric: 19.6395 - lr: 1.1111e-04 - 76s/epoch - 387ms/step
Epoch 275/1000
2023-09-27 15:11:29.484 
Epoch 275/1000 
	 loss: 19.5449, MinusLogProbMetric: 19.5449, val_loss: 19.8028, val_MinusLogProbMetric: 19.8028

Epoch 275: val_loss did not improve from 19.62568
196/196 - 76s - loss: 19.5449 - MinusLogProbMetric: 19.5449 - val_loss: 19.8028 - val_MinusLogProbMetric: 19.8028 - lr: 1.1111e-04 - 76s/epoch - 388ms/step
Epoch 276/1000
2023-09-27 15:12:46.357 
Epoch 276/1000 
	 loss: 19.5257, MinusLogProbMetric: 19.5257, val_loss: 19.7345, val_MinusLogProbMetric: 19.7345

Epoch 276: val_loss did not improve from 19.62568
196/196 - 77s - loss: 19.5257 - MinusLogProbMetric: 19.5257 - val_loss: 19.7345 - val_MinusLogProbMetric: 19.7345 - lr: 1.1111e-04 - 77s/epoch - 392ms/step
Epoch 277/1000
2023-09-27 15:14:01.893 
Epoch 277/1000 
	 loss: 19.5152, MinusLogProbMetric: 19.5152, val_loss: 19.5416, val_MinusLogProbMetric: 19.5416

Epoch 277: val_loss improved from 19.62568 to 19.54165, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 77s - loss: 19.5152 - MinusLogProbMetric: 19.5152 - val_loss: 19.5416 - val_MinusLogProbMetric: 19.5416 - lr: 1.1111e-04 - 77s/epoch - 392ms/step
Epoch 278/1000
2023-09-27 15:15:20.073 
Epoch 278/1000 
	 loss: 19.4882, MinusLogProbMetric: 19.4882, val_loss: 19.5680, val_MinusLogProbMetric: 19.5680

Epoch 278: val_loss did not improve from 19.54165
196/196 - 77s - loss: 19.4882 - MinusLogProbMetric: 19.4882 - val_loss: 19.5680 - val_MinusLogProbMetric: 19.5680 - lr: 1.1111e-04 - 77s/epoch - 393ms/step
Epoch 279/1000
2023-09-27 15:16:36.546 
Epoch 279/1000 
	 loss: 19.4906, MinusLogProbMetric: 19.4906, val_loss: 19.6592, val_MinusLogProbMetric: 19.6592

Epoch 279: val_loss did not improve from 19.54165
196/196 - 76s - loss: 19.4906 - MinusLogProbMetric: 19.4906 - val_loss: 19.6592 - val_MinusLogProbMetric: 19.6592 - lr: 1.1111e-04 - 76s/epoch - 390ms/step
Epoch 280/1000
2023-09-27 15:17:52.334 
Epoch 280/1000 
	 loss: 19.4831, MinusLogProbMetric: 19.4831, val_loss: 19.5843, val_MinusLogProbMetric: 19.5843

Epoch 280: val_loss did not improve from 19.54165
196/196 - 76s - loss: 19.4831 - MinusLogProbMetric: 19.4831 - val_loss: 19.5843 - val_MinusLogProbMetric: 19.5843 - lr: 1.1111e-04 - 76s/epoch - 387ms/step
Epoch 281/1000
2023-09-27 15:19:07.886 
Epoch 281/1000 
	 loss: 19.4984, MinusLogProbMetric: 19.4984, val_loss: 19.4780, val_MinusLogProbMetric: 19.4780

Epoch 281: val_loss improved from 19.54165 to 19.47796, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 77s - loss: 19.4984 - MinusLogProbMetric: 19.4984 - val_loss: 19.4780 - val_MinusLogProbMetric: 19.4780 - lr: 1.1111e-04 - 77s/epoch - 392ms/step
Epoch 282/1000
2023-09-27 15:20:24.846 
Epoch 282/1000 
	 loss: 19.4495, MinusLogProbMetric: 19.4495, val_loss: 19.6033, val_MinusLogProbMetric: 19.6033

Epoch 282: val_loss did not improve from 19.47796
196/196 - 76s - loss: 19.4495 - MinusLogProbMetric: 19.4495 - val_loss: 19.6033 - val_MinusLogProbMetric: 19.6033 - lr: 1.1111e-04 - 76s/epoch - 387ms/step
Epoch 283/1000
2023-09-27 15:21:39.425 
Epoch 283/1000 
	 loss: 19.4707, MinusLogProbMetric: 19.4707, val_loss: 19.5269, val_MinusLogProbMetric: 19.5269

Epoch 283: val_loss did not improve from 19.47796
196/196 - 75s - loss: 19.4707 - MinusLogProbMetric: 19.4707 - val_loss: 19.5269 - val_MinusLogProbMetric: 19.5269 - lr: 1.1111e-04 - 75s/epoch - 381ms/step
Epoch 284/1000
2023-09-27 15:22:55.030 
Epoch 284/1000 
	 loss: 19.4197, MinusLogProbMetric: 19.4197, val_loss: 19.4290, val_MinusLogProbMetric: 19.4290

Epoch 284: val_loss improved from 19.47796 to 19.42898, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 77s - loss: 19.4197 - MinusLogProbMetric: 19.4197 - val_loss: 19.4290 - val_MinusLogProbMetric: 19.4290 - lr: 1.1111e-04 - 77s/epoch - 392ms/step
Epoch 285/1000
2023-09-27 15:24:12.160 
Epoch 285/1000 
	 loss: 19.4287, MinusLogProbMetric: 19.4287, val_loss: 19.6002, val_MinusLogProbMetric: 19.6002

Epoch 285: val_loss did not improve from 19.42898
196/196 - 76s - loss: 19.4287 - MinusLogProbMetric: 19.4287 - val_loss: 19.6002 - val_MinusLogProbMetric: 19.6002 - lr: 1.1111e-04 - 76s/epoch - 387ms/step
Epoch 286/1000
2023-09-27 15:25:28.167 
Epoch 286/1000 
	 loss: 19.4151, MinusLogProbMetric: 19.4151, val_loss: 19.5507, val_MinusLogProbMetric: 19.5507

Epoch 286: val_loss did not improve from 19.42898
196/196 - 76s - loss: 19.4151 - MinusLogProbMetric: 19.4151 - val_loss: 19.5507 - val_MinusLogProbMetric: 19.5507 - lr: 1.1111e-04 - 76s/epoch - 388ms/step
Epoch 287/1000
2023-09-27 15:26:45.259 
Epoch 287/1000 
	 loss: 19.4005, MinusLogProbMetric: 19.4005, val_loss: 19.6498, val_MinusLogProbMetric: 19.6498

Epoch 287: val_loss did not improve from 19.42898
196/196 - 77s - loss: 19.4005 - MinusLogProbMetric: 19.4005 - val_loss: 19.6498 - val_MinusLogProbMetric: 19.6498 - lr: 1.1111e-04 - 77s/epoch - 393ms/step
Epoch 288/1000
2023-09-27 15:28:01.184 
Epoch 288/1000 
	 loss: 19.4078, MinusLogProbMetric: 19.4078, val_loss: 19.5952, val_MinusLogProbMetric: 19.5952

Epoch 288: val_loss did not improve from 19.42898
196/196 - 76s - loss: 19.4078 - MinusLogProbMetric: 19.4078 - val_loss: 19.5952 - val_MinusLogProbMetric: 19.5952 - lr: 1.1111e-04 - 76s/epoch - 387ms/step
Epoch 289/1000
2023-09-27 15:29:16.882 
Epoch 289/1000 
	 loss: 19.3847, MinusLogProbMetric: 19.3847, val_loss: 19.6637, val_MinusLogProbMetric: 19.6637

Epoch 289: val_loss did not improve from 19.42898
196/196 - 76s - loss: 19.3847 - MinusLogProbMetric: 19.3847 - val_loss: 19.6637 - val_MinusLogProbMetric: 19.6637 - lr: 1.1111e-04 - 76s/epoch - 386ms/step
Epoch 290/1000
2023-09-27 15:30:32.814 
Epoch 290/1000 
	 loss: 19.4014, MinusLogProbMetric: 19.4014, val_loss: 19.5633, val_MinusLogProbMetric: 19.5633

Epoch 290: val_loss did not improve from 19.42898
196/196 - 76s - loss: 19.4014 - MinusLogProbMetric: 19.4014 - val_loss: 19.5633 - val_MinusLogProbMetric: 19.5633 - lr: 1.1111e-04 - 76s/epoch - 387ms/step
Epoch 291/1000
2023-09-27 15:31:48.556 
Epoch 291/1000 
	 loss: 19.3647, MinusLogProbMetric: 19.3647, val_loss: 19.5040, val_MinusLogProbMetric: 19.5040

Epoch 291: val_loss did not improve from 19.42898
196/196 - 76s - loss: 19.3647 - MinusLogProbMetric: 19.3647 - val_loss: 19.5040 - val_MinusLogProbMetric: 19.5040 - lr: 1.1111e-04 - 76s/epoch - 386ms/step
Epoch 292/1000
2023-09-27 15:33:04.797 
Epoch 292/1000 
	 loss: 19.3346, MinusLogProbMetric: 19.3346, val_loss: 19.4615, val_MinusLogProbMetric: 19.4615

Epoch 292: val_loss did not improve from 19.42898
196/196 - 76s - loss: 19.3346 - MinusLogProbMetric: 19.3346 - val_loss: 19.4615 - val_MinusLogProbMetric: 19.4615 - lr: 1.1111e-04 - 76s/epoch - 389ms/step
Epoch 293/1000
2023-09-27 15:34:19.997 
Epoch 293/1000 
	 loss: 19.3624, MinusLogProbMetric: 19.3624, val_loss: 19.4456, val_MinusLogProbMetric: 19.4456

Epoch 293: val_loss did not improve from 19.42898
196/196 - 75s - loss: 19.3624 - MinusLogProbMetric: 19.3624 - val_loss: 19.4456 - val_MinusLogProbMetric: 19.4456 - lr: 1.1111e-04 - 75s/epoch - 384ms/step
Epoch 294/1000
2023-09-27 15:35:29.370 
Epoch 294/1000 
	 loss: 19.3448, MinusLogProbMetric: 19.3448, val_loss: 19.4397, val_MinusLogProbMetric: 19.4397

Epoch 294: val_loss did not improve from 19.42898
196/196 - 69s - loss: 19.3448 - MinusLogProbMetric: 19.3448 - val_loss: 19.4397 - val_MinusLogProbMetric: 19.4397 - lr: 1.1111e-04 - 69s/epoch - 354ms/step
Epoch 295/1000
2023-09-27 15:36:44.409 
Epoch 295/1000 
	 loss: 19.3329, MinusLogProbMetric: 19.3329, val_loss: 19.4356, val_MinusLogProbMetric: 19.4356

Epoch 295: val_loss did not improve from 19.42898
196/196 - 75s - loss: 19.3329 - MinusLogProbMetric: 19.3329 - val_loss: 19.4356 - val_MinusLogProbMetric: 19.4356 - lr: 1.1111e-04 - 75s/epoch - 383ms/step
Epoch 296/1000
2023-09-27 15:37:55.089 
Epoch 296/1000 
	 loss: 19.3040, MinusLogProbMetric: 19.3040, val_loss: 19.3825, val_MinusLogProbMetric: 19.3825

Epoch 296: val_loss improved from 19.42898 to 19.38253, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 72s - loss: 19.3040 - MinusLogProbMetric: 19.3040 - val_loss: 19.3825 - val_MinusLogProbMetric: 19.3825 - lr: 1.1111e-04 - 72s/epoch - 367ms/step
Epoch 297/1000
2023-09-27 15:39:11.854 
Epoch 297/1000 
	 loss: 19.2910, MinusLogProbMetric: 19.2910, val_loss: 19.4658, val_MinusLogProbMetric: 19.4658

Epoch 297: val_loss did not improve from 19.38253
196/196 - 76s - loss: 19.2910 - MinusLogProbMetric: 19.2910 - val_loss: 19.4658 - val_MinusLogProbMetric: 19.4658 - lr: 1.1111e-04 - 76s/epoch - 385ms/step
Epoch 298/1000
2023-09-27 15:40:27.058 
Epoch 298/1000 
	 loss: 19.2953, MinusLogProbMetric: 19.2953, val_loss: 19.4730, val_MinusLogProbMetric: 19.4730

Epoch 298: val_loss did not improve from 19.38253
196/196 - 75s - loss: 19.2953 - MinusLogProbMetric: 19.2953 - val_loss: 19.4730 - val_MinusLogProbMetric: 19.4730 - lr: 1.1111e-04 - 75s/epoch - 384ms/step
Epoch 299/1000
2023-09-27 15:41:42.097 
Epoch 299/1000 
	 loss: 19.2945, MinusLogProbMetric: 19.2945, val_loss: 19.4357, val_MinusLogProbMetric: 19.4357

Epoch 299: val_loss did not improve from 19.38253
196/196 - 75s - loss: 19.2945 - MinusLogProbMetric: 19.2945 - val_loss: 19.4357 - val_MinusLogProbMetric: 19.4357 - lr: 1.1111e-04 - 75s/epoch - 383ms/step
Epoch 300/1000
2023-09-27 15:42:57.499 
Epoch 300/1000 
	 loss: 19.2541, MinusLogProbMetric: 19.2541, val_loss: 19.4735, val_MinusLogProbMetric: 19.4735

Epoch 300: val_loss did not improve from 19.38253
196/196 - 75s - loss: 19.2541 - MinusLogProbMetric: 19.2541 - val_loss: 19.4735 - val_MinusLogProbMetric: 19.4735 - lr: 1.1111e-04 - 75s/epoch - 385ms/step
Epoch 301/1000
2023-09-27 15:44:14.174 
Epoch 301/1000 
	 loss: 19.2602, MinusLogProbMetric: 19.2602, val_loss: 19.3016, val_MinusLogProbMetric: 19.3016

Epoch 301: val_loss improved from 19.38253 to 19.30162, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 78s - loss: 19.2602 - MinusLogProbMetric: 19.2602 - val_loss: 19.3016 - val_MinusLogProbMetric: 19.3016 - lr: 1.1111e-04 - 78s/epoch - 400ms/step
Epoch 302/1000
2023-09-27 15:45:32.098 
Epoch 302/1000 
	 loss: 19.2575, MinusLogProbMetric: 19.2575, val_loss: 19.4053, val_MinusLogProbMetric: 19.4053

Epoch 302: val_loss did not improve from 19.30162
196/196 - 76s - loss: 19.2575 - MinusLogProbMetric: 19.2575 - val_loss: 19.4053 - val_MinusLogProbMetric: 19.4053 - lr: 1.1111e-04 - 76s/epoch - 389ms/step
Epoch 303/1000
2023-09-27 15:46:48.190 
Epoch 303/1000 
	 loss: 19.2560, MinusLogProbMetric: 19.2560, val_loss: 19.3375, val_MinusLogProbMetric: 19.3375

Epoch 303: val_loss did not improve from 19.30162
196/196 - 76s - loss: 19.2560 - MinusLogProbMetric: 19.2560 - val_loss: 19.3375 - val_MinusLogProbMetric: 19.3375 - lr: 1.1111e-04 - 76s/epoch - 388ms/step
Epoch 304/1000
2023-09-27 15:48:05.202 
Epoch 304/1000 
	 loss: 19.2288, MinusLogProbMetric: 19.2288, val_loss: 19.3621, val_MinusLogProbMetric: 19.3621

Epoch 304: val_loss did not improve from 19.30162
196/196 - 77s - loss: 19.2288 - MinusLogProbMetric: 19.2288 - val_loss: 19.3621 - val_MinusLogProbMetric: 19.3621 - lr: 1.1111e-04 - 77s/epoch - 393ms/step
Epoch 305/1000
2023-09-27 15:49:13.010 
Epoch 305/1000 
	 loss: 19.2467, MinusLogProbMetric: 19.2467, val_loss: 19.3705, val_MinusLogProbMetric: 19.3705

Epoch 305: val_loss did not improve from 19.30162
196/196 - 68s - loss: 19.2467 - MinusLogProbMetric: 19.2467 - val_loss: 19.3705 - val_MinusLogProbMetric: 19.3705 - lr: 1.1111e-04 - 68s/epoch - 346ms/step
Epoch 306/1000
2023-09-27 15:50:28.786 
Epoch 306/1000 
	 loss: 19.2209, MinusLogProbMetric: 19.2209, val_loss: 19.3614, val_MinusLogProbMetric: 19.3614

Epoch 306: val_loss did not improve from 19.30162
196/196 - 76s - loss: 19.2209 - MinusLogProbMetric: 19.2209 - val_loss: 19.3614 - val_MinusLogProbMetric: 19.3614 - lr: 1.1111e-04 - 76s/epoch - 387ms/step
Epoch 307/1000
2023-09-27 15:51:37.358 
Epoch 307/1000 
	 loss: 19.2191, MinusLogProbMetric: 19.2191, val_loss: 19.3310, val_MinusLogProbMetric: 19.3310

Epoch 307: val_loss did not improve from 19.30162
196/196 - 69s - loss: 19.2191 - MinusLogProbMetric: 19.2191 - val_loss: 19.3310 - val_MinusLogProbMetric: 19.3310 - lr: 1.1111e-04 - 69s/epoch - 350ms/step
Epoch 308/1000
2023-09-27 15:52:54.470 
Epoch 308/1000 
	 loss: 19.2051, MinusLogProbMetric: 19.2051, val_loss: 19.2339, val_MinusLogProbMetric: 19.2339

Epoch 308: val_loss improved from 19.30162 to 19.23390, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 79s - loss: 19.2051 - MinusLogProbMetric: 19.2051 - val_loss: 19.2339 - val_MinusLogProbMetric: 19.2339 - lr: 1.1111e-04 - 79s/epoch - 402ms/step
Epoch 309/1000
2023-09-27 15:54:12.445 
Epoch 309/1000 
	 loss: 19.1844, MinusLogProbMetric: 19.1844, val_loss: 19.2526, val_MinusLogProbMetric: 19.2526

Epoch 309: val_loss did not improve from 19.23390
196/196 - 76s - loss: 19.1844 - MinusLogProbMetric: 19.1844 - val_loss: 19.2526 - val_MinusLogProbMetric: 19.2526 - lr: 1.1111e-04 - 76s/epoch - 390ms/step
Epoch 310/1000
2023-09-27 15:55:30.380 
Epoch 310/1000 
	 loss: 19.1725, MinusLogProbMetric: 19.1725, val_loss: 19.3750, val_MinusLogProbMetric: 19.3750

Epoch 310: val_loss did not improve from 19.23390
196/196 - 78s - loss: 19.1725 - MinusLogProbMetric: 19.1725 - val_loss: 19.3750 - val_MinusLogProbMetric: 19.3750 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 311/1000
2023-09-27 15:56:46.949 
Epoch 311/1000 
	 loss: 19.1970, MinusLogProbMetric: 19.1970, val_loss: 19.4160, val_MinusLogProbMetric: 19.4160

Epoch 311: val_loss did not improve from 19.23390
196/196 - 77s - loss: 19.1970 - MinusLogProbMetric: 19.1970 - val_loss: 19.4160 - val_MinusLogProbMetric: 19.4160 - lr: 1.1111e-04 - 77s/epoch - 391ms/step
Epoch 312/1000
2023-09-27 15:58:05.013 
Epoch 312/1000 
	 loss: 19.1759, MinusLogProbMetric: 19.1759, val_loss: 19.2615, val_MinusLogProbMetric: 19.2615

Epoch 312: val_loss did not improve from 19.23390
196/196 - 78s - loss: 19.1759 - MinusLogProbMetric: 19.1759 - val_loss: 19.2615 - val_MinusLogProbMetric: 19.2615 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 313/1000
2023-09-27 15:59:21.440 
Epoch 313/1000 
	 loss: 19.1573, MinusLogProbMetric: 19.1573, val_loss: 19.2285, val_MinusLogProbMetric: 19.2285

Epoch 313: val_loss improved from 19.23390 to 19.22850, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 78s - loss: 19.1573 - MinusLogProbMetric: 19.1573 - val_loss: 19.2285 - val_MinusLogProbMetric: 19.2285 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 314/1000
2023-09-27 16:00:39.005 
Epoch 314/1000 
	 loss: 19.1440, MinusLogProbMetric: 19.1440, val_loss: 19.1614, val_MinusLogProbMetric: 19.1614

Epoch 314: val_loss improved from 19.22850 to 19.16139, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 77s - loss: 19.1440 - MinusLogProbMetric: 19.1440 - val_loss: 19.1614 - val_MinusLogProbMetric: 19.1614 - lr: 1.1111e-04 - 77s/epoch - 394ms/step
Epoch 315/1000
2023-09-27 16:01:57.015 
Epoch 315/1000 
	 loss: 19.1309, MinusLogProbMetric: 19.1309, val_loss: 19.1744, val_MinusLogProbMetric: 19.1744

Epoch 315: val_loss did not improve from 19.16139
196/196 - 77s - loss: 19.1309 - MinusLogProbMetric: 19.1309 - val_loss: 19.1744 - val_MinusLogProbMetric: 19.1744 - lr: 1.1111e-04 - 77s/epoch - 392ms/step
Epoch 316/1000
2023-09-27 16:03:13.853 
Epoch 316/1000 
	 loss: 19.1153, MinusLogProbMetric: 19.1153, val_loss: 19.1466, val_MinusLogProbMetric: 19.1466

Epoch 316: val_loss improved from 19.16139 to 19.14660, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 78s - loss: 19.1153 - MinusLogProbMetric: 19.1153 - val_loss: 19.1466 - val_MinusLogProbMetric: 19.1466 - lr: 1.1111e-04 - 78s/epoch - 399ms/step
Epoch 317/1000
2023-09-27 16:04:31.420 
Epoch 317/1000 
	 loss: 19.1335, MinusLogProbMetric: 19.1335, val_loss: 19.2173, val_MinusLogProbMetric: 19.2173

Epoch 317: val_loss did not improve from 19.14660
196/196 - 76s - loss: 19.1335 - MinusLogProbMetric: 19.1335 - val_loss: 19.2173 - val_MinusLogProbMetric: 19.2173 - lr: 1.1111e-04 - 76s/epoch - 389ms/step
Epoch 318/1000
2023-09-27 16:05:47.258 
Epoch 318/1000 
	 loss: 19.1162, MinusLogProbMetric: 19.1162, val_loss: 19.1739, val_MinusLogProbMetric: 19.1739

Epoch 318: val_loss did not improve from 19.14660
196/196 - 76s - loss: 19.1162 - MinusLogProbMetric: 19.1162 - val_loss: 19.1739 - val_MinusLogProbMetric: 19.1739 - lr: 1.1111e-04 - 76s/epoch - 387ms/step
Epoch 319/1000
2023-09-27 16:07:04.232 
Epoch 319/1000 
	 loss: 19.0930, MinusLogProbMetric: 19.0930, val_loss: 19.2982, val_MinusLogProbMetric: 19.2982

Epoch 319: val_loss did not improve from 19.14660
196/196 - 77s - loss: 19.0930 - MinusLogProbMetric: 19.0930 - val_loss: 19.2982 - val_MinusLogProbMetric: 19.2982 - lr: 1.1111e-04 - 77s/epoch - 393ms/step
Epoch 320/1000
2023-09-27 16:08:19.170 
Epoch 320/1000 
	 loss: 19.1026, MinusLogProbMetric: 19.1026, val_loss: 19.2591, val_MinusLogProbMetric: 19.2591

Epoch 320: val_loss did not improve from 19.14660
196/196 - 75s - loss: 19.1026 - MinusLogProbMetric: 19.1026 - val_loss: 19.2591 - val_MinusLogProbMetric: 19.2591 - lr: 1.1111e-04 - 75s/epoch - 382ms/step
Epoch 321/1000
2023-09-27 16:09:33.879 
Epoch 321/1000 
	 loss: 19.0887, MinusLogProbMetric: 19.0887, val_loss: 19.3087, val_MinusLogProbMetric: 19.3087

Epoch 321: val_loss did not improve from 19.14660
196/196 - 75s - loss: 19.0887 - MinusLogProbMetric: 19.0887 - val_loss: 19.3087 - val_MinusLogProbMetric: 19.3087 - lr: 1.1111e-04 - 75s/epoch - 381ms/step
Epoch 322/1000
2023-09-27 16:10:50.248 
Epoch 322/1000 
	 loss: 19.1080, MinusLogProbMetric: 19.1080, val_loss: 19.1959, val_MinusLogProbMetric: 19.1959

Epoch 322: val_loss did not improve from 19.14660
196/196 - 76s - loss: 19.1080 - MinusLogProbMetric: 19.1080 - val_loss: 19.1959 - val_MinusLogProbMetric: 19.1959 - lr: 1.1111e-04 - 76s/epoch - 390ms/step
Epoch 323/1000
2023-09-27 16:12:07.245 
Epoch 323/1000 
	 loss: 19.0465, MinusLogProbMetric: 19.0465, val_loss: 19.1319, val_MinusLogProbMetric: 19.1319

Epoch 323: val_loss improved from 19.14660 to 19.13193, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 79s - loss: 19.0465 - MinusLogProbMetric: 19.0465 - val_loss: 19.1319 - val_MinusLogProbMetric: 19.1319 - lr: 1.1111e-04 - 79s/epoch - 401ms/step
Epoch 324/1000
2023-09-27 16:13:24.203 
Epoch 324/1000 
	 loss: 19.0599, MinusLogProbMetric: 19.0599, val_loss: 19.3017, val_MinusLogProbMetric: 19.3017

Epoch 324: val_loss did not improve from 19.13193
196/196 - 75s - loss: 19.0599 - MinusLogProbMetric: 19.0599 - val_loss: 19.3017 - val_MinusLogProbMetric: 19.3017 - lr: 1.1111e-04 - 75s/epoch - 385ms/step
Epoch 325/1000
2023-09-27 16:14:38.353 
Epoch 325/1000 
	 loss: 19.0638, MinusLogProbMetric: 19.0638, val_loss: 19.2687, val_MinusLogProbMetric: 19.2687

Epoch 325: val_loss did not improve from 19.13193
196/196 - 74s - loss: 19.0638 - MinusLogProbMetric: 19.0638 - val_loss: 19.2687 - val_MinusLogProbMetric: 19.2687 - lr: 1.1111e-04 - 74s/epoch - 378ms/step
Epoch 326/1000
2023-09-27 16:15:53.849 
Epoch 326/1000 
	 loss: 19.0534, MinusLogProbMetric: 19.0534, val_loss: 19.2447, val_MinusLogProbMetric: 19.2447

Epoch 326: val_loss did not improve from 19.13193
196/196 - 75s - loss: 19.0534 - MinusLogProbMetric: 19.0534 - val_loss: 19.2447 - val_MinusLogProbMetric: 19.2447 - lr: 1.1111e-04 - 75s/epoch - 385ms/step
Epoch 327/1000
2023-09-27 16:17:08.988 
Epoch 327/1000 
	 loss: 19.0908, MinusLogProbMetric: 19.0908, val_loss: 19.1065, val_MinusLogProbMetric: 19.1065

Epoch 327: val_loss improved from 19.13193 to 19.10650, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 77s - loss: 19.0908 - MinusLogProbMetric: 19.0908 - val_loss: 19.1065 - val_MinusLogProbMetric: 19.1065 - lr: 1.1111e-04 - 77s/epoch - 391ms/step
Epoch 328/1000
2023-09-27 16:18:27.729 
Epoch 328/1000 
	 loss: 19.0273, MinusLogProbMetric: 19.0273, val_loss: 19.0567, val_MinusLogProbMetric: 19.0567

Epoch 328: val_loss improved from 19.10650 to 19.05668, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 79s - loss: 19.0273 - MinusLogProbMetric: 19.0273 - val_loss: 19.0567 - val_MinusLogProbMetric: 19.0567 - lr: 1.1111e-04 - 79s/epoch - 402ms/step
Epoch 329/1000
2023-09-27 16:19:43.784 
Epoch 329/1000 
	 loss: 19.0327, MinusLogProbMetric: 19.0327, val_loss: 19.2404, val_MinusLogProbMetric: 19.2404

Epoch 329: val_loss did not improve from 19.05668
196/196 - 75s - loss: 19.0327 - MinusLogProbMetric: 19.0327 - val_loss: 19.2404 - val_MinusLogProbMetric: 19.2404 - lr: 1.1111e-04 - 75s/epoch - 380ms/step
Epoch 330/1000
2023-09-27 16:21:02.141 
Epoch 330/1000 
	 loss: 19.0112, MinusLogProbMetric: 19.0112, val_loss: 19.1597, val_MinusLogProbMetric: 19.1597

Epoch 330: val_loss did not improve from 19.05668
196/196 - 78s - loss: 19.0112 - MinusLogProbMetric: 19.0112 - val_loss: 19.1597 - val_MinusLogProbMetric: 19.1597 - lr: 1.1111e-04 - 78s/epoch - 400ms/step
Epoch 331/1000
2023-09-27 16:22:19.079 
Epoch 331/1000 
	 loss: 19.0091, MinusLogProbMetric: 19.0091, val_loss: 19.0302, val_MinusLogProbMetric: 19.0302

Epoch 331: val_loss improved from 19.05668 to 19.03019, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 78s - loss: 19.0091 - MinusLogProbMetric: 19.0091 - val_loss: 19.0302 - val_MinusLogProbMetric: 19.0302 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 332/1000
2023-09-27 16:23:33.954 
Epoch 332/1000 
	 loss: 19.0269, MinusLogProbMetric: 19.0269, val_loss: 19.1455, val_MinusLogProbMetric: 19.1455

Epoch 332: val_loss did not improve from 19.03019
196/196 - 74s - loss: 19.0269 - MinusLogProbMetric: 19.0269 - val_loss: 19.1455 - val_MinusLogProbMetric: 19.1455 - lr: 1.1111e-04 - 74s/epoch - 376ms/step
Epoch 333/1000
2023-09-27 16:24:49.079 
Epoch 333/1000 
	 loss: 18.9950, MinusLogProbMetric: 18.9950, val_loss: 19.0513, val_MinusLogProbMetric: 19.0513

Epoch 333: val_loss did not improve from 19.03019
196/196 - 75s - loss: 18.9950 - MinusLogProbMetric: 18.9950 - val_loss: 19.0513 - val_MinusLogProbMetric: 19.0513 - lr: 1.1111e-04 - 75s/epoch - 383ms/step
Epoch 334/1000
2023-09-27 16:26:04.645 
Epoch 334/1000 
	 loss: 19.0043, MinusLogProbMetric: 19.0043, val_loss: 19.0640, val_MinusLogProbMetric: 19.0640

Epoch 334: val_loss did not improve from 19.03019
196/196 - 76s - loss: 19.0043 - MinusLogProbMetric: 19.0043 - val_loss: 19.0640 - val_MinusLogProbMetric: 19.0640 - lr: 1.1111e-04 - 76s/epoch - 385ms/step
Epoch 335/1000
2023-09-27 16:27:19.275 
Epoch 335/1000 
	 loss: 18.9787, MinusLogProbMetric: 18.9787, val_loss: 19.0212, val_MinusLogProbMetric: 19.0212

Epoch 335: val_loss improved from 19.03019 to 19.02121, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 76s - loss: 18.9787 - MinusLogProbMetric: 18.9787 - val_loss: 19.0212 - val_MinusLogProbMetric: 19.0212 - lr: 1.1111e-04 - 76s/epoch - 387ms/step
Epoch 336/1000
2023-09-27 16:28:35.949 
Epoch 336/1000 
	 loss: 18.9877, MinusLogProbMetric: 18.9877, val_loss: 19.1352, val_MinusLogProbMetric: 19.1352

Epoch 336: val_loss did not improve from 19.02121
196/196 - 75s - loss: 18.9877 - MinusLogProbMetric: 18.9877 - val_loss: 19.1352 - val_MinusLogProbMetric: 19.1352 - lr: 1.1111e-04 - 75s/epoch - 384ms/step
Epoch 337/1000
2023-09-27 16:29:53.726 
Epoch 337/1000 
	 loss: 19.0137, MinusLogProbMetric: 19.0137, val_loss: 19.1129, val_MinusLogProbMetric: 19.1129

Epoch 337: val_loss did not improve from 19.02121
196/196 - 78s - loss: 19.0137 - MinusLogProbMetric: 19.0137 - val_loss: 19.1129 - val_MinusLogProbMetric: 19.1129 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 338/1000
2023-09-27 16:31:08.470 
Epoch 338/1000 
	 loss: 18.9712, MinusLogProbMetric: 18.9712, val_loss: 19.2057, val_MinusLogProbMetric: 19.2057

Epoch 338: val_loss did not improve from 19.02121
196/196 - 75s - loss: 18.9712 - MinusLogProbMetric: 18.9712 - val_loss: 19.2057 - val_MinusLogProbMetric: 19.2057 - lr: 1.1111e-04 - 75s/epoch - 381ms/step
Epoch 339/1000
2023-09-27 16:32:21.653 
Epoch 339/1000 
	 loss: 18.9775, MinusLogProbMetric: 18.9775, val_loss: 19.1066, val_MinusLogProbMetric: 19.1066

Epoch 339: val_loss did not improve from 19.02121
196/196 - 73s - loss: 18.9775 - MinusLogProbMetric: 18.9775 - val_loss: 19.1066 - val_MinusLogProbMetric: 19.1066 - lr: 1.1111e-04 - 73s/epoch - 373ms/step
Epoch 340/1000
2023-09-27 16:33:37.290 
Epoch 340/1000 
	 loss: 18.9823, MinusLogProbMetric: 18.9823, val_loss: 19.0067, val_MinusLogProbMetric: 19.0067

Epoch 340: val_loss improved from 19.02121 to 19.00670, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 77s - loss: 18.9823 - MinusLogProbMetric: 18.9823 - val_loss: 19.0067 - val_MinusLogProbMetric: 19.0067 - lr: 1.1111e-04 - 77s/epoch - 392ms/step
Epoch 341/1000
2023-09-27 16:34:56.551 
Epoch 341/1000 
	 loss: 18.9546, MinusLogProbMetric: 18.9546, val_loss: 19.0955, val_MinusLogProbMetric: 19.0955

Epoch 341: val_loss did not improve from 19.00670
196/196 - 78s - loss: 18.9546 - MinusLogProbMetric: 18.9546 - val_loss: 19.0955 - val_MinusLogProbMetric: 19.0955 - lr: 1.1111e-04 - 78s/epoch - 399ms/step
Epoch 342/1000
2023-09-27 16:36:14.625 
Epoch 342/1000 
	 loss: 18.9808, MinusLogProbMetric: 18.9808, val_loss: 19.0499, val_MinusLogProbMetric: 19.0499

Epoch 342: val_loss did not improve from 19.00670
196/196 - 78s - loss: 18.9808 - MinusLogProbMetric: 18.9808 - val_loss: 19.0499 - val_MinusLogProbMetric: 19.0499 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 343/1000
2023-09-27 16:37:31.408 
Epoch 343/1000 
	 loss: 18.9357, MinusLogProbMetric: 18.9357, val_loss: 19.0415, val_MinusLogProbMetric: 19.0415

Epoch 343: val_loss did not improve from 19.00670
196/196 - 77s - loss: 18.9357 - MinusLogProbMetric: 18.9357 - val_loss: 19.0415 - val_MinusLogProbMetric: 19.0415 - lr: 1.1111e-04 - 77s/epoch - 392ms/step
Epoch 344/1000
2023-09-27 16:38:47.906 
Epoch 344/1000 
	 loss: 18.9321, MinusLogProbMetric: 18.9321, val_loss: 19.0006, val_MinusLogProbMetric: 19.0006

Epoch 344: val_loss improved from 19.00670 to 19.00063, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 78s - loss: 18.9321 - MinusLogProbMetric: 18.9321 - val_loss: 19.0006 - val_MinusLogProbMetric: 19.0006 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 345/1000
2023-09-27 16:40:05.504 
Epoch 345/1000 
	 loss: 18.9268, MinusLogProbMetric: 18.9268, val_loss: 19.1160, val_MinusLogProbMetric: 19.1160

Epoch 345: val_loss did not improve from 19.00063
196/196 - 76s - loss: 18.9268 - MinusLogProbMetric: 18.9268 - val_loss: 19.1160 - val_MinusLogProbMetric: 19.1160 - lr: 1.1111e-04 - 76s/epoch - 388ms/step
Epoch 346/1000
2023-09-27 16:41:23.770 
Epoch 346/1000 
	 loss: 18.9270, MinusLogProbMetric: 18.9270, val_loss: 19.0230, val_MinusLogProbMetric: 19.0230

Epoch 346: val_loss did not improve from 19.00063
196/196 - 78s - loss: 18.9270 - MinusLogProbMetric: 18.9270 - val_loss: 19.0230 - val_MinusLogProbMetric: 19.0230 - lr: 1.1111e-04 - 78s/epoch - 399ms/step
Epoch 347/1000
2023-09-27 16:42:39.767 
Epoch 347/1000 
	 loss: 18.9088, MinusLogProbMetric: 18.9088, val_loss: 19.0529, val_MinusLogProbMetric: 19.0529

Epoch 347: val_loss did not improve from 19.00063
196/196 - 76s - loss: 18.9088 - MinusLogProbMetric: 18.9088 - val_loss: 19.0529 - val_MinusLogProbMetric: 19.0529 - lr: 1.1111e-04 - 76s/epoch - 388ms/step
Epoch 348/1000
2023-09-27 16:43:55.982 
Epoch 348/1000 
	 loss: 18.8796, MinusLogProbMetric: 18.8796, val_loss: 19.0853, val_MinusLogProbMetric: 19.0853

Epoch 348: val_loss did not improve from 19.00063
196/196 - 76s - loss: 18.8796 - MinusLogProbMetric: 18.8796 - val_loss: 19.0853 - val_MinusLogProbMetric: 19.0853 - lr: 1.1111e-04 - 76s/epoch - 389ms/step
Epoch 349/1000
2023-09-27 16:45:13.097 
Epoch 349/1000 
	 loss: 18.9055, MinusLogProbMetric: 18.9055, val_loss: 19.0879, val_MinusLogProbMetric: 19.0879

Epoch 349: val_loss did not improve from 19.00063
196/196 - 77s - loss: 18.9055 - MinusLogProbMetric: 18.9055 - val_loss: 19.0879 - val_MinusLogProbMetric: 19.0879 - lr: 1.1111e-04 - 77s/epoch - 393ms/step
Epoch 350/1000
2023-09-27 16:46:29.629 
Epoch 350/1000 
	 loss: 18.8805, MinusLogProbMetric: 18.8805, val_loss: 18.9328, val_MinusLogProbMetric: 18.9328

Epoch 350: val_loss improved from 19.00063 to 18.93277, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 78s - loss: 18.8805 - MinusLogProbMetric: 18.8805 - val_loss: 18.9328 - val_MinusLogProbMetric: 18.9328 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 351/1000
2023-09-27 16:47:47.526 
Epoch 351/1000 
	 loss: 18.8826, MinusLogProbMetric: 18.8826, val_loss: 18.9494, val_MinusLogProbMetric: 18.9494

Epoch 351: val_loss did not improve from 18.93277
196/196 - 77s - loss: 18.8826 - MinusLogProbMetric: 18.8826 - val_loss: 18.9494 - val_MinusLogProbMetric: 18.9494 - lr: 1.1111e-04 - 77s/epoch - 391ms/step
Epoch 352/1000
2023-09-27 16:49:04.017 
Epoch 352/1000 
	 loss: 18.9035, MinusLogProbMetric: 18.9035, val_loss: 18.9020, val_MinusLogProbMetric: 18.9020

Epoch 352: val_loss improved from 18.93277 to 18.90199, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 78s - loss: 18.9035 - MinusLogProbMetric: 18.9035 - val_loss: 18.9020 - val_MinusLogProbMetric: 18.9020 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 353/1000
2023-09-27 16:50:22.547 
Epoch 353/1000 
	 loss: 18.8539, MinusLogProbMetric: 18.8539, val_loss: 18.9531, val_MinusLogProbMetric: 18.9531

Epoch 353: val_loss did not improve from 18.90199
196/196 - 77s - loss: 18.8539 - MinusLogProbMetric: 18.8539 - val_loss: 18.9531 - val_MinusLogProbMetric: 18.9531 - lr: 1.1111e-04 - 77s/epoch - 393ms/step
Epoch 354/1000
2023-09-27 16:51:39.294 
Epoch 354/1000 
	 loss: 18.9033, MinusLogProbMetric: 18.9033, val_loss: 18.9682, val_MinusLogProbMetric: 18.9682

Epoch 354: val_loss did not improve from 18.90199
196/196 - 77s - loss: 18.9033 - MinusLogProbMetric: 18.9033 - val_loss: 18.9682 - val_MinusLogProbMetric: 18.9682 - lr: 1.1111e-04 - 77s/epoch - 392ms/step
Epoch 355/1000
2023-09-27 16:52:57.094 
Epoch 355/1000 
	 loss: 18.8992, MinusLogProbMetric: 18.8992, val_loss: 19.2265, val_MinusLogProbMetric: 19.2265

Epoch 355: val_loss did not improve from 18.90199
196/196 - 78s - loss: 18.8992 - MinusLogProbMetric: 18.8992 - val_loss: 19.2265 - val_MinusLogProbMetric: 19.2265 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 356/1000
2023-09-27 16:54:14.418 
Epoch 356/1000 
	 loss: 18.8505, MinusLogProbMetric: 18.8505, val_loss: 18.9969, val_MinusLogProbMetric: 18.9969

Epoch 356: val_loss did not improve from 18.90199
196/196 - 77s - loss: 18.8505 - MinusLogProbMetric: 18.8505 - val_loss: 18.9969 - val_MinusLogProbMetric: 18.9969 - lr: 1.1111e-04 - 77s/epoch - 394ms/step
Epoch 357/1000
2023-09-27 16:55:31.953 
Epoch 357/1000 
	 loss: 18.8732, MinusLogProbMetric: 18.8732, val_loss: 19.0294, val_MinusLogProbMetric: 19.0294

Epoch 357: val_loss did not improve from 18.90199
196/196 - 78s - loss: 18.8732 - MinusLogProbMetric: 18.8732 - val_loss: 19.0294 - val_MinusLogProbMetric: 19.0294 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 358/1000
2023-09-27 16:56:47.637 
Epoch 358/1000 
	 loss: 18.8395, MinusLogProbMetric: 18.8395, val_loss: 18.9443, val_MinusLogProbMetric: 18.9443

Epoch 358: val_loss did not improve from 18.90199
196/196 - 76s - loss: 18.8395 - MinusLogProbMetric: 18.8395 - val_loss: 18.9443 - val_MinusLogProbMetric: 18.9443 - lr: 1.1111e-04 - 76s/epoch - 386ms/step
Epoch 359/1000
2023-09-27 16:58:05.990 
Epoch 359/1000 
	 loss: 18.8205, MinusLogProbMetric: 18.8205, val_loss: 18.8914, val_MinusLogProbMetric: 18.8914

Epoch 359: val_loss improved from 18.90199 to 18.89141, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 80s - loss: 18.8205 - MinusLogProbMetric: 18.8205 - val_loss: 18.8914 - val_MinusLogProbMetric: 18.8914 - lr: 1.1111e-04 - 80s/epoch - 407ms/step
Epoch 360/1000
2023-09-27 16:59:25.117 
Epoch 360/1000 
	 loss: 18.8029, MinusLogProbMetric: 18.8029, val_loss: 19.0070, val_MinusLogProbMetric: 19.0070

Epoch 360: val_loss did not improve from 18.89141
196/196 - 78s - loss: 18.8029 - MinusLogProbMetric: 18.8029 - val_loss: 19.0070 - val_MinusLogProbMetric: 19.0070 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 361/1000
2023-09-27 17:00:43.073 
Epoch 361/1000 
	 loss: 18.8035, MinusLogProbMetric: 18.8035, val_loss: 18.9606, val_MinusLogProbMetric: 18.9606

Epoch 361: val_loss did not improve from 18.89141
196/196 - 78s - loss: 18.8035 - MinusLogProbMetric: 18.8035 - val_loss: 18.9606 - val_MinusLogProbMetric: 18.9606 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 362/1000
2023-09-27 17:02:00.715 
Epoch 362/1000 
	 loss: 18.8155, MinusLogProbMetric: 18.8155, val_loss: 18.9032, val_MinusLogProbMetric: 18.9032

Epoch 362: val_loss did not improve from 18.89141
196/196 - 78s - loss: 18.8155 - MinusLogProbMetric: 18.8155 - val_loss: 18.9032 - val_MinusLogProbMetric: 18.9032 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 363/1000
2023-09-27 17:03:17.224 
Epoch 363/1000 
	 loss: 18.7926, MinusLogProbMetric: 18.7926, val_loss: 18.9214, val_MinusLogProbMetric: 18.9214

Epoch 363: val_loss did not improve from 18.89141
196/196 - 77s - loss: 18.7926 - MinusLogProbMetric: 18.7926 - val_loss: 18.9214 - val_MinusLogProbMetric: 18.9214 - lr: 1.1111e-04 - 77s/epoch - 390ms/step
Epoch 364/1000
2023-09-27 17:04:35.522 
Epoch 364/1000 
	 loss: 18.8286, MinusLogProbMetric: 18.8286, val_loss: 18.9006, val_MinusLogProbMetric: 18.9006

Epoch 364: val_loss did not improve from 18.89141
196/196 - 78s - loss: 18.8286 - MinusLogProbMetric: 18.8286 - val_loss: 18.9006 - val_MinusLogProbMetric: 18.9006 - lr: 1.1111e-04 - 78s/epoch - 399ms/step
Epoch 365/1000
2023-09-27 17:05:52.408 
Epoch 365/1000 
	 loss: 18.7736, MinusLogProbMetric: 18.7736, val_loss: 18.8653, val_MinusLogProbMetric: 18.8653

Epoch 365: val_loss improved from 18.89141 to 18.86531, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 78s - loss: 18.7736 - MinusLogProbMetric: 18.7736 - val_loss: 18.8653 - val_MinusLogProbMetric: 18.8653 - lr: 1.1111e-04 - 78s/epoch - 400ms/step
Epoch 366/1000
2023-09-27 17:07:10.239 
Epoch 366/1000 
	 loss: 18.7927, MinusLogProbMetric: 18.7927, val_loss: 18.8027, val_MinusLogProbMetric: 18.8027

Epoch 366: val_loss improved from 18.86531 to 18.80267, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 78s - loss: 18.7927 - MinusLogProbMetric: 18.7927 - val_loss: 18.8027 - val_MinusLogProbMetric: 18.8027 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 367/1000
2023-09-27 17:08:20.643 
Epoch 367/1000 
	 loss: 18.7753, MinusLogProbMetric: 18.7753, val_loss: 19.0062, val_MinusLogProbMetric: 19.0062

Epoch 367: val_loss did not improve from 18.80267
196/196 - 69s - loss: 18.7753 - MinusLogProbMetric: 18.7753 - val_loss: 19.0062 - val_MinusLogProbMetric: 19.0062 - lr: 1.1111e-04 - 69s/epoch - 352ms/step
Epoch 368/1000
2023-09-27 17:09:36.176 
Epoch 368/1000 
	 loss: 18.7899, MinusLogProbMetric: 18.7899, val_loss: 19.2546, val_MinusLogProbMetric: 19.2546

Epoch 368: val_loss did not improve from 18.80267
196/196 - 76s - loss: 18.7899 - MinusLogProbMetric: 18.7899 - val_loss: 19.2546 - val_MinusLogProbMetric: 19.2546 - lr: 1.1111e-04 - 76s/epoch - 385ms/step
Epoch 369/1000
2023-09-27 17:10:48.852 
Epoch 369/1000 
	 loss: 18.7506, MinusLogProbMetric: 18.7506, val_loss: 19.1015, val_MinusLogProbMetric: 19.1015

Epoch 369: val_loss did not improve from 18.80267
196/196 - 73s - loss: 18.7506 - MinusLogProbMetric: 18.7506 - val_loss: 19.1015 - val_MinusLogProbMetric: 19.1015 - lr: 1.1111e-04 - 73s/epoch - 371ms/step
Epoch 370/1000
2023-09-27 17:12:01.582 
Epoch 370/1000 
	 loss: 18.7715, MinusLogProbMetric: 18.7715, val_loss: 19.0822, val_MinusLogProbMetric: 19.0822

Epoch 370: val_loss did not improve from 18.80267
196/196 - 73s - loss: 18.7715 - MinusLogProbMetric: 18.7715 - val_loss: 19.0822 - val_MinusLogProbMetric: 19.0822 - lr: 1.1111e-04 - 73s/epoch - 371ms/step
Epoch 371/1000
2023-09-27 17:13:20.610 
Epoch 371/1000 
	 loss: 18.7700, MinusLogProbMetric: 18.7700, val_loss: 18.8607, val_MinusLogProbMetric: 18.8607

Epoch 371: val_loss did not improve from 18.80267
196/196 - 79s - loss: 18.7700 - MinusLogProbMetric: 18.7700 - val_loss: 18.8607 - val_MinusLogProbMetric: 18.8607 - lr: 1.1111e-04 - 79s/epoch - 403ms/step
Epoch 372/1000
2023-09-27 17:14:37.649 
Epoch 372/1000 
	 loss: 18.7626, MinusLogProbMetric: 18.7626, val_loss: 18.8686, val_MinusLogProbMetric: 18.8686

Epoch 372: val_loss did not improve from 18.80267
196/196 - 77s - loss: 18.7626 - MinusLogProbMetric: 18.7626 - val_loss: 18.8686 - val_MinusLogProbMetric: 18.8686 - lr: 1.1111e-04 - 77s/epoch - 393ms/step
Epoch 373/1000
2023-09-27 17:15:55.443 
Epoch 373/1000 
	 loss: 18.7437, MinusLogProbMetric: 18.7437, val_loss: 18.8636, val_MinusLogProbMetric: 18.8636

Epoch 373: val_loss did not improve from 18.80267
196/196 - 78s - loss: 18.7437 - MinusLogProbMetric: 18.7437 - val_loss: 18.8636 - val_MinusLogProbMetric: 18.8636 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 374/1000
2023-09-27 17:17:12.848 
Epoch 374/1000 
	 loss: 18.7515, MinusLogProbMetric: 18.7515, val_loss: 18.8433, val_MinusLogProbMetric: 18.8433

Epoch 374: val_loss did not improve from 18.80267
196/196 - 77s - loss: 18.7515 - MinusLogProbMetric: 18.7515 - val_loss: 18.8433 - val_MinusLogProbMetric: 18.8433 - lr: 1.1111e-04 - 77s/epoch - 395ms/step
Epoch 375/1000
2023-09-27 17:18:29.322 
Epoch 375/1000 
	 loss: 18.7496, MinusLogProbMetric: 18.7496, val_loss: 18.9499, val_MinusLogProbMetric: 18.9499

Epoch 375: val_loss did not improve from 18.80267
196/196 - 76s - loss: 18.7496 - MinusLogProbMetric: 18.7496 - val_loss: 18.9499 - val_MinusLogProbMetric: 18.9499 - lr: 1.1111e-04 - 76s/epoch - 390ms/step
Epoch 376/1000
2023-09-27 17:19:46.272 
Epoch 376/1000 
	 loss: 18.7453, MinusLogProbMetric: 18.7453, val_loss: 18.9623, val_MinusLogProbMetric: 18.9623

Epoch 376: val_loss did not improve from 18.80267
196/196 - 77s - loss: 18.7453 - MinusLogProbMetric: 18.7453 - val_loss: 18.9623 - val_MinusLogProbMetric: 18.9623 - lr: 1.1111e-04 - 77s/epoch - 393ms/step
Epoch 377/1000
2023-09-27 17:21:02.974 
Epoch 377/1000 
	 loss: 18.7473, MinusLogProbMetric: 18.7473, val_loss: 18.8118, val_MinusLogProbMetric: 18.8118

Epoch 377: val_loss did not improve from 18.80267
196/196 - 77s - loss: 18.7473 - MinusLogProbMetric: 18.7473 - val_loss: 18.8118 - val_MinusLogProbMetric: 18.8118 - lr: 1.1111e-04 - 77s/epoch - 391ms/step
Epoch 378/1000
2023-09-27 17:22:21.515 
Epoch 378/1000 
	 loss: 18.7387, MinusLogProbMetric: 18.7387, val_loss: 19.0010, val_MinusLogProbMetric: 19.0010

Epoch 378: val_loss did not improve from 18.80267
196/196 - 79s - loss: 18.7387 - MinusLogProbMetric: 18.7387 - val_loss: 19.0010 - val_MinusLogProbMetric: 19.0010 - lr: 1.1111e-04 - 79s/epoch - 401ms/step
Epoch 379/1000
2023-09-27 17:23:37.842 
Epoch 379/1000 
	 loss: 18.6985, MinusLogProbMetric: 18.6985, val_loss: 18.7962, val_MinusLogProbMetric: 18.7962

Epoch 379: val_loss improved from 18.80267 to 18.79622, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 78s - loss: 18.6985 - MinusLogProbMetric: 18.6985 - val_loss: 18.7962 - val_MinusLogProbMetric: 18.7962 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 380/1000
2023-09-27 17:24:55.954 
Epoch 380/1000 
	 loss: 18.7320, MinusLogProbMetric: 18.7320, val_loss: 18.9278, val_MinusLogProbMetric: 18.9278

Epoch 380: val_loss did not improve from 18.79622
196/196 - 77s - loss: 18.7320 - MinusLogProbMetric: 18.7320 - val_loss: 18.9278 - val_MinusLogProbMetric: 18.9278 - lr: 1.1111e-04 - 77s/epoch - 392ms/step
Epoch 381/1000
2023-09-27 17:26:12.978 
Epoch 381/1000 
	 loss: 18.7416, MinusLogProbMetric: 18.7416, val_loss: 18.7876, val_MinusLogProbMetric: 18.7876

Epoch 381: val_loss improved from 18.79622 to 18.78758, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 78s - loss: 18.7416 - MinusLogProbMetric: 18.7416 - val_loss: 18.7876 - val_MinusLogProbMetric: 18.7876 - lr: 1.1111e-04 - 78s/epoch - 399ms/step
Epoch 382/1000
2023-09-27 17:27:30.555 
Epoch 382/1000 
	 loss: 18.7035, MinusLogProbMetric: 18.7035, val_loss: 18.9746, val_MinusLogProbMetric: 18.9746

Epoch 382: val_loss did not improve from 18.78758
196/196 - 76s - loss: 18.7035 - MinusLogProbMetric: 18.7035 - val_loss: 18.9746 - val_MinusLogProbMetric: 18.9746 - lr: 1.1111e-04 - 76s/epoch - 389ms/step
Epoch 383/1000
2023-09-27 17:28:46.762 
Epoch 383/1000 
	 loss: 18.7175, MinusLogProbMetric: 18.7175, val_loss: 18.8771, val_MinusLogProbMetric: 18.8771

Epoch 383: val_loss did not improve from 18.78758
196/196 - 76s - loss: 18.7175 - MinusLogProbMetric: 18.7175 - val_loss: 18.8771 - val_MinusLogProbMetric: 18.8771 - lr: 1.1111e-04 - 76s/epoch - 389ms/step
Epoch 384/1000
2023-09-27 17:30:02.803 
Epoch 384/1000 
	 loss: 18.7157, MinusLogProbMetric: 18.7157, val_loss: 18.8188, val_MinusLogProbMetric: 18.8188

Epoch 384: val_loss did not improve from 18.78758
196/196 - 76s - loss: 18.7157 - MinusLogProbMetric: 18.7157 - val_loss: 18.8188 - val_MinusLogProbMetric: 18.8188 - lr: 1.1111e-04 - 76s/epoch - 388ms/step
Epoch 385/1000
2023-09-27 17:31:20.521 
Epoch 385/1000 
	 loss: 18.7090, MinusLogProbMetric: 18.7090, val_loss: 18.8518, val_MinusLogProbMetric: 18.8518

Epoch 385: val_loss did not improve from 18.78758
196/196 - 78s - loss: 18.7090 - MinusLogProbMetric: 18.7090 - val_loss: 18.8518 - val_MinusLogProbMetric: 18.8518 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 386/1000
2023-09-27 17:32:37.092 
Epoch 386/1000 
	 loss: 18.7016, MinusLogProbMetric: 18.7016, val_loss: 18.7424, val_MinusLogProbMetric: 18.7424

Epoch 386: val_loss improved from 18.78758 to 18.74236, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 78s - loss: 18.7016 - MinusLogProbMetric: 18.7016 - val_loss: 18.7424 - val_MinusLogProbMetric: 18.7424 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 387/1000
2023-09-27 17:33:54.957 
Epoch 387/1000 
	 loss: 18.6802, MinusLogProbMetric: 18.6802, val_loss: 19.0528, val_MinusLogProbMetric: 19.0528

Epoch 387: val_loss did not improve from 18.74236
196/196 - 76s - loss: 18.6802 - MinusLogProbMetric: 18.6802 - val_loss: 19.0528 - val_MinusLogProbMetric: 19.0528 - lr: 1.1111e-04 - 76s/epoch - 390ms/step
Epoch 388/1000
2023-09-27 17:35:11.936 
Epoch 388/1000 
	 loss: 18.6964, MinusLogProbMetric: 18.6964, val_loss: 18.7857, val_MinusLogProbMetric: 18.7857

Epoch 388: val_loss did not improve from 18.74236
196/196 - 77s - loss: 18.6964 - MinusLogProbMetric: 18.6964 - val_loss: 18.7857 - val_MinusLogProbMetric: 18.7857 - lr: 1.1111e-04 - 77s/epoch - 393ms/step
Epoch 389/1000
2023-09-27 17:36:29.304 
Epoch 389/1000 
	 loss: 18.6722, MinusLogProbMetric: 18.6722, val_loss: 18.7212, val_MinusLogProbMetric: 18.7212

Epoch 389: val_loss improved from 18.74236 to 18.72121, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 79s - loss: 18.6722 - MinusLogProbMetric: 18.6722 - val_loss: 18.7212 - val_MinusLogProbMetric: 18.7212 - lr: 1.1111e-04 - 79s/epoch - 402ms/step
Epoch 390/1000
2023-09-27 17:37:48.483 
Epoch 390/1000 
	 loss: 18.6681, MinusLogProbMetric: 18.6681, val_loss: 18.8651, val_MinusLogProbMetric: 18.8651

Epoch 390: val_loss did not improve from 18.72121
196/196 - 78s - loss: 18.6681 - MinusLogProbMetric: 18.6681 - val_loss: 18.8651 - val_MinusLogProbMetric: 18.8651 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 391/1000
2023-09-27 17:39:05.445 
Epoch 391/1000 
	 loss: 18.6725, MinusLogProbMetric: 18.6725, val_loss: 18.8501, val_MinusLogProbMetric: 18.8501

Epoch 391: val_loss did not improve from 18.72121
196/196 - 77s - loss: 18.6725 - MinusLogProbMetric: 18.6725 - val_loss: 18.8501 - val_MinusLogProbMetric: 18.8501 - lr: 1.1111e-04 - 77s/epoch - 393ms/step
Epoch 392/1000
2023-09-27 17:40:21.982 
Epoch 392/1000 
	 loss: 18.6418, MinusLogProbMetric: 18.6418, val_loss: 18.9804, val_MinusLogProbMetric: 18.9804

Epoch 392: val_loss did not improve from 18.72121
196/196 - 77s - loss: 18.6418 - MinusLogProbMetric: 18.6418 - val_loss: 18.9804 - val_MinusLogProbMetric: 18.9804 - lr: 1.1111e-04 - 77s/epoch - 390ms/step
Epoch 393/1000
2023-09-27 17:41:41.646 
Epoch 393/1000 
	 loss: 18.6661, MinusLogProbMetric: 18.6661, val_loss: 18.7989, val_MinusLogProbMetric: 18.7989

Epoch 393: val_loss did not improve from 18.72121
196/196 - 80s - loss: 18.6661 - MinusLogProbMetric: 18.6661 - val_loss: 18.7989 - val_MinusLogProbMetric: 18.7989 - lr: 1.1111e-04 - 80s/epoch - 406ms/step
Epoch 394/1000
2023-09-27 17:43:00.807 
Epoch 394/1000 
	 loss: 18.6439, MinusLogProbMetric: 18.6439, val_loss: 18.7753, val_MinusLogProbMetric: 18.7753

Epoch 394: val_loss did not improve from 18.72121
196/196 - 79s - loss: 18.6439 - MinusLogProbMetric: 18.6439 - val_loss: 18.7753 - val_MinusLogProbMetric: 18.7753 - lr: 1.1111e-04 - 79s/epoch - 404ms/step
Epoch 395/1000
2023-09-27 17:44:17.169 
Epoch 395/1000 
	 loss: 18.6352, MinusLogProbMetric: 18.6352, val_loss: 18.6960, val_MinusLogProbMetric: 18.6960

Epoch 395: val_loss improved from 18.72121 to 18.69596, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 78s - loss: 18.6352 - MinusLogProbMetric: 18.6352 - val_loss: 18.6960 - val_MinusLogProbMetric: 18.6960 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 396/1000
2023-09-27 17:45:35.550 
Epoch 396/1000 
	 loss: 18.6326, MinusLogProbMetric: 18.6326, val_loss: 18.7534, val_MinusLogProbMetric: 18.7534

Epoch 396: val_loss did not improve from 18.69596
196/196 - 77s - loss: 18.6326 - MinusLogProbMetric: 18.6326 - val_loss: 18.7534 - val_MinusLogProbMetric: 18.7534 - lr: 1.1111e-04 - 77s/epoch - 394ms/step
Epoch 397/1000
2023-09-27 17:46:52.272 
Epoch 397/1000 
	 loss: 18.6220, MinusLogProbMetric: 18.6220, val_loss: 18.7211, val_MinusLogProbMetric: 18.7211

Epoch 397: val_loss did not improve from 18.69596
196/196 - 77s - loss: 18.6220 - MinusLogProbMetric: 18.6220 - val_loss: 18.7211 - val_MinusLogProbMetric: 18.7211 - lr: 1.1111e-04 - 77s/epoch - 391ms/step
Epoch 398/1000
2023-09-27 17:48:08.665 
Epoch 398/1000 
	 loss: 18.6158, MinusLogProbMetric: 18.6158, val_loss: 18.7658, val_MinusLogProbMetric: 18.7658

Epoch 398: val_loss did not improve from 18.69596
196/196 - 76s - loss: 18.6158 - MinusLogProbMetric: 18.6158 - val_loss: 18.7658 - val_MinusLogProbMetric: 18.7658 - lr: 1.1111e-04 - 76s/epoch - 390ms/step
Epoch 399/1000
2023-09-27 17:49:25.973 
Epoch 399/1000 
	 loss: 18.6444, MinusLogProbMetric: 18.6444, val_loss: 18.9059, val_MinusLogProbMetric: 18.9059

Epoch 399: val_loss did not improve from 18.69596
196/196 - 77s - loss: 18.6444 - MinusLogProbMetric: 18.6444 - val_loss: 18.9059 - val_MinusLogProbMetric: 18.9059 - lr: 1.1111e-04 - 77s/epoch - 394ms/step
Epoch 400/1000
2023-09-27 17:50:41.890 
Epoch 400/1000 
	 loss: 18.6232, MinusLogProbMetric: 18.6232, val_loss: 18.7514, val_MinusLogProbMetric: 18.7514

Epoch 400: val_loss did not improve from 18.69596
196/196 - 76s - loss: 18.6232 - MinusLogProbMetric: 18.6232 - val_loss: 18.7514 - val_MinusLogProbMetric: 18.7514 - lr: 1.1111e-04 - 76s/epoch - 387ms/step
Epoch 401/1000
2023-09-27 17:51:58.704 
Epoch 401/1000 
	 loss: 18.6007, MinusLogProbMetric: 18.6007, val_loss: 18.9237, val_MinusLogProbMetric: 18.9237

Epoch 401: val_loss did not improve from 18.69596
196/196 - 77s - loss: 18.6007 - MinusLogProbMetric: 18.6007 - val_loss: 18.9237 - val_MinusLogProbMetric: 18.9237 - lr: 1.1111e-04 - 77s/epoch - 392ms/step
Epoch 402/1000
2023-09-27 17:53:16.843 
Epoch 402/1000 
	 loss: 18.6177, MinusLogProbMetric: 18.6177, val_loss: 18.6635, val_MinusLogProbMetric: 18.6635

Epoch 402: val_loss improved from 18.69596 to 18.66348, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 79s - loss: 18.6177 - MinusLogProbMetric: 18.6177 - val_loss: 18.6635 - val_MinusLogProbMetric: 18.6635 - lr: 1.1111e-04 - 79s/epoch - 405ms/step
Epoch 403/1000
2023-09-27 17:54:35.077 
Epoch 403/1000 
	 loss: 18.5985, MinusLogProbMetric: 18.5985, val_loss: 18.7868, val_MinusLogProbMetric: 18.7868

Epoch 403: val_loss did not improve from 18.66348
196/196 - 77s - loss: 18.5985 - MinusLogProbMetric: 18.5985 - val_loss: 18.7868 - val_MinusLogProbMetric: 18.7868 - lr: 1.1111e-04 - 77s/epoch - 392ms/step
Epoch 404/1000
2023-09-27 17:55:51.804 
Epoch 404/1000 
	 loss: 18.6090, MinusLogProbMetric: 18.6090, val_loss: 18.5855, val_MinusLogProbMetric: 18.5855

Epoch 404: val_loss improved from 18.66348 to 18.58553, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 78s - loss: 18.6090 - MinusLogProbMetric: 18.6090 - val_loss: 18.5855 - val_MinusLogProbMetric: 18.5855 - lr: 1.1111e-04 - 78s/epoch - 399ms/step
Epoch 405/1000
2023-09-27 17:57:09.860 
Epoch 405/1000 
	 loss: 18.5923, MinusLogProbMetric: 18.5923, val_loss: 18.6239, val_MinusLogProbMetric: 18.6239

Epoch 405: val_loss did not improve from 18.58553
196/196 - 77s - loss: 18.5923 - MinusLogProbMetric: 18.5923 - val_loss: 18.6239 - val_MinusLogProbMetric: 18.6239 - lr: 1.1111e-04 - 77s/epoch - 391ms/step
Epoch 406/1000
2023-09-27 17:58:26.970 
Epoch 406/1000 
	 loss: 18.5888, MinusLogProbMetric: 18.5888, val_loss: 18.7742, val_MinusLogProbMetric: 18.7742

Epoch 406: val_loss did not improve from 18.58553
196/196 - 77s - loss: 18.5888 - MinusLogProbMetric: 18.5888 - val_loss: 18.7742 - val_MinusLogProbMetric: 18.7742 - lr: 1.1111e-04 - 77s/epoch - 393ms/step
Epoch 407/1000
2023-09-27 17:59:44.717 
Epoch 407/1000 
	 loss: 18.5831, MinusLogProbMetric: 18.5831, val_loss: 18.6615, val_MinusLogProbMetric: 18.6615

Epoch 407: val_loss did not improve from 18.58553
196/196 - 78s - loss: 18.5831 - MinusLogProbMetric: 18.5831 - val_loss: 18.6615 - val_MinusLogProbMetric: 18.6615 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 408/1000
2023-09-27 18:01:01.303 
Epoch 408/1000 
	 loss: 18.5874, MinusLogProbMetric: 18.5874, val_loss: 18.6860, val_MinusLogProbMetric: 18.6860

Epoch 408: val_loss did not improve from 18.58553
196/196 - 77s - loss: 18.5874 - MinusLogProbMetric: 18.5874 - val_loss: 18.6860 - val_MinusLogProbMetric: 18.6860 - lr: 1.1111e-04 - 77s/epoch - 391ms/step
Epoch 409/1000
2023-09-27 18:02:16.981 
Epoch 409/1000 
	 loss: 18.5812, MinusLogProbMetric: 18.5812, val_loss: 18.8989, val_MinusLogProbMetric: 18.8989

Epoch 409: val_loss did not improve from 18.58553
196/196 - 76s - loss: 18.5812 - MinusLogProbMetric: 18.5812 - val_loss: 18.8989 - val_MinusLogProbMetric: 18.8989 - lr: 1.1111e-04 - 76s/epoch - 386ms/step
Epoch 410/1000
2023-09-27 18:03:34.058 
Epoch 410/1000 
	 loss: 18.5647, MinusLogProbMetric: 18.5647, val_loss: 18.6636, val_MinusLogProbMetric: 18.6636

Epoch 410: val_loss did not improve from 18.58553
196/196 - 77s - loss: 18.5647 - MinusLogProbMetric: 18.5647 - val_loss: 18.6636 - val_MinusLogProbMetric: 18.6636 - lr: 1.1111e-04 - 77s/epoch - 393ms/step
Epoch 411/1000
2023-09-27 18:04:51.174 
Epoch 411/1000 
	 loss: 18.5511, MinusLogProbMetric: 18.5511, val_loss: 18.7212, val_MinusLogProbMetric: 18.7212

Epoch 411: val_loss did not improve from 18.58553
196/196 - 77s - loss: 18.5511 - MinusLogProbMetric: 18.5511 - val_loss: 18.7212 - val_MinusLogProbMetric: 18.7212 - lr: 1.1111e-04 - 77s/epoch - 393ms/step
Epoch 412/1000
2023-09-27 18:06:09.146 
Epoch 412/1000 
	 loss: 18.5842, MinusLogProbMetric: 18.5842, val_loss: 18.8331, val_MinusLogProbMetric: 18.8331

Epoch 412: val_loss did not improve from 18.58553
196/196 - 78s - loss: 18.5842 - MinusLogProbMetric: 18.5842 - val_loss: 18.8331 - val_MinusLogProbMetric: 18.8331 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 413/1000
2023-09-27 18:07:26.907 
Epoch 413/1000 
	 loss: 18.5566, MinusLogProbMetric: 18.5566, val_loss: 18.6552, val_MinusLogProbMetric: 18.6552

Epoch 413: val_loss did not improve from 18.58553
196/196 - 78s - loss: 18.5566 - MinusLogProbMetric: 18.5566 - val_loss: 18.6552 - val_MinusLogProbMetric: 18.6552 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 414/1000
2023-09-27 18:08:43.307 
Epoch 414/1000 
	 loss: 18.5798, MinusLogProbMetric: 18.5798, val_loss: 18.6509, val_MinusLogProbMetric: 18.6509

Epoch 414: val_loss did not improve from 18.58553
196/196 - 76s - loss: 18.5798 - MinusLogProbMetric: 18.5798 - val_loss: 18.6509 - val_MinusLogProbMetric: 18.6509 - lr: 1.1111e-04 - 76s/epoch - 390ms/step
Epoch 415/1000
2023-09-27 18:10:01.316 
Epoch 415/1000 
	 loss: 18.5628, MinusLogProbMetric: 18.5628, val_loss: 18.7086, val_MinusLogProbMetric: 18.7086

Epoch 415: val_loss did not improve from 18.58553
196/196 - 78s - loss: 18.5628 - MinusLogProbMetric: 18.5628 - val_loss: 18.7086 - val_MinusLogProbMetric: 18.7086 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 416/1000
2023-09-27 18:11:18.385 
Epoch 416/1000 
	 loss: 18.5528, MinusLogProbMetric: 18.5528, val_loss: 18.6625, val_MinusLogProbMetric: 18.6625

Epoch 416: val_loss did not improve from 18.58553
196/196 - 77s - loss: 18.5528 - MinusLogProbMetric: 18.5528 - val_loss: 18.6625 - val_MinusLogProbMetric: 18.6625 - lr: 1.1111e-04 - 77s/epoch - 393ms/step
Epoch 417/1000
2023-09-27 18:12:35.019 
Epoch 417/1000 
	 loss: 18.5517, MinusLogProbMetric: 18.5517, val_loss: 18.6293, val_MinusLogProbMetric: 18.6293

Epoch 417: val_loss did not improve from 18.58553
196/196 - 77s - loss: 18.5517 - MinusLogProbMetric: 18.5517 - val_loss: 18.6293 - val_MinusLogProbMetric: 18.6293 - lr: 1.1111e-04 - 77s/epoch - 391ms/step
Epoch 418/1000
2023-09-27 18:13:52.917 
Epoch 418/1000 
	 loss: 18.5495, MinusLogProbMetric: 18.5495, val_loss: 18.7932, val_MinusLogProbMetric: 18.7932

Epoch 418: val_loss did not improve from 18.58553
196/196 - 78s - loss: 18.5495 - MinusLogProbMetric: 18.5495 - val_loss: 18.7932 - val_MinusLogProbMetric: 18.7932 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 419/1000
2023-09-27 18:15:09.298 
Epoch 419/1000 
	 loss: 18.5445, MinusLogProbMetric: 18.5445, val_loss: 18.7192, val_MinusLogProbMetric: 18.7192

Epoch 419: val_loss did not improve from 18.58553
196/196 - 76s - loss: 18.5445 - MinusLogProbMetric: 18.5445 - val_loss: 18.7192 - val_MinusLogProbMetric: 18.7192 - lr: 1.1111e-04 - 76s/epoch - 390ms/step
Epoch 420/1000
2023-09-27 18:16:26.117 
Epoch 420/1000 
	 loss: 18.5220, MinusLogProbMetric: 18.5220, val_loss: 18.6474, val_MinusLogProbMetric: 18.6474

Epoch 420: val_loss did not improve from 18.58553
196/196 - 77s - loss: 18.5220 - MinusLogProbMetric: 18.5220 - val_loss: 18.6474 - val_MinusLogProbMetric: 18.6474 - lr: 1.1111e-04 - 77s/epoch - 392ms/step
Epoch 421/1000
2023-09-27 18:17:41.173 
Epoch 421/1000 
	 loss: 18.5264, MinusLogProbMetric: 18.5264, val_loss: 19.0280, val_MinusLogProbMetric: 19.0280

Epoch 421: val_loss did not improve from 18.58553
196/196 - 75s - loss: 18.5264 - MinusLogProbMetric: 18.5264 - val_loss: 19.0280 - val_MinusLogProbMetric: 19.0280 - lr: 1.1111e-04 - 75s/epoch - 383ms/step
Epoch 422/1000
2023-09-27 18:18:57.357 
Epoch 422/1000 
	 loss: 18.5294, MinusLogProbMetric: 18.5294, val_loss: 18.5492, val_MinusLogProbMetric: 18.5492

Epoch 422: val_loss improved from 18.58553 to 18.54915, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 78s - loss: 18.5294 - MinusLogProbMetric: 18.5294 - val_loss: 18.5492 - val_MinusLogProbMetric: 18.5492 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 423/1000
2023-09-27 18:20:15.365 
Epoch 423/1000 
	 loss: 18.5202, MinusLogProbMetric: 18.5202, val_loss: 18.6352, val_MinusLogProbMetric: 18.6352

Epoch 423: val_loss did not improve from 18.54915
196/196 - 76s - loss: 18.5202 - MinusLogProbMetric: 18.5202 - val_loss: 18.6352 - val_MinusLogProbMetric: 18.6352 - lr: 1.1111e-04 - 76s/epoch - 390ms/step
Epoch 424/1000
2023-09-27 18:21:31.085 
Epoch 424/1000 
	 loss: 18.5060, MinusLogProbMetric: 18.5060, val_loss: 18.8511, val_MinusLogProbMetric: 18.8511

Epoch 424: val_loss did not improve from 18.54915
196/196 - 76s - loss: 18.5060 - MinusLogProbMetric: 18.5060 - val_loss: 18.8511 - val_MinusLogProbMetric: 18.8511 - lr: 1.1111e-04 - 76s/epoch - 386ms/step
Epoch 425/1000
2023-09-27 18:22:47.855 
Epoch 425/1000 
	 loss: 18.5223, MinusLogProbMetric: 18.5223, val_loss: 18.6177, val_MinusLogProbMetric: 18.6177

Epoch 425: val_loss did not improve from 18.54915
196/196 - 77s - loss: 18.5223 - MinusLogProbMetric: 18.5223 - val_loss: 18.6177 - val_MinusLogProbMetric: 18.6177 - lr: 1.1111e-04 - 77s/epoch - 392ms/step
Epoch 426/1000
2023-09-27 18:24:03.144 
Epoch 426/1000 
	 loss: 18.5176, MinusLogProbMetric: 18.5176, val_loss: 18.5783, val_MinusLogProbMetric: 18.5783

Epoch 426: val_loss did not improve from 18.54915
196/196 - 75s - loss: 18.5176 - MinusLogProbMetric: 18.5176 - val_loss: 18.5783 - val_MinusLogProbMetric: 18.5783 - lr: 1.1111e-04 - 75s/epoch - 384ms/step
Epoch 427/1000
2023-09-27 18:25:20.288 
Epoch 427/1000 
	 loss: 18.4988, MinusLogProbMetric: 18.4988, val_loss: 18.9050, val_MinusLogProbMetric: 18.9050

Epoch 427: val_loss did not improve from 18.54915
196/196 - 77s - loss: 18.4988 - MinusLogProbMetric: 18.4988 - val_loss: 18.9050 - val_MinusLogProbMetric: 18.9050 - lr: 1.1111e-04 - 77s/epoch - 394ms/step
Epoch 428/1000
2023-09-27 18:26:37.175 
Epoch 428/1000 
	 loss: 18.5345, MinusLogProbMetric: 18.5345, val_loss: 18.5798, val_MinusLogProbMetric: 18.5798

Epoch 428: val_loss did not improve from 18.54915
196/196 - 77s - loss: 18.5345 - MinusLogProbMetric: 18.5345 - val_loss: 18.5798 - val_MinusLogProbMetric: 18.5798 - lr: 1.1111e-04 - 77s/epoch - 392ms/step
Epoch 429/1000
2023-09-27 18:27:53.644 
Epoch 429/1000 
	 loss: 18.5154, MinusLogProbMetric: 18.5154, val_loss: 18.6634, val_MinusLogProbMetric: 18.6634

Epoch 429: val_loss did not improve from 18.54915
196/196 - 76s - loss: 18.5154 - MinusLogProbMetric: 18.5154 - val_loss: 18.6634 - val_MinusLogProbMetric: 18.6634 - lr: 1.1111e-04 - 76s/epoch - 390ms/step
Epoch 430/1000
2023-09-27 18:29:09.998 
Epoch 430/1000 
	 loss: 18.4837, MinusLogProbMetric: 18.4837, val_loss: 18.5624, val_MinusLogProbMetric: 18.5624

Epoch 430: val_loss did not improve from 18.54915
196/196 - 76s - loss: 18.4837 - MinusLogProbMetric: 18.4837 - val_loss: 18.5624 - val_MinusLogProbMetric: 18.5624 - lr: 1.1111e-04 - 76s/epoch - 390ms/step
Epoch 431/1000
2023-09-27 18:30:27.328 
Epoch 431/1000 
	 loss: 18.4680, MinusLogProbMetric: 18.4680, val_loss: 18.6010, val_MinusLogProbMetric: 18.6010

Epoch 431: val_loss did not improve from 18.54915
196/196 - 77s - loss: 18.4680 - MinusLogProbMetric: 18.4680 - val_loss: 18.6010 - val_MinusLogProbMetric: 18.6010 - lr: 1.1111e-04 - 77s/epoch - 395ms/step
Epoch 432/1000
2023-09-27 18:31:43.775 
Epoch 432/1000 
	 loss: 18.4700, MinusLogProbMetric: 18.4700, val_loss: 18.6834, val_MinusLogProbMetric: 18.6834

Epoch 432: val_loss did not improve from 18.54915
196/196 - 76s - loss: 18.4700 - MinusLogProbMetric: 18.4700 - val_loss: 18.6834 - val_MinusLogProbMetric: 18.6834 - lr: 1.1111e-04 - 76s/epoch - 390ms/step
Epoch 433/1000
2023-09-27 18:33:00.949 
Epoch 433/1000 
	 loss: 18.4810, MinusLogProbMetric: 18.4810, val_loss: 18.5113, val_MinusLogProbMetric: 18.5113

Epoch 433: val_loss improved from 18.54915 to 18.51131, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 78s - loss: 18.4810 - MinusLogProbMetric: 18.4810 - val_loss: 18.5113 - val_MinusLogProbMetric: 18.5113 - lr: 1.1111e-04 - 78s/epoch - 400ms/step
Epoch 434/1000
2023-09-27 18:34:13.795 
Epoch 434/1000 
	 loss: 18.4532, MinusLogProbMetric: 18.4532, val_loss: 18.7216, val_MinusLogProbMetric: 18.7216

Epoch 434: val_loss did not improve from 18.51131
196/196 - 72s - loss: 18.4532 - MinusLogProbMetric: 18.4532 - val_loss: 18.7216 - val_MinusLogProbMetric: 18.7216 - lr: 1.1111e-04 - 72s/epoch - 366ms/step
Epoch 435/1000
2023-09-27 18:35:18.492 
Epoch 435/1000 
	 loss: 18.4957, MinusLogProbMetric: 18.4957, val_loss: 18.7405, val_MinusLogProbMetric: 18.7405

Epoch 435: val_loss did not improve from 18.51131
196/196 - 65s - loss: 18.4957 - MinusLogProbMetric: 18.4957 - val_loss: 18.7405 - val_MinusLogProbMetric: 18.7405 - lr: 1.1111e-04 - 65s/epoch - 330ms/step
Epoch 436/1000
2023-09-27 18:36:22.233 
Epoch 436/1000 
	 loss: 18.4600, MinusLogProbMetric: 18.4600, val_loss: 18.5947, val_MinusLogProbMetric: 18.5947

Epoch 436: val_loss did not improve from 18.51131
196/196 - 64s - loss: 18.4600 - MinusLogProbMetric: 18.4600 - val_loss: 18.5947 - val_MinusLogProbMetric: 18.5947 - lr: 1.1111e-04 - 64s/epoch - 325ms/step
Epoch 437/1000
2023-09-27 18:37:33.376 
Epoch 437/1000 
	 loss: 18.4823, MinusLogProbMetric: 18.4823, val_loss: 18.6218, val_MinusLogProbMetric: 18.6218

Epoch 437: val_loss did not improve from 18.51131
196/196 - 71s - loss: 18.4823 - MinusLogProbMetric: 18.4823 - val_loss: 18.6218 - val_MinusLogProbMetric: 18.6218 - lr: 1.1111e-04 - 71s/epoch - 363ms/step
Epoch 438/1000
2023-09-27 18:38:34.817 
Epoch 438/1000 
	 loss: 18.4767, MinusLogProbMetric: 18.4767, val_loss: 18.5335, val_MinusLogProbMetric: 18.5335

Epoch 438: val_loss did not improve from 18.51131
196/196 - 61s - loss: 18.4767 - MinusLogProbMetric: 18.4767 - val_loss: 18.5335 - val_MinusLogProbMetric: 18.5335 - lr: 1.1111e-04 - 61s/epoch - 313ms/step
Epoch 439/1000
2023-09-27 18:39:40.772 
Epoch 439/1000 
	 loss: 18.4635, MinusLogProbMetric: 18.4635, val_loss: 18.5937, val_MinusLogProbMetric: 18.5937

Epoch 439: val_loss did not improve from 18.51131
196/196 - 66s - loss: 18.4635 - MinusLogProbMetric: 18.4635 - val_loss: 18.5937 - val_MinusLogProbMetric: 18.5937 - lr: 1.1111e-04 - 66s/epoch - 337ms/step
Epoch 440/1000
2023-09-27 18:40:56.045 
Epoch 440/1000 
	 loss: 18.4691, MinusLogProbMetric: 18.4691, val_loss: 18.6629, val_MinusLogProbMetric: 18.6629

Epoch 440: val_loss did not improve from 18.51131
196/196 - 75s - loss: 18.4691 - MinusLogProbMetric: 18.4691 - val_loss: 18.6629 - val_MinusLogProbMetric: 18.6629 - lr: 1.1111e-04 - 75s/epoch - 384ms/step
Epoch 441/1000
2023-09-27 18:42:11.071 
Epoch 441/1000 
	 loss: 18.4403, MinusLogProbMetric: 18.4403, val_loss: 18.7517, val_MinusLogProbMetric: 18.7517

Epoch 441: val_loss did not improve from 18.51131
196/196 - 75s - loss: 18.4403 - MinusLogProbMetric: 18.4403 - val_loss: 18.7517 - val_MinusLogProbMetric: 18.7517 - lr: 1.1111e-04 - 75s/epoch - 383ms/step
Epoch 442/1000
2023-09-27 18:43:27.885 
Epoch 442/1000 
	 loss: 18.4309, MinusLogProbMetric: 18.4309, val_loss: 18.5649, val_MinusLogProbMetric: 18.5649

Epoch 442: val_loss did not improve from 18.51131
196/196 - 77s - loss: 18.4309 - MinusLogProbMetric: 18.4309 - val_loss: 18.5649 - val_MinusLogProbMetric: 18.5649 - lr: 1.1111e-04 - 77s/epoch - 392ms/step
Epoch 443/1000
2023-09-27 18:44:45.848 
Epoch 443/1000 
	 loss: 18.4384, MinusLogProbMetric: 18.4384, val_loss: 18.5752, val_MinusLogProbMetric: 18.5752

Epoch 443: val_loss did not improve from 18.51131
196/196 - 78s - loss: 18.4384 - MinusLogProbMetric: 18.4384 - val_loss: 18.5752 - val_MinusLogProbMetric: 18.5752 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 444/1000
2023-09-27 18:46:03.020 
Epoch 444/1000 
	 loss: 18.4213, MinusLogProbMetric: 18.4213, val_loss: 18.6873, val_MinusLogProbMetric: 18.6873

Epoch 444: val_loss did not improve from 18.51131
196/196 - 77s - loss: 18.4213 - MinusLogProbMetric: 18.4213 - val_loss: 18.6873 - val_MinusLogProbMetric: 18.6873 - lr: 1.1111e-04 - 77s/epoch - 394ms/step
Epoch 445/1000
2023-09-27 18:47:19.916 
Epoch 445/1000 
	 loss: 18.4319, MinusLogProbMetric: 18.4319, val_loss: 18.5028, val_MinusLogProbMetric: 18.5028

Epoch 445: val_loss improved from 18.51131 to 18.50282, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 78s - loss: 18.4319 - MinusLogProbMetric: 18.4319 - val_loss: 18.5028 - val_MinusLogProbMetric: 18.5028 - lr: 1.1111e-04 - 78s/epoch - 400ms/step
Epoch 446/1000
2023-09-27 18:48:38.500 
Epoch 446/1000 
	 loss: 18.4227, MinusLogProbMetric: 18.4227, val_loss: 18.4701, val_MinusLogProbMetric: 18.4701

Epoch 446: val_loss improved from 18.50282 to 18.47011, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 79s - loss: 18.4227 - MinusLogProbMetric: 18.4227 - val_loss: 18.4701 - val_MinusLogProbMetric: 18.4701 - lr: 1.1111e-04 - 79s/epoch - 401ms/step
Epoch 447/1000
2023-09-27 18:49:57.437 
Epoch 447/1000 
	 loss: 18.4182, MinusLogProbMetric: 18.4182, val_loss: 18.6325, val_MinusLogProbMetric: 18.6325

Epoch 447: val_loss did not improve from 18.47011
196/196 - 77s - loss: 18.4182 - MinusLogProbMetric: 18.4182 - val_loss: 18.6325 - val_MinusLogProbMetric: 18.6325 - lr: 1.1111e-04 - 77s/epoch - 395ms/step
Epoch 448/1000
2023-09-27 18:51:15.016 
Epoch 448/1000 
	 loss: 18.4173, MinusLogProbMetric: 18.4173, val_loss: 18.5282, val_MinusLogProbMetric: 18.5282

Epoch 448: val_loss did not improve from 18.47011
196/196 - 78s - loss: 18.4173 - MinusLogProbMetric: 18.4173 - val_loss: 18.5282 - val_MinusLogProbMetric: 18.5282 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 449/1000
2023-09-27 18:52:32.476 
Epoch 449/1000 
	 loss: 18.4268, MinusLogProbMetric: 18.4268, val_loss: 18.6509, val_MinusLogProbMetric: 18.6509

Epoch 449: val_loss did not improve from 18.47011
196/196 - 77s - loss: 18.4268 - MinusLogProbMetric: 18.4268 - val_loss: 18.6509 - val_MinusLogProbMetric: 18.6509 - lr: 1.1111e-04 - 77s/epoch - 395ms/step
Epoch 450/1000
2023-09-27 18:53:50.065 
Epoch 450/1000 
	 loss: 18.4128, MinusLogProbMetric: 18.4128, val_loss: 18.4835, val_MinusLogProbMetric: 18.4835

Epoch 450: val_loss did not improve from 18.47011
196/196 - 78s - loss: 18.4128 - MinusLogProbMetric: 18.4128 - val_loss: 18.4835 - val_MinusLogProbMetric: 18.4835 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 451/1000
2023-09-27 18:55:07.702 
Epoch 451/1000 
	 loss: 18.4045, MinusLogProbMetric: 18.4045, val_loss: 18.7286, val_MinusLogProbMetric: 18.7286

Epoch 451: val_loss did not improve from 18.47011
196/196 - 78s - loss: 18.4045 - MinusLogProbMetric: 18.4045 - val_loss: 18.7286 - val_MinusLogProbMetric: 18.7286 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 452/1000
2023-09-27 18:56:25.388 
Epoch 452/1000 
	 loss: 18.4182, MinusLogProbMetric: 18.4182, val_loss: 18.4766, val_MinusLogProbMetric: 18.4766

Epoch 452: val_loss did not improve from 18.47011
196/196 - 78s - loss: 18.4182 - MinusLogProbMetric: 18.4182 - val_loss: 18.4766 - val_MinusLogProbMetric: 18.4766 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 453/1000
2023-09-27 18:57:43.034 
Epoch 453/1000 
	 loss: 18.3972, MinusLogProbMetric: 18.3972, val_loss: 18.5785, val_MinusLogProbMetric: 18.5785

Epoch 453: val_loss did not improve from 18.47011
196/196 - 78s - loss: 18.3972 - MinusLogProbMetric: 18.3972 - val_loss: 18.5785 - val_MinusLogProbMetric: 18.5785 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 454/1000
2023-09-27 18:59:00.280 
Epoch 454/1000 
	 loss: 18.3919, MinusLogProbMetric: 18.3919, val_loss: 18.6303, val_MinusLogProbMetric: 18.6303

Epoch 454: val_loss did not improve from 18.47011
196/196 - 77s - loss: 18.3919 - MinusLogProbMetric: 18.3919 - val_loss: 18.6303 - val_MinusLogProbMetric: 18.6303 - lr: 1.1111e-04 - 77s/epoch - 394ms/step
Epoch 455/1000
2023-09-27 19:00:18.202 
Epoch 455/1000 
	 loss: 18.4042, MinusLogProbMetric: 18.4042, val_loss: 18.5625, val_MinusLogProbMetric: 18.5625

Epoch 455: val_loss did not improve from 18.47011
196/196 - 78s - loss: 18.4042 - MinusLogProbMetric: 18.4042 - val_loss: 18.5625 - val_MinusLogProbMetric: 18.5625 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 456/1000
2023-09-27 19:01:36.472 
Epoch 456/1000 
	 loss: 18.3898, MinusLogProbMetric: 18.3898, val_loss: 18.4655, val_MinusLogProbMetric: 18.4655

Epoch 456: val_loss improved from 18.47011 to 18.46547, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 80s - loss: 18.3898 - MinusLogProbMetric: 18.3898 - val_loss: 18.4655 - val_MinusLogProbMetric: 18.4655 - lr: 1.1111e-04 - 80s/epoch - 407ms/step
Epoch 457/1000
2023-09-27 19:02:55.182 
Epoch 457/1000 
	 loss: 18.3673, MinusLogProbMetric: 18.3673, val_loss: 18.4939, val_MinusLogProbMetric: 18.4939

Epoch 457: val_loss did not improve from 18.46547
196/196 - 77s - loss: 18.3673 - MinusLogProbMetric: 18.3673 - val_loss: 18.4939 - val_MinusLogProbMetric: 18.4939 - lr: 1.1111e-04 - 77s/epoch - 394ms/step
Epoch 458/1000
2023-09-27 19:04:12.618 
Epoch 458/1000 
	 loss: 18.3842, MinusLogProbMetric: 18.3842, val_loss: 18.4769, val_MinusLogProbMetric: 18.4769

Epoch 458: val_loss did not improve from 18.46547
196/196 - 77s - loss: 18.3842 - MinusLogProbMetric: 18.3842 - val_loss: 18.4769 - val_MinusLogProbMetric: 18.4769 - lr: 1.1111e-04 - 77s/epoch - 395ms/step
Epoch 459/1000
2023-09-27 19:05:30.641 
Epoch 459/1000 
	 loss: 18.3814, MinusLogProbMetric: 18.3814, val_loss: 18.5282, val_MinusLogProbMetric: 18.5282

Epoch 459: val_loss did not improve from 18.46547
196/196 - 78s - loss: 18.3814 - MinusLogProbMetric: 18.3814 - val_loss: 18.5282 - val_MinusLogProbMetric: 18.5282 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 460/1000
2023-09-27 19:06:48.390 
Epoch 460/1000 
	 loss: 18.3614, MinusLogProbMetric: 18.3614, val_loss: 18.5170, val_MinusLogProbMetric: 18.5170

Epoch 460: val_loss did not improve from 18.46547
196/196 - 78s - loss: 18.3614 - MinusLogProbMetric: 18.3614 - val_loss: 18.5170 - val_MinusLogProbMetric: 18.5170 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 461/1000
2023-09-27 19:08:05.997 
Epoch 461/1000 
	 loss: 18.3812, MinusLogProbMetric: 18.3812, val_loss: 18.4225, val_MinusLogProbMetric: 18.4225

Epoch 461: val_loss improved from 18.46547 to 18.42249, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 79s - loss: 18.3812 - MinusLogProbMetric: 18.3812 - val_loss: 18.4225 - val_MinusLogProbMetric: 18.4225 - lr: 1.1111e-04 - 79s/epoch - 402ms/step
Epoch 462/1000
2023-09-27 19:09:25.321 
Epoch 462/1000 
	 loss: 18.3670, MinusLogProbMetric: 18.3670, val_loss: 18.5077, val_MinusLogProbMetric: 18.5077

Epoch 462: val_loss did not improve from 18.42249
196/196 - 78s - loss: 18.3670 - MinusLogProbMetric: 18.3670 - val_loss: 18.5077 - val_MinusLogProbMetric: 18.5077 - lr: 1.1111e-04 - 78s/epoch - 399ms/step
Epoch 463/1000
2023-09-27 19:10:42.511 
Epoch 463/1000 
	 loss: 18.3727, MinusLogProbMetric: 18.3727, val_loss: 18.5164, val_MinusLogProbMetric: 18.5164

Epoch 463: val_loss did not improve from 18.42249
196/196 - 77s - loss: 18.3727 - MinusLogProbMetric: 18.3727 - val_loss: 18.5164 - val_MinusLogProbMetric: 18.5164 - lr: 1.1111e-04 - 77s/epoch - 394ms/step
Epoch 464/1000
2023-09-27 19:12:00.094 
Epoch 464/1000 
	 loss: 18.3459, MinusLogProbMetric: 18.3459, val_loss: 18.6488, val_MinusLogProbMetric: 18.6488

Epoch 464: val_loss did not improve from 18.42249
196/196 - 78s - loss: 18.3459 - MinusLogProbMetric: 18.3459 - val_loss: 18.6488 - val_MinusLogProbMetric: 18.6488 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 465/1000
2023-09-27 19:13:17.983 
Epoch 465/1000 
	 loss: 18.3823, MinusLogProbMetric: 18.3823, val_loss: 18.4923, val_MinusLogProbMetric: 18.4923

Epoch 465: val_loss did not improve from 18.42249
196/196 - 78s - loss: 18.3823 - MinusLogProbMetric: 18.3823 - val_loss: 18.4923 - val_MinusLogProbMetric: 18.4923 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 466/1000
2023-09-27 19:14:35.876 
Epoch 466/1000 
	 loss: 18.3617, MinusLogProbMetric: 18.3617, val_loss: 18.4146, val_MinusLogProbMetric: 18.4146

Epoch 466: val_loss improved from 18.42249 to 18.41459, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 79s - loss: 18.3617 - MinusLogProbMetric: 18.3617 - val_loss: 18.4146 - val_MinusLogProbMetric: 18.4146 - lr: 1.1111e-04 - 79s/epoch - 405ms/step
Epoch 467/1000
2023-09-27 19:15:55.037 
Epoch 467/1000 
	 loss: 18.3277, MinusLogProbMetric: 18.3277, val_loss: 18.4940, val_MinusLogProbMetric: 18.4940

Epoch 467: val_loss did not improve from 18.41459
196/196 - 78s - loss: 18.3277 - MinusLogProbMetric: 18.3277 - val_loss: 18.4940 - val_MinusLogProbMetric: 18.4940 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 468/1000
2023-09-27 19:17:12.887 
Epoch 468/1000 
	 loss: 18.3521, MinusLogProbMetric: 18.3521, val_loss: 18.6067, val_MinusLogProbMetric: 18.6067

Epoch 468: val_loss did not improve from 18.41459
196/196 - 78s - loss: 18.3521 - MinusLogProbMetric: 18.3521 - val_loss: 18.6067 - val_MinusLogProbMetric: 18.6067 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 469/1000
2023-09-27 19:18:29.846 
Epoch 469/1000 
	 loss: 18.3520, MinusLogProbMetric: 18.3520, val_loss: 18.4392, val_MinusLogProbMetric: 18.4392

Epoch 469: val_loss did not improve from 18.41459
196/196 - 77s - loss: 18.3520 - MinusLogProbMetric: 18.3520 - val_loss: 18.4392 - val_MinusLogProbMetric: 18.4392 - lr: 1.1111e-04 - 77s/epoch - 393ms/step
Epoch 470/1000
2023-09-27 19:19:47.509 
Epoch 470/1000 
	 loss: 18.3500, MinusLogProbMetric: 18.3500, val_loss: 18.4263, val_MinusLogProbMetric: 18.4263

Epoch 470: val_loss did not improve from 18.41459
196/196 - 78s - loss: 18.3500 - MinusLogProbMetric: 18.3500 - val_loss: 18.4263 - val_MinusLogProbMetric: 18.4263 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 471/1000
2023-09-27 19:21:05.149 
Epoch 471/1000 
	 loss: 18.3277, MinusLogProbMetric: 18.3277, val_loss: 18.6104, val_MinusLogProbMetric: 18.6104

Epoch 471: val_loss did not improve from 18.41459
196/196 - 78s - loss: 18.3277 - MinusLogProbMetric: 18.3277 - val_loss: 18.6104 - val_MinusLogProbMetric: 18.6104 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 472/1000
2023-09-27 19:22:23.028 
Epoch 472/1000 
	 loss: 18.3262, MinusLogProbMetric: 18.3262, val_loss: 18.5226, val_MinusLogProbMetric: 18.5226

Epoch 472: val_loss did not improve from 18.41459
196/196 - 78s - loss: 18.3262 - MinusLogProbMetric: 18.3262 - val_loss: 18.5226 - val_MinusLogProbMetric: 18.5226 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 473/1000
2023-09-27 19:23:41.555 
Epoch 473/1000 
	 loss: 18.3381, MinusLogProbMetric: 18.3381, val_loss: 18.4485, val_MinusLogProbMetric: 18.4485

Epoch 473: val_loss did not improve from 18.41459
196/196 - 79s - loss: 18.3381 - MinusLogProbMetric: 18.3381 - val_loss: 18.4485 - val_MinusLogProbMetric: 18.4485 - lr: 1.1111e-04 - 79s/epoch - 401ms/step
Epoch 474/1000
2023-09-27 19:25:00.142 
Epoch 474/1000 
	 loss: 18.3320, MinusLogProbMetric: 18.3320, val_loss: 18.4881, val_MinusLogProbMetric: 18.4881

Epoch 474: val_loss did not improve from 18.41459
196/196 - 79s - loss: 18.3320 - MinusLogProbMetric: 18.3320 - val_loss: 18.4881 - val_MinusLogProbMetric: 18.4881 - lr: 1.1111e-04 - 79s/epoch - 401ms/step
Epoch 475/1000
2023-09-27 19:26:13.827 
Epoch 475/1000 
	 loss: 18.3179, MinusLogProbMetric: 18.3179, val_loss: 18.3623, val_MinusLogProbMetric: 18.3623

Epoch 475: val_loss improved from 18.41459 to 18.36228, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 75s - loss: 18.3179 - MinusLogProbMetric: 18.3179 - val_loss: 18.3623 - val_MinusLogProbMetric: 18.3623 - lr: 1.1111e-04 - 75s/epoch - 382ms/step
Epoch 476/1000
2023-09-27 19:27:24.700 
Epoch 476/1000 
	 loss: 18.3142, MinusLogProbMetric: 18.3142, val_loss: 18.5831, val_MinusLogProbMetric: 18.5831

Epoch 476: val_loss did not improve from 18.36228
196/196 - 70s - loss: 18.3142 - MinusLogProbMetric: 18.3142 - val_loss: 18.5831 - val_MinusLogProbMetric: 18.5831 - lr: 1.1111e-04 - 70s/epoch - 356ms/step
Epoch 477/1000
2023-09-27 19:28:36.729 
Epoch 477/1000 
	 loss: 18.3279, MinusLogProbMetric: 18.3279, val_loss: 18.5677, val_MinusLogProbMetric: 18.5677

Epoch 477: val_loss did not improve from 18.36228
196/196 - 72s - loss: 18.3279 - MinusLogProbMetric: 18.3279 - val_loss: 18.5677 - val_MinusLogProbMetric: 18.5677 - lr: 1.1111e-04 - 72s/epoch - 367ms/step
Epoch 478/1000
2023-09-27 19:29:54.651 
Epoch 478/1000 
	 loss: 18.3083, MinusLogProbMetric: 18.3083, val_loss: 18.4180, val_MinusLogProbMetric: 18.4180

Epoch 478: val_loss did not improve from 18.36228
196/196 - 78s - loss: 18.3083 - MinusLogProbMetric: 18.3083 - val_loss: 18.4180 - val_MinusLogProbMetric: 18.4180 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 479/1000
2023-09-27 19:31:12.515 
Epoch 479/1000 
	 loss: 18.3206, MinusLogProbMetric: 18.3206, val_loss: 18.4972, val_MinusLogProbMetric: 18.4972

Epoch 479: val_loss did not improve from 18.36228
196/196 - 78s - loss: 18.3206 - MinusLogProbMetric: 18.3206 - val_loss: 18.4972 - val_MinusLogProbMetric: 18.4972 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 480/1000
2023-09-27 19:32:30.637 
Epoch 480/1000 
	 loss: 18.3160, MinusLogProbMetric: 18.3160, val_loss: 18.4594, val_MinusLogProbMetric: 18.4594

Epoch 480: val_loss did not improve from 18.36228
196/196 - 78s - loss: 18.3160 - MinusLogProbMetric: 18.3160 - val_loss: 18.4594 - val_MinusLogProbMetric: 18.4594 - lr: 1.1111e-04 - 78s/epoch - 399ms/step
Epoch 481/1000
2023-09-27 19:33:48.999 
Epoch 481/1000 
	 loss: 18.3360, MinusLogProbMetric: 18.3360, val_loss: 18.3966, val_MinusLogProbMetric: 18.3966

Epoch 481: val_loss did not improve from 18.36228
196/196 - 78s - loss: 18.3360 - MinusLogProbMetric: 18.3360 - val_loss: 18.3966 - val_MinusLogProbMetric: 18.3966 - lr: 1.1111e-04 - 78s/epoch - 400ms/step
Epoch 482/1000
2023-09-27 19:35:07.964 
Epoch 482/1000 
	 loss: 18.3030, MinusLogProbMetric: 18.3030, val_loss: 18.5894, val_MinusLogProbMetric: 18.5894

Epoch 482: val_loss did not improve from 18.36228
196/196 - 79s - loss: 18.3030 - MinusLogProbMetric: 18.3030 - val_loss: 18.5894 - val_MinusLogProbMetric: 18.5894 - lr: 1.1111e-04 - 79s/epoch - 403ms/step
Epoch 483/1000
2023-09-27 19:36:26.713 
Epoch 483/1000 
	 loss: 18.3051, MinusLogProbMetric: 18.3051, val_loss: 18.4992, val_MinusLogProbMetric: 18.4992

Epoch 483: val_loss did not improve from 18.36228
196/196 - 79s - loss: 18.3051 - MinusLogProbMetric: 18.3051 - val_loss: 18.4992 - val_MinusLogProbMetric: 18.4992 - lr: 1.1111e-04 - 79s/epoch - 402ms/step
Epoch 484/1000
2023-09-27 19:37:45.240 
Epoch 484/1000 
	 loss: 18.2978, MinusLogProbMetric: 18.2978, val_loss: 18.2964, val_MinusLogProbMetric: 18.2964

Epoch 484: val_loss improved from 18.36228 to 18.29642, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 80s - loss: 18.2978 - MinusLogProbMetric: 18.2978 - val_loss: 18.2964 - val_MinusLogProbMetric: 18.2964 - lr: 1.1111e-04 - 80s/epoch - 407ms/step
Epoch 485/1000
2023-09-27 19:39:05.033 
Epoch 485/1000 
	 loss: 18.2940, MinusLogProbMetric: 18.2940, val_loss: 18.3651, val_MinusLogProbMetric: 18.3651

Epoch 485: val_loss did not improve from 18.29642
196/196 - 78s - loss: 18.2940 - MinusLogProbMetric: 18.2940 - val_loss: 18.3651 - val_MinusLogProbMetric: 18.3651 - lr: 1.1111e-04 - 78s/epoch - 400ms/step
Epoch 486/1000
2023-09-27 19:40:23.439 
Epoch 486/1000 
	 loss: 18.3108, MinusLogProbMetric: 18.3108, val_loss: 18.4410, val_MinusLogProbMetric: 18.4410

Epoch 486: val_loss did not improve from 18.29642
196/196 - 78s - loss: 18.3108 - MinusLogProbMetric: 18.3108 - val_loss: 18.4410 - val_MinusLogProbMetric: 18.4410 - lr: 1.1111e-04 - 78s/epoch - 400ms/step
Epoch 487/1000
2023-09-27 19:41:42.784 
Epoch 487/1000 
	 loss: 18.2826, MinusLogProbMetric: 18.2826, val_loss: 18.4642, val_MinusLogProbMetric: 18.4642

Epoch 487: val_loss did not improve from 18.29642
196/196 - 79s - loss: 18.2826 - MinusLogProbMetric: 18.2826 - val_loss: 18.4642 - val_MinusLogProbMetric: 18.4642 - lr: 1.1111e-04 - 79s/epoch - 405ms/step
Epoch 488/1000
2023-09-27 19:43:00.030 
Epoch 488/1000 
	 loss: 18.2925, MinusLogProbMetric: 18.2925, val_loss: 18.4127, val_MinusLogProbMetric: 18.4127

Epoch 488: val_loss did not improve from 18.29642
196/196 - 77s - loss: 18.2925 - MinusLogProbMetric: 18.2925 - val_loss: 18.4127 - val_MinusLogProbMetric: 18.4127 - lr: 1.1111e-04 - 77s/epoch - 394ms/step
Epoch 489/1000
2023-09-27 19:44:17.906 
Epoch 489/1000 
	 loss: 18.2749, MinusLogProbMetric: 18.2749, val_loss: 18.4615, val_MinusLogProbMetric: 18.4615

Epoch 489: val_loss did not improve from 18.29642
196/196 - 78s - loss: 18.2749 - MinusLogProbMetric: 18.2749 - val_loss: 18.4615 - val_MinusLogProbMetric: 18.4615 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 490/1000
2023-09-27 19:45:36.414 
Epoch 490/1000 
	 loss: 18.2894, MinusLogProbMetric: 18.2894, val_loss: 18.3880, val_MinusLogProbMetric: 18.3880

Epoch 490: val_loss did not improve from 18.29642
196/196 - 79s - loss: 18.2894 - MinusLogProbMetric: 18.2894 - val_loss: 18.3880 - val_MinusLogProbMetric: 18.3880 - lr: 1.1111e-04 - 79s/epoch - 401ms/step
Epoch 491/1000
2023-09-27 19:46:54.178 
Epoch 491/1000 
	 loss: 18.2711, MinusLogProbMetric: 18.2711, val_loss: 18.3547, val_MinusLogProbMetric: 18.3547

Epoch 491: val_loss did not improve from 18.29642
196/196 - 78s - loss: 18.2711 - MinusLogProbMetric: 18.2711 - val_loss: 18.3547 - val_MinusLogProbMetric: 18.3547 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 492/1000
2023-09-27 19:48:12.555 
Epoch 492/1000 
	 loss: 18.2681, MinusLogProbMetric: 18.2681, val_loss: 18.3858, val_MinusLogProbMetric: 18.3858

Epoch 492: val_loss did not improve from 18.29642
196/196 - 78s - loss: 18.2681 - MinusLogProbMetric: 18.2681 - val_loss: 18.3858 - val_MinusLogProbMetric: 18.3858 - lr: 1.1111e-04 - 78s/epoch - 400ms/step
Epoch 493/1000
2023-09-27 19:49:29.837 
Epoch 493/1000 
	 loss: 18.2677, MinusLogProbMetric: 18.2677, val_loss: 18.4405, val_MinusLogProbMetric: 18.4405

Epoch 493: val_loss did not improve from 18.29642
196/196 - 77s - loss: 18.2677 - MinusLogProbMetric: 18.2677 - val_loss: 18.4405 - val_MinusLogProbMetric: 18.4405 - lr: 1.1111e-04 - 77s/epoch - 394ms/step
Epoch 494/1000
2023-09-27 19:50:47.079 
Epoch 494/1000 
	 loss: 18.2590, MinusLogProbMetric: 18.2590, val_loss: 18.5432, val_MinusLogProbMetric: 18.5432

Epoch 494: val_loss did not improve from 18.29642
196/196 - 77s - loss: 18.2590 - MinusLogProbMetric: 18.2590 - val_loss: 18.5432 - val_MinusLogProbMetric: 18.5432 - lr: 1.1111e-04 - 77s/epoch - 394ms/step
Epoch 495/1000
2023-09-27 19:52:05.060 
Epoch 495/1000 
	 loss: 18.2459, MinusLogProbMetric: 18.2459, val_loss: 18.4608, val_MinusLogProbMetric: 18.4608

Epoch 495: val_loss did not improve from 18.29642
196/196 - 78s - loss: 18.2459 - MinusLogProbMetric: 18.2459 - val_loss: 18.4608 - val_MinusLogProbMetric: 18.4608 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 496/1000
2023-09-27 19:53:24.069 
Epoch 496/1000 
	 loss: 18.2488, MinusLogProbMetric: 18.2488, val_loss: 18.4552, val_MinusLogProbMetric: 18.4552

Epoch 496: val_loss did not improve from 18.29642
196/196 - 79s - loss: 18.2488 - MinusLogProbMetric: 18.2488 - val_loss: 18.4552 - val_MinusLogProbMetric: 18.4552 - lr: 1.1111e-04 - 79s/epoch - 403ms/step
Epoch 497/1000
2023-09-27 19:54:42.469 
Epoch 497/1000 
	 loss: 18.2832, MinusLogProbMetric: 18.2832, val_loss: 18.6409, val_MinusLogProbMetric: 18.6409

Epoch 497: val_loss did not improve from 18.29642
196/196 - 78s - loss: 18.2832 - MinusLogProbMetric: 18.2832 - val_loss: 18.6409 - val_MinusLogProbMetric: 18.6409 - lr: 1.1111e-04 - 78s/epoch - 400ms/step
Epoch 498/1000
2023-09-27 19:55:59.915 
Epoch 498/1000 
	 loss: 18.2666, MinusLogProbMetric: 18.2666, val_loss: 18.4548, val_MinusLogProbMetric: 18.4548

Epoch 498: val_loss did not improve from 18.29642
196/196 - 77s - loss: 18.2666 - MinusLogProbMetric: 18.2666 - val_loss: 18.4548 - val_MinusLogProbMetric: 18.4548 - lr: 1.1111e-04 - 77s/epoch - 395ms/step
Epoch 499/1000
2023-09-27 19:57:17.997 
Epoch 499/1000 
	 loss: 18.2397, MinusLogProbMetric: 18.2397, val_loss: 18.3723, val_MinusLogProbMetric: 18.3723

Epoch 499: val_loss did not improve from 18.29642
196/196 - 78s - loss: 18.2397 - MinusLogProbMetric: 18.2397 - val_loss: 18.3723 - val_MinusLogProbMetric: 18.3723 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 500/1000
2023-09-27 19:58:36.617 
Epoch 500/1000 
	 loss: 18.2748, MinusLogProbMetric: 18.2748, val_loss: 18.3181, val_MinusLogProbMetric: 18.3181

Epoch 500: val_loss did not improve from 18.29642
196/196 - 79s - loss: 18.2748 - MinusLogProbMetric: 18.2748 - val_loss: 18.3181 - val_MinusLogProbMetric: 18.3181 - lr: 1.1111e-04 - 79s/epoch - 401ms/step
Epoch 501/1000
2023-09-27 19:59:54.663 
Epoch 501/1000 
	 loss: 18.2432, MinusLogProbMetric: 18.2432, val_loss: 18.5320, val_MinusLogProbMetric: 18.5320

Epoch 501: val_loss did not improve from 18.29642
196/196 - 78s - loss: 18.2432 - MinusLogProbMetric: 18.2432 - val_loss: 18.5320 - val_MinusLogProbMetric: 18.5320 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 502/1000
2023-09-27 20:01:13.227 
Epoch 502/1000 
	 loss: 18.2347, MinusLogProbMetric: 18.2347, val_loss: 18.3936, val_MinusLogProbMetric: 18.3936

Epoch 502: val_loss did not improve from 18.29642
196/196 - 79s - loss: 18.2347 - MinusLogProbMetric: 18.2347 - val_loss: 18.3936 - val_MinusLogProbMetric: 18.3936 - lr: 1.1111e-04 - 79s/epoch - 401ms/step
Epoch 503/1000
2023-09-27 20:02:30.781 
Epoch 503/1000 
	 loss: 18.2762, MinusLogProbMetric: 18.2762, val_loss: 18.4761, val_MinusLogProbMetric: 18.4761

Epoch 503: val_loss did not improve from 18.29642
196/196 - 78s - loss: 18.2762 - MinusLogProbMetric: 18.2762 - val_loss: 18.4761 - val_MinusLogProbMetric: 18.4761 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 504/1000
2023-09-27 20:03:49.363 
Epoch 504/1000 
	 loss: 18.2409, MinusLogProbMetric: 18.2409, val_loss: 18.2630, val_MinusLogProbMetric: 18.2630

Epoch 504: val_loss improved from 18.29642 to 18.26298, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 80s - loss: 18.2409 - MinusLogProbMetric: 18.2409 - val_loss: 18.2630 - val_MinusLogProbMetric: 18.2630 - lr: 1.1111e-04 - 80s/epoch - 408ms/step
Epoch 505/1000
2023-09-27 20:05:09.555 
Epoch 505/1000 
	 loss: 18.2536, MinusLogProbMetric: 18.2536, val_loss: 18.3753, val_MinusLogProbMetric: 18.3753

Epoch 505: val_loss did not improve from 18.26298
196/196 - 79s - loss: 18.2536 - MinusLogProbMetric: 18.2536 - val_loss: 18.3753 - val_MinusLogProbMetric: 18.3753 - lr: 1.1111e-04 - 79s/epoch - 402ms/step
Epoch 506/1000
2023-09-27 20:06:28.864 
Epoch 506/1000 
	 loss: 18.2379, MinusLogProbMetric: 18.2379, val_loss: 18.3108, val_MinusLogProbMetric: 18.3108

Epoch 506: val_loss did not improve from 18.26298
196/196 - 79s - loss: 18.2379 - MinusLogProbMetric: 18.2379 - val_loss: 18.3108 - val_MinusLogProbMetric: 18.3108 - lr: 1.1111e-04 - 79s/epoch - 405ms/step
Epoch 507/1000
2023-09-27 20:07:46.865 
Epoch 507/1000 
	 loss: 18.2433, MinusLogProbMetric: 18.2433, val_loss: 18.3083, val_MinusLogProbMetric: 18.3083

Epoch 507: val_loss did not improve from 18.26298
196/196 - 78s - loss: 18.2433 - MinusLogProbMetric: 18.2433 - val_loss: 18.3083 - val_MinusLogProbMetric: 18.3083 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 508/1000
2023-09-27 20:09:04.370 
Epoch 508/1000 
	 loss: 18.2327, MinusLogProbMetric: 18.2327, val_loss: 18.2350, val_MinusLogProbMetric: 18.2350

Epoch 508: val_loss improved from 18.26298 to 18.23501, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 79s - loss: 18.2327 - MinusLogProbMetric: 18.2327 - val_loss: 18.2350 - val_MinusLogProbMetric: 18.2350 - lr: 1.1111e-04 - 79s/epoch - 402ms/step
Epoch 509/1000
2023-09-27 20:10:24.808 
Epoch 509/1000 
	 loss: 18.2203, MinusLogProbMetric: 18.2203, val_loss: 18.2885, val_MinusLogProbMetric: 18.2885

Epoch 509: val_loss did not improve from 18.23501
196/196 - 79s - loss: 18.2203 - MinusLogProbMetric: 18.2203 - val_loss: 18.2885 - val_MinusLogProbMetric: 18.2885 - lr: 1.1111e-04 - 79s/epoch - 403ms/step
Epoch 510/1000
2023-09-27 20:11:43.436 
Epoch 510/1000 
	 loss: 18.2147, MinusLogProbMetric: 18.2147, val_loss: 18.3439, val_MinusLogProbMetric: 18.3439

Epoch 510: val_loss did not improve from 18.23501
196/196 - 79s - loss: 18.2147 - MinusLogProbMetric: 18.2147 - val_loss: 18.3439 - val_MinusLogProbMetric: 18.3439 - lr: 1.1111e-04 - 79s/epoch - 401ms/step
Epoch 511/1000
2023-09-27 20:13:02.775 
Epoch 511/1000 
	 loss: 18.2383, MinusLogProbMetric: 18.2383, val_loss: 18.2939, val_MinusLogProbMetric: 18.2939

Epoch 511: val_loss did not improve from 18.23501
196/196 - 79s - loss: 18.2383 - MinusLogProbMetric: 18.2383 - val_loss: 18.2939 - val_MinusLogProbMetric: 18.2939 - lr: 1.1111e-04 - 79s/epoch - 405ms/step
Epoch 512/1000
2023-09-27 20:14:21.629 
Epoch 512/1000 
	 loss: 18.2177, MinusLogProbMetric: 18.2177, val_loss: 18.3646, val_MinusLogProbMetric: 18.3646

Epoch 512: val_loss did not improve from 18.23501
196/196 - 79s - loss: 18.2177 - MinusLogProbMetric: 18.2177 - val_loss: 18.3646 - val_MinusLogProbMetric: 18.3646 - lr: 1.1111e-04 - 79s/epoch - 402ms/step
Epoch 513/1000
2023-09-27 20:15:40.617 
Epoch 513/1000 
	 loss: 18.2083, MinusLogProbMetric: 18.2083, val_loss: 18.2429, val_MinusLogProbMetric: 18.2429

Epoch 513: val_loss did not improve from 18.23501
196/196 - 79s - loss: 18.2083 - MinusLogProbMetric: 18.2083 - val_loss: 18.2429 - val_MinusLogProbMetric: 18.2429 - lr: 1.1111e-04 - 79s/epoch - 403ms/step
Epoch 514/1000
2023-09-27 20:16:59.925 
Epoch 514/1000 
	 loss: 18.2041, MinusLogProbMetric: 18.2041, val_loss: 18.3364, val_MinusLogProbMetric: 18.3364

Epoch 514: val_loss did not improve from 18.23501
196/196 - 79s - loss: 18.2041 - MinusLogProbMetric: 18.2041 - val_loss: 18.3364 - val_MinusLogProbMetric: 18.3364 - lr: 1.1111e-04 - 79s/epoch - 405ms/step
Epoch 515/1000
2023-09-27 20:18:19.014 
Epoch 515/1000 
	 loss: 18.2003, MinusLogProbMetric: 18.2003, val_loss: 18.2832, val_MinusLogProbMetric: 18.2832

Epoch 515: val_loss did not improve from 18.23501
196/196 - 79s - loss: 18.2003 - MinusLogProbMetric: 18.2003 - val_loss: 18.2832 - val_MinusLogProbMetric: 18.2832 - lr: 1.1111e-04 - 79s/epoch - 403ms/step
Epoch 516/1000
2023-09-27 20:19:38.207 
Epoch 516/1000 
	 loss: 18.2213, MinusLogProbMetric: 18.2213, val_loss: 18.3649, val_MinusLogProbMetric: 18.3649

Epoch 516: val_loss did not improve from 18.23501
196/196 - 79s - loss: 18.2213 - MinusLogProbMetric: 18.2213 - val_loss: 18.3649 - val_MinusLogProbMetric: 18.3649 - lr: 1.1111e-04 - 79s/epoch - 404ms/step
Epoch 517/1000
2023-09-27 20:20:56.571 
Epoch 517/1000 
	 loss: 18.2060, MinusLogProbMetric: 18.2060, val_loss: 18.3546, val_MinusLogProbMetric: 18.3546

Epoch 517: val_loss did not improve from 18.23501
196/196 - 78s - loss: 18.2060 - MinusLogProbMetric: 18.2060 - val_loss: 18.3546 - val_MinusLogProbMetric: 18.3546 - lr: 1.1111e-04 - 78s/epoch - 400ms/step
Epoch 518/1000
2023-09-27 20:22:15.016 
Epoch 518/1000 
	 loss: 18.1925, MinusLogProbMetric: 18.1925, val_loss: 18.2513, val_MinusLogProbMetric: 18.2513

Epoch 518: val_loss did not improve from 18.23501
196/196 - 78s - loss: 18.1925 - MinusLogProbMetric: 18.1925 - val_loss: 18.2513 - val_MinusLogProbMetric: 18.2513 - lr: 1.1111e-04 - 78s/epoch - 400ms/step
Epoch 519/1000
2023-09-27 20:23:33.623 
Epoch 519/1000 
	 loss: 18.2149, MinusLogProbMetric: 18.2149, val_loss: 18.3531, val_MinusLogProbMetric: 18.3531

Epoch 519: val_loss did not improve from 18.23501
196/196 - 79s - loss: 18.2149 - MinusLogProbMetric: 18.2149 - val_loss: 18.3531 - val_MinusLogProbMetric: 18.3531 - lr: 1.1111e-04 - 79s/epoch - 401ms/step
Epoch 520/1000
2023-09-27 20:24:51.557 
Epoch 520/1000 
	 loss: 18.2012, MinusLogProbMetric: 18.2012, val_loss: 18.3006, val_MinusLogProbMetric: 18.3006

Epoch 520: val_loss did not improve from 18.23501
196/196 - 78s - loss: 18.2012 - MinusLogProbMetric: 18.2012 - val_loss: 18.3006 - val_MinusLogProbMetric: 18.3006 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 521/1000
2023-09-27 20:26:09.732 
Epoch 521/1000 
	 loss: 18.2089, MinusLogProbMetric: 18.2089, val_loss: 18.2482, val_MinusLogProbMetric: 18.2482

Epoch 521: val_loss did not improve from 18.23501
196/196 - 78s - loss: 18.2089 - MinusLogProbMetric: 18.2089 - val_loss: 18.2482 - val_MinusLogProbMetric: 18.2482 - lr: 1.1111e-04 - 78s/epoch - 399ms/step
Epoch 522/1000
2023-09-27 20:27:28.170 
Epoch 522/1000 
	 loss: 18.1725, MinusLogProbMetric: 18.1725, val_loss: 18.4586, val_MinusLogProbMetric: 18.4586

Epoch 522: val_loss did not improve from 18.23501
196/196 - 78s - loss: 18.1725 - MinusLogProbMetric: 18.1725 - val_loss: 18.4586 - val_MinusLogProbMetric: 18.4586 - lr: 1.1111e-04 - 78s/epoch - 400ms/step
Epoch 523/1000
2023-09-27 20:28:47.314 
Epoch 523/1000 
	 loss: 18.1977, MinusLogProbMetric: 18.1977, val_loss: 18.3766, val_MinusLogProbMetric: 18.3766

Epoch 523: val_loss did not improve from 18.23501
196/196 - 79s - loss: 18.1977 - MinusLogProbMetric: 18.1977 - val_loss: 18.3766 - val_MinusLogProbMetric: 18.3766 - lr: 1.1111e-04 - 79s/epoch - 404ms/step
Epoch 524/1000
2023-09-27 20:30:05.528 
Epoch 524/1000 
	 loss: 18.1838, MinusLogProbMetric: 18.1838, val_loss: 18.4789, val_MinusLogProbMetric: 18.4789

Epoch 524: val_loss did not improve from 18.23501
196/196 - 78s - loss: 18.1838 - MinusLogProbMetric: 18.1838 - val_loss: 18.4789 - val_MinusLogProbMetric: 18.4789 - lr: 1.1111e-04 - 78s/epoch - 399ms/step
Epoch 525/1000
2023-09-27 20:31:24.508 
Epoch 525/1000 
	 loss: 18.1994, MinusLogProbMetric: 18.1994, val_loss: 18.2649, val_MinusLogProbMetric: 18.2649

Epoch 525: val_loss did not improve from 18.23501
196/196 - 79s - loss: 18.1994 - MinusLogProbMetric: 18.1994 - val_loss: 18.2649 - val_MinusLogProbMetric: 18.2649 - lr: 1.1111e-04 - 79s/epoch - 403ms/step
Epoch 526/1000
2023-09-27 20:32:42.129 
Epoch 526/1000 
	 loss: 18.1766, MinusLogProbMetric: 18.1766, val_loss: 18.2067, val_MinusLogProbMetric: 18.2067

Epoch 526: val_loss improved from 18.23501 to 18.20671, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 79s - loss: 18.1766 - MinusLogProbMetric: 18.1766 - val_loss: 18.2067 - val_MinusLogProbMetric: 18.2067 - lr: 1.1111e-04 - 79s/epoch - 403ms/step
Epoch 527/1000
2023-09-27 20:34:01.954 
Epoch 527/1000 
	 loss: 18.1712, MinusLogProbMetric: 18.1712, val_loss: 18.2813, val_MinusLogProbMetric: 18.2813

Epoch 527: val_loss did not improve from 18.20671
196/196 - 78s - loss: 18.1712 - MinusLogProbMetric: 18.1712 - val_loss: 18.2813 - val_MinusLogProbMetric: 18.2813 - lr: 1.1111e-04 - 78s/epoch - 400ms/step
Epoch 528/1000
2023-09-27 20:35:21.249 
Epoch 528/1000 
	 loss: 18.1491, MinusLogProbMetric: 18.1491, val_loss: 18.2928, val_MinusLogProbMetric: 18.2928

Epoch 528: val_loss did not improve from 18.20671
196/196 - 79s - loss: 18.1491 - MinusLogProbMetric: 18.1491 - val_loss: 18.2928 - val_MinusLogProbMetric: 18.2928 - lr: 1.1111e-04 - 79s/epoch - 405ms/step
Epoch 529/1000
2023-09-27 20:36:39.577 
Epoch 529/1000 
	 loss: 18.1729, MinusLogProbMetric: 18.1729, val_loss: 18.2816, val_MinusLogProbMetric: 18.2816

Epoch 529: val_loss did not improve from 18.20671
196/196 - 78s - loss: 18.1729 - MinusLogProbMetric: 18.1729 - val_loss: 18.2816 - val_MinusLogProbMetric: 18.2816 - lr: 1.1111e-04 - 78s/epoch - 400ms/step
Epoch 530/1000
2023-09-27 20:37:56.540 
Epoch 530/1000 
	 loss: 18.1773, MinusLogProbMetric: 18.1773, val_loss: 18.2347, val_MinusLogProbMetric: 18.2347

Epoch 530: val_loss did not improve from 18.20671
196/196 - 77s - loss: 18.1773 - MinusLogProbMetric: 18.1773 - val_loss: 18.2347 - val_MinusLogProbMetric: 18.2347 - lr: 1.1111e-04 - 77s/epoch - 393ms/step
Epoch 531/1000
2023-09-27 20:39:14.700 
Epoch 531/1000 
	 loss: 18.1718, MinusLogProbMetric: 18.1718, val_loss: 18.1834, val_MinusLogProbMetric: 18.1834

Epoch 531: val_loss improved from 18.20671 to 18.18342, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 79s - loss: 18.1718 - MinusLogProbMetric: 18.1718 - val_loss: 18.1834 - val_MinusLogProbMetric: 18.1834 - lr: 1.1111e-04 - 79s/epoch - 405ms/step
Epoch 532/1000
2023-09-27 20:40:34.215 
Epoch 532/1000 
	 loss: 18.1459, MinusLogProbMetric: 18.1459, val_loss: 18.3706, val_MinusLogProbMetric: 18.3706

Epoch 532: val_loss did not improve from 18.18342
196/196 - 78s - loss: 18.1459 - MinusLogProbMetric: 18.1459 - val_loss: 18.3706 - val_MinusLogProbMetric: 18.3706 - lr: 1.1111e-04 - 78s/epoch - 399ms/step
Epoch 533/1000
2023-09-27 20:41:51.974 
Epoch 533/1000 
	 loss: 18.1637, MinusLogProbMetric: 18.1637, val_loss: 18.2428, val_MinusLogProbMetric: 18.2428

Epoch 533: val_loss did not improve from 18.18342
196/196 - 78s - loss: 18.1637 - MinusLogProbMetric: 18.1637 - val_loss: 18.2428 - val_MinusLogProbMetric: 18.2428 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 534/1000
2023-09-27 20:43:09.524 
Epoch 534/1000 
	 loss: 18.1559, MinusLogProbMetric: 18.1559, val_loss: 18.4749, val_MinusLogProbMetric: 18.4749

Epoch 534: val_loss did not improve from 18.18342
196/196 - 78s - loss: 18.1559 - MinusLogProbMetric: 18.1559 - val_loss: 18.4749 - val_MinusLogProbMetric: 18.4749 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 535/1000
2023-09-27 20:44:23.967 
Epoch 535/1000 
	 loss: 18.1638, MinusLogProbMetric: 18.1638, val_loss: 18.1983, val_MinusLogProbMetric: 18.1983

Epoch 535: val_loss did not improve from 18.18342
196/196 - 74s - loss: 18.1638 - MinusLogProbMetric: 18.1638 - val_loss: 18.1983 - val_MinusLogProbMetric: 18.1983 - lr: 1.1111e-04 - 74s/epoch - 380ms/step
Epoch 536/1000
2023-09-27 20:45:41.459 
Epoch 536/1000 
	 loss: 18.1435, MinusLogProbMetric: 18.1435, val_loss: 18.1653, val_MinusLogProbMetric: 18.1653

Epoch 536: val_loss improved from 18.18342 to 18.16529, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 79s - loss: 18.1435 - MinusLogProbMetric: 18.1435 - val_loss: 18.1653 - val_MinusLogProbMetric: 18.1653 - lr: 1.1111e-04 - 79s/epoch - 401ms/step
Epoch 537/1000
2023-09-27 20:47:00.973 
Epoch 537/1000 
	 loss: 18.1413, MinusLogProbMetric: 18.1413, val_loss: 18.3693, val_MinusLogProbMetric: 18.3693

Epoch 537: val_loss did not improve from 18.16529
196/196 - 78s - loss: 18.1413 - MinusLogProbMetric: 18.1413 - val_loss: 18.3693 - val_MinusLogProbMetric: 18.3693 - lr: 1.1111e-04 - 78s/epoch - 400ms/step
Epoch 538/1000
2023-09-27 20:48:18.390 
Epoch 538/1000 
	 loss: 18.1556, MinusLogProbMetric: 18.1556, val_loss: 18.2584, val_MinusLogProbMetric: 18.2584

Epoch 538: val_loss did not improve from 18.16529
196/196 - 77s - loss: 18.1556 - MinusLogProbMetric: 18.1556 - val_loss: 18.2584 - val_MinusLogProbMetric: 18.2584 - lr: 1.1111e-04 - 77s/epoch - 395ms/step
Epoch 539/1000
2023-09-27 20:49:36.505 
Epoch 539/1000 
	 loss: 18.1668, MinusLogProbMetric: 18.1668, val_loss: 18.2752, val_MinusLogProbMetric: 18.2752

Epoch 539: val_loss did not improve from 18.16529
196/196 - 78s - loss: 18.1668 - MinusLogProbMetric: 18.1668 - val_loss: 18.2752 - val_MinusLogProbMetric: 18.2752 - lr: 1.1111e-04 - 78s/epoch - 399ms/step
Epoch 540/1000
2023-09-27 20:50:54.401 
Epoch 540/1000 
	 loss: 18.1260, MinusLogProbMetric: 18.1260, val_loss: 18.2393, val_MinusLogProbMetric: 18.2393

Epoch 540: val_loss did not improve from 18.16529
196/196 - 78s - loss: 18.1260 - MinusLogProbMetric: 18.1260 - val_loss: 18.2393 - val_MinusLogProbMetric: 18.2393 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 541/1000
2023-09-27 20:52:12.333 
Epoch 541/1000 
	 loss: 18.1468, MinusLogProbMetric: 18.1468, val_loss: 18.2116, val_MinusLogProbMetric: 18.2116

Epoch 541: val_loss did not improve from 18.16529
196/196 - 78s - loss: 18.1468 - MinusLogProbMetric: 18.1468 - val_loss: 18.2116 - val_MinusLogProbMetric: 18.2116 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 542/1000
2023-09-27 20:53:30.393 
Epoch 542/1000 
	 loss: 18.1393, MinusLogProbMetric: 18.1393, val_loss: 18.2848, val_MinusLogProbMetric: 18.2848

Epoch 542: val_loss did not improve from 18.16529
196/196 - 78s - loss: 18.1393 - MinusLogProbMetric: 18.1393 - val_loss: 18.2848 - val_MinusLogProbMetric: 18.2848 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 543/1000
2023-09-27 20:54:48.794 
Epoch 543/1000 
	 loss: 18.1180, MinusLogProbMetric: 18.1180, val_loss: 18.2985, val_MinusLogProbMetric: 18.2985

Epoch 543: val_loss did not improve from 18.16529
196/196 - 78s - loss: 18.1180 - MinusLogProbMetric: 18.1180 - val_loss: 18.2985 - val_MinusLogProbMetric: 18.2985 - lr: 1.1111e-04 - 78s/epoch - 400ms/step
Epoch 544/1000
2023-09-27 20:56:06.992 
Epoch 544/1000 
	 loss: 18.1339, MinusLogProbMetric: 18.1339, val_loss: 18.1944, val_MinusLogProbMetric: 18.1944

Epoch 544: val_loss did not improve from 18.16529
196/196 - 78s - loss: 18.1339 - MinusLogProbMetric: 18.1339 - val_loss: 18.1944 - val_MinusLogProbMetric: 18.1944 - lr: 1.1111e-04 - 78s/epoch - 399ms/step
Epoch 545/1000
2023-09-27 20:57:26.156 
Epoch 545/1000 
	 loss: 18.1628, MinusLogProbMetric: 18.1628, val_loss: 18.2668, val_MinusLogProbMetric: 18.2668

Epoch 545: val_loss did not improve from 18.16529
196/196 - 79s - loss: 18.1628 - MinusLogProbMetric: 18.1628 - val_loss: 18.2668 - val_MinusLogProbMetric: 18.2668 - lr: 1.1111e-04 - 79s/epoch - 404ms/step
Epoch 546/1000
2023-09-27 20:58:43.915 
Epoch 546/1000 
	 loss: 18.1461, MinusLogProbMetric: 18.1461, val_loss: 18.1963, val_MinusLogProbMetric: 18.1963

Epoch 546: val_loss did not improve from 18.16529
196/196 - 78s - loss: 18.1461 - MinusLogProbMetric: 18.1461 - val_loss: 18.1963 - val_MinusLogProbMetric: 18.1963 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 547/1000
2023-09-27 21:00:02.356 
Epoch 547/1000 
	 loss: 18.1099, MinusLogProbMetric: 18.1099, val_loss: 18.2958, val_MinusLogProbMetric: 18.2958

Epoch 547: val_loss did not improve from 18.16529
196/196 - 78s - loss: 18.1099 - MinusLogProbMetric: 18.1099 - val_loss: 18.2958 - val_MinusLogProbMetric: 18.2958 - lr: 1.1111e-04 - 78s/epoch - 400ms/step
Epoch 548/1000
2023-09-27 21:01:20.969 
Epoch 548/1000 
	 loss: 18.1106, MinusLogProbMetric: 18.1106, val_loss: 18.2813, val_MinusLogProbMetric: 18.2813

Epoch 548: val_loss did not improve from 18.16529
196/196 - 79s - loss: 18.1106 - MinusLogProbMetric: 18.1106 - val_loss: 18.2813 - val_MinusLogProbMetric: 18.2813 - lr: 1.1111e-04 - 79s/epoch - 401ms/step
Epoch 549/1000
2023-09-27 21:02:39.332 
Epoch 549/1000 
	 loss: 18.1167, MinusLogProbMetric: 18.1167, val_loss: 18.2310, val_MinusLogProbMetric: 18.2310

Epoch 549: val_loss did not improve from 18.16529
196/196 - 78s - loss: 18.1167 - MinusLogProbMetric: 18.1167 - val_loss: 18.2310 - val_MinusLogProbMetric: 18.2310 - lr: 1.1111e-04 - 78s/epoch - 400ms/step
Epoch 550/1000
2023-09-27 21:03:57.575 
Epoch 550/1000 
	 loss: 18.1313, MinusLogProbMetric: 18.1313, val_loss: 18.1909, val_MinusLogProbMetric: 18.1909

Epoch 550: val_loss did not improve from 18.16529
196/196 - 78s - loss: 18.1313 - MinusLogProbMetric: 18.1313 - val_loss: 18.1909 - val_MinusLogProbMetric: 18.1909 - lr: 1.1111e-04 - 78s/epoch - 399ms/step
Epoch 551/1000
2023-09-27 21:05:16.369 
Epoch 551/1000 
	 loss: 18.0955, MinusLogProbMetric: 18.0955, val_loss: 18.2340, val_MinusLogProbMetric: 18.2340

Epoch 551: val_loss did not improve from 18.16529
196/196 - 79s - loss: 18.0955 - MinusLogProbMetric: 18.0955 - val_loss: 18.2340 - val_MinusLogProbMetric: 18.2340 - lr: 1.1111e-04 - 79s/epoch - 402ms/step
Epoch 552/1000
2023-09-27 21:06:33.286 
Epoch 552/1000 
	 loss: 18.1118, MinusLogProbMetric: 18.1118, val_loss: 18.3255, val_MinusLogProbMetric: 18.3255

Epoch 552: val_loss did not improve from 18.16529
196/196 - 77s - loss: 18.1118 - MinusLogProbMetric: 18.1118 - val_loss: 18.3255 - val_MinusLogProbMetric: 18.3255 - lr: 1.1111e-04 - 77s/epoch - 392ms/step
Epoch 553/1000
2023-09-27 21:07:50.118 
Epoch 553/1000 
	 loss: 18.1315, MinusLogProbMetric: 18.1315, val_loss: 18.1717, val_MinusLogProbMetric: 18.1717

Epoch 553: val_loss did not improve from 18.16529
196/196 - 77s - loss: 18.1315 - MinusLogProbMetric: 18.1315 - val_loss: 18.1717 - val_MinusLogProbMetric: 18.1717 - lr: 1.1111e-04 - 77s/epoch - 392ms/step
Epoch 554/1000
2023-09-27 21:09:07.548 
Epoch 554/1000 
	 loss: 18.1164, MinusLogProbMetric: 18.1164, val_loss: 18.2751, val_MinusLogProbMetric: 18.2751

Epoch 554: val_loss did not improve from 18.16529
196/196 - 77s - loss: 18.1164 - MinusLogProbMetric: 18.1164 - val_loss: 18.2751 - val_MinusLogProbMetric: 18.2751 - lr: 1.1111e-04 - 77s/epoch - 395ms/step
Epoch 555/1000
2023-09-27 21:10:25.279 
Epoch 555/1000 
	 loss: 18.1139, MinusLogProbMetric: 18.1139, val_loss: 18.3233, val_MinusLogProbMetric: 18.3233

Epoch 555: val_loss did not improve from 18.16529
196/196 - 78s - loss: 18.1139 - MinusLogProbMetric: 18.1139 - val_loss: 18.3233 - val_MinusLogProbMetric: 18.3233 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 556/1000
2023-09-27 21:11:42.793 
Epoch 556/1000 
	 loss: 18.1181, MinusLogProbMetric: 18.1181, val_loss: 18.2020, val_MinusLogProbMetric: 18.2020

Epoch 556: val_loss did not improve from 18.16529
196/196 - 78s - loss: 18.1181 - MinusLogProbMetric: 18.1181 - val_loss: 18.2020 - val_MinusLogProbMetric: 18.2020 - lr: 1.1111e-04 - 78s/epoch - 395ms/step
Epoch 557/1000
2023-09-27 21:13:01.094 
Epoch 557/1000 
	 loss: 18.0787, MinusLogProbMetric: 18.0787, val_loss: 18.2813, val_MinusLogProbMetric: 18.2813

Epoch 557: val_loss did not improve from 18.16529
196/196 - 78s - loss: 18.0787 - MinusLogProbMetric: 18.0787 - val_loss: 18.2813 - val_MinusLogProbMetric: 18.2813 - lr: 1.1111e-04 - 78s/epoch - 399ms/step
Epoch 558/1000
2023-09-27 21:14:18.536 
Epoch 558/1000 
	 loss: 18.1038, MinusLogProbMetric: 18.1038, val_loss: 18.1919, val_MinusLogProbMetric: 18.1919

Epoch 558: val_loss did not improve from 18.16529
196/196 - 77s - loss: 18.1038 - MinusLogProbMetric: 18.1038 - val_loss: 18.1919 - val_MinusLogProbMetric: 18.1919 - lr: 1.1111e-04 - 77s/epoch - 395ms/step
Epoch 559/1000
2023-09-27 21:15:35.679 
Epoch 559/1000 
	 loss: 18.0680, MinusLogProbMetric: 18.0680, val_loss: 18.2259, val_MinusLogProbMetric: 18.2259

Epoch 559: val_loss did not improve from 18.16529
196/196 - 77s - loss: 18.0680 - MinusLogProbMetric: 18.0680 - val_loss: 18.2259 - val_MinusLogProbMetric: 18.2259 - lr: 1.1111e-04 - 77s/epoch - 394ms/step
Epoch 560/1000
2023-09-27 21:16:54.630 
Epoch 560/1000 
	 loss: 18.1083, MinusLogProbMetric: 18.1083, val_loss: 18.2862, val_MinusLogProbMetric: 18.2862

Epoch 560: val_loss did not improve from 18.16529
196/196 - 79s - loss: 18.1083 - MinusLogProbMetric: 18.1083 - val_loss: 18.2862 - val_MinusLogProbMetric: 18.2862 - lr: 1.1111e-04 - 79s/epoch - 403ms/step
Epoch 561/1000
2023-09-27 21:18:12.996 
Epoch 561/1000 
	 loss: 18.1003, MinusLogProbMetric: 18.1003, val_loss: 18.1225, val_MinusLogProbMetric: 18.1225

Epoch 561: val_loss improved from 18.16529 to 18.12253, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 80s - loss: 18.1003 - MinusLogProbMetric: 18.1003 - val_loss: 18.1225 - val_MinusLogProbMetric: 18.1225 - lr: 1.1111e-04 - 80s/epoch - 407ms/step
Epoch 562/1000
2023-09-27 21:19:32.873 
Epoch 562/1000 
	 loss: 18.0761, MinusLogProbMetric: 18.0761, val_loss: 18.2138, val_MinusLogProbMetric: 18.2138

Epoch 562: val_loss did not improve from 18.12253
196/196 - 78s - loss: 18.0761 - MinusLogProbMetric: 18.0761 - val_loss: 18.2138 - val_MinusLogProbMetric: 18.2138 - lr: 1.1111e-04 - 78s/epoch - 400ms/step
Epoch 563/1000
2023-09-27 21:20:52.076 
Epoch 563/1000 
	 loss: 18.1046, MinusLogProbMetric: 18.1046, val_loss: 18.1818, val_MinusLogProbMetric: 18.1818

Epoch 563: val_loss did not improve from 18.12253
196/196 - 79s - loss: 18.1046 - MinusLogProbMetric: 18.1046 - val_loss: 18.1818 - val_MinusLogProbMetric: 18.1818 - lr: 1.1111e-04 - 79s/epoch - 404ms/step
Epoch 564/1000
2023-09-27 21:22:09.474 
Epoch 564/1000 
	 loss: 18.0934, MinusLogProbMetric: 18.0934, val_loss: 18.1766, val_MinusLogProbMetric: 18.1766

Epoch 564: val_loss did not improve from 18.12253
196/196 - 77s - loss: 18.0934 - MinusLogProbMetric: 18.0934 - val_loss: 18.1766 - val_MinusLogProbMetric: 18.1766 - lr: 1.1111e-04 - 77s/epoch - 395ms/step
Epoch 565/1000
2023-09-27 21:23:27.668 
Epoch 565/1000 
	 loss: 18.0762, MinusLogProbMetric: 18.0762, val_loss: 18.3377, val_MinusLogProbMetric: 18.3377

Epoch 565: val_loss did not improve from 18.12253
196/196 - 78s - loss: 18.0762 - MinusLogProbMetric: 18.0762 - val_loss: 18.3377 - val_MinusLogProbMetric: 18.3377 - lr: 1.1111e-04 - 78s/epoch - 399ms/step
Epoch 566/1000
2023-09-27 21:24:45.286 
Epoch 566/1000 
	 loss: 18.0950, MinusLogProbMetric: 18.0950, val_loss: 18.1821, val_MinusLogProbMetric: 18.1821

Epoch 566: val_loss did not improve from 18.12253
196/196 - 78s - loss: 18.0950 - MinusLogProbMetric: 18.0950 - val_loss: 18.1821 - val_MinusLogProbMetric: 18.1821 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 567/1000
2023-09-27 21:26:03.714 
Epoch 567/1000 
	 loss: 18.0961, MinusLogProbMetric: 18.0961, val_loss: 18.2233, val_MinusLogProbMetric: 18.2233

Epoch 567: val_loss did not improve from 18.12253
196/196 - 78s - loss: 18.0961 - MinusLogProbMetric: 18.0961 - val_loss: 18.2233 - val_MinusLogProbMetric: 18.2233 - lr: 1.1111e-04 - 78s/epoch - 400ms/step
Epoch 568/1000
2023-09-27 21:27:22.567 
Epoch 568/1000 
	 loss: 18.0727, MinusLogProbMetric: 18.0727, val_loss: 18.2236, val_MinusLogProbMetric: 18.2236

Epoch 568: val_loss did not improve from 18.12253
196/196 - 79s - loss: 18.0727 - MinusLogProbMetric: 18.0727 - val_loss: 18.2236 - val_MinusLogProbMetric: 18.2236 - lr: 1.1111e-04 - 79s/epoch - 402ms/step
Epoch 569/1000
2023-09-27 21:28:41.142 
Epoch 569/1000 
	 loss: 18.0721, MinusLogProbMetric: 18.0721, val_loss: 18.1867, val_MinusLogProbMetric: 18.1867

Epoch 569: val_loss did not improve from 18.12253
196/196 - 79s - loss: 18.0721 - MinusLogProbMetric: 18.0721 - val_loss: 18.1867 - val_MinusLogProbMetric: 18.1867 - lr: 1.1111e-04 - 79s/epoch - 401ms/step
Epoch 570/1000
2023-09-27 21:30:00.442 
Epoch 570/1000 
	 loss: 18.0839, MinusLogProbMetric: 18.0839, val_loss: 18.2098, val_MinusLogProbMetric: 18.2098

Epoch 570: val_loss did not improve from 18.12253
196/196 - 79s - loss: 18.0839 - MinusLogProbMetric: 18.0839 - val_loss: 18.2098 - val_MinusLogProbMetric: 18.2098 - lr: 1.1111e-04 - 79s/epoch - 405ms/step
Epoch 571/1000
2023-09-27 21:31:18.591 
Epoch 571/1000 
	 loss: 18.0827, MinusLogProbMetric: 18.0827, val_loss: 18.0797, val_MinusLogProbMetric: 18.0797

Epoch 571: val_loss improved from 18.12253 to 18.07966, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 79s - loss: 18.0827 - MinusLogProbMetric: 18.0827 - val_loss: 18.0797 - val_MinusLogProbMetric: 18.0797 - lr: 1.1111e-04 - 79s/epoch - 406ms/step
Epoch 572/1000
2023-09-27 21:32:38.414 
Epoch 572/1000 
	 loss: 18.0665, MinusLogProbMetric: 18.0665, val_loss: 18.1658, val_MinusLogProbMetric: 18.1658

Epoch 572: val_loss did not improve from 18.07966
196/196 - 78s - loss: 18.0665 - MinusLogProbMetric: 18.0665 - val_loss: 18.1658 - val_MinusLogProbMetric: 18.1658 - lr: 1.1111e-04 - 78s/epoch - 400ms/step
Epoch 573/1000
2023-09-27 21:33:57.164 
Epoch 573/1000 
	 loss: 18.0752, MinusLogProbMetric: 18.0752, val_loss: 18.2706, val_MinusLogProbMetric: 18.2706

Epoch 573: val_loss did not improve from 18.07966
196/196 - 79s - loss: 18.0752 - MinusLogProbMetric: 18.0752 - val_loss: 18.2706 - val_MinusLogProbMetric: 18.2706 - lr: 1.1111e-04 - 79s/epoch - 402ms/step
Epoch 574/1000
2023-09-27 21:35:16.027 
Epoch 574/1000 
	 loss: 18.0591, MinusLogProbMetric: 18.0591, val_loss: 18.0962, val_MinusLogProbMetric: 18.0962

Epoch 574: val_loss did not improve from 18.07966
196/196 - 79s - loss: 18.0591 - MinusLogProbMetric: 18.0591 - val_loss: 18.0962 - val_MinusLogProbMetric: 18.0962 - lr: 1.1111e-04 - 79s/epoch - 402ms/step
Epoch 575/1000
2023-09-27 21:36:33.001 
Epoch 575/1000 
	 loss: 18.0721, MinusLogProbMetric: 18.0721, val_loss: 18.1292, val_MinusLogProbMetric: 18.1292

Epoch 575: val_loss did not improve from 18.07966
196/196 - 77s - loss: 18.0721 - MinusLogProbMetric: 18.0721 - val_loss: 18.1292 - val_MinusLogProbMetric: 18.1292 - lr: 1.1111e-04 - 77s/epoch - 393ms/step
Epoch 576/1000
2023-09-27 21:37:51.487 
Epoch 576/1000 
	 loss: 18.0610, MinusLogProbMetric: 18.0610, val_loss: 18.1845, val_MinusLogProbMetric: 18.1845

Epoch 576: val_loss did not improve from 18.07966
196/196 - 78s - loss: 18.0610 - MinusLogProbMetric: 18.0610 - val_loss: 18.1845 - val_MinusLogProbMetric: 18.1845 - lr: 1.1111e-04 - 78s/epoch - 400ms/step
Epoch 577/1000
2023-09-27 21:39:09.545 
Epoch 577/1000 
	 loss: 18.0815, MinusLogProbMetric: 18.0815, val_loss: 18.7128, val_MinusLogProbMetric: 18.7128

Epoch 577: val_loss did not improve from 18.07966
196/196 - 78s - loss: 18.0815 - MinusLogProbMetric: 18.0815 - val_loss: 18.7128 - val_MinusLogProbMetric: 18.7128 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 578/1000
2023-09-27 21:40:27.306 
Epoch 578/1000 
	 loss: 18.0781, MinusLogProbMetric: 18.0781, val_loss: 18.1207, val_MinusLogProbMetric: 18.1207

Epoch 578: val_loss did not improve from 18.07966
196/196 - 78s - loss: 18.0781 - MinusLogProbMetric: 18.0781 - val_loss: 18.1207 - val_MinusLogProbMetric: 18.1207 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 579/1000
2023-09-27 21:41:45.968 
Epoch 579/1000 
	 loss: 18.0635, MinusLogProbMetric: 18.0635, val_loss: 18.2059, val_MinusLogProbMetric: 18.2059

Epoch 579: val_loss did not improve from 18.07966
196/196 - 79s - loss: 18.0635 - MinusLogProbMetric: 18.0635 - val_loss: 18.2059 - val_MinusLogProbMetric: 18.2059 - lr: 1.1111e-04 - 79s/epoch - 401ms/step
Epoch 580/1000
2023-09-27 21:43:03.675 
Epoch 580/1000 
	 loss: 18.0604, MinusLogProbMetric: 18.0604, val_loss: 18.2508, val_MinusLogProbMetric: 18.2508

Epoch 580: val_loss did not improve from 18.07966
196/196 - 78s - loss: 18.0604 - MinusLogProbMetric: 18.0604 - val_loss: 18.2508 - val_MinusLogProbMetric: 18.2508 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 581/1000
2023-09-27 21:44:22.357 
Epoch 581/1000 
	 loss: 18.0605, MinusLogProbMetric: 18.0605, val_loss: 18.1783, val_MinusLogProbMetric: 18.1783

Epoch 581: val_loss did not improve from 18.07966
196/196 - 79s - loss: 18.0605 - MinusLogProbMetric: 18.0605 - val_loss: 18.1783 - val_MinusLogProbMetric: 18.1783 - lr: 1.1111e-04 - 79s/epoch - 401ms/step
Epoch 582/1000
2023-09-27 21:45:39.806 
Epoch 582/1000 
	 loss: 18.0507, MinusLogProbMetric: 18.0507, val_loss: 18.1786, val_MinusLogProbMetric: 18.1786

Epoch 582: val_loss did not improve from 18.07966
196/196 - 77s - loss: 18.0507 - MinusLogProbMetric: 18.0507 - val_loss: 18.1786 - val_MinusLogProbMetric: 18.1786 - lr: 1.1111e-04 - 77s/epoch - 395ms/step
Epoch 583/1000
2023-09-27 21:46:57.524 
Epoch 583/1000 
	 loss: 18.0362, MinusLogProbMetric: 18.0362, val_loss: 18.0844, val_MinusLogProbMetric: 18.0844

Epoch 583: val_loss did not improve from 18.07966
196/196 - 78s - loss: 18.0362 - MinusLogProbMetric: 18.0362 - val_loss: 18.0844 - val_MinusLogProbMetric: 18.0844 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 584/1000
2023-09-27 21:48:16.192 
Epoch 584/1000 
	 loss: 18.0456, MinusLogProbMetric: 18.0456, val_loss: 18.2328, val_MinusLogProbMetric: 18.2328

Epoch 584: val_loss did not improve from 18.07966
196/196 - 79s - loss: 18.0456 - MinusLogProbMetric: 18.0456 - val_loss: 18.2328 - val_MinusLogProbMetric: 18.2328 - lr: 1.1111e-04 - 79s/epoch - 401ms/step
Epoch 585/1000
2023-09-27 21:49:34.143 
Epoch 585/1000 
	 loss: 18.0316, MinusLogProbMetric: 18.0316, val_loss: 18.2677, val_MinusLogProbMetric: 18.2677

Epoch 585: val_loss did not improve from 18.07966
196/196 - 78s - loss: 18.0316 - MinusLogProbMetric: 18.0316 - val_loss: 18.2677 - val_MinusLogProbMetric: 18.2677 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 586/1000
2023-09-27 21:50:51.713 
Epoch 586/1000 
	 loss: 18.0778, MinusLogProbMetric: 18.0778, val_loss: 18.3307, val_MinusLogProbMetric: 18.3307

Epoch 586: val_loss did not improve from 18.07966
196/196 - 78s - loss: 18.0778 - MinusLogProbMetric: 18.0778 - val_loss: 18.3307 - val_MinusLogProbMetric: 18.3307 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 587/1000
2023-09-27 21:52:09.550 
Epoch 587/1000 
	 loss: 18.0557, MinusLogProbMetric: 18.0557, val_loss: 18.1070, val_MinusLogProbMetric: 18.1070

Epoch 587: val_loss did not improve from 18.07966
196/196 - 78s - loss: 18.0557 - MinusLogProbMetric: 18.0557 - val_loss: 18.1070 - val_MinusLogProbMetric: 18.1070 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 588/1000
2023-09-27 21:53:27.366 
Epoch 588/1000 
	 loss: 18.0551, MinusLogProbMetric: 18.0551, val_loss: 18.3373, val_MinusLogProbMetric: 18.3373

Epoch 588: val_loss did not improve from 18.07966
196/196 - 78s - loss: 18.0551 - MinusLogProbMetric: 18.0551 - val_loss: 18.3373 - val_MinusLogProbMetric: 18.3373 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 589/1000
2023-09-27 21:54:46.186 
Epoch 589/1000 
	 loss: 18.0116, MinusLogProbMetric: 18.0116, val_loss: 18.1719, val_MinusLogProbMetric: 18.1719

Epoch 589: val_loss did not improve from 18.07966
196/196 - 79s - loss: 18.0116 - MinusLogProbMetric: 18.0116 - val_loss: 18.1719 - val_MinusLogProbMetric: 18.1719 - lr: 1.1111e-04 - 79s/epoch - 402ms/step
Epoch 590/1000
2023-09-27 21:56:03.240 
Epoch 590/1000 
	 loss: 18.0068, MinusLogProbMetric: 18.0068, val_loss: 18.1735, val_MinusLogProbMetric: 18.1735

Epoch 590: val_loss did not improve from 18.07966
196/196 - 77s - loss: 18.0068 - MinusLogProbMetric: 18.0068 - val_loss: 18.1735 - val_MinusLogProbMetric: 18.1735 - lr: 1.1111e-04 - 77s/epoch - 393ms/step
Epoch 591/1000
2023-09-27 21:57:20.967 
Epoch 591/1000 
	 loss: 18.0319, MinusLogProbMetric: 18.0319, val_loss: 18.2738, val_MinusLogProbMetric: 18.2738

Epoch 591: val_loss did not improve from 18.07966
196/196 - 78s - loss: 18.0319 - MinusLogProbMetric: 18.0319 - val_loss: 18.2738 - val_MinusLogProbMetric: 18.2738 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 592/1000
2023-09-27 21:58:38.507 
Epoch 592/1000 
	 loss: 18.0399, MinusLogProbMetric: 18.0399, val_loss: 18.4445, val_MinusLogProbMetric: 18.4445

Epoch 592: val_loss did not improve from 18.07966
196/196 - 78s - loss: 18.0399 - MinusLogProbMetric: 18.0399 - val_loss: 18.4445 - val_MinusLogProbMetric: 18.4445 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 593/1000
2023-09-27 21:59:56.222 
Epoch 593/1000 
	 loss: 18.0785, MinusLogProbMetric: 18.0785, val_loss: 18.1408, val_MinusLogProbMetric: 18.1408

Epoch 593: val_loss did not improve from 18.07966
196/196 - 78s - loss: 18.0785 - MinusLogProbMetric: 18.0785 - val_loss: 18.1408 - val_MinusLogProbMetric: 18.1408 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 594/1000
2023-09-27 22:01:14.362 
Epoch 594/1000 
	 loss: 18.0276, MinusLogProbMetric: 18.0276, val_loss: 18.1727, val_MinusLogProbMetric: 18.1727

Epoch 594: val_loss did not improve from 18.07966
196/196 - 78s - loss: 18.0276 - MinusLogProbMetric: 18.0276 - val_loss: 18.1727 - val_MinusLogProbMetric: 18.1727 - lr: 1.1111e-04 - 78s/epoch - 399ms/step
Epoch 595/1000
2023-09-27 22:02:33.019 
Epoch 595/1000 
	 loss: 18.0114, MinusLogProbMetric: 18.0114, val_loss: 18.0652, val_MinusLogProbMetric: 18.0652

Epoch 595: val_loss improved from 18.07966 to 18.06520, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 80s - loss: 18.0114 - MinusLogProbMetric: 18.0114 - val_loss: 18.0652 - val_MinusLogProbMetric: 18.0652 - lr: 1.1111e-04 - 80s/epoch - 408ms/step
Epoch 596/1000
2023-09-27 22:03:52.495 
Epoch 596/1000 
	 loss: 18.0204, MinusLogProbMetric: 18.0204, val_loss: 18.1468, val_MinusLogProbMetric: 18.1468

Epoch 596: val_loss did not improve from 18.06520
196/196 - 78s - loss: 18.0204 - MinusLogProbMetric: 18.0204 - val_loss: 18.1468 - val_MinusLogProbMetric: 18.1468 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 597/1000
2023-09-27 22:05:11.237 
Epoch 597/1000 
	 loss: 18.0191, MinusLogProbMetric: 18.0191, val_loss: 18.2469, val_MinusLogProbMetric: 18.2469

Epoch 597: val_loss did not improve from 18.06520
196/196 - 79s - loss: 18.0191 - MinusLogProbMetric: 18.0191 - val_loss: 18.2469 - val_MinusLogProbMetric: 18.2469 - lr: 1.1111e-04 - 79s/epoch - 402ms/step
Epoch 598/1000
2023-09-27 22:06:29.425 
Epoch 598/1000 
	 loss: 18.0328, MinusLogProbMetric: 18.0328, val_loss: 18.1826, val_MinusLogProbMetric: 18.1826

Epoch 598: val_loss did not improve from 18.06520
196/196 - 78s - loss: 18.0328 - MinusLogProbMetric: 18.0328 - val_loss: 18.1826 - val_MinusLogProbMetric: 18.1826 - lr: 1.1111e-04 - 78s/epoch - 399ms/step
Epoch 599/1000
2023-09-27 22:07:48.182 
Epoch 599/1000 
	 loss: 18.0183, MinusLogProbMetric: 18.0183, val_loss: 18.1805, val_MinusLogProbMetric: 18.1805

Epoch 599: val_loss did not improve from 18.06520
196/196 - 79s - loss: 18.0183 - MinusLogProbMetric: 18.0183 - val_loss: 18.1805 - val_MinusLogProbMetric: 18.1805 - lr: 1.1111e-04 - 79s/epoch - 402ms/step
Epoch 600/1000
2023-09-27 22:09:04.973 
Epoch 600/1000 
	 loss: 18.0061, MinusLogProbMetric: 18.0061, val_loss: 18.0754, val_MinusLogProbMetric: 18.0754

Epoch 600: val_loss did not improve from 18.06520
196/196 - 77s - loss: 18.0061 - MinusLogProbMetric: 18.0061 - val_loss: 18.0754 - val_MinusLogProbMetric: 18.0754 - lr: 1.1111e-04 - 77s/epoch - 392ms/step
Epoch 601/1000
2023-09-27 22:10:16.274 
Epoch 601/1000 
	 loss: 18.0341, MinusLogProbMetric: 18.0341, val_loss: 18.2227, val_MinusLogProbMetric: 18.2227

Epoch 601: val_loss did not improve from 18.06520
196/196 - 71s - loss: 18.0341 - MinusLogProbMetric: 18.0341 - val_loss: 18.2227 - val_MinusLogProbMetric: 18.2227 - lr: 1.1111e-04 - 71s/epoch - 364ms/step
Epoch 602/1000
2023-09-27 22:11:34.254 
Epoch 602/1000 
	 loss: 18.0054, MinusLogProbMetric: 18.0054, val_loss: 18.1432, val_MinusLogProbMetric: 18.1432

Epoch 602: val_loss did not improve from 18.06520
196/196 - 78s - loss: 18.0054 - MinusLogProbMetric: 18.0054 - val_loss: 18.1432 - val_MinusLogProbMetric: 18.1432 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 603/1000
2023-09-27 22:12:52.217 
Epoch 603/1000 
	 loss: 18.0113, MinusLogProbMetric: 18.0113, val_loss: 18.2060, val_MinusLogProbMetric: 18.2060

Epoch 603: val_loss did not improve from 18.06520
196/196 - 78s - loss: 18.0113 - MinusLogProbMetric: 18.0113 - val_loss: 18.2060 - val_MinusLogProbMetric: 18.2060 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 604/1000
2023-09-27 22:14:10.653 
Epoch 604/1000 
	 loss: 18.0072, MinusLogProbMetric: 18.0072, val_loss: 18.2086, val_MinusLogProbMetric: 18.2086

Epoch 604: val_loss did not improve from 18.06520
196/196 - 78s - loss: 18.0072 - MinusLogProbMetric: 18.0072 - val_loss: 18.2086 - val_MinusLogProbMetric: 18.2086 - lr: 1.1111e-04 - 78s/epoch - 400ms/step
Epoch 605/1000
2023-09-27 22:15:28.720 
Epoch 605/1000 
	 loss: 18.0019, MinusLogProbMetric: 18.0019, val_loss: 18.2961, val_MinusLogProbMetric: 18.2961

Epoch 605: val_loss did not improve from 18.06520
196/196 - 78s - loss: 18.0019 - MinusLogProbMetric: 18.0019 - val_loss: 18.2961 - val_MinusLogProbMetric: 18.2961 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 606/1000
2023-09-27 22:16:47.199 
Epoch 606/1000 
	 loss: 18.0087, MinusLogProbMetric: 18.0087, val_loss: 18.2601, val_MinusLogProbMetric: 18.2601

Epoch 606: val_loss did not improve from 18.06520
196/196 - 78s - loss: 18.0087 - MinusLogProbMetric: 18.0087 - val_loss: 18.2601 - val_MinusLogProbMetric: 18.2601 - lr: 1.1111e-04 - 78s/epoch - 400ms/step
Epoch 607/1000
2023-09-27 22:18:04.496 
Epoch 607/1000 
	 loss: 18.0158, MinusLogProbMetric: 18.0158, val_loss: 18.1541, val_MinusLogProbMetric: 18.1541

Epoch 607: val_loss did not improve from 18.06520
196/196 - 77s - loss: 18.0158 - MinusLogProbMetric: 18.0158 - val_loss: 18.1541 - val_MinusLogProbMetric: 18.1541 - lr: 1.1111e-04 - 77s/epoch - 394ms/step
Epoch 608/1000
2023-09-27 22:19:21.027 
Epoch 608/1000 
	 loss: 17.9940, MinusLogProbMetric: 17.9940, val_loss: 18.0755, val_MinusLogProbMetric: 18.0755

Epoch 608: val_loss did not improve from 18.06520
196/196 - 77s - loss: 17.9940 - MinusLogProbMetric: 17.9940 - val_loss: 18.0755 - val_MinusLogProbMetric: 18.0755 - lr: 1.1111e-04 - 77s/epoch - 390ms/step
Epoch 609/1000
2023-09-27 22:20:37.904 
Epoch 609/1000 
	 loss: 17.9867, MinusLogProbMetric: 17.9867, val_loss: 18.1670, val_MinusLogProbMetric: 18.1670

Epoch 609: val_loss did not improve from 18.06520
196/196 - 77s - loss: 17.9867 - MinusLogProbMetric: 17.9867 - val_loss: 18.1670 - val_MinusLogProbMetric: 18.1670 - lr: 1.1111e-04 - 77s/epoch - 392ms/step
Epoch 610/1000
2023-09-27 22:21:56.206 
Epoch 610/1000 
	 loss: 17.9882, MinusLogProbMetric: 17.9882, val_loss: 18.1881, val_MinusLogProbMetric: 18.1881

Epoch 610: val_loss did not improve from 18.06520
196/196 - 78s - loss: 17.9882 - MinusLogProbMetric: 17.9882 - val_loss: 18.1881 - val_MinusLogProbMetric: 18.1881 - lr: 1.1111e-04 - 78s/epoch - 399ms/step
Epoch 611/1000
2023-09-27 22:23:13.672 
Epoch 611/1000 
	 loss: 17.9877, MinusLogProbMetric: 17.9877, val_loss: 18.2243, val_MinusLogProbMetric: 18.2243

Epoch 611: val_loss did not improve from 18.06520
196/196 - 77s - loss: 17.9877 - MinusLogProbMetric: 17.9877 - val_loss: 18.2243 - val_MinusLogProbMetric: 18.2243 - lr: 1.1111e-04 - 77s/epoch - 395ms/step
Epoch 612/1000
2023-09-27 22:24:32.429 
Epoch 612/1000 
	 loss: 17.9820, MinusLogProbMetric: 17.9820, val_loss: 18.0868, val_MinusLogProbMetric: 18.0868

Epoch 612: val_loss did not improve from 18.06520
196/196 - 79s - loss: 17.9820 - MinusLogProbMetric: 17.9820 - val_loss: 18.0868 - val_MinusLogProbMetric: 18.0868 - lr: 1.1111e-04 - 79s/epoch - 402ms/step
Epoch 613/1000
2023-09-27 22:25:51.080 
Epoch 613/1000 
	 loss: 18.0060, MinusLogProbMetric: 18.0060, val_loss: 18.2186, val_MinusLogProbMetric: 18.2186

Epoch 613: val_loss did not improve from 18.06520
196/196 - 79s - loss: 18.0060 - MinusLogProbMetric: 18.0060 - val_loss: 18.2186 - val_MinusLogProbMetric: 18.2186 - lr: 1.1111e-04 - 79s/epoch - 401ms/step
Epoch 614/1000
2023-09-27 22:27:10.329 
Epoch 614/1000 
	 loss: 17.9969, MinusLogProbMetric: 17.9969, val_loss: 18.1253, val_MinusLogProbMetric: 18.1253

Epoch 614: val_loss did not improve from 18.06520
196/196 - 79s - loss: 17.9969 - MinusLogProbMetric: 17.9969 - val_loss: 18.1253 - val_MinusLogProbMetric: 18.1253 - lr: 1.1111e-04 - 79s/epoch - 404ms/step
Epoch 615/1000
2023-09-27 22:28:28.559 
Epoch 615/1000 
	 loss: 17.9934, MinusLogProbMetric: 17.9934, val_loss: 18.1944, val_MinusLogProbMetric: 18.1944

Epoch 615: val_loss did not improve from 18.06520
196/196 - 78s - loss: 17.9934 - MinusLogProbMetric: 17.9934 - val_loss: 18.1944 - val_MinusLogProbMetric: 18.1944 - lr: 1.1111e-04 - 78s/epoch - 399ms/step
Epoch 616/1000
2023-09-27 22:29:47.416 
Epoch 616/1000 
	 loss: 17.9538, MinusLogProbMetric: 17.9538, val_loss: 18.0715, val_MinusLogProbMetric: 18.0715

Epoch 616: val_loss did not improve from 18.06520
196/196 - 79s - loss: 17.9538 - MinusLogProbMetric: 17.9538 - val_loss: 18.0715 - val_MinusLogProbMetric: 18.0715 - lr: 1.1111e-04 - 79s/epoch - 402ms/step
Epoch 617/1000
2023-09-27 22:31:06.393 
Epoch 617/1000 
	 loss: 17.9733, MinusLogProbMetric: 17.9733, val_loss: 18.1053, val_MinusLogProbMetric: 18.1053

Epoch 617: val_loss did not improve from 18.06520
196/196 - 79s - loss: 17.9733 - MinusLogProbMetric: 17.9733 - val_loss: 18.1053 - val_MinusLogProbMetric: 18.1053 - lr: 1.1111e-04 - 79s/epoch - 403ms/step
Epoch 618/1000
2023-09-27 22:32:24.942 
Epoch 618/1000 
	 loss: 17.9717, MinusLogProbMetric: 17.9717, val_loss: 18.1014, val_MinusLogProbMetric: 18.1014

Epoch 618: val_loss did not improve from 18.06520
196/196 - 79s - loss: 17.9717 - MinusLogProbMetric: 17.9717 - val_loss: 18.1014 - val_MinusLogProbMetric: 18.1014 - lr: 1.1111e-04 - 79s/epoch - 401ms/step
Epoch 619/1000
2023-09-27 22:33:42.767 
Epoch 619/1000 
	 loss: 17.9801, MinusLogProbMetric: 17.9801, val_loss: 18.0839, val_MinusLogProbMetric: 18.0839

Epoch 619: val_loss did not improve from 18.06520
196/196 - 78s - loss: 17.9801 - MinusLogProbMetric: 17.9801 - val_loss: 18.0839 - val_MinusLogProbMetric: 18.0839 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 620/1000
2023-09-27 22:35:00.715 
Epoch 620/1000 
	 loss: 17.9867, MinusLogProbMetric: 17.9867, val_loss: 18.0837, val_MinusLogProbMetric: 18.0837

Epoch 620: val_loss did not improve from 18.06520
196/196 - 78s - loss: 17.9867 - MinusLogProbMetric: 17.9867 - val_loss: 18.0837 - val_MinusLogProbMetric: 18.0837 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 621/1000
2023-09-27 22:36:18.937 
Epoch 621/1000 
	 loss: 17.9733, MinusLogProbMetric: 17.9733, val_loss: 18.0406, val_MinusLogProbMetric: 18.0406

Epoch 621: val_loss improved from 18.06520 to 18.04062, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 79s - loss: 17.9733 - MinusLogProbMetric: 17.9733 - val_loss: 18.0406 - val_MinusLogProbMetric: 18.0406 - lr: 1.1111e-04 - 79s/epoch - 406ms/step
Epoch 622/1000
2023-09-27 22:37:37.358 
Epoch 622/1000 
	 loss: 17.9572, MinusLogProbMetric: 17.9572, val_loss: 18.1683, val_MinusLogProbMetric: 18.1683

Epoch 622: val_loss did not improve from 18.04062
196/196 - 77s - loss: 17.9572 - MinusLogProbMetric: 17.9572 - val_loss: 18.1683 - val_MinusLogProbMetric: 18.1683 - lr: 1.1111e-04 - 77s/epoch - 394ms/step
Epoch 623/1000
2023-09-27 22:38:56.572 
Epoch 623/1000 
	 loss: 17.9594, MinusLogProbMetric: 17.9594, val_loss: 18.0668, val_MinusLogProbMetric: 18.0668

Epoch 623: val_loss did not improve from 18.04062
196/196 - 79s - loss: 17.9594 - MinusLogProbMetric: 17.9594 - val_loss: 18.0668 - val_MinusLogProbMetric: 18.0668 - lr: 1.1111e-04 - 79s/epoch - 404ms/step
Epoch 624/1000
2023-09-27 22:40:14.490 
Epoch 624/1000 
	 loss: 17.9498, MinusLogProbMetric: 17.9498, val_loss: 18.1600, val_MinusLogProbMetric: 18.1600

Epoch 624: val_loss did not improve from 18.04062
196/196 - 78s - loss: 17.9498 - MinusLogProbMetric: 17.9498 - val_loss: 18.1600 - val_MinusLogProbMetric: 18.1600 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 625/1000
2023-09-27 22:41:32.883 
Epoch 625/1000 
	 loss: 17.9725, MinusLogProbMetric: 17.9725, val_loss: 17.9771, val_MinusLogProbMetric: 17.9771

Epoch 625: val_loss improved from 18.04062 to 17.97713, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 80s - loss: 17.9725 - MinusLogProbMetric: 17.9725 - val_loss: 17.9771 - val_MinusLogProbMetric: 17.9771 - lr: 1.1111e-04 - 80s/epoch - 407ms/step
Epoch 626/1000
2023-09-27 22:42:53.321 
Epoch 626/1000 
	 loss: 17.9618, MinusLogProbMetric: 17.9618, val_loss: 18.2412, val_MinusLogProbMetric: 18.2412

Epoch 626: val_loss did not improve from 17.97713
196/196 - 79s - loss: 17.9618 - MinusLogProbMetric: 17.9618 - val_loss: 18.2412 - val_MinusLogProbMetric: 18.2412 - lr: 1.1111e-04 - 79s/epoch - 404ms/step
Epoch 627/1000
2023-09-27 22:44:11.552 
Epoch 627/1000 
	 loss: 17.9639, MinusLogProbMetric: 17.9639, val_loss: 18.1000, val_MinusLogProbMetric: 18.1000

Epoch 627: val_loss did not improve from 17.97713
196/196 - 78s - loss: 17.9639 - MinusLogProbMetric: 17.9639 - val_loss: 18.1000 - val_MinusLogProbMetric: 18.1000 - lr: 1.1111e-04 - 78s/epoch - 399ms/step
Epoch 628/1000
2023-09-27 22:45:28.515 
Epoch 628/1000 
	 loss: 17.9779, MinusLogProbMetric: 17.9779, val_loss: 18.0715, val_MinusLogProbMetric: 18.0715

Epoch 628: val_loss did not improve from 17.97713
196/196 - 77s - loss: 17.9779 - MinusLogProbMetric: 17.9779 - val_loss: 18.0715 - val_MinusLogProbMetric: 18.0715 - lr: 1.1111e-04 - 77s/epoch - 393ms/step
Epoch 629/1000
2023-09-27 22:46:46.811 
Epoch 629/1000 
	 loss: 17.9562, MinusLogProbMetric: 17.9562, val_loss: 18.1272, val_MinusLogProbMetric: 18.1272

Epoch 629: val_loss did not improve from 17.97713
196/196 - 78s - loss: 17.9562 - MinusLogProbMetric: 17.9562 - val_loss: 18.1272 - val_MinusLogProbMetric: 18.1272 - lr: 1.1111e-04 - 78s/epoch - 399ms/step
Epoch 630/1000
2023-09-27 22:48:05.257 
Epoch 630/1000 
	 loss: 17.9411, MinusLogProbMetric: 17.9411, val_loss: 18.0523, val_MinusLogProbMetric: 18.0523

Epoch 630: val_loss did not improve from 17.97713
196/196 - 78s - loss: 17.9411 - MinusLogProbMetric: 17.9411 - val_loss: 18.0523 - val_MinusLogProbMetric: 18.0523 - lr: 1.1111e-04 - 78s/epoch - 400ms/step
Epoch 631/1000
2023-09-27 22:49:23.221 
Epoch 631/1000 
	 loss: 17.9586, MinusLogProbMetric: 17.9586, val_loss: 18.0929, val_MinusLogProbMetric: 18.0929

Epoch 631: val_loss did not improve from 17.97713
196/196 - 78s - loss: 17.9586 - MinusLogProbMetric: 17.9586 - val_loss: 18.0929 - val_MinusLogProbMetric: 18.0929 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 632/1000
2023-09-27 22:50:40.790 
Epoch 632/1000 
	 loss: 17.9689, MinusLogProbMetric: 17.9689, val_loss: 18.1894, val_MinusLogProbMetric: 18.1894

Epoch 632: val_loss did not improve from 17.97713
196/196 - 78s - loss: 17.9689 - MinusLogProbMetric: 17.9689 - val_loss: 18.1894 - val_MinusLogProbMetric: 18.1894 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 633/1000
2023-09-27 22:51:58.154 
Epoch 633/1000 
	 loss: 17.9301, MinusLogProbMetric: 17.9301, val_loss: 18.0989, val_MinusLogProbMetric: 18.0989

Epoch 633: val_loss did not improve from 17.97713
196/196 - 77s - loss: 17.9301 - MinusLogProbMetric: 17.9301 - val_loss: 18.0989 - val_MinusLogProbMetric: 18.0989 - lr: 1.1111e-04 - 77s/epoch - 395ms/step
Epoch 634/1000
2023-09-27 22:53:17.173 
Epoch 634/1000 
	 loss: 17.9449, MinusLogProbMetric: 17.9449, val_loss: 18.0329, val_MinusLogProbMetric: 18.0329

Epoch 634: val_loss did not improve from 17.97713
196/196 - 79s - loss: 17.9449 - MinusLogProbMetric: 17.9449 - val_loss: 18.0329 - val_MinusLogProbMetric: 18.0329 - lr: 1.1111e-04 - 79s/epoch - 403ms/step
Epoch 635/1000
2023-09-27 22:54:35.635 
Epoch 635/1000 
	 loss: 17.9337, MinusLogProbMetric: 17.9337, val_loss: 18.1569, val_MinusLogProbMetric: 18.1569

Epoch 635: val_loss did not improve from 17.97713
196/196 - 78s - loss: 17.9337 - MinusLogProbMetric: 17.9337 - val_loss: 18.1569 - val_MinusLogProbMetric: 18.1569 - lr: 1.1111e-04 - 78s/epoch - 400ms/step
Epoch 636/1000
2023-09-27 22:55:53.190 
Epoch 636/1000 
	 loss: 17.9537, MinusLogProbMetric: 17.9537, val_loss: 18.1533, val_MinusLogProbMetric: 18.1533

Epoch 636: val_loss did not improve from 17.97713
196/196 - 78s - loss: 17.9537 - MinusLogProbMetric: 17.9537 - val_loss: 18.1533 - val_MinusLogProbMetric: 18.1533 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 637/1000
2023-09-27 22:57:10.796 
Epoch 637/1000 
	 loss: 17.9371, MinusLogProbMetric: 17.9371, val_loss: 18.1742, val_MinusLogProbMetric: 18.1742

Epoch 637: val_loss did not improve from 17.97713
196/196 - 78s - loss: 17.9371 - MinusLogProbMetric: 17.9371 - val_loss: 18.1742 - val_MinusLogProbMetric: 18.1742 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 638/1000
2023-09-27 22:58:29.496 
Epoch 638/1000 
	 loss: 17.9402, MinusLogProbMetric: 17.9402, val_loss: 18.1461, val_MinusLogProbMetric: 18.1461

Epoch 638: val_loss did not improve from 17.97713
196/196 - 79s - loss: 17.9402 - MinusLogProbMetric: 17.9402 - val_loss: 18.1461 - val_MinusLogProbMetric: 18.1461 - lr: 1.1111e-04 - 79s/epoch - 402ms/step
Epoch 639/1000
2023-09-27 22:59:47.492 
Epoch 639/1000 
	 loss: 17.9371, MinusLogProbMetric: 17.9371, val_loss: 18.0537, val_MinusLogProbMetric: 18.0537

Epoch 639: val_loss did not improve from 17.97713
196/196 - 78s - loss: 17.9371 - MinusLogProbMetric: 17.9371 - val_loss: 18.0537 - val_MinusLogProbMetric: 18.0537 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 640/1000
2023-09-27 23:01:05.956 
Epoch 640/1000 
	 loss: 17.9109, MinusLogProbMetric: 17.9109, val_loss: 18.0594, val_MinusLogProbMetric: 18.0594

Epoch 640: val_loss did not improve from 17.97713
196/196 - 78s - loss: 17.9109 - MinusLogProbMetric: 17.9109 - val_loss: 18.0594 - val_MinusLogProbMetric: 18.0594 - lr: 1.1111e-04 - 78s/epoch - 400ms/step
Epoch 641/1000
2023-09-27 23:02:23.242 
Epoch 641/1000 
	 loss: 17.9500, MinusLogProbMetric: 17.9500, val_loss: 18.1041, val_MinusLogProbMetric: 18.1041

Epoch 641: val_loss did not improve from 17.97713
196/196 - 77s - loss: 17.9500 - MinusLogProbMetric: 17.9500 - val_loss: 18.1041 - val_MinusLogProbMetric: 18.1041 - lr: 1.1111e-04 - 77s/epoch - 394ms/step
Epoch 642/1000
2023-09-27 23:03:40.698 
Epoch 642/1000 
	 loss: 17.9435, MinusLogProbMetric: 17.9435, val_loss: 18.1012, val_MinusLogProbMetric: 18.1012

Epoch 642: val_loss did not improve from 17.97713
196/196 - 77s - loss: 17.9435 - MinusLogProbMetric: 17.9435 - val_loss: 18.1012 - val_MinusLogProbMetric: 18.1012 - lr: 1.1111e-04 - 77s/epoch - 395ms/step
Epoch 643/1000
2023-09-27 23:04:58.341 
Epoch 643/1000 
	 loss: 17.9234, MinusLogProbMetric: 17.9234, val_loss: 18.0349, val_MinusLogProbMetric: 18.0349

Epoch 643: val_loss did not improve from 17.97713
196/196 - 78s - loss: 17.9234 - MinusLogProbMetric: 17.9234 - val_loss: 18.0349 - val_MinusLogProbMetric: 18.0349 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 644/1000
2023-09-27 23:06:15.805 
Epoch 644/1000 
	 loss: 17.9178, MinusLogProbMetric: 17.9178, val_loss: 18.1298, val_MinusLogProbMetric: 18.1298

Epoch 644: val_loss did not improve from 17.97713
196/196 - 77s - loss: 17.9178 - MinusLogProbMetric: 17.9178 - val_loss: 18.1298 - val_MinusLogProbMetric: 18.1298 - lr: 1.1111e-04 - 77s/epoch - 395ms/step
Epoch 645/1000
2023-09-27 23:07:32.786 
Epoch 645/1000 
	 loss: 17.9048, MinusLogProbMetric: 17.9048, val_loss: 18.0680, val_MinusLogProbMetric: 18.0680

Epoch 645: val_loss did not improve from 17.97713
196/196 - 77s - loss: 17.9048 - MinusLogProbMetric: 17.9048 - val_loss: 18.0680 - val_MinusLogProbMetric: 18.0680 - lr: 1.1111e-04 - 77s/epoch - 393ms/step
Epoch 646/1000
2023-09-27 23:08:51.010 
Epoch 646/1000 
	 loss: 17.9019, MinusLogProbMetric: 17.9019, val_loss: 18.1150, val_MinusLogProbMetric: 18.1150

Epoch 646: val_loss did not improve from 17.97713
196/196 - 78s - loss: 17.9019 - MinusLogProbMetric: 17.9019 - val_loss: 18.1150 - val_MinusLogProbMetric: 18.1150 - lr: 1.1111e-04 - 78s/epoch - 399ms/step
Epoch 647/1000
2023-09-27 23:10:08.246 
Epoch 647/1000 
	 loss: 17.9086, MinusLogProbMetric: 17.9086, val_loss: 17.9773, val_MinusLogProbMetric: 17.9773

Epoch 647: val_loss did not improve from 17.97713
196/196 - 77s - loss: 17.9086 - MinusLogProbMetric: 17.9086 - val_loss: 17.9773 - val_MinusLogProbMetric: 17.9773 - lr: 1.1111e-04 - 77s/epoch - 394ms/step
Epoch 648/1000
2023-09-27 23:11:25.890 
Epoch 648/1000 
	 loss: 17.9100, MinusLogProbMetric: 17.9100, val_loss: 18.0046, val_MinusLogProbMetric: 18.0046

Epoch 648: val_loss did not improve from 17.97713
196/196 - 78s - loss: 17.9100 - MinusLogProbMetric: 17.9100 - val_loss: 18.0046 - val_MinusLogProbMetric: 18.0046 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 649/1000
2023-09-27 23:12:43.924 
Epoch 649/1000 
	 loss: 17.9016, MinusLogProbMetric: 17.9016, val_loss: 18.0133, val_MinusLogProbMetric: 18.0133

Epoch 649: val_loss did not improve from 17.97713
196/196 - 78s - loss: 17.9016 - MinusLogProbMetric: 17.9016 - val_loss: 18.0133 - val_MinusLogProbMetric: 18.0133 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 650/1000
2023-09-27 23:14:00.972 
Epoch 650/1000 
	 loss: 17.9166, MinusLogProbMetric: 17.9166, val_loss: 18.0808, val_MinusLogProbMetric: 18.0808

Epoch 650: val_loss did not improve from 17.97713
196/196 - 77s - loss: 17.9166 - MinusLogProbMetric: 17.9166 - val_loss: 18.0808 - val_MinusLogProbMetric: 18.0808 - lr: 1.1111e-04 - 77s/epoch - 393ms/step
Epoch 651/1000
2023-09-27 23:15:17.190 
Epoch 651/1000 
	 loss: 17.9134, MinusLogProbMetric: 17.9134, val_loss: 18.0777, val_MinusLogProbMetric: 18.0777

Epoch 651: val_loss did not improve from 17.97713
196/196 - 76s - loss: 17.9134 - MinusLogProbMetric: 17.9134 - val_loss: 18.0777 - val_MinusLogProbMetric: 18.0777 - lr: 1.1111e-04 - 76s/epoch - 389ms/step
Epoch 652/1000
2023-09-27 23:16:35.061 
Epoch 652/1000 
	 loss: 17.9096, MinusLogProbMetric: 17.9096, val_loss: 18.0099, val_MinusLogProbMetric: 18.0099

Epoch 652: val_loss did not improve from 17.97713
196/196 - 78s - loss: 17.9096 - MinusLogProbMetric: 17.9096 - val_loss: 18.0099 - val_MinusLogProbMetric: 18.0099 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 653/1000
2023-09-27 23:17:53.223 
Epoch 653/1000 
	 loss: 17.8811, MinusLogProbMetric: 17.8811, val_loss: 17.9819, val_MinusLogProbMetric: 17.9819

Epoch 653: val_loss did not improve from 17.97713
196/196 - 78s - loss: 17.8811 - MinusLogProbMetric: 17.8811 - val_loss: 17.9819 - val_MinusLogProbMetric: 17.9819 - lr: 1.1111e-04 - 78s/epoch - 399ms/step
Epoch 654/1000
2023-09-27 23:19:10.617 
Epoch 654/1000 
	 loss: 17.8872, MinusLogProbMetric: 17.8872, val_loss: 17.9915, val_MinusLogProbMetric: 17.9915

Epoch 654: val_loss did not improve from 17.97713
196/196 - 77s - loss: 17.8872 - MinusLogProbMetric: 17.8872 - val_loss: 17.9915 - val_MinusLogProbMetric: 17.9915 - lr: 1.1111e-04 - 77s/epoch - 395ms/step
Epoch 655/1000
2023-09-27 23:20:27.691 
Epoch 655/1000 
	 loss: 17.9055, MinusLogProbMetric: 17.9055, val_loss: 18.0539, val_MinusLogProbMetric: 18.0539

Epoch 655: val_loss did not improve from 17.97713
196/196 - 77s - loss: 17.9055 - MinusLogProbMetric: 17.9055 - val_loss: 18.0539 - val_MinusLogProbMetric: 18.0539 - lr: 1.1111e-04 - 77s/epoch - 393ms/step
Epoch 656/1000
2023-09-27 23:21:44.608 
Epoch 656/1000 
	 loss: 17.9074, MinusLogProbMetric: 17.9074, val_loss: 18.0592, val_MinusLogProbMetric: 18.0592

Epoch 656: val_loss did not improve from 17.97713
196/196 - 77s - loss: 17.9074 - MinusLogProbMetric: 17.9074 - val_loss: 18.0592 - val_MinusLogProbMetric: 18.0592 - lr: 1.1111e-04 - 77s/epoch - 392ms/step
Epoch 657/1000
2023-09-27 23:23:01.586 
Epoch 657/1000 
	 loss: 17.9029, MinusLogProbMetric: 17.9029, val_loss: 17.9410, val_MinusLogProbMetric: 17.9410

Epoch 657: val_loss improved from 17.97713 to 17.94096, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 78s - loss: 17.9029 - MinusLogProbMetric: 17.9029 - val_loss: 17.9410 - val_MinusLogProbMetric: 17.9410 - lr: 1.1111e-04 - 78s/epoch - 399ms/step
Epoch 658/1000
2023-09-27 23:24:20.361 
Epoch 658/1000 
	 loss: 17.8704, MinusLogProbMetric: 17.8704, val_loss: 18.1787, val_MinusLogProbMetric: 18.1787

Epoch 658: val_loss did not improve from 17.94096
196/196 - 77s - loss: 17.8704 - MinusLogProbMetric: 17.8704 - val_loss: 18.1787 - val_MinusLogProbMetric: 18.1787 - lr: 1.1111e-04 - 77s/epoch - 395ms/step
Epoch 659/1000
2023-09-27 23:25:32.742 
Epoch 659/1000 
	 loss: 17.9056, MinusLogProbMetric: 17.9056, val_loss: 17.9587, val_MinusLogProbMetric: 17.9587

Epoch 659: val_loss did not improve from 17.94096
196/196 - 72s - loss: 17.9056 - MinusLogProbMetric: 17.9056 - val_loss: 17.9587 - val_MinusLogProbMetric: 17.9587 - lr: 1.1111e-04 - 72s/epoch - 369ms/step
Epoch 660/1000
2023-09-27 23:26:40.170 
Epoch 660/1000 
	 loss: 17.9055, MinusLogProbMetric: 17.9055, val_loss: 18.0535, val_MinusLogProbMetric: 18.0535

Epoch 660: val_loss did not improve from 17.94096
196/196 - 67s - loss: 17.9055 - MinusLogProbMetric: 17.9055 - val_loss: 18.0535 - val_MinusLogProbMetric: 18.0535 - lr: 1.1111e-04 - 67s/epoch - 344ms/step
Epoch 661/1000
2023-09-27 23:27:56.550 
Epoch 661/1000 
	 loss: 17.8918, MinusLogProbMetric: 17.8918, val_loss: 18.0008, val_MinusLogProbMetric: 18.0008

Epoch 661: val_loss did not improve from 17.94096
196/196 - 76s - loss: 17.8918 - MinusLogProbMetric: 17.8918 - val_loss: 18.0008 - val_MinusLogProbMetric: 18.0008 - lr: 1.1111e-04 - 76s/epoch - 390ms/step
Epoch 662/1000
2023-09-27 23:29:13.985 
Epoch 662/1000 
	 loss: 17.9075, MinusLogProbMetric: 17.9075, val_loss: 18.0331, val_MinusLogProbMetric: 18.0331

Epoch 662: val_loss did not improve from 17.94096
196/196 - 77s - loss: 17.9075 - MinusLogProbMetric: 17.9075 - val_loss: 18.0331 - val_MinusLogProbMetric: 18.0331 - lr: 1.1111e-04 - 77s/epoch - 395ms/step
Epoch 663/1000
2023-09-27 23:30:31.698 
Epoch 663/1000 
	 loss: 17.9103, MinusLogProbMetric: 17.9103, val_loss: 18.5514, val_MinusLogProbMetric: 18.5514

Epoch 663: val_loss did not improve from 17.94096
196/196 - 78s - loss: 17.9103 - MinusLogProbMetric: 17.9103 - val_loss: 18.5514 - val_MinusLogProbMetric: 18.5514 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 664/1000
2023-09-27 23:31:50.922 
Epoch 664/1000 
	 loss: 17.8959, MinusLogProbMetric: 17.8959, val_loss: 18.0582, val_MinusLogProbMetric: 18.0582

Epoch 664: val_loss did not improve from 17.94096
196/196 - 79s - loss: 17.8959 - MinusLogProbMetric: 17.8959 - val_loss: 18.0582 - val_MinusLogProbMetric: 18.0582 - lr: 1.1111e-04 - 79s/epoch - 404ms/step
Epoch 665/1000
2023-09-27 23:33:08.391 
Epoch 665/1000 
	 loss: 17.8850, MinusLogProbMetric: 17.8850, val_loss: 18.0733, val_MinusLogProbMetric: 18.0733

Epoch 665: val_loss did not improve from 17.94096
196/196 - 77s - loss: 17.8850 - MinusLogProbMetric: 17.8850 - val_loss: 18.0733 - val_MinusLogProbMetric: 18.0733 - lr: 1.1111e-04 - 77s/epoch - 395ms/step
Epoch 666/1000
2023-09-27 23:34:26.265 
Epoch 666/1000 
	 loss: 17.9014, MinusLogProbMetric: 17.9014, val_loss: 18.0201, val_MinusLogProbMetric: 18.0201

Epoch 666: val_loss did not improve from 17.94096
196/196 - 78s - loss: 17.9014 - MinusLogProbMetric: 17.9014 - val_loss: 18.0201 - val_MinusLogProbMetric: 18.0201 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 667/1000
2023-09-27 23:35:44.040 
Epoch 667/1000 
	 loss: 17.8807, MinusLogProbMetric: 17.8807, val_loss: 18.0984, val_MinusLogProbMetric: 18.0984

Epoch 667: val_loss did not improve from 17.94096
196/196 - 78s - loss: 17.8807 - MinusLogProbMetric: 17.8807 - val_loss: 18.0984 - val_MinusLogProbMetric: 18.0984 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 668/1000
2023-09-27 23:37:01.931 
Epoch 668/1000 
	 loss: 17.9131, MinusLogProbMetric: 17.9131, val_loss: 17.9815, val_MinusLogProbMetric: 17.9815

Epoch 668: val_loss did not improve from 17.94096
196/196 - 78s - loss: 17.9131 - MinusLogProbMetric: 17.9131 - val_loss: 17.9815 - val_MinusLogProbMetric: 17.9815 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 669/1000
2023-09-27 23:38:19.433 
Epoch 669/1000 
	 loss: 17.9025, MinusLogProbMetric: 17.9025, val_loss: 17.9829, val_MinusLogProbMetric: 17.9829

Epoch 669: val_loss did not improve from 17.94096
196/196 - 77s - loss: 17.9025 - MinusLogProbMetric: 17.9025 - val_loss: 17.9829 - val_MinusLogProbMetric: 17.9829 - lr: 1.1111e-04 - 77s/epoch - 395ms/step
Epoch 670/1000
2023-09-27 23:39:37.014 
Epoch 670/1000 
	 loss: 17.8955, MinusLogProbMetric: 17.8955, val_loss: 17.8949, val_MinusLogProbMetric: 17.8949

Epoch 670: val_loss improved from 17.94096 to 17.89491, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 79s - loss: 17.8955 - MinusLogProbMetric: 17.8955 - val_loss: 17.8949 - val_MinusLogProbMetric: 17.8949 - lr: 1.1111e-04 - 79s/epoch - 403ms/step
Epoch 671/1000
2023-09-27 23:40:56.827 
Epoch 671/1000 
	 loss: 17.8648, MinusLogProbMetric: 17.8648, val_loss: 18.0919, val_MinusLogProbMetric: 18.0919

Epoch 671: val_loss did not improve from 17.89491
196/196 - 78s - loss: 17.8648 - MinusLogProbMetric: 17.8648 - val_loss: 18.0919 - val_MinusLogProbMetric: 18.0919 - lr: 1.1111e-04 - 78s/epoch - 400ms/step
Epoch 672/1000
2023-09-27 23:42:14.816 
Epoch 672/1000 
	 loss: 17.8678, MinusLogProbMetric: 17.8678, val_loss: 17.9402, val_MinusLogProbMetric: 17.9402

Epoch 672: val_loss did not improve from 17.89491
196/196 - 78s - loss: 17.8678 - MinusLogProbMetric: 17.8678 - val_loss: 17.9402 - val_MinusLogProbMetric: 17.9402 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 673/1000
2023-09-27 23:43:32.627 
Epoch 673/1000 
	 loss: 17.8596, MinusLogProbMetric: 17.8596, val_loss: 18.0113, val_MinusLogProbMetric: 18.0113

Epoch 673: val_loss did not improve from 17.89491
196/196 - 78s - loss: 17.8596 - MinusLogProbMetric: 17.8596 - val_loss: 18.0113 - val_MinusLogProbMetric: 18.0113 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 674/1000
2023-09-27 23:44:50.980 
Epoch 674/1000 
	 loss: 17.8763, MinusLogProbMetric: 17.8763, val_loss: 17.8815, val_MinusLogProbMetric: 17.8815

Epoch 674: val_loss improved from 17.89491 to 17.88149, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 80s - loss: 17.8763 - MinusLogProbMetric: 17.8763 - val_loss: 17.8815 - val_MinusLogProbMetric: 17.8815 - lr: 1.1111e-04 - 80s/epoch - 407ms/step
Epoch 675/1000
2023-09-27 23:46:10.635 
Epoch 675/1000 
	 loss: 17.8610, MinusLogProbMetric: 17.8610, val_loss: 18.1161, val_MinusLogProbMetric: 18.1161

Epoch 675: val_loss did not improve from 17.88149
196/196 - 78s - loss: 17.8610 - MinusLogProbMetric: 17.8610 - val_loss: 18.1161 - val_MinusLogProbMetric: 18.1161 - lr: 1.1111e-04 - 78s/epoch - 399ms/step
Epoch 676/1000
2023-09-27 23:47:28.544 
Epoch 676/1000 
	 loss: 17.8622, MinusLogProbMetric: 17.8622, val_loss: 18.0494, val_MinusLogProbMetric: 18.0494

Epoch 676: val_loss did not improve from 17.88149
196/196 - 78s - loss: 17.8622 - MinusLogProbMetric: 17.8622 - val_loss: 18.0494 - val_MinusLogProbMetric: 18.0494 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 677/1000
2023-09-27 23:48:46.525 
Epoch 677/1000 
	 loss: 17.8538, MinusLogProbMetric: 17.8538, val_loss: 18.0186, val_MinusLogProbMetric: 18.0186

Epoch 677: val_loss did not improve from 17.88149
196/196 - 78s - loss: 17.8538 - MinusLogProbMetric: 17.8538 - val_loss: 18.0186 - val_MinusLogProbMetric: 18.0186 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 678/1000
2023-09-27 23:50:04.364 
Epoch 678/1000 
	 loss: 17.8805, MinusLogProbMetric: 17.8805, val_loss: 18.1426, val_MinusLogProbMetric: 18.1426

Epoch 678: val_loss did not improve from 17.88149
196/196 - 78s - loss: 17.8805 - MinusLogProbMetric: 17.8805 - val_loss: 18.1426 - val_MinusLogProbMetric: 18.1426 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 679/1000
2023-09-27 23:51:22.038 
Epoch 679/1000 
	 loss: 17.8465, MinusLogProbMetric: 17.8465, val_loss: 17.9894, val_MinusLogProbMetric: 17.9894

Epoch 679: val_loss did not improve from 17.88149
196/196 - 78s - loss: 17.8465 - MinusLogProbMetric: 17.8465 - val_loss: 17.9894 - val_MinusLogProbMetric: 17.9894 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 680/1000
2023-09-27 23:52:40.760 
Epoch 680/1000 
	 loss: 17.8718, MinusLogProbMetric: 17.8718, val_loss: 17.9811, val_MinusLogProbMetric: 17.9811

Epoch 680: val_loss did not improve from 17.88149
196/196 - 79s - loss: 17.8718 - MinusLogProbMetric: 17.8718 - val_loss: 17.9811 - val_MinusLogProbMetric: 17.9811 - lr: 1.1111e-04 - 79s/epoch - 402ms/step
Epoch 681/1000
2023-09-27 23:53:58.450 
Epoch 681/1000 
	 loss: 17.8384, MinusLogProbMetric: 17.8384, val_loss: 18.2190, val_MinusLogProbMetric: 18.2190

Epoch 681: val_loss did not improve from 17.88149
196/196 - 78s - loss: 17.8384 - MinusLogProbMetric: 17.8384 - val_loss: 18.2190 - val_MinusLogProbMetric: 18.2190 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 682/1000
2023-09-27 23:55:16.837 
Epoch 682/1000 
	 loss: 17.8565, MinusLogProbMetric: 17.8565, val_loss: 18.1026, val_MinusLogProbMetric: 18.1026

Epoch 682: val_loss did not improve from 17.88149
196/196 - 78s - loss: 17.8565 - MinusLogProbMetric: 17.8565 - val_loss: 18.1026 - val_MinusLogProbMetric: 18.1026 - lr: 1.1111e-04 - 78s/epoch - 400ms/step
Epoch 683/1000
2023-09-27 23:56:35.380 
Epoch 683/1000 
	 loss: 17.8508, MinusLogProbMetric: 17.8508, val_loss: 17.9734, val_MinusLogProbMetric: 17.9734

Epoch 683: val_loss did not improve from 17.88149
196/196 - 79s - loss: 17.8508 - MinusLogProbMetric: 17.8508 - val_loss: 17.9734 - val_MinusLogProbMetric: 17.9734 - lr: 1.1111e-04 - 79s/epoch - 401ms/step
Epoch 684/1000
2023-09-27 23:57:53.624 
Epoch 684/1000 
	 loss: 17.8412, MinusLogProbMetric: 17.8412, val_loss: 18.0768, val_MinusLogProbMetric: 18.0768

Epoch 684: val_loss did not improve from 17.88149
196/196 - 78s - loss: 17.8412 - MinusLogProbMetric: 17.8412 - val_loss: 18.0768 - val_MinusLogProbMetric: 18.0768 - lr: 1.1111e-04 - 78s/epoch - 399ms/step
Epoch 685/1000
2023-09-27 23:59:12.501 
Epoch 685/1000 
	 loss: 17.8549, MinusLogProbMetric: 17.8549, val_loss: 18.1023, val_MinusLogProbMetric: 18.1023

Epoch 685: val_loss did not improve from 17.88149
196/196 - 79s - loss: 17.8549 - MinusLogProbMetric: 17.8549 - val_loss: 18.1023 - val_MinusLogProbMetric: 18.1023 - lr: 1.1111e-04 - 79s/epoch - 402ms/step
Epoch 686/1000
2023-09-28 00:00:29.737 
Epoch 686/1000 
	 loss: 17.8592, MinusLogProbMetric: 17.8592, val_loss: 18.0485, val_MinusLogProbMetric: 18.0485

Epoch 686: val_loss did not improve from 17.88149
196/196 - 77s - loss: 17.8592 - MinusLogProbMetric: 17.8592 - val_loss: 18.0485 - val_MinusLogProbMetric: 18.0485 - lr: 1.1111e-04 - 77s/epoch - 394ms/step
Epoch 687/1000
2023-09-28 00:01:47.624 
Epoch 687/1000 
	 loss: 17.8582, MinusLogProbMetric: 17.8582, val_loss: 17.9419, val_MinusLogProbMetric: 17.9419

Epoch 687: val_loss did not improve from 17.88149
196/196 - 78s - loss: 17.8582 - MinusLogProbMetric: 17.8582 - val_loss: 17.9419 - val_MinusLogProbMetric: 17.9419 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 688/1000
2023-09-28 00:03:06.805 
Epoch 688/1000 
	 loss: 17.8573, MinusLogProbMetric: 17.8573, val_loss: 18.1662, val_MinusLogProbMetric: 18.1662

Epoch 688: val_loss did not improve from 17.88149
196/196 - 79s - loss: 17.8573 - MinusLogProbMetric: 17.8573 - val_loss: 18.1662 - val_MinusLogProbMetric: 18.1662 - lr: 1.1111e-04 - 79s/epoch - 404ms/step
Epoch 689/1000
2023-09-28 00:04:24.094 
Epoch 689/1000 
	 loss: 17.8540, MinusLogProbMetric: 17.8540, val_loss: 17.9677, val_MinusLogProbMetric: 17.9677

Epoch 689: val_loss did not improve from 17.88149
196/196 - 77s - loss: 17.8540 - MinusLogProbMetric: 17.8540 - val_loss: 17.9677 - val_MinusLogProbMetric: 17.9677 - lr: 1.1111e-04 - 77s/epoch - 394ms/step
Epoch 690/1000
2023-09-28 00:05:42.006 
Epoch 690/1000 
	 loss: 17.8429, MinusLogProbMetric: 17.8429, val_loss: 18.0937, val_MinusLogProbMetric: 18.0937

Epoch 690: val_loss did not improve from 17.88149
196/196 - 78s - loss: 17.8429 - MinusLogProbMetric: 17.8429 - val_loss: 18.0937 - val_MinusLogProbMetric: 18.0937 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 691/1000
2023-09-28 00:07:00.240 
Epoch 691/1000 
	 loss: 17.8407, MinusLogProbMetric: 17.8407, val_loss: 18.0802, val_MinusLogProbMetric: 18.0802

Epoch 691: val_loss did not improve from 17.88149
196/196 - 78s - loss: 17.8407 - MinusLogProbMetric: 17.8407 - val_loss: 18.0802 - val_MinusLogProbMetric: 18.0802 - lr: 1.1111e-04 - 78s/epoch - 399ms/step
Epoch 692/1000
2023-09-28 00:08:18.861 
Epoch 692/1000 
	 loss: 17.8329, MinusLogProbMetric: 17.8329, val_loss: 18.1073, val_MinusLogProbMetric: 18.1073

Epoch 692: val_loss did not improve from 17.88149
196/196 - 79s - loss: 17.8329 - MinusLogProbMetric: 17.8329 - val_loss: 18.1073 - val_MinusLogProbMetric: 18.1073 - lr: 1.1111e-04 - 79s/epoch - 401ms/step
Epoch 693/1000
2023-09-28 00:09:37.376 
Epoch 693/1000 
	 loss: 17.8549, MinusLogProbMetric: 17.8549, val_loss: 17.9565, val_MinusLogProbMetric: 17.9565

Epoch 693: val_loss did not improve from 17.88149
196/196 - 79s - loss: 17.8549 - MinusLogProbMetric: 17.8549 - val_loss: 17.9565 - val_MinusLogProbMetric: 17.9565 - lr: 1.1111e-04 - 79s/epoch - 401ms/step
Epoch 694/1000
2023-09-28 00:10:55.494 
Epoch 694/1000 
	 loss: 17.8299, MinusLogProbMetric: 17.8299, val_loss: 17.9498, val_MinusLogProbMetric: 17.9498

Epoch 694: val_loss did not improve from 17.88149
196/196 - 78s - loss: 17.8299 - MinusLogProbMetric: 17.8299 - val_loss: 17.9498 - val_MinusLogProbMetric: 17.9498 - lr: 1.1111e-04 - 78s/epoch - 399ms/step
Epoch 695/1000
2023-09-28 00:12:14.006 
Epoch 695/1000 
	 loss: 17.8284, MinusLogProbMetric: 17.8284, val_loss: 17.9355, val_MinusLogProbMetric: 17.9355

Epoch 695: val_loss did not improve from 17.88149
196/196 - 79s - loss: 17.8284 - MinusLogProbMetric: 17.8284 - val_loss: 17.9355 - val_MinusLogProbMetric: 17.9355 - lr: 1.1111e-04 - 79s/epoch - 401ms/step
Epoch 696/1000
2023-09-28 00:13:32.375 
Epoch 696/1000 
	 loss: 17.8253, MinusLogProbMetric: 17.8253, val_loss: 18.0999, val_MinusLogProbMetric: 18.0999

Epoch 696: val_loss did not improve from 17.88149
196/196 - 78s - loss: 17.8253 - MinusLogProbMetric: 17.8253 - val_loss: 18.0999 - val_MinusLogProbMetric: 18.0999 - lr: 1.1111e-04 - 78s/epoch - 400ms/step
Epoch 697/1000
2023-09-28 00:14:50.373 
Epoch 697/1000 
	 loss: 17.8358, MinusLogProbMetric: 17.8358, val_loss: 17.8598, val_MinusLogProbMetric: 17.8598

Epoch 697: val_loss improved from 17.88149 to 17.85980, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 79s - loss: 17.8358 - MinusLogProbMetric: 17.8358 - val_loss: 17.8598 - val_MinusLogProbMetric: 17.8598 - lr: 1.1111e-04 - 79s/epoch - 404ms/step
Epoch 698/1000
2023-09-28 00:16:09.161 
Epoch 698/1000 
	 loss: 17.8378, MinusLogProbMetric: 17.8378, val_loss: 18.0334, val_MinusLogProbMetric: 18.0334

Epoch 698: val_loss did not improve from 17.85980
196/196 - 78s - loss: 17.8378 - MinusLogProbMetric: 17.8378 - val_loss: 18.0334 - val_MinusLogProbMetric: 18.0334 - lr: 1.1111e-04 - 78s/epoch - 395ms/step
Epoch 699/1000
2023-09-28 00:17:27.117 
Epoch 699/1000 
	 loss: 17.8404, MinusLogProbMetric: 17.8404, val_loss: 17.9914, val_MinusLogProbMetric: 17.9914

Epoch 699: val_loss did not improve from 17.85980
196/196 - 78s - loss: 17.8404 - MinusLogProbMetric: 17.8404 - val_loss: 17.9914 - val_MinusLogProbMetric: 17.9914 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 700/1000
2023-09-28 00:18:44.830 
Epoch 700/1000 
	 loss: 17.8281, MinusLogProbMetric: 17.8281, val_loss: 17.9043, val_MinusLogProbMetric: 17.9043

Epoch 700: val_loss did not improve from 17.85980
196/196 - 78s - loss: 17.8281 - MinusLogProbMetric: 17.8281 - val_loss: 17.9043 - val_MinusLogProbMetric: 17.9043 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 701/1000
2023-09-28 00:20:02.665 
Epoch 701/1000 
	 loss: 17.8235, MinusLogProbMetric: 17.8235, val_loss: 17.8628, val_MinusLogProbMetric: 17.8628

Epoch 701: val_loss did not improve from 17.85980
196/196 - 78s - loss: 17.8235 - MinusLogProbMetric: 17.8235 - val_loss: 17.8628 - val_MinusLogProbMetric: 17.8628 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 702/1000
2023-09-28 00:21:20.228 
Epoch 702/1000 
	 loss: 17.8360, MinusLogProbMetric: 17.8360, val_loss: 17.8632, val_MinusLogProbMetric: 17.8632

Epoch 702: val_loss did not improve from 17.85980
196/196 - 78s - loss: 17.8360 - MinusLogProbMetric: 17.8360 - val_loss: 17.8632 - val_MinusLogProbMetric: 17.8632 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 703/1000
2023-09-28 00:22:38.002 
Epoch 703/1000 
	 loss: 17.8163, MinusLogProbMetric: 17.8163, val_loss: 17.9864, val_MinusLogProbMetric: 17.9864

Epoch 703: val_loss did not improve from 17.85980
196/196 - 78s - loss: 17.8163 - MinusLogProbMetric: 17.8163 - val_loss: 17.9864 - val_MinusLogProbMetric: 17.9864 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 704/1000
2023-09-28 00:23:55.726 
Epoch 704/1000 
	 loss: 17.8043, MinusLogProbMetric: 17.8043, val_loss: 17.8623, val_MinusLogProbMetric: 17.8623

Epoch 704: val_loss did not improve from 17.85980
196/196 - 78s - loss: 17.8043 - MinusLogProbMetric: 17.8043 - val_loss: 17.8623 - val_MinusLogProbMetric: 17.8623 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 705/1000
2023-09-28 00:25:13.280 
Epoch 705/1000 
	 loss: 17.8314, MinusLogProbMetric: 17.8314, val_loss: 18.0130, val_MinusLogProbMetric: 18.0130

Epoch 705: val_loss did not improve from 17.85980
196/196 - 78s - loss: 17.8314 - MinusLogProbMetric: 17.8314 - val_loss: 18.0130 - val_MinusLogProbMetric: 18.0130 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 706/1000
2023-09-28 00:26:30.499 
Epoch 706/1000 
	 loss: 17.8159, MinusLogProbMetric: 17.8159, val_loss: 17.9284, val_MinusLogProbMetric: 17.9284

Epoch 706: val_loss did not improve from 17.85980
196/196 - 77s - loss: 17.8159 - MinusLogProbMetric: 17.8159 - val_loss: 17.9284 - val_MinusLogProbMetric: 17.9284 - lr: 1.1111e-04 - 77s/epoch - 394ms/step
Epoch 707/1000
2023-09-28 00:27:48.448 
Epoch 707/1000 
	 loss: 17.8327, MinusLogProbMetric: 17.8327, val_loss: 17.9542, val_MinusLogProbMetric: 17.9542

Epoch 707: val_loss did not improve from 17.85980
196/196 - 78s - loss: 17.8327 - MinusLogProbMetric: 17.8327 - val_loss: 17.9542 - val_MinusLogProbMetric: 17.9542 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 708/1000
2023-09-28 00:29:06.739 
Epoch 708/1000 
	 loss: 17.8015, MinusLogProbMetric: 17.8015, val_loss: 17.9873, val_MinusLogProbMetric: 17.9873

Epoch 708: val_loss did not improve from 17.85980
196/196 - 78s - loss: 17.8015 - MinusLogProbMetric: 17.8015 - val_loss: 17.9873 - val_MinusLogProbMetric: 17.9873 - lr: 1.1111e-04 - 78s/epoch - 399ms/step
Epoch 709/1000
2023-09-28 00:30:25.255 
Epoch 709/1000 
	 loss: 17.8233, MinusLogProbMetric: 17.8233, val_loss: 18.0004, val_MinusLogProbMetric: 18.0004

Epoch 709: val_loss did not improve from 17.85980
196/196 - 79s - loss: 17.8233 - MinusLogProbMetric: 17.8233 - val_loss: 18.0004 - val_MinusLogProbMetric: 18.0004 - lr: 1.1111e-04 - 79s/epoch - 401ms/step
Epoch 710/1000
2023-09-28 00:31:43.345 
Epoch 710/1000 
	 loss: 17.8033, MinusLogProbMetric: 17.8033, val_loss: 17.9439, val_MinusLogProbMetric: 17.9439

Epoch 710: val_loss did not improve from 17.85980
196/196 - 78s - loss: 17.8033 - MinusLogProbMetric: 17.8033 - val_loss: 17.9439 - val_MinusLogProbMetric: 17.9439 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 711/1000
2023-09-28 00:33:01.905 
Epoch 711/1000 
	 loss: 17.8100, MinusLogProbMetric: 17.8100, val_loss: 17.9336, val_MinusLogProbMetric: 17.9336

Epoch 711: val_loss did not improve from 17.85980
196/196 - 79s - loss: 17.8100 - MinusLogProbMetric: 17.8100 - val_loss: 17.9336 - val_MinusLogProbMetric: 17.9336 - lr: 1.1111e-04 - 79s/epoch - 401ms/step
Epoch 712/1000
2023-09-28 00:34:20.244 
Epoch 712/1000 
	 loss: 17.8017, MinusLogProbMetric: 17.8017, val_loss: 17.9809, val_MinusLogProbMetric: 17.9809

Epoch 712: val_loss did not improve from 17.85980
196/196 - 78s - loss: 17.8017 - MinusLogProbMetric: 17.8017 - val_loss: 17.9809 - val_MinusLogProbMetric: 17.9809 - lr: 1.1111e-04 - 78s/epoch - 400ms/step
Epoch 713/1000
2023-09-28 00:35:39.370 
Epoch 713/1000 
	 loss: 17.8190, MinusLogProbMetric: 17.8190, val_loss: 17.9057, val_MinusLogProbMetric: 17.9057

Epoch 713: val_loss did not improve from 17.85980
196/196 - 79s - loss: 17.8190 - MinusLogProbMetric: 17.8190 - val_loss: 17.9057 - val_MinusLogProbMetric: 17.9057 - lr: 1.1111e-04 - 79s/epoch - 404ms/step
Epoch 714/1000
2023-09-28 00:36:54.591 
Epoch 714/1000 
	 loss: 17.8267, MinusLogProbMetric: 17.8267, val_loss: 18.0427, val_MinusLogProbMetric: 18.0427

Epoch 714: val_loss did not improve from 17.85980
196/196 - 75s - loss: 17.8267 - MinusLogProbMetric: 17.8267 - val_loss: 18.0427 - val_MinusLogProbMetric: 18.0427 - lr: 1.1111e-04 - 75s/epoch - 384ms/step
Epoch 715/1000
2023-09-28 00:38:00.422 
Epoch 715/1000 
	 loss: 17.7968, MinusLogProbMetric: 17.7968, val_loss: 18.1708, val_MinusLogProbMetric: 18.1708

Epoch 715: val_loss did not improve from 17.85980
196/196 - 66s - loss: 17.7968 - MinusLogProbMetric: 17.7968 - val_loss: 18.1708 - val_MinusLogProbMetric: 18.1708 - lr: 1.1111e-04 - 66s/epoch - 336ms/step
Epoch 716/1000
2023-09-28 00:39:10.151 
Epoch 716/1000 
	 loss: 17.8101, MinusLogProbMetric: 17.8101, val_loss: 17.9354, val_MinusLogProbMetric: 17.9354

Epoch 716: val_loss did not improve from 17.85980
196/196 - 70s - loss: 17.8101 - MinusLogProbMetric: 17.8101 - val_loss: 17.9354 - val_MinusLogProbMetric: 17.9354 - lr: 1.1111e-04 - 70s/epoch - 356ms/step
Epoch 717/1000
2023-09-28 00:40:15.102 
Epoch 717/1000 
	 loss: 17.7806, MinusLogProbMetric: 17.7806, val_loss: 17.9204, val_MinusLogProbMetric: 17.9204

Epoch 717: val_loss did not improve from 17.85980
196/196 - 65s - loss: 17.7806 - MinusLogProbMetric: 17.7806 - val_loss: 17.9204 - val_MinusLogProbMetric: 17.9204 - lr: 1.1111e-04 - 65s/epoch - 331ms/step
Epoch 718/1000
2023-09-28 00:41:22.285 
Epoch 718/1000 
	 loss: 17.7904, MinusLogProbMetric: 17.7904, val_loss: 17.8694, val_MinusLogProbMetric: 17.8694

Epoch 718: val_loss did not improve from 17.85980
196/196 - 67s - loss: 17.7904 - MinusLogProbMetric: 17.7904 - val_loss: 17.8694 - val_MinusLogProbMetric: 17.8694 - lr: 1.1111e-04 - 67s/epoch - 343ms/step
Epoch 719/1000
2023-09-28 00:42:39.385 
Epoch 719/1000 
	 loss: 17.8080, MinusLogProbMetric: 17.8080, val_loss: 17.9692, val_MinusLogProbMetric: 17.9692

Epoch 719: val_loss did not improve from 17.85980
196/196 - 77s - loss: 17.8080 - MinusLogProbMetric: 17.8080 - val_loss: 17.9692 - val_MinusLogProbMetric: 17.9692 - lr: 1.1111e-04 - 77s/epoch - 393ms/step
Epoch 720/1000
2023-09-28 00:43:57.074 
Epoch 720/1000 
	 loss: 17.8010, MinusLogProbMetric: 17.8010, val_loss: 17.8919, val_MinusLogProbMetric: 17.8919

Epoch 720: val_loss did not improve from 17.85980
196/196 - 78s - loss: 17.8010 - MinusLogProbMetric: 17.8010 - val_loss: 17.8919 - val_MinusLogProbMetric: 17.8919 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 721/1000
2023-09-28 00:45:15.754 
Epoch 721/1000 
	 loss: 17.8096, MinusLogProbMetric: 17.8096, val_loss: 18.1045, val_MinusLogProbMetric: 18.1045

Epoch 721: val_loss did not improve from 17.85980
196/196 - 79s - loss: 17.8096 - MinusLogProbMetric: 17.8096 - val_loss: 18.1045 - val_MinusLogProbMetric: 18.1045 - lr: 1.1111e-04 - 79s/epoch - 401ms/step
Epoch 722/1000
2023-09-28 00:46:34.203 
Epoch 722/1000 
	 loss: 17.7807, MinusLogProbMetric: 17.7807, val_loss: 17.9104, val_MinusLogProbMetric: 17.9104

Epoch 722: val_loss did not improve from 17.85980
196/196 - 78s - loss: 17.7807 - MinusLogProbMetric: 17.7807 - val_loss: 17.9104 - val_MinusLogProbMetric: 17.9104 - lr: 1.1111e-04 - 78s/epoch - 400ms/step
Epoch 723/1000
2023-09-28 00:47:53.238 
Epoch 723/1000 
	 loss: 17.7905, MinusLogProbMetric: 17.7905, val_loss: 17.9107, val_MinusLogProbMetric: 17.9107

Epoch 723: val_loss did not improve from 17.85980
196/196 - 79s - loss: 17.7905 - MinusLogProbMetric: 17.7905 - val_loss: 17.9107 - val_MinusLogProbMetric: 17.9107 - lr: 1.1111e-04 - 79s/epoch - 403ms/step
Epoch 724/1000
2023-09-28 00:49:11.445 
Epoch 724/1000 
	 loss: 17.8026, MinusLogProbMetric: 17.8026, val_loss: 17.9742, val_MinusLogProbMetric: 17.9742

Epoch 724: val_loss did not improve from 17.85980
196/196 - 78s - loss: 17.8026 - MinusLogProbMetric: 17.8026 - val_loss: 17.9742 - val_MinusLogProbMetric: 17.9742 - lr: 1.1111e-04 - 78s/epoch - 399ms/step
Epoch 725/1000
2023-09-28 00:50:29.660 
Epoch 725/1000 
	 loss: 17.7893, MinusLogProbMetric: 17.7893, val_loss: 18.0031, val_MinusLogProbMetric: 18.0031

Epoch 725: val_loss did not improve from 17.85980
196/196 - 78s - loss: 17.7893 - MinusLogProbMetric: 17.7893 - val_loss: 18.0031 - val_MinusLogProbMetric: 18.0031 - lr: 1.1111e-04 - 78s/epoch - 399ms/step
Epoch 726/1000
2023-09-28 00:51:47.837 
Epoch 726/1000 
	 loss: 17.7893, MinusLogProbMetric: 17.7893, val_loss: 17.8422, val_MinusLogProbMetric: 17.8422

Epoch 726: val_loss improved from 17.85980 to 17.84219, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 80s - loss: 17.7893 - MinusLogProbMetric: 17.7893 - val_loss: 17.8422 - val_MinusLogProbMetric: 17.8422 - lr: 1.1111e-04 - 80s/epoch - 407ms/step
Epoch 727/1000
2023-09-28 00:53:07.823 
Epoch 727/1000 
	 loss: 17.7789, MinusLogProbMetric: 17.7789, val_loss: 17.9330, val_MinusLogProbMetric: 17.9330

Epoch 727: val_loss did not improve from 17.84219
196/196 - 78s - loss: 17.7789 - MinusLogProbMetric: 17.7789 - val_loss: 17.9330 - val_MinusLogProbMetric: 17.9330 - lr: 1.1111e-04 - 78s/epoch - 400ms/step
Epoch 728/1000
2023-09-28 00:54:26.438 
Epoch 728/1000 
	 loss: 17.7925, MinusLogProbMetric: 17.7925, val_loss: 17.9131, val_MinusLogProbMetric: 17.9131

Epoch 728: val_loss did not improve from 17.84219
196/196 - 79s - loss: 17.7925 - MinusLogProbMetric: 17.7925 - val_loss: 17.9131 - val_MinusLogProbMetric: 17.9131 - lr: 1.1111e-04 - 79s/epoch - 401ms/step
Epoch 729/1000
2023-09-28 00:55:44.768 
Epoch 729/1000 
	 loss: 17.7772, MinusLogProbMetric: 17.7772, val_loss: 17.9169, val_MinusLogProbMetric: 17.9169

Epoch 729: val_loss did not improve from 17.84219
196/196 - 78s - loss: 17.7772 - MinusLogProbMetric: 17.7772 - val_loss: 17.9169 - val_MinusLogProbMetric: 17.9169 - lr: 1.1111e-04 - 78s/epoch - 400ms/step
Epoch 730/1000
2023-09-28 00:57:02.404 
Epoch 730/1000 
	 loss: 17.7646, MinusLogProbMetric: 17.7646, val_loss: 17.9594, val_MinusLogProbMetric: 17.9594

Epoch 730: val_loss did not improve from 17.84219
196/196 - 78s - loss: 17.7646 - MinusLogProbMetric: 17.7646 - val_loss: 17.9594 - val_MinusLogProbMetric: 17.9594 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 731/1000
2023-09-28 00:58:21.225 
Epoch 731/1000 
	 loss: 17.7844, MinusLogProbMetric: 17.7844, val_loss: 18.0018, val_MinusLogProbMetric: 18.0018

Epoch 731: val_loss did not improve from 17.84219
196/196 - 79s - loss: 17.7844 - MinusLogProbMetric: 17.7844 - val_loss: 18.0018 - val_MinusLogProbMetric: 18.0018 - lr: 1.1111e-04 - 79s/epoch - 402ms/step
Epoch 732/1000
2023-09-28 00:59:39.249 
Epoch 732/1000 
	 loss: 17.7895, MinusLogProbMetric: 17.7895, val_loss: 18.0009, val_MinusLogProbMetric: 18.0009

Epoch 732: val_loss did not improve from 17.84219
196/196 - 78s - loss: 17.7895 - MinusLogProbMetric: 17.7895 - val_loss: 18.0009 - val_MinusLogProbMetric: 18.0009 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 733/1000
2023-09-28 01:00:57.206 
Epoch 733/1000 
	 loss: 17.8122, MinusLogProbMetric: 17.8122, val_loss: 17.9321, val_MinusLogProbMetric: 17.9321

Epoch 733: val_loss did not improve from 17.84219
196/196 - 78s - loss: 17.8122 - MinusLogProbMetric: 17.8122 - val_loss: 17.9321 - val_MinusLogProbMetric: 17.9321 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 734/1000
2023-09-28 01:02:16.560 
Epoch 734/1000 
	 loss: 17.7807, MinusLogProbMetric: 17.7807, val_loss: 17.9928, val_MinusLogProbMetric: 17.9928

Epoch 734: val_loss did not improve from 17.84219
196/196 - 79s - loss: 17.7807 - MinusLogProbMetric: 17.7807 - val_loss: 17.9928 - val_MinusLogProbMetric: 17.9928 - lr: 1.1111e-04 - 79s/epoch - 405ms/step
Epoch 735/1000
2023-09-28 01:03:34.534 
Epoch 735/1000 
	 loss: 17.8078, MinusLogProbMetric: 17.8078, val_loss: 17.8533, val_MinusLogProbMetric: 17.8533

Epoch 735: val_loss did not improve from 17.84219
196/196 - 78s - loss: 17.8078 - MinusLogProbMetric: 17.8078 - val_loss: 17.8533 - val_MinusLogProbMetric: 17.8533 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 736/1000
2023-09-28 01:04:52.589 
Epoch 736/1000 
	 loss: 17.7812, MinusLogProbMetric: 17.7812, val_loss: 17.9073, val_MinusLogProbMetric: 17.9073

Epoch 736: val_loss did not improve from 17.84219
196/196 - 78s - loss: 17.7812 - MinusLogProbMetric: 17.7812 - val_loss: 17.9073 - val_MinusLogProbMetric: 17.9073 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 737/1000
2023-09-28 01:06:09.713 
Epoch 737/1000 
	 loss: 17.7820, MinusLogProbMetric: 17.7820, val_loss: 17.8653, val_MinusLogProbMetric: 17.8653

Epoch 737: val_loss did not improve from 17.84219
196/196 - 77s - loss: 17.7820 - MinusLogProbMetric: 17.7820 - val_loss: 17.8653 - val_MinusLogProbMetric: 17.8653 - lr: 1.1111e-04 - 77s/epoch - 393ms/step
Epoch 738/1000
2023-09-28 01:07:27.883 
Epoch 738/1000 
	 loss: 17.7729, MinusLogProbMetric: 17.7729, val_loss: 17.8448, val_MinusLogProbMetric: 17.8448

Epoch 738: val_loss did not improve from 17.84219
196/196 - 78s - loss: 17.7729 - MinusLogProbMetric: 17.7729 - val_loss: 17.8448 - val_MinusLogProbMetric: 17.8448 - lr: 1.1111e-04 - 78s/epoch - 399ms/step
Epoch 739/1000
2023-09-28 01:08:46.070 
Epoch 739/1000 
	 loss: 17.7585, MinusLogProbMetric: 17.7585, val_loss: 17.8937, val_MinusLogProbMetric: 17.8937

Epoch 739: val_loss did not improve from 17.84219
196/196 - 78s - loss: 17.7585 - MinusLogProbMetric: 17.7585 - val_loss: 17.8937 - val_MinusLogProbMetric: 17.8937 - lr: 1.1111e-04 - 78s/epoch - 399ms/step
Epoch 740/1000
2023-09-28 01:10:03.605 
Epoch 740/1000 
	 loss: 17.7826, MinusLogProbMetric: 17.7826, val_loss: 17.9812, val_MinusLogProbMetric: 17.9812

Epoch 740: val_loss did not improve from 17.84219
196/196 - 78s - loss: 17.7826 - MinusLogProbMetric: 17.7826 - val_loss: 17.9812 - val_MinusLogProbMetric: 17.9812 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 741/1000
2023-09-28 01:11:20.627 
Epoch 741/1000 
	 loss: 17.7899, MinusLogProbMetric: 17.7899, val_loss: 17.8067, val_MinusLogProbMetric: 17.8067

Epoch 741: val_loss improved from 17.84219 to 17.80673, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 78s - loss: 17.7899 - MinusLogProbMetric: 17.7899 - val_loss: 17.8067 - val_MinusLogProbMetric: 17.8067 - lr: 1.1111e-04 - 78s/epoch - 399ms/step
Epoch 742/1000
2023-09-28 01:12:39.190 
Epoch 742/1000 
	 loss: 17.7744, MinusLogProbMetric: 17.7744, val_loss: 17.9746, val_MinusLogProbMetric: 17.9746

Epoch 742: val_loss did not improve from 17.80673
196/196 - 77s - loss: 17.7744 - MinusLogProbMetric: 17.7744 - val_loss: 17.9746 - val_MinusLogProbMetric: 17.9746 - lr: 1.1111e-04 - 77s/epoch - 394ms/step
Epoch 743/1000
2023-09-28 01:13:56.780 
Epoch 743/1000 
	 loss: 17.7941, MinusLogProbMetric: 17.7941, val_loss: 17.8811, val_MinusLogProbMetric: 17.8811

Epoch 743: val_loss did not improve from 17.80673
196/196 - 78s - loss: 17.7941 - MinusLogProbMetric: 17.7941 - val_loss: 17.8811 - val_MinusLogProbMetric: 17.8811 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 744/1000
2023-09-28 01:15:14.876 
Epoch 744/1000 
	 loss: 17.7627, MinusLogProbMetric: 17.7627, val_loss: 18.0240, val_MinusLogProbMetric: 18.0240

Epoch 744: val_loss did not improve from 17.80673
196/196 - 78s - loss: 17.7627 - MinusLogProbMetric: 17.7627 - val_loss: 18.0240 - val_MinusLogProbMetric: 18.0240 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 745/1000
2023-09-28 01:16:32.718 
Epoch 745/1000 
	 loss: 17.7771, MinusLogProbMetric: 17.7771, val_loss: 17.8283, val_MinusLogProbMetric: 17.8283

Epoch 745: val_loss did not improve from 17.80673
196/196 - 78s - loss: 17.7771 - MinusLogProbMetric: 17.7771 - val_loss: 17.8283 - val_MinusLogProbMetric: 17.8283 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 746/1000
2023-09-28 01:17:50.127 
Epoch 746/1000 
	 loss: 17.7469, MinusLogProbMetric: 17.7469, val_loss: 17.8959, val_MinusLogProbMetric: 17.8959

Epoch 746: val_loss did not improve from 17.80673
196/196 - 77s - loss: 17.7469 - MinusLogProbMetric: 17.7469 - val_loss: 17.8959 - val_MinusLogProbMetric: 17.8959 - lr: 1.1111e-04 - 77s/epoch - 395ms/step
Epoch 747/1000
2023-09-28 01:19:07.004 
Epoch 747/1000 
	 loss: 17.7521, MinusLogProbMetric: 17.7521, val_loss: 17.8447, val_MinusLogProbMetric: 17.8447

Epoch 747: val_loss did not improve from 17.80673
196/196 - 77s - loss: 17.7521 - MinusLogProbMetric: 17.7521 - val_loss: 17.8447 - val_MinusLogProbMetric: 17.8447 - lr: 1.1111e-04 - 77s/epoch - 392ms/step
Epoch 748/1000
2023-09-28 01:20:24.226 
Epoch 748/1000 
	 loss: 17.7396, MinusLogProbMetric: 17.7396, val_loss: 17.8619, val_MinusLogProbMetric: 17.8619

Epoch 748: val_loss did not improve from 17.80673
196/196 - 77s - loss: 17.7396 - MinusLogProbMetric: 17.7396 - val_loss: 17.8619 - val_MinusLogProbMetric: 17.8619 - lr: 1.1111e-04 - 77s/epoch - 394ms/step
Epoch 749/1000
2023-09-28 01:21:43.217 
Epoch 749/1000 
	 loss: 17.7648, MinusLogProbMetric: 17.7648, val_loss: 18.0446, val_MinusLogProbMetric: 18.0446

Epoch 749: val_loss did not improve from 17.80673
196/196 - 79s - loss: 17.7648 - MinusLogProbMetric: 17.7648 - val_loss: 18.0446 - val_MinusLogProbMetric: 18.0446 - lr: 1.1111e-04 - 79s/epoch - 403ms/step
Epoch 750/1000
2023-09-28 01:23:00.651 
Epoch 750/1000 
	 loss: 17.7401, MinusLogProbMetric: 17.7401, val_loss: 17.9821, val_MinusLogProbMetric: 17.9821

Epoch 750: val_loss did not improve from 17.80673
196/196 - 77s - loss: 17.7401 - MinusLogProbMetric: 17.7401 - val_loss: 17.9821 - val_MinusLogProbMetric: 17.9821 - lr: 1.1111e-04 - 77s/epoch - 395ms/step
Epoch 751/1000
2023-09-28 01:24:18.691 
Epoch 751/1000 
	 loss: 17.7453, MinusLogProbMetric: 17.7453, val_loss: 17.9172, val_MinusLogProbMetric: 17.9172

Epoch 751: val_loss did not improve from 17.80673
196/196 - 78s - loss: 17.7453 - MinusLogProbMetric: 17.7453 - val_loss: 17.9172 - val_MinusLogProbMetric: 17.9172 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 752/1000
2023-09-28 01:25:36.844 
Epoch 752/1000 
	 loss: 17.7742, MinusLogProbMetric: 17.7742, val_loss: 17.8891, val_MinusLogProbMetric: 17.8891

Epoch 752: val_loss did not improve from 17.80673
196/196 - 78s - loss: 17.7742 - MinusLogProbMetric: 17.7742 - val_loss: 17.8891 - val_MinusLogProbMetric: 17.8891 - lr: 1.1111e-04 - 78s/epoch - 399ms/step
Epoch 753/1000
2023-09-28 01:26:54.902 
Epoch 753/1000 
	 loss: 17.7316, MinusLogProbMetric: 17.7316, val_loss: 17.9390, val_MinusLogProbMetric: 17.9390

Epoch 753: val_loss did not improve from 17.80673
196/196 - 78s - loss: 17.7316 - MinusLogProbMetric: 17.7316 - val_loss: 17.9390 - val_MinusLogProbMetric: 17.9390 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 754/1000
2023-09-28 01:28:12.563 
Epoch 754/1000 
	 loss: 17.7305, MinusLogProbMetric: 17.7305, val_loss: 17.8124, val_MinusLogProbMetric: 17.8124

Epoch 754: val_loss did not improve from 17.80673
196/196 - 78s - loss: 17.7305 - MinusLogProbMetric: 17.7305 - val_loss: 17.8124 - val_MinusLogProbMetric: 17.8124 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 755/1000
2023-09-28 01:29:30.233 
Epoch 755/1000 
	 loss: 17.7490, MinusLogProbMetric: 17.7490, val_loss: 17.9615, val_MinusLogProbMetric: 17.9615

Epoch 755: val_loss did not improve from 17.80673
196/196 - 78s - loss: 17.7490 - MinusLogProbMetric: 17.7490 - val_loss: 17.9615 - val_MinusLogProbMetric: 17.9615 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 756/1000
2023-09-28 01:30:47.358 
Epoch 756/1000 
	 loss: 17.7479, MinusLogProbMetric: 17.7479, val_loss: 17.9891, val_MinusLogProbMetric: 17.9891

Epoch 756: val_loss did not improve from 17.80673
196/196 - 77s - loss: 17.7479 - MinusLogProbMetric: 17.7479 - val_loss: 17.9891 - val_MinusLogProbMetric: 17.9891 - lr: 1.1111e-04 - 77s/epoch - 393ms/step
Epoch 757/1000
2023-09-28 01:32:04.965 
Epoch 757/1000 
	 loss: 17.7512, MinusLogProbMetric: 17.7512, val_loss: 17.8868, val_MinusLogProbMetric: 17.8868

Epoch 757: val_loss did not improve from 17.80673
196/196 - 78s - loss: 17.7512 - MinusLogProbMetric: 17.7512 - val_loss: 17.8868 - val_MinusLogProbMetric: 17.8868 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 758/1000
2023-09-28 01:33:21.985 
Epoch 758/1000 
	 loss: 17.7521, MinusLogProbMetric: 17.7521, val_loss: 17.8891, val_MinusLogProbMetric: 17.8891

Epoch 758: val_loss did not improve from 17.80673
196/196 - 77s - loss: 17.7521 - MinusLogProbMetric: 17.7521 - val_loss: 17.8891 - val_MinusLogProbMetric: 17.8891 - lr: 1.1111e-04 - 77s/epoch - 393ms/step
Epoch 759/1000
2023-09-28 01:34:38.407 
Epoch 759/1000 
	 loss: 17.7418, MinusLogProbMetric: 17.7418, val_loss: 17.8477, val_MinusLogProbMetric: 17.8477

Epoch 759: val_loss did not improve from 17.80673
196/196 - 76s - loss: 17.7418 - MinusLogProbMetric: 17.7418 - val_loss: 17.8477 - val_MinusLogProbMetric: 17.8477 - lr: 1.1111e-04 - 76s/epoch - 390ms/step
Epoch 760/1000
2023-09-28 01:35:56.751 
Epoch 760/1000 
	 loss: 17.7570, MinusLogProbMetric: 17.7570, val_loss: 17.9485, val_MinusLogProbMetric: 17.9485

Epoch 760: val_loss did not improve from 17.80673
196/196 - 78s - loss: 17.7570 - MinusLogProbMetric: 17.7570 - val_loss: 17.9485 - val_MinusLogProbMetric: 17.9485 - lr: 1.1111e-04 - 78s/epoch - 400ms/step
Epoch 761/1000
2023-09-28 01:37:14.144 
Epoch 761/1000 
	 loss: 17.7411, MinusLogProbMetric: 17.7411, val_loss: 17.8537, val_MinusLogProbMetric: 17.8537

Epoch 761: val_loss did not improve from 17.80673
196/196 - 77s - loss: 17.7411 - MinusLogProbMetric: 17.7411 - val_loss: 17.8537 - val_MinusLogProbMetric: 17.8537 - lr: 1.1111e-04 - 77s/epoch - 395ms/step
Epoch 762/1000
2023-09-28 01:38:31.845 
Epoch 762/1000 
	 loss: 17.7364, MinusLogProbMetric: 17.7364, val_loss: 17.8794, val_MinusLogProbMetric: 17.8794

Epoch 762: val_loss did not improve from 17.80673
196/196 - 78s - loss: 17.7364 - MinusLogProbMetric: 17.7364 - val_loss: 17.8794 - val_MinusLogProbMetric: 17.8794 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 763/1000
2023-09-28 01:39:47.291 
Epoch 763/1000 
	 loss: 17.7458, MinusLogProbMetric: 17.7458, val_loss: 17.9012, val_MinusLogProbMetric: 17.9012

Epoch 763: val_loss did not improve from 17.80673
196/196 - 75s - loss: 17.7458 - MinusLogProbMetric: 17.7458 - val_loss: 17.9012 - val_MinusLogProbMetric: 17.9012 - lr: 1.1111e-04 - 75s/epoch - 385ms/step
Epoch 764/1000
2023-09-28 01:40:53.343 
Epoch 764/1000 
	 loss: 17.7448, MinusLogProbMetric: 17.7448, val_loss: 17.8561, val_MinusLogProbMetric: 17.8561

Epoch 764: val_loss did not improve from 17.80673
196/196 - 66s - loss: 17.7448 - MinusLogProbMetric: 17.7448 - val_loss: 17.8561 - val_MinusLogProbMetric: 17.8561 - lr: 1.1111e-04 - 66s/epoch - 337ms/step
Epoch 765/1000
2023-09-28 01:41:57.315 
Epoch 765/1000 
	 loss: 17.7658, MinusLogProbMetric: 17.7658, val_loss: 17.8973, val_MinusLogProbMetric: 17.8973

Epoch 765: val_loss did not improve from 17.80673
196/196 - 64s - loss: 17.7658 - MinusLogProbMetric: 17.7658 - val_loss: 17.8973 - val_MinusLogProbMetric: 17.8973 - lr: 1.1111e-04 - 64s/epoch - 326ms/step
Epoch 766/1000
2023-09-28 01:43:06.329 
Epoch 766/1000 
	 loss: 17.7507, MinusLogProbMetric: 17.7507, val_loss: 17.8416, val_MinusLogProbMetric: 17.8416

Epoch 766: val_loss did not improve from 17.80673
196/196 - 69s - loss: 17.7507 - MinusLogProbMetric: 17.7507 - val_loss: 17.8416 - val_MinusLogProbMetric: 17.8416 - lr: 1.1111e-04 - 69s/epoch - 352ms/step
Epoch 767/1000
2023-09-28 01:44:09.382 
Epoch 767/1000 
	 loss: 17.7324, MinusLogProbMetric: 17.7324, val_loss: 18.0383, val_MinusLogProbMetric: 18.0383

Epoch 767: val_loss did not improve from 17.80673
196/196 - 63s - loss: 17.7324 - MinusLogProbMetric: 17.7324 - val_loss: 18.0383 - val_MinusLogProbMetric: 18.0383 - lr: 1.1111e-04 - 63s/epoch - 322ms/step
Epoch 768/1000
2023-09-28 01:45:24.527 
Epoch 768/1000 
	 loss: 17.7531, MinusLogProbMetric: 17.7531, val_loss: 17.8361, val_MinusLogProbMetric: 17.8361

Epoch 768: val_loss did not improve from 17.80673
196/196 - 75s - loss: 17.7531 - MinusLogProbMetric: 17.7531 - val_loss: 17.8361 - val_MinusLogProbMetric: 17.8361 - lr: 1.1111e-04 - 75s/epoch - 383ms/step
Epoch 769/1000
2023-09-28 01:46:41.316 
Epoch 769/1000 
	 loss: 17.7413, MinusLogProbMetric: 17.7413, val_loss: 17.8322, val_MinusLogProbMetric: 17.8322

Epoch 769: val_loss did not improve from 17.80673
196/196 - 77s - loss: 17.7413 - MinusLogProbMetric: 17.7413 - val_loss: 17.8322 - val_MinusLogProbMetric: 17.8322 - lr: 1.1111e-04 - 77s/epoch - 392ms/step
Epoch 770/1000
2023-09-28 01:47:59.216 
Epoch 770/1000 
	 loss: 17.7394, MinusLogProbMetric: 17.7394, val_loss: 17.9157, val_MinusLogProbMetric: 17.9157

Epoch 770: val_loss did not improve from 17.80673
196/196 - 78s - loss: 17.7394 - MinusLogProbMetric: 17.7394 - val_loss: 17.9157 - val_MinusLogProbMetric: 17.9157 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 771/1000
2023-09-28 01:49:16.365 
Epoch 771/1000 
	 loss: 17.7236, MinusLogProbMetric: 17.7236, val_loss: 17.8967, val_MinusLogProbMetric: 17.8967

Epoch 771: val_loss did not improve from 17.80673
196/196 - 77s - loss: 17.7236 - MinusLogProbMetric: 17.7236 - val_loss: 17.8967 - val_MinusLogProbMetric: 17.8967 - lr: 1.1111e-04 - 77s/epoch - 394ms/step
Epoch 772/1000
2023-09-28 01:50:34.045 
Epoch 772/1000 
	 loss: 17.7197, MinusLogProbMetric: 17.7197, val_loss: 17.9200, val_MinusLogProbMetric: 17.9200

Epoch 772: val_loss did not improve from 17.80673
196/196 - 78s - loss: 17.7197 - MinusLogProbMetric: 17.7197 - val_loss: 17.9200 - val_MinusLogProbMetric: 17.9200 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 773/1000
2023-09-28 01:51:52.611 
Epoch 773/1000 
	 loss: 17.7411, MinusLogProbMetric: 17.7411, val_loss: 17.7551, val_MinusLogProbMetric: 17.7551

Epoch 773: val_loss improved from 17.80673 to 17.75511, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 80s - loss: 17.7411 - MinusLogProbMetric: 17.7411 - val_loss: 17.7551 - val_MinusLogProbMetric: 17.7551 - lr: 1.1111e-04 - 80s/epoch - 408ms/step
Epoch 774/1000
2023-09-28 01:53:11.965 
Epoch 774/1000 
	 loss: 17.7338, MinusLogProbMetric: 17.7338, val_loss: 17.9536, val_MinusLogProbMetric: 17.9536

Epoch 774: val_loss did not improve from 17.75511
196/196 - 78s - loss: 17.7338 - MinusLogProbMetric: 17.7338 - val_loss: 17.9536 - val_MinusLogProbMetric: 17.9536 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 775/1000
2023-09-28 01:54:28.532 
Epoch 775/1000 
	 loss: 17.7256, MinusLogProbMetric: 17.7256, val_loss: 17.8403, val_MinusLogProbMetric: 17.8403

Epoch 775: val_loss did not improve from 17.75511
196/196 - 77s - loss: 17.7256 - MinusLogProbMetric: 17.7256 - val_loss: 17.8403 - val_MinusLogProbMetric: 17.8403 - lr: 1.1111e-04 - 77s/epoch - 391ms/step
Epoch 776/1000
2023-09-28 01:55:46.940 
Epoch 776/1000 
	 loss: 17.7459, MinusLogProbMetric: 17.7459, val_loss: 17.7611, val_MinusLogProbMetric: 17.7611

Epoch 776: val_loss did not improve from 17.75511
196/196 - 78s - loss: 17.7459 - MinusLogProbMetric: 17.7459 - val_loss: 17.7611 - val_MinusLogProbMetric: 17.7611 - lr: 1.1111e-04 - 78s/epoch - 400ms/step
Epoch 777/1000
2023-09-28 01:57:04.908 
Epoch 777/1000 
	 loss: 17.7089, MinusLogProbMetric: 17.7089, val_loss: 17.8487, val_MinusLogProbMetric: 17.8487

Epoch 777: val_loss did not improve from 17.75511
196/196 - 78s - loss: 17.7089 - MinusLogProbMetric: 17.7089 - val_loss: 17.8487 - val_MinusLogProbMetric: 17.8487 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 778/1000
2023-09-28 01:58:23.106 
Epoch 778/1000 
	 loss: 17.7166, MinusLogProbMetric: 17.7166, val_loss: 17.9055, val_MinusLogProbMetric: 17.9055

Epoch 778: val_loss did not improve from 17.75511
196/196 - 78s - loss: 17.7166 - MinusLogProbMetric: 17.7166 - val_loss: 17.9055 - val_MinusLogProbMetric: 17.9055 - lr: 1.1111e-04 - 78s/epoch - 399ms/step
Epoch 779/1000
2023-09-28 01:59:40.643 
Epoch 779/1000 
	 loss: 17.7122, MinusLogProbMetric: 17.7122, val_loss: 17.8735, val_MinusLogProbMetric: 17.8735

Epoch 779: val_loss did not improve from 17.75511
196/196 - 78s - loss: 17.7122 - MinusLogProbMetric: 17.7122 - val_loss: 17.8735 - val_MinusLogProbMetric: 17.8735 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 780/1000
2023-09-28 02:00:58.688 
Epoch 780/1000 
	 loss: 17.7178, MinusLogProbMetric: 17.7178, val_loss: 17.8117, val_MinusLogProbMetric: 17.8117

Epoch 780: val_loss did not improve from 17.75511
196/196 - 78s - loss: 17.7178 - MinusLogProbMetric: 17.7178 - val_loss: 17.8117 - val_MinusLogProbMetric: 17.8117 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 781/1000
2023-09-28 02:02:16.435 
Epoch 781/1000 
	 loss: 17.7115, MinusLogProbMetric: 17.7115, val_loss: 17.8695, val_MinusLogProbMetric: 17.8695

Epoch 781: val_loss did not improve from 17.75511
196/196 - 78s - loss: 17.7115 - MinusLogProbMetric: 17.7115 - val_loss: 17.8695 - val_MinusLogProbMetric: 17.8695 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 782/1000
2023-09-28 02:03:34.581 
Epoch 782/1000 
	 loss: 17.7093, MinusLogProbMetric: 17.7093, val_loss: 18.0147, val_MinusLogProbMetric: 18.0147

Epoch 782: val_loss did not improve from 17.75511
196/196 - 78s - loss: 17.7093 - MinusLogProbMetric: 17.7093 - val_loss: 18.0147 - val_MinusLogProbMetric: 18.0147 - lr: 1.1111e-04 - 78s/epoch - 399ms/step
Epoch 783/1000
2023-09-28 02:04:51.145 
Epoch 783/1000 
	 loss: 17.7174, MinusLogProbMetric: 17.7174, val_loss: 17.8800, val_MinusLogProbMetric: 17.8800

Epoch 783: val_loss did not improve from 17.75511
196/196 - 77s - loss: 17.7174 - MinusLogProbMetric: 17.7174 - val_loss: 17.8800 - val_MinusLogProbMetric: 17.8800 - lr: 1.1111e-04 - 77s/epoch - 391ms/step
Epoch 784/1000
2023-09-28 02:06:09.901 
Epoch 784/1000 
	 loss: 17.7096, MinusLogProbMetric: 17.7096, val_loss: 17.9557, val_MinusLogProbMetric: 17.9557

Epoch 784: val_loss did not improve from 17.75511
196/196 - 79s - loss: 17.7096 - MinusLogProbMetric: 17.7096 - val_loss: 17.9557 - val_MinusLogProbMetric: 17.9557 - lr: 1.1111e-04 - 79s/epoch - 402ms/step
Epoch 785/1000
2023-09-28 02:07:27.860 
Epoch 785/1000 
	 loss: 17.7128, MinusLogProbMetric: 17.7128, val_loss: 17.7959, val_MinusLogProbMetric: 17.7959

Epoch 785: val_loss did not improve from 17.75511
196/196 - 78s - loss: 17.7128 - MinusLogProbMetric: 17.7128 - val_loss: 17.7959 - val_MinusLogProbMetric: 17.7959 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 786/1000
2023-09-28 02:08:46.179 
Epoch 786/1000 
	 loss: 17.7052, MinusLogProbMetric: 17.7052, val_loss: 17.9750, val_MinusLogProbMetric: 17.9750

Epoch 786: val_loss did not improve from 17.75511
196/196 - 78s - loss: 17.7052 - MinusLogProbMetric: 17.7052 - val_loss: 17.9750 - val_MinusLogProbMetric: 17.9750 - lr: 1.1111e-04 - 78s/epoch - 400ms/step
Epoch 787/1000
2023-09-28 02:10:04.455 
Epoch 787/1000 
	 loss: 17.6909, MinusLogProbMetric: 17.6909, val_loss: 17.8377, val_MinusLogProbMetric: 17.8377

Epoch 787: val_loss did not improve from 17.75511
196/196 - 78s - loss: 17.6909 - MinusLogProbMetric: 17.6909 - val_loss: 17.8377 - val_MinusLogProbMetric: 17.8377 - lr: 1.1111e-04 - 78s/epoch - 399ms/step
Epoch 788/1000
2023-09-28 02:11:22.380 
Epoch 788/1000 
	 loss: 17.7095, MinusLogProbMetric: 17.7095, val_loss: 17.8110, val_MinusLogProbMetric: 17.8110

Epoch 788: val_loss did not improve from 17.75511
196/196 - 78s - loss: 17.7095 - MinusLogProbMetric: 17.7095 - val_loss: 17.8110 - val_MinusLogProbMetric: 17.8110 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 789/1000
2023-09-28 02:12:40.903 
Epoch 789/1000 
	 loss: 17.7005, MinusLogProbMetric: 17.7005, val_loss: 17.8214, val_MinusLogProbMetric: 17.8214

Epoch 789: val_loss did not improve from 17.75511
196/196 - 79s - loss: 17.7005 - MinusLogProbMetric: 17.7005 - val_loss: 17.8214 - val_MinusLogProbMetric: 17.8214 - lr: 1.1111e-04 - 79s/epoch - 401ms/step
Epoch 790/1000
2023-09-28 02:13:59.460 
Epoch 790/1000 
	 loss: 17.7104, MinusLogProbMetric: 17.7104, val_loss: 17.7902, val_MinusLogProbMetric: 17.7902

Epoch 790: val_loss did not improve from 17.75511
196/196 - 79s - loss: 17.7104 - MinusLogProbMetric: 17.7104 - val_loss: 17.7902 - val_MinusLogProbMetric: 17.7902 - lr: 1.1111e-04 - 79s/epoch - 401ms/step
Epoch 791/1000
2023-09-28 02:15:17.377 
Epoch 791/1000 
	 loss: 17.6936, MinusLogProbMetric: 17.6936, val_loss: 17.7603, val_MinusLogProbMetric: 17.7603

Epoch 791: val_loss did not improve from 17.75511
196/196 - 78s - loss: 17.6936 - MinusLogProbMetric: 17.6936 - val_loss: 17.7603 - val_MinusLogProbMetric: 17.7603 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 792/1000
2023-09-28 02:16:34.223 
Epoch 792/1000 
	 loss: 17.7076, MinusLogProbMetric: 17.7076, val_loss: 17.9342, val_MinusLogProbMetric: 17.9342

Epoch 792: val_loss did not improve from 17.75511
196/196 - 77s - loss: 17.7076 - MinusLogProbMetric: 17.7076 - val_loss: 17.9342 - val_MinusLogProbMetric: 17.9342 - lr: 1.1111e-04 - 77s/epoch - 392ms/step
Epoch 793/1000
2023-09-28 02:17:52.974 
Epoch 793/1000 
	 loss: 17.7009, MinusLogProbMetric: 17.7009, val_loss: 17.9842, val_MinusLogProbMetric: 17.9842

Epoch 793: val_loss did not improve from 17.75511
196/196 - 79s - loss: 17.7009 - MinusLogProbMetric: 17.7009 - val_loss: 17.9842 - val_MinusLogProbMetric: 17.9842 - lr: 1.1111e-04 - 79s/epoch - 402ms/step
Epoch 794/1000
2023-09-28 02:19:10.793 
Epoch 794/1000 
	 loss: 17.7169, MinusLogProbMetric: 17.7169, val_loss: 17.8386, val_MinusLogProbMetric: 17.8386

Epoch 794: val_loss did not improve from 17.75511
196/196 - 78s - loss: 17.7169 - MinusLogProbMetric: 17.7169 - val_loss: 17.8386 - val_MinusLogProbMetric: 17.8386 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 795/1000
2023-09-28 02:20:28.805 
Epoch 795/1000 
	 loss: 17.6969, MinusLogProbMetric: 17.6969, val_loss: 17.8276, val_MinusLogProbMetric: 17.8276

Epoch 795: val_loss did not improve from 17.75511
196/196 - 78s - loss: 17.6969 - MinusLogProbMetric: 17.6969 - val_loss: 17.8276 - val_MinusLogProbMetric: 17.8276 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 796/1000
2023-09-28 02:21:47.048 
Epoch 796/1000 
	 loss: 17.7210, MinusLogProbMetric: 17.7210, val_loss: 17.8399, val_MinusLogProbMetric: 17.8399

Epoch 796: val_loss did not improve from 17.75511
196/196 - 78s - loss: 17.7210 - MinusLogProbMetric: 17.7210 - val_loss: 17.8399 - val_MinusLogProbMetric: 17.8399 - lr: 1.1111e-04 - 78s/epoch - 399ms/step
Epoch 797/1000
2023-09-28 02:23:04.626 
Epoch 797/1000 
	 loss: 17.6851, MinusLogProbMetric: 17.6851, val_loss: 17.7706, val_MinusLogProbMetric: 17.7706

Epoch 797: val_loss did not improve from 17.75511
196/196 - 78s - loss: 17.6851 - MinusLogProbMetric: 17.6851 - val_loss: 17.7706 - val_MinusLogProbMetric: 17.7706 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 798/1000
2023-09-28 02:24:22.733 
Epoch 798/1000 
	 loss: 17.7043, MinusLogProbMetric: 17.7043, val_loss: 17.9987, val_MinusLogProbMetric: 17.9987

Epoch 798: val_loss did not improve from 17.75511
196/196 - 78s - loss: 17.7043 - MinusLogProbMetric: 17.7043 - val_loss: 17.9987 - val_MinusLogProbMetric: 17.9987 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 799/1000
2023-09-28 02:25:40.824 
Epoch 799/1000 
	 loss: 17.7000, MinusLogProbMetric: 17.7000, val_loss: 17.8124, val_MinusLogProbMetric: 17.8124

Epoch 799: val_loss did not improve from 17.75511
196/196 - 78s - loss: 17.7000 - MinusLogProbMetric: 17.7000 - val_loss: 17.8124 - val_MinusLogProbMetric: 17.8124 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 800/1000
2023-09-28 02:26:59.470 
Epoch 800/1000 
	 loss: 17.6665, MinusLogProbMetric: 17.6665, val_loss: 17.8401, val_MinusLogProbMetric: 17.8401

Epoch 800: val_loss did not improve from 17.75511
196/196 - 79s - loss: 17.6665 - MinusLogProbMetric: 17.6665 - val_loss: 17.8401 - val_MinusLogProbMetric: 17.8401 - lr: 1.1111e-04 - 79s/epoch - 401ms/step
Epoch 801/1000
2023-09-28 02:28:17.160 
Epoch 801/1000 
	 loss: 17.6896, MinusLogProbMetric: 17.6896, val_loss: 17.7951, val_MinusLogProbMetric: 17.7951

Epoch 801: val_loss did not improve from 17.75511
196/196 - 78s - loss: 17.6896 - MinusLogProbMetric: 17.6896 - val_loss: 17.7951 - val_MinusLogProbMetric: 17.7951 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 802/1000
2023-09-28 02:29:34.463 
Epoch 802/1000 
	 loss: 17.7148, MinusLogProbMetric: 17.7148, val_loss: 18.0223, val_MinusLogProbMetric: 18.0223

Epoch 802: val_loss did not improve from 17.75511
196/196 - 77s - loss: 17.7148 - MinusLogProbMetric: 17.7148 - val_loss: 18.0223 - val_MinusLogProbMetric: 18.0223 - lr: 1.1111e-04 - 77s/epoch - 394ms/step
Epoch 803/1000
2023-09-28 02:30:52.258 
Epoch 803/1000 
	 loss: 17.6900, MinusLogProbMetric: 17.6900, val_loss: 17.8548, val_MinusLogProbMetric: 17.8548

Epoch 803: val_loss did not improve from 17.75511
196/196 - 78s - loss: 17.6900 - MinusLogProbMetric: 17.6900 - val_loss: 17.8548 - val_MinusLogProbMetric: 17.8548 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 804/1000
2023-09-28 02:32:10.417 
Epoch 804/1000 
	 loss: 17.6849, MinusLogProbMetric: 17.6849, val_loss: 17.8515, val_MinusLogProbMetric: 17.8515

Epoch 804: val_loss did not improve from 17.75511
196/196 - 78s - loss: 17.6849 - MinusLogProbMetric: 17.6849 - val_loss: 17.8515 - val_MinusLogProbMetric: 17.8515 - lr: 1.1111e-04 - 78s/epoch - 399ms/step
Epoch 805/1000
2023-09-28 02:33:29.235 
Epoch 805/1000 
	 loss: 17.6712, MinusLogProbMetric: 17.6712, val_loss: 17.8673, val_MinusLogProbMetric: 17.8673

Epoch 805: val_loss did not improve from 17.75511
196/196 - 79s - loss: 17.6712 - MinusLogProbMetric: 17.6712 - val_loss: 17.8673 - val_MinusLogProbMetric: 17.8673 - lr: 1.1111e-04 - 79s/epoch - 402ms/step
Epoch 806/1000
2023-09-28 02:34:47.350 
Epoch 806/1000 
	 loss: 17.6686, MinusLogProbMetric: 17.6686, val_loss: 17.7741, val_MinusLogProbMetric: 17.7741

Epoch 806: val_loss did not improve from 17.75511
196/196 - 78s - loss: 17.6686 - MinusLogProbMetric: 17.6686 - val_loss: 17.7741 - val_MinusLogProbMetric: 17.7741 - lr: 1.1111e-04 - 78s/epoch - 399ms/step
Epoch 807/1000
2023-09-28 02:36:04.559 
Epoch 807/1000 
	 loss: 17.6827, MinusLogProbMetric: 17.6827, val_loss: 17.9063, val_MinusLogProbMetric: 17.9063

Epoch 807: val_loss did not improve from 17.75511
196/196 - 77s - loss: 17.6827 - MinusLogProbMetric: 17.6827 - val_loss: 17.9063 - val_MinusLogProbMetric: 17.9063 - lr: 1.1111e-04 - 77s/epoch - 394ms/step
Epoch 808/1000
2023-09-28 02:37:23.254 
Epoch 808/1000 
	 loss: 17.6801, MinusLogProbMetric: 17.6801, val_loss: 17.7759, val_MinusLogProbMetric: 17.7759

Epoch 808: val_loss did not improve from 17.75511
196/196 - 79s - loss: 17.6801 - MinusLogProbMetric: 17.6801 - val_loss: 17.7759 - val_MinusLogProbMetric: 17.7759 - lr: 1.1111e-04 - 79s/epoch - 401ms/step
Epoch 809/1000
2023-09-28 02:38:41.401 
Epoch 809/1000 
	 loss: 17.6799, MinusLogProbMetric: 17.6799, val_loss: 18.0150, val_MinusLogProbMetric: 18.0150

Epoch 809: val_loss did not improve from 17.75511
196/196 - 78s - loss: 17.6799 - MinusLogProbMetric: 17.6799 - val_loss: 18.0150 - val_MinusLogProbMetric: 18.0150 - lr: 1.1111e-04 - 78s/epoch - 399ms/step
Epoch 810/1000
2023-09-28 02:39:59.430 
Epoch 810/1000 
	 loss: 17.6815, MinusLogProbMetric: 17.6815, val_loss: 17.7693, val_MinusLogProbMetric: 17.7693

Epoch 810: val_loss did not improve from 17.75511
196/196 - 78s - loss: 17.6815 - MinusLogProbMetric: 17.6815 - val_loss: 17.7693 - val_MinusLogProbMetric: 17.7693 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 811/1000
2023-09-28 02:41:18.855 
Epoch 811/1000 
	 loss: 17.6738, MinusLogProbMetric: 17.6738, val_loss: 17.9704, val_MinusLogProbMetric: 17.9704

Epoch 811: val_loss did not improve from 17.75511
196/196 - 79s - loss: 17.6738 - MinusLogProbMetric: 17.6738 - val_loss: 17.9704 - val_MinusLogProbMetric: 17.9704 - lr: 1.1111e-04 - 79s/epoch - 405ms/step
Epoch 812/1000
2023-09-28 02:42:36.606 
Epoch 812/1000 
	 loss: 17.6675, MinusLogProbMetric: 17.6675, val_loss: 17.7572, val_MinusLogProbMetric: 17.7572

Epoch 812: val_loss did not improve from 17.75511
196/196 - 78s - loss: 17.6675 - MinusLogProbMetric: 17.6675 - val_loss: 17.7572 - val_MinusLogProbMetric: 17.7572 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 813/1000
2023-09-28 02:43:54.293 
Epoch 813/1000 
	 loss: 17.6976, MinusLogProbMetric: 17.6976, val_loss: 17.9062, val_MinusLogProbMetric: 17.9062

Epoch 813: val_loss did not improve from 17.75511
196/196 - 78s - loss: 17.6976 - MinusLogProbMetric: 17.6976 - val_loss: 17.9062 - val_MinusLogProbMetric: 17.9062 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 814/1000
2023-09-28 02:45:11.859 
Epoch 814/1000 
	 loss: 17.6684, MinusLogProbMetric: 17.6684, val_loss: 17.8457, val_MinusLogProbMetric: 17.8457

Epoch 814: val_loss did not improve from 17.75511
196/196 - 78s - loss: 17.6684 - MinusLogProbMetric: 17.6684 - val_loss: 17.8457 - val_MinusLogProbMetric: 17.8457 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 815/1000
2023-09-28 02:46:30.872 
Epoch 815/1000 
	 loss: 17.6904, MinusLogProbMetric: 17.6904, val_loss: 17.8263, val_MinusLogProbMetric: 17.8263

Epoch 815: val_loss did not improve from 17.75511
196/196 - 79s - loss: 17.6904 - MinusLogProbMetric: 17.6904 - val_loss: 17.8263 - val_MinusLogProbMetric: 17.8263 - lr: 1.1111e-04 - 79s/epoch - 403ms/step
Epoch 816/1000
2023-09-28 02:47:48.971 
Epoch 816/1000 
	 loss: 17.6849, MinusLogProbMetric: 17.6849, val_loss: 17.8213, val_MinusLogProbMetric: 17.8213

Epoch 816: val_loss did not improve from 17.75511
196/196 - 78s - loss: 17.6849 - MinusLogProbMetric: 17.6849 - val_loss: 17.8213 - val_MinusLogProbMetric: 17.8213 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 817/1000
2023-09-28 02:49:07.663 
Epoch 817/1000 
	 loss: 17.6773, MinusLogProbMetric: 17.6773, val_loss: 17.8477, val_MinusLogProbMetric: 17.8477

Epoch 817: val_loss did not improve from 17.75511
196/196 - 79s - loss: 17.6773 - MinusLogProbMetric: 17.6773 - val_loss: 17.8477 - val_MinusLogProbMetric: 17.8477 - lr: 1.1111e-04 - 79s/epoch - 401ms/step
Epoch 818/1000
2023-09-28 02:50:26.759 
Epoch 818/1000 
	 loss: 17.6522, MinusLogProbMetric: 17.6522, val_loss: 17.7914, val_MinusLogProbMetric: 17.7914

Epoch 818: val_loss did not improve from 17.75511
196/196 - 79s - loss: 17.6522 - MinusLogProbMetric: 17.6522 - val_loss: 17.7914 - val_MinusLogProbMetric: 17.7914 - lr: 1.1111e-04 - 79s/epoch - 404ms/step
Epoch 819/1000
2023-09-28 02:51:44.582 
Epoch 819/1000 
	 loss: 17.6650, MinusLogProbMetric: 17.6650, val_loss: 17.7502, val_MinusLogProbMetric: 17.7502

Epoch 819: val_loss improved from 17.75511 to 17.75020, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 79s - loss: 17.6650 - MinusLogProbMetric: 17.6650 - val_loss: 17.7502 - val_MinusLogProbMetric: 17.7502 - lr: 1.1111e-04 - 79s/epoch - 403ms/step
Epoch 820/1000
2023-09-28 02:53:03.085 
Epoch 820/1000 
	 loss: 17.6561, MinusLogProbMetric: 17.6561, val_loss: 17.8256, val_MinusLogProbMetric: 17.8256

Epoch 820: val_loss did not improve from 17.75020
196/196 - 77s - loss: 17.6561 - MinusLogProbMetric: 17.6561 - val_loss: 17.8256 - val_MinusLogProbMetric: 17.8256 - lr: 1.1111e-04 - 77s/epoch - 394ms/step
Epoch 821/1000
2023-09-28 02:54:20.739 
Epoch 821/1000 
	 loss: 17.6615, MinusLogProbMetric: 17.6615, val_loss: 17.7353, val_MinusLogProbMetric: 17.7353

Epoch 821: val_loss improved from 17.75020 to 17.73532, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 79s - loss: 17.6615 - MinusLogProbMetric: 17.6615 - val_loss: 17.7353 - val_MinusLogProbMetric: 17.7353 - lr: 1.1111e-04 - 79s/epoch - 403ms/step
Epoch 822/1000
2023-09-28 02:55:39.878 
Epoch 822/1000 
	 loss: 17.6710, MinusLogProbMetric: 17.6710, val_loss: 17.7227, val_MinusLogProbMetric: 17.7227

Epoch 822: val_loss improved from 17.73532 to 17.72265, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 79s - loss: 17.6710 - MinusLogProbMetric: 17.6710 - val_loss: 17.7227 - val_MinusLogProbMetric: 17.7227 - lr: 1.1111e-04 - 79s/epoch - 403ms/step
Epoch 823/1000
2023-09-28 02:56:57.867 
Epoch 823/1000 
	 loss: 17.6674, MinusLogProbMetric: 17.6674, val_loss: 17.8709, val_MinusLogProbMetric: 17.8709

Epoch 823: val_loss did not improve from 17.72265
196/196 - 77s - loss: 17.6674 - MinusLogProbMetric: 17.6674 - val_loss: 17.8709 - val_MinusLogProbMetric: 17.8709 - lr: 1.1111e-04 - 77s/epoch - 392ms/step
Epoch 824/1000
2023-09-28 02:58:16.450 
Epoch 824/1000 
	 loss: 17.6665, MinusLogProbMetric: 17.6665, val_loss: 17.8134, val_MinusLogProbMetric: 17.8134

Epoch 824: val_loss did not improve from 17.72265
196/196 - 79s - loss: 17.6665 - MinusLogProbMetric: 17.6665 - val_loss: 17.8134 - val_MinusLogProbMetric: 17.8134 - lr: 1.1111e-04 - 79s/epoch - 401ms/step
Epoch 825/1000
2023-09-28 02:59:34.910 
Epoch 825/1000 
	 loss: 17.6687, MinusLogProbMetric: 17.6687, val_loss: 17.7964, val_MinusLogProbMetric: 17.7964

Epoch 825: val_loss did not improve from 17.72265
196/196 - 78s - loss: 17.6687 - MinusLogProbMetric: 17.6687 - val_loss: 17.7964 - val_MinusLogProbMetric: 17.7964 - lr: 1.1111e-04 - 78s/epoch - 400ms/step
Epoch 826/1000
2023-09-28 03:00:53.954 
Epoch 826/1000 
	 loss: 17.6531, MinusLogProbMetric: 17.6531, val_loss: 17.7586, val_MinusLogProbMetric: 17.7586

Epoch 826: val_loss did not improve from 17.72265
196/196 - 79s - loss: 17.6531 - MinusLogProbMetric: 17.6531 - val_loss: 17.7586 - val_MinusLogProbMetric: 17.7586 - lr: 1.1111e-04 - 79s/epoch - 403ms/step
Epoch 827/1000
2023-09-28 03:02:11.009 
Epoch 827/1000 
	 loss: 17.6305, MinusLogProbMetric: 17.6305, val_loss: 17.8640, val_MinusLogProbMetric: 17.8640

Epoch 827: val_loss did not improve from 17.72265
196/196 - 77s - loss: 17.6305 - MinusLogProbMetric: 17.6305 - val_loss: 17.8640 - val_MinusLogProbMetric: 17.8640 - lr: 1.1111e-04 - 77s/epoch - 393ms/step
Epoch 828/1000
2023-09-28 03:03:28.913 
Epoch 828/1000 
	 loss: 17.6655, MinusLogProbMetric: 17.6655, val_loss: 17.9585, val_MinusLogProbMetric: 17.9585

Epoch 828: val_loss did not improve from 17.72265
196/196 - 78s - loss: 17.6655 - MinusLogProbMetric: 17.6655 - val_loss: 17.9585 - val_MinusLogProbMetric: 17.9585 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 829/1000
2023-09-28 03:04:46.595 
Epoch 829/1000 
	 loss: 17.6318, MinusLogProbMetric: 17.6318, val_loss: 17.8168, val_MinusLogProbMetric: 17.8168

Epoch 829: val_loss did not improve from 17.72265
196/196 - 78s - loss: 17.6318 - MinusLogProbMetric: 17.6318 - val_loss: 17.8168 - val_MinusLogProbMetric: 17.8168 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 830/1000
2023-09-28 03:06:03.499 
Epoch 830/1000 
	 loss: 17.6664, MinusLogProbMetric: 17.6664, val_loss: 17.7575, val_MinusLogProbMetric: 17.7575

Epoch 830: val_loss did not improve from 17.72265
196/196 - 77s - loss: 17.6664 - MinusLogProbMetric: 17.6664 - val_loss: 17.7575 - val_MinusLogProbMetric: 17.7575 - lr: 1.1111e-04 - 77s/epoch - 392ms/step
Epoch 831/1000
2023-09-28 03:07:21.248 
Epoch 831/1000 
	 loss: 17.6785, MinusLogProbMetric: 17.6785, val_loss: 17.8498, val_MinusLogProbMetric: 17.8498

Epoch 831: val_loss did not improve from 17.72265
196/196 - 78s - loss: 17.6785 - MinusLogProbMetric: 17.6785 - val_loss: 17.8498 - val_MinusLogProbMetric: 17.8498 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 832/1000
2023-09-28 03:08:38.721 
Epoch 832/1000 
	 loss: 17.6504, MinusLogProbMetric: 17.6504, val_loss: 17.8093, val_MinusLogProbMetric: 17.8093

Epoch 832: val_loss did not improve from 17.72265
196/196 - 77s - loss: 17.6504 - MinusLogProbMetric: 17.6504 - val_loss: 17.8093 - val_MinusLogProbMetric: 17.8093 - lr: 1.1111e-04 - 77s/epoch - 395ms/step
Epoch 833/1000
2023-09-28 03:09:56.539 
Epoch 833/1000 
	 loss: 17.6471, MinusLogProbMetric: 17.6471, val_loss: 17.7579, val_MinusLogProbMetric: 17.7579

Epoch 833: val_loss did not improve from 17.72265
196/196 - 78s - loss: 17.6471 - MinusLogProbMetric: 17.6471 - val_loss: 17.7579 - val_MinusLogProbMetric: 17.7579 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 834/1000
2023-09-28 03:11:14.891 
Epoch 834/1000 
	 loss: 17.6507, MinusLogProbMetric: 17.6507, val_loss: 17.8910, val_MinusLogProbMetric: 17.8910

Epoch 834: val_loss did not improve from 17.72265
196/196 - 78s - loss: 17.6507 - MinusLogProbMetric: 17.6507 - val_loss: 17.8910 - val_MinusLogProbMetric: 17.8910 - lr: 1.1111e-04 - 78s/epoch - 400ms/step
Epoch 835/1000
2023-09-28 03:12:32.744 
Epoch 835/1000 
	 loss: 17.6556, MinusLogProbMetric: 17.6556, val_loss: 17.7454, val_MinusLogProbMetric: 17.7454

Epoch 835: val_loss did not improve from 17.72265
196/196 - 78s - loss: 17.6556 - MinusLogProbMetric: 17.6556 - val_loss: 17.7454 - val_MinusLogProbMetric: 17.7454 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 836/1000
2023-09-28 03:13:50.550 
Epoch 836/1000 
	 loss: 17.6393, MinusLogProbMetric: 17.6393, val_loss: 17.8012, val_MinusLogProbMetric: 17.8012

Epoch 836: val_loss did not improve from 17.72265
196/196 - 78s - loss: 17.6393 - MinusLogProbMetric: 17.6393 - val_loss: 17.8012 - val_MinusLogProbMetric: 17.8012 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 837/1000
2023-09-28 03:15:08.655 
Epoch 837/1000 
	 loss: 17.6379, MinusLogProbMetric: 17.6379, val_loss: 17.9817, val_MinusLogProbMetric: 17.9817

Epoch 837: val_loss did not improve from 17.72265
196/196 - 78s - loss: 17.6379 - MinusLogProbMetric: 17.6379 - val_loss: 17.9817 - val_MinusLogProbMetric: 17.9817 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 838/1000
2023-09-28 03:16:26.464 
Epoch 838/1000 
	 loss: 17.6408, MinusLogProbMetric: 17.6408, val_loss: 17.7617, val_MinusLogProbMetric: 17.7617

Epoch 838: val_loss did not improve from 17.72265
196/196 - 78s - loss: 17.6408 - MinusLogProbMetric: 17.6408 - val_loss: 17.7617 - val_MinusLogProbMetric: 17.7617 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 839/1000
2023-09-28 03:17:44.411 
Epoch 839/1000 
	 loss: 17.6504, MinusLogProbMetric: 17.6504, val_loss: 17.8874, val_MinusLogProbMetric: 17.8874

Epoch 839: val_loss did not improve from 17.72265
196/196 - 78s - loss: 17.6504 - MinusLogProbMetric: 17.6504 - val_loss: 17.8874 - val_MinusLogProbMetric: 17.8874 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 840/1000
2023-09-28 03:19:02.617 
Epoch 840/1000 
	 loss: 17.6523, MinusLogProbMetric: 17.6523, val_loss: 18.0003, val_MinusLogProbMetric: 18.0003

Epoch 840: val_loss did not improve from 17.72265
196/196 - 78s - loss: 17.6523 - MinusLogProbMetric: 17.6523 - val_loss: 18.0003 - val_MinusLogProbMetric: 18.0003 - lr: 1.1111e-04 - 78s/epoch - 399ms/step
Epoch 841/1000
2023-09-28 03:20:21.114 
Epoch 841/1000 
	 loss: 17.6472, MinusLogProbMetric: 17.6472, val_loss: 17.7214, val_MinusLogProbMetric: 17.7214

Epoch 841: val_loss improved from 17.72265 to 17.72136, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 80s - loss: 17.6472 - MinusLogProbMetric: 17.6472 - val_loss: 17.7214 - val_MinusLogProbMetric: 17.7214 - lr: 1.1111e-04 - 80s/epoch - 408ms/step
Epoch 842/1000
2023-09-28 03:21:40.542 
Epoch 842/1000 
	 loss: 17.6305, MinusLogProbMetric: 17.6305, val_loss: 17.8871, val_MinusLogProbMetric: 17.8871

Epoch 842: val_loss did not improve from 17.72136
196/196 - 78s - loss: 17.6305 - MinusLogProbMetric: 17.6305 - val_loss: 17.8871 - val_MinusLogProbMetric: 17.8871 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 843/1000
2023-09-28 03:22:57.952 
Epoch 843/1000 
	 loss: 17.6343, MinusLogProbMetric: 17.6343, val_loss: 17.8342, val_MinusLogProbMetric: 17.8342

Epoch 843: val_loss did not improve from 17.72136
196/196 - 77s - loss: 17.6343 - MinusLogProbMetric: 17.6343 - val_loss: 17.8342 - val_MinusLogProbMetric: 17.8342 - lr: 1.1111e-04 - 77s/epoch - 395ms/step
Epoch 844/1000
2023-09-28 03:24:15.665 
Epoch 844/1000 
	 loss: 17.6310, MinusLogProbMetric: 17.6310, val_loss: 17.7448, val_MinusLogProbMetric: 17.7448

Epoch 844: val_loss did not improve from 17.72136
196/196 - 78s - loss: 17.6310 - MinusLogProbMetric: 17.6310 - val_loss: 17.7448 - val_MinusLogProbMetric: 17.7448 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 845/1000
2023-09-28 03:25:33.338 
Epoch 845/1000 
	 loss: 17.6615, MinusLogProbMetric: 17.6615, val_loss: 17.8345, val_MinusLogProbMetric: 17.8345

Epoch 845: val_loss did not improve from 17.72136
196/196 - 78s - loss: 17.6615 - MinusLogProbMetric: 17.6615 - val_loss: 17.8345 - val_MinusLogProbMetric: 17.8345 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 846/1000
2023-09-28 03:26:51.029 
Epoch 846/1000 
	 loss: 17.6685, MinusLogProbMetric: 17.6685, val_loss: 17.8866, val_MinusLogProbMetric: 17.8866

Epoch 846: val_loss did not improve from 17.72136
196/196 - 78s - loss: 17.6685 - MinusLogProbMetric: 17.6685 - val_loss: 17.8866 - val_MinusLogProbMetric: 17.8866 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 847/1000
2023-09-28 03:28:09.116 
Epoch 847/1000 
	 loss: 17.6256, MinusLogProbMetric: 17.6256, val_loss: 17.7556, val_MinusLogProbMetric: 17.7556

Epoch 847: val_loss did not improve from 17.72136
196/196 - 78s - loss: 17.6256 - MinusLogProbMetric: 17.6256 - val_loss: 17.7556 - val_MinusLogProbMetric: 17.7556 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 848/1000
2023-09-28 03:29:26.976 
Epoch 848/1000 
	 loss: 17.6340, MinusLogProbMetric: 17.6340, val_loss: 17.8097, val_MinusLogProbMetric: 17.8097

Epoch 848: val_loss did not improve from 17.72136
196/196 - 78s - loss: 17.6340 - MinusLogProbMetric: 17.6340 - val_loss: 17.8097 - val_MinusLogProbMetric: 17.8097 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 849/1000
2023-09-28 03:30:44.819 
Epoch 849/1000 
	 loss: 17.6457, MinusLogProbMetric: 17.6457, val_loss: 17.8338, val_MinusLogProbMetric: 17.8338

Epoch 849: val_loss did not improve from 17.72136
196/196 - 78s - loss: 17.6457 - MinusLogProbMetric: 17.6457 - val_loss: 17.8338 - val_MinusLogProbMetric: 17.8338 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 850/1000
2023-09-28 03:32:03.138 
Epoch 850/1000 
	 loss: 17.6058, MinusLogProbMetric: 17.6058, val_loss: 17.7709, val_MinusLogProbMetric: 17.7709

Epoch 850: val_loss did not improve from 17.72136
196/196 - 78s - loss: 17.6058 - MinusLogProbMetric: 17.6058 - val_loss: 17.7709 - val_MinusLogProbMetric: 17.7709 - lr: 1.1111e-04 - 78s/epoch - 400ms/step
Epoch 851/1000
2023-09-28 03:33:21.269 
Epoch 851/1000 
	 loss: 17.6258, MinusLogProbMetric: 17.6258, val_loss: 17.7564, val_MinusLogProbMetric: 17.7564

Epoch 851: val_loss did not improve from 17.72136
196/196 - 78s - loss: 17.6258 - MinusLogProbMetric: 17.6258 - val_loss: 17.7564 - val_MinusLogProbMetric: 17.7564 - lr: 1.1111e-04 - 78s/epoch - 399ms/step
Epoch 852/1000
2023-09-28 03:34:39.379 
Epoch 852/1000 
	 loss: 17.6566, MinusLogProbMetric: 17.6566, val_loss: 17.7230, val_MinusLogProbMetric: 17.7230

Epoch 852: val_loss did not improve from 17.72136
196/196 - 78s - loss: 17.6566 - MinusLogProbMetric: 17.6566 - val_loss: 17.7230 - val_MinusLogProbMetric: 17.7230 - lr: 1.1111e-04 - 78s/epoch - 399ms/step
Epoch 853/1000
2023-09-28 03:35:57.483 
Epoch 853/1000 
	 loss: 17.6217, MinusLogProbMetric: 17.6217, val_loss: 17.7716, val_MinusLogProbMetric: 17.7716

Epoch 853: val_loss did not improve from 17.72136
196/196 - 78s - loss: 17.6217 - MinusLogProbMetric: 17.6217 - val_loss: 17.7716 - val_MinusLogProbMetric: 17.7716 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 854/1000
2023-09-28 03:37:14.797 
Epoch 854/1000 
	 loss: 17.6485, MinusLogProbMetric: 17.6485, val_loss: 17.7551, val_MinusLogProbMetric: 17.7551

Epoch 854: val_loss did not improve from 17.72136
196/196 - 77s - loss: 17.6485 - MinusLogProbMetric: 17.6485 - val_loss: 17.7551 - val_MinusLogProbMetric: 17.7551 - lr: 1.1111e-04 - 77s/epoch - 394ms/step
Epoch 855/1000
2023-09-28 03:38:32.958 
Epoch 855/1000 
	 loss: 17.6229, MinusLogProbMetric: 17.6229, val_loss: 17.8137, val_MinusLogProbMetric: 17.8137

Epoch 855: val_loss did not improve from 17.72136
196/196 - 78s - loss: 17.6229 - MinusLogProbMetric: 17.6229 - val_loss: 17.8137 - val_MinusLogProbMetric: 17.8137 - lr: 1.1111e-04 - 78s/epoch - 399ms/step
Epoch 856/1000
2023-09-28 03:39:51.768 
Epoch 856/1000 
	 loss: 17.6397, MinusLogProbMetric: 17.6397, val_loss: 17.7633, val_MinusLogProbMetric: 17.7633

Epoch 856: val_loss did not improve from 17.72136
196/196 - 79s - loss: 17.6397 - MinusLogProbMetric: 17.6397 - val_loss: 17.7633 - val_MinusLogProbMetric: 17.7633 - lr: 1.1111e-04 - 79s/epoch - 402ms/step
Epoch 857/1000
2023-09-28 03:41:09.712 
Epoch 857/1000 
	 loss: 17.6271, MinusLogProbMetric: 17.6271, val_loss: 17.7834, val_MinusLogProbMetric: 17.7834

Epoch 857: val_loss did not improve from 17.72136
196/196 - 78s - loss: 17.6271 - MinusLogProbMetric: 17.6271 - val_loss: 17.7834 - val_MinusLogProbMetric: 17.7834 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 858/1000
2023-09-28 03:42:27.346 
Epoch 858/1000 
	 loss: 17.6243, MinusLogProbMetric: 17.6243, val_loss: 17.8211, val_MinusLogProbMetric: 17.8211

Epoch 858: val_loss did not improve from 17.72136
196/196 - 78s - loss: 17.6243 - MinusLogProbMetric: 17.6243 - val_loss: 17.8211 - val_MinusLogProbMetric: 17.8211 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 859/1000
2023-09-28 03:43:44.283 
Epoch 859/1000 
	 loss: 17.6403, MinusLogProbMetric: 17.6403, val_loss: 17.7380, val_MinusLogProbMetric: 17.7380

Epoch 859: val_loss did not improve from 17.72136
196/196 - 77s - loss: 17.6403 - MinusLogProbMetric: 17.6403 - val_loss: 17.7380 - val_MinusLogProbMetric: 17.7380 - lr: 1.1111e-04 - 77s/epoch - 393ms/step
Epoch 860/1000
2023-09-28 03:45:02.134 
Epoch 860/1000 
	 loss: 17.6107, MinusLogProbMetric: 17.6107, val_loss: 17.8061, val_MinusLogProbMetric: 17.8061

Epoch 860: val_loss did not improve from 17.72136
196/196 - 78s - loss: 17.6107 - MinusLogProbMetric: 17.6107 - val_loss: 17.8061 - val_MinusLogProbMetric: 17.8061 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 861/1000
2023-09-28 03:46:20.847 
Epoch 861/1000 
	 loss: 17.6160, MinusLogProbMetric: 17.6160, val_loss: 17.7584, val_MinusLogProbMetric: 17.7584

Epoch 861: val_loss did not improve from 17.72136
196/196 - 79s - loss: 17.6160 - MinusLogProbMetric: 17.6160 - val_loss: 17.7584 - val_MinusLogProbMetric: 17.7584 - lr: 1.1111e-04 - 79s/epoch - 402ms/step
Epoch 862/1000
2023-09-28 03:47:39.506 
Epoch 862/1000 
	 loss: 17.6175, MinusLogProbMetric: 17.6175, val_loss: 17.7977, val_MinusLogProbMetric: 17.7977

Epoch 862: val_loss did not improve from 17.72136
196/196 - 79s - loss: 17.6175 - MinusLogProbMetric: 17.6175 - val_loss: 17.7977 - val_MinusLogProbMetric: 17.7977 - lr: 1.1111e-04 - 79s/epoch - 401ms/step
Epoch 863/1000
2023-09-28 03:48:57.829 
Epoch 863/1000 
	 loss: 17.6178, MinusLogProbMetric: 17.6178, val_loss: 17.8313, val_MinusLogProbMetric: 17.8313

Epoch 863: val_loss did not improve from 17.72136
196/196 - 78s - loss: 17.6178 - MinusLogProbMetric: 17.6178 - val_loss: 17.8313 - val_MinusLogProbMetric: 17.8313 - lr: 1.1111e-04 - 78s/epoch - 400ms/step
Epoch 864/1000
2023-09-28 03:50:15.786 
Epoch 864/1000 
	 loss: 17.6216, MinusLogProbMetric: 17.6216, val_loss: 17.7183, val_MinusLogProbMetric: 17.7183

Epoch 864: val_loss improved from 17.72136 to 17.71834, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 79s - loss: 17.6216 - MinusLogProbMetric: 17.6216 - val_loss: 17.7183 - val_MinusLogProbMetric: 17.7183 - lr: 1.1111e-04 - 79s/epoch - 405ms/step
Epoch 865/1000
2023-09-28 03:51:35.156 
Epoch 865/1000 
	 loss: 17.6138, MinusLogProbMetric: 17.6138, val_loss: 17.7633, val_MinusLogProbMetric: 17.7633

Epoch 865: val_loss did not improve from 17.71834
196/196 - 78s - loss: 17.6138 - MinusLogProbMetric: 17.6138 - val_loss: 17.7633 - val_MinusLogProbMetric: 17.7633 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 866/1000
2023-09-28 03:52:52.092 
Epoch 866/1000 
	 loss: 17.6301, MinusLogProbMetric: 17.6301, val_loss: 17.7520, val_MinusLogProbMetric: 17.7520

Epoch 866: val_loss did not improve from 17.71834
196/196 - 77s - loss: 17.6301 - MinusLogProbMetric: 17.6301 - val_loss: 17.7520 - val_MinusLogProbMetric: 17.7520 - lr: 1.1111e-04 - 77s/epoch - 393ms/step
Epoch 867/1000
2023-09-28 03:54:10.298 
Epoch 867/1000 
	 loss: 17.6126, MinusLogProbMetric: 17.6126, val_loss: 17.7387, val_MinusLogProbMetric: 17.7387

Epoch 867: val_loss did not improve from 17.71834
196/196 - 78s - loss: 17.6126 - MinusLogProbMetric: 17.6126 - val_loss: 17.7387 - val_MinusLogProbMetric: 17.7387 - lr: 1.1111e-04 - 78s/epoch - 399ms/step
Epoch 868/1000
2023-09-28 03:55:27.920 
Epoch 868/1000 
	 loss: 17.6096, MinusLogProbMetric: 17.6096, val_loss: 17.6941, val_MinusLogProbMetric: 17.6941

Epoch 868: val_loss improved from 17.71834 to 17.69406, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 79s - loss: 17.6096 - MinusLogProbMetric: 17.6096 - val_loss: 17.6941 - val_MinusLogProbMetric: 17.6941 - lr: 1.1111e-04 - 79s/epoch - 403ms/step
Epoch 869/1000
2023-09-28 03:56:47.193 
Epoch 869/1000 
	 loss: 17.6097, MinusLogProbMetric: 17.6097, val_loss: 17.6907, val_MinusLogProbMetric: 17.6907

Epoch 869: val_loss improved from 17.69406 to 17.69073, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 79s - loss: 17.6097 - MinusLogProbMetric: 17.6097 - val_loss: 17.6907 - val_MinusLogProbMetric: 17.6907 - lr: 1.1111e-04 - 79s/epoch - 404ms/step
Epoch 870/1000
2023-09-28 03:58:07.671 
Epoch 870/1000 
	 loss: 17.5880, MinusLogProbMetric: 17.5880, val_loss: 17.7002, val_MinusLogProbMetric: 17.7002

Epoch 870: val_loss did not improve from 17.69073
196/196 - 79s - loss: 17.5880 - MinusLogProbMetric: 17.5880 - val_loss: 17.7002 - val_MinusLogProbMetric: 17.7002 - lr: 1.1111e-04 - 79s/epoch - 404ms/step
Epoch 871/1000
2023-09-28 03:59:25.195 
Epoch 871/1000 
	 loss: 17.5991, MinusLogProbMetric: 17.5991, val_loss: 17.7929, val_MinusLogProbMetric: 17.7929

Epoch 871: val_loss did not improve from 17.69073
196/196 - 78s - loss: 17.5991 - MinusLogProbMetric: 17.5991 - val_loss: 17.7929 - val_MinusLogProbMetric: 17.7929 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 872/1000
2023-09-28 04:00:42.783 
Epoch 872/1000 
	 loss: 17.6015, MinusLogProbMetric: 17.6015, val_loss: 17.7939, val_MinusLogProbMetric: 17.7939

Epoch 872: val_loss did not improve from 17.69073
196/196 - 78s - loss: 17.6015 - MinusLogProbMetric: 17.6015 - val_loss: 17.7939 - val_MinusLogProbMetric: 17.7939 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 873/1000
2023-09-28 04:02:01.546 
Epoch 873/1000 
	 loss: 17.5931, MinusLogProbMetric: 17.5931, val_loss: 17.7448, val_MinusLogProbMetric: 17.7448

Epoch 873: val_loss did not improve from 17.69073
196/196 - 79s - loss: 17.5931 - MinusLogProbMetric: 17.5931 - val_loss: 17.7448 - val_MinusLogProbMetric: 17.7448 - lr: 1.1111e-04 - 79s/epoch - 402ms/step
Epoch 874/1000
2023-09-28 04:03:18.911 
Epoch 874/1000 
	 loss: 17.6108, MinusLogProbMetric: 17.6108, val_loss: 17.6961, val_MinusLogProbMetric: 17.6961

Epoch 874: val_loss did not improve from 17.69073
196/196 - 77s - loss: 17.6108 - MinusLogProbMetric: 17.6108 - val_loss: 17.6961 - val_MinusLogProbMetric: 17.6961 - lr: 1.1111e-04 - 77s/epoch - 395ms/step
Epoch 875/1000
2023-09-28 04:04:37.362 
Epoch 875/1000 
	 loss: 17.6113, MinusLogProbMetric: 17.6113, val_loss: 18.2491, val_MinusLogProbMetric: 18.2491

Epoch 875: val_loss did not improve from 17.69073
196/196 - 78s - loss: 17.6113 - MinusLogProbMetric: 17.6113 - val_loss: 18.2491 - val_MinusLogProbMetric: 18.2491 - lr: 1.1111e-04 - 78s/epoch - 400ms/step
Epoch 876/1000
2023-09-28 04:05:55.493 
Epoch 876/1000 
	 loss: 17.6084, MinusLogProbMetric: 17.6084, val_loss: 17.7576, val_MinusLogProbMetric: 17.7576

Epoch 876: val_loss did not improve from 17.69073
196/196 - 78s - loss: 17.6084 - MinusLogProbMetric: 17.6084 - val_loss: 17.7576 - val_MinusLogProbMetric: 17.7576 - lr: 1.1111e-04 - 78s/epoch - 399ms/step
Epoch 877/1000
2023-09-28 04:07:13.847 
Epoch 877/1000 
	 loss: 17.5959, MinusLogProbMetric: 17.5959, val_loss: 17.7478, val_MinusLogProbMetric: 17.7478

Epoch 877: val_loss did not improve from 17.69073
196/196 - 78s - loss: 17.5959 - MinusLogProbMetric: 17.5959 - val_loss: 17.7478 - val_MinusLogProbMetric: 17.7478 - lr: 1.1111e-04 - 78s/epoch - 400ms/step
Epoch 878/1000
2023-09-28 04:08:31.627 
Epoch 878/1000 
	 loss: 17.6083, MinusLogProbMetric: 17.6083, val_loss: 17.6999, val_MinusLogProbMetric: 17.6999

Epoch 878: val_loss did not improve from 17.69073
196/196 - 78s - loss: 17.6083 - MinusLogProbMetric: 17.6083 - val_loss: 17.6999 - val_MinusLogProbMetric: 17.6999 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 879/1000
2023-09-28 04:09:50.194 
Epoch 879/1000 
	 loss: 17.5931, MinusLogProbMetric: 17.5931, val_loss: 17.7182, val_MinusLogProbMetric: 17.7182

Epoch 879: val_loss did not improve from 17.69073
196/196 - 79s - loss: 17.5931 - MinusLogProbMetric: 17.5931 - val_loss: 17.7182 - val_MinusLogProbMetric: 17.7182 - lr: 1.1111e-04 - 79s/epoch - 401ms/step
Epoch 880/1000
2023-09-28 04:11:09.136 
Epoch 880/1000 
	 loss: 17.6051, MinusLogProbMetric: 17.6051, val_loss: 17.7215, val_MinusLogProbMetric: 17.7215

Epoch 880: val_loss did not improve from 17.69073
196/196 - 79s - loss: 17.6051 - MinusLogProbMetric: 17.6051 - val_loss: 17.7215 - val_MinusLogProbMetric: 17.7215 - lr: 1.1111e-04 - 79s/epoch - 403ms/step
Epoch 881/1000
2023-09-28 04:12:26.484 
Epoch 881/1000 
	 loss: 17.5914, MinusLogProbMetric: 17.5914, val_loss: 17.7353, val_MinusLogProbMetric: 17.7353

Epoch 881: val_loss did not improve from 17.69073
196/196 - 77s - loss: 17.5914 - MinusLogProbMetric: 17.5914 - val_loss: 17.7353 - val_MinusLogProbMetric: 17.7353 - lr: 1.1111e-04 - 77s/epoch - 395ms/step
Epoch 882/1000
2023-09-28 04:13:44.465 
Epoch 882/1000 
	 loss: 17.6051, MinusLogProbMetric: 17.6051, val_loss: 17.7465, val_MinusLogProbMetric: 17.7465

Epoch 882: val_loss did not improve from 17.69073
196/196 - 78s - loss: 17.6051 - MinusLogProbMetric: 17.6051 - val_loss: 17.7465 - val_MinusLogProbMetric: 17.7465 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 883/1000
2023-09-28 04:15:02.096 
Epoch 883/1000 
	 loss: 17.5985, MinusLogProbMetric: 17.5985, val_loss: 17.7298, val_MinusLogProbMetric: 17.7298

Epoch 883: val_loss did not improve from 17.69073
196/196 - 78s - loss: 17.5985 - MinusLogProbMetric: 17.5985 - val_loss: 17.7298 - val_MinusLogProbMetric: 17.7298 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 884/1000
2023-09-28 04:16:20.517 
Epoch 884/1000 
	 loss: 17.5917, MinusLogProbMetric: 17.5917, val_loss: 17.6840, val_MinusLogProbMetric: 17.6840

Epoch 884: val_loss improved from 17.69073 to 17.68396, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 80s - loss: 17.5917 - MinusLogProbMetric: 17.5917 - val_loss: 17.6840 - val_MinusLogProbMetric: 17.6840 - lr: 1.1111e-04 - 80s/epoch - 408ms/step
Epoch 885/1000
2023-09-28 04:17:40.326 
Epoch 885/1000 
	 loss: 17.5807, MinusLogProbMetric: 17.5807, val_loss: 17.7123, val_MinusLogProbMetric: 17.7123

Epoch 885: val_loss did not improve from 17.68396
196/196 - 78s - loss: 17.5807 - MinusLogProbMetric: 17.5807 - val_loss: 17.7123 - val_MinusLogProbMetric: 17.7123 - lr: 1.1111e-04 - 78s/epoch - 399ms/step
Epoch 886/1000
2023-09-28 04:18:58.524 
Epoch 886/1000 
	 loss: 17.6015, MinusLogProbMetric: 17.6015, val_loss: 17.7441, val_MinusLogProbMetric: 17.7441

Epoch 886: val_loss did not improve from 17.68396
196/196 - 78s - loss: 17.6015 - MinusLogProbMetric: 17.6015 - val_loss: 17.7441 - val_MinusLogProbMetric: 17.7441 - lr: 1.1111e-04 - 78s/epoch - 399ms/step
Epoch 887/1000
2023-09-28 04:20:15.919 
Epoch 887/1000 
	 loss: 17.6149, MinusLogProbMetric: 17.6149, val_loss: 17.8156, val_MinusLogProbMetric: 17.8156

Epoch 887: val_loss did not improve from 17.68396
196/196 - 77s - loss: 17.6149 - MinusLogProbMetric: 17.6149 - val_loss: 17.8156 - val_MinusLogProbMetric: 17.8156 - lr: 1.1111e-04 - 77s/epoch - 395ms/step
Epoch 888/1000
2023-09-28 04:21:33.858 
Epoch 888/1000 
	 loss: 17.6002, MinusLogProbMetric: 17.6002, val_loss: 17.7633, val_MinusLogProbMetric: 17.7633

Epoch 888: val_loss did not improve from 17.68396
196/196 - 78s - loss: 17.6002 - MinusLogProbMetric: 17.6002 - val_loss: 17.7633 - val_MinusLogProbMetric: 17.7633 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 889/1000
2023-09-28 04:22:52.130 
Epoch 889/1000 
	 loss: 17.5881, MinusLogProbMetric: 17.5881, val_loss: 17.7689, val_MinusLogProbMetric: 17.7689

Epoch 889: val_loss did not improve from 17.68396
196/196 - 78s - loss: 17.5881 - MinusLogProbMetric: 17.5881 - val_loss: 17.7689 - val_MinusLogProbMetric: 17.7689 - lr: 1.1111e-04 - 78s/epoch - 399ms/step
Epoch 890/1000
2023-09-28 04:24:09.729 
Epoch 890/1000 
	 loss: 17.6061, MinusLogProbMetric: 17.6061, val_loss: 17.7884, val_MinusLogProbMetric: 17.7884

Epoch 890: val_loss did not improve from 17.68396
196/196 - 78s - loss: 17.6061 - MinusLogProbMetric: 17.6061 - val_loss: 17.7884 - val_MinusLogProbMetric: 17.7884 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 891/1000
2023-09-28 04:25:27.628 
Epoch 891/1000 
	 loss: 17.6035, MinusLogProbMetric: 17.6035, val_loss: 17.9259, val_MinusLogProbMetric: 17.9259

Epoch 891: val_loss did not improve from 17.68396
196/196 - 78s - loss: 17.6035 - MinusLogProbMetric: 17.6035 - val_loss: 17.9259 - val_MinusLogProbMetric: 17.9259 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 892/1000
2023-09-28 04:26:45.449 
Epoch 892/1000 
	 loss: 17.5873, MinusLogProbMetric: 17.5873, val_loss: 17.7488, val_MinusLogProbMetric: 17.7488

Epoch 892: val_loss did not improve from 17.68396
196/196 - 78s - loss: 17.5873 - MinusLogProbMetric: 17.5873 - val_loss: 17.7488 - val_MinusLogProbMetric: 17.7488 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 893/1000
2023-09-28 04:28:03.143 
Epoch 893/1000 
	 loss: 17.5923, MinusLogProbMetric: 17.5923, val_loss: 17.7902, val_MinusLogProbMetric: 17.7902

Epoch 893: val_loss did not improve from 17.68396
196/196 - 78s - loss: 17.5923 - MinusLogProbMetric: 17.5923 - val_loss: 17.7902 - val_MinusLogProbMetric: 17.7902 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 894/1000
2023-09-28 04:29:19.553 
Epoch 894/1000 
	 loss: 17.6045, MinusLogProbMetric: 17.6045, val_loss: 17.6866, val_MinusLogProbMetric: 17.6866

Epoch 894: val_loss did not improve from 17.68396
196/196 - 76s - loss: 17.6045 - MinusLogProbMetric: 17.6045 - val_loss: 17.6866 - val_MinusLogProbMetric: 17.6866 - lr: 1.1111e-04 - 76s/epoch - 390ms/step
Epoch 895/1000
2023-09-28 04:30:37.966 
Epoch 895/1000 
	 loss: 17.5851, MinusLogProbMetric: 17.5851, val_loss: 17.7826, val_MinusLogProbMetric: 17.7826

Epoch 895: val_loss did not improve from 17.68396
196/196 - 78s - loss: 17.5851 - MinusLogProbMetric: 17.5851 - val_loss: 17.7826 - val_MinusLogProbMetric: 17.7826 - lr: 1.1111e-04 - 78s/epoch - 400ms/step
Epoch 896/1000
2023-09-28 04:31:55.797 
Epoch 896/1000 
	 loss: 17.5825, MinusLogProbMetric: 17.5825, val_loss: 17.6961, val_MinusLogProbMetric: 17.6961

Epoch 896: val_loss did not improve from 17.68396
196/196 - 78s - loss: 17.5825 - MinusLogProbMetric: 17.5825 - val_loss: 17.6961 - val_MinusLogProbMetric: 17.6961 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 897/1000
2023-09-28 04:33:14.321 
Epoch 897/1000 
	 loss: 17.5651, MinusLogProbMetric: 17.5651, val_loss: 17.6784, val_MinusLogProbMetric: 17.6784

Epoch 897: val_loss improved from 17.68396 to 17.67842, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 80s - loss: 17.5651 - MinusLogProbMetric: 17.5651 - val_loss: 17.6784 - val_MinusLogProbMetric: 17.6784 - lr: 1.1111e-04 - 80s/epoch - 408ms/step
Epoch 898/1000
2023-09-28 04:34:34.915 
Epoch 898/1000 
	 loss: 17.5619, MinusLogProbMetric: 17.5619, val_loss: 17.7897, val_MinusLogProbMetric: 17.7897

Epoch 898: val_loss did not improve from 17.67842
196/196 - 79s - loss: 17.5619 - MinusLogProbMetric: 17.5619 - val_loss: 17.7897 - val_MinusLogProbMetric: 17.7897 - lr: 1.1111e-04 - 79s/epoch - 404ms/step
Epoch 899/1000
2023-09-28 04:35:53.304 
Epoch 899/1000 
	 loss: 17.5815, MinusLogProbMetric: 17.5815, val_loss: 17.6859, val_MinusLogProbMetric: 17.6859

Epoch 899: val_loss did not improve from 17.67842
196/196 - 78s - loss: 17.5815 - MinusLogProbMetric: 17.5815 - val_loss: 17.6859 - val_MinusLogProbMetric: 17.6859 - lr: 1.1111e-04 - 78s/epoch - 400ms/step
Epoch 900/1000
2023-09-28 04:37:11.354 
Epoch 900/1000 
	 loss: 17.5952, MinusLogProbMetric: 17.5952, val_loss: 17.6314, val_MinusLogProbMetric: 17.6314

Epoch 900: val_loss improved from 17.67842 to 17.63136, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 79s - loss: 17.5952 - MinusLogProbMetric: 17.5952 - val_loss: 17.6314 - val_MinusLogProbMetric: 17.6314 - lr: 1.1111e-04 - 79s/epoch - 405ms/step
Epoch 901/1000
2023-09-28 04:38:29.789 
Epoch 901/1000 
	 loss: 17.5957, MinusLogProbMetric: 17.5957, val_loss: 17.7390, val_MinusLogProbMetric: 17.7390

Epoch 901: val_loss did not improve from 17.63136
196/196 - 77s - loss: 17.5957 - MinusLogProbMetric: 17.5957 - val_loss: 17.7390 - val_MinusLogProbMetric: 17.7390 - lr: 1.1111e-04 - 77s/epoch - 393ms/step
Epoch 902/1000
2023-09-28 04:39:47.930 
Epoch 902/1000 
	 loss: 17.5760, MinusLogProbMetric: 17.5760, val_loss: 17.6847, val_MinusLogProbMetric: 17.6847

Epoch 902: val_loss did not improve from 17.63136
196/196 - 78s - loss: 17.5760 - MinusLogProbMetric: 17.5760 - val_loss: 17.6847 - val_MinusLogProbMetric: 17.6847 - lr: 1.1111e-04 - 78s/epoch - 399ms/step
Epoch 903/1000
2023-09-28 04:41:05.500 
Epoch 903/1000 
	 loss: 17.5964, MinusLogProbMetric: 17.5964, val_loss: 17.7840, val_MinusLogProbMetric: 17.7840

Epoch 903: val_loss did not improve from 17.63136
196/196 - 78s - loss: 17.5964 - MinusLogProbMetric: 17.5964 - val_loss: 17.7840 - val_MinusLogProbMetric: 17.7840 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 904/1000
2023-09-28 04:42:22.442 
Epoch 904/1000 
	 loss: 17.5772, MinusLogProbMetric: 17.5772, val_loss: 17.6632, val_MinusLogProbMetric: 17.6632

Epoch 904: val_loss did not improve from 17.63136
196/196 - 77s - loss: 17.5772 - MinusLogProbMetric: 17.5772 - val_loss: 17.6632 - val_MinusLogProbMetric: 17.6632 - lr: 1.1111e-04 - 77s/epoch - 393ms/step
Epoch 905/1000
2023-09-28 04:43:40.315 
Epoch 905/1000 
	 loss: 17.5661, MinusLogProbMetric: 17.5661, val_loss: 17.8010, val_MinusLogProbMetric: 17.8010

Epoch 905: val_loss did not improve from 17.63136
196/196 - 78s - loss: 17.5661 - MinusLogProbMetric: 17.5661 - val_loss: 17.8010 - val_MinusLogProbMetric: 17.8010 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 906/1000
2023-09-28 04:44:58.330 
Epoch 906/1000 
	 loss: 17.5641, MinusLogProbMetric: 17.5641, val_loss: 17.7278, val_MinusLogProbMetric: 17.7278

Epoch 906: val_loss did not improve from 17.63136
196/196 - 78s - loss: 17.5641 - MinusLogProbMetric: 17.5641 - val_loss: 17.7278 - val_MinusLogProbMetric: 17.7278 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 907/1000
2023-09-28 04:46:16.632 
Epoch 907/1000 
	 loss: 17.5607, MinusLogProbMetric: 17.5607, val_loss: 17.7219, val_MinusLogProbMetric: 17.7219

Epoch 907: val_loss did not improve from 17.63136
196/196 - 78s - loss: 17.5607 - MinusLogProbMetric: 17.5607 - val_loss: 17.7219 - val_MinusLogProbMetric: 17.7219 - lr: 1.1111e-04 - 78s/epoch - 399ms/step
Epoch 908/1000
2023-09-28 04:47:34.943 
Epoch 908/1000 
	 loss: 17.5493, MinusLogProbMetric: 17.5493, val_loss: 17.7690, val_MinusLogProbMetric: 17.7690

Epoch 908: val_loss did not improve from 17.63136
196/196 - 78s - loss: 17.5493 - MinusLogProbMetric: 17.5493 - val_loss: 17.7690 - val_MinusLogProbMetric: 17.7690 - lr: 1.1111e-04 - 78s/epoch - 400ms/step
Epoch 909/1000
2023-09-28 04:48:52.120 
Epoch 909/1000 
	 loss: 17.5682, MinusLogProbMetric: 17.5682, val_loss: 17.7662, val_MinusLogProbMetric: 17.7662

Epoch 909: val_loss did not improve from 17.63136
196/196 - 77s - loss: 17.5682 - MinusLogProbMetric: 17.5682 - val_loss: 17.7662 - val_MinusLogProbMetric: 17.7662 - lr: 1.1111e-04 - 77s/epoch - 394ms/step
Epoch 910/1000
2023-09-28 04:50:09.812 
Epoch 910/1000 
	 loss: 17.5547, MinusLogProbMetric: 17.5547, val_loss: 17.7455, val_MinusLogProbMetric: 17.7455

Epoch 910: val_loss did not improve from 17.63136
196/196 - 78s - loss: 17.5547 - MinusLogProbMetric: 17.5547 - val_loss: 17.7455 - val_MinusLogProbMetric: 17.7455 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 911/1000
2023-09-28 04:51:28.313 
Epoch 911/1000 
	 loss: 17.5725, MinusLogProbMetric: 17.5725, val_loss: 17.6835, val_MinusLogProbMetric: 17.6835

Epoch 911: val_loss did not improve from 17.63136
196/196 - 78s - loss: 17.5725 - MinusLogProbMetric: 17.5725 - val_loss: 17.6835 - val_MinusLogProbMetric: 17.6835 - lr: 1.1111e-04 - 78s/epoch - 400ms/step
Epoch 912/1000
2023-09-28 04:52:45.360 
Epoch 912/1000 
	 loss: 17.5762, MinusLogProbMetric: 17.5762, val_loss: 17.7402, val_MinusLogProbMetric: 17.7402

Epoch 912: val_loss did not improve from 17.63136
196/196 - 77s - loss: 17.5762 - MinusLogProbMetric: 17.5762 - val_loss: 17.7402 - val_MinusLogProbMetric: 17.7402 - lr: 1.1111e-04 - 77s/epoch - 393ms/step
Epoch 913/1000
2023-09-28 04:54:02.363 
Epoch 913/1000 
	 loss: 17.5724, MinusLogProbMetric: 17.5724, val_loss: 17.6456, val_MinusLogProbMetric: 17.6456

Epoch 913: val_loss did not improve from 17.63136
196/196 - 77s - loss: 17.5724 - MinusLogProbMetric: 17.5724 - val_loss: 17.6456 - val_MinusLogProbMetric: 17.6456 - lr: 1.1111e-04 - 77s/epoch - 393ms/step
Epoch 914/1000
2023-09-28 04:55:21.121 
Epoch 914/1000 
	 loss: 17.5682, MinusLogProbMetric: 17.5682, val_loss: 17.6458, val_MinusLogProbMetric: 17.6458

Epoch 914: val_loss did not improve from 17.63136
196/196 - 79s - loss: 17.5682 - MinusLogProbMetric: 17.5682 - val_loss: 17.6458 - val_MinusLogProbMetric: 17.6458 - lr: 1.1111e-04 - 79s/epoch - 402ms/step
Epoch 915/1000
2023-09-28 04:56:38.605 
Epoch 915/1000 
	 loss: 17.5561, MinusLogProbMetric: 17.5561, val_loss: 17.8044, val_MinusLogProbMetric: 17.8044

Epoch 915: val_loss did not improve from 17.63136
196/196 - 77s - loss: 17.5561 - MinusLogProbMetric: 17.5561 - val_loss: 17.8044 - val_MinusLogProbMetric: 17.8044 - lr: 1.1111e-04 - 77s/epoch - 395ms/step
Epoch 916/1000
2023-09-28 04:57:56.969 
Epoch 916/1000 
	 loss: 17.6029, MinusLogProbMetric: 17.6029, val_loss: 17.7843, val_MinusLogProbMetric: 17.7843

Epoch 916: val_loss did not improve from 17.63136
196/196 - 78s - loss: 17.6029 - MinusLogProbMetric: 17.6029 - val_loss: 17.7843 - val_MinusLogProbMetric: 17.7843 - lr: 1.1111e-04 - 78s/epoch - 400ms/step
Epoch 917/1000
2023-09-28 04:59:15.065 
Epoch 917/1000 
	 loss: 17.5478, MinusLogProbMetric: 17.5478, val_loss: 17.6237, val_MinusLogProbMetric: 17.6237

Epoch 917: val_loss improved from 17.63136 to 17.62374, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 79s - loss: 17.5478 - MinusLogProbMetric: 17.5478 - val_loss: 17.6237 - val_MinusLogProbMetric: 17.6237 - lr: 1.1111e-04 - 79s/epoch - 405ms/step
Epoch 918/1000
2023-09-28 05:00:34.662 
Epoch 918/1000 
	 loss: 17.5467, MinusLogProbMetric: 17.5467, val_loss: 17.7541, val_MinusLogProbMetric: 17.7541

Epoch 918: val_loss did not improve from 17.62374
196/196 - 78s - loss: 17.5467 - MinusLogProbMetric: 17.5467 - val_loss: 17.7541 - val_MinusLogProbMetric: 17.7541 - lr: 1.1111e-04 - 78s/epoch - 400ms/step
Epoch 919/1000
2023-09-28 05:01:52.721 
Epoch 919/1000 
	 loss: 17.5544, MinusLogProbMetric: 17.5544, val_loss: 17.6950, val_MinusLogProbMetric: 17.6950

Epoch 919: val_loss did not improve from 17.62374
196/196 - 78s - loss: 17.5544 - MinusLogProbMetric: 17.5544 - val_loss: 17.6950 - val_MinusLogProbMetric: 17.6950 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 920/1000
2023-09-28 05:03:10.745 
Epoch 920/1000 
	 loss: 17.5634, MinusLogProbMetric: 17.5634, val_loss: 17.7603, val_MinusLogProbMetric: 17.7603

Epoch 920: val_loss did not improve from 17.62374
196/196 - 78s - loss: 17.5634 - MinusLogProbMetric: 17.5634 - val_loss: 17.7603 - val_MinusLogProbMetric: 17.7603 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 921/1000
2023-09-28 05:04:28.994 
Epoch 921/1000 
	 loss: 17.5804, MinusLogProbMetric: 17.5804, val_loss: 17.7378, val_MinusLogProbMetric: 17.7378

Epoch 921: val_loss did not improve from 17.62374
196/196 - 78s - loss: 17.5804 - MinusLogProbMetric: 17.5804 - val_loss: 17.7378 - val_MinusLogProbMetric: 17.7378 - lr: 1.1111e-04 - 78s/epoch - 399ms/step
Epoch 922/1000
2023-09-28 05:05:47.309 
Epoch 922/1000 
	 loss: 17.5479, MinusLogProbMetric: 17.5479, val_loss: 17.7049, val_MinusLogProbMetric: 17.7049

Epoch 922: val_loss did not improve from 17.62374
196/196 - 78s - loss: 17.5479 - MinusLogProbMetric: 17.5479 - val_loss: 17.7049 - val_MinusLogProbMetric: 17.7049 - lr: 1.1111e-04 - 78s/epoch - 400ms/step
Epoch 923/1000
2023-09-28 05:07:05.446 
Epoch 923/1000 
	 loss: 17.5599, MinusLogProbMetric: 17.5599, val_loss: 17.6455, val_MinusLogProbMetric: 17.6455

Epoch 923: val_loss did not improve from 17.62374
196/196 - 78s - loss: 17.5599 - MinusLogProbMetric: 17.5599 - val_loss: 17.6455 - val_MinusLogProbMetric: 17.6455 - lr: 1.1111e-04 - 78s/epoch - 399ms/step
Epoch 924/1000
2023-09-28 05:08:23.734 
Epoch 924/1000 
	 loss: 17.5660, MinusLogProbMetric: 17.5660, val_loss: 17.6773, val_MinusLogProbMetric: 17.6773

Epoch 924: val_loss did not improve from 17.62374
196/196 - 78s - loss: 17.5660 - MinusLogProbMetric: 17.5660 - val_loss: 17.6773 - val_MinusLogProbMetric: 17.6773 - lr: 1.1111e-04 - 78s/epoch - 399ms/step
Epoch 925/1000
2023-09-28 05:09:42.224 
Epoch 925/1000 
	 loss: 17.5546, MinusLogProbMetric: 17.5546, val_loss: 17.6075, val_MinusLogProbMetric: 17.6075

Epoch 925: val_loss improved from 17.62374 to 17.60753, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 80s - loss: 17.5546 - MinusLogProbMetric: 17.5546 - val_loss: 17.6075 - val_MinusLogProbMetric: 17.6075 - lr: 1.1111e-04 - 80s/epoch - 409ms/step
Epoch 926/1000
2023-09-28 05:11:01.432 
Epoch 926/1000 
	 loss: 17.5579, MinusLogProbMetric: 17.5579, val_loss: 17.6120, val_MinusLogProbMetric: 17.6120

Epoch 926: val_loss did not improve from 17.60753
196/196 - 78s - loss: 17.5579 - MinusLogProbMetric: 17.5579 - val_loss: 17.6120 - val_MinusLogProbMetric: 17.6120 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 927/1000
2023-09-28 05:12:19.926 
Epoch 927/1000 
	 loss: 17.5690, MinusLogProbMetric: 17.5690, val_loss: 17.7131, val_MinusLogProbMetric: 17.7131

Epoch 927: val_loss did not improve from 17.60753
196/196 - 78s - loss: 17.5690 - MinusLogProbMetric: 17.5690 - val_loss: 17.7131 - val_MinusLogProbMetric: 17.7131 - lr: 1.1111e-04 - 78s/epoch - 400ms/step
Epoch 928/1000
2023-09-28 05:13:37.640 
Epoch 928/1000 
	 loss: 17.5526, MinusLogProbMetric: 17.5526, val_loss: 17.6715, val_MinusLogProbMetric: 17.6715

Epoch 928: val_loss did not improve from 17.60753
196/196 - 78s - loss: 17.5526 - MinusLogProbMetric: 17.5526 - val_loss: 17.6715 - val_MinusLogProbMetric: 17.6715 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 929/1000
2023-09-28 05:14:56.136 
Epoch 929/1000 
	 loss: 17.5441, MinusLogProbMetric: 17.5441, val_loss: 17.7366, val_MinusLogProbMetric: 17.7366

Epoch 929: val_loss did not improve from 17.60753
196/196 - 78s - loss: 17.5441 - MinusLogProbMetric: 17.5441 - val_loss: 17.7366 - val_MinusLogProbMetric: 17.7366 - lr: 1.1111e-04 - 78s/epoch - 400ms/step
Epoch 930/1000
2023-09-28 05:16:14.205 
Epoch 930/1000 
	 loss: 17.5550, MinusLogProbMetric: 17.5550, val_loss: 17.5963, val_MinusLogProbMetric: 17.5963

Epoch 930: val_loss improved from 17.60753 to 17.59634, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 80s - loss: 17.5550 - MinusLogProbMetric: 17.5550 - val_loss: 17.5963 - val_MinusLogProbMetric: 17.5963 - lr: 1.1111e-04 - 80s/epoch - 406ms/step
Epoch 931/1000
2023-09-28 05:17:33.084 
Epoch 931/1000 
	 loss: 17.5455, MinusLogProbMetric: 17.5455, val_loss: 17.9071, val_MinusLogProbMetric: 17.9071

Epoch 931: val_loss did not improve from 17.59634
196/196 - 77s - loss: 17.5455 - MinusLogProbMetric: 17.5455 - val_loss: 17.9071 - val_MinusLogProbMetric: 17.9071 - lr: 1.1111e-04 - 77s/epoch - 395ms/step
Epoch 932/1000
2023-09-28 05:18:50.806 
Epoch 932/1000 
	 loss: 17.5322, MinusLogProbMetric: 17.5322, val_loss: 17.6789, val_MinusLogProbMetric: 17.6789

Epoch 932: val_loss did not improve from 17.59634
196/196 - 78s - loss: 17.5322 - MinusLogProbMetric: 17.5322 - val_loss: 17.6789 - val_MinusLogProbMetric: 17.6789 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 933/1000
2023-09-28 05:20:08.129 
Epoch 933/1000 
	 loss: 17.5200, MinusLogProbMetric: 17.5200, val_loss: 17.7223, val_MinusLogProbMetric: 17.7223

Epoch 933: val_loss did not improve from 17.59634
196/196 - 77s - loss: 17.5200 - MinusLogProbMetric: 17.5200 - val_loss: 17.7223 - val_MinusLogProbMetric: 17.7223 - lr: 1.1111e-04 - 77s/epoch - 394ms/step
Epoch 934/1000
2023-09-28 05:21:25.511 
Epoch 934/1000 
	 loss: 17.5589, MinusLogProbMetric: 17.5589, val_loss: 17.8670, val_MinusLogProbMetric: 17.8670

Epoch 934: val_loss did not improve from 17.59634
196/196 - 77s - loss: 17.5589 - MinusLogProbMetric: 17.5589 - val_loss: 17.8670 - val_MinusLogProbMetric: 17.8670 - lr: 1.1111e-04 - 77s/epoch - 395ms/step
Epoch 935/1000
2023-09-28 05:22:43.774 
Epoch 935/1000 
	 loss: 17.5370, MinusLogProbMetric: 17.5370, val_loss: 17.7694, val_MinusLogProbMetric: 17.7694

Epoch 935: val_loss did not improve from 17.59634
196/196 - 78s - loss: 17.5370 - MinusLogProbMetric: 17.5370 - val_loss: 17.7694 - val_MinusLogProbMetric: 17.7694 - lr: 1.1111e-04 - 78s/epoch - 399ms/step
Epoch 936/1000
2023-09-28 05:24:01.950 
Epoch 936/1000 
	 loss: 17.5289, MinusLogProbMetric: 17.5289, val_loss: 17.6802, val_MinusLogProbMetric: 17.6802

Epoch 936: val_loss did not improve from 17.59634
196/196 - 78s - loss: 17.5289 - MinusLogProbMetric: 17.5289 - val_loss: 17.6802 - val_MinusLogProbMetric: 17.6802 - lr: 1.1111e-04 - 78s/epoch - 399ms/step
Epoch 937/1000
2023-09-28 05:25:21.201 
Epoch 937/1000 
	 loss: 17.5639, MinusLogProbMetric: 17.5639, val_loss: 17.7621, val_MinusLogProbMetric: 17.7621

Epoch 937: val_loss did not improve from 17.59634
196/196 - 79s - loss: 17.5639 - MinusLogProbMetric: 17.5639 - val_loss: 17.7621 - val_MinusLogProbMetric: 17.7621 - lr: 1.1111e-04 - 79s/epoch - 404ms/step
Epoch 938/1000
2023-09-28 05:26:38.943 
Epoch 938/1000 
	 loss: 17.5557, MinusLogProbMetric: 17.5557, val_loss: 17.7741, val_MinusLogProbMetric: 17.7741

Epoch 938: val_loss did not improve from 17.59634
196/196 - 78s - loss: 17.5557 - MinusLogProbMetric: 17.5557 - val_loss: 17.7741 - val_MinusLogProbMetric: 17.7741 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 939/1000
2023-09-28 05:27:56.496 
Epoch 939/1000 
	 loss: 17.5337, MinusLogProbMetric: 17.5337, val_loss: 17.6609, val_MinusLogProbMetric: 17.6609

Epoch 939: val_loss did not improve from 17.59634
196/196 - 78s - loss: 17.5337 - MinusLogProbMetric: 17.5337 - val_loss: 17.6609 - val_MinusLogProbMetric: 17.6609 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 940/1000
2023-09-28 05:29:14.403 
Epoch 940/1000 
	 loss: 17.5407, MinusLogProbMetric: 17.5407, val_loss: 17.6630, val_MinusLogProbMetric: 17.6630

Epoch 940: val_loss did not improve from 17.59634
196/196 - 78s - loss: 17.5407 - MinusLogProbMetric: 17.5407 - val_loss: 17.6630 - val_MinusLogProbMetric: 17.6630 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 941/1000
2023-09-28 05:30:32.840 
Epoch 941/1000 
	 loss: 17.5412, MinusLogProbMetric: 17.5412, val_loss: 17.6829, val_MinusLogProbMetric: 17.6829

Epoch 941: val_loss did not improve from 17.59634
196/196 - 78s - loss: 17.5412 - MinusLogProbMetric: 17.5412 - val_loss: 17.6829 - val_MinusLogProbMetric: 17.6829 - lr: 1.1111e-04 - 78s/epoch - 400ms/step
Epoch 942/1000
2023-09-28 05:31:51.503 
Epoch 942/1000 
	 loss: 17.5303, MinusLogProbMetric: 17.5303, val_loss: 17.7931, val_MinusLogProbMetric: 17.7931

Epoch 942: val_loss did not improve from 17.59634
196/196 - 79s - loss: 17.5303 - MinusLogProbMetric: 17.5303 - val_loss: 17.7931 - val_MinusLogProbMetric: 17.7931 - lr: 1.1111e-04 - 79s/epoch - 401ms/step
Epoch 943/1000
2023-09-28 05:33:09.918 
Epoch 943/1000 
	 loss: 17.5277, MinusLogProbMetric: 17.5277, val_loss: 17.7883, val_MinusLogProbMetric: 17.7883

Epoch 943: val_loss did not improve from 17.59634
196/196 - 78s - loss: 17.5277 - MinusLogProbMetric: 17.5277 - val_loss: 17.7883 - val_MinusLogProbMetric: 17.7883 - lr: 1.1111e-04 - 78s/epoch - 400ms/step
Epoch 944/1000
2023-09-28 05:34:28.247 
Epoch 944/1000 
	 loss: 17.5311, MinusLogProbMetric: 17.5311, val_loss: 17.6670, val_MinusLogProbMetric: 17.6670

Epoch 944: val_loss did not improve from 17.59634
196/196 - 78s - loss: 17.5311 - MinusLogProbMetric: 17.5311 - val_loss: 17.6670 - val_MinusLogProbMetric: 17.6670 - lr: 1.1111e-04 - 78s/epoch - 400ms/step
Epoch 945/1000
2023-09-28 05:35:46.765 
Epoch 945/1000 
	 loss: 17.5163, MinusLogProbMetric: 17.5163, val_loss: 17.6048, val_MinusLogProbMetric: 17.6048

Epoch 945: val_loss did not improve from 17.59634
196/196 - 79s - loss: 17.5163 - MinusLogProbMetric: 17.5163 - val_loss: 17.6048 - val_MinusLogProbMetric: 17.6048 - lr: 1.1111e-04 - 79s/epoch - 401ms/step
Epoch 946/1000
2023-09-28 05:37:05.244 
Epoch 946/1000 
	 loss: 17.5416, MinusLogProbMetric: 17.5416, val_loss: 17.6293, val_MinusLogProbMetric: 17.6293

Epoch 946: val_loss did not improve from 17.59634
196/196 - 78s - loss: 17.5416 - MinusLogProbMetric: 17.5416 - val_loss: 17.6293 - val_MinusLogProbMetric: 17.6293 - lr: 1.1111e-04 - 78s/epoch - 400ms/step
Epoch 947/1000
2023-09-28 05:38:23.343 
Epoch 947/1000 
	 loss: 17.5335, MinusLogProbMetric: 17.5335, val_loss: 17.6816, val_MinusLogProbMetric: 17.6816

Epoch 947: val_loss did not improve from 17.59634
196/196 - 78s - loss: 17.5335 - MinusLogProbMetric: 17.5335 - val_loss: 17.6816 - val_MinusLogProbMetric: 17.6816 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 948/1000
2023-09-28 05:39:40.743 
Epoch 948/1000 
	 loss: 17.5295, MinusLogProbMetric: 17.5295, val_loss: 18.0355, val_MinusLogProbMetric: 18.0355

Epoch 948: val_loss did not improve from 17.59634
196/196 - 77s - loss: 17.5295 - MinusLogProbMetric: 17.5295 - val_loss: 18.0355 - val_MinusLogProbMetric: 18.0355 - lr: 1.1111e-04 - 77s/epoch - 395ms/step
Epoch 949/1000
2023-09-28 05:40:58.690 
Epoch 949/1000 
	 loss: 17.5272, MinusLogProbMetric: 17.5272, val_loss: 17.6491, val_MinusLogProbMetric: 17.6491

Epoch 949: val_loss did not improve from 17.59634
196/196 - 78s - loss: 17.5272 - MinusLogProbMetric: 17.5272 - val_loss: 17.6491 - val_MinusLogProbMetric: 17.6491 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 950/1000
2023-09-28 05:42:17.217 
Epoch 950/1000 
	 loss: 17.5258, MinusLogProbMetric: 17.5258, val_loss: 17.7200, val_MinusLogProbMetric: 17.7200

Epoch 950: val_loss did not improve from 17.59634
196/196 - 79s - loss: 17.5258 - MinusLogProbMetric: 17.5258 - val_loss: 17.7200 - val_MinusLogProbMetric: 17.7200 - lr: 1.1111e-04 - 79s/epoch - 401ms/step
Epoch 951/1000
2023-09-28 05:43:35.938 
Epoch 951/1000 
	 loss: 17.5238, MinusLogProbMetric: 17.5238, val_loss: 17.5968, val_MinusLogProbMetric: 17.5968

Epoch 951: val_loss did not improve from 17.59634
196/196 - 79s - loss: 17.5238 - MinusLogProbMetric: 17.5238 - val_loss: 17.5968 - val_MinusLogProbMetric: 17.5968 - lr: 1.1111e-04 - 79s/epoch - 402ms/step
Epoch 952/1000
2023-09-28 05:44:53.578 
Epoch 952/1000 
	 loss: 17.5487, MinusLogProbMetric: 17.5487, val_loss: 17.6901, val_MinusLogProbMetric: 17.6901

Epoch 952: val_loss did not improve from 17.59634
196/196 - 78s - loss: 17.5487 - MinusLogProbMetric: 17.5487 - val_loss: 17.6901 - val_MinusLogProbMetric: 17.6901 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 953/1000
2023-09-28 05:46:10.714 
Epoch 953/1000 
	 loss: 17.5462, MinusLogProbMetric: 17.5462, val_loss: 17.5918, val_MinusLogProbMetric: 17.5918

Epoch 953: val_loss improved from 17.59634 to 17.59176, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 78s - loss: 17.5462 - MinusLogProbMetric: 17.5462 - val_loss: 17.5918 - val_MinusLogProbMetric: 17.5918 - lr: 1.1111e-04 - 78s/epoch - 400ms/step
Epoch 954/1000
2023-09-28 05:47:29.459 
Epoch 954/1000 
	 loss: 17.5227, MinusLogProbMetric: 17.5227, val_loss: 17.6769, val_MinusLogProbMetric: 17.6769

Epoch 954: val_loss did not improve from 17.59176
196/196 - 78s - loss: 17.5227 - MinusLogProbMetric: 17.5227 - val_loss: 17.6769 - val_MinusLogProbMetric: 17.6769 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 955/1000
2023-09-28 05:48:47.499 
Epoch 955/1000 
	 loss: 17.5065, MinusLogProbMetric: 17.5065, val_loss: 17.6862, val_MinusLogProbMetric: 17.6862

Epoch 955: val_loss did not improve from 17.59176
196/196 - 78s - loss: 17.5065 - MinusLogProbMetric: 17.5065 - val_loss: 17.6862 - val_MinusLogProbMetric: 17.6862 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 956/1000
2023-09-28 05:50:05.618 
Epoch 956/1000 
	 loss: 17.5143, MinusLogProbMetric: 17.5143, val_loss: 17.6707, val_MinusLogProbMetric: 17.6707

Epoch 956: val_loss did not improve from 17.59176
196/196 - 78s - loss: 17.5143 - MinusLogProbMetric: 17.5143 - val_loss: 17.6707 - val_MinusLogProbMetric: 17.6707 - lr: 1.1111e-04 - 78s/epoch - 399ms/step
Epoch 957/1000
2023-09-28 05:51:22.720 
Epoch 957/1000 
	 loss: 17.5388, MinusLogProbMetric: 17.5388, val_loss: 17.6259, val_MinusLogProbMetric: 17.6259

Epoch 957: val_loss did not improve from 17.59176
196/196 - 77s - loss: 17.5388 - MinusLogProbMetric: 17.5388 - val_loss: 17.6259 - val_MinusLogProbMetric: 17.6259 - lr: 1.1111e-04 - 77s/epoch - 393ms/step
Epoch 958/1000
2023-09-28 05:52:40.447 
Epoch 958/1000 
	 loss: 17.5000, MinusLogProbMetric: 17.5000, val_loss: 17.9216, val_MinusLogProbMetric: 17.9216

Epoch 958: val_loss did not improve from 17.59176
196/196 - 78s - loss: 17.5000 - MinusLogProbMetric: 17.5000 - val_loss: 17.9216 - val_MinusLogProbMetric: 17.9216 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 959/1000
2023-09-28 05:53:57.821 
Epoch 959/1000 
	 loss: 17.5224, MinusLogProbMetric: 17.5224, val_loss: 17.6442, val_MinusLogProbMetric: 17.6442

Epoch 959: val_loss did not improve from 17.59176
196/196 - 77s - loss: 17.5224 - MinusLogProbMetric: 17.5224 - val_loss: 17.6442 - val_MinusLogProbMetric: 17.6442 - lr: 1.1111e-04 - 77s/epoch - 395ms/step
Epoch 960/1000
2023-09-28 05:55:15.773 
Epoch 960/1000 
	 loss: 17.5217, MinusLogProbMetric: 17.5217, val_loss: 17.7064, val_MinusLogProbMetric: 17.7064

Epoch 960: val_loss did not improve from 17.59176
196/196 - 78s - loss: 17.5217 - MinusLogProbMetric: 17.5217 - val_loss: 17.7064 - val_MinusLogProbMetric: 17.7064 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 961/1000
2023-09-28 05:56:33.430 
Epoch 961/1000 
	 loss: 17.5286, MinusLogProbMetric: 17.5286, val_loss: 17.5744, val_MinusLogProbMetric: 17.5744

Epoch 961: val_loss improved from 17.59176 to 17.57440, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 79s - loss: 17.5286 - MinusLogProbMetric: 17.5286 - val_loss: 17.5744 - val_MinusLogProbMetric: 17.5744 - lr: 1.1111e-04 - 79s/epoch - 403ms/step
Epoch 962/1000
2023-09-28 05:57:52.364 
Epoch 962/1000 
	 loss: 17.5156, MinusLogProbMetric: 17.5156, val_loss: 17.7211, val_MinusLogProbMetric: 17.7211

Epoch 962: val_loss did not improve from 17.57440
196/196 - 78s - loss: 17.5156 - MinusLogProbMetric: 17.5156 - val_loss: 17.7211 - val_MinusLogProbMetric: 17.7211 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 963/1000
2023-09-28 05:59:09.018 
Epoch 963/1000 
	 loss: 17.5284, MinusLogProbMetric: 17.5284, val_loss: 17.8719, val_MinusLogProbMetric: 17.8719

Epoch 963: val_loss did not improve from 17.57440
196/196 - 77s - loss: 17.5284 - MinusLogProbMetric: 17.5284 - val_loss: 17.8719 - val_MinusLogProbMetric: 17.8719 - lr: 1.1111e-04 - 77s/epoch - 391ms/step
Epoch 964/1000
2023-09-28 06:00:27.608 
Epoch 964/1000 
	 loss: 17.5384, MinusLogProbMetric: 17.5384, val_loss: 17.6895, val_MinusLogProbMetric: 17.6895

Epoch 964: val_loss did not improve from 17.57440
196/196 - 79s - loss: 17.5384 - MinusLogProbMetric: 17.5384 - val_loss: 17.6895 - val_MinusLogProbMetric: 17.6895 - lr: 1.1111e-04 - 79s/epoch - 401ms/step
Epoch 965/1000
2023-09-28 06:01:46.120 
Epoch 965/1000 
	 loss: 17.5025, MinusLogProbMetric: 17.5025, val_loss: 17.6101, val_MinusLogProbMetric: 17.6101

Epoch 965: val_loss did not improve from 17.57440
196/196 - 79s - loss: 17.5025 - MinusLogProbMetric: 17.5025 - val_loss: 17.6101 - val_MinusLogProbMetric: 17.6101 - lr: 1.1111e-04 - 79s/epoch - 401ms/step
Epoch 966/1000
2023-09-28 06:03:03.629 
Epoch 966/1000 
	 loss: 17.5390, MinusLogProbMetric: 17.5390, val_loss: 17.6490, val_MinusLogProbMetric: 17.6490

Epoch 966: val_loss did not improve from 17.57440
196/196 - 78s - loss: 17.5390 - MinusLogProbMetric: 17.5390 - val_loss: 17.6490 - val_MinusLogProbMetric: 17.6490 - lr: 1.1111e-04 - 78s/epoch - 395ms/step
Epoch 967/1000
2023-09-28 06:04:20.482 
Epoch 967/1000 
	 loss: 17.5292, MinusLogProbMetric: 17.5292, val_loss: 17.6134, val_MinusLogProbMetric: 17.6134

Epoch 967: val_loss did not improve from 17.57440
196/196 - 77s - loss: 17.5292 - MinusLogProbMetric: 17.5292 - val_loss: 17.6134 - val_MinusLogProbMetric: 17.6134 - lr: 1.1111e-04 - 77s/epoch - 392ms/step
Epoch 968/1000
2023-09-28 06:05:38.328 
Epoch 968/1000 
	 loss: 17.5106, MinusLogProbMetric: 17.5106, val_loss: 17.8988, val_MinusLogProbMetric: 17.8988

Epoch 968: val_loss did not improve from 17.57440
196/196 - 78s - loss: 17.5106 - MinusLogProbMetric: 17.5106 - val_loss: 17.8988 - val_MinusLogProbMetric: 17.8988 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 969/1000
2023-09-28 06:06:56.228 
Epoch 969/1000 
	 loss: 17.5243, MinusLogProbMetric: 17.5243, val_loss: 17.7710, val_MinusLogProbMetric: 17.7710

Epoch 969: val_loss did not improve from 17.57440
196/196 - 78s - loss: 17.5243 - MinusLogProbMetric: 17.5243 - val_loss: 17.7710 - val_MinusLogProbMetric: 17.7710 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 970/1000
2023-09-28 06:08:14.291 
Epoch 970/1000 
	 loss: 17.5184, MinusLogProbMetric: 17.5184, val_loss: 17.6632, val_MinusLogProbMetric: 17.6632

Epoch 970: val_loss did not improve from 17.57440
196/196 - 78s - loss: 17.5184 - MinusLogProbMetric: 17.5184 - val_loss: 17.6632 - val_MinusLogProbMetric: 17.6632 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 971/1000
2023-09-28 06:09:33.303 
Epoch 971/1000 
	 loss: 17.5233, MinusLogProbMetric: 17.5233, val_loss: 17.6752, val_MinusLogProbMetric: 17.6752

Epoch 971: val_loss did not improve from 17.57440
196/196 - 79s - loss: 17.5233 - MinusLogProbMetric: 17.5233 - val_loss: 17.6752 - val_MinusLogProbMetric: 17.6752 - lr: 1.1111e-04 - 79s/epoch - 403ms/step
Epoch 972/1000
2023-09-28 06:10:50.960 
Epoch 972/1000 
	 loss: 17.5081, MinusLogProbMetric: 17.5081, val_loss: 17.9509, val_MinusLogProbMetric: 17.9509

Epoch 972: val_loss did not improve from 17.57440
196/196 - 78s - loss: 17.5081 - MinusLogProbMetric: 17.5081 - val_loss: 17.9509 - val_MinusLogProbMetric: 17.9509 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 973/1000
2023-09-28 06:12:09.242 
Epoch 973/1000 
	 loss: 17.5088, MinusLogProbMetric: 17.5088, val_loss: 17.6764, val_MinusLogProbMetric: 17.6764

Epoch 973: val_loss did not improve from 17.57440
196/196 - 78s - loss: 17.5088 - MinusLogProbMetric: 17.5088 - val_loss: 17.6764 - val_MinusLogProbMetric: 17.6764 - lr: 1.1111e-04 - 78s/epoch - 399ms/step
Epoch 974/1000
2023-09-28 06:13:26.782 
Epoch 974/1000 
	 loss: 17.5149, MinusLogProbMetric: 17.5149, val_loss: 17.7138, val_MinusLogProbMetric: 17.7138

Epoch 974: val_loss did not improve from 17.57440
196/196 - 78s - loss: 17.5149 - MinusLogProbMetric: 17.5149 - val_loss: 17.7138 - val_MinusLogProbMetric: 17.7138 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 975/1000
2023-09-28 06:14:44.035 
Epoch 975/1000 
	 loss: 17.5169, MinusLogProbMetric: 17.5169, val_loss: 17.6033, val_MinusLogProbMetric: 17.6033

Epoch 975: val_loss did not improve from 17.57440
196/196 - 77s - loss: 17.5169 - MinusLogProbMetric: 17.5169 - val_loss: 17.6033 - val_MinusLogProbMetric: 17.6033 - lr: 1.1111e-04 - 77s/epoch - 394ms/step
Epoch 976/1000
2023-09-28 06:16:01.389 
Epoch 976/1000 
	 loss: 17.4894, MinusLogProbMetric: 17.4894, val_loss: 17.6551, val_MinusLogProbMetric: 17.6551

Epoch 976: val_loss did not improve from 17.57440
196/196 - 77s - loss: 17.4894 - MinusLogProbMetric: 17.4894 - val_loss: 17.6551 - val_MinusLogProbMetric: 17.6551 - lr: 1.1111e-04 - 77s/epoch - 395ms/step
Epoch 977/1000
2023-09-28 06:17:19.127 
Epoch 977/1000 
	 loss: 17.4932, MinusLogProbMetric: 17.4932, val_loss: 17.6688, val_MinusLogProbMetric: 17.6688

Epoch 977: val_loss did not improve from 17.57440
196/196 - 78s - loss: 17.4932 - MinusLogProbMetric: 17.4932 - val_loss: 17.6688 - val_MinusLogProbMetric: 17.6688 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 978/1000
2023-09-28 06:18:36.372 
Epoch 978/1000 
	 loss: 17.4937, MinusLogProbMetric: 17.4937, val_loss: 17.6459, val_MinusLogProbMetric: 17.6459

Epoch 978: val_loss did not improve from 17.57440
196/196 - 77s - loss: 17.4937 - MinusLogProbMetric: 17.4937 - val_loss: 17.6459 - val_MinusLogProbMetric: 17.6459 - lr: 1.1111e-04 - 77s/epoch - 394ms/step
Epoch 979/1000
2023-09-28 06:19:53.773 
Epoch 979/1000 
	 loss: 17.5098, MinusLogProbMetric: 17.5098, val_loss: 17.6866, val_MinusLogProbMetric: 17.6866

Epoch 979: val_loss did not improve from 17.57440
196/196 - 77s - loss: 17.5098 - MinusLogProbMetric: 17.5098 - val_loss: 17.6866 - val_MinusLogProbMetric: 17.6866 - lr: 1.1111e-04 - 77s/epoch - 395ms/step
Epoch 980/1000
2023-09-28 06:21:12.186 
Epoch 980/1000 
	 loss: 17.5344, MinusLogProbMetric: 17.5344, val_loss: 17.6838, val_MinusLogProbMetric: 17.6838

Epoch 980: val_loss did not improve from 17.57440
196/196 - 78s - loss: 17.5344 - MinusLogProbMetric: 17.5344 - val_loss: 17.6838 - val_MinusLogProbMetric: 17.6838 - lr: 1.1111e-04 - 78s/epoch - 400ms/step
Epoch 981/1000
2023-09-28 06:22:29.607 
Epoch 981/1000 
	 loss: 17.4779, MinusLogProbMetric: 17.4779, val_loss: 17.6749, val_MinusLogProbMetric: 17.6749

Epoch 981: val_loss did not improve from 17.57440
196/196 - 77s - loss: 17.4779 - MinusLogProbMetric: 17.4779 - val_loss: 17.6749 - val_MinusLogProbMetric: 17.6749 - lr: 1.1111e-04 - 77s/epoch - 395ms/step
Epoch 982/1000
2023-09-28 06:23:47.208 
Epoch 982/1000 
	 loss: 17.5083, MinusLogProbMetric: 17.5083, val_loss: 17.6041, val_MinusLogProbMetric: 17.6041

Epoch 982: val_loss did not improve from 17.57440
196/196 - 78s - loss: 17.5083 - MinusLogProbMetric: 17.5083 - val_loss: 17.6041 - val_MinusLogProbMetric: 17.6041 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 983/1000
2023-09-28 06:25:04.179 
Epoch 983/1000 
	 loss: 17.5011, MinusLogProbMetric: 17.5011, val_loss: 17.5920, val_MinusLogProbMetric: 17.5920

Epoch 983: val_loss did not improve from 17.57440
196/196 - 77s - loss: 17.5011 - MinusLogProbMetric: 17.5011 - val_loss: 17.5920 - val_MinusLogProbMetric: 17.5920 - lr: 1.1111e-04 - 77s/epoch - 393ms/step
Epoch 984/1000
2023-09-28 06:26:21.114 
Epoch 984/1000 
	 loss: 17.4971, MinusLogProbMetric: 17.4971, val_loss: 17.5995, val_MinusLogProbMetric: 17.5995

Epoch 984: val_loss did not improve from 17.57440
196/196 - 77s - loss: 17.4971 - MinusLogProbMetric: 17.4971 - val_loss: 17.5995 - val_MinusLogProbMetric: 17.5995 - lr: 1.1111e-04 - 77s/epoch - 393ms/step
Epoch 985/1000
2023-09-28 06:27:33.801 
Epoch 985/1000 
	 loss: 17.4893, MinusLogProbMetric: 17.4893, val_loss: 17.5794, val_MinusLogProbMetric: 17.5794

Epoch 985: val_loss did not improve from 17.57440
196/196 - 73s - loss: 17.4893 - MinusLogProbMetric: 17.4893 - val_loss: 17.5794 - val_MinusLogProbMetric: 17.5794 - lr: 1.1111e-04 - 73s/epoch - 371ms/step
Epoch 986/1000
2023-09-28 06:28:41.422 
Epoch 986/1000 
	 loss: 17.5001, MinusLogProbMetric: 17.5001, val_loss: 17.5977, val_MinusLogProbMetric: 17.5977

Epoch 986: val_loss did not improve from 17.57440
196/196 - 68s - loss: 17.5001 - MinusLogProbMetric: 17.5001 - val_loss: 17.5977 - val_MinusLogProbMetric: 17.5977 - lr: 1.1111e-04 - 68s/epoch - 345ms/step
Epoch 987/1000
2023-09-28 06:29:56.207 
Epoch 987/1000 
	 loss: 17.4894, MinusLogProbMetric: 17.4894, val_loss: 17.7186, val_MinusLogProbMetric: 17.7186

Epoch 987: val_loss did not improve from 17.57440
196/196 - 75s - loss: 17.4894 - MinusLogProbMetric: 17.4894 - val_loss: 17.7186 - val_MinusLogProbMetric: 17.7186 - lr: 1.1111e-04 - 75s/epoch - 382ms/step
Epoch 988/1000
2023-09-28 06:31:09.112 
Epoch 988/1000 
	 loss: 17.4970, MinusLogProbMetric: 17.4970, val_loss: 17.7033, val_MinusLogProbMetric: 17.7033

Epoch 988: val_loss did not improve from 17.57440
196/196 - 73s - loss: 17.4970 - MinusLogProbMetric: 17.4970 - val_loss: 17.7033 - val_MinusLogProbMetric: 17.7033 - lr: 1.1111e-04 - 73s/epoch - 372ms/step
Epoch 989/1000
2023-09-28 06:32:17.313 
Epoch 989/1000 
	 loss: 17.4982, MinusLogProbMetric: 17.4982, val_loss: 17.6145, val_MinusLogProbMetric: 17.6145

Epoch 989: val_loss did not improve from 17.57440
196/196 - 68s - loss: 17.4982 - MinusLogProbMetric: 17.4982 - val_loss: 17.6145 - val_MinusLogProbMetric: 17.6145 - lr: 1.1111e-04 - 68s/epoch - 348ms/step
Epoch 990/1000
2023-09-28 06:33:34.206 
Epoch 990/1000 
	 loss: 17.4866, MinusLogProbMetric: 17.4866, val_loss: 17.7220, val_MinusLogProbMetric: 17.7220

Epoch 990: val_loss did not improve from 17.57440
196/196 - 77s - loss: 17.4866 - MinusLogProbMetric: 17.4866 - val_loss: 17.7220 - val_MinusLogProbMetric: 17.7220 - lr: 1.1111e-04 - 77s/epoch - 392ms/step
Epoch 991/1000
2023-09-28 06:34:43.657 
Epoch 991/1000 
	 loss: 17.4958, MinusLogProbMetric: 17.4958, val_loss: 17.6779, val_MinusLogProbMetric: 17.6779

Epoch 991: val_loss did not improve from 17.57440
196/196 - 69s - loss: 17.4958 - MinusLogProbMetric: 17.4958 - val_loss: 17.6779 - val_MinusLogProbMetric: 17.6779 - lr: 1.1111e-04 - 69s/epoch - 354ms/step
Epoch 992/1000
2023-09-28 06:35:55.606 
Epoch 992/1000 
	 loss: 17.5063, MinusLogProbMetric: 17.5063, val_loss: 17.5869, val_MinusLogProbMetric: 17.5869

Epoch 992: val_loss did not improve from 17.57440
196/196 - 72s - loss: 17.5063 - MinusLogProbMetric: 17.5063 - val_loss: 17.5869 - val_MinusLogProbMetric: 17.5869 - lr: 1.1111e-04 - 72s/epoch - 367ms/step
Epoch 993/1000
2023-09-28 06:37:11.519 
Epoch 993/1000 
	 loss: 17.4820, MinusLogProbMetric: 17.4820, val_loss: 17.6566, val_MinusLogProbMetric: 17.6566

Epoch 993: val_loss did not improve from 17.57440
196/196 - 76s - loss: 17.4820 - MinusLogProbMetric: 17.4820 - val_loss: 17.6566 - val_MinusLogProbMetric: 17.6566 - lr: 1.1111e-04 - 76s/epoch - 387ms/step
Epoch 994/1000
2023-09-28 06:38:19.818 
Epoch 994/1000 
	 loss: 17.4856, MinusLogProbMetric: 17.4856, val_loss: 17.6523, val_MinusLogProbMetric: 17.6523

Epoch 994: val_loss did not improve from 17.57440
196/196 - 68s - loss: 17.4856 - MinusLogProbMetric: 17.4856 - val_loss: 17.6523 - val_MinusLogProbMetric: 17.6523 - lr: 1.1111e-04 - 68s/epoch - 348ms/step
Epoch 995/1000
2023-09-28 06:39:35.912 
Epoch 995/1000 
	 loss: 17.4826, MinusLogProbMetric: 17.4826, val_loss: 17.5966, val_MinusLogProbMetric: 17.5966

Epoch 995: val_loss did not improve from 17.57440
196/196 - 76s - loss: 17.4826 - MinusLogProbMetric: 17.4826 - val_loss: 17.5966 - val_MinusLogProbMetric: 17.5966 - lr: 1.1111e-04 - 76s/epoch - 388ms/step
Epoch 996/1000
2023-09-28 06:40:44.907 
Epoch 996/1000 
	 loss: 17.5009, MinusLogProbMetric: 17.5009, val_loss: 17.7775, val_MinusLogProbMetric: 17.7775

Epoch 996: val_loss did not improve from 17.57440
196/196 - 69s - loss: 17.5009 - MinusLogProbMetric: 17.5009 - val_loss: 17.7775 - val_MinusLogProbMetric: 17.7775 - lr: 1.1111e-04 - 69s/epoch - 352ms/step
Epoch 997/1000
2023-09-28 06:41:56.142 
Epoch 997/1000 
	 loss: 17.4913, MinusLogProbMetric: 17.4913, val_loss: 17.5951, val_MinusLogProbMetric: 17.5951

Epoch 997: val_loss did not improve from 17.57440
196/196 - 71s - loss: 17.4913 - MinusLogProbMetric: 17.4913 - val_loss: 17.5951 - val_MinusLogProbMetric: 17.5951 - lr: 1.1111e-04 - 71s/epoch - 363ms/step
Epoch 998/1000
2023-09-28 06:43:09.963 
Epoch 998/1000 
	 loss: 17.4962, MinusLogProbMetric: 17.4962, val_loss: 17.7220, val_MinusLogProbMetric: 17.7220

Epoch 998: val_loss did not improve from 17.57440
196/196 - 74s - loss: 17.4962 - MinusLogProbMetric: 17.4962 - val_loss: 17.7220 - val_MinusLogProbMetric: 17.7220 - lr: 1.1111e-04 - 74s/epoch - 377ms/step
Epoch 999/1000
2023-09-28 06:44:17.771 
Epoch 999/1000 
	 loss: 17.5200, MinusLogProbMetric: 17.5200, val_loss: 17.6089, val_MinusLogProbMetric: 17.6089

Epoch 999: val_loss did not improve from 17.57440
196/196 - 68s - loss: 17.5200 - MinusLogProbMetric: 17.5200 - val_loss: 17.6089 - val_MinusLogProbMetric: 17.6089 - lr: 1.1111e-04 - 68s/epoch - 346ms/step
Epoch 1000/1000
2023-09-28 06:45:31.991 
Epoch 1000/1000 
	 loss: 17.4919, MinusLogProbMetric: 17.4919, val_loss: 17.5711, val_MinusLogProbMetric: 17.5711

Epoch 1000: val_loss improved from 17.57440 to 17.57109, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_303/weights/best_weights.h5
196/196 - 76s - loss: 17.4919 - MinusLogProbMetric: 17.4919 - val_loss: 17.5711 - val_MinusLogProbMetric: 17.5711 - lr: 1.1111e-04 - 76s/epoch - 386ms/step
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.

------------------------------------------
Starting LR metric calculation...
Running TF LR calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
LR metric calculation completed in 42.13069864700083 seconds.

------------------------------------------
Starting KS tests calculation...
Running TF KS tests...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
KS tests calculation completed in 15.702287253981922 seconds.

------------------------------------------
Starting SWD metric calculation...
Running TF SWD calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
SWD metric calculation completed in 12.239985743013676 seconds.

------------------------------------------
Starting FN metric calculation...
Running TF FN calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
FN metric calculation completed in 13.342862876015715 seconds.
Training succeeded with seed 869.
Model trained in 77341.81 s.

===========
Computing predictions
===========

Computing metrics...
Metrics computed in 85.65 s.
                ===========
                print("===========
Failed to plot
===========
")
                Exception type: ValueError
                Exception message: Number of rows must be a positive integer, not 0
                Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN.py , Line : 469, Func.Name : prediction_function, Message : Plotters.cornerplotter(X_data_test.numpy(),X_data_nf.numpy(),path_to_results,ndims,norm=True) # type: ignore', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/Plotters.py , Line : 156, Func.Name : cornerplotter, Message : figure=corner.corner(target_samples,color=\'red\',bins=n_bins,labels=[r"%s" % s for s in labels],normalize1d=True)', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/corner.py , Line : 235, Func.Name : corner, Message : fig, axes = pl.subplots(K, K, figsize=(dim, dim))', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/pyplot.py , Line : 1502, Func.Name : subplots, Message : axs = fig.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 905, Func.Name : subplots, Message : gs = self.add_gridspec(nrows, ncols, figure=self, **gridspec_kw)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 1527, Func.Name : add_gridspec, Message : gs = GridSpec(nrows=nrows, ncols=ncols, figure=self, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 379, Func.Name : __init__, Message : super().__init__(nrows, ncols,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 49, Func.Name : __init__, Message : raise ValueError(']
                ===========

results.txt saved
results.json saved
Results log saved
Model predictions computed in 85.95 s.
===========
Run 303/720 done in 78678.56 s.
===========

Directory ../../results/CsplineN_new/run_304/ already exists.
Skipping it.
===========
Run 304/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_305/ already exists.
Skipping it.
===========
Run 305/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_306/ already exists.
Skipping it.
===========
Run 306/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_307/ already exists.
Skipping it.
===========
Run 307/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_308/ already exists.
Skipping it.
===========
Run 308/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_309/ already exists.
Skipping it.
===========
Run 309/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_310/ already exists.
Skipping it.
===========
Run 310/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_311/ already exists.
Skipping it.
===========
Run 311/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_312/ already exists.
Skipping it.
===========
Run 312/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_313/ already exists.
Skipping it.
===========
Run 313/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_314/ already exists.
Skipping it.
===========
Run 314/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_315/ already exists.
Skipping it.
===========
Run 315/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_316/ already exists.
Skipping it.
===========
Run 316/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_317/ already exists.
Skipping it.
===========
Run 317/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_318/ already exists.
Skipping it.
===========
Run 318/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_319/ already exists.
Skipping it.
===========
Run 319/720 already exists. Skipping it.
===========

===========
Generating train data for run 320.
===========
Train data generated in 0.17 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_320/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 933}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_320/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[2.7429588 , 2.5700626 , 7.723717  , ..., 7.7501717 , 2.9186718 ,
        1.775845  ],
       [4.728784  , 6.065677  , 0.9645814 , ..., 1.2628946 , 6.7253027 ,
        1.4381479 ],
       [3.8513129 , 6.1744533 , 0.2601294 , ..., 1.1662469 , 6.572044  ,
        1.3500103 ],
       ...,
       [2.8332918 , 4.2526407 , 9.701975  , ..., 7.0738525 , 2.5952196 ,
        1.8190176 ],
       [3.118363  , 5.4312434 , 0.32481733, ..., 1.9350358 , 6.6219816 ,
        1.3230293 ],
       [1.792576  , 4.9344916 , 9.207544  , ..., 7.378074  , 2.857349  ,
        1.56005   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_320/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_320
self.data_kwargs: {'seed': 933}
self.x_data: [[4.6118093  6.294248   0.6753535  ... 1.3966799  6.5528636  1.404321  ]
 [5.0913153  7.181254   5.302016   ... 4.927679   2.6276157  7.5562596 ]
 [2.3468595  3.6813745  8.449473   ... 7.5625257  2.3049273  1.7164713 ]
 ...
 [1.7774892  4.58926    7.003348   ... 7.297376   3.211844   1.3889377 ]
 [5.1396513  5.5684643  1.4578001  ... 0.69219434 6.9603205  1.3834548 ]
 [3.2499232  3.5331872  8.207085   ... 6.760554   3.3946218  2.0297966 ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_71"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_72 (InputLayer)       [(None, 32)]              0         
                                                                 
 log_prob_layer_6 (LogProbLa  (None,)                  2798560   
 yer)                                                            
                                                                 
=================================================================
Total params: 2,798,560
Trainable params: 2,798,560
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_6/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_6'")
self.model: <keras.engine.functional.Functional object at 0x7fb0946692a0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fb0fc7e8880>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fb0fc7e8880>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fb0941d8280>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fb21c445360>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_320/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fb21c447610>, <keras.callbacks.ModelCheckpoint object at 0x7fb21c447c70>, <keras.callbacks.EarlyStopping object at 0x7fb21c447e80>, <keras.callbacks.ReduceLROnPlateau object at 0x7fb21c447f10>, <keras.callbacks.TerminateOnNaN object at 0x7fb21c4475e0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[2.7429588 , 2.5700626 , 7.723717  , ..., 7.7501717 , 2.9186718 ,
        1.775845  ],
       [4.728784  , 6.065677  , 0.9645814 , ..., 1.2628946 , 6.7253027 ,
        1.4381479 ],
       [3.8513129 , 6.1744533 , 0.2601294 , ..., 1.1662469 , 6.572044  ,
        1.3500103 ],
       ...,
       [2.8332918 , 4.2526407 , 9.701975  , ..., 7.0738525 , 2.5952196 ,
        1.8190176 ],
       [3.118363  , 5.4312434 , 0.32481733, ..., 1.9350358 , 6.6219816 ,
        1.3230293 ],
       [1.792576  , 4.9344916 , 9.207544  , ..., 7.378074  , 2.857349  ,
        1.56005   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_320/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 320/720 with hyperparameters:
timestamp = 2023-09-28 06:47:09.700597
ndims = 32
seed_train = 933
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 2798560
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 4.6118093   6.294248    0.6753535   5.561676    6.128757    6.145322
  9.785121    7.1052794   3.7474928   5.3494825   6.3791265   0.5788075
  5.683702    6.3188157   1.6447939   1.0492666   3.6812766   3.7191741
  5.81123     3.2186081  10.359394    0.68370855  2.036205    1.0944312
  6.489696    3.6441135   4.6169076   1.7655768   1.5199437   1.3966799
  6.5528636   1.404321  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 31: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-28 06:50:39.120 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 1424.6708, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 209s - loss: nan - MinusLogProbMetric: 1424.6708 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 209s/epoch - 1s/step
The loss history contains NaN values.
Training failed: trying again with seed 402839 and lr 0.0003333333333333333.
===========
Generating train data for run 320.
===========
Train data generated in 0.17 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_320/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 933}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_320/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[2.7429588 , 2.5700626 , 7.723717  , ..., 7.7501717 , 2.9186718 ,
        1.775845  ],
       [4.728784  , 6.065677  , 0.9645814 , ..., 1.2628946 , 6.7253027 ,
        1.4381479 ],
       [3.8513129 , 6.1744533 , 0.2601294 , ..., 1.1662469 , 6.572044  ,
        1.3500103 ],
       ...,
       [2.8332918 , 4.2526407 , 9.701975  , ..., 7.0738525 , 2.5952196 ,
        1.8190176 ],
       [3.118363  , 5.4312434 , 0.32481733, ..., 1.9350358 , 6.6219816 ,
        1.3230293 ],
       [1.792576  , 4.9344916 , 9.207544  , ..., 7.378074  , 2.857349  ,
        1.56005   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_320/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_320
self.data_kwargs: {'seed': 933}
self.x_data: [[4.6118093  6.294248   0.6753535  ... 1.3966799  6.5528636  1.404321  ]
 [5.0913153  7.181254   5.302016   ... 4.927679   2.6276157  7.5562596 ]
 [2.3468595  3.6813745  8.449473   ... 7.5625257  2.3049273  1.7164713 ]
 ...
 [1.7774892  4.58926    7.003348   ... 7.297376   3.211844   1.3889377 ]
 [5.1396513  5.5684643  1.4578001  ... 0.69219434 6.9603205  1.3834548 ]
 [3.2499232  3.5331872  8.207085   ... 6.760554   3.3946218  2.0297966 ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_82"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_83 (InputLayer)       [(None, 32)]              0         
                                                                 
 log_prob_layer_7 (LogProbLa  (None,)                  2798560   
 yer)                                                            
                                                                 
=================================================================
Total params: 2,798,560
Trainable params: 2,798,560
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_7/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_7'")
self.model: <keras.engine.functional.Functional object at 0x7fb1ce9919f0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fb0843c46d0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fb0843c46d0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fb0745984f0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fb07459ac20>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_320/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fb074599420>, <keras.callbacks.ModelCheckpoint object at 0x7fb07459bb50>, <keras.callbacks.EarlyStopping object at 0x7fb0745999c0>, <keras.callbacks.ReduceLROnPlateau object at 0x7fb07459b7f0>, <keras.callbacks.TerminateOnNaN object at 0x7fb0745988b0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[2.7429588 , 2.5700626 , 7.723717  , ..., 7.7501717 , 2.9186718 ,
        1.775845  ],
       [4.728784  , 6.065677  , 0.9645814 , ..., 1.2628946 , 6.7253027 ,
        1.4381479 ],
       [3.8513129 , 6.1744533 , 0.2601294 , ..., 1.1662469 , 6.572044  ,
        1.3500103 ],
       ...,
       [2.8332918 , 4.2526407 , 9.701975  , ..., 7.0738525 , 2.5952196 ,
        1.8190176 ],
       [3.118363  , 5.4312434 , 0.32481733, ..., 1.9350358 , 6.6219816 ,
        1.3230293 ],
       [1.792576  , 4.9344916 , 9.207544  , ..., 7.378074  , 2.857349  ,
        1.56005   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_320/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 320/720 with hyperparameters:
timestamp = 2023-09-28 06:50:53.428053
ndims = 32
seed_train = 933
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 2798560
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 4.6118093   6.294248    0.6753535   5.561676    6.128757    6.145322
  9.785121    7.1052794   3.7474928   5.3494825   6.3791265   0.5788075
  5.683702    6.3188157   1.6447939   1.0492666   3.6812766   3.7191741
  5.81123     3.2186081  10.359394    0.68370855  2.036205    1.0944312
  6.489696    3.6441135   4.6169076   1.7655768   1.5199437   1.3966799
  6.5528636   1.404321  ]
Epoch 1/1000
2023-09-28 06:55:37.191 
Epoch 1/1000 
	 loss: 213.8754, MinusLogProbMetric: 213.8754, val_loss: 55.5926, val_MinusLogProbMetric: 55.5926

Epoch 1: val_loss improved from inf to 55.59263, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_320/weights/best_weights.h5
196/196 - 285s - loss: 213.8754 - MinusLogProbMetric: 213.8754 - val_loss: 55.5926 - val_MinusLogProbMetric: 55.5926 - lr: 3.3333e-04 - 285s/epoch - 1s/step
Epoch 2/1000
2023-09-28 06:56:49.685 
Epoch 2/1000 
	 loss: 41.7278, MinusLogProbMetric: 41.7278, val_loss: 33.6781, val_MinusLogProbMetric: 33.6781

Epoch 2: val_loss improved from 55.59263 to 33.67805, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_320/weights/best_weights.h5
196/196 - 72s - loss: 41.7278 - MinusLogProbMetric: 41.7278 - val_loss: 33.6781 - val_MinusLogProbMetric: 33.6781 - lr: 3.3333e-04 - 72s/epoch - 367ms/step
Epoch 3/1000
2023-09-28 06:58:00.702 
Epoch 3/1000 
	 loss: 32.0107, MinusLogProbMetric: 32.0107, val_loss: 29.5888, val_MinusLogProbMetric: 29.5888

Epoch 3: val_loss improved from 33.67805 to 29.58878, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_320/weights/best_weights.h5
196/196 - 71s - loss: 32.0107 - MinusLogProbMetric: 32.0107 - val_loss: 29.5888 - val_MinusLogProbMetric: 29.5888 - lr: 3.3333e-04 - 71s/epoch - 363ms/step
Epoch 4/1000
2023-09-28 06:59:19.545 
Epoch 4/1000 
	 loss: 27.5168, MinusLogProbMetric: 27.5168, val_loss: 26.2528, val_MinusLogProbMetric: 26.2528

Epoch 4: val_loss improved from 29.58878 to 26.25275, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_320/weights/best_weights.h5
196/196 - 79s - loss: 27.5168 - MinusLogProbMetric: 27.5168 - val_loss: 26.2528 - val_MinusLogProbMetric: 26.2528 - lr: 3.3333e-04 - 79s/epoch - 402ms/step
Epoch 5/1000
2023-09-28 07:00:29.129 
Epoch 5/1000 
	 loss: 25.5165, MinusLogProbMetric: 25.5165, val_loss: 25.7393, val_MinusLogProbMetric: 25.7393

Epoch 5: val_loss improved from 26.25275 to 25.73933, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_320/weights/best_weights.h5
196/196 - 69s - loss: 25.5165 - MinusLogProbMetric: 25.5165 - val_loss: 25.7393 - val_MinusLogProbMetric: 25.7393 - lr: 3.3333e-04 - 69s/epoch - 352ms/step
Epoch 6/1000
2023-09-28 07:01:43.519 
Epoch 6/1000 
	 loss: 24.4947, MinusLogProbMetric: 24.4947, val_loss: 25.4837, val_MinusLogProbMetric: 25.4837

Epoch 6: val_loss improved from 25.73933 to 25.48369, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_320/weights/best_weights.h5
196/196 - 75s - loss: 24.4947 - MinusLogProbMetric: 24.4947 - val_loss: 25.4837 - val_MinusLogProbMetric: 25.4837 - lr: 3.3333e-04 - 75s/epoch - 381ms/step
Epoch 7/1000
2023-09-28 07:03:02.225 
Epoch 7/1000 
	 loss: 23.3444, MinusLogProbMetric: 23.3444, val_loss: 23.6648, val_MinusLogProbMetric: 23.6648

Epoch 7: val_loss improved from 25.48369 to 23.66482, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_320/weights/best_weights.h5
196/196 - 79s - loss: 23.3444 - MinusLogProbMetric: 23.3444 - val_loss: 23.6648 - val_MinusLogProbMetric: 23.6648 - lr: 3.3333e-04 - 79s/epoch - 401ms/step
Epoch 8/1000
2023-09-28 07:04:16.213 
Epoch 8/1000 
	 loss: 23.4186, MinusLogProbMetric: 23.4186, val_loss: 22.8822, val_MinusLogProbMetric: 22.8822

Epoch 8: val_loss improved from 23.66482 to 22.88219, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_320/weights/best_weights.h5
196/196 - 74s - loss: 23.4186 - MinusLogProbMetric: 23.4186 - val_loss: 22.8822 - val_MinusLogProbMetric: 22.8822 - lr: 3.3333e-04 - 74s/epoch - 377ms/step
Epoch 9/1000
2023-09-28 07:05:34.434 
Epoch 9/1000 
	 loss: 22.3937, MinusLogProbMetric: 22.3937, val_loss: 21.5568, val_MinusLogProbMetric: 21.5568

Epoch 9: val_loss improved from 22.88219 to 21.55683, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_320/weights/best_weights.h5
196/196 - 79s - loss: 22.3937 - MinusLogProbMetric: 22.3937 - val_loss: 21.5568 - val_MinusLogProbMetric: 21.5568 - lr: 3.3333e-04 - 79s/epoch - 401ms/step
Epoch 10/1000
2023-09-28 07:06:53.507 
Epoch 10/1000 
	 loss: 21.7650, MinusLogProbMetric: 21.7650, val_loss: 22.5787, val_MinusLogProbMetric: 22.5787

Epoch 10: val_loss did not improve from 21.55683
196/196 - 78s - loss: 21.7650 - MinusLogProbMetric: 21.7650 - val_loss: 22.5787 - val_MinusLogProbMetric: 22.5787 - lr: 3.3333e-04 - 78s/epoch - 396ms/step
Epoch 11/1000
2023-09-28 07:08:11.568 
Epoch 11/1000 
	 loss: 21.5068, MinusLogProbMetric: 21.5068, val_loss: 21.1850, val_MinusLogProbMetric: 21.1850

Epoch 11: val_loss improved from 21.55683 to 21.18502, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_320/weights/best_weights.h5
196/196 - 80s - loss: 21.5068 - MinusLogProbMetric: 21.5068 - val_loss: 21.1850 - val_MinusLogProbMetric: 21.1850 - lr: 3.3333e-04 - 80s/epoch - 406ms/step
Epoch 12/1000
2023-09-28 07:09:29.276 
Epoch 12/1000 
	 loss: 21.2507, MinusLogProbMetric: 21.2507, val_loss: 20.9122, val_MinusLogProbMetric: 20.9122

Epoch 12: val_loss improved from 21.18502 to 20.91222, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_320/weights/best_weights.h5
196/196 - 78s - loss: 21.2507 - MinusLogProbMetric: 21.2507 - val_loss: 20.9122 - val_MinusLogProbMetric: 20.9122 - lr: 3.3333e-04 - 78s/epoch - 396ms/step
Epoch 13/1000
2023-09-28 07:10:43.898 
Epoch 13/1000 
	 loss: 21.0954, MinusLogProbMetric: 21.0954, val_loss: 21.2271, val_MinusLogProbMetric: 21.2271

Epoch 13: val_loss did not improve from 20.91222
196/196 - 73s - loss: 21.0954 - MinusLogProbMetric: 21.0954 - val_loss: 21.2271 - val_MinusLogProbMetric: 21.2271 - lr: 3.3333e-04 - 73s/epoch - 373ms/step
Epoch 14/1000
2023-09-28 07:12:01.141 
Epoch 14/1000 
	 loss: 21.0893, MinusLogProbMetric: 21.0893, val_loss: 22.4222, val_MinusLogProbMetric: 22.4222

Epoch 14: val_loss did not improve from 20.91222
196/196 - 77s - loss: 21.0893 - MinusLogProbMetric: 21.0893 - val_loss: 22.4222 - val_MinusLogProbMetric: 22.4222 - lr: 3.3333e-04 - 77s/epoch - 394ms/step
Epoch 15/1000
2023-09-28 07:13:18.784 
Epoch 15/1000 
	 loss: 20.6213, MinusLogProbMetric: 20.6213, val_loss: 20.1048, val_MinusLogProbMetric: 20.1048

Epoch 15: val_loss improved from 20.91222 to 20.10480, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_320/weights/best_weights.h5
196/196 - 79s - loss: 20.6213 - MinusLogProbMetric: 20.6213 - val_loss: 20.1048 - val_MinusLogProbMetric: 20.1048 - lr: 3.3333e-04 - 79s/epoch - 402ms/step
Epoch 16/1000
2023-09-28 07:14:37.437 
Epoch 16/1000 
	 loss: 20.4635, MinusLogProbMetric: 20.4635, val_loss: 21.2145, val_MinusLogProbMetric: 21.2145

Epoch 16: val_loss did not improve from 20.10480
196/196 - 77s - loss: 20.4635 - MinusLogProbMetric: 20.4635 - val_loss: 21.2145 - val_MinusLogProbMetric: 21.2145 - lr: 3.3333e-04 - 77s/epoch - 395ms/step
Epoch 17/1000
2023-09-28 07:15:55.241 
Epoch 17/1000 
	 loss: 20.2729, MinusLogProbMetric: 20.2729, val_loss: 21.5808, val_MinusLogProbMetric: 21.5808

Epoch 17: val_loss did not improve from 20.10480
196/196 - 78s - loss: 20.2729 - MinusLogProbMetric: 20.2729 - val_loss: 21.5808 - val_MinusLogProbMetric: 21.5808 - lr: 3.3333e-04 - 78s/epoch - 397ms/step
Epoch 18/1000
2023-09-28 07:17:13.943 
Epoch 18/1000 
	 loss: 20.3107, MinusLogProbMetric: 20.3107, val_loss: 20.0061, val_MinusLogProbMetric: 20.0061

Epoch 18: val_loss improved from 20.10480 to 20.00613, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_320/weights/best_weights.h5
196/196 - 80s - loss: 20.3107 - MinusLogProbMetric: 20.3107 - val_loss: 20.0061 - val_MinusLogProbMetric: 20.0061 - lr: 3.3333e-04 - 80s/epoch - 408ms/step
Epoch 19/1000
2023-09-28 07:18:34.206 
Epoch 19/1000 
	 loss: 20.1580, MinusLogProbMetric: 20.1580, val_loss: 20.1489, val_MinusLogProbMetric: 20.1489

Epoch 19: val_loss did not improve from 20.00613
196/196 - 79s - loss: 20.1580 - MinusLogProbMetric: 20.1580 - val_loss: 20.1489 - val_MinusLogProbMetric: 20.1489 - lr: 3.3333e-04 - 79s/epoch - 403ms/step
Epoch 20/1000
2023-09-28 07:19:52.256 
Epoch 20/1000 
	 loss: 19.9698, MinusLogProbMetric: 19.9698, val_loss: 21.7003, val_MinusLogProbMetric: 21.7003

Epoch 20: val_loss did not improve from 20.00613
196/196 - 78s - loss: 19.9698 - MinusLogProbMetric: 19.9698 - val_loss: 21.7003 - val_MinusLogProbMetric: 21.7003 - lr: 3.3333e-04 - 78s/epoch - 398ms/step
Epoch 21/1000
2023-09-28 07:21:09.754 
Epoch 21/1000 
	 loss: 19.8214, MinusLogProbMetric: 19.8214, val_loss: 20.3056, val_MinusLogProbMetric: 20.3056

Epoch 21: val_loss did not improve from 20.00613
196/196 - 77s - loss: 19.8214 - MinusLogProbMetric: 19.8214 - val_loss: 20.3056 - val_MinusLogProbMetric: 20.3056 - lr: 3.3333e-04 - 77s/epoch - 395ms/step
Epoch 22/1000
2023-09-28 07:22:28.038 
Epoch 22/1000 
	 loss: 19.6974, MinusLogProbMetric: 19.6974, val_loss: 20.0347, val_MinusLogProbMetric: 20.0347

Epoch 22: val_loss did not improve from 20.00613
196/196 - 78s - loss: 19.6974 - MinusLogProbMetric: 19.6974 - val_loss: 20.0347 - val_MinusLogProbMetric: 20.0347 - lr: 3.3333e-04 - 78s/epoch - 399ms/step
Epoch 23/1000
2023-09-28 07:23:46.579 
Epoch 23/1000 
	 loss: 19.7388, MinusLogProbMetric: 19.7388, val_loss: 19.4419, val_MinusLogProbMetric: 19.4419

Epoch 23: val_loss improved from 20.00613 to 19.44193, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_320/weights/best_weights.h5
196/196 - 80s - loss: 19.7388 - MinusLogProbMetric: 19.7388 - val_loss: 19.4419 - val_MinusLogProbMetric: 19.4419 - lr: 3.3333e-04 - 80s/epoch - 408ms/step
Epoch 24/1000
2023-09-28 07:25:06.354 
Epoch 24/1000 
	 loss: 19.6433, MinusLogProbMetric: 19.6433, val_loss: 19.5467, val_MinusLogProbMetric: 19.5467

Epoch 24: val_loss did not improve from 19.44193
196/196 - 78s - loss: 19.6433 - MinusLogProbMetric: 19.6433 - val_loss: 19.5467 - val_MinusLogProbMetric: 19.5467 - lr: 3.3333e-04 - 78s/epoch - 399ms/step
Epoch 25/1000
2023-09-28 07:26:24.032 
Epoch 25/1000 
	 loss: 19.4734, MinusLogProbMetric: 19.4734, val_loss: 21.0603, val_MinusLogProbMetric: 21.0603

Epoch 25: val_loss did not improve from 19.44193
196/196 - 78s - loss: 19.4734 - MinusLogProbMetric: 19.4734 - val_loss: 21.0603 - val_MinusLogProbMetric: 21.0603 - lr: 3.3333e-04 - 78s/epoch - 396ms/step
Epoch 26/1000
2023-09-28 07:27:41.773 
Epoch 26/1000 
	 loss: 19.5115, MinusLogProbMetric: 19.5115, val_loss: 19.1883, val_MinusLogProbMetric: 19.1883

Epoch 26: val_loss improved from 19.44193 to 19.18831, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_320/weights/best_weights.h5
196/196 - 79s - loss: 19.5115 - MinusLogProbMetric: 19.5115 - val_loss: 19.1883 - val_MinusLogProbMetric: 19.1883 - lr: 3.3333e-04 - 79s/epoch - 404ms/step
Epoch 27/1000
2023-09-28 07:29:01.755 
Epoch 27/1000 
	 loss: 19.5219, MinusLogProbMetric: 19.5219, val_loss: 19.7643, val_MinusLogProbMetric: 19.7643

Epoch 27: val_loss did not improve from 19.18831
196/196 - 78s - loss: 19.5219 - MinusLogProbMetric: 19.5219 - val_loss: 19.7643 - val_MinusLogProbMetric: 19.7643 - lr: 3.3333e-04 - 78s/epoch - 400ms/step
Epoch 28/1000
2023-09-28 07:30:19.650 
Epoch 28/1000 
	 loss: 19.2542, MinusLogProbMetric: 19.2542, val_loss: 19.2699, val_MinusLogProbMetric: 19.2699

Epoch 28: val_loss did not improve from 19.18831
196/196 - 78s - loss: 19.2542 - MinusLogProbMetric: 19.2542 - val_loss: 19.2699 - val_MinusLogProbMetric: 19.2699 - lr: 3.3333e-04 - 78s/epoch - 397ms/step
Epoch 29/1000
2023-09-28 07:31:37.842 
Epoch 29/1000 
	 loss: 19.2693, MinusLogProbMetric: 19.2693, val_loss: 20.7078, val_MinusLogProbMetric: 20.7078

Epoch 29: val_loss did not improve from 19.18831
196/196 - 78s - loss: 19.2693 - MinusLogProbMetric: 19.2693 - val_loss: 20.7078 - val_MinusLogProbMetric: 20.7078 - lr: 3.3333e-04 - 78s/epoch - 399ms/step
Epoch 30/1000
2023-09-28 07:32:56.185 
Epoch 30/1000 
	 loss: 19.2400, MinusLogProbMetric: 19.2400, val_loss: 18.9914, val_MinusLogProbMetric: 18.9914

Epoch 30: val_loss improved from 19.18831 to 18.99140, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_320/weights/best_weights.h5
196/196 - 80s - loss: 19.2400 - MinusLogProbMetric: 19.2400 - val_loss: 18.9914 - val_MinusLogProbMetric: 18.9914 - lr: 3.3333e-04 - 80s/epoch - 408ms/step
Epoch 31/1000
2023-09-28 07:34:15.531 
Epoch 31/1000 
	 loss: 19.1294, MinusLogProbMetric: 19.1294, val_loss: 19.4371, val_MinusLogProbMetric: 19.4371

Epoch 31: val_loss did not improve from 18.99140
196/196 - 78s - loss: 19.1294 - MinusLogProbMetric: 19.1294 - val_loss: 19.4371 - val_MinusLogProbMetric: 19.4371 - lr: 3.3333e-04 - 78s/epoch - 397ms/step
Epoch 32/1000
2023-09-28 07:35:32.718 
Epoch 32/1000 
	 loss: 19.0499, MinusLogProbMetric: 19.0499, val_loss: 18.9696, val_MinusLogProbMetric: 18.9696

Epoch 32: val_loss improved from 18.99140 to 18.96959, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_320/weights/best_weights.h5
196/196 - 78s - loss: 19.0499 - MinusLogProbMetric: 19.0499 - val_loss: 18.9696 - val_MinusLogProbMetric: 18.9696 - lr: 3.3333e-04 - 78s/epoch - 400ms/step
Epoch 33/1000
2023-09-28 07:36:51.297 
Epoch 33/1000 
	 loss: 19.0708, MinusLogProbMetric: 19.0708, val_loss: 19.1664, val_MinusLogProbMetric: 19.1664

Epoch 33: val_loss did not improve from 18.96959
196/196 - 77s - loss: 19.0708 - MinusLogProbMetric: 19.0708 - val_loss: 19.1664 - val_MinusLogProbMetric: 19.1664 - lr: 3.3333e-04 - 77s/epoch - 394ms/step
Epoch 34/1000
2023-09-28 07:38:09.738 
Epoch 34/1000 
	 loss: 19.0257, MinusLogProbMetric: 19.0257, val_loss: 20.5869, val_MinusLogProbMetric: 20.5869

Epoch 34: val_loss did not improve from 18.96959
196/196 - 78s - loss: 19.0257 - MinusLogProbMetric: 19.0257 - val_loss: 20.5869 - val_MinusLogProbMetric: 20.5869 - lr: 3.3333e-04 - 78s/epoch - 400ms/step
Epoch 35/1000
2023-09-28 07:39:27.649 
Epoch 35/1000 
	 loss: 19.0356, MinusLogProbMetric: 19.0356, val_loss: 19.1542, val_MinusLogProbMetric: 19.1542

Epoch 35: val_loss did not improve from 18.96959
196/196 - 78s - loss: 19.0356 - MinusLogProbMetric: 19.0356 - val_loss: 19.1542 - val_MinusLogProbMetric: 19.1542 - lr: 3.3333e-04 - 78s/epoch - 397ms/step
Epoch 36/1000
2023-09-28 07:40:45.405 
Epoch 36/1000 
	 loss: 18.9358, MinusLogProbMetric: 18.9358, val_loss: 19.6259, val_MinusLogProbMetric: 19.6259

Epoch 36: val_loss did not improve from 18.96959
196/196 - 78s - loss: 18.9358 - MinusLogProbMetric: 18.9358 - val_loss: 19.6259 - val_MinusLogProbMetric: 19.6259 - lr: 3.3333e-04 - 78s/epoch - 397ms/step
Epoch 37/1000
2023-09-28 07:42:03.440 
Epoch 37/1000 
	 loss: 18.8828, MinusLogProbMetric: 18.8828, val_loss: 18.8947, val_MinusLogProbMetric: 18.8947

Epoch 37: val_loss improved from 18.96959 to 18.89473, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_320/weights/best_weights.h5
196/196 - 79s - loss: 18.8828 - MinusLogProbMetric: 18.8828 - val_loss: 18.8947 - val_MinusLogProbMetric: 18.8947 - lr: 3.3333e-04 - 79s/epoch - 406ms/step
Epoch 38/1000
2023-09-28 07:43:23.635 
Epoch 38/1000 
	 loss: 18.8284, MinusLogProbMetric: 18.8284, val_loss: 19.6969, val_MinusLogProbMetric: 19.6969

Epoch 38: val_loss did not improve from 18.89473
196/196 - 79s - loss: 18.8284 - MinusLogProbMetric: 18.8284 - val_loss: 19.6969 - val_MinusLogProbMetric: 19.6969 - lr: 3.3333e-04 - 79s/epoch - 402ms/step
Epoch 39/1000
2023-09-28 07:44:41.315 
Epoch 39/1000 
	 loss: 18.9303, MinusLogProbMetric: 18.9303, val_loss: 18.7722, val_MinusLogProbMetric: 18.7722

Epoch 39: val_loss improved from 18.89473 to 18.77217, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_320/weights/best_weights.h5
196/196 - 79s - loss: 18.9303 - MinusLogProbMetric: 18.9303 - val_loss: 18.7722 - val_MinusLogProbMetric: 18.7722 - lr: 3.3333e-04 - 79s/epoch - 404ms/step
Epoch 40/1000
2023-09-28 07:46:01.475 
Epoch 40/1000 
	 loss: 18.7250, MinusLogProbMetric: 18.7250, val_loss: 18.6450, val_MinusLogProbMetric: 18.6450

Epoch 40: val_loss improved from 18.77217 to 18.64495, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_320/weights/best_weights.h5
196/196 - 80s - loss: 18.7250 - MinusLogProbMetric: 18.7250 - val_loss: 18.6450 - val_MinusLogProbMetric: 18.6450 - lr: 3.3333e-04 - 80s/epoch - 408ms/step
Epoch 41/1000
2023-09-28 07:47:21.519 
Epoch 41/1000 
	 loss: 18.8026, MinusLogProbMetric: 18.8026, val_loss: 19.6783, val_MinusLogProbMetric: 19.6783

Epoch 41: val_loss did not improve from 18.64495
196/196 - 79s - loss: 18.8026 - MinusLogProbMetric: 18.8026 - val_loss: 19.6783 - val_MinusLogProbMetric: 19.6783 - lr: 3.3333e-04 - 79s/epoch - 401ms/step
Epoch 42/1000
2023-09-28 07:48:40.118 
Epoch 42/1000 
	 loss: 18.6550, MinusLogProbMetric: 18.6550, val_loss: 19.9957, val_MinusLogProbMetric: 19.9957

Epoch 42: val_loss did not improve from 18.64495
196/196 - 79s - loss: 18.6550 - MinusLogProbMetric: 18.6550 - val_loss: 19.9957 - val_MinusLogProbMetric: 19.9957 - lr: 3.3333e-04 - 79s/epoch - 401ms/step
Epoch 43/1000
2023-09-28 07:49:58.413 
Epoch 43/1000 
	 loss: 18.7083, MinusLogProbMetric: 18.7083, val_loss: 19.4087, val_MinusLogProbMetric: 19.4087

Epoch 43: val_loss did not improve from 18.64495
196/196 - 78s - loss: 18.7083 - MinusLogProbMetric: 18.7083 - val_loss: 19.4087 - val_MinusLogProbMetric: 19.4087 - lr: 3.3333e-04 - 78s/epoch - 399ms/step
Epoch 44/1000
2023-09-28 07:51:16.730 
Epoch 44/1000 
	 loss: 18.6579, MinusLogProbMetric: 18.6579, val_loss: 18.7761, val_MinusLogProbMetric: 18.7761

Epoch 44: val_loss did not improve from 18.64495
196/196 - 78s - loss: 18.6579 - MinusLogProbMetric: 18.6579 - val_loss: 18.7761 - val_MinusLogProbMetric: 18.7761 - lr: 3.3333e-04 - 78s/epoch - 400ms/step
Epoch 45/1000
2023-09-28 07:52:34.862 
Epoch 45/1000 
	 loss: 18.6581, MinusLogProbMetric: 18.6581, val_loss: 18.6371, val_MinusLogProbMetric: 18.6371

Epoch 45: val_loss improved from 18.64495 to 18.63712, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_320/weights/best_weights.h5
196/196 - 79s - loss: 18.6581 - MinusLogProbMetric: 18.6581 - val_loss: 18.6371 - val_MinusLogProbMetric: 18.6371 - lr: 3.3333e-04 - 79s/epoch - 405ms/step
Epoch 46/1000
2023-09-28 07:53:54.572 
Epoch 46/1000 
	 loss: 18.6693, MinusLogProbMetric: 18.6693, val_loss: 18.6185, val_MinusLogProbMetric: 18.6185

Epoch 46: val_loss improved from 18.63712 to 18.61845, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_320/weights/best_weights.h5
196/196 - 80s - loss: 18.6693 - MinusLogProbMetric: 18.6693 - val_loss: 18.6185 - val_MinusLogProbMetric: 18.6185 - lr: 3.3333e-04 - 80s/epoch - 407ms/step
Epoch 47/1000
2023-09-28 07:55:14.350 
Epoch 47/1000 
	 loss: 18.5744, MinusLogProbMetric: 18.5744, val_loss: 18.9831, val_MinusLogProbMetric: 18.9831

Epoch 47: val_loss did not improve from 18.61845
196/196 - 78s - loss: 18.5744 - MinusLogProbMetric: 18.5744 - val_loss: 18.9831 - val_MinusLogProbMetric: 18.9831 - lr: 3.3333e-04 - 78s/epoch - 400ms/step
Epoch 48/1000
2023-09-28 07:56:32.746 
Epoch 48/1000 
	 loss: 18.4921, MinusLogProbMetric: 18.4921, val_loss: 19.0760, val_MinusLogProbMetric: 19.0760

Epoch 48: val_loss did not improve from 18.61845
196/196 - 78s - loss: 18.4921 - MinusLogProbMetric: 18.4921 - val_loss: 19.0760 - val_MinusLogProbMetric: 19.0760 - lr: 3.3333e-04 - 78s/epoch - 400ms/step
Epoch 49/1000
2023-09-28 07:57:51.658 
Epoch 49/1000 
	 loss: 18.5075, MinusLogProbMetric: 18.5075, val_loss: 19.0051, val_MinusLogProbMetric: 19.0051

Epoch 49: val_loss did not improve from 18.61845
196/196 - 79s - loss: 18.5075 - MinusLogProbMetric: 18.5075 - val_loss: 19.0051 - val_MinusLogProbMetric: 19.0051 - lr: 3.3333e-04 - 79s/epoch - 403ms/step
Epoch 50/1000
2023-09-28 07:59:10.364 
Epoch 50/1000 
	 loss: 18.4933, MinusLogProbMetric: 18.4933, val_loss: 18.5033, val_MinusLogProbMetric: 18.5033

Epoch 50: val_loss improved from 18.61845 to 18.50331, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_320/weights/best_weights.h5
196/196 - 80s - loss: 18.4933 - MinusLogProbMetric: 18.4933 - val_loss: 18.5033 - val_MinusLogProbMetric: 18.5033 - lr: 3.3333e-04 - 80s/epoch - 409ms/step
Epoch 51/1000
2023-09-28 08:00:29.225 
Epoch 51/1000 
	 loss: 18.4241, MinusLogProbMetric: 18.4241, val_loss: 18.7967, val_MinusLogProbMetric: 18.7967

Epoch 51: val_loss did not improve from 18.50331
196/196 - 77s - loss: 18.4241 - MinusLogProbMetric: 18.4241 - val_loss: 18.7967 - val_MinusLogProbMetric: 18.7967 - lr: 3.3333e-04 - 77s/epoch - 395ms/step
Epoch 52/1000
2023-09-28 08:01:47.426 
Epoch 52/1000 
	 loss: 18.4298, MinusLogProbMetric: 18.4298, val_loss: 18.5782, val_MinusLogProbMetric: 18.5782

Epoch 52: val_loss did not improve from 18.50331
196/196 - 78s - loss: 18.4298 - MinusLogProbMetric: 18.4298 - val_loss: 18.5782 - val_MinusLogProbMetric: 18.5782 - lr: 3.3333e-04 - 78s/epoch - 399ms/step
Epoch 53/1000
2023-09-28 08:03:05.044 
Epoch 53/1000 
	 loss: 18.3712, MinusLogProbMetric: 18.3712, val_loss: 18.3383, val_MinusLogProbMetric: 18.3383

Epoch 53: val_loss improved from 18.50331 to 18.33831, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_320/weights/best_weights.h5
196/196 - 79s - loss: 18.3712 - MinusLogProbMetric: 18.3712 - val_loss: 18.3383 - val_MinusLogProbMetric: 18.3383 - lr: 3.3333e-04 - 79s/epoch - 403ms/step
Epoch 54/1000
2023-09-28 08:04:24.331 
Epoch 54/1000 
	 loss: 18.3562, MinusLogProbMetric: 18.3562, val_loss: 18.4630, val_MinusLogProbMetric: 18.4630

Epoch 54: val_loss did not improve from 18.33831
196/196 - 78s - loss: 18.3562 - MinusLogProbMetric: 18.3562 - val_loss: 18.4630 - val_MinusLogProbMetric: 18.4630 - lr: 3.3333e-04 - 78s/epoch - 397ms/step
Epoch 55/1000
2023-09-28 08:05:42.545 
Epoch 55/1000 
	 loss: 18.3693, MinusLogProbMetric: 18.3693, val_loss: 18.5222, val_MinusLogProbMetric: 18.5222

Epoch 55: val_loss did not improve from 18.33831
196/196 - 78s - loss: 18.3693 - MinusLogProbMetric: 18.3693 - val_loss: 18.5222 - val_MinusLogProbMetric: 18.5222 - lr: 3.3333e-04 - 78s/epoch - 399ms/step
Epoch 56/1000
2023-09-28 08:07:00.657 
Epoch 56/1000 
	 loss: 18.3620, MinusLogProbMetric: 18.3620, val_loss: 19.0793, val_MinusLogProbMetric: 19.0793

Epoch 56: val_loss did not improve from 18.33831
196/196 - 78s - loss: 18.3620 - MinusLogProbMetric: 18.3620 - val_loss: 19.0793 - val_MinusLogProbMetric: 19.0793 - lr: 3.3333e-04 - 78s/epoch - 399ms/step
Epoch 57/1000
2023-09-28 08:08:18.680 
Epoch 57/1000 
	 loss: 18.2470, MinusLogProbMetric: 18.2470, val_loss: 18.8752, val_MinusLogProbMetric: 18.8752

Epoch 57: val_loss did not improve from 18.33831
196/196 - 78s - loss: 18.2470 - MinusLogProbMetric: 18.2470 - val_loss: 18.8752 - val_MinusLogProbMetric: 18.8752 - lr: 3.3333e-04 - 78s/epoch - 398ms/step
Epoch 58/1000
2023-09-28 08:09:36.633 
Epoch 58/1000 
	 loss: 18.3464, MinusLogProbMetric: 18.3464, val_loss: 18.3375, val_MinusLogProbMetric: 18.3375

Epoch 58: val_loss improved from 18.33831 to 18.33748, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_320/weights/best_weights.h5
196/196 - 79s - loss: 18.3464 - MinusLogProbMetric: 18.3464 - val_loss: 18.3375 - val_MinusLogProbMetric: 18.3375 - lr: 3.3333e-04 - 79s/epoch - 404ms/step
Epoch 59/1000
2023-09-28 08:10:55.672 
Epoch 59/1000 
	 loss: 18.3067, MinusLogProbMetric: 18.3067, val_loss: 18.7935, val_MinusLogProbMetric: 18.7935

Epoch 59: val_loss did not improve from 18.33748
196/196 - 78s - loss: 18.3067 - MinusLogProbMetric: 18.3067 - val_loss: 18.7935 - val_MinusLogProbMetric: 18.7935 - lr: 3.3333e-04 - 78s/epoch - 397ms/step
Epoch 60/1000
2023-09-28 08:12:13.798 
Epoch 60/1000 
	 loss: 18.2537, MinusLogProbMetric: 18.2537, val_loss: 18.6123, val_MinusLogProbMetric: 18.6123

Epoch 60: val_loss did not improve from 18.33748
196/196 - 78s - loss: 18.2537 - MinusLogProbMetric: 18.2537 - val_loss: 18.6123 - val_MinusLogProbMetric: 18.6123 - lr: 3.3333e-04 - 78s/epoch - 399ms/step
Epoch 61/1000
2023-09-28 08:13:31.962 
Epoch 61/1000 
	 loss: 18.2582, MinusLogProbMetric: 18.2582, val_loss: 18.4687, val_MinusLogProbMetric: 18.4687

Epoch 61: val_loss did not improve from 18.33748
196/196 - 78s - loss: 18.2582 - MinusLogProbMetric: 18.2582 - val_loss: 18.4687 - val_MinusLogProbMetric: 18.4687 - lr: 3.3333e-04 - 78s/epoch - 399ms/step
Epoch 62/1000
2023-09-28 08:14:49.915 
Epoch 62/1000 
	 loss: 18.2726, MinusLogProbMetric: 18.2726, val_loss: 18.3172, val_MinusLogProbMetric: 18.3172

Epoch 62: val_loss improved from 18.33748 to 18.31722, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_320/weights/best_weights.h5
196/196 - 79s - loss: 18.2726 - MinusLogProbMetric: 18.2726 - val_loss: 18.3172 - val_MinusLogProbMetric: 18.3172 - lr: 3.3333e-04 - 79s/epoch - 405ms/step
Epoch 63/1000
2023-09-28 08:16:09.796 
Epoch 63/1000 
	 loss: 18.1606, MinusLogProbMetric: 18.1606, val_loss: 18.0943, val_MinusLogProbMetric: 18.0943

Epoch 63: val_loss improved from 18.31722 to 18.09428, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_320/weights/best_weights.h5
196/196 - 80s - loss: 18.1606 - MinusLogProbMetric: 18.1606 - val_loss: 18.0943 - val_MinusLogProbMetric: 18.0943 - lr: 3.3333e-04 - 80s/epoch - 408ms/step
Epoch 64/1000
2023-09-28 08:17:28.998 
Epoch 64/1000 
	 loss: 18.1571, MinusLogProbMetric: 18.1571, val_loss: 18.5004, val_MinusLogProbMetric: 18.5004

Epoch 64: val_loss did not improve from 18.09428
196/196 - 78s - loss: 18.1571 - MinusLogProbMetric: 18.1571 - val_loss: 18.5004 - val_MinusLogProbMetric: 18.5004 - lr: 3.3333e-04 - 78s/epoch - 397ms/step
Epoch 65/1000
2023-09-28 08:18:47.815 
Epoch 65/1000 
	 loss: 18.2008, MinusLogProbMetric: 18.2008, val_loss: 18.1737, val_MinusLogProbMetric: 18.1737

Epoch 65: val_loss did not improve from 18.09428
196/196 - 79s - loss: 18.2008 - MinusLogProbMetric: 18.2008 - val_loss: 18.1737 - val_MinusLogProbMetric: 18.1737 - lr: 3.3333e-04 - 79s/epoch - 402ms/step
Epoch 66/1000
2023-09-28 08:20:05.573 
Epoch 66/1000 
	 loss: 18.1443, MinusLogProbMetric: 18.1443, val_loss: 18.1060, val_MinusLogProbMetric: 18.1060

Epoch 66: val_loss did not improve from 18.09428
196/196 - 78s - loss: 18.1443 - MinusLogProbMetric: 18.1443 - val_loss: 18.1060 - val_MinusLogProbMetric: 18.1060 - lr: 3.3333e-04 - 78s/epoch - 397ms/step
Epoch 67/1000
2023-09-28 08:21:23.669 
Epoch 67/1000 
	 loss: 18.2193, MinusLogProbMetric: 18.2193, val_loss: 19.4652, val_MinusLogProbMetric: 19.4652

Epoch 67: val_loss did not improve from 18.09428
196/196 - 78s - loss: 18.2193 - MinusLogProbMetric: 18.2193 - val_loss: 19.4652 - val_MinusLogProbMetric: 19.4652 - lr: 3.3333e-04 - 78s/epoch - 398ms/step
Epoch 68/1000
2023-09-28 08:22:41.938 
Epoch 68/1000 
	 loss: 18.4446, MinusLogProbMetric: 18.4446, val_loss: 18.1155, val_MinusLogProbMetric: 18.1155

Epoch 68: val_loss did not improve from 18.09428
196/196 - 78s - loss: 18.4446 - MinusLogProbMetric: 18.4446 - val_loss: 18.1155 - val_MinusLogProbMetric: 18.1155 - lr: 3.3333e-04 - 78s/epoch - 399ms/step
Epoch 69/1000
2023-09-28 08:24:00.162 
Epoch 69/1000 
	 loss: 18.0538, MinusLogProbMetric: 18.0538, val_loss: 18.5392, val_MinusLogProbMetric: 18.5392

Epoch 69: val_loss did not improve from 18.09428
196/196 - 78s - loss: 18.0538 - MinusLogProbMetric: 18.0538 - val_loss: 18.5392 - val_MinusLogProbMetric: 18.5392 - lr: 3.3333e-04 - 78s/epoch - 399ms/step
Epoch 70/1000
2023-09-28 08:25:19.173 
Epoch 70/1000 
	 loss: 18.1712, MinusLogProbMetric: 18.1712, val_loss: 18.4750, val_MinusLogProbMetric: 18.4750

Epoch 70: val_loss did not improve from 18.09428
196/196 - 79s - loss: 18.1712 - MinusLogProbMetric: 18.1712 - val_loss: 18.4750 - val_MinusLogProbMetric: 18.4750 - lr: 3.3333e-04 - 79s/epoch - 403ms/step
Epoch 71/1000
2023-09-28 08:26:36.985 
Epoch 71/1000 
	 loss: 18.1007, MinusLogProbMetric: 18.1007, val_loss: 18.4828, val_MinusLogProbMetric: 18.4828

Epoch 71: val_loss did not improve from 18.09428
196/196 - 78s - loss: 18.1007 - MinusLogProbMetric: 18.1007 - val_loss: 18.4828 - val_MinusLogProbMetric: 18.4828 - lr: 3.3333e-04 - 78s/epoch - 397ms/step
Epoch 72/1000
2023-09-28 08:27:55.193 
Epoch 72/1000 
	 loss: 18.0074, MinusLogProbMetric: 18.0074, val_loss: 18.2211, val_MinusLogProbMetric: 18.2211

Epoch 72: val_loss did not improve from 18.09428
196/196 - 78s - loss: 18.0074 - MinusLogProbMetric: 18.0074 - val_loss: 18.2211 - val_MinusLogProbMetric: 18.2211 - lr: 3.3333e-04 - 78s/epoch - 399ms/step
Epoch 73/1000
2023-09-28 08:29:13.571 
Epoch 73/1000 
	 loss: 18.0693, MinusLogProbMetric: 18.0693, val_loss: 19.0875, val_MinusLogProbMetric: 19.0875

Epoch 73: val_loss did not improve from 18.09428
196/196 - 78s - loss: 18.0693 - MinusLogProbMetric: 18.0693 - val_loss: 19.0875 - val_MinusLogProbMetric: 19.0875 - lr: 3.3333e-04 - 78s/epoch - 400ms/step
Epoch 74/1000
2023-09-28 08:30:31.527 
Epoch 74/1000 
	 loss: 18.0368, MinusLogProbMetric: 18.0368, val_loss: 18.3642, val_MinusLogProbMetric: 18.3642

Epoch 74: val_loss did not improve from 18.09428
196/196 - 78s - loss: 18.0368 - MinusLogProbMetric: 18.0368 - val_loss: 18.3642 - val_MinusLogProbMetric: 18.3642 - lr: 3.3333e-04 - 78s/epoch - 398ms/step
Epoch 75/1000
2023-09-28 08:31:50.298 
Epoch 75/1000 
	 loss: 17.9965, MinusLogProbMetric: 17.9965, val_loss: 18.3832, val_MinusLogProbMetric: 18.3832

Epoch 75: val_loss did not improve from 18.09428
196/196 - 79s - loss: 17.9965 - MinusLogProbMetric: 17.9965 - val_loss: 18.3832 - val_MinusLogProbMetric: 18.3832 - lr: 3.3333e-04 - 79s/epoch - 402ms/step
Epoch 76/1000
2023-09-28 08:33:07.882 
Epoch 76/1000 
	 loss: 18.0735, MinusLogProbMetric: 18.0735, val_loss: 18.1899, val_MinusLogProbMetric: 18.1899

Epoch 76: val_loss did not improve from 18.09428
196/196 - 78s - loss: 18.0735 - MinusLogProbMetric: 18.0735 - val_loss: 18.1899 - val_MinusLogProbMetric: 18.1899 - lr: 3.3333e-04 - 78s/epoch - 396ms/step
Epoch 77/1000
2023-09-28 08:34:26.500 
Epoch 77/1000 
	 loss: 17.9625, MinusLogProbMetric: 17.9625, val_loss: 18.1060, val_MinusLogProbMetric: 18.1060

Epoch 77: val_loss did not improve from 18.09428
196/196 - 79s - loss: 17.9625 - MinusLogProbMetric: 17.9625 - val_loss: 18.1060 - val_MinusLogProbMetric: 18.1060 - lr: 3.3333e-04 - 79s/epoch - 401ms/step
Epoch 78/1000
2023-09-28 08:35:44.102 
Epoch 78/1000 
	 loss: 18.0429, MinusLogProbMetric: 18.0429, val_loss: 18.1802, val_MinusLogProbMetric: 18.1802

Epoch 78: val_loss did not improve from 18.09428
196/196 - 78s - loss: 18.0429 - MinusLogProbMetric: 18.0429 - val_loss: 18.1802 - val_MinusLogProbMetric: 18.1802 - lr: 3.3333e-04 - 78s/epoch - 396ms/step
Epoch 79/1000
2023-09-28 08:37:02.332 
Epoch 79/1000 
	 loss: 18.0505, MinusLogProbMetric: 18.0505, val_loss: 18.5786, val_MinusLogProbMetric: 18.5786

Epoch 79: val_loss did not improve from 18.09428
196/196 - 78s - loss: 18.0505 - MinusLogProbMetric: 18.0505 - val_loss: 18.5786 - val_MinusLogProbMetric: 18.5786 - lr: 3.3333e-04 - 78s/epoch - 399ms/step
Epoch 80/1000
2023-09-28 08:38:20.607 
Epoch 80/1000 
	 loss: 18.0239, MinusLogProbMetric: 18.0239, val_loss: 18.4270, val_MinusLogProbMetric: 18.4270

Epoch 80: val_loss did not improve from 18.09428
196/196 - 78s - loss: 18.0239 - MinusLogProbMetric: 18.0239 - val_loss: 18.4270 - val_MinusLogProbMetric: 18.4270 - lr: 3.3333e-04 - 78s/epoch - 399ms/step
Epoch 81/1000
2023-09-28 08:39:38.433 
Epoch 81/1000 
	 loss: 17.9556, MinusLogProbMetric: 17.9556, val_loss: 18.3875, val_MinusLogProbMetric: 18.3875

Epoch 81: val_loss did not improve from 18.09428
196/196 - 78s - loss: 17.9556 - MinusLogProbMetric: 17.9556 - val_loss: 18.3875 - val_MinusLogProbMetric: 18.3875 - lr: 3.3333e-04 - 78s/epoch - 397ms/step
Epoch 82/1000
2023-09-28 08:40:56.504 
Epoch 82/1000 
	 loss: 17.9104, MinusLogProbMetric: 17.9104, val_loss: 17.9706, val_MinusLogProbMetric: 17.9706

Epoch 82: val_loss improved from 18.09428 to 17.97064, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_320/weights/best_weights.h5
196/196 - 80s - loss: 17.9104 - MinusLogProbMetric: 17.9104 - val_loss: 17.9706 - val_MinusLogProbMetric: 17.9706 - lr: 3.3333e-04 - 80s/epoch - 406ms/step
Epoch 83/1000
2023-09-28 08:42:16.173 
Epoch 83/1000 
	 loss: 17.8801, MinusLogProbMetric: 17.8801, val_loss: 18.2140, val_MinusLogProbMetric: 18.2140

Epoch 83: val_loss did not improve from 17.97064
196/196 - 78s - loss: 17.8801 - MinusLogProbMetric: 17.8801 - val_loss: 18.2140 - val_MinusLogProbMetric: 18.2140 - lr: 3.3333e-04 - 78s/epoch - 399ms/step
Epoch 84/1000
2023-09-28 08:43:33.908 
Epoch 84/1000 
	 loss: 17.8475, MinusLogProbMetric: 17.8475, val_loss: 18.7110, val_MinusLogProbMetric: 18.7110

Epoch 84: val_loss did not improve from 17.97064
196/196 - 78s - loss: 17.8475 - MinusLogProbMetric: 17.8475 - val_loss: 18.7110 - val_MinusLogProbMetric: 18.7110 - lr: 3.3333e-04 - 78s/epoch - 397ms/step
Epoch 85/1000
2023-09-28 08:44:48.638 
Epoch 85/1000 
	 loss: 17.9851, MinusLogProbMetric: 17.9851, val_loss: 18.2431, val_MinusLogProbMetric: 18.2431

Epoch 85: val_loss did not improve from 17.97064
196/196 - 75s - loss: 17.9851 - MinusLogProbMetric: 17.9851 - val_loss: 18.2431 - val_MinusLogProbMetric: 18.2431 - lr: 3.3333e-04 - 75s/epoch - 381ms/step
Epoch 86/1000
2023-09-28 08:46:02.580 
Epoch 86/1000 
	 loss: 17.9350, MinusLogProbMetric: 17.9350, val_loss: 18.0498, val_MinusLogProbMetric: 18.0498

Epoch 86: val_loss did not improve from 17.97064
196/196 - 74s - loss: 17.9350 - MinusLogProbMetric: 17.9350 - val_loss: 18.0498 - val_MinusLogProbMetric: 18.0498 - lr: 3.3333e-04 - 74s/epoch - 377ms/step
Epoch 87/1000
2023-09-28 08:47:16.001 
Epoch 87/1000 
	 loss: 17.9739, MinusLogProbMetric: 17.9739, val_loss: 18.5167, val_MinusLogProbMetric: 18.5167

Epoch 87: val_loss did not improve from 17.97064
196/196 - 73s - loss: 17.9739 - MinusLogProbMetric: 17.9739 - val_loss: 18.5167 - val_MinusLogProbMetric: 18.5167 - lr: 3.3333e-04 - 73s/epoch - 375ms/step
Epoch 88/1000
2023-09-28 08:48:30.739 
Epoch 88/1000 
	 loss: 17.8646, MinusLogProbMetric: 17.8646, val_loss: 19.8956, val_MinusLogProbMetric: 19.8956

Epoch 88: val_loss did not improve from 17.97064
196/196 - 75s - loss: 17.8646 - MinusLogProbMetric: 17.8646 - val_loss: 19.8956 - val_MinusLogProbMetric: 19.8956 - lr: 3.3333e-04 - 75s/epoch - 381ms/step
Epoch 89/1000
2023-09-28 08:49:37.758 
Epoch 89/1000 
	 loss: 17.9622, MinusLogProbMetric: 17.9622, val_loss: 18.2721, val_MinusLogProbMetric: 18.2721

Epoch 89: val_loss did not improve from 17.97064
196/196 - 67s - loss: 17.9622 - MinusLogProbMetric: 17.9622 - val_loss: 18.2721 - val_MinusLogProbMetric: 18.2721 - lr: 3.3333e-04 - 67s/epoch - 342ms/step
Epoch 90/1000
2023-09-28 08:50:56.613 
Epoch 90/1000 
	 loss: 17.8190, MinusLogProbMetric: 17.8190, val_loss: 18.2325, val_MinusLogProbMetric: 18.2325

Epoch 90: val_loss did not improve from 17.97064
196/196 - 79s - loss: 17.8190 - MinusLogProbMetric: 17.8190 - val_loss: 18.2325 - val_MinusLogProbMetric: 18.2325 - lr: 3.3333e-04 - 79s/epoch - 402ms/step
Epoch 91/1000
2023-09-28 08:52:15.197 
Epoch 91/1000 
	 loss: 17.7984, MinusLogProbMetric: 17.7984, val_loss: 19.3979, val_MinusLogProbMetric: 19.3979

Epoch 91: val_loss did not improve from 17.97064
196/196 - 79s - loss: 17.7984 - MinusLogProbMetric: 17.7984 - val_loss: 19.3979 - val_MinusLogProbMetric: 19.3979 - lr: 3.3333e-04 - 79s/epoch - 401ms/step
Epoch 92/1000
2023-09-28 08:53:32.054 
Epoch 92/1000 
	 loss: 17.8065, MinusLogProbMetric: 17.8065, val_loss: 18.2034, val_MinusLogProbMetric: 18.2034

Epoch 92: val_loss did not improve from 17.97064
196/196 - 77s - loss: 17.8065 - MinusLogProbMetric: 17.8065 - val_loss: 18.2034 - val_MinusLogProbMetric: 18.2034 - lr: 3.3333e-04 - 77s/epoch - 392ms/step
Epoch 93/1000
2023-09-28 08:54:48.764 
Epoch 93/1000 
	 loss: 17.8944, MinusLogProbMetric: 17.8944, val_loss: 17.9569, val_MinusLogProbMetric: 17.9569

Epoch 93: val_loss improved from 17.97064 to 17.95686, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_320/weights/best_weights.h5
196/196 - 78s - loss: 17.8944 - MinusLogProbMetric: 17.8944 - val_loss: 17.9569 - val_MinusLogProbMetric: 17.9569 - lr: 3.3333e-04 - 78s/epoch - 398ms/step
Epoch 94/1000
2023-09-28 08:56:06.832 
Epoch 94/1000 
	 loss: 17.8965, MinusLogProbMetric: 17.8965, val_loss: 18.3870, val_MinusLogProbMetric: 18.3870

Epoch 94: val_loss did not improve from 17.95686
196/196 - 77s - loss: 17.8965 - MinusLogProbMetric: 17.8965 - val_loss: 18.3870 - val_MinusLogProbMetric: 18.3870 - lr: 3.3333e-04 - 77s/epoch - 392ms/step
Epoch 95/1000
2023-09-28 08:57:24.654 
Epoch 95/1000 
	 loss: 17.8488, MinusLogProbMetric: 17.8488, val_loss: 17.9305, val_MinusLogProbMetric: 17.9305

Epoch 95: val_loss improved from 17.95686 to 17.93051, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_320/weights/best_weights.h5
196/196 - 79s - loss: 17.8488 - MinusLogProbMetric: 17.8488 - val_loss: 17.9305 - val_MinusLogProbMetric: 17.9305 - lr: 3.3333e-04 - 79s/epoch - 404ms/step
Epoch 96/1000
2023-09-28 08:58:43.046 
Epoch 96/1000 
	 loss: 17.7560, MinusLogProbMetric: 17.7560, val_loss: 17.8082, val_MinusLogProbMetric: 17.8082

Epoch 96: val_loss improved from 17.93051 to 17.80820, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_320/weights/best_weights.h5
196/196 - 78s - loss: 17.7560 - MinusLogProbMetric: 17.7560 - val_loss: 17.8082 - val_MinusLogProbMetric: 17.8082 - lr: 3.3333e-04 - 78s/epoch - 399ms/step
Epoch 97/1000
2023-09-28 09:00:01.120 
Epoch 97/1000 
	 loss: 17.8283, MinusLogProbMetric: 17.8283, val_loss: 18.4443, val_MinusLogProbMetric: 18.4443

Epoch 97: val_loss did not improve from 17.80820
196/196 - 77s - loss: 17.8283 - MinusLogProbMetric: 17.8283 - val_loss: 18.4443 - val_MinusLogProbMetric: 18.4443 - lr: 3.3333e-04 - 77s/epoch - 392ms/step
Epoch 98/1000
2023-09-28 09:01:18.135 
Epoch 98/1000 
	 loss: 17.8177, MinusLogProbMetric: 17.8177, val_loss: 18.7545, val_MinusLogProbMetric: 18.7545

Epoch 98: val_loss did not improve from 17.80820
196/196 - 77s - loss: 17.8177 - MinusLogProbMetric: 17.8177 - val_loss: 18.7545 - val_MinusLogProbMetric: 18.7545 - lr: 3.3333e-04 - 77s/epoch - 393ms/step
Epoch 99/1000
2023-09-28 09:02:35.282 
Epoch 99/1000 
	 loss: 17.8908, MinusLogProbMetric: 17.8908, val_loss: 18.2838, val_MinusLogProbMetric: 18.2838

Epoch 99: val_loss did not improve from 17.80820
196/196 - 77s - loss: 17.8908 - MinusLogProbMetric: 17.8908 - val_loss: 18.2838 - val_MinusLogProbMetric: 18.2838 - lr: 3.3333e-04 - 77s/epoch - 394ms/step
Epoch 100/1000
2023-09-28 09:03:52.493 
Epoch 100/1000 
	 loss: 17.7539, MinusLogProbMetric: 17.7539, val_loss: 17.8266, val_MinusLogProbMetric: 17.8266

Epoch 100: val_loss did not improve from 17.80820
196/196 - 77s - loss: 17.7539 - MinusLogProbMetric: 17.7539 - val_loss: 17.8266 - val_MinusLogProbMetric: 17.8266 - lr: 3.3333e-04 - 77s/epoch - 394ms/step
Epoch 101/1000
2023-09-28 09:05:10.121 
Epoch 101/1000 
	 loss: 17.7538, MinusLogProbMetric: 17.7538, val_loss: 17.7813, val_MinusLogProbMetric: 17.7813

Epoch 101: val_loss improved from 17.80820 to 17.78127, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_320/weights/best_weights.h5
196/196 - 79s - loss: 17.7538 - MinusLogProbMetric: 17.7538 - val_loss: 17.7813 - val_MinusLogProbMetric: 17.7813 - lr: 3.3333e-04 - 79s/epoch - 404ms/step
Epoch 102/1000
2023-09-28 09:06:28.712 
Epoch 102/1000 
	 loss: 17.7535, MinusLogProbMetric: 17.7535, val_loss: 18.2539, val_MinusLogProbMetric: 18.2539

Epoch 102: val_loss did not improve from 17.78127
196/196 - 77s - loss: 17.7535 - MinusLogProbMetric: 17.7535 - val_loss: 18.2539 - val_MinusLogProbMetric: 18.2539 - lr: 3.3333e-04 - 77s/epoch - 393ms/step
Epoch 103/1000
2023-09-28 09:07:45.646 
Epoch 103/1000 
	 loss: 17.6933, MinusLogProbMetric: 17.6933, val_loss: 18.2060, val_MinusLogProbMetric: 18.2060

Epoch 103: val_loss did not improve from 17.78127
196/196 - 77s - loss: 17.6933 - MinusLogProbMetric: 17.6933 - val_loss: 18.2060 - val_MinusLogProbMetric: 18.2060 - lr: 3.3333e-04 - 77s/epoch - 392ms/step
Epoch 104/1000
2023-09-28 09:09:02.620 
Epoch 104/1000 
	 loss: 17.7159, MinusLogProbMetric: 17.7159, val_loss: 18.7015, val_MinusLogProbMetric: 18.7015

Epoch 104: val_loss did not improve from 17.78127
196/196 - 77s - loss: 17.7159 - MinusLogProbMetric: 17.7159 - val_loss: 18.7015 - val_MinusLogProbMetric: 18.7015 - lr: 3.3333e-04 - 77s/epoch - 393ms/step
Epoch 105/1000
2023-09-28 09:10:19.440 
Epoch 105/1000 
	 loss: 17.6709, MinusLogProbMetric: 17.6709, val_loss: 18.0983, val_MinusLogProbMetric: 18.0983

Epoch 105: val_loss did not improve from 17.78127
196/196 - 77s - loss: 17.6709 - MinusLogProbMetric: 17.6709 - val_loss: 18.0983 - val_MinusLogProbMetric: 18.0983 - lr: 3.3333e-04 - 77s/epoch - 392ms/step
Epoch 106/1000
2023-09-28 09:11:35.516 
Epoch 106/1000 
	 loss: 17.6963, MinusLogProbMetric: 17.6963, val_loss: 18.4476, val_MinusLogProbMetric: 18.4476

Epoch 106: val_loss did not improve from 17.78127
196/196 - 76s - loss: 17.6963 - MinusLogProbMetric: 17.6963 - val_loss: 18.4476 - val_MinusLogProbMetric: 18.4476 - lr: 3.3333e-04 - 76s/epoch - 388ms/step
Epoch 107/1000
2023-09-28 09:12:52.365 
Epoch 107/1000 
	 loss: 17.7495, MinusLogProbMetric: 17.7495, val_loss: 17.9582, val_MinusLogProbMetric: 17.9582

Epoch 107: val_loss did not improve from 17.78127
196/196 - 77s - loss: 17.7495 - MinusLogProbMetric: 17.7495 - val_loss: 17.9582 - val_MinusLogProbMetric: 17.9582 - lr: 3.3333e-04 - 77s/epoch - 392ms/step
Epoch 108/1000
2023-09-28 09:14:08.865 
Epoch 108/1000 
	 loss: 17.7190, MinusLogProbMetric: 17.7190, val_loss: 18.2082, val_MinusLogProbMetric: 18.2082

Epoch 108: val_loss did not improve from 17.78127
196/196 - 76s - loss: 17.7190 - MinusLogProbMetric: 17.7190 - val_loss: 18.2082 - val_MinusLogProbMetric: 18.2082 - lr: 3.3333e-04 - 76s/epoch - 390ms/step
Epoch 109/1000
2023-09-28 09:15:25.628 
Epoch 109/1000 
	 loss: 17.7343, MinusLogProbMetric: 17.7343, val_loss: 18.2336, val_MinusLogProbMetric: 18.2336

Epoch 109: val_loss did not improve from 17.78127
196/196 - 77s - loss: 17.7343 - MinusLogProbMetric: 17.7343 - val_loss: 18.2336 - val_MinusLogProbMetric: 18.2336 - lr: 3.3333e-04 - 77s/epoch - 392ms/step
Epoch 110/1000
2023-09-28 09:16:42.515 
Epoch 110/1000 
	 loss: 17.6663, MinusLogProbMetric: 17.6663, val_loss: 18.3986, val_MinusLogProbMetric: 18.3986

Epoch 110: val_loss did not improve from 17.78127
196/196 - 77s - loss: 17.6663 - MinusLogProbMetric: 17.6663 - val_loss: 18.3986 - val_MinusLogProbMetric: 18.3986 - lr: 3.3333e-04 - 77s/epoch - 392ms/step
Epoch 111/1000
2023-09-28 09:17:59.945 
Epoch 111/1000 
	 loss: 17.6626, MinusLogProbMetric: 17.6626, val_loss: 18.1933, val_MinusLogProbMetric: 18.1933

Epoch 111: val_loss did not improve from 17.78127
196/196 - 77s - loss: 17.6626 - MinusLogProbMetric: 17.6626 - val_loss: 18.1933 - val_MinusLogProbMetric: 18.1933 - lr: 3.3333e-04 - 77s/epoch - 395ms/step
Epoch 112/1000
2023-09-28 09:19:17.123 
Epoch 112/1000 
	 loss: 17.6479, MinusLogProbMetric: 17.6479, val_loss: 17.7778, val_MinusLogProbMetric: 17.7778

Epoch 112: val_loss improved from 17.78127 to 17.77777, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_320/weights/best_weights.h5
196/196 - 79s - loss: 17.6479 - MinusLogProbMetric: 17.6479 - val_loss: 17.7778 - val_MinusLogProbMetric: 17.7778 - lr: 3.3333e-04 - 79s/epoch - 401ms/step
Epoch 113/1000
2023-09-28 09:20:35.204 
Epoch 113/1000 
	 loss: 17.5669, MinusLogProbMetric: 17.5669, val_loss: 18.7784, val_MinusLogProbMetric: 18.7784

Epoch 113: val_loss did not improve from 17.77777
196/196 - 77s - loss: 17.5669 - MinusLogProbMetric: 17.5669 - val_loss: 18.7784 - val_MinusLogProbMetric: 18.7784 - lr: 3.3333e-04 - 77s/epoch - 391ms/step
Epoch 114/1000
2023-09-28 09:21:53.100 
Epoch 114/1000 
	 loss: 17.6269, MinusLogProbMetric: 17.6269, val_loss: 18.0169, val_MinusLogProbMetric: 18.0169

Epoch 114: val_loss did not improve from 17.77777
196/196 - 78s - loss: 17.6269 - MinusLogProbMetric: 17.6269 - val_loss: 18.0169 - val_MinusLogProbMetric: 18.0169 - lr: 3.3333e-04 - 78s/epoch - 397ms/step
Epoch 115/1000
2023-09-28 09:23:09.777 
Epoch 115/1000 
	 loss: 17.6396, MinusLogProbMetric: 17.6396, val_loss: 18.3670, val_MinusLogProbMetric: 18.3670

Epoch 115: val_loss did not improve from 17.77777
196/196 - 77s - loss: 17.6396 - MinusLogProbMetric: 17.6396 - val_loss: 18.3670 - val_MinusLogProbMetric: 18.3670 - lr: 3.3333e-04 - 77s/epoch - 391ms/step
Epoch 116/1000
2023-09-28 09:24:27.718 
Epoch 116/1000 
	 loss: 17.6112, MinusLogProbMetric: 17.6112, val_loss: 18.0138, val_MinusLogProbMetric: 18.0138

Epoch 116: val_loss did not improve from 17.77777
196/196 - 78s - loss: 17.6112 - MinusLogProbMetric: 17.6112 - val_loss: 18.0138 - val_MinusLogProbMetric: 18.0138 - lr: 3.3333e-04 - 78s/epoch - 398ms/step
Epoch 117/1000
2023-09-28 09:25:44.866 
Epoch 117/1000 
	 loss: 17.6184, MinusLogProbMetric: 17.6184, val_loss: 18.3520, val_MinusLogProbMetric: 18.3520

Epoch 117: val_loss did not improve from 17.77777
196/196 - 77s - loss: 17.6184 - MinusLogProbMetric: 17.6184 - val_loss: 18.3520 - val_MinusLogProbMetric: 18.3520 - lr: 3.3333e-04 - 77s/epoch - 394ms/step
Epoch 118/1000
2023-09-28 09:27:02.263 
Epoch 118/1000 
	 loss: 17.5859, MinusLogProbMetric: 17.5859, val_loss: 17.9958, val_MinusLogProbMetric: 17.9958

Epoch 118: val_loss did not improve from 17.77777
196/196 - 77s - loss: 17.5859 - MinusLogProbMetric: 17.5859 - val_loss: 17.9958 - val_MinusLogProbMetric: 17.9958 - lr: 3.3333e-04 - 77s/epoch - 395ms/step
Epoch 119/1000
2023-09-28 09:28:20.472 
Epoch 119/1000 
	 loss: 17.5608, MinusLogProbMetric: 17.5608, val_loss: 17.6354, val_MinusLogProbMetric: 17.6354

Epoch 119: val_loss improved from 17.77777 to 17.63542, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_320/weights/best_weights.h5
196/196 - 79s - loss: 17.5608 - MinusLogProbMetric: 17.5608 - val_loss: 17.6354 - val_MinusLogProbMetric: 17.6354 - lr: 3.3333e-04 - 79s/epoch - 404ms/step
Epoch 120/1000
2023-09-28 09:29:38.921 
Epoch 120/1000 
	 loss: 17.6246, MinusLogProbMetric: 17.6246, val_loss: 17.8017, val_MinusLogProbMetric: 17.8017

Epoch 120: val_loss did not improve from 17.63542
196/196 - 77s - loss: 17.6246 - MinusLogProbMetric: 17.6246 - val_loss: 17.8017 - val_MinusLogProbMetric: 17.8017 - lr: 3.3333e-04 - 77s/epoch - 393ms/step
Epoch 121/1000
2023-09-28 09:30:55.967 
Epoch 121/1000 
	 loss: 17.5299, MinusLogProbMetric: 17.5299, val_loss: 18.4601, val_MinusLogProbMetric: 18.4601

Epoch 121: val_loss did not improve from 17.63542
196/196 - 77s - loss: 17.5299 - MinusLogProbMetric: 17.5299 - val_loss: 18.4601 - val_MinusLogProbMetric: 18.4601 - lr: 3.3333e-04 - 77s/epoch - 393ms/step
Epoch 122/1000
2023-09-28 09:32:15.929 
Epoch 122/1000 
	 loss: 17.5511, MinusLogProbMetric: 17.5511, val_loss: 17.7572, val_MinusLogProbMetric: 17.7572

Epoch 122: val_loss did not improve from 17.63542
196/196 - 80s - loss: 17.5511 - MinusLogProbMetric: 17.5511 - val_loss: 17.7572 - val_MinusLogProbMetric: 17.7572 - lr: 3.3333e-04 - 80s/epoch - 408ms/step
Epoch 123/1000
2023-09-28 09:33:36.230 
Epoch 123/1000 
	 loss: 17.5310, MinusLogProbMetric: 17.5310, val_loss: 17.9250, val_MinusLogProbMetric: 17.9250

Epoch 123: val_loss did not improve from 17.63542
196/196 - 80s - loss: 17.5310 - MinusLogProbMetric: 17.5310 - val_loss: 17.9250 - val_MinusLogProbMetric: 17.9250 - lr: 3.3333e-04 - 80s/epoch - 410ms/step
Epoch 124/1000
2023-09-28 09:34:58.364 
Epoch 124/1000 
	 loss: 17.5352, MinusLogProbMetric: 17.5352, val_loss: 18.2936, val_MinusLogProbMetric: 18.2936

Epoch 124: val_loss did not improve from 17.63542
196/196 - 82s - loss: 17.5352 - MinusLogProbMetric: 17.5352 - val_loss: 18.2936 - val_MinusLogProbMetric: 18.2936 - lr: 3.3333e-04 - 82s/epoch - 419ms/step
Epoch 125/1000
2023-09-28 09:36:23.659 
Epoch 125/1000 
	 loss: 17.6168, MinusLogProbMetric: 17.6168, val_loss: 18.0383, val_MinusLogProbMetric: 18.0383

Epoch 125: val_loss did not improve from 17.63542
196/196 - 85s - loss: 17.6168 - MinusLogProbMetric: 17.6168 - val_loss: 18.0383 - val_MinusLogProbMetric: 18.0383 - lr: 3.3333e-04 - 85s/epoch - 435ms/step
Epoch 126/1000
2023-09-28 09:37:48.778 
Epoch 126/1000 
	 loss: 17.5473, MinusLogProbMetric: 17.5473, val_loss: 18.0143, val_MinusLogProbMetric: 18.0143

Epoch 126: val_loss did not improve from 17.63542
196/196 - 85s - loss: 17.5473 - MinusLogProbMetric: 17.5473 - val_loss: 18.0143 - val_MinusLogProbMetric: 18.0143 - lr: 3.3333e-04 - 85s/epoch - 434ms/step
Epoch 127/1000
2023-09-28 09:39:15.213 
Epoch 127/1000 
	 loss: 17.5462, MinusLogProbMetric: 17.5462, val_loss: 17.7942, val_MinusLogProbMetric: 17.7942

Epoch 127: val_loss did not improve from 17.63542
196/196 - 86s - loss: 17.5462 - MinusLogProbMetric: 17.5462 - val_loss: 17.7942 - val_MinusLogProbMetric: 17.7942 - lr: 3.3333e-04 - 86s/epoch - 441ms/step
Epoch 128/1000
2023-09-28 09:40:40.933 
Epoch 128/1000 
	 loss: 17.5349, MinusLogProbMetric: 17.5349, val_loss: 17.7066, val_MinusLogProbMetric: 17.7066

Epoch 128: val_loss did not improve from 17.63542
196/196 - 86s - loss: 17.5349 - MinusLogProbMetric: 17.5349 - val_loss: 17.7066 - val_MinusLogProbMetric: 17.7066 - lr: 3.3333e-04 - 86s/epoch - 437ms/step
Epoch 129/1000
2023-09-28 09:42:07.131 
Epoch 129/1000 
	 loss: 17.5161, MinusLogProbMetric: 17.5161, val_loss: 17.7256, val_MinusLogProbMetric: 17.7256

Epoch 129: val_loss did not improve from 17.63542
196/196 - 86s - loss: 17.5161 - MinusLogProbMetric: 17.5161 - val_loss: 17.7256 - val_MinusLogProbMetric: 17.7256 - lr: 3.3333e-04 - 86s/epoch - 440ms/step
Epoch 130/1000
2023-09-28 09:43:33.583 
Epoch 130/1000 
	 loss: 17.4360, MinusLogProbMetric: 17.4360, val_loss: 17.6886, val_MinusLogProbMetric: 17.6886

Epoch 130: val_loss did not improve from 17.63542
196/196 - 86s - loss: 17.4360 - MinusLogProbMetric: 17.4360 - val_loss: 17.6886 - val_MinusLogProbMetric: 17.6886 - lr: 3.3333e-04 - 86s/epoch - 441ms/step
Epoch 131/1000
2023-09-28 09:44:59.783 
Epoch 131/1000 
	 loss: 17.5081, MinusLogProbMetric: 17.5081, val_loss: 17.9582, val_MinusLogProbMetric: 17.9582

Epoch 131: val_loss did not improve from 17.63542
196/196 - 86s - loss: 17.5081 - MinusLogProbMetric: 17.5081 - val_loss: 17.9582 - val_MinusLogProbMetric: 17.9582 - lr: 3.3333e-04 - 86s/epoch - 440ms/step
Epoch 132/1000
2023-09-28 09:46:26.130 
Epoch 132/1000 
	 loss: 17.4843, MinusLogProbMetric: 17.4843, val_loss: 18.2664, val_MinusLogProbMetric: 18.2664

Epoch 132: val_loss did not improve from 17.63542
196/196 - 86s - loss: 17.4843 - MinusLogProbMetric: 17.4843 - val_loss: 18.2664 - val_MinusLogProbMetric: 18.2664 - lr: 3.3333e-04 - 86s/epoch - 441ms/step
Epoch 133/1000
2023-09-28 09:47:52.400 
Epoch 133/1000 
	 loss: 17.4717, MinusLogProbMetric: 17.4717, val_loss: 18.1986, val_MinusLogProbMetric: 18.1986

Epoch 133: val_loss did not improve from 17.63542
196/196 - 86s - loss: 17.4717 - MinusLogProbMetric: 17.4717 - val_loss: 18.1986 - val_MinusLogProbMetric: 18.1986 - lr: 3.3333e-04 - 86s/epoch - 440ms/step
Epoch 134/1000
2023-09-28 09:49:18.930 
Epoch 134/1000 
	 loss: 17.5115, MinusLogProbMetric: 17.5115, val_loss: 18.3523, val_MinusLogProbMetric: 18.3523

Epoch 134: val_loss did not improve from 17.63542
196/196 - 87s - loss: 17.5115 - MinusLogProbMetric: 17.5115 - val_loss: 18.3523 - val_MinusLogProbMetric: 18.3523 - lr: 3.3333e-04 - 87s/epoch - 441ms/step
Epoch 135/1000
2023-09-28 09:50:44.114 
Epoch 135/1000 
	 loss: 17.4628, MinusLogProbMetric: 17.4628, val_loss: 17.8656, val_MinusLogProbMetric: 17.8656

Epoch 135: val_loss did not improve from 17.63542
196/196 - 85s - loss: 17.4628 - MinusLogProbMetric: 17.4628 - val_loss: 17.8656 - val_MinusLogProbMetric: 17.8656 - lr: 3.3333e-04 - 85s/epoch - 435ms/step
Epoch 136/1000
2023-09-28 09:52:11.174 
Epoch 136/1000 
	 loss: 17.4114, MinusLogProbMetric: 17.4114, val_loss: 17.7293, val_MinusLogProbMetric: 17.7293

Epoch 136: val_loss did not improve from 17.63542
196/196 - 87s - loss: 17.4114 - MinusLogProbMetric: 17.4114 - val_loss: 17.7293 - val_MinusLogProbMetric: 17.7293 - lr: 3.3333e-04 - 87s/epoch - 444ms/step
Epoch 137/1000
2023-09-28 09:53:38.010 
Epoch 137/1000 
	 loss: 17.4546, MinusLogProbMetric: 17.4546, val_loss: 17.9128, val_MinusLogProbMetric: 17.9128

Epoch 137: val_loss did not improve from 17.63542
196/196 - 87s - loss: 17.4546 - MinusLogProbMetric: 17.4546 - val_loss: 17.9128 - val_MinusLogProbMetric: 17.9128 - lr: 3.3333e-04 - 87s/epoch - 443ms/step
Epoch 138/1000
2023-09-28 09:55:05.280 
Epoch 138/1000 
	 loss: 17.5108, MinusLogProbMetric: 17.5108, val_loss: 17.6585, val_MinusLogProbMetric: 17.6585

Epoch 138: val_loss did not improve from 17.63542
196/196 - 87s - loss: 17.5108 - MinusLogProbMetric: 17.5108 - val_loss: 17.6585 - val_MinusLogProbMetric: 17.6585 - lr: 3.3333e-04 - 87s/epoch - 445ms/step
Epoch 139/1000
2023-09-28 09:56:31.431 
Epoch 139/1000 
	 loss: 17.4176, MinusLogProbMetric: 17.4176, val_loss: 17.6176, val_MinusLogProbMetric: 17.6176

Epoch 139: val_loss improved from 17.63542 to 17.61760, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_320/weights/best_weights.h5
196/196 - 87s - loss: 17.4176 - MinusLogProbMetric: 17.4176 - val_loss: 17.6176 - val_MinusLogProbMetric: 17.6176 - lr: 3.3333e-04 - 87s/epoch - 446ms/step
Epoch 140/1000
2023-09-28 09:57:59.079 
Epoch 140/1000 
	 loss: 17.4145, MinusLogProbMetric: 17.4145, val_loss: 18.0087, val_MinusLogProbMetric: 18.0087

Epoch 140: val_loss did not improve from 17.61760
196/196 - 86s - loss: 17.4145 - MinusLogProbMetric: 17.4145 - val_loss: 18.0087 - val_MinusLogProbMetric: 18.0087 - lr: 3.3333e-04 - 86s/epoch - 440ms/step
Epoch 141/1000
2023-09-28 09:59:25.236 
Epoch 141/1000 
	 loss: 17.5332, MinusLogProbMetric: 17.5332, val_loss: 17.8391, val_MinusLogProbMetric: 17.8391

Epoch 141: val_loss did not improve from 17.61760
196/196 - 86s - loss: 17.5332 - MinusLogProbMetric: 17.5332 - val_loss: 17.8391 - val_MinusLogProbMetric: 17.8391 - lr: 3.3333e-04 - 86s/epoch - 440ms/step
Epoch 142/1000
2023-09-28 10:00:51.887 
Epoch 142/1000 
	 loss: 17.4153, MinusLogProbMetric: 17.4153, val_loss: 17.9347, val_MinusLogProbMetric: 17.9347

Epoch 142: val_loss did not improve from 17.61760
196/196 - 87s - loss: 17.4153 - MinusLogProbMetric: 17.4153 - val_loss: 17.9347 - val_MinusLogProbMetric: 17.9347 - lr: 3.3333e-04 - 87s/epoch - 442ms/step
Epoch 143/1000
2023-09-28 10:02:17.609 
Epoch 143/1000 
	 loss: 17.4419, MinusLogProbMetric: 17.4419, val_loss: 18.2117, val_MinusLogProbMetric: 18.2117

Epoch 143: val_loss did not improve from 17.61760
196/196 - 86s - loss: 17.4419 - MinusLogProbMetric: 17.4419 - val_loss: 18.2117 - val_MinusLogProbMetric: 18.2117 - lr: 3.3333e-04 - 86s/epoch - 437ms/step
Epoch 144/1000
2023-09-28 10:03:43.675 
Epoch 144/1000 
	 loss: 17.4642, MinusLogProbMetric: 17.4642, val_loss: 17.7098, val_MinusLogProbMetric: 17.7098

Epoch 144: val_loss did not improve from 17.61760
196/196 - 86s - loss: 17.4642 - MinusLogProbMetric: 17.4642 - val_loss: 17.7098 - val_MinusLogProbMetric: 17.7098 - lr: 3.3333e-04 - 86s/epoch - 439ms/step
Epoch 145/1000
2023-09-28 10:05:10.330 
Epoch 145/1000 
	 loss: 17.4369, MinusLogProbMetric: 17.4369, val_loss: 18.1135, val_MinusLogProbMetric: 18.1135

Epoch 145: val_loss did not improve from 17.61760
196/196 - 87s - loss: 17.4369 - MinusLogProbMetric: 17.4369 - val_loss: 18.1135 - val_MinusLogProbMetric: 18.1135 - lr: 3.3333e-04 - 87s/epoch - 442ms/step
Epoch 146/1000
2023-09-28 10:06:37.391 
Epoch 146/1000 
	 loss: 17.4250, MinusLogProbMetric: 17.4250, val_loss: 17.8327, val_MinusLogProbMetric: 17.8327

Epoch 146: val_loss did not improve from 17.61760
196/196 - 87s - loss: 17.4250 - MinusLogProbMetric: 17.4250 - val_loss: 17.8327 - val_MinusLogProbMetric: 17.8327 - lr: 3.3333e-04 - 87s/epoch - 444ms/step
Epoch 147/1000
2023-09-28 10:08:00.570 
Epoch 147/1000 
	 loss: 17.4060, MinusLogProbMetric: 17.4060, val_loss: 18.0726, val_MinusLogProbMetric: 18.0726

Epoch 147: val_loss did not improve from 17.61760
196/196 - 83s - loss: 17.4060 - MinusLogProbMetric: 17.4060 - val_loss: 18.0726 - val_MinusLogProbMetric: 18.0726 - lr: 3.3333e-04 - 83s/epoch - 424ms/step
Epoch 148/1000
2023-09-28 10:09:22.889 
Epoch 148/1000 
	 loss: 17.3880, MinusLogProbMetric: 17.3880, val_loss: 17.5095, val_MinusLogProbMetric: 17.5095

Epoch 148: val_loss improved from 17.61760 to 17.50952, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_320/weights/best_weights.h5
196/196 - 84s - loss: 17.3880 - MinusLogProbMetric: 17.3880 - val_loss: 17.5095 - val_MinusLogProbMetric: 17.5095 - lr: 3.3333e-04 - 84s/epoch - 428ms/step
Epoch 149/1000
2023-09-28 10:10:47.246 
Epoch 149/1000 
	 loss: 17.3763, MinusLogProbMetric: 17.3763, val_loss: 18.9654, val_MinusLogProbMetric: 18.9654

Epoch 149: val_loss did not improve from 17.50952
196/196 - 83s - loss: 17.3763 - MinusLogProbMetric: 17.3763 - val_loss: 18.9654 - val_MinusLogProbMetric: 18.9654 - lr: 3.3333e-04 - 83s/epoch - 422ms/step
Epoch 150/1000
2023-09-28 10:12:11.452 
Epoch 150/1000 
	 loss: 17.3603, MinusLogProbMetric: 17.3603, val_loss: 17.6771, val_MinusLogProbMetric: 17.6771

Epoch 150: val_loss did not improve from 17.50952
196/196 - 84s - loss: 17.3603 - MinusLogProbMetric: 17.3603 - val_loss: 17.6771 - val_MinusLogProbMetric: 17.6771 - lr: 3.3333e-04 - 84s/epoch - 430ms/step
Epoch 151/1000
2023-09-28 10:13:35.186 
Epoch 151/1000 
	 loss: 17.3721, MinusLogProbMetric: 17.3721, val_loss: 17.6746, val_MinusLogProbMetric: 17.6746

Epoch 151: val_loss did not improve from 17.50952
196/196 - 84s - loss: 17.3721 - MinusLogProbMetric: 17.3721 - val_loss: 17.6746 - val_MinusLogProbMetric: 17.6746 - lr: 3.3333e-04 - 84s/epoch - 427ms/step
Epoch 152/1000
2023-09-28 10:14:59.830 
Epoch 152/1000 
	 loss: 17.4071, MinusLogProbMetric: 17.4071, val_loss: 17.5361, val_MinusLogProbMetric: 17.5361

Epoch 152: val_loss did not improve from 17.50952
196/196 - 85s - loss: 17.4071 - MinusLogProbMetric: 17.4071 - val_loss: 17.5361 - val_MinusLogProbMetric: 17.5361 - lr: 3.3333e-04 - 85s/epoch - 432ms/step
Epoch 153/1000
2023-09-28 10:16:26.382 
Epoch 153/1000 
	 loss: 17.3171, MinusLogProbMetric: 17.3171, val_loss: 17.5300, val_MinusLogProbMetric: 17.5300

Epoch 153: val_loss did not improve from 17.50952
196/196 - 87s - loss: 17.3171 - MinusLogProbMetric: 17.3171 - val_loss: 17.5300 - val_MinusLogProbMetric: 17.5300 - lr: 3.3333e-04 - 87s/epoch - 442ms/step
Epoch 154/1000
2023-09-28 10:17:53.098 
Epoch 154/1000 
	 loss: 17.3467, MinusLogProbMetric: 17.3467, val_loss: 17.8157, val_MinusLogProbMetric: 17.8157

Epoch 154: val_loss did not improve from 17.50952
196/196 - 87s - loss: 17.3467 - MinusLogProbMetric: 17.3467 - val_loss: 17.8157 - val_MinusLogProbMetric: 17.8157 - lr: 3.3333e-04 - 87s/epoch - 442ms/step
Epoch 155/1000
2023-09-28 10:19:17.503 
Epoch 155/1000 
	 loss: 17.4103, MinusLogProbMetric: 17.4103, val_loss: 17.5999, val_MinusLogProbMetric: 17.5999

Epoch 155: val_loss did not improve from 17.50952
196/196 - 84s - loss: 17.4103 - MinusLogProbMetric: 17.4103 - val_loss: 17.5999 - val_MinusLogProbMetric: 17.5999 - lr: 3.3333e-04 - 84s/epoch - 431ms/step
Epoch 156/1000
2023-09-28 10:20:40.308 
Epoch 156/1000 
	 loss: 17.3730, MinusLogProbMetric: 17.3730, val_loss: 17.7281, val_MinusLogProbMetric: 17.7281

Epoch 156: val_loss did not improve from 17.50952
196/196 - 83s - loss: 17.3730 - MinusLogProbMetric: 17.3730 - val_loss: 17.7281 - val_MinusLogProbMetric: 17.7281 - lr: 3.3333e-04 - 83s/epoch - 422ms/step
Epoch 157/1000
2023-09-28 10:22:02.932 
Epoch 157/1000 
	 loss: 17.3284, MinusLogProbMetric: 17.3284, val_loss: 18.1333, val_MinusLogProbMetric: 18.1333

Epoch 157: val_loss did not improve from 17.50952
196/196 - 83s - loss: 17.3284 - MinusLogProbMetric: 17.3284 - val_loss: 18.1333 - val_MinusLogProbMetric: 18.1333 - lr: 3.3333e-04 - 83s/epoch - 422ms/step
Epoch 158/1000
2023-09-28 10:23:26.426 
Epoch 158/1000 
	 loss: 17.3179, MinusLogProbMetric: 17.3179, val_loss: 17.6282, val_MinusLogProbMetric: 17.6282

Epoch 158: val_loss did not improve from 17.50952
196/196 - 83s - loss: 17.3179 - MinusLogProbMetric: 17.3179 - val_loss: 17.6282 - val_MinusLogProbMetric: 17.6282 - lr: 3.3333e-04 - 83s/epoch - 426ms/step
Epoch 159/1000
2023-09-28 10:24:50.003 
Epoch 159/1000 
	 loss: 17.3663, MinusLogProbMetric: 17.3663, val_loss: 17.6431, val_MinusLogProbMetric: 17.6431

Epoch 159: val_loss did not improve from 17.50952
196/196 - 84s - loss: 17.3663 - MinusLogProbMetric: 17.3663 - val_loss: 17.6431 - val_MinusLogProbMetric: 17.6431 - lr: 3.3333e-04 - 84s/epoch - 426ms/step
Epoch 160/1000
2023-09-28 10:26:11.314 
Epoch 160/1000 
	 loss: 17.3127, MinusLogProbMetric: 17.3127, val_loss: 17.6822, val_MinusLogProbMetric: 17.6822

Epoch 160: val_loss did not improve from 17.50952
196/196 - 81s - loss: 17.3127 - MinusLogProbMetric: 17.3127 - val_loss: 17.6822 - val_MinusLogProbMetric: 17.6822 - lr: 3.3333e-04 - 81s/epoch - 415ms/step
Epoch 161/1000
2023-09-28 10:27:33.273 
Epoch 161/1000 
	 loss: 17.3421, MinusLogProbMetric: 17.3421, val_loss: 17.6454, val_MinusLogProbMetric: 17.6454

Epoch 161: val_loss did not improve from 17.50952
196/196 - 82s - loss: 17.3421 - MinusLogProbMetric: 17.3421 - val_loss: 17.6454 - val_MinusLogProbMetric: 17.6454 - lr: 3.3333e-04 - 82s/epoch - 418ms/step
Epoch 162/1000
2023-09-28 10:28:55.996 
Epoch 162/1000 
	 loss: 17.3216, MinusLogProbMetric: 17.3216, val_loss: 17.7718, val_MinusLogProbMetric: 17.7718

Epoch 162: val_loss did not improve from 17.50952
196/196 - 83s - loss: 17.3216 - MinusLogProbMetric: 17.3216 - val_loss: 17.7718 - val_MinusLogProbMetric: 17.7718 - lr: 3.3333e-04 - 83s/epoch - 422ms/step
Epoch 163/1000
2023-09-28 10:30:21.597 
Epoch 163/1000 
	 loss: 17.2858, MinusLogProbMetric: 17.2858, val_loss: 18.1108, val_MinusLogProbMetric: 18.1108

Epoch 163: val_loss did not improve from 17.50952
196/196 - 86s - loss: 17.2858 - MinusLogProbMetric: 17.2858 - val_loss: 18.1108 - val_MinusLogProbMetric: 18.1108 - lr: 3.3333e-04 - 86s/epoch - 437ms/step
Epoch 164/1000
2023-09-28 10:31:46.307 
Epoch 164/1000 
	 loss: 17.2858, MinusLogProbMetric: 17.2858, val_loss: 18.3443, val_MinusLogProbMetric: 18.3443

Epoch 164: val_loss did not improve from 17.50952
196/196 - 85s - loss: 17.2858 - MinusLogProbMetric: 17.2858 - val_loss: 18.3443 - val_MinusLogProbMetric: 18.3443 - lr: 3.3333e-04 - 85s/epoch - 432ms/step
Epoch 165/1000
2023-09-28 10:33:08.007 
Epoch 165/1000 
	 loss: 17.3029, MinusLogProbMetric: 17.3029, val_loss: 17.5378, val_MinusLogProbMetric: 17.5378

Epoch 165: val_loss did not improve from 17.50952
196/196 - 82s - loss: 17.3029 - MinusLogProbMetric: 17.3029 - val_loss: 17.5378 - val_MinusLogProbMetric: 17.5378 - lr: 3.3333e-04 - 82s/epoch - 417ms/step
Epoch 166/1000
2023-09-28 10:34:29.801 
Epoch 166/1000 
	 loss: 17.2886, MinusLogProbMetric: 17.2886, val_loss: 17.8823, val_MinusLogProbMetric: 17.8823

Epoch 166: val_loss did not improve from 17.50952
196/196 - 82s - loss: 17.2886 - MinusLogProbMetric: 17.2886 - val_loss: 17.8823 - val_MinusLogProbMetric: 17.8823 - lr: 3.3333e-04 - 82s/epoch - 417ms/step
Epoch 167/1000
2023-09-28 10:35:49.702 
Epoch 167/1000 
	 loss: 17.3792, MinusLogProbMetric: 17.3792, val_loss: 17.6345, val_MinusLogProbMetric: 17.6345

Epoch 167: val_loss did not improve from 17.50952
196/196 - 80s - loss: 17.3792 - MinusLogProbMetric: 17.3792 - val_loss: 17.6345 - val_MinusLogProbMetric: 17.6345 - lr: 3.3333e-04 - 80s/epoch - 408ms/step
Epoch 168/1000
2023-09-28 10:37:16.131 
Epoch 168/1000 
	 loss: 17.3242, MinusLogProbMetric: 17.3242, val_loss: 17.7410, val_MinusLogProbMetric: 17.7410

Epoch 168: val_loss did not improve from 17.50952
196/196 - 86s - loss: 17.3242 - MinusLogProbMetric: 17.3242 - val_loss: 17.7410 - val_MinusLogProbMetric: 17.7410 - lr: 3.3333e-04 - 86s/epoch - 441ms/step
Epoch 169/1000
2023-09-28 10:38:37.261 
Epoch 169/1000 
	 loss: 17.2641, MinusLogProbMetric: 17.2641, val_loss: 17.4954, val_MinusLogProbMetric: 17.4954

Epoch 169: val_loss improved from 17.50952 to 17.49544, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_320/weights/best_weights.h5
196/196 - 83s - loss: 17.2641 - MinusLogProbMetric: 17.2641 - val_loss: 17.4954 - val_MinusLogProbMetric: 17.4954 - lr: 3.3333e-04 - 83s/epoch - 422ms/step
Epoch 170/1000
2023-09-28 10:40:04.374 
Epoch 170/1000 
	 loss: 17.2792, MinusLogProbMetric: 17.2792, val_loss: 18.0275, val_MinusLogProbMetric: 18.0275

Epoch 170: val_loss did not improve from 17.49544
196/196 - 85s - loss: 17.2792 - MinusLogProbMetric: 17.2792 - val_loss: 18.0275 - val_MinusLogProbMetric: 18.0275 - lr: 3.3333e-04 - 85s/epoch - 436ms/step
Epoch 171/1000
2023-09-28 10:41:30.337 
Epoch 171/1000 
	 loss: 17.2735, MinusLogProbMetric: 17.2735, val_loss: 17.9395, val_MinusLogProbMetric: 17.9395

Epoch 171: val_loss did not improve from 17.49544
196/196 - 86s - loss: 17.2735 - MinusLogProbMetric: 17.2735 - val_loss: 17.9395 - val_MinusLogProbMetric: 17.9395 - lr: 3.3333e-04 - 86s/epoch - 439ms/step
Epoch 172/1000
2023-09-28 10:42:52.727 
Epoch 172/1000 
	 loss: 17.2552, MinusLogProbMetric: 17.2552, val_loss: 18.2873, val_MinusLogProbMetric: 18.2873

Epoch 172: val_loss did not improve from 17.49544
196/196 - 82s - loss: 17.2552 - MinusLogProbMetric: 17.2552 - val_loss: 18.2873 - val_MinusLogProbMetric: 18.2873 - lr: 3.3333e-04 - 82s/epoch - 420ms/step
Epoch 173/1000
2023-09-28 10:44:17.527 
Epoch 173/1000 
	 loss: 17.3056, MinusLogProbMetric: 17.3056, val_loss: 17.8271, val_MinusLogProbMetric: 17.8271

Epoch 173: val_loss did not improve from 17.49544
196/196 - 85s - loss: 17.3056 - MinusLogProbMetric: 17.3056 - val_loss: 17.8271 - val_MinusLogProbMetric: 17.8271 - lr: 3.3333e-04 - 85s/epoch - 433ms/step
Epoch 174/1000
2023-09-28 10:45:32.956 
Epoch 174/1000 
	 loss: 17.2482, MinusLogProbMetric: 17.2482, val_loss: 17.5972, val_MinusLogProbMetric: 17.5972

Epoch 174: val_loss did not improve from 17.49544
196/196 - 75s - loss: 17.2482 - MinusLogProbMetric: 17.2482 - val_loss: 17.5972 - val_MinusLogProbMetric: 17.5972 - lr: 3.3333e-04 - 75s/epoch - 385ms/step
Epoch 175/1000
2023-09-28 10:46:49.286 
Epoch 175/1000 
	 loss: 17.2250, MinusLogProbMetric: 17.2250, val_loss: 17.8022, val_MinusLogProbMetric: 17.8022

Epoch 175: val_loss did not improve from 17.49544
196/196 - 76s - loss: 17.2250 - MinusLogProbMetric: 17.2250 - val_loss: 17.8022 - val_MinusLogProbMetric: 17.8022 - lr: 3.3333e-04 - 76s/epoch - 389ms/step
Epoch 176/1000
2023-09-28 10:48:04.664 
Epoch 176/1000 
	 loss: 17.2839, MinusLogProbMetric: 17.2839, val_loss: 17.8301, val_MinusLogProbMetric: 17.8301

Epoch 176: val_loss did not improve from 17.49544
196/196 - 75s - loss: 17.2839 - MinusLogProbMetric: 17.2839 - val_loss: 17.8301 - val_MinusLogProbMetric: 17.8301 - lr: 3.3333e-04 - 75s/epoch - 385ms/step
Epoch 177/1000
2023-09-28 10:49:21.200 
Epoch 177/1000 
	 loss: 17.2986, MinusLogProbMetric: 17.2986, val_loss: 17.6888, val_MinusLogProbMetric: 17.6888

Epoch 177: val_loss did not improve from 17.49544
196/196 - 77s - loss: 17.2986 - MinusLogProbMetric: 17.2986 - val_loss: 17.6888 - val_MinusLogProbMetric: 17.6888 - lr: 3.3333e-04 - 77s/epoch - 390ms/step
Epoch 178/1000
2023-09-28 10:50:35.319 
Epoch 178/1000 
	 loss: 17.2006, MinusLogProbMetric: 17.2006, val_loss: 17.8479, val_MinusLogProbMetric: 17.8479

Epoch 178: val_loss did not improve from 17.49544
196/196 - 74s - loss: 17.2006 - MinusLogProbMetric: 17.2006 - val_loss: 17.8479 - val_MinusLogProbMetric: 17.8479 - lr: 3.3333e-04 - 74s/epoch - 378ms/step
Epoch 179/1000
2023-09-28 10:51:52.974 
Epoch 179/1000 
	 loss: 17.2595, MinusLogProbMetric: 17.2595, val_loss: 17.8386, val_MinusLogProbMetric: 17.8386

Epoch 179: val_loss did not improve from 17.49544
196/196 - 78s - loss: 17.2595 - MinusLogProbMetric: 17.2595 - val_loss: 17.8386 - val_MinusLogProbMetric: 17.8386 - lr: 3.3333e-04 - 78s/epoch - 396ms/step
Epoch 180/1000
2023-09-28 10:53:10.969 
Epoch 180/1000 
	 loss: 17.2168, MinusLogProbMetric: 17.2168, val_loss: 17.9937, val_MinusLogProbMetric: 17.9937

Epoch 180: val_loss did not improve from 17.49544
196/196 - 78s - loss: 17.2168 - MinusLogProbMetric: 17.2168 - val_loss: 17.9937 - val_MinusLogProbMetric: 17.9937 - lr: 3.3333e-04 - 78s/epoch - 398ms/step
Epoch 181/1000
2023-09-28 10:54:28.257 
Epoch 181/1000 
	 loss: 17.2398, MinusLogProbMetric: 17.2398, val_loss: 17.8863, val_MinusLogProbMetric: 17.8863

Epoch 181: val_loss did not improve from 17.49544
196/196 - 77s - loss: 17.2398 - MinusLogProbMetric: 17.2398 - val_loss: 17.8863 - val_MinusLogProbMetric: 17.8863 - lr: 3.3333e-04 - 77s/epoch - 394ms/step
Epoch 182/1000
2023-09-28 10:55:42.039 
Epoch 182/1000 
	 loss: 17.2264, MinusLogProbMetric: 17.2264, val_loss: 17.7518, val_MinusLogProbMetric: 17.7518

Epoch 182: val_loss did not improve from 17.49544
196/196 - 74s - loss: 17.2264 - MinusLogProbMetric: 17.2264 - val_loss: 17.7518 - val_MinusLogProbMetric: 17.7518 - lr: 3.3333e-04 - 74s/epoch - 376ms/step
Epoch 183/1000
2023-09-28 10:56:57.661 
Epoch 183/1000 
	 loss: 17.1769, MinusLogProbMetric: 17.1769, val_loss: 18.4957, val_MinusLogProbMetric: 18.4957

Epoch 183: val_loss did not improve from 17.49544
196/196 - 76s - loss: 17.1769 - MinusLogProbMetric: 17.1769 - val_loss: 18.4957 - val_MinusLogProbMetric: 18.4957 - lr: 3.3333e-04 - 76s/epoch - 386ms/step
Epoch 184/1000
2023-09-28 10:58:11.902 
Epoch 184/1000 
	 loss: 17.2505, MinusLogProbMetric: 17.2505, val_loss: 17.9720, val_MinusLogProbMetric: 17.9720

Epoch 184: val_loss did not improve from 17.49544
196/196 - 74s - loss: 17.2505 - MinusLogProbMetric: 17.2505 - val_loss: 17.9720 - val_MinusLogProbMetric: 17.9720 - lr: 3.3333e-04 - 74s/epoch - 379ms/step
Epoch 185/1000
2023-09-28 10:59:26.537 
Epoch 185/1000 
	 loss: 17.1723, MinusLogProbMetric: 17.1723, val_loss: 17.7283, val_MinusLogProbMetric: 17.7283

Epoch 185: val_loss did not improve from 17.49544
196/196 - 75s - loss: 17.1723 - MinusLogProbMetric: 17.1723 - val_loss: 17.7283 - val_MinusLogProbMetric: 17.7283 - lr: 3.3333e-04 - 75s/epoch - 381ms/step
Epoch 186/1000
2023-09-28 11:00:40.530 
Epoch 186/1000 
	 loss: 17.1767, MinusLogProbMetric: 17.1767, val_loss: 17.7530, val_MinusLogProbMetric: 17.7530

Epoch 186: val_loss did not improve from 17.49544
196/196 - 74s - loss: 17.1767 - MinusLogProbMetric: 17.1767 - val_loss: 17.7530 - val_MinusLogProbMetric: 17.7530 - lr: 3.3333e-04 - 74s/epoch - 377ms/step
Epoch 187/1000
2023-09-28 11:01:56.049 
Epoch 187/1000 
	 loss: 17.2550, MinusLogProbMetric: 17.2550, val_loss: 17.8706, val_MinusLogProbMetric: 17.8706

Epoch 187: val_loss did not improve from 17.49544
196/196 - 76s - loss: 17.2550 - MinusLogProbMetric: 17.2550 - val_loss: 17.8706 - val_MinusLogProbMetric: 17.8706 - lr: 3.3333e-04 - 76s/epoch - 385ms/step
Epoch 188/1000
2023-09-28 11:03:11.297 
Epoch 188/1000 
	 loss: 17.1719, MinusLogProbMetric: 17.1719, val_loss: 17.5395, val_MinusLogProbMetric: 17.5395

Epoch 188: val_loss did not improve from 17.49544
196/196 - 75s - loss: 17.1719 - MinusLogProbMetric: 17.1719 - val_loss: 17.5395 - val_MinusLogProbMetric: 17.5395 - lr: 3.3333e-04 - 75s/epoch - 384ms/step
Epoch 189/1000
2023-09-28 11:04:30.139 
Epoch 189/1000 
	 loss: 17.2195, MinusLogProbMetric: 17.2195, val_loss: 17.5109, val_MinusLogProbMetric: 17.5109

Epoch 189: val_loss did not improve from 17.49544
196/196 - 79s - loss: 17.2195 - MinusLogProbMetric: 17.2195 - val_loss: 17.5109 - val_MinusLogProbMetric: 17.5109 - lr: 3.3333e-04 - 79s/epoch - 402ms/step
Epoch 190/1000
2023-09-28 11:05:44.282 
Epoch 190/1000 
	 loss: 17.1334, MinusLogProbMetric: 17.1334, val_loss: 17.6827, val_MinusLogProbMetric: 17.6827

Epoch 190: val_loss did not improve from 17.49544
196/196 - 74s - loss: 17.1334 - MinusLogProbMetric: 17.1334 - val_loss: 17.6827 - val_MinusLogProbMetric: 17.6827 - lr: 3.3333e-04 - 74s/epoch - 378ms/step
Epoch 191/1000
2023-09-28 11:07:02.921 
Epoch 191/1000 
	 loss: 17.1406, MinusLogProbMetric: 17.1406, val_loss: 17.5220, val_MinusLogProbMetric: 17.5220

Epoch 191: val_loss did not improve from 17.49544
196/196 - 79s - loss: 17.1406 - MinusLogProbMetric: 17.1406 - val_loss: 17.5220 - val_MinusLogProbMetric: 17.5220 - lr: 3.3333e-04 - 79s/epoch - 401ms/step
Epoch 192/1000
2023-09-28 11:08:18.793 
Epoch 192/1000 
	 loss: 17.1439, MinusLogProbMetric: 17.1439, val_loss: 17.4959, val_MinusLogProbMetric: 17.4959

Epoch 192: val_loss did not improve from 17.49544
196/196 - 76s - loss: 17.1439 - MinusLogProbMetric: 17.1439 - val_loss: 17.4959 - val_MinusLogProbMetric: 17.4959 - lr: 3.3333e-04 - 76s/epoch - 387ms/step
Epoch 193/1000
2023-09-28 11:09:33.375 
Epoch 193/1000 
	 loss: 17.1857, MinusLogProbMetric: 17.1857, val_loss: 17.6811, val_MinusLogProbMetric: 17.6811

Epoch 193: val_loss did not improve from 17.49544
196/196 - 75s - loss: 17.1857 - MinusLogProbMetric: 17.1857 - val_loss: 17.6811 - val_MinusLogProbMetric: 17.6811 - lr: 3.3333e-04 - 75s/epoch - 381ms/step
Epoch 194/1000
2023-09-28 11:10:46.860 
Epoch 194/1000 
	 loss: 17.1945, MinusLogProbMetric: 17.1945, val_loss: 17.8639, val_MinusLogProbMetric: 17.8639

Epoch 194: val_loss did not improve from 17.49544
196/196 - 73s - loss: 17.1945 - MinusLogProbMetric: 17.1945 - val_loss: 17.8639 - val_MinusLogProbMetric: 17.8639 - lr: 3.3333e-04 - 73s/epoch - 375ms/step
Epoch 195/1000
2023-09-28 11:11:59.840 
Epoch 195/1000 
	 loss: 17.1754, MinusLogProbMetric: 17.1754, val_loss: 17.8978, val_MinusLogProbMetric: 17.8978

Epoch 195: val_loss did not improve from 17.49544
196/196 - 73s - loss: 17.1754 - MinusLogProbMetric: 17.1754 - val_loss: 17.8978 - val_MinusLogProbMetric: 17.8978 - lr: 3.3333e-04 - 73s/epoch - 372ms/step
Epoch 196/1000
2023-09-28 11:13:19.986 
Epoch 196/1000 
	 loss: 17.2152, MinusLogProbMetric: 17.2152, val_loss: 17.6085, val_MinusLogProbMetric: 17.6085

Epoch 196: val_loss did not improve from 17.49544
196/196 - 80s - loss: 17.2152 - MinusLogProbMetric: 17.2152 - val_loss: 17.6085 - val_MinusLogProbMetric: 17.6085 - lr: 3.3333e-04 - 80s/epoch - 409ms/step
Epoch 197/1000
2023-09-28 11:14:40.247 
Epoch 197/1000 
	 loss: 17.1123, MinusLogProbMetric: 17.1123, val_loss: 17.9741, val_MinusLogProbMetric: 17.9741

Epoch 197: val_loss did not improve from 17.49544
196/196 - 80s - loss: 17.1123 - MinusLogProbMetric: 17.1123 - val_loss: 17.9741 - val_MinusLogProbMetric: 17.9741 - lr: 3.3333e-04 - 80s/epoch - 409ms/step
Epoch 198/1000
2023-09-28 11:15:59.968 
Epoch 198/1000 
	 loss: 17.1317, MinusLogProbMetric: 17.1317, val_loss: 17.7535, val_MinusLogProbMetric: 17.7535

Epoch 198: val_loss did not improve from 17.49544
196/196 - 80s - loss: 17.1317 - MinusLogProbMetric: 17.1317 - val_loss: 17.7535 - val_MinusLogProbMetric: 17.7535 - lr: 3.3333e-04 - 80s/epoch - 407ms/step
Epoch 199/1000
2023-09-28 11:17:19.324 
Epoch 199/1000 
	 loss: 17.0676, MinusLogProbMetric: 17.0676, val_loss: 17.6099, val_MinusLogProbMetric: 17.6099

Epoch 199: val_loss did not improve from 17.49544
196/196 - 79s - loss: 17.0676 - MinusLogProbMetric: 17.0676 - val_loss: 17.6099 - val_MinusLogProbMetric: 17.6099 - lr: 3.3333e-04 - 79s/epoch - 405ms/step
Epoch 200/1000
2023-09-28 11:18:41.011 
Epoch 200/1000 
	 loss: 17.1354, MinusLogProbMetric: 17.1354, val_loss: 17.7479, val_MinusLogProbMetric: 17.7479

Epoch 200: val_loss did not improve from 17.49544
196/196 - 82s - loss: 17.1354 - MinusLogProbMetric: 17.1354 - val_loss: 17.7479 - val_MinusLogProbMetric: 17.7479 - lr: 3.3333e-04 - 82s/epoch - 417ms/step
Epoch 201/1000
2023-09-28 11:20:02.208 
Epoch 201/1000 
	 loss: 17.1341, MinusLogProbMetric: 17.1341, val_loss: 17.9731, val_MinusLogProbMetric: 17.9731

Epoch 201: val_loss did not improve from 17.49544
196/196 - 81s - loss: 17.1341 - MinusLogProbMetric: 17.1341 - val_loss: 17.9731 - val_MinusLogProbMetric: 17.9731 - lr: 3.3333e-04 - 81s/epoch - 414ms/step
Epoch 202/1000
2023-09-28 11:21:20.321 
Epoch 202/1000 
	 loss: 17.1533, MinusLogProbMetric: 17.1533, val_loss: 17.6422, val_MinusLogProbMetric: 17.6422

Epoch 202: val_loss did not improve from 17.49544
196/196 - 78s - loss: 17.1533 - MinusLogProbMetric: 17.1533 - val_loss: 17.6422 - val_MinusLogProbMetric: 17.6422 - lr: 3.3333e-04 - 78s/epoch - 399ms/step
Epoch 203/1000
2023-09-28 11:22:41.147 
Epoch 203/1000 
	 loss: 17.0878, MinusLogProbMetric: 17.0878, val_loss: 17.5962, val_MinusLogProbMetric: 17.5962

Epoch 203: val_loss did not improve from 17.49544
196/196 - 81s - loss: 17.0878 - MinusLogProbMetric: 17.0878 - val_loss: 17.5962 - val_MinusLogProbMetric: 17.5962 - lr: 3.3333e-04 - 81s/epoch - 412ms/step
Epoch 204/1000
2023-09-28 11:24:00.167 
Epoch 204/1000 
	 loss: 17.1174, MinusLogProbMetric: 17.1174, val_loss: 17.5778, val_MinusLogProbMetric: 17.5778

Epoch 204: val_loss did not improve from 17.49544
196/196 - 79s - loss: 17.1174 - MinusLogProbMetric: 17.1174 - val_loss: 17.5778 - val_MinusLogProbMetric: 17.5778 - lr: 3.3333e-04 - 79s/epoch - 403ms/step
Epoch 205/1000
2023-09-28 11:25:24.533 
Epoch 205/1000 
	 loss: 17.1797, MinusLogProbMetric: 17.1797, val_loss: 17.7302, val_MinusLogProbMetric: 17.7302

Epoch 205: val_loss did not improve from 17.49544
196/196 - 84s - loss: 17.1797 - MinusLogProbMetric: 17.1797 - val_loss: 17.7302 - val_MinusLogProbMetric: 17.7302 - lr: 3.3333e-04 - 84s/epoch - 430ms/step
Epoch 206/1000
2023-09-28 11:26:47.182 
Epoch 206/1000 
	 loss: 17.1138, MinusLogProbMetric: 17.1138, val_loss: 17.9708, val_MinusLogProbMetric: 17.9708

Epoch 206: val_loss did not improve from 17.49544
196/196 - 83s - loss: 17.1138 - MinusLogProbMetric: 17.1138 - val_loss: 17.9708 - val_MinusLogProbMetric: 17.9708 - lr: 3.3333e-04 - 83s/epoch - 422ms/step
Epoch 207/1000
2023-09-28 11:28:11.674 
Epoch 207/1000 
	 loss: 17.1034, MinusLogProbMetric: 17.1034, val_loss: 17.7921, val_MinusLogProbMetric: 17.7921

Epoch 207: val_loss did not improve from 17.49544
196/196 - 84s - loss: 17.1034 - MinusLogProbMetric: 17.1034 - val_loss: 17.7921 - val_MinusLogProbMetric: 17.7921 - lr: 3.3333e-04 - 84s/epoch - 431ms/step
Epoch 208/1000
2023-09-28 11:29:37.634 
Epoch 208/1000 
	 loss: 17.0951, MinusLogProbMetric: 17.0951, val_loss: 17.6274, val_MinusLogProbMetric: 17.6274

Epoch 208: val_loss did not improve from 17.49544
196/196 - 86s - loss: 17.0951 - MinusLogProbMetric: 17.0951 - val_loss: 17.6274 - val_MinusLogProbMetric: 17.6274 - lr: 3.3333e-04 - 86s/epoch - 439ms/step
Epoch 209/1000
2023-09-28 11:31:02.950 
Epoch 209/1000 
	 loss: 17.0947, MinusLogProbMetric: 17.0947, val_loss: 17.9363, val_MinusLogProbMetric: 17.9363

Epoch 209: val_loss did not improve from 17.49544
196/196 - 85s - loss: 17.0947 - MinusLogProbMetric: 17.0947 - val_loss: 17.9363 - val_MinusLogProbMetric: 17.9363 - lr: 3.3333e-04 - 85s/epoch - 435ms/step
Epoch 210/1000
2023-09-28 11:32:28.757 
Epoch 210/1000 
	 loss: 17.1003, MinusLogProbMetric: 17.1003, val_loss: 17.7040, val_MinusLogProbMetric: 17.7040

Epoch 210: val_loss did not improve from 17.49544
196/196 - 86s - loss: 17.1003 - MinusLogProbMetric: 17.1003 - val_loss: 17.7040 - val_MinusLogProbMetric: 17.7040 - lr: 3.3333e-04 - 86s/epoch - 438ms/step
Epoch 211/1000
2023-09-28 11:33:53.973 
Epoch 211/1000 
	 loss: 17.0592, MinusLogProbMetric: 17.0592, val_loss: 18.0280, val_MinusLogProbMetric: 18.0280

Epoch 211: val_loss did not improve from 17.49544
196/196 - 85s - loss: 17.0592 - MinusLogProbMetric: 17.0592 - val_loss: 18.0280 - val_MinusLogProbMetric: 18.0280 - lr: 3.3333e-04 - 85s/epoch - 435ms/step
Epoch 212/1000
2023-09-28 11:35:19.258 
Epoch 212/1000 
	 loss: 17.1238, MinusLogProbMetric: 17.1238, val_loss: 17.4792, val_MinusLogProbMetric: 17.4792

Epoch 212: val_loss improved from 17.49544 to 17.47923, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_320/weights/best_weights.h5
196/196 - 87s - loss: 17.1238 - MinusLogProbMetric: 17.1238 - val_loss: 17.4792 - val_MinusLogProbMetric: 17.4792 - lr: 3.3333e-04 - 87s/epoch - 442ms/step
Epoch 213/1000
2023-09-28 11:36:42.588 
Epoch 213/1000 
	 loss: 17.0970, MinusLogProbMetric: 17.0970, val_loss: 17.4842, val_MinusLogProbMetric: 17.4842

Epoch 213: val_loss did not improve from 17.47923
196/196 - 82s - loss: 17.0970 - MinusLogProbMetric: 17.0970 - val_loss: 17.4842 - val_MinusLogProbMetric: 17.4842 - lr: 3.3333e-04 - 82s/epoch - 418ms/step
Epoch 214/1000
2023-09-28 11:38:00.439 
Epoch 214/1000 
	 loss: 17.0612, MinusLogProbMetric: 17.0612, val_loss: 17.7161, val_MinusLogProbMetric: 17.7161

Epoch 214: val_loss did not improve from 17.47923
196/196 - 78s - loss: 17.0612 - MinusLogProbMetric: 17.0612 - val_loss: 17.7161 - val_MinusLogProbMetric: 17.7161 - lr: 3.3333e-04 - 78s/epoch - 397ms/step
Epoch 215/1000
2023-09-28 11:39:18.214 
Epoch 215/1000 
	 loss: 17.0785, MinusLogProbMetric: 17.0785, val_loss: 17.5951, val_MinusLogProbMetric: 17.5951

Epoch 215: val_loss did not improve from 17.47923
196/196 - 78s - loss: 17.0785 - MinusLogProbMetric: 17.0785 - val_loss: 17.5951 - val_MinusLogProbMetric: 17.5951 - lr: 3.3333e-04 - 78s/epoch - 397ms/step
Epoch 216/1000
2023-09-28 11:40:36.874 
Epoch 216/1000 
	 loss: 17.0233, MinusLogProbMetric: 17.0233, val_loss: 17.5256, val_MinusLogProbMetric: 17.5256

Epoch 216: val_loss did not improve from 17.47923
196/196 - 79s - loss: 17.0233 - MinusLogProbMetric: 17.0233 - val_loss: 17.5256 - val_MinusLogProbMetric: 17.5256 - lr: 3.3333e-04 - 79s/epoch - 401ms/step
Epoch 217/1000
2023-09-28 11:41:59.595 
Epoch 217/1000 
	 loss: 17.0446, MinusLogProbMetric: 17.0446, val_loss: 17.6160, val_MinusLogProbMetric: 17.6160

Epoch 217: val_loss did not improve from 17.47923
196/196 - 83s - loss: 17.0446 - MinusLogProbMetric: 17.0446 - val_loss: 17.6160 - val_MinusLogProbMetric: 17.6160 - lr: 3.3333e-04 - 83s/epoch - 422ms/step
Epoch 218/1000
2023-09-28 11:43:21.763 
Epoch 218/1000 
	 loss: 17.1100, MinusLogProbMetric: 17.1100, val_loss: 17.4980, val_MinusLogProbMetric: 17.4980

Epoch 218: val_loss did not improve from 17.47923
196/196 - 82s - loss: 17.1100 - MinusLogProbMetric: 17.1100 - val_loss: 17.4980 - val_MinusLogProbMetric: 17.4980 - lr: 3.3333e-04 - 82s/epoch - 419ms/step
Epoch 219/1000
2023-09-28 11:44:44.358 
Epoch 219/1000 
	 loss: 17.1161, MinusLogProbMetric: 17.1161, val_loss: 17.4851, val_MinusLogProbMetric: 17.4851

Epoch 219: val_loss did not improve from 17.47923
196/196 - 83s - loss: 17.1161 - MinusLogProbMetric: 17.1161 - val_loss: 17.4851 - val_MinusLogProbMetric: 17.4851 - lr: 3.3333e-04 - 83s/epoch - 421ms/step
Epoch 220/1000
2023-09-28 11:46:07.889 
Epoch 220/1000 
	 loss: 17.0973, MinusLogProbMetric: 17.0973, val_loss: 17.7179, val_MinusLogProbMetric: 17.7179

Epoch 220: val_loss did not improve from 17.47923
196/196 - 84s - loss: 17.0973 - MinusLogProbMetric: 17.0973 - val_loss: 17.7179 - val_MinusLogProbMetric: 17.7179 - lr: 3.3333e-04 - 84s/epoch - 426ms/step
Epoch 221/1000
2023-09-28 11:47:26.850 
Epoch 221/1000 
	 loss: 17.0991, MinusLogProbMetric: 17.0991, val_loss: 17.4996, val_MinusLogProbMetric: 17.4996

Epoch 221: val_loss did not improve from 17.47923
196/196 - 79s - loss: 17.0991 - MinusLogProbMetric: 17.0991 - val_loss: 17.4996 - val_MinusLogProbMetric: 17.4996 - lr: 3.3333e-04 - 79s/epoch - 403ms/step
Epoch 222/1000
2023-09-28 11:48:37.176 
Epoch 222/1000 
	 loss: 17.0277, MinusLogProbMetric: 17.0277, val_loss: 17.4448, val_MinusLogProbMetric: 17.4448

Epoch 222: val_loss improved from 17.47923 to 17.44476, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_320/weights/best_weights.h5
196/196 - 71s - loss: 17.0277 - MinusLogProbMetric: 17.0277 - val_loss: 17.4448 - val_MinusLogProbMetric: 17.4448 - lr: 3.3333e-04 - 71s/epoch - 364ms/step
Epoch 223/1000
2023-09-28 11:49:50.136 
Epoch 223/1000 
	 loss: 17.0883, MinusLogProbMetric: 17.0883, val_loss: 17.7687, val_MinusLogProbMetric: 17.7687

Epoch 223: val_loss did not improve from 17.44476
196/196 - 72s - loss: 17.0883 - MinusLogProbMetric: 17.0883 - val_loss: 17.7687 - val_MinusLogProbMetric: 17.7687 - lr: 3.3333e-04 - 72s/epoch - 367ms/step
Epoch 224/1000
2023-09-28 11:51:05.207 
Epoch 224/1000 
	 loss: 17.0550, MinusLogProbMetric: 17.0550, val_loss: 17.6995, val_MinusLogProbMetric: 17.6995

Epoch 224: val_loss did not improve from 17.44476
196/196 - 75s - loss: 17.0550 - MinusLogProbMetric: 17.0550 - val_loss: 17.6995 - val_MinusLogProbMetric: 17.6995 - lr: 3.3333e-04 - 75s/epoch - 383ms/step
Epoch 225/1000
2023-09-28 11:52:17.218 
Epoch 225/1000 
	 loss: 17.0393, MinusLogProbMetric: 17.0393, val_loss: 18.1685, val_MinusLogProbMetric: 18.1685

Epoch 225: val_loss did not improve from 17.44476
196/196 - 72s - loss: 17.0393 - MinusLogProbMetric: 17.0393 - val_loss: 18.1685 - val_MinusLogProbMetric: 18.1685 - lr: 3.3333e-04 - 72s/epoch - 367ms/step
Epoch 226/1000
2023-09-28 11:53:31.684 
Epoch 226/1000 
	 loss: 17.0091, MinusLogProbMetric: 17.0091, val_loss: 17.7894, val_MinusLogProbMetric: 17.7894

Epoch 226: val_loss did not improve from 17.44476
196/196 - 74s - loss: 17.0091 - MinusLogProbMetric: 17.0091 - val_loss: 17.7894 - val_MinusLogProbMetric: 17.7894 - lr: 3.3333e-04 - 74s/epoch - 380ms/step
Epoch 227/1000
2023-09-28 11:54:47.388 
Epoch 227/1000 
	 loss: 16.9930, MinusLogProbMetric: 16.9930, val_loss: 17.9286, val_MinusLogProbMetric: 17.9286

Epoch 227: val_loss did not improve from 17.44476
196/196 - 76s - loss: 16.9930 - MinusLogProbMetric: 16.9930 - val_loss: 17.9286 - val_MinusLogProbMetric: 17.9286 - lr: 3.3333e-04 - 76s/epoch - 386ms/step
Epoch 228/1000
2023-09-28 11:56:02.563 
Epoch 228/1000 
	 loss: 16.9872, MinusLogProbMetric: 16.9872, val_loss: 17.9506, val_MinusLogProbMetric: 17.9506

Epoch 228: val_loss did not improve from 17.44476
196/196 - 75s - loss: 16.9872 - MinusLogProbMetric: 16.9872 - val_loss: 17.9506 - val_MinusLogProbMetric: 17.9506 - lr: 3.3333e-04 - 75s/epoch - 384ms/step
Epoch 229/1000
2023-09-28 11:57:17.274 
Epoch 229/1000 
	 loss: 17.0299, MinusLogProbMetric: 17.0299, val_loss: 17.6464, val_MinusLogProbMetric: 17.6464

Epoch 229: val_loss did not improve from 17.44476
196/196 - 75s - loss: 17.0299 - MinusLogProbMetric: 17.0299 - val_loss: 17.6464 - val_MinusLogProbMetric: 17.6464 - lr: 3.3333e-04 - 75s/epoch - 381ms/step
Epoch 230/1000
2023-09-28 11:58:43.113 
Epoch 230/1000 
	 loss: 17.0359, MinusLogProbMetric: 17.0359, val_loss: 17.9666, val_MinusLogProbMetric: 17.9666

Epoch 230: val_loss did not improve from 17.44476
196/196 - 86s - loss: 17.0359 - MinusLogProbMetric: 17.0359 - val_loss: 17.9666 - val_MinusLogProbMetric: 17.9666 - lr: 3.3333e-04 - 86s/epoch - 438ms/step
Epoch 231/1000
2023-09-28 12:00:09.287 
Epoch 231/1000 
	 loss: 17.0357, MinusLogProbMetric: 17.0357, val_loss: 18.2057, val_MinusLogProbMetric: 18.2057

Epoch 231: val_loss did not improve from 17.44476
196/196 - 86s - loss: 17.0357 - MinusLogProbMetric: 17.0357 - val_loss: 18.2057 - val_MinusLogProbMetric: 18.2057 - lr: 3.3333e-04 - 86s/epoch - 440ms/step
Epoch 232/1000
2023-09-28 12:01:31.194 
Epoch 232/1000 
	 loss: 17.0254, MinusLogProbMetric: 17.0254, val_loss: 17.7936, val_MinusLogProbMetric: 17.7936

Epoch 232: val_loss did not improve from 17.44476
196/196 - 82s - loss: 17.0254 - MinusLogProbMetric: 17.0254 - val_loss: 17.7936 - val_MinusLogProbMetric: 17.7936 - lr: 3.3333e-04 - 82s/epoch - 418ms/step
Epoch 233/1000
2023-09-28 12:02:53.769 
Epoch 233/1000 
	 loss: 17.0719, MinusLogProbMetric: 17.0719, val_loss: 17.9328, val_MinusLogProbMetric: 17.9328

Epoch 233: val_loss did not improve from 17.44476
196/196 - 83s - loss: 17.0719 - MinusLogProbMetric: 17.0719 - val_loss: 17.9328 - val_MinusLogProbMetric: 17.9328 - lr: 3.3333e-04 - 83s/epoch - 421ms/step
Epoch 234/1000
2023-09-28 12:04:14.629 
Epoch 234/1000 
	 loss: 17.0229, MinusLogProbMetric: 17.0229, val_loss: 17.4023, val_MinusLogProbMetric: 17.4023

Epoch 234: val_loss improved from 17.44476 to 17.40233, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_320/weights/best_weights.h5
196/196 - 82s - loss: 17.0229 - MinusLogProbMetric: 17.0229 - val_loss: 17.4023 - val_MinusLogProbMetric: 17.4023 - lr: 3.3333e-04 - 82s/epoch - 421ms/step
Epoch 235/1000
2023-09-28 12:05:38.081 
Epoch 235/1000 
	 loss: 17.0629, MinusLogProbMetric: 17.0629, val_loss: 17.8016, val_MinusLogProbMetric: 17.8016

Epoch 235: val_loss did not improve from 17.40233
196/196 - 82s - loss: 17.0629 - MinusLogProbMetric: 17.0629 - val_loss: 17.8016 - val_MinusLogProbMetric: 17.8016 - lr: 3.3333e-04 - 82s/epoch - 418ms/step
Epoch 236/1000
2023-09-28 12:06:52.180 
Epoch 236/1000 
	 loss: 17.0420, MinusLogProbMetric: 17.0420, val_loss: 17.6066, val_MinusLogProbMetric: 17.6066

Epoch 236: val_loss did not improve from 17.40233
196/196 - 74s - loss: 17.0420 - MinusLogProbMetric: 17.0420 - val_loss: 17.6066 - val_MinusLogProbMetric: 17.6066 - lr: 3.3333e-04 - 74s/epoch - 378ms/step
Epoch 237/1000
2023-09-28 12:08:03.067 
Epoch 237/1000 
	 loss: 16.9831, MinusLogProbMetric: 16.9831, val_loss: 17.4344, val_MinusLogProbMetric: 17.4344

Epoch 237: val_loss did not improve from 17.40233
196/196 - 71s - loss: 16.9831 - MinusLogProbMetric: 16.9831 - val_loss: 17.4344 - val_MinusLogProbMetric: 17.4344 - lr: 3.3333e-04 - 71s/epoch - 362ms/step
Epoch 238/1000
2023-09-28 12:09:14.098 
Epoch 238/1000 
	 loss: 16.9893, MinusLogProbMetric: 16.9893, val_loss: 18.0144, val_MinusLogProbMetric: 18.0144

Epoch 238: val_loss did not improve from 17.40233
196/196 - 71s - loss: 16.9893 - MinusLogProbMetric: 16.9893 - val_loss: 18.0144 - val_MinusLogProbMetric: 18.0144 - lr: 3.3333e-04 - 71s/epoch - 362ms/step
Epoch 239/1000
2023-09-28 12:10:27.757 
Epoch 239/1000 
	 loss: 17.0135, MinusLogProbMetric: 17.0135, val_loss: 17.7008, val_MinusLogProbMetric: 17.7008

Epoch 239: val_loss did not improve from 17.40233
196/196 - 74s - loss: 17.0135 - MinusLogProbMetric: 17.0135 - val_loss: 17.7008 - val_MinusLogProbMetric: 17.7008 - lr: 3.3333e-04 - 74s/epoch - 376ms/step
Epoch 240/1000
2023-09-28 12:11:39.128 
Epoch 240/1000 
	 loss: 16.9947, MinusLogProbMetric: 16.9947, val_loss: 17.5314, val_MinusLogProbMetric: 17.5314

Epoch 240: val_loss did not improve from 17.40233
196/196 - 71s - loss: 16.9947 - MinusLogProbMetric: 16.9947 - val_loss: 17.5314 - val_MinusLogProbMetric: 17.5314 - lr: 3.3333e-04 - 71s/epoch - 364ms/step
Epoch 241/1000
2023-09-28 12:12:54.388 
Epoch 241/1000 
	 loss: 17.0274, MinusLogProbMetric: 17.0274, val_loss: 17.8994, val_MinusLogProbMetric: 17.8994

Epoch 241: val_loss did not improve from 17.40233
196/196 - 75s - loss: 17.0274 - MinusLogProbMetric: 17.0274 - val_loss: 17.8994 - val_MinusLogProbMetric: 17.8994 - lr: 3.3333e-04 - 75s/epoch - 384ms/step
Epoch 242/1000
2023-09-28 12:14:10.831 
Epoch 242/1000 
	 loss: 17.0083, MinusLogProbMetric: 17.0083, val_loss: 17.6084, val_MinusLogProbMetric: 17.6084

Epoch 242: val_loss did not improve from 17.40233
196/196 - 76s - loss: 17.0083 - MinusLogProbMetric: 17.0083 - val_loss: 17.6084 - val_MinusLogProbMetric: 17.6084 - lr: 3.3333e-04 - 76s/epoch - 390ms/step
Epoch 243/1000
2023-09-28 12:15:23.776 
Epoch 243/1000 
	 loss: 16.9405, MinusLogProbMetric: 16.9405, val_loss: 18.0438, val_MinusLogProbMetric: 18.0438

Epoch 243: val_loss did not improve from 17.40233
196/196 - 73s - loss: 16.9405 - MinusLogProbMetric: 16.9405 - val_loss: 18.0438 - val_MinusLogProbMetric: 18.0438 - lr: 3.3333e-04 - 73s/epoch - 372ms/step
Epoch 244/1000
2023-09-28 12:16:35.960 
Epoch 244/1000 
	 loss: 16.9693, MinusLogProbMetric: 16.9693, val_loss: 17.5046, val_MinusLogProbMetric: 17.5046

Epoch 244: val_loss did not improve from 17.40233
196/196 - 72s - loss: 16.9693 - MinusLogProbMetric: 16.9693 - val_loss: 17.5046 - val_MinusLogProbMetric: 17.5046 - lr: 3.3333e-04 - 72s/epoch - 368ms/step
Epoch 245/1000
2023-09-28 12:17:48.220 
Epoch 245/1000 
	 loss: 16.9626, MinusLogProbMetric: 16.9626, val_loss: 17.6714, val_MinusLogProbMetric: 17.6714

Epoch 245: val_loss did not improve from 17.40233
196/196 - 72s - loss: 16.9626 - MinusLogProbMetric: 16.9626 - val_loss: 17.6714 - val_MinusLogProbMetric: 17.6714 - lr: 3.3333e-04 - 72s/epoch - 369ms/step
Epoch 246/1000
2023-09-28 12:18:59.279 
Epoch 246/1000 
	 loss: 16.9889, MinusLogProbMetric: 16.9889, val_loss: 17.5808, val_MinusLogProbMetric: 17.5808

Epoch 246: val_loss did not improve from 17.40233
196/196 - 71s - loss: 16.9889 - MinusLogProbMetric: 16.9889 - val_loss: 17.5808 - val_MinusLogProbMetric: 17.5808 - lr: 3.3333e-04 - 71s/epoch - 363ms/step
Epoch 247/1000
2023-09-28 12:20:14.569 
Epoch 247/1000 
	 loss: 16.9786, MinusLogProbMetric: 16.9786, val_loss: 17.5633, val_MinusLogProbMetric: 17.5633

Epoch 247: val_loss did not improve from 17.40233
196/196 - 75s - loss: 16.9786 - MinusLogProbMetric: 16.9786 - val_loss: 17.5633 - val_MinusLogProbMetric: 17.5633 - lr: 3.3333e-04 - 75s/epoch - 384ms/step
Epoch 248/1000
2023-09-28 12:21:25.973 
Epoch 248/1000 
	 loss: 16.9443, MinusLogProbMetric: 16.9443, val_loss: 17.9258, val_MinusLogProbMetric: 17.9258

Epoch 248: val_loss did not improve from 17.40233
196/196 - 71s - loss: 16.9443 - MinusLogProbMetric: 16.9443 - val_loss: 17.9258 - val_MinusLogProbMetric: 17.9258 - lr: 3.3333e-04 - 71s/epoch - 364ms/step
Epoch 249/1000
2023-09-28 12:22:48.042 
Epoch 249/1000 
	 loss: 16.9663, MinusLogProbMetric: 16.9663, val_loss: 17.6096, val_MinusLogProbMetric: 17.6096

Epoch 249: val_loss did not improve from 17.40233
196/196 - 82s - loss: 16.9663 - MinusLogProbMetric: 16.9663 - val_loss: 17.6096 - val_MinusLogProbMetric: 17.6096 - lr: 3.3333e-04 - 82s/epoch - 419ms/step
Epoch 250/1000
2023-09-28 12:24:14.345 
Epoch 250/1000 
	 loss: 16.9457, MinusLogProbMetric: 16.9457, val_loss: 17.9743, val_MinusLogProbMetric: 17.9743

Epoch 250: val_loss did not improve from 17.40233
196/196 - 86s - loss: 16.9457 - MinusLogProbMetric: 16.9457 - val_loss: 17.9743 - val_MinusLogProbMetric: 17.9743 - lr: 3.3333e-04 - 86s/epoch - 440ms/step
Epoch 251/1000
2023-09-28 12:25:34.715 
Epoch 251/1000 
	 loss: 16.9343, MinusLogProbMetric: 16.9343, val_loss: 17.5954, val_MinusLogProbMetric: 17.5954

Epoch 251: val_loss did not improve from 17.40233
196/196 - 80s - loss: 16.9343 - MinusLogProbMetric: 16.9343 - val_loss: 17.5954 - val_MinusLogProbMetric: 17.5954 - lr: 3.3333e-04 - 80s/epoch - 410ms/step
Epoch 252/1000
2023-09-28 12:26:55.624 
Epoch 252/1000 
	 loss: 16.9255, MinusLogProbMetric: 16.9255, val_loss: 17.5968, val_MinusLogProbMetric: 17.5968

Epoch 252: val_loss did not improve from 17.40233
196/196 - 81s - loss: 16.9255 - MinusLogProbMetric: 16.9255 - val_loss: 17.5968 - val_MinusLogProbMetric: 17.5968 - lr: 3.3333e-04 - 81s/epoch - 413ms/step
Epoch 253/1000
2023-09-28 12:28:18.125 
Epoch 253/1000 
	 loss: 16.9765, MinusLogProbMetric: 16.9765, val_loss: 17.5219, val_MinusLogProbMetric: 17.5219

Epoch 253: val_loss did not improve from 17.40233
196/196 - 82s - loss: 16.9765 - MinusLogProbMetric: 16.9765 - val_loss: 17.5219 - val_MinusLogProbMetric: 17.5219 - lr: 3.3333e-04 - 82s/epoch - 421ms/step
Epoch 254/1000
2023-09-28 12:29:40.005 
Epoch 254/1000 
	 loss: 16.9220, MinusLogProbMetric: 16.9220, val_loss: 18.5342, val_MinusLogProbMetric: 18.5342

Epoch 254: val_loss did not improve from 17.40233
196/196 - 82s - loss: 16.9220 - MinusLogProbMetric: 16.9220 - val_loss: 18.5342 - val_MinusLogProbMetric: 18.5342 - lr: 3.3333e-04 - 82s/epoch - 418ms/step
Epoch 255/1000
2023-09-28 12:31:06.111 
Epoch 255/1000 
	 loss: 16.9721, MinusLogProbMetric: 16.9721, val_loss: 17.4969, val_MinusLogProbMetric: 17.4969

Epoch 255: val_loss did not improve from 17.40233
196/196 - 86s - loss: 16.9721 - MinusLogProbMetric: 16.9721 - val_loss: 17.4969 - val_MinusLogProbMetric: 17.4969 - lr: 3.3333e-04 - 86s/epoch - 439ms/step
Epoch 256/1000
2023-09-28 12:32:31.985 
Epoch 256/1000 
	 loss: 16.9291, MinusLogProbMetric: 16.9291, val_loss: 18.2707, val_MinusLogProbMetric: 18.2707

Epoch 256: val_loss did not improve from 17.40233
196/196 - 86s - loss: 16.9291 - MinusLogProbMetric: 16.9291 - val_loss: 18.2707 - val_MinusLogProbMetric: 18.2707 - lr: 3.3333e-04 - 86s/epoch - 438ms/step
Epoch 257/1000
2023-09-28 12:33:58.180 
Epoch 257/1000 
	 loss: 16.9613, MinusLogProbMetric: 16.9613, val_loss: 17.7641, val_MinusLogProbMetric: 17.7641

Epoch 257: val_loss did not improve from 17.40233
196/196 - 86s - loss: 16.9613 - MinusLogProbMetric: 16.9613 - val_loss: 17.7641 - val_MinusLogProbMetric: 17.7641 - lr: 3.3333e-04 - 86s/epoch - 440ms/step
Epoch 258/1000
2023-09-28 12:35:23.770 
Epoch 258/1000 
	 loss: 16.9084, MinusLogProbMetric: 16.9084, val_loss: 17.7976, val_MinusLogProbMetric: 17.7976

Epoch 258: val_loss did not improve from 17.40233
196/196 - 86s - loss: 16.9084 - MinusLogProbMetric: 16.9084 - val_loss: 17.7976 - val_MinusLogProbMetric: 17.7976 - lr: 3.3333e-04 - 86s/epoch - 437ms/step
Epoch 259/1000
2023-09-28 12:36:48.976 
Epoch 259/1000 
	 loss: 16.9586, MinusLogProbMetric: 16.9586, val_loss: 17.5367, val_MinusLogProbMetric: 17.5367

Epoch 259: val_loss did not improve from 17.40233
196/196 - 85s - loss: 16.9586 - MinusLogProbMetric: 16.9586 - val_loss: 17.5367 - val_MinusLogProbMetric: 17.5367 - lr: 3.3333e-04 - 85s/epoch - 435ms/step
Epoch 260/1000
2023-09-28 12:38:13.756 
Epoch 260/1000 
	 loss: 16.9001, MinusLogProbMetric: 16.9001, val_loss: 17.4672, val_MinusLogProbMetric: 17.4672

Epoch 260: val_loss did not improve from 17.40233
196/196 - 85s - loss: 16.9001 - MinusLogProbMetric: 16.9001 - val_loss: 17.4672 - val_MinusLogProbMetric: 17.4672 - lr: 3.3333e-04 - 85s/epoch - 433ms/step
Epoch 261/1000
2023-09-28 12:39:39.408 
Epoch 261/1000 
	 loss: 16.8837, MinusLogProbMetric: 16.8837, val_loss: 17.4462, val_MinusLogProbMetric: 17.4462

Epoch 261: val_loss did not improve from 17.40233
196/196 - 86s - loss: 16.8837 - MinusLogProbMetric: 16.8837 - val_loss: 17.4462 - val_MinusLogProbMetric: 17.4462 - lr: 3.3333e-04 - 86s/epoch - 437ms/step
Epoch 262/1000
2023-09-28 12:41:05.726 
Epoch 262/1000 
	 loss: 17.0060, MinusLogProbMetric: 17.0060, val_loss: 17.7673, val_MinusLogProbMetric: 17.7673

Epoch 262: val_loss did not improve from 17.40233
196/196 - 86s - loss: 17.0060 - MinusLogProbMetric: 17.0060 - val_loss: 17.7673 - val_MinusLogProbMetric: 17.7673 - lr: 3.3333e-04 - 86s/epoch - 440ms/step
Epoch 263/1000
2023-09-28 12:42:31.777 
Epoch 263/1000 
	 loss: 16.8866, MinusLogProbMetric: 16.8866, val_loss: 17.4405, val_MinusLogProbMetric: 17.4405

Epoch 263: val_loss did not improve from 17.40233
196/196 - 86s - loss: 16.8866 - MinusLogProbMetric: 16.8866 - val_loss: 17.4405 - val_MinusLogProbMetric: 17.4405 - lr: 3.3333e-04 - 86s/epoch - 439ms/step
Epoch 264/1000
2023-09-28 12:43:57.272 
Epoch 264/1000 
	 loss: 16.8742, MinusLogProbMetric: 16.8742, val_loss: 17.6512, val_MinusLogProbMetric: 17.6512

Epoch 264: val_loss did not improve from 17.40233
196/196 - 85s - loss: 16.8742 - MinusLogProbMetric: 16.8742 - val_loss: 17.6512 - val_MinusLogProbMetric: 17.6512 - lr: 3.3333e-04 - 85s/epoch - 436ms/step
Epoch 265/1000
2023-09-28 12:45:23.178 
Epoch 265/1000 
	 loss: 16.9054, MinusLogProbMetric: 16.9054, val_loss: 17.4936, val_MinusLogProbMetric: 17.4936

Epoch 265: val_loss did not improve from 17.40233
196/196 - 86s - loss: 16.9054 - MinusLogProbMetric: 16.9054 - val_loss: 17.4936 - val_MinusLogProbMetric: 17.4936 - lr: 3.3333e-04 - 86s/epoch - 438ms/step
Epoch 266/1000
2023-09-28 12:46:48.626 
Epoch 266/1000 
	 loss: 16.9624, MinusLogProbMetric: 16.9624, val_loss: 17.4946, val_MinusLogProbMetric: 17.4946

Epoch 266: val_loss did not improve from 17.40233
196/196 - 85s - loss: 16.9624 - MinusLogProbMetric: 16.9624 - val_loss: 17.4946 - val_MinusLogProbMetric: 17.4946 - lr: 3.3333e-04 - 85s/epoch - 436ms/step
Epoch 267/1000
2023-09-28 12:48:14.336 
Epoch 267/1000 
	 loss: 16.9011, MinusLogProbMetric: 16.9011, val_loss: 17.4772, val_MinusLogProbMetric: 17.4772

Epoch 267: val_loss did not improve from 17.40233
196/196 - 86s - loss: 16.9011 - MinusLogProbMetric: 16.9011 - val_loss: 17.4772 - val_MinusLogProbMetric: 17.4772 - lr: 3.3333e-04 - 86s/epoch - 437ms/step
Epoch 268/1000
2023-09-28 12:49:39.398 
Epoch 268/1000 
	 loss: 16.9077, MinusLogProbMetric: 16.9077, val_loss: 17.5514, val_MinusLogProbMetric: 17.5514

Epoch 268: val_loss did not improve from 17.40233
196/196 - 85s - loss: 16.9077 - MinusLogProbMetric: 16.9077 - val_loss: 17.5514 - val_MinusLogProbMetric: 17.5514 - lr: 3.3333e-04 - 85s/epoch - 434ms/step
Epoch 269/1000
2023-09-28 12:51:03.764 
Epoch 269/1000 
	 loss: 16.9262, MinusLogProbMetric: 16.9262, val_loss: 18.1890, val_MinusLogProbMetric: 18.1890

Epoch 269: val_loss did not improve from 17.40233
196/196 - 84s - loss: 16.9262 - MinusLogProbMetric: 16.9262 - val_loss: 18.1890 - val_MinusLogProbMetric: 18.1890 - lr: 3.3333e-04 - 84s/epoch - 430ms/step
Epoch 270/1000
2023-09-28 12:52:28.867 
Epoch 270/1000 
	 loss: 16.9481, MinusLogProbMetric: 16.9481, val_loss: 18.2698, val_MinusLogProbMetric: 18.2698

Epoch 270: val_loss did not improve from 17.40233
196/196 - 85s - loss: 16.9481 - MinusLogProbMetric: 16.9481 - val_loss: 18.2698 - val_MinusLogProbMetric: 18.2698 - lr: 3.3333e-04 - 85s/epoch - 434ms/step
Epoch 271/1000
2023-09-28 12:53:54.072 
Epoch 271/1000 
	 loss: 16.9409, MinusLogProbMetric: 16.9409, val_loss: 17.6066, val_MinusLogProbMetric: 17.6066

Epoch 271: val_loss did not improve from 17.40233
196/196 - 85s - loss: 16.9409 - MinusLogProbMetric: 16.9409 - val_loss: 17.6066 - val_MinusLogProbMetric: 17.6066 - lr: 3.3333e-04 - 85s/epoch - 435ms/step
Epoch 272/1000
2023-09-28 12:55:19.750 
Epoch 272/1000 
	 loss: 16.8858, MinusLogProbMetric: 16.8858, val_loss: 17.4072, val_MinusLogProbMetric: 17.4072

Epoch 272: val_loss did not improve from 17.40233
196/196 - 86s - loss: 16.8858 - MinusLogProbMetric: 16.8858 - val_loss: 17.4072 - val_MinusLogProbMetric: 17.4072 - lr: 3.3333e-04 - 86s/epoch - 437ms/step
Epoch 273/1000
2023-09-28 12:56:44.839 
Epoch 273/1000 
	 loss: 16.8669, MinusLogProbMetric: 16.8669, val_loss: 17.6581, val_MinusLogProbMetric: 17.6581

Epoch 273: val_loss did not improve from 17.40233
196/196 - 85s - loss: 16.8669 - MinusLogProbMetric: 16.8669 - val_loss: 17.6581 - val_MinusLogProbMetric: 17.6581 - lr: 3.3333e-04 - 85s/epoch - 434ms/step
Epoch 274/1000
2023-09-28 12:58:10.116 
Epoch 274/1000 
	 loss: 16.8359, MinusLogProbMetric: 16.8359, val_loss: 17.4153, val_MinusLogProbMetric: 17.4153

Epoch 274: val_loss did not improve from 17.40233
196/196 - 85s - loss: 16.8359 - MinusLogProbMetric: 16.8359 - val_loss: 17.4153 - val_MinusLogProbMetric: 17.4153 - lr: 3.3333e-04 - 85s/epoch - 435ms/step
Epoch 275/1000
2023-09-28 12:59:35.728 
Epoch 275/1000 
	 loss: 16.9023, MinusLogProbMetric: 16.9023, val_loss: 17.4867, val_MinusLogProbMetric: 17.4867

Epoch 275: val_loss did not improve from 17.40233
196/196 - 86s - loss: 16.9023 - MinusLogProbMetric: 16.9023 - val_loss: 17.4867 - val_MinusLogProbMetric: 17.4867 - lr: 3.3333e-04 - 86s/epoch - 437ms/step
Epoch 276/1000
2023-09-28 13:01:00.632 
Epoch 276/1000 
	 loss: 16.8434, MinusLogProbMetric: 16.8434, val_loss: 17.8165, val_MinusLogProbMetric: 17.8165

Epoch 276: val_loss did not improve from 17.40233
196/196 - 85s - loss: 16.8434 - MinusLogProbMetric: 16.8434 - val_loss: 17.8165 - val_MinusLogProbMetric: 17.8165 - lr: 3.3333e-04 - 85s/epoch - 433ms/step
Epoch 277/1000
2023-09-28 13:02:25.530 
Epoch 277/1000 
	 loss: 16.9386, MinusLogProbMetric: 16.9386, val_loss: 17.6615, val_MinusLogProbMetric: 17.6615

Epoch 277: val_loss did not improve from 17.40233
196/196 - 85s - loss: 16.9386 - MinusLogProbMetric: 16.9386 - val_loss: 17.6615 - val_MinusLogProbMetric: 17.6615 - lr: 3.3333e-04 - 85s/epoch - 433ms/step
Epoch 278/1000
2023-09-28 13:03:51.307 
Epoch 278/1000 
	 loss: 16.8147, MinusLogProbMetric: 16.8147, val_loss: 17.6283, val_MinusLogProbMetric: 17.6283

Epoch 278: val_loss did not improve from 17.40233
196/196 - 86s - loss: 16.8147 - MinusLogProbMetric: 16.8147 - val_loss: 17.6283 - val_MinusLogProbMetric: 17.6283 - lr: 3.3333e-04 - 86s/epoch - 438ms/step
Epoch 279/1000
2023-09-28 13:05:16.542 
Epoch 279/1000 
	 loss: 16.8645, MinusLogProbMetric: 16.8645, val_loss: 17.7512, val_MinusLogProbMetric: 17.7512

Epoch 279: val_loss did not improve from 17.40233
196/196 - 85s - loss: 16.8645 - MinusLogProbMetric: 16.8645 - val_loss: 17.7512 - val_MinusLogProbMetric: 17.7512 - lr: 3.3333e-04 - 85s/epoch - 435ms/step
Epoch 280/1000
2023-09-28 13:06:41.289 
Epoch 280/1000 
	 loss: 16.8785, MinusLogProbMetric: 16.8785, val_loss: 17.5833, val_MinusLogProbMetric: 17.5833

Epoch 280: val_loss did not improve from 17.40233
196/196 - 85s - loss: 16.8785 - MinusLogProbMetric: 16.8785 - val_loss: 17.5833 - val_MinusLogProbMetric: 17.5833 - lr: 3.3333e-04 - 85s/epoch - 432ms/step
Epoch 281/1000
2023-09-28 13:08:00.944 
Epoch 281/1000 
	 loss: 16.8732, MinusLogProbMetric: 16.8732, val_loss: 17.5572, val_MinusLogProbMetric: 17.5572

Epoch 281: val_loss did not improve from 17.40233
196/196 - 80s - loss: 16.8732 - MinusLogProbMetric: 16.8732 - val_loss: 17.5572 - val_MinusLogProbMetric: 17.5572 - lr: 3.3333e-04 - 80s/epoch - 406ms/step
Epoch 282/1000
2023-09-28 13:09:21.797 
Epoch 282/1000 
	 loss: 16.8703, MinusLogProbMetric: 16.8703, val_loss: 17.8570, val_MinusLogProbMetric: 17.8570

Epoch 282: val_loss did not improve from 17.40233
196/196 - 81s - loss: 16.8703 - MinusLogProbMetric: 16.8703 - val_loss: 17.8570 - val_MinusLogProbMetric: 17.8570 - lr: 3.3333e-04 - 81s/epoch - 412ms/step
Epoch 283/1000
2023-09-28 13:10:41.215 
Epoch 283/1000 
	 loss: 16.8635, MinusLogProbMetric: 16.8635, val_loss: 18.2985, val_MinusLogProbMetric: 18.2985

Epoch 283: val_loss did not improve from 17.40233
196/196 - 79s - loss: 16.8635 - MinusLogProbMetric: 16.8635 - val_loss: 18.2985 - val_MinusLogProbMetric: 18.2985 - lr: 3.3333e-04 - 79s/epoch - 405ms/step
Epoch 284/1000
2023-09-28 13:12:02.261 
Epoch 284/1000 
	 loss: 16.8968, MinusLogProbMetric: 16.8968, val_loss: 18.0077, val_MinusLogProbMetric: 18.0077

Epoch 284: val_loss did not improve from 17.40233
196/196 - 81s - loss: 16.8968 - MinusLogProbMetric: 16.8968 - val_loss: 18.0077 - val_MinusLogProbMetric: 18.0077 - lr: 3.3333e-04 - 81s/epoch - 413ms/step
Epoch 285/1000
2023-09-28 13:13:22.085 
Epoch 285/1000 
	 loss: 16.4980, MinusLogProbMetric: 16.4980, val_loss: 17.3125, val_MinusLogProbMetric: 17.3125

Epoch 285: val_loss improved from 17.40233 to 17.31254, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_320/weights/best_weights.h5
196/196 - 81s - loss: 16.4980 - MinusLogProbMetric: 16.4980 - val_loss: 17.3125 - val_MinusLogProbMetric: 17.3125 - lr: 1.6667e-04 - 81s/epoch - 414ms/step
Epoch 286/1000
2023-09-28 13:14:46.821 
Epoch 286/1000 
	 loss: 16.4898, MinusLogProbMetric: 16.4898, val_loss: 17.2829, val_MinusLogProbMetric: 17.2829

Epoch 286: val_loss improved from 17.31254 to 17.28291, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_320/weights/best_weights.h5
196/196 - 85s - loss: 16.4898 - MinusLogProbMetric: 16.4898 - val_loss: 17.2829 - val_MinusLogProbMetric: 17.2829 - lr: 1.6667e-04 - 85s/epoch - 433ms/step
Epoch 287/1000
2023-09-28 13:16:12.305 
Epoch 287/1000 
	 loss: 16.4813, MinusLogProbMetric: 16.4813, val_loss: 17.2183, val_MinusLogProbMetric: 17.2183

Epoch 287: val_loss improved from 17.28291 to 17.21829, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_320/weights/best_weights.h5
196/196 - 86s - loss: 16.4813 - MinusLogProbMetric: 16.4813 - val_loss: 17.2183 - val_MinusLogProbMetric: 17.2183 - lr: 1.6667e-04 - 86s/epoch - 437ms/step
Epoch 288/1000
2023-09-28 13:17:38.060 
Epoch 288/1000 
	 loss: 16.4852, MinusLogProbMetric: 16.4852, val_loss: 17.3446, val_MinusLogProbMetric: 17.3446

Epoch 288: val_loss did not improve from 17.21829
196/196 - 84s - loss: 16.4852 - MinusLogProbMetric: 16.4852 - val_loss: 17.3446 - val_MinusLogProbMetric: 17.3446 - lr: 1.6667e-04 - 84s/epoch - 430ms/step
Epoch 289/1000
2023-09-28 13:18:58.218 
Epoch 289/1000 
	 loss: 16.5145, MinusLogProbMetric: 16.5145, val_loss: 17.5965, val_MinusLogProbMetric: 17.5965

Epoch 289: val_loss did not improve from 17.21829
196/196 - 80s - loss: 16.5145 - MinusLogProbMetric: 16.5145 - val_loss: 17.5965 - val_MinusLogProbMetric: 17.5965 - lr: 1.6667e-04 - 80s/epoch - 409ms/step
Epoch 290/1000
2023-09-28 13:20:21.007 
Epoch 290/1000 
	 loss: 16.4861, MinusLogProbMetric: 16.4861, val_loss: 17.2576, val_MinusLogProbMetric: 17.2576

Epoch 290: val_loss did not improve from 17.21829
196/196 - 83s - loss: 16.4861 - MinusLogProbMetric: 16.4861 - val_loss: 17.2576 - val_MinusLogProbMetric: 17.2576 - lr: 1.6667e-04 - 83s/epoch - 422ms/step
Epoch 291/1000
2023-09-28 13:21:43.635 
Epoch 291/1000 
	 loss: 16.4553, MinusLogProbMetric: 16.4553, val_loss: 17.2409, val_MinusLogProbMetric: 17.2409

Epoch 291: val_loss did not improve from 17.21829
196/196 - 83s - loss: 16.4553 - MinusLogProbMetric: 16.4553 - val_loss: 17.2409 - val_MinusLogProbMetric: 17.2409 - lr: 1.6667e-04 - 83s/epoch - 422ms/step
Epoch 292/1000
2023-09-28 13:23:10.243 
Epoch 292/1000 
	 loss: 16.4847, MinusLogProbMetric: 16.4847, val_loss: 17.3521, val_MinusLogProbMetric: 17.3521

Epoch 292: val_loss did not improve from 17.21829
196/196 - 87s - loss: 16.4847 - MinusLogProbMetric: 16.4847 - val_loss: 17.3521 - val_MinusLogProbMetric: 17.3521 - lr: 1.6667e-04 - 87s/epoch - 442ms/step
Epoch 293/1000
2023-09-28 13:24:33.652 
Epoch 293/1000 
	 loss: 16.4947, MinusLogProbMetric: 16.4947, val_loss: 17.2594, val_MinusLogProbMetric: 17.2594

Epoch 293: val_loss did not improve from 17.21829
196/196 - 83s - loss: 16.4947 - MinusLogProbMetric: 16.4947 - val_loss: 17.2594 - val_MinusLogProbMetric: 17.2594 - lr: 1.6667e-04 - 83s/epoch - 426ms/step
Epoch 294/1000
2023-09-28 13:25:56.115 
Epoch 294/1000 
	 loss: 16.4875, MinusLogProbMetric: 16.4875, val_loss: 17.2499, val_MinusLogProbMetric: 17.2499

Epoch 294: val_loss did not improve from 17.21829
196/196 - 82s - loss: 16.4875 - MinusLogProbMetric: 16.4875 - val_loss: 17.2499 - val_MinusLogProbMetric: 17.2499 - lr: 1.6667e-04 - 82s/epoch - 421ms/step
Epoch 295/1000
2023-09-28 13:27:23.164 
Epoch 295/1000 
	 loss: 16.4727, MinusLogProbMetric: 16.4727, val_loss: 17.3942, val_MinusLogProbMetric: 17.3942

Epoch 295: val_loss did not improve from 17.21829
196/196 - 87s - loss: 16.4727 - MinusLogProbMetric: 16.4727 - val_loss: 17.3942 - val_MinusLogProbMetric: 17.3942 - lr: 1.6667e-04 - 87s/epoch - 444ms/step
Epoch 296/1000
2023-09-28 13:28:50.529 
Epoch 296/1000 
	 loss: 16.4728, MinusLogProbMetric: 16.4728, val_loss: 17.3685, val_MinusLogProbMetric: 17.3685

Epoch 296: val_loss did not improve from 17.21829
196/196 - 87s - loss: 16.4728 - MinusLogProbMetric: 16.4728 - val_loss: 17.3685 - val_MinusLogProbMetric: 17.3685 - lr: 1.6667e-04 - 87s/epoch - 446ms/step
Epoch 297/1000
2023-09-28 13:30:14.389 
Epoch 297/1000 
	 loss: 16.4951, MinusLogProbMetric: 16.4951, val_loss: 17.2583, val_MinusLogProbMetric: 17.2583

Epoch 297: val_loss did not improve from 17.21829
196/196 - 84s - loss: 16.4951 - MinusLogProbMetric: 16.4951 - val_loss: 17.2583 - val_MinusLogProbMetric: 17.2583 - lr: 1.6667e-04 - 84s/epoch - 428ms/step
Epoch 298/1000
2023-09-28 13:31:40.946 
Epoch 298/1000 
	 loss: 16.4445, MinusLogProbMetric: 16.4445, val_loss: 17.4945, val_MinusLogProbMetric: 17.4945

Epoch 298: val_loss did not improve from 17.21829
196/196 - 87s - loss: 16.4445 - MinusLogProbMetric: 16.4445 - val_loss: 17.4945 - val_MinusLogProbMetric: 17.4945 - lr: 1.6667e-04 - 87s/epoch - 442ms/step
Epoch 299/1000
2023-09-28 13:33:04.309 
Epoch 299/1000 
	 loss: 16.4624, MinusLogProbMetric: 16.4624, val_loss: 17.3891, val_MinusLogProbMetric: 17.3891

Epoch 299: val_loss did not improve from 17.21829
196/196 - 83s - loss: 16.4624 - MinusLogProbMetric: 16.4624 - val_loss: 17.3891 - val_MinusLogProbMetric: 17.3891 - lr: 1.6667e-04 - 83s/epoch - 425ms/step
Epoch 300/1000
2023-09-28 13:34:25.776 
Epoch 300/1000 
	 loss: 16.4698, MinusLogProbMetric: 16.4698, val_loss: 17.2286, val_MinusLogProbMetric: 17.2286

Epoch 300: val_loss did not improve from 17.21829
196/196 - 81s - loss: 16.4698 - MinusLogProbMetric: 16.4698 - val_loss: 17.2286 - val_MinusLogProbMetric: 17.2286 - lr: 1.6667e-04 - 81s/epoch - 416ms/step
Epoch 301/1000
2023-09-28 13:35:46.783 
Epoch 301/1000 
	 loss: 16.4841, MinusLogProbMetric: 16.4841, val_loss: 17.3139, val_MinusLogProbMetric: 17.3139

Epoch 301: val_loss did not improve from 17.21829
196/196 - 81s - loss: 16.4841 - MinusLogProbMetric: 16.4841 - val_loss: 17.3139 - val_MinusLogProbMetric: 17.3139 - lr: 1.6667e-04 - 81s/epoch - 413ms/step
Epoch 302/1000
2023-09-28 13:37:03.826 
Epoch 302/1000 
	 loss: 16.4745, MinusLogProbMetric: 16.4745, val_loss: 18.1597, val_MinusLogProbMetric: 18.1597

Epoch 302: val_loss did not improve from 17.21829
196/196 - 77s - loss: 16.4745 - MinusLogProbMetric: 16.4745 - val_loss: 18.1597 - val_MinusLogProbMetric: 18.1597 - lr: 1.6667e-04 - 77s/epoch - 393ms/step
Epoch 303/1000
2023-09-28 13:38:24.811 
Epoch 303/1000 
	 loss: 16.5009, MinusLogProbMetric: 16.5009, val_loss: 17.5280, val_MinusLogProbMetric: 17.5280

Epoch 303: val_loss did not improve from 17.21829
196/196 - 81s - loss: 16.5009 - MinusLogProbMetric: 16.5009 - val_loss: 17.5280 - val_MinusLogProbMetric: 17.5280 - lr: 1.6667e-04 - 81s/epoch - 413ms/step
Epoch 304/1000
2023-09-28 13:39:45.888 
Epoch 304/1000 
	 loss: 16.4632, MinusLogProbMetric: 16.4632, val_loss: 17.2705, val_MinusLogProbMetric: 17.2705

Epoch 304: val_loss did not improve from 17.21829
196/196 - 81s - loss: 16.4632 - MinusLogProbMetric: 16.4632 - val_loss: 17.2705 - val_MinusLogProbMetric: 17.2705 - lr: 1.6667e-04 - 81s/epoch - 414ms/step
Epoch 305/1000
2023-09-28 13:41:05.648 
Epoch 305/1000 
	 loss: 16.4677, MinusLogProbMetric: 16.4677, val_loss: 17.3458, val_MinusLogProbMetric: 17.3458

Epoch 305: val_loss did not improve from 17.21829
196/196 - 80s - loss: 16.4677 - MinusLogProbMetric: 16.4677 - val_loss: 17.3458 - val_MinusLogProbMetric: 17.3458 - lr: 1.6667e-04 - 80s/epoch - 407ms/step
Epoch 306/1000
2023-09-28 13:42:26.857 
Epoch 306/1000 
	 loss: 16.4524, MinusLogProbMetric: 16.4524, val_loss: 17.4912, val_MinusLogProbMetric: 17.4912

Epoch 306: val_loss did not improve from 17.21829
196/196 - 81s - loss: 16.4524 - MinusLogProbMetric: 16.4524 - val_loss: 17.4912 - val_MinusLogProbMetric: 17.4912 - lr: 1.6667e-04 - 81s/epoch - 414ms/step
Epoch 307/1000
2023-09-28 13:43:48.975 
Epoch 307/1000 
	 loss: 16.4374, MinusLogProbMetric: 16.4374, val_loss: 17.2509, val_MinusLogProbMetric: 17.2509

Epoch 307: val_loss did not improve from 17.21829
196/196 - 82s - loss: 16.4374 - MinusLogProbMetric: 16.4374 - val_loss: 17.2509 - val_MinusLogProbMetric: 17.2509 - lr: 1.6667e-04 - 82s/epoch - 419ms/step
Epoch 308/1000
2023-09-28 13:45:05.797 
Epoch 308/1000 
	 loss: 16.4796, MinusLogProbMetric: 16.4796, val_loss: 17.2709, val_MinusLogProbMetric: 17.2709

Epoch 308: val_loss did not improve from 17.21829
196/196 - 77s - loss: 16.4796 - MinusLogProbMetric: 16.4796 - val_loss: 17.2709 - val_MinusLogProbMetric: 17.2709 - lr: 1.6667e-04 - 77s/epoch - 392ms/step
Epoch 309/1000
2023-09-28 13:46:25.077 
Epoch 309/1000 
	 loss: 16.4592, MinusLogProbMetric: 16.4592, val_loss: 17.5880, val_MinusLogProbMetric: 17.5880

Epoch 309: val_loss did not improve from 17.21829
196/196 - 79s - loss: 16.4592 - MinusLogProbMetric: 16.4592 - val_loss: 17.5880 - val_MinusLogProbMetric: 17.5880 - lr: 1.6667e-04 - 79s/epoch - 404ms/step
Epoch 310/1000
2023-09-28 13:47:49.220 
Epoch 310/1000 
	 loss: 16.4610, MinusLogProbMetric: 16.4610, val_loss: 17.3645, val_MinusLogProbMetric: 17.3645

Epoch 310: val_loss did not improve from 17.21829
196/196 - 84s - loss: 16.4610 - MinusLogProbMetric: 16.4610 - val_loss: 17.3645 - val_MinusLogProbMetric: 17.3645 - lr: 1.6667e-04 - 84s/epoch - 429ms/step
Epoch 311/1000
2023-09-28 13:49:12.733 
Epoch 311/1000 
	 loss: 16.4463, MinusLogProbMetric: 16.4463, val_loss: 17.3372, val_MinusLogProbMetric: 17.3372

Epoch 311: val_loss did not improve from 17.21829
196/196 - 84s - loss: 16.4463 - MinusLogProbMetric: 16.4463 - val_loss: 17.3372 - val_MinusLogProbMetric: 17.3372 - lr: 1.6667e-04 - 84s/epoch - 426ms/step
Epoch 312/1000
2023-09-28 13:50:38.481 
Epoch 312/1000 
	 loss: 16.4880, MinusLogProbMetric: 16.4880, val_loss: 17.2624, val_MinusLogProbMetric: 17.2624

Epoch 312: val_loss did not improve from 17.21829
196/196 - 86s - loss: 16.4880 - MinusLogProbMetric: 16.4880 - val_loss: 17.2624 - val_MinusLogProbMetric: 17.2624 - lr: 1.6667e-04 - 86s/epoch - 437ms/step
Epoch 313/1000
2023-09-28 13:52:03.330 
Epoch 313/1000 
	 loss: 16.4657, MinusLogProbMetric: 16.4657, val_loss: 17.5448, val_MinusLogProbMetric: 17.5448

Epoch 313: val_loss did not improve from 17.21829
196/196 - 85s - loss: 16.4657 - MinusLogProbMetric: 16.4657 - val_loss: 17.5448 - val_MinusLogProbMetric: 17.5448 - lr: 1.6667e-04 - 85s/epoch - 433ms/step
Epoch 314/1000
2023-09-28 13:53:28.623 
Epoch 314/1000 
	 loss: 16.4749, MinusLogProbMetric: 16.4749, val_loss: 17.2494, val_MinusLogProbMetric: 17.2494

Epoch 314: val_loss did not improve from 17.21829
196/196 - 85s - loss: 16.4749 - MinusLogProbMetric: 16.4749 - val_loss: 17.2494 - val_MinusLogProbMetric: 17.2494 - lr: 1.6667e-04 - 85s/epoch - 435ms/step
Epoch 315/1000
2023-09-28 13:54:54.766 
Epoch 315/1000 
	 loss: 16.4423, MinusLogProbMetric: 16.4423, val_loss: 17.2807, val_MinusLogProbMetric: 17.2807

Epoch 315: val_loss did not improve from 17.21829
196/196 - 86s - loss: 16.4423 - MinusLogProbMetric: 16.4423 - val_loss: 17.2807 - val_MinusLogProbMetric: 17.2807 - lr: 1.6667e-04 - 86s/epoch - 439ms/step
Epoch 316/1000
2023-09-28 13:56:20.266 
Epoch 316/1000 
	 loss: 16.4556, MinusLogProbMetric: 16.4556, val_loss: 17.2957, val_MinusLogProbMetric: 17.2957

Epoch 316: val_loss did not improve from 17.21829
196/196 - 85s - loss: 16.4556 - MinusLogProbMetric: 16.4556 - val_loss: 17.2957 - val_MinusLogProbMetric: 17.2957 - lr: 1.6667e-04 - 85s/epoch - 436ms/step
Epoch 317/1000
2023-09-28 13:57:45.310 
Epoch 317/1000 
	 loss: 16.4400, MinusLogProbMetric: 16.4400, val_loss: 17.2674, val_MinusLogProbMetric: 17.2674

Epoch 317: val_loss did not improve from 17.21829
196/196 - 85s - loss: 16.4400 - MinusLogProbMetric: 16.4400 - val_loss: 17.2674 - val_MinusLogProbMetric: 17.2674 - lr: 1.6667e-04 - 85s/epoch - 434ms/step
Epoch 318/1000
2023-09-28 13:59:11.141 
Epoch 318/1000 
	 loss: 16.4542, MinusLogProbMetric: 16.4542, val_loss: 17.3657, val_MinusLogProbMetric: 17.3657

Epoch 318: val_loss did not improve from 17.21829
196/196 - 86s - loss: 16.4542 - MinusLogProbMetric: 16.4542 - val_loss: 17.3657 - val_MinusLogProbMetric: 17.3657 - lr: 1.6667e-04 - 86s/epoch - 438ms/step
Epoch 319/1000
2023-09-28 14:00:36.360 
Epoch 319/1000 
	 loss: 16.4521, MinusLogProbMetric: 16.4521, val_loss: 17.3032, val_MinusLogProbMetric: 17.3032

Epoch 319: val_loss did not improve from 17.21829
196/196 - 85s - loss: 16.4521 - MinusLogProbMetric: 16.4521 - val_loss: 17.3032 - val_MinusLogProbMetric: 17.3032 - lr: 1.6667e-04 - 85s/epoch - 435ms/step
Epoch 320/1000
2023-09-28 14:02:02.626 
Epoch 320/1000 
	 loss: 16.4142, MinusLogProbMetric: 16.4142, val_loss: 17.3931, val_MinusLogProbMetric: 17.3931

Epoch 320: val_loss did not improve from 17.21829
196/196 - 86s - loss: 16.4142 - MinusLogProbMetric: 16.4142 - val_loss: 17.3931 - val_MinusLogProbMetric: 17.3931 - lr: 1.6667e-04 - 86s/epoch - 440ms/step
Epoch 321/1000
2023-09-28 14:03:28.940 
Epoch 321/1000 
	 loss: 16.4141, MinusLogProbMetric: 16.4141, val_loss: 17.2638, val_MinusLogProbMetric: 17.2638

Epoch 321: val_loss did not improve from 17.21829
196/196 - 86s - loss: 16.4141 - MinusLogProbMetric: 16.4141 - val_loss: 17.2638 - val_MinusLogProbMetric: 17.2638 - lr: 1.6667e-04 - 86s/epoch - 440ms/step
Epoch 322/1000
2023-09-28 14:04:54.563 
Epoch 322/1000 
	 loss: 16.4350, MinusLogProbMetric: 16.4350, val_loss: 17.3383, val_MinusLogProbMetric: 17.3383

Epoch 322: val_loss did not improve from 17.21829
196/196 - 86s - loss: 16.4350 - MinusLogProbMetric: 16.4350 - val_loss: 17.3383 - val_MinusLogProbMetric: 17.3383 - lr: 1.6667e-04 - 86s/epoch - 437ms/step
Epoch 323/1000
2023-09-28 14:06:19.982 
Epoch 323/1000 
	 loss: 16.4096, MinusLogProbMetric: 16.4096, val_loss: 17.3335, val_MinusLogProbMetric: 17.3335

Epoch 323: val_loss did not improve from 17.21829
196/196 - 85s - loss: 16.4096 - MinusLogProbMetric: 16.4096 - val_loss: 17.3335 - val_MinusLogProbMetric: 17.3335 - lr: 1.6667e-04 - 85s/epoch - 436ms/step
Epoch 324/1000
2023-09-28 14:07:44.824 
Epoch 324/1000 
	 loss: 16.4417, MinusLogProbMetric: 16.4417, val_loss: 17.3396, val_MinusLogProbMetric: 17.3396

Epoch 324: val_loss did not improve from 17.21829
196/196 - 85s - loss: 16.4417 - MinusLogProbMetric: 16.4417 - val_loss: 17.3396 - val_MinusLogProbMetric: 17.3396 - lr: 1.6667e-04 - 85s/epoch - 433ms/step
Epoch 325/1000
2023-09-28 14:09:09.886 
Epoch 325/1000 
	 loss: 16.4412, MinusLogProbMetric: 16.4412, val_loss: 17.2451, val_MinusLogProbMetric: 17.2451

Epoch 325: val_loss did not improve from 17.21829
196/196 - 85s - loss: 16.4412 - MinusLogProbMetric: 16.4412 - val_loss: 17.2451 - val_MinusLogProbMetric: 17.2451 - lr: 1.6667e-04 - 85s/epoch - 434ms/step
Epoch 326/1000
2023-09-28 14:10:36.107 
Epoch 326/1000 
	 loss: 16.4491, MinusLogProbMetric: 16.4491, val_loss: 17.2656, val_MinusLogProbMetric: 17.2656

Epoch 326: val_loss did not improve from 17.21829
196/196 - 86s - loss: 16.4491 - MinusLogProbMetric: 16.4491 - val_loss: 17.2656 - val_MinusLogProbMetric: 17.2656 - lr: 1.6667e-04 - 86s/epoch - 440ms/step
Epoch 327/1000
2023-09-28 14:12:01.143 
Epoch 327/1000 
	 loss: 16.4285, MinusLogProbMetric: 16.4285, val_loss: 17.3581, val_MinusLogProbMetric: 17.3581

Epoch 327: val_loss did not improve from 17.21829
196/196 - 85s - loss: 16.4285 - MinusLogProbMetric: 16.4285 - val_loss: 17.3581 - val_MinusLogProbMetric: 17.3581 - lr: 1.6667e-04 - 85s/epoch - 434ms/step
Epoch 328/1000
2023-09-28 14:13:27.186 
Epoch 328/1000 
	 loss: 16.4514, MinusLogProbMetric: 16.4514, val_loss: 17.4554, val_MinusLogProbMetric: 17.4554

Epoch 328: val_loss did not improve from 17.21829
196/196 - 86s - loss: 16.4514 - MinusLogProbMetric: 16.4514 - val_loss: 17.4554 - val_MinusLogProbMetric: 17.4554 - lr: 1.6667e-04 - 86s/epoch - 439ms/step
Epoch 329/1000
2023-09-28 14:14:51.727 
Epoch 329/1000 
	 loss: 16.4524, MinusLogProbMetric: 16.4524, val_loss: 17.2874, val_MinusLogProbMetric: 17.2874

Epoch 329: val_loss did not improve from 17.21829
196/196 - 85s - loss: 16.4524 - MinusLogProbMetric: 16.4524 - val_loss: 17.2874 - val_MinusLogProbMetric: 17.2874 - lr: 1.6667e-04 - 85s/epoch - 431ms/step
Epoch 330/1000
2023-09-28 14:16:18.449 
Epoch 330/1000 
	 loss: 16.4226, MinusLogProbMetric: 16.4226, val_loss: 17.3476, val_MinusLogProbMetric: 17.3476

Epoch 330: val_loss did not improve from 17.21829
196/196 - 87s - loss: 16.4226 - MinusLogProbMetric: 16.4226 - val_loss: 17.3476 - val_MinusLogProbMetric: 17.3476 - lr: 1.6667e-04 - 87s/epoch - 442ms/step
Epoch 331/1000
2023-09-28 14:17:43.677 
Epoch 331/1000 
	 loss: 16.4445, MinusLogProbMetric: 16.4445, val_loss: 17.4026, val_MinusLogProbMetric: 17.4026

Epoch 331: val_loss did not improve from 17.21829
196/196 - 85s - loss: 16.4445 - MinusLogProbMetric: 16.4445 - val_loss: 17.4026 - val_MinusLogProbMetric: 17.4026 - lr: 1.6667e-04 - 85s/epoch - 435ms/step
Epoch 332/1000
2023-09-28 14:19:10.462 
Epoch 332/1000 
	 loss: 16.4580, MinusLogProbMetric: 16.4580, val_loss: 17.4728, val_MinusLogProbMetric: 17.4728

Epoch 332: val_loss did not improve from 17.21829
196/196 - 87s - loss: 16.4580 - MinusLogProbMetric: 16.4580 - val_loss: 17.4728 - val_MinusLogProbMetric: 17.4728 - lr: 1.6667e-04 - 87s/epoch - 443ms/step
Epoch 333/1000
2023-09-28 14:20:35.166 
Epoch 333/1000 
	 loss: 16.4120, MinusLogProbMetric: 16.4120, val_loss: 17.3427, val_MinusLogProbMetric: 17.3427

Epoch 333: val_loss did not improve from 17.21829
196/196 - 85s - loss: 16.4120 - MinusLogProbMetric: 16.4120 - val_loss: 17.3427 - val_MinusLogProbMetric: 17.3427 - lr: 1.6667e-04 - 85s/epoch - 432ms/step
Epoch 334/1000
2023-09-28 14:21:55.697 
Epoch 334/1000 
	 loss: 16.4033, MinusLogProbMetric: 16.4033, val_loss: 17.5181, val_MinusLogProbMetric: 17.5181

Epoch 334: val_loss did not improve from 17.21829
196/196 - 81s - loss: 16.4033 - MinusLogProbMetric: 16.4033 - val_loss: 17.5181 - val_MinusLogProbMetric: 17.5181 - lr: 1.6667e-04 - 81s/epoch - 411ms/step
Epoch 335/1000
2023-09-28 14:23:20.827 
Epoch 335/1000 
	 loss: 16.4591, MinusLogProbMetric: 16.4591, val_loss: 17.2513, val_MinusLogProbMetric: 17.2513

Epoch 335: val_loss did not improve from 17.21829
196/196 - 85s - loss: 16.4591 - MinusLogProbMetric: 16.4591 - val_loss: 17.2513 - val_MinusLogProbMetric: 17.2513 - lr: 1.6667e-04 - 85s/epoch - 434ms/step
Epoch 336/1000
2023-09-28 14:24:41.688 
Epoch 336/1000 
	 loss: 16.4257, MinusLogProbMetric: 16.4257, val_loss: 17.2569, val_MinusLogProbMetric: 17.2569

Epoch 336: val_loss did not improve from 17.21829
196/196 - 81s - loss: 16.4257 - MinusLogProbMetric: 16.4257 - val_loss: 17.2569 - val_MinusLogProbMetric: 17.2569 - lr: 1.6667e-04 - 81s/epoch - 413ms/step
Epoch 337/1000
2023-09-28 14:25:57.660 
Epoch 337/1000 
	 loss: 16.4058, MinusLogProbMetric: 16.4058, val_loss: 17.4495, val_MinusLogProbMetric: 17.4495

Epoch 337: val_loss did not improve from 17.21829
196/196 - 76s - loss: 16.4058 - MinusLogProbMetric: 16.4058 - val_loss: 17.4495 - val_MinusLogProbMetric: 17.4495 - lr: 1.6667e-04 - 76s/epoch - 388ms/step
Epoch 338/1000
2023-09-28 14:27:16.741 
Epoch 338/1000 
	 loss: 16.2598, MinusLogProbMetric: 16.2598, val_loss: 17.2552, val_MinusLogProbMetric: 17.2552

Epoch 338: val_loss did not improve from 17.21829
196/196 - 79s - loss: 16.2598 - MinusLogProbMetric: 16.2598 - val_loss: 17.2552 - val_MinusLogProbMetric: 17.2552 - lr: 8.3333e-05 - 79s/epoch - 403ms/step
Epoch 339/1000
2023-09-28 14:28:38.600 
Epoch 339/1000 
	 loss: 16.2503, MinusLogProbMetric: 16.2503, val_loss: 17.2955, val_MinusLogProbMetric: 17.2955

Epoch 339: val_loss did not improve from 17.21829
196/196 - 82s - loss: 16.2503 - MinusLogProbMetric: 16.2503 - val_loss: 17.2955 - val_MinusLogProbMetric: 17.2955 - lr: 8.3333e-05 - 82s/epoch - 418ms/step
Epoch 340/1000
2023-09-28 14:29:52.644 
Epoch 340/1000 
	 loss: 16.2632, MinusLogProbMetric: 16.2632, val_loss: 17.2072, val_MinusLogProbMetric: 17.2072

Epoch 340: val_loss improved from 17.21829 to 17.20719, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_320/weights/best_weights.h5
196/196 - 75s - loss: 16.2632 - MinusLogProbMetric: 16.2632 - val_loss: 17.2072 - val_MinusLogProbMetric: 17.2072 - lr: 8.3333e-05 - 75s/epoch - 384ms/step
Epoch 341/1000
2023-09-28 14:31:03.751 
Epoch 341/1000 
	 loss: 16.2749, MinusLogProbMetric: 16.2749, val_loss: 17.2046, val_MinusLogProbMetric: 17.2046

Epoch 341: val_loss improved from 17.20719 to 17.20462, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_320/weights/best_weights.h5
196/196 - 71s - loss: 16.2749 - MinusLogProbMetric: 16.2749 - val_loss: 17.2046 - val_MinusLogProbMetric: 17.2046 - lr: 8.3333e-05 - 71s/epoch - 363ms/step
Epoch 342/1000
2023-09-28 14:32:13.913 
Epoch 342/1000 
	 loss: 16.2708, MinusLogProbMetric: 16.2708, val_loss: 17.2895, val_MinusLogProbMetric: 17.2895

Epoch 342: val_loss did not improve from 17.20462
196/196 - 69s - loss: 16.2708 - MinusLogProbMetric: 16.2708 - val_loss: 17.2895 - val_MinusLogProbMetric: 17.2895 - lr: 8.3333e-05 - 69s/epoch - 352ms/step
Epoch 343/1000
2023-09-28 14:33:24.503 
Epoch 343/1000 
	 loss: 16.2341, MinusLogProbMetric: 16.2341, val_loss: 17.2198, val_MinusLogProbMetric: 17.2198

Epoch 343: val_loss did not improve from 17.20462
196/196 - 71s - loss: 16.2341 - MinusLogProbMetric: 16.2341 - val_loss: 17.2198 - val_MinusLogProbMetric: 17.2198 - lr: 8.3333e-05 - 71s/epoch - 360ms/step
Epoch 344/1000
2023-09-28 14:34:37.714 
Epoch 344/1000 
	 loss: 16.2397, MinusLogProbMetric: 16.2397, val_loss: 17.1575, val_MinusLogProbMetric: 17.1575

Epoch 344: val_loss improved from 17.20462 to 17.15746, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_320/weights/best_weights.h5
196/196 - 75s - loss: 16.2397 - MinusLogProbMetric: 16.2397 - val_loss: 17.1575 - val_MinusLogProbMetric: 17.1575 - lr: 8.3333e-05 - 75s/epoch - 380ms/step
Epoch 345/1000
2023-09-28 14:35:47.820 
Epoch 345/1000 
	 loss: 16.2565, MinusLogProbMetric: 16.2565, val_loss: 17.1601, val_MinusLogProbMetric: 17.1601

Epoch 345: val_loss did not improve from 17.15746
196/196 - 69s - loss: 16.2565 - MinusLogProbMetric: 16.2565 - val_loss: 17.1601 - val_MinusLogProbMetric: 17.1601 - lr: 8.3333e-05 - 69s/epoch - 351ms/step
Epoch 346/1000
2023-09-28 14:36:59.697 
Epoch 346/1000 
	 loss: 16.2671, MinusLogProbMetric: 16.2671, val_loss: 17.3043, val_MinusLogProbMetric: 17.3043

Epoch 346: val_loss did not improve from 17.15746
196/196 - 72s - loss: 16.2671 - MinusLogProbMetric: 16.2671 - val_loss: 17.3043 - val_MinusLogProbMetric: 17.3043 - lr: 8.3333e-05 - 72s/epoch - 367ms/step
Epoch 347/1000
2023-09-28 14:38:09.057 
Epoch 347/1000 
	 loss: 16.2437, MinusLogProbMetric: 16.2437, val_loss: 17.2204, val_MinusLogProbMetric: 17.2204

Epoch 347: val_loss did not improve from 17.15746
196/196 - 69s - loss: 16.2437 - MinusLogProbMetric: 16.2437 - val_loss: 17.2204 - val_MinusLogProbMetric: 17.2204 - lr: 8.3333e-05 - 69s/epoch - 354ms/step
Epoch 348/1000
2023-09-28 14:39:23.608 
Epoch 348/1000 
	 loss: 16.2548, MinusLogProbMetric: 16.2548, val_loss: 17.2503, val_MinusLogProbMetric: 17.2503

Epoch 348: val_loss did not improve from 17.15746
196/196 - 75s - loss: 16.2548 - MinusLogProbMetric: 16.2548 - val_loss: 17.2503 - val_MinusLogProbMetric: 17.2503 - lr: 8.3333e-05 - 75s/epoch - 380ms/step
Epoch 349/1000
2023-09-28 14:40:41.190 
Epoch 349/1000 
	 loss: 16.2765, MinusLogProbMetric: 16.2765, val_loss: 17.2365, val_MinusLogProbMetric: 17.2365

Epoch 349: val_loss did not improve from 17.15746
196/196 - 78s - loss: 16.2765 - MinusLogProbMetric: 16.2765 - val_loss: 17.2365 - val_MinusLogProbMetric: 17.2365 - lr: 8.3333e-05 - 78s/epoch - 396ms/step
Epoch 350/1000
2023-09-28 14:42:01.756 
Epoch 350/1000 
	 loss: 16.2485, MinusLogProbMetric: 16.2485, val_loss: 17.1701, val_MinusLogProbMetric: 17.1701

Epoch 350: val_loss did not improve from 17.15746
196/196 - 81s - loss: 16.2485 - MinusLogProbMetric: 16.2485 - val_loss: 17.1701 - val_MinusLogProbMetric: 17.1701 - lr: 8.3333e-05 - 81s/epoch - 411ms/step
Epoch 351/1000
2023-09-28 14:43:24.276 
Epoch 351/1000 
	 loss: 16.2464, MinusLogProbMetric: 16.2464, val_loss: 17.2227, val_MinusLogProbMetric: 17.2227

Epoch 351: val_loss did not improve from 17.15746
196/196 - 83s - loss: 16.2464 - MinusLogProbMetric: 16.2464 - val_loss: 17.2227 - val_MinusLogProbMetric: 17.2227 - lr: 8.3333e-05 - 83s/epoch - 421ms/step
Epoch 352/1000
2023-09-28 14:44:40.785 
Epoch 352/1000 
	 loss: 16.2342, MinusLogProbMetric: 16.2342, val_loss: 17.2185, val_MinusLogProbMetric: 17.2185

Epoch 352: val_loss did not improve from 17.15746
196/196 - 77s - loss: 16.2342 - MinusLogProbMetric: 16.2342 - val_loss: 17.2185 - val_MinusLogProbMetric: 17.2185 - lr: 8.3333e-05 - 77s/epoch - 390ms/step
Epoch 353/1000
2023-09-28 14:46:05.144 
Epoch 353/1000 
	 loss: 16.2681, MinusLogProbMetric: 16.2681, val_loss: 17.2395, val_MinusLogProbMetric: 17.2395

Epoch 353: val_loss did not improve from 17.15746
196/196 - 84s - loss: 16.2681 - MinusLogProbMetric: 16.2681 - val_loss: 17.2395 - val_MinusLogProbMetric: 17.2395 - lr: 8.3333e-05 - 84s/epoch - 430ms/step
Epoch 354/1000
2023-09-28 14:47:28.453 
Epoch 354/1000 
	 loss: 16.2590, MinusLogProbMetric: 16.2590, val_loss: 17.1580, val_MinusLogProbMetric: 17.1580

Epoch 354: val_loss did not improve from 17.15746
196/196 - 83s - loss: 16.2590 - MinusLogProbMetric: 16.2590 - val_loss: 17.1580 - val_MinusLogProbMetric: 17.1580 - lr: 8.3333e-05 - 83s/epoch - 425ms/step
Epoch 355/1000
2023-09-28 14:48:48.674 
Epoch 355/1000 
	 loss: 16.2296, MinusLogProbMetric: 16.2296, val_loss: 17.2042, val_MinusLogProbMetric: 17.2042

Epoch 355: val_loss did not improve from 17.15746
196/196 - 80s - loss: 16.2296 - MinusLogProbMetric: 16.2296 - val_loss: 17.2042 - val_MinusLogProbMetric: 17.2042 - lr: 8.3333e-05 - 80s/epoch - 409ms/step
Epoch 356/1000
2023-09-28 14:50:10.917 
Epoch 356/1000 
	 loss: 16.2332, MinusLogProbMetric: 16.2332, val_loss: 17.3931, val_MinusLogProbMetric: 17.3931

Epoch 356: val_loss did not improve from 17.15746
196/196 - 82s - loss: 16.2332 - MinusLogProbMetric: 16.2332 - val_loss: 17.3931 - val_MinusLogProbMetric: 17.3931 - lr: 8.3333e-05 - 82s/epoch - 420ms/step
Epoch 357/1000
2023-09-28 14:51:33.034 
Epoch 357/1000 
	 loss: 16.2700, MinusLogProbMetric: 16.2700, val_loss: 17.1737, val_MinusLogProbMetric: 17.1737

Epoch 357: val_loss did not improve from 17.15746
196/196 - 82s - loss: 16.2700 - MinusLogProbMetric: 16.2700 - val_loss: 17.1737 - val_MinusLogProbMetric: 17.1737 - lr: 8.3333e-05 - 82s/epoch - 419ms/step
Epoch 358/1000
2023-09-28 14:52:57.744 
Epoch 358/1000 
	 loss: 16.2543, MinusLogProbMetric: 16.2543, val_loss: 17.1841, val_MinusLogProbMetric: 17.1841

Epoch 358: val_loss did not improve from 17.15746
196/196 - 85s - loss: 16.2543 - MinusLogProbMetric: 16.2543 - val_loss: 17.1841 - val_MinusLogProbMetric: 17.1841 - lr: 8.3333e-05 - 85s/epoch - 432ms/step
Epoch 359/1000
2023-09-28 14:54:23.089 
Epoch 359/1000 
	 loss: 16.2462, MinusLogProbMetric: 16.2462, val_loss: 17.1674, val_MinusLogProbMetric: 17.1674

Epoch 359: val_loss did not improve from 17.15746
196/196 - 85s - loss: 16.2462 - MinusLogProbMetric: 16.2462 - val_loss: 17.1674 - val_MinusLogProbMetric: 17.1674 - lr: 8.3333e-05 - 85s/epoch - 435ms/step
Epoch 360/1000
2023-09-28 14:55:48.674 
Epoch 360/1000 
	 loss: 16.2367, MinusLogProbMetric: 16.2367, val_loss: 17.2140, val_MinusLogProbMetric: 17.2140

Epoch 360: val_loss did not improve from 17.15746
196/196 - 86s - loss: 16.2367 - MinusLogProbMetric: 16.2367 - val_loss: 17.2140 - val_MinusLogProbMetric: 17.2140 - lr: 8.3333e-05 - 86s/epoch - 437ms/step
Epoch 361/1000
2023-09-28 14:57:13.044 
Epoch 361/1000 
	 loss: 16.2564, MinusLogProbMetric: 16.2564, val_loss: 17.1763, val_MinusLogProbMetric: 17.1763

Epoch 361: val_loss did not improve from 17.15746
196/196 - 84s - loss: 16.2564 - MinusLogProbMetric: 16.2564 - val_loss: 17.1763 - val_MinusLogProbMetric: 17.1763 - lr: 8.3333e-05 - 84s/epoch - 430ms/step
Epoch 362/1000
2023-09-28 14:58:36.944 
Epoch 362/1000 
	 loss: 16.2411, MinusLogProbMetric: 16.2411, val_loss: 17.1855, val_MinusLogProbMetric: 17.1855

Epoch 362: val_loss did not improve from 17.15746
196/196 - 84s - loss: 16.2411 - MinusLogProbMetric: 16.2411 - val_loss: 17.1855 - val_MinusLogProbMetric: 17.1855 - lr: 8.3333e-05 - 84s/epoch - 428ms/step
Epoch 363/1000
2023-09-28 14:59:59.932 
Epoch 363/1000 
	 loss: 16.2298, MinusLogProbMetric: 16.2298, val_loss: 17.1767, val_MinusLogProbMetric: 17.1767

Epoch 363: val_loss did not improve from 17.15746
196/196 - 83s - loss: 16.2298 - MinusLogProbMetric: 16.2298 - val_loss: 17.1767 - val_MinusLogProbMetric: 17.1767 - lr: 8.3333e-05 - 83s/epoch - 423ms/step
Epoch 364/1000
2023-09-28 15:01:23.896 
Epoch 364/1000 
	 loss: 16.2437, MinusLogProbMetric: 16.2437, val_loss: 17.1576, val_MinusLogProbMetric: 17.1576

Epoch 364: val_loss did not improve from 17.15746
196/196 - 84s - loss: 16.2437 - MinusLogProbMetric: 16.2437 - val_loss: 17.1576 - val_MinusLogProbMetric: 17.1576 - lr: 8.3333e-05 - 84s/epoch - 428ms/step
Epoch 365/1000
2023-09-28 15:02:48.874 
Epoch 365/1000 
	 loss: 16.2298, MinusLogProbMetric: 16.2298, val_loss: 17.1970, val_MinusLogProbMetric: 17.1970

Epoch 365: val_loss did not improve from 17.15746
196/196 - 85s - loss: 16.2298 - MinusLogProbMetric: 16.2298 - val_loss: 17.1970 - val_MinusLogProbMetric: 17.1970 - lr: 8.3333e-05 - 85s/epoch - 434ms/step
Epoch 366/1000
2023-09-28 15:04:11.786 
Epoch 366/1000 
	 loss: 16.2463, MinusLogProbMetric: 16.2463, val_loss: 17.1827, val_MinusLogProbMetric: 17.1827

Epoch 366: val_loss did not improve from 17.15746
196/196 - 83s - loss: 16.2463 - MinusLogProbMetric: 16.2463 - val_loss: 17.1827 - val_MinusLogProbMetric: 17.1827 - lr: 8.3333e-05 - 83s/epoch - 423ms/step
Epoch 367/1000
2023-09-28 15:05:35.479 
Epoch 367/1000 
	 loss: 16.2446, MinusLogProbMetric: 16.2446, val_loss: 17.2644, val_MinusLogProbMetric: 17.2644

Epoch 367: val_loss did not improve from 17.15746
196/196 - 84s - loss: 16.2446 - MinusLogProbMetric: 16.2446 - val_loss: 17.2644 - val_MinusLogProbMetric: 17.2644 - lr: 8.3333e-05 - 84s/epoch - 427ms/step
Epoch 368/1000
2023-09-28 15:07:00.259 
Epoch 368/1000 
	 loss: 16.2240, MinusLogProbMetric: 16.2240, val_loss: 17.2236, val_MinusLogProbMetric: 17.2236

Epoch 368: val_loss did not improve from 17.15746
196/196 - 85s - loss: 16.2240 - MinusLogProbMetric: 16.2240 - val_loss: 17.2236 - val_MinusLogProbMetric: 17.2236 - lr: 8.3333e-05 - 85s/epoch - 433ms/step
Epoch 369/1000
2023-09-28 15:08:24.046 
Epoch 369/1000 
	 loss: 16.2377, MinusLogProbMetric: 16.2377, val_loss: 17.1996, val_MinusLogProbMetric: 17.1996

Epoch 369: val_loss did not improve from 17.15746
196/196 - 84s - loss: 16.2377 - MinusLogProbMetric: 16.2377 - val_loss: 17.1996 - val_MinusLogProbMetric: 17.1996 - lr: 8.3333e-05 - 84s/epoch - 427ms/step
Epoch 370/1000
2023-09-28 15:09:48.816 
Epoch 370/1000 
	 loss: 16.2577, MinusLogProbMetric: 16.2577, val_loss: 17.2290, val_MinusLogProbMetric: 17.2290

Epoch 370: val_loss did not improve from 17.15746
196/196 - 85s - loss: 16.2577 - MinusLogProbMetric: 16.2577 - val_loss: 17.2290 - val_MinusLogProbMetric: 17.2290 - lr: 8.3333e-05 - 85s/epoch - 432ms/step
Epoch 371/1000
2023-09-28 15:11:13.533 
Epoch 371/1000 
	 loss: 16.2339, MinusLogProbMetric: 16.2339, val_loss: 17.1568, val_MinusLogProbMetric: 17.1568

Epoch 371: val_loss improved from 17.15746 to 17.15676, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_320/weights/best_weights.h5
196/196 - 86s - loss: 16.2339 - MinusLogProbMetric: 16.2339 - val_loss: 17.1568 - val_MinusLogProbMetric: 17.1568 - lr: 8.3333e-05 - 86s/epoch - 440ms/step
Epoch 372/1000
2023-09-28 15:12:40.105 
Epoch 372/1000 
	 loss: 16.2528, MinusLogProbMetric: 16.2528, val_loss: 17.2529, val_MinusLogProbMetric: 17.2529

Epoch 372: val_loss did not improve from 17.15676
196/196 - 85s - loss: 16.2528 - MinusLogProbMetric: 16.2528 - val_loss: 17.2529 - val_MinusLogProbMetric: 17.2529 - lr: 8.3333e-05 - 85s/epoch - 434ms/step
Epoch 373/1000
2023-09-28 15:14:03.560 
Epoch 373/1000 
	 loss: 16.2471, MinusLogProbMetric: 16.2471, val_loss: 17.3348, val_MinusLogProbMetric: 17.3348

Epoch 373: val_loss did not improve from 17.15676
196/196 - 83s - loss: 16.2471 - MinusLogProbMetric: 16.2471 - val_loss: 17.3348 - val_MinusLogProbMetric: 17.3348 - lr: 8.3333e-05 - 83s/epoch - 426ms/step
Epoch 374/1000
2023-09-28 15:15:27.734 
Epoch 374/1000 
	 loss: 16.2265, MinusLogProbMetric: 16.2265, val_loss: 17.2454, val_MinusLogProbMetric: 17.2454

Epoch 374: val_loss did not improve from 17.15676
196/196 - 84s - loss: 16.2265 - MinusLogProbMetric: 16.2265 - val_loss: 17.2454 - val_MinusLogProbMetric: 17.2454 - lr: 8.3333e-05 - 84s/epoch - 429ms/step
Epoch 375/1000
2023-09-28 15:16:51.573 
Epoch 375/1000 
	 loss: 16.2201, MinusLogProbMetric: 16.2201, val_loss: 17.2778, val_MinusLogProbMetric: 17.2778

Epoch 375: val_loss did not improve from 17.15676
196/196 - 84s - loss: 16.2201 - MinusLogProbMetric: 16.2201 - val_loss: 17.2778 - val_MinusLogProbMetric: 17.2778 - lr: 8.3333e-05 - 84s/epoch - 428ms/step
Epoch 376/1000
2023-09-28 15:18:15.091 
Epoch 376/1000 
	 loss: 16.2472, MinusLogProbMetric: 16.2472, val_loss: 17.1986, val_MinusLogProbMetric: 17.1986

Epoch 376: val_loss did not improve from 17.15676
196/196 - 84s - loss: 16.2472 - MinusLogProbMetric: 16.2472 - val_loss: 17.1986 - val_MinusLogProbMetric: 17.1986 - lr: 8.3333e-05 - 84s/epoch - 426ms/step
Epoch 377/1000
2023-09-28 15:19:39.338 
Epoch 377/1000 
	 loss: 16.2166, MinusLogProbMetric: 16.2166, val_loss: 17.2245, val_MinusLogProbMetric: 17.2245

Epoch 377: val_loss did not improve from 17.15676
196/196 - 84s - loss: 16.2166 - MinusLogProbMetric: 16.2166 - val_loss: 17.2245 - val_MinusLogProbMetric: 17.2245 - lr: 8.3333e-05 - 84s/epoch - 430ms/step
Epoch 378/1000
2023-09-28 15:21:03.375 
Epoch 378/1000 
	 loss: 16.2331, MinusLogProbMetric: 16.2331, val_loss: 17.1745, val_MinusLogProbMetric: 17.1745

Epoch 378: val_loss did not improve from 17.15676
196/196 - 84s - loss: 16.2331 - MinusLogProbMetric: 16.2331 - val_loss: 17.1745 - val_MinusLogProbMetric: 17.1745 - lr: 8.3333e-05 - 84s/epoch - 429ms/step
Epoch 379/1000
2023-09-28 15:22:27.065 
Epoch 379/1000 
	 loss: 16.2461, MinusLogProbMetric: 16.2461, val_loss: 17.2674, val_MinusLogProbMetric: 17.2674

Epoch 379: val_loss did not improve from 17.15676
196/196 - 84s - loss: 16.2461 - MinusLogProbMetric: 16.2461 - val_loss: 17.2674 - val_MinusLogProbMetric: 17.2674 - lr: 8.3333e-05 - 84s/epoch - 427ms/step
Epoch 380/1000
2023-09-28 15:23:50.814 
Epoch 380/1000 
	 loss: 16.2273, MinusLogProbMetric: 16.2273, val_loss: 17.2115, val_MinusLogProbMetric: 17.2115

Epoch 380: val_loss did not improve from 17.15676
196/196 - 84s - loss: 16.2273 - MinusLogProbMetric: 16.2273 - val_loss: 17.2115 - val_MinusLogProbMetric: 17.2115 - lr: 8.3333e-05 - 84s/epoch - 427ms/step
Epoch 381/1000
2023-09-28 15:25:12.068 
Epoch 381/1000 
	 loss: 16.2357, MinusLogProbMetric: 16.2357, val_loss: 17.1667, val_MinusLogProbMetric: 17.1667

Epoch 381: val_loss did not improve from 17.15676
196/196 - 81s - loss: 16.2357 - MinusLogProbMetric: 16.2357 - val_loss: 17.1667 - val_MinusLogProbMetric: 17.1667 - lr: 8.3333e-05 - 81s/epoch - 415ms/step
Epoch 382/1000
2023-09-28 15:26:33.650 
Epoch 382/1000 
	 loss: 16.2235, MinusLogProbMetric: 16.2235, val_loss: 17.1522, val_MinusLogProbMetric: 17.1522

Epoch 382: val_loss improved from 17.15676 to 17.15221, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_320/weights/best_weights.h5
196/196 - 83s - loss: 16.2235 - MinusLogProbMetric: 16.2235 - val_loss: 17.1522 - val_MinusLogProbMetric: 17.1522 - lr: 8.3333e-05 - 83s/epoch - 423ms/step
Epoch 383/1000
2023-09-28 15:27:57.789 
Epoch 383/1000 
	 loss: 16.2276, MinusLogProbMetric: 16.2276, val_loss: 17.1904, val_MinusLogProbMetric: 17.1904

Epoch 383: val_loss did not improve from 17.15221
196/196 - 83s - loss: 16.2276 - MinusLogProbMetric: 16.2276 - val_loss: 17.1904 - val_MinusLogProbMetric: 17.1904 - lr: 8.3333e-05 - 83s/epoch - 422ms/step
Epoch 384/1000
2023-09-28 15:29:19.772 
Epoch 384/1000 
	 loss: 16.2149, MinusLogProbMetric: 16.2149, val_loss: 17.2541, val_MinusLogProbMetric: 17.2541

Epoch 384: val_loss did not improve from 17.15221
196/196 - 82s - loss: 16.2149 - MinusLogProbMetric: 16.2149 - val_loss: 17.2541 - val_MinusLogProbMetric: 17.2541 - lr: 8.3333e-05 - 82s/epoch - 418ms/step
Epoch 385/1000
2023-09-28 15:30:42.906 
Epoch 385/1000 
	 loss: 16.2370, MinusLogProbMetric: 16.2370, val_loss: 17.2035, val_MinusLogProbMetric: 17.2035

Epoch 385: val_loss did not improve from 17.15221
196/196 - 83s - loss: 16.2370 - MinusLogProbMetric: 16.2370 - val_loss: 17.2035 - val_MinusLogProbMetric: 17.2035 - lr: 8.3333e-05 - 83s/epoch - 424ms/step
Epoch 386/1000
2023-09-28 15:32:03.951 
Epoch 386/1000 
	 loss: 16.2131, MinusLogProbMetric: 16.2131, val_loss: 17.1629, val_MinusLogProbMetric: 17.1629

Epoch 386: val_loss did not improve from 17.15221
196/196 - 81s - loss: 16.2131 - MinusLogProbMetric: 16.2131 - val_loss: 17.1629 - val_MinusLogProbMetric: 17.1629 - lr: 8.3333e-05 - 81s/epoch - 413ms/step
Epoch 387/1000
2023-09-28 15:33:25.829 
Epoch 387/1000 
	 loss: 16.2250, MinusLogProbMetric: 16.2250, val_loss: 17.2840, val_MinusLogProbMetric: 17.2840

Epoch 387: val_loss did not improve from 17.15221
196/196 - 82s - loss: 16.2250 - MinusLogProbMetric: 16.2250 - val_loss: 17.2840 - val_MinusLogProbMetric: 17.2840 - lr: 8.3333e-05 - 82s/epoch - 418ms/step
Epoch 388/1000
2023-09-28 15:34:47.324 
Epoch 388/1000 
	 loss: 16.2453, MinusLogProbMetric: 16.2453, val_loss: 17.1689, val_MinusLogProbMetric: 17.1689

Epoch 388: val_loss did not improve from 17.15221
196/196 - 81s - loss: 16.2453 - MinusLogProbMetric: 16.2453 - val_loss: 17.1689 - val_MinusLogProbMetric: 17.1689 - lr: 8.3333e-05 - 81s/epoch - 416ms/step
Epoch 389/1000
2023-09-28 15:36:09.530 
Epoch 389/1000 
	 loss: 16.2308, MinusLogProbMetric: 16.2308, val_loss: 17.1643, val_MinusLogProbMetric: 17.1643

Epoch 389: val_loss did not improve from 17.15221
196/196 - 82s - loss: 16.2308 - MinusLogProbMetric: 16.2308 - val_loss: 17.1643 - val_MinusLogProbMetric: 17.1643 - lr: 8.3333e-05 - 82s/epoch - 419ms/step
Epoch 390/1000
2023-09-28 15:37:28.352 
Epoch 390/1000 
	 loss: 16.2102, MinusLogProbMetric: 16.2102, val_loss: 17.3395, val_MinusLogProbMetric: 17.3395

Epoch 390: val_loss did not improve from 17.15221
196/196 - 79s - loss: 16.2102 - MinusLogProbMetric: 16.2102 - val_loss: 17.3395 - val_MinusLogProbMetric: 17.3395 - lr: 8.3333e-05 - 79s/epoch - 402ms/step
Epoch 391/1000
2023-09-28 15:38:52.398 
Epoch 391/1000 
	 loss: 16.2224, MinusLogProbMetric: 16.2224, val_loss: 17.1439, val_MinusLogProbMetric: 17.1439

Epoch 391: val_loss improved from 17.15221 to 17.14391, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_320/weights/best_weights.h5
196/196 - 86s - loss: 16.2224 - MinusLogProbMetric: 16.2224 - val_loss: 17.1439 - val_MinusLogProbMetric: 17.1439 - lr: 8.3333e-05 - 86s/epoch - 436ms/step
Epoch 392/1000
2023-09-28 15:40:13.587 
Epoch 392/1000 
	 loss: 16.2010, MinusLogProbMetric: 16.2010, val_loss: 17.1703, val_MinusLogProbMetric: 17.1703

Epoch 392: val_loss did not improve from 17.14391
196/196 - 80s - loss: 16.2010 - MinusLogProbMetric: 16.2010 - val_loss: 17.1703 - val_MinusLogProbMetric: 17.1703 - lr: 8.3333e-05 - 80s/epoch - 407ms/step
Epoch 393/1000
2023-09-28 15:41:36.322 
Epoch 393/1000 
	 loss: 16.2173, MinusLogProbMetric: 16.2173, val_loss: 17.2728, val_MinusLogProbMetric: 17.2728

Epoch 393: val_loss did not improve from 17.14391
196/196 - 83s - loss: 16.2173 - MinusLogProbMetric: 16.2173 - val_loss: 17.2728 - val_MinusLogProbMetric: 17.2728 - lr: 8.3333e-05 - 83s/epoch - 422ms/step
Epoch 394/1000
2023-09-28 15:43:00.437 
Epoch 394/1000 
	 loss: 16.2253, MinusLogProbMetric: 16.2253, val_loss: 17.2132, val_MinusLogProbMetric: 17.2132

Epoch 394: val_loss did not improve from 17.14391
196/196 - 84s - loss: 16.2253 - MinusLogProbMetric: 16.2253 - val_loss: 17.2132 - val_MinusLogProbMetric: 17.2132 - lr: 8.3333e-05 - 84s/epoch - 429ms/step
Epoch 395/1000
2023-09-28 15:44:25.686 
Epoch 395/1000 
	 loss: 16.2433, MinusLogProbMetric: 16.2433, val_loss: 17.1706, val_MinusLogProbMetric: 17.1706

Epoch 395: val_loss did not improve from 17.14391
196/196 - 85s - loss: 16.2433 - MinusLogProbMetric: 16.2433 - val_loss: 17.1706 - val_MinusLogProbMetric: 17.1706 - lr: 8.3333e-05 - 85s/epoch - 435ms/step
Epoch 396/1000
2023-09-28 15:45:50.498 
Epoch 396/1000 
	 loss: 16.2131, MinusLogProbMetric: 16.2131, val_loss: 17.1766, val_MinusLogProbMetric: 17.1766

Epoch 396: val_loss did not improve from 17.14391
196/196 - 85s - loss: 16.2131 - MinusLogProbMetric: 16.2131 - val_loss: 17.1766 - val_MinusLogProbMetric: 17.1766 - lr: 8.3333e-05 - 85s/epoch - 433ms/step
Epoch 397/1000
2023-09-28 15:47:15.655 
Epoch 397/1000 
	 loss: 16.2325, MinusLogProbMetric: 16.2325, val_loss: 17.1622, val_MinusLogProbMetric: 17.1622

Epoch 397: val_loss did not improve from 17.14391
196/196 - 85s - loss: 16.2325 - MinusLogProbMetric: 16.2325 - val_loss: 17.1622 - val_MinusLogProbMetric: 17.1622 - lr: 8.3333e-05 - 85s/epoch - 434ms/step
Epoch 398/1000
2023-09-28 15:48:40.382 
Epoch 398/1000 
	 loss: 16.1918, MinusLogProbMetric: 16.1918, val_loss: 17.2725, val_MinusLogProbMetric: 17.2725

Epoch 398: val_loss did not improve from 17.14391
196/196 - 85s - loss: 16.1918 - MinusLogProbMetric: 16.1918 - val_loss: 17.2725 - val_MinusLogProbMetric: 17.2725 - lr: 8.3333e-05 - 85s/epoch - 432ms/step
Epoch 399/1000
2023-09-28 15:50:04.938 
Epoch 399/1000 
	 loss: 16.2251, MinusLogProbMetric: 16.2251, val_loss: 17.2206, val_MinusLogProbMetric: 17.2206

Epoch 399: val_loss did not improve from 17.14391
196/196 - 85s - loss: 16.2251 - MinusLogProbMetric: 16.2251 - val_loss: 17.2206 - val_MinusLogProbMetric: 17.2206 - lr: 8.3333e-05 - 85s/epoch - 431ms/step
Epoch 400/1000
2023-09-28 15:51:27.628 
Epoch 400/1000 
	 loss: 16.2136, MinusLogProbMetric: 16.2136, val_loss: 17.1732, val_MinusLogProbMetric: 17.1732

Epoch 400: val_loss did not improve from 17.14391
196/196 - 83s - loss: 16.2136 - MinusLogProbMetric: 16.2136 - val_loss: 17.1732 - val_MinusLogProbMetric: 17.1732 - lr: 8.3333e-05 - 83s/epoch - 422ms/step
Epoch 401/1000
2023-09-28 15:52:51.755 
Epoch 401/1000 
	 loss: 16.2134, MinusLogProbMetric: 16.2134, val_loss: 17.3434, val_MinusLogProbMetric: 17.3434

Epoch 401: val_loss did not improve from 17.14391
196/196 - 84s - loss: 16.2134 - MinusLogProbMetric: 16.2134 - val_loss: 17.3434 - val_MinusLogProbMetric: 17.3434 - lr: 8.3333e-05 - 84s/epoch - 429ms/step
Epoch 402/1000
2023-09-28 15:54:15.557 
Epoch 402/1000 
	 loss: 16.1950, MinusLogProbMetric: 16.1950, val_loss: 17.2268, val_MinusLogProbMetric: 17.2268

Epoch 402: val_loss did not improve from 17.14391
196/196 - 84s - loss: 16.1950 - MinusLogProbMetric: 16.1950 - val_loss: 17.2268 - val_MinusLogProbMetric: 17.2268 - lr: 8.3333e-05 - 84s/epoch - 428ms/step
Epoch 403/1000
2023-09-28 15:55:41.610 
Epoch 403/1000 
	 loss: 16.2124, MinusLogProbMetric: 16.2124, val_loss: 17.1883, val_MinusLogProbMetric: 17.1883

Epoch 403: val_loss did not improve from 17.14391
196/196 - 86s - loss: 16.2124 - MinusLogProbMetric: 16.2124 - val_loss: 17.1883 - val_MinusLogProbMetric: 17.1883 - lr: 8.3333e-05 - 86s/epoch - 439ms/step
Epoch 404/1000
2023-09-28 15:57:06.459 
Epoch 404/1000 
	 loss: 16.2112, MinusLogProbMetric: 16.2112, val_loss: 17.2276, val_MinusLogProbMetric: 17.2276

Epoch 404: val_loss did not improve from 17.14391
196/196 - 85s - loss: 16.2112 - MinusLogProbMetric: 16.2112 - val_loss: 17.2276 - val_MinusLogProbMetric: 17.2276 - lr: 8.3333e-05 - 85s/epoch - 433ms/step
Epoch 405/1000
2023-09-28 15:58:26.140 
Epoch 405/1000 
	 loss: 16.2285, MinusLogProbMetric: 16.2285, val_loss: 17.1632, val_MinusLogProbMetric: 17.1632

Epoch 405: val_loss did not improve from 17.14391
196/196 - 80s - loss: 16.2285 - MinusLogProbMetric: 16.2285 - val_loss: 17.1632 - val_MinusLogProbMetric: 17.1632 - lr: 8.3333e-05 - 80s/epoch - 407ms/step
Epoch 406/1000
2023-09-28 15:59:45.043 
Epoch 406/1000 
	 loss: 16.2015, MinusLogProbMetric: 16.2015, val_loss: 17.1814, val_MinusLogProbMetric: 17.1814

Epoch 406: val_loss did not improve from 17.14391
196/196 - 79s - loss: 16.2015 - MinusLogProbMetric: 16.2015 - val_loss: 17.1814 - val_MinusLogProbMetric: 17.1814 - lr: 8.3333e-05 - 79s/epoch - 403ms/step
Epoch 407/1000
2023-09-28 16:01:04.235 
Epoch 407/1000 
	 loss: 16.2140, MinusLogProbMetric: 16.2140, val_loss: 17.2634, val_MinusLogProbMetric: 17.2634

Epoch 407: val_loss did not improve from 17.14391
196/196 - 79s - loss: 16.2140 - MinusLogProbMetric: 16.2140 - val_loss: 17.2634 - val_MinusLogProbMetric: 17.2634 - lr: 8.3333e-05 - 79s/epoch - 404ms/step
Epoch 408/1000
2023-09-28 16:02:24.061 
Epoch 408/1000 
	 loss: 16.2029, MinusLogProbMetric: 16.2029, val_loss: 17.1840, val_MinusLogProbMetric: 17.1840

Epoch 408: val_loss did not improve from 17.14391
196/196 - 80s - loss: 16.2029 - MinusLogProbMetric: 16.2029 - val_loss: 17.1840 - val_MinusLogProbMetric: 17.1840 - lr: 8.3333e-05 - 80s/epoch - 407ms/step
Epoch 409/1000
2023-09-28 16:03:46.318 
Epoch 409/1000 
	 loss: 16.2169, MinusLogProbMetric: 16.2169, val_loss: 17.2227, val_MinusLogProbMetric: 17.2227

Epoch 409: val_loss did not improve from 17.14391
196/196 - 82s - loss: 16.2169 - MinusLogProbMetric: 16.2169 - val_loss: 17.2227 - val_MinusLogProbMetric: 17.2227 - lr: 8.3333e-05 - 82s/epoch - 420ms/step
Epoch 410/1000
2023-09-28 16:05:04.782 
Epoch 410/1000 
	 loss: 16.2243, MinusLogProbMetric: 16.2243, val_loss: 17.2965, val_MinusLogProbMetric: 17.2965

Epoch 410: val_loss did not improve from 17.14391
196/196 - 78s - loss: 16.2243 - MinusLogProbMetric: 16.2243 - val_loss: 17.2965 - val_MinusLogProbMetric: 17.2965 - lr: 8.3333e-05 - 78s/epoch - 400ms/step
Epoch 411/1000
2023-09-28 16:06:25.220 
Epoch 411/1000 
	 loss: 16.1964, MinusLogProbMetric: 16.1964, val_loss: 17.2679, val_MinusLogProbMetric: 17.2679

Epoch 411: val_loss did not improve from 17.14391
196/196 - 80s - loss: 16.1964 - MinusLogProbMetric: 16.1964 - val_loss: 17.2679 - val_MinusLogProbMetric: 17.2679 - lr: 8.3333e-05 - 80s/epoch - 410ms/step
Epoch 412/1000
2023-09-28 16:07:44.442 
Epoch 412/1000 
	 loss: 16.2177, MinusLogProbMetric: 16.2177, val_loss: 17.1996, val_MinusLogProbMetric: 17.1996

Epoch 412: val_loss did not improve from 17.14391
196/196 - 79s - loss: 16.2177 - MinusLogProbMetric: 16.2177 - val_loss: 17.1996 - val_MinusLogProbMetric: 17.1996 - lr: 8.3333e-05 - 79s/epoch - 404ms/step
Epoch 413/1000
2023-09-28 16:09:06.576 
Epoch 413/1000 
	 loss: 16.2051, MinusLogProbMetric: 16.2051, val_loss: 17.2191, val_MinusLogProbMetric: 17.2191

Epoch 413: val_loss did not improve from 17.14391
196/196 - 82s - loss: 16.2051 - MinusLogProbMetric: 16.2051 - val_loss: 17.2191 - val_MinusLogProbMetric: 17.2191 - lr: 8.3333e-05 - 82s/epoch - 419ms/step
Epoch 414/1000
2023-09-28 16:10:28.116 
Epoch 414/1000 
	 loss: 16.2169, MinusLogProbMetric: 16.2169, val_loss: 17.1890, val_MinusLogProbMetric: 17.1890

Epoch 414: val_loss did not improve from 17.14391
196/196 - 82s - loss: 16.2169 - MinusLogProbMetric: 16.2169 - val_loss: 17.1890 - val_MinusLogProbMetric: 17.1890 - lr: 8.3333e-05 - 82s/epoch - 416ms/step
Epoch 415/1000
2023-09-28 16:11:46.680 
Epoch 415/1000 
	 loss: 16.1897, MinusLogProbMetric: 16.1897, val_loss: 17.2064, val_MinusLogProbMetric: 17.2064

Epoch 415: val_loss did not improve from 17.14391
196/196 - 79s - loss: 16.1897 - MinusLogProbMetric: 16.1897 - val_loss: 17.2064 - val_MinusLogProbMetric: 17.2064 - lr: 8.3333e-05 - 79s/epoch - 401ms/step
Epoch 416/1000
2023-09-28 16:13:08.595 
Epoch 416/1000 
	 loss: 16.2055, MinusLogProbMetric: 16.2055, val_loss: 17.2864, val_MinusLogProbMetric: 17.2864

Epoch 416: val_loss did not improve from 17.14391
196/196 - 82s - loss: 16.2055 - MinusLogProbMetric: 16.2055 - val_loss: 17.2864 - val_MinusLogProbMetric: 17.2864 - lr: 8.3333e-05 - 82s/epoch - 418ms/step
Epoch 417/1000
2023-09-28 16:14:24.418 
Epoch 417/1000 
	 loss: 16.2009, MinusLogProbMetric: 16.2009, val_loss: 17.3838, val_MinusLogProbMetric: 17.3838

Epoch 417: val_loss did not improve from 17.14391
196/196 - 76s - loss: 16.2009 - MinusLogProbMetric: 16.2009 - val_loss: 17.3838 - val_MinusLogProbMetric: 17.3838 - lr: 8.3333e-05 - 76s/epoch - 387ms/step
Epoch 418/1000
2023-09-28 16:15:48.547 
Epoch 418/1000 
	 loss: 16.1985, MinusLogProbMetric: 16.1985, val_loss: 17.2037, val_MinusLogProbMetric: 17.2037

Epoch 418: val_loss did not improve from 17.14391
196/196 - 84s - loss: 16.1985 - MinusLogProbMetric: 16.1985 - val_loss: 17.2037 - val_MinusLogProbMetric: 17.2037 - lr: 8.3333e-05 - 84s/epoch - 429ms/step
Epoch 419/1000
2023-09-28 16:17:12.912 
Epoch 419/1000 
	 loss: 16.1917, MinusLogProbMetric: 16.1917, val_loss: 17.2499, val_MinusLogProbMetric: 17.2499

Epoch 419: val_loss did not improve from 17.14391
196/196 - 84s - loss: 16.1917 - MinusLogProbMetric: 16.1917 - val_loss: 17.2499 - val_MinusLogProbMetric: 17.2499 - lr: 8.3333e-05 - 84s/epoch - 430ms/step
Epoch 420/1000
2023-09-28 16:18:37.200 
Epoch 420/1000 
	 loss: 16.2280, MinusLogProbMetric: 16.2280, val_loss: 17.1916, val_MinusLogProbMetric: 17.1916

Epoch 420: val_loss did not improve from 17.14391
196/196 - 84s - loss: 16.2280 - MinusLogProbMetric: 16.2280 - val_loss: 17.1916 - val_MinusLogProbMetric: 17.1916 - lr: 8.3333e-05 - 84s/epoch - 430ms/step
Epoch 421/1000
2023-09-28 16:20:00.891 
Epoch 421/1000 
	 loss: 16.2009, MinusLogProbMetric: 16.2009, val_loss: 17.2607, val_MinusLogProbMetric: 17.2607

Epoch 421: val_loss did not improve from 17.14391
196/196 - 84s - loss: 16.2009 - MinusLogProbMetric: 16.2009 - val_loss: 17.2607 - val_MinusLogProbMetric: 17.2607 - lr: 8.3333e-05 - 84s/epoch - 427ms/step
Epoch 422/1000
2023-09-28 16:21:24.927 
Epoch 422/1000 
	 loss: 16.1965, MinusLogProbMetric: 16.1965, val_loss: 17.2745, val_MinusLogProbMetric: 17.2745

Epoch 422: val_loss did not improve from 17.14391
196/196 - 84s - loss: 16.1965 - MinusLogProbMetric: 16.1965 - val_loss: 17.2745 - val_MinusLogProbMetric: 17.2745 - lr: 8.3333e-05 - 84s/epoch - 429ms/step
Epoch 423/1000
2023-09-28 16:22:49.383 
Epoch 423/1000 
	 loss: 16.2065, MinusLogProbMetric: 16.2065, val_loss: 17.2033, val_MinusLogProbMetric: 17.2033

Epoch 423: val_loss did not improve from 17.14391
196/196 - 84s - loss: 16.2065 - MinusLogProbMetric: 16.2065 - val_loss: 17.2033 - val_MinusLogProbMetric: 17.2033 - lr: 8.3333e-05 - 84s/epoch - 431ms/step
Epoch 424/1000
2023-09-28 16:24:09.734 
Epoch 424/1000 
	 loss: 16.1846, MinusLogProbMetric: 16.1846, val_loss: 17.2496, val_MinusLogProbMetric: 17.2496

Epoch 424: val_loss did not improve from 17.14391
196/196 - 80s - loss: 16.1846 - MinusLogProbMetric: 16.1846 - val_loss: 17.2496 - val_MinusLogProbMetric: 17.2496 - lr: 8.3333e-05 - 80s/epoch - 410ms/step
Epoch 425/1000
2023-09-28 16:25:30.881 
Epoch 425/1000 
	 loss: 16.2242, MinusLogProbMetric: 16.2242, val_loss: 17.3375, val_MinusLogProbMetric: 17.3375

Epoch 425: val_loss did not improve from 17.14391
196/196 - 81s - loss: 16.2242 - MinusLogProbMetric: 16.2242 - val_loss: 17.3375 - val_MinusLogProbMetric: 17.3375 - lr: 8.3333e-05 - 81s/epoch - 414ms/step
Epoch 426/1000
2023-09-28 16:26:52.425 
Epoch 426/1000 
	 loss: 16.1782, MinusLogProbMetric: 16.1782, val_loss: 17.1879, val_MinusLogProbMetric: 17.1879

Epoch 426: val_loss did not improve from 17.14391
196/196 - 82s - loss: 16.1782 - MinusLogProbMetric: 16.1782 - val_loss: 17.1879 - val_MinusLogProbMetric: 17.1879 - lr: 8.3333e-05 - 82s/epoch - 416ms/step
Epoch 427/1000
2023-09-28 16:28:12.621 
Epoch 427/1000 
	 loss: 16.2421, MinusLogProbMetric: 16.2421, val_loss: 17.1915, val_MinusLogProbMetric: 17.1915

Epoch 427: val_loss did not improve from 17.14391
196/196 - 80s - loss: 16.2421 - MinusLogProbMetric: 16.2421 - val_loss: 17.1915 - val_MinusLogProbMetric: 17.1915 - lr: 8.3333e-05 - 80s/epoch - 409ms/step
Epoch 428/1000
2023-09-28 16:29:33.071 
Epoch 428/1000 
	 loss: 16.1947, MinusLogProbMetric: 16.1947, val_loss: 17.1925, val_MinusLogProbMetric: 17.1925

Epoch 428: val_loss did not improve from 17.14391
196/196 - 80s - loss: 16.1947 - MinusLogProbMetric: 16.1947 - val_loss: 17.1925 - val_MinusLogProbMetric: 17.1925 - lr: 8.3333e-05 - 80s/epoch - 410ms/step
Epoch 429/1000
2023-09-28 16:30:52.749 
Epoch 429/1000 
	 loss: 16.2078, MinusLogProbMetric: 16.2078, val_loss: 17.4126, val_MinusLogProbMetric: 17.4126

Epoch 429: val_loss did not improve from 17.14391
196/196 - 80s - loss: 16.2078 - MinusLogProbMetric: 16.2078 - val_loss: 17.4126 - val_MinusLogProbMetric: 17.4126 - lr: 8.3333e-05 - 80s/epoch - 407ms/step
Epoch 430/1000
2023-09-28 16:32:11.936 
Epoch 430/1000 
	 loss: 16.2064, MinusLogProbMetric: 16.2064, val_loss: 17.2229, val_MinusLogProbMetric: 17.2229

Epoch 430: val_loss did not improve from 17.14391
196/196 - 79s - loss: 16.2064 - MinusLogProbMetric: 16.2064 - val_loss: 17.2229 - val_MinusLogProbMetric: 17.2229 - lr: 8.3333e-05 - 79s/epoch - 404ms/step
Epoch 431/1000
2023-09-28 16:33:34.226 
Epoch 431/1000 
	 loss: 16.1957, MinusLogProbMetric: 16.1957, val_loss: 17.1996, val_MinusLogProbMetric: 17.1996

Epoch 431: val_loss did not improve from 17.14391
196/196 - 82s - loss: 16.1957 - MinusLogProbMetric: 16.1957 - val_loss: 17.1996 - val_MinusLogProbMetric: 17.1996 - lr: 8.3333e-05 - 82s/epoch - 420ms/step
Epoch 432/1000
2023-09-28 16:34:50.451 
Epoch 432/1000 
	 loss: 16.1899, MinusLogProbMetric: 16.1899, val_loss: 17.2072, val_MinusLogProbMetric: 17.2072

Epoch 432: val_loss did not improve from 17.14391
196/196 - 76s - loss: 16.1899 - MinusLogProbMetric: 16.1899 - val_loss: 17.2072 - val_MinusLogProbMetric: 17.2072 - lr: 8.3333e-05 - 76s/epoch - 389ms/step
Epoch 433/1000
2023-09-28 16:36:03.246 
Epoch 433/1000 
	 loss: 16.2042, MinusLogProbMetric: 16.2042, val_loss: 17.1972, val_MinusLogProbMetric: 17.1972

Epoch 433: val_loss did not improve from 17.14391
196/196 - 73s - loss: 16.2042 - MinusLogProbMetric: 16.2042 - val_loss: 17.1972 - val_MinusLogProbMetric: 17.1972 - lr: 8.3333e-05 - 73s/epoch - 371ms/step
Epoch 434/1000
2023-09-28 16:37:13.751 
Epoch 434/1000 
	 loss: 16.2003, MinusLogProbMetric: 16.2003, val_loss: 17.2228, val_MinusLogProbMetric: 17.2228

Epoch 434: val_loss did not improve from 17.14391
196/196 - 71s - loss: 16.2003 - MinusLogProbMetric: 16.2003 - val_loss: 17.2228 - val_MinusLogProbMetric: 17.2228 - lr: 8.3333e-05 - 71s/epoch - 360ms/step
Epoch 435/1000
2023-09-28 16:38:28.328 
Epoch 435/1000 
	 loss: 16.1787, MinusLogProbMetric: 16.1787, val_loss: 17.1887, val_MinusLogProbMetric: 17.1887

Epoch 435: val_loss did not improve from 17.14391
196/196 - 75s - loss: 16.1787 - MinusLogProbMetric: 16.1787 - val_loss: 17.1887 - val_MinusLogProbMetric: 17.1887 - lr: 8.3333e-05 - 75s/epoch - 380ms/step
Epoch 436/1000
2023-09-28 16:39:52.750 
Epoch 436/1000 
	 loss: 16.1878, MinusLogProbMetric: 16.1878, val_loss: 17.4891, val_MinusLogProbMetric: 17.4891

Epoch 436: val_loss did not improve from 17.14391
196/196 - 84s - loss: 16.1878 - MinusLogProbMetric: 16.1878 - val_loss: 17.4891 - val_MinusLogProbMetric: 17.4891 - lr: 8.3333e-05 - 84s/epoch - 431ms/step
Epoch 437/1000
2023-09-28 16:41:15.109 
Epoch 437/1000 
	 loss: 16.1925, MinusLogProbMetric: 16.1925, val_loss: 17.1814, val_MinusLogProbMetric: 17.1814

Epoch 437: val_loss did not improve from 17.14391
196/196 - 82s - loss: 16.1925 - MinusLogProbMetric: 16.1925 - val_loss: 17.1814 - val_MinusLogProbMetric: 17.1814 - lr: 8.3333e-05 - 82s/epoch - 420ms/step
Epoch 438/1000
2023-09-28 16:42:33.160 
Epoch 438/1000 
	 loss: 16.2090, MinusLogProbMetric: 16.2090, val_loss: 17.1863, val_MinusLogProbMetric: 17.1863

Epoch 438: val_loss did not improve from 17.14391
196/196 - 78s - loss: 16.2090 - MinusLogProbMetric: 16.2090 - val_loss: 17.1863 - val_MinusLogProbMetric: 17.1863 - lr: 8.3333e-05 - 78s/epoch - 398ms/step
Epoch 439/1000
2023-09-28 16:43:55.571 
Epoch 439/1000 
	 loss: 16.1703, MinusLogProbMetric: 16.1703, val_loss: 17.2506, val_MinusLogProbMetric: 17.2506

Epoch 439: val_loss did not improve from 17.14391
196/196 - 82s - loss: 16.1703 - MinusLogProbMetric: 16.1703 - val_loss: 17.2506 - val_MinusLogProbMetric: 17.2506 - lr: 8.3333e-05 - 82s/epoch - 420ms/step
Epoch 440/1000
2023-09-28 16:45:21.141 
Epoch 440/1000 
	 loss: 16.1994, MinusLogProbMetric: 16.1994, val_loss: 17.1934, val_MinusLogProbMetric: 17.1934

Epoch 440: val_loss did not improve from 17.14391
196/196 - 86s - loss: 16.1994 - MinusLogProbMetric: 16.1994 - val_loss: 17.1934 - val_MinusLogProbMetric: 17.1934 - lr: 8.3333e-05 - 86s/epoch - 437ms/step
Epoch 441/1000
2023-09-28 16:46:46.291 
Epoch 441/1000 
	 loss: 16.1838, MinusLogProbMetric: 16.1838, val_loss: 17.2743, val_MinusLogProbMetric: 17.2743

Epoch 441: val_loss did not improve from 17.14391
196/196 - 85s - loss: 16.1838 - MinusLogProbMetric: 16.1838 - val_loss: 17.2743 - val_MinusLogProbMetric: 17.2743 - lr: 8.3333e-05 - 85s/epoch - 434ms/step
Epoch 442/1000
2023-09-28 16:48:11.697 
Epoch 442/1000 
	 loss: 16.0989, MinusLogProbMetric: 16.0989, val_loss: 17.2452, val_MinusLogProbMetric: 17.2452

Epoch 442: val_loss did not improve from 17.14391
196/196 - 85s - loss: 16.0989 - MinusLogProbMetric: 16.0989 - val_loss: 17.2452 - val_MinusLogProbMetric: 17.2452 - lr: 4.1667e-05 - 85s/epoch - 436ms/step
Epoch 443/1000
2023-09-28 16:49:38.761 
Epoch 443/1000 
	 loss: 16.0998, MinusLogProbMetric: 16.0998, val_loss: 17.1593, val_MinusLogProbMetric: 17.1593

Epoch 443: val_loss did not improve from 17.14391
196/196 - 87s - loss: 16.0998 - MinusLogProbMetric: 16.0998 - val_loss: 17.1593 - val_MinusLogProbMetric: 17.1593 - lr: 4.1667e-05 - 87s/epoch - 444ms/step
Epoch 444/1000
2023-09-28 16:51:05.345 
Epoch 444/1000 
	 loss: 16.1021, MinusLogProbMetric: 16.1021, val_loss: 17.1540, val_MinusLogProbMetric: 17.1540

Epoch 444: val_loss did not improve from 17.14391
196/196 - 87s - loss: 16.1021 - MinusLogProbMetric: 16.1021 - val_loss: 17.1540 - val_MinusLogProbMetric: 17.1540 - lr: 4.1667e-05 - 87s/epoch - 442ms/step
Epoch 445/1000
2023-09-28 16:52:30.689 
Epoch 445/1000 
	 loss: 16.0981, MinusLogProbMetric: 16.0981, val_loss: 17.1541, val_MinusLogProbMetric: 17.1541

Epoch 445: val_loss did not improve from 17.14391
196/196 - 85s - loss: 16.0981 - MinusLogProbMetric: 16.0981 - val_loss: 17.1541 - val_MinusLogProbMetric: 17.1541 - lr: 4.1667e-05 - 85s/epoch - 435ms/step
Epoch 446/1000
2023-09-28 16:53:57.658 
Epoch 446/1000 
	 loss: 16.1112, MinusLogProbMetric: 16.1112, val_loss: 17.2017, val_MinusLogProbMetric: 17.2017

Epoch 446: val_loss did not improve from 17.14391
196/196 - 87s - loss: 16.1112 - MinusLogProbMetric: 16.1112 - val_loss: 17.2017 - val_MinusLogProbMetric: 17.2017 - lr: 4.1667e-05 - 87s/epoch - 444ms/step
Epoch 447/1000
2023-09-28 16:55:23.631 
Epoch 447/1000 
	 loss: 16.0984, MinusLogProbMetric: 16.0984, val_loss: 17.1946, val_MinusLogProbMetric: 17.1946

Epoch 447: val_loss did not improve from 17.14391
196/196 - 86s - loss: 16.0984 - MinusLogProbMetric: 16.0984 - val_loss: 17.1946 - val_MinusLogProbMetric: 17.1946 - lr: 4.1667e-05 - 86s/epoch - 439ms/step
Epoch 448/1000
2023-09-28 16:56:49.843 
Epoch 448/1000 
	 loss: 16.0980, MinusLogProbMetric: 16.0980, val_loss: 17.1783, val_MinusLogProbMetric: 17.1783

Epoch 448: val_loss did not improve from 17.14391
196/196 - 86s - loss: 16.0980 - MinusLogProbMetric: 16.0980 - val_loss: 17.1783 - val_MinusLogProbMetric: 17.1783 - lr: 4.1667e-05 - 86s/epoch - 440ms/step
Epoch 449/1000
2023-09-28 16:58:14.913 
Epoch 449/1000 
	 loss: 16.0973, MinusLogProbMetric: 16.0973, val_loss: 17.2225, val_MinusLogProbMetric: 17.2225

Epoch 449: val_loss did not improve from 17.14391
196/196 - 85s - loss: 16.0973 - MinusLogProbMetric: 16.0973 - val_loss: 17.2225 - val_MinusLogProbMetric: 17.2225 - lr: 4.1667e-05 - 85s/epoch - 434ms/step
Epoch 450/1000
2023-09-28 16:59:40.475 
Epoch 450/1000 
	 loss: 16.1012, MinusLogProbMetric: 16.1012, val_loss: 17.1710, val_MinusLogProbMetric: 17.1710

Epoch 450: val_loss did not improve from 17.14391
196/196 - 86s - loss: 16.1012 - MinusLogProbMetric: 16.1012 - val_loss: 17.1710 - val_MinusLogProbMetric: 17.1710 - lr: 4.1667e-05 - 86s/epoch - 437ms/step
Epoch 451/1000
2023-09-28 17:01:04.455 
Epoch 451/1000 
	 loss: 16.0995, MinusLogProbMetric: 16.0995, val_loss: 17.1484, val_MinusLogProbMetric: 17.1484

Epoch 451: val_loss did not improve from 17.14391
196/196 - 84s - loss: 16.0995 - MinusLogProbMetric: 16.0995 - val_loss: 17.1484 - val_MinusLogProbMetric: 17.1484 - lr: 4.1667e-05 - 84s/epoch - 428ms/step
Epoch 452/1000
2023-09-28 17:02:29.675 
Epoch 452/1000 
	 loss: 16.1033, MinusLogProbMetric: 16.1033, val_loss: 17.1644, val_MinusLogProbMetric: 17.1644

Epoch 452: val_loss did not improve from 17.14391
196/196 - 85s - loss: 16.1033 - MinusLogProbMetric: 16.1033 - val_loss: 17.1644 - val_MinusLogProbMetric: 17.1644 - lr: 4.1667e-05 - 85s/epoch - 435ms/step
Epoch 453/1000
2023-09-28 17:03:53.428 
Epoch 453/1000 
	 loss: 16.1053, MinusLogProbMetric: 16.1053, val_loss: 17.1527, val_MinusLogProbMetric: 17.1527

Epoch 453: val_loss did not improve from 17.14391
196/196 - 84s - loss: 16.1053 - MinusLogProbMetric: 16.1053 - val_loss: 17.1527 - val_MinusLogProbMetric: 17.1527 - lr: 4.1667e-05 - 84s/epoch - 427ms/step
Epoch 454/1000
2023-09-28 17:05:17.850 
Epoch 454/1000 
	 loss: 16.0948, MinusLogProbMetric: 16.0948, val_loss: 17.1651, val_MinusLogProbMetric: 17.1651

Epoch 454: val_loss did not improve from 17.14391
196/196 - 84s - loss: 16.0948 - MinusLogProbMetric: 16.0948 - val_loss: 17.1651 - val_MinusLogProbMetric: 17.1651 - lr: 4.1667e-05 - 84s/epoch - 431ms/step
Epoch 455/1000
2023-09-28 17:06:41.644 
Epoch 455/1000 
	 loss: 16.0946, MinusLogProbMetric: 16.0946, val_loss: 17.1894, val_MinusLogProbMetric: 17.1894

Epoch 455: val_loss did not improve from 17.14391
196/196 - 84s - loss: 16.0946 - MinusLogProbMetric: 16.0946 - val_loss: 17.1894 - val_MinusLogProbMetric: 17.1894 - lr: 4.1667e-05 - 84s/epoch - 428ms/step
Epoch 456/1000
2023-09-28 17:08:03.178 
Epoch 456/1000 
	 loss: 16.0925, MinusLogProbMetric: 16.0925, val_loss: 17.1571, val_MinusLogProbMetric: 17.1571

Epoch 456: val_loss did not improve from 17.14391
196/196 - 82s - loss: 16.0925 - MinusLogProbMetric: 16.0925 - val_loss: 17.1571 - val_MinusLogProbMetric: 17.1571 - lr: 4.1667e-05 - 82s/epoch - 416ms/step
Epoch 457/1000
2023-09-28 17:09:27.714 
Epoch 457/1000 
	 loss: 16.0929, MinusLogProbMetric: 16.0929, val_loss: 17.1561, val_MinusLogProbMetric: 17.1561

Epoch 457: val_loss did not improve from 17.14391
196/196 - 85s - loss: 16.0929 - MinusLogProbMetric: 16.0929 - val_loss: 17.1561 - val_MinusLogProbMetric: 17.1561 - lr: 4.1667e-05 - 85s/epoch - 431ms/step
Epoch 458/1000
2023-09-28 17:10:52.293 
Epoch 458/1000 
	 loss: 16.0935, MinusLogProbMetric: 16.0935, val_loss: 17.1667, val_MinusLogProbMetric: 17.1667

Epoch 458: val_loss did not improve from 17.14391
196/196 - 85s - loss: 16.0935 - MinusLogProbMetric: 16.0935 - val_loss: 17.1667 - val_MinusLogProbMetric: 17.1667 - lr: 4.1667e-05 - 85s/epoch - 431ms/step
Epoch 459/1000
2023-09-28 17:12:15.358 
Epoch 459/1000 
	 loss: 16.0929, MinusLogProbMetric: 16.0929, val_loss: 17.1635, val_MinusLogProbMetric: 17.1635

Epoch 459: val_loss did not improve from 17.14391
196/196 - 83s - loss: 16.0929 - MinusLogProbMetric: 16.0929 - val_loss: 17.1635 - val_MinusLogProbMetric: 17.1635 - lr: 4.1667e-05 - 83s/epoch - 424ms/step
Epoch 460/1000
2023-09-28 17:13:37.032 
Epoch 460/1000 
	 loss: 16.0874, MinusLogProbMetric: 16.0874, val_loss: 17.1794, val_MinusLogProbMetric: 17.1794

Epoch 460: val_loss did not improve from 17.14391
196/196 - 82s - loss: 16.0874 - MinusLogProbMetric: 16.0874 - val_loss: 17.1794 - val_MinusLogProbMetric: 17.1794 - lr: 4.1667e-05 - 82s/epoch - 417ms/step
Epoch 461/1000
2023-09-28 17:14:57.245 
Epoch 461/1000 
	 loss: 16.0913, MinusLogProbMetric: 16.0913, val_loss: 17.2225, val_MinusLogProbMetric: 17.2225

Epoch 461: val_loss did not improve from 17.14391
196/196 - 80s - loss: 16.0913 - MinusLogProbMetric: 16.0913 - val_loss: 17.2225 - val_MinusLogProbMetric: 17.2225 - lr: 4.1667e-05 - 80s/epoch - 409ms/step
Epoch 462/1000
2023-09-28 17:16:12.165 
Epoch 462/1000 
	 loss: 16.1051, MinusLogProbMetric: 16.1051, val_loss: 17.1656, val_MinusLogProbMetric: 17.1656

Epoch 462: val_loss did not improve from 17.14391
196/196 - 75s - loss: 16.1051 - MinusLogProbMetric: 16.1051 - val_loss: 17.1656 - val_MinusLogProbMetric: 17.1656 - lr: 4.1667e-05 - 75s/epoch - 382ms/step
Epoch 463/1000
2023-09-28 17:17:36.481 
Epoch 463/1000 
	 loss: 16.0862, MinusLogProbMetric: 16.0862, val_loss: 17.1783, val_MinusLogProbMetric: 17.1783

Epoch 463: val_loss did not improve from 17.14391
196/196 - 84s - loss: 16.0862 - MinusLogProbMetric: 16.0862 - val_loss: 17.1783 - val_MinusLogProbMetric: 17.1783 - lr: 4.1667e-05 - 84s/epoch - 430ms/step
Epoch 464/1000
2023-09-28 17:19:02.049 
Epoch 464/1000 
	 loss: 16.1063, MinusLogProbMetric: 16.1063, val_loss: 17.1617, val_MinusLogProbMetric: 17.1617

Epoch 464: val_loss did not improve from 17.14391
196/196 - 86s - loss: 16.1063 - MinusLogProbMetric: 16.1063 - val_loss: 17.1617 - val_MinusLogProbMetric: 17.1617 - lr: 4.1667e-05 - 86s/epoch - 437ms/step
Epoch 465/1000
2023-09-28 17:20:26.193 
Epoch 465/1000 
	 loss: 16.0948, MinusLogProbMetric: 16.0948, val_loss: 17.2126, val_MinusLogProbMetric: 17.2126

Epoch 465: val_loss did not improve from 17.14391
196/196 - 84s - loss: 16.0948 - MinusLogProbMetric: 16.0948 - val_loss: 17.2126 - val_MinusLogProbMetric: 17.2126 - lr: 4.1667e-05 - 84s/epoch - 429ms/step
Epoch 466/1000
2023-09-28 17:21:52.702 
Epoch 466/1000 
	 loss: 16.0921, MinusLogProbMetric: 16.0921, val_loss: 17.2790, val_MinusLogProbMetric: 17.2790

Epoch 466: val_loss did not improve from 17.14391
196/196 - 87s - loss: 16.0921 - MinusLogProbMetric: 16.0921 - val_loss: 17.2790 - val_MinusLogProbMetric: 17.2790 - lr: 4.1667e-05 - 87s/epoch - 441ms/step
Epoch 467/1000
2023-09-28 17:23:14.330 
Epoch 467/1000 
	 loss: 16.1001, MinusLogProbMetric: 16.1001, val_loss: 17.1618, val_MinusLogProbMetric: 17.1618

Epoch 467: val_loss did not improve from 17.14391
196/196 - 82s - loss: 16.1001 - MinusLogProbMetric: 16.1001 - val_loss: 17.1618 - val_MinusLogProbMetric: 17.1618 - lr: 4.1667e-05 - 82s/epoch - 416ms/step
Epoch 468/1000
2023-09-28 17:24:38.443 
Epoch 468/1000 
	 loss: 16.0913, MinusLogProbMetric: 16.0913, val_loss: 17.1732, val_MinusLogProbMetric: 17.1732

Epoch 468: val_loss did not improve from 17.14391
196/196 - 84s - loss: 16.0913 - MinusLogProbMetric: 16.0913 - val_loss: 17.1732 - val_MinusLogProbMetric: 17.1732 - lr: 4.1667e-05 - 84s/epoch - 429ms/step
Epoch 469/1000
2023-09-28 17:26:01.080 
Epoch 469/1000 
	 loss: 16.0985, MinusLogProbMetric: 16.0985, val_loss: 17.1638, val_MinusLogProbMetric: 17.1638

Epoch 469: val_loss did not improve from 17.14391
196/196 - 83s - loss: 16.0985 - MinusLogProbMetric: 16.0985 - val_loss: 17.1638 - val_MinusLogProbMetric: 17.1638 - lr: 4.1667e-05 - 83s/epoch - 422ms/step
Epoch 470/1000
2023-09-28 17:27:24.575 
Epoch 470/1000 
	 loss: 16.0901, MinusLogProbMetric: 16.0901, val_loss: 17.1494, val_MinusLogProbMetric: 17.1494

Epoch 470: val_loss did not improve from 17.14391
196/196 - 83s - loss: 16.0901 - MinusLogProbMetric: 16.0901 - val_loss: 17.1494 - val_MinusLogProbMetric: 17.1494 - lr: 4.1667e-05 - 83s/epoch - 426ms/step
Epoch 471/1000
2023-09-28 17:28:45.851 
Epoch 471/1000 
	 loss: 16.0887, MinusLogProbMetric: 16.0887, val_loss: 17.1496, val_MinusLogProbMetric: 17.1496

Epoch 471: val_loss did not improve from 17.14391
196/196 - 81s - loss: 16.0887 - MinusLogProbMetric: 16.0887 - val_loss: 17.1496 - val_MinusLogProbMetric: 17.1496 - lr: 4.1667e-05 - 81s/epoch - 415ms/step
Epoch 472/1000
2023-09-28 17:30:08.397 
Epoch 472/1000 
	 loss: 16.0969, MinusLogProbMetric: 16.0969, val_loss: 17.1977, val_MinusLogProbMetric: 17.1977

Epoch 472: val_loss did not improve from 17.14391
196/196 - 83s - loss: 16.0969 - MinusLogProbMetric: 16.0969 - val_loss: 17.1977 - val_MinusLogProbMetric: 17.1977 - lr: 4.1667e-05 - 83s/epoch - 421ms/step
Epoch 473/1000
2023-09-28 17:31:30.468 
Epoch 473/1000 
	 loss: 16.0886, MinusLogProbMetric: 16.0886, val_loss: 17.1949, val_MinusLogProbMetric: 17.1949

Epoch 473: val_loss did not improve from 17.14391
196/196 - 82s - loss: 16.0886 - MinusLogProbMetric: 16.0886 - val_loss: 17.1949 - val_MinusLogProbMetric: 17.1949 - lr: 4.1667e-05 - 82s/epoch - 419ms/step
Epoch 474/1000
2023-09-28 17:32:52.092 
Epoch 474/1000 
	 loss: 16.0919, MinusLogProbMetric: 16.0919, val_loss: 17.1810, val_MinusLogProbMetric: 17.1810

Epoch 474: val_loss did not improve from 17.14391
196/196 - 82s - loss: 16.0919 - MinusLogProbMetric: 16.0919 - val_loss: 17.1810 - val_MinusLogProbMetric: 17.1810 - lr: 4.1667e-05 - 82s/epoch - 416ms/step
Epoch 475/1000
2023-09-28 17:34:17.431 
Epoch 475/1000 
	 loss: 16.0827, MinusLogProbMetric: 16.0827, val_loss: 17.1669, val_MinusLogProbMetric: 17.1669

Epoch 475: val_loss did not improve from 17.14391
196/196 - 85s - loss: 16.0827 - MinusLogProbMetric: 16.0827 - val_loss: 17.1669 - val_MinusLogProbMetric: 17.1669 - lr: 4.1667e-05 - 85s/epoch - 435ms/step
Epoch 476/1000
2023-09-28 17:35:43.148 
Epoch 476/1000 
	 loss: 16.0837, MinusLogProbMetric: 16.0837, val_loss: 17.2273, val_MinusLogProbMetric: 17.2273

Epoch 476: val_loss did not improve from 17.14391
196/196 - 86s - loss: 16.0837 - MinusLogProbMetric: 16.0837 - val_loss: 17.2273 - val_MinusLogProbMetric: 17.2273 - lr: 4.1667e-05 - 86s/epoch - 437ms/step
Epoch 477/1000
2023-09-28 17:37:09.257 
Epoch 477/1000 
	 loss: 16.0865, MinusLogProbMetric: 16.0865, val_loss: 17.2193, val_MinusLogProbMetric: 17.2193

Epoch 477: val_loss did not improve from 17.14391
196/196 - 86s - loss: 16.0865 - MinusLogProbMetric: 16.0865 - val_loss: 17.2193 - val_MinusLogProbMetric: 17.2193 - lr: 4.1667e-05 - 86s/epoch - 439ms/step
Epoch 478/1000
2023-09-28 17:38:34.629 
Epoch 478/1000 
	 loss: 16.0958, MinusLogProbMetric: 16.0958, val_loss: 17.2271, val_MinusLogProbMetric: 17.2271

Epoch 478: val_loss did not improve from 17.14391
196/196 - 85s - loss: 16.0958 - MinusLogProbMetric: 16.0958 - val_loss: 17.2271 - val_MinusLogProbMetric: 17.2271 - lr: 4.1667e-05 - 85s/epoch - 436ms/step
Epoch 479/1000
2023-09-28 17:40:01.117 
Epoch 479/1000 
	 loss: 16.0957, MinusLogProbMetric: 16.0957, val_loss: 17.2039, val_MinusLogProbMetric: 17.2039

Epoch 479: val_loss did not improve from 17.14391
196/196 - 87s - loss: 16.0957 - MinusLogProbMetric: 16.0957 - val_loss: 17.2039 - val_MinusLogProbMetric: 17.2039 - lr: 4.1667e-05 - 87s/epoch - 441ms/step
Epoch 480/1000
2023-09-28 17:41:27.007 
Epoch 480/1000 
	 loss: 16.0820, MinusLogProbMetric: 16.0820, val_loss: 17.1869, val_MinusLogProbMetric: 17.1869

Epoch 480: val_loss did not improve from 17.14391
196/196 - 86s - loss: 16.0820 - MinusLogProbMetric: 16.0820 - val_loss: 17.1869 - val_MinusLogProbMetric: 17.1869 - lr: 4.1667e-05 - 86s/epoch - 438ms/step
Epoch 481/1000
2023-09-28 17:42:51.557 
Epoch 481/1000 
	 loss: 16.0938, MinusLogProbMetric: 16.0938, val_loss: 17.1722, val_MinusLogProbMetric: 17.1722

Epoch 481: val_loss did not improve from 17.14391
196/196 - 85s - loss: 16.0938 - MinusLogProbMetric: 16.0938 - val_loss: 17.1722 - val_MinusLogProbMetric: 17.1722 - lr: 4.1667e-05 - 85s/epoch - 431ms/step
Epoch 482/1000
2023-09-28 17:44:17.225 
Epoch 482/1000 
	 loss: 16.0938, MinusLogProbMetric: 16.0938, val_loss: 17.1536, val_MinusLogProbMetric: 17.1536

Epoch 482: val_loss did not improve from 17.14391
196/196 - 86s - loss: 16.0938 - MinusLogProbMetric: 16.0938 - val_loss: 17.1536 - val_MinusLogProbMetric: 17.1536 - lr: 4.1667e-05 - 86s/epoch - 437ms/step
Epoch 483/1000
2023-09-28 17:45:41.930 
Epoch 483/1000 
	 loss: 16.0945, MinusLogProbMetric: 16.0945, val_loss: 17.1959, val_MinusLogProbMetric: 17.1959

Epoch 483: val_loss did not improve from 17.14391
196/196 - 85s - loss: 16.0945 - MinusLogProbMetric: 16.0945 - val_loss: 17.1959 - val_MinusLogProbMetric: 17.1959 - lr: 4.1667e-05 - 85s/epoch - 432ms/step
Epoch 484/1000
2023-09-28 17:47:07.154 
Epoch 484/1000 
	 loss: 16.0797, MinusLogProbMetric: 16.0797, val_loss: 17.1736, val_MinusLogProbMetric: 17.1736

Epoch 484: val_loss did not improve from 17.14391
196/196 - 85s - loss: 16.0797 - MinusLogProbMetric: 16.0797 - val_loss: 17.1736 - val_MinusLogProbMetric: 17.1736 - lr: 4.1667e-05 - 85s/epoch - 435ms/step
Epoch 485/1000
2023-09-28 17:48:32.481 
Epoch 485/1000 
	 loss: 16.0933, MinusLogProbMetric: 16.0933, val_loss: 17.1859, val_MinusLogProbMetric: 17.1859

Epoch 485: val_loss did not improve from 17.14391
196/196 - 85s - loss: 16.0933 - MinusLogProbMetric: 16.0933 - val_loss: 17.1859 - val_MinusLogProbMetric: 17.1859 - lr: 4.1667e-05 - 85s/epoch - 435ms/step
Epoch 486/1000
2023-09-28 17:49:57.765 
Epoch 486/1000 
	 loss: 16.0830, MinusLogProbMetric: 16.0830, val_loss: 17.3958, val_MinusLogProbMetric: 17.3958

Epoch 486: val_loss did not improve from 17.14391
196/196 - 85s - loss: 16.0830 - MinusLogProbMetric: 16.0830 - val_loss: 17.3958 - val_MinusLogProbMetric: 17.3958 - lr: 4.1667e-05 - 85s/epoch - 435ms/step
Epoch 487/1000
2023-09-28 17:51:22.254 
Epoch 487/1000 
	 loss: 16.0951, MinusLogProbMetric: 16.0951, val_loss: 17.2219, val_MinusLogProbMetric: 17.2219

Epoch 487: val_loss did not improve from 17.14391
196/196 - 84s - loss: 16.0951 - MinusLogProbMetric: 16.0951 - val_loss: 17.2219 - val_MinusLogProbMetric: 17.2219 - lr: 4.1667e-05 - 84s/epoch - 431ms/step
Epoch 488/1000
2023-09-28 17:52:47.247 
Epoch 488/1000 
	 loss: 16.0880, MinusLogProbMetric: 16.0880, val_loss: 17.2091, val_MinusLogProbMetric: 17.2091

Epoch 488: val_loss did not improve from 17.14391
196/196 - 85s - loss: 16.0880 - MinusLogProbMetric: 16.0880 - val_loss: 17.2091 - val_MinusLogProbMetric: 17.2091 - lr: 4.1667e-05 - 85s/epoch - 434ms/step
Epoch 489/1000
2023-09-28 17:54:11.610 
Epoch 489/1000 
	 loss: 16.0887, MinusLogProbMetric: 16.0887, val_loss: 17.1570, val_MinusLogProbMetric: 17.1570

Epoch 489: val_loss did not improve from 17.14391
196/196 - 84s - loss: 16.0887 - MinusLogProbMetric: 16.0887 - val_loss: 17.1570 - val_MinusLogProbMetric: 17.1570 - lr: 4.1667e-05 - 84s/epoch - 430ms/step
Epoch 490/1000
2023-09-28 17:55:36.226 
Epoch 490/1000 
	 loss: 16.0811, MinusLogProbMetric: 16.0811, val_loss: 17.2502, val_MinusLogProbMetric: 17.2502

Epoch 490: val_loss did not improve from 17.14391
196/196 - 85s - loss: 16.0811 - MinusLogProbMetric: 16.0811 - val_loss: 17.2502 - val_MinusLogProbMetric: 17.2502 - lr: 4.1667e-05 - 85s/epoch - 432ms/step
Epoch 491/1000
2023-09-28 17:57:01.406 
Epoch 491/1000 
	 loss: 16.0889, MinusLogProbMetric: 16.0889, val_loss: 17.1899, val_MinusLogProbMetric: 17.1899

Epoch 491: val_loss did not improve from 17.14391
Restoring model weights from the end of the best epoch: 391.
196/196 - 86s - loss: 16.0889 - MinusLogProbMetric: 16.0889 - val_loss: 17.1899 - val_MinusLogProbMetric: 17.1899 - lr: 4.1667e-05 - 86s/epoch - 439ms/step
Epoch 491: early stopping
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.

------------------------------------------
Starting LR metric calculation...
Running TF LR calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
LR metric calculation completed in 38.58843197999522 seconds.

------------------------------------------
Starting KS tests calculation...
Running TF KS tests...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
KS tests calculation completed in 19.31327937799506 seconds.

------------------------------------------
Starting SWD metric calculation...
Running TF SWD calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
SWD metric calculation completed in 15.021446029015351 seconds.

------------------------------------------
Starting FN metric calculation...
Running TF FN calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
FN metric calculation completed in 15.880915225017816 seconds.
Training succeeded with seed 933.
Model trained in 39969.11 s.

===========
Computing predictions
===========

Computing metrics...
Metrics computed in 91.08 s.
                ===========
                print("===========
Failed to plot
===========
")
                Exception type: ValueError
                Exception message: Number of rows must be a positive integer, not 0
                Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN.py , Line : 469, Func.Name : prediction_function, Message : Plotters.cornerplotter(X_data_test.numpy(),X_data_nf.numpy(),path_to_results,ndims,norm=True) # type: ignore', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/Plotters.py , Line : 156, Func.Name : cornerplotter, Message : figure=corner.corner(target_samples,color=\'red\',bins=n_bins,labels=[r"%s" % s for s in labels],normalize1d=True)', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/corner.py , Line : 235, Func.Name : corner, Message : fig, axes = pl.subplots(K, K, figsize=(dim, dim))', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/pyplot.py , Line : 1502, Func.Name : subplots, Message : axs = fig.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 905, Func.Name : subplots, Message : gs = self.add_gridspec(nrows, ncols, figure=self, **gridspec_kw)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 1527, Func.Name : add_gridspec, Message : gs = GridSpec(nrows=nrows, ncols=ncols, figure=self, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 379, Func.Name : __init__, Message : super().__init__(nrows, ncols,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 49, Func.Name : __init__, Message : raise ValueError(']
                ===========

results.txt saved
results.json saved
Results log saved
Model predictions computed in 91.39 s.
===========
Run 320/720 done in 40294.32 s.
===========

Directory ../../results/CsplineN_new/run_321/ already exists.
Skipping it.
===========
Run 321/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_322/ already exists.
Skipping it.
===========
Run 322/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_323/ already exists.
Skipping it.
===========
Run 323/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_324/ already exists.
Skipping it.
===========
Run 324/720 already exists. Skipping it.
===========

===========
Generating train data for run 325.
===========
Train data generated in 0.24 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_325/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_325/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_325/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_325
self.data_kwargs: {'seed': 0}
self.x_data: [[ 6.6721983   2.7232099   6.150583   ...  3.6440284   4.565682
   2.645935  ]
 [ 1.86333     2.9602146   9.095053   ...  6.366897   -0.51058275
   3.6708503 ]
 [ 5.5981846   7.771953    4.990416   ...  1.6793485   6.8585963
   1.4719678 ]
 ...
 [ 1.690249    4.096276    8.408299   ...  5.907632    0.7324666
   3.17104   ]
 [ 1.5949699   2.976067    6.189886   ...  6.072563    1.3667407
   3.7841148 ]
 [ 6.6605377   2.9536047   6.141097   ...  3.8587723   3.7946694
   1.5606189 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_93"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_94 (InputLayer)       [(None, 64)]              0         
                                                                 
 log_prob_layer_8 (LogProbLa  (None,)                  1321920   
 yer)                                                            
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_8/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_8'")
self.model: <keras.engine.functional.Functional object at 0x7fb1ed727760>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fb1ed95bd30>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fb1ed95bd30>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fb1ed385330>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fb144107820>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_325/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fb144107d90>, <keras.callbacks.ModelCheckpoint object at 0x7fb144107e50>, <keras.callbacks.EarlyStopping object at 0x7fb144107fd0>, <keras.callbacks.ReduceLROnPlateau object at 0x7fb144107ee0>, <keras.callbacks.TerminateOnNaN object at 0x7fb144107f10>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_325/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 325/720 with hyperparameters:
timestamp = 2023-09-28 17:58:42.407861
ndims = 64
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 6.6721983   2.7232099   6.150583    4.580923    1.5652926   2.6565456
  7.1790314   4.9339914   5.7110434   6.7498174   6.386396    3.9689803
  8.906224    3.806384    4.1225185   9.359665    7.9177184   7.261933
  0.1027934   9.199222    7.196588   10.08044     2.3912797   8.714102
  2.4626331   6.1591697   1.6348177   8.815128    7.787716    5.5076594
  3.3412566   0.7007286   7.2919664   4.3464866   6.6338673   7.1474853
  9.7648      8.511822   -0.5374965   5.0141335   7.365311    0.6638582
  5.9037857   0.7669884   1.383983    0.33108917  8.1157875   2.689867
  4.5051117   9.846115    7.6886387   0.35349095  2.5627694   4.5428576
  5.8595724   2.7118683   9.446846    6.2642756   4.7844443   6.0548897
  8.103386    3.6440284   4.565682    2.645935  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-28 18:01:31.389 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7476.8237, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 169s - loss: nan - MinusLogProbMetric: 7476.8237 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 169s/epoch - 862ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 0.0003333333333333333.
===========
Generating train data for run 325.
===========
Train data generated in 0.31 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_325/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_325/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_325/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_325
self.data_kwargs: {'seed': 0}
self.x_data: [[ 6.6721983   2.7232099   6.150583   ...  3.6440284   4.565682
   2.645935  ]
 [ 1.86333     2.9602146   9.095053   ...  6.366897   -0.51058275
   3.6708503 ]
 [ 5.5981846   7.771953    4.990416   ...  1.6793485   6.8585963
   1.4719678 ]
 ...
 [ 1.690249    4.096276    8.408299   ...  5.907632    0.7324666
   3.17104   ]
 [ 1.5949699   2.976067    6.189886   ...  6.072563    1.3667407
   3.7841148 ]
 [ 6.6605377   2.9536047   6.141097   ...  3.8587723   3.7946694
   1.5606189 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_104"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_105 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_9 (LogProbLa  (None,)                  1321920   
 yer)                                                            
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_9/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_9'")
self.model: <keras.engine.functional.Functional object at 0x7fb488720700>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fb1dfe564d0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fb1dfe564d0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fb21c2a0490>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fb1dfc12230>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_325/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fb1dfc127a0>, <keras.callbacks.ModelCheckpoint object at 0x7fb1dfc12860>, <keras.callbacks.EarlyStopping object at 0x7fb1dfc12ad0>, <keras.callbacks.ReduceLROnPlateau object at 0x7fb1dfc12b00>, <keras.callbacks.TerminateOnNaN object at 0x7fb1dfc12740>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_325/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 325/720 with hyperparameters:
timestamp = 2023-09-28 18:01:43.059600
ndims = 64
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 6.6721983   2.7232099   6.150583    4.580923    1.5652926   2.6565456
  7.1790314   4.9339914   5.7110434   6.7498174   6.386396    3.9689803
  8.906224    3.806384    4.1225185   9.359665    7.9177184   7.261933
  0.1027934   9.199222    7.196588   10.08044     2.3912797   8.714102
  2.4626331   6.1591697   1.6348177   8.815128    7.787716    5.5076594
  3.3412566   0.7007286   7.2919664   4.3464866   6.6338673   7.1474853
  9.7648      8.511822   -0.5374965   5.0141335   7.365311    0.6638582
  5.9037857   0.7669884   1.383983    0.33108917  8.1157875   2.689867
  4.5051117   9.846115    7.6886387   0.35349095  2.5627694   4.5428576
  5.8595724   2.7118683   9.446846    6.2642756   4.7844443   6.0548897
  8.103386    3.6440284   4.565682    2.645935  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-28 18:04:21.386 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7476.8237, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 158s - loss: nan - MinusLogProbMetric: 7476.8237 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 158s/epoch - 807ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 0.0001111111111111111.
===========
Generating train data for run 325.
===========
Train data generated in 0.40 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_325/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_325/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_325/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_325
self.data_kwargs: {'seed': 0}
self.x_data: [[ 6.6721983   2.7232099   6.150583   ...  3.6440284   4.565682
   2.645935  ]
 [ 1.86333     2.9602146   9.095053   ...  6.366897   -0.51058275
   3.6708503 ]
 [ 5.5981846   7.771953    4.990416   ...  1.6793485   6.8585963
   1.4719678 ]
 ...
 [ 1.690249    4.096276    8.408299   ...  5.907632    0.7324666
   3.17104   ]
 [ 1.5949699   2.976067    6.189886   ...  6.072563    1.3667407
   3.7841148 ]
 [ 6.6605377   2.9536047   6.141097   ...  3.8587723   3.7946694
   1.5606189 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_115"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_116 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_10 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_10/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_10'")
self.model: <keras.engine.functional.Functional object at 0x7fb328764130>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fb3287f1930>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fb3287f1930>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fb3306b9e40>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fb8a92c8d00>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_325/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fb8a92cb460>, <keras.callbacks.ModelCheckpoint object at 0x7fb8a92ca6e0>, <keras.callbacks.EarlyStopping object at 0x7fb8a92ca620>, <keras.callbacks.ReduceLROnPlateau object at 0x7fb8a92ca470>, <keras.callbacks.TerminateOnNaN object at 0x7fb8a92c9b10>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_325/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 325/720 with hyperparameters:
timestamp = 2023-09-28 18:04:32.879936
ndims = 64
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 6.6721983   2.7232099   6.150583    4.580923    1.5652926   2.6565456
  7.1790314   4.9339914   5.7110434   6.7498174   6.386396    3.9689803
  8.906224    3.806384    4.1225185   9.359665    7.9177184   7.261933
  0.1027934   9.199222    7.196588   10.08044     2.3912797   8.714102
  2.4626331   6.1591697   1.6348177   8.815128    7.787716    5.5076594
  3.3412566   0.7007286   7.2919664   4.3464866   6.6338673   7.1474853
  9.7648      8.511822   -0.5374965   5.0141335   7.365311    0.6638582
  5.9037857   0.7669884   1.383983    0.33108917  8.1157875   2.689867
  4.5051117   9.846115    7.6886387   0.35349095  2.5627694   4.5428576
  5.8595724   2.7118683   9.446846    6.2642756   4.7844443   6.0548897
  8.103386    3.6440284   4.565682    2.645935  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-28 18:07:23.266 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7476.8237, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 170s - loss: nan - MinusLogProbMetric: 7476.8237 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.1111e-04 - 170s/epoch - 868ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 3.703703703703703e-05.
===========
Generating train data for run 325.
===========
Train data generated in 0.28 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_325/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_325/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_325/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_325
self.data_kwargs: {'seed': 0}
self.x_data: [[ 6.6721983   2.7232099   6.150583   ...  3.6440284   4.565682
   2.645935  ]
 [ 1.86333     2.9602146   9.095053   ...  6.366897   -0.51058275
   3.6708503 ]
 [ 5.5981846   7.771953    4.990416   ...  1.6793485   6.8585963
   1.4719678 ]
 ...
 [ 1.690249    4.096276    8.408299   ...  5.907632    0.7324666
   3.17104   ]
 [ 1.5949699   2.976067    6.189886   ...  6.072563    1.3667407
   3.7841148 ]
 [ 6.6605377   2.9536047   6.141097   ...  3.8587723   3.7946694
   1.5606189 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_126"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_127 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_11 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_11/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_11'")
self.model: <keras.engine.functional.Functional object at 0x7fb14c1c5060>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fb3944499f0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fb3944499f0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fb3947d7be0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fb2b83826b0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_325/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fb2b8381c00>, <keras.callbacks.ModelCheckpoint object at 0x7fb2b8380580>, <keras.callbacks.EarlyStopping object at 0x7fb2b8380910>, <keras.callbacks.ReduceLROnPlateau object at 0x7fb2b8381d80>, <keras.callbacks.TerminateOnNaN object at 0x7fb2b8382b30>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_325/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 325/720 with hyperparameters:
timestamp = 2023-09-28 18:07:32.747939
ndims = 64
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 3.703703703703703e-05...
Train first sample: [ 6.6721983   2.7232099   6.150583    4.580923    1.5652926   2.6565456
  7.1790314   4.9339914   5.7110434   6.7498174   6.386396    3.9689803
  8.906224    3.806384    4.1225185   9.359665    7.9177184   7.261933
  0.1027934   9.199222    7.196588   10.08044     2.3912797   8.714102
  2.4626331   6.1591697   1.6348177   8.815128    7.787716    5.5076594
  3.3412566   0.7007286   7.2919664   4.3464866   6.6338673   7.1474853
  9.7648      8.511822   -0.5374965   5.0141335   7.365311    0.6638582
  5.9037857   0.7669884   1.383983    0.33108917  8.1157875   2.689867
  4.5051117   9.846115    7.6886387   0.35349095  2.5627694   4.5428576
  5.8595724   2.7118683   9.446846    6.2642756   4.7844443   6.0548897
  8.103386    3.6440284   4.565682    2.645935  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
WARNING:tensorflow:5 out of the last 96240 calls to <function Model.make_train_function.<locals>.train_function at 0x7fb3505cf640> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-28 18:09:51.777 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7476.8237, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 139s - loss: nan - MinusLogProbMetric: 7476.8237 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.7037e-05 - 139s/epoch - 708ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 1.2345679012345677e-05.
===========
Generating train data for run 325.
===========
Train data generated in 0.17 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_325/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_325/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_325/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_325
self.data_kwargs: {'seed': 0}
self.x_data: [[ 6.6721983   2.7232099   6.150583   ...  3.6440284   4.565682
   2.645935  ]
 [ 1.86333     2.9602146   9.095053   ...  6.366897   -0.51058275
   3.6708503 ]
 [ 5.5981846   7.771953    4.990416   ...  1.6793485   6.8585963
   1.4719678 ]
 ...
 [ 1.690249    4.096276    8.408299   ...  5.907632    0.7324666
   3.17104   ]
 [ 1.5949699   2.976067    6.189886   ...  6.072563    1.3667407
   3.7841148 ]
 [ 6.6605377   2.9536047   6.141097   ...  3.8587723   3.7946694
   1.5606189 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_137"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_138 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_12 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_12/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_12'")
self.model: <keras.engine.functional.Functional object at 0x7fb8a065eaa0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fb8a068b640>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fb8a068b640>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fb350105f30>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fb350106ce0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_325/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fb350107fd0>, <keras.callbacks.ModelCheckpoint object at 0x7fb350104580>, <keras.callbacks.EarlyStopping object at 0x7fb3501060e0>, <keras.callbacks.ReduceLROnPlateau object at 0x7fb3501055a0>, <keras.callbacks.TerminateOnNaN object at 0x7fb350105d20>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_325/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 325/720 with hyperparameters:
timestamp = 2023-09-28 18:10:11.766747
ndims = 64
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.2345679012345677e-05...
Train first sample: [ 6.6721983   2.7232099   6.150583    4.580923    1.5652926   2.6565456
  7.1790314   4.9339914   5.7110434   6.7498174   6.386396    3.9689803
  8.906224    3.806384    4.1225185   9.359665    7.9177184   7.261933
  0.1027934   9.199222    7.196588   10.08044     2.3912797   8.714102
  2.4626331   6.1591697   1.6348177   8.815128    7.787716    5.5076594
  3.3412566   0.7007286   7.2919664   4.3464866   6.6338673   7.1474853
  9.7648      8.511822   -0.5374965   5.0141335   7.365311    0.6638582
  5.9037857   0.7669884   1.383983    0.33108917  8.1157875   2.689867
  4.5051117   9.846115    7.6886387   0.35349095  2.5627694   4.5428576
  5.8595724   2.7118683   9.446846    6.2642756   4.7844443   6.0548897
  8.103386    3.6440284   4.565682    2.645935  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
WARNING:tensorflow:6 out of the last 96241 calls to <function Model.make_train_function.<locals>.train_function at 0x7fae797300d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-28 18:12:52.152 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7476.8237, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 160s - loss: nan - MinusLogProbMetric: 7476.8237 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.2346e-05 - 160s/epoch - 817ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 4.115226337448558e-06.
===========
Generating train data for run 325.
===========
Train data generated in 0.24 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_325/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_325/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_325/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_325
self.data_kwargs: {'seed': 0}
self.x_data: [[ 6.6721983   2.7232099   6.150583   ...  3.6440284   4.565682
   2.645935  ]
 [ 1.86333     2.9602146   9.095053   ...  6.366897   -0.51058275
   3.6708503 ]
 [ 5.5981846   7.771953    4.990416   ...  1.6793485   6.8585963
   1.4719678 ]
 ...
 [ 1.690249    4.096276    8.408299   ...  5.907632    0.7324666
   3.17104   ]
 [ 1.5949699   2.976067    6.189886   ...  6.072563    1.3667407
   3.7841148 ]
 [ 6.6605377   2.9536047   6.141097   ...  3.8587723   3.7946694
   1.5606189 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_148"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_149 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_13 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_13/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_13'")
self.model: <keras.engine.functional.Functional object at 0x7fb897ab4f70>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7faeba94cac0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7faeba94cac0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fb897d92380>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fb86d5d3a30>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_325/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fb86d5d3fa0>, <keras.callbacks.ModelCheckpoint object at 0x7fb86d5d3f40>, <keras.callbacks.EarlyStopping object at 0x7fb86d5d3f70>, <keras.callbacks.ReduceLROnPlateau object at 0x7fb86d604130>, <keras.callbacks.TerminateOnNaN object at 0x7fb86d604310>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_325/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 325/720 with hyperparameters:
timestamp = 2023-09-28 18:13:03.016286
ndims = 64
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.115226337448558e-06...
Train first sample: [ 6.6721983   2.7232099   6.150583    4.580923    1.5652926   2.6565456
  7.1790314   4.9339914   5.7110434   6.7498174   6.386396    3.9689803
  8.906224    3.806384    4.1225185   9.359665    7.9177184   7.261933
  0.1027934   9.199222    7.196588   10.08044     2.3912797   8.714102
  2.4626331   6.1591697   1.6348177   8.815128    7.787716    5.5076594
  3.3412566   0.7007286   7.2919664   4.3464866   6.6338673   7.1474853
  9.7648      8.511822   -0.5374965   5.0141335   7.365311    0.6638582
  5.9037857   0.7669884   1.383983    0.33108917  8.1157875   2.689867
  4.5051117   9.846115    7.6886387   0.35349095  2.5627694   4.5428576
  5.8595724   2.7118683   9.446846    6.2642756   4.7844443   6.0548897
  8.103386    3.6440284   4.565682    2.645935  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-28 18:15:34.177 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7476.8237, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 151s - loss: nan - MinusLogProbMetric: 7476.8237 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.1152e-06 - 151s/epoch - 769ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 1.3717421124828526e-06.
===========
Generating train data for run 325.
===========
Train data generated in 0.41 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_325/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_325/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_325/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_325
self.data_kwargs: {'seed': 0}
self.x_data: [[ 6.6721983   2.7232099   6.150583   ...  3.6440284   4.565682
   2.645935  ]
 [ 1.86333     2.9602146   9.095053   ...  6.366897   -0.51058275
   3.6708503 ]
 [ 5.5981846   7.771953    4.990416   ...  1.6793485   6.8585963
   1.4719678 ]
 ...
 [ 1.690249    4.096276    8.408299   ...  5.907632    0.7324666
   3.17104   ]
 [ 1.5949699   2.976067    6.189886   ...  6.072563    1.3667407
   3.7841148 ]
 [ 6.6605377   2.9536047   6.141097   ...  3.8587723   3.7946694
   1.5606189 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_159"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_160 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_14 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_14/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_14'")
self.model: <keras.engine.functional.Functional object at 0x7fb48063b430>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fb38c5bebc0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fb38c5bebc0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fb394118e20>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fb1ee24bb80>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_325/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fb1ee249360>, <keras.callbacks.ModelCheckpoint object at 0x7fb1ee24acb0>, <keras.callbacks.EarlyStopping object at 0x7fb1ee24bd90>, <keras.callbacks.ReduceLROnPlateau object at 0x7fb1ee249120>, <keras.callbacks.TerminateOnNaN object at 0x7fb1ee24a260>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_325/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 325/720 with hyperparameters:
timestamp = 2023-09-28 18:15:44.991715
ndims = 64
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.3717421124828526e-06...
Train first sample: [ 6.6721983   2.7232099   6.150583    4.580923    1.5652926   2.6565456
  7.1790314   4.9339914   5.7110434   6.7498174   6.386396    3.9689803
  8.906224    3.806384    4.1225185   9.359665    7.9177184   7.261933
  0.1027934   9.199222    7.196588   10.08044     2.3912797   8.714102
  2.4626331   6.1591697   1.6348177   8.815128    7.787716    5.5076594
  3.3412566   0.7007286   7.2919664   4.3464866   6.6338673   7.1474853
  9.7648      8.511822   -0.5374965   5.0141335   7.365311    0.6638582
  5.9037857   0.7669884   1.383983    0.33108917  8.1157875   2.689867
  4.5051117   9.846115    7.6886387   0.35349095  2.5627694   4.5428576
  5.8595724   2.7118683   9.446846    6.2642756   4.7844443   6.0548897
  8.103386    3.6440284   4.565682    2.645935  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-28 18:18:01.996 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7476.8237, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 137s - loss: nan - MinusLogProbMetric: 7476.8237 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.3717e-06 - 137s/epoch - 699ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 4.572473708276175e-07.
===========
Generating train data for run 325.
===========
Train data generated in 0.27 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_325/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_325/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_325/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_325
self.data_kwargs: {'seed': 0}
self.x_data: [[ 6.6721983   2.7232099   6.150583   ...  3.6440284   4.565682
   2.645935  ]
 [ 1.86333     2.9602146   9.095053   ...  6.366897   -0.51058275
   3.6708503 ]
 [ 5.5981846   7.771953    4.990416   ...  1.6793485   6.8585963
   1.4719678 ]
 ...
 [ 1.690249    4.096276    8.408299   ...  5.907632    0.7324666
   3.17104   ]
 [ 1.5949699   2.976067    6.189886   ...  6.072563    1.3667407
   3.7841148 ]
 [ 6.6605377   2.9536047   6.141097   ...  3.8587723   3.7946694
   1.5606189 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_170"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_171 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_15 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_15/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_15'")
self.model: <keras.engine.functional.Functional object at 0x7fb1ecc3a380>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fb2f87834f0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fb2f87834f0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fb428138520>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fb3bc263070>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_325/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fb3bc2635e0>, <keras.callbacks.ModelCheckpoint object at 0x7fb3bc2636a0>, <keras.callbacks.EarlyStopping object at 0x7fb3bc263910>, <keras.callbacks.ReduceLROnPlateau object at 0x7fb3bc263940>, <keras.callbacks.TerminateOnNaN object at 0x7fb3bc263580>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_325/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 325/720 with hyperparameters:
timestamp = 2023-09-28 18:18:12.764437
ndims = 64
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.572473708276175e-07...
Train first sample: [ 6.6721983   2.7232099   6.150583    4.580923    1.5652926   2.6565456
  7.1790314   4.9339914   5.7110434   6.7498174   6.386396    3.9689803
  8.906224    3.806384    4.1225185   9.359665    7.9177184   7.261933
  0.1027934   9.199222    7.196588   10.08044     2.3912797   8.714102
  2.4626331   6.1591697   1.6348177   8.815128    7.787716    5.5076594
  3.3412566   0.7007286   7.2919664   4.3464866   6.6338673   7.1474853
  9.7648      8.511822   -0.5374965   5.0141335   7.365311    0.6638582
  5.9037857   0.7669884   1.383983    0.33108917  8.1157875   2.689867
  4.5051117   9.846115    7.6886387   0.35349095  2.5627694   4.5428576
  5.8595724   2.7118683   9.446846    6.2642756   4.7844443   6.0548897
  8.103386    3.6440284   4.565682    2.645935  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-28 18:20:36.115 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7476.8237, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 143s - loss: nan - MinusLogProbMetric: 7476.8237 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.5725e-07 - 143s/epoch - 731ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 1.524157902758725e-07.
===========
Generating train data for run 325.
===========
Train data generated in 0.20 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_325/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_325/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_325/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_325
self.data_kwargs: {'seed': 0}
self.x_data: [[ 6.6721983   2.7232099   6.150583   ...  3.6440284   4.565682
   2.645935  ]
 [ 1.86333     2.9602146   9.095053   ...  6.366897   -0.51058275
   3.6708503 ]
 [ 5.5981846   7.771953    4.990416   ...  1.6793485   6.8585963
   1.4719678 ]
 ...
 [ 1.690249    4.096276    8.408299   ...  5.907632    0.7324666
   3.17104   ]
 [ 1.5949699   2.976067    6.189886   ...  6.072563    1.3667407
   3.7841148 ]
 [ 6.6605377   2.9536047   6.141097   ...  3.8587723   3.7946694
   1.5606189 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_181"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_182 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_16 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_16/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_16'")
self.model: <keras.engine.functional.Functional object at 0x7fb33047c130>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fb897f6fa30>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fb897f6fa30>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fb3942c7b20>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fb3f027f8b0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_325/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fb8e5f281c0>, <keras.callbacks.ModelCheckpoint object at 0x7fb3307e0e20>, <keras.callbacks.EarlyStopping object at 0x7fb8e5f2a0e0>, <keras.callbacks.ReduceLROnPlateau object at 0x7fb3307e0a30>, <keras.callbacks.TerminateOnNaN object at 0x7fb3307e0d90>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_325/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 325/720 with hyperparameters:
timestamp = 2023-09-28 18:20:44.413928
ndims = 64
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.524157902758725e-07...
Train first sample: [ 6.6721983   2.7232099   6.150583    4.580923    1.5652926   2.6565456
  7.1790314   4.9339914   5.7110434   6.7498174   6.386396    3.9689803
  8.906224    3.806384    4.1225185   9.359665    7.9177184   7.261933
  0.1027934   9.199222    7.196588   10.08044     2.3912797   8.714102
  2.4626331   6.1591697   1.6348177   8.815128    7.787716    5.5076594
  3.3412566   0.7007286   7.2919664   4.3464866   6.6338673   7.1474853
  9.7648      8.511822   -0.5374965   5.0141335   7.365311    0.6638582
  5.9037857   0.7669884   1.383983    0.33108917  8.1157875   2.689867
  4.5051117   9.846115    7.6886387   0.35349095  2.5627694   4.5428576
  5.8595724   2.7118683   9.446846    6.2642756   4.7844443   6.0548897
  8.103386    3.6440284   4.565682    2.645935  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-28 18:23:11.494 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7476.8237, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 147s - loss: nan - MinusLogProbMetric: 7476.8237 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.5242e-07 - 147s/epoch - 750ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 5.0805263425290834e-08.
===========
Generating train data for run 325.
===========
Train data generated in 0.16 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_325/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_325/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_325/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_325
self.data_kwargs: {'seed': 0}
self.x_data: [[ 6.6721983   2.7232099   6.150583   ...  3.6440284   4.565682
   2.645935  ]
 [ 1.86333     2.9602146   9.095053   ...  6.366897   -0.51058275
   3.6708503 ]
 [ 5.5981846   7.771953    4.990416   ...  1.6793485   6.8585963
   1.4719678 ]
 ...
 [ 1.690249    4.096276    8.408299   ...  5.907632    0.7324666
   3.17104   ]
 [ 1.5949699   2.976067    6.189886   ...  6.072563    1.3667407
   3.7841148 ]
 [ 6.6605377   2.9536047   6.141097   ...  3.8587723   3.7946694
   1.5606189 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_192"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_193 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_17 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_17/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_17'")
self.model: <keras.engine.functional.Functional object at 0x7fb86d227910>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fb84c8b6290>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fb84c8b6290>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fb84c59b5b0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fb84c5c9390>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_325/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fb84c5c9900>, <keras.callbacks.ModelCheckpoint object at 0x7fb84c5c99c0>, <keras.callbacks.EarlyStopping object at 0x7fb84c5c9c30>, <keras.callbacks.ReduceLROnPlateau object at 0x7fb84c5c9c60>, <keras.callbacks.TerminateOnNaN object at 0x7fb84c5c98a0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_325/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 325/720 with hyperparameters:
timestamp = 2023-09-28 18:23:20.582793
ndims = 64
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 5.0805263425290834e-08...
Train first sample: [ 6.6721983   2.7232099   6.150583    4.580923    1.5652926   2.6565456
  7.1790314   4.9339914   5.7110434   6.7498174   6.386396    3.9689803
  8.906224    3.806384    4.1225185   9.359665    7.9177184   7.261933
  0.1027934   9.199222    7.196588   10.08044     2.3912797   8.714102
  2.4626331   6.1591697   1.6348177   8.815128    7.787716    5.5076594
  3.3412566   0.7007286   7.2919664   4.3464866   6.6338673   7.1474853
  9.7648      8.511822   -0.5374965   5.0141335   7.365311    0.6638582
  5.9037857   0.7669884   1.383983    0.33108917  8.1157875   2.689867
  4.5051117   9.846115    7.6886387   0.35349095  2.5627694   4.5428576
  5.8595724   2.7118683   9.446846    6.2642756   4.7844443   6.0548897
  8.103386    3.6440284   4.565682    2.645935  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-28 18:25:58.349 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7476.8237, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 158s - loss: nan - MinusLogProbMetric: 7476.8237 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 5.0805e-08 - 158s/epoch - 804ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 1.6935087808430278e-08.
===========
Generating train data for run 325.
===========
Train data generated in 0.20 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_325/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_325/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_325/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_325
self.data_kwargs: {'seed': 0}
self.x_data: [[ 6.6721983   2.7232099   6.150583   ...  3.6440284   4.565682
   2.645935  ]
 [ 1.86333     2.9602146   9.095053   ...  6.366897   -0.51058275
   3.6708503 ]
 [ 5.5981846   7.771953    4.990416   ...  1.6793485   6.8585963
   1.4719678 ]
 ...
 [ 1.690249    4.096276    8.408299   ...  5.907632    0.7324666
   3.17104   ]
 [ 1.5949699   2.976067    6.189886   ...  6.072563    1.3667407
   3.7841148 ]
 [ 6.6605377   2.9536047   6.141097   ...  3.8587723   3.7946694
   1.5606189 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_203"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_204 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_18 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_18/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_18'")
self.model: <keras.engine.functional.Functional object at 0x7fb04c26b4c0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fb1dd756c50>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fb1dd756c50>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fb0843e6470>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fb1ede179a0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_325/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fb1ede17f10>, <keras.callbacks.ModelCheckpoint object at 0x7fb1ede17fd0>, <keras.callbacks.EarlyStopping object at 0x7fb1ede17ee0>, <keras.callbacks.ReduceLROnPlateau object at 0x7fb1ede17eb0>, <keras.callbacks.TerminateOnNaN object at 0x7fb1edea8280>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_325/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 325/720 with hyperparameters:
timestamp = 2023-09-28 18:26:07.938999
ndims = 64
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.6935087808430278e-08...
Train first sample: [ 6.6721983   2.7232099   6.150583    4.580923    1.5652926   2.6565456
  7.1790314   4.9339914   5.7110434   6.7498174   6.386396    3.9689803
  8.906224    3.806384    4.1225185   9.359665    7.9177184   7.261933
  0.1027934   9.199222    7.196588   10.08044     2.3912797   8.714102
  2.4626331   6.1591697   1.6348177   8.815128    7.787716    5.5076594
  3.3412566   0.7007286   7.2919664   4.3464866   6.6338673   7.1474853
  9.7648      8.511822   -0.5374965   5.0141335   7.365311    0.6638582
  5.9037857   0.7669884   1.383983    0.33108917  8.1157875   2.689867
  4.5051117   9.846115    7.6886387   0.35349095  2.5627694   4.5428576
  5.8595724   2.7118683   9.446846    6.2642756   4.7844443   6.0548897
  8.103386    3.6440284   4.565682    2.645935  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-28 18:28:59.322 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7476.8237, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 171s - loss: nan - MinusLogProbMetric: 7476.8237 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.6935e-08 - 171s/epoch - 874ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 5.645029269476759e-09.
===========
Run 325/720 failed.
Exception type: Exception
Exception message: Training failed for the maximum number of retry.
Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN.py , Line : 637, Func.Name : <module>, Message : hyperparams_dict, NFObject, seed_train, training_time = train_function(seeds = [seed_train, seed_test, seed_dist, seed_metrics],', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN.py , Line : 313, Func.Name : train_function, Message : raise Exception("Training failed for the maximum number of retry.")']
===========

===========
Generating train data for run 326.
===========
Train data generated in 0.38 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_326/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_326/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_326/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_326
self.data_kwargs: {'seed': 0}
self.x_data: [[ 6.6721983   2.7232099   6.150583   ...  3.6440284   4.565682
   2.645935  ]
 [ 1.86333     2.9602146   9.095053   ...  6.366897   -0.51058275
   3.6708503 ]
 [ 5.5981846   7.771953    4.990416   ...  1.6793485   6.8585963
   1.4719678 ]
 ...
 [ 1.690249    4.096276    8.408299   ...  5.907632    0.7324666
   3.17104   ]
 [ 1.5949699   2.976067    6.189886   ...  6.072563    1.3667407
   3.7841148 ]
 [ 6.6605377   2.9536047   6.141097   ...  3.8587723   3.7946694
   1.5606189 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_214"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_215 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_19 (LogProbL  (None,)                  3291840   
 ayer)                                                           
                                                                 
=================================================================
Total params: 3,291,840
Trainable params: 3,291,840
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_19/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_19'")
self.model: <keras.engine.functional.Functional object at 0x7fb1cfa8bd90>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fb06417ff10>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fb06417ff10>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fb11c6fe770>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fb1cfa37580>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_326/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fb1cfa37af0>, <keras.callbacks.ModelCheckpoint object at 0x7fb1cfa37bb0>, <keras.callbacks.EarlyStopping object at 0x7fb1cfa37e20>, <keras.callbacks.ReduceLROnPlateau object at 0x7fb1cfa37e50>, <keras.callbacks.TerminateOnNaN object at 0x7fb1cfa37a90>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_326/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 326/720 with hyperparameters:
timestamp = 2023-09-28 18:29:11.204998
ndims = 64
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 3291840
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 6.6721983   2.7232099   6.150583    4.580923    1.5652926   2.6565456
  7.1790314   4.9339914   5.7110434   6.7498174   6.386396    3.9689803
  8.906224    3.806384    4.1225185   9.359665    7.9177184   7.261933
  0.1027934   9.199222    7.196588   10.08044     2.3912797   8.714102
  2.4626331   6.1591697   1.6348177   8.815128    7.787716    5.5076594
  3.3412566   0.7007286   7.2919664   4.3464866   6.6338673   7.1474853
  9.7648      8.511822   -0.5374965   5.0141335   7.365311    0.6638582
  5.9037857   0.7669884   1.383983    0.33108917  8.1157875   2.689867
  4.5051117   9.846115    7.6886387   0.35349095  2.5627694   4.5428576
  5.8595724   2.7118683   9.446846    6.2642756   4.7844443   6.0548897
  8.103386    3.6440284   4.565682    2.645935  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 7: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-28 18:31:30.280 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6282.9419, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 139s - loss: nan - MinusLogProbMetric: 6282.9419 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 139s/epoch - 708ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 0.0003333333333333333.
===========
Generating train data for run 326.
===========
Train data generated in 0.27 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_326/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_326/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_326/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_326
self.data_kwargs: {'seed': 0}
self.x_data: [[ 6.6721983   2.7232099   6.150583   ...  3.6440284   4.565682
   2.645935  ]
 [ 1.86333     2.9602146   9.095053   ...  6.366897   -0.51058275
   3.6708503 ]
 [ 5.5981846   7.771953    4.990416   ...  1.6793485   6.8585963
   1.4719678 ]
 ...
 [ 1.690249    4.096276    8.408299   ...  5.907632    0.7324666
   3.17104   ]
 [ 1.5949699   2.976067    6.189886   ...  6.072563    1.3667407
   3.7841148 ]
 [ 6.6605377   2.9536047   6.141097   ...  3.8587723   3.7946694
   1.5606189 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_225"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_226 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_20 (LogProbL  (None,)                  3291840   
 ayer)                                                           
                                                                 
=================================================================
Total params: 3,291,840
Trainable params: 3,291,840
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_20/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_20'")
self.model: <keras.engine.functional.Functional object at 0x7fb843563a00>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fb84368fc10>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fb84368fc10>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fb195249ed0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fb843591d20>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_326/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fb843592290>, <keras.callbacks.ModelCheckpoint object at 0x7fb843592350>, <keras.callbacks.EarlyStopping object at 0x7fb8435925c0>, <keras.callbacks.ReduceLROnPlateau object at 0x7fb8435925f0>, <keras.callbacks.TerminateOnNaN object at 0x7fb843592230>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_326/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 326/720 with hyperparameters:
timestamp = 2023-09-28 18:31:40.428940
ndims = 64
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 3291840
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 6.6721983   2.7232099   6.150583    4.580923    1.5652926   2.6565456
  7.1790314   4.9339914   5.7110434   6.7498174   6.386396    3.9689803
  8.906224    3.806384    4.1225185   9.359665    7.9177184   7.261933
  0.1027934   9.199222    7.196588   10.08044     2.3912797   8.714102
  2.4626331   6.1591697   1.6348177   8.815128    7.787716    5.5076594
  3.3412566   0.7007286   7.2919664   4.3464866   6.6338673   7.1474853
  9.7648      8.511822   -0.5374965   5.0141335   7.365311    0.6638582
  5.9037857   0.7669884   1.383983    0.33108917  8.1157875   2.689867
  4.5051117   9.846115    7.6886387   0.35349095  2.5627694   4.5428576
  5.8595724   2.7118683   9.446846    6.2642756   4.7844443   6.0548897
  8.103386    3.6440284   4.565682    2.645935  ]
Epoch 1/1000
2023-09-28 18:34:47.887 
Epoch 1/1000 
	 loss: 2200.5879, MinusLogProbMetric: 2200.5879, val_loss: 895.9645, val_MinusLogProbMetric: 895.9645

Epoch 1: val_loss improved from inf to 895.96454, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_326/weights/best_weights.h5
196/196 - 188s - loss: 2200.5879 - MinusLogProbMetric: 2200.5879 - val_loss: 895.9645 - val_MinusLogProbMetric: 895.9645 - lr: 3.3333e-04 - 188s/epoch - 960ms/step
Epoch 2/1000
2023-09-28 18:35:57.261 
Epoch 2/1000 
	 loss: 741.9849, MinusLogProbMetric: 741.9849, val_loss: 645.8956, val_MinusLogProbMetric: 645.8956

Epoch 2: val_loss improved from 895.96454 to 645.89563, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_326/weights/best_weights.h5
196/196 - 69s - loss: 741.9849 - MinusLogProbMetric: 741.9849 - val_loss: 645.8956 - val_MinusLogProbMetric: 645.8956 - lr: 3.3333e-04 - 69s/epoch - 353ms/step
Epoch 3/1000
2023-09-28 18:37:01.438 
Epoch 3/1000 
	 loss: 545.9691, MinusLogProbMetric: 545.9691, val_loss: 460.9556, val_MinusLogProbMetric: 460.9556

Epoch 3: val_loss improved from 645.89563 to 460.95557, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_326/weights/best_weights.h5
196/196 - 64s - loss: 545.9691 - MinusLogProbMetric: 545.9691 - val_loss: 460.9556 - val_MinusLogProbMetric: 460.9556 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 4/1000
2023-09-28 18:38:07.227 
Epoch 4/1000 
	 loss: 483.0293, MinusLogProbMetric: 483.0293, val_loss: 371.3273, val_MinusLogProbMetric: 371.3273

Epoch 4: val_loss improved from 460.95557 to 371.32733, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_326/weights/best_weights.h5
196/196 - 66s - loss: 483.0293 - MinusLogProbMetric: 483.0293 - val_loss: 371.3273 - val_MinusLogProbMetric: 371.3273 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 5/1000
2023-09-28 18:39:14.947 
Epoch 5/1000 
	 loss: 318.1729, MinusLogProbMetric: 318.1729, val_loss: 279.1263, val_MinusLogProbMetric: 279.1263

Epoch 5: val_loss improved from 371.32733 to 279.12634, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_326/weights/best_weights.h5
196/196 - 68s - loss: 318.1729 - MinusLogProbMetric: 318.1729 - val_loss: 279.1263 - val_MinusLogProbMetric: 279.1263 - lr: 3.3333e-04 - 68s/epoch - 345ms/step
Epoch 6/1000
2023-09-28 18:40:17.650 
Epoch 6/1000 
	 loss: 294.5357, MinusLogProbMetric: 294.5357, val_loss: 605.3785, val_MinusLogProbMetric: 605.3785

Epoch 6: val_loss did not improve from 279.12634
196/196 - 62s - loss: 294.5357 - MinusLogProbMetric: 294.5357 - val_loss: 605.3785 - val_MinusLogProbMetric: 605.3785 - lr: 3.3333e-04 - 62s/epoch - 315ms/step
Epoch 7/1000
2023-09-28 18:41:24.654 
Epoch 7/1000 
	 loss: 462.0613, MinusLogProbMetric: 462.0613, val_loss: 423.8692, val_MinusLogProbMetric: 423.8692

Epoch 7: val_loss did not improve from 279.12634
196/196 - 67s - loss: 462.0613 - MinusLogProbMetric: 462.0613 - val_loss: 423.8692 - val_MinusLogProbMetric: 423.8692 - lr: 3.3333e-04 - 67s/epoch - 342ms/step
Epoch 8/1000
2023-09-28 18:42:27.924 
Epoch 8/1000 
	 loss: 293.8641, MinusLogProbMetric: 293.8641, val_loss: 225.0320, val_MinusLogProbMetric: 225.0320

Epoch 8: val_loss improved from 279.12634 to 225.03204, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_326/weights/best_weights.h5
196/196 - 65s - loss: 293.8641 - MinusLogProbMetric: 293.8641 - val_loss: 225.0320 - val_MinusLogProbMetric: 225.0320 - lr: 3.3333e-04 - 65s/epoch - 329ms/step
Epoch 9/1000
2023-09-28 18:43:34.938 
Epoch 9/1000 
	 loss: 452.7118, MinusLogProbMetric: 452.7118, val_loss: 287.9364, val_MinusLogProbMetric: 287.9364

Epoch 9: val_loss did not improve from 225.03204
196/196 - 66s - loss: 452.7118 - MinusLogProbMetric: 452.7118 - val_loss: 287.9364 - val_MinusLogProbMetric: 287.9364 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 10/1000
2023-09-28 18:44:41.289 
Epoch 10/1000 
	 loss: 231.3926, MinusLogProbMetric: 231.3926, val_loss: 192.3651, val_MinusLogProbMetric: 192.3651

Epoch 10: val_loss improved from 225.03204 to 192.36511, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_326/weights/best_weights.h5
196/196 - 67s - loss: 231.3926 - MinusLogProbMetric: 231.3926 - val_loss: 192.3651 - val_MinusLogProbMetric: 192.3651 - lr: 3.3333e-04 - 67s/epoch - 343ms/step
Epoch 11/1000
2023-09-28 18:45:51.703 
Epoch 11/1000 
	 loss: 185.6682, MinusLogProbMetric: 185.6682, val_loss: 169.5321, val_MinusLogProbMetric: 169.5321

Epoch 11: val_loss improved from 192.36511 to 169.53212, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_326/weights/best_weights.h5
196/196 - 71s - loss: 185.6682 - MinusLogProbMetric: 185.6682 - val_loss: 169.5321 - val_MinusLogProbMetric: 169.5321 - lr: 3.3333e-04 - 71s/epoch - 361ms/step
Epoch 12/1000
2023-09-28 18:47:00.614 
Epoch 12/1000 
	 loss: 234.4713, MinusLogProbMetric: 234.4713, val_loss: 159.4698, val_MinusLogProbMetric: 159.4698

Epoch 12: val_loss improved from 169.53212 to 159.46982, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_326/weights/best_weights.h5
196/196 - 69s - loss: 234.4713 - MinusLogProbMetric: 234.4713 - val_loss: 159.4698 - val_MinusLogProbMetric: 159.4698 - lr: 3.3333e-04 - 69s/epoch - 351ms/step
Epoch 13/1000
2023-09-28 18:48:11.160 
Epoch 13/1000 
	 loss: 176.0271, MinusLogProbMetric: 176.0271, val_loss: 157.4149, val_MinusLogProbMetric: 157.4149

Epoch 13: val_loss improved from 159.46982 to 157.41487, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_326/weights/best_weights.h5
196/196 - 71s - loss: 176.0271 - MinusLogProbMetric: 176.0271 - val_loss: 157.4149 - val_MinusLogProbMetric: 157.4149 - lr: 3.3333e-04 - 71s/epoch - 360ms/step
Epoch 14/1000
2023-09-28 18:49:20.770 
Epoch 14/1000 
	 loss: 139.3664, MinusLogProbMetric: 139.3664, val_loss: 126.3415, val_MinusLogProbMetric: 126.3415

Epoch 14: val_loss improved from 157.41487 to 126.34145, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_326/weights/best_weights.h5
196/196 - 70s - loss: 139.3664 - MinusLogProbMetric: 139.3664 - val_loss: 126.3415 - val_MinusLogProbMetric: 126.3415 - lr: 3.3333e-04 - 70s/epoch - 355ms/step
Epoch 15/1000
2023-09-28 18:50:30.796 
Epoch 15/1000 
	 loss: 284.3072, MinusLogProbMetric: 284.3072, val_loss: 1047.9053, val_MinusLogProbMetric: 1047.9053

Epoch 15: val_loss did not improve from 126.34145
196/196 - 69s - loss: 284.3072 - MinusLogProbMetric: 284.3072 - val_loss: 1047.9053 - val_MinusLogProbMetric: 1047.9053 - lr: 3.3333e-04 - 69s/epoch - 351ms/step
Epoch 16/1000
2023-09-28 18:51:39.851 
Epoch 16/1000 
	 loss: 516.5515, MinusLogProbMetric: 516.5515, val_loss: 284.7725, val_MinusLogProbMetric: 284.7725

Epoch 16: val_loss did not improve from 126.34145
196/196 - 69s - loss: 516.5515 - MinusLogProbMetric: 516.5515 - val_loss: 284.7725 - val_MinusLogProbMetric: 284.7725 - lr: 3.3333e-04 - 69s/epoch - 352ms/step
Epoch 17/1000
2023-09-28 18:52:46.964 
Epoch 17/1000 
	 loss: 239.1551, MinusLogProbMetric: 239.1551, val_loss: 217.0820, val_MinusLogProbMetric: 217.0820

Epoch 17: val_loss did not improve from 126.34145
196/196 - 67s - loss: 239.1551 - MinusLogProbMetric: 239.1551 - val_loss: 217.0820 - val_MinusLogProbMetric: 217.0820 - lr: 3.3333e-04 - 67s/epoch - 342ms/step
Epoch 18/1000
2023-09-28 18:53:51.829 
Epoch 18/1000 
	 loss: 199.0651, MinusLogProbMetric: 199.0651, val_loss: 185.8669, val_MinusLogProbMetric: 185.8669

Epoch 18: val_loss did not improve from 126.34145
196/196 - 65s - loss: 199.0651 - MinusLogProbMetric: 199.0651 - val_loss: 185.8669 - val_MinusLogProbMetric: 185.8669 - lr: 3.3333e-04 - 65s/epoch - 331ms/step
Epoch 19/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 61: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-28 18:54:16.332 
Epoch 19/1000 
	 loss: nan, MinusLogProbMetric: 348.5935, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 19: val_loss did not improve from 126.34145
196/196 - 25s - loss: nan - MinusLogProbMetric: 348.5935 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 25s/epoch - 125ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 0.0001111111111111111.
===========
Generating train data for run 326.
===========
Train data generated in 0.33 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_326/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_326/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_326/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_326
self.data_kwargs: {'seed': 0}
self.x_data: [[ 6.6721983   2.7232099   6.150583   ...  3.6440284   4.565682
   2.645935  ]
 [ 1.86333     2.9602146   9.095053   ...  6.366897   -0.51058275
   3.6708503 ]
 [ 5.5981846   7.771953    4.990416   ...  1.6793485   6.8585963
   1.4719678 ]
 ...
 [ 1.690249    4.096276    8.408299   ...  5.907632    0.7324666
   3.17104   ]
 [ 1.5949699   2.976067    6.189886   ...  6.072563    1.3667407
   3.7841148 ]
 [ 6.6605377   2.9536047   6.141097   ...  3.8587723   3.7946694
   1.5606189 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_236"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_237 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_21 (LogProbL  (None,)                  3291840   
 ayer)                                                           
                                                                 
=================================================================
Total params: 3,291,840
Trainable params: 3,291,840
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_21/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_21'")
self.model: <keras.engine.functional.Functional object at 0x7fb842de7be0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fb842d9ff70>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fb842d9ff70>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fb0ff0d8040>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fb83a5fdab0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_326/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fb83a5fe020>, <keras.callbacks.ModelCheckpoint object at 0x7fb83a5fe0e0>, <keras.callbacks.EarlyStopping object at 0x7fb83a5fe350>, <keras.callbacks.ReduceLROnPlateau object at 0x7fb83a5fe380>, <keras.callbacks.TerminateOnNaN object at 0x7fb83a5fdfc0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 326/720 with hyperparameters:
timestamp = 2023-09-28 18:54:26.540705
ndims = 64
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 3291840
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 6.6721983   2.7232099   6.150583    4.580923    1.5652926   2.6565456
  7.1790314   4.9339914   5.7110434   6.7498174   6.386396    3.9689803
  8.906224    3.806384    4.1225185   9.359665    7.9177184   7.261933
  0.1027934   9.199222    7.196588   10.08044     2.3912797   8.714102
  2.4626331   6.1591697   1.6348177   8.815128    7.787716    5.5076594
  3.3412566   0.7007286   7.2919664   4.3464866   6.6338673   7.1474853
  9.7648      8.511822   -0.5374965   5.0141335   7.365311    0.6638582
  5.9037857   0.7669884   1.383983    0.33108917  8.1157875   2.689867
  4.5051117   9.846115    7.6886387   0.35349095  2.5627694   4.5428576
  5.8595724   2.7118683   9.446846    6.2642756   4.7844443   6.0548897
  8.103386    3.6440284   4.565682    2.645935  ]
Epoch 1/1000
2023-09-28 18:58:09.777 
Epoch 1/1000 
	 loss: 218.1828, MinusLogProbMetric: 218.1828, val_loss: 130.5612, val_MinusLogProbMetric: 130.5612

Epoch 1: val_loss improved from inf to 130.56125, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_326/weights/best_weights.h5
196/196 - 224s - loss: 218.1828 - MinusLogProbMetric: 218.1828 - val_loss: 130.5612 - val_MinusLogProbMetric: 130.5612 - lr: 1.1111e-04 - 224s/epoch - 1s/step
Epoch 2/1000
2023-09-28 18:59:19.449 
Epoch 2/1000 
	 loss: 114.2774, MinusLogProbMetric: 114.2774, val_loss: 105.9139, val_MinusLogProbMetric: 105.9139

Epoch 2: val_loss improved from 130.56125 to 105.91394, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_326/weights/best_weights.h5
196/196 - 69s - loss: 114.2774 - MinusLogProbMetric: 114.2774 - val_loss: 105.9139 - val_MinusLogProbMetric: 105.9139 - lr: 1.1111e-04 - 69s/epoch - 354ms/step
Epoch 3/1000
2023-09-28 19:00:29.159 
Epoch 3/1000 
	 loss: 189.0195, MinusLogProbMetric: 189.0195, val_loss: 259.3407, val_MinusLogProbMetric: 259.3407

Epoch 3: val_loss did not improve from 105.91394
196/196 - 68s - loss: 189.0195 - MinusLogProbMetric: 189.0195 - val_loss: 259.3407 - val_MinusLogProbMetric: 259.3407 - lr: 1.1111e-04 - 68s/epoch - 349ms/step
Epoch 4/1000
2023-09-28 19:01:37.778 
Epoch 4/1000 
	 loss: 172.9227, MinusLogProbMetric: 172.9227, val_loss: 130.7722, val_MinusLogProbMetric: 130.7722

Epoch 4: val_loss did not improve from 105.91394
196/196 - 69s - loss: 172.9227 - MinusLogProbMetric: 172.9227 - val_loss: 130.7722 - val_MinusLogProbMetric: 130.7722 - lr: 1.1111e-04 - 69s/epoch - 350ms/step
Epoch 5/1000
2023-09-28 19:02:47.925 
Epoch 5/1000 
	 loss: 131.5929, MinusLogProbMetric: 131.5929, val_loss: 127.3287, val_MinusLogProbMetric: 127.3287

Epoch 5: val_loss did not improve from 105.91394
196/196 - 70s - loss: 131.5929 - MinusLogProbMetric: 131.5929 - val_loss: 127.3287 - val_MinusLogProbMetric: 127.3287 - lr: 1.1111e-04 - 70s/epoch - 358ms/step
Epoch 6/1000
2023-09-28 19:03:58.105 
Epoch 6/1000 
	 loss: 136.2597, MinusLogProbMetric: 136.2597, val_loss: 111.8903, val_MinusLogProbMetric: 111.8903

Epoch 6: val_loss did not improve from 105.91394
196/196 - 70s - loss: 136.2597 - MinusLogProbMetric: 136.2597 - val_loss: 111.8903 - val_MinusLogProbMetric: 111.8903 - lr: 1.1111e-04 - 70s/epoch - 358ms/step
Epoch 7/1000
2023-09-28 19:05:08.435 
Epoch 7/1000 
	 loss: 99.8436, MinusLogProbMetric: 99.8436, val_loss: 88.5686, val_MinusLogProbMetric: 88.5686

Epoch 7: val_loss improved from 105.91394 to 88.56860, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_326/weights/best_weights.h5
196/196 - 71s - loss: 99.8436 - MinusLogProbMetric: 99.8436 - val_loss: 88.5686 - val_MinusLogProbMetric: 88.5686 - lr: 1.1111e-04 - 71s/epoch - 365ms/step
Epoch 8/1000
2023-09-28 19:06:18.960 
Epoch 8/1000 
	 loss: 106.6129, MinusLogProbMetric: 106.6129, val_loss: 91.8993, val_MinusLogProbMetric: 91.8993

Epoch 8: val_loss did not improve from 88.56860
196/196 - 69s - loss: 106.6129 - MinusLogProbMetric: 106.6129 - val_loss: 91.8993 - val_MinusLogProbMetric: 91.8993 - lr: 1.1111e-04 - 69s/epoch - 354ms/step
Epoch 9/1000
2023-09-28 19:07:27.355 
Epoch 9/1000 
	 loss: 98.0364, MinusLogProbMetric: 98.0364, val_loss: 88.8172, val_MinusLogProbMetric: 88.8172

Epoch 9: val_loss did not improve from 88.56860
196/196 - 68s - loss: 98.0364 - MinusLogProbMetric: 98.0364 - val_loss: 88.8172 - val_MinusLogProbMetric: 88.8172 - lr: 1.1111e-04 - 68s/epoch - 349ms/step
Epoch 10/1000
2023-09-28 19:08:35.607 
Epoch 10/1000 
	 loss: 80.6911, MinusLogProbMetric: 80.6911, val_loss: 75.8618, val_MinusLogProbMetric: 75.8618

Epoch 10: val_loss improved from 88.56860 to 75.86185, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_326/weights/best_weights.h5
196/196 - 69s - loss: 80.6911 - MinusLogProbMetric: 80.6911 - val_loss: 75.8618 - val_MinusLogProbMetric: 75.8618 - lr: 1.1111e-04 - 69s/epoch - 354ms/step
Epoch 11/1000
2023-09-28 19:09:44.852 
Epoch 11/1000 
	 loss: 104.3943, MinusLogProbMetric: 104.3943, val_loss: 111.0129, val_MinusLogProbMetric: 111.0129

Epoch 11: val_loss did not improve from 75.86185
196/196 - 68s - loss: 104.3943 - MinusLogProbMetric: 104.3943 - val_loss: 111.0129 - val_MinusLogProbMetric: 111.0129 - lr: 1.1111e-04 - 68s/epoch - 348ms/step
Epoch 12/1000
2023-09-28 19:10:52.334 
Epoch 12/1000 
	 loss: 89.2055, MinusLogProbMetric: 89.2055, val_loss: 79.9307, val_MinusLogProbMetric: 79.9307

Epoch 12: val_loss did not improve from 75.86185
196/196 - 67s - loss: 89.2055 - MinusLogProbMetric: 89.2055 - val_loss: 79.9307 - val_MinusLogProbMetric: 79.9307 - lr: 1.1111e-04 - 67s/epoch - 344ms/step
Epoch 13/1000
2023-09-28 19:12:00.250 
Epoch 13/1000 
	 loss: 77.5960, MinusLogProbMetric: 77.5960, val_loss: 99.7895, val_MinusLogProbMetric: 99.7895

Epoch 13: val_loss did not improve from 75.86185
196/196 - 68s - loss: 77.5960 - MinusLogProbMetric: 77.5960 - val_loss: 99.7895 - val_MinusLogProbMetric: 99.7895 - lr: 1.1111e-04 - 68s/epoch - 347ms/step
Epoch 14/1000
2023-09-28 19:13:08.277 
Epoch 14/1000 
	 loss: 77.4051, MinusLogProbMetric: 77.4051, val_loss: 71.4631, val_MinusLogProbMetric: 71.4631

Epoch 14: val_loss improved from 75.86185 to 71.46309, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_326/weights/best_weights.h5
196/196 - 69s - loss: 77.4051 - MinusLogProbMetric: 77.4051 - val_loss: 71.4631 - val_MinusLogProbMetric: 71.4631 - lr: 1.1111e-04 - 69s/epoch - 353ms/step
Epoch 15/1000
2023-09-28 19:14:17.994 
Epoch 15/1000 
	 loss: 79.0579, MinusLogProbMetric: 79.0579, val_loss: 76.2345, val_MinusLogProbMetric: 76.2345

Epoch 15: val_loss did not improve from 71.46309
196/196 - 68s - loss: 79.0579 - MinusLogProbMetric: 79.0579 - val_loss: 76.2345 - val_MinusLogProbMetric: 76.2345 - lr: 1.1111e-04 - 68s/epoch - 349ms/step
Epoch 16/1000
2023-09-28 19:15:25.926 
Epoch 16/1000 
	 loss: 71.2039, MinusLogProbMetric: 71.2039, val_loss: 65.9738, val_MinusLogProbMetric: 65.9738

Epoch 16: val_loss improved from 71.46309 to 65.97385, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_326/weights/best_weights.h5
196/196 - 69s - loss: 71.2039 - MinusLogProbMetric: 71.2039 - val_loss: 65.9738 - val_MinusLogProbMetric: 65.9738 - lr: 1.1111e-04 - 69s/epoch - 352ms/step
Epoch 17/1000
2023-09-28 19:16:35.040 
Epoch 17/1000 
	 loss: 103.7121, MinusLogProbMetric: 103.7121, val_loss: 316.4507, val_MinusLogProbMetric: 316.4507

Epoch 17: val_loss did not improve from 65.97385
196/196 - 68s - loss: 103.7121 - MinusLogProbMetric: 103.7121 - val_loss: 316.4507 - val_MinusLogProbMetric: 316.4507 - lr: 1.1111e-04 - 68s/epoch - 347ms/step
Epoch 18/1000
2023-09-28 19:17:42.488 
Epoch 18/1000 
	 loss: 172.6619, MinusLogProbMetric: 172.6619, val_loss: 114.8932, val_MinusLogProbMetric: 114.8932

Epoch 18: val_loss did not improve from 65.97385
196/196 - 67s - loss: 172.6619 - MinusLogProbMetric: 172.6619 - val_loss: 114.8932 - val_MinusLogProbMetric: 114.8932 - lr: 1.1111e-04 - 67s/epoch - 344ms/step
Epoch 19/1000
2023-09-28 19:18:49.377 
Epoch 19/1000 
	 loss: 108.1729, MinusLogProbMetric: 108.1729, val_loss: 96.9864, val_MinusLogProbMetric: 96.9864

Epoch 19: val_loss did not improve from 65.97385
196/196 - 67s - loss: 108.1729 - MinusLogProbMetric: 108.1729 - val_loss: 96.9864 - val_MinusLogProbMetric: 96.9864 - lr: 1.1111e-04 - 67s/epoch - 341ms/step
Epoch 20/1000
2023-09-28 19:19:57.412 
Epoch 20/1000 
	 loss: 90.0357, MinusLogProbMetric: 90.0357, val_loss: 86.2695, val_MinusLogProbMetric: 86.2695

Epoch 20: val_loss did not improve from 65.97385
196/196 - 68s - loss: 90.0357 - MinusLogProbMetric: 90.0357 - val_loss: 86.2695 - val_MinusLogProbMetric: 86.2695 - lr: 1.1111e-04 - 68s/epoch - 347ms/step
Epoch 21/1000
2023-09-28 19:21:05.803 
Epoch 21/1000 
	 loss: 81.4214, MinusLogProbMetric: 81.4214, val_loss: 77.6698, val_MinusLogProbMetric: 77.6698

Epoch 21: val_loss did not improve from 65.97385
196/196 - 68s - loss: 81.4214 - MinusLogProbMetric: 81.4214 - val_loss: 77.6698 - val_MinusLogProbMetric: 77.6698 - lr: 1.1111e-04 - 68s/epoch - 349ms/step
Epoch 22/1000
2023-09-28 19:22:14.159 
Epoch 22/1000 
	 loss: 76.3502, MinusLogProbMetric: 76.3502, val_loss: 75.6459, val_MinusLogProbMetric: 75.6459

Epoch 22: val_loss did not improve from 65.97385
196/196 - 68s - loss: 76.3502 - MinusLogProbMetric: 76.3502 - val_loss: 75.6459 - val_MinusLogProbMetric: 75.6459 - lr: 1.1111e-04 - 68s/epoch - 349ms/step
Epoch 23/1000
2023-09-28 19:23:20.873 
Epoch 23/1000 
	 loss: 72.2727, MinusLogProbMetric: 72.2727, val_loss: 70.7012, val_MinusLogProbMetric: 70.7012

Epoch 23: val_loss did not improve from 65.97385
196/196 - 67s - loss: 72.2727 - MinusLogProbMetric: 72.2727 - val_loss: 70.7012 - val_MinusLogProbMetric: 70.7012 - lr: 1.1111e-04 - 67s/epoch - 340ms/step
Epoch 24/1000
2023-09-28 19:24:30.886 
Epoch 24/1000 
	 loss: 73.3405, MinusLogProbMetric: 73.3405, val_loss: 76.6777, val_MinusLogProbMetric: 76.6777

Epoch 24: val_loss did not improve from 65.97385
196/196 - 70s - loss: 73.3405 - MinusLogProbMetric: 73.3405 - val_loss: 76.6777 - val_MinusLogProbMetric: 76.6777 - lr: 1.1111e-04 - 70s/epoch - 357ms/step
Epoch 25/1000
2023-09-28 19:25:40.451 
Epoch 25/1000 
	 loss: 72.7374, MinusLogProbMetric: 72.7374, val_loss: 69.3528, val_MinusLogProbMetric: 69.3528

Epoch 25: val_loss did not improve from 65.97385
196/196 - 70s - loss: 72.7374 - MinusLogProbMetric: 72.7374 - val_loss: 69.3528 - val_MinusLogProbMetric: 69.3528 - lr: 1.1111e-04 - 70s/epoch - 355ms/step
Epoch 26/1000
2023-09-28 19:26:49.113 
Epoch 26/1000 
	 loss: 69.2576, MinusLogProbMetric: 69.2576, val_loss: 69.0446, val_MinusLogProbMetric: 69.0446

Epoch 26: val_loss did not improve from 65.97385
196/196 - 69s - loss: 69.2576 - MinusLogProbMetric: 69.2576 - val_loss: 69.0446 - val_MinusLogProbMetric: 69.0446 - lr: 1.1111e-04 - 69s/epoch - 350ms/step
Epoch 27/1000
2023-09-28 19:27:59.605 
Epoch 27/1000 
	 loss: 65.9254, MinusLogProbMetric: 65.9254, val_loss: 64.4777, val_MinusLogProbMetric: 64.4777

Epoch 27: val_loss improved from 65.97385 to 64.47768, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_326/weights/best_weights.h5
196/196 - 72s - loss: 65.9254 - MinusLogProbMetric: 65.9254 - val_loss: 64.4777 - val_MinusLogProbMetric: 64.4777 - lr: 1.1111e-04 - 72s/epoch - 366ms/step
Epoch 28/1000
2023-09-28 19:29:08.382 
Epoch 28/1000 
	 loss: 64.2913, MinusLogProbMetric: 64.2913, val_loss: 62.7014, val_MinusLogProbMetric: 62.7014

Epoch 28: val_loss improved from 64.47768 to 62.70139, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_326/weights/best_weights.h5
196/196 - 69s - loss: 64.2913 - MinusLogProbMetric: 64.2913 - val_loss: 62.7014 - val_MinusLogProbMetric: 62.7014 - lr: 1.1111e-04 - 69s/epoch - 351ms/step
Epoch 29/1000
2023-09-28 19:30:19.961 
Epoch 29/1000 
	 loss: 114.5752, MinusLogProbMetric: 114.5752, val_loss: 75.4107, val_MinusLogProbMetric: 75.4107

Epoch 29: val_loss did not improve from 62.70139
196/196 - 70s - loss: 114.5752 - MinusLogProbMetric: 114.5752 - val_loss: 75.4107 - val_MinusLogProbMetric: 75.4107 - lr: 1.1111e-04 - 70s/epoch - 359ms/step
Epoch 30/1000
2023-09-28 19:31:28.521 
Epoch 30/1000 
	 loss: 148.3261, MinusLogProbMetric: 148.3261, val_loss: 105.2943, val_MinusLogProbMetric: 105.2943

Epoch 30: val_loss did not improve from 62.70139
196/196 - 69s - loss: 148.3261 - MinusLogProbMetric: 148.3261 - val_loss: 105.2943 - val_MinusLogProbMetric: 105.2943 - lr: 1.1111e-04 - 69s/epoch - 350ms/step
Epoch 31/1000
2023-09-28 19:32:40.264 
Epoch 31/1000 
	 loss: 86.3594, MinusLogProbMetric: 86.3594, val_loss: 75.7082, val_MinusLogProbMetric: 75.7082

Epoch 31: val_loss did not improve from 62.70139
196/196 - 72s - loss: 86.3594 - MinusLogProbMetric: 86.3594 - val_loss: 75.7082 - val_MinusLogProbMetric: 75.7082 - lr: 1.1111e-04 - 72s/epoch - 366ms/step
Epoch 32/1000
2023-09-28 19:33:53.504 
Epoch 32/1000 
	 loss: 70.4396, MinusLogProbMetric: 70.4396, val_loss: 66.5989, val_MinusLogProbMetric: 66.5989

Epoch 32: val_loss did not improve from 62.70139
196/196 - 73s - loss: 70.4396 - MinusLogProbMetric: 70.4396 - val_loss: 66.5989 - val_MinusLogProbMetric: 66.5989 - lr: 1.1111e-04 - 73s/epoch - 374ms/step
Epoch 33/1000
2023-09-28 19:35:01.113 
Epoch 33/1000 
	 loss: 64.5383, MinusLogProbMetric: 64.5383, val_loss: 61.3887, val_MinusLogProbMetric: 61.3887

Epoch 33: val_loss improved from 62.70139 to 61.38873, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_326/weights/best_weights.h5
196/196 - 69s - loss: 64.5383 - MinusLogProbMetric: 64.5383 - val_loss: 61.3887 - val_MinusLogProbMetric: 61.3887 - lr: 1.1111e-04 - 69s/epoch - 352ms/step
Epoch 34/1000
2023-09-28 19:36:12.128 
Epoch 34/1000 
	 loss: 60.0066, MinusLogProbMetric: 60.0066, val_loss: 57.4409, val_MinusLogProbMetric: 57.4409

Epoch 34: val_loss improved from 61.38873 to 57.44094, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_326/weights/best_weights.h5
196/196 - 71s - loss: 60.0066 - MinusLogProbMetric: 60.0066 - val_loss: 57.4409 - val_MinusLogProbMetric: 57.4409 - lr: 1.1111e-04 - 71s/epoch - 362ms/step
Epoch 35/1000
2023-09-28 19:37:25.890 
Epoch 35/1000 
	 loss: 56.3646, MinusLogProbMetric: 56.3646, val_loss: 55.2890, val_MinusLogProbMetric: 55.2890

Epoch 35: val_loss improved from 57.44094 to 55.28897, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_326/weights/best_weights.h5
196/196 - 73s - loss: 56.3646 - MinusLogProbMetric: 56.3646 - val_loss: 55.2890 - val_MinusLogProbMetric: 55.2890 - lr: 1.1111e-04 - 73s/epoch - 374ms/step
Epoch 36/1000
2023-09-28 19:38:33.331 
Epoch 36/1000 
	 loss: 118.1065, MinusLogProbMetric: 118.1065, val_loss: 489.1874, val_MinusLogProbMetric: 489.1874

Epoch 36: val_loss did not improve from 55.28897
196/196 - 67s - loss: 118.1065 - MinusLogProbMetric: 118.1065 - val_loss: 489.1874 - val_MinusLogProbMetric: 489.1874 - lr: 1.1111e-04 - 67s/epoch - 340ms/step
Epoch 37/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 14: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-28 19:38:41.622 
Epoch 37/1000 
	 loss: nan, MinusLogProbMetric: 1164.5344, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 37: val_loss did not improve from 55.28897
196/196 - 8s - loss: nan - MinusLogProbMetric: 1164.5344 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.1111e-04 - 8s/epoch - 42ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 3.703703703703703e-05.
===========
Generating train data for run 326.
===========
Train data generated in 0.28 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_326/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_326/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_326/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_326
self.data_kwargs: {'seed': 0}
self.x_data: [[ 6.6721983   2.7232099   6.150583   ...  3.6440284   4.565682
   2.645935  ]
 [ 1.86333     2.9602146   9.095053   ...  6.366897   -0.51058275
   3.6708503 ]
 [ 5.5981846   7.771953    4.990416   ...  1.6793485   6.8585963
   1.4719678 ]
 ...
 [ 1.690249    4.096276    8.408299   ...  5.907632    0.7324666
   3.17104   ]
 [ 1.5949699   2.976067    6.189886   ...  6.072563    1.3667407
   3.7841148 ]
 [ 6.6605377   2.9536047   6.141097   ...  3.8587723   3.7946694
   1.5606189 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_247"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_248 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_22 (LogProbL  (None,)                  3291840   
 ayer)                                                           
                                                                 
=================================================================
Total params: 3,291,840
Trainable params: 3,291,840
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_22/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_22'")
self.model: <keras.engine.functional.Functional object at 0x7fb1cf0eead0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fb1975d5b10>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fb1975d5b10>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fb0fda2ba30>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fb0fdafbca0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_326/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fb0fda2c250>, <keras.callbacks.ModelCheckpoint object at 0x7fb0fda2c310>, <keras.callbacks.EarlyStopping object at 0x7fb0fda2c580>, <keras.callbacks.ReduceLROnPlateau object at 0x7fb0fda2c5b0>, <keras.callbacks.TerminateOnNaN object at 0x7fb0fda2c1f0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 326/720 with hyperparameters:
timestamp = 2023-09-28 19:38:51.368216
ndims = 64
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 3291840
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 3.703703703703703e-05...
Train first sample: [ 6.6721983   2.7232099   6.150583    4.580923    1.5652926   2.6565456
  7.1790314   4.9339914   5.7110434   6.7498174   6.386396    3.9689803
  8.906224    3.806384    4.1225185   9.359665    7.9177184   7.261933
  0.1027934   9.199222    7.196588   10.08044     2.3912797   8.714102
  2.4626331   6.1591697   1.6348177   8.815128    7.787716    5.5076594
  3.3412566   0.7007286   7.2919664   4.3464866   6.6338673   7.1474853
  9.7648      8.511822   -0.5374965   5.0141335   7.365311    0.6638582
  5.9037857   0.7669884   1.383983    0.33108917  8.1157875   2.689867
  4.5051117   9.846115    7.6886387   0.35349095  2.5627694   4.5428576
  5.8595724   2.7118683   9.446846    6.2642756   4.7844443   6.0548897
  8.103386    3.6440284   4.565682    2.645935  ]
Epoch 1/1000
2023-09-28 19:42:36.594 
Epoch 1/1000 
	 loss: 56.8147, MinusLogProbMetric: 56.8147, val_loss: 63.7633, val_MinusLogProbMetric: 63.7633

Epoch 1: val_loss improved from inf to 63.76329, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_326/weights/best_weights.h5
196/196 - 226s - loss: 56.8147 - MinusLogProbMetric: 56.8147 - val_loss: 63.7633 - val_MinusLogProbMetric: 63.7633 - lr: 3.7037e-05 - 226s/epoch - 1s/step
Epoch 2/1000
2023-09-28 19:43:40.327 
Epoch 2/1000 
	 loss: 52.9495, MinusLogProbMetric: 52.9495, val_loss: 50.6657, val_MinusLogProbMetric: 50.6657

Epoch 2: val_loss improved from 63.76329 to 50.66568, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_326/weights/best_weights.h5
196/196 - 64s - loss: 52.9495 - MinusLogProbMetric: 52.9495 - val_loss: 50.6657 - val_MinusLogProbMetric: 50.6657 - lr: 3.7037e-05 - 64s/epoch - 325ms/step
Epoch 3/1000
2023-09-28 19:44:43.812 
Epoch 3/1000 
	 loss: 55.7590, MinusLogProbMetric: 55.7590, val_loss: 53.6919, val_MinusLogProbMetric: 53.6919

Epoch 3: val_loss did not improve from 50.66568
196/196 - 62s - loss: 55.7590 - MinusLogProbMetric: 55.7590 - val_loss: 53.6919 - val_MinusLogProbMetric: 53.6919 - lr: 3.7037e-05 - 62s/epoch - 318ms/step
Epoch 4/1000
2023-09-28 19:45:51.477 
Epoch 4/1000 
	 loss: 50.8312, MinusLogProbMetric: 50.8312, val_loss: 46.9100, val_MinusLogProbMetric: 46.9100

Epoch 4: val_loss improved from 50.66568 to 46.90997, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_326/weights/best_weights.h5
196/196 - 69s - loss: 50.8312 - MinusLogProbMetric: 50.8312 - val_loss: 46.9100 - val_MinusLogProbMetric: 46.9100 - lr: 3.7037e-05 - 69s/epoch - 351ms/step
Epoch 5/1000
2023-09-28 19:47:00.793 
Epoch 5/1000 
	 loss: 46.2757, MinusLogProbMetric: 46.2757, val_loss: 46.1330, val_MinusLogProbMetric: 46.1330

Epoch 5: val_loss improved from 46.90997 to 46.13303, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_326/weights/best_weights.h5
196/196 - 69s - loss: 46.2757 - MinusLogProbMetric: 46.2757 - val_loss: 46.1330 - val_MinusLogProbMetric: 46.1330 - lr: 3.7037e-05 - 69s/epoch - 354ms/step
Epoch 6/1000
2023-09-28 19:48:05.004 
Epoch 6/1000 
	 loss: 49.4225, MinusLogProbMetric: 49.4225, val_loss: 45.1258, val_MinusLogProbMetric: 45.1258

Epoch 6: val_loss improved from 46.13303 to 45.12576, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_326/weights/best_weights.h5
196/196 - 64s - loss: 49.4225 - MinusLogProbMetric: 49.4225 - val_loss: 45.1258 - val_MinusLogProbMetric: 45.1258 - lr: 3.7037e-05 - 64s/epoch - 326ms/step
Epoch 7/1000
2023-09-28 19:49:09.168 
Epoch 7/1000 
	 loss: 93.8206, MinusLogProbMetric: 93.8206, val_loss: 83.4187, val_MinusLogProbMetric: 83.4187

Epoch 7: val_loss did not improve from 45.12576
196/196 - 63s - loss: 93.8206 - MinusLogProbMetric: 93.8206 - val_loss: 83.4187 - val_MinusLogProbMetric: 83.4187 - lr: 3.7037e-05 - 63s/epoch - 322ms/step
Epoch 8/1000
2023-09-28 19:50:12.851 
Epoch 8/1000 
	 loss: 88.8118, MinusLogProbMetric: 88.8118, val_loss: 86.2827, val_MinusLogProbMetric: 86.2827

Epoch 8: val_loss did not improve from 45.12576
196/196 - 64s - loss: 88.8118 - MinusLogProbMetric: 88.8118 - val_loss: 86.2827 - val_MinusLogProbMetric: 86.2827 - lr: 3.7037e-05 - 64s/epoch - 325ms/step
Epoch 9/1000
2023-09-28 19:51:19.207 
Epoch 9/1000 
	 loss: 71.8781, MinusLogProbMetric: 71.8781, val_loss: 138.8376, val_MinusLogProbMetric: 138.8376

Epoch 9: val_loss did not improve from 45.12576
196/196 - 66s - loss: 71.8781 - MinusLogProbMetric: 71.8781 - val_loss: 138.8376 - val_MinusLogProbMetric: 138.8376 - lr: 3.7037e-05 - 66s/epoch - 339ms/step
Epoch 10/1000
2023-09-28 19:52:27.657 
Epoch 10/1000 
	 loss: 85.7728, MinusLogProbMetric: 85.7728, val_loss: 92.1308, val_MinusLogProbMetric: 92.1308

Epoch 10: val_loss did not improve from 45.12576
196/196 - 68s - loss: 85.7728 - MinusLogProbMetric: 85.7728 - val_loss: 92.1308 - val_MinusLogProbMetric: 92.1308 - lr: 3.7037e-05 - 68s/epoch - 349ms/step
Epoch 11/1000
2023-09-28 19:53:33.665 
Epoch 11/1000 
	 loss: 83.3267, MinusLogProbMetric: 83.3267, val_loss: 108.1401, val_MinusLogProbMetric: 108.1401

Epoch 11: val_loss did not improve from 45.12576
196/196 - 66s - loss: 83.3267 - MinusLogProbMetric: 83.3267 - val_loss: 108.1401 - val_MinusLogProbMetric: 108.1401 - lr: 3.7037e-05 - 66s/epoch - 337ms/step
Epoch 12/1000
2023-09-28 19:54:38.995 
Epoch 12/1000 
	 loss: 80.9689, MinusLogProbMetric: 80.9689, val_loss: 71.3600, val_MinusLogProbMetric: 71.3600

Epoch 12: val_loss did not improve from 45.12576
196/196 - 65s - loss: 80.9689 - MinusLogProbMetric: 80.9689 - val_loss: 71.3600 - val_MinusLogProbMetric: 71.3600 - lr: 3.7037e-05 - 65s/epoch - 333ms/step
Epoch 13/1000
2023-09-28 19:55:45.070 
Epoch 13/1000 
	 loss: 80.4103, MinusLogProbMetric: 80.4103, val_loss: 82.4700, val_MinusLogProbMetric: 82.4700

Epoch 13: val_loss did not improve from 45.12576
196/196 - 66s - loss: 80.4103 - MinusLogProbMetric: 80.4103 - val_loss: 82.4700 - val_MinusLogProbMetric: 82.4700 - lr: 3.7037e-05 - 66s/epoch - 337ms/step
Epoch 14/1000
2023-09-28 19:56:54.451 
Epoch 14/1000 
	 loss: 75.6569, MinusLogProbMetric: 75.6569, val_loss: 70.3645, val_MinusLogProbMetric: 70.3645

Epoch 14: val_loss did not improve from 45.12576
196/196 - 69s - loss: 75.6569 - MinusLogProbMetric: 75.6569 - val_loss: 70.3645 - val_MinusLogProbMetric: 70.3645 - lr: 3.7037e-05 - 69s/epoch - 354ms/step
Epoch 15/1000
2023-09-28 19:58:00.500 
Epoch 15/1000 
	 loss: 95.6187, MinusLogProbMetric: 95.6187, val_loss: 77.8074, val_MinusLogProbMetric: 77.8074

Epoch 15: val_loss did not improve from 45.12576
196/196 - 66s - loss: 95.6187 - MinusLogProbMetric: 95.6187 - val_loss: 77.8074 - val_MinusLogProbMetric: 77.8074 - lr: 3.7037e-05 - 66s/epoch - 337ms/step
Epoch 16/1000
2023-09-28 19:59:07.173 
Epoch 16/1000 
	 loss: 69.5919, MinusLogProbMetric: 69.5919, val_loss: 64.1054, val_MinusLogProbMetric: 64.1054

Epoch 16: val_loss did not improve from 45.12576
196/196 - 67s - loss: 69.5919 - MinusLogProbMetric: 69.5919 - val_loss: 64.1054 - val_MinusLogProbMetric: 64.1054 - lr: 3.7037e-05 - 67s/epoch - 340ms/step
Epoch 17/1000
2023-09-28 20:00:14.114 
Epoch 17/1000 
	 loss: 170.3973, MinusLogProbMetric: 170.3973, val_loss: 168.5233, val_MinusLogProbMetric: 168.5233

Epoch 17: val_loss did not improve from 45.12576
196/196 - 67s - loss: 170.3973 - MinusLogProbMetric: 170.3973 - val_loss: 168.5233 - val_MinusLogProbMetric: 168.5233 - lr: 3.7037e-05 - 67s/epoch - 342ms/step
Epoch 18/1000
2023-09-28 20:01:22.640 
Epoch 18/1000 
	 loss: 103.6544, MinusLogProbMetric: 103.6544, val_loss: 98.0377, val_MinusLogProbMetric: 98.0377

Epoch 18: val_loss did not improve from 45.12576
196/196 - 69s - loss: 103.6544 - MinusLogProbMetric: 103.6544 - val_loss: 98.0377 - val_MinusLogProbMetric: 98.0377 - lr: 3.7037e-05 - 69s/epoch - 350ms/step
Epoch 19/1000
2023-09-28 20:02:30.524 
Epoch 19/1000 
	 loss: 103.1954, MinusLogProbMetric: 103.1954, val_loss: 97.2045, val_MinusLogProbMetric: 97.2045

Epoch 19: val_loss did not improve from 45.12576
196/196 - 68s - loss: 103.1954 - MinusLogProbMetric: 103.1954 - val_loss: 97.2045 - val_MinusLogProbMetric: 97.2045 - lr: 3.7037e-05 - 68s/epoch - 346ms/step
Epoch 20/1000
2023-09-28 20:03:42.059 
Epoch 20/1000 
	 loss: 94.4528, MinusLogProbMetric: 94.4528, val_loss: 91.9072, val_MinusLogProbMetric: 91.9072

Epoch 20: val_loss did not improve from 45.12576
196/196 - 72s - loss: 94.4528 - MinusLogProbMetric: 94.4528 - val_loss: 91.9072 - val_MinusLogProbMetric: 91.9072 - lr: 3.7037e-05 - 72s/epoch - 365ms/step
Epoch 21/1000
2023-09-28 20:04:49.253 
Epoch 21/1000 
	 loss: 90.5888, MinusLogProbMetric: 90.5888, val_loss: 88.3993, val_MinusLogProbMetric: 88.3993

Epoch 21: val_loss did not improve from 45.12576
196/196 - 67s - loss: 90.5888 - MinusLogProbMetric: 90.5888 - val_loss: 88.3993 - val_MinusLogProbMetric: 88.3993 - lr: 3.7037e-05 - 67s/epoch - 343ms/step
Epoch 22/1000
2023-09-28 20:05:58.373 
Epoch 22/1000 
	 loss: 87.1518, MinusLogProbMetric: 87.1518, val_loss: 85.8305, val_MinusLogProbMetric: 85.8305

Epoch 22: val_loss did not improve from 45.12576
196/196 - 69s - loss: 87.1518 - MinusLogProbMetric: 87.1518 - val_loss: 85.8305 - val_MinusLogProbMetric: 85.8305 - lr: 3.7037e-05 - 69s/epoch - 353ms/step
Epoch 23/1000
2023-09-28 20:07:09.195 
Epoch 23/1000 
	 loss: 89.0252, MinusLogProbMetric: 89.0252, val_loss: 85.0433, val_MinusLogProbMetric: 85.0433

Epoch 23: val_loss did not improve from 45.12576
196/196 - 71s - loss: 89.0252 - MinusLogProbMetric: 89.0252 - val_loss: 85.0433 - val_MinusLogProbMetric: 85.0433 - lr: 3.7037e-05 - 71s/epoch - 361ms/step
Epoch 24/1000
2023-09-28 20:08:20.306 
Epoch 24/1000 
	 loss: 83.8372, MinusLogProbMetric: 83.8372, val_loss: 82.7447, val_MinusLogProbMetric: 82.7447

Epoch 24: val_loss did not improve from 45.12576
196/196 - 71s - loss: 83.8372 - MinusLogProbMetric: 83.8372 - val_loss: 82.7447 - val_MinusLogProbMetric: 82.7447 - lr: 3.7037e-05 - 71s/epoch - 363ms/step
Epoch 25/1000
2023-09-28 20:09:30.652 
Epoch 25/1000 
	 loss: 87.1448, MinusLogProbMetric: 87.1448, val_loss: 86.0973, val_MinusLogProbMetric: 86.0973

Epoch 25: val_loss did not improve from 45.12576
196/196 - 70s - loss: 87.1448 - MinusLogProbMetric: 87.1448 - val_loss: 86.0973 - val_MinusLogProbMetric: 86.0973 - lr: 3.7037e-05 - 70s/epoch - 359ms/step
Epoch 26/1000
2023-09-28 20:10:41.668 
Epoch 26/1000 
	 loss: 97.0917, MinusLogProbMetric: 97.0917, val_loss: 86.5999, val_MinusLogProbMetric: 86.5999

Epoch 26: val_loss did not improve from 45.12576
196/196 - 71s - loss: 97.0917 - MinusLogProbMetric: 97.0917 - val_loss: 86.5999 - val_MinusLogProbMetric: 86.5999 - lr: 3.7037e-05 - 71s/epoch - 362ms/step
Epoch 27/1000
2023-09-28 20:11:50.637 
Epoch 27/1000 
	 loss: 84.8427, MinusLogProbMetric: 84.8427, val_loss: 83.0615, val_MinusLogProbMetric: 83.0615

Epoch 27: val_loss did not improve from 45.12576
196/196 - 69s - loss: 84.8427 - MinusLogProbMetric: 84.8427 - val_loss: 83.0615 - val_MinusLogProbMetric: 83.0615 - lr: 3.7037e-05 - 69s/epoch - 352ms/step
Epoch 28/1000
2023-09-28 20:13:00.896 
Epoch 28/1000 
	 loss: 82.1526, MinusLogProbMetric: 82.1526, val_loss: 81.5376, val_MinusLogProbMetric: 81.5376

Epoch 28: val_loss did not improve from 45.12576
196/196 - 70s - loss: 82.1526 - MinusLogProbMetric: 82.1526 - val_loss: 81.5376 - val_MinusLogProbMetric: 81.5376 - lr: 3.7037e-05 - 70s/epoch - 358ms/step
Epoch 29/1000
2023-09-28 20:14:10.046 
Epoch 29/1000 
	 loss: 86.8054, MinusLogProbMetric: 86.8054, val_loss: 167.4571, val_MinusLogProbMetric: 167.4571

Epoch 29: val_loss did not improve from 45.12576
196/196 - 69s - loss: 86.8054 - MinusLogProbMetric: 86.8054 - val_loss: 167.4571 - val_MinusLogProbMetric: 167.4571 - lr: 3.7037e-05 - 69s/epoch - 353ms/step
Epoch 30/1000
2023-09-28 20:15:17.784 
Epoch 30/1000 
	 loss: 96.5947, MinusLogProbMetric: 96.5947, val_loss: 84.6163, val_MinusLogProbMetric: 84.6163

Epoch 30: val_loss did not improve from 45.12576
196/196 - 68s - loss: 96.5947 - MinusLogProbMetric: 96.5947 - val_loss: 84.6163 - val_MinusLogProbMetric: 84.6163 - lr: 3.7037e-05 - 68s/epoch - 346ms/step
Epoch 31/1000
2023-09-28 20:16:25.128 
Epoch 31/1000 
	 loss: 107.8254, MinusLogProbMetric: 107.8254, val_loss: 108.7295, val_MinusLogProbMetric: 108.7295

Epoch 31: val_loss did not improve from 45.12576
196/196 - 67s - loss: 107.8254 - MinusLogProbMetric: 107.8254 - val_loss: 108.7295 - val_MinusLogProbMetric: 108.7295 - lr: 3.7037e-05 - 67s/epoch - 344ms/step
Epoch 32/1000
2023-09-28 20:17:33.561 
Epoch 32/1000 
	 loss: 103.0151, MinusLogProbMetric: 103.0151, val_loss: 98.9287, val_MinusLogProbMetric: 98.9287

Epoch 32: val_loss did not improve from 45.12576
196/196 - 68s - loss: 103.0151 - MinusLogProbMetric: 103.0151 - val_loss: 98.9287 - val_MinusLogProbMetric: 98.9287 - lr: 3.7037e-05 - 68s/epoch - 349ms/step
Epoch 33/1000
2023-09-28 20:18:42.223 
Epoch 33/1000 
	 loss: 96.4516, MinusLogProbMetric: 96.4516, val_loss: 95.8107, val_MinusLogProbMetric: 95.8107

Epoch 33: val_loss did not improve from 45.12576
196/196 - 69s - loss: 96.4516 - MinusLogProbMetric: 96.4516 - val_loss: 95.8107 - val_MinusLogProbMetric: 95.8107 - lr: 3.7037e-05 - 69s/epoch - 350ms/step
Epoch 34/1000
2023-09-28 20:19:51.175 
Epoch 34/1000 
	 loss: 110.3876, MinusLogProbMetric: 110.3876, val_loss: 124.7802, val_MinusLogProbMetric: 124.7802

Epoch 34: val_loss did not improve from 45.12576
196/196 - 69s - loss: 110.3876 - MinusLogProbMetric: 110.3876 - val_loss: 124.7802 - val_MinusLogProbMetric: 124.7802 - lr: 3.7037e-05 - 69s/epoch - 352ms/step
Epoch 35/1000
2023-09-28 20:21:00.246 
Epoch 35/1000 
	 loss: 122.2236, MinusLogProbMetric: 122.2236, val_loss: 119.5287, val_MinusLogProbMetric: 119.5287

Epoch 35: val_loss did not improve from 45.12576
196/196 - 69s - loss: 122.2236 - MinusLogProbMetric: 122.2236 - val_loss: 119.5287 - val_MinusLogProbMetric: 119.5287 - lr: 3.7037e-05 - 69s/epoch - 352ms/step
Epoch 36/1000
2023-09-28 20:22:07.868 
Epoch 36/1000 
	 loss: 118.5007, MinusLogProbMetric: 118.5007, val_loss: 117.7640, val_MinusLogProbMetric: 117.7640

Epoch 36: val_loss did not improve from 45.12576
196/196 - 68s - loss: 118.5007 - MinusLogProbMetric: 118.5007 - val_loss: 117.7640 - val_MinusLogProbMetric: 117.7640 - lr: 3.7037e-05 - 68s/epoch - 345ms/step
Epoch 37/1000
2023-09-28 20:23:17.243 
Epoch 37/1000 
	 loss: 117.3537, MinusLogProbMetric: 117.3537, val_loss: 116.9605, val_MinusLogProbMetric: 116.9605

Epoch 37: val_loss did not improve from 45.12576
196/196 - 69s - loss: 117.3537 - MinusLogProbMetric: 117.3537 - val_loss: 116.9605 - val_MinusLogProbMetric: 116.9605 - lr: 3.7037e-05 - 69s/epoch - 354ms/step
Epoch 38/1000
2023-09-28 20:24:22.190 
Epoch 38/1000 
	 loss: 116.1680, MinusLogProbMetric: 116.1680, val_loss: 115.7009, val_MinusLogProbMetric: 115.7009

Epoch 38: val_loss did not improve from 45.12576
196/196 - 65s - loss: 116.1680 - MinusLogProbMetric: 116.1680 - val_loss: 115.7009 - val_MinusLogProbMetric: 115.7009 - lr: 3.7037e-05 - 65s/epoch - 331ms/step
Epoch 39/1000
2023-09-28 20:25:32.867 
Epoch 39/1000 
	 loss: 115.1974, MinusLogProbMetric: 115.1974, val_loss: 114.5800, val_MinusLogProbMetric: 114.5800

Epoch 39: val_loss did not improve from 45.12576
196/196 - 71s - loss: 115.1974 - MinusLogProbMetric: 115.1974 - val_loss: 114.5800 - val_MinusLogProbMetric: 114.5800 - lr: 3.7037e-05 - 71s/epoch - 361ms/step
Epoch 40/1000
2023-09-28 20:26:44.237 
Epoch 40/1000 
	 loss: 116.0289, MinusLogProbMetric: 116.0289, val_loss: 100.1278, val_MinusLogProbMetric: 100.1278

Epoch 40: val_loss did not improve from 45.12576
196/196 - 71s - loss: 116.0289 - MinusLogProbMetric: 116.0289 - val_loss: 100.1278 - val_MinusLogProbMetric: 100.1278 - lr: 3.7037e-05 - 71s/epoch - 364ms/step
Epoch 41/1000
2023-09-28 20:27:54.839 
Epoch 41/1000 
	 loss: 96.8881, MinusLogProbMetric: 96.8881, val_loss: 92.3099, val_MinusLogProbMetric: 92.3099

Epoch 41: val_loss did not improve from 45.12576
196/196 - 71s - loss: 96.8881 - MinusLogProbMetric: 96.8881 - val_loss: 92.3099 - val_MinusLogProbMetric: 92.3099 - lr: 3.7037e-05 - 71s/epoch - 360ms/step
Epoch 42/1000
2023-09-28 20:29:05.860 
Epoch 42/1000 
	 loss: 91.1841, MinusLogProbMetric: 91.1841, val_loss: 90.4363, val_MinusLogProbMetric: 90.4363

Epoch 42: val_loss did not improve from 45.12576
196/196 - 71s - loss: 91.1841 - MinusLogProbMetric: 91.1841 - val_loss: 90.4363 - val_MinusLogProbMetric: 90.4363 - lr: 3.7037e-05 - 71s/epoch - 362ms/step
Epoch 43/1000
2023-09-28 20:30:17.164 
Epoch 43/1000 
	 loss: 89.9089, MinusLogProbMetric: 89.9089, val_loss: 89.8933, val_MinusLogProbMetric: 89.8933

Epoch 43: val_loss did not improve from 45.12576
196/196 - 71s - loss: 89.9089 - MinusLogProbMetric: 89.9089 - val_loss: 89.8933 - val_MinusLogProbMetric: 89.8933 - lr: 3.7037e-05 - 71s/epoch - 364ms/step
Epoch 44/1000
2023-09-28 20:31:27.910 
Epoch 44/1000 
	 loss: 89.0465, MinusLogProbMetric: 89.0465, val_loss: 88.9370, val_MinusLogProbMetric: 88.9370

Epoch 44: val_loss did not improve from 45.12576
196/196 - 71s - loss: 89.0465 - MinusLogProbMetric: 89.0465 - val_loss: 88.9370 - val_MinusLogProbMetric: 88.9370 - lr: 3.7037e-05 - 71s/epoch - 361ms/step
Epoch 45/1000
2023-09-28 20:32:38.794 
Epoch 45/1000 
	 loss: 88.1823, MinusLogProbMetric: 88.1823, val_loss: 88.2347, val_MinusLogProbMetric: 88.2347

Epoch 45: val_loss did not improve from 45.12576
196/196 - 71s - loss: 88.1823 - MinusLogProbMetric: 88.1823 - val_loss: 88.2347 - val_MinusLogProbMetric: 88.2347 - lr: 3.7037e-05 - 71s/epoch - 362ms/step
Epoch 46/1000
2023-09-28 20:33:49.801 
Epoch 46/1000 
	 loss: 90.1376, MinusLogProbMetric: 90.1376, val_loss: 88.0871, val_MinusLogProbMetric: 88.0871

Epoch 46: val_loss did not improve from 45.12576
196/196 - 71s - loss: 90.1376 - MinusLogProbMetric: 90.1376 - val_loss: 88.0871 - val_MinusLogProbMetric: 88.0871 - lr: 3.7037e-05 - 71s/epoch - 362ms/step
Epoch 47/1000
2023-09-28 20:35:00.556 
Epoch 47/1000 
	 loss: 244.8857, MinusLogProbMetric: 244.8857, val_loss: 221.6920, val_MinusLogProbMetric: 221.6920

Epoch 47: val_loss did not improve from 45.12576
196/196 - 71s - loss: 244.8857 - MinusLogProbMetric: 244.8857 - val_loss: 221.6920 - val_MinusLogProbMetric: 221.6920 - lr: 3.7037e-05 - 71s/epoch - 361ms/step
Epoch 48/1000
2023-09-28 20:36:11.154 
Epoch 48/1000 
	 loss: 183.4394, MinusLogProbMetric: 183.4394, val_loss: 155.6321, val_MinusLogProbMetric: 155.6321

Epoch 48: val_loss did not improve from 45.12576
196/196 - 71s - loss: 183.4394 - MinusLogProbMetric: 183.4394 - val_loss: 155.6321 - val_MinusLogProbMetric: 155.6321 - lr: 3.7037e-05 - 71s/epoch - 360ms/step
Epoch 49/1000
2023-09-28 20:37:21.993 
Epoch 49/1000 
	 loss: 149.8274, MinusLogProbMetric: 149.8274, val_loss: 141.6643, val_MinusLogProbMetric: 141.6643

Epoch 49: val_loss did not improve from 45.12576
196/196 - 71s - loss: 149.8274 - MinusLogProbMetric: 149.8274 - val_loss: 141.6643 - val_MinusLogProbMetric: 141.6643 - lr: 3.7037e-05 - 71s/epoch - 361ms/step
Epoch 50/1000
2023-09-28 20:38:30.247 
Epoch 50/1000 
	 loss: 149.1657, MinusLogProbMetric: 149.1657, val_loss: 137.8273, val_MinusLogProbMetric: 137.8273

Epoch 50: val_loss did not improve from 45.12576
196/196 - 68s - loss: 149.1657 - MinusLogProbMetric: 149.1657 - val_loss: 137.8273 - val_MinusLogProbMetric: 137.8273 - lr: 3.7037e-05 - 68s/epoch - 348ms/step
Epoch 51/1000
2023-09-28 20:39:39.396 
Epoch 51/1000 
	 loss: 145.8791, MinusLogProbMetric: 145.8791, val_loss: 144.8053, val_MinusLogProbMetric: 144.8053

Epoch 51: val_loss did not improve from 45.12576
196/196 - 69s - loss: 145.8791 - MinusLogProbMetric: 145.8791 - val_loss: 144.8053 - val_MinusLogProbMetric: 144.8053 - lr: 3.7037e-05 - 69s/epoch - 353ms/step
Epoch 52/1000
2023-09-28 20:40:48.553 
Epoch 52/1000 
	 loss: 133.7996, MinusLogProbMetric: 133.7996, val_loss: 127.5166, val_MinusLogProbMetric: 127.5166

Epoch 52: val_loss did not improve from 45.12576
196/196 - 69s - loss: 133.7996 - MinusLogProbMetric: 133.7996 - val_loss: 127.5166 - val_MinusLogProbMetric: 127.5166 - lr: 3.7037e-05 - 69s/epoch - 353ms/step
Epoch 53/1000
2023-09-28 20:41:56.955 
Epoch 53/1000 
	 loss: 123.4792, MinusLogProbMetric: 123.4792, val_loss: 120.4743, val_MinusLogProbMetric: 120.4743

Epoch 53: val_loss did not improve from 45.12576
196/196 - 68s - loss: 123.4792 - MinusLogProbMetric: 123.4792 - val_loss: 120.4743 - val_MinusLogProbMetric: 120.4743 - lr: 3.7037e-05 - 68s/epoch - 349ms/step
Epoch 54/1000
2023-09-28 20:43:04.701 
Epoch 54/1000 
	 loss: 120.2894, MinusLogProbMetric: 120.2894, val_loss: 116.7904, val_MinusLogProbMetric: 116.7904

Epoch 54: val_loss did not improve from 45.12576
196/196 - 68s - loss: 120.2894 - MinusLogProbMetric: 120.2894 - val_loss: 116.7904 - val_MinusLogProbMetric: 116.7904 - lr: 3.7037e-05 - 68s/epoch - 346ms/step
Epoch 55/1000
2023-09-28 20:44:12.739 
Epoch 55/1000 
	 loss: 114.5584, MinusLogProbMetric: 114.5584, val_loss: 113.0053, val_MinusLogProbMetric: 113.0053

Epoch 55: val_loss did not improve from 45.12576
196/196 - 68s - loss: 114.5584 - MinusLogProbMetric: 114.5584 - val_loss: 113.0053 - val_MinusLogProbMetric: 113.0053 - lr: 3.7037e-05 - 68s/epoch - 347ms/step
Epoch 56/1000
2023-09-28 20:45:20.819 
Epoch 56/1000 
	 loss: 111.4946, MinusLogProbMetric: 111.4946, val_loss: 110.1592, val_MinusLogProbMetric: 110.1592

Epoch 56: val_loss did not improve from 45.12576
196/196 - 68s - loss: 111.4946 - MinusLogProbMetric: 111.4946 - val_loss: 110.1592 - val_MinusLogProbMetric: 110.1592 - lr: 3.7037e-05 - 68s/epoch - 347ms/step
Epoch 57/1000
2023-09-28 20:46:29.794 
Epoch 57/1000 
	 loss: 109.4231, MinusLogProbMetric: 109.4231, val_loss: 108.9053, val_MinusLogProbMetric: 108.9053

Epoch 57: val_loss did not improve from 45.12576
196/196 - 69s - loss: 109.4231 - MinusLogProbMetric: 109.4231 - val_loss: 108.9053 - val_MinusLogProbMetric: 108.9053 - lr: 1.8519e-05 - 69s/epoch - 352ms/step
Epoch 58/1000
2023-09-28 20:47:38.750 
Epoch 58/1000 
	 loss: 108.3191, MinusLogProbMetric: 108.3191, val_loss: 107.8328, val_MinusLogProbMetric: 107.8328

Epoch 58: val_loss did not improve from 45.12576
196/196 - 69s - loss: 108.3191 - MinusLogProbMetric: 108.3191 - val_loss: 107.8328 - val_MinusLogProbMetric: 107.8328 - lr: 1.8519e-05 - 69s/epoch - 352ms/step
Epoch 59/1000
2023-09-28 20:48:47.310 
Epoch 59/1000 
	 loss: 107.2852, MinusLogProbMetric: 107.2852, val_loss: 106.8463, val_MinusLogProbMetric: 106.8463

Epoch 59: val_loss did not improve from 45.12576
196/196 - 69s - loss: 107.2852 - MinusLogProbMetric: 107.2852 - val_loss: 106.8463 - val_MinusLogProbMetric: 106.8463 - lr: 1.8519e-05 - 69s/epoch - 350ms/step
Epoch 60/1000
2023-09-28 20:49:55.433 
Epoch 60/1000 
	 loss: 106.3873, MinusLogProbMetric: 106.3873, val_loss: 105.9877, val_MinusLogProbMetric: 105.9877

Epoch 60: val_loss did not improve from 45.12576
196/196 - 68s - loss: 106.3873 - MinusLogProbMetric: 106.3873 - val_loss: 105.9877 - val_MinusLogProbMetric: 105.9877 - lr: 1.8519e-05 - 68s/epoch - 348ms/step
Epoch 61/1000
2023-09-28 20:51:04.218 
Epoch 61/1000 
	 loss: 105.5292, MinusLogProbMetric: 105.5292, val_loss: 105.1697, val_MinusLogProbMetric: 105.1697

Epoch 61: val_loss did not improve from 45.12576
196/196 - 69s - loss: 105.5292 - MinusLogProbMetric: 105.5292 - val_loss: 105.1697 - val_MinusLogProbMetric: 105.1697 - lr: 1.8519e-05 - 69s/epoch - 351ms/step
Epoch 62/1000
2023-09-28 20:52:13.276 
Epoch 62/1000 
	 loss: 104.8445, MinusLogProbMetric: 104.8445, val_loss: 104.5620, val_MinusLogProbMetric: 104.5620

Epoch 62: val_loss did not improve from 45.12576
196/196 - 69s - loss: 104.8445 - MinusLogProbMetric: 104.8445 - val_loss: 104.5620 - val_MinusLogProbMetric: 104.5620 - lr: 1.8519e-05 - 69s/epoch - 352ms/step
Epoch 63/1000
2023-09-28 20:53:22.835 
Epoch 63/1000 
	 loss: 104.2173, MinusLogProbMetric: 104.2173, val_loss: 103.8833, val_MinusLogProbMetric: 103.8833

Epoch 63: val_loss did not improve from 45.12576
196/196 - 70s - loss: 104.2173 - MinusLogProbMetric: 104.2173 - val_loss: 103.8833 - val_MinusLogProbMetric: 103.8833 - lr: 1.8519e-05 - 70s/epoch - 355ms/step
Epoch 64/1000
2023-09-28 20:54:32.488 
Epoch 64/1000 
	 loss: 103.5399, MinusLogProbMetric: 103.5399, val_loss: 103.3430, val_MinusLogProbMetric: 103.3430

Epoch 64: val_loss did not improve from 45.12576
196/196 - 70s - loss: 103.5399 - MinusLogProbMetric: 103.5399 - val_loss: 103.3430 - val_MinusLogProbMetric: 103.3430 - lr: 1.8519e-05 - 70s/epoch - 355ms/step
Epoch 65/1000
2023-09-28 20:55:41.780 
Epoch 65/1000 
	 loss: 103.4007, MinusLogProbMetric: 103.4007, val_loss: 103.6850, val_MinusLogProbMetric: 103.6850

Epoch 65: val_loss did not improve from 45.12576
196/196 - 69s - loss: 103.4007 - MinusLogProbMetric: 103.4007 - val_loss: 103.6850 - val_MinusLogProbMetric: 103.6850 - lr: 1.8519e-05 - 69s/epoch - 353ms/step
Epoch 66/1000
2023-09-28 20:56:47.140 
Epoch 66/1000 
	 loss: 102.5941, MinusLogProbMetric: 102.5941, val_loss: 102.2188, val_MinusLogProbMetric: 102.2188

Epoch 66: val_loss did not improve from 45.12576
196/196 - 65s - loss: 102.5941 - MinusLogProbMetric: 102.5941 - val_loss: 102.2188 - val_MinusLogProbMetric: 102.2188 - lr: 1.8519e-05 - 65s/epoch - 333ms/step
Epoch 67/1000
2023-09-28 20:57:58.483 
Epoch 67/1000 
	 loss: 101.8615, MinusLogProbMetric: 101.8615, val_loss: 101.7075, val_MinusLogProbMetric: 101.7075

Epoch 67: val_loss did not improve from 45.12576
196/196 - 71s - loss: 101.8615 - MinusLogProbMetric: 101.8615 - val_loss: 101.7075 - val_MinusLogProbMetric: 101.7075 - lr: 1.8519e-05 - 71s/epoch - 364ms/step
Epoch 68/1000
2023-09-28 20:59:06.252 
Epoch 68/1000 
	 loss: 101.3866, MinusLogProbMetric: 101.3866, val_loss: 101.1893, val_MinusLogProbMetric: 101.1893

Epoch 68: val_loss did not improve from 45.12576
196/196 - 68s - loss: 101.3866 - MinusLogProbMetric: 101.3866 - val_loss: 101.1893 - val_MinusLogProbMetric: 101.1893 - lr: 1.8519e-05 - 68s/epoch - 346ms/step
Epoch 69/1000
2023-09-28 21:00:17.637 
Epoch 69/1000 
	 loss: 100.9903, MinusLogProbMetric: 100.9903, val_loss: 100.9499, val_MinusLogProbMetric: 100.9499

Epoch 69: val_loss did not improve from 45.12576
196/196 - 71s - loss: 100.9903 - MinusLogProbMetric: 100.9903 - val_loss: 100.9499 - val_MinusLogProbMetric: 100.9499 - lr: 1.8519e-05 - 71s/epoch - 364ms/step
Epoch 70/1000
2023-09-28 21:01:25.481 
Epoch 70/1000 
	 loss: 100.5567, MinusLogProbMetric: 100.5567, val_loss: 100.4699, val_MinusLogProbMetric: 100.4699

Epoch 70: val_loss did not improve from 45.12576
196/196 - 68s - loss: 100.5567 - MinusLogProbMetric: 100.5567 - val_loss: 100.4699 - val_MinusLogProbMetric: 100.4699 - lr: 1.8519e-05 - 68s/epoch - 346ms/step
Epoch 71/1000
2023-09-28 21:02:33.474 
Epoch 71/1000 
	 loss: 100.1845, MinusLogProbMetric: 100.1845, val_loss: 99.9421, val_MinusLogProbMetric: 99.9421

Epoch 71: val_loss did not improve from 45.12576
196/196 - 68s - loss: 100.1845 - MinusLogProbMetric: 100.1845 - val_loss: 99.9421 - val_MinusLogProbMetric: 99.9421 - lr: 1.8519e-05 - 68s/epoch - 347ms/step
Epoch 72/1000
2023-09-28 21:03:38.474 
Epoch 72/1000 
	 loss: 99.6639, MinusLogProbMetric: 99.6639, val_loss: 99.5294, val_MinusLogProbMetric: 99.5294

Epoch 72: val_loss did not improve from 45.12576
196/196 - 65s - loss: 99.6639 - MinusLogProbMetric: 99.6639 - val_loss: 99.5294 - val_MinusLogProbMetric: 99.5294 - lr: 1.8519e-05 - 65s/epoch - 332ms/step
Epoch 73/1000
2023-09-28 21:04:41.000 
Epoch 73/1000 
	 loss: 99.2796, MinusLogProbMetric: 99.2796, val_loss: 99.1011, val_MinusLogProbMetric: 99.1011

Epoch 73: val_loss did not improve from 45.12576
196/196 - 63s - loss: 99.2796 - MinusLogProbMetric: 99.2796 - val_loss: 99.1011 - val_MinusLogProbMetric: 99.1011 - lr: 1.8519e-05 - 63s/epoch - 319ms/step
Epoch 74/1000
2023-09-28 21:05:51.633 
Epoch 74/1000 
	 loss: 99.0321, MinusLogProbMetric: 99.0321, val_loss: 98.7605, val_MinusLogProbMetric: 98.7605

Epoch 74: val_loss did not improve from 45.12576
196/196 - 71s - loss: 99.0321 - MinusLogProbMetric: 99.0321 - val_loss: 98.7605 - val_MinusLogProbMetric: 98.7605 - lr: 1.8519e-05 - 71s/epoch - 360ms/step
Epoch 75/1000
2023-09-28 21:06:54.509 
Epoch 75/1000 
	 loss: 98.5960, MinusLogProbMetric: 98.5960, val_loss: 98.4581, val_MinusLogProbMetric: 98.4581

Epoch 75: val_loss did not improve from 45.12576
196/196 - 63s - loss: 98.5960 - MinusLogProbMetric: 98.5960 - val_loss: 98.4581 - val_MinusLogProbMetric: 98.4581 - lr: 1.8519e-05 - 63s/epoch - 321ms/step
Epoch 76/1000
2023-09-28 21:07:57.244 
Epoch 76/1000 
	 loss: 98.2112, MinusLogProbMetric: 98.2112, val_loss: 98.4579, val_MinusLogProbMetric: 98.4579

Epoch 76: val_loss did not improve from 45.12576
196/196 - 63s - loss: 98.2112 - MinusLogProbMetric: 98.2112 - val_loss: 98.4579 - val_MinusLogProbMetric: 98.4579 - lr: 1.8519e-05 - 63s/epoch - 320ms/step
Epoch 77/1000
2023-09-28 21:09:09.143 
Epoch 77/1000 
	 loss: 98.0906, MinusLogProbMetric: 98.0906, val_loss: 97.8965, val_MinusLogProbMetric: 97.8965

Epoch 77: val_loss did not improve from 45.12576
196/196 - 72s - loss: 98.0906 - MinusLogProbMetric: 98.0906 - val_loss: 97.8965 - val_MinusLogProbMetric: 97.8965 - lr: 1.8519e-05 - 72s/epoch - 367ms/step
Epoch 78/1000
2023-09-28 21:10:12.137 
Epoch 78/1000 
	 loss: 97.5843, MinusLogProbMetric: 97.5843, val_loss: 97.4138, val_MinusLogProbMetric: 97.4138

Epoch 78: val_loss did not improve from 45.12576
196/196 - 63s - loss: 97.5843 - MinusLogProbMetric: 97.5843 - val_loss: 97.4138 - val_MinusLogProbMetric: 97.4138 - lr: 1.8519e-05 - 63s/epoch - 321ms/step
Epoch 79/1000
2023-09-28 21:11:17.711 
Epoch 79/1000 
	 loss: 97.6278, MinusLogProbMetric: 97.6278, val_loss: 97.3353, val_MinusLogProbMetric: 97.3353

Epoch 79: val_loss did not improve from 45.12576
196/196 - 66s - loss: 97.6278 - MinusLogProbMetric: 97.6278 - val_loss: 97.3353 - val_MinusLogProbMetric: 97.3353 - lr: 1.8519e-05 - 66s/epoch - 335ms/step
Epoch 80/1000
2023-09-28 21:12:27.139 
Epoch 80/1000 
	 loss: 96.9515, MinusLogProbMetric: 96.9515, val_loss: 96.8708, val_MinusLogProbMetric: 96.8708

Epoch 80: val_loss did not improve from 45.12576
196/196 - 69s - loss: 96.9515 - MinusLogProbMetric: 96.9515 - val_loss: 96.8708 - val_MinusLogProbMetric: 96.8708 - lr: 1.8519e-05 - 69s/epoch - 354ms/step
Epoch 81/1000
2023-09-28 21:13:29.248 
Epoch 81/1000 
	 loss: 96.6975, MinusLogProbMetric: 96.6975, val_loss: 96.4718, val_MinusLogProbMetric: 96.4718

Epoch 81: val_loss did not improve from 45.12576
196/196 - 62s - loss: 96.6975 - MinusLogProbMetric: 96.6975 - val_loss: 96.4718 - val_MinusLogProbMetric: 96.4718 - lr: 1.8519e-05 - 62s/epoch - 317ms/step
Epoch 82/1000
2023-09-28 21:14:34.676 
Epoch 82/1000 
	 loss: 96.6922, MinusLogProbMetric: 96.6922, val_loss: 96.2329, val_MinusLogProbMetric: 96.2329

Epoch 82: val_loss did not improve from 45.12576
196/196 - 65s - loss: 96.6922 - MinusLogProbMetric: 96.6922 - val_loss: 96.2329 - val_MinusLogProbMetric: 96.2329 - lr: 1.8519e-05 - 65s/epoch - 334ms/step
Epoch 83/1000
2023-09-28 21:15:41.983 
Epoch 83/1000 
	 loss: 96.2585, MinusLogProbMetric: 96.2585, val_loss: 95.9411, val_MinusLogProbMetric: 95.9411

Epoch 83: val_loss did not improve from 45.12576
196/196 - 67s - loss: 96.2585 - MinusLogProbMetric: 96.2585 - val_loss: 95.9411 - val_MinusLogProbMetric: 95.9411 - lr: 1.8519e-05 - 67s/epoch - 343ms/step
Epoch 84/1000
2023-09-28 21:16:45.021 
Epoch 84/1000 
	 loss: 95.7820, MinusLogProbMetric: 95.7820, val_loss: 95.7533, val_MinusLogProbMetric: 95.7533

Epoch 84: val_loss did not improve from 45.12576
196/196 - 63s - loss: 95.7820 - MinusLogProbMetric: 95.7820 - val_loss: 95.7533 - val_MinusLogProbMetric: 95.7533 - lr: 1.8519e-05 - 63s/epoch - 322ms/step
Epoch 85/1000
2023-09-28 21:17:52.759 
Epoch 85/1000 
	 loss: 95.4553, MinusLogProbMetric: 95.4553, val_loss: 95.3444, val_MinusLogProbMetric: 95.3444

Epoch 85: val_loss did not improve from 45.12576
196/196 - 68s - loss: 95.4553 - MinusLogProbMetric: 95.4553 - val_loss: 95.3444 - val_MinusLogProbMetric: 95.3444 - lr: 1.8519e-05 - 68s/epoch - 346ms/step
Epoch 86/1000
2023-09-28 21:18:58.254 
Epoch 86/1000 
	 loss: 95.1802, MinusLogProbMetric: 95.1802, val_loss: 95.2907, val_MinusLogProbMetric: 95.2907

Epoch 86: val_loss did not improve from 45.12576
196/196 - 65s - loss: 95.1802 - MinusLogProbMetric: 95.1802 - val_loss: 95.2907 - val_MinusLogProbMetric: 95.2907 - lr: 1.8519e-05 - 65s/epoch - 334ms/step
Epoch 87/1000
2023-09-28 21:20:00.112 
Epoch 87/1000 
	 loss: 95.1578, MinusLogProbMetric: 95.1578, val_loss: 95.5421, val_MinusLogProbMetric: 95.5421

Epoch 87: val_loss did not improve from 45.12576
196/196 - 62s - loss: 95.1578 - MinusLogProbMetric: 95.1578 - val_loss: 95.5421 - val_MinusLogProbMetric: 95.5421 - lr: 1.8519e-05 - 62s/epoch - 316ms/step
Epoch 88/1000
2023-09-28 21:21:08.339 
Epoch 88/1000 
	 loss: 94.7706, MinusLogProbMetric: 94.7706, val_loss: 94.4936, val_MinusLogProbMetric: 94.4936

Epoch 88: val_loss did not improve from 45.12576
196/196 - 68s - loss: 94.7706 - MinusLogProbMetric: 94.7706 - val_loss: 94.4936 - val_MinusLogProbMetric: 94.4936 - lr: 1.8519e-05 - 68s/epoch - 348ms/step
Epoch 89/1000
2023-09-28 21:22:13.127 
Epoch 89/1000 
	 loss: 94.3858, MinusLogProbMetric: 94.3858, val_loss: 94.4135, val_MinusLogProbMetric: 94.4135

Epoch 89: val_loss did not improve from 45.12576
196/196 - 65s - loss: 94.3858 - MinusLogProbMetric: 94.3858 - val_loss: 94.4135 - val_MinusLogProbMetric: 94.4135 - lr: 1.8519e-05 - 65s/epoch - 331ms/step
Epoch 90/1000
2023-09-28 21:23:15.255 
Epoch 90/1000 
	 loss: 94.0346, MinusLogProbMetric: 94.0346, val_loss: 93.8717, val_MinusLogProbMetric: 93.8717

Epoch 90: val_loss did not improve from 45.12576
196/196 - 62s - loss: 94.0346 - MinusLogProbMetric: 94.0346 - val_loss: 93.8717 - val_MinusLogProbMetric: 93.8717 - lr: 1.8519e-05 - 62s/epoch - 317ms/step
Epoch 91/1000
2023-09-28 21:24:22.674 
Epoch 91/1000 
	 loss: 93.7513, MinusLogProbMetric: 93.7513, val_loss: 93.5092, val_MinusLogProbMetric: 93.5092

Epoch 91: val_loss did not improve from 45.12576
196/196 - 67s - loss: 93.7513 - MinusLogProbMetric: 93.7513 - val_loss: 93.5092 - val_MinusLogProbMetric: 93.5092 - lr: 1.8519e-05 - 67s/epoch - 344ms/step
Epoch 92/1000
2023-09-28 21:25:26.041 
Epoch 92/1000 
	 loss: 94.9141, MinusLogProbMetric: 94.9141, val_loss: 93.8495, val_MinusLogProbMetric: 93.8495

Epoch 92: val_loss did not improve from 45.12576
196/196 - 63s - loss: 94.9141 - MinusLogProbMetric: 94.9141 - val_loss: 93.8495 - val_MinusLogProbMetric: 93.8495 - lr: 1.8519e-05 - 63s/epoch - 323ms/step
Epoch 93/1000
2023-09-28 21:26:28.401 
Epoch 93/1000 
	 loss: 93.5044, MinusLogProbMetric: 93.5044, val_loss: 93.3086, val_MinusLogProbMetric: 93.3086

Epoch 93: val_loss did not improve from 45.12576
196/196 - 62s - loss: 93.5044 - MinusLogProbMetric: 93.5044 - val_loss: 93.3086 - val_MinusLogProbMetric: 93.3086 - lr: 1.8519e-05 - 62s/epoch - 318ms/step
Epoch 94/1000
2023-09-28 21:27:39.426 
Epoch 94/1000 
	 loss: 93.1018, MinusLogProbMetric: 93.1018, val_loss: 93.0468, val_MinusLogProbMetric: 93.0468

Epoch 94: val_loss did not improve from 45.12576
196/196 - 71s - loss: 93.1018 - MinusLogProbMetric: 93.1018 - val_loss: 93.0468 - val_MinusLogProbMetric: 93.0468 - lr: 1.8519e-05 - 71s/epoch - 362ms/step
Epoch 95/1000
2023-09-28 21:28:43.602 
Epoch 95/1000 
	 loss: 92.8745, MinusLogProbMetric: 92.8745, val_loss: 92.6628, val_MinusLogProbMetric: 92.6628

Epoch 95: val_loss did not improve from 45.12576
196/196 - 64s - loss: 92.8745 - MinusLogProbMetric: 92.8745 - val_loss: 92.6628 - val_MinusLogProbMetric: 92.6628 - lr: 1.8519e-05 - 64s/epoch - 327ms/step
Epoch 96/1000
2023-09-28 21:29:46.402 
Epoch 96/1000 
	 loss: 92.5263, MinusLogProbMetric: 92.5263, val_loss: 92.4316, val_MinusLogProbMetric: 92.4316

Epoch 96: val_loss did not improve from 45.12576
196/196 - 63s - loss: 92.5263 - MinusLogProbMetric: 92.5263 - val_loss: 92.4316 - val_MinusLogProbMetric: 92.4316 - lr: 1.8519e-05 - 63s/epoch - 320ms/step
Epoch 97/1000
2023-09-28 21:30:50.904 
Epoch 97/1000 
	 loss: 92.3024, MinusLogProbMetric: 92.3024, val_loss: 92.1590, val_MinusLogProbMetric: 92.1590

Epoch 97: val_loss did not improve from 45.12576
196/196 - 65s - loss: 92.3024 - MinusLogProbMetric: 92.3024 - val_loss: 92.1590 - val_MinusLogProbMetric: 92.1590 - lr: 1.8519e-05 - 65s/epoch - 329ms/step
Epoch 98/1000
2023-09-28 21:31:51.073 
Epoch 98/1000 
	 loss: 93.6455, MinusLogProbMetric: 93.6455, val_loss: 92.2497, val_MinusLogProbMetric: 92.2497

Epoch 98: val_loss did not improve from 45.12576
196/196 - 60s - loss: 93.6455 - MinusLogProbMetric: 93.6455 - val_loss: 92.2497 - val_MinusLogProbMetric: 92.2497 - lr: 1.8519e-05 - 60s/epoch - 307ms/step
Epoch 99/1000
2023-09-28 21:32:51.785 
Epoch 99/1000 
	 loss: 92.0749, MinusLogProbMetric: 92.0749, val_loss: 91.9210, val_MinusLogProbMetric: 91.9210

Epoch 99: val_loss did not improve from 45.12576
196/196 - 61s - loss: 92.0749 - MinusLogProbMetric: 92.0749 - val_loss: 91.9210 - val_MinusLogProbMetric: 91.9210 - lr: 1.8519e-05 - 61s/epoch - 310ms/step
Epoch 100/1000
2023-09-28 21:33:53.022 
Epoch 100/1000 
	 loss: 91.7960, MinusLogProbMetric: 91.7960, val_loss: 91.6952, val_MinusLogProbMetric: 91.6952

Epoch 100: val_loss did not improve from 45.12576
196/196 - 61s - loss: 91.7960 - MinusLogProbMetric: 91.7960 - val_loss: 91.6952 - val_MinusLogProbMetric: 91.6952 - lr: 1.8519e-05 - 61s/epoch - 312ms/step
Epoch 101/1000
2023-09-28 21:34:52.648 
Epoch 101/1000 
	 loss: 91.5363, MinusLogProbMetric: 91.5363, val_loss: 91.4718, val_MinusLogProbMetric: 91.4718

Epoch 101: val_loss did not improve from 45.12576
196/196 - 60s - loss: 91.5363 - MinusLogProbMetric: 91.5363 - val_loss: 91.4718 - val_MinusLogProbMetric: 91.4718 - lr: 1.8519e-05 - 60s/epoch - 304ms/step
Epoch 102/1000
2023-09-28 21:35:53.484 
Epoch 102/1000 
	 loss: 91.4397, MinusLogProbMetric: 91.4397, val_loss: 91.3099, val_MinusLogProbMetric: 91.3099

Epoch 102: val_loss did not improve from 45.12576
196/196 - 61s - loss: 91.4397 - MinusLogProbMetric: 91.4397 - val_loss: 91.3099 - val_MinusLogProbMetric: 91.3099 - lr: 1.8519e-05 - 61s/epoch - 310ms/step
Epoch 103/1000
2023-09-28 21:36:55.389 
Epoch 103/1000 
	 loss: 91.2564, MinusLogProbMetric: 91.2564, val_loss: 91.1061, val_MinusLogProbMetric: 91.1061

Epoch 103: val_loss did not improve from 45.12576
196/196 - 62s - loss: 91.2564 - MinusLogProbMetric: 91.2564 - val_loss: 91.1061 - val_MinusLogProbMetric: 91.1061 - lr: 1.8519e-05 - 62s/epoch - 316ms/step
Epoch 104/1000
2023-09-28 21:37:56.125 
Epoch 104/1000 
	 loss: 91.0278, MinusLogProbMetric: 91.0278, val_loss: 91.2047, val_MinusLogProbMetric: 91.2047

Epoch 104: val_loss did not improve from 45.12576
196/196 - 61s - loss: 91.0278 - MinusLogProbMetric: 91.0278 - val_loss: 91.2047 - val_MinusLogProbMetric: 91.2047 - lr: 1.8519e-05 - 61s/epoch - 310ms/step
Epoch 105/1000
2023-09-28 21:38:56.184 
Epoch 105/1000 
	 loss: 90.9098, MinusLogProbMetric: 90.9098, val_loss: 90.8255, val_MinusLogProbMetric: 90.8255

Epoch 105: val_loss did not improve from 45.12576
196/196 - 60s - loss: 90.9098 - MinusLogProbMetric: 90.9098 - val_loss: 90.8255 - val_MinusLogProbMetric: 90.8255 - lr: 1.8519e-05 - 60s/epoch - 306ms/step
Epoch 106/1000
2023-09-28 21:39:59.783 
Epoch 106/1000 
	 loss: 91.0169, MinusLogProbMetric: 91.0169, val_loss: 90.7556, val_MinusLogProbMetric: 90.7556

Epoch 106: val_loss did not improve from 45.12576
Restoring model weights from the end of the best epoch: 6.
196/196 - 64s - loss: 91.0169 - MinusLogProbMetric: 91.0169 - val_loss: 90.7556 - val_MinusLogProbMetric: 90.7556 - lr: 1.8519e-05 - 64s/epoch - 327ms/step
Epoch 106: early stopping
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.

------------------------------------------
Starting LR metric calculation...
Running TF LR calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
WARNING:tensorflow:5 out of the last 5 calls to <function LRMetric.Test_tf.<locals>.compute_test at 0x7fb038499f30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
LR metric calculation completed in 30.6050295419991 seconds.

------------------------------------------
Starting KS tests calculation...
Running TF KS tests...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
WARNING:tensorflow:5 out of the last 5 calls to <function KSTest.Test_tf.<locals>.compute_test at 0x7fb03849b880> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
KS tests calculation completed in 15.939899300981779 seconds.

------------------------------------------
Starting SWD metric calculation...
Running TF SWD calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
WARNING:tensorflow:5 out of the last 5 calls to <function SWDMetric.Test_tf.<locals>.compute_test at 0x7fb0644d0310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
SWD metric calculation completed in 11.955034750979394 seconds.

------------------------------------------
Starting FN metric calculation...
Running TF FN calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
WARNING:tensorflow:5 out of the last 5 calls to <function FNMetric.Test_tf.<locals>.compute_test at 0x7fb2151340d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
FN metric calculation completed in 13.05120565200923 seconds.
Training succeeded with seed 0.
Model trained in 7269.12 s.

===========
Computing predictions
===========

Computing metrics...
Metrics computed in 73.13 s.
                ===========
                print("===========
Failed to plot
===========
")
                Exception type: ValueError
                Exception message: Number of rows must be a positive integer, not 0
                Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN.py , Line : 469, Func.Name : prediction_function, Message : Plotters.cornerplotter(X_data_test.numpy(),X_data_nf.numpy(),path_to_results,ndims,norm=True) # type: ignore', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/Plotters.py , Line : 156, Func.Name : cornerplotter, Message : figure=corner.corner(target_samples,color=\'red\',bins=n_bins,labels=[r"%s" % s for s in labels],normalize1d=True)', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/corner.py , Line : 235, Func.Name : corner, Message : fig, axes = pl.subplots(K, K, figsize=(dim, dim))', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/pyplot.py , Line : 1502, Func.Name : subplots, Message : axs = fig.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 905, Func.Name : subplots, Message : gs = self.add_gridspec(nrows, ncols, figure=self, **gridspec_kw)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 1527, Func.Name : add_gridspec, Message : gs = GridSpec(nrows=nrows, ncols=ncols, figure=self, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 379, Func.Name : __init__, Message : super().__init__(nrows, ncols,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 49, Func.Name : __init__, Message : raise ValueError(']
                ===========

results.txt saved
results.json saved
Results log saved
Model predictions computed in 73.41 s.
===========
Run 326/720 done in 11534.06 s.
===========

Directory ../../results/CsplineN_new/run_327/ already exists.
Skipping it.
===========
Run 327/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_328/ already exists.
Skipping it.
===========
Run 328/720 already exists. Skipping it.
===========

===========
Generating train data for run 329.
===========
Train data generated in 0.15 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_329/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_329/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_329/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_329
self.data_kwargs: {'seed': 187}
self.x_data: [[ 6.8153725   2.8430347   6.1778684  ...  2.16058     2.2525268
   1.7267085 ]
 [ 2.1836705   1.800204    9.035551   ...  6.5503054   1.5567943
   2.9656742 ]
 [ 0.87054014  3.1855702   7.8237348  ...  6.3044343  -0.7088827
   2.3190107 ]
 ...
 [ 0.35442734  3.573855   10.922302   ...  5.8448343   0.8037275
   2.171598  ]
 [ 1.4908957   3.8775673   8.757837   ...  5.7039022  -0.88000226
   2.3087878 ]
 [ 1.7291393   3.7802503   9.770119   ...  5.5898924   0.22995539
   4.1106014 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_253"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_254 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_23 (LogProbL  (None,)                  660960    
 ayer)                                                           
                                                                 
=================================================================
Total params: 660,960
Trainable params: 660,960
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_23/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_23'")
self.model: <keras.engine.functional.Functional object at 0x7fb054fc6e30>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fb1dff20100>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fb1dff20100>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fb1ee07d450>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fb3283abaf0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_329/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fb3283abfd0>, <keras.callbacks.ModelCheckpoint object at 0x7fb054f28160>, <keras.callbacks.EarlyStopping object at 0x7fb054f283d0>, <keras.callbacks.ReduceLROnPlateau object at 0x7fb054f28400>, <keras.callbacks.TerminateOnNaN object at 0x7fb054f28040>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_329/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 329/720 with hyperparameters:
timestamp = 2023-09-28 21:41:17.464720
ndims = 64
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 660960
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 6.8153725e+00  2.8430347e+00  6.1778684e+00  4.2452879e+00
  1.6284513e+00  3.3207846e+00  7.0928354e+00  6.4340367e+00
  6.0301833e+00  6.4328456e+00  6.5495849e+00  4.9321003e+00
  8.9532204e+00  3.4481030e+00  4.8284545e+00  8.0901794e+00
  8.1549883e+00  7.3900299e+00  1.3510420e+00  9.5149851e+00
  7.5267930e+00  1.0017762e+01  1.1636026e+00  8.6080103e+00
  2.2276068e+00  6.2524571e+00  1.5594521e+00  8.2755499e+00
  8.5135918e+00  6.3978677e+00  3.8566861e+00  6.8423492e-01
  7.5210962e+00  5.4144373e+00  6.8548636e+00  8.7448330e+00
  9.6529083e+00  8.6500130e+00  9.1009712e-01  4.1672401e+00
  7.6439004e+00  9.0885794e-01  5.2691178e+00  8.8166595e-03
  7.4780917e-01 -4.8575500e-01  7.9515800e+00  2.7509699e+00
  4.8898096e+00  1.0073138e+01  7.4751706e+00  6.5633142e-01
  1.5831217e+00  6.0002384e+00  5.6322212e+00  2.2999794e+00
  9.1524439e+00  5.5740438e+00  5.8525505e+00  6.6422234e+00
  6.9292612e+00  2.1605799e+00  2.2525268e+00  1.7267085e+00]
Epoch 1/1000
2023-09-28 21:42:51.432 
Epoch 1/1000 
	 loss: 710.9142, MinusLogProbMetric: 710.9142, val_loss: 193.5753, val_MinusLogProbMetric: 193.5753

Epoch 1: val_loss improved from inf to 193.57526, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_329/weights/best_weights.h5
196/196 - 94s - loss: 710.9142 - MinusLogProbMetric: 710.9142 - val_loss: 193.5753 - val_MinusLogProbMetric: 193.5753 - lr: 0.0010 - 94s/epoch - 481ms/step
Epoch 2/1000
2023-09-28 21:43:22.460 
Epoch 2/1000 
	 loss: 156.2729, MinusLogProbMetric: 156.2729, val_loss: 129.5305, val_MinusLogProbMetric: 129.5305

Epoch 2: val_loss improved from 193.57526 to 129.53049, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_329/weights/best_weights.h5
196/196 - 31s - loss: 156.2729 - MinusLogProbMetric: 156.2729 - val_loss: 129.5305 - val_MinusLogProbMetric: 129.5305 - lr: 0.0010 - 31s/epoch - 157ms/step
Epoch 3/1000
2023-09-28 21:43:53.138 
Epoch 3/1000 
	 loss: 128.2212, MinusLogProbMetric: 128.2212, val_loss: 103.1924, val_MinusLogProbMetric: 103.1924

Epoch 3: val_loss improved from 129.53049 to 103.19236, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_329/weights/best_weights.h5
196/196 - 31s - loss: 128.2212 - MinusLogProbMetric: 128.2212 - val_loss: 103.1924 - val_MinusLogProbMetric: 103.1924 - lr: 0.0010 - 31s/epoch - 157ms/step
Epoch 4/1000
2023-09-28 21:44:23.984 
Epoch 4/1000 
	 loss: 93.6867, MinusLogProbMetric: 93.6867, val_loss: 84.8612, val_MinusLogProbMetric: 84.8612

Epoch 4: val_loss improved from 103.19236 to 84.86118, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_329/weights/best_weights.h5
196/196 - 31s - loss: 93.6867 - MinusLogProbMetric: 93.6867 - val_loss: 84.8612 - val_MinusLogProbMetric: 84.8612 - lr: 0.0010 - 31s/epoch - 158ms/step
Epoch 5/1000
2023-09-28 21:44:55.191 
Epoch 5/1000 
	 loss: 79.1905, MinusLogProbMetric: 79.1905, val_loss: 73.8586, val_MinusLogProbMetric: 73.8586

Epoch 5: val_loss improved from 84.86118 to 73.85856, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_329/weights/best_weights.h5
196/196 - 31s - loss: 79.1905 - MinusLogProbMetric: 79.1905 - val_loss: 73.8586 - val_MinusLogProbMetric: 73.8586 - lr: 0.0010 - 31s/epoch - 159ms/step
Epoch 6/1000
2023-09-28 21:45:27.061 
Epoch 6/1000 
	 loss: 69.3643, MinusLogProbMetric: 69.3643, val_loss: 65.0152, val_MinusLogProbMetric: 65.0152

Epoch 6: val_loss improved from 73.85856 to 65.01523, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_329/weights/best_weights.h5
196/196 - 32s - loss: 69.3643 - MinusLogProbMetric: 69.3643 - val_loss: 65.0152 - val_MinusLogProbMetric: 65.0152 - lr: 0.0010 - 32s/epoch - 163ms/step
Epoch 7/1000
2023-09-28 21:46:00.770 
Epoch 7/1000 
	 loss: 61.9796, MinusLogProbMetric: 61.9796, val_loss: 60.4600, val_MinusLogProbMetric: 60.4600

Epoch 7: val_loss improved from 65.01523 to 60.45996, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_329/weights/best_weights.h5
196/196 - 34s - loss: 61.9796 - MinusLogProbMetric: 61.9796 - val_loss: 60.4600 - val_MinusLogProbMetric: 60.4600 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 8/1000
2023-09-28 21:46:37.594 
Epoch 8/1000 
	 loss: 56.8621, MinusLogProbMetric: 56.8621, val_loss: 54.3458, val_MinusLogProbMetric: 54.3458

Epoch 8: val_loss improved from 60.45996 to 54.34584, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_329/weights/best_weights.h5
196/196 - 37s - loss: 56.8621 - MinusLogProbMetric: 56.8621 - val_loss: 54.3458 - val_MinusLogProbMetric: 54.3458 - lr: 0.0010 - 37s/epoch - 188ms/step
Epoch 9/1000
2023-09-28 21:47:11.010 
Epoch 9/1000 
	 loss: 52.7535, MinusLogProbMetric: 52.7535, val_loss: 52.3283, val_MinusLogProbMetric: 52.3283

Epoch 9: val_loss improved from 54.34584 to 52.32830, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_329/weights/best_weights.h5
196/196 - 33s - loss: 52.7535 - MinusLogProbMetric: 52.7535 - val_loss: 52.3283 - val_MinusLogProbMetric: 52.3283 - lr: 0.0010 - 33s/epoch - 169ms/step
Epoch 10/1000
2023-09-28 21:47:42.183 
Epoch 10/1000 
	 loss: 49.5855, MinusLogProbMetric: 49.5855, val_loss: 48.8313, val_MinusLogProbMetric: 48.8313

Epoch 10: val_loss improved from 52.32830 to 48.83130, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_329/weights/best_weights.h5
196/196 - 31s - loss: 49.5855 - MinusLogProbMetric: 49.5855 - val_loss: 48.8313 - val_MinusLogProbMetric: 48.8313 - lr: 0.0010 - 31s/epoch - 159ms/step
Epoch 11/1000
2023-09-28 21:48:13.242 
Epoch 11/1000 
	 loss: 47.2049, MinusLogProbMetric: 47.2049, val_loss: 45.9834, val_MinusLogProbMetric: 45.9834

Epoch 11: val_loss improved from 48.83130 to 45.98343, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_329/weights/best_weights.h5
196/196 - 31s - loss: 47.2049 - MinusLogProbMetric: 47.2049 - val_loss: 45.9834 - val_MinusLogProbMetric: 45.9834 - lr: 0.0010 - 31s/epoch - 158ms/step
Epoch 12/1000
2023-09-28 21:48:44.682 
Epoch 12/1000 
	 loss: 45.2447, MinusLogProbMetric: 45.2447, val_loss: 44.9002, val_MinusLogProbMetric: 44.9002

Epoch 12: val_loss improved from 45.98343 to 44.90020, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_329/weights/best_weights.h5
196/196 - 31s - loss: 45.2447 - MinusLogProbMetric: 45.2447 - val_loss: 44.9002 - val_MinusLogProbMetric: 44.9002 - lr: 0.0010 - 31s/epoch - 160ms/step
Epoch 13/1000
2023-09-28 21:49:18.907 
Epoch 13/1000 
	 loss: 43.7105, MinusLogProbMetric: 43.7105, val_loss: 42.8031, val_MinusLogProbMetric: 42.8031

Epoch 13: val_loss improved from 44.90020 to 42.80307, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_329/weights/best_weights.h5
196/196 - 34s - loss: 43.7105 - MinusLogProbMetric: 43.7105 - val_loss: 42.8031 - val_MinusLogProbMetric: 42.8031 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 14/1000
2023-09-28 21:49:53.630 
Epoch 14/1000 
	 loss: 42.4338, MinusLogProbMetric: 42.4338, val_loss: 43.3143, val_MinusLogProbMetric: 43.3143

Epoch 14: val_loss did not improve from 42.80307
196/196 - 34s - loss: 42.4338 - MinusLogProbMetric: 42.4338 - val_loss: 43.3143 - val_MinusLogProbMetric: 43.3143 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 15/1000
2023-09-28 21:50:28.569 
Epoch 15/1000 
	 loss: 42.0825, MinusLogProbMetric: 42.0825, val_loss: 41.6755, val_MinusLogProbMetric: 41.6755

Epoch 15: val_loss improved from 42.80307 to 41.67547, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_329/weights/best_weights.h5
196/196 - 35s - loss: 42.0825 - MinusLogProbMetric: 42.0825 - val_loss: 41.6755 - val_MinusLogProbMetric: 41.6755 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 16/1000
2023-09-28 21:51:02.732 
Epoch 16/1000 
	 loss: 40.4449, MinusLogProbMetric: 40.4449, val_loss: 40.9164, val_MinusLogProbMetric: 40.9164

Epoch 16: val_loss improved from 41.67547 to 40.91640, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_329/weights/best_weights.h5
196/196 - 34s - loss: 40.4449 - MinusLogProbMetric: 40.4449 - val_loss: 40.9164 - val_MinusLogProbMetric: 40.9164 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 17/1000
2023-09-28 21:51:37.799 
Epoch 17/1000 
	 loss: 39.5332, MinusLogProbMetric: 39.5332, val_loss: 39.2562, val_MinusLogProbMetric: 39.2562

Epoch 17: val_loss improved from 40.91640 to 39.25617, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_329/weights/best_weights.h5
196/196 - 35s - loss: 39.5332 - MinusLogProbMetric: 39.5332 - val_loss: 39.2562 - val_MinusLogProbMetric: 39.2562 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 18/1000
2023-09-28 21:52:15.264 
Epoch 18/1000 
	 loss: 39.0536, MinusLogProbMetric: 39.0536, val_loss: 38.4445, val_MinusLogProbMetric: 38.4445

Epoch 18: val_loss improved from 39.25617 to 38.44455, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_329/weights/best_weights.h5
196/196 - 37s - loss: 39.0536 - MinusLogProbMetric: 39.0536 - val_loss: 38.4445 - val_MinusLogProbMetric: 38.4445 - lr: 0.0010 - 37s/epoch - 191ms/step
Epoch 19/1000
2023-09-28 21:52:47.802 
Epoch 19/1000 
	 loss: 38.3037, MinusLogProbMetric: 38.3037, val_loss: 38.4662, val_MinusLogProbMetric: 38.4662

Epoch 19: val_loss did not improve from 38.44455
196/196 - 32s - loss: 38.3037 - MinusLogProbMetric: 38.3037 - val_loss: 38.4662 - val_MinusLogProbMetric: 38.4662 - lr: 0.0010 - 32s/epoch - 163ms/step
Epoch 20/1000
2023-09-28 21:53:18.433 
Epoch 20/1000 
	 loss: 38.3533, MinusLogProbMetric: 38.3533, val_loss: 37.6548, val_MinusLogProbMetric: 37.6548

Epoch 20: val_loss improved from 38.44455 to 37.65482, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_329/weights/best_weights.h5
196/196 - 31s - loss: 38.3533 - MinusLogProbMetric: 38.3533 - val_loss: 37.6548 - val_MinusLogProbMetric: 37.6548 - lr: 0.0010 - 31s/epoch - 159ms/step
Epoch 21/1000
2023-09-28 21:53:49.504 
Epoch 21/1000 
	 loss: 37.2552, MinusLogProbMetric: 37.2552, val_loss: 38.2794, val_MinusLogProbMetric: 38.2794

Epoch 21: val_loss did not improve from 37.65482
196/196 - 31s - loss: 37.2552 - MinusLogProbMetric: 37.2552 - val_loss: 38.2794 - val_MinusLogProbMetric: 38.2794 - lr: 0.0010 - 31s/epoch - 156ms/step
Epoch 22/1000
2023-09-28 21:54:19.679 
Epoch 22/1000 
	 loss: 36.9114, MinusLogProbMetric: 36.9114, val_loss: 37.2309, val_MinusLogProbMetric: 37.2309

Epoch 22: val_loss improved from 37.65482 to 37.23088, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_329/weights/best_weights.h5
196/196 - 31s - loss: 36.9114 - MinusLogProbMetric: 36.9114 - val_loss: 37.2309 - val_MinusLogProbMetric: 37.2309 - lr: 0.0010 - 31s/epoch - 156ms/step
Epoch 23/1000
2023-09-28 21:54:51.262 
Epoch 23/1000 
	 loss: 36.5345, MinusLogProbMetric: 36.5345, val_loss: 36.1926, val_MinusLogProbMetric: 36.1926

Epoch 23: val_loss improved from 37.23088 to 36.19262, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_329/weights/best_weights.h5
196/196 - 32s - loss: 36.5345 - MinusLogProbMetric: 36.5345 - val_loss: 36.1926 - val_MinusLogProbMetric: 36.1926 - lr: 0.0010 - 32s/epoch - 161ms/step
Epoch 24/1000
2023-09-28 21:55:25.488 
Epoch 24/1000 
	 loss: 36.3893, MinusLogProbMetric: 36.3893, val_loss: 36.1415, val_MinusLogProbMetric: 36.1415

Epoch 24: val_loss improved from 36.19262 to 36.14149, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_329/weights/best_weights.h5
196/196 - 34s - loss: 36.3893 - MinusLogProbMetric: 36.3893 - val_loss: 36.1415 - val_MinusLogProbMetric: 36.1415 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 25/1000
2023-09-28 21:56:02.212 
Epoch 25/1000 
	 loss: 35.9666, MinusLogProbMetric: 35.9666, val_loss: 35.7778, val_MinusLogProbMetric: 35.7778

Epoch 25: val_loss improved from 36.14149 to 35.77776, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_329/weights/best_weights.h5
196/196 - 37s - loss: 35.9666 - MinusLogProbMetric: 35.9666 - val_loss: 35.7778 - val_MinusLogProbMetric: 35.7778 - lr: 0.0010 - 37s/epoch - 187ms/step
Epoch 26/1000
2023-09-28 21:56:37.022 
Epoch 26/1000 
	 loss: 35.5908, MinusLogProbMetric: 35.5908, val_loss: 36.2544, val_MinusLogProbMetric: 36.2544

Epoch 26: val_loss did not improve from 35.77776
196/196 - 34s - loss: 35.5908 - MinusLogProbMetric: 35.5908 - val_loss: 36.2544 - val_MinusLogProbMetric: 36.2544 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 27/1000
2023-09-28 21:57:12.180 
Epoch 27/1000 
	 loss: 35.4721, MinusLogProbMetric: 35.4721, val_loss: 34.7225, val_MinusLogProbMetric: 34.7225

Epoch 27: val_loss improved from 35.77776 to 34.72252, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_329/weights/best_weights.h5
196/196 - 36s - loss: 35.4721 - MinusLogProbMetric: 35.4721 - val_loss: 34.7225 - val_MinusLogProbMetric: 34.7225 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 28/1000
2023-09-28 21:57:48.455 
Epoch 28/1000 
	 loss: 35.2232, MinusLogProbMetric: 35.2232, val_loss: 38.2870, val_MinusLogProbMetric: 38.2870

Epoch 28: val_loss did not improve from 34.72252
196/196 - 36s - loss: 35.2232 - MinusLogProbMetric: 35.2232 - val_loss: 38.2870 - val_MinusLogProbMetric: 38.2870 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 29/1000
2023-09-28 21:58:26.268 
Epoch 29/1000 
	 loss: 35.1204, MinusLogProbMetric: 35.1204, val_loss: 34.9974, val_MinusLogProbMetric: 34.9974

Epoch 29: val_loss did not improve from 34.72252
196/196 - 38s - loss: 35.1204 - MinusLogProbMetric: 35.1204 - val_loss: 34.9974 - val_MinusLogProbMetric: 34.9974 - lr: 0.0010 - 38s/epoch - 193ms/step
Epoch 30/1000
2023-09-28 21:58:59.256 
Epoch 30/1000 
	 loss: 34.8553, MinusLogProbMetric: 34.8553, val_loss: 34.1185, val_MinusLogProbMetric: 34.1185

Epoch 30: val_loss improved from 34.72252 to 34.11848, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_329/weights/best_weights.h5
196/196 - 34s - loss: 34.8553 - MinusLogProbMetric: 34.8553 - val_loss: 34.1185 - val_MinusLogProbMetric: 34.1185 - lr: 0.0010 - 34s/epoch - 171ms/step
Epoch 31/1000
2023-09-28 21:59:34.423 
Epoch 31/1000 
	 loss: 34.5035, MinusLogProbMetric: 34.5035, val_loss: 34.2190, val_MinusLogProbMetric: 34.2190

Epoch 31: val_loss did not improve from 34.11848
196/196 - 35s - loss: 34.5035 - MinusLogProbMetric: 34.5035 - val_loss: 34.2190 - val_MinusLogProbMetric: 34.2190 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 32/1000
2023-09-28 22:00:09.784 
Epoch 32/1000 
	 loss: 34.1948, MinusLogProbMetric: 34.1948, val_loss: 33.9392, val_MinusLogProbMetric: 33.9392

Epoch 32: val_loss improved from 34.11848 to 33.93919, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_329/weights/best_weights.h5
196/196 - 36s - loss: 34.1948 - MinusLogProbMetric: 34.1948 - val_loss: 33.9392 - val_MinusLogProbMetric: 33.9392 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 33/1000
2023-09-28 22:00:43.798 
Epoch 33/1000 
	 loss: 34.0993, MinusLogProbMetric: 34.0993, val_loss: 35.4282, val_MinusLogProbMetric: 35.4282

Epoch 33: val_loss did not improve from 33.93919
196/196 - 33s - loss: 34.0993 - MinusLogProbMetric: 34.0993 - val_loss: 35.4282 - val_MinusLogProbMetric: 35.4282 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 34/1000
2023-09-28 22:01:21.086 
Epoch 34/1000 
	 loss: 34.0466, MinusLogProbMetric: 34.0466, val_loss: 33.1205, val_MinusLogProbMetric: 33.1205

Epoch 34: val_loss improved from 33.93919 to 33.12053, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_329/weights/best_weights.h5
196/196 - 38s - loss: 34.0466 - MinusLogProbMetric: 34.0466 - val_loss: 33.1205 - val_MinusLogProbMetric: 33.1205 - lr: 0.0010 - 38s/epoch - 193ms/step
Epoch 35/1000
2023-09-28 22:01:55.251 
Epoch 35/1000 
	 loss: 33.8141, MinusLogProbMetric: 33.8141, val_loss: 33.6175, val_MinusLogProbMetric: 33.6175

Epoch 35: val_loss did not improve from 33.12053
196/196 - 34s - loss: 33.8141 - MinusLogProbMetric: 33.8141 - val_loss: 33.6175 - val_MinusLogProbMetric: 33.6175 - lr: 0.0010 - 34s/epoch - 171ms/step
Epoch 36/1000
2023-09-28 22:02:31.014 
Epoch 36/1000 
	 loss: 33.9376, MinusLogProbMetric: 33.9376, val_loss: 37.1271, val_MinusLogProbMetric: 37.1271

Epoch 36: val_loss did not improve from 33.12053
196/196 - 36s - loss: 33.9376 - MinusLogProbMetric: 33.9376 - val_loss: 37.1271 - val_MinusLogProbMetric: 37.1271 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 37/1000
2023-09-28 22:03:08.661 
Epoch 37/1000 
	 loss: 33.4786, MinusLogProbMetric: 33.4786, val_loss: 35.6977, val_MinusLogProbMetric: 35.6977

Epoch 37: val_loss did not improve from 33.12053
196/196 - 38s - loss: 33.4786 - MinusLogProbMetric: 33.4786 - val_loss: 35.6977 - val_MinusLogProbMetric: 35.6977 - lr: 0.0010 - 38s/epoch - 192ms/step
Epoch 38/1000
2023-09-28 22:03:44.751 
Epoch 38/1000 
	 loss: 33.5103, MinusLogProbMetric: 33.5103, val_loss: 33.2354, val_MinusLogProbMetric: 33.2354

Epoch 38: val_loss did not improve from 33.12053
196/196 - 36s - loss: 33.5103 - MinusLogProbMetric: 33.5103 - val_loss: 33.2354 - val_MinusLogProbMetric: 33.2354 - lr: 0.0010 - 36s/epoch - 184ms/step
Epoch 39/1000
2023-09-28 22:04:19.491 
Epoch 39/1000 
	 loss: 33.2538, MinusLogProbMetric: 33.2538, val_loss: 33.7518, val_MinusLogProbMetric: 33.7518

Epoch 39: val_loss did not improve from 33.12053
196/196 - 35s - loss: 33.2538 - MinusLogProbMetric: 33.2538 - val_loss: 33.7518 - val_MinusLogProbMetric: 33.7518 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 40/1000
2023-09-28 22:04:56.179 
Epoch 40/1000 
	 loss: 33.1241, MinusLogProbMetric: 33.1241, val_loss: 32.8841, val_MinusLogProbMetric: 32.8841

Epoch 40: val_loss improved from 33.12053 to 32.88414, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_329/weights/best_weights.h5
196/196 - 37s - loss: 33.1241 - MinusLogProbMetric: 33.1241 - val_loss: 32.8841 - val_MinusLogProbMetric: 32.8841 - lr: 0.0010 - 37s/epoch - 190ms/step
Epoch 41/1000
2023-09-28 22:05:32.818 
Epoch 41/1000 
	 loss: 32.9803, MinusLogProbMetric: 32.9803, val_loss: 33.1389, val_MinusLogProbMetric: 33.1389

Epoch 41: val_loss did not improve from 32.88414
196/196 - 36s - loss: 32.9803 - MinusLogProbMetric: 32.9803 - val_loss: 33.1389 - val_MinusLogProbMetric: 33.1389 - lr: 0.0010 - 36s/epoch - 184ms/step
Epoch 42/1000
2023-09-28 22:06:09.268 
Epoch 42/1000 
	 loss: 33.0608, MinusLogProbMetric: 33.0608, val_loss: 33.0354, val_MinusLogProbMetric: 33.0354

Epoch 42: val_loss did not improve from 32.88414
196/196 - 36s - loss: 33.0608 - MinusLogProbMetric: 33.0608 - val_loss: 33.0354 - val_MinusLogProbMetric: 33.0354 - lr: 0.0010 - 36s/epoch - 186ms/step
Epoch 43/1000
2023-09-28 22:06:44.993 
Epoch 43/1000 
	 loss: 32.8191, MinusLogProbMetric: 32.8191, val_loss: 32.4544, val_MinusLogProbMetric: 32.4544

Epoch 43: val_loss improved from 32.88414 to 32.45443, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_329/weights/best_weights.h5
196/196 - 36s - loss: 32.8191 - MinusLogProbMetric: 32.8191 - val_loss: 32.4544 - val_MinusLogProbMetric: 32.4544 - lr: 0.0010 - 36s/epoch - 185ms/step
Epoch 44/1000
2023-09-28 22:07:20.732 
Epoch 44/1000 
	 loss: 32.6803, MinusLogProbMetric: 32.6803, val_loss: 32.3055, val_MinusLogProbMetric: 32.3055

Epoch 44: val_loss improved from 32.45443 to 32.30551, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_329/weights/best_weights.h5
196/196 - 36s - loss: 32.6803 - MinusLogProbMetric: 32.6803 - val_loss: 32.3055 - val_MinusLogProbMetric: 32.3055 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 45/1000
2023-09-28 22:07:56.434 
Epoch 45/1000 
	 loss: 32.6466, MinusLogProbMetric: 32.6466, val_loss: 32.3879, val_MinusLogProbMetric: 32.3879

Epoch 45: val_loss did not improve from 32.30551
196/196 - 35s - loss: 32.6466 - MinusLogProbMetric: 32.6466 - val_loss: 32.3879 - val_MinusLogProbMetric: 32.3879 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 46/1000
2023-09-28 22:08:31.540 
Epoch 46/1000 
	 loss: 32.6453, MinusLogProbMetric: 32.6453, val_loss: 32.8228, val_MinusLogProbMetric: 32.8228

Epoch 46: val_loss did not improve from 32.30551
196/196 - 35s - loss: 32.6453 - MinusLogProbMetric: 32.6453 - val_loss: 32.8228 - val_MinusLogProbMetric: 32.8228 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 47/1000
2023-09-28 22:09:06.462 
Epoch 47/1000 
	 loss: 32.5782, MinusLogProbMetric: 32.5782, val_loss: 32.4448, val_MinusLogProbMetric: 32.4448

Epoch 47: val_loss did not improve from 32.30551
196/196 - 35s - loss: 32.5782 - MinusLogProbMetric: 32.5782 - val_loss: 32.4448 - val_MinusLogProbMetric: 32.4448 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 48/1000
2023-09-28 22:09:40.454 
Epoch 48/1000 
	 loss: 32.2957, MinusLogProbMetric: 32.2957, val_loss: 32.6848, val_MinusLogProbMetric: 32.6848

Epoch 48: val_loss did not improve from 32.30551
196/196 - 34s - loss: 32.2957 - MinusLogProbMetric: 32.2957 - val_loss: 32.6848 - val_MinusLogProbMetric: 32.6848 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 49/1000
2023-09-28 22:10:15.287 
Epoch 49/1000 
	 loss: 32.4181, MinusLogProbMetric: 32.4181, val_loss: 31.7795, val_MinusLogProbMetric: 31.7795

Epoch 49: val_loss improved from 32.30551 to 31.77952, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_329/weights/best_weights.h5
196/196 - 35s - loss: 32.4181 - MinusLogProbMetric: 32.4181 - val_loss: 31.7795 - val_MinusLogProbMetric: 31.7795 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 50/1000
2023-09-28 22:10:49.159 
Epoch 50/1000 
	 loss: 32.2821, MinusLogProbMetric: 32.2821, val_loss: 32.1052, val_MinusLogProbMetric: 32.1052

Epoch 50: val_loss did not improve from 31.77952
196/196 - 33s - loss: 32.2821 - MinusLogProbMetric: 32.2821 - val_loss: 32.1052 - val_MinusLogProbMetric: 32.1052 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 51/1000
2023-09-28 22:11:19.091 
Epoch 51/1000 
	 loss: 32.2807, MinusLogProbMetric: 32.2807, val_loss: 32.1503, val_MinusLogProbMetric: 32.1503

Epoch 51: val_loss did not improve from 31.77952
196/196 - 30s - loss: 32.2807 - MinusLogProbMetric: 32.2807 - val_loss: 32.1503 - val_MinusLogProbMetric: 32.1503 - lr: 0.0010 - 30s/epoch - 153ms/step
Epoch 52/1000
2023-09-28 22:11:49.105 
Epoch 52/1000 
	 loss: 32.2672, MinusLogProbMetric: 32.2672, val_loss: 32.2075, val_MinusLogProbMetric: 32.2075

Epoch 52: val_loss did not improve from 31.77952
196/196 - 30s - loss: 32.2672 - MinusLogProbMetric: 32.2672 - val_loss: 32.2075 - val_MinusLogProbMetric: 32.2075 - lr: 0.0010 - 30s/epoch - 153ms/step
Epoch 53/1000
2023-09-28 22:12:19.400 
Epoch 53/1000 
	 loss: 32.0305, MinusLogProbMetric: 32.0305, val_loss: 33.0855, val_MinusLogProbMetric: 33.0855

Epoch 53: val_loss did not improve from 31.77952
196/196 - 30s - loss: 32.0305 - MinusLogProbMetric: 32.0305 - val_loss: 33.0855 - val_MinusLogProbMetric: 33.0855 - lr: 0.0010 - 30s/epoch - 155ms/step
Epoch 54/1000
2023-09-28 22:12:51.230 
Epoch 54/1000 
	 loss: 32.1280, MinusLogProbMetric: 32.1280, val_loss: 31.6849, val_MinusLogProbMetric: 31.6849

Epoch 54: val_loss improved from 31.77952 to 31.68489, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_329/weights/best_weights.h5
196/196 - 32s - loss: 32.1280 - MinusLogProbMetric: 32.1280 - val_loss: 31.6849 - val_MinusLogProbMetric: 31.6849 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 55/1000
2023-09-28 22:13:25.074 
Epoch 55/1000 
	 loss: 31.7768, MinusLogProbMetric: 31.7768, val_loss: 31.4379, val_MinusLogProbMetric: 31.4379

Epoch 55: val_loss improved from 31.68489 to 31.43791, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_329/weights/best_weights.h5
196/196 - 34s - loss: 31.7768 - MinusLogProbMetric: 31.7768 - val_loss: 31.4379 - val_MinusLogProbMetric: 31.4379 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 56/1000
2023-09-28 22:14:00.823 
Epoch 56/1000 
	 loss: 31.7724, MinusLogProbMetric: 31.7724, val_loss: 35.6217, val_MinusLogProbMetric: 35.6217

Epoch 56: val_loss did not improve from 31.43791
196/196 - 35s - loss: 31.7724 - MinusLogProbMetric: 31.7724 - val_loss: 35.6217 - val_MinusLogProbMetric: 35.6217 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 57/1000
2023-09-28 22:14:35.917 
Epoch 57/1000 
	 loss: 31.8015, MinusLogProbMetric: 31.8015, val_loss: 31.9932, val_MinusLogProbMetric: 31.9932

Epoch 57: val_loss did not improve from 31.43791
196/196 - 35s - loss: 31.8015 - MinusLogProbMetric: 31.8015 - val_loss: 31.9932 - val_MinusLogProbMetric: 31.9932 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 58/1000
2023-09-28 22:15:09.557 
Epoch 58/1000 
	 loss: 31.7304, MinusLogProbMetric: 31.7304, val_loss: 32.4567, val_MinusLogProbMetric: 32.4567

Epoch 58: val_loss did not improve from 31.43791
196/196 - 34s - loss: 31.7304 - MinusLogProbMetric: 31.7304 - val_loss: 32.4567 - val_MinusLogProbMetric: 32.4567 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 59/1000
2023-09-28 22:15:44.898 
Epoch 59/1000 
	 loss: 31.6944, MinusLogProbMetric: 31.6944, val_loss: 32.0703, val_MinusLogProbMetric: 32.0703

Epoch 59: val_loss did not improve from 31.43791
196/196 - 35s - loss: 31.6944 - MinusLogProbMetric: 31.6944 - val_loss: 32.0703 - val_MinusLogProbMetric: 32.0703 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 60/1000
2023-09-28 22:16:19.744 
Epoch 60/1000 
	 loss: 31.6522, MinusLogProbMetric: 31.6522, val_loss: 31.9703, val_MinusLogProbMetric: 31.9703

Epoch 60: val_loss did not improve from 31.43791
196/196 - 35s - loss: 31.6522 - MinusLogProbMetric: 31.6522 - val_loss: 31.9703 - val_MinusLogProbMetric: 31.9703 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 61/1000
2023-09-28 22:16:54.922 
Epoch 61/1000 
	 loss: 31.5248, MinusLogProbMetric: 31.5248, val_loss: 31.4302, val_MinusLogProbMetric: 31.4302

Epoch 61: val_loss improved from 31.43791 to 31.43018, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_329/weights/best_weights.h5
196/196 - 36s - loss: 31.5248 - MinusLogProbMetric: 31.5248 - val_loss: 31.4302 - val_MinusLogProbMetric: 31.4302 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 62/1000
2023-09-28 22:17:31.384 
Epoch 62/1000 
	 loss: 31.6420, MinusLogProbMetric: 31.6420, val_loss: 31.0788, val_MinusLogProbMetric: 31.0788

Epoch 62: val_loss improved from 31.43018 to 31.07881, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_329/weights/best_weights.h5
196/196 - 36s - loss: 31.6420 - MinusLogProbMetric: 31.6420 - val_loss: 31.0788 - val_MinusLogProbMetric: 31.0788 - lr: 0.0010 - 36s/epoch - 185ms/step
Epoch 63/1000
2023-09-28 22:18:06.982 
Epoch 63/1000 
	 loss: 31.5960, MinusLogProbMetric: 31.5960, val_loss: 31.1093, val_MinusLogProbMetric: 31.1093

Epoch 63: val_loss did not improve from 31.07881
196/196 - 35s - loss: 31.5960 - MinusLogProbMetric: 31.5960 - val_loss: 31.1093 - val_MinusLogProbMetric: 31.1093 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 64/1000
2023-09-28 22:18:39.230 
Epoch 64/1000 
	 loss: 31.4476, MinusLogProbMetric: 31.4476, val_loss: 31.6198, val_MinusLogProbMetric: 31.6198

Epoch 64: val_loss did not improve from 31.07881
196/196 - 32s - loss: 31.4476 - MinusLogProbMetric: 31.4476 - val_loss: 31.6198 - val_MinusLogProbMetric: 31.6198 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 65/1000
2023-09-28 22:19:13.445 
Epoch 65/1000 
	 loss: 31.3696, MinusLogProbMetric: 31.3696, val_loss: 32.7050, val_MinusLogProbMetric: 32.7050

Epoch 65: val_loss did not improve from 31.07881
196/196 - 34s - loss: 31.3696 - MinusLogProbMetric: 31.3696 - val_loss: 32.7050 - val_MinusLogProbMetric: 32.7050 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 66/1000
2023-09-28 22:19:48.207 
Epoch 66/1000 
	 loss: 31.4408, MinusLogProbMetric: 31.4408, val_loss: 31.7085, val_MinusLogProbMetric: 31.7085

Epoch 66: val_loss did not improve from 31.07881
196/196 - 35s - loss: 31.4408 - MinusLogProbMetric: 31.4408 - val_loss: 31.7085 - val_MinusLogProbMetric: 31.7085 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 67/1000
2023-09-28 22:20:23.370 
Epoch 67/1000 
	 loss: 31.3429, MinusLogProbMetric: 31.3429, val_loss: 32.2017, val_MinusLogProbMetric: 32.2017

Epoch 67: val_loss did not improve from 31.07881
196/196 - 35s - loss: 31.3429 - MinusLogProbMetric: 31.3429 - val_loss: 32.2017 - val_MinusLogProbMetric: 32.2017 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 68/1000
2023-09-28 22:20:58.258 
Epoch 68/1000 
	 loss: 31.2510, MinusLogProbMetric: 31.2510, val_loss: 31.6367, val_MinusLogProbMetric: 31.6367

Epoch 68: val_loss did not improve from 31.07881
196/196 - 35s - loss: 31.2510 - MinusLogProbMetric: 31.2510 - val_loss: 31.6367 - val_MinusLogProbMetric: 31.6367 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 69/1000
2023-09-28 22:21:33.667 
Epoch 69/1000 
	 loss: 31.3601, MinusLogProbMetric: 31.3601, val_loss: 31.2416, val_MinusLogProbMetric: 31.2416

Epoch 69: val_loss did not improve from 31.07881
196/196 - 35s - loss: 31.3601 - MinusLogProbMetric: 31.3601 - val_loss: 31.2416 - val_MinusLogProbMetric: 31.2416 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 70/1000
2023-09-28 22:22:08.982 
Epoch 70/1000 
	 loss: 31.1813, MinusLogProbMetric: 31.1813, val_loss: 31.5846, val_MinusLogProbMetric: 31.5846

Epoch 70: val_loss did not improve from 31.07881
196/196 - 35s - loss: 31.1813 - MinusLogProbMetric: 31.1813 - val_loss: 31.5846 - val_MinusLogProbMetric: 31.5846 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 71/1000
2023-09-28 22:22:44.893 
Epoch 71/1000 
	 loss: 31.2039, MinusLogProbMetric: 31.2039, val_loss: 30.9873, val_MinusLogProbMetric: 30.9873

Epoch 71: val_loss improved from 31.07881 to 30.98729, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_329/weights/best_weights.h5
196/196 - 37s - loss: 31.2039 - MinusLogProbMetric: 31.2039 - val_loss: 30.9873 - val_MinusLogProbMetric: 30.9873 - lr: 0.0010 - 37s/epoch - 186ms/step
Epoch 72/1000
2023-09-28 22:23:20.435 
Epoch 72/1000 
	 loss: 31.1585, MinusLogProbMetric: 31.1585, val_loss: 31.7068, val_MinusLogProbMetric: 31.7068

Epoch 72: val_loss did not improve from 30.98729
196/196 - 35s - loss: 31.1585 - MinusLogProbMetric: 31.1585 - val_loss: 31.7068 - val_MinusLogProbMetric: 31.7068 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 73/1000
2023-09-28 22:23:55.741 
Epoch 73/1000 
	 loss: 31.0472, MinusLogProbMetric: 31.0472, val_loss: 31.5978, val_MinusLogProbMetric: 31.5978

Epoch 73: val_loss did not improve from 30.98729
196/196 - 35s - loss: 31.0472 - MinusLogProbMetric: 31.0472 - val_loss: 31.5978 - val_MinusLogProbMetric: 31.5978 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 74/1000
2023-09-28 22:24:31.106 
Epoch 74/1000 
	 loss: 31.2053, MinusLogProbMetric: 31.2053, val_loss: 31.2998, val_MinusLogProbMetric: 31.2998

Epoch 74: val_loss did not improve from 30.98729
196/196 - 35s - loss: 31.2053 - MinusLogProbMetric: 31.2053 - val_loss: 31.2998 - val_MinusLogProbMetric: 31.2998 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 75/1000
2023-09-28 22:25:05.692 
Epoch 75/1000 
	 loss: 30.8614, MinusLogProbMetric: 30.8614, val_loss: 31.9713, val_MinusLogProbMetric: 31.9713

Epoch 75: val_loss did not improve from 30.98729
196/196 - 35s - loss: 30.8614 - MinusLogProbMetric: 30.8614 - val_loss: 31.9713 - val_MinusLogProbMetric: 31.9713 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 76/1000
2023-09-28 22:25:40.867 
Epoch 76/1000 
	 loss: 30.8324, MinusLogProbMetric: 30.8324, val_loss: 31.1474, val_MinusLogProbMetric: 31.1474

Epoch 76: val_loss did not improve from 30.98729
196/196 - 35s - loss: 30.8324 - MinusLogProbMetric: 30.8324 - val_loss: 31.1474 - val_MinusLogProbMetric: 31.1474 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 77/1000
2023-09-28 22:26:15.955 
Epoch 77/1000 
	 loss: 30.6297, MinusLogProbMetric: 30.6297, val_loss: 31.7445, val_MinusLogProbMetric: 31.7445

Epoch 77: val_loss did not improve from 30.98729
196/196 - 35s - loss: 30.6297 - MinusLogProbMetric: 30.6297 - val_loss: 31.7445 - val_MinusLogProbMetric: 31.7445 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 78/1000
2023-09-28 22:26:51.138 
Epoch 78/1000 
	 loss: 30.7450, MinusLogProbMetric: 30.7450, val_loss: 30.3816, val_MinusLogProbMetric: 30.3816

Epoch 78: val_loss improved from 30.98729 to 30.38157, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_329/weights/best_weights.h5
196/196 - 36s - loss: 30.7450 - MinusLogProbMetric: 30.7450 - val_loss: 30.3816 - val_MinusLogProbMetric: 30.3816 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 79/1000
2023-09-28 22:27:26.835 
Epoch 79/1000 
	 loss: 30.6129, MinusLogProbMetric: 30.6129, val_loss: 31.0047, val_MinusLogProbMetric: 31.0047

Epoch 79: val_loss did not improve from 30.38157
196/196 - 35s - loss: 30.6129 - MinusLogProbMetric: 30.6129 - val_loss: 31.0047 - val_MinusLogProbMetric: 31.0047 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 80/1000
2023-09-28 22:28:01.747 
Epoch 80/1000 
	 loss: 30.5667, MinusLogProbMetric: 30.5667, val_loss: 30.9971, val_MinusLogProbMetric: 30.9971

Epoch 80: val_loss did not improve from 30.38157
196/196 - 35s - loss: 30.5667 - MinusLogProbMetric: 30.5667 - val_loss: 30.9971 - val_MinusLogProbMetric: 30.9971 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 81/1000
2023-09-28 22:28:36.610 
Epoch 81/1000 
	 loss: 30.4724, MinusLogProbMetric: 30.4724, val_loss: 30.7949, val_MinusLogProbMetric: 30.7949

Epoch 81: val_loss did not improve from 30.38157
196/196 - 35s - loss: 30.4724 - MinusLogProbMetric: 30.4724 - val_loss: 30.7949 - val_MinusLogProbMetric: 30.7949 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 82/1000
2023-09-28 22:29:11.682 
Epoch 82/1000 
	 loss: 30.4633, MinusLogProbMetric: 30.4633, val_loss: 31.5343, val_MinusLogProbMetric: 31.5343

Epoch 82: val_loss did not improve from 30.38157
196/196 - 35s - loss: 30.4633 - MinusLogProbMetric: 30.4633 - val_loss: 31.5343 - val_MinusLogProbMetric: 31.5343 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 83/1000
2023-09-28 22:29:46.812 
Epoch 83/1000 
	 loss: 30.4254, MinusLogProbMetric: 30.4254, val_loss: 30.6738, val_MinusLogProbMetric: 30.6738

Epoch 83: val_loss did not improve from 30.38157
196/196 - 35s - loss: 30.4254 - MinusLogProbMetric: 30.4254 - val_loss: 30.6738 - val_MinusLogProbMetric: 30.6738 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 84/1000
2023-09-28 22:30:22.213 
Epoch 84/1000 
	 loss: 30.4773, MinusLogProbMetric: 30.4773, val_loss: 31.9047, val_MinusLogProbMetric: 31.9047

Epoch 84: val_loss did not improve from 30.38157
196/196 - 35s - loss: 30.4773 - MinusLogProbMetric: 30.4773 - val_loss: 31.9047 - val_MinusLogProbMetric: 31.9047 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 85/1000
2023-09-28 22:30:57.562 
Epoch 85/1000 
	 loss: 30.4172, MinusLogProbMetric: 30.4172, val_loss: 30.1251, val_MinusLogProbMetric: 30.1251

Epoch 85: val_loss improved from 30.38157 to 30.12513, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_329/weights/best_weights.h5
196/196 - 36s - loss: 30.4172 - MinusLogProbMetric: 30.4172 - val_loss: 30.1251 - val_MinusLogProbMetric: 30.1251 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 86/1000
2023-09-28 22:31:33.652 
Epoch 86/1000 
	 loss: 30.3263, MinusLogProbMetric: 30.3263, val_loss: 30.4635, val_MinusLogProbMetric: 30.4635

Epoch 86: val_loss did not improve from 30.12513
196/196 - 36s - loss: 30.3263 - MinusLogProbMetric: 30.3263 - val_loss: 30.4635 - val_MinusLogProbMetric: 30.4635 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 87/1000
2023-09-28 22:32:07.989 
Epoch 87/1000 
	 loss: 30.4149, MinusLogProbMetric: 30.4149, val_loss: 30.3140, val_MinusLogProbMetric: 30.3140

Epoch 87: val_loss did not improve from 30.12513
196/196 - 34s - loss: 30.4149 - MinusLogProbMetric: 30.4149 - val_loss: 30.3140 - val_MinusLogProbMetric: 30.3140 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 88/1000
2023-09-28 22:32:44.023 
Epoch 88/1000 
	 loss: 30.3911, MinusLogProbMetric: 30.3911, val_loss: 30.7975, val_MinusLogProbMetric: 30.7975

Epoch 88: val_loss did not improve from 30.12513
196/196 - 36s - loss: 30.3911 - MinusLogProbMetric: 30.3911 - val_loss: 30.7975 - val_MinusLogProbMetric: 30.7975 - lr: 0.0010 - 36s/epoch - 184ms/step
Epoch 89/1000
2023-09-28 22:33:19.330 
Epoch 89/1000 
	 loss: 30.2863, MinusLogProbMetric: 30.2863, val_loss: 30.9925, val_MinusLogProbMetric: 30.9925

Epoch 89: val_loss did not improve from 30.12513
196/196 - 35s - loss: 30.2863 - MinusLogProbMetric: 30.2863 - val_loss: 30.9925 - val_MinusLogProbMetric: 30.9925 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 90/1000
2023-09-28 22:33:55.175 
Epoch 90/1000 
	 loss: 30.2328, MinusLogProbMetric: 30.2328, val_loss: 30.5196, val_MinusLogProbMetric: 30.5196

Epoch 90: val_loss did not improve from 30.12513
196/196 - 36s - loss: 30.2328 - MinusLogProbMetric: 30.2328 - val_loss: 30.5196 - val_MinusLogProbMetric: 30.5196 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 91/1000
2023-09-28 22:34:30.341 
Epoch 91/1000 
	 loss: 30.2303, MinusLogProbMetric: 30.2303, val_loss: 30.3732, val_MinusLogProbMetric: 30.3732

Epoch 91: val_loss did not improve from 30.12513
196/196 - 35s - loss: 30.2303 - MinusLogProbMetric: 30.2303 - val_loss: 30.3732 - val_MinusLogProbMetric: 30.3732 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 92/1000
2023-09-28 22:35:04.352 
Epoch 92/1000 
	 loss: 30.1955, MinusLogProbMetric: 30.1955, val_loss: 30.2789, val_MinusLogProbMetric: 30.2789

Epoch 92: val_loss did not improve from 30.12513
196/196 - 34s - loss: 30.1955 - MinusLogProbMetric: 30.1955 - val_loss: 30.2789 - val_MinusLogProbMetric: 30.2789 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 93/1000
2023-09-28 22:35:40.229 
Epoch 93/1000 
	 loss: 30.2292, MinusLogProbMetric: 30.2292, val_loss: 30.7619, val_MinusLogProbMetric: 30.7619

Epoch 93: val_loss did not improve from 30.12513
196/196 - 36s - loss: 30.2292 - MinusLogProbMetric: 30.2292 - val_loss: 30.7619 - val_MinusLogProbMetric: 30.7619 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 94/1000
2023-09-28 22:36:15.326 
Epoch 94/1000 
	 loss: 30.1624, MinusLogProbMetric: 30.1624, val_loss: 30.1143, val_MinusLogProbMetric: 30.1143

Epoch 94: val_loss improved from 30.12513 to 30.11433, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_329/weights/best_weights.h5
196/196 - 36s - loss: 30.1624 - MinusLogProbMetric: 30.1624 - val_loss: 30.1143 - val_MinusLogProbMetric: 30.1143 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 95/1000
2023-09-28 22:36:51.529 
Epoch 95/1000 
	 loss: 30.1567, MinusLogProbMetric: 30.1567, val_loss: 29.8074, val_MinusLogProbMetric: 29.8074

Epoch 95: val_loss improved from 30.11433 to 29.80740, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_329/weights/best_weights.h5
196/196 - 36s - loss: 30.1567 - MinusLogProbMetric: 30.1567 - val_loss: 29.8074 - val_MinusLogProbMetric: 29.8074 - lr: 0.0010 - 36s/epoch - 186ms/step
Epoch 96/1000
2023-09-28 22:37:25.079 
Epoch 96/1000 
	 loss: 30.1487, MinusLogProbMetric: 30.1487, val_loss: 30.6580, val_MinusLogProbMetric: 30.6580

Epoch 96: val_loss did not improve from 29.80740
196/196 - 33s - loss: 30.1487 - MinusLogProbMetric: 30.1487 - val_loss: 30.6580 - val_MinusLogProbMetric: 30.6580 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 97/1000
2023-09-28 22:37:59.472 
Epoch 97/1000 
	 loss: 30.1638, MinusLogProbMetric: 30.1638, val_loss: 30.3811, val_MinusLogProbMetric: 30.3811

Epoch 97: val_loss did not improve from 29.80740
196/196 - 34s - loss: 30.1638 - MinusLogProbMetric: 30.1638 - val_loss: 30.3811 - val_MinusLogProbMetric: 30.3811 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 98/1000
2023-09-28 22:38:34.251 
Epoch 98/1000 
	 loss: 30.0660, MinusLogProbMetric: 30.0660, val_loss: 31.3115, val_MinusLogProbMetric: 31.3115

Epoch 98: val_loss did not improve from 29.80740
196/196 - 35s - loss: 30.0660 - MinusLogProbMetric: 30.0660 - val_loss: 31.3115 - val_MinusLogProbMetric: 31.3115 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 99/1000
2023-09-28 22:39:09.869 
Epoch 99/1000 
	 loss: 30.1939, MinusLogProbMetric: 30.1939, val_loss: 30.0862, val_MinusLogProbMetric: 30.0862

Epoch 99: val_loss did not improve from 29.80740
196/196 - 36s - loss: 30.1939 - MinusLogProbMetric: 30.1939 - val_loss: 30.0862 - val_MinusLogProbMetric: 30.0862 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 100/1000
2023-09-28 22:39:42.105 
Epoch 100/1000 
	 loss: 29.9766, MinusLogProbMetric: 29.9766, val_loss: 29.9806, val_MinusLogProbMetric: 29.9806

Epoch 100: val_loss did not improve from 29.80740
196/196 - 32s - loss: 29.9766 - MinusLogProbMetric: 29.9766 - val_loss: 29.9806 - val_MinusLogProbMetric: 29.9806 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 101/1000
2023-09-28 22:40:18.004 
Epoch 101/1000 
	 loss: 30.0707, MinusLogProbMetric: 30.0707, val_loss: 30.1767, val_MinusLogProbMetric: 30.1767

Epoch 101: val_loss did not improve from 29.80740
196/196 - 36s - loss: 30.0707 - MinusLogProbMetric: 30.0707 - val_loss: 30.1767 - val_MinusLogProbMetric: 30.1767 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 102/1000
2023-09-28 22:40:52.267 
Epoch 102/1000 
	 loss: 29.9718, MinusLogProbMetric: 29.9718, val_loss: 30.1295, val_MinusLogProbMetric: 30.1295

Epoch 102: val_loss did not improve from 29.80740
196/196 - 34s - loss: 29.9718 - MinusLogProbMetric: 29.9718 - val_loss: 30.1295 - val_MinusLogProbMetric: 30.1295 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 103/1000
2023-09-28 22:41:27.317 
Epoch 103/1000 
	 loss: 30.0229, MinusLogProbMetric: 30.0229, val_loss: 30.1604, val_MinusLogProbMetric: 30.1604

Epoch 103: val_loss did not improve from 29.80740
196/196 - 35s - loss: 30.0229 - MinusLogProbMetric: 30.0229 - val_loss: 30.1604 - val_MinusLogProbMetric: 30.1604 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 104/1000
2023-09-28 22:42:02.145 
Epoch 104/1000 
	 loss: 29.9204, MinusLogProbMetric: 29.9204, val_loss: 29.9987, val_MinusLogProbMetric: 29.9987

Epoch 104: val_loss did not improve from 29.80740
196/196 - 35s - loss: 29.9204 - MinusLogProbMetric: 29.9204 - val_loss: 29.9987 - val_MinusLogProbMetric: 29.9987 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 105/1000
2023-09-28 22:42:37.832 
Epoch 105/1000 
	 loss: 29.9158, MinusLogProbMetric: 29.9158, val_loss: 30.2014, val_MinusLogProbMetric: 30.2014

Epoch 105: val_loss did not improve from 29.80740
196/196 - 36s - loss: 29.9158 - MinusLogProbMetric: 29.9158 - val_loss: 30.2014 - val_MinusLogProbMetric: 30.2014 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 106/1000
2023-09-28 22:43:12.386 
Epoch 106/1000 
	 loss: 29.9228, MinusLogProbMetric: 29.9228, val_loss: 30.1051, val_MinusLogProbMetric: 30.1051

Epoch 106: val_loss did not improve from 29.80740
196/196 - 35s - loss: 29.9228 - MinusLogProbMetric: 29.9228 - val_loss: 30.1051 - val_MinusLogProbMetric: 30.1051 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 107/1000
2023-09-28 22:43:48.130 
Epoch 107/1000 
	 loss: 29.9629, MinusLogProbMetric: 29.9629, val_loss: 30.0223, val_MinusLogProbMetric: 30.0223

Epoch 107: val_loss did not improve from 29.80740
196/196 - 36s - loss: 29.9629 - MinusLogProbMetric: 29.9629 - val_loss: 30.0223 - val_MinusLogProbMetric: 30.0223 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 108/1000
2023-09-28 22:44:23.218 
Epoch 108/1000 
	 loss: 30.0821, MinusLogProbMetric: 30.0821, val_loss: 29.8999, val_MinusLogProbMetric: 29.8999

Epoch 108: val_loss did not improve from 29.80740
196/196 - 35s - loss: 30.0821 - MinusLogProbMetric: 30.0821 - val_loss: 29.8999 - val_MinusLogProbMetric: 29.8999 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 109/1000
2023-09-28 22:44:58.659 
Epoch 109/1000 
	 loss: 29.8790, MinusLogProbMetric: 29.8790, val_loss: 30.0395, val_MinusLogProbMetric: 30.0395

Epoch 109: val_loss did not improve from 29.80740
196/196 - 35s - loss: 29.8790 - MinusLogProbMetric: 29.8790 - val_loss: 30.0395 - val_MinusLogProbMetric: 30.0395 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 110/1000
2023-09-28 22:45:34.241 
Epoch 110/1000 
	 loss: 29.8509, MinusLogProbMetric: 29.8509, val_loss: 30.0402, val_MinusLogProbMetric: 30.0402

Epoch 110: val_loss did not improve from 29.80740
196/196 - 36s - loss: 29.8509 - MinusLogProbMetric: 29.8509 - val_loss: 30.0402 - val_MinusLogProbMetric: 30.0402 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 111/1000
2023-09-28 22:46:10.090 
Epoch 111/1000 
	 loss: 29.7905, MinusLogProbMetric: 29.7905, val_loss: 30.2456, val_MinusLogProbMetric: 30.2456

Epoch 111: val_loss did not improve from 29.80740
196/196 - 36s - loss: 29.7905 - MinusLogProbMetric: 29.7905 - val_loss: 30.2456 - val_MinusLogProbMetric: 30.2456 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 112/1000
2023-09-28 22:46:45.721 
Epoch 112/1000 
	 loss: 29.8011, MinusLogProbMetric: 29.8011, val_loss: 30.0816, val_MinusLogProbMetric: 30.0816

Epoch 112: val_loss did not improve from 29.80740
196/196 - 36s - loss: 29.8011 - MinusLogProbMetric: 29.8011 - val_loss: 30.0816 - val_MinusLogProbMetric: 30.0816 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 113/1000
2023-09-28 22:47:21.792 
Epoch 113/1000 
	 loss: 29.8131, MinusLogProbMetric: 29.8131, val_loss: 31.2114, val_MinusLogProbMetric: 31.2114

Epoch 113: val_loss did not improve from 29.80740
196/196 - 36s - loss: 29.8131 - MinusLogProbMetric: 29.8131 - val_loss: 31.2114 - val_MinusLogProbMetric: 31.2114 - lr: 0.0010 - 36s/epoch - 184ms/step
Epoch 114/1000
2023-09-28 22:47:54.890 
Epoch 114/1000 
	 loss: 29.7422, MinusLogProbMetric: 29.7422, val_loss: 29.7187, val_MinusLogProbMetric: 29.7187

Epoch 114: val_loss improved from 29.80740 to 29.71874, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_329/weights/best_weights.h5
196/196 - 34s - loss: 29.7422 - MinusLogProbMetric: 29.7422 - val_loss: 29.7187 - val_MinusLogProbMetric: 29.7187 - lr: 0.0010 - 34s/epoch - 171ms/step
Epoch 115/1000
2023-09-28 22:48:29.862 
Epoch 115/1000 
	 loss: 29.8361, MinusLogProbMetric: 29.8361, val_loss: 29.8610, val_MinusLogProbMetric: 29.8610

Epoch 115: val_loss did not improve from 29.71874
196/196 - 35s - loss: 29.8361 - MinusLogProbMetric: 29.8361 - val_loss: 29.8610 - val_MinusLogProbMetric: 29.8610 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 116/1000
2023-09-28 22:49:05.384 
Epoch 116/1000 
	 loss: 29.7974, MinusLogProbMetric: 29.7974, val_loss: 29.5566, val_MinusLogProbMetric: 29.5566

Epoch 116: val_loss improved from 29.71874 to 29.55657, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_329/weights/best_weights.h5
196/196 - 36s - loss: 29.7974 - MinusLogProbMetric: 29.7974 - val_loss: 29.5566 - val_MinusLogProbMetric: 29.5566 - lr: 0.0010 - 36s/epoch - 184ms/step
Epoch 117/1000
2023-09-28 22:49:40.265 
Epoch 117/1000 
	 loss: 29.7467, MinusLogProbMetric: 29.7467, val_loss: 29.7115, val_MinusLogProbMetric: 29.7115

Epoch 117: val_loss did not improve from 29.55657
196/196 - 34s - loss: 29.7467 - MinusLogProbMetric: 29.7467 - val_loss: 29.7115 - val_MinusLogProbMetric: 29.7115 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 118/1000
2023-09-28 22:50:15.930 
Epoch 118/1000 
	 loss: 29.7541, MinusLogProbMetric: 29.7541, val_loss: 30.0453, val_MinusLogProbMetric: 30.0453

Epoch 118: val_loss did not improve from 29.55657
196/196 - 36s - loss: 29.7541 - MinusLogProbMetric: 29.7541 - val_loss: 30.0453 - val_MinusLogProbMetric: 30.0453 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 119/1000
2023-09-28 22:50:51.047 
Epoch 119/1000 
	 loss: 29.7808, MinusLogProbMetric: 29.7808, val_loss: 29.8338, val_MinusLogProbMetric: 29.8338

Epoch 119: val_loss did not improve from 29.55657
196/196 - 35s - loss: 29.7808 - MinusLogProbMetric: 29.7808 - val_loss: 29.8338 - val_MinusLogProbMetric: 29.8338 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 120/1000
2023-09-28 22:51:26.093 
Epoch 120/1000 
	 loss: 29.7412, MinusLogProbMetric: 29.7412, val_loss: 29.6632, val_MinusLogProbMetric: 29.6632

Epoch 120: val_loss did not improve from 29.55657
196/196 - 35s - loss: 29.7412 - MinusLogProbMetric: 29.7412 - val_loss: 29.6632 - val_MinusLogProbMetric: 29.6632 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 121/1000
2023-09-28 22:52:00.932 
Epoch 121/1000 
	 loss: 29.6768, MinusLogProbMetric: 29.6768, val_loss: 30.1974, val_MinusLogProbMetric: 30.1974

Epoch 121: val_loss did not improve from 29.55657
196/196 - 35s - loss: 29.6768 - MinusLogProbMetric: 29.6768 - val_loss: 30.1974 - val_MinusLogProbMetric: 30.1974 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 122/1000
2023-09-28 22:52:35.189 
Epoch 122/1000 
	 loss: 29.6746, MinusLogProbMetric: 29.6746, val_loss: 30.1559, val_MinusLogProbMetric: 30.1559

Epoch 122: val_loss did not improve from 29.55657
196/196 - 34s - loss: 29.6746 - MinusLogProbMetric: 29.6746 - val_loss: 30.1559 - val_MinusLogProbMetric: 30.1559 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 123/1000
2023-09-28 22:53:08.737 
Epoch 123/1000 
	 loss: 29.7205, MinusLogProbMetric: 29.7205, val_loss: 29.6909, val_MinusLogProbMetric: 29.6909

Epoch 123: val_loss did not improve from 29.55657
196/196 - 34s - loss: 29.7205 - MinusLogProbMetric: 29.7205 - val_loss: 29.6909 - val_MinusLogProbMetric: 29.6909 - lr: 0.0010 - 34s/epoch - 171ms/step
Epoch 124/1000
2023-09-28 22:53:43.155 
Epoch 124/1000 
	 loss: 29.6565, MinusLogProbMetric: 29.6565, val_loss: 30.1174, val_MinusLogProbMetric: 30.1174

Epoch 124: val_loss did not improve from 29.55657
196/196 - 34s - loss: 29.6565 - MinusLogProbMetric: 29.6565 - val_loss: 30.1174 - val_MinusLogProbMetric: 30.1174 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 125/1000
2023-09-28 22:54:16.337 
Epoch 125/1000 
	 loss: 29.6118, MinusLogProbMetric: 29.6118, val_loss: 29.6282, val_MinusLogProbMetric: 29.6282

Epoch 125: val_loss did not improve from 29.55657
196/196 - 33s - loss: 29.6118 - MinusLogProbMetric: 29.6118 - val_loss: 29.6282 - val_MinusLogProbMetric: 29.6282 - lr: 0.0010 - 33s/epoch - 169ms/step
Epoch 126/1000
2023-09-28 22:54:49.247 
Epoch 126/1000 
	 loss: 29.6311, MinusLogProbMetric: 29.6311, val_loss: 30.4475, val_MinusLogProbMetric: 30.4475

Epoch 126: val_loss did not improve from 29.55657
196/196 - 33s - loss: 29.6311 - MinusLogProbMetric: 29.6311 - val_loss: 30.4475 - val_MinusLogProbMetric: 30.4475 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 127/1000
2023-09-28 22:55:23.087 
Epoch 127/1000 
	 loss: 29.7335, MinusLogProbMetric: 29.7335, val_loss: 29.6704, val_MinusLogProbMetric: 29.6704

Epoch 127: val_loss did not improve from 29.55657
196/196 - 34s - loss: 29.7335 - MinusLogProbMetric: 29.7335 - val_loss: 29.6704 - val_MinusLogProbMetric: 29.6704 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 128/1000
2023-09-28 22:55:56.012 
Epoch 128/1000 
	 loss: 29.5345, MinusLogProbMetric: 29.5345, val_loss: 29.9674, val_MinusLogProbMetric: 29.9674

Epoch 128: val_loss did not improve from 29.55657
196/196 - 33s - loss: 29.5345 - MinusLogProbMetric: 29.5345 - val_loss: 29.9674 - val_MinusLogProbMetric: 29.9674 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 129/1000
2023-09-28 22:56:30.740 
Epoch 129/1000 
	 loss: 29.5537, MinusLogProbMetric: 29.5537, val_loss: 29.8121, val_MinusLogProbMetric: 29.8121

Epoch 129: val_loss did not improve from 29.55657
196/196 - 35s - loss: 29.5537 - MinusLogProbMetric: 29.5537 - val_loss: 29.8121 - val_MinusLogProbMetric: 29.8121 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 130/1000
2023-09-28 22:57:03.381 
Epoch 130/1000 
	 loss: 29.5383, MinusLogProbMetric: 29.5383, val_loss: 30.1157, val_MinusLogProbMetric: 30.1157

Epoch 130: val_loss did not improve from 29.55657
196/196 - 33s - loss: 29.5383 - MinusLogProbMetric: 29.5383 - val_loss: 30.1157 - val_MinusLogProbMetric: 30.1157 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 131/1000
2023-09-28 22:57:36.997 
Epoch 131/1000 
	 loss: 29.5873, MinusLogProbMetric: 29.5873, val_loss: 29.9488, val_MinusLogProbMetric: 29.9488

Epoch 131: val_loss did not improve from 29.55657
196/196 - 34s - loss: 29.5873 - MinusLogProbMetric: 29.5873 - val_loss: 29.9488 - val_MinusLogProbMetric: 29.9488 - lr: 0.0010 - 34s/epoch - 171ms/step
Epoch 132/1000
2023-09-28 22:58:10.528 
Epoch 132/1000 
	 loss: 29.5186, MinusLogProbMetric: 29.5186, val_loss: 29.4625, val_MinusLogProbMetric: 29.4625

Epoch 132: val_loss improved from 29.55657 to 29.46249, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_329/weights/best_weights.h5
196/196 - 34s - loss: 29.5186 - MinusLogProbMetric: 29.5186 - val_loss: 29.4625 - val_MinusLogProbMetric: 29.4625 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 133/1000
2023-09-28 22:58:43.036 
Epoch 133/1000 
	 loss: 29.5343, MinusLogProbMetric: 29.5343, val_loss: 30.6738, val_MinusLogProbMetric: 30.6738

Epoch 133: val_loss did not improve from 29.46249
196/196 - 32s - loss: 29.5343 - MinusLogProbMetric: 29.5343 - val_loss: 30.6738 - val_MinusLogProbMetric: 30.6738 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 134/1000
2023-09-28 22:59:16.882 
Epoch 134/1000 
	 loss: 29.5567, MinusLogProbMetric: 29.5567, val_loss: 29.7277, val_MinusLogProbMetric: 29.7277

Epoch 134: val_loss did not improve from 29.46249
196/196 - 34s - loss: 29.5567 - MinusLogProbMetric: 29.5567 - val_loss: 29.7277 - val_MinusLogProbMetric: 29.7277 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 135/1000
2023-09-28 22:59:49.870 
Epoch 135/1000 
	 loss: 29.4872, MinusLogProbMetric: 29.4872, val_loss: 29.8255, val_MinusLogProbMetric: 29.8255

Epoch 135: val_loss did not improve from 29.46249
196/196 - 33s - loss: 29.4872 - MinusLogProbMetric: 29.4872 - val_loss: 29.8255 - val_MinusLogProbMetric: 29.8255 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 136/1000
2023-09-28 23:00:23.287 
Epoch 136/1000 
	 loss: 29.4691, MinusLogProbMetric: 29.4691, val_loss: 30.0134, val_MinusLogProbMetric: 30.0134

Epoch 136: val_loss did not improve from 29.46249
196/196 - 33s - loss: 29.4691 - MinusLogProbMetric: 29.4691 - val_loss: 30.0134 - val_MinusLogProbMetric: 30.0134 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 137/1000
2023-09-28 23:00:54.177 
Epoch 137/1000 
	 loss: 29.4956, MinusLogProbMetric: 29.4956, val_loss: 29.5680, val_MinusLogProbMetric: 29.5680

Epoch 137: val_loss did not improve from 29.46249
196/196 - 31s - loss: 29.4956 - MinusLogProbMetric: 29.4956 - val_loss: 29.5680 - val_MinusLogProbMetric: 29.5680 - lr: 0.0010 - 31s/epoch - 158ms/step
Epoch 138/1000
2023-09-28 23:01:23.941 
Epoch 138/1000 
	 loss: 29.4553, MinusLogProbMetric: 29.4553, val_loss: 30.0637, val_MinusLogProbMetric: 30.0637

Epoch 138: val_loss did not improve from 29.46249
196/196 - 30s - loss: 29.4553 - MinusLogProbMetric: 29.4553 - val_loss: 30.0637 - val_MinusLogProbMetric: 30.0637 - lr: 0.0010 - 30s/epoch - 152ms/step
Epoch 139/1000
2023-09-28 23:01:55.785 
Epoch 139/1000 
	 loss: 29.5872, MinusLogProbMetric: 29.5872, val_loss: 29.7984, val_MinusLogProbMetric: 29.7984

Epoch 139: val_loss did not improve from 29.46249
196/196 - 32s - loss: 29.5872 - MinusLogProbMetric: 29.5872 - val_loss: 29.7984 - val_MinusLogProbMetric: 29.7984 - lr: 0.0010 - 32s/epoch - 162ms/step
Epoch 140/1000
2023-09-28 23:02:30.059 
Epoch 140/1000 
	 loss: 29.4647, MinusLogProbMetric: 29.4647, val_loss: 29.4555, val_MinusLogProbMetric: 29.4555

Epoch 140: val_loss improved from 29.46249 to 29.45553, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_329/weights/best_weights.h5
196/196 - 35s - loss: 29.4647 - MinusLogProbMetric: 29.4647 - val_loss: 29.4555 - val_MinusLogProbMetric: 29.4555 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 141/1000
2023-09-28 23:03:04.775 
Epoch 141/1000 
	 loss: 29.5331, MinusLogProbMetric: 29.5331, val_loss: 29.9089, val_MinusLogProbMetric: 29.9089

Epoch 141: val_loss did not improve from 29.45553
196/196 - 34s - loss: 29.5331 - MinusLogProbMetric: 29.5331 - val_loss: 29.9089 - val_MinusLogProbMetric: 29.9089 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 142/1000
2023-09-28 23:03:37.723 
Epoch 142/1000 
	 loss: 29.3906, MinusLogProbMetric: 29.3906, val_loss: 29.9003, val_MinusLogProbMetric: 29.9003

Epoch 142: val_loss did not improve from 29.45553
196/196 - 33s - loss: 29.3906 - MinusLogProbMetric: 29.3906 - val_loss: 29.9003 - val_MinusLogProbMetric: 29.9003 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 143/1000
2023-09-28 23:04:12.102 
Epoch 143/1000 
	 loss: 29.4440, MinusLogProbMetric: 29.4440, val_loss: 29.8919, val_MinusLogProbMetric: 29.8919

Epoch 143: val_loss did not improve from 29.45553
196/196 - 34s - loss: 29.4440 - MinusLogProbMetric: 29.4440 - val_loss: 29.8919 - val_MinusLogProbMetric: 29.8919 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 144/1000
2023-09-28 23:04:45.494 
Epoch 144/1000 
	 loss: 29.4586, MinusLogProbMetric: 29.4586, val_loss: 29.7036, val_MinusLogProbMetric: 29.7036

Epoch 144: val_loss did not improve from 29.45553
196/196 - 33s - loss: 29.4586 - MinusLogProbMetric: 29.4586 - val_loss: 29.7036 - val_MinusLogProbMetric: 29.7036 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 145/1000
2023-09-28 23:05:18.034 
Epoch 145/1000 
	 loss: 29.3832, MinusLogProbMetric: 29.3832, val_loss: 29.4395, val_MinusLogProbMetric: 29.4395

Epoch 145: val_loss improved from 29.45553 to 29.43953, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_329/weights/best_weights.h5
196/196 - 33s - loss: 29.3832 - MinusLogProbMetric: 29.3832 - val_loss: 29.4395 - val_MinusLogProbMetric: 29.4395 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 146/1000
2023-09-28 23:05:51.426 
Epoch 146/1000 
	 loss: 29.3985, MinusLogProbMetric: 29.3985, val_loss: 29.7266, val_MinusLogProbMetric: 29.7266

Epoch 146: val_loss did not improve from 29.43953
196/196 - 33s - loss: 29.3985 - MinusLogProbMetric: 29.3985 - val_loss: 29.7266 - val_MinusLogProbMetric: 29.7266 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 147/1000
2023-09-28 23:06:23.783 
Epoch 147/1000 
	 loss: 29.3740, MinusLogProbMetric: 29.3740, val_loss: 29.7733, val_MinusLogProbMetric: 29.7733

Epoch 147: val_loss did not improve from 29.43953
196/196 - 32s - loss: 29.3740 - MinusLogProbMetric: 29.3740 - val_loss: 29.7733 - val_MinusLogProbMetric: 29.7733 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 148/1000
2023-09-28 23:06:55.330 
Epoch 148/1000 
	 loss: 29.3353, MinusLogProbMetric: 29.3353, val_loss: 30.1612, val_MinusLogProbMetric: 30.1612

Epoch 148: val_loss did not improve from 29.43953
196/196 - 32s - loss: 29.3353 - MinusLogProbMetric: 29.3353 - val_loss: 30.1612 - val_MinusLogProbMetric: 30.1612 - lr: 0.0010 - 32s/epoch - 161ms/step
Epoch 149/1000
2023-09-28 23:07:29.169 
Epoch 149/1000 
	 loss: 29.3993, MinusLogProbMetric: 29.3993, val_loss: 29.4731, val_MinusLogProbMetric: 29.4731

Epoch 149: val_loss did not improve from 29.43953
196/196 - 34s - loss: 29.3993 - MinusLogProbMetric: 29.3993 - val_loss: 29.4731 - val_MinusLogProbMetric: 29.4731 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 150/1000
2023-09-28 23:07:59.993 
Epoch 150/1000 
	 loss: 29.3609, MinusLogProbMetric: 29.3609, val_loss: 29.7428, val_MinusLogProbMetric: 29.7428

Epoch 150: val_loss did not improve from 29.43953
196/196 - 31s - loss: 29.3609 - MinusLogProbMetric: 29.3609 - val_loss: 29.7428 - val_MinusLogProbMetric: 29.7428 - lr: 0.0010 - 31s/epoch - 157ms/step
Epoch 151/1000
2023-09-28 23:08:34.356 
Epoch 151/1000 
	 loss: 29.3415, MinusLogProbMetric: 29.3415, val_loss: 30.2659, val_MinusLogProbMetric: 30.2659

Epoch 151: val_loss did not improve from 29.43953
196/196 - 34s - loss: 29.3415 - MinusLogProbMetric: 29.3415 - val_loss: 30.2659 - val_MinusLogProbMetric: 30.2659 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 152/1000
2023-09-28 23:09:07.787 
Epoch 152/1000 
	 loss: 29.3800, MinusLogProbMetric: 29.3800, val_loss: 29.2748, val_MinusLogProbMetric: 29.2748

Epoch 152: val_loss improved from 29.43953 to 29.27483, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_329/weights/best_weights.h5
196/196 - 34s - loss: 29.3800 - MinusLogProbMetric: 29.3800 - val_loss: 29.2748 - val_MinusLogProbMetric: 29.2748 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 153/1000
2023-09-28 23:09:38.236 
Epoch 153/1000 
	 loss: 29.3364, MinusLogProbMetric: 29.3364, val_loss: 29.6367, val_MinusLogProbMetric: 29.6367

Epoch 153: val_loss did not improve from 29.27483
196/196 - 30s - loss: 29.3364 - MinusLogProbMetric: 29.3364 - val_loss: 29.6367 - val_MinusLogProbMetric: 29.6367 - lr: 0.0010 - 30s/epoch - 153ms/step
Epoch 154/1000
2023-09-28 23:10:10.494 
Epoch 154/1000 
	 loss: 29.3109, MinusLogProbMetric: 29.3109, val_loss: 29.9944, val_MinusLogProbMetric: 29.9944

Epoch 154: val_loss did not improve from 29.27483
196/196 - 32s - loss: 29.3109 - MinusLogProbMetric: 29.3109 - val_loss: 29.9944 - val_MinusLogProbMetric: 29.9944 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 155/1000
2023-09-28 23:10:43.539 
Epoch 155/1000 
	 loss: 29.4198, MinusLogProbMetric: 29.4198, val_loss: 29.5635, val_MinusLogProbMetric: 29.5635

Epoch 155: val_loss did not improve from 29.27483
196/196 - 33s - loss: 29.4198 - MinusLogProbMetric: 29.4198 - val_loss: 29.5635 - val_MinusLogProbMetric: 29.5635 - lr: 0.0010 - 33s/epoch - 169ms/step
Epoch 156/1000
2023-09-28 23:11:17.535 
Epoch 156/1000 
	 loss: 29.4314, MinusLogProbMetric: 29.4314, val_loss: 29.4056, val_MinusLogProbMetric: 29.4056

Epoch 156: val_loss did not improve from 29.27483
196/196 - 34s - loss: 29.4314 - MinusLogProbMetric: 29.4314 - val_loss: 29.4056 - val_MinusLogProbMetric: 29.4056 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 157/1000
2023-09-28 23:11:51.806 
Epoch 157/1000 
	 loss: 29.2955, MinusLogProbMetric: 29.2955, val_loss: 30.7238, val_MinusLogProbMetric: 30.7238

Epoch 157: val_loss did not improve from 29.27483
196/196 - 34s - loss: 29.2955 - MinusLogProbMetric: 29.2955 - val_loss: 30.7238 - val_MinusLogProbMetric: 30.7238 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 158/1000
2023-09-28 23:12:26.057 
Epoch 158/1000 
	 loss: 29.2802, MinusLogProbMetric: 29.2802, val_loss: 29.4317, val_MinusLogProbMetric: 29.4317

Epoch 158: val_loss did not improve from 29.27483
196/196 - 34s - loss: 29.2802 - MinusLogProbMetric: 29.2802 - val_loss: 29.4317 - val_MinusLogProbMetric: 29.4317 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 159/1000
2023-09-28 23:12:58.850 
Epoch 159/1000 
	 loss: 29.3320, MinusLogProbMetric: 29.3320, val_loss: 30.4014, val_MinusLogProbMetric: 30.4014

Epoch 159: val_loss did not improve from 29.27483
196/196 - 33s - loss: 29.3320 - MinusLogProbMetric: 29.3320 - val_loss: 30.4014 - val_MinusLogProbMetric: 30.4014 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 160/1000
2023-09-28 23:13:32.376 
Epoch 160/1000 
	 loss: 29.2944, MinusLogProbMetric: 29.2944, val_loss: 29.6263, val_MinusLogProbMetric: 29.6263

Epoch 160: val_loss did not improve from 29.27483
196/196 - 34s - loss: 29.2944 - MinusLogProbMetric: 29.2944 - val_loss: 29.6263 - val_MinusLogProbMetric: 29.6263 - lr: 0.0010 - 34s/epoch - 171ms/step
Epoch 161/1000
2023-09-28 23:14:06.791 
Epoch 161/1000 
	 loss: 29.3570, MinusLogProbMetric: 29.3570, val_loss: 29.8989, val_MinusLogProbMetric: 29.8989

Epoch 161: val_loss did not improve from 29.27483
196/196 - 34s - loss: 29.3570 - MinusLogProbMetric: 29.3570 - val_loss: 29.8989 - val_MinusLogProbMetric: 29.8989 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 162/1000
2023-09-28 23:14:39.837 
Epoch 162/1000 
	 loss: 29.1995, MinusLogProbMetric: 29.1995, val_loss: 29.4500, val_MinusLogProbMetric: 29.4500

Epoch 162: val_loss did not improve from 29.27483
196/196 - 33s - loss: 29.1995 - MinusLogProbMetric: 29.1995 - val_loss: 29.4500 - val_MinusLogProbMetric: 29.4500 - lr: 0.0010 - 33s/epoch - 169ms/step
Epoch 163/1000
2023-09-28 23:15:11.791 
Epoch 163/1000 
	 loss: 29.2694, MinusLogProbMetric: 29.2694, val_loss: 29.4452, val_MinusLogProbMetric: 29.4452

Epoch 163: val_loss did not improve from 29.27483
196/196 - 32s - loss: 29.2694 - MinusLogProbMetric: 29.2694 - val_loss: 29.4452 - val_MinusLogProbMetric: 29.4452 - lr: 0.0010 - 32s/epoch - 163ms/step
Epoch 164/1000
2023-09-28 23:15:42.830 
Epoch 164/1000 
	 loss: 29.2402, MinusLogProbMetric: 29.2402, val_loss: 29.7264, val_MinusLogProbMetric: 29.7264

Epoch 164: val_loss did not improve from 29.27483
196/196 - 31s - loss: 29.2402 - MinusLogProbMetric: 29.2402 - val_loss: 29.7264 - val_MinusLogProbMetric: 29.7264 - lr: 0.0010 - 31s/epoch - 158ms/step
Epoch 165/1000
2023-09-28 23:16:16.567 
Epoch 165/1000 
	 loss: 29.2673, MinusLogProbMetric: 29.2673, val_loss: 29.6470, val_MinusLogProbMetric: 29.6470

Epoch 165: val_loss did not improve from 29.27483
196/196 - 34s - loss: 29.2673 - MinusLogProbMetric: 29.2673 - val_loss: 29.6470 - val_MinusLogProbMetric: 29.6470 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 166/1000
2023-09-28 23:16:49.880 
Epoch 166/1000 
	 loss: 29.2797, MinusLogProbMetric: 29.2797, val_loss: 29.3137, val_MinusLogProbMetric: 29.3137

Epoch 166: val_loss did not improve from 29.27483
196/196 - 33s - loss: 29.2797 - MinusLogProbMetric: 29.2797 - val_loss: 29.3137 - val_MinusLogProbMetric: 29.3137 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 167/1000
2023-09-28 23:17:21.610 
Epoch 167/1000 
	 loss: 29.2723, MinusLogProbMetric: 29.2723, val_loss: 29.4680, val_MinusLogProbMetric: 29.4680

Epoch 167: val_loss did not improve from 29.27483
196/196 - 32s - loss: 29.2723 - MinusLogProbMetric: 29.2723 - val_loss: 29.4680 - val_MinusLogProbMetric: 29.4680 - lr: 0.0010 - 32s/epoch - 162ms/step
Epoch 168/1000
2023-09-28 23:17:53.291 
Epoch 168/1000 
	 loss: 29.1921, MinusLogProbMetric: 29.1921, val_loss: 29.6209, val_MinusLogProbMetric: 29.6209

Epoch 168: val_loss did not improve from 29.27483
196/196 - 32s - loss: 29.1921 - MinusLogProbMetric: 29.1921 - val_loss: 29.6209 - val_MinusLogProbMetric: 29.6209 - lr: 0.0010 - 32s/epoch - 162ms/step
Epoch 169/1000
2023-09-28 23:18:26.622 
Epoch 169/1000 
	 loss: 29.1977, MinusLogProbMetric: 29.1977, val_loss: 29.0442, val_MinusLogProbMetric: 29.0442

Epoch 169: val_loss improved from 29.27483 to 29.04417, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_329/weights/best_weights.h5
196/196 - 34s - loss: 29.1977 - MinusLogProbMetric: 29.1977 - val_loss: 29.0442 - val_MinusLogProbMetric: 29.0442 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 170/1000
2023-09-28 23:19:00.156 
Epoch 170/1000 
	 loss: 29.1829, MinusLogProbMetric: 29.1829, val_loss: 29.4896, val_MinusLogProbMetric: 29.4896

Epoch 170: val_loss did not improve from 29.04417
196/196 - 33s - loss: 29.1829 - MinusLogProbMetric: 29.1829 - val_loss: 29.4896 - val_MinusLogProbMetric: 29.4896 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 171/1000
2023-09-28 23:19:34.291 
Epoch 171/1000 
	 loss: 29.2016, MinusLogProbMetric: 29.2016, val_loss: 29.4282, val_MinusLogProbMetric: 29.4282

Epoch 171: val_loss did not improve from 29.04417
196/196 - 34s - loss: 29.2016 - MinusLogProbMetric: 29.2016 - val_loss: 29.4282 - val_MinusLogProbMetric: 29.4282 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 172/1000
2023-09-28 23:20:05.812 
Epoch 172/1000 
	 loss: 29.0714, MinusLogProbMetric: 29.0714, val_loss: 29.2194, val_MinusLogProbMetric: 29.2194

Epoch 172: val_loss did not improve from 29.04417
196/196 - 32s - loss: 29.0714 - MinusLogProbMetric: 29.0714 - val_loss: 29.2194 - val_MinusLogProbMetric: 29.2194 - lr: 0.0010 - 32s/epoch - 161ms/step
Epoch 173/1000
2023-09-28 23:20:36.826 
Epoch 173/1000 
	 loss: 29.2245, MinusLogProbMetric: 29.2245, val_loss: 29.3989, val_MinusLogProbMetric: 29.3989

Epoch 173: val_loss did not improve from 29.04417
196/196 - 31s - loss: 29.2245 - MinusLogProbMetric: 29.2245 - val_loss: 29.3989 - val_MinusLogProbMetric: 29.3989 - lr: 0.0010 - 31s/epoch - 158ms/step
Epoch 174/1000
2023-09-28 23:21:08.411 
Epoch 174/1000 
	 loss: 29.1438, MinusLogProbMetric: 29.1438, val_loss: 29.7645, val_MinusLogProbMetric: 29.7645

Epoch 174: val_loss did not improve from 29.04417
196/196 - 32s - loss: 29.1438 - MinusLogProbMetric: 29.1438 - val_loss: 29.7645 - val_MinusLogProbMetric: 29.7645 - lr: 0.0010 - 32s/epoch - 161ms/step
Epoch 175/1000
2023-09-28 23:21:39.837 
Epoch 175/1000 
	 loss: 29.1521, MinusLogProbMetric: 29.1521, val_loss: 29.6985, val_MinusLogProbMetric: 29.6985

Epoch 175: val_loss did not improve from 29.04417
196/196 - 31s - loss: 29.1521 - MinusLogProbMetric: 29.1521 - val_loss: 29.6985 - val_MinusLogProbMetric: 29.6985 - lr: 0.0010 - 31s/epoch - 160ms/step
Epoch 176/1000
2023-09-28 23:22:11.603 
Epoch 176/1000 
	 loss: 29.1994, MinusLogProbMetric: 29.1994, val_loss: 29.4456, val_MinusLogProbMetric: 29.4456

Epoch 176: val_loss did not improve from 29.04417
196/196 - 32s - loss: 29.1994 - MinusLogProbMetric: 29.1994 - val_loss: 29.4456 - val_MinusLogProbMetric: 29.4456 - lr: 0.0010 - 32s/epoch - 162ms/step
Epoch 177/1000
2023-09-28 23:22:40.987 
Epoch 177/1000 
	 loss: 29.1107, MinusLogProbMetric: 29.1107, val_loss: 29.7207, val_MinusLogProbMetric: 29.7207

Epoch 177: val_loss did not improve from 29.04417
196/196 - 29s - loss: 29.1107 - MinusLogProbMetric: 29.1107 - val_loss: 29.7207 - val_MinusLogProbMetric: 29.7207 - lr: 0.0010 - 29s/epoch - 150ms/step
Epoch 178/1000
2023-09-28 23:23:10.235 
Epoch 178/1000 
	 loss: 29.2224, MinusLogProbMetric: 29.2224, val_loss: 29.7077, val_MinusLogProbMetric: 29.7077

Epoch 178: val_loss did not improve from 29.04417
196/196 - 29s - loss: 29.2224 - MinusLogProbMetric: 29.2224 - val_loss: 29.7077 - val_MinusLogProbMetric: 29.7077 - lr: 0.0010 - 29s/epoch - 149ms/step
Epoch 179/1000
2023-09-28 23:23:43.204 
Epoch 179/1000 
	 loss: 29.1532, MinusLogProbMetric: 29.1532, val_loss: 28.9340, val_MinusLogProbMetric: 28.9340

Epoch 179: val_loss improved from 29.04417 to 28.93397, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_329/weights/best_weights.h5
196/196 - 33s - loss: 29.1532 - MinusLogProbMetric: 29.1532 - val_loss: 28.9340 - val_MinusLogProbMetric: 28.9340 - lr: 0.0010 - 33s/epoch - 171ms/step
Epoch 180/1000
2023-09-28 23:24:16.343 
Epoch 180/1000 
	 loss: 29.0877, MinusLogProbMetric: 29.0877, val_loss: 29.3450, val_MinusLogProbMetric: 29.3450

Epoch 180: val_loss did not improve from 28.93397
196/196 - 33s - loss: 29.0877 - MinusLogProbMetric: 29.0877 - val_loss: 29.3450 - val_MinusLogProbMetric: 29.3450 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 181/1000
2023-09-28 23:24:50.641 
Epoch 181/1000 
	 loss: 29.1499, MinusLogProbMetric: 29.1499, val_loss: 29.1013, val_MinusLogProbMetric: 29.1013

Epoch 181: val_loss did not improve from 28.93397
196/196 - 34s - loss: 29.1499 - MinusLogProbMetric: 29.1499 - val_loss: 29.1013 - val_MinusLogProbMetric: 29.1013 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 182/1000
2023-09-28 23:25:21.395 
Epoch 182/1000 
	 loss: 29.1290, MinusLogProbMetric: 29.1290, val_loss: 29.1576, val_MinusLogProbMetric: 29.1576

Epoch 182: val_loss did not improve from 28.93397
196/196 - 31s - loss: 29.1290 - MinusLogProbMetric: 29.1290 - val_loss: 29.1576 - val_MinusLogProbMetric: 29.1576 - lr: 0.0010 - 31s/epoch - 157ms/step
Epoch 183/1000
2023-09-28 23:25:50.764 
Epoch 183/1000 
	 loss: 29.0969, MinusLogProbMetric: 29.0969, val_loss: 29.1478, val_MinusLogProbMetric: 29.1478

Epoch 183: val_loss did not improve from 28.93397
196/196 - 29s - loss: 29.0969 - MinusLogProbMetric: 29.0969 - val_loss: 29.1478 - val_MinusLogProbMetric: 29.1478 - lr: 0.0010 - 29s/epoch - 150ms/step
Epoch 184/1000
2023-09-28 23:26:21.586 
Epoch 184/1000 
	 loss: 29.0501, MinusLogProbMetric: 29.0501, val_loss: 29.3585, val_MinusLogProbMetric: 29.3585

Epoch 184: val_loss did not improve from 28.93397
196/196 - 31s - loss: 29.0501 - MinusLogProbMetric: 29.0501 - val_loss: 29.3585 - val_MinusLogProbMetric: 29.3585 - lr: 0.0010 - 31s/epoch - 157ms/step
Epoch 185/1000
2023-09-28 23:26:52.996 
Epoch 185/1000 
	 loss: 29.0011, MinusLogProbMetric: 29.0011, val_loss: 29.5022, val_MinusLogProbMetric: 29.5022

Epoch 185: val_loss did not improve from 28.93397
196/196 - 31s - loss: 29.0011 - MinusLogProbMetric: 29.0011 - val_loss: 29.5022 - val_MinusLogProbMetric: 29.5022 - lr: 0.0010 - 31s/epoch - 160ms/step
Epoch 186/1000
2023-09-28 23:27:22.471 
Epoch 186/1000 
	 loss: 29.0184, MinusLogProbMetric: 29.0184, val_loss: 29.5370, val_MinusLogProbMetric: 29.5370

Epoch 186: val_loss did not improve from 28.93397
196/196 - 29s - loss: 29.0184 - MinusLogProbMetric: 29.0184 - val_loss: 29.5370 - val_MinusLogProbMetric: 29.5370 - lr: 0.0010 - 29s/epoch - 150ms/step
Epoch 187/1000
2023-09-28 23:27:54.301 
Epoch 187/1000 
	 loss: 29.1389, MinusLogProbMetric: 29.1389, val_loss: 29.1134, val_MinusLogProbMetric: 29.1134

Epoch 187: val_loss did not improve from 28.93397
196/196 - 32s - loss: 29.1389 - MinusLogProbMetric: 29.1389 - val_loss: 29.1134 - val_MinusLogProbMetric: 29.1134 - lr: 0.0010 - 32s/epoch - 162ms/step
Epoch 188/1000
2023-09-28 23:28:25.907 
Epoch 188/1000 
	 loss: 29.0922, MinusLogProbMetric: 29.0922, val_loss: 30.6598, val_MinusLogProbMetric: 30.6598

Epoch 188: val_loss did not improve from 28.93397
196/196 - 32s - loss: 29.0922 - MinusLogProbMetric: 29.0922 - val_loss: 30.6598 - val_MinusLogProbMetric: 30.6598 - lr: 0.0010 - 32s/epoch - 161ms/step
Epoch 189/1000
2023-09-28 23:28:56.414 
Epoch 189/1000 
	 loss: 29.0189, MinusLogProbMetric: 29.0189, val_loss: 29.2909, val_MinusLogProbMetric: 29.2909

Epoch 189: val_loss did not improve from 28.93397
196/196 - 31s - loss: 29.0189 - MinusLogProbMetric: 29.0189 - val_loss: 29.2909 - val_MinusLogProbMetric: 29.2909 - lr: 0.0010 - 31s/epoch - 156ms/step
Epoch 190/1000
2023-09-28 23:29:30.720 
Epoch 190/1000 
	 loss: 29.0618, MinusLogProbMetric: 29.0618, val_loss: 29.2766, val_MinusLogProbMetric: 29.2766

Epoch 190: val_loss did not improve from 28.93397
196/196 - 34s - loss: 29.0618 - MinusLogProbMetric: 29.0618 - val_loss: 29.2766 - val_MinusLogProbMetric: 29.2766 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 191/1000
2023-09-28 23:30:05.149 
Epoch 191/1000 
	 loss: 28.9845, MinusLogProbMetric: 28.9845, val_loss: 29.4653, val_MinusLogProbMetric: 29.4653

Epoch 191: val_loss did not improve from 28.93397
196/196 - 34s - loss: 28.9845 - MinusLogProbMetric: 28.9845 - val_loss: 29.4653 - val_MinusLogProbMetric: 29.4653 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 192/1000
2023-09-28 23:30:39.456 
Epoch 192/1000 
	 loss: 29.0285, MinusLogProbMetric: 29.0285, val_loss: 29.0704, val_MinusLogProbMetric: 29.0704

Epoch 192: val_loss did not improve from 28.93397
196/196 - 34s - loss: 29.0285 - MinusLogProbMetric: 29.0285 - val_loss: 29.0704 - val_MinusLogProbMetric: 29.0704 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 193/1000
2023-09-28 23:31:13.736 
Epoch 193/1000 
	 loss: 29.0415, MinusLogProbMetric: 29.0415, val_loss: 29.1172, val_MinusLogProbMetric: 29.1172

Epoch 193: val_loss did not improve from 28.93397
196/196 - 34s - loss: 29.0415 - MinusLogProbMetric: 29.0415 - val_loss: 29.1172 - val_MinusLogProbMetric: 29.1172 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 194/1000
2023-09-28 23:31:46.370 
Epoch 194/1000 
	 loss: 28.9996, MinusLogProbMetric: 28.9996, val_loss: 28.8607, val_MinusLogProbMetric: 28.8607

Epoch 194: val_loss improved from 28.93397 to 28.86069, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_329/weights/best_weights.h5
196/196 - 33s - loss: 28.9996 - MinusLogProbMetric: 28.9996 - val_loss: 28.8607 - val_MinusLogProbMetric: 28.8607 - lr: 0.0010 - 33s/epoch - 169ms/step
Epoch 195/1000
2023-09-28 23:32:19.071 
Epoch 195/1000 
	 loss: 29.0379, MinusLogProbMetric: 29.0379, val_loss: 29.2889, val_MinusLogProbMetric: 29.2889

Epoch 195: val_loss did not improve from 28.86069
196/196 - 32s - loss: 29.0379 - MinusLogProbMetric: 29.0379 - val_loss: 29.2889 - val_MinusLogProbMetric: 29.2889 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 196/1000
2023-09-28 23:32:54.429 
Epoch 196/1000 
	 loss: 28.9940, MinusLogProbMetric: 28.9940, val_loss: 29.9431, val_MinusLogProbMetric: 29.9431

Epoch 196: val_loss did not improve from 28.86069
196/196 - 35s - loss: 28.9940 - MinusLogProbMetric: 28.9940 - val_loss: 29.9431 - val_MinusLogProbMetric: 29.9431 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 197/1000
2023-09-28 23:33:29.644 
Epoch 197/1000 
	 loss: 28.9970, MinusLogProbMetric: 28.9970, val_loss: 29.8813, val_MinusLogProbMetric: 29.8813

Epoch 197: val_loss did not improve from 28.86069
196/196 - 35s - loss: 28.9970 - MinusLogProbMetric: 28.9970 - val_loss: 29.8813 - val_MinusLogProbMetric: 29.8813 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 198/1000
2023-09-28 23:34:04.513 
Epoch 198/1000 
	 loss: 28.9668, MinusLogProbMetric: 28.9668, val_loss: 29.2417, val_MinusLogProbMetric: 29.2417

Epoch 198: val_loss did not improve from 28.86069
196/196 - 35s - loss: 28.9668 - MinusLogProbMetric: 28.9668 - val_loss: 29.2417 - val_MinusLogProbMetric: 29.2417 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 199/1000
2023-09-28 23:34:38.761 
Epoch 199/1000 
	 loss: 29.0192, MinusLogProbMetric: 29.0192, val_loss: 29.1895, val_MinusLogProbMetric: 29.1895

Epoch 199: val_loss did not improve from 28.86069
196/196 - 34s - loss: 29.0192 - MinusLogProbMetric: 29.0192 - val_loss: 29.1895 - val_MinusLogProbMetric: 29.1895 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 200/1000
2023-09-28 23:35:13.119 
Epoch 200/1000 
	 loss: 29.0245, MinusLogProbMetric: 29.0245, val_loss: 29.1170, val_MinusLogProbMetric: 29.1170

Epoch 200: val_loss did not improve from 28.86069
196/196 - 34s - loss: 29.0245 - MinusLogProbMetric: 29.0245 - val_loss: 29.1170 - val_MinusLogProbMetric: 29.1170 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 201/1000
2023-09-28 23:35:48.317 
Epoch 201/1000 
	 loss: 28.9597, MinusLogProbMetric: 28.9597, val_loss: 29.0664, val_MinusLogProbMetric: 29.0664

Epoch 201: val_loss did not improve from 28.86069
196/196 - 35s - loss: 28.9597 - MinusLogProbMetric: 28.9597 - val_loss: 29.0664 - val_MinusLogProbMetric: 29.0664 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 202/1000
2023-09-28 23:36:23.698 
Epoch 202/1000 
	 loss: 28.9558, MinusLogProbMetric: 28.9558, val_loss: 29.3965, val_MinusLogProbMetric: 29.3965

Epoch 202: val_loss did not improve from 28.86069
196/196 - 35s - loss: 28.9558 - MinusLogProbMetric: 28.9558 - val_loss: 29.3965 - val_MinusLogProbMetric: 29.3965 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 203/1000
2023-09-28 23:36:58.332 
Epoch 203/1000 
	 loss: 28.9928, MinusLogProbMetric: 28.9928, val_loss: 29.7984, val_MinusLogProbMetric: 29.7984

Epoch 203: val_loss did not improve from 28.86069
196/196 - 35s - loss: 28.9928 - MinusLogProbMetric: 28.9928 - val_loss: 29.7984 - val_MinusLogProbMetric: 29.7984 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 204/1000
2023-09-28 23:37:32.932 
Epoch 204/1000 
	 loss: 28.9846, MinusLogProbMetric: 28.9846, val_loss: 28.9014, val_MinusLogProbMetric: 28.9014

Epoch 204: val_loss did not improve from 28.86069
196/196 - 35s - loss: 28.9846 - MinusLogProbMetric: 28.9846 - val_loss: 28.9014 - val_MinusLogProbMetric: 28.9014 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 205/1000
2023-09-28 23:38:07.528 
Epoch 205/1000 
	 loss: 28.8990, MinusLogProbMetric: 28.8990, val_loss: 29.0372, val_MinusLogProbMetric: 29.0372

Epoch 205: val_loss did not improve from 28.86069
196/196 - 35s - loss: 28.8990 - MinusLogProbMetric: 28.8990 - val_loss: 29.0372 - val_MinusLogProbMetric: 29.0372 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 206/1000
2023-09-28 23:38:42.659 
Epoch 206/1000 
	 loss: 29.0174, MinusLogProbMetric: 29.0174, val_loss: 28.9532, val_MinusLogProbMetric: 28.9532

Epoch 206: val_loss did not improve from 28.86069
196/196 - 35s - loss: 29.0174 - MinusLogProbMetric: 29.0174 - val_loss: 28.9532 - val_MinusLogProbMetric: 28.9532 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 207/1000
2023-09-28 23:39:17.661 
Epoch 207/1000 
	 loss: 28.9210, MinusLogProbMetric: 28.9210, val_loss: 29.0938, val_MinusLogProbMetric: 29.0938

Epoch 207: val_loss did not improve from 28.86069
196/196 - 35s - loss: 28.9210 - MinusLogProbMetric: 28.9210 - val_loss: 29.0938 - val_MinusLogProbMetric: 29.0938 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 208/1000
2023-09-28 23:39:52.776 
Epoch 208/1000 
	 loss: 28.9485, MinusLogProbMetric: 28.9485, val_loss: 29.0903, val_MinusLogProbMetric: 29.0903

Epoch 208: val_loss did not improve from 28.86069
196/196 - 35s - loss: 28.9485 - MinusLogProbMetric: 28.9485 - val_loss: 29.0903 - val_MinusLogProbMetric: 29.0903 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 209/1000
2023-09-28 23:40:27.845 
Epoch 209/1000 
	 loss: 28.8911, MinusLogProbMetric: 28.8911, val_loss: 29.0378, val_MinusLogProbMetric: 29.0378

Epoch 209: val_loss did not improve from 28.86069
196/196 - 35s - loss: 28.8911 - MinusLogProbMetric: 28.8911 - val_loss: 29.0378 - val_MinusLogProbMetric: 29.0378 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 210/1000
2023-09-28 23:41:03.154 
Epoch 210/1000 
	 loss: 28.9713, MinusLogProbMetric: 28.9713, val_loss: 29.1039, val_MinusLogProbMetric: 29.1039

Epoch 210: val_loss did not improve from 28.86069
196/196 - 35s - loss: 28.9713 - MinusLogProbMetric: 28.9713 - val_loss: 29.1039 - val_MinusLogProbMetric: 29.1039 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 211/1000
2023-09-28 23:41:38.640 
Epoch 211/1000 
	 loss: 28.9372, MinusLogProbMetric: 28.9372, val_loss: 30.1224, val_MinusLogProbMetric: 30.1224

Epoch 211: val_loss did not improve from 28.86069
196/196 - 35s - loss: 28.9372 - MinusLogProbMetric: 28.9372 - val_loss: 30.1224 - val_MinusLogProbMetric: 30.1224 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 212/1000
2023-09-28 23:42:13.673 
Epoch 212/1000 
	 loss: 28.9906, MinusLogProbMetric: 28.9906, val_loss: 28.9813, val_MinusLogProbMetric: 28.9813

Epoch 212: val_loss did not improve from 28.86069
196/196 - 35s - loss: 28.9906 - MinusLogProbMetric: 28.9906 - val_loss: 28.9813 - val_MinusLogProbMetric: 28.9813 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 213/1000
2023-09-28 23:42:48.457 
Epoch 213/1000 
	 loss: 28.8973, MinusLogProbMetric: 28.8973, val_loss: 29.1189, val_MinusLogProbMetric: 29.1189

Epoch 213: val_loss did not improve from 28.86069
196/196 - 35s - loss: 28.8973 - MinusLogProbMetric: 28.8973 - val_loss: 29.1189 - val_MinusLogProbMetric: 29.1189 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 214/1000
2023-09-28 23:43:22.950 
Epoch 214/1000 
	 loss: 28.9008, MinusLogProbMetric: 28.9008, val_loss: 28.7914, val_MinusLogProbMetric: 28.7914

Epoch 214: val_loss improved from 28.86069 to 28.79137, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_329/weights/best_weights.h5
196/196 - 35s - loss: 28.9008 - MinusLogProbMetric: 28.9008 - val_loss: 28.7914 - val_MinusLogProbMetric: 28.7914 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 215/1000
2023-09-28 23:43:57.524 
Epoch 215/1000 
	 loss: 28.8394, MinusLogProbMetric: 28.8394, val_loss: 29.3817, val_MinusLogProbMetric: 29.3817

Epoch 215: val_loss did not improve from 28.79137
196/196 - 34s - loss: 28.8394 - MinusLogProbMetric: 28.8394 - val_loss: 29.3817 - val_MinusLogProbMetric: 29.3817 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 216/1000
2023-09-28 23:44:32.013 
Epoch 216/1000 
	 loss: 28.9414, MinusLogProbMetric: 28.9414, val_loss: 29.1966, val_MinusLogProbMetric: 29.1966

Epoch 216: val_loss did not improve from 28.79137
196/196 - 34s - loss: 28.9414 - MinusLogProbMetric: 28.9414 - val_loss: 29.1966 - val_MinusLogProbMetric: 29.1966 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 217/1000
2023-09-28 23:45:07.282 
Epoch 217/1000 
	 loss: 28.8557, MinusLogProbMetric: 28.8557, val_loss: 29.0781, val_MinusLogProbMetric: 29.0781

Epoch 217: val_loss did not improve from 28.79137
196/196 - 35s - loss: 28.8557 - MinusLogProbMetric: 28.8557 - val_loss: 29.0781 - val_MinusLogProbMetric: 29.0781 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 218/1000
2023-09-28 23:45:42.078 
Epoch 218/1000 
	 loss: 28.9439, MinusLogProbMetric: 28.9439, val_loss: 29.4214, val_MinusLogProbMetric: 29.4214

Epoch 218: val_loss did not improve from 28.79137
196/196 - 35s - loss: 28.9439 - MinusLogProbMetric: 28.9439 - val_loss: 29.4214 - val_MinusLogProbMetric: 29.4214 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 219/1000
2023-09-28 23:46:17.259 
Epoch 219/1000 
	 loss: 28.8146, MinusLogProbMetric: 28.8146, val_loss: 28.9535, val_MinusLogProbMetric: 28.9535

Epoch 219: val_loss did not improve from 28.79137
196/196 - 35s - loss: 28.8146 - MinusLogProbMetric: 28.8146 - val_loss: 28.9535 - val_MinusLogProbMetric: 28.9535 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 220/1000
2023-09-28 23:46:52.421 
Epoch 220/1000 
	 loss: 28.9461, MinusLogProbMetric: 28.9461, val_loss: 29.8010, val_MinusLogProbMetric: 29.8010

Epoch 220: val_loss did not improve from 28.79137
196/196 - 35s - loss: 28.9461 - MinusLogProbMetric: 28.9461 - val_loss: 29.8010 - val_MinusLogProbMetric: 29.8010 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 221/1000
2023-09-28 23:47:26.656 
Epoch 221/1000 
	 loss: 28.8566, MinusLogProbMetric: 28.8566, val_loss: 28.9331, val_MinusLogProbMetric: 28.9331

Epoch 221: val_loss did not improve from 28.79137
196/196 - 34s - loss: 28.8566 - MinusLogProbMetric: 28.8566 - val_loss: 28.9331 - val_MinusLogProbMetric: 28.9331 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 222/1000
2023-09-28 23:48:01.493 
Epoch 222/1000 
	 loss: 28.8426, MinusLogProbMetric: 28.8426, val_loss: 28.9674, val_MinusLogProbMetric: 28.9674

Epoch 222: val_loss did not improve from 28.79137
196/196 - 35s - loss: 28.8426 - MinusLogProbMetric: 28.8426 - val_loss: 28.9674 - val_MinusLogProbMetric: 28.9674 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 223/1000
2023-09-28 23:48:36.745 
Epoch 223/1000 
	 loss: 28.8083, MinusLogProbMetric: 28.8083, val_loss: 29.3608, val_MinusLogProbMetric: 29.3608

Epoch 223: val_loss did not improve from 28.79137
196/196 - 35s - loss: 28.8083 - MinusLogProbMetric: 28.8083 - val_loss: 29.3608 - val_MinusLogProbMetric: 29.3608 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 224/1000
2023-09-28 23:49:11.646 
Epoch 224/1000 
	 loss: 28.8584, MinusLogProbMetric: 28.8584, val_loss: 29.2298, val_MinusLogProbMetric: 29.2298

Epoch 224: val_loss did not improve from 28.79137
196/196 - 35s - loss: 28.8584 - MinusLogProbMetric: 28.8584 - val_loss: 29.2298 - val_MinusLogProbMetric: 29.2298 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 225/1000
2023-09-28 23:49:45.312 
Epoch 225/1000 
	 loss: 28.8200, MinusLogProbMetric: 28.8200, val_loss: 29.4385, val_MinusLogProbMetric: 29.4385

Epoch 225: val_loss did not improve from 28.79137
196/196 - 34s - loss: 28.8200 - MinusLogProbMetric: 28.8200 - val_loss: 29.4385 - val_MinusLogProbMetric: 29.4385 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 226/1000
2023-09-28 23:50:20.521 
Epoch 226/1000 
	 loss: 28.8805, MinusLogProbMetric: 28.8805, val_loss: 29.0315, val_MinusLogProbMetric: 29.0315

Epoch 226: val_loss did not improve from 28.79137
196/196 - 35s - loss: 28.8805 - MinusLogProbMetric: 28.8805 - val_loss: 29.0315 - val_MinusLogProbMetric: 29.0315 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 227/1000
2023-09-28 23:50:55.609 
Epoch 227/1000 
	 loss: 28.8375, MinusLogProbMetric: 28.8375, val_loss: 29.2928, val_MinusLogProbMetric: 29.2928

Epoch 227: val_loss did not improve from 28.79137
196/196 - 35s - loss: 28.8375 - MinusLogProbMetric: 28.8375 - val_loss: 29.2928 - val_MinusLogProbMetric: 29.2928 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 228/1000
2023-09-28 23:51:29.708 
Epoch 228/1000 
	 loss: 28.8193, MinusLogProbMetric: 28.8193, val_loss: 28.8077, val_MinusLogProbMetric: 28.8077

Epoch 228: val_loss did not improve from 28.79137
196/196 - 34s - loss: 28.8193 - MinusLogProbMetric: 28.8193 - val_loss: 28.8077 - val_MinusLogProbMetric: 28.8077 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 229/1000
2023-09-28 23:52:05.103 
Epoch 229/1000 
	 loss: 28.8416, MinusLogProbMetric: 28.8416, val_loss: 28.8573, val_MinusLogProbMetric: 28.8573

Epoch 229: val_loss did not improve from 28.79137
196/196 - 35s - loss: 28.8416 - MinusLogProbMetric: 28.8416 - val_loss: 28.8573 - val_MinusLogProbMetric: 28.8573 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 230/1000
2023-09-28 23:52:40.386 
Epoch 230/1000 
	 loss: 28.8392, MinusLogProbMetric: 28.8392, val_loss: 29.0523, val_MinusLogProbMetric: 29.0523

Epoch 230: val_loss did not improve from 28.79137
196/196 - 35s - loss: 28.8392 - MinusLogProbMetric: 28.8392 - val_loss: 29.0523 - val_MinusLogProbMetric: 29.0523 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 231/1000
2023-09-28 23:53:15.087 
Epoch 231/1000 
	 loss: 28.7913, MinusLogProbMetric: 28.7913, val_loss: 28.9552, val_MinusLogProbMetric: 28.9552

Epoch 231: val_loss did not improve from 28.79137
196/196 - 35s - loss: 28.7913 - MinusLogProbMetric: 28.7913 - val_loss: 28.9552 - val_MinusLogProbMetric: 28.9552 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 232/1000
2023-09-28 23:53:50.119 
Epoch 232/1000 
	 loss: 28.7847, MinusLogProbMetric: 28.7847, val_loss: 28.9413, val_MinusLogProbMetric: 28.9413

Epoch 232: val_loss did not improve from 28.79137
196/196 - 35s - loss: 28.7847 - MinusLogProbMetric: 28.7847 - val_loss: 28.9413 - val_MinusLogProbMetric: 28.9413 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 233/1000
2023-09-28 23:54:24.729 
Epoch 233/1000 
	 loss: 28.8523, MinusLogProbMetric: 28.8523, val_loss: 29.0043, val_MinusLogProbMetric: 29.0043

Epoch 233: val_loss did not improve from 28.79137
196/196 - 35s - loss: 28.8523 - MinusLogProbMetric: 28.8523 - val_loss: 29.0043 - val_MinusLogProbMetric: 29.0043 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 234/1000
2023-09-28 23:54:59.125 
Epoch 234/1000 
	 loss: 28.7375, MinusLogProbMetric: 28.7375, val_loss: 28.9141, val_MinusLogProbMetric: 28.9141

Epoch 234: val_loss did not improve from 28.79137
196/196 - 34s - loss: 28.7375 - MinusLogProbMetric: 28.7375 - val_loss: 28.9141 - val_MinusLogProbMetric: 28.9141 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 235/1000
2023-09-28 23:55:33.955 
Epoch 235/1000 
	 loss: 28.7988, MinusLogProbMetric: 28.7988, val_loss: 28.9018, val_MinusLogProbMetric: 28.9018

Epoch 235: val_loss did not improve from 28.79137
196/196 - 35s - loss: 28.7988 - MinusLogProbMetric: 28.7988 - val_loss: 28.9018 - val_MinusLogProbMetric: 28.9018 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 236/1000
2023-09-28 23:56:09.298 
Epoch 236/1000 
	 loss: 28.7791, MinusLogProbMetric: 28.7791, val_loss: 28.9741, val_MinusLogProbMetric: 28.9741

Epoch 236: val_loss did not improve from 28.79137
196/196 - 35s - loss: 28.7791 - MinusLogProbMetric: 28.7791 - val_loss: 28.9741 - val_MinusLogProbMetric: 28.9741 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 237/1000
2023-09-28 23:56:43.692 
Epoch 237/1000 
	 loss: 28.8471, MinusLogProbMetric: 28.8471, val_loss: 28.9733, val_MinusLogProbMetric: 28.9733

Epoch 237: val_loss did not improve from 28.79137
196/196 - 34s - loss: 28.8471 - MinusLogProbMetric: 28.8471 - val_loss: 28.9733 - val_MinusLogProbMetric: 28.9733 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 238/1000
2023-09-28 23:57:18.894 
Epoch 238/1000 
	 loss: 28.8030, MinusLogProbMetric: 28.8030, val_loss: 29.5751, val_MinusLogProbMetric: 29.5751

Epoch 238: val_loss did not improve from 28.79137
196/196 - 35s - loss: 28.8030 - MinusLogProbMetric: 28.8030 - val_loss: 29.5751 - val_MinusLogProbMetric: 29.5751 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 239/1000
2023-09-28 23:57:53.993 
Epoch 239/1000 
	 loss: 28.7894, MinusLogProbMetric: 28.7894, val_loss: 29.1732, val_MinusLogProbMetric: 29.1732

Epoch 239: val_loss did not improve from 28.79137
196/196 - 35s - loss: 28.7894 - MinusLogProbMetric: 28.7894 - val_loss: 29.1732 - val_MinusLogProbMetric: 29.1732 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 240/1000
2023-09-28 23:58:29.139 
Epoch 240/1000 
	 loss: 28.8160, MinusLogProbMetric: 28.8160, val_loss: 29.0085, val_MinusLogProbMetric: 29.0085

Epoch 240: val_loss did not improve from 28.79137
196/196 - 35s - loss: 28.8160 - MinusLogProbMetric: 28.8160 - val_loss: 29.0085 - val_MinusLogProbMetric: 29.0085 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 241/1000
2023-09-28 23:59:03.703 
Epoch 241/1000 
	 loss: 28.8173, MinusLogProbMetric: 28.8173, val_loss: 28.8853, val_MinusLogProbMetric: 28.8853

Epoch 241: val_loss did not improve from 28.79137
196/196 - 35s - loss: 28.8173 - MinusLogProbMetric: 28.8173 - val_loss: 28.8853 - val_MinusLogProbMetric: 28.8853 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 242/1000
2023-09-28 23:59:38.520 
Epoch 242/1000 
	 loss: 28.7296, MinusLogProbMetric: 28.7296, val_loss: 29.0781, val_MinusLogProbMetric: 29.0781

Epoch 242: val_loss did not improve from 28.79137
196/196 - 35s - loss: 28.7296 - MinusLogProbMetric: 28.7296 - val_loss: 29.0781 - val_MinusLogProbMetric: 29.0781 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 243/1000
2023-09-29 00:00:13.748 
Epoch 243/1000 
	 loss: 28.7420, MinusLogProbMetric: 28.7420, val_loss: 28.8645, val_MinusLogProbMetric: 28.8645

Epoch 243: val_loss did not improve from 28.79137
196/196 - 35s - loss: 28.7420 - MinusLogProbMetric: 28.7420 - val_loss: 28.8645 - val_MinusLogProbMetric: 28.8645 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 244/1000
2023-09-29 00:00:49.070 
Epoch 244/1000 
	 loss: 28.7733, MinusLogProbMetric: 28.7733, val_loss: 29.2861, val_MinusLogProbMetric: 29.2861

Epoch 244: val_loss did not improve from 28.79137
196/196 - 35s - loss: 28.7733 - MinusLogProbMetric: 28.7733 - val_loss: 29.2861 - val_MinusLogProbMetric: 29.2861 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 245/1000
2023-09-29 00:01:24.260 
Epoch 245/1000 
	 loss: 28.7574, MinusLogProbMetric: 28.7574, val_loss: 28.7298, val_MinusLogProbMetric: 28.7298

Epoch 245: val_loss improved from 28.79137 to 28.72977, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_329/weights/best_weights.h5
196/196 - 36s - loss: 28.7574 - MinusLogProbMetric: 28.7574 - val_loss: 28.7298 - val_MinusLogProbMetric: 28.7298 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 246/1000
2023-09-29 00:02:00.179 
Epoch 246/1000 
	 loss: 28.7387, MinusLogProbMetric: 28.7387, val_loss: 28.9194, val_MinusLogProbMetric: 28.9194

Epoch 246: val_loss did not improve from 28.72977
196/196 - 35s - loss: 28.7387 - MinusLogProbMetric: 28.7387 - val_loss: 28.9194 - val_MinusLogProbMetric: 28.9194 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 247/1000
2023-09-29 00:02:35.375 
Epoch 247/1000 
	 loss: 28.7094, MinusLogProbMetric: 28.7094, val_loss: 29.3510, val_MinusLogProbMetric: 29.3510

Epoch 247: val_loss did not improve from 28.72977
196/196 - 35s - loss: 28.7094 - MinusLogProbMetric: 28.7094 - val_loss: 29.3510 - val_MinusLogProbMetric: 29.3510 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 248/1000
2023-09-29 00:03:10.585 
Epoch 248/1000 
	 loss: 28.7891, MinusLogProbMetric: 28.7891, val_loss: 28.9131, val_MinusLogProbMetric: 28.9131

Epoch 248: val_loss did not improve from 28.72977
196/196 - 35s - loss: 28.7891 - MinusLogProbMetric: 28.7891 - val_loss: 28.9131 - val_MinusLogProbMetric: 28.9131 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 249/1000
2023-09-29 00:03:45.703 
Epoch 249/1000 
	 loss: 28.7438, MinusLogProbMetric: 28.7438, val_loss: 29.0118, val_MinusLogProbMetric: 29.0118

Epoch 249: val_loss did not improve from 28.72977
196/196 - 35s - loss: 28.7438 - MinusLogProbMetric: 28.7438 - val_loss: 29.0118 - val_MinusLogProbMetric: 29.0118 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 250/1000
2023-09-29 00:04:20.982 
Epoch 250/1000 
	 loss: 28.7664, MinusLogProbMetric: 28.7664, val_loss: 29.0020, val_MinusLogProbMetric: 29.0020

Epoch 250: val_loss did not improve from 28.72977
196/196 - 35s - loss: 28.7664 - MinusLogProbMetric: 28.7664 - val_loss: 29.0020 - val_MinusLogProbMetric: 29.0020 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 251/1000
2023-09-29 00:04:56.275 
Epoch 251/1000 
	 loss: 28.6885, MinusLogProbMetric: 28.6885, val_loss: 28.8473, val_MinusLogProbMetric: 28.8473

Epoch 251: val_loss did not improve from 28.72977
196/196 - 35s - loss: 28.6885 - MinusLogProbMetric: 28.6885 - val_loss: 28.8473 - val_MinusLogProbMetric: 28.8473 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 252/1000
2023-09-29 00:05:30.717 
Epoch 252/1000 
	 loss: 28.7400, MinusLogProbMetric: 28.7400, val_loss: 28.8348, val_MinusLogProbMetric: 28.8348

Epoch 252: val_loss did not improve from 28.72977
196/196 - 34s - loss: 28.7400 - MinusLogProbMetric: 28.7400 - val_loss: 28.8348 - val_MinusLogProbMetric: 28.8348 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 253/1000
2023-09-29 00:06:06.191 
Epoch 253/1000 
	 loss: 28.7295, MinusLogProbMetric: 28.7295, val_loss: 28.9018, val_MinusLogProbMetric: 28.9018

Epoch 253: val_loss did not improve from 28.72977
196/196 - 35s - loss: 28.7295 - MinusLogProbMetric: 28.7295 - val_loss: 28.9018 - val_MinusLogProbMetric: 28.9018 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 254/1000
2023-09-29 00:06:41.390 
Epoch 254/1000 
	 loss: 28.7384, MinusLogProbMetric: 28.7384, val_loss: 29.1917, val_MinusLogProbMetric: 29.1917

Epoch 254: val_loss did not improve from 28.72977
196/196 - 35s - loss: 28.7384 - MinusLogProbMetric: 28.7384 - val_loss: 29.1917 - val_MinusLogProbMetric: 29.1917 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 255/1000
2023-09-29 00:07:16.025 
Epoch 255/1000 
	 loss: 28.7614, MinusLogProbMetric: 28.7614, val_loss: 28.8237, val_MinusLogProbMetric: 28.8237

Epoch 255: val_loss did not improve from 28.72977
196/196 - 35s - loss: 28.7614 - MinusLogProbMetric: 28.7614 - val_loss: 28.8237 - val_MinusLogProbMetric: 28.8237 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 256/1000
2023-09-29 00:07:49.768 
Epoch 256/1000 
	 loss: 28.6511, MinusLogProbMetric: 28.6511, val_loss: 28.7503, val_MinusLogProbMetric: 28.7503

Epoch 256: val_loss did not improve from 28.72977
196/196 - 34s - loss: 28.6511 - MinusLogProbMetric: 28.6511 - val_loss: 28.7503 - val_MinusLogProbMetric: 28.7503 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 257/1000
2023-09-29 00:08:24.682 
Epoch 257/1000 
	 loss: 28.7608, MinusLogProbMetric: 28.7608, val_loss: 28.7842, val_MinusLogProbMetric: 28.7842

Epoch 257: val_loss did not improve from 28.72977
196/196 - 35s - loss: 28.7608 - MinusLogProbMetric: 28.7608 - val_loss: 28.7842 - val_MinusLogProbMetric: 28.7842 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 258/1000
2023-09-29 00:08:59.918 
Epoch 258/1000 
	 loss: 28.6651, MinusLogProbMetric: 28.6651, val_loss: 28.8711, val_MinusLogProbMetric: 28.8711

Epoch 258: val_loss did not improve from 28.72977
196/196 - 35s - loss: 28.6651 - MinusLogProbMetric: 28.6651 - val_loss: 28.8711 - val_MinusLogProbMetric: 28.8711 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 259/1000
2023-09-29 00:09:35.149 
Epoch 259/1000 
	 loss: 28.7669, MinusLogProbMetric: 28.7669, val_loss: 28.9545, val_MinusLogProbMetric: 28.9545

Epoch 259: val_loss did not improve from 28.72977
196/196 - 35s - loss: 28.7669 - MinusLogProbMetric: 28.7669 - val_loss: 28.9545 - val_MinusLogProbMetric: 28.9545 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 260/1000
2023-09-29 00:10:09.744 
Epoch 260/1000 
	 loss: 28.6692, MinusLogProbMetric: 28.6692, val_loss: 28.8044, val_MinusLogProbMetric: 28.8044

Epoch 260: val_loss did not improve from 28.72977
196/196 - 35s - loss: 28.6692 - MinusLogProbMetric: 28.6692 - val_loss: 28.8044 - val_MinusLogProbMetric: 28.8044 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 261/1000
2023-09-29 00:10:44.731 
Epoch 261/1000 
	 loss: 28.6710, MinusLogProbMetric: 28.6710, val_loss: 29.1837, val_MinusLogProbMetric: 29.1837

Epoch 261: val_loss did not improve from 28.72977
196/196 - 35s - loss: 28.6710 - MinusLogProbMetric: 28.6710 - val_loss: 29.1837 - val_MinusLogProbMetric: 29.1837 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 262/1000
2023-09-29 00:11:19.883 
Epoch 262/1000 
	 loss: 28.6872, MinusLogProbMetric: 28.6872, val_loss: 28.6737, val_MinusLogProbMetric: 28.6737

Epoch 262: val_loss improved from 28.72977 to 28.67370, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_329/weights/best_weights.h5
196/196 - 36s - loss: 28.6872 - MinusLogProbMetric: 28.6872 - val_loss: 28.6737 - val_MinusLogProbMetric: 28.6737 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 263/1000
2023-09-29 00:11:55.480 
Epoch 263/1000 
	 loss: 28.6983, MinusLogProbMetric: 28.6983, val_loss: 29.1182, val_MinusLogProbMetric: 29.1182

Epoch 263: val_loss did not improve from 28.67370
196/196 - 35s - loss: 28.6983 - MinusLogProbMetric: 28.6983 - val_loss: 29.1182 - val_MinusLogProbMetric: 29.1182 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 264/1000
2023-09-29 00:12:30.623 
Epoch 264/1000 
	 loss: 28.6815, MinusLogProbMetric: 28.6815, val_loss: 29.3627, val_MinusLogProbMetric: 29.3627

Epoch 264: val_loss did not improve from 28.67370
196/196 - 35s - loss: 28.6815 - MinusLogProbMetric: 28.6815 - val_loss: 29.3627 - val_MinusLogProbMetric: 29.3627 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 265/1000
2023-09-29 00:13:05.441 
Epoch 265/1000 
	 loss: 28.6298, MinusLogProbMetric: 28.6298, val_loss: 28.6782, val_MinusLogProbMetric: 28.6782

Epoch 265: val_loss did not improve from 28.67370
196/196 - 35s - loss: 28.6298 - MinusLogProbMetric: 28.6298 - val_loss: 28.6782 - val_MinusLogProbMetric: 28.6782 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 266/1000
2023-09-29 00:13:40.649 
Epoch 266/1000 
	 loss: 28.7383, MinusLogProbMetric: 28.7383, val_loss: 28.8574, val_MinusLogProbMetric: 28.8574

Epoch 266: val_loss did not improve from 28.67370
196/196 - 35s - loss: 28.7383 - MinusLogProbMetric: 28.7383 - val_loss: 28.8574 - val_MinusLogProbMetric: 28.8574 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 267/1000
2023-09-29 00:14:16.205 
Epoch 267/1000 
	 loss: 28.6858, MinusLogProbMetric: 28.6858, val_loss: 28.9757, val_MinusLogProbMetric: 28.9757

Epoch 267: val_loss did not improve from 28.67370
196/196 - 36s - loss: 28.6858 - MinusLogProbMetric: 28.6858 - val_loss: 28.9757 - val_MinusLogProbMetric: 28.9757 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 268/1000
2023-09-29 00:14:51.679 
Epoch 268/1000 
	 loss: 28.7019, MinusLogProbMetric: 28.7019, val_loss: 29.0001, val_MinusLogProbMetric: 29.0001

Epoch 268: val_loss did not improve from 28.67370
196/196 - 35s - loss: 28.7019 - MinusLogProbMetric: 28.7019 - val_loss: 29.0001 - val_MinusLogProbMetric: 29.0001 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 269/1000
2023-09-29 00:15:27.169 
Epoch 269/1000 
	 loss: 28.6471, MinusLogProbMetric: 28.6471, val_loss: 29.5423, val_MinusLogProbMetric: 29.5423

Epoch 269: val_loss did not improve from 28.67370
196/196 - 35s - loss: 28.6471 - MinusLogProbMetric: 28.6471 - val_loss: 29.5423 - val_MinusLogProbMetric: 29.5423 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 270/1000
2023-09-29 00:16:01.496 
Epoch 270/1000 
	 loss: 28.6541, MinusLogProbMetric: 28.6541, val_loss: 28.6761, val_MinusLogProbMetric: 28.6761

Epoch 270: val_loss did not improve from 28.67370
196/196 - 34s - loss: 28.6541 - MinusLogProbMetric: 28.6541 - val_loss: 28.6761 - val_MinusLogProbMetric: 28.6761 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 271/1000
2023-09-29 00:16:36.258 
Epoch 271/1000 
	 loss: 28.6768, MinusLogProbMetric: 28.6768, val_loss: 28.9449, val_MinusLogProbMetric: 28.9449

Epoch 271: val_loss did not improve from 28.67370
196/196 - 35s - loss: 28.6768 - MinusLogProbMetric: 28.6768 - val_loss: 28.9449 - val_MinusLogProbMetric: 28.9449 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 272/1000
2023-09-29 00:17:11.596 
Epoch 272/1000 
	 loss: 28.5985, MinusLogProbMetric: 28.5985, val_loss: 28.8905, val_MinusLogProbMetric: 28.8905

Epoch 272: val_loss did not improve from 28.67370
196/196 - 35s - loss: 28.5985 - MinusLogProbMetric: 28.5985 - val_loss: 28.8905 - val_MinusLogProbMetric: 28.8905 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 273/1000
2023-09-29 00:17:46.895 
Epoch 273/1000 
	 loss: 28.6187, MinusLogProbMetric: 28.6187, val_loss: 28.9956, val_MinusLogProbMetric: 28.9956

Epoch 273: val_loss did not improve from 28.67370
196/196 - 35s - loss: 28.6187 - MinusLogProbMetric: 28.6187 - val_loss: 28.9956 - val_MinusLogProbMetric: 28.9956 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 274/1000
2023-09-29 00:18:18.352 
Epoch 274/1000 
	 loss: 28.6512, MinusLogProbMetric: 28.6512, val_loss: 29.2860, val_MinusLogProbMetric: 29.2860

Epoch 274: val_loss did not improve from 28.67370
196/196 - 31s - loss: 28.6512 - MinusLogProbMetric: 28.6512 - val_loss: 29.2860 - val_MinusLogProbMetric: 29.2860 - lr: 0.0010 - 31s/epoch - 160ms/step
Epoch 275/1000
2023-09-29 00:18:54.048 
Epoch 275/1000 
	 loss: 28.7059, MinusLogProbMetric: 28.7059, val_loss: 28.6333, val_MinusLogProbMetric: 28.6333

Epoch 275: val_loss improved from 28.67370 to 28.63326, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_329/weights/best_weights.h5
196/196 - 36s - loss: 28.7059 - MinusLogProbMetric: 28.7059 - val_loss: 28.6333 - val_MinusLogProbMetric: 28.6333 - lr: 0.0010 - 36s/epoch - 184ms/step
Epoch 276/1000
2023-09-29 00:19:29.634 
Epoch 276/1000 
	 loss: 28.6816, MinusLogProbMetric: 28.6816, val_loss: 29.1595, val_MinusLogProbMetric: 29.1595

Epoch 276: val_loss did not improve from 28.63326
196/196 - 35s - loss: 28.6816 - MinusLogProbMetric: 28.6816 - val_loss: 29.1595 - val_MinusLogProbMetric: 29.1595 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 277/1000
2023-09-29 00:20:03.924 
Epoch 277/1000 
	 loss: 28.6081, MinusLogProbMetric: 28.6081, val_loss: 28.8993, val_MinusLogProbMetric: 28.8993

Epoch 277: val_loss did not improve from 28.63326
196/196 - 34s - loss: 28.6081 - MinusLogProbMetric: 28.6081 - val_loss: 28.8993 - val_MinusLogProbMetric: 28.8993 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 278/1000
2023-09-29 00:20:40.203 
Epoch 278/1000 
	 loss: 28.5883, MinusLogProbMetric: 28.5883, val_loss: 28.6298, val_MinusLogProbMetric: 28.6298

Epoch 278: val_loss improved from 28.63326 to 28.62985, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_329/weights/best_weights.h5
196/196 - 37s - loss: 28.5883 - MinusLogProbMetric: 28.5883 - val_loss: 28.6298 - val_MinusLogProbMetric: 28.6298 - lr: 0.0010 - 37s/epoch - 189ms/step
Epoch 279/1000
2023-09-29 00:21:15.984 
Epoch 279/1000 
	 loss: 28.6284, MinusLogProbMetric: 28.6284, val_loss: 29.8881, val_MinusLogProbMetric: 29.8881

Epoch 279: val_loss did not improve from 28.62985
196/196 - 35s - loss: 28.6284 - MinusLogProbMetric: 28.6284 - val_loss: 29.8881 - val_MinusLogProbMetric: 29.8881 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 280/1000
2023-09-29 00:21:49.827 
Epoch 280/1000 
	 loss: 28.6260, MinusLogProbMetric: 28.6260, val_loss: 28.7510, val_MinusLogProbMetric: 28.7510

Epoch 280: val_loss did not improve from 28.62985
196/196 - 34s - loss: 28.6260 - MinusLogProbMetric: 28.6260 - val_loss: 28.7510 - val_MinusLogProbMetric: 28.7510 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 281/1000
2023-09-29 00:22:25.286 
Epoch 281/1000 
	 loss: 28.6749, MinusLogProbMetric: 28.6749, val_loss: 28.5160, val_MinusLogProbMetric: 28.5160

Epoch 281: val_loss improved from 28.62985 to 28.51605, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_329/weights/best_weights.h5
196/196 - 36s - loss: 28.6749 - MinusLogProbMetric: 28.6749 - val_loss: 28.5160 - val_MinusLogProbMetric: 28.5160 - lr: 0.0010 - 36s/epoch - 184ms/step
Epoch 282/1000
2023-09-29 00:22:59.766 
Epoch 282/1000 
	 loss: 28.6002, MinusLogProbMetric: 28.6002, val_loss: 28.8457, val_MinusLogProbMetric: 28.8457

Epoch 282: val_loss did not improve from 28.51605
196/196 - 34s - loss: 28.6002 - MinusLogProbMetric: 28.6002 - val_loss: 28.8457 - val_MinusLogProbMetric: 28.8457 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 283/1000
2023-09-29 00:23:32.057 
Epoch 283/1000 
	 loss: 28.6045, MinusLogProbMetric: 28.6045, val_loss: 28.6586, val_MinusLogProbMetric: 28.6586

Epoch 283: val_loss did not improve from 28.51605
196/196 - 32s - loss: 28.6045 - MinusLogProbMetric: 28.6045 - val_loss: 28.6586 - val_MinusLogProbMetric: 28.6586 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 284/1000
2023-09-29 00:24:06.968 
Epoch 284/1000 
	 loss: 28.6378, MinusLogProbMetric: 28.6378, val_loss: 29.1791, val_MinusLogProbMetric: 29.1791

Epoch 284: val_loss did not improve from 28.51605
196/196 - 35s - loss: 28.6378 - MinusLogProbMetric: 28.6378 - val_loss: 29.1791 - val_MinusLogProbMetric: 29.1791 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 285/1000
2023-09-29 00:24:41.021 
Epoch 285/1000 
	 loss: 28.6221, MinusLogProbMetric: 28.6221, val_loss: 28.5233, val_MinusLogProbMetric: 28.5233

Epoch 285: val_loss did not improve from 28.51605
196/196 - 34s - loss: 28.6221 - MinusLogProbMetric: 28.6221 - val_loss: 28.5233 - val_MinusLogProbMetric: 28.5233 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 286/1000
2023-09-29 00:25:14.141 
Epoch 286/1000 
	 loss: 28.5671, MinusLogProbMetric: 28.5671, val_loss: 28.8567, val_MinusLogProbMetric: 28.8567

Epoch 286: val_loss did not improve from 28.51605
196/196 - 33s - loss: 28.5671 - MinusLogProbMetric: 28.5671 - val_loss: 28.8567 - val_MinusLogProbMetric: 28.8567 - lr: 0.0010 - 33s/epoch - 169ms/step
Epoch 287/1000
2023-09-29 00:25:47.948 
Epoch 287/1000 
	 loss: 28.6301, MinusLogProbMetric: 28.6301, val_loss: 28.6135, val_MinusLogProbMetric: 28.6135

Epoch 287: val_loss did not improve from 28.51605
196/196 - 34s - loss: 28.6301 - MinusLogProbMetric: 28.6301 - val_loss: 28.6135 - val_MinusLogProbMetric: 28.6135 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 288/1000
2023-09-29 00:26:23.208 
Epoch 288/1000 
	 loss: 28.6264, MinusLogProbMetric: 28.6264, val_loss: 28.9417, val_MinusLogProbMetric: 28.9417

Epoch 288: val_loss did not improve from 28.51605
196/196 - 35s - loss: 28.6264 - MinusLogProbMetric: 28.6264 - val_loss: 28.9417 - val_MinusLogProbMetric: 28.9417 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 289/1000
2023-09-29 00:26:56.434 
Epoch 289/1000 
	 loss: 28.5835, MinusLogProbMetric: 28.5835, val_loss: 28.5603, val_MinusLogProbMetric: 28.5603

Epoch 289: val_loss did not improve from 28.51605
196/196 - 33s - loss: 28.5835 - MinusLogProbMetric: 28.5835 - val_loss: 28.5603 - val_MinusLogProbMetric: 28.5603 - lr: 0.0010 - 33s/epoch - 169ms/step
Epoch 290/1000
2023-09-29 00:27:30.994 
Epoch 290/1000 
	 loss: 28.5239, MinusLogProbMetric: 28.5239, val_loss: 28.5152, val_MinusLogProbMetric: 28.5152

Epoch 290: val_loss improved from 28.51605 to 28.51524, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_329/weights/best_weights.h5
196/196 - 35s - loss: 28.5239 - MinusLogProbMetric: 28.5239 - val_loss: 28.5152 - val_MinusLogProbMetric: 28.5152 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 291/1000
2023-09-29 00:28:04.967 
Epoch 291/1000 
	 loss: 28.5843, MinusLogProbMetric: 28.5843, val_loss: 29.1332, val_MinusLogProbMetric: 29.1332

Epoch 291: val_loss did not improve from 28.51524
196/196 - 33s - loss: 28.5843 - MinusLogProbMetric: 28.5843 - val_loss: 29.1332 - val_MinusLogProbMetric: 29.1332 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 292/1000
2023-09-29 00:28:39.221 
Epoch 292/1000 
	 loss: 28.6293, MinusLogProbMetric: 28.6293, val_loss: 29.1442, val_MinusLogProbMetric: 29.1442

Epoch 292: val_loss did not improve from 28.51524
196/196 - 34s - loss: 28.6293 - MinusLogProbMetric: 28.6293 - val_loss: 29.1442 - val_MinusLogProbMetric: 29.1442 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 293/1000
2023-09-29 00:29:11.777 
Epoch 293/1000 
	 loss: 28.5400, MinusLogProbMetric: 28.5400, val_loss: 28.5569, val_MinusLogProbMetric: 28.5569

Epoch 293: val_loss did not improve from 28.51524
196/196 - 33s - loss: 28.5400 - MinusLogProbMetric: 28.5400 - val_loss: 28.5569 - val_MinusLogProbMetric: 28.5569 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 294/1000
2023-09-29 00:29:45.774 
Epoch 294/1000 
	 loss: 28.5682, MinusLogProbMetric: 28.5682, val_loss: 28.6979, val_MinusLogProbMetric: 28.6979

Epoch 294: val_loss did not improve from 28.51524
196/196 - 34s - loss: 28.5682 - MinusLogProbMetric: 28.5682 - val_loss: 28.6979 - val_MinusLogProbMetric: 28.6979 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 295/1000
2023-09-29 00:30:19.644 
Epoch 295/1000 
	 loss: 28.5883, MinusLogProbMetric: 28.5883, val_loss: 29.0061, val_MinusLogProbMetric: 29.0061

Epoch 295: val_loss did not improve from 28.51524
196/196 - 34s - loss: 28.5883 - MinusLogProbMetric: 28.5883 - val_loss: 29.0061 - val_MinusLogProbMetric: 29.0061 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 296/1000
2023-09-29 00:30:54.056 
Epoch 296/1000 
	 loss: 28.5381, MinusLogProbMetric: 28.5381, val_loss: 28.8431, val_MinusLogProbMetric: 28.8431

Epoch 296: val_loss did not improve from 28.51524
196/196 - 34s - loss: 28.5381 - MinusLogProbMetric: 28.5381 - val_loss: 28.8431 - val_MinusLogProbMetric: 28.8431 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 297/1000
2023-09-29 00:31:28.634 
Epoch 297/1000 
	 loss: 28.5980, MinusLogProbMetric: 28.5980, val_loss: 28.8875, val_MinusLogProbMetric: 28.8875

Epoch 297: val_loss did not improve from 28.51524
196/196 - 35s - loss: 28.5980 - MinusLogProbMetric: 28.5980 - val_loss: 28.8875 - val_MinusLogProbMetric: 28.8875 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 298/1000
2023-09-29 00:32:02.784 
Epoch 298/1000 
	 loss: 28.5499, MinusLogProbMetric: 28.5499, val_loss: 28.7890, val_MinusLogProbMetric: 28.7890

Epoch 298: val_loss did not improve from 28.51524
196/196 - 34s - loss: 28.5499 - MinusLogProbMetric: 28.5499 - val_loss: 28.7890 - val_MinusLogProbMetric: 28.7890 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 299/1000
2023-09-29 00:32:38.322 
Epoch 299/1000 
	 loss: 28.6602, MinusLogProbMetric: 28.6602, val_loss: 28.7814, val_MinusLogProbMetric: 28.7814

Epoch 299: val_loss did not improve from 28.51524
196/196 - 36s - loss: 28.6602 - MinusLogProbMetric: 28.6602 - val_loss: 28.7814 - val_MinusLogProbMetric: 28.7814 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 300/1000
2023-09-29 00:33:09.710 
Epoch 300/1000 
	 loss: 28.5719, MinusLogProbMetric: 28.5719, val_loss: 30.2247, val_MinusLogProbMetric: 30.2247

Epoch 300: val_loss did not improve from 28.51524
196/196 - 31s - loss: 28.5719 - MinusLogProbMetric: 28.5719 - val_loss: 30.2247 - val_MinusLogProbMetric: 30.2247 - lr: 0.0010 - 31s/epoch - 160ms/step
Epoch 301/1000
2023-09-29 00:33:43.257 
Epoch 301/1000 
	 loss: 28.6438, MinusLogProbMetric: 28.6438, val_loss: 28.6580, val_MinusLogProbMetric: 28.6580

Epoch 301: val_loss did not improve from 28.51524
196/196 - 34s - loss: 28.6438 - MinusLogProbMetric: 28.6438 - val_loss: 28.6580 - val_MinusLogProbMetric: 28.6580 - lr: 0.0010 - 34s/epoch - 171ms/step
Epoch 302/1000
2023-09-29 00:34:15.215 
Epoch 302/1000 
	 loss: 28.5578, MinusLogProbMetric: 28.5578, val_loss: 28.8602, val_MinusLogProbMetric: 28.8602

Epoch 302: val_loss did not improve from 28.51524
196/196 - 32s - loss: 28.5578 - MinusLogProbMetric: 28.5578 - val_loss: 28.8602 - val_MinusLogProbMetric: 28.8602 - lr: 0.0010 - 32s/epoch - 163ms/step
Epoch 303/1000
2023-09-29 00:34:50.688 
Epoch 303/1000 
	 loss: 28.5396, MinusLogProbMetric: 28.5396, val_loss: 29.1578, val_MinusLogProbMetric: 29.1578

Epoch 303: val_loss did not improve from 28.51524
196/196 - 35s - loss: 28.5396 - MinusLogProbMetric: 28.5396 - val_loss: 29.1578 - val_MinusLogProbMetric: 29.1578 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 304/1000
2023-09-29 00:35:25.983 
Epoch 304/1000 
	 loss: 28.5456, MinusLogProbMetric: 28.5456, val_loss: 28.7163, val_MinusLogProbMetric: 28.7163

Epoch 304: val_loss did not improve from 28.51524
196/196 - 35s - loss: 28.5456 - MinusLogProbMetric: 28.5456 - val_loss: 28.7163 - val_MinusLogProbMetric: 28.7163 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 305/1000
2023-09-29 00:36:01.745 
Epoch 305/1000 
	 loss: 28.5654, MinusLogProbMetric: 28.5654, val_loss: 29.0867, val_MinusLogProbMetric: 29.0867

Epoch 305: val_loss did not improve from 28.51524
196/196 - 36s - loss: 28.5654 - MinusLogProbMetric: 28.5654 - val_loss: 29.0867 - val_MinusLogProbMetric: 29.0867 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 306/1000
2023-09-29 00:36:37.074 
Epoch 306/1000 
	 loss: 28.5224, MinusLogProbMetric: 28.5224, val_loss: 29.6879, val_MinusLogProbMetric: 29.6879

Epoch 306: val_loss did not improve from 28.51524
196/196 - 35s - loss: 28.5224 - MinusLogProbMetric: 28.5224 - val_loss: 29.6879 - val_MinusLogProbMetric: 29.6879 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 307/1000
2023-09-29 00:37:11.742 
Epoch 307/1000 
	 loss: 28.5426, MinusLogProbMetric: 28.5426, val_loss: 28.6400, val_MinusLogProbMetric: 28.6400

Epoch 307: val_loss did not improve from 28.51524
196/196 - 35s - loss: 28.5426 - MinusLogProbMetric: 28.5426 - val_loss: 28.6400 - val_MinusLogProbMetric: 28.6400 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 308/1000
2023-09-29 00:37:46.508 
Epoch 308/1000 
	 loss: 28.4882, MinusLogProbMetric: 28.4882, val_loss: 29.5652, val_MinusLogProbMetric: 29.5652

Epoch 308: val_loss did not improve from 28.51524
196/196 - 35s - loss: 28.4882 - MinusLogProbMetric: 28.4882 - val_loss: 29.5652 - val_MinusLogProbMetric: 29.5652 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 309/1000
2023-09-29 00:38:18.175 
Epoch 309/1000 
	 loss: 28.5680, MinusLogProbMetric: 28.5680, val_loss: 29.0064, val_MinusLogProbMetric: 29.0064

Epoch 309: val_loss did not improve from 28.51524
196/196 - 32s - loss: 28.5680 - MinusLogProbMetric: 28.5680 - val_loss: 29.0064 - val_MinusLogProbMetric: 29.0064 - lr: 0.0010 - 32s/epoch - 162ms/step
Epoch 310/1000
2023-09-29 00:38:51.399 
Epoch 310/1000 
	 loss: 28.5731, MinusLogProbMetric: 28.5731, val_loss: 29.3163, val_MinusLogProbMetric: 29.3163

Epoch 310: val_loss did not improve from 28.51524
196/196 - 33s - loss: 28.5731 - MinusLogProbMetric: 28.5731 - val_loss: 29.3163 - val_MinusLogProbMetric: 29.3163 - lr: 0.0010 - 33s/epoch - 169ms/step
Epoch 311/1000
2023-09-29 00:39:25.527 
Epoch 311/1000 
	 loss: 28.5105, MinusLogProbMetric: 28.5105, val_loss: 28.9544, val_MinusLogProbMetric: 28.9544

Epoch 311: val_loss did not improve from 28.51524
196/196 - 34s - loss: 28.5105 - MinusLogProbMetric: 28.5105 - val_loss: 28.9544 - val_MinusLogProbMetric: 28.9544 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 312/1000
2023-09-29 00:40:01.384 
Epoch 312/1000 
	 loss: 28.4663, MinusLogProbMetric: 28.4663, val_loss: 28.9494, val_MinusLogProbMetric: 28.9494

Epoch 312: val_loss did not improve from 28.51524
196/196 - 36s - loss: 28.4663 - MinusLogProbMetric: 28.4663 - val_loss: 28.9494 - val_MinusLogProbMetric: 28.9494 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 313/1000
2023-09-29 00:40:35.910 
Epoch 313/1000 
	 loss: 28.5459, MinusLogProbMetric: 28.5459, val_loss: 28.6411, val_MinusLogProbMetric: 28.6411

Epoch 313: val_loss did not improve from 28.51524
196/196 - 35s - loss: 28.5459 - MinusLogProbMetric: 28.5459 - val_loss: 28.6411 - val_MinusLogProbMetric: 28.6411 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 314/1000
2023-09-29 00:41:11.035 
Epoch 314/1000 
	 loss: 28.5168, MinusLogProbMetric: 28.5168, val_loss: 29.2223, val_MinusLogProbMetric: 29.2223

Epoch 314: val_loss did not improve from 28.51524
196/196 - 35s - loss: 28.5168 - MinusLogProbMetric: 28.5168 - val_loss: 29.2223 - val_MinusLogProbMetric: 29.2223 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 315/1000
2023-09-29 00:41:43.385 
Epoch 315/1000 
	 loss: 28.5617, MinusLogProbMetric: 28.5617, val_loss: 28.8306, val_MinusLogProbMetric: 28.8306

Epoch 315: val_loss did not improve from 28.51524
196/196 - 32s - loss: 28.5617 - MinusLogProbMetric: 28.5617 - val_loss: 28.8306 - val_MinusLogProbMetric: 28.8306 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 316/1000
2023-09-29 00:42:19.227 
Epoch 316/1000 
	 loss: 28.4829, MinusLogProbMetric: 28.4829, val_loss: 28.5382, val_MinusLogProbMetric: 28.5382

Epoch 316: val_loss did not improve from 28.51524
196/196 - 36s - loss: 28.4829 - MinusLogProbMetric: 28.4829 - val_loss: 28.5382 - val_MinusLogProbMetric: 28.5382 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 317/1000
2023-09-29 00:42:52.641 
Epoch 317/1000 
	 loss: 28.4706, MinusLogProbMetric: 28.4706, val_loss: 28.7323, val_MinusLogProbMetric: 28.7323

Epoch 317: val_loss did not improve from 28.51524
196/196 - 33s - loss: 28.4706 - MinusLogProbMetric: 28.4706 - val_loss: 28.7323 - val_MinusLogProbMetric: 28.7323 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 318/1000
2023-09-29 00:43:27.919 
Epoch 318/1000 
	 loss: 28.4871, MinusLogProbMetric: 28.4871, val_loss: 28.4066, val_MinusLogProbMetric: 28.4066

Epoch 318: val_loss improved from 28.51524 to 28.40662, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_329/weights/best_weights.h5
196/196 - 36s - loss: 28.4871 - MinusLogProbMetric: 28.4871 - val_loss: 28.4066 - val_MinusLogProbMetric: 28.4066 - lr: 0.0010 - 36s/epoch - 184ms/step
Epoch 319/1000
2023-09-29 00:44:02.295 
Epoch 319/1000 
	 loss: 28.5521, MinusLogProbMetric: 28.5521, val_loss: 28.8144, val_MinusLogProbMetric: 28.8144

Epoch 319: val_loss did not improve from 28.40662
196/196 - 34s - loss: 28.5521 - MinusLogProbMetric: 28.5521 - val_loss: 28.8144 - val_MinusLogProbMetric: 28.8144 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 320/1000
2023-09-29 00:44:34.832 
Epoch 320/1000 
	 loss: 28.5167, MinusLogProbMetric: 28.5167, val_loss: 28.6202, val_MinusLogProbMetric: 28.6202

Epoch 320: val_loss did not improve from 28.40662
196/196 - 33s - loss: 28.5167 - MinusLogProbMetric: 28.5167 - val_loss: 28.6202 - val_MinusLogProbMetric: 28.6202 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 321/1000
2023-09-29 00:45:09.268 
Epoch 321/1000 
	 loss: 28.5136, MinusLogProbMetric: 28.5136, val_loss: 29.1195, val_MinusLogProbMetric: 29.1195

Epoch 321: val_loss did not improve from 28.40662
196/196 - 34s - loss: 28.5136 - MinusLogProbMetric: 28.5136 - val_loss: 29.1195 - val_MinusLogProbMetric: 29.1195 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 322/1000
2023-09-29 00:45:45.623 
Epoch 322/1000 
	 loss: 28.4711, MinusLogProbMetric: 28.4711, val_loss: 29.7258, val_MinusLogProbMetric: 29.7258

Epoch 322: val_loss did not improve from 28.40662
196/196 - 36s - loss: 28.4711 - MinusLogProbMetric: 28.4711 - val_loss: 29.7258 - val_MinusLogProbMetric: 29.7258 - lr: 0.0010 - 36s/epoch - 185ms/step
Epoch 323/1000
2023-09-29 00:46:21.635 
Epoch 323/1000 
	 loss: 28.5205, MinusLogProbMetric: 28.5205, val_loss: 28.6199, val_MinusLogProbMetric: 28.6199

Epoch 323: val_loss did not improve from 28.40662
196/196 - 36s - loss: 28.5205 - MinusLogProbMetric: 28.5205 - val_loss: 28.6199 - val_MinusLogProbMetric: 28.6199 - lr: 0.0010 - 36s/epoch - 184ms/step
Epoch 324/1000
2023-09-29 00:46:57.855 
Epoch 324/1000 
	 loss: 28.4917, MinusLogProbMetric: 28.4917, val_loss: 28.5940, val_MinusLogProbMetric: 28.5940

Epoch 324: val_loss did not improve from 28.40662
196/196 - 36s - loss: 28.4917 - MinusLogProbMetric: 28.4917 - val_loss: 28.5940 - val_MinusLogProbMetric: 28.5940 - lr: 0.0010 - 36s/epoch - 185ms/step
Epoch 325/1000
2023-09-29 00:47:33.730 
Epoch 325/1000 
	 loss: 28.5003, MinusLogProbMetric: 28.5003, val_loss: 28.8464, val_MinusLogProbMetric: 28.8464

Epoch 325: val_loss did not improve from 28.40662
196/196 - 36s - loss: 28.5003 - MinusLogProbMetric: 28.5003 - val_loss: 28.8464 - val_MinusLogProbMetric: 28.8464 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 326/1000
2023-09-29 00:48:09.897 
Epoch 326/1000 
	 loss: 28.4870, MinusLogProbMetric: 28.4870, val_loss: 28.6135, val_MinusLogProbMetric: 28.6135

Epoch 326: val_loss did not improve from 28.40662
196/196 - 36s - loss: 28.4870 - MinusLogProbMetric: 28.4870 - val_loss: 28.6135 - val_MinusLogProbMetric: 28.6135 - lr: 0.0010 - 36s/epoch - 185ms/step
Epoch 327/1000
2023-09-29 00:48:45.985 
Epoch 327/1000 
	 loss: 28.4628, MinusLogProbMetric: 28.4628, val_loss: 28.9476, val_MinusLogProbMetric: 28.9476

Epoch 327: val_loss did not improve from 28.40662
196/196 - 36s - loss: 28.4628 - MinusLogProbMetric: 28.4628 - val_loss: 28.9476 - val_MinusLogProbMetric: 28.9476 - lr: 0.0010 - 36s/epoch - 184ms/step
Epoch 328/1000
2023-09-29 00:49:22.045 
Epoch 328/1000 
	 loss: 28.5021, MinusLogProbMetric: 28.5021, val_loss: 28.5624, val_MinusLogProbMetric: 28.5624

Epoch 328: val_loss did not improve from 28.40662
196/196 - 36s - loss: 28.5021 - MinusLogProbMetric: 28.5021 - val_loss: 28.5624 - val_MinusLogProbMetric: 28.5624 - lr: 0.0010 - 36s/epoch - 184ms/step
Epoch 329/1000
2023-09-29 00:49:58.335 
Epoch 329/1000 
	 loss: 28.4668, MinusLogProbMetric: 28.4668, val_loss: 28.5795, val_MinusLogProbMetric: 28.5795

Epoch 329: val_loss did not improve from 28.40662
196/196 - 36s - loss: 28.4668 - MinusLogProbMetric: 28.4668 - val_loss: 28.5795 - val_MinusLogProbMetric: 28.5795 - lr: 0.0010 - 36s/epoch - 185ms/step
Epoch 330/1000
2023-09-29 00:50:33.625 
Epoch 330/1000 
	 loss: 28.5041, MinusLogProbMetric: 28.5041, val_loss: 29.3150, val_MinusLogProbMetric: 29.3150

Epoch 330: val_loss did not improve from 28.40662
196/196 - 35s - loss: 28.5041 - MinusLogProbMetric: 28.5041 - val_loss: 29.3150 - val_MinusLogProbMetric: 29.3150 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 331/1000
2023-09-29 00:51:09.509 
Epoch 331/1000 
	 loss: 28.4552, MinusLogProbMetric: 28.4552, val_loss: 28.6721, val_MinusLogProbMetric: 28.6721

Epoch 331: val_loss did not improve from 28.40662
196/196 - 36s - loss: 28.4552 - MinusLogProbMetric: 28.4552 - val_loss: 28.6721 - val_MinusLogProbMetric: 28.6721 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 332/1000
2023-09-29 00:51:44.549 
Epoch 332/1000 
	 loss: 28.4187, MinusLogProbMetric: 28.4187, val_loss: 28.6196, val_MinusLogProbMetric: 28.6196

Epoch 332: val_loss did not improve from 28.40662
196/196 - 35s - loss: 28.4187 - MinusLogProbMetric: 28.4187 - val_loss: 28.6196 - val_MinusLogProbMetric: 28.6196 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 333/1000
2023-09-29 00:52:19.563 
Epoch 333/1000 
	 loss: 28.3821, MinusLogProbMetric: 28.3821, val_loss: 28.9525, val_MinusLogProbMetric: 28.9525

Epoch 333: val_loss did not improve from 28.40662
196/196 - 35s - loss: 28.3821 - MinusLogProbMetric: 28.3821 - val_loss: 28.9525 - val_MinusLogProbMetric: 28.9525 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 334/1000
2023-09-29 00:52:54.408 
Epoch 334/1000 
	 loss: 28.4591, MinusLogProbMetric: 28.4591, val_loss: 28.9215, val_MinusLogProbMetric: 28.9215

Epoch 334: val_loss did not improve from 28.40662
196/196 - 35s - loss: 28.4591 - MinusLogProbMetric: 28.4591 - val_loss: 28.9215 - val_MinusLogProbMetric: 28.9215 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 335/1000
2023-09-29 00:53:30.895 
Epoch 335/1000 
	 loss: 28.4466, MinusLogProbMetric: 28.4466, val_loss: 29.1691, val_MinusLogProbMetric: 29.1691

Epoch 335: val_loss did not improve from 28.40662
196/196 - 36s - loss: 28.4466 - MinusLogProbMetric: 28.4466 - val_loss: 29.1691 - val_MinusLogProbMetric: 29.1691 - lr: 0.0010 - 36s/epoch - 186ms/step
Epoch 336/1000
2023-09-29 00:54:05.138 
Epoch 336/1000 
	 loss: 28.4815, MinusLogProbMetric: 28.4815, val_loss: 28.6035, val_MinusLogProbMetric: 28.6035

Epoch 336: val_loss did not improve from 28.40662
196/196 - 34s - loss: 28.4815 - MinusLogProbMetric: 28.4815 - val_loss: 28.6035 - val_MinusLogProbMetric: 28.6035 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 337/1000
2023-09-29 00:54:41.196 
Epoch 337/1000 
	 loss: 28.4759, MinusLogProbMetric: 28.4759, val_loss: 28.7912, val_MinusLogProbMetric: 28.7912

Epoch 337: val_loss did not improve from 28.40662
196/196 - 36s - loss: 28.4759 - MinusLogProbMetric: 28.4759 - val_loss: 28.7912 - val_MinusLogProbMetric: 28.7912 - lr: 0.0010 - 36s/epoch - 184ms/step
Epoch 338/1000
2023-09-29 00:55:15.177 
Epoch 338/1000 
	 loss: 28.5154, MinusLogProbMetric: 28.5154, val_loss: 28.5467, val_MinusLogProbMetric: 28.5467

Epoch 338: val_loss did not improve from 28.40662
196/196 - 34s - loss: 28.5154 - MinusLogProbMetric: 28.5154 - val_loss: 28.5467 - val_MinusLogProbMetric: 28.5467 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 339/1000
2023-09-29 00:55:49.965 
Epoch 339/1000 
	 loss: 28.4321, MinusLogProbMetric: 28.4321, val_loss: 28.7218, val_MinusLogProbMetric: 28.7218

Epoch 339: val_loss did not improve from 28.40662
196/196 - 35s - loss: 28.4321 - MinusLogProbMetric: 28.4321 - val_loss: 28.7218 - val_MinusLogProbMetric: 28.7218 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 340/1000
2023-09-29 00:56:23.648 
Epoch 340/1000 
	 loss: 28.4062, MinusLogProbMetric: 28.4062, val_loss: 28.8057, val_MinusLogProbMetric: 28.8057

Epoch 340: val_loss did not improve from 28.40662
196/196 - 34s - loss: 28.4062 - MinusLogProbMetric: 28.4062 - val_loss: 28.8057 - val_MinusLogProbMetric: 28.8057 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 341/1000
2023-09-29 00:57:00.616 
Epoch 341/1000 
	 loss: 28.4942, MinusLogProbMetric: 28.4942, val_loss: 28.6950, val_MinusLogProbMetric: 28.6950

Epoch 341: val_loss did not improve from 28.40662
196/196 - 37s - loss: 28.4942 - MinusLogProbMetric: 28.4942 - val_loss: 28.6950 - val_MinusLogProbMetric: 28.6950 - lr: 0.0010 - 37s/epoch - 189ms/step
Epoch 342/1000
2023-09-29 00:57:35.613 
Epoch 342/1000 
	 loss: 28.4592, MinusLogProbMetric: 28.4592, val_loss: 28.7163, val_MinusLogProbMetric: 28.7163

Epoch 342: val_loss did not improve from 28.40662
196/196 - 35s - loss: 28.4592 - MinusLogProbMetric: 28.4592 - val_loss: 28.7163 - val_MinusLogProbMetric: 28.7163 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 343/1000
2023-09-29 00:58:10.257 
Epoch 343/1000 
	 loss: 28.4208, MinusLogProbMetric: 28.4208, val_loss: 28.7333, val_MinusLogProbMetric: 28.7333

Epoch 343: val_loss did not improve from 28.40662
196/196 - 35s - loss: 28.4208 - MinusLogProbMetric: 28.4208 - val_loss: 28.7333 - val_MinusLogProbMetric: 28.7333 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 344/1000
2023-09-29 00:58:44.830 
Epoch 344/1000 
	 loss: 28.4355, MinusLogProbMetric: 28.4355, val_loss: 28.5218, val_MinusLogProbMetric: 28.5218

Epoch 344: val_loss did not improve from 28.40662
196/196 - 35s - loss: 28.4355 - MinusLogProbMetric: 28.4355 - val_loss: 28.5218 - val_MinusLogProbMetric: 28.5218 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 345/1000
2023-09-29 00:59:18.076 
Epoch 345/1000 
	 loss: 28.4426, MinusLogProbMetric: 28.4426, val_loss: 28.4618, val_MinusLogProbMetric: 28.4618

Epoch 345: val_loss did not improve from 28.40662
196/196 - 33s - loss: 28.4426 - MinusLogProbMetric: 28.4426 - val_loss: 28.4618 - val_MinusLogProbMetric: 28.4618 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 346/1000
2023-09-29 00:59:48.978 
Epoch 346/1000 
	 loss: 28.4383, MinusLogProbMetric: 28.4383, val_loss: 28.7774, val_MinusLogProbMetric: 28.7774

Epoch 346: val_loss did not improve from 28.40662
196/196 - 31s - loss: 28.4383 - MinusLogProbMetric: 28.4383 - val_loss: 28.7774 - val_MinusLogProbMetric: 28.7774 - lr: 0.0010 - 31s/epoch - 158ms/step
Epoch 347/1000
2023-09-29 01:00:19.910 
Epoch 347/1000 
	 loss: 28.4632, MinusLogProbMetric: 28.4632, val_loss: 28.6598, val_MinusLogProbMetric: 28.6598

Epoch 347: val_loss did not improve from 28.40662
196/196 - 31s - loss: 28.4632 - MinusLogProbMetric: 28.4632 - val_loss: 28.6598 - val_MinusLogProbMetric: 28.6598 - lr: 0.0010 - 31s/epoch - 158ms/step
Epoch 348/1000
2023-09-29 01:00:55.545 
Epoch 348/1000 
	 loss: 28.3539, MinusLogProbMetric: 28.3539, val_loss: 28.6829, val_MinusLogProbMetric: 28.6829

Epoch 348: val_loss did not improve from 28.40662
196/196 - 36s - loss: 28.3539 - MinusLogProbMetric: 28.3539 - val_loss: 28.6829 - val_MinusLogProbMetric: 28.6829 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 349/1000
2023-09-29 01:01:31.473 
Epoch 349/1000 
	 loss: 28.4425, MinusLogProbMetric: 28.4425, val_loss: 28.8368, val_MinusLogProbMetric: 28.8368

Epoch 349: val_loss did not improve from 28.40662
196/196 - 36s - loss: 28.4425 - MinusLogProbMetric: 28.4425 - val_loss: 28.8368 - val_MinusLogProbMetric: 28.8368 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 350/1000
2023-09-29 01:02:05.492 
Epoch 350/1000 
	 loss: 28.4065, MinusLogProbMetric: 28.4065, val_loss: 28.8024, val_MinusLogProbMetric: 28.8024

Epoch 350: val_loss did not improve from 28.40662
196/196 - 34s - loss: 28.4065 - MinusLogProbMetric: 28.4065 - val_loss: 28.8024 - val_MinusLogProbMetric: 28.8024 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 351/1000
2023-09-29 01:02:41.392 
Epoch 351/1000 
	 loss: 28.4690, MinusLogProbMetric: 28.4690, val_loss: 28.7333, val_MinusLogProbMetric: 28.7333

Epoch 351: val_loss did not improve from 28.40662
196/196 - 36s - loss: 28.4690 - MinusLogProbMetric: 28.4690 - val_loss: 28.7333 - val_MinusLogProbMetric: 28.7333 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 352/1000
2023-09-29 01:03:17.204 
Epoch 352/1000 
	 loss: 28.3976, MinusLogProbMetric: 28.3976, val_loss: 28.8203, val_MinusLogProbMetric: 28.8203

Epoch 352: val_loss did not improve from 28.40662
196/196 - 36s - loss: 28.3976 - MinusLogProbMetric: 28.3976 - val_loss: 28.8203 - val_MinusLogProbMetric: 28.8203 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 353/1000
2023-09-29 01:03:52.390 
Epoch 353/1000 
	 loss: 28.4281, MinusLogProbMetric: 28.4281, val_loss: 28.6692, val_MinusLogProbMetric: 28.6692

Epoch 353: val_loss did not improve from 28.40662
196/196 - 35s - loss: 28.4281 - MinusLogProbMetric: 28.4281 - val_loss: 28.6692 - val_MinusLogProbMetric: 28.6692 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 354/1000
2023-09-29 01:04:27.903 
Epoch 354/1000 
	 loss: 28.4636, MinusLogProbMetric: 28.4636, val_loss: 28.6469, val_MinusLogProbMetric: 28.6469

Epoch 354: val_loss did not improve from 28.40662
196/196 - 36s - loss: 28.4636 - MinusLogProbMetric: 28.4636 - val_loss: 28.6469 - val_MinusLogProbMetric: 28.6469 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 355/1000
2023-09-29 01:05:05.150 
Epoch 355/1000 
	 loss: 28.3842, MinusLogProbMetric: 28.3842, val_loss: 28.6647, val_MinusLogProbMetric: 28.6647

Epoch 355: val_loss did not improve from 28.40662
196/196 - 37s - loss: 28.3842 - MinusLogProbMetric: 28.3842 - val_loss: 28.6647 - val_MinusLogProbMetric: 28.6647 - lr: 0.0010 - 37s/epoch - 190ms/step
Epoch 356/1000
2023-09-29 01:05:42.334 
Epoch 356/1000 
	 loss: 28.3639, MinusLogProbMetric: 28.3639, val_loss: 28.6823, val_MinusLogProbMetric: 28.6823

Epoch 356: val_loss did not improve from 28.40662
196/196 - 37s - loss: 28.3639 - MinusLogProbMetric: 28.3639 - val_loss: 28.6823 - val_MinusLogProbMetric: 28.6823 - lr: 0.0010 - 37s/epoch - 190ms/step
Epoch 357/1000
2023-09-29 01:06:17.313 
Epoch 357/1000 
	 loss: 28.3666, MinusLogProbMetric: 28.3666, val_loss: 28.5232, val_MinusLogProbMetric: 28.5232

Epoch 357: val_loss did not improve from 28.40662
196/196 - 35s - loss: 28.3666 - MinusLogProbMetric: 28.3666 - val_loss: 28.5232 - val_MinusLogProbMetric: 28.5232 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 358/1000
2023-09-29 01:06:54.826 
Epoch 358/1000 
	 loss: 28.4416, MinusLogProbMetric: 28.4416, val_loss: 28.7285, val_MinusLogProbMetric: 28.7285

Epoch 358: val_loss did not improve from 28.40662
196/196 - 38s - loss: 28.4416 - MinusLogProbMetric: 28.4416 - val_loss: 28.7285 - val_MinusLogProbMetric: 28.7285 - lr: 0.0010 - 38s/epoch - 191ms/step
Epoch 359/1000
2023-09-29 01:07:29.388 
Epoch 359/1000 
	 loss: 28.4008, MinusLogProbMetric: 28.4008, val_loss: 28.9213, val_MinusLogProbMetric: 28.9213

Epoch 359: val_loss did not improve from 28.40662
196/196 - 35s - loss: 28.4008 - MinusLogProbMetric: 28.4008 - val_loss: 28.9213 - val_MinusLogProbMetric: 28.9213 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 360/1000
2023-09-29 01:08:03.553 
Epoch 360/1000 
	 loss: 28.4119, MinusLogProbMetric: 28.4119, val_loss: 28.7640, val_MinusLogProbMetric: 28.7640

Epoch 360: val_loss did not improve from 28.40662
196/196 - 34s - loss: 28.4119 - MinusLogProbMetric: 28.4119 - val_loss: 28.7640 - val_MinusLogProbMetric: 28.7640 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 361/1000
2023-09-29 01:08:39.082 
Epoch 361/1000 
	 loss: 28.3654, MinusLogProbMetric: 28.3654, val_loss: 28.6033, val_MinusLogProbMetric: 28.6033

Epoch 361: val_loss did not improve from 28.40662
196/196 - 36s - loss: 28.3654 - MinusLogProbMetric: 28.3654 - val_loss: 28.6033 - val_MinusLogProbMetric: 28.6033 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 362/1000
2023-09-29 01:09:09.980 
Epoch 362/1000 
	 loss: 28.3773, MinusLogProbMetric: 28.3773, val_loss: 28.6384, val_MinusLogProbMetric: 28.6384

Epoch 362: val_loss did not improve from 28.40662
196/196 - 31s - loss: 28.3773 - MinusLogProbMetric: 28.3773 - val_loss: 28.6384 - val_MinusLogProbMetric: 28.6384 - lr: 0.0010 - 31s/epoch - 158ms/step
Epoch 363/1000
2023-09-29 01:09:39.441 
Epoch 363/1000 
	 loss: 28.3894, MinusLogProbMetric: 28.3894, val_loss: 28.5473, val_MinusLogProbMetric: 28.5473

Epoch 363: val_loss did not improve from 28.40662
196/196 - 29s - loss: 28.3894 - MinusLogProbMetric: 28.3894 - val_loss: 28.5473 - val_MinusLogProbMetric: 28.5473 - lr: 0.0010 - 29s/epoch - 150ms/step
Epoch 364/1000
2023-09-29 01:10:10.267 
Epoch 364/1000 
	 loss: 28.3708, MinusLogProbMetric: 28.3708, val_loss: 28.7078, val_MinusLogProbMetric: 28.7078

Epoch 364: val_loss did not improve from 28.40662
196/196 - 31s - loss: 28.3708 - MinusLogProbMetric: 28.3708 - val_loss: 28.7078 - val_MinusLogProbMetric: 28.7078 - lr: 0.0010 - 31s/epoch - 157ms/step
Epoch 365/1000
2023-09-29 01:10:41.141 
Epoch 365/1000 
	 loss: 28.3796, MinusLogProbMetric: 28.3796, val_loss: 28.5681, val_MinusLogProbMetric: 28.5681

Epoch 365: val_loss did not improve from 28.40662
196/196 - 31s - loss: 28.3796 - MinusLogProbMetric: 28.3796 - val_loss: 28.5681 - val_MinusLogProbMetric: 28.5681 - lr: 0.0010 - 31s/epoch - 158ms/step
Epoch 366/1000
2023-09-29 01:11:11.746 
Epoch 366/1000 
	 loss: 28.3464, MinusLogProbMetric: 28.3464, val_loss: 29.9581, val_MinusLogProbMetric: 29.9581

Epoch 366: val_loss did not improve from 28.40662
196/196 - 31s - loss: 28.3464 - MinusLogProbMetric: 28.3464 - val_loss: 29.9581 - val_MinusLogProbMetric: 29.9581 - lr: 0.0010 - 31s/epoch - 156ms/step
Epoch 367/1000
2023-09-29 01:11:42.344 
Epoch 367/1000 
	 loss: 28.4373, MinusLogProbMetric: 28.4373, val_loss: 28.6061, val_MinusLogProbMetric: 28.6061

Epoch 367: val_loss did not improve from 28.40662
196/196 - 31s - loss: 28.4373 - MinusLogProbMetric: 28.4373 - val_loss: 28.6061 - val_MinusLogProbMetric: 28.6061 - lr: 0.0010 - 31s/epoch - 156ms/step
Epoch 368/1000
2023-09-29 01:12:12.140 
Epoch 368/1000 
	 loss: 28.3707, MinusLogProbMetric: 28.3707, val_loss: 28.4657, val_MinusLogProbMetric: 28.4657

Epoch 368: val_loss did not improve from 28.40662
196/196 - 30s - loss: 28.3707 - MinusLogProbMetric: 28.3707 - val_loss: 28.4657 - val_MinusLogProbMetric: 28.4657 - lr: 0.0010 - 30s/epoch - 152ms/step
Epoch 369/1000
2023-09-29 01:12:41.556 
Epoch 369/1000 
	 loss: 27.9662, MinusLogProbMetric: 27.9662, val_loss: 28.2933, val_MinusLogProbMetric: 28.2933

Epoch 369: val_loss improved from 28.40662 to 28.29330, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_329/weights/best_weights.h5
196/196 - 30s - loss: 27.9662 - MinusLogProbMetric: 27.9662 - val_loss: 28.2933 - val_MinusLogProbMetric: 28.2933 - lr: 5.0000e-04 - 30s/epoch - 153ms/step
Epoch 370/1000
2023-09-29 01:13:11.993 
Epoch 370/1000 
	 loss: 27.9597, MinusLogProbMetric: 27.9597, val_loss: 28.1354, val_MinusLogProbMetric: 28.1354

Epoch 370: val_loss improved from 28.29330 to 28.13538, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_329/weights/best_weights.h5
196/196 - 31s - loss: 27.9597 - MinusLogProbMetric: 27.9597 - val_loss: 28.1354 - val_MinusLogProbMetric: 28.1354 - lr: 5.0000e-04 - 31s/epoch - 156ms/step
Epoch 371/1000
2023-09-29 01:13:42.844 
Epoch 371/1000 
	 loss: 27.9644, MinusLogProbMetric: 27.9644, val_loss: 28.1666, val_MinusLogProbMetric: 28.1666

Epoch 371: val_loss did not improve from 28.13538
196/196 - 30s - loss: 27.9644 - MinusLogProbMetric: 27.9644 - val_loss: 28.1666 - val_MinusLogProbMetric: 28.1666 - lr: 5.0000e-04 - 30s/epoch - 154ms/step
Epoch 372/1000
2023-09-29 01:14:15.581 
Epoch 372/1000 
	 loss: 27.9678, MinusLogProbMetric: 27.9678, val_loss: 28.2100, val_MinusLogProbMetric: 28.2100

Epoch 372: val_loss did not improve from 28.13538
196/196 - 33s - loss: 27.9678 - MinusLogProbMetric: 27.9678 - val_loss: 28.2100 - val_MinusLogProbMetric: 28.2100 - lr: 5.0000e-04 - 33s/epoch - 167ms/step
Epoch 373/1000
2023-09-29 01:14:47.240 
Epoch 373/1000 
	 loss: 27.9705, MinusLogProbMetric: 27.9705, val_loss: 28.1789, val_MinusLogProbMetric: 28.1789

Epoch 373: val_loss did not improve from 28.13538
196/196 - 32s - loss: 27.9705 - MinusLogProbMetric: 27.9705 - val_loss: 28.1789 - val_MinusLogProbMetric: 28.1789 - lr: 5.0000e-04 - 32s/epoch - 162ms/step
Epoch 374/1000
2023-09-29 01:15:18.454 
Epoch 374/1000 
	 loss: 27.9615, MinusLogProbMetric: 27.9615, val_loss: 28.3269, val_MinusLogProbMetric: 28.3269

Epoch 374: val_loss did not improve from 28.13538
196/196 - 31s - loss: 27.9615 - MinusLogProbMetric: 27.9615 - val_loss: 28.3269 - val_MinusLogProbMetric: 28.3269 - lr: 5.0000e-04 - 31s/epoch - 159ms/step
Epoch 375/1000
2023-09-29 01:15:47.874 
Epoch 375/1000 
	 loss: 27.9911, MinusLogProbMetric: 27.9911, val_loss: 28.2283, val_MinusLogProbMetric: 28.2283

Epoch 375: val_loss did not improve from 28.13538
196/196 - 29s - loss: 27.9911 - MinusLogProbMetric: 27.9911 - val_loss: 28.2283 - val_MinusLogProbMetric: 28.2283 - lr: 5.0000e-04 - 29s/epoch - 150ms/step
Epoch 376/1000
2023-09-29 01:16:17.788 
Epoch 376/1000 
	 loss: 27.9796, MinusLogProbMetric: 27.9796, val_loss: 28.4269, val_MinusLogProbMetric: 28.4269

Epoch 376: val_loss did not improve from 28.13538
196/196 - 30s - loss: 27.9796 - MinusLogProbMetric: 27.9796 - val_loss: 28.4269 - val_MinusLogProbMetric: 28.4269 - lr: 5.0000e-04 - 30s/epoch - 153ms/step
Epoch 377/1000
2023-09-29 01:16:50.092 
Epoch 377/1000 
	 loss: 27.9634, MinusLogProbMetric: 27.9634, val_loss: 28.2517, val_MinusLogProbMetric: 28.2517

Epoch 377: val_loss did not improve from 28.13538
196/196 - 32s - loss: 27.9634 - MinusLogProbMetric: 27.9634 - val_loss: 28.2517 - val_MinusLogProbMetric: 28.2517 - lr: 5.0000e-04 - 32s/epoch - 165ms/step
Epoch 378/1000
2023-09-29 01:17:20.675 
Epoch 378/1000 
	 loss: 27.9691, MinusLogProbMetric: 27.9691, val_loss: 28.2094, val_MinusLogProbMetric: 28.2094

Epoch 378: val_loss did not improve from 28.13538
196/196 - 31s - loss: 27.9691 - MinusLogProbMetric: 27.9691 - val_loss: 28.2094 - val_MinusLogProbMetric: 28.2094 - lr: 5.0000e-04 - 31s/epoch - 156ms/step
Epoch 379/1000
2023-09-29 01:17:51.584 
Epoch 379/1000 
	 loss: 27.9848, MinusLogProbMetric: 27.9848, val_loss: 28.2326, val_MinusLogProbMetric: 28.2326

Epoch 379: val_loss did not improve from 28.13538
196/196 - 31s - loss: 27.9848 - MinusLogProbMetric: 27.9848 - val_loss: 28.2326 - val_MinusLogProbMetric: 28.2326 - lr: 5.0000e-04 - 31s/epoch - 158ms/step
Epoch 380/1000
2023-09-29 01:18:21.570 
Epoch 380/1000 
	 loss: 27.9649, MinusLogProbMetric: 27.9649, val_loss: 28.4553, val_MinusLogProbMetric: 28.4553

Epoch 380: val_loss did not improve from 28.13538
196/196 - 30s - loss: 27.9649 - MinusLogProbMetric: 27.9649 - val_loss: 28.4553 - val_MinusLogProbMetric: 28.4553 - lr: 5.0000e-04 - 30s/epoch - 153ms/step
Epoch 381/1000
2023-09-29 01:18:52.810 
Epoch 381/1000 
	 loss: 27.9767, MinusLogProbMetric: 27.9767, val_loss: 28.0938, val_MinusLogProbMetric: 28.0938

Epoch 381: val_loss improved from 28.13538 to 28.09384, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_329/weights/best_weights.h5
196/196 - 32s - loss: 27.9767 - MinusLogProbMetric: 27.9767 - val_loss: 28.0938 - val_MinusLogProbMetric: 28.0938 - lr: 5.0000e-04 - 32s/epoch - 162ms/step
Epoch 382/1000
2023-09-29 01:19:24.803 
Epoch 382/1000 
	 loss: 27.9588, MinusLogProbMetric: 27.9588, val_loss: 28.0957, val_MinusLogProbMetric: 28.0957

Epoch 382: val_loss did not improve from 28.09384
196/196 - 31s - loss: 27.9588 - MinusLogProbMetric: 27.9588 - val_loss: 28.0957 - val_MinusLogProbMetric: 28.0957 - lr: 5.0000e-04 - 31s/epoch - 161ms/step
Epoch 383/1000
2023-09-29 01:19:55.223 
Epoch 383/1000 
	 loss: 27.9658, MinusLogProbMetric: 27.9658, val_loss: 28.3403, val_MinusLogProbMetric: 28.3403

Epoch 383: val_loss did not improve from 28.09384
196/196 - 30s - loss: 27.9658 - MinusLogProbMetric: 27.9658 - val_loss: 28.3403 - val_MinusLogProbMetric: 28.3403 - lr: 5.0000e-04 - 30s/epoch - 155ms/step
Epoch 384/1000
2023-09-29 01:20:24.645 
Epoch 384/1000 
	 loss: 27.9735, MinusLogProbMetric: 27.9735, val_loss: 28.0882, val_MinusLogProbMetric: 28.0882

Epoch 384: val_loss improved from 28.09384 to 28.08824, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_329/weights/best_weights.h5
196/196 - 30s - loss: 27.9735 - MinusLogProbMetric: 27.9735 - val_loss: 28.0882 - val_MinusLogProbMetric: 28.0882 - lr: 5.0000e-04 - 30s/epoch - 153ms/step
Epoch 385/1000
2023-09-29 01:20:55.153 
Epoch 385/1000 
	 loss: 27.9715, MinusLogProbMetric: 27.9715, val_loss: 28.5578, val_MinusLogProbMetric: 28.5578

Epoch 385: val_loss did not improve from 28.08824
196/196 - 30s - loss: 27.9715 - MinusLogProbMetric: 27.9715 - val_loss: 28.5578 - val_MinusLogProbMetric: 28.5578 - lr: 5.0000e-04 - 30s/epoch - 153ms/step
Epoch 386/1000
2023-09-29 01:21:24.923 
Epoch 386/1000 
	 loss: 27.9782, MinusLogProbMetric: 27.9782, val_loss: 28.3622, val_MinusLogProbMetric: 28.3622

Epoch 386: val_loss did not improve from 28.08824
196/196 - 30s - loss: 27.9782 - MinusLogProbMetric: 27.9782 - val_loss: 28.3622 - val_MinusLogProbMetric: 28.3622 - lr: 5.0000e-04 - 30s/epoch - 152ms/step
Epoch 387/1000
2023-09-29 01:21:54.185 
Epoch 387/1000 
	 loss: 27.9632, MinusLogProbMetric: 27.9632, val_loss: 28.2263, val_MinusLogProbMetric: 28.2263

Epoch 387: val_loss did not improve from 28.08824
196/196 - 29s - loss: 27.9632 - MinusLogProbMetric: 27.9632 - val_loss: 28.2263 - val_MinusLogProbMetric: 28.2263 - lr: 5.0000e-04 - 29s/epoch - 149ms/step
Epoch 388/1000
2023-09-29 01:22:24.663 
Epoch 388/1000 
	 loss: 27.9633, MinusLogProbMetric: 27.9633, val_loss: 28.3104, val_MinusLogProbMetric: 28.3104

Epoch 388: val_loss did not improve from 28.08824
196/196 - 30s - loss: 27.9633 - MinusLogProbMetric: 27.9633 - val_loss: 28.3104 - val_MinusLogProbMetric: 28.3104 - lr: 5.0000e-04 - 30s/epoch - 155ms/step
Epoch 389/1000
2023-09-29 01:22:56.325 
Epoch 389/1000 
	 loss: 27.9458, MinusLogProbMetric: 27.9458, val_loss: 28.3470, val_MinusLogProbMetric: 28.3470

Epoch 389: val_loss did not improve from 28.08824
196/196 - 32s - loss: 27.9458 - MinusLogProbMetric: 27.9458 - val_loss: 28.3470 - val_MinusLogProbMetric: 28.3470 - lr: 5.0000e-04 - 32s/epoch - 162ms/step
Epoch 390/1000
2023-09-29 01:23:30.337 
Epoch 390/1000 
	 loss: 27.9625, MinusLogProbMetric: 27.9625, val_loss: 28.2888, val_MinusLogProbMetric: 28.2888

Epoch 390: val_loss did not improve from 28.08824
196/196 - 34s - loss: 27.9625 - MinusLogProbMetric: 27.9625 - val_loss: 28.2888 - val_MinusLogProbMetric: 28.2888 - lr: 5.0000e-04 - 34s/epoch - 173ms/step
Epoch 391/1000
2023-09-29 01:23:59.840 
Epoch 391/1000 
	 loss: 27.9510, MinusLogProbMetric: 27.9510, val_loss: 28.1656, val_MinusLogProbMetric: 28.1656

Epoch 391: val_loss did not improve from 28.08824
196/196 - 30s - loss: 27.9510 - MinusLogProbMetric: 27.9510 - val_loss: 28.1656 - val_MinusLogProbMetric: 28.1656 - lr: 5.0000e-04 - 30s/epoch - 151ms/step
Epoch 392/1000
2023-09-29 01:24:33.770 
Epoch 392/1000 
	 loss: 27.9802, MinusLogProbMetric: 27.9802, val_loss: 28.1445, val_MinusLogProbMetric: 28.1445

Epoch 392: val_loss did not improve from 28.08824
196/196 - 34s - loss: 27.9802 - MinusLogProbMetric: 27.9802 - val_loss: 28.1445 - val_MinusLogProbMetric: 28.1445 - lr: 5.0000e-04 - 34s/epoch - 173ms/step
Epoch 393/1000
2023-09-29 01:25:06.202 
Epoch 393/1000 
	 loss: 27.9451, MinusLogProbMetric: 27.9451, val_loss: 28.1614, val_MinusLogProbMetric: 28.1614

Epoch 393: val_loss did not improve from 28.08824
196/196 - 32s - loss: 27.9451 - MinusLogProbMetric: 27.9451 - val_loss: 28.1614 - val_MinusLogProbMetric: 28.1614 - lr: 5.0000e-04 - 32s/epoch - 165ms/step
Epoch 394/1000
2023-09-29 01:25:38.853 
Epoch 394/1000 
	 loss: 27.9722, MinusLogProbMetric: 27.9722, val_loss: 28.1790, val_MinusLogProbMetric: 28.1790

Epoch 394: val_loss did not improve from 28.08824
196/196 - 33s - loss: 27.9722 - MinusLogProbMetric: 27.9722 - val_loss: 28.1790 - val_MinusLogProbMetric: 28.1790 - lr: 5.0000e-04 - 33s/epoch - 167ms/step
Epoch 395/1000
2023-09-29 01:26:09.717 
Epoch 395/1000 
	 loss: 27.9684, MinusLogProbMetric: 27.9684, val_loss: 28.5167, val_MinusLogProbMetric: 28.5167

Epoch 395: val_loss did not improve from 28.08824
196/196 - 31s - loss: 27.9684 - MinusLogProbMetric: 27.9684 - val_loss: 28.5167 - val_MinusLogProbMetric: 28.5167 - lr: 5.0000e-04 - 31s/epoch - 157ms/step
Epoch 396/1000
2023-09-29 01:26:40.444 
Epoch 396/1000 
	 loss: 27.9633, MinusLogProbMetric: 27.9633, val_loss: 28.1857, val_MinusLogProbMetric: 28.1857

Epoch 396: val_loss did not improve from 28.08824
196/196 - 31s - loss: 27.9633 - MinusLogProbMetric: 27.9633 - val_loss: 28.1857 - val_MinusLogProbMetric: 28.1857 - lr: 5.0000e-04 - 31s/epoch - 157ms/step
Epoch 397/1000
2023-09-29 01:27:12.022 
Epoch 397/1000 
	 loss: 27.9273, MinusLogProbMetric: 27.9273, val_loss: 28.2152, val_MinusLogProbMetric: 28.2152

Epoch 397: val_loss did not improve from 28.08824
196/196 - 32s - loss: 27.9273 - MinusLogProbMetric: 27.9273 - val_loss: 28.2152 - val_MinusLogProbMetric: 28.2152 - lr: 5.0000e-04 - 32s/epoch - 161ms/step
Epoch 398/1000
2023-09-29 01:27:43.975 
Epoch 398/1000 
	 loss: 27.9869, MinusLogProbMetric: 27.9869, val_loss: 28.1782, val_MinusLogProbMetric: 28.1782

Epoch 398: val_loss did not improve from 28.08824
196/196 - 32s - loss: 27.9869 - MinusLogProbMetric: 27.9869 - val_loss: 28.1782 - val_MinusLogProbMetric: 28.1782 - lr: 5.0000e-04 - 32s/epoch - 163ms/step
Epoch 399/1000
2023-09-29 01:28:13.991 
Epoch 399/1000 
	 loss: 27.9528, MinusLogProbMetric: 27.9528, val_loss: 28.0983, val_MinusLogProbMetric: 28.0983

Epoch 399: val_loss did not improve from 28.08824
196/196 - 30s - loss: 27.9528 - MinusLogProbMetric: 27.9528 - val_loss: 28.0983 - val_MinusLogProbMetric: 28.0983 - lr: 5.0000e-04 - 30s/epoch - 153ms/step
Epoch 400/1000
2023-09-29 01:28:45.533 
Epoch 400/1000 
	 loss: 27.9383, MinusLogProbMetric: 27.9383, val_loss: 28.1174, val_MinusLogProbMetric: 28.1174

Epoch 400: val_loss did not improve from 28.08824
196/196 - 32s - loss: 27.9383 - MinusLogProbMetric: 27.9383 - val_loss: 28.1174 - val_MinusLogProbMetric: 28.1174 - lr: 5.0000e-04 - 32s/epoch - 161ms/step
Epoch 401/1000
2023-09-29 01:29:17.553 
Epoch 401/1000 
	 loss: 27.9613, MinusLogProbMetric: 27.9613, val_loss: 28.1148, val_MinusLogProbMetric: 28.1148

Epoch 401: val_loss did not improve from 28.08824
196/196 - 32s - loss: 27.9613 - MinusLogProbMetric: 27.9613 - val_loss: 28.1148 - val_MinusLogProbMetric: 28.1148 - lr: 5.0000e-04 - 32s/epoch - 163ms/step
Epoch 402/1000
2023-09-29 01:29:48.665 
Epoch 402/1000 
	 loss: 27.9866, MinusLogProbMetric: 27.9866, val_loss: 28.1490, val_MinusLogProbMetric: 28.1490

Epoch 402: val_loss did not improve from 28.08824
196/196 - 31s - loss: 27.9866 - MinusLogProbMetric: 27.9866 - val_loss: 28.1490 - val_MinusLogProbMetric: 28.1490 - lr: 5.0000e-04 - 31s/epoch - 159ms/step
Epoch 403/1000
2023-09-29 01:30:18.575 
Epoch 403/1000 
	 loss: 27.9554, MinusLogProbMetric: 27.9554, val_loss: 28.1247, val_MinusLogProbMetric: 28.1247

Epoch 403: val_loss did not improve from 28.08824
196/196 - 30s - loss: 27.9554 - MinusLogProbMetric: 27.9554 - val_loss: 28.1247 - val_MinusLogProbMetric: 28.1247 - lr: 5.0000e-04 - 30s/epoch - 153ms/step
Epoch 404/1000
2023-09-29 01:30:50.434 
Epoch 404/1000 
	 loss: 27.9583, MinusLogProbMetric: 27.9583, val_loss: 28.1820, val_MinusLogProbMetric: 28.1820

Epoch 404: val_loss did not improve from 28.08824
196/196 - 32s - loss: 27.9583 - MinusLogProbMetric: 27.9583 - val_loss: 28.1820 - val_MinusLogProbMetric: 28.1820 - lr: 5.0000e-04 - 32s/epoch - 163ms/step
Epoch 405/1000
2023-09-29 01:31:23.182 
Epoch 405/1000 
	 loss: 27.9315, MinusLogProbMetric: 27.9315, val_loss: 28.1276, val_MinusLogProbMetric: 28.1276

Epoch 405: val_loss did not improve from 28.08824
196/196 - 33s - loss: 27.9315 - MinusLogProbMetric: 27.9315 - val_loss: 28.1276 - val_MinusLogProbMetric: 28.1276 - lr: 5.0000e-04 - 33s/epoch - 167ms/step
Epoch 406/1000
2023-09-29 01:31:55.665 
Epoch 406/1000 
	 loss: 27.9677, MinusLogProbMetric: 27.9677, val_loss: 28.1132, val_MinusLogProbMetric: 28.1132

Epoch 406: val_loss did not improve from 28.08824
196/196 - 32s - loss: 27.9677 - MinusLogProbMetric: 27.9677 - val_loss: 28.1132 - val_MinusLogProbMetric: 28.1132 - lr: 5.0000e-04 - 32s/epoch - 166ms/step
Epoch 407/1000
2023-09-29 01:32:29.253 
Epoch 407/1000 
	 loss: 27.9609, MinusLogProbMetric: 27.9609, val_loss: 28.3143, val_MinusLogProbMetric: 28.3143

Epoch 407: val_loss did not improve from 28.08824
196/196 - 34s - loss: 27.9609 - MinusLogProbMetric: 27.9609 - val_loss: 28.3143 - val_MinusLogProbMetric: 28.3143 - lr: 5.0000e-04 - 34s/epoch - 171ms/step
Epoch 408/1000
2023-09-29 01:32:59.285 
Epoch 408/1000 
	 loss: 27.9777, MinusLogProbMetric: 27.9777, val_loss: 28.1716, val_MinusLogProbMetric: 28.1716

Epoch 408: val_loss did not improve from 28.08824
196/196 - 30s - loss: 27.9777 - MinusLogProbMetric: 27.9777 - val_loss: 28.1716 - val_MinusLogProbMetric: 28.1716 - lr: 5.0000e-04 - 30s/epoch - 153ms/step
Epoch 409/1000
2023-09-29 01:33:30.936 
Epoch 409/1000 
	 loss: 27.9751, MinusLogProbMetric: 27.9751, val_loss: 28.2637, val_MinusLogProbMetric: 28.2637

Epoch 409: val_loss did not improve from 28.08824
196/196 - 32s - loss: 27.9751 - MinusLogProbMetric: 27.9751 - val_loss: 28.2637 - val_MinusLogProbMetric: 28.2637 - lr: 5.0000e-04 - 32s/epoch - 161ms/step
Epoch 410/1000
2023-09-29 01:34:03.048 
Epoch 410/1000 
	 loss: 27.9372, MinusLogProbMetric: 27.9372, val_loss: 28.1168, val_MinusLogProbMetric: 28.1168

Epoch 410: val_loss did not improve from 28.08824
196/196 - 32s - loss: 27.9372 - MinusLogProbMetric: 27.9372 - val_loss: 28.1168 - val_MinusLogProbMetric: 28.1168 - lr: 5.0000e-04 - 32s/epoch - 164ms/step
Epoch 411/1000
2023-09-29 01:34:35.344 
Epoch 411/1000 
	 loss: 27.9668, MinusLogProbMetric: 27.9668, val_loss: 28.1941, val_MinusLogProbMetric: 28.1941

Epoch 411: val_loss did not improve from 28.08824
196/196 - 32s - loss: 27.9668 - MinusLogProbMetric: 27.9668 - val_loss: 28.1941 - val_MinusLogProbMetric: 28.1941 - lr: 5.0000e-04 - 32s/epoch - 165ms/step
Epoch 412/1000
2023-09-29 01:35:06.218 
Epoch 412/1000 
	 loss: 27.9512, MinusLogProbMetric: 27.9512, val_loss: 28.1848, val_MinusLogProbMetric: 28.1848

Epoch 412: val_loss did not improve from 28.08824
196/196 - 31s - loss: 27.9512 - MinusLogProbMetric: 27.9512 - val_loss: 28.1848 - val_MinusLogProbMetric: 28.1848 - lr: 5.0000e-04 - 31s/epoch - 157ms/step
Epoch 413/1000
2023-09-29 01:35:38.603 
Epoch 413/1000 
	 loss: 27.9485, MinusLogProbMetric: 27.9485, val_loss: 28.0873, val_MinusLogProbMetric: 28.0873

Epoch 413: val_loss improved from 28.08824 to 28.08731, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_329/weights/best_weights.h5
196/196 - 33s - loss: 27.9485 - MinusLogProbMetric: 27.9485 - val_loss: 28.0873 - val_MinusLogProbMetric: 28.0873 - lr: 5.0000e-04 - 33s/epoch - 169ms/step
Epoch 414/1000
2023-09-29 01:36:09.762 
Epoch 414/1000 
	 loss: 27.9531, MinusLogProbMetric: 27.9531, val_loss: 28.1246, val_MinusLogProbMetric: 28.1246

Epoch 414: val_loss did not improve from 28.08731
196/196 - 30s - loss: 27.9531 - MinusLogProbMetric: 27.9531 - val_loss: 28.1246 - val_MinusLogProbMetric: 28.1246 - lr: 5.0000e-04 - 30s/epoch - 155ms/step
Epoch 415/1000
2023-09-29 01:36:43.942 
Epoch 415/1000 
	 loss: 27.9237, MinusLogProbMetric: 27.9237, val_loss: 28.0968, val_MinusLogProbMetric: 28.0968

Epoch 415: val_loss did not improve from 28.08731
196/196 - 34s - loss: 27.9237 - MinusLogProbMetric: 27.9237 - val_loss: 28.0968 - val_MinusLogProbMetric: 28.0968 - lr: 5.0000e-04 - 34s/epoch - 174ms/step
Epoch 416/1000
2023-09-29 01:37:13.629 
Epoch 416/1000 
	 loss: 27.9392, MinusLogProbMetric: 27.9392, val_loss: 28.1405, val_MinusLogProbMetric: 28.1405

Epoch 416: val_loss did not improve from 28.08731
196/196 - 30s - loss: 27.9392 - MinusLogProbMetric: 27.9392 - val_loss: 28.1405 - val_MinusLogProbMetric: 28.1405 - lr: 5.0000e-04 - 30s/epoch - 151ms/step
Epoch 417/1000
2023-09-29 01:37:45.020 
Epoch 417/1000 
	 loss: 27.9487, MinusLogProbMetric: 27.9487, val_loss: 28.2765, val_MinusLogProbMetric: 28.2765

Epoch 417: val_loss did not improve from 28.08731
196/196 - 31s - loss: 27.9487 - MinusLogProbMetric: 27.9487 - val_loss: 28.2765 - val_MinusLogProbMetric: 28.2765 - lr: 5.0000e-04 - 31s/epoch - 160ms/step
Epoch 418/1000
2023-09-29 01:38:17.496 
Epoch 418/1000 
	 loss: 27.9331, MinusLogProbMetric: 27.9331, val_loss: 28.3622, val_MinusLogProbMetric: 28.3622

Epoch 418: val_loss did not improve from 28.08731
196/196 - 32s - loss: 27.9331 - MinusLogProbMetric: 27.9331 - val_loss: 28.3622 - val_MinusLogProbMetric: 28.3622 - lr: 5.0000e-04 - 32s/epoch - 166ms/step
Epoch 419/1000
2023-09-29 01:38:48.096 
Epoch 419/1000 
	 loss: 27.9473, MinusLogProbMetric: 27.9473, val_loss: 28.1867, val_MinusLogProbMetric: 28.1867

Epoch 419: val_loss did not improve from 28.08731
196/196 - 31s - loss: 27.9473 - MinusLogProbMetric: 27.9473 - val_loss: 28.1867 - val_MinusLogProbMetric: 28.1867 - lr: 5.0000e-04 - 31s/epoch - 156ms/step
Epoch 420/1000
2023-09-29 01:39:20.960 
Epoch 420/1000 
	 loss: 27.9414, MinusLogProbMetric: 27.9414, val_loss: 28.0981, val_MinusLogProbMetric: 28.0981

Epoch 420: val_loss did not improve from 28.08731
196/196 - 33s - loss: 27.9414 - MinusLogProbMetric: 27.9414 - val_loss: 28.0981 - val_MinusLogProbMetric: 28.0981 - lr: 5.0000e-04 - 33s/epoch - 168ms/step
Epoch 421/1000
2023-09-29 01:39:52.360 
Epoch 421/1000 
	 loss: 27.9170, MinusLogProbMetric: 27.9170, val_loss: 28.2292, val_MinusLogProbMetric: 28.2292

Epoch 421: val_loss did not improve from 28.08731
196/196 - 31s - loss: 27.9170 - MinusLogProbMetric: 27.9170 - val_loss: 28.2292 - val_MinusLogProbMetric: 28.2292 - lr: 5.0000e-04 - 31s/epoch - 160ms/step
Epoch 422/1000
2023-09-29 01:40:23.097 
Epoch 422/1000 
	 loss: 27.9131, MinusLogProbMetric: 27.9131, val_loss: 28.1285, val_MinusLogProbMetric: 28.1285

Epoch 422: val_loss did not improve from 28.08731
196/196 - 31s - loss: 27.9131 - MinusLogProbMetric: 27.9131 - val_loss: 28.1285 - val_MinusLogProbMetric: 28.1285 - lr: 5.0000e-04 - 31s/epoch - 157ms/step
Epoch 423/1000
2023-09-29 01:40:57.335 
Epoch 423/1000 
	 loss: 27.9289, MinusLogProbMetric: 27.9289, val_loss: 28.1460, val_MinusLogProbMetric: 28.1460

Epoch 423: val_loss did not improve from 28.08731
196/196 - 34s - loss: 27.9289 - MinusLogProbMetric: 27.9289 - val_loss: 28.1460 - val_MinusLogProbMetric: 28.1460 - lr: 5.0000e-04 - 34s/epoch - 175ms/step
Epoch 424/1000
2023-09-29 01:41:31.641 
Epoch 424/1000 
	 loss: 27.9801, MinusLogProbMetric: 27.9801, val_loss: 28.1887, val_MinusLogProbMetric: 28.1887

Epoch 424: val_loss did not improve from 28.08731
196/196 - 34s - loss: 27.9801 - MinusLogProbMetric: 27.9801 - val_loss: 28.1887 - val_MinusLogProbMetric: 28.1887 - lr: 5.0000e-04 - 34s/epoch - 175ms/step
Epoch 425/1000
2023-09-29 01:42:04.244 
Epoch 425/1000 
	 loss: 27.9502, MinusLogProbMetric: 27.9502, val_loss: 28.1888, val_MinusLogProbMetric: 28.1888

Epoch 425: val_loss did not improve from 28.08731
196/196 - 33s - loss: 27.9502 - MinusLogProbMetric: 27.9502 - val_loss: 28.1888 - val_MinusLogProbMetric: 28.1888 - lr: 5.0000e-04 - 33s/epoch - 166ms/step
Epoch 426/1000
2023-09-29 01:42:35.084 
Epoch 426/1000 
	 loss: 27.9141, MinusLogProbMetric: 27.9141, val_loss: 28.1564, val_MinusLogProbMetric: 28.1564

Epoch 426: val_loss did not improve from 28.08731
196/196 - 31s - loss: 27.9141 - MinusLogProbMetric: 27.9141 - val_loss: 28.1564 - val_MinusLogProbMetric: 28.1564 - lr: 5.0000e-04 - 31s/epoch - 157ms/step
Epoch 427/1000
2023-09-29 01:43:08.365 
Epoch 427/1000 
	 loss: 27.9147, MinusLogProbMetric: 27.9147, val_loss: 28.2421, val_MinusLogProbMetric: 28.2421

Epoch 427: val_loss did not improve from 28.08731
196/196 - 33s - loss: 27.9147 - MinusLogProbMetric: 27.9147 - val_loss: 28.2421 - val_MinusLogProbMetric: 28.2421 - lr: 5.0000e-04 - 33s/epoch - 170ms/step
Epoch 428/1000
2023-09-29 01:43:38.278 
Epoch 428/1000 
	 loss: 27.9693, MinusLogProbMetric: 27.9693, val_loss: 29.4375, val_MinusLogProbMetric: 29.4375

Epoch 428: val_loss did not improve from 28.08731
196/196 - 30s - loss: 27.9693 - MinusLogProbMetric: 27.9693 - val_loss: 29.4375 - val_MinusLogProbMetric: 29.4375 - lr: 5.0000e-04 - 30s/epoch - 153ms/step
Epoch 429/1000
2023-09-29 01:44:12.204 
Epoch 429/1000 
	 loss: 28.0198, MinusLogProbMetric: 28.0198, val_loss: 28.1790, val_MinusLogProbMetric: 28.1790

Epoch 429: val_loss did not improve from 28.08731
196/196 - 34s - loss: 28.0198 - MinusLogProbMetric: 28.0198 - val_loss: 28.1790 - val_MinusLogProbMetric: 28.1790 - lr: 5.0000e-04 - 34s/epoch - 173ms/step
Epoch 430/1000
2023-09-29 01:44:43.863 
Epoch 430/1000 
	 loss: 27.9522, MinusLogProbMetric: 27.9522, val_loss: 28.2110, val_MinusLogProbMetric: 28.2110

Epoch 430: val_loss did not improve from 28.08731
196/196 - 32s - loss: 27.9522 - MinusLogProbMetric: 27.9522 - val_loss: 28.2110 - val_MinusLogProbMetric: 28.2110 - lr: 5.0000e-04 - 32s/epoch - 162ms/step
Epoch 431/1000
2023-09-29 01:45:16.883 
Epoch 431/1000 
	 loss: 27.9528, MinusLogProbMetric: 27.9528, val_loss: 28.3305, val_MinusLogProbMetric: 28.3305

Epoch 431: val_loss did not improve from 28.08731
196/196 - 33s - loss: 27.9528 - MinusLogProbMetric: 27.9528 - val_loss: 28.3305 - val_MinusLogProbMetric: 28.3305 - lr: 5.0000e-04 - 33s/epoch - 168ms/step
Epoch 432/1000
2023-09-29 01:45:49.246 
Epoch 432/1000 
	 loss: 27.9289, MinusLogProbMetric: 27.9289, val_loss: 28.1189, val_MinusLogProbMetric: 28.1189

Epoch 432: val_loss did not improve from 28.08731
196/196 - 32s - loss: 27.9289 - MinusLogProbMetric: 27.9289 - val_loss: 28.1189 - val_MinusLogProbMetric: 28.1189 - lr: 5.0000e-04 - 32s/epoch - 165ms/step
Epoch 433/1000
2023-09-29 01:46:19.628 
Epoch 433/1000 
	 loss: 27.9628, MinusLogProbMetric: 27.9628, val_loss: 28.2079, val_MinusLogProbMetric: 28.2079

Epoch 433: val_loss did not improve from 28.08731
196/196 - 30s - loss: 27.9628 - MinusLogProbMetric: 27.9628 - val_loss: 28.2079 - val_MinusLogProbMetric: 28.2079 - lr: 5.0000e-04 - 30s/epoch - 155ms/step
Epoch 434/1000
2023-09-29 01:46:52.084 
Epoch 434/1000 
	 loss: 27.9281, MinusLogProbMetric: 27.9281, val_loss: 28.1673, val_MinusLogProbMetric: 28.1673

Epoch 434: val_loss did not improve from 28.08731
196/196 - 32s - loss: 27.9281 - MinusLogProbMetric: 27.9281 - val_loss: 28.1673 - val_MinusLogProbMetric: 28.1673 - lr: 5.0000e-04 - 32s/epoch - 166ms/step
Epoch 435/1000
2023-09-29 01:47:24.942 
Epoch 435/1000 
	 loss: 27.9265, MinusLogProbMetric: 27.9265, val_loss: 28.1338, val_MinusLogProbMetric: 28.1338

Epoch 435: val_loss did not improve from 28.08731
196/196 - 33s - loss: 27.9265 - MinusLogProbMetric: 27.9265 - val_loss: 28.1338 - val_MinusLogProbMetric: 28.1338 - lr: 5.0000e-04 - 33s/epoch - 168ms/step
Epoch 436/1000
2023-09-29 01:47:57.437 
Epoch 436/1000 
	 loss: 27.9622, MinusLogProbMetric: 27.9622, val_loss: 28.1993, val_MinusLogProbMetric: 28.1993

Epoch 436: val_loss did not improve from 28.08731
196/196 - 32s - loss: 27.9622 - MinusLogProbMetric: 27.9622 - val_loss: 28.1993 - val_MinusLogProbMetric: 28.1993 - lr: 5.0000e-04 - 32s/epoch - 166ms/step
Epoch 437/1000
2023-09-29 01:48:30.542 
Epoch 437/1000 
	 loss: 27.9395, MinusLogProbMetric: 27.9395, val_loss: 28.4059, val_MinusLogProbMetric: 28.4059

Epoch 437: val_loss did not improve from 28.08731
196/196 - 33s - loss: 27.9395 - MinusLogProbMetric: 27.9395 - val_loss: 28.4059 - val_MinusLogProbMetric: 28.4059 - lr: 5.0000e-04 - 33s/epoch - 169ms/step
Epoch 438/1000
2023-09-29 01:49:05.665 
Epoch 438/1000 
	 loss: 27.9489, MinusLogProbMetric: 27.9489, val_loss: 28.3086, val_MinusLogProbMetric: 28.3086

Epoch 438: val_loss did not improve from 28.08731
196/196 - 35s - loss: 27.9489 - MinusLogProbMetric: 27.9489 - val_loss: 28.3086 - val_MinusLogProbMetric: 28.3086 - lr: 5.0000e-04 - 35s/epoch - 179ms/step
Epoch 439/1000
2023-09-29 01:49:35.780 
Epoch 439/1000 
	 loss: 27.9675, MinusLogProbMetric: 27.9675, val_loss: 28.2314, val_MinusLogProbMetric: 28.2314

Epoch 439: val_loss did not improve from 28.08731
196/196 - 30s - loss: 27.9675 - MinusLogProbMetric: 27.9675 - val_loss: 28.2314 - val_MinusLogProbMetric: 28.2314 - lr: 5.0000e-04 - 30s/epoch - 154ms/step
Epoch 440/1000
2023-09-29 01:50:07.290 
Epoch 440/1000 
	 loss: 27.9233, MinusLogProbMetric: 27.9233, val_loss: 28.1656, val_MinusLogProbMetric: 28.1656

Epoch 440: val_loss did not improve from 28.08731
196/196 - 32s - loss: 27.9233 - MinusLogProbMetric: 27.9233 - val_loss: 28.1656 - val_MinusLogProbMetric: 28.1656 - lr: 5.0000e-04 - 32s/epoch - 161ms/step
Epoch 441/1000
2023-09-29 01:50:40.406 
Epoch 441/1000 
	 loss: 27.9240, MinusLogProbMetric: 27.9240, val_loss: 28.2035, val_MinusLogProbMetric: 28.2035

Epoch 441: val_loss did not improve from 28.08731
196/196 - 33s - loss: 27.9240 - MinusLogProbMetric: 27.9240 - val_loss: 28.2035 - val_MinusLogProbMetric: 28.2035 - lr: 5.0000e-04 - 33s/epoch - 169ms/step
Epoch 442/1000
2023-09-29 01:51:11.676 
Epoch 442/1000 
	 loss: 27.9407, MinusLogProbMetric: 27.9407, val_loss: 28.1470, val_MinusLogProbMetric: 28.1470

Epoch 442: val_loss did not improve from 28.08731
196/196 - 31s - loss: 27.9407 - MinusLogProbMetric: 27.9407 - val_loss: 28.1470 - val_MinusLogProbMetric: 28.1470 - lr: 5.0000e-04 - 31s/epoch - 160ms/step
Epoch 443/1000
2023-09-29 01:51:43.705 
Epoch 443/1000 
	 loss: 27.9152, MinusLogProbMetric: 27.9152, val_loss: 28.2696, val_MinusLogProbMetric: 28.2696

Epoch 443: val_loss did not improve from 28.08731
196/196 - 32s - loss: 27.9152 - MinusLogProbMetric: 27.9152 - val_loss: 28.2696 - val_MinusLogProbMetric: 28.2696 - lr: 5.0000e-04 - 32s/epoch - 163ms/step
Epoch 444/1000
2023-09-29 01:52:14.266 
Epoch 444/1000 
	 loss: 27.9136, MinusLogProbMetric: 27.9136, val_loss: 28.1777, val_MinusLogProbMetric: 28.1777

Epoch 444: val_loss did not improve from 28.08731
196/196 - 31s - loss: 27.9136 - MinusLogProbMetric: 27.9136 - val_loss: 28.1777 - val_MinusLogProbMetric: 28.1777 - lr: 5.0000e-04 - 31s/epoch - 156ms/step
Epoch 445/1000
2023-09-29 01:52:46.710 
Epoch 445/1000 
	 loss: 27.9587, MinusLogProbMetric: 27.9587, val_loss: 28.2936, val_MinusLogProbMetric: 28.2936

Epoch 445: val_loss did not improve from 28.08731
196/196 - 32s - loss: 27.9587 - MinusLogProbMetric: 27.9587 - val_loss: 28.2936 - val_MinusLogProbMetric: 28.2936 - lr: 5.0000e-04 - 32s/epoch - 166ms/step
Epoch 446/1000
2023-09-29 01:53:17.536 
Epoch 446/1000 
	 loss: 27.9665, MinusLogProbMetric: 27.9665, val_loss: 28.1174, val_MinusLogProbMetric: 28.1174

Epoch 446: val_loss did not improve from 28.08731
196/196 - 31s - loss: 27.9665 - MinusLogProbMetric: 27.9665 - val_loss: 28.1174 - val_MinusLogProbMetric: 28.1174 - lr: 5.0000e-04 - 31s/epoch - 157ms/step
Epoch 447/1000
2023-09-29 01:53:48.512 
Epoch 447/1000 
	 loss: 27.9495, MinusLogProbMetric: 27.9495, val_loss: 28.0943, val_MinusLogProbMetric: 28.0943

Epoch 447: val_loss did not improve from 28.08731
196/196 - 31s - loss: 27.9495 - MinusLogProbMetric: 27.9495 - val_loss: 28.0943 - val_MinusLogProbMetric: 28.0943 - lr: 5.0000e-04 - 31s/epoch - 158ms/step
Epoch 448/1000
2023-09-29 01:54:19.840 
Epoch 448/1000 
	 loss: 27.9384, MinusLogProbMetric: 27.9384, val_loss: 28.1215, val_MinusLogProbMetric: 28.1215

Epoch 448: val_loss did not improve from 28.08731
196/196 - 31s - loss: 27.9384 - MinusLogProbMetric: 27.9384 - val_loss: 28.1215 - val_MinusLogProbMetric: 28.1215 - lr: 5.0000e-04 - 31s/epoch - 160ms/step
Epoch 449/1000
2023-09-29 01:54:51.741 
Epoch 449/1000 
	 loss: 27.9352, MinusLogProbMetric: 27.9352, val_loss: 28.2201, val_MinusLogProbMetric: 28.2201

Epoch 449: val_loss did not improve from 28.08731
196/196 - 32s - loss: 27.9352 - MinusLogProbMetric: 27.9352 - val_loss: 28.2201 - val_MinusLogProbMetric: 28.2201 - lr: 5.0000e-04 - 32s/epoch - 163ms/step
Epoch 450/1000
2023-09-29 01:55:22.932 
Epoch 450/1000 
	 loss: 27.9281, MinusLogProbMetric: 27.9281, val_loss: 28.1067, val_MinusLogProbMetric: 28.1067

Epoch 450: val_loss did not improve from 28.08731
196/196 - 31s - loss: 27.9281 - MinusLogProbMetric: 27.9281 - val_loss: 28.1067 - val_MinusLogProbMetric: 28.1067 - lr: 5.0000e-04 - 31s/epoch - 159ms/step
Epoch 451/1000
2023-09-29 01:55:52.917 
Epoch 451/1000 
	 loss: 27.9365, MinusLogProbMetric: 27.9365, val_loss: 28.1223, val_MinusLogProbMetric: 28.1223

Epoch 451: val_loss did not improve from 28.08731
196/196 - 30s - loss: 27.9365 - MinusLogProbMetric: 27.9365 - val_loss: 28.1223 - val_MinusLogProbMetric: 28.1223 - lr: 5.0000e-04 - 30s/epoch - 153ms/step
Epoch 452/1000
2023-09-29 01:56:23.784 
Epoch 452/1000 
	 loss: 27.9103, MinusLogProbMetric: 27.9103, val_loss: 28.0979, val_MinusLogProbMetric: 28.0979

Epoch 452: val_loss did not improve from 28.08731
196/196 - 31s - loss: 27.9103 - MinusLogProbMetric: 27.9103 - val_loss: 28.0979 - val_MinusLogProbMetric: 28.0979 - lr: 5.0000e-04 - 31s/epoch - 157ms/step
Epoch 453/1000
2023-09-29 01:56:54.885 
Epoch 453/1000 
	 loss: 27.9110, MinusLogProbMetric: 27.9110, val_loss: 28.2080, val_MinusLogProbMetric: 28.2080

Epoch 453: val_loss did not improve from 28.08731
196/196 - 31s - loss: 27.9110 - MinusLogProbMetric: 27.9110 - val_loss: 28.2080 - val_MinusLogProbMetric: 28.2080 - lr: 5.0000e-04 - 31s/epoch - 159ms/step
Epoch 454/1000
2023-09-29 01:57:24.402 
Epoch 454/1000 
	 loss: 27.9431, MinusLogProbMetric: 27.9431, val_loss: 28.1164, val_MinusLogProbMetric: 28.1164

Epoch 454: val_loss did not improve from 28.08731
196/196 - 30s - loss: 27.9431 - MinusLogProbMetric: 27.9431 - val_loss: 28.1164 - val_MinusLogProbMetric: 28.1164 - lr: 5.0000e-04 - 30s/epoch - 151ms/step
Epoch 455/1000
2023-09-29 01:57:54.221 
Epoch 455/1000 
	 loss: 27.9133, MinusLogProbMetric: 27.9133, val_loss: 28.1644, val_MinusLogProbMetric: 28.1644

Epoch 455: val_loss did not improve from 28.08731
196/196 - 30s - loss: 27.9133 - MinusLogProbMetric: 27.9133 - val_loss: 28.1644 - val_MinusLogProbMetric: 28.1644 - lr: 5.0000e-04 - 30s/epoch - 152ms/step
Epoch 456/1000
2023-09-29 01:58:24.139 
Epoch 456/1000 
	 loss: 27.9527, MinusLogProbMetric: 27.9527, val_loss: 28.2397, val_MinusLogProbMetric: 28.2397

Epoch 456: val_loss did not improve from 28.08731
196/196 - 30s - loss: 27.9527 - MinusLogProbMetric: 27.9527 - val_loss: 28.2397 - val_MinusLogProbMetric: 28.2397 - lr: 5.0000e-04 - 30s/epoch - 153ms/step
Epoch 457/1000
2023-09-29 01:58:55.347 
Epoch 457/1000 
	 loss: 27.9026, MinusLogProbMetric: 27.9026, val_loss: 28.3004, val_MinusLogProbMetric: 28.3004

Epoch 457: val_loss did not improve from 28.08731
196/196 - 31s - loss: 27.9026 - MinusLogProbMetric: 27.9026 - val_loss: 28.3004 - val_MinusLogProbMetric: 28.3004 - lr: 5.0000e-04 - 31s/epoch - 159ms/step
Epoch 458/1000
2023-09-29 01:59:30.176 
Epoch 458/1000 
	 loss: 27.9111, MinusLogProbMetric: 27.9111, val_loss: 28.2242, val_MinusLogProbMetric: 28.2242

Epoch 458: val_loss did not improve from 28.08731
196/196 - 35s - loss: 27.9111 - MinusLogProbMetric: 27.9111 - val_loss: 28.2242 - val_MinusLogProbMetric: 28.2242 - lr: 5.0000e-04 - 35s/epoch - 178ms/step
Epoch 459/1000
2023-09-29 02:00:01.537 
Epoch 459/1000 
	 loss: 27.9246, MinusLogProbMetric: 27.9246, val_loss: 28.1230, val_MinusLogProbMetric: 28.1230

Epoch 459: val_loss did not improve from 28.08731
196/196 - 31s - loss: 27.9246 - MinusLogProbMetric: 27.9246 - val_loss: 28.1230 - val_MinusLogProbMetric: 28.1230 - lr: 5.0000e-04 - 31s/epoch - 160ms/step
Epoch 460/1000
2023-09-29 02:00:31.686 
Epoch 460/1000 
	 loss: 27.9307, MinusLogProbMetric: 27.9307, val_loss: 28.1252, val_MinusLogProbMetric: 28.1252

Epoch 460: val_loss did not improve from 28.08731
196/196 - 30s - loss: 27.9307 - MinusLogProbMetric: 27.9307 - val_loss: 28.1252 - val_MinusLogProbMetric: 28.1252 - lr: 5.0000e-04 - 30s/epoch - 154ms/step
Epoch 461/1000
2023-09-29 02:01:04.264 
Epoch 461/1000 
	 loss: 27.9048, MinusLogProbMetric: 27.9048, val_loss: 28.1693, val_MinusLogProbMetric: 28.1693

Epoch 461: val_loss did not improve from 28.08731
196/196 - 33s - loss: 27.9048 - MinusLogProbMetric: 27.9048 - val_loss: 28.1693 - val_MinusLogProbMetric: 28.1693 - lr: 5.0000e-04 - 33s/epoch - 166ms/step
Epoch 462/1000
2023-09-29 02:01:38.471 
Epoch 462/1000 
	 loss: 27.9073, MinusLogProbMetric: 27.9073, val_loss: 28.0916, val_MinusLogProbMetric: 28.0916

Epoch 462: val_loss did not improve from 28.08731
196/196 - 34s - loss: 27.9073 - MinusLogProbMetric: 27.9073 - val_loss: 28.0916 - val_MinusLogProbMetric: 28.0916 - lr: 5.0000e-04 - 34s/epoch - 175ms/step
Epoch 463/1000
2023-09-29 02:02:08.695 
Epoch 463/1000 
	 loss: 27.9091, MinusLogProbMetric: 27.9091, val_loss: 28.1412, val_MinusLogProbMetric: 28.1412

Epoch 463: val_loss did not improve from 28.08731
196/196 - 30s - loss: 27.9091 - MinusLogProbMetric: 27.9091 - val_loss: 28.1412 - val_MinusLogProbMetric: 28.1412 - lr: 5.0000e-04 - 30s/epoch - 154ms/step
Epoch 464/1000
2023-09-29 02:02:41.295 
Epoch 464/1000 
	 loss: 27.7679, MinusLogProbMetric: 27.7679, val_loss: 28.0207, val_MinusLogProbMetric: 28.0207

Epoch 464: val_loss improved from 28.08731 to 28.02072, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_329/weights/best_weights.h5
196/196 - 33s - loss: 27.7679 - MinusLogProbMetric: 27.7679 - val_loss: 28.0207 - val_MinusLogProbMetric: 28.0207 - lr: 2.5000e-04 - 33s/epoch - 170ms/step
Epoch 465/1000
2023-09-29 02:03:13.637 
Epoch 465/1000 
	 loss: 27.7667, MinusLogProbMetric: 27.7667, val_loss: 27.9728, val_MinusLogProbMetric: 27.9728

Epoch 465: val_loss improved from 28.02072 to 27.97282, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_329/weights/best_weights.h5
196/196 - 32s - loss: 27.7667 - MinusLogProbMetric: 27.7667 - val_loss: 27.9728 - val_MinusLogProbMetric: 27.9728 - lr: 2.5000e-04 - 32s/epoch - 164ms/step
Epoch 466/1000
2023-09-29 02:03:44.774 
Epoch 466/1000 
	 loss: 27.7521, MinusLogProbMetric: 27.7521, val_loss: 28.0374, val_MinusLogProbMetric: 28.0374

Epoch 466: val_loss did not improve from 27.97282
196/196 - 31s - loss: 27.7521 - MinusLogProbMetric: 27.7521 - val_loss: 28.0374 - val_MinusLogProbMetric: 28.0374 - lr: 2.5000e-04 - 31s/epoch - 156ms/step
Epoch 467/1000
2023-09-29 02:04:14.796 
Epoch 467/1000 
	 loss: 27.7562, MinusLogProbMetric: 27.7562, val_loss: 28.0438, val_MinusLogProbMetric: 28.0438

Epoch 467: val_loss did not improve from 27.97282
196/196 - 30s - loss: 27.7562 - MinusLogProbMetric: 27.7562 - val_loss: 28.0438 - val_MinusLogProbMetric: 28.0438 - lr: 2.5000e-04 - 30s/epoch - 153ms/step
Epoch 468/1000
2023-09-29 02:04:45.496 
Epoch 468/1000 
	 loss: 27.7559, MinusLogProbMetric: 27.7559, val_loss: 28.0225, val_MinusLogProbMetric: 28.0225

Epoch 468: val_loss did not improve from 27.97282
196/196 - 31s - loss: 27.7559 - MinusLogProbMetric: 27.7559 - val_loss: 28.0225 - val_MinusLogProbMetric: 28.0225 - lr: 2.5000e-04 - 31s/epoch - 157ms/step
Epoch 469/1000
2023-09-29 02:05:17.458 
Epoch 469/1000 
	 loss: 27.7505, MinusLogProbMetric: 27.7505, val_loss: 27.9902, val_MinusLogProbMetric: 27.9902

Epoch 469: val_loss did not improve from 27.97282
196/196 - 32s - loss: 27.7505 - MinusLogProbMetric: 27.7505 - val_loss: 27.9902 - val_MinusLogProbMetric: 27.9902 - lr: 2.5000e-04 - 32s/epoch - 163ms/step
Epoch 470/1000
2023-09-29 02:05:48.558 
Epoch 470/1000 
	 loss: 27.7583, MinusLogProbMetric: 27.7583, val_loss: 28.0053, val_MinusLogProbMetric: 28.0053

Epoch 470: val_loss did not improve from 27.97282
196/196 - 31s - loss: 27.7583 - MinusLogProbMetric: 27.7583 - val_loss: 28.0053 - val_MinusLogProbMetric: 28.0053 - lr: 2.5000e-04 - 31s/epoch - 159ms/step
Epoch 471/1000
2023-09-29 02:06:20.198 
Epoch 471/1000 
	 loss: 27.7550, MinusLogProbMetric: 27.7550, val_loss: 28.0189, val_MinusLogProbMetric: 28.0189

Epoch 471: val_loss did not improve from 27.97282
196/196 - 32s - loss: 27.7550 - MinusLogProbMetric: 27.7550 - val_loss: 28.0189 - val_MinusLogProbMetric: 28.0189 - lr: 2.5000e-04 - 32s/epoch - 161ms/step
Epoch 472/1000
2023-09-29 02:06:50.690 
Epoch 472/1000 
	 loss: 27.7650, MinusLogProbMetric: 27.7650, val_loss: 27.9818, val_MinusLogProbMetric: 27.9818

Epoch 472: val_loss did not improve from 27.97282
196/196 - 30s - loss: 27.7650 - MinusLogProbMetric: 27.7650 - val_loss: 27.9818 - val_MinusLogProbMetric: 27.9818 - lr: 2.5000e-04 - 30s/epoch - 156ms/step
Epoch 473/1000
2023-09-29 02:07:21.492 
Epoch 473/1000 
	 loss: 27.7526, MinusLogProbMetric: 27.7526, val_loss: 28.0050, val_MinusLogProbMetric: 28.0050

Epoch 473: val_loss did not improve from 27.97282
196/196 - 31s - loss: 27.7526 - MinusLogProbMetric: 27.7526 - val_loss: 28.0050 - val_MinusLogProbMetric: 28.0050 - lr: 2.5000e-04 - 31s/epoch - 157ms/step
Epoch 474/1000
2023-09-29 02:07:51.511 
Epoch 474/1000 
	 loss: 27.7690, MinusLogProbMetric: 27.7690, val_loss: 28.0554, val_MinusLogProbMetric: 28.0554

Epoch 474: val_loss did not improve from 27.97282
196/196 - 30s - loss: 27.7690 - MinusLogProbMetric: 27.7690 - val_loss: 28.0554 - val_MinusLogProbMetric: 28.0554 - lr: 2.5000e-04 - 30s/epoch - 153ms/step
Epoch 475/1000
2023-09-29 02:08:24.162 
Epoch 475/1000 
	 loss: 27.7687, MinusLogProbMetric: 27.7687, val_loss: 27.9890, val_MinusLogProbMetric: 27.9890

Epoch 475: val_loss did not improve from 27.97282
196/196 - 33s - loss: 27.7687 - MinusLogProbMetric: 27.7687 - val_loss: 27.9890 - val_MinusLogProbMetric: 27.9890 - lr: 2.5000e-04 - 33s/epoch - 167ms/step
Epoch 476/1000
2023-09-29 02:08:56.124 
Epoch 476/1000 
	 loss: 27.7528, MinusLogProbMetric: 27.7528, val_loss: 27.9937, val_MinusLogProbMetric: 27.9937

Epoch 476: val_loss did not improve from 27.97282
196/196 - 32s - loss: 27.7528 - MinusLogProbMetric: 27.7528 - val_loss: 27.9937 - val_MinusLogProbMetric: 27.9937 - lr: 2.5000e-04 - 32s/epoch - 163ms/step
Epoch 477/1000
2023-09-29 02:09:26.500 
Epoch 477/1000 
	 loss: 27.7539, MinusLogProbMetric: 27.7539, val_loss: 27.9932, val_MinusLogProbMetric: 27.9932

Epoch 477: val_loss did not improve from 27.97282
196/196 - 30s - loss: 27.7539 - MinusLogProbMetric: 27.7539 - val_loss: 27.9932 - val_MinusLogProbMetric: 27.9932 - lr: 2.5000e-04 - 30s/epoch - 155ms/step
Epoch 478/1000
2023-09-29 02:09:58.106 
Epoch 478/1000 
	 loss: 27.7603, MinusLogProbMetric: 27.7603, val_loss: 27.9807, val_MinusLogProbMetric: 27.9807

Epoch 478: val_loss did not improve from 27.97282
196/196 - 32s - loss: 27.7603 - MinusLogProbMetric: 27.7603 - val_loss: 27.9807 - val_MinusLogProbMetric: 27.9807 - lr: 2.5000e-04 - 32s/epoch - 161ms/step
Epoch 479/1000
2023-09-29 02:10:30.573 
Epoch 479/1000 
	 loss: 27.7607, MinusLogProbMetric: 27.7607, val_loss: 27.9733, val_MinusLogProbMetric: 27.9733

Epoch 479: val_loss did not improve from 27.97282
196/196 - 32s - loss: 27.7607 - MinusLogProbMetric: 27.7607 - val_loss: 27.9733 - val_MinusLogProbMetric: 27.9733 - lr: 2.5000e-04 - 32s/epoch - 166ms/step
Epoch 480/1000
2023-09-29 02:11:05.183 
Epoch 480/1000 
	 loss: 27.7540, MinusLogProbMetric: 27.7540, val_loss: 27.9791, val_MinusLogProbMetric: 27.9791

Epoch 480: val_loss did not improve from 27.97282
196/196 - 35s - loss: 27.7540 - MinusLogProbMetric: 27.7540 - val_loss: 27.9791 - val_MinusLogProbMetric: 27.9791 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 481/1000
2023-09-29 02:11:39.657 
Epoch 481/1000 
	 loss: 27.7661, MinusLogProbMetric: 27.7661, val_loss: 28.0101, val_MinusLogProbMetric: 28.0101

Epoch 481: val_loss did not improve from 27.97282
196/196 - 34s - loss: 27.7661 - MinusLogProbMetric: 27.7661 - val_loss: 28.0101 - val_MinusLogProbMetric: 28.0101 - lr: 2.5000e-04 - 34s/epoch - 176ms/step
Epoch 482/1000
2023-09-29 02:12:14.818 
Epoch 482/1000 
	 loss: 27.7575, MinusLogProbMetric: 27.7575, val_loss: 27.9873, val_MinusLogProbMetric: 27.9873

Epoch 482: val_loss did not improve from 27.97282
196/196 - 35s - loss: 27.7575 - MinusLogProbMetric: 27.7575 - val_loss: 27.9873 - val_MinusLogProbMetric: 27.9873 - lr: 2.5000e-04 - 35s/epoch - 179ms/step
Epoch 483/1000
2023-09-29 02:12:49.021 
Epoch 483/1000 
	 loss: 27.7686, MinusLogProbMetric: 27.7686, val_loss: 27.9764, val_MinusLogProbMetric: 27.9764

Epoch 483: val_loss did not improve from 27.97282
196/196 - 34s - loss: 27.7686 - MinusLogProbMetric: 27.7686 - val_loss: 27.9764 - val_MinusLogProbMetric: 27.9764 - lr: 2.5000e-04 - 34s/epoch - 174ms/step
Epoch 484/1000
2023-09-29 02:13:22.399 
Epoch 484/1000 
	 loss: 27.7583, MinusLogProbMetric: 27.7583, val_loss: 27.9472, val_MinusLogProbMetric: 27.9472

Epoch 484: val_loss improved from 27.97282 to 27.94720, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_329/weights/best_weights.h5
196/196 - 34s - loss: 27.7583 - MinusLogProbMetric: 27.7583 - val_loss: 27.9472 - val_MinusLogProbMetric: 27.9472 - lr: 2.5000e-04 - 34s/epoch - 173ms/step
Epoch 485/1000
2023-09-29 02:13:57.414 
Epoch 485/1000 
	 loss: 27.7535, MinusLogProbMetric: 27.7535, val_loss: 27.9674, val_MinusLogProbMetric: 27.9674

Epoch 485: val_loss did not improve from 27.94720
196/196 - 34s - loss: 27.7535 - MinusLogProbMetric: 27.7535 - val_loss: 27.9674 - val_MinusLogProbMetric: 27.9674 - lr: 2.5000e-04 - 34s/epoch - 176ms/step
Epoch 486/1000
2023-09-29 02:14:28.700 
Epoch 486/1000 
	 loss: 27.7552, MinusLogProbMetric: 27.7552, val_loss: 28.0224, val_MinusLogProbMetric: 28.0224

Epoch 486: val_loss did not improve from 27.94720
196/196 - 31s - loss: 27.7552 - MinusLogProbMetric: 27.7552 - val_loss: 28.0224 - val_MinusLogProbMetric: 28.0224 - lr: 2.5000e-04 - 31s/epoch - 160ms/step
Epoch 487/1000
2023-09-29 02:14:58.586 
Epoch 487/1000 
	 loss: 27.7628, MinusLogProbMetric: 27.7628, val_loss: 27.9695, val_MinusLogProbMetric: 27.9695

Epoch 487: val_loss did not improve from 27.94720
196/196 - 30s - loss: 27.7628 - MinusLogProbMetric: 27.7628 - val_loss: 27.9695 - val_MinusLogProbMetric: 27.9695 - lr: 2.5000e-04 - 30s/epoch - 152ms/step
Epoch 488/1000
2023-09-29 02:15:28.549 
Epoch 488/1000 
	 loss: 27.7481, MinusLogProbMetric: 27.7481, val_loss: 28.0196, val_MinusLogProbMetric: 28.0196

Epoch 488: val_loss did not improve from 27.94720
196/196 - 30s - loss: 27.7481 - MinusLogProbMetric: 27.7481 - val_loss: 28.0196 - val_MinusLogProbMetric: 28.0196 - lr: 2.5000e-04 - 30s/epoch - 153ms/step
Epoch 489/1000
2023-09-29 02:15:58.636 
Epoch 489/1000 
	 loss: 27.7522, MinusLogProbMetric: 27.7522, val_loss: 27.9789, val_MinusLogProbMetric: 27.9789

Epoch 489: val_loss did not improve from 27.94720
196/196 - 30s - loss: 27.7522 - MinusLogProbMetric: 27.7522 - val_loss: 27.9789 - val_MinusLogProbMetric: 27.9789 - lr: 2.5000e-04 - 30s/epoch - 153ms/step
Epoch 490/1000
2023-09-29 02:16:31.010 
Epoch 490/1000 
	 loss: 27.7674, MinusLogProbMetric: 27.7674, val_loss: 28.0478, val_MinusLogProbMetric: 28.0478

Epoch 490: val_loss did not improve from 27.94720
196/196 - 32s - loss: 27.7674 - MinusLogProbMetric: 27.7674 - val_loss: 28.0478 - val_MinusLogProbMetric: 28.0478 - lr: 2.5000e-04 - 32s/epoch - 165ms/step
Epoch 491/1000
2023-09-29 02:17:01.950 
Epoch 491/1000 
	 loss: 27.7516, MinusLogProbMetric: 27.7516, val_loss: 28.0258, val_MinusLogProbMetric: 28.0258

Epoch 491: val_loss did not improve from 27.94720
196/196 - 31s - loss: 27.7516 - MinusLogProbMetric: 27.7516 - val_loss: 28.0258 - val_MinusLogProbMetric: 28.0258 - lr: 2.5000e-04 - 31s/epoch - 158ms/step
Epoch 492/1000
2023-09-29 02:17:33.980 
Epoch 492/1000 
	 loss: 27.7638, MinusLogProbMetric: 27.7638, val_loss: 28.0632, val_MinusLogProbMetric: 28.0632

Epoch 492: val_loss did not improve from 27.94720
196/196 - 32s - loss: 27.7638 - MinusLogProbMetric: 27.7638 - val_loss: 28.0632 - val_MinusLogProbMetric: 28.0632 - lr: 2.5000e-04 - 32s/epoch - 163ms/step
Epoch 493/1000
2023-09-29 02:18:07.380 
Epoch 493/1000 
	 loss: 27.7618, MinusLogProbMetric: 27.7618, val_loss: 28.1127, val_MinusLogProbMetric: 28.1127

Epoch 493: val_loss did not improve from 27.94720
196/196 - 33s - loss: 27.7618 - MinusLogProbMetric: 27.7618 - val_loss: 28.1127 - val_MinusLogProbMetric: 28.1127 - lr: 2.5000e-04 - 33s/epoch - 170ms/step
Epoch 494/1000
2023-09-29 02:18:40.588 
Epoch 494/1000 
	 loss: 27.7409, MinusLogProbMetric: 27.7409, val_loss: 27.9788, val_MinusLogProbMetric: 27.9788

Epoch 494: val_loss did not improve from 27.94720
196/196 - 33s - loss: 27.7409 - MinusLogProbMetric: 27.7409 - val_loss: 27.9788 - val_MinusLogProbMetric: 27.9788 - lr: 2.5000e-04 - 33s/epoch - 169ms/step
Epoch 495/1000
2023-09-29 02:19:13.324 
Epoch 495/1000 
	 loss: 27.7519, MinusLogProbMetric: 27.7519, val_loss: 28.0584, val_MinusLogProbMetric: 28.0584

Epoch 495: val_loss did not improve from 27.94720
196/196 - 33s - loss: 27.7519 - MinusLogProbMetric: 27.7519 - val_loss: 28.0584 - val_MinusLogProbMetric: 28.0584 - lr: 2.5000e-04 - 33s/epoch - 167ms/step
Epoch 496/1000
2023-09-29 02:19:45.260 
Epoch 496/1000 
	 loss: 27.7424, MinusLogProbMetric: 27.7424, val_loss: 28.0455, val_MinusLogProbMetric: 28.0455

Epoch 496: val_loss did not improve from 27.94720
196/196 - 32s - loss: 27.7424 - MinusLogProbMetric: 27.7424 - val_loss: 28.0455 - val_MinusLogProbMetric: 28.0455 - lr: 2.5000e-04 - 32s/epoch - 163ms/step
Epoch 497/1000
2023-09-29 02:20:16.406 
Epoch 497/1000 
	 loss: 27.7530, MinusLogProbMetric: 27.7530, val_loss: 27.9883, val_MinusLogProbMetric: 27.9883

Epoch 497: val_loss did not improve from 27.94720
196/196 - 31s - loss: 27.7530 - MinusLogProbMetric: 27.7530 - val_loss: 27.9883 - val_MinusLogProbMetric: 27.9883 - lr: 2.5000e-04 - 31s/epoch - 159ms/step
Epoch 498/1000
2023-09-29 02:20:49.074 
Epoch 498/1000 
	 loss: 27.7569, MinusLogProbMetric: 27.7569, val_loss: 28.0080, val_MinusLogProbMetric: 28.0080

Epoch 498: val_loss did not improve from 27.94720
196/196 - 33s - loss: 27.7569 - MinusLogProbMetric: 27.7569 - val_loss: 28.0080 - val_MinusLogProbMetric: 28.0080 - lr: 2.5000e-04 - 33s/epoch - 167ms/step
Epoch 499/1000
2023-09-29 02:21:21.385 
Epoch 499/1000 
	 loss: 27.7555, MinusLogProbMetric: 27.7555, val_loss: 27.9718, val_MinusLogProbMetric: 27.9718

Epoch 499: val_loss did not improve from 27.94720
196/196 - 32s - loss: 27.7555 - MinusLogProbMetric: 27.7555 - val_loss: 27.9718 - val_MinusLogProbMetric: 27.9718 - lr: 2.5000e-04 - 32s/epoch - 165ms/step
Epoch 500/1000
2023-09-29 02:21:51.868 
Epoch 500/1000 
	 loss: 27.7577, MinusLogProbMetric: 27.7577, val_loss: 27.9569, val_MinusLogProbMetric: 27.9569

Epoch 500: val_loss did not improve from 27.94720
196/196 - 30s - loss: 27.7577 - MinusLogProbMetric: 27.7577 - val_loss: 27.9569 - val_MinusLogProbMetric: 27.9569 - lr: 2.5000e-04 - 30s/epoch - 156ms/step
Epoch 501/1000
2023-09-29 02:22:20.589 
Epoch 501/1000 
	 loss: 27.7554, MinusLogProbMetric: 27.7554, val_loss: 27.9474, val_MinusLogProbMetric: 27.9474

Epoch 501: val_loss did not improve from 27.94720
196/196 - 29s - loss: 27.7554 - MinusLogProbMetric: 27.7554 - val_loss: 27.9474 - val_MinusLogProbMetric: 27.9474 - lr: 2.5000e-04 - 29s/epoch - 147ms/step
Epoch 502/1000
2023-09-29 02:22:51.437 
Epoch 502/1000 
	 loss: 27.7474, MinusLogProbMetric: 27.7474, val_loss: 27.9742, val_MinusLogProbMetric: 27.9742

Epoch 502: val_loss did not improve from 27.94720
196/196 - 31s - loss: 27.7474 - MinusLogProbMetric: 27.7474 - val_loss: 27.9742 - val_MinusLogProbMetric: 27.9742 - lr: 2.5000e-04 - 31s/epoch - 157ms/step
Epoch 503/1000
2023-09-29 02:23:24.600 
Epoch 503/1000 
	 loss: 27.7458, MinusLogProbMetric: 27.7458, val_loss: 27.9541, val_MinusLogProbMetric: 27.9541

Epoch 503: val_loss did not improve from 27.94720
196/196 - 33s - loss: 27.7458 - MinusLogProbMetric: 27.7458 - val_loss: 27.9541 - val_MinusLogProbMetric: 27.9541 - lr: 2.5000e-04 - 33s/epoch - 169ms/step
Epoch 504/1000
2023-09-29 02:23:57.245 
Epoch 504/1000 
	 loss: 27.7529, MinusLogProbMetric: 27.7529, val_loss: 27.9793, val_MinusLogProbMetric: 27.9793

Epoch 504: val_loss did not improve from 27.94720
196/196 - 33s - loss: 27.7529 - MinusLogProbMetric: 27.7529 - val_loss: 27.9793 - val_MinusLogProbMetric: 27.9793 - lr: 2.5000e-04 - 33s/epoch - 167ms/step
Epoch 505/1000
2023-09-29 02:24:29.229 
Epoch 505/1000 
	 loss: 27.7468, MinusLogProbMetric: 27.7468, val_loss: 27.9951, val_MinusLogProbMetric: 27.9951

Epoch 505: val_loss did not improve from 27.94720
196/196 - 32s - loss: 27.7468 - MinusLogProbMetric: 27.7468 - val_loss: 27.9951 - val_MinusLogProbMetric: 27.9951 - lr: 2.5000e-04 - 32s/epoch - 163ms/step
Epoch 506/1000
2023-09-29 02:24:59.090 
Epoch 506/1000 
	 loss: 27.7485, MinusLogProbMetric: 27.7485, val_loss: 27.9946, val_MinusLogProbMetric: 27.9946

Epoch 506: val_loss did not improve from 27.94720
196/196 - 30s - loss: 27.7485 - MinusLogProbMetric: 27.7485 - val_loss: 27.9946 - val_MinusLogProbMetric: 27.9946 - lr: 2.5000e-04 - 30s/epoch - 152ms/step
Epoch 507/1000
2023-09-29 02:25:30.034 
Epoch 507/1000 
	 loss: 27.7490, MinusLogProbMetric: 27.7490, val_loss: 27.9713, val_MinusLogProbMetric: 27.9713

Epoch 507: val_loss did not improve from 27.94720
196/196 - 31s - loss: 27.7490 - MinusLogProbMetric: 27.7490 - val_loss: 27.9713 - val_MinusLogProbMetric: 27.9713 - lr: 2.5000e-04 - 31s/epoch - 158ms/step
Epoch 508/1000
2023-09-29 02:25:59.396 
Epoch 508/1000 
	 loss: 27.7452, MinusLogProbMetric: 27.7452, val_loss: 27.9922, val_MinusLogProbMetric: 27.9922

Epoch 508: val_loss did not improve from 27.94720
196/196 - 29s - loss: 27.7452 - MinusLogProbMetric: 27.7452 - val_loss: 27.9922 - val_MinusLogProbMetric: 27.9922 - lr: 2.5000e-04 - 29s/epoch - 150ms/step
Epoch 509/1000
2023-09-29 02:26:30.076 
Epoch 509/1000 
	 loss: 27.7440, MinusLogProbMetric: 27.7440, val_loss: 28.0062, val_MinusLogProbMetric: 28.0062

Epoch 509: val_loss did not improve from 27.94720
196/196 - 31s - loss: 27.7440 - MinusLogProbMetric: 27.7440 - val_loss: 28.0062 - val_MinusLogProbMetric: 28.0062 - lr: 2.5000e-04 - 31s/epoch - 157ms/step
Epoch 510/1000
2023-09-29 02:27:03.964 
Epoch 510/1000 
	 loss: 27.7532, MinusLogProbMetric: 27.7532, val_loss: 27.9748, val_MinusLogProbMetric: 27.9748

Epoch 510: val_loss did not improve from 27.94720
196/196 - 34s - loss: 27.7532 - MinusLogProbMetric: 27.7532 - val_loss: 27.9748 - val_MinusLogProbMetric: 27.9748 - lr: 2.5000e-04 - 34s/epoch - 173ms/step
Epoch 511/1000
2023-09-29 02:27:38.323 
Epoch 511/1000 
	 loss: 27.7470, MinusLogProbMetric: 27.7470, val_loss: 27.9785, val_MinusLogProbMetric: 27.9785

Epoch 511: val_loss did not improve from 27.94720
196/196 - 34s - loss: 27.7470 - MinusLogProbMetric: 27.7470 - val_loss: 27.9785 - val_MinusLogProbMetric: 27.9785 - lr: 2.5000e-04 - 34s/epoch - 175ms/step
Epoch 512/1000
2023-09-29 02:28:12.453 
Epoch 512/1000 
	 loss: 27.7401, MinusLogProbMetric: 27.7401, val_loss: 27.9522, val_MinusLogProbMetric: 27.9522

Epoch 512: val_loss did not improve from 27.94720
196/196 - 34s - loss: 27.7401 - MinusLogProbMetric: 27.7401 - val_loss: 27.9522 - val_MinusLogProbMetric: 27.9522 - lr: 2.5000e-04 - 34s/epoch - 174ms/step
Epoch 513/1000
2023-09-29 02:28:40.997 
Epoch 513/1000 
	 loss: 27.7622, MinusLogProbMetric: 27.7622, val_loss: 28.0071, val_MinusLogProbMetric: 28.0071

Epoch 513: val_loss did not improve from 27.94720
196/196 - 29s - loss: 27.7622 - MinusLogProbMetric: 27.7622 - val_loss: 28.0071 - val_MinusLogProbMetric: 28.0071 - lr: 2.5000e-04 - 29s/epoch - 146ms/step
Epoch 514/1000
2023-09-29 02:29:14.344 
Epoch 514/1000 
	 loss: 27.7356, MinusLogProbMetric: 27.7356, val_loss: 28.0035, val_MinusLogProbMetric: 28.0035

Epoch 514: val_loss did not improve from 27.94720
196/196 - 33s - loss: 27.7356 - MinusLogProbMetric: 27.7356 - val_loss: 28.0035 - val_MinusLogProbMetric: 28.0035 - lr: 2.5000e-04 - 33s/epoch - 170ms/step
Epoch 515/1000
2023-09-29 02:29:48.773 
Epoch 515/1000 
	 loss: 27.7481, MinusLogProbMetric: 27.7481, val_loss: 28.0104, val_MinusLogProbMetric: 28.0104

Epoch 515: val_loss did not improve from 27.94720
196/196 - 34s - loss: 27.7481 - MinusLogProbMetric: 27.7481 - val_loss: 28.0104 - val_MinusLogProbMetric: 28.0104 - lr: 2.5000e-04 - 34s/epoch - 176ms/step
Epoch 516/1000
2023-09-29 02:30:22.936 
Epoch 516/1000 
	 loss: 27.7508, MinusLogProbMetric: 27.7508, val_loss: 27.9969, val_MinusLogProbMetric: 27.9969

Epoch 516: val_loss did not improve from 27.94720
196/196 - 34s - loss: 27.7508 - MinusLogProbMetric: 27.7508 - val_loss: 27.9969 - val_MinusLogProbMetric: 27.9969 - lr: 2.5000e-04 - 34s/epoch - 174ms/step
Epoch 517/1000
2023-09-29 02:30:53.140 
Epoch 517/1000 
	 loss: 27.7534, MinusLogProbMetric: 27.7534, val_loss: 28.0330, val_MinusLogProbMetric: 28.0330

Epoch 517: val_loss did not improve from 27.94720
196/196 - 30s - loss: 27.7534 - MinusLogProbMetric: 27.7534 - val_loss: 28.0330 - val_MinusLogProbMetric: 28.0330 - lr: 2.5000e-04 - 30s/epoch - 154ms/step
Epoch 518/1000
2023-09-29 02:31:24.898 
Epoch 518/1000 
	 loss: 27.7422, MinusLogProbMetric: 27.7422, val_loss: 27.9966, val_MinusLogProbMetric: 27.9966

Epoch 518: val_loss did not improve from 27.94720
196/196 - 32s - loss: 27.7422 - MinusLogProbMetric: 27.7422 - val_loss: 27.9966 - val_MinusLogProbMetric: 27.9966 - lr: 2.5000e-04 - 32s/epoch - 162ms/step
Epoch 519/1000
2023-09-29 02:31:55.662 
Epoch 519/1000 
	 loss: 27.7434, MinusLogProbMetric: 27.7434, val_loss: 27.9694, val_MinusLogProbMetric: 27.9694

Epoch 519: val_loss did not improve from 27.94720
196/196 - 31s - loss: 27.7434 - MinusLogProbMetric: 27.7434 - val_loss: 27.9694 - val_MinusLogProbMetric: 27.9694 - lr: 2.5000e-04 - 31s/epoch - 157ms/step
Epoch 520/1000
2023-09-29 02:32:24.272 
Epoch 520/1000 
	 loss: 27.7293, MinusLogProbMetric: 27.7293, val_loss: 27.9615, val_MinusLogProbMetric: 27.9615

Epoch 520: val_loss did not improve from 27.94720
196/196 - 29s - loss: 27.7293 - MinusLogProbMetric: 27.7293 - val_loss: 27.9615 - val_MinusLogProbMetric: 27.9615 - lr: 2.5000e-04 - 29s/epoch - 146ms/step
Epoch 521/1000
2023-09-29 02:32:56.685 
Epoch 521/1000 
	 loss: 27.7422, MinusLogProbMetric: 27.7422, val_loss: 27.9970, val_MinusLogProbMetric: 27.9970

Epoch 521: val_loss did not improve from 27.94720
196/196 - 32s - loss: 27.7422 - MinusLogProbMetric: 27.7422 - val_loss: 27.9970 - val_MinusLogProbMetric: 27.9970 - lr: 2.5000e-04 - 32s/epoch - 165ms/step
Epoch 522/1000
2023-09-29 02:33:28.875 
Epoch 522/1000 
	 loss: 27.7428, MinusLogProbMetric: 27.7428, val_loss: 27.9898, val_MinusLogProbMetric: 27.9898

Epoch 522: val_loss did not improve from 27.94720
196/196 - 32s - loss: 27.7428 - MinusLogProbMetric: 27.7428 - val_loss: 27.9898 - val_MinusLogProbMetric: 27.9898 - lr: 2.5000e-04 - 32s/epoch - 164ms/step
Epoch 523/1000
2023-09-29 02:33:59.273 
Epoch 523/1000 
	 loss: 27.7494, MinusLogProbMetric: 27.7494, val_loss: 27.9869, val_MinusLogProbMetric: 27.9869

Epoch 523: val_loss did not improve from 27.94720
196/196 - 30s - loss: 27.7494 - MinusLogProbMetric: 27.7494 - val_loss: 27.9869 - val_MinusLogProbMetric: 27.9869 - lr: 2.5000e-04 - 30s/epoch - 155ms/step
Epoch 524/1000
2023-09-29 02:34:30.776 
Epoch 524/1000 
	 loss: 27.7361, MinusLogProbMetric: 27.7361, val_loss: 27.9302, val_MinusLogProbMetric: 27.9302

Epoch 524: val_loss improved from 27.94720 to 27.93022, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_329/weights/best_weights.h5
196/196 - 32s - loss: 27.7361 - MinusLogProbMetric: 27.7361 - val_loss: 27.9302 - val_MinusLogProbMetric: 27.9302 - lr: 2.5000e-04 - 32s/epoch - 166ms/step
Epoch 525/1000
2023-09-29 02:35:01.698 
Epoch 525/1000 
	 loss: 27.7328, MinusLogProbMetric: 27.7328, val_loss: 28.0263, val_MinusLogProbMetric: 28.0263

Epoch 525: val_loss did not improve from 27.93022
196/196 - 30s - loss: 27.7328 - MinusLogProbMetric: 27.7328 - val_loss: 28.0263 - val_MinusLogProbMetric: 28.0263 - lr: 2.5000e-04 - 30s/epoch - 153ms/step
Epoch 526/1000
2023-09-29 02:35:32.897 
Epoch 526/1000 
	 loss: 27.7648, MinusLogProbMetric: 27.7648, val_loss: 27.9507, val_MinusLogProbMetric: 27.9507

Epoch 526: val_loss did not improve from 27.93022
196/196 - 31s - loss: 27.7648 - MinusLogProbMetric: 27.7648 - val_loss: 27.9507 - val_MinusLogProbMetric: 27.9507 - lr: 2.5000e-04 - 31s/epoch - 159ms/step
Epoch 527/1000
2023-09-29 02:36:06.203 
Epoch 527/1000 
	 loss: 27.7419, MinusLogProbMetric: 27.7419, val_loss: 28.0247, val_MinusLogProbMetric: 28.0247

Epoch 527: val_loss did not improve from 27.93022
196/196 - 33s - loss: 27.7419 - MinusLogProbMetric: 27.7419 - val_loss: 28.0247 - val_MinusLogProbMetric: 28.0247 - lr: 2.5000e-04 - 33s/epoch - 170ms/step
Epoch 528/1000
2023-09-29 02:36:40.728 
Epoch 528/1000 
	 loss: 27.7483, MinusLogProbMetric: 27.7483, val_loss: 27.9843, val_MinusLogProbMetric: 27.9843

Epoch 528: val_loss did not improve from 27.93022
196/196 - 35s - loss: 27.7483 - MinusLogProbMetric: 27.7483 - val_loss: 27.9843 - val_MinusLogProbMetric: 27.9843 - lr: 2.5000e-04 - 35s/epoch - 176ms/step
Epoch 529/1000
2023-09-29 02:37:15.114 
Epoch 529/1000 
	 loss: 27.7397, MinusLogProbMetric: 27.7397, val_loss: 28.0023, val_MinusLogProbMetric: 28.0023

Epoch 529: val_loss did not improve from 27.93022
196/196 - 34s - loss: 27.7397 - MinusLogProbMetric: 27.7397 - val_loss: 28.0023 - val_MinusLogProbMetric: 28.0023 - lr: 2.5000e-04 - 34s/epoch - 175ms/step
Epoch 530/1000
2023-09-29 02:37:45.461 
Epoch 530/1000 
	 loss: 27.7353, MinusLogProbMetric: 27.7353, val_loss: 27.9533, val_MinusLogProbMetric: 27.9533

Epoch 530: val_loss did not improve from 27.93022
196/196 - 30s - loss: 27.7353 - MinusLogProbMetric: 27.7353 - val_loss: 27.9533 - val_MinusLogProbMetric: 27.9533 - lr: 2.5000e-04 - 30s/epoch - 155ms/step
Epoch 531/1000
2023-09-29 02:38:18.469 
Epoch 531/1000 
	 loss: 27.7346, MinusLogProbMetric: 27.7346, val_loss: 28.0013, val_MinusLogProbMetric: 28.0013

Epoch 531: val_loss did not improve from 27.93022
196/196 - 33s - loss: 27.7346 - MinusLogProbMetric: 27.7346 - val_loss: 28.0013 - val_MinusLogProbMetric: 28.0013 - lr: 2.5000e-04 - 33s/epoch - 168ms/step
Epoch 532/1000
2023-09-29 02:38:50.377 
Epoch 532/1000 
	 loss: 27.7403, MinusLogProbMetric: 27.7403, val_loss: 28.0806, val_MinusLogProbMetric: 28.0806

Epoch 532: val_loss did not improve from 27.93022
196/196 - 32s - loss: 27.7403 - MinusLogProbMetric: 27.7403 - val_loss: 28.0806 - val_MinusLogProbMetric: 28.0806 - lr: 2.5000e-04 - 32s/epoch - 163ms/step
Epoch 533/1000
2023-09-29 02:39:24.826 
Epoch 533/1000 
	 loss: 27.7466, MinusLogProbMetric: 27.7466, val_loss: 28.0944, val_MinusLogProbMetric: 28.0944

Epoch 533: val_loss did not improve from 27.93022
196/196 - 34s - loss: 27.7466 - MinusLogProbMetric: 27.7466 - val_loss: 28.0944 - val_MinusLogProbMetric: 28.0944 - lr: 2.5000e-04 - 34s/epoch - 176ms/step
Epoch 534/1000
2023-09-29 02:39:57.540 
Epoch 534/1000 
	 loss: 27.7417, MinusLogProbMetric: 27.7417, val_loss: 28.1556, val_MinusLogProbMetric: 28.1556

Epoch 534: val_loss did not improve from 27.93022
196/196 - 33s - loss: 27.7417 - MinusLogProbMetric: 27.7417 - val_loss: 28.1556 - val_MinusLogProbMetric: 28.1556 - lr: 2.5000e-04 - 33s/epoch - 167ms/step
Epoch 535/1000
2023-09-29 02:40:29.632 
Epoch 535/1000 
	 loss: 27.7516, MinusLogProbMetric: 27.7516, val_loss: 27.9991, val_MinusLogProbMetric: 27.9991

Epoch 535: val_loss did not improve from 27.93022
196/196 - 32s - loss: 27.7516 - MinusLogProbMetric: 27.7516 - val_loss: 27.9991 - val_MinusLogProbMetric: 27.9991 - lr: 2.5000e-04 - 32s/epoch - 164ms/step
Epoch 536/1000
2023-09-29 02:41:01.492 
Epoch 536/1000 
	 loss: 27.7405, MinusLogProbMetric: 27.7405, val_loss: 27.9601, val_MinusLogProbMetric: 27.9601

Epoch 536: val_loss did not improve from 27.93022
196/196 - 32s - loss: 27.7405 - MinusLogProbMetric: 27.7405 - val_loss: 27.9601 - val_MinusLogProbMetric: 27.9601 - lr: 2.5000e-04 - 32s/epoch - 163ms/step
Epoch 537/1000
2023-09-29 02:41:33.927 
Epoch 537/1000 
	 loss: 27.7380, MinusLogProbMetric: 27.7380, val_loss: 27.9556, val_MinusLogProbMetric: 27.9556

Epoch 537: val_loss did not improve from 27.93022
196/196 - 32s - loss: 27.7380 - MinusLogProbMetric: 27.7380 - val_loss: 27.9556 - val_MinusLogProbMetric: 27.9556 - lr: 2.5000e-04 - 32s/epoch - 165ms/step
Epoch 538/1000
2023-09-29 02:42:02.555 
Epoch 538/1000 
	 loss: 27.7412, MinusLogProbMetric: 27.7412, val_loss: 27.9555, val_MinusLogProbMetric: 27.9555

Epoch 538: val_loss did not improve from 27.93022
196/196 - 29s - loss: 27.7412 - MinusLogProbMetric: 27.7412 - val_loss: 27.9555 - val_MinusLogProbMetric: 27.9555 - lr: 2.5000e-04 - 29s/epoch - 146ms/step
Epoch 539/1000
2023-09-29 02:42:33.273 
Epoch 539/1000 
	 loss: 27.7444, MinusLogProbMetric: 27.7444, val_loss: 27.9832, val_MinusLogProbMetric: 27.9832

Epoch 539: val_loss did not improve from 27.93022
196/196 - 31s - loss: 27.7444 - MinusLogProbMetric: 27.7444 - val_loss: 27.9832 - val_MinusLogProbMetric: 27.9832 - lr: 2.5000e-04 - 31s/epoch - 157ms/step
Epoch 540/1000
2023-09-29 02:43:05.104 
Epoch 540/1000 
	 loss: 27.7464, MinusLogProbMetric: 27.7464, val_loss: 27.9328, val_MinusLogProbMetric: 27.9328

Epoch 540: val_loss did not improve from 27.93022
196/196 - 32s - loss: 27.7464 - MinusLogProbMetric: 27.7464 - val_loss: 27.9328 - val_MinusLogProbMetric: 27.9328 - lr: 2.5000e-04 - 32s/epoch - 162ms/step
Epoch 541/1000
2023-09-29 02:43:34.229 
Epoch 541/1000 
	 loss: 27.7312, MinusLogProbMetric: 27.7312, val_loss: 27.9267, val_MinusLogProbMetric: 27.9267

Epoch 541: val_loss improved from 27.93022 to 27.92669, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_329/weights/best_weights.h5
196/196 - 30s - loss: 27.7312 - MinusLogProbMetric: 27.7312 - val_loss: 27.9267 - val_MinusLogProbMetric: 27.9267 - lr: 2.5000e-04 - 30s/epoch - 151ms/step
Epoch 542/1000
2023-09-29 02:44:02.961 
Epoch 542/1000 
	 loss: 27.7365, MinusLogProbMetric: 27.7365, val_loss: 27.9487, val_MinusLogProbMetric: 27.9487

Epoch 542: val_loss did not improve from 27.92669
196/196 - 28s - loss: 27.7365 - MinusLogProbMetric: 27.7365 - val_loss: 27.9487 - val_MinusLogProbMetric: 27.9487 - lr: 2.5000e-04 - 28s/epoch - 144ms/step
Epoch 543/1000
2023-09-29 02:44:37.081 
Epoch 543/1000 
	 loss: 27.7446, MinusLogProbMetric: 27.7446, val_loss: 27.9948, val_MinusLogProbMetric: 27.9948

Epoch 543: val_loss did not improve from 27.92669
196/196 - 34s - loss: 27.7446 - MinusLogProbMetric: 27.7446 - val_loss: 27.9948 - val_MinusLogProbMetric: 27.9948 - lr: 2.5000e-04 - 34s/epoch - 174ms/step
Epoch 544/1000
2023-09-29 02:45:11.555 
Epoch 544/1000 
	 loss: 27.7476, MinusLogProbMetric: 27.7476, val_loss: 27.9645, val_MinusLogProbMetric: 27.9645

Epoch 544: val_loss did not improve from 27.92669
196/196 - 34s - loss: 27.7476 - MinusLogProbMetric: 27.7476 - val_loss: 27.9645 - val_MinusLogProbMetric: 27.9645 - lr: 2.5000e-04 - 34s/epoch - 176ms/step
Epoch 545/1000
2023-09-29 02:45:45.929 
Epoch 545/1000 
	 loss: 27.7314, MinusLogProbMetric: 27.7314, val_loss: 28.0064, val_MinusLogProbMetric: 28.0064

Epoch 545: val_loss did not improve from 27.92669
196/196 - 34s - loss: 27.7314 - MinusLogProbMetric: 27.7314 - val_loss: 28.0064 - val_MinusLogProbMetric: 28.0064 - lr: 2.5000e-04 - 34s/epoch - 175ms/step
Epoch 546/1000
2023-09-29 02:46:20.122 
Epoch 546/1000 
	 loss: 27.7451, MinusLogProbMetric: 27.7451, val_loss: 28.0226, val_MinusLogProbMetric: 28.0226

Epoch 546: val_loss did not improve from 27.92669
196/196 - 34s - loss: 27.7451 - MinusLogProbMetric: 27.7451 - val_loss: 28.0226 - val_MinusLogProbMetric: 28.0226 - lr: 2.5000e-04 - 34s/epoch - 174ms/step
Epoch 547/1000
2023-09-29 02:46:53.630 
Epoch 547/1000 
	 loss: 27.7442, MinusLogProbMetric: 27.7442, val_loss: 28.0154, val_MinusLogProbMetric: 28.0154

Epoch 547: val_loss did not improve from 27.92669
196/196 - 34s - loss: 27.7442 - MinusLogProbMetric: 27.7442 - val_loss: 28.0154 - val_MinusLogProbMetric: 28.0154 - lr: 2.5000e-04 - 34s/epoch - 171ms/step
Epoch 548/1000
2023-09-29 02:47:27.615 
Epoch 548/1000 
	 loss: 27.7494, MinusLogProbMetric: 27.7494, val_loss: 28.0696, val_MinusLogProbMetric: 28.0696

Epoch 548: val_loss did not improve from 27.92669
196/196 - 34s - loss: 27.7494 - MinusLogProbMetric: 27.7494 - val_loss: 28.0696 - val_MinusLogProbMetric: 28.0696 - lr: 2.5000e-04 - 34s/epoch - 173ms/step
Epoch 549/1000
2023-09-29 02:48:00.761 
Epoch 549/1000 
	 loss: 27.7326, MinusLogProbMetric: 27.7326, val_loss: 28.0345, val_MinusLogProbMetric: 28.0345

Epoch 549: val_loss did not improve from 27.92669
196/196 - 33s - loss: 27.7326 - MinusLogProbMetric: 27.7326 - val_loss: 28.0345 - val_MinusLogProbMetric: 28.0345 - lr: 2.5000e-04 - 33s/epoch - 169ms/step
Epoch 550/1000
2023-09-29 02:48:34.371 
Epoch 550/1000 
	 loss: 27.7635, MinusLogProbMetric: 27.7635, val_loss: 28.0437, val_MinusLogProbMetric: 28.0437

Epoch 550: val_loss did not improve from 27.92669
196/196 - 34s - loss: 27.7635 - MinusLogProbMetric: 27.7635 - val_loss: 28.0437 - val_MinusLogProbMetric: 28.0437 - lr: 2.5000e-04 - 34s/epoch - 171ms/step
Epoch 551/1000
2023-09-29 02:49:02.920 
Epoch 551/1000 
	 loss: 27.7463, MinusLogProbMetric: 27.7463, val_loss: 27.9470, val_MinusLogProbMetric: 27.9470

Epoch 551: val_loss did not improve from 27.92669
196/196 - 29s - loss: 27.7463 - MinusLogProbMetric: 27.7463 - val_loss: 27.9470 - val_MinusLogProbMetric: 27.9470 - lr: 2.5000e-04 - 29s/epoch - 146ms/step
Epoch 552/1000
2023-09-29 02:49:33.390 
Epoch 552/1000 
	 loss: 27.7276, MinusLogProbMetric: 27.7276, val_loss: 28.0687, val_MinusLogProbMetric: 28.0687

Epoch 552: val_loss did not improve from 27.92669
196/196 - 30s - loss: 27.7276 - MinusLogProbMetric: 27.7276 - val_loss: 28.0687 - val_MinusLogProbMetric: 28.0687 - lr: 2.5000e-04 - 30s/epoch - 155ms/step
Epoch 553/1000
2023-09-29 02:50:03.039 
Epoch 553/1000 
	 loss: 27.7508, MinusLogProbMetric: 27.7508, val_loss: 27.9319, val_MinusLogProbMetric: 27.9319

Epoch 553: val_loss did not improve from 27.92669
196/196 - 30s - loss: 27.7508 - MinusLogProbMetric: 27.7508 - val_loss: 27.9319 - val_MinusLogProbMetric: 27.9319 - lr: 2.5000e-04 - 30s/epoch - 151ms/step
Epoch 554/1000
2023-09-29 02:50:34.426 
Epoch 554/1000 
	 loss: 27.7366, MinusLogProbMetric: 27.7366, val_loss: 28.0016, val_MinusLogProbMetric: 28.0016

Epoch 554: val_loss did not improve from 27.92669
196/196 - 31s - loss: 27.7366 - MinusLogProbMetric: 27.7366 - val_loss: 28.0016 - val_MinusLogProbMetric: 28.0016 - lr: 2.5000e-04 - 31s/epoch - 160ms/step
Epoch 555/1000
2023-09-29 02:51:05.773 
Epoch 555/1000 
	 loss: 27.7397, MinusLogProbMetric: 27.7397, val_loss: 28.0168, val_MinusLogProbMetric: 28.0168

Epoch 555: val_loss did not improve from 27.92669
196/196 - 31s - loss: 27.7397 - MinusLogProbMetric: 27.7397 - val_loss: 28.0168 - val_MinusLogProbMetric: 28.0168 - lr: 2.5000e-04 - 31s/epoch - 160ms/step
Epoch 556/1000
2023-09-29 02:51:35.542 
Epoch 556/1000 
	 loss: 27.7378, MinusLogProbMetric: 27.7378, val_loss: 27.9948, val_MinusLogProbMetric: 27.9948

Epoch 556: val_loss did not improve from 27.92669
196/196 - 30s - loss: 27.7378 - MinusLogProbMetric: 27.7378 - val_loss: 27.9948 - val_MinusLogProbMetric: 27.9948 - lr: 2.5000e-04 - 30s/epoch - 152ms/step
Epoch 557/1000
2023-09-29 02:52:08.336 
Epoch 557/1000 
	 loss: 27.7577, MinusLogProbMetric: 27.7577, val_loss: 27.9538, val_MinusLogProbMetric: 27.9538

Epoch 557: val_loss did not improve from 27.92669
196/196 - 33s - loss: 27.7577 - MinusLogProbMetric: 27.7577 - val_loss: 27.9538 - val_MinusLogProbMetric: 27.9538 - lr: 2.5000e-04 - 33s/epoch - 167ms/step
Epoch 558/1000
2023-09-29 02:52:39.368 
Epoch 558/1000 
	 loss: 27.7441, MinusLogProbMetric: 27.7441, val_loss: 27.9950, val_MinusLogProbMetric: 27.9950

Epoch 558: val_loss did not improve from 27.92669
196/196 - 31s - loss: 27.7441 - MinusLogProbMetric: 27.7441 - val_loss: 27.9950 - val_MinusLogProbMetric: 27.9950 - lr: 2.5000e-04 - 31s/epoch - 158ms/step
Epoch 559/1000
2023-09-29 02:53:11.596 
Epoch 559/1000 
	 loss: 27.7458, MinusLogProbMetric: 27.7458, val_loss: 27.9655, val_MinusLogProbMetric: 27.9655

Epoch 559: val_loss did not improve from 27.92669
196/196 - 32s - loss: 27.7458 - MinusLogProbMetric: 27.7458 - val_loss: 27.9655 - val_MinusLogProbMetric: 27.9655 - lr: 2.5000e-04 - 32s/epoch - 164ms/step
Epoch 560/1000
2023-09-29 02:53:43.212 
Epoch 560/1000 
	 loss: 27.7381, MinusLogProbMetric: 27.7381, val_loss: 27.9782, val_MinusLogProbMetric: 27.9782

Epoch 560: val_loss did not improve from 27.92669
196/196 - 32s - loss: 27.7381 - MinusLogProbMetric: 27.7381 - val_loss: 27.9782 - val_MinusLogProbMetric: 27.9782 - lr: 2.5000e-04 - 32s/epoch - 161ms/step
Epoch 561/1000
2023-09-29 02:54:18.172 
Epoch 561/1000 
	 loss: 27.7381, MinusLogProbMetric: 27.7381, val_loss: 28.0124, val_MinusLogProbMetric: 28.0124

Epoch 561: val_loss did not improve from 27.92669
196/196 - 35s - loss: 27.7381 - MinusLogProbMetric: 27.7381 - val_loss: 28.0124 - val_MinusLogProbMetric: 28.0124 - lr: 2.5000e-04 - 35s/epoch - 178ms/step
Epoch 562/1000
2023-09-29 02:54:52.800 
Epoch 562/1000 
	 loss: 27.7372, MinusLogProbMetric: 27.7372, val_loss: 27.9570, val_MinusLogProbMetric: 27.9570

Epoch 562: val_loss did not improve from 27.92669
196/196 - 35s - loss: 27.7372 - MinusLogProbMetric: 27.7372 - val_loss: 27.9570 - val_MinusLogProbMetric: 27.9570 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 563/1000
2023-09-29 02:55:28.158 
Epoch 563/1000 
	 loss: 27.7394, MinusLogProbMetric: 27.7394, val_loss: 27.9856, val_MinusLogProbMetric: 27.9856

Epoch 563: val_loss did not improve from 27.92669
196/196 - 35s - loss: 27.7394 - MinusLogProbMetric: 27.7394 - val_loss: 27.9856 - val_MinusLogProbMetric: 27.9856 - lr: 2.5000e-04 - 35s/epoch - 180ms/step
Epoch 564/1000
2023-09-29 02:56:01.026 
Epoch 564/1000 
	 loss: 27.7348, MinusLogProbMetric: 27.7348, val_loss: 28.0037, val_MinusLogProbMetric: 28.0037

Epoch 564: val_loss did not improve from 27.92669
196/196 - 33s - loss: 27.7348 - MinusLogProbMetric: 27.7348 - val_loss: 28.0037 - val_MinusLogProbMetric: 28.0037 - lr: 2.5000e-04 - 33s/epoch - 168ms/step
Epoch 565/1000
2023-09-29 02:56:35.983 
Epoch 565/1000 
	 loss: 27.7356, MinusLogProbMetric: 27.7356, val_loss: 28.0378, val_MinusLogProbMetric: 28.0378

Epoch 565: val_loss did not improve from 27.92669
196/196 - 35s - loss: 27.7356 - MinusLogProbMetric: 27.7356 - val_loss: 28.0378 - val_MinusLogProbMetric: 28.0378 - lr: 2.5000e-04 - 35s/epoch - 178ms/step
Epoch 566/1000
2023-09-29 02:57:10.949 
Epoch 566/1000 
	 loss: 27.7295, MinusLogProbMetric: 27.7295, val_loss: 27.9637, val_MinusLogProbMetric: 27.9637

Epoch 566: val_loss did not improve from 27.92669
196/196 - 35s - loss: 27.7295 - MinusLogProbMetric: 27.7295 - val_loss: 27.9637 - val_MinusLogProbMetric: 27.9637 - lr: 2.5000e-04 - 35s/epoch - 178ms/step
Epoch 567/1000
2023-09-29 02:57:46.182 
Epoch 567/1000 
	 loss: 27.7315, MinusLogProbMetric: 27.7315, val_loss: 28.0018, val_MinusLogProbMetric: 28.0018

Epoch 567: val_loss did not improve from 27.92669
196/196 - 35s - loss: 27.7315 - MinusLogProbMetric: 27.7315 - val_loss: 28.0018 - val_MinusLogProbMetric: 28.0018 - lr: 2.5000e-04 - 35s/epoch - 180ms/step
Epoch 568/1000
2023-09-29 02:58:21.451 
Epoch 568/1000 
	 loss: 27.7366, MinusLogProbMetric: 27.7366, val_loss: 27.9846, val_MinusLogProbMetric: 27.9846

Epoch 568: val_loss did not improve from 27.92669
196/196 - 35s - loss: 27.7366 - MinusLogProbMetric: 27.7366 - val_loss: 27.9846 - val_MinusLogProbMetric: 27.9846 - lr: 2.5000e-04 - 35s/epoch - 180ms/step
Epoch 569/1000
2023-09-29 02:58:55.936 
Epoch 569/1000 
	 loss: 27.7472, MinusLogProbMetric: 27.7472, val_loss: 27.9141, val_MinusLogProbMetric: 27.9141

Epoch 569: val_loss improved from 27.92669 to 27.91411, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_329/weights/best_weights.h5
196/196 - 35s - loss: 27.7472 - MinusLogProbMetric: 27.7472 - val_loss: 27.9141 - val_MinusLogProbMetric: 27.9141 - lr: 2.5000e-04 - 35s/epoch - 180ms/step
Epoch 570/1000
2023-09-29 02:59:31.947 
Epoch 570/1000 
	 loss: 27.7286, MinusLogProbMetric: 27.7286, val_loss: 27.9424, val_MinusLogProbMetric: 27.9424

Epoch 570: val_loss did not improve from 27.91411
196/196 - 35s - loss: 27.7286 - MinusLogProbMetric: 27.7286 - val_loss: 27.9424 - val_MinusLogProbMetric: 27.9424 - lr: 2.5000e-04 - 35s/epoch - 180ms/step
Epoch 571/1000
2023-09-29 03:00:03.390 
Epoch 571/1000 
	 loss: 27.7332, MinusLogProbMetric: 27.7332, val_loss: 27.9762, val_MinusLogProbMetric: 27.9762

Epoch 571: val_loss did not improve from 27.91411
196/196 - 31s - loss: 27.7332 - MinusLogProbMetric: 27.7332 - val_loss: 27.9762 - val_MinusLogProbMetric: 27.9762 - lr: 2.5000e-04 - 31s/epoch - 160ms/step
Epoch 572/1000
2023-09-29 03:00:36.345 
Epoch 572/1000 
	 loss: 27.7354, MinusLogProbMetric: 27.7354, val_loss: 28.0052, val_MinusLogProbMetric: 28.0052

Epoch 572: val_loss did not improve from 27.91411
196/196 - 33s - loss: 27.7354 - MinusLogProbMetric: 27.7354 - val_loss: 28.0052 - val_MinusLogProbMetric: 28.0052 - lr: 2.5000e-04 - 33s/epoch - 168ms/step
Epoch 573/1000
2023-09-29 03:01:10.651 
Epoch 573/1000 
	 loss: 27.7303, MinusLogProbMetric: 27.7303, val_loss: 28.0480, val_MinusLogProbMetric: 28.0480

Epoch 573: val_loss did not improve from 27.91411
196/196 - 34s - loss: 27.7303 - MinusLogProbMetric: 27.7303 - val_loss: 28.0480 - val_MinusLogProbMetric: 28.0480 - lr: 2.5000e-04 - 34s/epoch - 175ms/step
Epoch 574/1000
2023-09-29 03:01:45.677 
Epoch 574/1000 
	 loss: 27.7311, MinusLogProbMetric: 27.7311, val_loss: 27.9883, val_MinusLogProbMetric: 27.9883

Epoch 574: val_loss did not improve from 27.91411
196/196 - 35s - loss: 27.7311 - MinusLogProbMetric: 27.7311 - val_loss: 27.9883 - val_MinusLogProbMetric: 27.9883 - lr: 2.5000e-04 - 35s/epoch - 179ms/step
Epoch 575/1000
2023-09-29 03:02:19.274 
Epoch 575/1000 
	 loss: 27.7327, MinusLogProbMetric: 27.7327, val_loss: 27.9546, val_MinusLogProbMetric: 27.9546

Epoch 575: val_loss did not improve from 27.91411
196/196 - 34s - loss: 27.7327 - MinusLogProbMetric: 27.7327 - val_loss: 27.9546 - val_MinusLogProbMetric: 27.9546 - lr: 2.5000e-04 - 34s/epoch - 171ms/step
Epoch 576/1000
2023-09-29 03:02:54.181 
Epoch 576/1000 
	 loss: 27.7424, MinusLogProbMetric: 27.7424, val_loss: 28.0069, val_MinusLogProbMetric: 28.0069

Epoch 576: val_loss did not improve from 27.91411
196/196 - 35s - loss: 27.7424 - MinusLogProbMetric: 27.7424 - val_loss: 28.0069 - val_MinusLogProbMetric: 28.0069 - lr: 2.5000e-04 - 35s/epoch - 178ms/step
Epoch 577/1000
2023-09-29 03:03:26.953 
Epoch 577/1000 
	 loss: 27.7360, MinusLogProbMetric: 27.7360, val_loss: 27.9461, val_MinusLogProbMetric: 27.9461

Epoch 577: val_loss did not improve from 27.91411
196/196 - 33s - loss: 27.7360 - MinusLogProbMetric: 27.7360 - val_loss: 27.9461 - val_MinusLogProbMetric: 27.9461 - lr: 2.5000e-04 - 33s/epoch - 167ms/step
Epoch 578/1000
2023-09-29 03:04:01.957 
Epoch 578/1000 
	 loss: 27.7311, MinusLogProbMetric: 27.7311, val_loss: 27.9708, val_MinusLogProbMetric: 27.9708

Epoch 578: val_loss did not improve from 27.91411
196/196 - 35s - loss: 27.7311 - MinusLogProbMetric: 27.7311 - val_loss: 27.9708 - val_MinusLogProbMetric: 27.9708 - lr: 2.5000e-04 - 35s/epoch - 179ms/step
Epoch 579/1000
2023-09-29 03:04:37.083 
Epoch 579/1000 
	 loss: 27.7269, MinusLogProbMetric: 27.7269, val_loss: 28.0285, val_MinusLogProbMetric: 28.0285

Epoch 579: val_loss did not improve from 27.91411
196/196 - 35s - loss: 27.7269 - MinusLogProbMetric: 27.7269 - val_loss: 28.0285 - val_MinusLogProbMetric: 28.0285 - lr: 2.5000e-04 - 35s/epoch - 179ms/step
Epoch 580/1000
2023-09-29 03:05:12.299 
Epoch 580/1000 
	 loss: 27.7256, MinusLogProbMetric: 27.7256, val_loss: 27.9474, val_MinusLogProbMetric: 27.9474

Epoch 580: val_loss did not improve from 27.91411
196/196 - 35s - loss: 27.7256 - MinusLogProbMetric: 27.7256 - val_loss: 27.9474 - val_MinusLogProbMetric: 27.9474 - lr: 2.5000e-04 - 35s/epoch - 180ms/step
Epoch 581/1000
2023-09-29 03:05:47.372 
Epoch 581/1000 
	 loss: 27.7241, MinusLogProbMetric: 27.7241, val_loss: 27.9783, val_MinusLogProbMetric: 27.9783

Epoch 581: val_loss did not improve from 27.91411
196/196 - 35s - loss: 27.7241 - MinusLogProbMetric: 27.7241 - val_loss: 27.9783 - val_MinusLogProbMetric: 27.9783 - lr: 2.5000e-04 - 35s/epoch - 179ms/step
Epoch 582/1000
2023-09-29 03:06:21.844 
Epoch 582/1000 
	 loss: 27.7241, MinusLogProbMetric: 27.7241, val_loss: 27.9837, val_MinusLogProbMetric: 27.9837

Epoch 582: val_loss did not improve from 27.91411
196/196 - 34s - loss: 27.7241 - MinusLogProbMetric: 27.7241 - val_loss: 27.9837 - val_MinusLogProbMetric: 27.9837 - lr: 2.5000e-04 - 34s/epoch - 176ms/step
Epoch 583/1000
2023-09-29 03:06:55.044 
Epoch 583/1000 
	 loss: 27.7453, MinusLogProbMetric: 27.7453, val_loss: 27.9658, val_MinusLogProbMetric: 27.9658

Epoch 583: val_loss did not improve from 27.91411
196/196 - 33s - loss: 27.7453 - MinusLogProbMetric: 27.7453 - val_loss: 27.9658 - val_MinusLogProbMetric: 27.9658 - lr: 2.5000e-04 - 33s/epoch - 169ms/step
Epoch 584/1000
2023-09-29 03:07:28.896 
Epoch 584/1000 
	 loss: 27.7208, MinusLogProbMetric: 27.7208, val_loss: 27.9947, val_MinusLogProbMetric: 27.9947

Epoch 584: val_loss did not improve from 27.91411
196/196 - 34s - loss: 27.7208 - MinusLogProbMetric: 27.7208 - val_loss: 27.9947 - val_MinusLogProbMetric: 27.9947 - lr: 2.5000e-04 - 34s/epoch - 173ms/step
Epoch 585/1000
2023-09-29 03:08:02.847 
Epoch 585/1000 
	 loss: 27.7412, MinusLogProbMetric: 27.7412, val_loss: 27.9686, val_MinusLogProbMetric: 27.9686

Epoch 585: val_loss did not improve from 27.91411
196/196 - 34s - loss: 27.7412 - MinusLogProbMetric: 27.7412 - val_loss: 27.9686 - val_MinusLogProbMetric: 27.9686 - lr: 2.5000e-04 - 34s/epoch - 173ms/step
Epoch 586/1000
2023-09-29 03:08:35.614 
Epoch 586/1000 
	 loss: 27.7418, MinusLogProbMetric: 27.7418, val_loss: 27.9953, val_MinusLogProbMetric: 27.9953

Epoch 586: val_loss did not improve from 27.91411
196/196 - 33s - loss: 27.7418 - MinusLogProbMetric: 27.7418 - val_loss: 27.9953 - val_MinusLogProbMetric: 27.9953 - lr: 2.5000e-04 - 33s/epoch - 167ms/step
Epoch 587/1000
2023-09-29 03:09:10.594 
Epoch 587/1000 
	 loss: 27.7300, MinusLogProbMetric: 27.7300, val_loss: 27.9556, val_MinusLogProbMetric: 27.9556

Epoch 587: val_loss did not improve from 27.91411
196/196 - 35s - loss: 27.7300 - MinusLogProbMetric: 27.7300 - val_loss: 27.9556 - val_MinusLogProbMetric: 27.9556 - lr: 2.5000e-04 - 35s/epoch - 178ms/step
Epoch 588/1000
2023-09-29 03:09:44.420 
Epoch 588/1000 
	 loss: 27.7203, MinusLogProbMetric: 27.7203, val_loss: 27.9610, val_MinusLogProbMetric: 27.9610

Epoch 588: val_loss did not improve from 27.91411
196/196 - 34s - loss: 27.7203 - MinusLogProbMetric: 27.7203 - val_loss: 27.9610 - val_MinusLogProbMetric: 27.9610 - lr: 2.5000e-04 - 34s/epoch - 173ms/step
Epoch 589/1000
2023-09-29 03:10:19.715 
Epoch 589/1000 
	 loss: 27.7256, MinusLogProbMetric: 27.7256, val_loss: 28.0160, val_MinusLogProbMetric: 28.0160

Epoch 589: val_loss did not improve from 27.91411
196/196 - 35s - loss: 27.7256 - MinusLogProbMetric: 27.7256 - val_loss: 28.0160 - val_MinusLogProbMetric: 28.0160 - lr: 2.5000e-04 - 35s/epoch - 180ms/step
Epoch 590/1000
2023-09-29 03:10:52.614 
Epoch 590/1000 
	 loss: 27.7314, MinusLogProbMetric: 27.7314, val_loss: 27.9741, val_MinusLogProbMetric: 27.9741

Epoch 590: val_loss did not improve from 27.91411
196/196 - 33s - loss: 27.7314 - MinusLogProbMetric: 27.7314 - val_loss: 27.9741 - val_MinusLogProbMetric: 27.9741 - lr: 2.5000e-04 - 33s/epoch - 168ms/step
Epoch 591/1000
2023-09-29 03:11:27.460 
Epoch 591/1000 
	 loss: 27.7287, MinusLogProbMetric: 27.7287, val_loss: 27.9767, val_MinusLogProbMetric: 27.9767

Epoch 591: val_loss did not improve from 27.91411
196/196 - 35s - loss: 27.7287 - MinusLogProbMetric: 27.7287 - val_loss: 27.9767 - val_MinusLogProbMetric: 27.9767 - lr: 2.5000e-04 - 35s/epoch - 178ms/step
Epoch 592/1000
2023-09-29 03:12:02.569 
Epoch 592/1000 
	 loss: 27.7382, MinusLogProbMetric: 27.7382, val_loss: 27.9851, val_MinusLogProbMetric: 27.9851

Epoch 592: val_loss did not improve from 27.91411
196/196 - 35s - loss: 27.7382 - MinusLogProbMetric: 27.7382 - val_loss: 27.9851 - val_MinusLogProbMetric: 27.9851 - lr: 2.5000e-04 - 35s/epoch - 179ms/step
Epoch 593/1000
2023-09-29 03:12:37.686 
Epoch 593/1000 
	 loss: 27.7355, MinusLogProbMetric: 27.7355, val_loss: 27.9990, val_MinusLogProbMetric: 27.9990

Epoch 593: val_loss did not improve from 27.91411
196/196 - 35s - loss: 27.7355 - MinusLogProbMetric: 27.7355 - val_loss: 27.9990 - val_MinusLogProbMetric: 27.9990 - lr: 2.5000e-04 - 35s/epoch - 179ms/step
Epoch 594/1000
2023-09-29 03:13:12.878 
Epoch 594/1000 
	 loss: 27.7387, MinusLogProbMetric: 27.7387, val_loss: 27.9388, val_MinusLogProbMetric: 27.9388

Epoch 594: val_loss did not improve from 27.91411
196/196 - 35s - loss: 27.7387 - MinusLogProbMetric: 27.7387 - val_loss: 27.9388 - val_MinusLogProbMetric: 27.9388 - lr: 2.5000e-04 - 35s/epoch - 180ms/step
Epoch 595/1000
2023-09-29 03:13:47.931 
Epoch 595/1000 
	 loss: 27.7275, MinusLogProbMetric: 27.7275, val_loss: 27.9572, val_MinusLogProbMetric: 27.9572

Epoch 595: val_loss did not improve from 27.91411
196/196 - 35s - loss: 27.7275 - MinusLogProbMetric: 27.7275 - val_loss: 27.9572 - val_MinusLogProbMetric: 27.9572 - lr: 2.5000e-04 - 35s/epoch - 179ms/step
Epoch 596/1000
2023-09-29 03:14:23.136 
Epoch 596/1000 
	 loss: 27.7296, MinusLogProbMetric: 27.7296, val_loss: 27.9590, val_MinusLogProbMetric: 27.9590

Epoch 596: val_loss did not improve from 27.91411
196/196 - 35s - loss: 27.7296 - MinusLogProbMetric: 27.7296 - val_loss: 27.9590 - val_MinusLogProbMetric: 27.9590 - lr: 2.5000e-04 - 35s/epoch - 180ms/step
Epoch 597/1000
2023-09-29 03:14:58.318 
Epoch 597/1000 
	 loss: 27.7366, MinusLogProbMetric: 27.7366, val_loss: 27.9201, val_MinusLogProbMetric: 27.9201

Epoch 597: val_loss did not improve from 27.91411
196/196 - 35s - loss: 27.7366 - MinusLogProbMetric: 27.7366 - val_loss: 27.9201 - val_MinusLogProbMetric: 27.9201 - lr: 2.5000e-04 - 35s/epoch - 179ms/step
Epoch 598/1000
2023-09-29 03:15:32.199 
Epoch 598/1000 
	 loss: 27.7262, MinusLogProbMetric: 27.7262, val_loss: 27.9821, val_MinusLogProbMetric: 27.9821

Epoch 598: val_loss did not improve from 27.91411
196/196 - 34s - loss: 27.7262 - MinusLogProbMetric: 27.7262 - val_loss: 27.9821 - val_MinusLogProbMetric: 27.9821 - lr: 2.5000e-04 - 34s/epoch - 173ms/step
Epoch 599/1000
2023-09-29 03:16:07.087 
Epoch 599/1000 
	 loss: 27.7278, MinusLogProbMetric: 27.7278, val_loss: 27.9528, val_MinusLogProbMetric: 27.9528

Epoch 599: val_loss did not improve from 27.91411
196/196 - 35s - loss: 27.7278 - MinusLogProbMetric: 27.7278 - val_loss: 27.9528 - val_MinusLogProbMetric: 27.9528 - lr: 2.5000e-04 - 35s/epoch - 178ms/step
Epoch 600/1000
2023-09-29 03:16:40.849 
Epoch 600/1000 
	 loss: 27.7197, MinusLogProbMetric: 27.7197, val_loss: 27.9683, val_MinusLogProbMetric: 27.9683

Epoch 600: val_loss did not improve from 27.91411
196/196 - 34s - loss: 27.7197 - MinusLogProbMetric: 27.7197 - val_loss: 27.9683 - val_MinusLogProbMetric: 27.9683 - lr: 2.5000e-04 - 34s/epoch - 172ms/step
Epoch 601/1000
2023-09-29 03:17:13.822 
Epoch 601/1000 
	 loss: 27.7238, MinusLogProbMetric: 27.7238, val_loss: 27.9345, val_MinusLogProbMetric: 27.9345

Epoch 601: val_loss did not improve from 27.91411
196/196 - 33s - loss: 27.7238 - MinusLogProbMetric: 27.7238 - val_loss: 27.9345 - val_MinusLogProbMetric: 27.9345 - lr: 2.5000e-04 - 33s/epoch - 168ms/step
Epoch 602/1000
2023-09-29 03:17:47.689 
Epoch 602/1000 
	 loss: 27.7190, MinusLogProbMetric: 27.7190, val_loss: 27.9597, val_MinusLogProbMetric: 27.9597

Epoch 602: val_loss did not improve from 27.91411
196/196 - 34s - loss: 27.7190 - MinusLogProbMetric: 27.7190 - val_loss: 27.9597 - val_MinusLogProbMetric: 27.9597 - lr: 2.5000e-04 - 34s/epoch - 173ms/step
Epoch 603/1000
2023-09-29 03:18:22.961 
Epoch 603/1000 
	 loss: 27.7222, MinusLogProbMetric: 27.7222, val_loss: 27.9670, val_MinusLogProbMetric: 27.9670

Epoch 603: val_loss did not improve from 27.91411
196/196 - 35s - loss: 27.7222 - MinusLogProbMetric: 27.7222 - val_loss: 27.9670 - val_MinusLogProbMetric: 27.9670 - lr: 2.5000e-04 - 35s/epoch - 180ms/step
Epoch 604/1000
2023-09-29 03:18:57.666 
Epoch 604/1000 
	 loss: 27.7278, MinusLogProbMetric: 27.7278, val_loss: 28.0175, val_MinusLogProbMetric: 28.0175

Epoch 604: val_loss did not improve from 27.91411
196/196 - 35s - loss: 27.7278 - MinusLogProbMetric: 27.7278 - val_loss: 28.0175 - val_MinusLogProbMetric: 28.0175 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 605/1000
2023-09-29 03:19:29.820 
Epoch 605/1000 
	 loss: 27.7147, MinusLogProbMetric: 27.7147, val_loss: 27.9213, val_MinusLogProbMetric: 27.9213

Epoch 605: val_loss did not improve from 27.91411
196/196 - 32s - loss: 27.7147 - MinusLogProbMetric: 27.7147 - val_loss: 27.9213 - val_MinusLogProbMetric: 27.9213 - lr: 2.5000e-04 - 32s/epoch - 164ms/step
Epoch 606/1000
2023-09-29 03:20:04.624 
Epoch 606/1000 
	 loss: 27.7377, MinusLogProbMetric: 27.7377, val_loss: 28.0782, val_MinusLogProbMetric: 28.0782

Epoch 606: val_loss did not improve from 27.91411
196/196 - 35s - loss: 27.7377 - MinusLogProbMetric: 27.7377 - val_loss: 28.0782 - val_MinusLogProbMetric: 28.0782 - lr: 2.5000e-04 - 35s/epoch - 178ms/step
Epoch 607/1000
2023-09-29 03:20:38.585 
Epoch 607/1000 
	 loss: 27.7245, MinusLogProbMetric: 27.7245, val_loss: 27.9538, val_MinusLogProbMetric: 27.9538

Epoch 607: val_loss did not improve from 27.91411
196/196 - 34s - loss: 27.7245 - MinusLogProbMetric: 27.7245 - val_loss: 27.9538 - val_MinusLogProbMetric: 27.9538 - lr: 2.5000e-04 - 34s/epoch - 173ms/step
Epoch 608/1000
2023-09-29 03:21:13.710 
Epoch 608/1000 
	 loss: 27.7394, MinusLogProbMetric: 27.7394, val_loss: 27.9280, val_MinusLogProbMetric: 27.9280

Epoch 608: val_loss did not improve from 27.91411
196/196 - 35s - loss: 27.7394 - MinusLogProbMetric: 27.7394 - val_loss: 27.9280 - val_MinusLogProbMetric: 27.9280 - lr: 2.5000e-04 - 35s/epoch - 179ms/step
Epoch 609/1000
2023-09-29 03:21:48.924 
Epoch 609/1000 
	 loss: 27.7242, MinusLogProbMetric: 27.7242, val_loss: 27.9464, val_MinusLogProbMetric: 27.9464

Epoch 609: val_loss did not improve from 27.91411
196/196 - 35s - loss: 27.7242 - MinusLogProbMetric: 27.7242 - val_loss: 27.9464 - val_MinusLogProbMetric: 27.9464 - lr: 2.5000e-04 - 35s/epoch - 180ms/step
Epoch 610/1000
2023-09-29 03:22:23.519 
Epoch 610/1000 
	 loss: 27.7282, MinusLogProbMetric: 27.7282, val_loss: 27.9509, val_MinusLogProbMetric: 27.9509

Epoch 610: val_loss did not improve from 27.91411
196/196 - 35s - loss: 27.7282 - MinusLogProbMetric: 27.7282 - val_loss: 27.9509 - val_MinusLogProbMetric: 27.9509 - lr: 2.5000e-04 - 35s/epoch - 176ms/step
Epoch 611/1000
2023-09-29 03:22:57.438 
Epoch 611/1000 
	 loss: 27.7259, MinusLogProbMetric: 27.7259, val_loss: 27.9725, val_MinusLogProbMetric: 27.9725

Epoch 611: val_loss did not improve from 27.91411
196/196 - 34s - loss: 27.7259 - MinusLogProbMetric: 27.7259 - val_loss: 27.9725 - val_MinusLogProbMetric: 27.9725 - lr: 2.5000e-04 - 34s/epoch - 173ms/step
Epoch 612/1000
2023-09-29 03:23:32.743 
Epoch 612/1000 
	 loss: 27.7316, MinusLogProbMetric: 27.7316, val_loss: 27.9501, val_MinusLogProbMetric: 27.9501

Epoch 612: val_loss did not improve from 27.91411
196/196 - 35s - loss: 27.7316 - MinusLogProbMetric: 27.7316 - val_loss: 27.9501 - val_MinusLogProbMetric: 27.9501 - lr: 2.5000e-04 - 35s/epoch - 180ms/step
Epoch 613/1000
2023-09-29 03:24:07.487 
Epoch 613/1000 
	 loss: 27.7278, MinusLogProbMetric: 27.7278, val_loss: 27.9866, val_MinusLogProbMetric: 27.9866

Epoch 613: val_loss did not improve from 27.91411
196/196 - 35s - loss: 27.7278 - MinusLogProbMetric: 27.7278 - val_loss: 27.9866 - val_MinusLogProbMetric: 27.9866 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 614/1000
2023-09-29 03:24:42.088 
Epoch 614/1000 
	 loss: 27.7138, MinusLogProbMetric: 27.7138, val_loss: 27.9969, val_MinusLogProbMetric: 27.9969

Epoch 614: val_loss did not improve from 27.91411
196/196 - 35s - loss: 27.7138 - MinusLogProbMetric: 27.7138 - val_loss: 27.9969 - val_MinusLogProbMetric: 27.9969 - lr: 2.5000e-04 - 35s/epoch - 176ms/step
Epoch 615/1000
2023-09-29 03:25:14.476 
Epoch 615/1000 
	 loss: 27.7228, MinusLogProbMetric: 27.7228, val_loss: 27.9693, val_MinusLogProbMetric: 27.9693

Epoch 615: val_loss did not improve from 27.91411
196/196 - 32s - loss: 27.7228 - MinusLogProbMetric: 27.7228 - val_loss: 27.9693 - val_MinusLogProbMetric: 27.9693 - lr: 2.5000e-04 - 32s/epoch - 165ms/step
Epoch 616/1000
2023-09-29 03:25:47.855 
Epoch 616/1000 
	 loss: 27.7257, MinusLogProbMetric: 27.7257, val_loss: 27.9707, val_MinusLogProbMetric: 27.9707

Epoch 616: val_loss did not improve from 27.91411
196/196 - 33s - loss: 27.7257 - MinusLogProbMetric: 27.7257 - val_loss: 27.9707 - val_MinusLogProbMetric: 27.9707 - lr: 2.5000e-04 - 33s/epoch - 170ms/step
Epoch 617/1000
2023-09-29 03:26:22.857 
Epoch 617/1000 
	 loss: 27.7262, MinusLogProbMetric: 27.7262, val_loss: 27.9812, val_MinusLogProbMetric: 27.9812

Epoch 617: val_loss did not improve from 27.91411
196/196 - 35s - loss: 27.7262 - MinusLogProbMetric: 27.7262 - val_loss: 27.9812 - val_MinusLogProbMetric: 27.9812 - lr: 2.5000e-04 - 35s/epoch - 179ms/step
Epoch 618/1000
2023-09-29 03:26:55.120 
Epoch 618/1000 
	 loss: 27.7204, MinusLogProbMetric: 27.7204, val_loss: 27.9849, val_MinusLogProbMetric: 27.9849

Epoch 618: val_loss did not improve from 27.91411
196/196 - 32s - loss: 27.7204 - MinusLogProbMetric: 27.7204 - val_loss: 27.9849 - val_MinusLogProbMetric: 27.9849 - lr: 2.5000e-04 - 32s/epoch - 165ms/step
Epoch 619/1000
2023-09-29 03:27:25.770 
Epoch 619/1000 
	 loss: 27.7149, MinusLogProbMetric: 27.7149, val_loss: 27.9810, val_MinusLogProbMetric: 27.9810

Epoch 619: val_loss did not improve from 27.91411
196/196 - 31s - loss: 27.7149 - MinusLogProbMetric: 27.7149 - val_loss: 27.9810 - val_MinusLogProbMetric: 27.9810 - lr: 2.5000e-04 - 31s/epoch - 156ms/step
Epoch 620/1000
2023-09-29 03:27:59.690 
Epoch 620/1000 
	 loss: 27.6537, MinusLogProbMetric: 27.6537, val_loss: 27.9022, val_MinusLogProbMetric: 27.9022

Epoch 620: val_loss improved from 27.91411 to 27.90221, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_329/weights/best_weights.h5
196/196 - 35s - loss: 27.6537 - MinusLogProbMetric: 27.6537 - val_loss: 27.9022 - val_MinusLogProbMetric: 27.9022 - lr: 1.2500e-04 - 35s/epoch - 177ms/step
Epoch 621/1000
2023-09-29 03:28:32.321 
Epoch 621/1000 
	 loss: 27.6549, MinusLogProbMetric: 27.6549, val_loss: 27.8995, val_MinusLogProbMetric: 27.8995

Epoch 621: val_loss improved from 27.90221 to 27.89948, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_329/weights/best_weights.h5
196/196 - 32s - loss: 27.6549 - MinusLogProbMetric: 27.6549 - val_loss: 27.8995 - val_MinusLogProbMetric: 27.8995 - lr: 1.2500e-04 - 32s/epoch - 165ms/step
Epoch 622/1000
2023-09-29 03:29:05.333 
Epoch 622/1000 
	 loss: 27.6492, MinusLogProbMetric: 27.6492, val_loss: 27.8946, val_MinusLogProbMetric: 27.8946

Epoch 622: val_loss improved from 27.89948 to 27.89461, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_329/weights/best_weights.h5
196/196 - 33s - loss: 27.6492 - MinusLogProbMetric: 27.6492 - val_loss: 27.8946 - val_MinusLogProbMetric: 27.8946 - lr: 1.2500e-04 - 33s/epoch - 169ms/step
Epoch 623/1000
2023-09-29 03:29:36.490 
Epoch 623/1000 
	 loss: 27.6517, MinusLogProbMetric: 27.6517, val_loss: 27.8946, val_MinusLogProbMetric: 27.8946

Epoch 623: val_loss did not improve from 27.89461
196/196 - 30s - loss: 27.6517 - MinusLogProbMetric: 27.6517 - val_loss: 27.8946 - val_MinusLogProbMetric: 27.8946 - lr: 1.2500e-04 - 30s/epoch - 155ms/step
Epoch 624/1000
2023-09-29 03:30:09.738 
Epoch 624/1000 
	 loss: 27.6552, MinusLogProbMetric: 27.6552, val_loss: 27.8943, val_MinusLogProbMetric: 27.8943

Epoch 624: val_loss improved from 27.89461 to 27.89430, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_329/weights/best_weights.h5
196/196 - 34s - loss: 27.6552 - MinusLogProbMetric: 27.6552 - val_loss: 27.8943 - val_MinusLogProbMetric: 27.8943 - lr: 1.2500e-04 - 34s/epoch - 173ms/step
Epoch 625/1000
2023-09-29 03:30:45.658 
Epoch 625/1000 
	 loss: 27.6519, MinusLogProbMetric: 27.6519, val_loss: 27.8918, val_MinusLogProbMetric: 27.8918

Epoch 625: val_loss improved from 27.89430 to 27.89181, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_329/weights/best_weights.h5
196/196 - 36s - loss: 27.6519 - MinusLogProbMetric: 27.6519 - val_loss: 27.8918 - val_MinusLogProbMetric: 27.8918 - lr: 1.2500e-04 - 36s/epoch - 184ms/step
Epoch 626/1000
2023-09-29 03:31:21.708 
Epoch 626/1000 
	 loss: 27.6550, MinusLogProbMetric: 27.6550, val_loss: 27.8924, val_MinusLogProbMetric: 27.8924

Epoch 626: val_loss did not improve from 27.89181
196/196 - 35s - loss: 27.6550 - MinusLogProbMetric: 27.6550 - val_loss: 27.8924 - val_MinusLogProbMetric: 27.8924 - lr: 1.2500e-04 - 35s/epoch - 180ms/step
Epoch 627/1000
2023-09-29 03:31:56.818 
Epoch 627/1000 
	 loss: 27.6560, MinusLogProbMetric: 27.6560, val_loss: 27.9047, val_MinusLogProbMetric: 27.9047

Epoch 627: val_loss did not improve from 27.89181
196/196 - 35s - loss: 27.6560 - MinusLogProbMetric: 27.6560 - val_loss: 27.9047 - val_MinusLogProbMetric: 27.9047 - lr: 1.2500e-04 - 35s/epoch - 179ms/step
Epoch 628/1000
2023-09-29 03:32:31.942 
Epoch 628/1000 
	 loss: 27.6563, MinusLogProbMetric: 27.6563, val_loss: 27.9060, val_MinusLogProbMetric: 27.9060

Epoch 628: val_loss did not improve from 27.89181
196/196 - 35s - loss: 27.6563 - MinusLogProbMetric: 27.6563 - val_loss: 27.9060 - val_MinusLogProbMetric: 27.9060 - lr: 1.2500e-04 - 35s/epoch - 179ms/step
Epoch 629/1000
2023-09-29 03:33:07.078 
Epoch 629/1000 
	 loss: 27.6523, MinusLogProbMetric: 27.6523, val_loss: 27.9175, val_MinusLogProbMetric: 27.9175

Epoch 629: val_loss did not improve from 27.89181
196/196 - 35s - loss: 27.6523 - MinusLogProbMetric: 27.6523 - val_loss: 27.9175 - val_MinusLogProbMetric: 27.9175 - lr: 1.2500e-04 - 35s/epoch - 179ms/step
Epoch 630/1000
2023-09-29 03:33:42.168 
Epoch 630/1000 
	 loss: 27.6592, MinusLogProbMetric: 27.6592, val_loss: 27.9017, val_MinusLogProbMetric: 27.9017

Epoch 630: val_loss did not improve from 27.89181
196/196 - 35s - loss: 27.6592 - MinusLogProbMetric: 27.6592 - val_loss: 27.9017 - val_MinusLogProbMetric: 27.9017 - lr: 1.2500e-04 - 35s/epoch - 179ms/step
Epoch 631/1000
2023-09-29 03:34:17.255 
Epoch 631/1000 
	 loss: 27.6580, MinusLogProbMetric: 27.6580, val_loss: 27.9153, val_MinusLogProbMetric: 27.9153

Epoch 631: val_loss did not improve from 27.89181
196/196 - 35s - loss: 27.6580 - MinusLogProbMetric: 27.6580 - val_loss: 27.9153 - val_MinusLogProbMetric: 27.9153 - lr: 1.2500e-04 - 35s/epoch - 179ms/step
Epoch 632/1000
2023-09-29 03:34:52.542 
Epoch 632/1000 
	 loss: 27.6587, MinusLogProbMetric: 27.6587, val_loss: 27.9170, val_MinusLogProbMetric: 27.9170

Epoch 632: val_loss did not improve from 27.89181
196/196 - 35s - loss: 27.6587 - MinusLogProbMetric: 27.6587 - val_loss: 27.9170 - val_MinusLogProbMetric: 27.9170 - lr: 1.2500e-04 - 35s/epoch - 180ms/step
Epoch 633/1000
2023-09-29 03:35:27.217 
Epoch 633/1000 
	 loss: 27.6542, MinusLogProbMetric: 27.6542, val_loss: 27.8964, val_MinusLogProbMetric: 27.8964

Epoch 633: val_loss did not improve from 27.89181
196/196 - 35s - loss: 27.6542 - MinusLogProbMetric: 27.6542 - val_loss: 27.8964 - val_MinusLogProbMetric: 27.8964 - lr: 1.2500e-04 - 35s/epoch - 177ms/step
Epoch 634/1000
2023-09-29 03:36:01.292 
Epoch 634/1000 
	 loss: 27.6557, MinusLogProbMetric: 27.6557, val_loss: 27.8991, val_MinusLogProbMetric: 27.8991

Epoch 634: val_loss did not improve from 27.89181
196/196 - 34s - loss: 27.6557 - MinusLogProbMetric: 27.6557 - val_loss: 27.8991 - val_MinusLogProbMetric: 27.8991 - lr: 1.2500e-04 - 34s/epoch - 174ms/step
Epoch 635/1000
2023-09-29 03:36:32.700 
Epoch 635/1000 
	 loss: 27.6516, MinusLogProbMetric: 27.6516, val_loss: 27.8991, val_MinusLogProbMetric: 27.8991

Epoch 635: val_loss did not improve from 27.89181
196/196 - 31s - loss: 27.6516 - MinusLogProbMetric: 27.6516 - val_loss: 27.8991 - val_MinusLogProbMetric: 27.8991 - lr: 1.2500e-04 - 31s/epoch - 160ms/step
Epoch 636/1000
2023-09-29 03:37:05.952 
Epoch 636/1000 
	 loss: 27.6595, MinusLogProbMetric: 27.6595, val_loss: 27.9242, val_MinusLogProbMetric: 27.9242

Epoch 636: val_loss did not improve from 27.89181
196/196 - 33s - loss: 27.6595 - MinusLogProbMetric: 27.6595 - val_loss: 27.9242 - val_MinusLogProbMetric: 27.9242 - lr: 1.2500e-04 - 33s/epoch - 170ms/step
Epoch 637/1000
2023-09-29 03:37:39.742 
Epoch 637/1000 
	 loss: 27.6534, MinusLogProbMetric: 27.6534, val_loss: 27.8904, val_MinusLogProbMetric: 27.8904

Epoch 637: val_loss improved from 27.89181 to 27.89037, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_329/weights/best_weights.h5
196/196 - 35s - loss: 27.6534 - MinusLogProbMetric: 27.6534 - val_loss: 27.8904 - val_MinusLogProbMetric: 27.8904 - lr: 1.2500e-04 - 35s/epoch - 177ms/step
Epoch 638/1000
2023-09-29 03:38:16.457 
Epoch 638/1000 
	 loss: 27.6541, MinusLogProbMetric: 27.6541, val_loss: 27.9132, val_MinusLogProbMetric: 27.9132

Epoch 638: val_loss did not improve from 27.89037
196/196 - 36s - loss: 27.6541 - MinusLogProbMetric: 27.6541 - val_loss: 27.9132 - val_MinusLogProbMetric: 27.9132 - lr: 1.2500e-04 - 36s/epoch - 183ms/step
Epoch 639/1000
2023-09-29 03:38:53.031 
Epoch 639/1000 
	 loss: 27.6541, MinusLogProbMetric: 27.6541, val_loss: 27.9017, val_MinusLogProbMetric: 27.9017

Epoch 639: val_loss did not improve from 27.89037
196/196 - 37s - loss: 27.6541 - MinusLogProbMetric: 27.6541 - val_loss: 27.9017 - val_MinusLogProbMetric: 27.9017 - lr: 1.2500e-04 - 37s/epoch - 187ms/step
Epoch 640/1000
2023-09-29 03:39:29.288 
Epoch 640/1000 
	 loss: 27.6509, MinusLogProbMetric: 27.6509, val_loss: 27.8821, val_MinusLogProbMetric: 27.8821

Epoch 640: val_loss improved from 27.89037 to 27.88208, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_329/weights/best_weights.h5
196/196 - 37s - loss: 27.6509 - MinusLogProbMetric: 27.6509 - val_loss: 27.8821 - val_MinusLogProbMetric: 27.8821 - lr: 1.2500e-04 - 37s/epoch - 189ms/step
Epoch 641/1000
2023-09-29 03:40:05.953 
Epoch 641/1000 
	 loss: 27.6505, MinusLogProbMetric: 27.6505, val_loss: 27.8896, val_MinusLogProbMetric: 27.8896

Epoch 641: val_loss did not improve from 27.88208
196/196 - 36s - loss: 27.6505 - MinusLogProbMetric: 27.6505 - val_loss: 27.8896 - val_MinusLogProbMetric: 27.8896 - lr: 1.2500e-04 - 36s/epoch - 183ms/step
Epoch 642/1000
2023-09-29 03:40:42.287 
Epoch 642/1000 
	 loss: 27.6522, MinusLogProbMetric: 27.6522, val_loss: 27.8953, val_MinusLogProbMetric: 27.8953

Epoch 642: val_loss did not improve from 27.88208
196/196 - 36s - loss: 27.6522 - MinusLogProbMetric: 27.6522 - val_loss: 27.8953 - val_MinusLogProbMetric: 27.8953 - lr: 1.2500e-04 - 36s/epoch - 185ms/step
Epoch 643/1000
2023-09-29 03:41:18.286 
Epoch 643/1000 
	 loss: 27.6525, MinusLogProbMetric: 27.6525, val_loss: 27.8815, val_MinusLogProbMetric: 27.8815

Epoch 643: val_loss improved from 27.88208 to 27.88146, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_329/weights/best_weights.h5
196/196 - 37s - loss: 27.6525 - MinusLogProbMetric: 27.6525 - val_loss: 27.8815 - val_MinusLogProbMetric: 27.8815 - lr: 1.2500e-04 - 37s/epoch - 188ms/step
Epoch 644/1000
2023-09-29 03:41:55.359 
Epoch 644/1000 
	 loss: 27.6505, MinusLogProbMetric: 27.6505, val_loss: 27.8888, val_MinusLogProbMetric: 27.8888

Epoch 644: val_loss did not improve from 27.88146
196/196 - 36s - loss: 27.6505 - MinusLogProbMetric: 27.6505 - val_loss: 27.8888 - val_MinusLogProbMetric: 27.8888 - lr: 1.2500e-04 - 36s/epoch - 185ms/step
Epoch 645/1000
2023-09-29 03:42:31.809 
Epoch 645/1000 
	 loss: 27.6504, MinusLogProbMetric: 27.6504, val_loss: 27.8852, val_MinusLogProbMetric: 27.8852

Epoch 645: val_loss did not improve from 27.88146
196/196 - 36s - loss: 27.6504 - MinusLogProbMetric: 27.6504 - val_loss: 27.8852 - val_MinusLogProbMetric: 27.8852 - lr: 1.2500e-04 - 36s/epoch - 186ms/step
Epoch 646/1000
2023-09-29 03:43:08.109 
Epoch 646/1000 
	 loss: 27.6510, MinusLogProbMetric: 27.6510, val_loss: 27.8750, val_MinusLogProbMetric: 27.8750

Epoch 646: val_loss improved from 27.88146 to 27.87498, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_329/weights/best_weights.h5
196/196 - 37s - loss: 27.6510 - MinusLogProbMetric: 27.6510 - val_loss: 27.8750 - val_MinusLogProbMetric: 27.8750 - lr: 1.2500e-04 - 37s/epoch - 189ms/step
Epoch 647/1000
2023-09-29 03:43:44.644 
Epoch 647/1000 
	 loss: 27.6550, MinusLogProbMetric: 27.6550, val_loss: 27.8866, val_MinusLogProbMetric: 27.8866

Epoch 647: val_loss did not improve from 27.87498
196/196 - 36s - loss: 27.6550 - MinusLogProbMetric: 27.6550 - val_loss: 27.8866 - val_MinusLogProbMetric: 27.8866 - lr: 1.2500e-04 - 36s/epoch - 182ms/step
Epoch 648/1000
2023-09-29 03:44:20.644 
Epoch 648/1000 
	 loss: 27.6559, MinusLogProbMetric: 27.6559, val_loss: 27.8960, val_MinusLogProbMetric: 27.8960

Epoch 648: val_loss did not improve from 27.87498
196/196 - 36s - loss: 27.6559 - MinusLogProbMetric: 27.6559 - val_loss: 27.8960 - val_MinusLogProbMetric: 27.8960 - lr: 1.2500e-04 - 36s/epoch - 184ms/step
Epoch 649/1000
2023-09-29 03:44:56.524 
Epoch 649/1000 
	 loss: 27.6464, MinusLogProbMetric: 27.6464, val_loss: 27.9081, val_MinusLogProbMetric: 27.9081

Epoch 649: val_loss did not improve from 27.87498
196/196 - 36s - loss: 27.6464 - MinusLogProbMetric: 27.6464 - val_loss: 27.9081 - val_MinusLogProbMetric: 27.9081 - lr: 1.2500e-04 - 36s/epoch - 183ms/step
Epoch 650/1000
2023-09-29 03:45:32.675 
Epoch 650/1000 
	 loss: 27.6498, MinusLogProbMetric: 27.6498, val_loss: 27.8856, val_MinusLogProbMetric: 27.8856

Epoch 650: val_loss did not improve from 27.87498
196/196 - 36s - loss: 27.6498 - MinusLogProbMetric: 27.6498 - val_loss: 27.8856 - val_MinusLogProbMetric: 27.8856 - lr: 1.2500e-04 - 36s/epoch - 184ms/step
Epoch 651/1000
2023-09-29 03:46:08.766 
Epoch 651/1000 
	 loss: 27.6529, MinusLogProbMetric: 27.6529, val_loss: 27.8752, val_MinusLogProbMetric: 27.8752

Epoch 651: val_loss did not improve from 27.87498
196/196 - 36s - loss: 27.6529 - MinusLogProbMetric: 27.6529 - val_loss: 27.8752 - val_MinusLogProbMetric: 27.8752 - lr: 1.2500e-04 - 36s/epoch - 184ms/step
Epoch 652/1000
2023-09-29 03:46:44.857 
Epoch 652/1000 
	 loss: 27.6503, MinusLogProbMetric: 27.6503, val_loss: 27.8896, val_MinusLogProbMetric: 27.8896

Epoch 652: val_loss did not improve from 27.87498
196/196 - 36s - loss: 27.6503 - MinusLogProbMetric: 27.6503 - val_loss: 27.8896 - val_MinusLogProbMetric: 27.8896 - lr: 1.2500e-04 - 36s/epoch - 184ms/step
Epoch 653/1000
2023-09-29 03:47:20.894 
Epoch 653/1000 
	 loss: 27.6489, MinusLogProbMetric: 27.6489, val_loss: 27.8822, val_MinusLogProbMetric: 27.8822

Epoch 653: val_loss did not improve from 27.87498
196/196 - 36s - loss: 27.6489 - MinusLogProbMetric: 27.6489 - val_loss: 27.8822 - val_MinusLogProbMetric: 27.8822 - lr: 1.2500e-04 - 36s/epoch - 184ms/step
Epoch 654/1000
2023-09-29 03:47:57.217 
Epoch 654/1000 
	 loss: 27.6549, MinusLogProbMetric: 27.6549, val_loss: 27.8827, val_MinusLogProbMetric: 27.8827

Epoch 654: val_loss did not improve from 27.87498
196/196 - 36s - loss: 27.6549 - MinusLogProbMetric: 27.6549 - val_loss: 27.8827 - val_MinusLogProbMetric: 27.8827 - lr: 1.2500e-04 - 36s/epoch - 185ms/step
Epoch 655/1000
2023-09-29 03:48:33.240 
Epoch 655/1000 
	 loss: 27.6525, MinusLogProbMetric: 27.6525, val_loss: 27.8622, val_MinusLogProbMetric: 27.8622

Epoch 655: val_loss improved from 27.87498 to 27.86217, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_329/weights/best_weights.h5
196/196 - 37s - loss: 27.6525 - MinusLogProbMetric: 27.6525 - val_loss: 27.8622 - val_MinusLogProbMetric: 27.8622 - lr: 1.2500e-04 - 37s/epoch - 188ms/step
Epoch 656/1000
2023-09-29 03:49:10.692 
Epoch 656/1000 
	 loss: 27.6508, MinusLogProbMetric: 27.6508, val_loss: 27.8989, val_MinusLogProbMetric: 27.8989

Epoch 656: val_loss did not improve from 27.86217
196/196 - 37s - loss: 27.6508 - MinusLogProbMetric: 27.6508 - val_loss: 27.8989 - val_MinusLogProbMetric: 27.8989 - lr: 1.2500e-04 - 37s/epoch - 187ms/step
Epoch 657/1000
2023-09-29 03:49:46.138 
Epoch 657/1000 
	 loss: 27.6535, MinusLogProbMetric: 27.6535, val_loss: 27.8945, val_MinusLogProbMetric: 27.8945

Epoch 657: val_loss did not improve from 27.86217
196/196 - 35s - loss: 27.6535 - MinusLogProbMetric: 27.6535 - val_loss: 27.8945 - val_MinusLogProbMetric: 27.8945 - lr: 1.2500e-04 - 35s/epoch - 181ms/step
Epoch 658/1000
2023-09-29 03:50:22.509 
Epoch 658/1000 
	 loss: 27.6454, MinusLogProbMetric: 27.6454, val_loss: 27.8878, val_MinusLogProbMetric: 27.8878

Epoch 658: val_loss did not improve from 27.86217
196/196 - 36s - loss: 27.6454 - MinusLogProbMetric: 27.6454 - val_loss: 27.8878 - val_MinusLogProbMetric: 27.8878 - lr: 1.2500e-04 - 36s/epoch - 186ms/step
Epoch 659/1000
2023-09-29 03:50:59.081 
Epoch 659/1000 
	 loss: 27.6522, MinusLogProbMetric: 27.6522, val_loss: 27.8807, val_MinusLogProbMetric: 27.8807

Epoch 659: val_loss did not improve from 27.86217
196/196 - 37s - loss: 27.6522 - MinusLogProbMetric: 27.6522 - val_loss: 27.8807 - val_MinusLogProbMetric: 27.8807 - lr: 1.2500e-04 - 37s/epoch - 187ms/step
Epoch 660/1000
2023-09-29 03:51:35.342 
Epoch 660/1000 
	 loss: 27.6485, MinusLogProbMetric: 27.6485, val_loss: 27.9450, val_MinusLogProbMetric: 27.9450

Epoch 660: val_loss did not improve from 27.86217
196/196 - 36s - loss: 27.6485 - MinusLogProbMetric: 27.6485 - val_loss: 27.9450 - val_MinusLogProbMetric: 27.9450 - lr: 1.2500e-04 - 36s/epoch - 185ms/step
Epoch 661/1000
2023-09-29 03:52:11.698 
Epoch 661/1000 
	 loss: 27.6548, MinusLogProbMetric: 27.6548, val_loss: 27.8864, val_MinusLogProbMetric: 27.8864

Epoch 661: val_loss did not improve from 27.86217
196/196 - 36s - loss: 27.6548 - MinusLogProbMetric: 27.6548 - val_loss: 27.8864 - val_MinusLogProbMetric: 27.8864 - lr: 1.2500e-04 - 36s/epoch - 185ms/step
Epoch 662/1000
2023-09-29 03:52:44.379 
Epoch 662/1000 
	 loss: 27.6491, MinusLogProbMetric: 27.6491, val_loss: 27.8734, val_MinusLogProbMetric: 27.8734

Epoch 662: val_loss did not improve from 27.86217
196/196 - 33s - loss: 27.6491 - MinusLogProbMetric: 27.6491 - val_loss: 27.8734 - val_MinusLogProbMetric: 27.8734 - lr: 1.2500e-04 - 33s/epoch - 167ms/step
Epoch 663/1000
2023-09-29 03:53:19.417 
Epoch 663/1000 
	 loss: 27.6496, MinusLogProbMetric: 27.6496, val_loss: 27.8892, val_MinusLogProbMetric: 27.8892

Epoch 663: val_loss did not improve from 27.86217
196/196 - 35s - loss: 27.6496 - MinusLogProbMetric: 27.6496 - val_loss: 27.8892 - val_MinusLogProbMetric: 27.8892 - lr: 1.2500e-04 - 35s/epoch - 179ms/step
Epoch 664/1000
2023-09-29 03:53:57.377 
Epoch 664/1000 
	 loss: 27.6517, MinusLogProbMetric: 27.6517, val_loss: 27.9145, val_MinusLogProbMetric: 27.9145

Epoch 664: val_loss did not improve from 27.86217
196/196 - 38s - loss: 27.6517 - MinusLogProbMetric: 27.6517 - val_loss: 27.9145 - val_MinusLogProbMetric: 27.9145 - lr: 1.2500e-04 - 38s/epoch - 194ms/step
Epoch 665/1000
2023-09-29 03:54:33.075 
Epoch 665/1000 
	 loss: 27.6516, MinusLogProbMetric: 27.6516, val_loss: 27.9075, val_MinusLogProbMetric: 27.9075

Epoch 665: val_loss did not improve from 27.86217
196/196 - 36s - loss: 27.6516 - MinusLogProbMetric: 27.6516 - val_loss: 27.9075 - val_MinusLogProbMetric: 27.9075 - lr: 1.2500e-04 - 36s/epoch - 182ms/step
Epoch 666/1000
2023-09-29 03:55:06.309 
Epoch 666/1000 
	 loss: 27.6496, MinusLogProbMetric: 27.6496, val_loss: 27.9044, val_MinusLogProbMetric: 27.9044

Epoch 666: val_loss did not improve from 27.86217
196/196 - 33s - loss: 27.6496 - MinusLogProbMetric: 27.6496 - val_loss: 27.9044 - val_MinusLogProbMetric: 27.9044 - lr: 1.2500e-04 - 33s/epoch - 170ms/step
Epoch 667/1000
2023-09-29 03:55:44.007 
Epoch 667/1000 
	 loss: 27.6454, MinusLogProbMetric: 27.6454, val_loss: 27.8952, val_MinusLogProbMetric: 27.8952

Epoch 667: val_loss did not improve from 27.86217
196/196 - 38s - loss: 27.6454 - MinusLogProbMetric: 27.6454 - val_loss: 27.8952 - val_MinusLogProbMetric: 27.8952 - lr: 1.2500e-04 - 38s/epoch - 192ms/step
Epoch 668/1000
2023-09-29 03:56:19.025 
Epoch 668/1000 
	 loss: 27.6466, MinusLogProbMetric: 27.6466, val_loss: 27.8861, val_MinusLogProbMetric: 27.8861

Epoch 668: val_loss did not improve from 27.86217
196/196 - 35s - loss: 27.6466 - MinusLogProbMetric: 27.6466 - val_loss: 27.8861 - val_MinusLogProbMetric: 27.8861 - lr: 1.2500e-04 - 35s/epoch - 179ms/step
Epoch 669/1000
2023-09-29 03:56:56.969 
Epoch 669/1000 
	 loss: 27.6540, MinusLogProbMetric: 27.6540, val_loss: 27.9066, val_MinusLogProbMetric: 27.9066

Epoch 669: val_loss did not improve from 27.86217
196/196 - 38s - loss: 27.6540 - MinusLogProbMetric: 27.6540 - val_loss: 27.9066 - val_MinusLogProbMetric: 27.9066 - lr: 1.2500e-04 - 38s/epoch - 194ms/step
Epoch 670/1000
2023-09-29 03:57:32.754 
Epoch 670/1000 
	 loss: 27.6512, MinusLogProbMetric: 27.6512, val_loss: 27.9036, val_MinusLogProbMetric: 27.9036

Epoch 670: val_loss did not improve from 27.86217
196/196 - 36s - loss: 27.6512 - MinusLogProbMetric: 27.6512 - val_loss: 27.9036 - val_MinusLogProbMetric: 27.9036 - lr: 1.2500e-04 - 36s/epoch - 183ms/step
Epoch 671/1000
2023-09-29 03:58:10.023 
Epoch 671/1000 
	 loss: 27.6486, MinusLogProbMetric: 27.6486, val_loss: 27.8841, val_MinusLogProbMetric: 27.8841

Epoch 671: val_loss did not improve from 27.86217
196/196 - 37s - loss: 27.6486 - MinusLogProbMetric: 27.6486 - val_loss: 27.8841 - val_MinusLogProbMetric: 27.8841 - lr: 1.2500e-04 - 37s/epoch - 190ms/step
Epoch 672/1000
2023-09-29 03:58:47.214 
Epoch 672/1000 
	 loss: 27.6500, MinusLogProbMetric: 27.6500, val_loss: 27.8741, val_MinusLogProbMetric: 27.8741

Epoch 672: val_loss did not improve from 27.86217
196/196 - 37s - loss: 27.6500 - MinusLogProbMetric: 27.6500 - val_loss: 27.8741 - val_MinusLogProbMetric: 27.8741 - lr: 1.2500e-04 - 37s/epoch - 190ms/step
Epoch 673/1000
2023-09-29 03:59:23.414 
Epoch 673/1000 
	 loss: 27.6463, MinusLogProbMetric: 27.6463, val_loss: 27.8866, val_MinusLogProbMetric: 27.8866

Epoch 673: val_loss did not improve from 27.86217
196/196 - 36s - loss: 27.6463 - MinusLogProbMetric: 27.6463 - val_loss: 27.8866 - val_MinusLogProbMetric: 27.8866 - lr: 1.2500e-04 - 36s/epoch - 185ms/step
Epoch 674/1000
2023-09-29 03:59:57.612 
Epoch 674/1000 
	 loss: 27.6465, MinusLogProbMetric: 27.6465, val_loss: 27.8978, val_MinusLogProbMetric: 27.8978

Epoch 674: val_loss did not improve from 27.86217
196/196 - 34s - loss: 27.6465 - MinusLogProbMetric: 27.6465 - val_loss: 27.8978 - val_MinusLogProbMetric: 27.8978 - lr: 1.2500e-04 - 34s/epoch - 174ms/step
Epoch 675/1000
2023-09-29 04:00:35.228 
Epoch 675/1000 
	 loss: 27.6529, MinusLogProbMetric: 27.6529, val_loss: 27.8876, val_MinusLogProbMetric: 27.8876

Epoch 675: val_loss did not improve from 27.86217
196/196 - 38s - loss: 27.6529 - MinusLogProbMetric: 27.6529 - val_loss: 27.8876 - val_MinusLogProbMetric: 27.8876 - lr: 1.2500e-04 - 38s/epoch - 192ms/step
Epoch 676/1000
2023-09-29 04:01:11.876 
Epoch 676/1000 
	 loss: 27.6507, MinusLogProbMetric: 27.6507, val_loss: 27.9008, val_MinusLogProbMetric: 27.9008

Epoch 676: val_loss did not improve from 27.86217
196/196 - 37s - loss: 27.6507 - MinusLogProbMetric: 27.6507 - val_loss: 27.9008 - val_MinusLogProbMetric: 27.9008 - lr: 1.2500e-04 - 37s/epoch - 187ms/step
Epoch 677/1000
2023-09-29 04:01:47.520 
Epoch 677/1000 
	 loss: 27.6471, MinusLogProbMetric: 27.6471, val_loss: 27.9051, val_MinusLogProbMetric: 27.9051

Epoch 677: val_loss did not improve from 27.86217
196/196 - 36s - loss: 27.6471 - MinusLogProbMetric: 27.6471 - val_loss: 27.9051 - val_MinusLogProbMetric: 27.9051 - lr: 1.2500e-04 - 36s/epoch - 182ms/step
Epoch 678/1000
2023-09-29 04:02:25.292 
Epoch 678/1000 
	 loss: 27.6556, MinusLogProbMetric: 27.6556, val_loss: 27.8781, val_MinusLogProbMetric: 27.8781

Epoch 678: val_loss did not improve from 27.86217
196/196 - 38s - loss: 27.6556 - MinusLogProbMetric: 27.6556 - val_loss: 27.8781 - val_MinusLogProbMetric: 27.8781 - lr: 1.2500e-04 - 38s/epoch - 193ms/step
Epoch 679/1000
2023-09-29 04:03:03.017 
Epoch 679/1000 
	 loss: 27.6492, MinusLogProbMetric: 27.6492, val_loss: 27.8924, val_MinusLogProbMetric: 27.8924

Epoch 679: val_loss did not improve from 27.86217
196/196 - 38s - loss: 27.6492 - MinusLogProbMetric: 27.6492 - val_loss: 27.8924 - val_MinusLogProbMetric: 27.8924 - lr: 1.2500e-04 - 38s/epoch - 192ms/step
Epoch 680/1000
2023-09-29 04:03:39.931 
Epoch 680/1000 
	 loss: 27.6507, MinusLogProbMetric: 27.6507, val_loss: 27.8941, val_MinusLogProbMetric: 27.8941

Epoch 680: val_loss did not improve from 27.86217
196/196 - 37s - loss: 27.6507 - MinusLogProbMetric: 27.6507 - val_loss: 27.8941 - val_MinusLogProbMetric: 27.8941 - lr: 1.2500e-04 - 37s/epoch - 188ms/step
Epoch 681/1000
2023-09-29 04:04:17.245 
Epoch 681/1000 
	 loss: 27.6534, MinusLogProbMetric: 27.6534, val_loss: 27.8804, val_MinusLogProbMetric: 27.8804

Epoch 681: val_loss did not improve from 27.86217
196/196 - 37s - loss: 27.6534 - MinusLogProbMetric: 27.6534 - val_loss: 27.8804 - val_MinusLogProbMetric: 27.8804 - lr: 1.2500e-04 - 37s/epoch - 190ms/step
Epoch 682/1000
2023-09-29 04:04:53.608 
Epoch 682/1000 
	 loss: 27.6519, MinusLogProbMetric: 27.6519, val_loss: 27.9259, val_MinusLogProbMetric: 27.9259

Epoch 682: val_loss did not improve from 27.86217
196/196 - 36s - loss: 27.6519 - MinusLogProbMetric: 27.6519 - val_loss: 27.9259 - val_MinusLogProbMetric: 27.9259 - lr: 1.2500e-04 - 36s/epoch - 186ms/step
Epoch 683/1000
2023-09-29 04:05:31.121 
Epoch 683/1000 
	 loss: 27.6476, MinusLogProbMetric: 27.6476, val_loss: 27.8959, val_MinusLogProbMetric: 27.8959

Epoch 683: val_loss did not improve from 27.86217
196/196 - 38s - loss: 27.6476 - MinusLogProbMetric: 27.6476 - val_loss: 27.8959 - val_MinusLogProbMetric: 27.8959 - lr: 1.2500e-04 - 38s/epoch - 191ms/step
Epoch 684/1000
2023-09-29 04:06:08.609 
Epoch 684/1000 
	 loss: 27.6467, MinusLogProbMetric: 27.6467, val_loss: 27.8835, val_MinusLogProbMetric: 27.8835

Epoch 684: val_loss did not improve from 27.86217
196/196 - 37s - loss: 27.6467 - MinusLogProbMetric: 27.6467 - val_loss: 27.8835 - val_MinusLogProbMetric: 27.8835 - lr: 1.2500e-04 - 37s/epoch - 191ms/step
Epoch 685/1000
2023-09-29 04:06:44.979 
Epoch 685/1000 
	 loss: 27.6458, MinusLogProbMetric: 27.6458, val_loss: 27.8804, val_MinusLogProbMetric: 27.8804

Epoch 685: val_loss did not improve from 27.86217
196/196 - 36s - loss: 27.6458 - MinusLogProbMetric: 27.6458 - val_loss: 27.8804 - val_MinusLogProbMetric: 27.8804 - lr: 1.2500e-04 - 36s/epoch - 186ms/step
Epoch 686/1000
2023-09-29 04:07:22.963 
Epoch 686/1000 
	 loss: 27.6545, MinusLogProbMetric: 27.6545, val_loss: 27.8803, val_MinusLogProbMetric: 27.8803

Epoch 686: val_loss did not improve from 27.86217
196/196 - 38s - loss: 27.6545 - MinusLogProbMetric: 27.6545 - val_loss: 27.8803 - val_MinusLogProbMetric: 27.8803 - lr: 1.2500e-04 - 38s/epoch - 194ms/step
Epoch 687/1000
2023-09-29 04:08:00.036 
Epoch 687/1000 
	 loss: 27.6438, MinusLogProbMetric: 27.6438, val_loss: 27.8973, val_MinusLogProbMetric: 27.8973

Epoch 687: val_loss did not improve from 27.86217
196/196 - 37s - loss: 27.6438 - MinusLogProbMetric: 27.6438 - val_loss: 27.8973 - val_MinusLogProbMetric: 27.8973 - lr: 1.2500e-04 - 37s/epoch - 189ms/step
Epoch 688/1000
2023-09-29 04:08:36.227 
Epoch 688/1000 
	 loss: 27.6491, MinusLogProbMetric: 27.6491, val_loss: 27.8869, val_MinusLogProbMetric: 27.8869

Epoch 688: val_loss did not improve from 27.86217
196/196 - 36s - loss: 27.6491 - MinusLogProbMetric: 27.6491 - val_loss: 27.8869 - val_MinusLogProbMetric: 27.8869 - lr: 1.2500e-04 - 36s/epoch - 185ms/step
Epoch 689/1000
2023-09-29 04:09:13.180 
Epoch 689/1000 
	 loss: 27.6459, MinusLogProbMetric: 27.6459, val_loss: 27.8973, val_MinusLogProbMetric: 27.8973

Epoch 689: val_loss did not improve from 27.86217
196/196 - 37s - loss: 27.6459 - MinusLogProbMetric: 27.6459 - val_loss: 27.8973 - val_MinusLogProbMetric: 27.8973 - lr: 1.2500e-04 - 37s/epoch - 189ms/step
Epoch 690/1000
2023-09-29 04:09:46.817 
Epoch 690/1000 
	 loss: 27.6471, MinusLogProbMetric: 27.6471, val_loss: 27.8752, val_MinusLogProbMetric: 27.8752

Epoch 690: val_loss did not improve from 27.86217
196/196 - 34s - loss: 27.6471 - MinusLogProbMetric: 27.6471 - val_loss: 27.8752 - val_MinusLogProbMetric: 27.8752 - lr: 1.2500e-04 - 34s/epoch - 172ms/step
Epoch 691/1000
2023-09-29 04:10:24.278 
Epoch 691/1000 
	 loss: 27.6483, MinusLogProbMetric: 27.6483, val_loss: 27.8931, val_MinusLogProbMetric: 27.8931

Epoch 691: val_loss did not improve from 27.86217
196/196 - 37s - loss: 27.6483 - MinusLogProbMetric: 27.6483 - val_loss: 27.8931 - val_MinusLogProbMetric: 27.8931 - lr: 1.2500e-04 - 37s/epoch - 191ms/step
Epoch 692/1000
2023-09-29 04:11:01.194 
Epoch 692/1000 
	 loss: 27.6506, MinusLogProbMetric: 27.6506, val_loss: 27.8950, val_MinusLogProbMetric: 27.8950

Epoch 692: val_loss did not improve from 27.86217
196/196 - 37s - loss: 27.6506 - MinusLogProbMetric: 27.6506 - val_loss: 27.8950 - val_MinusLogProbMetric: 27.8950 - lr: 1.2500e-04 - 37s/epoch - 188ms/step
Epoch 693/1000
2023-09-29 04:11:36.494 
Epoch 693/1000 
	 loss: 27.6497, MinusLogProbMetric: 27.6497, val_loss: 27.8846, val_MinusLogProbMetric: 27.8846

Epoch 693: val_loss did not improve from 27.86217
196/196 - 35s - loss: 27.6497 - MinusLogProbMetric: 27.6497 - val_loss: 27.8846 - val_MinusLogProbMetric: 27.8846 - lr: 1.2500e-04 - 35s/epoch - 180ms/step
Epoch 694/1000
2023-09-29 04:12:12.080 
Epoch 694/1000 
	 loss: 27.6478, MinusLogProbMetric: 27.6478, val_loss: 27.8890, val_MinusLogProbMetric: 27.8890

Epoch 694: val_loss did not improve from 27.86217
196/196 - 36s - loss: 27.6478 - MinusLogProbMetric: 27.6478 - val_loss: 27.8890 - val_MinusLogProbMetric: 27.8890 - lr: 1.2500e-04 - 36s/epoch - 182ms/step
Epoch 695/1000
2023-09-29 04:12:47.531 
Epoch 695/1000 
	 loss: 27.6509, MinusLogProbMetric: 27.6509, val_loss: 27.8798, val_MinusLogProbMetric: 27.8798

Epoch 695: val_loss did not improve from 27.86217
196/196 - 35s - loss: 27.6509 - MinusLogProbMetric: 27.6509 - val_loss: 27.8798 - val_MinusLogProbMetric: 27.8798 - lr: 1.2500e-04 - 35s/epoch - 181ms/step
Epoch 696/1000
2023-09-29 04:13:23.498 
Epoch 696/1000 
	 loss: 27.6518, MinusLogProbMetric: 27.6518, val_loss: 27.8858, val_MinusLogProbMetric: 27.8858

Epoch 696: val_loss did not improve from 27.86217
196/196 - 36s - loss: 27.6518 - MinusLogProbMetric: 27.6518 - val_loss: 27.8858 - val_MinusLogProbMetric: 27.8858 - lr: 1.2500e-04 - 36s/epoch - 184ms/step
Epoch 697/1000
2023-09-29 04:13:58.469 
Epoch 697/1000 
	 loss: 27.6529, MinusLogProbMetric: 27.6529, val_loss: 27.9231, val_MinusLogProbMetric: 27.9231

Epoch 697: val_loss did not improve from 27.86217
196/196 - 35s - loss: 27.6529 - MinusLogProbMetric: 27.6529 - val_loss: 27.9231 - val_MinusLogProbMetric: 27.9231 - lr: 1.2500e-04 - 35s/epoch - 178ms/step
Epoch 698/1000
2023-09-29 04:14:36.137 
Epoch 698/1000 
	 loss: 27.6489, MinusLogProbMetric: 27.6489, val_loss: 27.8928, val_MinusLogProbMetric: 27.8928

Epoch 698: val_loss did not improve from 27.86217
196/196 - 38s - loss: 27.6489 - MinusLogProbMetric: 27.6489 - val_loss: 27.8928 - val_MinusLogProbMetric: 27.8928 - lr: 1.2500e-04 - 38s/epoch - 192ms/step
Epoch 699/1000
2023-09-29 04:15:10.592 
Epoch 699/1000 
	 loss: 27.6434, MinusLogProbMetric: 27.6434, val_loss: 27.8886, val_MinusLogProbMetric: 27.8886

Epoch 699: val_loss did not improve from 27.86217
196/196 - 34s - loss: 27.6434 - MinusLogProbMetric: 27.6434 - val_loss: 27.8886 - val_MinusLogProbMetric: 27.8886 - lr: 1.2500e-04 - 34s/epoch - 176ms/step
Epoch 700/1000
2023-09-29 04:15:47.520 
Epoch 700/1000 
	 loss: 27.6446, MinusLogProbMetric: 27.6446, val_loss: 27.8781, val_MinusLogProbMetric: 27.8781

Epoch 700: val_loss did not improve from 27.86217
196/196 - 37s - loss: 27.6446 - MinusLogProbMetric: 27.6446 - val_loss: 27.8781 - val_MinusLogProbMetric: 27.8781 - lr: 1.2500e-04 - 37s/epoch - 188ms/step
Epoch 701/1000
2023-09-29 04:16:22.147 
Epoch 701/1000 
	 loss: 27.6460, MinusLogProbMetric: 27.6460, val_loss: 27.9100, val_MinusLogProbMetric: 27.9100

Epoch 701: val_loss did not improve from 27.86217
196/196 - 35s - loss: 27.6460 - MinusLogProbMetric: 27.6460 - val_loss: 27.9100 - val_MinusLogProbMetric: 27.9100 - lr: 1.2500e-04 - 35s/epoch - 177ms/step
Epoch 702/1000
2023-09-29 04:16:59.066 
Epoch 702/1000 
	 loss: 27.6442, MinusLogProbMetric: 27.6442, val_loss: 27.8865, val_MinusLogProbMetric: 27.8865

Epoch 702: val_loss did not improve from 27.86217
196/196 - 37s - loss: 27.6442 - MinusLogProbMetric: 27.6442 - val_loss: 27.8865 - val_MinusLogProbMetric: 27.8865 - lr: 1.2500e-04 - 37s/epoch - 188ms/step
Epoch 703/1000
2023-09-29 04:17:35.803 
Epoch 703/1000 
	 loss: 27.6469, MinusLogProbMetric: 27.6469, val_loss: 27.8827, val_MinusLogProbMetric: 27.8827

Epoch 703: val_loss did not improve from 27.86217
196/196 - 37s - loss: 27.6469 - MinusLogProbMetric: 27.6469 - val_loss: 27.8827 - val_MinusLogProbMetric: 27.8827 - lr: 1.2500e-04 - 37s/epoch - 187ms/step
Epoch 704/1000
2023-09-29 04:18:11.298 
Epoch 704/1000 
	 loss: 27.6413, MinusLogProbMetric: 27.6413, val_loss: 27.8933, val_MinusLogProbMetric: 27.8933

Epoch 704: val_loss did not improve from 27.86217
196/196 - 35s - loss: 27.6413 - MinusLogProbMetric: 27.6413 - val_loss: 27.8933 - val_MinusLogProbMetric: 27.8933 - lr: 1.2500e-04 - 35s/epoch - 181ms/step
Epoch 705/1000
2023-09-29 04:18:48.310 
Epoch 705/1000 
	 loss: 27.6458, MinusLogProbMetric: 27.6458, val_loss: 27.8695, val_MinusLogProbMetric: 27.8695

Epoch 705: val_loss did not improve from 27.86217
196/196 - 37s - loss: 27.6458 - MinusLogProbMetric: 27.6458 - val_loss: 27.8695 - val_MinusLogProbMetric: 27.8695 - lr: 1.2500e-04 - 37s/epoch - 189ms/step
Epoch 706/1000
2023-09-29 04:19:25.058 
Epoch 706/1000 
	 loss: 27.6193, MinusLogProbMetric: 27.6193, val_loss: 27.8613, val_MinusLogProbMetric: 27.8613

Epoch 706: val_loss improved from 27.86217 to 27.86128, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_329/weights/best_weights.h5
196/196 - 38s - loss: 27.6193 - MinusLogProbMetric: 27.6193 - val_loss: 27.8613 - val_MinusLogProbMetric: 27.8613 - lr: 6.2500e-05 - 38s/epoch - 191ms/step
Epoch 707/1000
2023-09-29 04:20:00.517 
Epoch 707/1000 
	 loss: 27.6143, MinusLogProbMetric: 27.6143, val_loss: 27.8554, val_MinusLogProbMetric: 27.8554

Epoch 707: val_loss improved from 27.86128 to 27.85541, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_329/weights/best_weights.h5
196/196 - 35s - loss: 27.6143 - MinusLogProbMetric: 27.6143 - val_loss: 27.8554 - val_MinusLogProbMetric: 27.8554 - lr: 6.2500e-05 - 35s/epoch - 181ms/step
Epoch 708/1000
2023-09-29 04:20:39.024 
Epoch 708/1000 
	 loss: 27.6142, MinusLogProbMetric: 27.6142, val_loss: 27.8679, val_MinusLogProbMetric: 27.8679

Epoch 708: val_loss did not improve from 27.85541
196/196 - 38s - loss: 27.6142 - MinusLogProbMetric: 27.6142 - val_loss: 27.8679 - val_MinusLogProbMetric: 27.8679 - lr: 6.2500e-05 - 38s/epoch - 193ms/step
Epoch 709/1000
2023-09-29 04:21:15.449 
Epoch 709/1000 
	 loss: 27.6172, MinusLogProbMetric: 27.6172, val_loss: 27.8599, val_MinusLogProbMetric: 27.8599

Epoch 709: val_loss did not improve from 27.85541
196/196 - 36s - loss: 27.6172 - MinusLogProbMetric: 27.6172 - val_loss: 27.8599 - val_MinusLogProbMetric: 27.8599 - lr: 6.2500e-05 - 36s/epoch - 186ms/step
Epoch 710/1000
2023-09-29 04:21:51.982 
Epoch 710/1000 
	 loss: 27.6174, MinusLogProbMetric: 27.6174, val_loss: 27.8648, val_MinusLogProbMetric: 27.8648

Epoch 710: val_loss did not improve from 27.85541
196/196 - 37s - loss: 27.6174 - MinusLogProbMetric: 27.6174 - val_loss: 27.8648 - val_MinusLogProbMetric: 27.8648 - lr: 6.2500e-05 - 37s/epoch - 186ms/step
Epoch 711/1000
2023-09-29 04:22:29.891 
Epoch 711/1000 
	 loss: 27.6182, MinusLogProbMetric: 27.6182, val_loss: 27.8754, val_MinusLogProbMetric: 27.8754

Epoch 711: val_loss did not improve from 27.85541
196/196 - 38s - loss: 27.6182 - MinusLogProbMetric: 27.6182 - val_loss: 27.8754 - val_MinusLogProbMetric: 27.8754 - lr: 6.2500e-05 - 38s/epoch - 193ms/step
Epoch 712/1000
2023-09-29 04:23:06.319 
Epoch 712/1000 
	 loss: 27.6167, MinusLogProbMetric: 27.6167, val_loss: 27.8661, val_MinusLogProbMetric: 27.8661

Epoch 712: val_loss did not improve from 27.85541
196/196 - 36s - loss: 27.6167 - MinusLogProbMetric: 27.6167 - val_loss: 27.8661 - val_MinusLogProbMetric: 27.8661 - lr: 6.2500e-05 - 36s/epoch - 186ms/step
Epoch 713/1000
2023-09-29 04:23:43.370 
Epoch 713/1000 
	 loss: 27.6150, MinusLogProbMetric: 27.6150, val_loss: 27.8772, val_MinusLogProbMetric: 27.8772

Epoch 713: val_loss did not improve from 27.85541
196/196 - 37s - loss: 27.6150 - MinusLogProbMetric: 27.6150 - val_loss: 27.8772 - val_MinusLogProbMetric: 27.8772 - lr: 6.2500e-05 - 37s/epoch - 189ms/step
Epoch 714/1000
2023-09-29 04:24:20.561 
Epoch 714/1000 
	 loss: 27.6174, MinusLogProbMetric: 27.6174, val_loss: 27.8596, val_MinusLogProbMetric: 27.8596

Epoch 714: val_loss did not improve from 27.85541
196/196 - 37s - loss: 27.6174 - MinusLogProbMetric: 27.6174 - val_loss: 27.8596 - val_MinusLogProbMetric: 27.8596 - lr: 6.2500e-05 - 37s/epoch - 190ms/step
Epoch 715/1000
2023-09-29 04:24:55.258 
Epoch 715/1000 
	 loss: 27.6145, MinusLogProbMetric: 27.6145, val_loss: 27.8591, val_MinusLogProbMetric: 27.8591

Epoch 715: val_loss did not improve from 27.85541
196/196 - 35s - loss: 27.6145 - MinusLogProbMetric: 27.6145 - val_loss: 27.8591 - val_MinusLogProbMetric: 27.8591 - lr: 6.2500e-05 - 35s/epoch - 177ms/step
Epoch 716/1000
2023-09-29 04:25:30.359 
Epoch 716/1000 
	 loss: 27.6174, MinusLogProbMetric: 27.6174, val_loss: 27.8539, val_MinusLogProbMetric: 27.8539

Epoch 716: val_loss improved from 27.85541 to 27.85385, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_329/weights/best_weights.h5
196/196 - 36s - loss: 27.6174 - MinusLogProbMetric: 27.6174 - val_loss: 27.8539 - val_MinusLogProbMetric: 27.8539 - lr: 6.2500e-05 - 36s/epoch - 183ms/step
Epoch 717/1000
2023-09-29 04:26:07.330 
Epoch 717/1000 
	 loss: 27.6149, MinusLogProbMetric: 27.6149, val_loss: 27.8565, val_MinusLogProbMetric: 27.8565

Epoch 717: val_loss did not improve from 27.85385
196/196 - 36s - loss: 27.6149 - MinusLogProbMetric: 27.6149 - val_loss: 27.8565 - val_MinusLogProbMetric: 27.8565 - lr: 6.2500e-05 - 36s/epoch - 185ms/step
Epoch 718/1000
2023-09-29 04:26:44.089 
Epoch 718/1000 
	 loss: 27.6151, MinusLogProbMetric: 27.6151, val_loss: 27.8966, val_MinusLogProbMetric: 27.8966

Epoch 718: val_loss did not improve from 27.85385
196/196 - 37s - loss: 27.6151 - MinusLogProbMetric: 27.6151 - val_loss: 27.8966 - val_MinusLogProbMetric: 27.8966 - lr: 6.2500e-05 - 37s/epoch - 188ms/step
Epoch 719/1000
2023-09-29 04:27:21.269 
Epoch 719/1000 
	 loss: 27.6207, MinusLogProbMetric: 27.6207, val_loss: 27.8625, val_MinusLogProbMetric: 27.8625

Epoch 719: val_loss did not improve from 27.85385
196/196 - 37s - loss: 27.6207 - MinusLogProbMetric: 27.6207 - val_loss: 27.8625 - val_MinusLogProbMetric: 27.8625 - lr: 6.2500e-05 - 37s/epoch - 190ms/step
Epoch 720/1000
2023-09-29 04:27:58.557 
Epoch 720/1000 
	 loss: 27.6134, MinusLogProbMetric: 27.6134, val_loss: 27.8727, val_MinusLogProbMetric: 27.8727

Epoch 720: val_loss did not improve from 27.85385
196/196 - 37s - loss: 27.6134 - MinusLogProbMetric: 27.6134 - val_loss: 27.8727 - val_MinusLogProbMetric: 27.8727 - lr: 6.2500e-05 - 37s/epoch - 190ms/step
Epoch 721/1000
2023-09-29 04:28:32.646 
Epoch 721/1000 
	 loss: 27.6166, MinusLogProbMetric: 27.6166, val_loss: 27.8588, val_MinusLogProbMetric: 27.8588

Epoch 721: val_loss did not improve from 27.85385
196/196 - 34s - loss: 27.6166 - MinusLogProbMetric: 27.6166 - val_loss: 27.8588 - val_MinusLogProbMetric: 27.8588 - lr: 6.2500e-05 - 34s/epoch - 174ms/step
Epoch 722/1000
2023-09-29 04:29:08.527 
Epoch 722/1000 
	 loss: 27.6143, MinusLogProbMetric: 27.6143, val_loss: 27.8632, val_MinusLogProbMetric: 27.8632

Epoch 722: val_loss did not improve from 27.85385
196/196 - 36s - loss: 27.6143 - MinusLogProbMetric: 27.6143 - val_loss: 27.8632 - val_MinusLogProbMetric: 27.8632 - lr: 6.2500e-05 - 36s/epoch - 183ms/step
Epoch 723/1000
2023-09-29 04:29:42.807 
Epoch 723/1000 
	 loss: 27.6131, MinusLogProbMetric: 27.6131, val_loss: 27.8598, val_MinusLogProbMetric: 27.8598

Epoch 723: val_loss did not improve from 27.85385
196/196 - 34s - loss: 27.6131 - MinusLogProbMetric: 27.6131 - val_loss: 27.8598 - val_MinusLogProbMetric: 27.8598 - lr: 6.2500e-05 - 34s/epoch - 175ms/step
Epoch 724/1000
2023-09-29 04:30:15.055 
Epoch 724/1000 
	 loss: 27.6140, MinusLogProbMetric: 27.6140, val_loss: 27.8828, val_MinusLogProbMetric: 27.8828

Epoch 724: val_loss did not improve from 27.85385
196/196 - 32s - loss: 27.6140 - MinusLogProbMetric: 27.6140 - val_loss: 27.8828 - val_MinusLogProbMetric: 27.8828 - lr: 6.2500e-05 - 32s/epoch - 165ms/step
Epoch 725/1000
2023-09-29 04:30:45.655 
Epoch 725/1000 
	 loss: 27.6152, MinusLogProbMetric: 27.6152, val_loss: 27.8553, val_MinusLogProbMetric: 27.8553

Epoch 725: val_loss did not improve from 27.85385
196/196 - 31s - loss: 27.6152 - MinusLogProbMetric: 27.6152 - val_loss: 27.8553 - val_MinusLogProbMetric: 27.8553 - lr: 6.2500e-05 - 31s/epoch - 156ms/step
Epoch 726/1000
2023-09-29 04:31:19.002 
Epoch 726/1000 
	 loss: 27.6137, MinusLogProbMetric: 27.6137, val_loss: 27.8640, val_MinusLogProbMetric: 27.8640

Epoch 726: val_loss did not improve from 27.85385
196/196 - 33s - loss: 27.6137 - MinusLogProbMetric: 27.6137 - val_loss: 27.8640 - val_MinusLogProbMetric: 27.8640 - lr: 6.2500e-05 - 33s/epoch - 170ms/step
Epoch 727/1000
2023-09-29 04:31:51.913 
Epoch 727/1000 
	 loss: 27.6139, MinusLogProbMetric: 27.6139, val_loss: 27.8591, val_MinusLogProbMetric: 27.8591

Epoch 727: val_loss did not improve from 27.85385
196/196 - 33s - loss: 27.6139 - MinusLogProbMetric: 27.6139 - val_loss: 27.8591 - val_MinusLogProbMetric: 27.8591 - lr: 6.2500e-05 - 33s/epoch - 168ms/step
Epoch 728/1000
2023-09-29 04:32:26.820 
Epoch 728/1000 
	 loss: 27.6141, MinusLogProbMetric: 27.6141, val_loss: 27.8595, val_MinusLogProbMetric: 27.8595

Epoch 728: val_loss did not improve from 27.85385
196/196 - 35s - loss: 27.6141 - MinusLogProbMetric: 27.6141 - val_loss: 27.8595 - val_MinusLogProbMetric: 27.8595 - lr: 6.2500e-05 - 35s/epoch - 178ms/step
Epoch 729/1000
2023-09-29 04:32:57.944 
Epoch 729/1000 
	 loss: 27.6182, MinusLogProbMetric: 27.6182, val_loss: 27.8706, val_MinusLogProbMetric: 27.8706

Epoch 729: val_loss did not improve from 27.85385
196/196 - 31s - loss: 27.6182 - MinusLogProbMetric: 27.6182 - val_loss: 27.8706 - val_MinusLogProbMetric: 27.8706 - lr: 6.2500e-05 - 31s/epoch - 159ms/step
Epoch 730/1000
2023-09-29 04:33:30.483 
Epoch 730/1000 
	 loss: 27.6140, MinusLogProbMetric: 27.6140, val_loss: 27.8715, val_MinusLogProbMetric: 27.8715

Epoch 730: val_loss did not improve from 27.85385
196/196 - 33s - loss: 27.6140 - MinusLogProbMetric: 27.6140 - val_loss: 27.8715 - val_MinusLogProbMetric: 27.8715 - lr: 6.2500e-05 - 33s/epoch - 166ms/step
Epoch 731/1000
2023-09-29 04:34:02.761 
Epoch 731/1000 
	 loss: 27.6125, MinusLogProbMetric: 27.6125, val_loss: 27.8656, val_MinusLogProbMetric: 27.8656

Epoch 731: val_loss did not improve from 27.85385
196/196 - 32s - loss: 27.6125 - MinusLogProbMetric: 27.6125 - val_loss: 27.8656 - val_MinusLogProbMetric: 27.8656 - lr: 6.2500e-05 - 32s/epoch - 165ms/step
Epoch 732/1000
2023-09-29 04:34:35.145 
Epoch 732/1000 
	 loss: 27.6163, MinusLogProbMetric: 27.6163, val_loss: 27.8663, val_MinusLogProbMetric: 27.8663

Epoch 732: val_loss did not improve from 27.85385
196/196 - 32s - loss: 27.6163 - MinusLogProbMetric: 27.6163 - val_loss: 27.8663 - val_MinusLogProbMetric: 27.8663 - lr: 6.2500e-05 - 32s/epoch - 165ms/step
Epoch 733/1000
2023-09-29 04:35:05.682 
Epoch 733/1000 
	 loss: 27.6134, MinusLogProbMetric: 27.6134, val_loss: 27.8638, val_MinusLogProbMetric: 27.8638

Epoch 733: val_loss did not improve from 27.85385
196/196 - 31s - loss: 27.6134 - MinusLogProbMetric: 27.6134 - val_loss: 27.8638 - val_MinusLogProbMetric: 27.8638 - lr: 6.2500e-05 - 31s/epoch - 156ms/step
Epoch 734/1000
2023-09-29 04:35:37.233 
Epoch 734/1000 
	 loss: 27.6147, MinusLogProbMetric: 27.6147, val_loss: 27.8594, val_MinusLogProbMetric: 27.8594

Epoch 734: val_loss did not improve from 27.85385
196/196 - 32s - loss: 27.6147 - MinusLogProbMetric: 27.6147 - val_loss: 27.8594 - val_MinusLogProbMetric: 27.8594 - lr: 6.2500e-05 - 32s/epoch - 161ms/step
Epoch 735/1000
2023-09-29 04:36:10.375 
Epoch 735/1000 
	 loss: 27.6143, MinusLogProbMetric: 27.6143, val_loss: 27.8627, val_MinusLogProbMetric: 27.8627

Epoch 735: val_loss did not improve from 27.85385
196/196 - 33s - loss: 27.6143 - MinusLogProbMetric: 27.6143 - val_loss: 27.8627 - val_MinusLogProbMetric: 27.8627 - lr: 6.2500e-05 - 33s/epoch - 169ms/step
Epoch 736/1000
2023-09-29 04:36:40.884 
Epoch 736/1000 
	 loss: 27.6157, MinusLogProbMetric: 27.6157, val_loss: 27.8782, val_MinusLogProbMetric: 27.8782

Epoch 736: val_loss did not improve from 27.85385
196/196 - 31s - loss: 27.6157 - MinusLogProbMetric: 27.6157 - val_loss: 27.8782 - val_MinusLogProbMetric: 27.8782 - lr: 6.2500e-05 - 31s/epoch - 156ms/step
Epoch 737/1000
2023-09-29 04:37:15.968 
Epoch 737/1000 
	 loss: 27.6123, MinusLogProbMetric: 27.6123, val_loss: 27.8575, val_MinusLogProbMetric: 27.8575

Epoch 737: val_loss did not improve from 27.85385
196/196 - 35s - loss: 27.6123 - MinusLogProbMetric: 27.6123 - val_loss: 27.8575 - val_MinusLogProbMetric: 27.8575 - lr: 6.2500e-05 - 35s/epoch - 179ms/step
Epoch 738/1000
2023-09-29 04:37:50.330 
Epoch 738/1000 
	 loss: 27.6132, MinusLogProbMetric: 27.6132, val_loss: 27.8624, val_MinusLogProbMetric: 27.8624

Epoch 738: val_loss did not improve from 27.85385
196/196 - 34s - loss: 27.6132 - MinusLogProbMetric: 27.6132 - val_loss: 27.8624 - val_MinusLogProbMetric: 27.8624 - lr: 6.2500e-05 - 34s/epoch - 175ms/step
Epoch 739/1000
2023-09-29 04:38:23.860 
Epoch 739/1000 
	 loss: 27.6141, MinusLogProbMetric: 27.6141, val_loss: 27.8720, val_MinusLogProbMetric: 27.8720

Epoch 739: val_loss did not improve from 27.85385
196/196 - 34s - loss: 27.6141 - MinusLogProbMetric: 27.6141 - val_loss: 27.8720 - val_MinusLogProbMetric: 27.8720 - lr: 6.2500e-05 - 34s/epoch - 171ms/step
Epoch 740/1000
2023-09-29 04:38:57.633 
Epoch 740/1000 
	 loss: 27.6139, MinusLogProbMetric: 27.6139, val_loss: 27.8728, val_MinusLogProbMetric: 27.8728

Epoch 740: val_loss did not improve from 27.85385
196/196 - 34s - loss: 27.6139 - MinusLogProbMetric: 27.6139 - val_loss: 27.8728 - val_MinusLogProbMetric: 27.8728 - lr: 6.2500e-05 - 34s/epoch - 172ms/step
Epoch 741/1000
2023-09-29 04:39:31.086 
Epoch 741/1000 
	 loss: 27.6146, MinusLogProbMetric: 27.6146, val_loss: 27.8626, val_MinusLogProbMetric: 27.8626

Epoch 741: val_loss did not improve from 27.85385
196/196 - 33s - loss: 27.6146 - MinusLogProbMetric: 27.6146 - val_loss: 27.8626 - val_MinusLogProbMetric: 27.8626 - lr: 6.2500e-05 - 33s/epoch - 171ms/step
Epoch 742/1000
2023-09-29 04:40:05.628 
Epoch 742/1000 
	 loss: 27.6152, MinusLogProbMetric: 27.6152, val_loss: 27.8576, val_MinusLogProbMetric: 27.8576

Epoch 742: val_loss did not improve from 27.85385
196/196 - 35s - loss: 27.6152 - MinusLogProbMetric: 27.6152 - val_loss: 27.8576 - val_MinusLogProbMetric: 27.8576 - lr: 6.2500e-05 - 35s/epoch - 176ms/step
Epoch 743/1000
2023-09-29 04:40:36.008 
Epoch 743/1000 
	 loss: 27.6164, MinusLogProbMetric: 27.6164, val_loss: 27.8697, val_MinusLogProbMetric: 27.8697

Epoch 743: val_loss did not improve from 27.85385
196/196 - 30s - loss: 27.6164 - MinusLogProbMetric: 27.6164 - val_loss: 27.8697 - val_MinusLogProbMetric: 27.8697 - lr: 6.2500e-05 - 30s/epoch - 155ms/step
Epoch 744/1000
2023-09-29 04:41:08.698 
Epoch 744/1000 
	 loss: 27.6133, MinusLogProbMetric: 27.6133, val_loss: 27.8640, val_MinusLogProbMetric: 27.8640

Epoch 744: val_loss did not improve from 27.85385
196/196 - 33s - loss: 27.6133 - MinusLogProbMetric: 27.6133 - val_loss: 27.8640 - val_MinusLogProbMetric: 27.8640 - lr: 6.2500e-05 - 33s/epoch - 167ms/step
Epoch 745/1000
2023-09-29 04:41:42.795 
Epoch 745/1000 
	 loss: 27.6143, MinusLogProbMetric: 27.6143, val_loss: 27.8716, val_MinusLogProbMetric: 27.8716

Epoch 745: val_loss did not improve from 27.85385
196/196 - 34s - loss: 27.6143 - MinusLogProbMetric: 27.6143 - val_loss: 27.8716 - val_MinusLogProbMetric: 27.8716 - lr: 6.2500e-05 - 34s/epoch - 174ms/step
Epoch 746/1000
2023-09-29 04:42:16.874 
Epoch 746/1000 
	 loss: 27.6156, MinusLogProbMetric: 27.6156, val_loss: 27.8699, val_MinusLogProbMetric: 27.8699

Epoch 746: val_loss did not improve from 27.85385
196/196 - 34s - loss: 27.6156 - MinusLogProbMetric: 27.6156 - val_loss: 27.8699 - val_MinusLogProbMetric: 27.8699 - lr: 6.2500e-05 - 34s/epoch - 174ms/step
Epoch 747/1000
2023-09-29 04:42:47.951 
Epoch 747/1000 
	 loss: 27.6147, MinusLogProbMetric: 27.6147, val_loss: 27.8846, val_MinusLogProbMetric: 27.8846

Epoch 747: val_loss did not improve from 27.85385
196/196 - 31s - loss: 27.6147 - MinusLogProbMetric: 27.6147 - val_loss: 27.8846 - val_MinusLogProbMetric: 27.8846 - lr: 6.2500e-05 - 31s/epoch - 159ms/step
Epoch 748/1000
2023-09-29 04:43:20.854 
Epoch 748/1000 
	 loss: 27.6146, MinusLogProbMetric: 27.6146, val_loss: 27.8536, val_MinusLogProbMetric: 27.8536

Epoch 748: val_loss improved from 27.85385 to 27.85361, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_329/weights/best_weights.h5
196/196 - 33s - loss: 27.6146 - MinusLogProbMetric: 27.6146 - val_loss: 27.8536 - val_MinusLogProbMetric: 27.8536 - lr: 6.2500e-05 - 33s/epoch - 170ms/step
Epoch 749/1000
2023-09-29 04:43:54.006 
Epoch 749/1000 
	 loss: 27.6120, MinusLogProbMetric: 27.6120, val_loss: 27.8502, val_MinusLogProbMetric: 27.8502

Epoch 749: val_loss improved from 27.85361 to 27.85021, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_329/weights/best_weights.h5
196/196 - 33s - loss: 27.6120 - MinusLogProbMetric: 27.6120 - val_loss: 27.8502 - val_MinusLogProbMetric: 27.8502 - lr: 6.2500e-05 - 33s/epoch - 170ms/step
Epoch 750/1000
2023-09-29 04:44:23.823 
Epoch 750/1000 
	 loss: 27.6111, MinusLogProbMetric: 27.6111, val_loss: 27.8819, val_MinusLogProbMetric: 27.8819

Epoch 750: val_loss did not improve from 27.85021
196/196 - 29s - loss: 27.6111 - MinusLogProbMetric: 27.6111 - val_loss: 27.8819 - val_MinusLogProbMetric: 27.8819 - lr: 6.2500e-05 - 29s/epoch - 149ms/step
Epoch 751/1000
2023-09-29 04:44:55.525 
Epoch 751/1000 
	 loss: 27.6144, MinusLogProbMetric: 27.6144, val_loss: 27.8742, val_MinusLogProbMetric: 27.8742

Epoch 751: val_loss did not improve from 27.85021
196/196 - 32s - loss: 27.6144 - MinusLogProbMetric: 27.6144 - val_loss: 27.8742 - val_MinusLogProbMetric: 27.8742 - lr: 6.2500e-05 - 32s/epoch - 162ms/step
Epoch 752/1000
2023-09-29 04:45:27.028 
Epoch 752/1000 
	 loss: 27.6135, MinusLogProbMetric: 27.6135, val_loss: 27.8705, val_MinusLogProbMetric: 27.8705

Epoch 752: val_loss did not improve from 27.85021
196/196 - 32s - loss: 27.6135 - MinusLogProbMetric: 27.6135 - val_loss: 27.8705 - val_MinusLogProbMetric: 27.8705 - lr: 6.2500e-05 - 32s/epoch - 161ms/step
Epoch 753/1000
2023-09-29 04:45:57.711 
Epoch 753/1000 
	 loss: 27.6134, MinusLogProbMetric: 27.6134, val_loss: 27.8548, val_MinusLogProbMetric: 27.8548

Epoch 753: val_loss did not improve from 27.85021
196/196 - 31s - loss: 27.6134 - MinusLogProbMetric: 27.6134 - val_loss: 27.8548 - val_MinusLogProbMetric: 27.8548 - lr: 6.2500e-05 - 31s/epoch - 157ms/step
Epoch 754/1000
2023-09-29 04:46:30.767 
Epoch 754/1000 
	 loss: 27.6152, MinusLogProbMetric: 27.6152, val_loss: 27.8716, val_MinusLogProbMetric: 27.8716

Epoch 754: val_loss did not improve from 27.85021
196/196 - 33s - loss: 27.6152 - MinusLogProbMetric: 27.6152 - val_loss: 27.8716 - val_MinusLogProbMetric: 27.8716 - lr: 6.2500e-05 - 33s/epoch - 169ms/step
Epoch 755/1000
2023-09-29 04:47:03.240 
Epoch 755/1000 
	 loss: 27.6164, MinusLogProbMetric: 27.6164, val_loss: 27.8836, val_MinusLogProbMetric: 27.8836

Epoch 755: val_loss did not improve from 27.85021
196/196 - 32s - loss: 27.6164 - MinusLogProbMetric: 27.6164 - val_loss: 27.8836 - val_MinusLogProbMetric: 27.8836 - lr: 6.2500e-05 - 32s/epoch - 166ms/step
Epoch 756/1000
2023-09-29 04:47:34.540 
Epoch 756/1000 
	 loss: 27.6162, MinusLogProbMetric: 27.6162, val_loss: 27.8612, val_MinusLogProbMetric: 27.8612

Epoch 756: val_loss did not improve from 27.85021
196/196 - 31s - loss: 27.6162 - MinusLogProbMetric: 27.6162 - val_loss: 27.8612 - val_MinusLogProbMetric: 27.8612 - lr: 6.2500e-05 - 31s/epoch - 160ms/step
Epoch 757/1000
2023-09-29 04:48:09.031 
Epoch 757/1000 
	 loss: 27.6136, MinusLogProbMetric: 27.6136, val_loss: 27.8691, val_MinusLogProbMetric: 27.8691

Epoch 757: val_loss did not improve from 27.85021
196/196 - 34s - loss: 27.6136 - MinusLogProbMetric: 27.6136 - val_loss: 27.8691 - val_MinusLogProbMetric: 27.8691 - lr: 6.2500e-05 - 34s/epoch - 176ms/step
Epoch 758/1000
2023-09-29 04:48:42.395 
Epoch 758/1000 
	 loss: 27.6141, MinusLogProbMetric: 27.6141, val_loss: 27.8700, val_MinusLogProbMetric: 27.8700

Epoch 758: val_loss did not improve from 27.85021
196/196 - 33s - loss: 27.6141 - MinusLogProbMetric: 27.6141 - val_loss: 27.8700 - val_MinusLogProbMetric: 27.8700 - lr: 6.2500e-05 - 33s/epoch - 170ms/step
Epoch 759/1000
2023-09-29 04:49:15.590 
Epoch 759/1000 
	 loss: 27.6150, MinusLogProbMetric: 27.6150, val_loss: 27.8739, val_MinusLogProbMetric: 27.8739

Epoch 759: val_loss did not improve from 27.85021
196/196 - 33s - loss: 27.6150 - MinusLogProbMetric: 27.6150 - val_loss: 27.8739 - val_MinusLogProbMetric: 27.8739 - lr: 6.2500e-05 - 33s/epoch - 169ms/step
Epoch 760/1000
2023-09-29 04:49:48.593 
Epoch 760/1000 
	 loss: 27.6143, MinusLogProbMetric: 27.6143, val_loss: 27.8656, val_MinusLogProbMetric: 27.8656

Epoch 760: val_loss did not improve from 27.85021
196/196 - 33s - loss: 27.6143 - MinusLogProbMetric: 27.6143 - val_loss: 27.8656 - val_MinusLogProbMetric: 27.8656 - lr: 6.2500e-05 - 33s/epoch - 168ms/step
Epoch 761/1000
2023-09-29 04:50:23.060 
Epoch 761/1000 
	 loss: 27.6139, MinusLogProbMetric: 27.6139, val_loss: 27.8532, val_MinusLogProbMetric: 27.8532

Epoch 761: val_loss did not improve from 27.85021
196/196 - 34s - loss: 27.6139 - MinusLogProbMetric: 27.6139 - val_loss: 27.8532 - val_MinusLogProbMetric: 27.8532 - lr: 6.2500e-05 - 34s/epoch - 176ms/step
Epoch 762/1000
2023-09-29 04:50:55.225 
Epoch 762/1000 
	 loss: 27.6125, MinusLogProbMetric: 27.6125, val_loss: 27.8763, val_MinusLogProbMetric: 27.8763

Epoch 762: val_loss did not improve from 27.85021
196/196 - 32s - loss: 27.6125 - MinusLogProbMetric: 27.6125 - val_loss: 27.8763 - val_MinusLogProbMetric: 27.8763 - lr: 6.2500e-05 - 32s/epoch - 164ms/step
Epoch 763/1000
2023-09-29 04:51:28.714 
Epoch 763/1000 
	 loss: 27.6107, MinusLogProbMetric: 27.6107, val_loss: 27.8771, val_MinusLogProbMetric: 27.8771

Epoch 763: val_loss did not improve from 27.85021
196/196 - 33s - loss: 27.6107 - MinusLogProbMetric: 27.6107 - val_loss: 27.8771 - val_MinusLogProbMetric: 27.8771 - lr: 6.2500e-05 - 33s/epoch - 171ms/step
Epoch 764/1000
2023-09-29 04:51:58.624 
Epoch 764/1000 
	 loss: 27.6175, MinusLogProbMetric: 27.6175, val_loss: 27.8606, val_MinusLogProbMetric: 27.8606

Epoch 764: val_loss did not improve from 27.85021
196/196 - 30s - loss: 27.6175 - MinusLogProbMetric: 27.6175 - val_loss: 27.8606 - val_MinusLogProbMetric: 27.8606 - lr: 6.2500e-05 - 30s/epoch - 153ms/step
Epoch 765/1000
2023-09-29 04:52:31.060 
Epoch 765/1000 
	 loss: 27.6121, MinusLogProbMetric: 27.6121, val_loss: 27.8671, val_MinusLogProbMetric: 27.8671

Epoch 765: val_loss did not improve from 27.85021
196/196 - 32s - loss: 27.6121 - MinusLogProbMetric: 27.6121 - val_loss: 27.8671 - val_MinusLogProbMetric: 27.8671 - lr: 6.2500e-05 - 32s/epoch - 165ms/step
Epoch 766/1000
2023-09-29 04:53:03.915 
Epoch 766/1000 
	 loss: 27.6195, MinusLogProbMetric: 27.6195, val_loss: 27.8580, val_MinusLogProbMetric: 27.8580

Epoch 766: val_loss did not improve from 27.85021
196/196 - 33s - loss: 27.6195 - MinusLogProbMetric: 27.6195 - val_loss: 27.8580 - val_MinusLogProbMetric: 27.8580 - lr: 6.2500e-05 - 33s/epoch - 168ms/step
Epoch 767/1000
2023-09-29 04:53:37.172 
Epoch 767/1000 
	 loss: 27.6141, MinusLogProbMetric: 27.6141, val_loss: 27.8564, val_MinusLogProbMetric: 27.8564

Epoch 767: val_loss did not improve from 27.85021
196/196 - 33s - loss: 27.6141 - MinusLogProbMetric: 27.6141 - val_loss: 27.8564 - val_MinusLogProbMetric: 27.8564 - lr: 6.2500e-05 - 33s/epoch - 170ms/step
Epoch 768/1000
2023-09-29 04:54:08.943 
Epoch 768/1000 
	 loss: 27.6093, MinusLogProbMetric: 27.6093, val_loss: 27.8557, val_MinusLogProbMetric: 27.8557

Epoch 768: val_loss did not improve from 27.85021
196/196 - 32s - loss: 27.6093 - MinusLogProbMetric: 27.6093 - val_loss: 27.8557 - val_MinusLogProbMetric: 27.8557 - lr: 6.2500e-05 - 32s/epoch - 162ms/step
Epoch 769/1000
2023-09-29 04:54:36.460 
Epoch 769/1000 
	 loss: 27.6126, MinusLogProbMetric: 27.6126, val_loss: 27.8583, val_MinusLogProbMetric: 27.8583

Epoch 769: val_loss did not improve from 27.85021
196/196 - 28s - loss: 27.6126 - MinusLogProbMetric: 27.6126 - val_loss: 27.8583 - val_MinusLogProbMetric: 27.8583 - lr: 6.2500e-05 - 28s/epoch - 140ms/step
Epoch 770/1000
2023-09-29 04:55:09.035 
Epoch 770/1000 
	 loss: 27.6118, MinusLogProbMetric: 27.6118, val_loss: 27.8589, val_MinusLogProbMetric: 27.8589

Epoch 770: val_loss did not improve from 27.85021
196/196 - 33s - loss: 27.6118 - MinusLogProbMetric: 27.6118 - val_loss: 27.8589 - val_MinusLogProbMetric: 27.8589 - lr: 6.2500e-05 - 33s/epoch - 166ms/step
Epoch 771/1000
2023-09-29 04:55:42.819 
Epoch 771/1000 
	 loss: 27.6135, MinusLogProbMetric: 27.6135, val_loss: 27.8598, val_MinusLogProbMetric: 27.8598

Epoch 771: val_loss did not improve from 27.85021
196/196 - 34s - loss: 27.6135 - MinusLogProbMetric: 27.6135 - val_loss: 27.8598 - val_MinusLogProbMetric: 27.8598 - lr: 6.2500e-05 - 34s/epoch - 172ms/step
Epoch 772/1000
2023-09-29 04:56:13.499 
Epoch 772/1000 
	 loss: 27.6099, MinusLogProbMetric: 27.6099, val_loss: 27.8548, val_MinusLogProbMetric: 27.8548

Epoch 772: val_loss did not improve from 27.85021
196/196 - 31s - loss: 27.6099 - MinusLogProbMetric: 27.6099 - val_loss: 27.8548 - val_MinusLogProbMetric: 27.8548 - lr: 6.2500e-05 - 31s/epoch - 157ms/step
Epoch 773/1000
2023-09-29 04:56:44.341 
Epoch 773/1000 
	 loss: 27.6132, MinusLogProbMetric: 27.6132, val_loss: 27.8677, val_MinusLogProbMetric: 27.8677

Epoch 773: val_loss did not improve from 27.85021
196/196 - 31s - loss: 27.6132 - MinusLogProbMetric: 27.6132 - val_loss: 27.8677 - val_MinusLogProbMetric: 27.8677 - lr: 6.2500e-05 - 31s/epoch - 157ms/step
Epoch 774/1000
2023-09-29 04:57:14.299 
Epoch 774/1000 
	 loss: 27.6105, MinusLogProbMetric: 27.6105, val_loss: 27.8522, val_MinusLogProbMetric: 27.8522

Epoch 774: val_loss did not improve from 27.85021
196/196 - 30s - loss: 27.6105 - MinusLogProbMetric: 27.6105 - val_loss: 27.8522 - val_MinusLogProbMetric: 27.8522 - lr: 6.2500e-05 - 30s/epoch - 153ms/step
Epoch 775/1000
2023-09-29 04:57:43.551 
Epoch 775/1000 
	 loss: 27.6142, MinusLogProbMetric: 27.6142, val_loss: 27.8664, val_MinusLogProbMetric: 27.8664

Epoch 775: val_loss did not improve from 27.85021
196/196 - 29s - loss: 27.6142 - MinusLogProbMetric: 27.6142 - val_loss: 27.8664 - val_MinusLogProbMetric: 27.8664 - lr: 6.2500e-05 - 29s/epoch - 149ms/step
Epoch 776/1000
2023-09-29 04:58:14.582 
Epoch 776/1000 
	 loss: 27.6105, MinusLogProbMetric: 27.6105, val_loss: 27.8566, val_MinusLogProbMetric: 27.8566

Epoch 776: val_loss did not improve from 27.85021
196/196 - 31s - loss: 27.6105 - MinusLogProbMetric: 27.6105 - val_loss: 27.8566 - val_MinusLogProbMetric: 27.8566 - lr: 6.2500e-05 - 31s/epoch - 158ms/step
Epoch 777/1000
2023-09-29 04:58:47.408 
Epoch 777/1000 
	 loss: 27.6102, MinusLogProbMetric: 27.6102, val_loss: 27.8556, val_MinusLogProbMetric: 27.8556

Epoch 777: val_loss did not improve from 27.85021
196/196 - 33s - loss: 27.6102 - MinusLogProbMetric: 27.6102 - val_loss: 27.8556 - val_MinusLogProbMetric: 27.8556 - lr: 6.2500e-05 - 33s/epoch - 167ms/step
Epoch 778/1000
2023-09-29 04:59:19.511 
Epoch 778/1000 
	 loss: 27.6128, MinusLogProbMetric: 27.6128, val_loss: 27.8728, val_MinusLogProbMetric: 27.8728

Epoch 778: val_loss did not improve from 27.85021
196/196 - 32s - loss: 27.6128 - MinusLogProbMetric: 27.6128 - val_loss: 27.8728 - val_MinusLogProbMetric: 27.8728 - lr: 6.2500e-05 - 32s/epoch - 164ms/step
Epoch 779/1000
2023-09-29 04:59:50.202 
Epoch 779/1000 
	 loss: 27.6128, MinusLogProbMetric: 27.6128, val_loss: 27.8577, val_MinusLogProbMetric: 27.8577

Epoch 779: val_loss did not improve from 27.85021
196/196 - 31s - loss: 27.6128 - MinusLogProbMetric: 27.6128 - val_loss: 27.8577 - val_MinusLogProbMetric: 27.8577 - lr: 6.2500e-05 - 31s/epoch - 157ms/step
Epoch 780/1000
2023-09-29 05:00:21.145 
Epoch 780/1000 
	 loss: 27.6107, MinusLogProbMetric: 27.6107, val_loss: 27.8617, val_MinusLogProbMetric: 27.8617

Epoch 780: val_loss did not improve from 27.85021
196/196 - 31s - loss: 27.6107 - MinusLogProbMetric: 27.6107 - val_loss: 27.8617 - val_MinusLogProbMetric: 27.8617 - lr: 6.2500e-05 - 31s/epoch - 158ms/step
Epoch 781/1000
2023-09-29 05:00:51.148 
Epoch 781/1000 
	 loss: 27.6096, MinusLogProbMetric: 27.6096, val_loss: 27.8577, val_MinusLogProbMetric: 27.8577

Epoch 781: val_loss did not improve from 27.85021
196/196 - 30s - loss: 27.6096 - MinusLogProbMetric: 27.6096 - val_loss: 27.8577 - val_MinusLogProbMetric: 27.8577 - lr: 6.2500e-05 - 30s/epoch - 153ms/step
Epoch 782/1000
2023-09-29 05:01:23.174 
Epoch 782/1000 
	 loss: 27.6124, MinusLogProbMetric: 27.6124, val_loss: 27.8577, val_MinusLogProbMetric: 27.8577

Epoch 782: val_loss did not improve from 27.85021
196/196 - 32s - loss: 27.6124 - MinusLogProbMetric: 27.6124 - val_loss: 27.8577 - val_MinusLogProbMetric: 27.8577 - lr: 6.2500e-05 - 32s/epoch - 163ms/step
Epoch 783/1000
2023-09-29 05:01:53.614 
Epoch 783/1000 
	 loss: 27.6099, MinusLogProbMetric: 27.6099, val_loss: 27.8619, val_MinusLogProbMetric: 27.8619

Epoch 783: val_loss did not improve from 27.85021
196/196 - 30s - loss: 27.6099 - MinusLogProbMetric: 27.6099 - val_loss: 27.8619 - val_MinusLogProbMetric: 27.8619 - lr: 6.2500e-05 - 30s/epoch - 155ms/step
Epoch 784/1000
2023-09-29 05:02:22.708 
Epoch 784/1000 
	 loss: 27.6084, MinusLogProbMetric: 27.6084, val_loss: 27.8665, val_MinusLogProbMetric: 27.8665

Epoch 784: val_loss did not improve from 27.85021
196/196 - 29s - loss: 27.6084 - MinusLogProbMetric: 27.6084 - val_loss: 27.8665 - val_MinusLogProbMetric: 27.8665 - lr: 6.2500e-05 - 29s/epoch - 148ms/step
Epoch 785/1000
2023-09-29 05:02:51.244 
Epoch 785/1000 
	 loss: 27.6100, MinusLogProbMetric: 27.6100, val_loss: 27.8562, val_MinusLogProbMetric: 27.8562

Epoch 785: val_loss did not improve from 27.85021
196/196 - 29s - loss: 27.6100 - MinusLogProbMetric: 27.6100 - val_loss: 27.8562 - val_MinusLogProbMetric: 27.8562 - lr: 6.2500e-05 - 29s/epoch - 146ms/step
Epoch 786/1000
2023-09-29 05:03:20.308 
Epoch 786/1000 
	 loss: 27.6095, MinusLogProbMetric: 27.6095, val_loss: 27.8639, val_MinusLogProbMetric: 27.8639

Epoch 786: val_loss did not improve from 27.85021
196/196 - 29s - loss: 27.6095 - MinusLogProbMetric: 27.6095 - val_loss: 27.8639 - val_MinusLogProbMetric: 27.8639 - lr: 6.2500e-05 - 29s/epoch - 148ms/step
Epoch 787/1000
2023-09-29 05:03:54.716 
Epoch 787/1000 
	 loss: 27.6124, MinusLogProbMetric: 27.6124, val_loss: 27.8654, val_MinusLogProbMetric: 27.8654

Epoch 787: val_loss did not improve from 27.85021
196/196 - 34s - loss: 27.6124 - MinusLogProbMetric: 27.6124 - val_loss: 27.8654 - val_MinusLogProbMetric: 27.8654 - lr: 6.2500e-05 - 34s/epoch - 176ms/step
Epoch 788/1000
2023-09-29 05:04:27.110 
Epoch 788/1000 
	 loss: 27.6105, MinusLogProbMetric: 27.6105, val_loss: 27.8667, val_MinusLogProbMetric: 27.8667

Epoch 788: val_loss did not improve from 27.85021
196/196 - 32s - loss: 27.6105 - MinusLogProbMetric: 27.6105 - val_loss: 27.8667 - val_MinusLogProbMetric: 27.8667 - lr: 6.2500e-05 - 32s/epoch - 165ms/step
Epoch 789/1000
2023-09-29 05:04:57.510 
Epoch 789/1000 
	 loss: 27.6082, MinusLogProbMetric: 27.6082, val_loss: 27.8846, val_MinusLogProbMetric: 27.8846

Epoch 789: val_loss did not improve from 27.85021
196/196 - 30s - loss: 27.6082 - MinusLogProbMetric: 27.6082 - val_loss: 27.8846 - val_MinusLogProbMetric: 27.8846 - lr: 6.2500e-05 - 30s/epoch - 155ms/step
Epoch 790/1000
2023-09-29 05:05:28.334 
Epoch 790/1000 
	 loss: 27.6130, MinusLogProbMetric: 27.6130, val_loss: 27.8672, val_MinusLogProbMetric: 27.8672

Epoch 790: val_loss did not improve from 27.85021
196/196 - 31s - loss: 27.6130 - MinusLogProbMetric: 27.6130 - val_loss: 27.8672 - val_MinusLogProbMetric: 27.8672 - lr: 6.2500e-05 - 31s/epoch - 157ms/step
Epoch 791/1000
2023-09-29 05:05:57.722 
Epoch 791/1000 
	 loss: 27.6112, MinusLogProbMetric: 27.6112, val_loss: 27.8619, val_MinusLogProbMetric: 27.8619

Epoch 791: val_loss did not improve from 27.85021
196/196 - 29s - loss: 27.6112 - MinusLogProbMetric: 27.6112 - val_loss: 27.8619 - val_MinusLogProbMetric: 27.8619 - lr: 6.2500e-05 - 29s/epoch - 150ms/step
Epoch 792/1000
2023-09-29 05:06:25.511 
Epoch 792/1000 
	 loss: 27.6127, MinusLogProbMetric: 27.6127, val_loss: 27.9104, val_MinusLogProbMetric: 27.9104

Epoch 792: val_loss did not improve from 27.85021
196/196 - 28s - loss: 27.6127 - MinusLogProbMetric: 27.6127 - val_loss: 27.9104 - val_MinusLogProbMetric: 27.9104 - lr: 6.2500e-05 - 28s/epoch - 142ms/step
Epoch 793/1000
2023-09-29 05:06:53.893 
Epoch 793/1000 
	 loss: 27.6118, MinusLogProbMetric: 27.6118, val_loss: 27.8753, val_MinusLogProbMetric: 27.8753

Epoch 793: val_loss did not improve from 27.85021
196/196 - 28s - loss: 27.6118 - MinusLogProbMetric: 27.6118 - val_loss: 27.8753 - val_MinusLogProbMetric: 27.8753 - lr: 6.2500e-05 - 28s/epoch - 145ms/step
Epoch 794/1000
2023-09-29 05:07:24.235 
Epoch 794/1000 
	 loss: 27.6118, MinusLogProbMetric: 27.6118, val_loss: 27.8614, val_MinusLogProbMetric: 27.8614

Epoch 794: val_loss did not improve from 27.85021
196/196 - 30s - loss: 27.6118 - MinusLogProbMetric: 27.6118 - val_loss: 27.8614 - val_MinusLogProbMetric: 27.8614 - lr: 6.2500e-05 - 30s/epoch - 155ms/step
Epoch 795/1000
2023-09-29 05:07:54.326 
Epoch 795/1000 
	 loss: 27.6102, MinusLogProbMetric: 27.6102, val_loss: 27.8613, val_MinusLogProbMetric: 27.8613

Epoch 795: val_loss did not improve from 27.85021
196/196 - 30s - loss: 27.6102 - MinusLogProbMetric: 27.6102 - val_loss: 27.8613 - val_MinusLogProbMetric: 27.8613 - lr: 6.2500e-05 - 30s/epoch - 153ms/step
Epoch 796/1000
2023-09-29 05:08:25.518 
Epoch 796/1000 
	 loss: 27.6084, MinusLogProbMetric: 27.6084, val_loss: 27.8770, val_MinusLogProbMetric: 27.8770

Epoch 796: val_loss did not improve from 27.85021
196/196 - 31s - loss: 27.6084 - MinusLogProbMetric: 27.6084 - val_loss: 27.8770 - val_MinusLogProbMetric: 27.8770 - lr: 6.2500e-05 - 31s/epoch - 159ms/step
Epoch 797/1000
2023-09-29 05:08:56.713 
Epoch 797/1000 
	 loss: 27.6100, MinusLogProbMetric: 27.6100, val_loss: 27.8679, val_MinusLogProbMetric: 27.8679

Epoch 797: val_loss did not improve from 27.85021
196/196 - 31s - loss: 27.6100 - MinusLogProbMetric: 27.6100 - val_loss: 27.8679 - val_MinusLogProbMetric: 27.8679 - lr: 6.2500e-05 - 31s/epoch - 159ms/step
Epoch 798/1000
2023-09-29 05:09:24.949 
Epoch 798/1000 
	 loss: 27.6102, MinusLogProbMetric: 27.6102, val_loss: 27.8491, val_MinusLogProbMetric: 27.8491

Epoch 798: val_loss improved from 27.85021 to 27.84905, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_329/weights/best_weights.h5
196/196 - 29s - loss: 27.6102 - MinusLogProbMetric: 27.6102 - val_loss: 27.8491 - val_MinusLogProbMetric: 27.8491 - lr: 6.2500e-05 - 29s/epoch - 147ms/step
Epoch 799/1000
2023-09-29 05:09:56.717 
Epoch 799/1000 
	 loss: 27.6110, MinusLogProbMetric: 27.6110, val_loss: 27.8659, val_MinusLogProbMetric: 27.8659

Epoch 799: val_loss did not improve from 27.84905
196/196 - 31s - loss: 27.6110 - MinusLogProbMetric: 27.6110 - val_loss: 27.8659 - val_MinusLogProbMetric: 27.8659 - lr: 6.2500e-05 - 31s/epoch - 159ms/step
Epoch 800/1000
2023-09-29 05:10:25.786 
Epoch 800/1000 
	 loss: 27.6121, MinusLogProbMetric: 27.6121, val_loss: 27.8657, val_MinusLogProbMetric: 27.8657

Epoch 800: val_loss did not improve from 27.84905
196/196 - 29s - loss: 27.6121 - MinusLogProbMetric: 27.6121 - val_loss: 27.8657 - val_MinusLogProbMetric: 27.8657 - lr: 6.2500e-05 - 29s/epoch - 148ms/step
Epoch 801/1000
2023-09-29 05:10:55.078 
Epoch 801/1000 
	 loss: 27.6129, MinusLogProbMetric: 27.6129, val_loss: 27.8528, val_MinusLogProbMetric: 27.8528

Epoch 801: val_loss did not improve from 27.84905
196/196 - 29s - loss: 27.6129 - MinusLogProbMetric: 27.6129 - val_loss: 27.8528 - val_MinusLogProbMetric: 27.8528 - lr: 6.2500e-05 - 29s/epoch - 149ms/step
Epoch 802/1000
2023-09-29 05:11:24.693 
Epoch 802/1000 
	 loss: 27.6078, MinusLogProbMetric: 27.6078, val_loss: 27.8590, val_MinusLogProbMetric: 27.8590

Epoch 802: val_loss did not improve from 27.84905
196/196 - 30s - loss: 27.6078 - MinusLogProbMetric: 27.6078 - val_loss: 27.8590 - val_MinusLogProbMetric: 27.8590 - lr: 6.2500e-05 - 30s/epoch - 151ms/step
Epoch 803/1000
2023-09-29 05:11:55.176 
Epoch 803/1000 
	 loss: 27.6086, MinusLogProbMetric: 27.6086, val_loss: 27.8717, val_MinusLogProbMetric: 27.8717

Epoch 803: val_loss did not improve from 27.84905
196/196 - 30s - loss: 27.6086 - MinusLogProbMetric: 27.6086 - val_loss: 27.8717 - val_MinusLogProbMetric: 27.8717 - lr: 6.2500e-05 - 30s/epoch - 156ms/step
Epoch 804/1000
2023-09-29 05:12:26.196 
Epoch 804/1000 
	 loss: 27.6092, MinusLogProbMetric: 27.6092, val_loss: 27.8729, val_MinusLogProbMetric: 27.8729

Epoch 804: val_loss did not improve from 27.84905
196/196 - 31s - loss: 27.6092 - MinusLogProbMetric: 27.6092 - val_loss: 27.8729 - val_MinusLogProbMetric: 27.8729 - lr: 6.2500e-05 - 31s/epoch - 158ms/step
Epoch 805/1000
2023-09-29 05:12:54.711 
Epoch 805/1000 
	 loss: 27.6112, MinusLogProbMetric: 27.6112, val_loss: 27.8763, val_MinusLogProbMetric: 27.8763

Epoch 805: val_loss did not improve from 27.84905
196/196 - 29s - loss: 27.6112 - MinusLogProbMetric: 27.6112 - val_loss: 27.8763 - val_MinusLogProbMetric: 27.8763 - lr: 6.2500e-05 - 29s/epoch - 145ms/step
Epoch 806/1000
2023-09-29 05:13:25.938 
Epoch 806/1000 
	 loss: 27.6110, MinusLogProbMetric: 27.6110, val_loss: 27.8618, val_MinusLogProbMetric: 27.8618

Epoch 806: val_loss did not improve from 27.84905
196/196 - 31s - loss: 27.6110 - MinusLogProbMetric: 27.6110 - val_loss: 27.8618 - val_MinusLogProbMetric: 27.8618 - lr: 6.2500e-05 - 31s/epoch - 159ms/step
Epoch 807/1000
2023-09-29 05:13:57.579 
Epoch 807/1000 
	 loss: 27.6091, MinusLogProbMetric: 27.6091, val_loss: 27.8610, val_MinusLogProbMetric: 27.8610

Epoch 807: val_loss did not improve from 27.84905
196/196 - 32s - loss: 27.6091 - MinusLogProbMetric: 27.6091 - val_loss: 27.8610 - val_MinusLogProbMetric: 27.8610 - lr: 6.2500e-05 - 32s/epoch - 161ms/step
Epoch 808/1000
2023-09-29 05:14:28.733 
Epoch 808/1000 
	 loss: 27.6099, MinusLogProbMetric: 27.6099, val_loss: 27.8515, val_MinusLogProbMetric: 27.8515

Epoch 808: val_loss did not improve from 27.84905
196/196 - 31s - loss: 27.6099 - MinusLogProbMetric: 27.6099 - val_loss: 27.8515 - val_MinusLogProbMetric: 27.8515 - lr: 6.2500e-05 - 31s/epoch - 159ms/step
Epoch 809/1000
2023-09-29 05:15:00.465 
Epoch 809/1000 
	 loss: 27.6116, MinusLogProbMetric: 27.6116, val_loss: 27.8641, val_MinusLogProbMetric: 27.8641

Epoch 809: val_loss did not improve from 27.84905
196/196 - 32s - loss: 27.6116 - MinusLogProbMetric: 27.6116 - val_loss: 27.8641 - val_MinusLogProbMetric: 27.8641 - lr: 6.2500e-05 - 32s/epoch - 162ms/step
Epoch 810/1000
2023-09-29 05:15:33.130 
Epoch 810/1000 
	 loss: 27.6089, MinusLogProbMetric: 27.6089, val_loss: 27.8532, val_MinusLogProbMetric: 27.8532

Epoch 810: val_loss did not improve from 27.84905
196/196 - 33s - loss: 27.6089 - MinusLogProbMetric: 27.6089 - val_loss: 27.8532 - val_MinusLogProbMetric: 27.8532 - lr: 6.2500e-05 - 33s/epoch - 167ms/step
Epoch 811/1000
2023-09-29 05:16:02.087 
Epoch 811/1000 
	 loss: 27.6105, MinusLogProbMetric: 27.6105, val_loss: 27.8667, val_MinusLogProbMetric: 27.8667

Epoch 811: val_loss did not improve from 27.84905
196/196 - 29s - loss: 27.6105 - MinusLogProbMetric: 27.6105 - val_loss: 27.8667 - val_MinusLogProbMetric: 27.8667 - lr: 6.2500e-05 - 29s/epoch - 148ms/step
Epoch 812/1000
2023-09-29 05:16:34.170 
Epoch 812/1000 
	 loss: 27.6107, MinusLogProbMetric: 27.6107, val_loss: 27.8662, val_MinusLogProbMetric: 27.8662

Epoch 812: val_loss did not improve from 27.84905
196/196 - 32s - loss: 27.6107 - MinusLogProbMetric: 27.6107 - val_loss: 27.8662 - val_MinusLogProbMetric: 27.8662 - lr: 6.2500e-05 - 32s/epoch - 164ms/step
Epoch 813/1000
2023-09-29 05:17:05.299 
Epoch 813/1000 
	 loss: 27.6091, MinusLogProbMetric: 27.6091, val_loss: 27.8697, val_MinusLogProbMetric: 27.8697

Epoch 813: val_loss did not improve from 27.84905
196/196 - 31s - loss: 27.6091 - MinusLogProbMetric: 27.6091 - val_loss: 27.8697 - val_MinusLogProbMetric: 27.8697 - lr: 6.2500e-05 - 31s/epoch - 159ms/step
Epoch 814/1000
2023-09-29 05:17:38.679 
Epoch 814/1000 
	 loss: 27.6087, MinusLogProbMetric: 27.6087, val_loss: 27.8555, val_MinusLogProbMetric: 27.8555

Epoch 814: val_loss did not improve from 27.84905
196/196 - 33s - loss: 27.6087 - MinusLogProbMetric: 27.6087 - val_loss: 27.8555 - val_MinusLogProbMetric: 27.8555 - lr: 6.2500e-05 - 33s/epoch - 170ms/step
Epoch 815/1000
2023-09-29 05:18:12.015 
Epoch 815/1000 
	 loss: 27.6109, MinusLogProbMetric: 27.6109, val_loss: 27.8569, val_MinusLogProbMetric: 27.8569

Epoch 815: val_loss did not improve from 27.84905
196/196 - 33s - loss: 27.6109 - MinusLogProbMetric: 27.6109 - val_loss: 27.8569 - val_MinusLogProbMetric: 27.8569 - lr: 6.2500e-05 - 33s/epoch - 170ms/step
Epoch 816/1000
2023-09-29 05:18:44.733 
Epoch 816/1000 
	 loss: 27.6081, MinusLogProbMetric: 27.6081, val_loss: 27.8659, val_MinusLogProbMetric: 27.8659

Epoch 816: val_loss did not improve from 27.84905
196/196 - 33s - loss: 27.6081 - MinusLogProbMetric: 27.6081 - val_loss: 27.8659 - val_MinusLogProbMetric: 27.8659 - lr: 6.2500e-05 - 33s/epoch - 167ms/step
Epoch 817/1000
2023-09-29 05:19:15.867 
Epoch 817/1000 
	 loss: 27.6099, MinusLogProbMetric: 27.6099, val_loss: 27.8544, val_MinusLogProbMetric: 27.8544

Epoch 817: val_loss did not improve from 27.84905
196/196 - 31s - loss: 27.6099 - MinusLogProbMetric: 27.6099 - val_loss: 27.8544 - val_MinusLogProbMetric: 27.8544 - lr: 6.2500e-05 - 31s/epoch - 159ms/step
Epoch 818/1000
2023-09-29 05:19:49.312 
Epoch 818/1000 
	 loss: 27.6069, MinusLogProbMetric: 27.6069, val_loss: 27.8604, val_MinusLogProbMetric: 27.8604

Epoch 818: val_loss did not improve from 27.84905
196/196 - 33s - loss: 27.6069 - MinusLogProbMetric: 27.6069 - val_loss: 27.8604 - val_MinusLogProbMetric: 27.8604 - lr: 6.2500e-05 - 33s/epoch - 171ms/step
Epoch 819/1000
2023-09-29 05:20:22.572 
Epoch 819/1000 
	 loss: 27.6091, MinusLogProbMetric: 27.6091, val_loss: 27.8764, val_MinusLogProbMetric: 27.8764

Epoch 819: val_loss did not improve from 27.84905
196/196 - 33s - loss: 27.6091 - MinusLogProbMetric: 27.6091 - val_loss: 27.8764 - val_MinusLogProbMetric: 27.8764 - lr: 6.2500e-05 - 33s/epoch - 170ms/step
Epoch 820/1000
2023-09-29 05:20:55.603 
Epoch 820/1000 
	 loss: 27.6110, MinusLogProbMetric: 27.6110, val_loss: 27.8915, val_MinusLogProbMetric: 27.8915

Epoch 820: val_loss did not improve from 27.84905
196/196 - 33s - loss: 27.6110 - MinusLogProbMetric: 27.6110 - val_loss: 27.8915 - val_MinusLogProbMetric: 27.8915 - lr: 6.2500e-05 - 33s/epoch - 169ms/step
Epoch 821/1000
2023-09-29 05:21:25.618 
Epoch 821/1000 
	 loss: 27.6111, MinusLogProbMetric: 27.6111, val_loss: 27.8732, val_MinusLogProbMetric: 27.8732

Epoch 821: val_loss did not improve from 27.84905
196/196 - 30s - loss: 27.6111 - MinusLogProbMetric: 27.6111 - val_loss: 27.8732 - val_MinusLogProbMetric: 27.8732 - lr: 6.2500e-05 - 30s/epoch - 153ms/step
Epoch 822/1000
2023-09-29 05:21:56.474 
Epoch 822/1000 
	 loss: 27.6116, MinusLogProbMetric: 27.6116, val_loss: 27.8528, val_MinusLogProbMetric: 27.8528

Epoch 822: val_loss did not improve from 27.84905
196/196 - 31s - loss: 27.6116 - MinusLogProbMetric: 27.6116 - val_loss: 27.8528 - val_MinusLogProbMetric: 27.8528 - lr: 6.2500e-05 - 31s/epoch - 157ms/step
Epoch 823/1000
2023-09-29 05:22:25.585 
Epoch 823/1000 
	 loss: 27.6099, MinusLogProbMetric: 27.6099, val_loss: 27.8496, val_MinusLogProbMetric: 27.8496

Epoch 823: val_loss did not improve from 27.84905
196/196 - 29s - loss: 27.6099 - MinusLogProbMetric: 27.6099 - val_loss: 27.8496 - val_MinusLogProbMetric: 27.8496 - lr: 6.2500e-05 - 29s/epoch - 149ms/step
Epoch 824/1000
2023-09-29 05:22:55.874 
Epoch 824/1000 
	 loss: 27.6118, MinusLogProbMetric: 27.6118, val_loss: 27.8554, val_MinusLogProbMetric: 27.8554

Epoch 824: val_loss did not improve from 27.84905
196/196 - 30s - loss: 27.6118 - MinusLogProbMetric: 27.6118 - val_loss: 27.8554 - val_MinusLogProbMetric: 27.8554 - lr: 6.2500e-05 - 30s/epoch - 155ms/step
Epoch 825/1000
2023-09-29 05:23:26.574 
Epoch 825/1000 
	 loss: 27.6072, MinusLogProbMetric: 27.6072, val_loss: 27.8536, val_MinusLogProbMetric: 27.8536

Epoch 825: val_loss did not improve from 27.84905
196/196 - 31s - loss: 27.6072 - MinusLogProbMetric: 27.6072 - val_loss: 27.8536 - val_MinusLogProbMetric: 27.8536 - lr: 6.2500e-05 - 31s/epoch - 157ms/step
Epoch 826/1000
2023-09-29 05:23:56.433 
Epoch 826/1000 
	 loss: 27.6131, MinusLogProbMetric: 27.6131, val_loss: 27.8626, val_MinusLogProbMetric: 27.8626

Epoch 826: val_loss did not improve from 27.84905
196/196 - 30s - loss: 27.6131 - MinusLogProbMetric: 27.6131 - val_loss: 27.8626 - val_MinusLogProbMetric: 27.8626 - lr: 6.2500e-05 - 30s/epoch - 152ms/step
Epoch 827/1000
2023-09-29 05:24:26.938 
Epoch 827/1000 
	 loss: 27.6073, MinusLogProbMetric: 27.6073, val_loss: 27.8557, val_MinusLogProbMetric: 27.8557

Epoch 827: val_loss did not improve from 27.84905
196/196 - 31s - loss: 27.6073 - MinusLogProbMetric: 27.6073 - val_loss: 27.8557 - val_MinusLogProbMetric: 27.8557 - lr: 6.2500e-05 - 31s/epoch - 156ms/step
Epoch 828/1000
2023-09-29 05:24:55.672 
Epoch 828/1000 
	 loss: 27.6114, MinusLogProbMetric: 27.6114, val_loss: 27.8624, val_MinusLogProbMetric: 27.8624

Epoch 828: val_loss did not improve from 27.84905
196/196 - 29s - loss: 27.6114 - MinusLogProbMetric: 27.6114 - val_loss: 27.8624 - val_MinusLogProbMetric: 27.8624 - lr: 6.2500e-05 - 29s/epoch - 147ms/step
Epoch 829/1000
2023-09-29 05:25:25.343 
Epoch 829/1000 
	 loss: 27.6077, MinusLogProbMetric: 27.6077, val_loss: 27.8610, val_MinusLogProbMetric: 27.8610

Epoch 829: val_loss did not improve from 27.84905
196/196 - 30s - loss: 27.6077 - MinusLogProbMetric: 27.6077 - val_loss: 27.8610 - val_MinusLogProbMetric: 27.8610 - lr: 6.2500e-05 - 30s/epoch - 151ms/step
Epoch 830/1000
2023-09-29 05:25:55.586 
Epoch 830/1000 
	 loss: 27.6090, MinusLogProbMetric: 27.6090, val_loss: 27.8598, val_MinusLogProbMetric: 27.8598

Epoch 830: val_loss did not improve from 27.84905
196/196 - 30s - loss: 27.6090 - MinusLogProbMetric: 27.6090 - val_loss: 27.8598 - val_MinusLogProbMetric: 27.8598 - lr: 6.2500e-05 - 30s/epoch - 154ms/step
Epoch 831/1000
2023-09-29 05:26:30.232 
Epoch 831/1000 
	 loss: 27.6077, MinusLogProbMetric: 27.6077, val_loss: 27.8571, val_MinusLogProbMetric: 27.8571

Epoch 831: val_loss did not improve from 27.84905
196/196 - 35s - loss: 27.6077 - MinusLogProbMetric: 27.6077 - val_loss: 27.8571 - val_MinusLogProbMetric: 27.8571 - lr: 6.2500e-05 - 35s/epoch - 177ms/step
Epoch 832/1000
2023-09-29 05:27:02.041 
Epoch 832/1000 
	 loss: 27.6111, MinusLogProbMetric: 27.6111, val_loss: 27.8547, val_MinusLogProbMetric: 27.8547

Epoch 832: val_loss did not improve from 27.84905
196/196 - 32s - loss: 27.6111 - MinusLogProbMetric: 27.6111 - val_loss: 27.8547 - val_MinusLogProbMetric: 27.8547 - lr: 6.2500e-05 - 32s/epoch - 162ms/step
Epoch 833/1000
2023-09-29 05:27:32.858 
Epoch 833/1000 
	 loss: 27.6101, MinusLogProbMetric: 27.6101, val_loss: 27.8628, val_MinusLogProbMetric: 27.8628

Epoch 833: val_loss did not improve from 27.84905
196/196 - 31s - loss: 27.6101 - MinusLogProbMetric: 27.6101 - val_loss: 27.8628 - val_MinusLogProbMetric: 27.8628 - lr: 6.2500e-05 - 31s/epoch - 157ms/step
Epoch 834/1000
2023-09-29 05:28:03.076 
Epoch 834/1000 
	 loss: 27.6095, MinusLogProbMetric: 27.6095, val_loss: 27.8731, val_MinusLogProbMetric: 27.8731

Epoch 834: val_loss did not improve from 27.84905
196/196 - 30s - loss: 27.6095 - MinusLogProbMetric: 27.6095 - val_loss: 27.8731 - val_MinusLogProbMetric: 27.8731 - lr: 6.2500e-05 - 30s/epoch - 154ms/step
Epoch 835/1000
2023-09-29 05:28:32.863 
Epoch 835/1000 
	 loss: 27.6087, MinusLogProbMetric: 27.6087, val_loss: 27.8727, val_MinusLogProbMetric: 27.8727

Epoch 835: val_loss did not improve from 27.84905
196/196 - 30s - loss: 27.6087 - MinusLogProbMetric: 27.6087 - val_loss: 27.8727 - val_MinusLogProbMetric: 27.8727 - lr: 6.2500e-05 - 30s/epoch - 152ms/step
Epoch 836/1000
2023-09-29 05:29:05.457 
Epoch 836/1000 
	 loss: 27.6083, MinusLogProbMetric: 27.6083, val_loss: 27.8535, val_MinusLogProbMetric: 27.8535

Epoch 836: val_loss did not improve from 27.84905
196/196 - 33s - loss: 27.6083 - MinusLogProbMetric: 27.6083 - val_loss: 27.8535 - val_MinusLogProbMetric: 27.8535 - lr: 6.2500e-05 - 33s/epoch - 166ms/step
Epoch 837/1000
2023-09-29 05:29:37.058 
Epoch 837/1000 
	 loss: 27.6088, MinusLogProbMetric: 27.6088, val_loss: 27.8668, val_MinusLogProbMetric: 27.8668

Epoch 837: val_loss did not improve from 27.84905
196/196 - 32s - loss: 27.6088 - MinusLogProbMetric: 27.6088 - val_loss: 27.8668 - val_MinusLogProbMetric: 27.8668 - lr: 6.2500e-05 - 32s/epoch - 161ms/step
Epoch 838/1000
2023-09-29 05:30:09.584 
Epoch 838/1000 
	 loss: 27.6074, MinusLogProbMetric: 27.6074, val_loss: 27.8544, val_MinusLogProbMetric: 27.8544

Epoch 838: val_loss did not improve from 27.84905
196/196 - 33s - loss: 27.6074 - MinusLogProbMetric: 27.6074 - val_loss: 27.8544 - val_MinusLogProbMetric: 27.8544 - lr: 6.2500e-05 - 33s/epoch - 166ms/step
Epoch 839/1000
2023-09-29 05:30:42.988 
Epoch 839/1000 
	 loss: 27.6085, MinusLogProbMetric: 27.6085, val_loss: 27.8507, val_MinusLogProbMetric: 27.8507

Epoch 839: val_loss did not improve from 27.84905
196/196 - 33s - loss: 27.6085 - MinusLogProbMetric: 27.6085 - val_loss: 27.8507 - val_MinusLogProbMetric: 27.8507 - lr: 6.2500e-05 - 33s/epoch - 170ms/step
Epoch 840/1000
2023-09-29 05:31:15.533 
Epoch 840/1000 
	 loss: 27.6101, MinusLogProbMetric: 27.6101, val_loss: 27.8707, val_MinusLogProbMetric: 27.8707

Epoch 840: val_loss did not improve from 27.84905
196/196 - 33s - loss: 27.6101 - MinusLogProbMetric: 27.6101 - val_loss: 27.8707 - val_MinusLogProbMetric: 27.8707 - lr: 6.2500e-05 - 33s/epoch - 166ms/step
Epoch 841/1000
2023-09-29 05:31:48.897 
Epoch 841/1000 
	 loss: 27.6073, MinusLogProbMetric: 27.6073, val_loss: 27.8589, val_MinusLogProbMetric: 27.8589

Epoch 841: val_loss did not improve from 27.84905
196/196 - 33s - loss: 27.6073 - MinusLogProbMetric: 27.6073 - val_loss: 27.8589 - val_MinusLogProbMetric: 27.8589 - lr: 6.2500e-05 - 33s/epoch - 170ms/step
Epoch 842/1000
2023-09-29 05:32:19.120 
Epoch 842/1000 
	 loss: 27.6089, MinusLogProbMetric: 27.6089, val_loss: 27.8578, val_MinusLogProbMetric: 27.8578

Epoch 842: val_loss did not improve from 27.84905
196/196 - 30s - loss: 27.6089 - MinusLogProbMetric: 27.6089 - val_loss: 27.8578 - val_MinusLogProbMetric: 27.8578 - lr: 6.2500e-05 - 30s/epoch - 154ms/step
Epoch 843/1000
2023-09-29 05:32:51.153 
Epoch 843/1000 
	 loss: 27.6083, MinusLogProbMetric: 27.6083, val_loss: 27.8708, val_MinusLogProbMetric: 27.8708

Epoch 843: val_loss did not improve from 27.84905
196/196 - 32s - loss: 27.6083 - MinusLogProbMetric: 27.6083 - val_loss: 27.8708 - val_MinusLogProbMetric: 27.8708 - lr: 6.2500e-05 - 32s/epoch - 163ms/step
Epoch 844/1000
2023-09-29 05:33:22.716 
Epoch 844/1000 
	 loss: 27.6078, MinusLogProbMetric: 27.6078, val_loss: 27.8533, val_MinusLogProbMetric: 27.8533

Epoch 844: val_loss did not improve from 27.84905
196/196 - 32s - loss: 27.6078 - MinusLogProbMetric: 27.6078 - val_loss: 27.8533 - val_MinusLogProbMetric: 27.8533 - lr: 6.2500e-05 - 32s/epoch - 161ms/step
Epoch 845/1000
2023-09-29 05:33:51.992 
Epoch 845/1000 
	 loss: 27.6092, MinusLogProbMetric: 27.6092, val_loss: 27.8569, val_MinusLogProbMetric: 27.8569

Epoch 845: val_loss did not improve from 27.84905
196/196 - 29s - loss: 27.6092 - MinusLogProbMetric: 27.6092 - val_loss: 27.8569 - val_MinusLogProbMetric: 27.8569 - lr: 6.2500e-05 - 29s/epoch - 149ms/step
Epoch 846/1000
2023-09-29 05:34:23.886 
Epoch 846/1000 
	 loss: 27.6130, MinusLogProbMetric: 27.6130, val_loss: 27.8681, val_MinusLogProbMetric: 27.8681

Epoch 846: val_loss did not improve from 27.84905
196/196 - 32s - loss: 27.6130 - MinusLogProbMetric: 27.6130 - val_loss: 27.8681 - val_MinusLogProbMetric: 27.8681 - lr: 6.2500e-05 - 32s/epoch - 163ms/step
Epoch 847/1000
2023-09-29 05:34:54.952 
Epoch 847/1000 
	 loss: 27.6086, MinusLogProbMetric: 27.6086, val_loss: 27.8565, val_MinusLogProbMetric: 27.8565

Epoch 847: val_loss did not improve from 27.84905
196/196 - 31s - loss: 27.6086 - MinusLogProbMetric: 27.6086 - val_loss: 27.8565 - val_MinusLogProbMetric: 27.8565 - lr: 6.2500e-05 - 31s/epoch - 158ms/step
Epoch 848/1000
2023-09-29 05:35:26.538 
Epoch 848/1000 
	 loss: 27.6084, MinusLogProbMetric: 27.6084, val_loss: 27.8581, val_MinusLogProbMetric: 27.8581

Epoch 848: val_loss did not improve from 27.84905
196/196 - 32s - loss: 27.6084 - MinusLogProbMetric: 27.6084 - val_loss: 27.8581 - val_MinusLogProbMetric: 27.8581 - lr: 6.2500e-05 - 32s/epoch - 161ms/step
Epoch 849/1000
2023-09-29 05:35:55.833 
Epoch 849/1000 
	 loss: 27.5935, MinusLogProbMetric: 27.5935, val_loss: 27.8534, val_MinusLogProbMetric: 27.8534

Epoch 849: val_loss did not improve from 27.84905
196/196 - 29s - loss: 27.5935 - MinusLogProbMetric: 27.5935 - val_loss: 27.8534 - val_MinusLogProbMetric: 27.8534 - lr: 3.1250e-05 - 29s/epoch - 149ms/step
Epoch 850/1000
2023-09-29 05:36:27.216 
Epoch 850/1000 
	 loss: 27.5925, MinusLogProbMetric: 27.5925, val_loss: 27.8418, val_MinusLogProbMetric: 27.8418

Epoch 850: val_loss improved from 27.84905 to 27.84176, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_329/weights/best_weights.h5
196/196 - 32s - loss: 27.5925 - MinusLogProbMetric: 27.5925 - val_loss: 27.8418 - val_MinusLogProbMetric: 27.8418 - lr: 3.1250e-05 - 32s/epoch - 162ms/step
Epoch 851/1000
2023-09-29 05:36:59.914 
Epoch 851/1000 
	 loss: 27.5923, MinusLogProbMetric: 27.5923, val_loss: 27.8436, val_MinusLogProbMetric: 27.8436

Epoch 851: val_loss did not improve from 27.84176
196/196 - 32s - loss: 27.5923 - MinusLogProbMetric: 27.5923 - val_loss: 27.8436 - val_MinusLogProbMetric: 27.8436 - lr: 3.1250e-05 - 32s/epoch - 164ms/step
Epoch 852/1000
2023-09-29 05:37:28.791 
Epoch 852/1000 
	 loss: 27.5929, MinusLogProbMetric: 27.5929, val_loss: 27.8412, val_MinusLogProbMetric: 27.8412

Epoch 852: val_loss improved from 27.84176 to 27.84124, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_329/weights/best_weights.h5
196/196 - 29s - loss: 27.5929 - MinusLogProbMetric: 27.5929 - val_loss: 27.8412 - val_MinusLogProbMetric: 27.8412 - lr: 3.1250e-05 - 29s/epoch - 150ms/step
Epoch 853/1000
2023-09-29 05:37:59.614 
Epoch 853/1000 
	 loss: 27.5920, MinusLogProbMetric: 27.5920, val_loss: 27.8433, val_MinusLogProbMetric: 27.8433

Epoch 853: val_loss did not improve from 27.84124
196/196 - 30s - loss: 27.5920 - MinusLogProbMetric: 27.5920 - val_loss: 27.8433 - val_MinusLogProbMetric: 27.8433 - lr: 3.1250e-05 - 30s/epoch - 155ms/step
Epoch 854/1000
2023-09-29 05:38:31.822 
Epoch 854/1000 
	 loss: 27.5932, MinusLogProbMetric: 27.5932, val_loss: 27.8437, val_MinusLogProbMetric: 27.8437

Epoch 854: val_loss did not improve from 27.84124
196/196 - 32s - loss: 27.5932 - MinusLogProbMetric: 27.5932 - val_loss: 27.8437 - val_MinusLogProbMetric: 27.8437 - lr: 3.1250e-05 - 32s/epoch - 164ms/step
Epoch 855/1000
2023-09-29 05:39:02.022 
Epoch 855/1000 
	 loss: 27.5913, MinusLogProbMetric: 27.5913, val_loss: 27.8462, val_MinusLogProbMetric: 27.8462

Epoch 855: val_loss did not improve from 27.84124
196/196 - 30s - loss: 27.5913 - MinusLogProbMetric: 27.5913 - val_loss: 27.8462 - val_MinusLogProbMetric: 27.8462 - lr: 3.1250e-05 - 30s/epoch - 154ms/step
Epoch 856/1000
2023-09-29 05:39:35.188 
Epoch 856/1000 
	 loss: 27.5927, MinusLogProbMetric: 27.5927, val_loss: 27.8450, val_MinusLogProbMetric: 27.8450

Epoch 856: val_loss did not improve from 27.84124
196/196 - 33s - loss: 27.5927 - MinusLogProbMetric: 27.5927 - val_loss: 27.8450 - val_MinusLogProbMetric: 27.8450 - lr: 3.1250e-05 - 33s/epoch - 169ms/step
Epoch 857/1000
2023-09-29 05:40:06.225 
Epoch 857/1000 
	 loss: 27.5920, MinusLogProbMetric: 27.5920, val_loss: 27.8523, val_MinusLogProbMetric: 27.8523

Epoch 857: val_loss did not improve from 27.84124
196/196 - 31s - loss: 27.5920 - MinusLogProbMetric: 27.5920 - val_loss: 27.8523 - val_MinusLogProbMetric: 27.8523 - lr: 3.1250e-05 - 31s/epoch - 158ms/step
Epoch 858/1000
2023-09-29 05:40:38.885 
Epoch 858/1000 
	 loss: 27.5933, MinusLogProbMetric: 27.5933, val_loss: 27.8448, val_MinusLogProbMetric: 27.8448

Epoch 858: val_loss did not improve from 27.84124
196/196 - 33s - loss: 27.5933 - MinusLogProbMetric: 27.5933 - val_loss: 27.8448 - val_MinusLogProbMetric: 27.8448 - lr: 3.1250e-05 - 33s/epoch - 167ms/step
Epoch 859/1000
2023-09-29 05:41:11.952 
Epoch 859/1000 
	 loss: 27.5925, MinusLogProbMetric: 27.5925, val_loss: 27.8374, val_MinusLogProbMetric: 27.8374

Epoch 859: val_loss improved from 27.84124 to 27.83743, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_329/weights/best_weights.h5
196/196 - 34s - loss: 27.5925 - MinusLogProbMetric: 27.5925 - val_loss: 27.8374 - val_MinusLogProbMetric: 27.8374 - lr: 3.1250e-05 - 34s/epoch - 172ms/step
Epoch 860/1000
2023-09-29 05:41:43.934 
Epoch 860/1000 
	 loss: 27.5921, MinusLogProbMetric: 27.5921, val_loss: 27.8452, val_MinusLogProbMetric: 27.8452

Epoch 860: val_loss did not improve from 27.83743
196/196 - 31s - loss: 27.5921 - MinusLogProbMetric: 27.5921 - val_loss: 27.8452 - val_MinusLogProbMetric: 27.8452 - lr: 3.1250e-05 - 31s/epoch - 160ms/step
Epoch 861/1000
2023-09-29 05:42:16.731 
Epoch 861/1000 
	 loss: 27.5930, MinusLogProbMetric: 27.5930, val_loss: 27.8460, val_MinusLogProbMetric: 27.8460

Epoch 861: val_loss did not improve from 27.83743
196/196 - 33s - loss: 27.5930 - MinusLogProbMetric: 27.5930 - val_loss: 27.8460 - val_MinusLogProbMetric: 27.8460 - lr: 3.1250e-05 - 33s/epoch - 167ms/step
Epoch 862/1000
2023-09-29 05:42:47.071 
Epoch 862/1000 
	 loss: 27.5930, MinusLogProbMetric: 27.5930, val_loss: 27.8450, val_MinusLogProbMetric: 27.8450

Epoch 862: val_loss did not improve from 27.83743
196/196 - 30s - loss: 27.5930 - MinusLogProbMetric: 27.5930 - val_loss: 27.8450 - val_MinusLogProbMetric: 27.8450 - lr: 3.1250e-05 - 30s/epoch - 155ms/step
Epoch 863/1000
2023-09-29 05:43:17.948 
Epoch 863/1000 
	 loss: 27.5911, MinusLogProbMetric: 27.5911, val_loss: 27.8452, val_MinusLogProbMetric: 27.8452

Epoch 863: val_loss did not improve from 27.83743
196/196 - 31s - loss: 27.5911 - MinusLogProbMetric: 27.5911 - val_loss: 27.8452 - val_MinusLogProbMetric: 27.8452 - lr: 3.1250e-05 - 31s/epoch - 158ms/step
Epoch 864/1000
2023-09-29 05:43:50.249 
Epoch 864/1000 
	 loss: 27.5920, MinusLogProbMetric: 27.5920, val_loss: 27.8461, val_MinusLogProbMetric: 27.8461

Epoch 864: val_loss did not improve from 27.83743
196/196 - 32s - loss: 27.5920 - MinusLogProbMetric: 27.5920 - val_loss: 27.8461 - val_MinusLogProbMetric: 27.8461 - lr: 3.1250e-05 - 32s/epoch - 165ms/step
Epoch 865/1000
2023-09-29 05:44:22.904 
Epoch 865/1000 
	 loss: 27.5928, MinusLogProbMetric: 27.5928, val_loss: 27.8391, val_MinusLogProbMetric: 27.8391

Epoch 865: val_loss did not improve from 27.83743
196/196 - 33s - loss: 27.5928 - MinusLogProbMetric: 27.5928 - val_loss: 27.8391 - val_MinusLogProbMetric: 27.8391 - lr: 3.1250e-05 - 33s/epoch - 167ms/step
Epoch 866/1000
2023-09-29 05:44:54.573 
Epoch 866/1000 
	 loss: 27.5918, MinusLogProbMetric: 27.5918, val_loss: 27.8412, val_MinusLogProbMetric: 27.8412

Epoch 866: val_loss did not improve from 27.83743
196/196 - 32s - loss: 27.5918 - MinusLogProbMetric: 27.5918 - val_loss: 27.8412 - val_MinusLogProbMetric: 27.8412 - lr: 3.1250e-05 - 32s/epoch - 162ms/step
Epoch 867/1000
2023-09-29 05:45:26.016 
Epoch 867/1000 
	 loss: 27.5910, MinusLogProbMetric: 27.5910, val_loss: 27.8433, val_MinusLogProbMetric: 27.8433

Epoch 867: val_loss did not improve from 27.83743
196/196 - 31s - loss: 27.5910 - MinusLogProbMetric: 27.5910 - val_loss: 27.8433 - val_MinusLogProbMetric: 27.8433 - lr: 3.1250e-05 - 31s/epoch - 160ms/step
Epoch 868/1000
2023-09-29 05:45:59.040 
Epoch 868/1000 
	 loss: 27.5920, MinusLogProbMetric: 27.5920, val_loss: 27.8393, val_MinusLogProbMetric: 27.8393

Epoch 868: val_loss did not improve from 27.83743
196/196 - 33s - loss: 27.5920 - MinusLogProbMetric: 27.5920 - val_loss: 27.8393 - val_MinusLogProbMetric: 27.8393 - lr: 3.1250e-05 - 33s/epoch - 168ms/step
Epoch 869/1000
2023-09-29 05:46:31.106 
Epoch 869/1000 
	 loss: 27.5907, MinusLogProbMetric: 27.5907, val_loss: 27.8520, val_MinusLogProbMetric: 27.8520

Epoch 869: val_loss did not improve from 27.83743
196/196 - 32s - loss: 27.5907 - MinusLogProbMetric: 27.5907 - val_loss: 27.8520 - val_MinusLogProbMetric: 27.8520 - lr: 3.1250e-05 - 32s/epoch - 164ms/step
Epoch 870/1000
2023-09-29 05:47:04.236 
Epoch 870/1000 
	 loss: 27.5930, MinusLogProbMetric: 27.5930, val_loss: 27.8479, val_MinusLogProbMetric: 27.8479

Epoch 870: val_loss did not improve from 27.83743
196/196 - 33s - loss: 27.5930 - MinusLogProbMetric: 27.5930 - val_loss: 27.8479 - val_MinusLogProbMetric: 27.8479 - lr: 3.1250e-05 - 33s/epoch - 169ms/step
Epoch 871/1000
2023-09-29 05:47:35.689 
Epoch 871/1000 
	 loss: 27.5921, MinusLogProbMetric: 27.5921, val_loss: 27.8453, val_MinusLogProbMetric: 27.8453

Epoch 871: val_loss did not improve from 27.83743
196/196 - 31s - loss: 27.5921 - MinusLogProbMetric: 27.5921 - val_loss: 27.8453 - val_MinusLogProbMetric: 27.8453 - lr: 3.1250e-05 - 31s/epoch - 160ms/step
Epoch 872/1000
2023-09-29 05:48:07.583 
Epoch 872/1000 
	 loss: 27.5921, MinusLogProbMetric: 27.5921, val_loss: 27.8471, val_MinusLogProbMetric: 27.8471

Epoch 872: val_loss did not improve from 27.83743
196/196 - 32s - loss: 27.5921 - MinusLogProbMetric: 27.5921 - val_loss: 27.8471 - val_MinusLogProbMetric: 27.8471 - lr: 3.1250e-05 - 32s/epoch - 163ms/step
Epoch 873/1000
2023-09-29 05:48:38.805 
Epoch 873/1000 
	 loss: 27.5910, MinusLogProbMetric: 27.5910, val_loss: 27.8405, val_MinusLogProbMetric: 27.8405

Epoch 873: val_loss did not improve from 27.83743
196/196 - 31s - loss: 27.5910 - MinusLogProbMetric: 27.5910 - val_loss: 27.8405 - val_MinusLogProbMetric: 27.8405 - lr: 3.1250e-05 - 31s/epoch - 159ms/step
Epoch 874/1000
2023-09-29 05:49:08.280 
Epoch 874/1000 
	 loss: 27.5908, MinusLogProbMetric: 27.5908, val_loss: 27.8436, val_MinusLogProbMetric: 27.8436

Epoch 874: val_loss did not improve from 27.83743
196/196 - 29s - loss: 27.5908 - MinusLogProbMetric: 27.5908 - val_loss: 27.8436 - val_MinusLogProbMetric: 27.8436 - lr: 3.1250e-05 - 29s/epoch - 150ms/step
Epoch 875/1000
2023-09-29 05:49:39.841 
Epoch 875/1000 
	 loss: 27.5908, MinusLogProbMetric: 27.5908, val_loss: 27.8467, val_MinusLogProbMetric: 27.8467

Epoch 875: val_loss did not improve from 27.83743
196/196 - 32s - loss: 27.5908 - MinusLogProbMetric: 27.5908 - val_loss: 27.8467 - val_MinusLogProbMetric: 27.8467 - lr: 3.1250e-05 - 32s/epoch - 161ms/step
Epoch 876/1000
2023-09-29 05:50:11.334 
Epoch 876/1000 
	 loss: 27.5932, MinusLogProbMetric: 27.5932, val_loss: 27.8401, val_MinusLogProbMetric: 27.8401

Epoch 876: val_loss did not improve from 27.83743
196/196 - 31s - loss: 27.5932 - MinusLogProbMetric: 27.5932 - val_loss: 27.8401 - val_MinusLogProbMetric: 27.8401 - lr: 3.1250e-05 - 31s/epoch - 161ms/step
Epoch 877/1000
2023-09-29 05:50:42.973 
Epoch 877/1000 
	 loss: 27.5902, MinusLogProbMetric: 27.5902, val_loss: 27.8418, val_MinusLogProbMetric: 27.8418

Epoch 877: val_loss did not improve from 27.83743
196/196 - 32s - loss: 27.5902 - MinusLogProbMetric: 27.5902 - val_loss: 27.8418 - val_MinusLogProbMetric: 27.8418 - lr: 3.1250e-05 - 32s/epoch - 161ms/step
Epoch 878/1000
2023-09-29 05:51:14.526 
Epoch 878/1000 
	 loss: 27.5917, MinusLogProbMetric: 27.5917, val_loss: 27.8502, val_MinusLogProbMetric: 27.8502

Epoch 878: val_loss did not improve from 27.83743
196/196 - 32s - loss: 27.5917 - MinusLogProbMetric: 27.5917 - val_loss: 27.8502 - val_MinusLogProbMetric: 27.8502 - lr: 3.1250e-05 - 32s/epoch - 161ms/step
Epoch 879/1000
2023-09-29 05:51:47.033 
Epoch 879/1000 
	 loss: 27.5907, MinusLogProbMetric: 27.5907, val_loss: 27.8416, val_MinusLogProbMetric: 27.8416

Epoch 879: val_loss did not improve from 27.83743
196/196 - 33s - loss: 27.5907 - MinusLogProbMetric: 27.5907 - val_loss: 27.8416 - val_MinusLogProbMetric: 27.8416 - lr: 3.1250e-05 - 33s/epoch - 166ms/step
Epoch 880/1000
2023-09-29 05:52:17.756 
Epoch 880/1000 
	 loss: 27.5909, MinusLogProbMetric: 27.5909, val_loss: 27.8473, val_MinusLogProbMetric: 27.8473

Epoch 880: val_loss did not improve from 27.83743
196/196 - 31s - loss: 27.5909 - MinusLogProbMetric: 27.5909 - val_loss: 27.8473 - val_MinusLogProbMetric: 27.8473 - lr: 3.1250e-05 - 31s/epoch - 157ms/step
Epoch 881/1000
2023-09-29 05:52:48.502 
Epoch 881/1000 
	 loss: 27.5924, MinusLogProbMetric: 27.5924, val_loss: 27.8415, val_MinusLogProbMetric: 27.8415

Epoch 881: val_loss did not improve from 27.83743
196/196 - 31s - loss: 27.5924 - MinusLogProbMetric: 27.5924 - val_loss: 27.8415 - val_MinusLogProbMetric: 27.8415 - lr: 3.1250e-05 - 31s/epoch - 157ms/step
Epoch 882/1000
2023-09-29 05:53:23.039 
Epoch 882/1000 
	 loss: 27.5904, MinusLogProbMetric: 27.5904, val_loss: 27.8412, val_MinusLogProbMetric: 27.8412

Epoch 882: val_loss did not improve from 27.83743
196/196 - 35s - loss: 27.5904 - MinusLogProbMetric: 27.5904 - val_loss: 27.8412 - val_MinusLogProbMetric: 27.8412 - lr: 3.1250e-05 - 35s/epoch - 176ms/step
Epoch 883/1000
2023-09-29 05:53:53.330 
Epoch 883/1000 
	 loss: 27.5911, MinusLogProbMetric: 27.5911, val_loss: 27.8406, val_MinusLogProbMetric: 27.8406

Epoch 883: val_loss did not improve from 27.83743
196/196 - 30s - loss: 27.5911 - MinusLogProbMetric: 27.5911 - val_loss: 27.8406 - val_MinusLogProbMetric: 27.8406 - lr: 3.1250e-05 - 30s/epoch - 155ms/step
Epoch 884/1000
2023-09-29 05:54:23.749 
Epoch 884/1000 
	 loss: 27.5911, MinusLogProbMetric: 27.5911, val_loss: 27.8424, val_MinusLogProbMetric: 27.8424

Epoch 884: val_loss did not improve from 27.83743
196/196 - 30s - loss: 27.5911 - MinusLogProbMetric: 27.5911 - val_loss: 27.8424 - val_MinusLogProbMetric: 27.8424 - lr: 3.1250e-05 - 30s/epoch - 155ms/step
Epoch 885/1000
2023-09-29 05:54:57.049 
Epoch 885/1000 
	 loss: 27.5920, MinusLogProbMetric: 27.5920, val_loss: 27.8386, val_MinusLogProbMetric: 27.8386

Epoch 885: val_loss did not improve from 27.83743
196/196 - 33s - loss: 27.5920 - MinusLogProbMetric: 27.5920 - val_loss: 27.8386 - val_MinusLogProbMetric: 27.8386 - lr: 3.1250e-05 - 33s/epoch - 170ms/step
Epoch 886/1000
2023-09-29 05:55:26.112 
Epoch 886/1000 
	 loss: 27.5911, MinusLogProbMetric: 27.5911, val_loss: 27.8484, val_MinusLogProbMetric: 27.8484

Epoch 886: val_loss did not improve from 27.83743
196/196 - 29s - loss: 27.5911 - MinusLogProbMetric: 27.5911 - val_loss: 27.8484 - val_MinusLogProbMetric: 27.8484 - lr: 3.1250e-05 - 29s/epoch - 148ms/step
Epoch 887/1000
2023-09-29 05:55:56.968 
Epoch 887/1000 
	 loss: 27.5924, MinusLogProbMetric: 27.5924, val_loss: 27.8415, val_MinusLogProbMetric: 27.8415

Epoch 887: val_loss did not improve from 27.83743
196/196 - 31s - loss: 27.5924 - MinusLogProbMetric: 27.5924 - val_loss: 27.8415 - val_MinusLogProbMetric: 27.8415 - lr: 3.1250e-05 - 31s/epoch - 157ms/step
Epoch 888/1000
2023-09-29 05:56:27.903 
Epoch 888/1000 
	 loss: 27.5912, MinusLogProbMetric: 27.5912, val_loss: 27.8421, val_MinusLogProbMetric: 27.8421

Epoch 888: val_loss did not improve from 27.83743
196/196 - 31s - loss: 27.5912 - MinusLogProbMetric: 27.5912 - val_loss: 27.8421 - val_MinusLogProbMetric: 27.8421 - lr: 3.1250e-05 - 31s/epoch - 158ms/step
Epoch 889/1000
2023-09-29 05:56:58.070 
Epoch 889/1000 
	 loss: 27.5914, MinusLogProbMetric: 27.5914, val_loss: 27.8428, val_MinusLogProbMetric: 27.8428

Epoch 889: val_loss did not improve from 27.83743
196/196 - 30s - loss: 27.5914 - MinusLogProbMetric: 27.5914 - val_loss: 27.8428 - val_MinusLogProbMetric: 27.8428 - lr: 3.1250e-05 - 30s/epoch - 154ms/step
Epoch 890/1000
2023-09-29 05:57:28.582 
Epoch 890/1000 
	 loss: 27.5926, MinusLogProbMetric: 27.5926, val_loss: 27.8540, val_MinusLogProbMetric: 27.8540

Epoch 890: val_loss did not improve from 27.83743
196/196 - 31s - loss: 27.5926 - MinusLogProbMetric: 27.5926 - val_loss: 27.8540 - val_MinusLogProbMetric: 27.8540 - lr: 3.1250e-05 - 31s/epoch - 156ms/step
Epoch 891/1000
2023-09-29 05:57:59.410 
Epoch 891/1000 
	 loss: 27.5922, MinusLogProbMetric: 27.5922, val_loss: 27.8399, val_MinusLogProbMetric: 27.8399

Epoch 891: val_loss did not improve from 27.83743
196/196 - 31s - loss: 27.5922 - MinusLogProbMetric: 27.5922 - val_loss: 27.8399 - val_MinusLogProbMetric: 27.8399 - lr: 3.1250e-05 - 31s/epoch - 157ms/step
Epoch 892/1000
2023-09-29 05:58:32.898 
Epoch 892/1000 
	 loss: 27.5925, MinusLogProbMetric: 27.5925, val_loss: 27.8422, val_MinusLogProbMetric: 27.8422

Epoch 892: val_loss did not improve from 27.83743
196/196 - 33s - loss: 27.5925 - MinusLogProbMetric: 27.5925 - val_loss: 27.8422 - val_MinusLogProbMetric: 27.8422 - lr: 3.1250e-05 - 33s/epoch - 171ms/step
Epoch 893/1000
2023-09-29 05:59:05.871 
Epoch 893/1000 
	 loss: 27.5915, MinusLogProbMetric: 27.5915, val_loss: 27.8427, val_MinusLogProbMetric: 27.8427

Epoch 893: val_loss did not improve from 27.83743
196/196 - 33s - loss: 27.5915 - MinusLogProbMetric: 27.5915 - val_loss: 27.8427 - val_MinusLogProbMetric: 27.8427 - lr: 3.1250e-05 - 33s/epoch - 168ms/step
Epoch 894/1000
2023-09-29 05:59:37.662 
Epoch 894/1000 
	 loss: 27.5919, MinusLogProbMetric: 27.5919, val_loss: 27.8481, val_MinusLogProbMetric: 27.8481

Epoch 894: val_loss did not improve from 27.83743
196/196 - 32s - loss: 27.5919 - MinusLogProbMetric: 27.5919 - val_loss: 27.8481 - val_MinusLogProbMetric: 27.8481 - lr: 3.1250e-05 - 32s/epoch - 162ms/step
Epoch 895/1000
2023-09-29 06:00:10.667 
Epoch 895/1000 
	 loss: 27.5913, MinusLogProbMetric: 27.5913, val_loss: 27.8465, val_MinusLogProbMetric: 27.8465

Epoch 895: val_loss did not improve from 27.83743
196/196 - 33s - loss: 27.5913 - MinusLogProbMetric: 27.5913 - val_loss: 27.8465 - val_MinusLogProbMetric: 27.8465 - lr: 3.1250e-05 - 33s/epoch - 168ms/step
Epoch 896/1000
2023-09-29 06:00:43.508 
Epoch 896/1000 
	 loss: 27.5916, MinusLogProbMetric: 27.5916, val_loss: 27.8411, val_MinusLogProbMetric: 27.8411

Epoch 896: val_loss did not improve from 27.83743
196/196 - 33s - loss: 27.5916 - MinusLogProbMetric: 27.5916 - val_loss: 27.8411 - val_MinusLogProbMetric: 27.8411 - lr: 3.1250e-05 - 33s/epoch - 168ms/step
Epoch 897/1000
2023-09-29 06:01:15.954 
Epoch 897/1000 
	 loss: 27.5912, MinusLogProbMetric: 27.5912, val_loss: 27.8521, val_MinusLogProbMetric: 27.8521

Epoch 897: val_loss did not improve from 27.83743
196/196 - 32s - loss: 27.5912 - MinusLogProbMetric: 27.5912 - val_loss: 27.8521 - val_MinusLogProbMetric: 27.8521 - lr: 3.1250e-05 - 32s/epoch - 166ms/step
Epoch 898/1000
2023-09-29 06:01:47.718 
Epoch 898/1000 
	 loss: 27.5930, MinusLogProbMetric: 27.5930, val_loss: 27.8411, val_MinusLogProbMetric: 27.8411

Epoch 898: val_loss did not improve from 27.83743
196/196 - 32s - loss: 27.5930 - MinusLogProbMetric: 27.5930 - val_loss: 27.8411 - val_MinusLogProbMetric: 27.8411 - lr: 3.1250e-05 - 32s/epoch - 162ms/step
Epoch 899/1000
2023-09-29 06:02:17.105 
Epoch 899/1000 
	 loss: 27.5902, MinusLogProbMetric: 27.5902, val_loss: 27.8440, val_MinusLogProbMetric: 27.8440

Epoch 899: val_loss did not improve from 27.83743
196/196 - 29s - loss: 27.5902 - MinusLogProbMetric: 27.5902 - val_loss: 27.8440 - val_MinusLogProbMetric: 27.8440 - lr: 3.1250e-05 - 29s/epoch - 150ms/step
Epoch 900/1000
2023-09-29 06:02:48.260 
Epoch 900/1000 
	 loss: 27.5903, MinusLogProbMetric: 27.5903, val_loss: 27.8459, val_MinusLogProbMetric: 27.8459

Epoch 900: val_loss did not improve from 27.83743
196/196 - 31s - loss: 27.5903 - MinusLogProbMetric: 27.5903 - val_loss: 27.8459 - val_MinusLogProbMetric: 27.8459 - lr: 3.1250e-05 - 31s/epoch - 159ms/step
Epoch 901/1000
2023-09-29 06:03:21.223 
Epoch 901/1000 
	 loss: 27.5908, MinusLogProbMetric: 27.5908, val_loss: 27.8447, val_MinusLogProbMetric: 27.8447

Epoch 901: val_loss did not improve from 27.83743
196/196 - 33s - loss: 27.5908 - MinusLogProbMetric: 27.5908 - val_loss: 27.8447 - val_MinusLogProbMetric: 27.8447 - lr: 3.1250e-05 - 33s/epoch - 168ms/step
Epoch 902/1000
2023-09-29 06:03:51.711 
Epoch 902/1000 
	 loss: 27.5914, MinusLogProbMetric: 27.5914, val_loss: 27.8427, val_MinusLogProbMetric: 27.8427

Epoch 902: val_loss did not improve from 27.83743
196/196 - 30s - loss: 27.5914 - MinusLogProbMetric: 27.5914 - val_loss: 27.8427 - val_MinusLogProbMetric: 27.8427 - lr: 3.1250e-05 - 30s/epoch - 156ms/step
Epoch 903/1000
2023-09-29 06:04:23.337 
Epoch 903/1000 
	 loss: 27.5912, MinusLogProbMetric: 27.5912, val_loss: 27.8417, val_MinusLogProbMetric: 27.8417

Epoch 903: val_loss did not improve from 27.83743
196/196 - 32s - loss: 27.5912 - MinusLogProbMetric: 27.5912 - val_loss: 27.8417 - val_MinusLogProbMetric: 27.8417 - lr: 3.1250e-05 - 32s/epoch - 161ms/step
Epoch 904/1000
2023-09-29 06:04:51.997 
Epoch 904/1000 
	 loss: 27.5903, MinusLogProbMetric: 27.5903, val_loss: 27.8538, val_MinusLogProbMetric: 27.8538

Epoch 904: val_loss did not improve from 27.83743
196/196 - 29s - loss: 27.5903 - MinusLogProbMetric: 27.5903 - val_loss: 27.8538 - val_MinusLogProbMetric: 27.8538 - lr: 3.1250e-05 - 29s/epoch - 146ms/step
Epoch 905/1000
2023-09-29 06:05:23.116 
Epoch 905/1000 
	 loss: 27.5935, MinusLogProbMetric: 27.5935, val_loss: 27.8472, val_MinusLogProbMetric: 27.8472

Epoch 905: val_loss did not improve from 27.83743
196/196 - 31s - loss: 27.5935 - MinusLogProbMetric: 27.5935 - val_loss: 27.8472 - val_MinusLogProbMetric: 27.8472 - lr: 3.1250e-05 - 31s/epoch - 159ms/step
Epoch 906/1000
2023-09-29 06:05:54.278 
Epoch 906/1000 
	 loss: 27.5917, MinusLogProbMetric: 27.5917, val_loss: 27.8403, val_MinusLogProbMetric: 27.8403

Epoch 906: val_loss did not improve from 27.83743
196/196 - 31s - loss: 27.5917 - MinusLogProbMetric: 27.5917 - val_loss: 27.8403 - val_MinusLogProbMetric: 27.8403 - lr: 3.1250e-05 - 31s/epoch - 159ms/step
Epoch 907/1000
2023-09-29 06:06:25.718 
Epoch 907/1000 
	 loss: 27.5922, MinusLogProbMetric: 27.5922, val_loss: 27.8472, val_MinusLogProbMetric: 27.8472

Epoch 907: val_loss did not improve from 27.83743
196/196 - 31s - loss: 27.5922 - MinusLogProbMetric: 27.5922 - val_loss: 27.8472 - val_MinusLogProbMetric: 27.8472 - lr: 3.1250e-05 - 31s/epoch - 160ms/step
Epoch 908/1000
2023-09-29 06:06:59.180 
Epoch 908/1000 
	 loss: 27.5898, MinusLogProbMetric: 27.5898, val_loss: 27.8408, val_MinusLogProbMetric: 27.8408

Epoch 908: val_loss did not improve from 27.83743
196/196 - 33s - loss: 27.5898 - MinusLogProbMetric: 27.5898 - val_loss: 27.8408 - val_MinusLogProbMetric: 27.8408 - lr: 3.1250e-05 - 33s/epoch - 171ms/step
Epoch 909/1000
2023-09-29 06:07:27.823 
Epoch 909/1000 
	 loss: 27.5898, MinusLogProbMetric: 27.5898, val_loss: 27.8485, val_MinusLogProbMetric: 27.8485

Epoch 909: val_loss did not improve from 27.83743
196/196 - 29s - loss: 27.5898 - MinusLogProbMetric: 27.5898 - val_loss: 27.8485 - val_MinusLogProbMetric: 27.8485 - lr: 3.1250e-05 - 29s/epoch - 146ms/step
Epoch 910/1000
2023-09-29 06:07:58.595 
Epoch 910/1000 
	 loss: 27.5846, MinusLogProbMetric: 27.5846, val_loss: 27.8364, val_MinusLogProbMetric: 27.8364

Epoch 910: val_loss improved from 27.83743 to 27.83640, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_329/weights/best_weights.h5
196/196 - 32s - loss: 27.5846 - MinusLogProbMetric: 27.5846 - val_loss: 27.8364 - val_MinusLogProbMetric: 27.8364 - lr: 1.5625e-05 - 32s/epoch - 161ms/step
Epoch 911/1000
2023-09-29 06:08:29.229 
Epoch 911/1000 
	 loss: 27.5829, MinusLogProbMetric: 27.5829, val_loss: 27.8370, val_MinusLogProbMetric: 27.8370

Epoch 911: val_loss did not improve from 27.83640
196/196 - 30s - loss: 27.5829 - MinusLogProbMetric: 27.5829 - val_loss: 27.8370 - val_MinusLogProbMetric: 27.8370 - lr: 1.5625e-05 - 30s/epoch - 152ms/step
Epoch 912/1000
2023-09-29 06:09:00.682 
Epoch 912/1000 
	 loss: 27.5844, MinusLogProbMetric: 27.5844, val_loss: 27.8378, val_MinusLogProbMetric: 27.8378

Epoch 912: val_loss did not improve from 27.83640
196/196 - 31s - loss: 27.5844 - MinusLogProbMetric: 27.5844 - val_loss: 27.8378 - val_MinusLogProbMetric: 27.8378 - lr: 1.5625e-05 - 31s/epoch - 160ms/step
Epoch 913/1000
2023-09-29 06:09:32.856 
Epoch 913/1000 
	 loss: 27.5830, MinusLogProbMetric: 27.5830, val_loss: 27.8393, val_MinusLogProbMetric: 27.8393

Epoch 913: val_loss did not improve from 27.83640
196/196 - 32s - loss: 27.5830 - MinusLogProbMetric: 27.5830 - val_loss: 27.8393 - val_MinusLogProbMetric: 27.8393 - lr: 1.5625e-05 - 32s/epoch - 164ms/step
Epoch 914/1000
2023-09-29 06:10:06.494 
Epoch 914/1000 
	 loss: 27.5825, MinusLogProbMetric: 27.5825, val_loss: 27.8343, val_MinusLogProbMetric: 27.8343

Epoch 914: val_loss improved from 27.83640 to 27.83430, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_329/weights/best_weights.h5
196/196 - 34s - loss: 27.5825 - MinusLogProbMetric: 27.5825 - val_loss: 27.8343 - val_MinusLogProbMetric: 27.8343 - lr: 1.5625e-05 - 34s/epoch - 175ms/step
Epoch 915/1000
2023-09-29 06:10:39.530 
Epoch 915/1000 
	 loss: 27.5823, MinusLogProbMetric: 27.5823, val_loss: 27.8383, val_MinusLogProbMetric: 27.8383

Epoch 915: val_loss did not improve from 27.83430
196/196 - 32s - loss: 27.5823 - MinusLogProbMetric: 27.5823 - val_loss: 27.8383 - val_MinusLogProbMetric: 27.8383 - lr: 1.5625e-05 - 32s/epoch - 166ms/step
Epoch 916/1000
2023-09-29 06:11:10.223 
Epoch 916/1000 
	 loss: 27.5824, MinusLogProbMetric: 27.5824, val_loss: 27.8398, val_MinusLogProbMetric: 27.8398

Epoch 916: val_loss did not improve from 27.83430
196/196 - 31s - loss: 27.5824 - MinusLogProbMetric: 27.5824 - val_loss: 27.8398 - val_MinusLogProbMetric: 27.8398 - lr: 1.5625e-05 - 31s/epoch - 157ms/step
Epoch 917/1000
2023-09-29 06:11:40.224 
Epoch 917/1000 
	 loss: 27.5822, MinusLogProbMetric: 27.5822, val_loss: 27.8381, val_MinusLogProbMetric: 27.8381

Epoch 917: val_loss did not improve from 27.83430
196/196 - 30s - loss: 27.5822 - MinusLogProbMetric: 27.5822 - val_loss: 27.8381 - val_MinusLogProbMetric: 27.8381 - lr: 1.5625e-05 - 30s/epoch - 153ms/step
Epoch 918/1000
2023-09-29 06:12:09.311 
Epoch 918/1000 
	 loss: 27.5826, MinusLogProbMetric: 27.5826, val_loss: 27.8388, val_MinusLogProbMetric: 27.8388

Epoch 918: val_loss did not improve from 27.83430
196/196 - 29s - loss: 27.5826 - MinusLogProbMetric: 27.5826 - val_loss: 27.8388 - val_MinusLogProbMetric: 27.8388 - lr: 1.5625e-05 - 29s/epoch - 148ms/step
Epoch 919/1000
2023-09-29 06:12:38.804 
Epoch 919/1000 
	 loss: 27.5824, MinusLogProbMetric: 27.5824, val_loss: 27.8359, val_MinusLogProbMetric: 27.8359

Epoch 919: val_loss did not improve from 27.83430
196/196 - 29s - loss: 27.5824 - MinusLogProbMetric: 27.5824 - val_loss: 27.8359 - val_MinusLogProbMetric: 27.8359 - lr: 1.5625e-05 - 29s/epoch - 150ms/step
Epoch 920/1000
2023-09-29 06:13:12.022 
Epoch 920/1000 
	 loss: 27.5833, MinusLogProbMetric: 27.5833, val_loss: 27.8346, val_MinusLogProbMetric: 27.8346

Epoch 920: val_loss did not improve from 27.83430
196/196 - 33s - loss: 27.5833 - MinusLogProbMetric: 27.5833 - val_loss: 27.8346 - val_MinusLogProbMetric: 27.8346 - lr: 1.5625e-05 - 33s/epoch - 169ms/step
Epoch 921/1000
2023-09-29 06:13:45.535 
Epoch 921/1000 
	 loss: 27.5817, MinusLogProbMetric: 27.5817, val_loss: 27.8355, val_MinusLogProbMetric: 27.8355

Epoch 921: val_loss did not improve from 27.83430
196/196 - 34s - loss: 27.5817 - MinusLogProbMetric: 27.5817 - val_loss: 27.8355 - val_MinusLogProbMetric: 27.8355 - lr: 1.5625e-05 - 34s/epoch - 171ms/step
Epoch 922/1000
2023-09-29 06:14:20.323 
Epoch 922/1000 
	 loss: 27.5833, MinusLogProbMetric: 27.5833, val_loss: 27.8352, val_MinusLogProbMetric: 27.8352

Epoch 922: val_loss did not improve from 27.83430
196/196 - 35s - loss: 27.5833 - MinusLogProbMetric: 27.5833 - val_loss: 27.8352 - val_MinusLogProbMetric: 27.8352 - lr: 1.5625e-05 - 35s/epoch - 177ms/step
Epoch 923/1000
2023-09-29 06:14:54.168 
Epoch 923/1000 
	 loss: 27.5827, MinusLogProbMetric: 27.5827, val_loss: 27.8343, val_MinusLogProbMetric: 27.8343

Epoch 923: val_loss improved from 27.83430 to 27.83426, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_329/weights/best_weights.h5
196/196 - 34s - loss: 27.5827 - MinusLogProbMetric: 27.5827 - val_loss: 27.8343 - val_MinusLogProbMetric: 27.8343 - lr: 1.5625e-05 - 34s/epoch - 176ms/step
Epoch 924/1000
2023-09-29 06:15:29.770 
Epoch 924/1000 
	 loss: 27.5834, MinusLogProbMetric: 27.5834, val_loss: 27.8331, val_MinusLogProbMetric: 27.8331

Epoch 924: val_loss improved from 27.83426 to 27.83305, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_329/weights/best_weights.h5
196/196 - 36s - loss: 27.5834 - MinusLogProbMetric: 27.5834 - val_loss: 27.8331 - val_MinusLogProbMetric: 27.8331 - lr: 1.5625e-05 - 36s/epoch - 182ms/step
Epoch 925/1000
2023-09-29 06:16:05.369 
Epoch 925/1000 
	 loss: 27.5824, MinusLogProbMetric: 27.5824, val_loss: 27.8379, val_MinusLogProbMetric: 27.8379

Epoch 925: val_loss did not improve from 27.83305
196/196 - 35s - loss: 27.5824 - MinusLogProbMetric: 27.5824 - val_loss: 27.8379 - val_MinusLogProbMetric: 27.8379 - lr: 1.5625e-05 - 35s/epoch - 179ms/step
Epoch 926/1000
2023-09-29 06:16:39.628 
Epoch 926/1000 
	 loss: 27.5819, MinusLogProbMetric: 27.5819, val_loss: 27.8345, val_MinusLogProbMetric: 27.8345

Epoch 926: val_loss did not improve from 27.83305
196/196 - 34s - loss: 27.5819 - MinusLogProbMetric: 27.5819 - val_loss: 27.8345 - val_MinusLogProbMetric: 27.8345 - lr: 1.5625e-05 - 34s/epoch - 175ms/step
Epoch 927/1000
2023-09-29 06:17:09.953 
Epoch 927/1000 
	 loss: 27.5821, MinusLogProbMetric: 27.5821, val_loss: 27.8363, val_MinusLogProbMetric: 27.8363

Epoch 927: val_loss did not improve from 27.83305
196/196 - 30s - loss: 27.5821 - MinusLogProbMetric: 27.5821 - val_loss: 27.8363 - val_MinusLogProbMetric: 27.8363 - lr: 1.5625e-05 - 30s/epoch - 155ms/step
Epoch 928/1000
2023-09-29 06:17:43.086 
Epoch 928/1000 
	 loss: 27.5821, MinusLogProbMetric: 27.5821, val_loss: 27.8379, val_MinusLogProbMetric: 27.8379

Epoch 928: val_loss did not improve from 27.83305
196/196 - 33s - loss: 27.5821 - MinusLogProbMetric: 27.5821 - val_loss: 27.8379 - val_MinusLogProbMetric: 27.8379 - lr: 1.5625e-05 - 33s/epoch - 169ms/step
Epoch 929/1000
2023-09-29 06:18:18.168 
Epoch 929/1000 
	 loss: 27.5833, MinusLogProbMetric: 27.5833, val_loss: 27.8357, val_MinusLogProbMetric: 27.8357

Epoch 929: val_loss did not improve from 27.83305
196/196 - 35s - loss: 27.5833 - MinusLogProbMetric: 27.5833 - val_loss: 27.8357 - val_MinusLogProbMetric: 27.8357 - lr: 1.5625e-05 - 35s/epoch - 179ms/step
Epoch 930/1000
2023-09-29 06:18:53.487 
Epoch 930/1000 
	 loss: 27.5821, MinusLogProbMetric: 27.5821, val_loss: 27.8366, val_MinusLogProbMetric: 27.8366

Epoch 930: val_loss did not improve from 27.83305
196/196 - 35s - loss: 27.5821 - MinusLogProbMetric: 27.5821 - val_loss: 27.8366 - val_MinusLogProbMetric: 27.8366 - lr: 1.5625e-05 - 35s/epoch - 180ms/step
Epoch 931/1000
2023-09-29 06:19:27.258 
Epoch 931/1000 
	 loss: 27.5828, MinusLogProbMetric: 27.5828, val_loss: 27.8356, val_MinusLogProbMetric: 27.8356

Epoch 931: val_loss did not improve from 27.83305
196/196 - 34s - loss: 27.5828 - MinusLogProbMetric: 27.5828 - val_loss: 27.8356 - val_MinusLogProbMetric: 27.8356 - lr: 1.5625e-05 - 34s/epoch - 172ms/step
Epoch 932/1000
2023-09-29 06:20:01.603 
Epoch 932/1000 
	 loss: 27.5828, MinusLogProbMetric: 27.5828, val_loss: 27.8340, val_MinusLogProbMetric: 27.8340

Epoch 932: val_loss did not improve from 27.83305
196/196 - 34s - loss: 27.5828 - MinusLogProbMetric: 27.5828 - val_loss: 27.8340 - val_MinusLogProbMetric: 27.8340 - lr: 1.5625e-05 - 34s/epoch - 175ms/step
Epoch 933/1000
2023-09-29 06:20:35.293 
Epoch 933/1000 
	 loss: 27.5836, MinusLogProbMetric: 27.5836, val_loss: 27.8353, val_MinusLogProbMetric: 27.8353

Epoch 933: val_loss did not improve from 27.83305
196/196 - 34s - loss: 27.5836 - MinusLogProbMetric: 27.5836 - val_loss: 27.8353 - val_MinusLogProbMetric: 27.8353 - lr: 1.5625e-05 - 34s/epoch - 172ms/step
Epoch 934/1000
2023-09-29 06:21:08.322 
Epoch 934/1000 
	 loss: 27.5820, MinusLogProbMetric: 27.5820, val_loss: 27.8351, val_MinusLogProbMetric: 27.8351

Epoch 934: val_loss did not improve from 27.83305
196/196 - 33s - loss: 27.5820 - MinusLogProbMetric: 27.5820 - val_loss: 27.8351 - val_MinusLogProbMetric: 27.8351 - lr: 1.5625e-05 - 33s/epoch - 168ms/step
Epoch 935/1000
2023-09-29 06:21:41.659 
Epoch 935/1000 
	 loss: 27.5825, MinusLogProbMetric: 27.5825, val_loss: 27.8341, val_MinusLogProbMetric: 27.8341

Epoch 935: val_loss did not improve from 27.83305
196/196 - 33s - loss: 27.5825 - MinusLogProbMetric: 27.5825 - val_loss: 27.8341 - val_MinusLogProbMetric: 27.8341 - lr: 1.5625e-05 - 33s/epoch - 170ms/step
Epoch 936/1000
2023-09-29 06:22:15.165 
Epoch 936/1000 
	 loss: 27.5825, MinusLogProbMetric: 27.5825, val_loss: 27.8327, val_MinusLogProbMetric: 27.8327

Epoch 936: val_loss improved from 27.83305 to 27.83268, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_329/weights/best_weights.h5
196/196 - 34s - loss: 27.5825 - MinusLogProbMetric: 27.5825 - val_loss: 27.8327 - val_MinusLogProbMetric: 27.8327 - lr: 1.5625e-05 - 34s/epoch - 174ms/step
Epoch 937/1000
2023-09-29 06:22:49.380 
Epoch 937/1000 
	 loss: 27.5818, MinusLogProbMetric: 27.5818, val_loss: 27.8339, val_MinusLogProbMetric: 27.8339

Epoch 937: val_loss did not improve from 27.83268
196/196 - 34s - loss: 27.5818 - MinusLogProbMetric: 27.5818 - val_loss: 27.8339 - val_MinusLogProbMetric: 27.8339 - lr: 1.5625e-05 - 34s/epoch - 172ms/step
Epoch 938/1000
2023-09-29 06:23:22.385 
Epoch 938/1000 
	 loss: 27.5828, MinusLogProbMetric: 27.5828, val_loss: 27.8344, val_MinusLogProbMetric: 27.8344

Epoch 938: val_loss did not improve from 27.83268
196/196 - 33s - loss: 27.5828 - MinusLogProbMetric: 27.5828 - val_loss: 27.8344 - val_MinusLogProbMetric: 27.8344 - lr: 1.5625e-05 - 33s/epoch - 168ms/step
Epoch 939/1000
2023-09-29 06:23:58.373 
Epoch 939/1000 
	 loss: 27.5821, MinusLogProbMetric: 27.5821, val_loss: 27.8453, val_MinusLogProbMetric: 27.8453

Epoch 939: val_loss did not improve from 27.83268
196/196 - 36s - loss: 27.5821 - MinusLogProbMetric: 27.5821 - val_loss: 27.8453 - val_MinusLogProbMetric: 27.8453 - lr: 1.5625e-05 - 36s/epoch - 184ms/step
Epoch 940/1000
2023-09-29 06:24:32.601 
Epoch 940/1000 
	 loss: 27.5835, MinusLogProbMetric: 27.5835, val_loss: 27.8337, val_MinusLogProbMetric: 27.8337

Epoch 940: val_loss did not improve from 27.83268
196/196 - 34s - loss: 27.5835 - MinusLogProbMetric: 27.5835 - val_loss: 27.8337 - val_MinusLogProbMetric: 27.8337 - lr: 1.5625e-05 - 34s/epoch - 175ms/step
Epoch 941/1000
2023-09-29 06:25:08.549 
Epoch 941/1000 
	 loss: 27.5814, MinusLogProbMetric: 27.5814, val_loss: 27.8322, val_MinusLogProbMetric: 27.8322

Epoch 941: val_loss improved from 27.83268 to 27.83215, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_329/weights/best_weights.h5
196/196 - 37s - loss: 27.5814 - MinusLogProbMetric: 27.5814 - val_loss: 27.8322 - val_MinusLogProbMetric: 27.8322 - lr: 1.5625e-05 - 37s/epoch - 187ms/step
Epoch 942/1000
2023-09-29 06:25:45.130 
Epoch 942/1000 
	 loss: 27.5825, MinusLogProbMetric: 27.5825, val_loss: 27.8386, val_MinusLogProbMetric: 27.8386

Epoch 942: val_loss did not improve from 27.83215
196/196 - 36s - loss: 27.5825 - MinusLogProbMetric: 27.5825 - val_loss: 27.8386 - val_MinusLogProbMetric: 27.8386 - lr: 1.5625e-05 - 36s/epoch - 183ms/step
Epoch 943/1000
2023-09-29 06:26:21.414 
Epoch 943/1000 
	 loss: 27.5829, MinusLogProbMetric: 27.5829, val_loss: 27.8339, val_MinusLogProbMetric: 27.8339

Epoch 943: val_loss did not improve from 27.83215
196/196 - 36s - loss: 27.5829 - MinusLogProbMetric: 27.5829 - val_loss: 27.8339 - val_MinusLogProbMetric: 27.8339 - lr: 1.5625e-05 - 36s/epoch - 185ms/step
Epoch 944/1000
2023-09-29 06:26:57.389 
Epoch 944/1000 
	 loss: 27.5821, MinusLogProbMetric: 27.5821, val_loss: 27.8363, val_MinusLogProbMetric: 27.8363

Epoch 944: val_loss did not improve from 27.83215
196/196 - 36s - loss: 27.5821 - MinusLogProbMetric: 27.5821 - val_loss: 27.8363 - val_MinusLogProbMetric: 27.8363 - lr: 1.5625e-05 - 36s/epoch - 184ms/step
Epoch 945/1000
2023-09-29 06:27:33.287 
Epoch 945/1000 
	 loss: 27.5828, MinusLogProbMetric: 27.5828, val_loss: 27.8363, val_MinusLogProbMetric: 27.8363

Epoch 945: val_loss did not improve from 27.83215
196/196 - 36s - loss: 27.5828 - MinusLogProbMetric: 27.5828 - val_loss: 27.8363 - val_MinusLogProbMetric: 27.8363 - lr: 1.5625e-05 - 36s/epoch - 183ms/step
Epoch 946/1000
2023-09-29 06:28:08.150 
Epoch 946/1000 
	 loss: 27.5812, MinusLogProbMetric: 27.5812, val_loss: 27.8378, val_MinusLogProbMetric: 27.8378

Epoch 946: val_loss did not improve from 27.83215
196/196 - 35s - loss: 27.5812 - MinusLogProbMetric: 27.5812 - val_loss: 27.8378 - val_MinusLogProbMetric: 27.8378 - lr: 1.5625e-05 - 35s/epoch - 178ms/step
Epoch 947/1000
2023-09-29 06:28:44.224 
Epoch 947/1000 
	 loss: 27.5814, MinusLogProbMetric: 27.5814, val_loss: 27.8335, val_MinusLogProbMetric: 27.8335

Epoch 947: val_loss did not improve from 27.83215
196/196 - 36s - loss: 27.5814 - MinusLogProbMetric: 27.5814 - val_loss: 27.8335 - val_MinusLogProbMetric: 27.8335 - lr: 1.5625e-05 - 36s/epoch - 184ms/step
Epoch 948/1000
2023-09-29 06:29:20.378 
Epoch 948/1000 
	 loss: 27.5822, MinusLogProbMetric: 27.5822, val_loss: 27.8355, val_MinusLogProbMetric: 27.8355

Epoch 948: val_loss did not improve from 27.83215
196/196 - 36s - loss: 27.5822 - MinusLogProbMetric: 27.5822 - val_loss: 27.8355 - val_MinusLogProbMetric: 27.8355 - lr: 1.5625e-05 - 36s/epoch - 184ms/step
Epoch 949/1000
2023-09-29 06:29:55.606 
Epoch 949/1000 
	 loss: 27.5822, MinusLogProbMetric: 27.5822, val_loss: 27.8353, val_MinusLogProbMetric: 27.8353

Epoch 949: val_loss did not improve from 27.83215
196/196 - 35s - loss: 27.5822 - MinusLogProbMetric: 27.5822 - val_loss: 27.8353 - val_MinusLogProbMetric: 27.8353 - lr: 1.5625e-05 - 35s/epoch - 180ms/step
Epoch 950/1000
2023-09-29 06:30:31.705 
Epoch 950/1000 
	 loss: 27.5823, MinusLogProbMetric: 27.5823, val_loss: 27.8333, val_MinusLogProbMetric: 27.8333

Epoch 950: val_loss did not improve from 27.83215
196/196 - 36s - loss: 27.5823 - MinusLogProbMetric: 27.5823 - val_loss: 27.8333 - val_MinusLogProbMetric: 27.8333 - lr: 1.5625e-05 - 36s/epoch - 184ms/step
Epoch 951/1000
2023-09-29 06:31:07.550 
Epoch 951/1000 
	 loss: 27.5822, MinusLogProbMetric: 27.5822, val_loss: 27.8352, val_MinusLogProbMetric: 27.8352

Epoch 951: val_loss did not improve from 27.83215
196/196 - 36s - loss: 27.5822 - MinusLogProbMetric: 27.5822 - val_loss: 27.8352 - val_MinusLogProbMetric: 27.8352 - lr: 1.5625e-05 - 36s/epoch - 183ms/step
Epoch 952/1000
2023-09-29 06:31:43.351 
Epoch 952/1000 
	 loss: 27.5817, MinusLogProbMetric: 27.5817, val_loss: 27.8366, val_MinusLogProbMetric: 27.8366

Epoch 952: val_loss did not improve from 27.83215
196/196 - 36s - loss: 27.5817 - MinusLogProbMetric: 27.5817 - val_loss: 27.8366 - val_MinusLogProbMetric: 27.8366 - lr: 1.5625e-05 - 36s/epoch - 183ms/step
Epoch 953/1000
2023-09-29 06:32:19.399 
Epoch 953/1000 
	 loss: 27.5826, MinusLogProbMetric: 27.5826, val_loss: 27.8340, val_MinusLogProbMetric: 27.8340

Epoch 953: val_loss did not improve from 27.83215
196/196 - 36s - loss: 27.5826 - MinusLogProbMetric: 27.5826 - val_loss: 27.8340 - val_MinusLogProbMetric: 27.8340 - lr: 1.5625e-05 - 36s/epoch - 184ms/step
Epoch 954/1000
2023-09-29 06:32:55.689 
Epoch 954/1000 
	 loss: 27.5827, MinusLogProbMetric: 27.5827, val_loss: 27.8329, val_MinusLogProbMetric: 27.8329

Epoch 954: val_loss did not improve from 27.83215
196/196 - 36s - loss: 27.5827 - MinusLogProbMetric: 27.5827 - val_loss: 27.8329 - val_MinusLogProbMetric: 27.8329 - lr: 1.5625e-05 - 36s/epoch - 185ms/step
Epoch 955/1000
2023-09-29 06:33:31.647 
Epoch 955/1000 
	 loss: 27.5819, MinusLogProbMetric: 27.5819, val_loss: 27.8338, val_MinusLogProbMetric: 27.8338

Epoch 955: val_loss did not improve from 27.83215
196/196 - 36s - loss: 27.5819 - MinusLogProbMetric: 27.5819 - val_loss: 27.8338 - val_MinusLogProbMetric: 27.8338 - lr: 1.5625e-05 - 36s/epoch - 183ms/step
Epoch 956/1000
2023-09-29 06:34:07.327 
Epoch 956/1000 
	 loss: 27.5823, MinusLogProbMetric: 27.5823, val_loss: 27.8340, val_MinusLogProbMetric: 27.8340

Epoch 956: val_loss did not improve from 27.83215
196/196 - 36s - loss: 27.5823 - MinusLogProbMetric: 27.5823 - val_loss: 27.8340 - val_MinusLogProbMetric: 27.8340 - lr: 1.5625e-05 - 36s/epoch - 182ms/step
Epoch 957/1000
2023-09-29 06:34:43.500 
Epoch 957/1000 
	 loss: 27.5821, MinusLogProbMetric: 27.5821, val_loss: 27.8337, val_MinusLogProbMetric: 27.8337

Epoch 957: val_loss did not improve from 27.83215
196/196 - 36s - loss: 27.5821 - MinusLogProbMetric: 27.5821 - val_loss: 27.8337 - val_MinusLogProbMetric: 27.8337 - lr: 1.5625e-05 - 36s/epoch - 185ms/step
Epoch 958/1000
2023-09-29 06:35:19.541 
Epoch 958/1000 
	 loss: 27.5825, MinusLogProbMetric: 27.5825, val_loss: 27.8342, val_MinusLogProbMetric: 27.8342

Epoch 958: val_loss did not improve from 27.83215
196/196 - 36s - loss: 27.5825 - MinusLogProbMetric: 27.5825 - val_loss: 27.8342 - val_MinusLogProbMetric: 27.8342 - lr: 1.5625e-05 - 36s/epoch - 184ms/step
Epoch 959/1000
2023-09-29 06:35:55.774 
Epoch 959/1000 
	 loss: 27.5813, MinusLogProbMetric: 27.5813, val_loss: 27.8347, val_MinusLogProbMetric: 27.8347

Epoch 959: val_loss did not improve from 27.83215
196/196 - 36s - loss: 27.5813 - MinusLogProbMetric: 27.5813 - val_loss: 27.8347 - val_MinusLogProbMetric: 27.8347 - lr: 1.5625e-05 - 36s/epoch - 185ms/step
Epoch 960/1000
2023-09-29 06:36:31.748 
Epoch 960/1000 
	 loss: 27.5821, MinusLogProbMetric: 27.5821, val_loss: 27.8350, val_MinusLogProbMetric: 27.8350

Epoch 960: val_loss did not improve from 27.83215
196/196 - 36s - loss: 27.5821 - MinusLogProbMetric: 27.5821 - val_loss: 27.8350 - val_MinusLogProbMetric: 27.8350 - lr: 1.5625e-05 - 36s/epoch - 184ms/step
Epoch 961/1000
2023-09-29 06:37:07.866 
Epoch 961/1000 
	 loss: 27.5824, MinusLogProbMetric: 27.5824, val_loss: 27.8339, val_MinusLogProbMetric: 27.8339

Epoch 961: val_loss did not improve from 27.83215
196/196 - 36s - loss: 27.5824 - MinusLogProbMetric: 27.5824 - val_loss: 27.8339 - val_MinusLogProbMetric: 27.8339 - lr: 1.5625e-05 - 36s/epoch - 184ms/step
Epoch 962/1000
2023-09-29 06:37:44.365 
Epoch 962/1000 
	 loss: 27.5819, MinusLogProbMetric: 27.5819, val_loss: 27.8339, val_MinusLogProbMetric: 27.8339

Epoch 962: val_loss did not improve from 27.83215
196/196 - 36s - loss: 27.5819 - MinusLogProbMetric: 27.5819 - val_loss: 27.8339 - val_MinusLogProbMetric: 27.8339 - lr: 1.5625e-05 - 36s/epoch - 186ms/step
Epoch 963/1000
2023-09-29 06:38:20.469 
Epoch 963/1000 
	 loss: 27.5818, MinusLogProbMetric: 27.5818, val_loss: 27.8343, val_MinusLogProbMetric: 27.8343

Epoch 963: val_loss did not improve from 27.83215
196/196 - 36s - loss: 27.5818 - MinusLogProbMetric: 27.5818 - val_loss: 27.8343 - val_MinusLogProbMetric: 27.8343 - lr: 1.5625e-05 - 36s/epoch - 184ms/step
Epoch 964/1000
2023-09-29 06:38:56.457 
Epoch 964/1000 
	 loss: 27.5819, MinusLogProbMetric: 27.5819, val_loss: 27.8326, val_MinusLogProbMetric: 27.8326

Epoch 964: val_loss did not improve from 27.83215
196/196 - 36s - loss: 27.5819 - MinusLogProbMetric: 27.5819 - val_loss: 27.8326 - val_MinusLogProbMetric: 27.8326 - lr: 1.5625e-05 - 36s/epoch - 184ms/step
Epoch 965/1000
2023-09-29 06:39:32.975 
Epoch 965/1000 
	 loss: 27.5824, MinusLogProbMetric: 27.5824, val_loss: 27.8347, val_MinusLogProbMetric: 27.8347

Epoch 965: val_loss did not improve from 27.83215
196/196 - 37s - loss: 27.5824 - MinusLogProbMetric: 27.5824 - val_loss: 27.8347 - val_MinusLogProbMetric: 27.8347 - lr: 1.5625e-05 - 37s/epoch - 186ms/step
Epoch 966/1000
2023-09-29 06:40:09.025 
Epoch 966/1000 
	 loss: 27.5818, MinusLogProbMetric: 27.5818, val_loss: 27.8341, val_MinusLogProbMetric: 27.8341

Epoch 966: val_loss did not improve from 27.83215
196/196 - 36s - loss: 27.5818 - MinusLogProbMetric: 27.5818 - val_loss: 27.8341 - val_MinusLogProbMetric: 27.8341 - lr: 1.5625e-05 - 36s/epoch - 184ms/step
Epoch 967/1000
2023-09-29 06:40:45.435 
Epoch 967/1000 
	 loss: 27.5816, MinusLogProbMetric: 27.5816, val_loss: 27.8389, val_MinusLogProbMetric: 27.8389

Epoch 967: val_loss did not improve from 27.83215
196/196 - 36s - loss: 27.5816 - MinusLogProbMetric: 27.5816 - val_loss: 27.8389 - val_MinusLogProbMetric: 27.8389 - lr: 1.5625e-05 - 36s/epoch - 186ms/step
Epoch 968/1000
2023-09-29 06:41:21.921 
Epoch 968/1000 
	 loss: 27.5826, MinusLogProbMetric: 27.5826, val_loss: 27.8380, val_MinusLogProbMetric: 27.8380

Epoch 968: val_loss did not improve from 27.83215
196/196 - 36s - loss: 27.5826 - MinusLogProbMetric: 27.5826 - val_loss: 27.8380 - val_MinusLogProbMetric: 27.8380 - lr: 1.5625e-05 - 36s/epoch - 186ms/step
Epoch 969/1000
2023-09-29 06:41:58.092 
Epoch 969/1000 
	 loss: 27.5820, MinusLogProbMetric: 27.5820, val_loss: 27.8334, val_MinusLogProbMetric: 27.8334

Epoch 969: val_loss did not improve from 27.83215
196/196 - 36s - loss: 27.5820 - MinusLogProbMetric: 27.5820 - val_loss: 27.8334 - val_MinusLogProbMetric: 27.8334 - lr: 1.5625e-05 - 36s/epoch - 185ms/step
Epoch 970/1000
2023-09-29 06:42:34.389 
Epoch 970/1000 
	 loss: 27.5823, MinusLogProbMetric: 27.5823, val_loss: 27.8378, val_MinusLogProbMetric: 27.8378

Epoch 970: val_loss did not improve from 27.83215
196/196 - 36s - loss: 27.5823 - MinusLogProbMetric: 27.5823 - val_loss: 27.8378 - val_MinusLogProbMetric: 27.8378 - lr: 1.5625e-05 - 36s/epoch - 185ms/step
Epoch 971/1000
2023-09-29 06:43:10.462 
Epoch 971/1000 
	 loss: 27.5832, MinusLogProbMetric: 27.5832, val_loss: 27.8340, val_MinusLogProbMetric: 27.8340

Epoch 971: val_loss did not improve from 27.83215
196/196 - 36s - loss: 27.5832 - MinusLogProbMetric: 27.5832 - val_loss: 27.8340 - val_MinusLogProbMetric: 27.8340 - lr: 1.5625e-05 - 36s/epoch - 184ms/step
Epoch 972/1000
2023-09-29 06:43:46.579 
Epoch 972/1000 
	 loss: 27.5818, MinusLogProbMetric: 27.5818, val_loss: 27.8422, val_MinusLogProbMetric: 27.8422

Epoch 972: val_loss did not improve from 27.83215
196/196 - 36s - loss: 27.5818 - MinusLogProbMetric: 27.5818 - val_loss: 27.8422 - val_MinusLogProbMetric: 27.8422 - lr: 1.5625e-05 - 36s/epoch - 184ms/step
Epoch 973/1000
2023-09-29 06:44:22.807 
Epoch 973/1000 
	 loss: 27.5829, MinusLogProbMetric: 27.5829, val_loss: 27.8425, val_MinusLogProbMetric: 27.8425

Epoch 973: val_loss did not improve from 27.83215
196/196 - 36s - loss: 27.5829 - MinusLogProbMetric: 27.5829 - val_loss: 27.8425 - val_MinusLogProbMetric: 27.8425 - lr: 1.5625e-05 - 36s/epoch - 185ms/step
Epoch 974/1000
2023-09-29 06:44:59.287 
Epoch 974/1000 
	 loss: 27.5823, MinusLogProbMetric: 27.5823, val_loss: 27.8320, val_MinusLogProbMetric: 27.8320

Epoch 974: val_loss improved from 27.83215 to 27.83203, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_329/weights/best_weights.h5
196/196 - 37s - loss: 27.5823 - MinusLogProbMetric: 27.5823 - val_loss: 27.8320 - val_MinusLogProbMetric: 27.8320 - lr: 1.5625e-05 - 37s/epoch - 190ms/step
Epoch 975/1000
2023-09-29 06:45:35.896 
Epoch 975/1000 
	 loss: 27.5828, MinusLogProbMetric: 27.5828, val_loss: 27.8338, val_MinusLogProbMetric: 27.8338

Epoch 975: val_loss did not improve from 27.83203
196/196 - 36s - loss: 27.5828 - MinusLogProbMetric: 27.5828 - val_loss: 27.8338 - val_MinusLogProbMetric: 27.8338 - lr: 1.5625e-05 - 36s/epoch - 183ms/step
Epoch 976/1000
2023-09-29 06:46:11.663 
Epoch 976/1000 
	 loss: 27.5822, MinusLogProbMetric: 27.5822, val_loss: 27.8356, val_MinusLogProbMetric: 27.8356

Epoch 976: val_loss did not improve from 27.83203
196/196 - 36s - loss: 27.5822 - MinusLogProbMetric: 27.5822 - val_loss: 27.8356 - val_MinusLogProbMetric: 27.8356 - lr: 1.5625e-05 - 36s/epoch - 182ms/step
Epoch 977/1000
2023-09-29 06:46:47.705 
Epoch 977/1000 
	 loss: 27.5825, MinusLogProbMetric: 27.5825, val_loss: 27.8372, val_MinusLogProbMetric: 27.8372

Epoch 977: val_loss did not improve from 27.83203
196/196 - 36s - loss: 27.5825 - MinusLogProbMetric: 27.5825 - val_loss: 27.8372 - val_MinusLogProbMetric: 27.8372 - lr: 1.5625e-05 - 36s/epoch - 184ms/step
Epoch 978/1000
2023-09-29 06:47:24.001 
Epoch 978/1000 
	 loss: 27.5816, MinusLogProbMetric: 27.5816, val_loss: 27.8358, val_MinusLogProbMetric: 27.8358

Epoch 978: val_loss did not improve from 27.83203
196/196 - 36s - loss: 27.5816 - MinusLogProbMetric: 27.5816 - val_loss: 27.8358 - val_MinusLogProbMetric: 27.8358 - lr: 1.5625e-05 - 36s/epoch - 185ms/step
Epoch 979/1000
2023-09-29 06:48:00.074 
Epoch 979/1000 
	 loss: 27.5819, MinusLogProbMetric: 27.5819, val_loss: 27.8334, val_MinusLogProbMetric: 27.8334

Epoch 979: val_loss did not improve from 27.83203
196/196 - 36s - loss: 27.5819 - MinusLogProbMetric: 27.5819 - val_loss: 27.8334 - val_MinusLogProbMetric: 27.8334 - lr: 1.5625e-05 - 36s/epoch - 184ms/step
Epoch 980/1000
2023-09-29 06:48:36.276 
Epoch 980/1000 
	 loss: 27.5823, MinusLogProbMetric: 27.5823, val_loss: 27.8340, val_MinusLogProbMetric: 27.8340

Epoch 980: val_loss did not improve from 27.83203
196/196 - 36s - loss: 27.5823 - MinusLogProbMetric: 27.5823 - val_loss: 27.8340 - val_MinusLogProbMetric: 27.8340 - lr: 1.5625e-05 - 36s/epoch - 185ms/step
Epoch 981/1000
2023-09-29 06:49:12.375 
Epoch 981/1000 
	 loss: 27.5824, MinusLogProbMetric: 27.5824, val_loss: 27.8378, val_MinusLogProbMetric: 27.8378

Epoch 981: val_loss did not improve from 27.83203
196/196 - 36s - loss: 27.5824 - MinusLogProbMetric: 27.5824 - val_loss: 27.8378 - val_MinusLogProbMetric: 27.8378 - lr: 1.5625e-05 - 36s/epoch - 184ms/step
Epoch 982/1000
2023-09-29 06:49:48.350 
Epoch 982/1000 
	 loss: 27.5819, MinusLogProbMetric: 27.5819, val_loss: 27.8351, val_MinusLogProbMetric: 27.8351

Epoch 982: val_loss did not improve from 27.83203
196/196 - 36s - loss: 27.5819 - MinusLogProbMetric: 27.5819 - val_loss: 27.8351 - val_MinusLogProbMetric: 27.8351 - lr: 1.5625e-05 - 36s/epoch - 184ms/step
Epoch 983/1000
2023-09-29 06:50:24.541 
Epoch 983/1000 
	 loss: 27.5820, MinusLogProbMetric: 27.5820, val_loss: 27.8353, val_MinusLogProbMetric: 27.8353

Epoch 983: val_loss did not improve from 27.83203
196/196 - 36s - loss: 27.5820 - MinusLogProbMetric: 27.5820 - val_loss: 27.8353 - val_MinusLogProbMetric: 27.8353 - lr: 1.5625e-05 - 36s/epoch - 185ms/step
Epoch 984/1000
2023-09-29 06:51:01.169 
Epoch 984/1000 
	 loss: 27.5815, MinusLogProbMetric: 27.5815, val_loss: 27.8340, val_MinusLogProbMetric: 27.8340

Epoch 984: val_loss did not improve from 27.83203
196/196 - 37s - loss: 27.5815 - MinusLogProbMetric: 27.5815 - val_loss: 27.8340 - val_MinusLogProbMetric: 27.8340 - lr: 1.5625e-05 - 37s/epoch - 187ms/step
Epoch 985/1000
2023-09-29 06:51:37.384 
Epoch 985/1000 
	 loss: 27.5825, MinusLogProbMetric: 27.5825, val_loss: 27.8359, val_MinusLogProbMetric: 27.8359

Epoch 985: val_loss did not improve from 27.83203
196/196 - 36s - loss: 27.5825 - MinusLogProbMetric: 27.5825 - val_loss: 27.8359 - val_MinusLogProbMetric: 27.8359 - lr: 1.5625e-05 - 36s/epoch - 185ms/step
Epoch 986/1000
2023-09-29 06:52:13.781 
Epoch 986/1000 
	 loss: 27.5827, MinusLogProbMetric: 27.5827, val_loss: 27.8335, val_MinusLogProbMetric: 27.8335

Epoch 986: val_loss did not improve from 27.83203
196/196 - 36s - loss: 27.5827 - MinusLogProbMetric: 27.5827 - val_loss: 27.8335 - val_MinusLogProbMetric: 27.8335 - lr: 1.5625e-05 - 36s/epoch - 186ms/step
Epoch 987/1000
2023-09-29 06:52:50.342 
Epoch 987/1000 
	 loss: 27.5815, MinusLogProbMetric: 27.5815, val_loss: 27.8334, val_MinusLogProbMetric: 27.8334

Epoch 987: val_loss did not improve from 27.83203
196/196 - 37s - loss: 27.5815 - MinusLogProbMetric: 27.5815 - val_loss: 27.8334 - val_MinusLogProbMetric: 27.8334 - lr: 1.5625e-05 - 37s/epoch - 187ms/step
Epoch 988/1000
2023-09-29 06:53:26.353 
Epoch 988/1000 
	 loss: 27.5820, MinusLogProbMetric: 27.5820, val_loss: 27.8355, val_MinusLogProbMetric: 27.8355

Epoch 988: val_loss did not improve from 27.83203
196/196 - 36s - loss: 27.5820 - MinusLogProbMetric: 27.5820 - val_loss: 27.8355 - val_MinusLogProbMetric: 27.8355 - lr: 1.5625e-05 - 36s/epoch - 184ms/step
Epoch 989/1000
2023-09-29 06:54:02.664 
Epoch 989/1000 
	 loss: 27.5831, MinusLogProbMetric: 27.5831, val_loss: 27.8414, val_MinusLogProbMetric: 27.8414

Epoch 989: val_loss did not improve from 27.83203
196/196 - 36s - loss: 27.5831 - MinusLogProbMetric: 27.5831 - val_loss: 27.8414 - val_MinusLogProbMetric: 27.8414 - lr: 1.5625e-05 - 36s/epoch - 185ms/step
Epoch 990/1000
2023-09-29 06:54:39.326 
Epoch 990/1000 
	 loss: 27.5820, MinusLogProbMetric: 27.5820, val_loss: 27.8332, val_MinusLogProbMetric: 27.8332

Epoch 990: val_loss did not improve from 27.83203
196/196 - 37s - loss: 27.5820 - MinusLogProbMetric: 27.5820 - val_loss: 27.8332 - val_MinusLogProbMetric: 27.8332 - lr: 1.5625e-05 - 37s/epoch - 187ms/step
Epoch 991/1000
2023-09-29 06:55:15.650 
Epoch 991/1000 
	 loss: 27.5820, MinusLogProbMetric: 27.5820, val_loss: 27.8356, val_MinusLogProbMetric: 27.8356

Epoch 991: val_loss did not improve from 27.83203
196/196 - 36s - loss: 27.5820 - MinusLogProbMetric: 27.5820 - val_loss: 27.8356 - val_MinusLogProbMetric: 27.8356 - lr: 1.5625e-05 - 36s/epoch - 185ms/step
Epoch 992/1000
2023-09-29 06:55:51.943 
Epoch 992/1000 
	 loss: 27.5816, MinusLogProbMetric: 27.5816, val_loss: 27.8347, val_MinusLogProbMetric: 27.8347

Epoch 992: val_loss did not improve from 27.83203
196/196 - 36s - loss: 27.5816 - MinusLogProbMetric: 27.5816 - val_loss: 27.8347 - val_MinusLogProbMetric: 27.8347 - lr: 1.5625e-05 - 36s/epoch - 185ms/step
Epoch 993/1000
2023-09-29 06:56:28.202 
Epoch 993/1000 
	 loss: 27.5815, MinusLogProbMetric: 27.5815, val_loss: 27.8340, val_MinusLogProbMetric: 27.8340

Epoch 993: val_loss did not improve from 27.83203
196/196 - 36s - loss: 27.5815 - MinusLogProbMetric: 27.5815 - val_loss: 27.8340 - val_MinusLogProbMetric: 27.8340 - lr: 1.5625e-05 - 36s/epoch - 185ms/step
Epoch 994/1000
2023-09-29 06:57:03.936 
Epoch 994/1000 
	 loss: 27.5819, MinusLogProbMetric: 27.5819, val_loss: 27.8393, val_MinusLogProbMetric: 27.8393

Epoch 994: val_loss did not improve from 27.83203
196/196 - 36s - loss: 27.5819 - MinusLogProbMetric: 27.5819 - val_loss: 27.8393 - val_MinusLogProbMetric: 27.8393 - lr: 1.5625e-05 - 36s/epoch - 182ms/step
Epoch 995/1000
2023-09-29 06:57:39.842 
Epoch 995/1000 
	 loss: 27.5817, MinusLogProbMetric: 27.5817, val_loss: 27.8348, val_MinusLogProbMetric: 27.8348

Epoch 995: val_loss did not improve from 27.83203
196/196 - 36s - loss: 27.5817 - MinusLogProbMetric: 27.5817 - val_loss: 27.8348 - val_MinusLogProbMetric: 27.8348 - lr: 1.5625e-05 - 36s/epoch - 183ms/step
Epoch 996/1000
2023-09-29 06:58:15.952 
Epoch 996/1000 
	 loss: 27.5816, MinusLogProbMetric: 27.5816, val_loss: 27.8371, val_MinusLogProbMetric: 27.8371

Epoch 996: val_loss did not improve from 27.83203
196/196 - 36s - loss: 27.5816 - MinusLogProbMetric: 27.5816 - val_loss: 27.8371 - val_MinusLogProbMetric: 27.8371 - lr: 1.5625e-05 - 36s/epoch - 184ms/step
Epoch 997/1000
2023-09-29 06:58:51.306 
Epoch 997/1000 
	 loss: 27.5822, MinusLogProbMetric: 27.5822, val_loss: 27.8358, val_MinusLogProbMetric: 27.8358

Epoch 997: val_loss did not improve from 27.83203
196/196 - 35s - loss: 27.5822 - MinusLogProbMetric: 27.5822 - val_loss: 27.8358 - val_MinusLogProbMetric: 27.8358 - lr: 1.5625e-05 - 35s/epoch - 180ms/step
Epoch 998/1000
2023-09-29 06:59:27.229 
Epoch 998/1000 
	 loss: 27.5846, MinusLogProbMetric: 27.5846, val_loss: 27.8374, val_MinusLogProbMetric: 27.8374

Epoch 998: val_loss did not improve from 27.83203
196/196 - 36s - loss: 27.5846 - MinusLogProbMetric: 27.5846 - val_loss: 27.8374 - val_MinusLogProbMetric: 27.8374 - lr: 1.5625e-05 - 36s/epoch - 183ms/step
Epoch 999/1000
2023-09-29 07:00:03.700 
Epoch 999/1000 
	 loss: 27.5819, MinusLogProbMetric: 27.5819, val_loss: 27.8352, val_MinusLogProbMetric: 27.8352

Epoch 999: val_loss did not improve from 27.83203
196/196 - 36s - loss: 27.5819 - MinusLogProbMetric: 27.5819 - val_loss: 27.8352 - val_MinusLogProbMetric: 27.8352 - lr: 1.5625e-05 - 36s/epoch - 186ms/step
Epoch 1000/1000
2023-09-29 07:00:40.343 
Epoch 1000/1000 
	 loss: 27.5811, MinusLogProbMetric: 27.5811, val_loss: 27.8363, val_MinusLogProbMetric: 27.8363

Epoch 1000: val_loss did not improve from 27.83203
196/196 - 37s - loss: 27.5811 - MinusLogProbMetric: 27.5811 - val_loss: 27.8363 - val_MinusLogProbMetric: 27.8363 - lr: 1.5625e-05 - 37s/epoch - 187ms/step
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.

------------------------------------------
Starting LR metric calculation...
Running TF LR calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
WARNING:tensorflow:6 out of the last 6 calls to <function LRMetric.Test_tf.<locals>.compute_test at 0x7fb11d4a9480> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
LR metric calculation completed in 16.75009438203415 seconds.

------------------------------------------
Starting KS tests calculation...
Running TF KS tests...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
WARNING:tensorflow:6 out of the last 6 calls to <function KSTest.Test_tf.<locals>.compute_test at 0x7fb11d4a8700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
KS tests calculation completed in 9.936451753019355 seconds.

------------------------------------------
Starting SWD metric calculation...
Running TF SWD calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
WARNING:tensorflow:6 out of the last 6 calls to <function SWDMetric.Test_tf.<locals>.compute_test at 0x7fb11d4a8700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
SWD metric calculation completed in 7.081422700022813 seconds.

------------------------------------------
Starting FN metric calculation...
Running TF FN calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
WARNING:tensorflow:6 out of the last 6 calls to <function FNMetric.Test_tf.<locals>.compute_test at 0x7fb11d4a8280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
FN metric calculation completed in 7.759897003998049 seconds.
Training succeeded with seed 187.
Model trained in 33562.98 s.

===========
Computing predictions
===========

Computing metrics...
Metrics computed in 42.91 s.
                ===========
                print("===========
Failed to plot
===========
")
                Exception type: ValueError
                Exception message: Number of rows must be a positive integer, not 0
                Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN.py , Line : 469, Func.Name : prediction_function, Message : Plotters.cornerplotter(X_data_test.numpy(),X_data_nf.numpy(),path_to_results,ndims,norm=True) # type: ignore', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/Plotters.py , Line : 156, Func.Name : cornerplotter, Message : figure=corner.corner(target_samples,color=\'red\',bins=n_bins,labels=[r"%s" % s for s in labels],normalize1d=True)', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/corner.py , Line : 235, Func.Name : corner, Message : fig, axes = pl.subplots(K, K, figsize=(dim, dim))', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/pyplot.py , Line : 1502, Func.Name : subplots, Message : axs = fig.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 905, Func.Name : subplots, Message : gs = self.add_gridspec(nrows, ncols, figure=self, **gridspec_kw)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 1527, Func.Name : add_gridspec, Message : gs = GridSpec(nrows=nrows, ncols=ncols, figure=self, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 379, Func.Name : __init__, Message : super().__init__(nrows, ncols,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 49, Func.Name : __init__, Message : raise ValueError(']
                ===========

results.txt saved
results.json saved
Results log saved
Model predictions computed in 43.15 s.
===========
Run 329/720 done in 33609.74 s.
===========

Directory ../../results/CsplineN_new/run_330/ already exists.
Skipping it.
===========
Run 330/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_331/ already exists.
Skipping it.
===========
Run 331/720 already exists. Skipping it.
===========

===========
Generating train data for run 332.
===========
Train data generated in 0.22 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_332/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_332/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_332/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_332
self.data_kwargs: {'seed': 187}
self.x_data: [[ 6.8153725   2.8430347   6.1778684  ...  2.16058     2.2525268
   1.7267085 ]
 [ 2.1836705   1.800204    9.035551   ...  6.5503054   1.5567943
   2.9656742 ]
 [ 0.87054014  3.1855702   7.8237348  ...  6.3044343  -0.7088827
   2.3190107 ]
 ...
 [ 0.35442734  3.573855   10.922302   ...  5.8448343   0.8037275
   2.171598  ]
 [ 1.4908957   3.8775673   8.757837   ...  5.7039022  -0.88000226
   2.3087878 ]
 [ 1.7291393   3.7802503   9.770119   ...  5.5898924   0.22995539
   4.1106014 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_259"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_260 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_24 (LogProbL  (None,)                  2139360   
 ayer)                                                           
                                                                 
=================================================================
Total params: 2,139,360
Trainable params: 2,139,360
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_24/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_24'")
self.model: <keras.engine.functional.Functional object at 0x7fb07540f910>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fb0755e3a30>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fb0755e3a30>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fb07540ef50>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fb065354670>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_332/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fb065354be0>, <keras.callbacks.ModelCheckpoint object at 0x7fb065354ca0>, <keras.callbacks.EarlyStopping object at 0x7fb065354f10>, <keras.callbacks.ReduceLROnPlateau object at 0x7fb065354f40>, <keras.callbacks.TerminateOnNaN object at 0x7fb065354b80>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_332/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 332/720 with hyperparameters:
timestamp = 2023-09-29 07:01:30.155485
ndims = 64
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 2139360
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 6.8153725e+00  2.8430347e+00  6.1778684e+00  4.2452879e+00
  1.6284513e+00  3.3207846e+00  7.0928354e+00  6.4340367e+00
  6.0301833e+00  6.4328456e+00  6.5495849e+00  4.9321003e+00
  8.9532204e+00  3.4481030e+00  4.8284545e+00  8.0901794e+00
  8.1549883e+00  7.3900299e+00  1.3510420e+00  9.5149851e+00
  7.5267930e+00  1.0017762e+01  1.1636026e+00  8.6080103e+00
  2.2276068e+00  6.2524571e+00  1.5594521e+00  8.2755499e+00
  8.5135918e+00  6.3978677e+00  3.8566861e+00  6.8423492e-01
  7.5210962e+00  5.4144373e+00  6.8548636e+00  8.7448330e+00
  9.6529083e+00  8.6500130e+00  9.1009712e-01  4.1672401e+00
  7.6439004e+00  9.0885794e-01  5.2691178e+00  8.8166595e-03
  7.4780917e-01 -4.8575500e-01  7.9515800e+00  2.7509699e+00
  4.8898096e+00  1.0073138e+01  7.4751706e+00  6.5633142e-01
  1.5831217e+00  6.0002384e+00  5.6322212e+00  2.2999794e+00
  9.1524439e+00  5.5740438e+00  5.8525505e+00  6.6422234e+00
  6.9292612e+00  2.1605799e+00  2.2525268e+00  1.7267085e+00]
Epoch 1/1000
2023-09-29 07:03:54.544 
Epoch 1/1000 
	 loss: 714.2961, MinusLogProbMetric: 714.2961, val_loss: 183.5580, val_MinusLogProbMetric: 183.5580

Epoch 1: val_loss improved from inf to 183.55800, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_332/weights/best_weights.h5
196/196 - 145s - loss: 714.2961 - MinusLogProbMetric: 714.2961 - val_loss: 183.5580 - val_MinusLogProbMetric: 183.5580 - lr: 0.0010 - 145s/epoch - 740ms/step
Epoch 2/1000
2023-09-29 07:04:42.192 
Epoch 2/1000 
	 loss: 149.1537, MinusLogProbMetric: 149.1537, val_loss: 122.9691, val_MinusLogProbMetric: 122.9691

Epoch 2: val_loss improved from 183.55800 to 122.96907, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_332/weights/best_weights.h5
196/196 - 48s - loss: 149.1537 - MinusLogProbMetric: 149.1537 - val_loss: 122.9691 - val_MinusLogProbMetric: 122.9691 - lr: 0.0010 - 48s/epoch - 243ms/step
Epoch 3/1000
2023-09-29 07:05:29.790 
Epoch 3/1000 
	 loss: 90.0972, MinusLogProbMetric: 90.0972, val_loss: 70.2625, val_MinusLogProbMetric: 70.2625

Epoch 3: val_loss improved from 122.96907 to 70.26249, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_332/weights/best_weights.h5
196/196 - 47s - loss: 90.0972 - MinusLogProbMetric: 90.0972 - val_loss: 70.2625 - val_MinusLogProbMetric: 70.2625 - lr: 0.0010 - 47s/epoch - 242ms/step
Epoch 4/1000
2023-09-29 07:06:16.471 
Epoch 4/1000 
	 loss: 63.5645, MinusLogProbMetric: 63.5645, val_loss: 56.7964, val_MinusLogProbMetric: 56.7964

Epoch 4: val_loss improved from 70.26249 to 56.79642, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_332/weights/best_weights.h5
196/196 - 47s - loss: 63.5645 - MinusLogProbMetric: 63.5645 - val_loss: 56.7964 - val_MinusLogProbMetric: 56.7964 - lr: 0.0010 - 47s/epoch - 238ms/step
Epoch 5/1000
2023-09-29 07:07:03.095 
Epoch 5/1000 
	 loss: 54.4482, MinusLogProbMetric: 54.4482, val_loss: 52.4125, val_MinusLogProbMetric: 52.4125

Epoch 5: val_loss improved from 56.79642 to 52.41251, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_332/weights/best_weights.h5
196/196 - 47s - loss: 54.4482 - MinusLogProbMetric: 54.4482 - val_loss: 52.4125 - val_MinusLogProbMetric: 52.4125 - lr: 0.0010 - 47s/epoch - 238ms/step
Epoch 6/1000
2023-09-29 07:07:49.100 
Epoch 6/1000 
	 loss: 50.0393, MinusLogProbMetric: 50.0393, val_loss: 48.1699, val_MinusLogProbMetric: 48.1699

Epoch 6: val_loss improved from 52.41251 to 48.16989, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_332/weights/best_weights.h5
196/196 - 46s - loss: 50.0393 - MinusLogProbMetric: 50.0393 - val_loss: 48.1699 - val_MinusLogProbMetric: 48.1699 - lr: 0.0010 - 46s/epoch - 234ms/step
Epoch 7/1000
2023-09-29 07:08:34.956 
Epoch 7/1000 
	 loss: 47.0099, MinusLogProbMetric: 47.0099, val_loss: 48.4819, val_MinusLogProbMetric: 48.4819

Epoch 7: val_loss did not improve from 48.16989
196/196 - 45s - loss: 47.0099 - MinusLogProbMetric: 47.0099 - val_loss: 48.4819 - val_MinusLogProbMetric: 48.4819 - lr: 0.0010 - 45s/epoch - 229ms/step
Epoch 8/1000
2023-09-29 07:09:20.222 
Epoch 8/1000 
	 loss: 45.5578, MinusLogProbMetric: 45.5578, val_loss: 47.5199, val_MinusLogProbMetric: 47.5199

Epoch 8: val_loss improved from 48.16989 to 47.51985, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_332/weights/best_weights.h5
196/196 - 46s - loss: 45.5578 - MinusLogProbMetric: 45.5578 - val_loss: 47.5199 - val_MinusLogProbMetric: 47.5199 - lr: 0.0010 - 46s/epoch - 236ms/step
Epoch 9/1000
2023-09-29 07:10:07.376 
Epoch 9/1000 
	 loss: 44.0102, MinusLogProbMetric: 44.0102, val_loss: 41.9019, val_MinusLogProbMetric: 41.9019

Epoch 9: val_loss improved from 47.51985 to 41.90192, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_332/weights/best_weights.h5
196/196 - 47s - loss: 44.0102 - MinusLogProbMetric: 44.0102 - val_loss: 41.9019 - val_MinusLogProbMetric: 41.9019 - lr: 0.0010 - 47s/epoch - 241ms/step
Epoch 10/1000
2023-09-29 07:10:54.139 
Epoch 10/1000 
	 loss: 42.5385, MinusLogProbMetric: 42.5385, val_loss: 44.2264, val_MinusLogProbMetric: 44.2264

Epoch 10: val_loss did not improve from 41.90192
196/196 - 46s - loss: 42.5385 - MinusLogProbMetric: 42.5385 - val_loss: 44.2264 - val_MinusLogProbMetric: 44.2264 - lr: 0.0010 - 46s/epoch - 234ms/step
Epoch 11/1000
2023-09-29 07:11:40.094 
Epoch 11/1000 
	 loss: 43.6644, MinusLogProbMetric: 43.6644, val_loss: 40.6410, val_MinusLogProbMetric: 40.6410

Epoch 11: val_loss improved from 41.90192 to 40.64100, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_332/weights/best_weights.h5
196/196 - 47s - loss: 43.6644 - MinusLogProbMetric: 43.6644 - val_loss: 40.6410 - val_MinusLogProbMetric: 40.6410 - lr: 0.0010 - 47s/epoch - 239ms/step
Epoch 12/1000
2023-09-29 07:12:25.501 
Epoch 12/1000 
	 loss: 40.9469, MinusLogProbMetric: 40.9469, val_loss: 48.7203, val_MinusLogProbMetric: 48.7203

Epoch 12: val_loss did not improve from 40.64100
196/196 - 44s - loss: 40.9469 - MinusLogProbMetric: 40.9469 - val_loss: 48.7203 - val_MinusLogProbMetric: 48.7203 - lr: 0.0010 - 44s/epoch - 227ms/step
Epoch 13/1000
2023-09-29 07:13:11.758 
Epoch 13/1000 
	 loss: 39.8534, MinusLogProbMetric: 39.8534, val_loss: 39.9251, val_MinusLogProbMetric: 39.9251

Epoch 13: val_loss improved from 40.64100 to 39.92508, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_332/weights/best_weights.h5
196/196 - 47s - loss: 39.8534 - MinusLogProbMetric: 39.8534 - val_loss: 39.9251 - val_MinusLogProbMetric: 39.9251 - lr: 0.0010 - 47s/epoch - 241ms/step
Epoch 14/1000
2023-09-29 07:14:00.063 
Epoch 14/1000 
	 loss: 39.4614, MinusLogProbMetric: 39.4614, val_loss: 41.1758, val_MinusLogProbMetric: 41.1758

Epoch 14: val_loss did not improve from 39.92508
196/196 - 47s - loss: 39.4614 - MinusLogProbMetric: 39.4614 - val_loss: 41.1758 - val_MinusLogProbMetric: 41.1758 - lr: 0.0010 - 47s/epoch - 241ms/step
Epoch 15/1000
2023-09-29 07:14:46.218 
Epoch 15/1000 
	 loss: 38.5850, MinusLogProbMetric: 38.5850, val_loss: 40.3818, val_MinusLogProbMetric: 40.3818

Epoch 15: val_loss did not improve from 39.92508
196/196 - 46s - loss: 38.5850 - MinusLogProbMetric: 38.5850 - val_loss: 40.3818 - val_MinusLogProbMetric: 40.3818 - lr: 0.0010 - 46s/epoch - 235ms/step
Epoch 16/1000
2023-09-29 07:15:30.414 
Epoch 16/1000 
	 loss: 38.1425, MinusLogProbMetric: 38.1425, val_loss: 39.2326, val_MinusLogProbMetric: 39.2326

Epoch 16: val_loss improved from 39.92508 to 39.23262, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_332/weights/best_weights.h5
196/196 - 45s - loss: 38.1425 - MinusLogProbMetric: 38.1425 - val_loss: 39.2326 - val_MinusLogProbMetric: 39.2326 - lr: 0.0010 - 45s/epoch - 231ms/step
Epoch 17/1000
2023-09-29 07:16:15.659 
Epoch 17/1000 
	 loss: 37.0935, MinusLogProbMetric: 37.0935, val_loss: 36.0855, val_MinusLogProbMetric: 36.0855

Epoch 17: val_loss improved from 39.23262 to 36.08549, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_332/weights/best_weights.h5
196/196 - 45s - loss: 37.0935 - MinusLogProbMetric: 37.0935 - val_loss: 36.0855 - val_MinusLogProbMetric: 36.0855 - lr: 0.0010 - 45s/epoch - 230ms/step
Epoch 18/1000
2023-09-29 07:17:08.626 
Epoch 18/1000 
	 loss: 37.1851, MinusLogProbMetric: 37.1851, val_loss: 37.5921, val_MinusLogProbMetric: 37.5921

Epoch 18: val_loss did not improve from 36.08549
196/196 - 52s - loss: 37.1851 - MinusLogProbMetric: 37.1851 - val_loss: 37.5921 - val_MinusLogProbMetric: 37.5921 - lr: 0.0010 - 52s/epoch - 266ms/step
Epoch 19/1000
2023-09-29 07:18:07.475 
Epoch 19/1000 
	 loss: 36.5673, MinusLogProbMetric: 36.5673, val_loss: 36.8607, val_MinusLogProbMetric: 36.8607

Epoch 19: val_loss did not improve from 36.08549
196/196 - 59s - loss: 36.5673 - MinusLogProbMetric: 36.5673 - val_loss: 36.8607 - val_MinusLogProbMetric: 36.8607 - lr: 0.0010 - 59s/epoch - 300ms/step
Epoch 20/1000
2023-09-29 07:18:50.614 
Epoch 20/1000 
	 loss: 36.2019, MinusLogProbMetric: 36.2019, val_loss: 36.5721, val_MinusLogProbMetric: 36.5721

Epoch 20: val_loss did not improve from 36.08549
196/196 - 43s - loss: 36.2019 - MinusLogProbMetric: 36.2019 - val_loss: 36.5721 - val_MinusLogProbMetric: 36.5721 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 21/1000
2023-09-29 07:19:28.408 
Epoch 21/1000 
	 loss: 35.8119, MinusLogProbMetric: 35.8119, val_loss: 35.9051, val_MinusLogProbMetric: 35.9051

Epoch 21: val_loss improved from 36.08549 to 35.90506, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_332/weights/best_weights.h5
196/196 - 39s - loss: 35.8119 - MinusLogProbMetric: 35.8119 - val_loss: 35.9051 - val_MinusLogProbMetric: 35.9051 - lr: 0.0010 - 39s/epoch - 197ms/step
Epoch 22/1000
2023-09-29 07:20:11.848 
Epoch 22/1000 
	 loss: 35.6285, MinusLogProbMetric: 35.6285, val_loss: 34.7108, val_MinusLogProbMetric: 34.7108

Epoch 22: val_loss improved from 35.90506 to 34.71077, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_332/weights/best_weights.h5
196/196 - 44s - loss: 35.6285 - MinusLogProbMetric: 35.6285 - val_loss: 34.7108 - val_MinusLogProbMetric: 34.7108 - lr: 0.0010 - 44s/epoch - 222ms/step
Epoch 23/1000
2023-09-29 07:20:53.613 
Epoch 23/1000 
	 loss: 35.4243, MinusLogProbMetric: 35.4243, val_loss: 34.9599, val_MinusLogProbMetric: 34.9599

Epoch 23: val_loss did not improve from 34.71077
196/196 - 41s - loss: 35.4243 - MinusLogProbMetric: 35.4243 - val_loss: 34.9599 - val_MinusLogProbMetric: 34.9599 - lr: 0.0010 - 41s/epoch - 209ms/step
Epoch 24/1000
2023-09-29 07:21:34.959 
Epoch 24/1000 
	 loss: 35.1483, MinusLogProbMetric: 35.1483, val_loss: 36.3805, val_MinusLogProbMetric: 36.3805

Epoch 24: val_loss did not improve from 34.71077
196/196 - 41s - loss: 35.1483 - MinusLogProbMetric: 35.1483 - val_loss: 36.3805 - val_MinusLogProbMetric: 36.3805 - lr: 0.0010 - 41s/epoch - 211ms/step
Epoch 25/1000
2023-09-29 07:22:13.443 
Epoch 25/1000 
	 loss: 35.0935, MinusLogProbMetric: 35.0935, val_loss: 34.5466, val_MinusLogProbMetric: 34.5466

Epoch 25: val_loss improved from 34.71077 to 34.54660, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_332/weights/best_weights.h5
196/196 - 39s - loss: 35.0935 - MinusLogProbMetric: 35.0935 - val_loss: 34.5466 - val_MinusLogProbMetric: 34.5466 - lr: 0.0010 - 39s/epoch - 200ms/step
Epoch 26/1000
2023-09-29 07:22:52.711 
Epoch 26/1000 
	 loss: 35.0313, MinusLogProbMetric: 35.0313, val_loss: 33.4398, val_MinusLogProbMetric: 33.4398

Epoch 26: val_loss improved from 34.54660 to 33.43983, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_332/weights/best_weights.h5
196/196 - 39s - loss: 35.0313 - MinusLogProbMetric: 35.0313 - val_loss: 33.4398 - val_MinusLogProbMetric: 33.4398 - lr: 0.0010 - 39s/epoch - 201ms/step
Epoch 27/1000
2023-09-29 07:23:32.387 
Epoch 27/1000 
	 loss: 34.5528, MinusLogProbMetric: 34.5528, val_loss: 34.2136, val_MinusLogProbMetric: 34.2136

Epoch 27: val_loss did not improve from 33.43983
196/196 - 39s - loss: 34.5528 - MinusLogProbMetric: 34.5528 - val_loss: 34.2136 - val_MinusLogProbMetric: 34.2136 - lr: 0.0010 - 39s/epoch - 198ms/step
Epoch 28/1000
2023-09-29 07:24:13.049 
Epoch 28/1000 
	 loss: 34.0258, MinusLogProbMetric: 34.0258, val_loss: 36.0148, val_MinusLogProbMetric: 36.0148

Epoch 28: val_loss did not improve from 33.43983
196/196 - 41s - loss: 34.0258 - MinusLogProbMetric: 34.0258 - val_loss: 36.0148 - val_MinusLogProbMetric: 36.0148 - lr: 0.0010 - 41s/epoch - 207ms/step
Epoch 29/1000
2023-09-29 07:24:51.335 
Epoch 29/1000 
	 loss: 34.1151, MinusLogProbMetric: 34.1151, val_loss: 33.4658, val_MinusLogProbMetric: 33.4658

Epoch 29: val_loss did not improve from 33.43983
196/196 - 38s - loss: 34.1151 - MinusLogProbMetric: 34.1151 - val_loss: 33.4658 - val_MinusLogProbMetric: 33.4658 - lr: 0.0010 - 38s/epoch - 195ms/step
Epoch 30/1000
2023-09-29 07:25:29.876 
Epoch 30/1000 
	 loss: 33.9264, MinusLogProbMetric: 33.9264, val_loss: 35.6558, val_MinusLogProbMetric: 35.6558

Epoch 30: val_loss did not improve from 33.43983
196/196 - 39s - loss: 33.9264 - MinusLogProbMetric: 33.9264 - val_loss: 35.6558 - val_MinusLogProbMetric: 35.6558 - lr: 0.0010 - 39s/epoch - 197ms/step
Epoch 31/1000
2023-09-29 07:26:09.158 
Epoch 31/1000 
	 loss: 33.7443, MinusLogProbMetric: 33.7443, val_loss: 32.9004, val_MinusLogProbMetric: 32.9004

Epoch 31: val_loss improved from 33.43983 to 32.90039, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_332/weights/best_weights.h5
196/196 - 40s - loss: 33.7443 - MinusLogProbMetric: 33.7443 - val_loss: 32.9004 - val_MinusLogProbMetric: 32.9004 - lr: 0.0010 - 40s/epoch - 205ms/step
Epoch 32/1000
2023-09-29 07:26:51.084 
Epoch 32/1000 
	 loss: 33.5662, MinusLogProbMetric: 33.5662, val_loss: 31.9818, val_MinusLogProbMetric: 31.9818

Epoch 32: val_loss improved from 32.90039 to 31.98178, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_332/weights/best_weights.h5
196/196 - 42s - loss: 33.5662 - MinusLogProbMetric: 33.5662 - val_loss: 31.9818 - val_MinusLogProbMetric: 31.9818 - lr: 0.0010 - 42s/epoch - 213ms/step
Epoch 33/1000
2023-09-29 07:27:30.546 
Epoch 33/1000 
	 loss: 33.4660, MinusLogProbMetric: 33.4660, val_loss: 32.9460, val_MinusLogProbMetric: 32.9460

Epoch 33: val_loss did not improve from 31.98178
196/196 - 39s - loss: 33.4660 - MinusLogProbMetric: 33.4660 - val_loss: 32.9460 - val_MinusLogProbMetric: 32.9460 - lr: 0.0010 - 39s/epoch - 198ms/step
Epoch 34/1000
2023-09-29 07:28:09.383 
Epoch 34/1000 
	 loss: 33.4776, MinusLogProbMetric: 33.4776, val_loss: 33.0455, val_MinusLogProbMetric: 33.0455

Epoch 34: val_loss did not improve from 31.98178
196/196 - 39s - loss: 33.4776 - MinusLogProbMetric: 33.4776 - val_loss: 33.0455 - val_MinusLogProbMetric: 33.0455 - lr: 0.0010 - 39s/epoch - 198ms/step
Epoch 35/1000
2023-09-29 07:28:49.565 
Epoch 35/1000 
	 loss: 33.0864, MinusLogProbMetric: 33.0864, val_loss: 34.0339, val_MinusLogProbMetric: 34.0339

Epoch 35: val_loss did not improve from 31.98178
196/196 - 40s - loss: 33.0864 - MinusLogProbMetric: 33.0864 - val_loss: 34.0339 - val_MinusLogProbMetric: 34.0339 - lr: 0.0010 - 40s/epoch - 205ms/step
Epoch 36/1000
2023-09-29 07:29:28.371 
Epoch 36/1000 
	 loss: 33.1939, MinusLogProbMetric: 33.1939, val_loss: 32.6321, val_MinusLogProbMetric: 32.6321

Epoch 36: val_loss did not improve from 31.98178
196/196 - 39s - loss: 33.1939 - MinusLogProbMetric: 33.1939 - val_loss: 32.6321 - val_MinusLogProbMetric: 32.6321 - lr: 0.0010 - 39s/epoch - 198ms/step
Epoch 37/1000
2023-09-29 07:30:06.645 
Epoch 37/1000 
	 loss: 33.0722, MinusLogProbMetric: 33.0722, val_loss: 32.5615, val_MinusLogProbMetric: 32.5615

Epoch 37: val_loss did not improve from 31.98178
196/196 - 38s - loss: 33.0722 - MinusLogProbMetric: 33.0722 - val_loss: 32.5615 - val_MinusLogProbMetric: 32.5615 - lr: 0.0010 - 38s/epoch - 195ms/step
Epoch 38/1000
2023-09-29 07:30:45.082 
Epoch 38/1000 
	 loss: 32.9350, MinusLogProbMetric: 32.9350, val_loss: 33.0939, val_MinusLogProbMetric: 33.0939

Epoch 38: val_loss did not improve from 31.98178
196/196 - 38s - loss: 32.9350 - MinusLogProbMetric: 32.9350 - val_loss: 33.0939 - val_MinusLogProbMetric: 33.0939 - lr: 0.0010 - 38s/epoch - 196ms/step
Epoch 39/1000
2023-09-29 07:31:26.782 
Epoch 39/1000 
	 loss: 32.7039, MinusLogProbMetric: 32.7039, val_loss: 33.9362, val_MinusLogProbMetric: 33.9362

Epoch 39: val_loss did not improve from 31.98178
196/196 - 42s - loss: 32.7039 - MinusLogProbMetric: 32.7039 - val_loss: 33.9362 - val_MinusLogProbMetric: 33.9362 - lr: 0.0010 - 42s/epoch - 213ms/step
Epoch 40/1000
2023-09-29 07:32:08.134 
Epoch 40/1000 
	 loss: 32.6682, MinusLogProbMetric: 32.6682, val_loss: 32.3781, val_MinusLogProbMetric: 32.3781

Epoch 40: val_loss did not improve from 31.98178
196/196 - 41s - loss: 32.6682 - MinusLogProbMetric: 32.6682 - val_loss: 32.3781 - val_MinusLogProbMetric: 32.3781 - lr: 0.0010 - 41s/epoch - 211ms/step
Epoch 41/1000
2023-09-29 07:32:48.966 
Epoch 41/1000 
	 loss: 32.4461, MinusLogProbMetric: 32.4461, val_loss: 32.2151, val_MinusLogProbMetric: 32.2151

Epoch 41: val_loss did not improve from 31.98178
196/196 - 41s - loss: 32.4461 - MinusLogProbMetric: 32.4461 - val_loss: 32.2151 - val_MinusLogProbMetric: 32.2151 - lr: 0.0010 - 41s/epoch - 208ms/step
Epoch 42/1000
2023-09-29 07:33:26.850 
Epoch 42/1000 
	 loss: 32.2224, MinusLogProbMetric: 32.2224, val_loss: 33.1512, val_MinusLogProbMetric: 33.1512

Epoch 42: val_loss did not improve from 31.98178
196/196 - 38s - loss: 32.2224 - MinusLogProbMetric: 32.2224 - val_loss: 33.1512 - val_MinusLogProbMetric: 33.1512 - lr: 0.0010 - 38s/epoch - 193ms/step
Epoch 43/1000
2023-09-29 07:34:06.296 
Epoch 43/1000 
	 loss: 32.3696, MinusLogProbMetric: 32.3696, val_loss: 33.5847, val_MinusLogProbMetric: 33.5847

Epoch 43: val_loss did not improve from 31.98178
196/196 - 39s - loss: 32.3696 - MinusLogProbMetric: 32.3696 - val_loss: 33.5847 - val_MinusLogProbMetric: 33.5847 - lr: 0.0010 - 39s/epoch - 201ms/step
Epoch 44/1000
2023-09-29 07:34:43.107 
Epoch 44/1000 
	 loss: 32.1055, MinusLogProbMetric: 32.1055, val_loss: 32.6440, val_MinusLogProbMetric: 32.6440

Epoch 44: val_loss did not improve from 31.98178
196/196 - 37s - loss: 32.1055 - MinusLogProbMetric: 32.1055 - val_loss: 32.6440 - val_MinusLogProbMetric: 32.6440 - lr: 0.0010 - 37s/epoch - 188ms/step
Epoch 45/1000
2023-09-29 07:35:20.954 
Epoch 45/1000 
	 loss: 32.2901, MinusLogProbMetric: 32.2901, val_loss: 34.8196, val_MinusLogProbMetric: 34.8196

Epoch 45: val_loss did not improve from 31.98178
196/196 - 38s - loss: 32.2901 - MinusLogProbMetric: 32.2901 - val_loss: 34.8196 - val_MinusLogProbMetric: 34.8196 - lr: 0.0010 - 38s/epoch - 193ms/step
Epoch 46/1000
2023-09-29 07:35:55.676 
Epoch 46/1000 
	 loss: 32.0737, MinusLogProbMetric: 32.0737, val_loss: 31.8972, val_MinusLogProbMetric: 31.8972

Epoch 46: val_loss improved from 31.98178 to 31.89717, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_332/weights/best_weights.h5
196/196 - 35s - loss: 32.0737 - MinusLogProbMetric: 32.0737 - val_loss: 31.8972 - val_MinusLogProbMetric: 31.8972 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 47/1000
2023-09-29 07:36:31.161 
Epoch 47/1000 
	 loss: 31.9106, MinusLogProbMetric: 31.9106, val_loss: 32.1151, val_MinusLogProbMetric: 32.1151

Epoch 47: val_loss did not improve from 31.89717
196/196 - 35s - loss: 31.9106 - MinusLogProbMetric: 31.9106 - val_loss: 32.1151 - val_MinusLogProbMetric: 32.1151 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 48/1000
2023-09-29 07:37:08.498 
Epoch 48/1000 
	 loss: 31.8437, MinusLogProbMetric: 31.8437, val_loss: 32.7524, val_MinusLogProbMetric: 32.7524

Epoch 48: val_loss did not improve from 31.89717
196/196 - 37s - loss: 31.8437 - MinusLogProbMetric: 31.8437 - val_loss: 32.7524 - val_MinusLogProbMetric: 32.7524 - lr: 0.0010 - 37s/epoch - 190ms/step
Epoch 49/1000
2023-09-29 07:37:46.727 
Epoch 49/1000 
	 loss: 31.8322, MinusLogProbMetric: 31.8322, val_loss: 31.7126, val_MinusLogProbMetric: 31.7126

Epoch 49: val_loss improved from 31.89717 to 31.71256, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_332/weights/best_weights.h5
196/196 - 39s - loss: 31.8322 - MinusLogProbMetric: 31.8322 - val_loss: 31.7126 - val_MinusLogProbMetric: 31.7126 - lr: 0.0010 - 39s/epoch - 199ms/step
Epoch 50/1000
2023-09-29 07:38:24.488 
Epoch 50/1000 
	 loss: 31.7575, MinusLogProbMetric: 31.7575, val_loss: 31.9449, val_MinusLogProbMetric: 31.9449

Epoch 50: val_loss did not improve from 31.71256
196/196 - 37s - loss: 31.7575 - MinusLogProbMetric: 31.7575 - val_loss: 31.9449 - val_MinusLogProbMetric: 31.9449 - lr: 0.0010 - 37s/epoch - 189ms/step
Epoch 51/1000
2023-09-29 07:39:01.965 
Epoch 51/1000 
	 loss: 31.5830, MinusLogProbMetric: 31.5830, val_loss: 31.9064, val_MinusLogProbMetric: 31.9064

Epoch 51: val_loss did not improve from 31.71256
196/196 - 37s - loss: 31.5830 - MinusLogProbMetric: 31.5830 - val_loss: 31.9064 - val_MinusLogProbMetric: 31.9064 - lr: 0.0010 - 37s/epoch - 191ms/step
Epoch 52/1000
2023-09-29 07:39:39.046 
Epoch 52/1000 
	 loss: 31.8000, MinusLogProbMetric: 31.8000, val_loss: 32.1778, val_MinusLogProbMetric: 32.1778

Epoch 52: val_loss did not improve from 31.71256
196/196 - 37s - loss: 31.8000 - MinusLogProbMetric: 31.8000 - val_loss: 32.1778 - val_MinusLogProbMetric: 32.1778 - lr: 0.0010 - 37s/epoch - 189ms/step
Epoch 53/1000
2023-09-29 07:40:17.506 
Epoch 53/1000 
	 loss: 31.5108, MinusLogProbMetric: 31.5108, val_loss: 31.9954, val_MinusLogProbMetric: 31.9954

Epoch 53: val_loss did not improve from 31.71256
196/196 - 38s - loss: 31.5108 - MinusLogProbMetric: 31.5108 - val_loss: 31.9954 - val_MinusLogProbMetric: 31.9954 - lr: 0.0010 - 38s/epoch - 196ms/step
Epoch 54/1000
2023-09-29 07:40:55.846 
Epoch 54/1000 
	 loss: 31.4439, MinusLogProbMetric: 31.4439, val_loss: 32.0660, val_MinusLogProbMetric: 32.0660

Epoch 54: val_loss did not improve from 31.71256
196/196 - 38s - loss: 31.4439 - MinusLogProbMetric: 31.4439 - val_loss: 32.0660 - val_MinusLogProbMetric: 32.0660 - lr: 0.0010 - 38s/epoch - 196ms/step
Epoch 55/1000
2023-09-29 07:41:32.923 
Epoch 55/1000 
	 loss: 31.2821, MinusLogProbMetric: 31.2821, val_loss: 31.9785, val_MinusLogProbMetric: 31.9785

Epoch 55: val_loss did not improve from 31.71256
196/196 - 37s - loss: 31.2821 - MinusLogProbMetric: 31.2821 - val_loss: 31.9785 - val_MinusLogProbMetric: 31.9785 - lr: 0.0010 - 37s/epoch - 189ms/step
Epoch 56/1000
2023-09-29 07:42:10.880 
Epoch 56/1000 
	 loss: 31.3747, MinusLogProbMetric: 31.3747, val_loss: 32.3917, val_MinusLogProbMetric: 32.3917

Epoch 56: val_loss did not improve from 31.71256
196/196 - 38s - loss: 31.3747 - MinusLogProbMetric: 31.3747 - val_loss: 32.3917 - val_MinusLogProbMetric: 32.3917 - lr: 0.0010 - 38s/epoch - 194ms/step
Epoch 57/1000
2023-09-29 07:42:48.376 
Epoch 57/1000 
	 loss: 31.2638, MinusLogProbMetric: 31.2638, val_loss: 32.9604, val_MinusLogProbMetric: 32.9604

Epoch 57: val_loss did not improve from 31.71256
196/196 - 37s - loss: 31.2638 - MinusLogProbMetric: 31.2638 - val_loss: 32.9604 - val_MinusLogProbMetric: 32.9604 - lr: 0.0010 - 37s/epoch - 191ms/step
Epoch 58/1000
2023-09-29 07:43:26.052 
Epoch 58/1000 
	 loss: 31.4361, MinusLogProbMetric: 31.4361, val_loss: 31.4031, val_MinusLogProbMetric: 31.4031

Epoch 58: val_loss improved from 31.71256 to 31.40309, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_332/weights/best_weights.h5
196/196 - 38s - loss: 31.4361 - MinusLogProbMetric: 31.4361 - val_loss: 31.4031 - val_MinusLogProbMetric: 31.4031 - lr: 0.0010 - 38s/epoch - 196ms/step
Epoch 59/1000
2023-09-29 07:44:02.590 
Epoch 59/1000 
	 loss: 31.2366, MinusLogProbMetric: 31.2366, val_loss: 31.0701, val_MinusLogProbMetric: 31.0701

Epoch 59: val_loss improved from 31.40309 to 31.07012, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_332/weights/best_weights.h5
196/196 - 36s - loss: 31.2366 - MinusLogProbMetric: 31.2366 - val_loss: 31.0701 - val_MinusLogProbMetric: 31.0701 - lr: 0.0010 - 36s/epoch - 186ms/step
Epoch 60/1000
2023-09-29 07:44:40.201 
Epoch 60/1000 
	 loss: 31.1148, MinusLogProbMetric: 31.1148, val_loss: 30.9403, val_MinusLogProbMetric: 30.9403

Epoch 60: val_loss improved from 31.07012 to 30.94029, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_332/weights/best_weights.h5
196/196 - 38s - loss: 31.1148 - MinusLogProbMetric: 31.1148 - val_loss: 30.9403 - val_MinusLogProbMetric: 30.9403 - lr: 0.0010 - 38s/epoch - 193ms/step
Epoch 61/1000
2023-09-29 07:45:20.153 
Epoch 61/1000 
	 loss: 31.1082, MinusLogProbMetric: 31.1082, val_loss: 31.3929, val_MinusLogProbMetric: 31.3929

Epoch 61: val_loss did not improve from 30.94029
196/196 - 39s - loss: 31.1082 - MinusLogProbMetric: 31.1082 - val_loss: 31.3929 - val_MinusLogProbMetric: 31.3929 - lr: 0.0010 - 39s/epoch - 200ms/step
Epoch 62/1000
2023-09-29 07:45:54.723 
Epoch 62/1000 
	 loss: 31.2422, MinusLogProbMetric: 31.2422, val_loss: 31.4020, val_MinusLogProbMetric: 31.4020

Epoch 62: val_loss did not improve from 30.94029
196/196 - 35s - loss: 31.2422 - MinusLogProbMetric: 31.2422 - val_loss: 31.4020 - val_MinusLogProbMetric: 31.4020 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 63/1000
2023-09-29 07:46:29.232 
Epoch 63/1000 
	 loss: 30.9910, MinusLogProbMetric: 30.9910, val_loss: 31.2103, val_MinusLogProbMetric: 31.2103

Epoch 63: val_loss did not improve from 30.94029
196/196 - 35s - loss: 30.9910 - MinusLogProbMetric: 30.9910 - val_loss: 31.2103 - val_MinusLogProbMetric: 31.2103 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 64/1000
2023-09-29 07:47:06.289 
Epoch 64/1000 
	 loss: 30.8987, MinusLogProbMetric: 30.8987, val_loss: 31.3574, val_MinusLogProbMetric: 31.3574

Epoch 64: val_loss did not improve from 30.94029
196/196 - 37s - loss: 30.8987 - MinusLogProbMetric: 30.8987 - val_loss: 31.3574 - val_MinusLogProbMetric: 31.3574 - lr: 0.0010 - 37s/epoch - 189ms/step
Epoch 65/1000
2023-09-29 07:47:45.880 
Epoch 65/1000 
	 loss: 30.9983, MinusLogProbMetric: 30.9983, val_loss: 32.5175, val_MinusLogProbMetric: 32.5175

Epoch 65: val_loss did not improve from 30.94029
196/196 - 40s - loss: 30.9983 - MinusLogProbMetric: 30.9983 - val_loss: 32.5175 - val_MinusLogProbMetric: 32.5175 - lr: 0.0010 - 40s/epoch - 202ms/step
Epoch 66/1000
2023-09-29 07:48:23.515 
Epoch 66/1000 
	 loss: 31.2022, MinusLogProbMetric: 31.2022, val_loss: 31.3493, val_MinusLogProbMetric: 31.3493

Epoch 66: val_loss did not improve from 30.94029
196/196 - 38s - loss: 31.2022 - MinusLogProbMetric: 31.2022 - val_loss: 31.3493 - val_MinusLogProbMetric: 31.3493 - lr: 0.0010 - 38s/epoch - 192ms/step
Epoch 67/1000
2023-09-29 07:49:01.555 
Epoch 67/1000 
	 loss: 30.6802, MinusLogProbMetric: 30.6802, val_loss: 31.5136, val_MinusLogProbMetric: 31.5136

Epoch 67: val_loss did not improve from 30.94029
196/196 - 38s - loss: 30.6802 - MinusLogProbMetric: 30.6802 - val_loss: 31.5136 - val_MinusLogProbMetric: 31.5136 - lr: 0.0010 - 38s/epoch - 194ms/step
Epoch 68/1000
2023-09-29 07:49:40.706 
Epoch 68/1000 
	 loss: 30.9592, MinusLogProbMetric: 30.9592, val_loss: 30.9546, val_MinusLogProbMetric: 30.9546

Epoch 68: val_loss did not improve from 30.94029
196/196 - 39s - loss: 30.9592 - MinusLogProbMetric: 30.9592 - val_loss: 30.9546 - val_MinusLogProbMetric: 30.9546 - lr: 0.0010 - 39s/epoch - 200ms/step
Epoch 69/1000
2023-09-29 07:50:18.835 
Epoch 69/1000 
	 loss: 30.6467, MinusLogProbMetric: 30.6467, val_loss: 31.1149, val_MinusLogProbMetric: 31.1149

Epoch 69: val_loss did not improve from 30.94029
196/196 - 38s - loss: 30.6467 - MinusLogProbMetric: 30.6467 - val_loss: 31.1149 - val_MinusLogProbMetric: 31.1149 - lr: 0.0010 - 38s/epoch - 195ms/step
Epoch 70/1000
2023-09-29 07:50:58.306 
Epoch 70/1000 
	 loss: 30.8963, MinusLogProbMetric: 30.8963, val_loss: 31.1015, val_MinusLogProbMetric: 31.1015

Epoch 70: val_loss did not improve from 30.94029
196/196 - 39s - loss: 30.8963 - MinusLogProbMetric: 30.8963 - val_loss: 31.1015 - val_MinusLogProbMetric: 31.1015 - lr: 0.0010 - 39s/epoch - 201ms/step
Epoch 71/1000
2023-09-29 07:51:35.992 
Epoch 71/1000 
	 loss: 30.7247, MinusLogProbMetric: 30.7247, val_loss: 31.0263, val_MinusLogProbMetric: 31.0263

Epoch 71: val_loss did not improve from 30.94029
196/196 - 38s - loss: 30.7247 - MinusLogProbMetric: 30.7247 - val_loss: 31.0263 - val_MinusLogProbMetric: 31.0263 - lr: 0.0010 - 38s/epoch - 192ms/step
Epoch 72/1000
2023-09-29 07:52:12.345 
Epoch 72/1000 
	 loss: 30.5669, MinusLogProbMetric: 30.5669, val_loss: 31.1249, val_MinusLogProbMetric: 31.1249

Epoch 72: val_loss did not improve from 30.94029
196/196 - 36s - loss: 30.5669 - MinusLogProbMetric: 30.5669 - val_loss: 31.1249 - val_MinusLogProbMetric: 31.1249 - lr: 0.0010 - 36s/epoch - 185ms/step
Epoch 73/1000
2023-09-29 07:52:46.517 
Epoch 73/1000 
	 loss: 30.6591, MinusLogProbMetric: 30.6591, val_loss: 31.0604, val_MinusLogProbMetric: 31.0604

Epoch 73: val_loss did not improve from 30.94029
196/196 - 34s - loss: 30.6591 - MinusLogProbMetric: 30.6591 - val_loss: 31.0604 - val_MinusLogProbMetric: 31.0604 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 74/1000
2023-09-29 07:53:22.277 
Epoch 74/1000 
	 loss: 30.6188, MinusLogProbMetric: 30.6188, val_loss: 30.9129, val_MinusLogProbMetric: 30.9129

Epoch 74: val_loss improved from 30.94029 to 30.91286, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_332/weights/best_weights.h5
196/196 - 36s - loss: 30.6188 - MinusLogProbMetric: 30.6188 - val_loss: 30.9129 - val_MinusLogProbMetric: 30.9129 - lr: 0.0010 - 36s/epoch - 186ms/step
Epoch 75/1000
2023-09-29 07:54:01.340 
Epoch 75/1000 
	 loss: 30.4887, MinusLogProbMetric: 30.4887, val_loss: 30.6663, val_MinusLogProbMetric: 30.6663

Epoch 75: val_loss improved from 30.91286 to 30.66635, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_332/weights/best_weights.h5
196/196 - 39s - loss: 30.4887 - MinusLogProbMetric: 30.4887 - val_loss: 30.6663 - val_MinusLogProbMetric: 30.6663 - lr: 0.0010 - 39s/epoch - 200ms/step
Epoch 76/1000
2023-09-29 07:54:41.262 
Epoch 76/1000 
	 loss: 30.5736, MinusLogProbMetric: 30.5736, val_loss: 32.6038, val_MinusLogProbMetric: 32.6038

Epoch 76: val_loss did not improve from 30.66635
196/196 - 39s - loss: 30.5736 - MinusLogProbMetric: 30.5736 - val_loss: 32.6038 - val_MinusLogProbMetric: 32.6038 - lr: 0.0010 - 39s/epoch - 200ms/step
Epoch 77/1000
2023-09-29 07:55:18.104 
Epoch 77/1000 
	 loss: 30.4971, MinusLogProbMetric: 30.4971, val_loss: 30.6106, val_MinusLogProbMetric: 30.6106

Epoch 77: val_loss improved from 30.66635 to 30.61059, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_332/weights/best_weights.h5
196/196 - 38s - loss: 30.4971 - MinusLogProbMetric: 30.4971 - val_loss: 30.6106 - val_MinusLogProbMetric: 30.6106 - lr: 0.0010 - 38s/epoch - 191ms/step
Epoch 78/1000
2023-09-29 07:55:56.603 
Epoch 78/1000 
	 loss: 30.4577, MinusLogProbMetric: 30.4577, val_loss: 30.0673, val_MinusLogProbMetric: 30.0673

Epoch 78: val_loss improved from 30.61059 to 30.06732, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_332/weights/best_weights.h5
196/196 - 39s - loss: 30.4577 - MinusLogProbMetric: 30.4577 - val_loss: 30.0673 - val_MinusLogProbMetric: 30.0673 - lr: 0.0010 - 39s/epoch - 197ms/step
Epoch 79/1000
2023-09-29 07:56:35.185 
Epoch 79/1000 
	 loss: 30.4651, MinusLogProbMetric: 30.4651, val_loss: 30.6313, val_MinusLogProbMetric: 30.6313

Epoch 79: val_loss did not improve from 30.06732
196/196 - 38s - loss: 30.4651 - MinusLogProbMetric: 30.4651 - val_loss: 30.6313 - val_MinusLogProbMetric: 30.6313 - lr: 0.0010 - 38s/epoch - 193ms/step
Epoch 80/1000
2023-09-29 07:57:13.450 
Epoch 80/1000 
	 loss: 30.3508, MinusLogProbMetric: 30.3508, val_loss: 31.7879, val_MinusLogProbMetric: 31.7879

Epoch 80: val_loss did not improve from 30.06732
196/196 - 38s - loss: 30.3508 - MinusLogProbMetric: 30.3508 - val_loss: 31.7879 - val_MinusLogProbMetric: 31.7879 - lr: 0.0010 - 38s/epoch - 195ms/step
Epoch 81/1000
2023-09-29 07:57:53.144 
Epoch 81/1000 
	 loss: 30.3708, MinusLogProbMetric: 30.3708, val_loss: 30.5473, val_MinusLogProbMetric: 30.5473

Epoch 81: val_loss did not improve from 30.06732
196/196 - 40s - loss: 30.3708 - MinusLogProbMetric: 30.3708 - val_loss: 30.5473 - val_MinusLogProbMetric: 30.5473 - lr: 0.0010 - 40s/epoch - 202ms/step
Epoch 82/1000
2023-09-29 07:58:30.648 
Epoch 82/1000 
	 loss: 30.2906, MinusLogProbMetric: 30.2906, val_loss: 31.1642, val_MinusLogProbMetric: 31.1642

Epoch 82: val_loss did not improve from 30.06732
196/196 - 38s - loss: 30.2906 - MinusLogProbMetric: 30.2906 - val_loss: 31.1642 - val_MinusLogProbMetric: 31.1642 - lr: 0.0010 - 38s/epoch - 191ms/step
Epoch 83/1000
2023-09-29 07:59:06.191 
Epoch 83/1000 
	 loss: 30.3351, MinusLogProbMetric: 30.3351, val_loss: 30.6527, val_MinusLogProbMetric: 30.6527

Epoch 83: val_loss did not improve from 30.06732
196/196 - 36s - loss: 30.3351 - MinusLogProbMetric: 30.3351 - val_loss: 30.6527 - val_MinusLogProbMetric: 30.6527 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 84/1000
2023-09-29 07:59:42.055 
Epoch 84/1000 
	 loss: 30.3753, MinusLogProbMetric: 30.3753, val_loss: 30.9291, val_MinusLogProbMetric: 30.9291

Epoch 84: val_loss did not improve from 30.06732
196/196 - 36s - loss: 30.3753 - MinusLogProbMetric: 30.3753 - val_loss: 30.9291 - val_MinusLogProbMetric: 30.9291 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 85/1000
2023-09-29 08:00:18.765 
Epoch 85/1000 
	 loss: 30.2532, MinusLogProbMetric: 30.2532, val_loss: 30.2703, val_MinusLogProbMetric: 30.2703

Epoch 85: val_loss did not improve from 30.06732
196/196 - 37s - loss: 30.2532 - MinusLogProbMetric: 30.2532 - val_loss: 30.2703 - val_MinusLogProbMetric: 30.2703 - lr: 0.0010 - 37s/epoch - 187ms/step
Epoch 86/1000
2023-09-29 08:00:56.566 
Epoch 86/1000 
	 loss: 30.2725, MinusLogProbMetric: 30.2725, val_loss: 29.6802, val_MinusLogProbMetric: 29.6802

Epoch 86: val_loss improved from 30.06732 to 29.68017, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_332/weights/best_weights.h5
196/196 - 39s - loss: 30.2725 - MinusLogProbMetric: 30.2725 - val_loss: 29.6802 - val_MinusLogProbMetric: 29.6802 - lr: 0.0010 - 39s/epoch - 198ms/step
Epoch 87/1000
2023-09-29 08:01:36.309 
Epoch 87/1000 
	 loss: 30.1930, MinusLogProbMetric: 30.1930, val_loss: 29.7451, val_MinusLogProbMetric: 29.7451

Epoch 87: val_loss did not improve from 29.68017
196/196 - 39s - loss: 30.1930 - MinusLogProbMetric: 30.1930 - val_loss: 29.7451 - val_MinusLogProbMetric: 29.7451 - lr: 0.0010 - 39s/epoch - 198ms/step
Epoch 88/1000
2023-09-29 08:02:14.185 
Epoch 88/1000 
	 loss: 30.1908, MinusLogProbMetric: 30.1908, val_loss: 30.8007, val_MinusLogProbMetric: 30.8007

Epoch 88: val_loss did not improve from 29.68017
196/196 - 38s - loss: 30.1908 - MinusLogProbMetric: 30.1908 - val_loss: 30.8007 - val_MinusLogProbMetric: 30.8007 - lr: 0.0010 - 38s/epoch - 193ms/step
Epoch 89/1000
2023-09-29 08:02:52.139 
Epoch 89/1000 
	 loss: 29.9126, MinusLogProbMetric: 29.9126, val_loss: 30.1451, val_MinusLogProbMetric: 30.1451

Epoch 89: val_loss did not improve from 29.68017
196/196 - 38s - loss: 29.9126 - MinusLogProbMetric: 29.9126 - val_loss: 30.1451 - val_MinusLogProbMetric: 30.1451 - lr: 0.0010 - 38s/epoch - 194ms/step
Epoch 90/1000
2023-09-29 08:03:29.209 
Epoch 90/1000 
	 loss: 30.2281, MinusLogProbMetric: 30.2281, val_loss: 30.0005, val_MinusLogProbMetric: 30.0005

Epoch 90: val_loss did not improve from 29.68017
196/196 - 37s - loss: 30.2281 - MinusLogProbMetric: 30.2281 - val_loss: 30.0005 - val_MinusLogProbMetric: 30.0005 - lr: 0.0010 - 37s/epoch - 189ms/step
Epoch 91/1000
2023-09-29 08:04:08.398 
Epoch 91/1000 
	 loss: 30.2059, MinusLogProbMetric: 30.2059, val_loss: 29.7542, val_MinusLogProbMetric: 29.7542

Epoch 91: val_loss did not improve from 29.68017
196/196 - 39s - loss: 30.2059 - MinusLogProbMetric: 30.2059 - val_loss: 29.7542 - val_MinusLogProbMetric: 29.7542 - lr: 0.0010 - 39s/epoch - 200ms/step
Epoch 92/1000
2023-09-29 08:04:45.881 
Epoch 92/1000 
	 loss: 30.1034, MinusLogProbMetric: 30.1034, val_loss: 32.1537, val_MinusLogProbMetric: 32.1537

Epoch 92: val_loss did not improve from 29.68017
196/196 - 37s - loss: 30.1034 - MinusLogProbMetric: 30.1034 - val_loss: 32.1537 - val_MinusLogProbMetric: 32.1537 - lr: 0.0010 - 37s/epoch - 191ms/step
Epoch 93/1000
2023-09-29 08:05:23.362 
Epoch 93/1000 
	 loss: 30.2268, MinusLogProbMetric: 30.2268, val_loss: 30.4883, val_MinusLogProbMetric: 30.4883

Epoch 93: val_loss did not improve from 29.68017
196/196 - 37s - loss: 30.2268 - MinusLogProbMetric: 30.2268 - val_loss: 30.4883 - val_MinusLogProbMetric: 30.4883 - lr: 0.0010 - 37s/epoch - 191ms/step
Epoch 94/1000
2023-09-29 08:05:58.343 
Epoch 94/1000 
	 loss: 30.0625, MinusLogProbMetric: 30.0625, val_loss: 30.0955, val_MinusLogProbMetric: 30.0955

Epoch 94: val_loss did not improve from 29.68017
196/196 - 35s - loss: 30.0625 - MinusLogProbMetric: 30.0625 - val_loss: 30.0955 - val_MinusLogProbMetric: 30.0955 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 95/1000
2023-09-29 08:06:32.758 
Epoch 95/1000 
	 loss: 29.9610, MinusLogProbMetric: 29.9610, val_loss: 31.4355, val_MinusLogProbMetric: 31.4355

Epoch 95: val_loss did not improve from 29.68017
196/196 - 34s - loss: 29.9610 - MinusLogProbMetric: 29.9610 - val_loss: 31.4355 - val_MinusLogProbMetric: 31.4355 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 96/1000
2023-09-29 08:07:10.382 
Epoch 96/1000 
	 loss: 29.9903, MinusLogProbMetric: 29.9903, val_loss: 30.1316, val_MinusLogProbMetric: 30.1316

Epoch 96: val_loss did not improve from 29.68017
196/196 - 38s - loss: 29.9903 - MinusLogProbMetric: 29.9903 - val_loss: 30.1316 - val_MinusLogProbMetric: 30.1316 - lr: 0.0010 - 38s/epoch - 192ms/step
Epoch 97/1000
2023-09-29 08:07:47.472 
Epoch 97/1000 
	 loss: 29.9786, MinusLogProbMetric: 29.9786, val_loss: 30.4429, val_MinusLogProbMetric: 30.4429

Epoch 97: val_loss did not improve from 29.68017
196/196 - 37s - loss: 29.9786 - MinusLogProbMetric: 29.9786 - val_loss: 30.4429 - val_MinusLogProbMetric: 30.4429 - lr: 0.0010 - 37s/epoch - 189ms/step
Epoch 98/1000
2023-09-29 08:08:28.518 
Epoch 98/1000 
	 loss: 30.1869, MinusLogProbMetric: 30.1869, val_loss: 30.5836, val_MinusLogProbMetric: 30.5836

Epoch 98: val_loss did not improve from 29.68017
196/196 - 41s - loss: 30.1869 - MinusLogProbMetric: 30.1869 - val_loss: 30.5836 - val_MinusLogProbMetric: 30.5836 - lr: 0.0010 - 41s/epoch - 209ms/step
Epoch 99/1000
2023-09-29 08:09:05.570 
Epoch 99/1000 
	 loss: 29.9763, MinusLogProbMetric: 29.9763, val_loss: 29.6471, val_MinusLogProbMetric: 29.6471

Epoch 99: val_loss improved from 29.68017 to 29.64711, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_332/weights/best_weights.h5
196/196 - 38s - loss: 29.9763 - MinusLogProbMetric: 29.9763 - val_loss: 29.6471 - val_MinusLogProbMetric: 29.6471 - lr: 0.0010 - 38s/epoch - 192ms/step
Epoch 100/1000
2023-09-29 08:09:43.913 
Epoch 100/1000 
	 loss: 30.0111, MinusLogProbMetric: 30.0111, val_loss: 30.0640, val_MinusLogProbMetric: 30.0640

Epoch 100: val_loss did not improve from 29.64711
196/196 - 38s - loss: 30.0111 - MinusLogProbMetric: 30.0111 - val_loss: 30.0640 - val_MinusLogProbMetric: 30.0640 - lr: 0.0010 - 38s/epoch - 193ms/step
Epoch 101/1000
2023-09-29 08:10:21.541 
Epoch 101/1000 
	 loss: 29.8103, MinusLogProbMetric: 29.8103, val_loss: 30.2823, val_MinusLogProbMetric: 30.2823

Epoch 101: val_loss did not improve from 29.64711
196/196 - 38s - loss: 29.8103 - MinusLogProbMetric: 29.8103 - val_loss: 30.2823 - val_MinusLogProbMetric: 30.2823 - lr: 0.0010 - 38s/epoch - 192ms/step
Epoch 102/1000
2023-09-29 08:11:00.895 
Epoch 102/1000 
	 loss: 29.8545, MinusLogProbMetric: 29.8545, val_loss: 30.1553, val_MinusLogProbMetric: 30.1553

Epoch 102: val_loss did not improve from 29.64711
196/196 - 39s - loss: 29.8545 - MinusLogProbMetric: 29.8545 - val_loss: 30.1553 - val_MinusLogProbMetric: 30.1553 - lr: 0.0010 - 39s/epoch - 201ms/step
Epoch 103/1000
2023-09-29 08:11:39.558 
Epoch 103/1000 
	 loss: 29.9993, MinusLogProbMetric: 29.9993, val_loss: 30.7199, val_MinusLogProbMetric: 30.7199

Epoch 103: val_loss did not improve from 29.64711
196/196 - 39s - loss: 29.9993 - MinusLogProbMetric: 29.9993 - val_loss: 30.7199 - val_MinusLogProbMetric: 30.7199 - lr: 0.0010 - 39s/epoch - 197ms/step
Epoch 104/1000
2023-09-29 08:12:16.605 
Epoch 104/1000 
	 loss: 29.7317, MinusLogProbMetric: 29.7317, val_loss: 29.7793, val_MinusLogProbMetric: 29.7793

Epoch 104: val_loss did not improve from 29.64711
196/196 - 37s - loss: 29.7317 - MinusLogProbMetric: 29.7317 - val_loss: 29.7793 - val_MinusLogProbMetric: 29.7793 - lr: 0.0010 - 37s/epoch - 189ms/step
Epoch 105/1000
2023-09-29 08:12:53.653 
Epoch 105/1000 
	 loss: 29.7827, MinusLogProbMetric: 29.7827, val_loss: 29.7472, val_MinusLogProbMetric: 29.7472

Epoch 105: val_loss did not improve from 29.64711
196/196 - 37s - loss: 29.7827 - MinusLogProbMetric: 29.7827 - val_loss: 29.7472 - val_MinusLogProbMetric: 29.7472 - lr: 0.0010 - 37s/epoch - 189ms/step
Epoch 106/1000
2023-09-29 08:13:31.225 
Epoch 106/1000 
	 loss: 29.7749, MinusLogProbMetric: 29.7749, val_loss: 29.6454, val_MinusLogProbMetric: 29.6454

Epoch 106: val_loss improved from 29.64711 to 29.64541, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_332/weights/best_weights.h5
196/196 - 38s - loss: 29.7749 - MinusLogProbMetric: 29.7749 - val_loss: 29.6454 - val_MinusLogProbMetric: 29.6454 - lr: 0.0010 - 38s/epoch - 195ms/step
Epoch 107/1000
2023-09-29 08:14:09.581 
Epoch 107/1000 
	 loss: 29.8404, MinusLogProbMetric: 29.8404, val_loss: 29.6352, val_MinusLogProbMetric: 29.6352

Epoch 107: val_loss improved from 29.64541 to 29.63525, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_332/weights/best_weights.h5
196/196 - 38s - loss: 29.8404 - MinusLogProbMetric: 29.8404 - val_loss: 29.6352 - val_MinusLogProbMetric: 29.6352 - lr: 0.0010 - 38s/epoch - 196ms/step
Epoch 108/1000
2023-09-29 08:14:48.577 
Epoch 108/1000 
	 loss: 29.9656, MinusLogProbMetric: 29.9656, val_loss: 29.7805, val_MinusLogProbMetric: 29.7805

Epoch 108: val_loss did not improve from 29.63525
196/196 - 38s - loss: 29.9656 - MinusLogProbMetric: 29.9656 - val_loss: 29.7805 - val_MinusLogProbMetric: 29.7805 - lr: 0.0010 - 38s/epoch - 196ms/step
Epoch 109/1000
2023-09-29 08:15:25.847 
Epoch 109/1000 
	 loss: 29.6419, MinusLogProbMetric: 29.6419, val_loss: 30.2928, val_MinusLogProbMetric: 30.2928

Epoch 109: val_loss did not improve from 29.63525
196/196 - 37s - loss: 29.6419 - MinusLogProbMetric: 29.6419 - val_loss: 30.2928 - val_MinusLogProbMetric: 30.2928 - lr: 0.0010 - 37s/epoch - 190ms/step
Epoch 110/1000
2023-09-29 08:16:03.114 
Epoch 110/1000 
	 loss: 29.6428, MinusLogProbMetric: 29.6428, val_loss: 29.5016, val_MinusLogProbMetric: 29.5016

Epoch 110: val_loss improved from 29.63525 to 29.50156, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_332/weights/best_weights.h5
196/196 - 38s - loss: 29.6428 - MinusLogProbMetric: 29.6428 - val_loss: 29.5016 - val_MinusLogProbMetric: 29.5016 - lr: 0.0010 - 38s/epoch - 193ms/step
Epoch 111/1000
2023-09-29 08:16:41.303 
Epoch 111/1000 
	 loss: 29.7832, MinusLogProbMetric: 29.7832, val_loss: 29.6379, val_MinusLogProbMetric: 29.6379

Epoch 111: val_loss did not improve from 29.50156
196/196 - 38s - loss: 29.7832 - MinusLogProbMetric: 29.7832 - val_loss: 29.6379 - val_MinusLogProbMetric: 29.6379 - lr: 0.0010 - 38s/epoch - 192ms/step
Epoch 112/1000
2023-09-29 08:17:18.779 
Epoch 112/1000 
	 loss: 29.5856, MinusLogProbMetric: 29.5856, val_loss: 29.8771, val_MinusLogProbMetric: 29.8771

Epoch 112: val_loss did not improve from 29.50156
196/196 - 37s - loss: 29.5856 - MinusLogProbMetric: 29.5856 - val_loss: 29.8771 - val_MinusLogProbMetric: 29.8771 - lr: 0.0010 - 37s/epoch - 191ms/step
Epoch 113/1000
2023-09-29 08:17:56.566 
Epoch 113/1000 
	 loss: 29.5881, MinusLogProbMetric: 29.5881, val_loss: 30.3882, val_MinusLogProbMetric: 30.3882

Epoch 113: val_loss did not improve from 29.50156
196/196 - 38s - loss: 29.5881 - MinusLogProbMetric: 29.5881 - val_loss: 30.3882 - val_MinusLogProbMetric: 30.3882 - lr: 0.0010 - 38s/epoch - 193ms/step
Epoch 114/1000
2023-09-29 08:18:34.903 
Epoch 114/1000 
	 loss: 29.6126, MinusLogProbMetric: 29.6126, val_loss: 30.2582, val_MinusLogProbMetric: 30.2582

Epoch 114: val_loss did not improve from 29.50156
196/196 - 38s - loss: 29.6126 - MinusLogProbMetric: 29.6126 - val_loss: 30.2582 - val_MinusLogProbMetric: 30.2582 - lr: 0.0010 - 38s/epoch - 196ms/step
Epoch 115/1000
2023-09-29 08:19:10.951 
Epoch 115/1000 
	 loss: 29.5970, MinusLogProbMetric: 29.5970, val_loss: 30.0008, val_MinusLogProbMetric: 30.0008

Epoch 115: val_loss did not improve from 29.50156
196/196 - 36s - loss: 29.5970 - MinusLogProbMetric: 29.5970 - val_loss: 30.0008 - val_MinusLogProbMetric: 30.0008 - lr: 0.0010 - 36s/epoch - 184ms/step
Epoch 116/1000
2023-09-29 08:19:46.507 
Epoch 116/1000 
	 loss: 29.5504, MinusLogProbMetric: 29.5504, val_loss: 30.0241, val_MinusLogProbMetric: 30.0241

Epoch 116: val_loss did not improve from 29.50156
196/196 - 36s - loss: 29.5504 - MinusLogProbMetric: 29.5504 - val_loss: 30.0241 - val_MinusLogProbMetric: 30.0241 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 117/1000
2023-09-29 08:20:22.357 
Epoch 117/1000 
	 loss: 29.6608, MinusLogProbMetric: 29.6608, val_loss: 29.8173, val_MinusLogProbMetric: 29.8173

Epoch 117: val_loss did not improve from 29.50156
196/196 - 36s - loss: 29.6608 - MinusLogProbMetric: 29.6608 - val_loss: 29.8173 - val_MinusLogProbMetric: 29.8173 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 118/1000
2023-09-29 08:20:59.571 
Epoch 118/1000 
	 loss: 29.5977, MinusLogProbMetric: 29.5977, val_loss: 29.6168, val_MinusLogProbMetric: 29.6168

Epoch 118: val_loss did not improve from 29.50156
196/196 - 37s - loss: 29.5977 - MinusLogProbMetric: 29.5977 - val_loss: 29.6168 - val_MinusLogProbMetric: 29.6168 - lr: 0.0010 - 37s/epoch - 190ms/step
Epoch 119/1000
2023-09-29 08:21:38.306 
Epoch 119/1000 
	 loss: 29.7054, MinusLogProbMetric: 29.7054, val_loss: 30.3182, val_MinusLogProbMetric: 30.3182

Epoch 119: val_loss did not improve from 29.50156
196/196 - 39s - loss: 29.7054 - MinusLogProbMetric: 29.7054 - val_loss: 30.3182 - val_MinusLogProbMetric: 30.3182 - lr: 0.0010 - 39s/epoch - 198ms/step
Epoch 120/1000
2023-09-29 08:22:16.294 
Epoch 120/1000 
	 loss: 29.4558, MinusLogProbMetric: 29.4558, val_loss: 30.0999, val_MinusLogProbMetric: 30.0999

Epoch 120: val_loss did not improve from 29.50156
196/196 - 38s - loss: 29.4558 - MinusLogProbMetric: 29.4558 - val_loss: 30.0999 - val_MinusLogProbMetric: 30.0999 - lr: 0.0010 - 38s/epoch - 194ms/step
Epoch 121/1000
2023-09-29 08:22:53.323 
Epoch 121/1000 
	 loss: 29.7544, MinusLogProbMetric: 29.7544, val_loss: 29.8040, val_MinusLogProbMetric: 29.8040

Epoch 121: val_loss did not improve from 29.50156
196/196 - 37s - loss: 29.7544 - MinusLogProbMetric: 29.7544 - val_loss: 29.8040 - val_MinusLogProbMetric: 29.8040 - lr: 0.0010 - 37s/epoch - 189ms/step
Epoch 122/1000
2023-09-29 08:23:30.236 
Epoch 122/1000 
	 loss: 29.5004, MinusLogProbMetric: 29.5004, val_loss: 29.6400, val_MinusLogProbMetric: 29.6400

Epoch 122: val_loss did not improve from 29.50156
196/196 - 37s - loss: 29.5004 - MinusLogProbMetric: 29.5004 - val_loss: 29.6400 - val_MinusLogProbMetric: 29.6400 - lr: 0.0010 - 37s/epoch - 188ms/step
Epoch 123/1000
2023-09-29 08:24:07.833 
Epoch 123/1000 
	 loss: 29.5500, MinusLogProbMetric: 29.5500, val_loss: 30.0013, val_MinusLogProbMetric: 30.0013

Epoch 123: val_loss did not improve from 29.50156
196/196 - 38s - loss: 29.5500 - MinusLogProbMetric: 29.5500 - val_loss: 30.0013 - val_MinusLogProbMetric: 30.0013 - lr: 0.0010 - 38s/epoch - 192ms/step
Epoch 124/1000
2023-09-29 08:24:46.089 
Epoch 124/1000 
	 loss: 29.3606, MinusLogProbMetric: 29.3606, val_loss: 30.4212, val_MinusLogProbMetric: 30.4212

Epoch 124: val_loss did not improve from 29.50156
196/196 - 38s - loss: 29.3606 - MinusLogProbMetric: 29.3606 - val_loss: 30.4212 - val_MinusLogProbMetric: 30.4212 - lr: 0.0010 - 38s/epoch - 195ms/step
Epoch 125/1000
2023-09-29 08:25:24.583 
Epoch 125/1000 
	 loss: 29.5808, MinusLogProbMetric: 29.5808, val_loss: 29.7548, val_MinusLogProbMetric: 29.7548

Epoch 125: val_loss did not improve from 29.50156
196/196 - 38s - loss: 29.5808 - MinusLogProbMetric: 29.5808 - val_loss: 29.7548 - val_MinusLogProbMetric: 29.7548 - lr: 0.0010 - 38s/epoch - 196ms/step
Epoch 126/1000
2023-09-29 08:26:02.671 
Epoch 126/1000 
	 loss: 29.3017, MinusLogProbMetric: 29.3017, val_loss: 30.0614, val_MinusLogProbMetric: 30.0614

Epoch 126: val_loss did not improve from 29.50156
196/196 - 38s - loss: 29.3017 - MinusLogProbMetric: 29.3017 - val_loss: 30.0614 - val_MinusLogProbMetric: 30.0614 - lr: 0.0010 - 38s/epoch - 194ms/step
Epoch 127/1000
2023-09-29 08:26:37.035 
Epoch 127/1000 
	 loss: 29.3128, MinusLogProbMetric: 29.3128, val_loss: 30.2263, val_MinusLogProbMetric: 30.2263

Epoch 127: val_loss did not improve from 29.50156
196/196 - 34s - loss: 29.3128 - MinusLogProbMetric: 29.3128 - val_loss: 30.2263 - val_MinusLogProbMetric: 30.2263 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 128/1000
2023-09-29 08:27:12.312 
Epoch 128/1000 
	 loss: 29.5994, MinusLogProbMetric: 29.5994, val_loss: 30.4745, val_MinusLogProbMetric: 30.4745

Epoch 128: val_loss did not improve from 29.50156
196/196 - 35s - loss: 29.5994 - MinusLogProbMetric: 29.5994 - val_loss: 30.4745 - val_MinusLogProbMetric: 30.4745 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 129/1000
2023-09-29 08:27:50.196 
Epoch 129/1000 
	 loss: 29.5148, MinusLogProbMetric: 29.5148, val_loss: 29.6609, val_MinusLogProbMetric: 29.6609

Epoch 129: val_loss did not improve from 29.50156
196/196 - 38s - loss: 29.5148 - MinusLogProbMetric: 29.5148 - val_loss: 29.6609 - val_MinusLogProbMetric: 29.6609 - lr: 0.0010 - 38s/epoch - 193ms/step
Epoch 130/1000
2023-09-29 08:28:30.855 
Epoch 130/1000 
	 loss: 29.2942, MinusLogProbMetric: 29.2942, val_loss: 29.6789, val_MinusLogProbMetric: 29.6789

Epoch 130: val_loss did not improve from 29.50156
196/196 - 41s - loss: 29.2942 - MinusLogProbMetric: 29.2942 - val_loss: 29.6789 - val_MinusLogProbMetric: 29.6789 - lr: 0.0010 - 41s/epoch - 207ms/step
Epoch 131/1000
2023-09-29 08:29:08.212 
Epoch 131/1000 
	 loss: 29.3253, MinusLogProbMetric: 29.3253, val_loss: 29.7797, val_MinusLogProbMetric: 29.7797

Epoch 131: val_loss did not improve from 29.50156
196/196 - 37s - loss: 29.3253 - MinusLogProbMetric: 29.3253 - val_loss: 29.7797 - val_MinusLogProbMetric: 29.7797 - lr: 0.0010 - 37s/epoch - 191ms/step
Epoch 132/1000
2023-09-29 08:29:43.021 
Epoch 132/1000 
	 loss: 29.5918, MinusLogProbMetric: 29.5918, val_loss: 29.9548, val_MinusLogProbMetric: 29.9548

Epoch 132: val_loss did not improve from 29.50156
196/196 - 35s - loss: 29.5918 - MinusLogProbMetric: 29.5918 - val_loss: 29.9548 - val_MinusLogProbMetric: 29.9548 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 133/1000
2023-09-29 08:30:18.171 
Epoch 133/1000 
	 loss: 29.3667, MinusLogProbMetric: 29.3667, val_loss: 29.2908, val_MinusLogProbMetric: 29.2908

Epoch 133: val_loss improved from 29.50156 to 29.29077, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_332/weights/best_weights.h5
196/196 - 36s - loss: 29.3667 - MinusLogProbMetric: 29.3667 - val_loss: 29.2908 - val_MinusLogProbMetric: 29.2908 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 134/1000
2023-09-29 08:30:56.792 
Epoch 134/1000 
	 loss: 29.3634, MinusLogProbMetric: 29.3634, val_loss: 29.1297, val_MinusLogProbMetric: 29.1297

Epoch 134: val_loss improved from 29.29077 to 29.12975, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_332/weights/best_weights.h5
196/196 - 39s - loss: 29.3634 - MinusLogProbMetric: 29.3634 - val_loss: 29.1297 - val_MinusLogProbMetric: 29.1297 - lr: 0.0010 - 39s/epoch - 197ms/step
Epoch 135/1000
2023-09-29 08:31:38.595 
Epoch 135/1000 
	 loss: 29.3752, MinusLogProbMetric: 29.3752, val_loss: 29.9203, val_MinusLogProbMetric: 29.9203

Epoch 135: val_loss did not improve from 29.12975
196/196 - 41s - loss: 29.3752 - MinusLogProbMetric: 29.3752 - val_loss: 29.9203 - val_MinusLogProbMetric: 29.9203 - lr: 0.0010 - 41s/epoch - 210ms/step
Epoch 136/1000
2023-09-29 08:32:18.315 
Epoch 136/1000 
	 loss: 29.7313, MinusLogProbMetric: 29.7313, val_loss: 29.5876, val_MinusLogProbMetric: 29.5876

Epoch 136: val_loss did not improve from 29.12975
196/196 - 40s - loss: 29.7313 - MinusLogProbMetric: 29.7313 - val_loss: 29.5876 - val_MinusLogProbMetric: 29.5876 - lr: 0.0010 - 40s/epoch - 203ms/step
Epoch 137/1000
2023-09-29 08:32:54.593 
Epoch 137/1000 
	 loss: 29.2040, MinusLogProbMetric: 29.2040, val_loss: 29.2823, val_MinusLogProbMetric: 29.2823

Epoch 137: val_loss did not improve from 29.12975
196/196 - 36s - loss: 29.2040 - MinusLogProbMetric: 29.2040 - val_loss: 29.2823 - val_MinusLogProbMetric: 29.2823 - lr: 0.0010 - 36s/epoch - 185ms/step
Epoch 138/1000
2023-09-29 08:33:29.539 
Epoch 138/1000 
	 loss: 29.1203, MinusLogProbMetric: 29.1203, val_loss: 28.9320, val_MinusLogProbMetric: 28.9320

Epoch 138: val_loss improved from 29.12975 to 28.93195, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_332/weights/best_weights.h5
196/196 - 36s - loss: 29.1203 - MinusLogProbMetric: 29.1203 - val_loss: 28.9320 - val_MinusLogProbMetric: 28.9320 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 139/1000
2023-09-29 08:34:05.412 
Epoch 139/1000 
	 loss: 29.1974, MinusLogProbMetric: 29.1974, val_loss: 30.1121, val_MinusLogProbMetric: 30.1121

Epoch 139: val_loss did not improve from 28.93195
196/196 - 35s - loss: 29.1974 - MinusLogProbMetric: 29.1974 - val_loss: 30.1121 - val_MinusLogProbMetric: 30.1121 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 140/1000
2023-09-29 08:34:44.963 
Epoch 140/1000 
	 loss: 29.2223, MinusLogProbMetric: 29.2223, val_loss: 29.2446, val_MinusLogProbMetric: 29.2446

Epoch 140: val_loss did not improve from 28.93195
196/196 - 40s - loss: 29.2223 - MinusLogProbMetric: 29.2223 - val_loss: 29.2446 - val_MinusLogProbMetric: 29.2446 - lr: 0.0010 - 40s/epoch - 202ms/step
Epoch 141/1000
2023-09-29 08:35:28.807 
Epoch 141/1000 
	 loss: 29.3285, MinusLogProbMetric: 29.3285, val_loss: 29.8347, val_MinusLogProbMetric: 29.8347

Epoch 141: val_loss did not improve from 28.93195
196/196 - 44s - loss: 29.3285 - MinusLogProbMetric: 29.3285 - val_loss: 29.8347 - val_MinusLogProbMetric: 29.8347 - lr: 0.0010 - 44s/epoch - 224ms/step
Epoch 142/1000
2023-09-29 08:36:12.226 
Epoch 142/1000 
	 loss: 29.2844, MinusLogProbMetric: 29.2844, val_loss: 29.5677, val_MinusLogProbMetric: 29.5677

Epoch 142: val_loss did not improve from 28.93195
196/196 - 43s - loss: 29.2844 - MinusLogProbMetric: 29.2844 - val_loss: 29.5677 - val_MinusLogProbMetric: 29.5677 - lr: 0.0010 - 43s/epoch - 221ms/step
Epoch 143/1000
2023-09-29 08:36:51.344 
Epoch 143/1000 
	 loss: 29.7198, MinusLogProbMetric: 29.7198, val_loss: 29.7861, val_MinusLogProbMetric: 29.7861

Epoch 143: val_loss did not improve from 28.93195
196/196 - 39s - loss: 29.7198 - MinusLogProbMetric: 29.7198 - val_loss: 29.7861 - val_MinusLogProbMetric: 29.7861 - lr: 0.0010 - 39s/epoch - 200ms/step
Epoch 144/1000
2023-09-29 08:37:27.782 
Epoch 144/1000 
	 loss: 29.1183, MinusLogProbMetric: 29.1183, val_loss: 28.9043, val_MinusLogProbMetric: 28.9043

Epoch 144: val_loss improved from 28.93195 to 28.90427, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_332/weights/best_weights.h5
196/196 - 37s - loss: 29.1183 - MinusLogProbMetric: 29.1183 - val_loss: 28.9043 - val_MinusLogProbMetric: 28.9043 - lr: 0.0010 - 37s/epoch - 189ms/step
Epoch 145/1000
2023-09-29 08:38:03.806 
Epoch 145/1000 
	 loss: 29.0306, MinusLogProbMetric: 29.0306, val_loss: 29.6138, val_MinusLogProbMetric: 29.6138

Epoch 145: val_loss did not improve from 28.90427
196/196 - 35s - loss: 29.0306 - MinusLogProbMetric: 29.0306 - val_loss: 29.6138 - val_MinusLogProbMetric: 29.6138 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 146/1000
2023-09-29 08:38:39.051 
Epoch 146/1000 
	 loss: 29.2469, MinusLogProbMetric: 29.2469, val_loss: 29.0227, val_MinusLogProbMetric: 29.0227

Epoch 146: val_loss did not improve from 28.90427
196/196 - 35s - loss: 29.2469 - MinusLogProbMetric: 29.2469 - val_loss: 29.0227 - val_MinusLogProbMetric: 29.0227 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 147/1000
2023-09-29 08:39:19.074 
Epoch 147/1000 
	 loss: 29.1903, MinusLogProbMetric: 29.1903, val_loss: 29.1126, val_MinusLogProbMetric: 29.1126

Epoch 147: val_loss did not improve from 28.90427
196/196 - 40s - loss: 29.1903 - MinusLogProbMetric: 29.1903 - val_loss: 29.1126 - val_MinusLogProbMetric: 29.1126 - lr: 0.0010 - 40s/epoch - 204ms/step
Epoch 148/1000
2023-09-29 08:40:02.690 
Epoch 148/1000 
	 loss: 29.0066, MinusLogProbMetric: 29.0066, val_loss: 29.9142, val_MinusLogProbMetric: 29.9142

Epoch 148: val_loss did not improve from 28.90427
196/196 - 44s - loss: 29.0066 - MinusLogProbMetric: 29.0066 - val_loss: 29.9142 - val_MinusLogProbMetric: 29.9142 - lr: 0.0010 - 44s/epoch - 222ms/step
Epoch 149/1000
2023-09-29 08:40:42.696 
Epoch 149/1000 
	 loss: 29.2025, MinusLogProbMetric: 29.2025, val_loss: 29.1791, val_MinusLogProbMetric: 29.1791

Epoch 149: val_loss did not improve from 28.90427
196/196 - 40s - loss: 29.2025 - MinusLogProbMetric: 29.2025 - val_loss: 29.1791 - val_MinusLogProbMetric: 29.1791 - lr: 0.0010 - 40s/epoch - 204ms/step
Epoch 150/1000
2023-09-29 08:41:18.660 
Epoch 150/1000 
	 loss: 29.0493, MinusLogProbMetric: 29.0493, val_loss: 29.6475, val_MinusLogProbMetric: 29.6475

Epoch 150: val_loss did not improve from 28.90427
196/196 - 36s - loss: 29.0493 - MinusLogProbMetric: 29.0493 - val_loss: 29.6475 - val_MinusLogProbMetric: 29.6475 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 151/1000
2023-09-29 08:41:54.649 
Epoch 151/1000 
	 loss: 29.0374, MinusLogProbMetric: 29.0374, val_loss: 29.2878, val_MinusLogProbMetric: 29.2878

Epoch 151: val_loss did not improve from 28.90427
196/196 - 36s - loss: 29.0374 - MinusLogProbMetric: 29.0374 - val_loss: 29.2878 - val_MinusLogProbMetric: 29.2878 - lr: 0.0010 - 36s/epoch - 184ms/step
Epoch 152/1000
2023-09-29 08:42:34.444 
Epoch 152/1000 
	 loss: 29.2426, MinusLogProbMetric: 29.2426, val_loss: 29.4370, val_MinusLogProbMetric: 29.4370

Epoch 152: val_loss did not improve from 28.90427
196/196 - 40s - loss: 29.2426 - MinusLogProbMetric: 29.2426 - val_loss: 29.4370 - val_MinusLogProbMetric: 29.4370 - lr: 0.0010 - 40s/epoch - 203ms/step
Epoch 153/1000
2023-09-29 08:43:13.588 
Epoch 153/1000 
	 loss: 29.2012, MinusLogProbMetric: 29.2012, val_loss: 29.3034, val_MinusLogProbMetric: 29.3034

Epoch 153: val_loss did not improve from 28.90427
196/196 - 39s - loss: 29.2012 - MinusLogProbMetric: 29.2012 - val_loss: 29.3034 - val_MinusLogProbMetric: 29.3034 - lr: 0.0010 - 39s/epoch - 200ms/step
Epoch 154/1000
2023-09-29 08:43:55.616 
Epoch 154/1000 
	 loss: 29.0297, MinusLogProbMetric: 29.0297, val_loss: 29.3568, val_MinusLogProbMetric: 29.3568

Epoch 154: val_loss did not improve from 28.90427
196/196 - 42s - loss: 29.0297 - MinusLogProbMetric: 29.0297 - val_loss: 29.3568 - val_MinusLogProbMetric: 29.3568 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 155/1000
2023-09-29 08:44:34.754 
Epoch 155/1000 
	 loss: 29.1215, MinusLogProbMetric: 29.1215, val_loss: 29.1358, val_MinusLogProbMetric: 29.1358

Epoch 155: val_loss did not improve from 28.90427
196/196 - 39s - loss: 29.1215 - MinusLogProbMetric: 29.1215 - val_loss: 29.1358 - val_MinusLogProbMetric: 29.1358 - lr: 0.0010 - 39s/epoch - 200ms/step
Epoch 156/1000
2023-09-29 08:45:11.611 
Epoch 156/1000 
	 loss: 29.0487, MinusLogProbMetric: 29.0487, val_loss: 29.0752, val_MinusLogProbMetric: 29.0752

Epoch 156: val_loss did not improve from 28.90427
196/196 - 37s - loss: 29.0487 - MinusLogProbMetric: 29.0487 - val_loss: 29.0752 - val_MinusLogProbMetric: 29.0752 - lr: 0.0010 - 37s/epoch - 188ms/step
Epoch 157/1000
2023-09-29 08:45:46.342 
Epoch 157/1000 
	 loss: 29.3316, MinusLogProbMetric: 29.3316, val_loss: 30.5587, val_MinusLogProbMetric: 30.5587

Epoch 157: val_loss did not improve from 28.90427
196/196 - 35s - loss: 29.3316 - MinusLogProbMetric: 29.3316 - val_loss: 30.5587 - val_MinusLogProbMetric: 30.5587 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 158/1000
2023-09-29 08:46:23.201 
Epoch 158/1000 
	 loss: 28.9760, MinusLogProbMetric: 28.9760, val_loss: 29.6080, val_MinusLogProbMetric: 29.6080

Epoch 158: val_loss did not improve from 28.90427
196/196 - 37s - loss: 28.9760 - MinusLogProbMetric: 28.9760 - val_loss: 29.6080 - val_MinusLogProbMetric: 29.6080 - lr: 0.0010 - 37s/epoch - 188ms/step
Epoch 159/1000
2023-09-29 08:47:04.887 
Epoch 159/1000 
	 loss: 29.1129, MinusLogProbMetric: 29.1129, val_loss: 30.0131, val_MinusLogProbMetric: 30.0131

Epoch 159: val_loss did not improve from 28.90427
196/196 - 42s - loss: 29.1129 - MinusLogProbMetric: 29.1129 - val_loss: 30.0131 - val_MinusLogProbMetric: 30.0131 - lr: 0.0010 - 42s/epoch - 213ms/step
Epoch 160/1000
2023-09-29 08:47:48.303 
Epoch 160/1000 
	 loss: 29.2123, MinusLogProbMetric: 29.2123, val_loss: 30.2054, val_MinusLogProbMetric: 30.2054

Epoch 160: val_loss did not improve from 28.90427
196/196 - 43s - loss: 29.2123 - MinusLogProbMetric: 29.2123 - val_loss: 30.2054 - val_MinusLogProbMetric: 30.2054 - lr: 0.0010 - 43s/epoch - 221ms/step
Epoch 161/1000
2023-09-29 08:48:31.918 
Epoch 161/1000 
	 loss: 29.2265, MinusLogProbMetric: 29.2265, val_loss: 29.3023, val_MinusLogProbMetric: 29.3023

Epoch 161: val_loss did not improve from 28.90427
196/196 - 44s - loss: 29.2265 - MinusLogProbMetric: 29.2265 - val_loss: 29.3023 - val_MinusLogProbMetric: 29.3023 - lr: 0.0010 - 44s/epoch - 222ms/step
Epoch 162/1000
2023-09-29 08:49:13.257 
Epoch 162/1000 
	 loss: 28.9530, MinusLogProbMetric: 28.9530, val_loss: 28.8321, val_MinusLogProbMetric: 28.8321

Epoch 162: val_loss improved from 28.90427 to 28.83208, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_332/weights/best_weights.h5
196/196 - 42s - loss: 28.9530 - MinusLogProbMetric: 28.9530 - val_loss: 28.8321 - val_MinusLogProbMetric: 28.8321 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 163/1000
2023-09-29 08:49:57.646 
Epoch 163/1000 
	 loss: 29.0089, MinusLogProbMetric: 29.0089, val_loss: 30.7170, val_MinusLogProbMetric: 30.7170

Epoch 163: val_loss did not improve from 28.83208
196/196 - 44s - loss: 29.0089 - MinusLogProbMetric: 29.0089 - val_loss: 30.7170 - val_MinusLogProbMetric: 30.7170 - lr: 0.0010 - 44s/epoch - 222ms/step
Epoch 164/1000
2023-09-29 08:50:38.322 
Epoch 164/1000 
	 loss: 29.0560, MinusLogProbMetric: 29.0560, val_loss: 29.6856, val_MinusLogProbMetric: 29.6856

Epoch 164: val_loss did not improve from 28.83208
196/196 - 41s - loss: 29.0560 - MinusLogProbMetric: 29.0560 - val_loss: 29.6856 - val_MinusLogProbMetric: 29.6856 - lr: 0.0010 - 41s/epoch - 207ms/step
Epoch 165/1000
2023-09-29 08:51:20.271 
Epoch 165/1000 
	 loss: 29.1471, MinusLogProbMetric: 29.1471, val_loss: 29.5547, val_MinusLogProbMetric: 29.5547

Epoch 165: val_loss did not improve from 28.83208
196/196 - 42s - loss: 29.1471 - MinusLogProbMetric: 29.1471 - val_loss: 29.5547 - val_MinusLogProbMetric: 29.5547 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 166/1000
2023-09-29 08:52:04.965 
Epoch 166/1000 
	 loss: 28.9958, MinusLogProbMetric: 28.9958, val_loss: 29.0546, val_MinusLogProbMetric: 29.0546

Epoch 166: val_loss did not improve from 28.83208
196/196 - 45s - loss: 28.9958 - MinusLogProbMetric: 28.9958 - val_loss: 29.0546 - val_MinusLogProbMetric: 29.0546 - lr: 0.0010 - 45s/epoch - 228ms/step
Epoch 167/1000
2023-09-29 08:52:49.269 
Epoch 167/1000 
	 loss: 29.0959, MinusLogProbMetric: 29.0959, val_loss: 30.1020, val_MinusLogProbMetric: 30.1020

Epoch 167: val_loss did not improve from 28.83208
196/196 - 44s - loss: 29.0959 - MinusLogProbMetric: 29.0959 - val_loss: 30.1020 - val_MinusLogProbMetric: 30.1020 - lr: 0.0010 - 44s/epoch - 226ms/step
Epoch 168/1000
2023-09-29 08:53:33.550 
Epoch 168/1000 
	 loss: 29.0064, MinusLogProbMetric: 29.0064, val_loss: 30.2943, val_MinusLogProbMetric: 30.2943

Epoch 168: val_loss did not improve from 28.83208
196/196 - 44s - loss: 29.0064 - MinusLogProbMetric: 29.0064 - val_loss: 30.2943 - val_MinusLogProbMetric: 30.2943 - lr: 0.0010 - 44s/epoch - 226ms/step
Epoch 169/1000
2023-09-29 08:54:18.207 
Epoch 169/1000 
	 loss: 28.9441, MinusLogProbMetric: 28.9441, val_loss: 29.4479, val_MinusLogProbMetric: 29.4479

Epoch 169: val_loss did not improve from 28.83208
196/196 - 45s - loss: 28.9441 - MinusLogProbMetric: 28.9441 - val_loss: 29.4479 - val_MinusLogProbMetric: 29.4479 - lr: 0.0010 - 45s/epoch - 228ms/step
Epoch 170/1000
2023-09-29 08:55:02.925 
Epoch 170/1000 
	 loss: 29.0581, MinusLogProbMetric: 29.0581, val_loss: 31.3920, val_MinusLogProbMetric: 31.3920

Epoch 170: val_loss did not improve from 28.83208
196/196 - 45s - loss: 29.0581 - MinusLogProbMetric: 29.0581 - val_loss: 31.3920 - val_MinusLogProbMetric: 31.3920 - lr: 0.0010 - 45s/epoch - 228ms/step
Epoch 171/1000
2023-09-29 08:55:47.834 
Epoch 171/1000 
	 loss: 28.9822, MinusLogProbMetric: 28.9822, val_loss: 29.4187, val_MinusLogProbMetric: 29.4187

Epoch 171: val_loss did not improve from 28.83208
196/196 - 45s - loss: 28.9822 - MinusLogProbMetric: 28.9822 - val_loss: 29.4187 - val_MinusLogProbMetric: 29.4187 - lr: 0.0010 - 45s/epoch - 229ms/step
Epoch 172/1000
2023-09-29 08:56:32.262 
Epoch 172/1000 
	 loss: 28.8573, MinusLogProbMetric: 28.8573, val_loss: 29.5322, val_MinusLogProbMetric: 29.5322

Epoch 172: val_loss did not improve from 28.83208
196/196 - 44s - loss: 28.8573 - MinusLogProbMetric: 28.8573 - val_loss: 29.5322 - val_MinusLogProbMetric: 29.5322 - lr: 0.0010 - 44s/epoch - 227ms/step
Epoch 173/1000
2023-09-29 08:57:17.332 
Epoch 173/1000 
	 loss: 28.8898, MinusLogProbMetric: 28.8898, val_loss: 28.8507, val_MinusLogProbMetric: 28.8507

Epoch 173: val_loss did not improve from 28.83208
196/196 - 45s - loss: 28.8898 - MinusLogProbMetric: 28.8898 - val_loss: 28.8507 - val_MinusLogProbMetric: 28.8507 - lr: 0.0010 - 45s/epoch - 230ms/step
Epoch 174/1000
2023-09-29 08:58:02.080 
Epoch 174/1000 
	 loss: 28.9096, MinusLogProbMetric: 28.9096, val_loss: 29.4415, val_MinusLogProbMetric: 29.4415

Epoch 174: val_loss did not improve from 28.83208
196/196 - 45s - loss: 28.9096 - MinusLogProbMetric: 28.9096 - val_loss: 29.4415 - val_MinusLogProbMetric: 29.4415 - lr: 0.0010 - 45s/epoch - 228ms/step
Epoch 175/1000
2023-09-29 08:58:45.545 
Epoch 175/1000 
	 loss: 28.9044, MinusLogProbMetric: 28.9044, val_loss: 29.2271, val_MinusLogProbMetric: 29.2271

Epoch 175: val_loss did not improve from 28.83208
196/196 - 43s - loss: 28.9044 - MinusLogProbMetric: 28.9044 - val_loss: 29.2271 - val_MinusLogProbMetric: 29.2271 - lr: 0.0010 - 43s/epoch - 222ms/step
Epoch 176/1000
2023-09-29 08:59:26.832 
Epoch 176/1000 
	 loss: 29.1452, MinusLogProbMetric: 29.1452, val_loss: 29.2942, val_MinusLogProbMetric: 29.2942

Epoch 176: val_loss did not improve from 28.83208
196/196 - 41s - loss: 29.1452 - MinusLogProbMetric: 29.1452 - val_loss: 29.2942 - val_MinusLogProbMetric: 29.2942 - lr: 0.0010 - 41s/epoch - 211ms/step
Epoch 177/1000
2023-09-29 09:00:06.963 
Epoch 177/1000 
	 loss: 28.8498, MinusLogProbMetric: 28.8498, val_loss: 29.3795, val_MinusLogProbMetric: 29.3795

Epoch 177: val_loss did not improve from 28.83208
196/196 - 40s - loss: 28.8498 - MinusLogProbMetric: 28.8498 - val_loss: 29.3795 - val_MinusLogProbMetric: 29.3795 - lr: 0.0010 - 40s/epoch - 205ms/step
Epoch 178/1000
2023-09-29 09:00:48.217 
Epoch 178/1000 
	 loss: 28.7987, MinusLogProbMetric: 28.7987, val_loss: 28.9362, val_MinusLogProbMetric: 28.9362

Epoch 178: val_loss did not improve from 28.83208
196/196 - 41s - loss: 28.7987 - MinusLogProbMetric: 28.7987 - val_loss: 28.9362 - val_MinusLogProbMetric: 28.9362 - lr: 0.0010 - 41s/epoch - 210ms/step
Epoch 179/1000
2023-09-29 09:01:24.988 
Epoch 179/1000 
	 loss: 28.8411, MinusLogProbMetric: 28.8411, val_loss: 29.3129, val_MinusLogProbMetric: 29.3129

Epoch 179: val_loss did not improve from 28.83208
196/196 - 37s - loss: 28.8411 - MinusLogProbMetric: 28.8411 - val_loss: 29.3129 - val_MinusLogProbMetric: 29.3129 - lr: 0.0010 - 37s/epoch - 188ms/step
Epoch 180/1000
2023-09-29 09:02:00.815 
Epoch 180/1000 
	 loss: 28.7704, MinusLogProbMetric: 28.7704, val_loss: 29.0261, val_MinusLogProbMetric: 29.0261

Epoch 180: val_loss did not improve from 28.83208
196/196 - 36s - loss: 28.7704 - MinusLogProbMetric: 28.7704 - val_loss: 29.0261 - val_MinusLogProbMetric: 29.0261 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 181/1000
2023-09-29 09:02:37.314 
Epoch 181/1000 
	 loss: 28.9469, MinusLogProbMetric: 28.9469, val_loss: 29.2727, val_MinusLogProbMetric: 29.2727

Epoch 181: val_loss did not improve from 28.83208
196/196 - 36s - loss: 28.9469 - MinusLogProbMetric: 28.9469 - val_loss: 29.2727 - val_MinusLogProbMetric: 29.2727 - lr: 0.0010 - 36s/epoch - 186ms/step
Epoch 182/1000
2023-09-29 09:03:16.031 
Epoch 182/1000 
	 loss: 28.7698, MinusLogProbMetric: 28.7698, val_loss: 28.8873, val_MinusLogProbMetric: 28.8873

Epoch 182: val_loss did not improve from 28.83208
196/196 - 39s - loss: 28.7698 - MinusLogProbMetric: 28.7698 - val_loss: 28.8873 - val_MinusLogProbMetric: 28.8873 - lr: 0.0010 - 39s/epoch - 198ms/step
Epoch 183/1000
2023-09-29 09:03:59.803 
Epoch 183/1000 
	 loss: 28.8110, MinusLogProbMetric: 28.8110, val_loss: 29.4239, val_MinusLogProbMetric: 29.4239

Epoch 183: val_loss did not improve from 28.83208
196/196 - 44s - loss: 28.8110 - MinusLogProbMetric: 28.8110 - val_loss: 29.4239 - val_MinusLogProbMetric: 29.4239 - lr: 0.0010 - 44s/epoch - 223ms/step
Epoch 184/1000
2023-09-29 09:04:43.303 
Epoch 184/1000 
	 loss: 28.9067, MinusLogProbMetric: 28.9067, val_loss: 29.1397, val_MinusLogProbMetric: 29.1397

Epoch 184: val_loss did not improve from 28.83208
196/196 - 43s - loss: 28.9067 - MinusLogProbMetric: 28.9067 - val_loss: 29.1397 - val_MinusLogProbMetric: 29.1397 - lr: 0.0010 - 43s/epoch - 222ms/step
Epoch 185/1000
2023-09-29 09:05:23.542 
Epoch 185/1000 
	 loss: 28.7834, MinusLogProbMetric: 28.7834, val_loss: 29.3168, val_MinusLogProbMetric: 29.3168

Epoch 185: val_loss did not improve from 28.83208
196/196 - 40s - loss: 28.7834 - MinusLogProbMetric: 28.7834 - val_loss: 29.3168 - val_MinusLogProbMetric: 29.3168 - lr: 0.0010 - 40s/epoch - 205ms/step
Epoch 186/1000
2023-09-29 09:05:59.472 
Epoch 186/1000 
	 loss: 28.9520, MinusLogProbMetric: 28.9520, val_loss: 29.2459, val_MinusLogProbMetric: 29.2459

Epoch 186: val_loss did not improve from 28.83208
196/196 - 36s - loss: 28.9520 - MinusLogProbMetric: 28.9520 - val_loss: 29.2459 - val_MinusLogProbMetric: 29.2459 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 187/1000
2023-09-29 09:06:35.328 
Epoch 187/1000 
	 loss: 28.8349, MinusLogProbMetric: 28.8349, val_loss: 29.4088, val_MinusLogProbMetric: 29.4088

Epoch 187: val_loss did not improve from 28.83208
196/196 - 36s - loss: 28.8349 - MinusLogProbMetric: 28.8349 - val_loss: 29.4088 - val_MinusLogProbMetric: 29.4088 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 188/1000
2023-09-29 09:07:12.197 
Epoch 188/1000 
	 loss: 28.8197, MinusLogProbMetric: 28.8197, val_loss: 29.9469, val_MinusLogProbMetric: 29.9469

Epoch 188: val_loss did not improve from 28.83208
196/196 - 37s - loss: 28.8197 - MinusLogProbMetric: 28.8197 - val_loss: 29.9469 - val_MinusLogProbMetric: 29.9469 - lr: 0.0010 - 37s/epoch - 188ms/step
Epoch 189/1000
2023-09-29 09:07:51.069 
Epoch 189/1000 
	 loss: 29.0163, MinusLogProbMetric: 29.0163, val_loss: 29.8086, val_MinusLogProbMetric: 29.8086

Epoch 189: val_loss did not improve from 28.83208
196/196 - 39s - loss: 29.0163 - MinusLogProbMetric: 29.0163 - val_loss: 29.8086 - val_MinusLogProbMetric: 29.8086 - lr: 0.0010 - 39s/epoch - 198ms/step
Epoch 190/1000
2023-09-29 09:08:33.038 
Epoch 190/1000 
	 loss: 28.8160, MinusLogProbMetric: 28.8160, val_loss: 29.4596, val_MinusLogProbMetric: 29.4596

Epoch 190: val_loss did not improve from 28.83208
196/196 - 42s - loss: 28.8160 - MinusLogProbMetric: 28.8160 - val_loss: 29.4596 - val_MinusLogProbMetric: 29.4596 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 191/1000
2023-09-29 09:09:16.701 
Epoch 191/1000 
	 loss: 28.8241, MinusLogProbMetric: 28.8241, val_loss: 29.3532, val_MinusLogProbMetric: 29.3532

Epoch 191: val_loss did not improve from 28.83208
196/196 - 44s - loss: 28.8241 - MinusLogProbMetric: 28.8241 - val_loss: 29.3532 - val_MinusLogProbMetric: 29.3532 - lr: 0.0010 - 44s/epoch - 223ms/step
Epoch 192/1000
2023-09-29 09:09:58.289 
Epoch 192/1000 
	 loss: 28.8392, MinusLogProbMetric: 28.8392, val_loss: 29.2807, val_MinusLogProbMetric: 29.2807

Epoch 192: val_loss did not improve from 28.83208
196/196 - 42s - loss: 28.8392 - MinusLogProbMetric: 28.8392 - val_loss: 29.2807 - val_MinusLogProbMetric: 29.2807 - lr: 0.0010 - 42s/epoch - 212ms/step
Epoch 193/1000
2023-09-29 09:10:39.940 
Epoch 193/1000 
	 loss: 28.7674, MinusLogProbMetric: 28.7674, val_loss: 28.8883, val_MinusLogProbMetric: 28.8883

Epoch 193: val_loss did not improve from 28.83208
196/196 - 42s - loss: 28.7674 - MinusLogProbMetric: 28.7674 - val_loss: 28.8883 - val_MinusLogProbMetric: 28.8883 - lr: 0.0010 - 42s/epoch - 212ms/step
Epoch 194/1000
2023-09-29 09:11:22.184 
Epoch 194/1000 
	 loss: 28.7094, MinusLogProbMetric: 28.7094, val_loss: 29.0762, val_MinusLogProbMetric: 29.0762

Epoch 194: val_loss did not improve from 28.83208
196/196 - 42s - loss: 28.7094 - MinusLogProbMetric: 28.7094 - val_loss: 29.0762 - val_MinusLogProbMetric: 29.0762 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 195/1000
2023-09-29 09:12:03.616 
Epoch 195/1000 
	 loss: 28.7716, MinusLogProbMetric: 28.7716, val_loss: 29.0584, val_MinusLogProbMetric: 29.0584

Epoch 195: val_loss did not improve from 28.83208
196/196 - 41s - loss: 28.7716 - MinusLogProbMetric: 28.7716 - val_loss: 29.0584 - val_MinusLogProbMetric: 29.0584 - lr: 0.0010 - 41s/epoch - 211ms/step
Epoch 196/1000
2023-09-29 09:12:44.903 
Epoch 196/1000 
	 loss: 28.8414, MinusLogProbMetric: 28.8414, val_loss: 29.0238, val_MinusLogProbMetric: 29.0238

Epoch 196: val_loss did not improve from 28.83208
196/196 - 41s - loss: 28.8414 - MinusLogProbMetric: 28.8414 - val_loss: 29.0238 - val_MinusLogProbMetric: 29.0238 - lr: 0.0010 - 41s/epoch - 211ms/step
Epoch 197/1000
2023-09-29 09:13:26.948 
Epoch 197/1000 
	 loss: 28.7074, MinusLogProbMetric: 28.7074, val_loss: 29.1139, val_MinusLogProbMetric: 29.1139

Epoch 197: val_loss did not improve from 28.83208
196/196 - 42s - loss: 28.7074 - MinusLogProbMetric: 28.7074 - val_loss: 29.1139 - val_MinusLogProbMetric: 29.1139 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 198/1000
2023-09-29 09:14:07.651 
Epoch 198/1000 
	 loss: 28.7210, MinusLogProbMetric: 28.7210, val_loss: 29.6018, val_MinusLogProbMetric: 29.6018

Epoch 198: val_loss did not improve from 28.83208
196/196 - 41s - loss: 28.7210 - MinusLogProbMetric: 28.7210 - val_loss: 29.6018 - val_MinusLogProbMetric: 29.6018 - lr: 0.0010 - 41s/epoch - 208ms/step
Epoch 199/1000
2023-09-29 09:14:51.017 
Epoch 199/1000 
	 loss: 28.7919, MinusLogProbMetric: 28.7919, val_loss: 28.7064, val_MinusLogProbMetric: 28.7064

Epoch 199: val_loss improved from 28.83208 to 28.70640, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_332/weights/best_weights.h5
196/196 - 44s - loss: 28.7919 - MinusLogProbMetric: 28.7919 - val_loss: 28.7064 - val_MinusLogProbMetric: 28.7064 - lr: 0.0010 - 44s/epoch - 225ms/step
Epoch 200/1000
2023-09-29 09:15:35.201 
Epoch 200/1000 
	 loss: 28.8469, MinusLogProbMetric: 28.8469, val_loss: 29.5349, val_MinusLogProbMetric: 29.5349

Epoch 200: val_loss did not improve from 28.70640
196/196 - 43s - loss: 28.8469 - MinusLogProbMetric: 28.8469 - val_loss: 29.5349 - val_MinusLogProbMetric: 29.5349 - lr: 0.0010 - 43s/epoch - 221ms/step
Epoch 201/1000
2023-09-29 09:16:18.671 
Epoch 201/1000 
	 loss: 28.7589, MinusLogProbMetric: 28.7589, val_loss: 29.5183, val_MinusLogProbMetric: 29.5183

Epoch 201: val_loss did not improve from 28.70640
196/196 - 43s - loss: 28.7589 - MinusLogProbMetric: 28.7589 - val_loss: 29.5183 - val_MinusLogProbMetric: 29.5183 - lr: 0.0010 - 43s/epoch - 222ms/step
Epoch 202/1000
2023-09-29 09:16:58.713 
Epoch 202/1000 
	 loss: 28.6853, MinusLogProbMetric: 28.6853, val_loss: 29.4431, val_MinusLogProbMetric: 29.4431

Epoch 202: val_loss did not improve from 28.70640
196/196 - 40s - loss: 28.6853 - MinusLogProbMetric: 28.6853 - val_loss: 29.4431 - val_MinusLogProbMetric: 29.4431 - lr: 0.0010 - 40s/epoch - 204ms/step
Epoch 203/1000
2023-09-29 09:17:41.062 
Epoch 203/1000 
	 loss: 28.6574, MinusLogProbMetric: 28.6574, val_loss: 29.1421, val_MinusLogProbMetric: 29.1421

Epoch 203: val_loss did not improve from 28.70640
196/196 - 42s - loss: 28.6574 - MinusLogProbMetric: 28.6574 - val_loss: 29.1421 - val_MinusLogProbMetric: 29.1421 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 204/1000
2023-09-29 09:18:24.898 
Epoch 204/1000 
	 loss: 28.7456, MinusLogProbMetric: 28.7456, val_loss: 29.4634, val_MinusLogProbMetric: 29.4634

Epoch 204: val_loss did not improve from 28.70640
196/196 - 44s - loss: 28.7456 - MinusLogProbMetric: 28.7456 - val_loss: 29.4634 - val_MinusLogProbMetric: 29.4634 - lr: 0.0010 - 44s/epoch - 224ms/step
Epoch 205/1000
2023-09-29 09:19:08.563 
Epoch 205/1000 
	 loss: 28.6583, MinusLogProbMetric: 28.6583, val_loss: 28.7692, val_MinusLogProbMetric: 28.7692

Epoch 205: val_loss did not improve from 28.70640
196/196 - 44s - loss: 28.6583 - MinusLogProbMetric: 28.6583 - val_loss: 28.7692 - val_MinusLogProbMetric: 28.7692 - lr: 0.0010 - 44s/epoch - 223ms/step
Epoch 206/1000
2023-09-29 09:19:52.065 
Epoch 206/1000 
	 loss: 28.7104, MinusLogProbMetric: 28.7104, val_loss: 28.8193, val_MinusLogProbMetric: 28.8193

Epoch 206: val_loss did not improve from 28.70640
196/196 - 44s - loss: 28.7104 - MinusLogProbMetric: 28.7104 - val_loss: 28.8193 - val_MinusLogProbMetric: 28.8193 - lr: 0.0010 - 44s/epoch - 222ms/step
Epoch 207/1000
2023-09-29 09:20:36.022 
Epoch 207/1000 
	 loss: 28.7237, MinusLogProbMetric: 28.7237, val_loss: 29.6059, val_MinusLogProbMetric: 29.6059

Epoch 207: val_loss did not improve from 28.70640
196/196 - 44s - loss: 28.7237 - MinusLogProbMetric: 28.7237 - val_loss: 29.6059 - val_MinusLogProbMetric: 29.6059 - lr: 0.0010 - 44s/epoch - 224ms/step
Epoch 208/1000
2023-09-29 09:21:19.816 
Epoch 208/1000 
	 loss: 28.7033, MinusLogProbMetric: 28.7033, val_loss: 29.0507, val_MinusLogProbMetric: 29.0507

Epoch 208: val_loss did not improve from 28.70640
196/196 - 44s - loss: 28.7033 - MinusLogProbMetric: 28.7033 - val_loss: 29.0507 - val_MinusLogProbMetric: 29.0507 - lr: 0.0010 - 44s/epoch - 223ms/step
Epoch 209/1000
2023-09-29 09:21:56.565 
Epoch 209/1000 
	 loss: 28.7758, MinusLogProbMetric: 28.7758, val_loss: 29.1764, val_MinusLogProbMetric: 29.1764

Epoch 209: val_loss did not improve from 28.70640
196/196 - 37s - loss: 28.7758 - MinusLogProbMetric: 28.7758 - val_loss: 29.1764 - val_MinusLogProbMetric: 29.1764 - lr: 0.0010 - 37s/epoch - 187ms/step
Epoch 210/1000
2023-09-29 09:22:33.173 
Epoch 210/1000 
	 loss: 28.7623, MinusLogProbMetric: 28.7623, val_loss: 28.7210, val_MinusLogProbMetric: 28.7210

Epoch 210: val_loss did not improve from 28.70640
196/196 - 37s - loss: 28.7623 - MinusLogProbMetric: 28.7623 - val_loss: 28.7210 - val_MinusLogProbMetric: 28.7210 - lr: 0.0010 - 37s/epoch - 187ms/step
Epoch 211/1000
2023-09-29 09:23:09.058 
Epoch 211/1000 
	 loss: 28.5661, MinusLogProbMetric: 28.5661, val_loss: 29.1098, val_MinusLogProbMetric: 29.1098

Epoch 211: val_loss did not improve from 28.70640
196/196 - 36s - loss: 28.5661 - MinusLogProbMetric: 28.5661 - val_loss: 29.1098 - val_MinusLogProbMetric: 29.1098 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 212/1000
2023-09-29 09:23:46.796 
Epoch 212/1000 
	 loss: 28.7040, MinusLogProbMetric: 28.7040, val_loss: 29.6403, val_MinusLogProbMetric: 29.6403

Epoch 212: val_loss did not improve from 28.70640
196/196 - 38s - loss: 28.7040 - MinusLogProbMetric: 28.7040 - val_loss: 29.6403 - val_MinusLogProbMetric: 29.6403 - lr: 0.0010 - 38s/epoch - 193ms/step
Epoch 213/1000
2023-09-29 09:24:25.644 
Epoch 213/1000 
	 loss: 28.6903, MinusLogProbMetric: 28.6903, val_loss: 29.4897, val_MinusLogProbMetric: 29.4897

Epoch 213: val_loss did not improve from 28.70640
196/196 - 39s - loss: 28.6903 - MinusLogProbMetric: 28.6903 - val_loss: 29.4897 - val_MinusLogProbMetric: 29.4897 - lr: 0.0010 - 39s/epoch - 198ms/step
Epoch 214/1000
2023-09-29 09:25:06.835 
Epoch 214/1000 
	 loss: 28.7864, MinusLogProbMetric: 28.7864, val_loss: 29.0653, val_MinusLogProbMetric: 29.0653

Epoch 214: val_loss did not improve from 28.70640
196/196 - 41s - loss: 28.7864 - MinusLogProbMetric: 28.7864 - val_loss: 29.0653 - val_MinusLogProbMetric: 29.0653 - lr: 0.0010 - 41s/epoch - 210ms/step
Epoch 215/1000
2023-09-29 09:25:48.418 
Epoch 215/1000 
	 loss: 28.7551, MinusLogProbMetric: 28.7551, val_loss: 29.7539, val_MinusLogProbMetric: 29.7539

Epoch 215: val_loss did not improve from 28.70640
196/196 - 42s - loss: 28.7551 - MinusLogProbMetric: 28.7551 - val_loss: 29.7539 - val_MinusLogProbMetric: 29.7539 - lr: 0.0010 - 42s/epoch - 212ms/step
Epoch 216/1000
2023-09-29 09:26:30.182 
Epoch 216/1000 
	 loss: 28.7099, MinusLogProbMetric: 28.7099, val_loss: 29.3296, val_MinusLogProbMetric: 29.3296

Epoch 216: val_loss did not improve from 28.70640
196/196 - 42s - loss: 28.7099 - MinusLogProbMetric: 28.7099 - val_loss: 29.3296 - val_MinusLogProbMetric: 29.3296 - lr: 0.0010 - 42s/epoch - 213ms/step
Epoch 217/1000
2023-09-29 09:27:14.498 
Epoch 217/1000 
	 loss: 28.5870, MinusLogProbMetric: 28.5870, val_loss: 28.9662, val_MinusLogProbMetric: 28.9662

Epoch 217: val_loss did not improve from 28.70640
196/196 - 44s - loss: 28.5870 - MinusLogProbMetric: 28.5870 - val_loss: 28.9662 - val_MinusLogProbMetric: 28.9662 - lr: 0.0010 - 44s/epoch - 226ms/step
Epoch 218/1000
2023-09-29 09:27:58.290 
Epoch 218/1000 
	 loss: 28.6708, MinusLogProbMetric: 28.6708, val_loss: 28.9409, val_MinusLogProbMetric: 28.9409

Epoch 218: val_loss did not improve from 28.70640
196/196 - 44s - loss: 28.6708 - MinusLogProbMetric: 28.6708 - val_loss: 28.9409 - val_MinusLogProbMetric: 28.9409 - lr: 0.0010 - 44s/epoch - 223ms/step
Epoch 219/1000
2023-09-29 09:28:41.967 
Epoch 219/1000 
	 loss: 28.7644, MinusLogProbMetric: 28.7644, val_loss: 29.0069, val_MinusLogProbMetric: 29.0069

Epoch 219: val_loss did not improve from 28.70640
196/196 - 44s - loss: 28.7644 - MinusLogProbMetric: 28.7644 - val_loss: 29.0069 - val_MinusLogProbMetric: 29.0069 - lr: 0.0010 - 44s/epoch - 223ms/step
Epoch 220/1000
2023-09-29 09:29:25.715 
Epoch 220/1000 
	 loss: 28.6902, MinusLogProbMetric: 28.6902, val_loss: 29.2357, val_MinusLogProbMetric: 29.2357

Epoch 220: val_loss did not improve from 28.70640
196/196 - 44s - loss: 28.6902 - MinusLogProbMetric: 28.6902 - val_loss: 29.2357 - val_MinusLogProbMetric: 29.2357 - lr: 0.0010 - 44s/epoch - 223ms/step
Epoch 221/1000
2023-09-29 09:30:09.394 
Epoch 221/1000 
	 loss: 28.5642, MinusLogProbMetric: 28.5642, val_loss: 29.3109, val_MinusLogProbMetric: 29.3109

Epoch 221: val_loss did not improve from 28.70640
196/196 - 44s - loss: 28.5642 - MinusLogProbMetric: 28.5642 - val_loss: 29.3109 - val_MinusLogProbMetric: 29.3109 - lr: 0.0010 - 44s/epoch - 223ms/step
Epoch 222/1000
2023-09-29 09:30:52.657 
Epoch 222/1000 
	 loss: 28.5714, MinusLogProbMetric: 28.5714, val_loss: 28.8250, val_MinusLogProbMetric: 28.8250

Epoch 222: val_loss did not improve from 28.70640
196/196 - 43s - loss: 28.5714 - MinusLogProbMetric: 28.5714 - val_loss: 28.8250 - val_MinusLogProbMetric: 28.8250 - lr: 0.0010 - 43s/epoch - 221ms/step
Epoch 223/1000
2023-09-29 09:31:34.670 
Epoch 223/1000 
	 loss: 28.5762, MinusLogProbMetric: 28.5762, val_loss: 28.9960, val_MinusLogProbMetric: 28.9960

Epoch 223: val_loss did not improve from 28.70640
196/196 - 42s - loss: 28.5762 - MinusLogProbMetric: 28.5762 - val_loss: 28.9960 - val_MinusLogProbMetric: 28.9960 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 224/1000
2023-09-29 09:32:17.450 
Epoch 224/1000 
	 loss: 28.6230, MinusLogProbMetric: 28.6230, val_loss: 28.9733, val_MinusLogProbMetric: 28.9733

Epoch 224: val_loss did not improve from 28.70640
196/196 - 43s - loss: 28.6230 - MinusLogProbMetric: 28.6230 - val_loss: 28.9733 - val_MinusLogProbMetric: 28.9733 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 225/1000
2023-09-29 09:32:59.460 
Epoch 225/1000 
	 loss: 28.6705, MinusLogProbMetric: 28.6705, val_loss: 28.9410, val_MinusLogProbMetric: 28.9410

Epoch 225: val_loss did not improve from 28.70640
196/196 - 42s - loss: 28.6705 - MinusLogProbMetric: 28.6705 - val_loss: 28.9410 - val_MinusLogProbMetric: 28.9410 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 226/1000
2023-09-29 09:33:39.086 
Epoch 226/1000 
	 loss: 28.7327, MinusLogProbMetric: 28.7327, val_loss: 28.8893, val_MinusLogProbMetric: 28.8893

Epoch 226: val_loss did not improve from 28.70640
196/196 - 40s - loss: 28.7327 - MinusLogProbMetric: 28.7327 - val_loss: 28.8893 - val_MinusLogProbMetric: 28.8893 - lr: 0.0010 - 40s/epoch - 202ms/step
Epoch 227/1000
2023-09-29 09:34:19.472 
Epoch 227/1000 
	 loss: 28.5376, MinusLogProbMetric: 28.5376, val_loss: 28.9648, val_MinusLogProbMetric: 28.9648

Epoch 227: val_loss did not improve from 28.70640
196/196 - 40s - loss: 28.5376 - MinusLogProbMetric: 28.5376 - val_loss: 28.9648 - val_MinusLogProbMetric: 28.9648 - lr: 0.0010 - 40s/epoch - 206ms/step
Epoch 228/1000
2023-09-29 09:35:01.875 
Epoch 228/1000 
	 loss: 28.4885, MinusLogProbMetric: 28.4885, val_loss: 29.3187, val_MinusLogProbMetric: 29.3187

Epoch 228: val_loss did not improve from 28.70640
196/196 - 42s - loss: 28.4885 - MinusLogProbMetric: 28.4885 - val_loss: 29.3187 - val_MinusLogProbMetric: 29.3187 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 229/1000
2023-09-29 09:35:43.055 
Epoch 229/1000 
	 loss: 28.5394, MinusLogProbMetric: 28.5394, val_loss: 28.9681, val_MinusLogProbMetric: 28.9681

Epoch 229: val_loss did not improve from 28.70640
196/196 - 41s - loss: 28.5394 - MinusLogProbMetric: 28.5394 - val_loss: 28.9681 - val_MinusLogProbMetric: 28.9681 - lr: 0.0010 - 41s/epoch - 210ms/step
Epoch 230/1000
2023-09-29 09:36:26.016 
Epoch 230/1000 
	 loss: 28.6448, MinusLogProbMetric: 28.6448, val_loss: 29.3626, val_MinusLogProbMetric: 29.3626

Epoch 230: val_loss did not improve from 28.70640
196/196 - 43s - loss: 28.6448 - MinusLogProbMetric: 28.6448 - val_loss: 29.3626 - val_MinusLogProbMetric: 29.3626 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 231/1000
2023-09-29 09:37:09.667 
Epoch 231/1000 
	 loss: 28.5601, MinusLogProbMetric: 28.5601, val_loss: 28.8105, val_MinusLogProbMetric: 28.8105

Epoch 231: val_loss did not improve from 28.70640
196/196 - 44s - loss: 28.5601 - MinusLogProbMetric: 28.5601 - val_loss: 28.8105 - val_MinusLogProbMetric: 28.8105 - lr: 0.0010 - 44s/epoch - 223ms/step
Epoch 232/1000
2023-09-29 09:37:50.689 
Epoch 232/1000 
	 loss: 28.6286, MinusLogProbMetric: 28.6286, val_loss: 29.3471, val_MinusLogProbMetric: 29.3471

Epoch 232: val_loss did not improve from 28.70640
196/196 - 41s - loss: 28.6286 - MinusLogProbMetric: 28.6286 - val_loss: 29.3471 - val_MinusLogProbMetric: 29.3471 - lr: 0.0010 - 41s/epoch - 209ms/step
Epoch 233/1000
2023-09-29 09:38:33.775 
Epoch 233/1000 
	 loss: 28.5558, MinusLogProbMetric: 28.5558, val_loss: 28.9593, val_MinusLogProbMetric: 28.9593

Epoch 233: val_loss did not improve from 28.70640
196/196 - 43s - loss: 28.5558 - MinusLogProbMetric: 28.5558 - val_loss: 28.9593 - val_MinusLogProbMetric: 28.9593 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 234/1000
2023-09-29 09:39:17.292 
Epoch 234/1000 
	 loss: 28.5640, MinusLogProbMetric: 28.5640, val_loss: 29.1258, val_MinusLogProbMetric: 29.1258

Epoch 234: val_loss did not improve from 28.70640
196/196 - 44s - loss: 28.5640 - MinusLogProbMetric: 28.5640 - val_loss: 29.1258 - val_MinusLogProbMetric: 29.1258 - lr: 0.0010 - 44s/epoch - 222ms/step
Epoch 235/1000
2023-09-29 09:40:01.406 
Epoch 235/1000 
	 loss: 28.5804, MinusLogProbMetric: 28.5804, val_loss: 28.7598, val_MinusLogProbMetric: 28.7598

Epoch 235: val_loss did not improve from 28.70640
196/196 - 44s - loss: 28.5804 - MinusLogProbMetric: 28.5804 - val_loss: 28.7598 - val_MinusLogProbMetric: 28.7598 - lr: 0.0010 - 44s/epoch - 225ms/step
Epoch 236/1000
2023-09-29 09:40:45.407 
Epoch 236/1000 
	 loss: 28.6179, MinusLogProbMetric: 28.6179, val_loss: 29.0447, val_MinusLogProbMetric: 29.0447

Epoch 236: val_loss did not improve from 28.70640
196/196 - 44s - loss: 28.6179 - MinusLogProbMetric: 28.6179 - val_loss: 29.0447 - val_MinusLogProbMetric: 29.0447 - lr: 0.0010 - 44s/epoch - 224ms/step
Epoch 237/1000
2023-09-29 09:41:29.418 
Epoch 237/1000 
	 loss: 28.5226, MinusLogProbMetric: 28.5226, val_loss: 29.0992, val_MinusLogProbMetric: 29.0992

Epoch 237: val_loss did not improve from 28.70640
196/196 - 44s - loss: 28.5226 - MinusLogProbMetric: 28.5226 - val_loss: 29.0992 - val_MinusLogProbMetric: 29.0992 - lr: 0.0010 - 44s/epoch - 225ms/step
Epoch 238/1000
2023-09-29 09:42:09.726 
Epoch 238/1000 
	 loss: 28.6140, MinusLogProbMetric: 28.6140, val_loss: 29.3892, val_MinusLogProbMetric: 29.3892

Epoch 238: val_loss did not improve from 28.70640
196/196 - 40s - loss: 28.6140 - MinusLogProbMetric: 28.6140 - val_loss: 29.3892 - val_MinusLogProbMetric: 29.3892 - lr: 0.0010 - 40s/epoch - 206ms/step
Epoch 239/1000
2023-09-29 09:42:51.710 
Epoch 239/1000 
	 loss: 28.4727, MinusLogProbMetric: 28.4727, val_loss: 28.8292, val_MinusLogProbMetric: 28.8292

Epoch 239: val_loss did not improve from 28.70640
196/196 - 42s - loss: 28.4727 - MinusLogProbMetric: 28.4727 - val_loss: 28.8292 - val_MinusLogProbMetric: 28.8292 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 240/1000
2023-09-29 09:43:32.477 
Epoch 240/1000 
	 loss: 28.4977, MinusLogProbMetric: 28.4977, val_loss: 28.8355, val_MinusLogProbMetric: 28.8355

Epoch 240: val_loss did not improve from 28.70640
196/196 - 41s - loss: 28.4977 - MinusLogProbMetric: 28.4977 - val_loss: 28.8355 - val_MinusLogProbMetric: 28.8355 - lr: 0.0010 - 41s/epoch - 208ms/step
Epoch 241/1000
2023-09-29 09:44:13.647 
Epoch 241/1000 
	 loss: 28.6046, MinusLogProbMetric: 28.6046, val_loss: 29.1923, val_MinusLogProbMetric: 29.1923

Epoch 241: val_loss did not improve from 28.70640
196/196 - 41s - loss: 28.6046 - MinusLogProbMetric: 28.6046 - val_loss: 29.1923 - val_MinusLogProbMetric: 29.1923 - lr: 0.0010 - 41s/epoch - 210ms/step
Epoch 242/1000
2023-09-29 09:44:57.414 
Epoch 242/1000 
	 loss: 28.4826, MinusLogProbMetric: 28.4826, val_loss: 28.9037, val_MinusLogProbMetric: 28.9037

Epoch 242: val_loss did not improve from 28.70640
196/196 - 44s - loss: 28.4826 - MinusLogProbMetric: 28.4826 - val_loss: 28.9037 - val_MinusLogProbMetric: 28.9037 - lr: 0.0010 - 44s/epoch - 223ms/step
Epoch 243/1000
2023-09-29 09:45:36.792 
Epoch 243/1000 
	 loss: 28.4765, MinusLogProbMetric: 28.4765, val_loss: 28.9192, val_MinusLogProbMetric: 28.9192

Epoch 243: val_loss did not improve from 28.70640
196/196 - 39s - loss: 28.4765 - MinusLogProbMetric: 28.4765 - val_loss: 28.9192 - val_MinusLogProbMetric: 28.9192 - lr: 0.0010 - 39s/epoch - 201ms/step
Epoch 244/1000
2023-09-29 09:46:19.882 
Epoch 244/1000 
	 loss: 28.5813, MinusLogProbMetric: 28.5813, val_loss: 29.1123, val_MinusLogProbMetric: 29.1123

Epoch 244: val_loss did not improve from 28.70640
196/196 - 43s - loss: 28.5813 - MinusLogProbMetric: 28.5813 - val_loss: 29.1123 - val_MinusLogProbMetric: 29.1123 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 245/1000
2023-09-29 09:47:03.338 
Epoch 245/1000 
	 loss: 28.5919, MinusLogProbMetric: 28.5919, val_loss: 29.0104, val_MinusLogProbMetric: 29.0104

Epoch 245: val_loss did not improve from 28.70640
196/196 - 43s - loss: 28.5919 - MinusLogProbMetric: 28.5919 - val_loss: 29.0104 - val_MinusLogProbMetric: 29.0104 - lr: 0.0010 - 43s/epoch - 222ms/step
Epoch 246/1000
2023-09-29 09:47:46.026 
Epoch 246/1000 
	 loss: 28.5287, MinusLogProbMetric: 28.5287, val_loss: 29.0473, val_MinusLogProbMetric: 29.0473

Epoch 246: val_loss did not improve from 28.70640
196/196 - 43s - loss: 28.5287 - MinusLogProbMetric: 28.5287 - val_loss: 29.0473 - val_MinusLogProbMetric: 29.0473 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 247/1000
2023-09-29 09:48:29.477 
Epoch 247/1000 
	 loss: 28.4770, MinusLogProbMetric: 28.4770, val_loss: 29.2524, val_MinusLogProbMetric: 29.2524

Epoch 247: val_loss did not improve from 28.70640
196/196 - 43s - loss: 28.4770 - MinusLogProbMetric: 28.4770 - val_loss: 29.2524 - val_MinusLogProbMetric: 29.2524 - lr: 0.0010 - 43s/epoch - 222ms/step
Epoch 248/1000
2023-09-29 09:49:12.885 
Epoch 248/1000 
	 loss: 28.4924, MinusLogProbMetric: 28.4924, val_loss: 28.7957, val_MinusLogProbMetric: 28.7957

Epoch 248: val_loss did not improve from 28.70640
196/196 - 43s - loss: 28.4924 - MinusLogProbMetric: 28.4924 - val_loss: 28.7957 - val_MinusLogProbMetric: 28.7957 - lr: 0.0010 - 43s/epoch - 221ms/step
Epoch 249/1000
2023-09-29 09:49:55.624 
Epoch 249/1000 
	 loss: 28.4658, MinusLogProbMetric: 28.4658, val_loss: 29.6940, val_MinusLogProbMetric: 29.6940

Epoch 249: val_loss did not improve from 28.70640
196/196 - 43s - loss: 28.4658 - MinusLogProbMetric: 28.4658 - val_loss: 29.6940 - val_MinusLogProbMetric: 29.6940 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 250/1000
2023-09-29 09:50:38.545 
Epoch 250/1000 
	 loss: 27.9184, MinusLogProbMetric: 27.9184, val_loss: 28.2463, val_MinusLogProbMetric: 28.2463

Epoch 250: val_loss improved from 28.70640 to 28.24631, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_332/weights/best_weights.h5
196/196 - 44s - loss: 27.9184 - MinusLogProbMetric: 27.9184 - val_loss: 28.2463 - val_MinusLogProbMetric: 28.2463 - lr: 5.0000e-04 - 44s/epoch - 224ms/step
Epoch 251/1000
2023-09-29 09:51:23.175 
Epoch 251/1000 
	 loss: 27.8495, MinusLogProbMetric: 27.8495, val_loss: 28.3251, val_MinusLogProbMetric: 28.3251

Epoch 251: val_loss did not improve from 28.24631
196/196 - 44s - loss: 27.8495 - MinusLogProbMetric: 27.8495 - val_loss: 28.3251 - val_MinusLogProbMetric: 28.3251 - lr: 5.0000e-04 - 44s/epoch - 223ms/step
Epoch 252/1000
2023-09-29 09:52:07.272 
Epoch 252/1000 
	 loss: 27.8424, MinusLogProbMetric: 27.8424, val_loss: 28.3456, val_MinusLogProbMetric: 28.3456

Epoch 252: val_loss did not improve from 28.24631
196/196 - 44s - loss: 27.8424 - MinusLogProbMetric: 27.8424 - val_loss: 28.3456 - val_MinusLogProbMetric: 28.3456 - lr: 5.0000e-04 - 44s/epoch - 225ms/step
Epoch 253/1000
2023-09-29 09:52:49.927 
Epoch 253/1000 
	 loss: 27.8531, MinusLogProbMetric: 27.8531, val_loss: 28.3283, val_MinusLogProbMetric: 28.3283

Epoch 253: val_loss did not improve from 28.24631
196/196 - 43s - loss: 27.8531 - MinusLogProbMetric: 27.8531 - val_loss: 28.3283 - val_MinusLogProbMetric: 28.3283 - lr: 5.0000e-04 - 43s/epoch - 218ms/step
Epoch 254/1000
2023-09-29 09:53:28.666 
Epoch 254/1000 
	 loss: 27.8754, MinusLogProbMetric: 27.8754, val_loss: 28.3030, val_MinusLogProbMetric: 28.3030

Epoch 254: val_loss did not improve from 28.24631
196/196 - 39s - loss: 27.8754 - MinusLogProbMetric: 27.8754 - val_loss: 28.3030 - val_MinusLogProbMetric: 28.3030 - lr: 5.0000e-04 - 39s/epoch - 198ms/step
Epoch 255/1000
2023-09-29 09:54:09.629 
Epoch 255/1000 
	 loss: 27.8455, MinusLogProbMetric: 27.8455, val_loss: 28.3821, val_MinusLogProbMetric: 28.3821

Epoch 255: val_loss did not improve from 28.24631
196/196 - 41s - loss: 27.8455 - MinusLogProbMetric: 27.8455 - val_loss: 28.3821 - val_MinusLogProbMetric: 28.3821 - lr: 5.0000e-04 - 41s/epoch - 209ms/step
Epoch 256/1000
2023-09-29 09:54:51.199 
Epoch 256/1000 
	 loss: 27.8420, MinusLogProbMetric: 27.8420, val_loss: 28.3114, val_MinusLogProbMetric: 28.3114

Epoch 256: val_loss did not improve from 28.24631
196/196 - 42s - loss: 27.8420 - MinusLogProbMetric: 27.8420 - val_loss: 28.3114 - val_MinusLogProbMetric: 28.3114 - lr: 5.0000e-04 - 42s/epoch - 212ms/step
Epoch 257/1000
2023-09-29 09:55:31.559 
Epoch 257/1000 
	 loss: 27.8615, MinusLogProbMetric: 27.8615, val_loss: 28.4606, val_MinusLogProbMetric: 28.4606

Epoch 257: val_loss did not improve from 28.24631
196/196 - 40s - loss: 27.8615 - MinusLogProbMetric: 27.8615 - val_loss: 28.4606 - val_MinusLogProbMetric: 28.4606 - lr: 5.0000e-04 - 40s/epoch - 206ms/step
Epoch 258/1000
2023-09-29 09:56:10.727 
Epoch 258/1000 
	 loss: 27.8766, MinusLogProbMetric: 27.8766, val_loss: 28.2752, val_MinusLogProbMetric: 28.2752

Epoch 258: val_loss did not improve from 28.24631
196/196 - 39s - loss: 27.8766 - MinusLogProbMetric: 27.8766 - val_loss: 28.2752 - val_MinusLogProbMetric: 28.2752 - lr: 5.0000e-04 - 39s/epoch - 200ms/step
Epoch 259/1000
2023-09-29 09:56:51.375 
Epoch 259/1000 
	 loss: 27.8707, MinusLogProbMetric: 27.8707, val_loss: 28.2610, val_MinusLogProbMetric: 28.2610

Epoch 259: val_loss did not improve from 28.24631
196/196 - 41s - loss: 27.8707 - MinusLogProbMetric: 27.8707 - val_loss: 28.2610 - val_MinusLogProbMetric: 28.2610 - lr: 5.0000e-04 - 41s/epoch - 207ms/step
Epoch 260/1000
2023-09-29 09:57:31.701 
Epoch 260/1000 
	 loss: 27.8698, MinusLogProbMetric: 27.8698, val_loss: 28.7089, val_MinusLogProbMetric: 28.7089

Epoch 260: val_loss did not improve from 28.24631
196/196 - 40s - loss: 27.8698 - MinusLogProbMetric: 27.8698 - val_loss: 28.7089 - val_MinusLogProbMetric: 28.7089 - lr: 5.0000e-04 - 40s/epoch - 206ms/step
Epoch 261/1000
2023-09-29 09:58:12.680 
Epoch 261/1000 
	 loss: 27.8849, MinusLogProbMetric: 27.8849, val_loss: 28.4753, val_MinusLogProbMetric: 28.4753

Epoch 261: val_loss did not improve from 28.24631
196/196 - 41s - loss: 27.8849 - MinusLogProbMetric: 27.8849 - val_loss: 28.4753 - val_MinusLogProbMetric: 28.4753 - lr: 5.0000e-04 - 41s/epoch - 209ms/step
Epoch 262/1000
2023-09-29 09:58:55.145 
Epoch 262/1000 
	 loss: 27.8677, MinusLogProbMetric: 27.8677, val_loss: 28.2573, val_MinusLogProbMetric: 28.2573

Epoch 262: val_loss did not improve from 28.24631
196/196 - 42s - loss: 27.8677 - MinusLogProbMetric: 27.8677 - val_loss: 28.2573 - val_MinusLogProbMetric: 28.2573 - lr: 5.0000e-04 - 42s/epoch - 217ms/step
Epoch 263/1000
2023-09-29 09:59:38.722 
Epoch 263/1000 
	 loss: 27.8689, MinusLogProbMetric: 27.8689, val_loss: 28.3358, val_MinusLogProbMetric: 28.3358

Epoch 263: val_loss did not improve from 28.24631
196/196 - 44s - loss: 27.8689 - MinusLogProbMetric: 27.8689 - val_loss: 28.3358 - val_MinusLogProbMetric: 28.3358 - lr: 5.0000e-04 - 44s/epoch - 222ms/step
Epoch 264/1000
2023-09-29 10:00:22.901 
Epoch 264/1000 
	 loss: 27.8438, MinusLogProbMetric: 27.8438, val_loss: 28.3440, val_MinusLogProbMetric: 28.3440

Epoch 264: val_loss did not improve from 28.24631
196/196 - 44s - loss: 27.8438 - MinusLogProbMetric: 27.8438 - val_loss: 28.3440 - val_MinusLogProbMetric: 28.3440 - lr: 5.0000e-04 - 44s/epoch - 225ms/step
Epoch 265/1000
2023-09-29 10:01:07.019 
Epoch 265/1000 
	 loss: 27.8563, MinusLogProbMetric: 27.8563, val_loss: 28.4106, val_MinusLogProbMetric: 28.4106

Epoch 265: val_loss did not improve from 28.24631
196/196 - 44s - loss: 27.8563 - MinusLogProbMetric: 27.8563 - val_loss: 28.4106 - val_MinusLogProbMetric: 28.4106 - lr: 5.0000e-04 - 44s/epoch - 225ms/step
Epoch 266/1000
2023-09-29 10:01:46.726 
Epoch 266/1000 
	 loss: 27.8724, MinusLogProbMetric: 27.8724, val_loss: 28.3602, val_MinusLogProbMetric: 28.3602

Epoch 266: val_loss did not improve from 28.24631
196/196 - 40s - loss: 27.8724 - MinusLogProbMetric: 27.8724 - val_loss: 28.3602 - val_MinusLogProbMetric: 28.3602 - lr: 5.0000e-04 - 40s/epoch - 203ms/step
Epoch 267/1000
2023-09-29 10:02:30.505 
Epoch 267/1000 
	 loss: 27.8727, MinusLogProbMetric: 27.8727, val_loss: 28.2163, val_MinusLogProbMetric: 28.2163

Epoch 267: val_loss improved from 28.24631 to 28.21625, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_332/weights/best_weights.h5
196/196 - 44s - loss: 27.8727 - MinusLogProbMetric: 27.8727 - val_loss: 28.2163 - val_MinusLogProbMetric: 28.2163 - lr: 5.0000e-04 - 44s/epoch - 227ms/step
Epoch 268/1000
2023-09-29 10:03:14.896 
Epoch 268/1000 
	 loss: 27.8772, MinusLogProbMetric: 27.8772, val_loss: 28.2748, val_MinusLogProbMetric: 28.2748

Epoch 268: val_loss did not improve from 28.21625
196/196 - 44s - loss: 27.8772 - MinusLogProbMetric: 27.8772 - val_loss: 28.2748 - val_MinusLogProbMetric: 28.2748 - lr: 5.0000e-04 - 44s/epoch - 223ms/step
Epoch 269/1000
2023-09-29 10:03:53.110 
Epoch 269/1000 
	 loss: 27.8637, MinusLogProbMetric: 27.8637, val_loss: 28.4470, val_MinusLogProbMetric: 28.4470

Epoch 269: val_loss did not improve from 28.21625
196/196 - 38s - loss: 27.8637 - MinusLogProbMetric: 27.8637 - val_loss: 28.4470 - val_MinusLogProbMetric: 28.4470 - lr: 5.0000e-04 - 38s/epoch - 195ms/step
Epoch 270/1000
2023-09-29 10:04:36.035 
Epoch 270/1000 
	 loss: 27.8492, MinusLogProbMetric: 27.8492, val_loss: 28.3775, val_MinusLogProbMetric: 28.3775

Epoch 270: val_loss did not improve from 28.21625
196/196 - 43s - loss: 27.8492 - MinusLogProbMetric: 27.8492 - val_loss: 28.3775 - val_MinusLogProbMetric: 28.3775 - lr: 5.0000e-04 - 43s/epoch - 219ms/step
Epoch 271/1000
2023-09-29 10:05:18.273 
Epoch 271/1000 
	 loss: 27.8855, MinusLogProbMetric: 27.8855, val_loss: 28.3822, val_MinusLogProbMetric: 28.3822

Epoch 271: val_loss did not improve from 28.21625
196/196 - 42s - loss: 27.8855 - MinusLogProbMetric: 27.8855 - val_loss: 28.3822 - val_MinusLogProbMetric: 28.3822 - lr: 5.0000e-04 - 42s/epoch - 215ms/step
Epoch 272/1000
2023-09-29 10:06:01.307 
Epoch 272/1000 
	 loss: 27.8472, MinusLogProbMetric: 27.8472, val_loss: 28.4060, val_MinusLogProbMetric: 28.4060

Epoch 272: val_loss did not improve from 28.21625
196/196 - 43s - loss: 27.8472 - MinusLogProbMetric: 27.8472 - val_loss: 28.4060 - val_MinusLogProbMetric: 28.4060 - lr: 5.0000e-04 - 43s/epoch - 220ms/step
Epoch 273/1000
2023-09-29 10:06:42.960 
Epoch 273/1000 
	 loss: 27.8351, MinusLogProbMetric: 27.8351, val_loss: 28.3060, val_MinusLogProbMetric: 28.3060

Epoch 273: val_loss did not improve from 28.21625
196/196 - 42s - loss: 27.8351 - MinusLogProbMetric: 27.8351 - val_loss: 28.3060 - val_MinusLogProbMetric: 28.3060 - lr: 5.0000e-04 - 42s/epoch - 212ms/step
Epoch 274/1000
2023-09-29 10:07:22.874 
Epoch 274/1000 
	 loss: 27.8309, MinusLogProbMetric: 27.8309, val_loss: 28.2728, val_MinusLogProbMetric: 28.2728

Epoch 274: val_loss did not improve from 28.21625
196/196 - 40s - loss: 27.8309 - MinusLogProbMetric: 27.8309 - val_loss: 28.2728 - val_MinusLogProbMetric: 28.2728 - lr: 5.0000e-04 - 40s/epoch - 204ms/step
Epoch 275/1000
2023-09-29 10:08:04.749 
Epoch 275/1000 
	 loss: 27.8026, MinusLogProbMetric: 27.8026, val_loss: 28.1839, val_MinusLogProbMetric: 28.1839

Epoch 275: val_loss improved from 28.21625 to 28.18393, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_332/weights/best_weights.h5
196/196 - 43s - loss: 27.8026 - MinusLogProbMetric: 27.8026 - val_loss: 28.1839 - val_MinusLogProbMetric: 28.1839 - lr: 5.0000e-04 - 43s/epoch - 218ms/step
Epoch 276/1000
2023-09-29 10:08:45.971 
Epoch 276/1000 
	 loss: 27.8351, MinusLogProbMetric: 27.8351, val_loss: 28.3872, val_MinusLogProbMetric: 28.3872

Epoch 276: val_loss did not improve from 28.18393
196/196 - 40s - loss: 27.8351 - MinusLogProbMetric: 27.8351 - val_loss: 28.3872 - val_MinusLogProbMetric: 28.3872 - lr: 5.0000e-04 - 40s/epoch - 206ms/step
Epoch 277/1000
2023-09-29 10:09:27.085 
Epoch 277/1000 
	 loss: 27.8329, MinusLogProbMetric: 27.8329, val_loss: 28.4082, val_MinusLogProbMetric: 28.4082

Epoch 277: val_loss did not improve from 28.18393
196/196 - 41s - loss: 27.8329 - MinusLogProbMetric: 27.8329 - val_loss: 28.4082 - val_MinusLogProbMetric: 28.4082 - lr: 5.0000e-04 - 41s/epoch - 210ms/step
Epoch 278/1000
2023-09-29 10:10:11.126 
Epoch 278/1000 
	 loss: 27.8439, MinusLogProbMetric: 27.8439, val_loss: 28.3414, val_MinusLogProbMetric: 28.3414

Epoch 278: val_loss did not improve from 28.18393
196/196 - 44s - loss: 27.8439 - MinusLogProbMetric: 27.8439 - val_loss: 28.3414 - val_MinusLogProbMetric: 28.3414 - lr: 5.0000e-04 - 44s/epoch - 225ms/step
Epoch 279/1000
2023-09-29 10:10:56.009 
Epoch 279/1000 
	 loss: 27.8505, MinusLogProbMetric: 27.8505, val_loss: 28.2128, val_MinusLogProbMetric: 28.2128

Epoch 279: val_loss did not improve from 28.18393
196/196 - 45s - loss: 27.8505 - MinusLogProbMetric: 27.8505 - val_loss: 28.2128 - val_MinusLogProbMetric: 28.2128 - lr: 5.0000e-04 - 45s/epoch - 229ms/step
Epoch 280/1000
2023-09-29 10:11:41.141 
Epoch 280/1000 
	 loss: 27.8465, MinusLogProbMetric: 27.8465, val_loss: 28.2794, val_MinusLogProbMetric: 28.2794

Epoch 280: val_loss did not improve from 28.18393
196/196 - 45s - loss: 27.8465 - MinusLogProbMetric: 27.8465 - val_loss: 28.2794 - val_MinusLogProbMetric: 28.2794 - lr: 5.0000e-04 - 45s/epoch - 230ms/step
Epoch 281/1000
2023-09-29 10:12:25.167 
Epoch 281/1000 
	 loss: 27.8239, MinusLogProbMetric: 27.8239, val_loss: 28.2113, val_MinusLogProbMetric: 28.2113

Epoch 281: val_loss did not improve from 28.18393
196/196 - 44s - loss: 27.8239 - MinusLogProbMetric: 27.8239 - val_loss: 28.2113 - val_MinusLogProbMetric: 28.2113 - lr: 5.0000e-04 - 44s/epoch - 225ms/step
Epoch 282/1000
2023-09-29 10:13:09.265 
Epoch 282/1000 
	 loss: 27.8559, MinusLogProbMetric: 27.8559, val_loss: 28.3670, val_MinusLogProbMetric: 28.3670

Epoch 282: val_loss did not improve from 28.18393
196/196 - 44s - loss: 27.8559 - MinusLogProbMetric: 27.8559 - val_loss: 28.3670 - val_MinusLogProbMetric: 28.3670 - lr: 5.0000e-04 - 44s/epoch - 225ms/step
Epoch 283/1000
2023-09-29 10:13:52.917 
Epoch 283/1000 
	 loss: 27.8208, MinusLogProbMetric: 27.8208, val_loss: 28.4000, val_MinusLogProbMetric: 28.4000

Epoch 283: val_loss did not improve from 28.18393
196/196 - 44s - loss: 27.8208 - MinusLogProbMetric: 27.8208 - val_loss: 28.4000 - val_MinusLogProbMetric: 28.4000 - lr: 5.0000e-04 - 44s/epoch - 223ms/step
Epoch 284/1000
2023-09-29 10:14:34.953 
Epoch 284/1000 
	 loss: 27.8275, MinusLogProbMetric: 27.8275, val_loss: 28.4770, val_MinusLogProbMetric: 28.4770

Epoch 284: val_loss did not improve from 28.18393
196/196 - 42s - loss: 27.8275 - MinusLogProbMetric: 27.8275 - val_loss: 28.4770 - val_MinusLogProbMetric: 28.4770 - lr: 5.0000e-04 - 42s/epoch - 214ms/step
Epoch 285/1000
2023-09-29 10:15:16.361 
Epoch 285/1000 
	 loss: 27.8450, MinusLogProbMetric: 27.8450, val_loss: 28.3695, val_MinusLogProbMetric: 28.3695

Epoch 285: val_loss did not improve from 28.18393
196/196 - 41s - loss: 27.8450 - MinusLogProbMetric: 27.8450 - val_loss: 28.3695 - val_MinusLogProbMetric: 28.3695 - lr: 5.0000e-04 - 41s/epoch - 211ms/step
Epoch 286/1000
2023-09-29 10:15:55.294 
Epoch 286/1000 
	 loss: 27.8499, MinusLogProbMetric: 27.8499, val_loss: 28.2955, val_MinusLogProbMetric: 28.2955

Epoch 286: val_loss did not improve from 28.18393
196/196 - 39s - loss: 27.8499 - MinusLogProbMetric: 27.8499 - val_loss: 28.2955 - val_MinusLogProbMetric: 28.2955 - lr: 5.0000e-04 - 39s/epoch - 199ms/step
Epoch 287/1000
2023-09-29 10:16:35.342 
Epoch 287/1000 
	 loss: 27.8072, MinusLogProbMetric: 27.8072, val_loss: 28.3457, val_MinusLogProbMetric: 28.3457

Epoch 287: val_loss did not improve from 28.18393
196/196 - 40s - loss: 27.8072 - MinusLogProbMetric: 27.8072 - val_loss: 28.3457 - val_MinusLogProbMetric: 28.3457 - lr: 5.0000e-04 - 40s/epoch - 204ms/step
Epoch 288/1000
2023-09-29 10:17:15.286 
Epoch 288/1000 
	 loss: 27.7978, MinusLogProbMetric: 27.7978, val_loss: 28.6370, val_MinusLogProbMetric: 28.6370

Epoch 288: val_loss did not improve from 28.18393
196/196 - 40s - loss: 27.7978 - MinusLogProbMetric: 27.7978 - val_loss: 28.6370 - val_MinusLogProbMetric: 28.6370 - lr: 5.0000e-04 - 40s/epoch - 204ms/step
Epoch 289/1000
2023-09-29 10:17:55.696 
Epoch 289/1000 
	 loss: 27.8381, MinusLogProbMetric: 27.8381, val_loss: 28.3814, val_MinusLogProbMetric: 28.3814

Epoch 289: val_loss did not improve from 28.18393
196/196 - 40s - loss: 27.8381 - MinusLogProbMetric: 27.8381 - val_loss: 28.3814 - val_MinusLogProbMetric: 28.3814 - lr: 5.0000e-04 - 40s/epoch - 206ms/step
Epoch 290/1000
2023-09-29 10:18:35.136 
Epoch 290/1000 
	 loss: 27.8240, MinusLogProbMetric: 27.8240, val_loss: 28.2319, val_MinusLogProbMetric: 28.2319

Epoch 290: val_loss did not improve from 28.18393
196/196 - 39s - loss: 27.8240 - MinusLogProbMetric: 27.8240 - val_loss: 28.2319 - val_MinusLogProbMetric: 28.2319 - lr: 5.0000e-04 - 39s/epoch - 201ms/step
Epoch 291/1000
2023-09-29 10:19:14.270 
Epoch 291/1000 
	 loss: 27.8389, MinusLogProbMetric: 27.8389, val_loss: 28.4447, val_MinusLogProbMetric: 28.4447

Epoch 291: val_loss did not improve from 28.18393
196/196 - 39s - loss: 27.8389 - MinusLogProbMetric: 27.8389 - val_loss: 28.4447 - val_MinusLogProbMetric: 28.4447 - lr: 5.0000e-04 - 39s/epoch - 200ms/step
Epoch 292/1000
2023-09-29 10:19:56.981 
Epoch 292/1000 
	 loss: 27.8070, MinusLogProbMetric: 27.8070, val_loss: 28.2355, val_MinusLogProbMetric: 28.2355

Epoch 292: val_loss did not improve from 28.18393
196/196 - 43s - loss: 27.8070 - MinusLogProbMetric: 27.8070 - val_loss: 28.2355 - val_MinusLogProbMetric: 28.2355 - lr: 5.0000e-04 - 43s/epoch - 218ms/step
Epoch 293/1000
2023-09-29 10:20:40.022 
Epoch 293/1000 
	 loss: 27.8295, MinusLogProbMetric: 27.8295, val_loss: 28.3546, val_MinusLogProbMetric: 28.3546

Epoch 293: val_loss did not improve from 28.18393
196/196 - 43s - loss: 27.8295 - MinusLogProbMetric: 27.8295 - val_loss: 28.3546 - val_MinusLogProbMetric: 28.3546 - lr: 5.0000e-04 - 43s/epoch - 220ms/step
Epoch 294/1000
2023-09-29 10:21:21.050 
Epoch 294/1000 
	 loss: 27.8588, MinusLogProbMetric: 27.8588, val_loss: 29.1901, val_MinusLogProbMetric: 29.1901

Epoch 294: val_loss did not improve from 28.18393
196/196 - 41s - loss: 27.8588 - MinusLogProbMetric: 27.8588 - val_loss: 29.1901 - val_MinusLogProbMetric: 29.1901 - lr: 5.0000e-04 - 41s/epoch - 209ms/step
Epoch 295/1000
2023-09-29 10:22:02.843 
Epoch 295/1000 
	 loss: 27.8446, MinusLogProbMetric: 27.8446, val_loss: 28.4142, val_MinusLogProbMetric: 28.4142

Epoch 295: val_loss did not improve from 28.18393
196/196 - 42s - loss: 27.8446 - MinusLogProbMetric: 27.8446 - val_loss: 28.4142 - val_MinusLogProbMetric: 28.4142 - lr: 5.0000e-04 - 42s/epoch - 213ms/step
Epoch 296/1000
2023-09-29 10:22:43.195 
Epoch 296/1000 
	 loss: 27.9041, MinusLogProbMetric: 27.9041, val_loss: 28.2406, val_MinusLogProbMetric: 28.2406

Epoch 296: val_loss did not improve from 28.18393
196/196 - 40s - loss: 27.9041 - MinusLogProbMetric: 27.9041 - val_loss: 28.2406 - val_MinusLogProbMetric: 28.2406 - lr: 5.0000e-04 - 40s/epoch - 206ms/step
Epoch 297/1000
2023-09-29 10:23:24.413 
Epoch 297/1000 
	 loss: 27.8515, MinusLogProbMetric: 27.8515, val_loss: 28.4688, val_MinusLogProbMetric: 28.4688

Epoch 297: val_loss did not improve from 28.18393
196/196 - 41s - loss: 27.8515 - MinusLogProbMetric: 27.8515 - val_loss: 28.4688 - val_MinusLogProbMetric: 28.4688 - lr: 5.0000e-04 - 41s/epoch - 210ms/step
Epoch 298/1000
2023-09-29 10:24:06.278 
Epoch 298/1000 
	 loss: 27.8190, MinusLogProbMetric: 27.8190, val_loss: 28.3380, val_MinusLogProbMetric: 28.3380

Epoch 298: val_loss did not improve from 28.18393
196/196 - 42s - loss: 27.8190 - MinusLogProbMetric: 27.8190 - val_loss: 28.3380 - val_MinusLogProbMetric: 28.3380 - lr: 5.0000e-04 - 42s/epoch - 214ms/step
Epoch 299/1000
2023-09-29 10:24:50.247 
Epoch 299/1000 
	 loss: 27.8293, MinusLogProbMetric: 27.8293, val_loss: 28.2801, val_MinusLogProbMetric: 28.2801

Epoch 299: val_loss did not improve from 28.18393
196/196 - 44s - loss: 27.8293 - MinusLogProbMetric: 27.8293 - val_loss: 28.2801 - val_MinusLogProbMetric: 28.2801 - lr: 5.0000e-04 - 44s/epoch - 224ms/step
Epoch 300/1000
2023-09-29 10:25:30.533 
Epoch 300/1000 
	 loss: 27.8172, MinusLogProbMetric: 27.8172, val_loss: 28.2744, val_MinusLogProbMetric: 28.2744

Epoch 300: val_loss did not improve from 28.18393
196/196 - 40s - loss: 27.8172 - MinusLogProbMetric: 27.8172 - val_loss: 28.2744 - val_MinusLogProbMetric: 28.2744 - lr: 5.0000e-04 - 40s/epoch - 206ms/step
Epoch 301/1000
2023-09-29 10:26:13.902 
Epoch 301/1000 
	 loss: 27.8624, MinusLogProbMetric: 27.8624, val_loss: 28.4768, val_MinusLogProbMetric: 28.4768

Epoch 301: val_loss did not improve from 28.18393
196/196 - 43s - loss: 27.8624 - MinusLogProbMetric: 27.8624 - val_loss: 28.4768 - val_MinusLogProbMetric: 28.4768 - lr: 5.0000e-04 - 43s/epoch - 221ms/step
Epoch 302/1000
2023-09-29 10:26:57.758 
Epoch 302/1000 
	 loss: 27.7958, MinusLogProbMetric: 27.7958, val_loss: 28.4078, val_MinusLogProbMetric: 28.4078

Epoch 302: val_loss did not improve from 28.18393
196/196 - 44s - loss: 27.7958 - MinusLogProbMetric: 27.7958 - val_loss: 28.4078 - val_MinusLogProbMetric: 28.4078 - lr: 5.0000e-04 - 44s/epoch - 224ms/step
Epoch 303/1000
2023-09-29 10:27:40.009 
Epoch 303/1000 
	 loss: 27.8437, MinusLogProbMetric: 27.8437, val_loss: 28.3140, val_MinusLogProbMetric: 28.3140

Epoch 303: val_loss did not improve from 28.18393
196/196 - 42s - loss: 27.8437 - MinusLogProbMetric: 27.8437 - val_loss: 28.3140 - val_MinusLogProbMetric: 28.3140 - lr: 5.0000e-04 - 42s/epoch - 216ms/step
Epoch 304/1000
2023-09-29 10:28:19.812 
Epoch 304/1000 
	 loss: 27.7943, MinusLogProbMetric: 27.7943, val_loss: 28.4303, val_MinusLogProbMetric: 28.4303

Epoch 304: val_loss did not improve from 28.18393
196/196 - 40s - loss: 27.7943 - MinusLogProbMetric: 27.7943 - val_loss: 28.4303 - val_MinusLogProbMetric: 28.4303 - lr: 5.0000e-04 - 40s/epoch - 203ms/step
Epoch 305/1000
2023-09-29 10:28:59.995 
Epoch 305/1000 
	 loss: 27.8727, MinusLogProbMetric: 27.8727, val_loss: 28.2397, val_MinusLogProbMetric: 28.2397

Epoch 305: val_loss did not improve from 28.18393
196/196 - 40s - loss: 27.8727 - MinusLogProbMetric: 27.8727 - val_loss: 28.2397 - val_MinusLogProbMetric: 28.2397 - lr: 5.0000e-04 - 40s/epoch - 205ms/step
Epoch 306/1000
2023-09-29 10:29:40.024 
Epoch 306/1000 
	 loss: 27.8206, MinusLogProbMetric: 27.8206, val_loss: 28.4102, val_MinusLogProbMetric: 28.4102

Epoch 306: val_loss did not improve from 28.18393
196/196 - 40s - loss: 27.8206 - MinusLogProbMetric: 27.8206 - val_loss: 28.4102 - val_MinusLogProbMetric: 28.4102 - lr: 5.0000e-04 - 40s/epoch - 204ms/step
Epoch 307/1000
2023-09-29 10:30:23.965 
Epoch 307/1000 
	 loss: 27.8012, MinusLogProbMetric: 27.8012, val_loss: 28.4035, val_MinusLogProbMetric: 28.4035

Epoch 307: val_loss did not improve from 28.18393
196/196 - 44s - loss: 27.8012 - MinusLogProbMetric: 27.8012 - val_loss: 28.4035 - val_MinusLogProbMetric: 28.4035 - lr: 5.0000e-04 - 44s/epoch - 224ms/step
Epoch 308/1000
2023-09-29 10:31:06.831 
Epoch 308/1000 
	 loss: 27.8147, MinusLogProbMetric: 27.8147, val_loss: 28.3060, val_MinusLogProbMetric: 28.3060

Epoch 308: val_loss did not improve from 28.18393
196/196 - 43s - loss: 27.8147 - MinusLogProbMetric: 27.8147 - val_loss: 28.3060 - val_MinusLogProbMetric: 28.3060 - lr: 5.0000e-04 - 43s/epoch - 219ms/step
Epoch 309/1000
2023-09-29 10:31:49.682 
Epoch 309/1000 
	 loss: 27.8523, MinusLogProbMetric: 27.8523, val_loss: 28.9085, val_MinusLogProbMetric: 28.9085

Epoch 309: val_loss did not improve from 28.18393
196/196 - 43s - loss: 27.8523 - MinusLogProbMetric: 27.8523 - val_loss: 28.9085 - val_MinusLogProbMetric: 28.9085 - lr: 5.0000e-04 - 43s/epoch - 219ms/step
Epoch 310/1000
2023-09-29 10:32:28.752 
Epoch 310/1000 
	 loss: 27.8572, MinusLogProbMetric: 27.8572, val_loss: 28.2858, val_MinusLogProbMetric: 28.2858

Epoch 310: val_loss did not improve from 28.18393
196/196 - 39s - loss: 27.8572 - MinusLogProbMetric: 27.8572 - val_loss: 28.2858 - val_MinusLogProbMetric: 28.2858 - lr: 5.0000e-04 - 39s/epoch - 199ms/step
Epoch 311/1000
2023-09-29 10:33:08.840 
Epoch 311/1000 
	 loss: 27.7824, MinusLogProbMetric: 27.7824, val_loss: 28.4865, val_MinusLogProbMetric: 28.4865

Epoch 311: val_loss did not improve from 28.18393
196/196 - 40s - loss: 27.7824 - MinusLogProbMetric: 27.7824 - val_loss: 28.4865 - val_MinusLogProbMetric: 28.4865 - lr: 5.0000e-04 - 40s/epoch - 205ms/step
Epoch 312/1000
2023-09-29 10:33:50.556 
Epoch 312/1000 
	 loss: 27.8308, MinusLogProbMetric: 27.8308, val_loss: 28.2357, val_MinusLogProbMetric: 28.2357

Epoch 312: val_loss did not improve from 28.18393
196/196 - 42s - loss: 27.8308 - MinusLogProbMetric: 27.8308 - val_loss: 28.2357 - val_MinusLogProbMetric: 28.2357 - lr: 5.0000e-04 - 42s/epoch - 213ms/step
Epoch 313/1000
2023-09-29 10:34:33.143 
Epoch 313/1000 
	 loss: 27.8249, MinusLogProbMetric: 27.8249, val_loss: 28.3176, val_MinusLogProbMetric: 28.3176

Epoch 313: val_loss did not improve from 28.18393
196/196 - 43s - loss: 27.8249 - MinusLogProbMetric: 27.8249 - val_loss: 28.3176 - val_MinusLogProbMetric: 28.3176 - lr: 5.0000e-04 - 43s/epoch - 217ms/step
Epoch 314/1000
2023-09-29 10:35:16.422 
Epoch 314/1000 
	 loss: 27.8290, MinusLogProbMetric: 27.8290, val_loss: 28.4786, val_MinusLogProbMetric: 28.4786

Epoch 314: val_loss did not improve from 28.18393
196/196 - 43s - loss: 27.8290 - MinusLogProbMetric: 27.8290 - val_loss: 28.4786 - val_MinusLogProbMetric: 28.4786 - lr: 5.0000e-04 - 43s/epoch - 221ms/step
Epoch 315/1000
2023-09-29 10:35:57.078 
Epoch 315/1000 
	 loss: 27.8217, MinusLogProbMetric: 27.8217, val_loss: 28.3167, val_MinusLogProbMetric: 28.3167

Epoch 315: val_loss did not improve from 28.18393
196/196 - 41s - loss: 27.8217 - MinusLogProbMetric: 27.8217 - val_loss: 28.3167 - val_MinusLogProbMetric: 28.3167 - lr: 5.0000e-04 - 41s/epoch - 207ms/step
Epoch 316/1000
2023-09-29 10:36:37.103 
Epoch 316/1000 
	 loss: 27.8175, MinusLogProbMetric: 27.8175, val_loss: 28.3334, val_MinusLogProbMetric: 28.3334

Epoch 316: val_loss did not improve from 28.18393
196/196 - 40s - loss: 27.8175 - MinusLogProbMetric: 27.8175 - val_loss: 28.3334 - val_MinusLogProbMetric: 28.3334 - lr: 5.0000e-04 - 40s/epoch - 204ms/step
Epoch 317/1000
2023-09-29 10:37:13.316 
Epoch 317/1000 
	 loss: 27.8360, MinusLogProbMetric: 27.8360, val_loss: 28.1631, val_MinusLogProbMetric: 28.1631

Epoch 317: val_loss improved from 28.18393 to 28.16308, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_332/weights/best_weights.h5
196/196 - 37s - loss: 27.8360 - MinusLogProbMetric: 27.8360 - val_loss: 28.1631 - val_MinusLogProbMetric: 28.1631 - lr: 5.0000e-04 - 37s/epoch - 189ms/step
Epoch 318/1000
2023-09-29 10:37:54.267 
Epoch 318/1000 
	 loss: 27.8129, MinusLogProbMetric: 27.8129, val_loss: 28.3847, val_MinusLogProbMetric: 28.3847

Epoch 318: val_loss did not improve from 28.16308
196/196 - 40s - loss: 27.8129 - MinusLogProbMetric: 27.8129 - val_loss: 28.3847 - val_MinusLogProbMetric: 28.3847 - lr: 5.0000e-04 - 40s/epoch - 204ms/step
Epoch 319/1000
2023-09-29 10:38:35.197 
Epoch 319/1000 
	 loss: 27.8173, MinusLogProbMetric: 27.8173, val_loss: 28.4313, val_MinusLogProbMetric: 28.4313

Epoch 319: val_loss did not improve from 28.16308
196/196 - 41s - loss: 27.8173 - MinusLogProbMetric: 27.8173 - val_loss: 28.4313 - val_MinusLogProbMetric: 28.4313 - lr: 5.0000e-04 - 41s/epoch - 209ms/step
Epoch 320/1000
2023-09-29 10:39:16.397 
Epoch 320/1000 
	 loss: 27.8031, MinusLogProbMetric: 27.8031, val_loss: 28.4239, val_MinusLogProbMetric: 28.4239

Epoch 320: val_loss did not improve from 28.16308
196/196 - 41s - loss: 27.8031 - MinusLogProbMetric: 27.8031 - val_loss: 28.4239 - val_MinusLogProbMetric: 28.4239 - lr: 5.0000e-04 - 41s/epoch - 210ms/step
Epoch 321/1000
2023-09-29 10:39:57.185 
Epoch 321/1000 
	 loss: 27.8003, MinusLogProbMetric: 27.8003, val_loss: 28.2546, val_MinusLogProbMetric: 28.2546

Epoch 321: val_loss did not improve from 28.16308
196/196 - 41s - loss: 27.8003 - MinusLogProbMetric: 27.8003 - val_loss: 28.2546 - val_MinusLogProbMetric: 28.2546 - lr: 5.0000e-04 - 41s/epoch - 208ms/step
Epoch 322/1000
2023-09-29 10:40:38.900 
Epoch 322/1000 
	 loss: 27.7895, MinusLogProbMetric: 27.7895, val_loss: 28.2906, val_MinusLogProbMetric: 28.2906

Epoch 322: val_loss did not improve from 28.16308
196/196 - 42s - loss: 27.7895 - MinusLogProbMetric: 27.7895 - val_loss: 28.2906 - val_MinusLogProbMetric: 28.2906 - lr: 5.0000e-04 - 42s/epoch - 213ms/step
Epoch 323/1000
2023-09-29 10:41:21.699 
Epoch 323/1000 
	 loss: 27.8270, MinusLogProbMetric: 27.8270, val_loss: 28.2984, val_MinusLogProbMetric: 28.2984

Epoch 323: val_loss did not improve from 28.16308
196/196 - 43s - loss: 27.8270 - MinusLogProbMetric: 27.8270 - val_loss: 28.2984 - val_MinusLogProbMetric: 28.2984 - lr: 5.0000e-04 - 43s/epoch - 218ms/step
Epoch 324/1000
2023-09-29 10:42:01.465 
Epoch 324/1000 
	 loss: 27.7747, MinusLogProbMetric: 27.7747, val_loss: 28.5221, val_MinusLogProbMetric: 28.5221

Epoch 324: val_loss did not improve from 28.16308
196/196 - 40s - loss: 27.7747 - MinusLogProbMetric: 27.7747 - val_loss: 28.5221 - val_MinusLogProbMetric: 28.5221 - lr: 5.0000e-04 - 40s/epoch - 203ms/step
Epoch 325/1000
2023-09-29 10:42:44.178 
Epoch 325/1000 
	 loss: 27.8054, MinusLogProbMetric: 27.8054, val_loss: 28.5212, val_MinusLogProbMetric: 28.5212

Epoch 325: val_loss did not improve from 28.16308
196/196 - 43s - loss: 27.8054 - MinusLogProbMetric: 27.8054 - val_loss: 28.5212 - val_MinusLogProbMetric: 28.5212 - lr: 5.0000e-04 - 43s/epoch - 218ms/step
Epoch 326/1000
2023-09-29 10:43:26.891 
Epoch 326/1000 
	 loss: 27.7953, MinusLogProbMetric: 27.7953, val_loss: 28.2636, val_MinusLogProbMetric: 28.2636

Epoch 326: val_loss did not improve from 28.16308
196/196 - 43s - loss: 27.7953 - MinusLogProbMetric: 27.7953 - val_loss: 28.2636 - val_MinusLogProbMetric: 28.2636 - lr: 5.0000e-04 - 43s/epoch - 218ms/step
Epoch 327/1000
2023-09-29 10:44:09.478 
Epoch 327/1000 
	 loss: 27.8232, MinusLogProbMetric: 27.8232, val_loss: 28.3332, val_MinusLogProbMetric: 28.3332

Epoch 327: val_loss did not improve from 28.16308
196/196 - 43s - loss: 27.8232 - MinusLogProbMetric: 27.8232 - val_loss: 28.3332 - val_MinusLogProbMetric: 28.3332 - lr: 5.0000e-04 - 43s/epoch - 217ms/step
Epoch 328/1000
2023-09-29 10:44:51.139 
Epoch 328/1000 
	 loss: 27.7713, MinusLogProbMetric: 27.7713, val_loss: 28.1980, val_MinusLogProbMetric: 28.1980

Epoch 328: val_loss did not improve from 28.16308
196/196 - 42s - loss: 27.7713 - MinusLogProbMetric: 27.7713 - val_loss: 28.1980 - val_MinusLogProbMetric: 28.1980 - lr: 5.0000e-04 - 42s/epoch - 213ms/step
Epoch 329/1000
2023-09-29 10:45:34.200 
Epoch 329/1000 
	 loss: 27.7914, MinusLogProbMetric: 27.7914, val_loss: 28.3272, val_MinusLogProbMetric: 28.3272

Epoch 329: val_loss did not improve from 28.16308
196/196 - 43s - loss: 27.7914 - MinusLogProbMetric: 27.7914 - val_loss: 28.3272 - val_MinusLogProbMetric: 28.3272 - lr: 5.0000e-04 - 43s/epoch - 220ms/step
Epoch 330/1000
2023-09-29 10:46:17.830 
Epoch 330/1000 
	 loss: 27.7825, MinusLogProbMetric: 27.7825, val_loss: 28.2047, val_MinusLogProbMetric: 28.2047

Epoch 330: val_loss did not improve from 28.16308
196/196 - 44s - loss: 27.7825 - MinusLogProbMetric: 27.7825 - val_loss: 28.2047 - val_MinusLogProbMetric: 28.2047 - lr: 5.0000e-04 - 44s/epoch - 223ms/step
Epoch 331/1000
2023-09-29 10:47:00.666 
Epoch 331/1000 
	 loss: 27.7687, MinusLogProbMetric: 27.7687, val_loss: 28.2592, val_MinusLogProbMetric: 28.2592

Epoch 331: val_loss did not improve from 28.16308
196/196 - 43s - loss: 27.7687 - MinusLogProbMetric: 27.7687 - val_loss: 28.2592 - val_MinusLogProbMetric: 28.2592 - lr: 5.0000e-04 - 43s/epoch - 219ms/step
Epoch 332/1000
2023-09-29 10:47:42.219 
Epoch 332/1000 
	 loss: 27.7857, MinusLogProbMetric: 27.7857, val_loss: 28.2815, val_MinusLogProbMetric: 28.2815

Epoch 332: val_loss did not improve from 28.16308
196/196 - 42s - loss: 27.7857 - MinusLogProbMetric: 27.7857 - val_loss: 28.2815 - val_MinusLogProbMetric: 28.2815 - lr: 5.0000e-04 - 42s/epoch - 212ms/step
Epoch 333/1000
2023-09-29 10:48:26.029 
Epoch 333/1000 
	 loss: 27.7385, MinusLogProbMetric: 27.7385, val_loss: 28.3423, val_MinusLogProbMetric: 28.3423

Epoch 333: val_loss did not improve from 28.16308
196/196 - 44s - loss: 27.7385 - MinusLogProbMetric: 27.7385 - val_loss: 28.3423 - val_MinusLogProbMetric: 28.3423 - lr: 5.0000e-04 - 44s/epoch - 224ms/step
Epoch 334/1000
2023-09-29 10:49:07.914 
Epoch 334/1000 
	 loss: 27.7715, MinusLogProbMetric: 27.7715, val_loss: 28.3418, val_MinusLogProbMetric: 28.3418

Epoch 334: val_loss did not improve from 28.16308
196/196 - 42s - loss: 27.7715 - MinusLogProbMetric: 27.7715 - val_loss: 28.3418 - val_MinusLogProbMetric: 28.3418 - lr: 5.0000e-04 - 42s/epoch - 214ms/step
Epoch 335/1000
2023-09-29 10:49:47.633 
Epoch 335/1000 
	 loss: 27.8327, MinusLogProbMetric: 27.8327, val_loss: 28.6428, val_MinusLogProbMetric: 28.6428

Epoch 335: val_loss did not improve from 28.16308
196/196 - 40s - loss: 27.8327 - MinusLogProbMetric: 27.8327 - val_loss: 28.6428 - val_MinusLogProbMetric: 28.6428 - lr: 5.0000e-04 - 40s/epoch - 203ms/step
Epoch 336/1000
2023-09-29 10:50:28.529 
Epoch 336/1000 
	 loss: 27.7854, MinusLogProbMetric: 27.7854, val_loss: 28.2412, val_MinusLogProbMetric: 28.2412

Epoch 336: val_loss did not improve from 28.16308
196/196 - 41s - loss: 27.7854 - MinusLogProbMetric: 27.7854 - val_loss: 28.2412 - val_MinusLogProbMetric: 28.2412 - lr: 5.0000e-04 - 41s/epoch - 209ms/step
Epoch 337/1000
2023-09-29 10:51:12.827 
Epoch 337/1000 
	 loss: 27.8182, MinusLogProbMetric: 27.8182, val_loss: 28.5011, val_MinusLogProbMetric: 28.5011

Epoch 337: val_loss did not improve from 28.16308
196/196 - 44s - loss: 27.8182 - MinusLogProbMetric: 27.8182 - val_loss: 28.5011 - val_MinusLogProbMetric: 28.5011 - lr: 5.0000e-04 - 44s/epoch - 226ms/step
Epoch 338/1000
2023-09-29 10:51:55.682 
Epoch 338/1000 
	 loss: 27.7694, MinusLogProbMetric: 27.7694, val_loss: 28.2238, val_MinusLogProbMetric: 28.2238

Epoch 338: val_loss did not improve from 28.16308
196/196 - 43s - loss: 27.7694 - MinusLogProbMetric: 27.7694 - val_loss: 28.2238 - val_MinusLogProbMetric: 28.2238 - lr: 5.0000e-04 - 43s/epoch - 219ms/step
Epoch 339/1000
2023-09-29 10:52:38.064 
Epoch 339/1000 
	 loss: 27.7840, MinusLogProbMetric: 27.7840, val_loss: 28.2440, val_MinusLogProbMetric: 28.2440

Epoch 339: val_loss did not improve from 28.16308
196/196 - 42s - loss: 27.7840 - MinusLogProbMetric: 27.7840 - val_loss: 28.2440 - val_MinusLogProbMetric: 28.2440 - lr: 5.0000e-04 - 42s/epoch - 216ms/step
Epoch 340/1000
2023-09-29 10:53:21.991 
Epoch 340/1000 
	 loss: 27.7600, MinusLogProbMetric: 27.7600, val_loss: 28.3133, val_MinusLogProbMetric: 28.3133

Epoch 340: val_loss did not improve from 28.16308
196/196 - 44s - loss: 27.7600 - MinusLogProbMetric: 27.7600 - val_loss: 28.3133 - val_MinusLogProbMetric: 28.3133 - lr: 5.0000e-04 - 44s/epoch - 224ms/step
Epoch 341/1000
2023-09-29 10:54:05.003 
Epoch 341/1000 
	 loss: 27.7814, MinusLogProbMetric: 27.7814, val_loss: 28.2503, val_MinusLogProbMetric: 28.2503

Epoch 341: val_loss did not improve from 28.16308
196/196 - 43s - loss: 27.7814 - MinusLogProbMetric: 27.7814 - val_loss: 28.2503 - val_MinusLogProbMetric: 28.2503 - lr: 5.0000e-04 - 43s/epoch - 219ms/step
Epoch 342/1000
2023-09-29 10:54:44.371 
Epoch 342/1000 
	 loss: 27.8035, MinusLogProbMetric: 27.8035, val_loss: 28.3175, val_MinusLogProbMetric: 28.3175

Epoch 342: val_loss did not improve from 28.16308
196/196 - 39s - loss: 27.8035 - MinusLogProbMetric: 27.8035 - val_loss: 28.3175 - val_MinusLogProbMetric: 28.3175 - lr: 5.0000e-04 - 39s/epoch - 201ms/step
Epoch 343/1000
2023-09-29 10:55:22.423 
Epoch 343/1000 
	 loss: 27.7686, MinusLogProbMetric: 27.7686, val_loss: 28.3556, val_MinusLogProbMetric: 28.3556

Epoch 343: val_loss did not improve from 28.16308
196/196 - 38s - loss: 27.7686 - MinusLogProbMetric: 27.7686 - val_loss: 28.3556 - val_MinusLogProbMetric: 28.3556 - lr: 5.0000e-04 - 38s/epoch - 194ms/step
Epoch 344/1000
2023-09-29 10:56:01.158 
Epoch 344/1000 
	 loss: 27.8036, MinusLogProbMetric: 27.8036, val_loss: 28.3658, val_MinusLogProbMetric: 28.3658

Epoch 344: val_loss did not improve from 28.16308
196/196 - 39s - loss: 27.8036 - MinusLogProbMetric: 27.8036 - val_loss: 28.3658 - val_MinusLogProbMetric: 28.3658 - lr: 5.0000e-04 - 39s/epoch - 198ms/step
Epoch 345/1000
2023-09-29 10:56:41.296 
Epoch 345/1000 
	 loss: 27.7887, MinusLogProbMetric: 27.7887, val_loss: 28.3744, val_MinusLogProbMetric: 28.3744

Epoch 345: val_loss did not improve from 28.16308
196/196 - 40s - loss: 27.7887 - MinusLogProbMetric: 27.7887 - val_loss: 28.3744 - val_MinusLogProbMetric: 28.3744 - lr: 5.0000e-04 - 40s/epoch - 205ms/step
Epoch 346/1000
2023-09-29 10:57:20.758 
Epoch 346/1000 
	 loss: 27.7556, MinusLogProbMetric: 27.7556, val_loss: 28.2811, val_MinusLogProbMetric: 28.2811

Epoch 346: val_loss did not improve from 28.16308
196/196 - 39s - loss: 27.7556 - MinusLogProbMetric: 27.7556 - val_loss: 28.2811 - val_MinusLogProbMetric: 28.2811 - lr: 5.0000e-04 - 39s/epoch - 201ms/step
Epoch 347/1000
2023-09-29 10:57:58.160 
Epoch 347/1000 
	 loss: 27.7959, MinusLogProbMetric: 27.7959, val_loss: 28.3023, val_MinusLogProbMetric: 28.3023

Epoch 347: val_loss did not improve from 28.16308
196/196 - 37s - loss: 27.7959 - MinusLogProbMetric: 27.7959 - val_loss: 28.3023 - val_MinusLogProbMetric: 28.3023 - lr: 5.0000e-04 - 37s/epoch - 191ms/step
Epoch 348/1000
2023-09-29 10:58:36.485 
Epoch 348/1000 
	 loss: 27.7956, MinusLogProbMetric: 27.7956, val_loss: 28.2914, val_MinusLogProbMetric: 28.2914

Epoch 348: val_loss did not improve from 28.16308
196/196 - 38s - loss: 27.7956 - MinusLogProbMetric: 27.7956 - val_loss: 28.2914 - val_MinusLogProbMetric: 28.2914 - lr: 5.0000e-04 - 38s/epoch - 196ms/step
Epoch 349/1000
2023-09-29 10:59:14.464 
Epoch 349/1000 
	 loss: 27.7747, MinusLogProbMetric: 27.7747, val_loss: 28.2091, val_MinusLogProbMetric: 28.2091

Epoch 349: val_loss did not improve from 28.16308
196/196 - 38s - loss: 27.7747 - MinusLogProbMetric: 27.7747 - val_loss: 28.2091 - val_MinusLogProbMetric: 28.2091 - lr: 5.0000e-04 - 38s/epoch - 194ms/step
Epoch 350/1000
2023-09-29 10:59:55.714 
Epoch 350/1000 
	 loss: 27.7483, MinusLogProbMetric: 27.7483, val_loss: 28.8479, val_MinusLogProbMetric: 28.8479

Epoch 350: val_loss did not improve from 28.16308
196/196 - 41s - loss: 27.7483 - MinusLogProbMetric: 27.7483 - val_loss: 28.8479 - val_MinusLogProbMetric: 28.8479 - lr: 5.0000e-04 - 41s/epoch - 210ms/step
Epoch 351/1000
2023-09-29 11:00:34.102 
Epoch 351/1000 
	 loss: 27.8020, MinusLogProbMetric: 27.8020, val_loss: 28.4269, val_MinusLogProbMetric: 28.4269

Epoch 351: val_loss did not improve from 28.16308
196/196 - 38s - loss: 27.8020 - MinusLogProbMetric: 27.8020 - val_loss: 28.4269 - val_MinusLogProbMetric: 28.4269 - lr: 5.0000e-04 - 38s/epoch - 196ms/step
Epoch 352/1000
2023-09-29 11:01:15.590 
Epoch 352/1000 
	 loss: 27.7511, MinusLogProbMetric: 27.7511, val_loss: 28.3474, val_MinusLogProbMetric: 28.3474

Epoch 352: val_loss did not improve from 28.16308
196/196 - 41s - loss: 27.7511 - MinusLogProbMetric: 27.7511 - val_loss: 28.3474 - val_MinusLogProbMetric: 28.3474 - lr: 5.0000e-04 - 41s/epoch - 212ms/step
Epoch 353/1000
2023-09-29 11:01:55.608 
Epoch 353/1000 
	 loss: 27.7873, MinusLogProbMetric: 27.7873, val_loss: 28.1876, val_MinusLogProbMetric: 28.1876

Epoch 353: val_loss did not improve from 28.16308
196/196 - 40s - loss: 27.7873 - MinusLogProbMetric: 27.7873 - val_loss: 28.1876 - val_MinusLogProbMetric: 28.1876 - lr: 5.0000e-04 - 40s/epoch - 204ms/step
Epoch 354/1000
2023-09-29 11:02:36.437 
Epoch 354/1000 
	 loss: 27.7781, MinusLogProbMetric: 27.7781, val_loss: 28.1562, val_MinusLogProbMetric: 28.1562

Epoch 354: val_loss improved from 28.16308 to 28.15617, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_332/weights/best_weights.h5
196/196 - 41s - loss: 27.7781 - MinusLogProbMetric: 27.7781 - val_loss: 28.1562 - val_MinusLogProbMetric: 28.1562 - lr: 5.0000e-04 - 41s/epoch - 212ms/step
Epoch 355/1000
2023-09-29 11:03:17.851 
Epoch 355/1000 
	 loss: 27.7572, MinusLogProbMetric: 27.7572, val_loss: 28.3639, val_MinusLogProbMetric: 28.3639

Epoch 355: val_loss did not improve from 28.15617
196/196 - 41s - loss: 27.7572 - MinusLogProbMetric: 27.7572 - val_loss: 28.3639 - val_MinusLogProbMetric: 28.3639 - lr: 5.0000e-04 - 41s/epoch - 208ms/step
Epoch 356/1000
2023-09-29 11:04:00.312 
Epoch 356/1000 
	 loss: 27.7251, MinusLogProbMetric: 27.7251, val_loss: 28.3316, val_MinusLogProbMetric: 28.3316

Epoch 356: val_loss did not improve from 28.15617
196/196 - 42s - loss: 27.7251 - MinusLogProbMetric: 27.7251 - val_loss: 28.3316 - val_MinusLogProbMetric: 28.3316 - lr: 5.0000e-04 - 42s/epoch - 217ms/step
Epoch 357/1000
2023-09-29 11:04:39.736 
Epoch 357/1000 
	 loss: 27.8035, MinusLogProbMetric: 27.8035, val_loss: 28.2792, val_MinusLogProbMetric: 28.2792

Epoch 357: val_loss did not improve from 28.15617
196/196 - 39s - loss: 27.8035 - MinusLogProbMetric: 27.8035 - val_loss: 28.2792 - val_MinusLogProbMetric: 28.2792 - lr: 5.0000e-04 - 39s/epoch - 201ms/step
Epoch 358/1000
2023-09-29 11:05:18.131 
Epoch 358/1000 
	 loss: 27.7706, MinusLogProbMetric: 27.7706, val_loss: 28.1307, val_MinusLogProbMetric: 28.1307

Epoch 358: val_loss improved from 28.15617 to 28.13069, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_332/weights/best_weights.h5
196/196 - 39s - loss: 27.7706 - MinusLogProbMetric: 27.7706 - val_loss: 28.1307 - val_MinusLogProbMetric: 28.1307 - lr: 5.0000e-04 - 39s/epoch - 200ms/step
Epoch 359/1000
2023-09-29 11:05:59.076 
Epoch 359/1000 
	 loss: 27.7615, MinusLogProbMetric: 27.7615, val_loss: 28.3432, val_MinusLogProbMetric: 28.3432

Epoch 359: val_loss did not improve from 28.13069
196/196 - 40s - loss: 27.7615 - MinusLogProbMetric: 27.7615 - val_loss: 28.3432 - val_MinusLogProbMetric: 28.3432 - lr: 5.0000e-04 - 40s/epoch - 205ms/step
Epoch 360/1000
2023-09-29 11:06:40.819 
Epoch 360/1000 
	 loss: 27.7499, MinusLogProbMetric: 27.7499, val_loss: 29.0062, val_MinusLogProbMetric: 29.0062

Epoch 360: val_loss did not improve from 28.13069
196/196 - 42s - loss: 27.7499 - MinusLogProbMetric: 27.7499 - val_loss: 29.0062 - val_MinusLogProbMetric: 29.0062 - lr: 5.0000e-04 - 42s/epoch - 213ms/step
Epoch 361/1000
2023-09-29 11:07:23.420 
Epoch 361/1000 
	 loss: 27.8257, MinusLogProbMetric: 27.8257, val_loss: 28.4126, val_MinusLogProbMetric: 28.4126

Epoch 361: val_loss did not improve from 28.13069
196/196 - 43s - loss: 27.8257 - MinusLogProbMetric: 27.8257 - val_loss: 28.4126 - val_MinusLogProbMetric: 28.4126 - lr: 5.0000e-04 - 43s/epoch - 217ms/step
Epoch 362/1000
2023-09-29 11:08:04.660 
Epoch 362/1000 
	 loss: 27.7580, MinusLogProbMetric: 27.7580, val_loss: 28.2861, val_MinusLogProbMetric: 28.2861

Epoch 362: val_loss did not improve from 28.13069
196/196 - 41s - loss: 27.7580 - MinusLogProbMetric: 27.7580 - val_loss: 28.2861 - val_MinusLogProbMetric: 28.2861 - lr: 5.0000e-04 - 41s/epoch - 210ms/step
Epoch 363/1000
2023-09-29 11:08:43.195 
Epoch 363/1000 
	 loss: 27.7153, MinusLogProbMetric: 27.7153, val_loss: 28.3544, val_MinusLogProbMetric: 28.3544

Epoch 363: val_loss did not improve from 28.13069
196/196 - 39s - loss: 27.7153 - MinusLogProbMetric: 27.7153 - val_loss: 28.3544 - val_MinusLogProbMetric: 28.3544 - lr: 5.0000e-04 - 39s/epoch - 197ms/step
Epoch 364/1000
2023-09-29 11:09:23.200 
Epoch 364/1000 
	 loss: 27.8019, MinusLogProbMetric: 27.8019, val_loss: 28.4508, val_MinusLogProbMetric: 28.4508

Epoch 364: val_loss did not improve from 28.13069
196/196 - 40s - loss: 27.8019 - MinusLogProbMetric: 27.8019 - val_loss: 28.4508 - val_MinusLogProbMetric: 28.4508 - lr: 5.0000e-04 - 40s/epoch - 204ms/step
Epoch 365/1000
2023-09-29 11:10:02.169 
Epoch 365/1000 
	 loss: 27.7778, MinusLogProbMetric: 27.7778, val_loss: 28.1867, val_MinusLogProbMetric: 28.1867

Epoch 365: val_loss did not improve from 28.13069
196/196 - 39s - loss: 27.7778 - MinusLogProbMetric: 27.7778 - val_loss: 28.1867 - val_MinusLogProbMetric: 28.1867 - lr: 5.0000e-04 - 39s/epoch - 199ms/step
Epoch 366/1000
2023-09-29 11:10:42.282 
Epoch 366/1000 
	 loss: 27.7254, MinusLogProbMetric: 27.7254, val_loss: 28.1870, val_MinusLogProbMetric: 28.1870

Epoch 366: val_loss did not improve from 28.13069
196/196 - 40s - loss: 27.7254 - MinusLogProbMetric: 27.7254 - val_loss: 28.1870 - val_MinusLogProbMetric: 28.1870 - lr: 5.0000e-04 - 40s/epoch - 205ms/step
Epoch 367/1000
2023-09-29 11:11:20.412 
Epoch 367/1000 
	 loss: 27.7507, MinusLogProbMetric: 27.7507, val_loss: 28.3254, val_MinusLogProbMetric: 28.3254

Epoch 367: val_loss did not improve from 28.13069
196/196 - 38s - loss: 27.7507 - MinusLogProbMetric: 27.7507 - val_loss: 28.3254 - val_MinusLogProbMetric: 28.3254 - lr: 5.0000e-04 - 38s/epoch - 195ms/step
Epoch 368/1000
2023-09-29 11:11:59.968 
Epoch 368/1000 
	 loss: 27.8038, MinusLogProbMetric: 27.8038, val_loss: 28.1535, val_MinusLogProbMetric: 28.1535

Epoch 368: val_loss did not improve from 28.13069
196/196 - 40s - loss: 27.8038 - MinusLogProbMetric: 27.8038 - val_loss: 28.1535 - val_MinusLogProbMetric: 28.1535 - lr: 5.0000e-04 - 40s/epoch - 202ms/step
Epoch 369/1000
2023-09-29 11:12:38.828 
Epoch 369/1000 
	 loss: 27.7483, MinusLogProbMetric: 27.7483, val_loss: 28.3217, val_MinusLogProbMetric: 28.3217

Epoch 369: val_loss did not improve from 28.13069
196/196 - 39s - loss: 27.7483 - MinusLogProbMetric: 27.7483 - val_loss: 28.3217 - val_MinusLogProbMetric: 28.3217 - lr: 5.0000e-04 - 39s/epoch - 198ms/step
Epoch 370/1000
2023-09-29 11:13:17.436 
Epoch 370/1000 
	 loss: 27.7688, MinusLogProbMetric: 27.7688, val_loss: 28.4132, val_MinusLogProbMetric: 28.4132

Epoch 370: val_loss did not improve from 28.13069
196/196 - 39s - loss: 27.7688 - MinusLogProbMetric: 27.7688 - val_loss: 28.4132 - val_MinusLogProbMetric: 28.4132 - lr: 5.0000e-04 - 39s/epoch - 197ms/step
Epoch 371/1000
2023-09-29 11:13:54.943 
Epoch 371/1000 
	 loss: 27.7647, MinusLogProbMetric: 27.7647, val_loss: 28.2936, val_MinusLogProbMetric: 28.2936

Epoch 371: val_loss did not improve from 28.13069
196/196 - 38s - loss: 27.7647 - MinusLogProbMetric: 27.7647 - val_loss: 28.2936 - val_MinusLogProbMetric: 28.2936 - lr: 5.0000e-04 - 38s/epoch - 191ms/step
Epoch 372/1000
2023-09-29 11:14:36.235 
Epoch 372/1000 
	 loss: 27.7472, MinusLogProbMetric: 27.7472, val_loss: 28.3598, val_MinusLogProbMetric: 28.3598

Epoch 372: val_loss did not improve from 28.13069
196/196 - 41s - loss: 27.7472 - MinusLogProbMetric: 27.7472 - val_loss: 28.3598 - val_MinusLogProbMetric: 28.3598 - lr: 5.0000e-04 - 41s/epoch - 211ms/step
Epoch 373/1000
2023-09-29 11:15:17.553 
Epoch 373/1000 
	 loss: 27.7348, MinusLogProbMetric: 27.7348, val_loss: 28.4003, val_MinusLogProbMetric: 28.4003

Epoch 373: val_loss did not improve from 28.13069
196/196 - 41s - loss: 27.7348 - MinusLogProbMetric: 27.7348 - val_loss: 28.4003 - val_MinusLogProbMetric: 28.4003 - lr: 5.0000e-04 - 41s/epoch - 211ms/step
Epoch 374/1000
2023-09-29 11:15:57.289 
Epoch 374/1000 
	 loss: 27.7859, MinusLogProbMetric: 27.7859, val_loss: 28.4233, val_MinusLogProbMetric: 28.4233

Epoch 374: val_loss did not improve from 28.13069
196/196 - 40s - loss: 27.7859 - MinusLogProbMetric: 27.7859 - val_loss: 28.4233 - val_MinusLogProbMetric: 28.4233 - lr: 5.0000e-04 - 40s/epoch - 203ms/step
Epoch 375/1000
2023-09-29 11:16:36.227 
Epoch 375/1000 
	 loss: 27.7814, MinusLogProbMetric: 27.7814, val_loss: 28.2426, val_MinusLogProbMetric: 28.2426

Epoch 375: val_loss did not improve from 28.13069
196/196 - 39s - loss: 27.7814 - MinusLogProbMetric: 27.7814 - val_loss: 28.2426 - val_MinusLogProbMetric: 28.2426 - lr: 5.0000e-04 - 39s/epoch - 199ms/step
Epoch 376/1000
2023-09-29 11:17:15.373 
Epoch 376/1000 
	 loss: 27.7347, MinusLogProbMetric: 27.7347, val_loss: 28.3096, val_MinusLogProbMetric: 28.3096

Epoch 376: val_loss did not improve from 28.13069
196/196 - 39s - loss: 27.7347 - MinusLogProbMetric: 27.7347 - val_loss: 28.3096 - val_MinusLogProbMetric: 28.3096 - lr: 5.0000e-04 - 39s/epoch - 200ms/step
Epoch 377/1000
2023-09-29 11:17:55.415 
Epoch 377/1000 
	 loss: 27.7722, MinusLogProbMetric: 27.7722, val_loss: 28.4064, val_MinusLogProbMetric: 28.4064

Epoch 377: val_loss did not improve from 28.13069
196/196 - 40s - loss: 27.7722 - MinusLogProbMetric: 27.7722 - val_loss: 28.4064 - val_MinusLogProbMetric: 28.4064 - lr: 5.0000e-04 - 40s/epoch - 204ms/step
Epoch 378/1000
2023-09-29 11:18:36.229 
Epoch 378/1000 
	 loss: 27.7383, MinusLogProbMetric: 27.7383, val_loss: 28.2718, val_MinusLogProbMetric: 28.2718

Epoch 378: val_loss did not improve from 28.13069
196/196 - 41s - loss: 27.7383 - MinusLogProbMetric: 27.7383 - val_loss: 28.2718 - val_MinusLogProbMetric: 28.2718 - lr: 5.0000e-04 - 41s/epoch - 208ms/step
Epoch 379/1000
2023-09-29 11:19:16.611 
Epoch 379/1000 
	 loss: 27.7428, MinusLogProbMetric: 27.7428, val_loss: 28.3598, val_MinusLogProbMetric: 28.3598

Epoch 379: val_loss did not improve from 28.13069
196/196 - 40s - loss: 27.7428 - MinusLogProbMetric: 27.7428 - val_loss: 28.3598 - val_MinusLogProbMetric: 28.3598 - lr: 5.0000e-04 - 40s/epoch - 206ms/step
Epoch 380/1000
2023-09-29 11:19:57.720 
Epoch 380/1000 
	 loss: 27.7400, MinusLogProbMetric: 27.7400, val_loss: 28.4825, val_MinusLogProbMetric: 28.4825

Epoch 380: val_loss did not improve from 28.13069
196/196 - 41s - loss: 27.7400 - MinusLogProbMetric: 27.7400 - val_loss: 28.4825 - val_MinusLogProbMetric: 28.4825 - lr: 5.0000e-04 - 41s/epoch - 210ms/step
Epoch 381/1000
2023-09-29 11:20:38.645 
Epoch 381/1000 
	 loss: 27.7329, MinusLogProbMetric: 27.7329, val_loss: 28.2082, val_MinusLogProbMetric: 28.2082

Epoch 381: val_loss did not improve from 28.13069
196/196 - 41s - loss: 27.7329 - MinusLogProbMetric: 27.7329 - val_loss: 28.2082 - val_MinusLogProbMetric: 28.2082 - lr: 5.0000e-04 - 41s/epoch - 209ms/step
Epoch 382/1000
2023-09-29 11:21:18.509 
Epoch 382/1000 
	 loss: 27.7204, MinusLogProbMetric: 27.7204, val_loss: 28.6589, val_MinusLogProbMetric: 28.6589

Epoch 382: val_loss did not improve from 28.13069
196/196 - 40s - loss: 27.7204 - MinusLogProbMetric: 27.7204 - val_loss: 28.6589 - val_MinusLogProbMetric: 28.6589 - lr: 5.0000e-04 - 40s/epoch - 203ms/step
Epoch 383/1000
2023-09-29 11:21:59.898 
Epoch 383/1000 
	 loss: 27.7540, MinusLogProbMetric: 27.7540, val_loss: 28.3257, val_MinusLogProbMetric: 28.3257

Epoch 383: val_loss did not improve from 28.13069
196/196 - 41s - loss: 27.7540 - MinusLogProbMetric: 27.7540 - val_loss: 28.3257 - val_MinusLogProbMetric: 28.3257 - lr: 5.0000e-04 - 41s/epoch - 211ms/step
Epoch 384/1000
2023-09-29 11:22:40.922 
Epoch 384/1000 
	 loss: 27.7242, MinusLogProbMetric: 27.7242, val_loss: 28.2336, val_MinusLogProbMetric: 28.2336

Epoch 384: val_loss did not improve from 28.13069
196/196 - 41s - loss: 27.7242 - MinusLogProbMetric: 27.7242 - val_loss: 28.2336 - val_MinusLogProbMetric: 28.2336 - lr: 5.0000e-04 - 41s/epoch - 209ms/step
Epoch 385/1000
2023-09-29 11:23:21.552 
Epoch 385/1000 
	 loss: 27.7325, MinusLogProbMetric: 27.7325, val_loss: 28.1915, val_MinusLogProbMetric: 28.1915

Epoch 385: val_loss did not improve from 28.13069
196/196 - 41s - loss: 27.7325 - MinusLogProbMetric: 27.7325 - val_loss: 28.1915 - val_MinusLogProbMetric: 28.1915 - lr: 5.0000e-04 - 41s/epoch - 207ms/step
Epoch 386/1000
2023-09-29 11:24:02.018 
Epoch 386/1000 
	 loss: 27.7040, MinusLogProbMetric: 27.7040, val_loss: 28.4382, val_MinusLogProbMetric: 28.4382

Epoch 386: val_loss did not improve from 28.13069
196/196 - 40s - loss: 27.7040 - MinusLogProbMetric: 27.7040 - val_loss: 28.4382 - val_MinusLogProbMetric: 28.4382 - lr: 5.0000e-04 - 40s/epoch - 206ms/step
Epoch 387/1000
2023-09-29 11:24:41.700 
Epoch 387/1000 
	 loss: 27.7284, MinusLogProbMetric: 27.7284, val_loss: 28.4454, val_MinusLogProbMetric: 28.4454

Epoch 387: val_loss did not improve from 28.13069
196/196 - 40s - loss: 27.7284 - MinusLogProbMetric: 27.7284 - val_loss: 28.4454 - val_MinusLogProbMetric: 28.4454 - lr: 5.0000e-04 - 40s/epoch - 202ms/step
Epoch 388/1000
2023-09-29 11:25:21.005 
Epoch 388/1000 
	 loss: 27.7420, MinusLogProbMetric: 27.7420, val_loss: 28.3045, val_MinusLogProbMetric: 28.3045

Epoch 388: val_loss did not improve from 28.13069
196/196 - 39s - loss: 27.7420 - MinusLogProbMetric: 27.7420 - val_loss: 28.3045 - val_MinusLogProbMetric: 28.3045 - lr: 5.0000e-04 - 39s/epoch - 201ms/step
Epoch 389/1000
2023-09-29 11:26:01.246 
Epoch 389/1000 
	 loss: 27.7669, MinusLogProbMetric: 27.7669, val_loss: 28.8051, val_MinusLogProbMetric: 28.8051

Epoch 389: val_loss did not improve from 28.13069
196/196 - 40s - loss: 27.7669 - MinusLogProbMetric: 27.7669 - val_loss: 28.8051 - val_MinusLogProbMetric: 28.8051 - lr: 5.0000e-04 - 40s/epoch - 205ms/step
Epoch 390/1000
2023-09-29 11:26:43.196 
Epoch 390/1000 
	 loss: 27.7306, MinusLogProbMetric: 27.7306, val_loss: 28.5023, val_MinusLogProbMetric: 28.5023

Epoch 390: val_loss did not improve from 28.13069
196/196 - 42s - loss: 27.7306 - MinusLogProbMetric: 27.7306 - val_loss: 28.5023 - val_MinusLogProbMetric: 28.5023 - lr: 5.0000e-04 - 42s/epoch - 214ms/step
Epoch 391/1000
2023-09-29 11:27:23.684 
Epoch 391/1000 
	 loss: 27.7753, MinusLogProbMetric: 27.7753, val_loss: 28.2771, val_MinusLogProbMetric: 28.2771

Epoch 391: val_loss did not improve from 28.13069
196/196 - 40s - loss: 27.7753 - MinusLogProbMetric: 27.7753 - val_loss: 28.2771 - val_MinusLogProbMetric: 28.2771 - lr: 5.0000e-04 - 40s/epoch - 207ms/step
Epoch 392/1000
2023-09-29 11:28:06.642 
Epoch 392/1000 
	 loss: 27.7385, MinusLogProbMetric: 27.7385, val_loss: 28.2553, val_MinusLogProbMetric: 28.2553

Epoch 392: val_loss did not improve from 28.13069
196/196 - 43s - loss: 27.7385 - MinusLogProbMetric: 27.7385 - val_loss: 28.2553 - val_MinusLogProbMetric: 28.2553 - lr: 5.0000e-04 - 43s/epoch - 219ms/step
Epoch 393/1000
2023-09-29 11:28:46.821 
Epoch 393/1000 
	 loss: 27.7243, MinusLogProbMetric: 27.7243, val_loss: 28.2734, val_MinusLogProbMetric: 28.2734

Epoch 393: val_loss did not improve from 28.13069
196/196 - 40s - loss: 27.7243 - MinusLogProbMetric: 27.7243 - val_loss: 28.2734 - val_MinusLogProbMetric: 28.2734 - lr: 5.0000e-04 - 40s/epoch - 205ms/step
Epoch 394/1000
2023-09-29 11:29:26.994 
Epoch 394/1000 
	 loss: 27.7256, MinusLogProbMetric: 27.7256, val_loss: 28.2449, val_MinusLogProbMetric: 28.2449

Epoch 394: val_loss did not improve from 28.13069
196/196 - 40s - loss: 27.7256 - MinusLogProbMetric: 27.7256 - val_loss: 28.2449 - val_MinusLogProbMetric: 28.2449 - lr: 5.0000e-04 - 40s/epoch - 205ms/step
Epoch 395/1000
2023-09-29 11:30:08.017 
Epoch 395/1000 
	 loss: 27.6996, MinusLogProbMetric: 27.6996, val_loss: 28.2444, val_MinusLogProbMetric: 28.2444

Epoch 395: val_loss did not improve from 28.13069
196/196 - 41s - loss: 27.6996 - MinusLogProbMetric: 27.6996 - val_loss: 28.2444 - val_MinusLogProbMetric: 28.2444 - lr: 5.0000e-04 - 41s/epoch - 209ms/step
Epoch 396/1000
2023-09-29 11:30:46.770 
Epoch 396/1000 
	 loss: 27.7377, MinusLogProbMetric: 27.7377, val_loss: 28.2978, val_MinusLogProbMetric: 28.2978

Epoch 396: val_loss did not improve from 28.13069
196/196 - 39s - loss: 27.7377 - MinusLogProbMetric: 27.7377 - val_loss: 28.2978 - val_MinusLogProbMetric: 28.2978 - lr: 5.0000e-04 - 39s/epoch - 198ms/step
Epoch 397/1000
2023-09-29 11:31:27.311 
Epoch 397/1000 
	 loss: 27.7115, MinusLogProbMetric: 27.7115, val_loss: 28.3144, val_MinusLogProbMetric: 28.3144

Epoch 397: val_loss did not improve from 28.13069
196/196 - 41s - loss: 27.7115 - MinusLogProbMetric: 27.7115 - val_loss: 28.3144 - val_MinusLogProbMetric: 28.3144 - lr: 5.0000e-04 - 41s/epoch - 207ms/step
Epoch 398/1000
2023-09-29 11:32:05.829 
Epoch 398/1000 
	 loss: 27.7344, MinusLogProbMetric: 27.7344, val_loss: 28.2586, val_MinusLogProbMetric: 28.2586

Epoch 398: val_loss did not improve from 28.13069
196/196 - 39s - loss: 27.7344 - MinusLogProbMetric: 27.7344 - val_loss: 28.2586 - val_MinusLogProbMetric: 28.2586 - lr: 5.0000e-04 - 39s/epoch - 197ms/step
Epoch 399/1000
2023-09-29 11:32:45.240 
Epoch 399/1000 
	 loss: 27.7381, MinusLogProbMetric: 27.7381, val_loss: 28.5157, val_MinusLogProbMetric: 28.5157

Epoch 399: val_loss did not improve from 28.13069
196/196 - 39s - loss: 27.7381 - MinusLogProbMetric: 27.7381 - val_loss: 28.5157 - val_MinusLogProbMetric: 28.5157 - lr: 5.0000e-04 - 39s/epoch - 201ms/step
Epoch 400/1000
2023-09-29 11:33:23.304 
Epoch 400/1000 
	 loss: 27.7268, MinusLogProbMetric: 27.7268, val_loss: 28.6902, val_MinusLogProbMetric: 28.6902

Epoch 400: val_loss did not improve from 28.13069
196/196 - 38s - loss: 27.7268 - MinusLogProbMetric: 27.7268 - val_loss: 28.6902 - val_MinusLogProbMetric: 28.6902 - lr: 5.0000e-04 - 38s/epoch - 194ms/step
Epoch 401/1000
2023-09-29 11:34:02.900 
Epoch 401/1000 
	 loss: 27.7355, MinusLogProbMetric: 27.7355, val_loss: 28.3584, val_MinusLogProbMetric: 28.3584

Epoch 401: val_loss did not improve from 28.13069
196/196 - 40s - loss: 27.7355 - MinusLogProbMetric: 27.7355 - val_loss: 28.3584 - val_MinusLogProbMetric: 28.3584 - lr: 5.0000e-04 - 40s/epoch - 202ms/step
Epoch 402/1000
2023-09-29 11:34:42.242 
Epoch 402/1000 
	 loss: 27.7273, MinusLogProbMetric: 27.7273, val_loss: 28.3824, val_MinusLogProbMetric: 28.3824

Epoch 402: val_loss did not improve from 28.13069
196/196 - 39s - loss: 27.7273 - MinusLogProbMetric: 27.7273 - val_loss: 28.3824 - val_MinusLogProbMetric: 28.3824 - lr: 5.0000e-04 - 39s/epoch - 201ms/step
Epoch 403/1000
2023-09-29 11:35:21.348 
Epoch 403/1000 
	 loss: 27.7407, MinusLogProbMetric: 27.7407, val_loss: 28.3819, val_MinusLogProbMetric: 28.3819

Epoch 403: val_loss did not improve from 28.13069
196/196 - 39s - loss: 27.7407 - MinusLogProbMetric: 27.7407 - val_loss: 28.3819 - val_MinusLogProbMetric: 28.3819 - lr: 5.0000e-04 - 39s/epoch - 200ms/step
Epoch 404/1000
2023-09-29 11:36:01.274 
Epoch 404/1000 
	 loss: 27.7164, MinusLogProbMetric: 27.7164, val_loss: 28.2652, val_MinusLogProbMetric: 28.2652

Epoch 404: val_loss did not improve from 28.13069
196/196 - 40s - loss: 27.7164 - MinusLogProbMetric: 27.7164 - val_loss: 28.2652 - val_MinusLogProbMetric: 28.2652 - lr: 5.0000e-04 - 40s/epoch - 204ms/step
Epoch 405/1000
2023-09-29 11:36:43.541 
Epoch 405/1000 
	 loss: 27.7478, MinusLogProbMetric: 27.7478, val_loss: 28.2429, val_MinusLogProbMetric: 28.2429

Epoch 405: val_loss did not improve from 28.13069
196/196 - 42s - loss: 27.7478 - MinusLogProbMetric: 27.7478 - val_loss: 28.2429 - val_MinusLogProbMetric: 28.2429 - lr: 5.0000e-04 - 42s/epoch - 216ms/step
Epoch 406/1000
2023-09-29 11:37:25.731 
Epoch 406/1000 
	 loss: 27.7041, MinusLogProbMetric: 27.7041, val_loss: 28.3949, val_MinusLogProbMetric: 28.3949

Epoch 406: val_loss did not improve from 28.13069
196/196 - 42s - loss: 27.7041 - MinusLogProbMetric: 27.7041 - val_loss: 28.3949 - val_MinusLogProbMetric: 28.3949 - lr: 5.0000e-04 - 42s/epoch - 215ms/step
Epoch 407/1000
2023-09-29 11:38:05.966 
Epoch 407/1000 
	 loss: 27.7372, MinusLogProbMetric: 27.7372, val_loss: 28.4663, val_MinusLogProbMetric: 28.4663

Epoch 407: val_loss did not improve from 28.13069
196/196 - 40s - loss: 27.7372 - MinusLogProbMetric: 27.7372 - val_loss: 28.4663 - val_MinusLogProbMetric: 28.4663 - lr: 5.0000e-04 - 40s/epoch - 205ms/step
Epoch 408/1000
2023-09-29 11:38:45.750 
Epoch 408/1000 
	 loss: 27.7337, MinusLogProbMetric: 27.7337, val_loss: 28.2037, val_MinusLogProbMetric: 28.2037

Epoch 408: val_loss did not improve from 28.13069
196/196 - 40s - loss: 27.7337 - MinusLogProbMetric: 27.7337 - val_loss: 28.2037 - val_MinusLogProbMetric: 28.2037 - lr: 5.0000e-04 - 40s/epoch - 203ms/step
Epoch 409/1000
2023-09-29 11:39:26.368 
Epoch 409/1000 
	 loss: 27.5224, MinusLogProbMetric: 27.5224, val_loss: 28.1462, val_MinusLogProbMetric: 28.1462

Epoch 409: val_loss did not improve from 28.13069
196/196 - 41s - loss: 27.5224 - MinusLogProbMetric: 27.5224 - val_loss: 28.1462 - val_MinusLogProbMetric: 28.1462 - lr: 2.5000e-04 - 41s/epoch - 207ms/step
Epoch 410/1000
2023-09-29 11:40:05.906 
Epoch 410/1000 
	 loss: 27.5229, MinusLogProbMetric: 27.5229, val_loss: 28.1116, val_MinusLogProbMetric: 28.1116

Epoch 410: val_loss improved from 28.13069 to 28.11162, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_332/weights/best_weights.h5
196/196 - 40s - loss: 27.5229 - MinusLogProbMetric: 27.5229 - val_loss: 28.1116 - val_MinusLogProbMetric: 28.1116 - lr: 2.5000e-04 - 40s/epoch - 205ms/step
Epoch 411/1000
2023-09-29 11:40:45.997 
Epoch 411/1000 
	 loss: 27.5040, MinusLogProbMetric: 27.5040, val_loss: 28.0630, val_MinusLogProbMetric: 28.0630

Epoch 411: val_loss improved from 28.11162 to 28.06300, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_332/weights/best_weights.h5
196/196 - 40s - loss: 27.5040 - MinusLogProbMetric: 27.5040 - val_loss: 28.0630 - val_MinusLogProbMetric: 28.0630 - lr: 2.5000e-04 - 40s/epoch - 205ms/step
Epoch 412/1000
2023-09-29 11:41:27.636 
Epoch 412/1000 
	 loss: 27.4996, MinusLogProbMetric: 27.4996, val_loss: 28.0758, val_MinusLogProbMetric: 28.0758

Epoch 412: val_loss did not improve from 28.06300
196/196 - 41s - loss: 27.4996 - MinusLogProbMetric: 27.4996 - val_loss: 28.0758 - val_MinusLogProbMetric: 28.0758 - lr: 2.5000e-04 - 41s/epoch - 209ms/step
Epoch 413/1000
2023-09-29 11:42:07.378 
Epoch 413/1000 
	 loss: 27.5258, MinusLogProbMetric: 27.5258, val_loss: 28.1092, val_MinusLogProbMetric: 28.1092

Epoch 413: val_loss did not improve from 28.06300
196/196 - 40s - loss: 27.5258 - MinusLogProbMetric: 27.5258 - val_loss: 28.1092 - val_MinusLogProbMetric: 28.1092 - lr: 2.5000e-04 - 40s/epoch - 203ms/step
Epoch 414/1000
2023-09-29 11:42:49.113 
Epoch 414/1000 
	 loss: 27.5110, MinusLogProbMetric: 27.5110, val_loss: 28.1096, val_MinusLogProbMetric: 28.1096

Epoch 414: val_loss did not improve from 28.06300
196/196 - 42s - loss: 27.5110 - MinusLogProbMetric: 27.5110 - val_loss: 28.1096 - val_MinusLogProbMetric: 28.1096 - lr: 2.5000e-04 - 42s/epoch - 213ms/step
Epoch 415/1000
2023-09-29 11:43:30.658 
Epoch 415/1000 
	 loss: 27.5052, MinusLogProbMetric: 27.5052, val_loss: 28.0801, val_MinusLogProbMetric: 28.0801

Epoch 415: val_loss did not improve from 28.06300
196/196 - 42s - loss: 27.5052 - MinusLogProbMetric: 27.5052 - val_loss: 28.0801 - val_MinusLogProbMetric: 28.0801 - lr: 2.5000e-04 - 42s/epoch - 212ms/step
Epoch 416/1000
2023-09-29 11:44:12.873 
Epoch 416/1000 
	 loss: 27.5054, MinusLogProbMetric: 27.5054, val_loss: 28.0930, val_MinusLogProbMetric: 28.0930

Epoch 416: val_loss did not improve from 28.06300
196/196 - 42s - loss: 27.5054 - MinusLogProbMetric: 27.5054 - val_loss: 28.0930 - val_MinusLogProbMetric: 28.0930 - lr: 2.5000e-04 - 42s/epoch - 215ms/step
Epoch 417/1000
2023-09-29 11:44:51.917 
Epoch 417/1000 
	 loss: 27.5183, MinusLogProbMetric: 27.5183, val_loss: 28.3107, val_MinusLogProbMetric: 28.3107

Epoch 417: val_loss did not improve from 28.06300
196/196 - 39s - loss: 27.5183 - MinusLogProbMetric: 27.5183 - val_loss: 28.3107 - val_MinusLogProbMetric: 28.3107 - lr: 2.5000e-04 - 39s/epoch - 199ms/step
Epoch 418/1000
2023-09-29 11:45:30.314 
Epoch 418/1000 
	 loss: 27.5158, MinusLogProbMetric: 27.5158, val_loss: 28.0733, val_MinusLogProbMetric: 28.0733

Epoch 418: val_loss did not improve from 28.06300
196/196 - 38s - loss: 27.5158 - MinusLogProbMetric: 27.5158 - val_loss: 28.0733 - val_MinusLogProbMetric: 28.0733 - lr: 2.5000e-04 - 38s/epoch - 196ms/step
Epoch 419/1000
2023-09-29 11:46:11.446 
Epoch 419/1000 
	 loss: 27.5041, MinusLogProbMetric: 27.5041, val_loss: 28.0845, val_MinusLogProbMetric: 28.0845

Epoch 419: val_loss did not improve from 28.06300
196/196 - 41s - loss: 27.5041 - MinusLogProbMetric: 27.5041 - val_loss: 28.0845 - val_MinusLogProbMetric: 28.0845 - lr: 2.5000e-04 - 41s/epoch - 210ms/step
Epoch 420/1000
2023-09-29 11:46:50.353 
Epoch 420/1000 
	 loss: 27.5152, MinusLogProbMetric: 27.5152, val_loss: 28.0937, val_MinusLogProbMetric: 28.0937

Epoch 420: val_loss did not improve from 28.06300
196/196 - 39s - loss: 27.5152 - MinusLogProbMetric: 27.5152 - val_loss: 28.0937 - val_MinusLogProbMetric: 28.0937 - lr: 2.5000e-04 - 39s/epoch - 199ms/step
Epoch 421/1000
2023-09-29 11:47:28.353 
Epoch 421/1000 
	 loss: 27.5072, MinusLogProbMetric: 27.5072, val_loss: 28.1340, val_MinusLogProbMetric: 28.1340

Epoch 421: val_loss did not improve from 28.06300
196/196 - 38s - loss: 27.5072 - MinusLogProbMetric: 27.5072 - val_loss: 28.1340 - val_MinusLogProbMetric: 28.1340 - lr: 2.5000e-04 - 38s/epoch - 194ms/step
Epoch 422/1000
2023-09-29 11:48:09.789 
Epoch 422/1000 
	 loss: 27.4987, MinusLogProbMetric: 27.4987, val_loss: 28.1100, val_MinusLogProbMetric: 28.1100

Epoch 422: val_loss did not improve from 28.06300
196/196 - 41s - loss: 27.4987 - MinusLogProbMetric: 27.4987 - val_loss: 28.1100 - val_MinusLogProbMetric: 28.1100 - lr: 2.5000e-04 - 41s/epoch - 211ms/step
Epoch 423/1000
2023-09-29 11:48:51.220 
Epoch 423/1000 
	 loss: 27.4889, MinusLogProbMetric: 27.4889, val_loss: 28.1390, val_MinusLogProbMetric: 28.1390

Epoch 423: val_loss did not improve from 28.06300
196/196 - 41s - loss: 27.4889 - MinusLogProbMetric: 27.4889 - val_loss: 28.1390 - val_MinusLogProbMetric: 28.1390 - lr: 2.5000e-04 - 41s/epoch - 211ms/step
Epoch 424/1000
2023-09-29 11:49:29.734 
Epoch 424/1000 
	 loss: 27.5059, MinusLogProbMetric: 27.5059, val_loss: 28.1461, val_MinusLogProbMetric: 28.1461

Epoch 424: val_loss did not improve from 28.06300
196/196 - 39s - loss: 27.5059 - MinusLogProbMetric: 27.5059 - val_loss: 28.1461 - val_MinusLogProbMetric: 28.1461 - lr: 2.5000e-04 - 39s/epoch - 196ms/step
Epoch 425/1000
2023-09-29 11:50:08.175 
Epoch 425/1000 
	 loss: 27.4932, MinusLogProbMetric: 27.4932, val_loss: 28.0676, val_MinusLogProbMetric: 28.0676

Epoch 425: val_loss did not improve from 28.06300
196/196 - 38s - loss: 27.4932 - MinusLogProbMetric: 27.4932 - val_loss: 28.0676 - val_MinusLogProbMetric: 28.0676 - lr: 2.5000e-04 - 38s/epoch - 196ms/step
Epoch 426/1000
2023-09-29 11:50:46.431 
Epoch 426/1000 
	 loss: 27.4968, MinusLogProbMetric: 27.4968, val_loss: 28.1148, val_MinusLogProbMetric: 28.1148

Epoch 426: val_loss did not improve from 28.06300
196/196 - 38s - loss: 27.4968 - MinusLogProbMetric: 27.4968 - val_loss: 28.1148 - val_MinusLogProbMetric: 28.1148 - lr: 2.5000e-04 - 38s/epoch - 195ms/step
Epoch 427/1000
2023-09-29 11:51:24.647 
Epoch 427/1000 
	 loss: 27.4951, MinusLogProbMetric: 27.4951, val_loss: 28.0543, val_MinusLogProbMetric: 28.0543

Epoch 427: val_loss improved from 28.06300 to 28.05425, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_332/weights/best_weights.h5
196/196 - 39s - loss: 27.4951 - MinusLogProbMetric: 27.4951 - val_loss: 28.0543 - val_MinusLogProbMetric: 28.0543 - lr: 2.5000e-04 - 39s/epoch - 199ms/step
Epoch 428/1000
2023-09-29 11:52:04.832 
Epoch 428/1000 
	 loss: 27.4934, MinusLogProbMetric: 27.4934, val_loss: 28.0769, val_MinusLogProbMetric: 28.0769

Epoch 428: val_loss did not improve from 28.05425
196/196 - 39s - loss: 27.4934 - MinusLogProbMetric: 27.4934 - val_loss: 28.0769 - val_MinusLogProbMetric: 28.0769 - lr: 2.5000e-04 - 39s/epoch - 201ms/step
Epoch 429/1000
2023-09-29 11:52:43.267 
Epoch 429/1000 
	 loss: 27.4946, MinusLogProbMetric: 27.4946, val_loss: 28.0922, val_MinusLogProbMetric: 28.0922

Epoch 429: val_loss did not improve from 28.05425
196/196 - 38s - loss: 27.4946 - MinusLogProbMetric: 27.4946 - val_loss: 28.0922 - val_MinusLogProbMetric: 28.0922 - lr: 2.5000e-04 - 38s/epoch - 196ms/step
Epoch 430/1000
2023-09-29 11:53:20.483 
Epoch 430/1000 
	 loss: 27.4955, MinusLogProbMetric: 27.4955, val_loss: 28.0793, val_MinusLogProbMetric: 28.0793

Epoch 430: val_loss did not improve from 28.05425
196/196 - 37s - loss: 27.4955 - MinusLogProbMetric: 27.4955 - val_loss: 28.0793 - val_MinusLogProbMetric: 28.0793 - lr: 2.5000e-04 - 37s/epoch - 190ms/step
Epoch 431/1000
2023-09-29 11:53:58.471 
Epoch 431/1000 
	 loss: 27.5026, MinusLogProbMetric: 27.5026, val_loss: 28.1166, val_MinusLogProbMetric: 28.1166

Epoch 431: val_loss did not improve from 28.05425
196/196 - 38s - loss: 27.5026 - MinusLogProbMetric: 27.5026 - val_loss: 28.1166 - val_MinusLogProbMetric: 28.1166 - lr: 2.5000e-04 - 38s/epoch - 194ms/step
Epoch 432/1000
2023-09-29 11:54:37.759 
Epoch 432/1000 
	 loss: 27.4964, MinusLogProbMetric: 27.4964, val_loss: 28.0365, val_MinusLogProbMetric: 28.0365

Epoch 432: val_loss improved from 28.05425 to 28.03651, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_332/weights/best_weights.h5
196/196 - 40s - loss: 27.4964 - MinusLogProbMetric: 27.4964 - val_loss: 28.0365 - val_MinusLogProbMetric: 28.0365 - lr: 2.5000e-04 - 40s/epoch - 206ms/step
Epoch 433/1000
2023-09-29 11:55:18.645 
Epoch 433/1000 
	 loss: 27.4981, MinusLogProbMetric: 27.4981, val_loss: 28.0698, val_MinusLogProbMetric: 28.0698

Epoch 433: val_loss did not improve from 28.03651
196/196 - 40s - loss: 27.4981 - MinusLogProbMetric: 27.4981 - val_loss: 28.0698 - val_MinusLogProbMetric: 28.0698 - lr: 2.5000e-04 - 40s/epoch - 203ms/step
Epoch 434/1000
2023-09-29 11:55:57.523 
Epoch 434/1000 
	 loss: 27.5051, MinusLogProbMetric: 27.5051, val_loss: 28.0808, val_MinusLogProbMetric: 28.0808

Epoch 434: val_loss did not improve from 28.03651
196/196 - 39s - loss: 27.5051 - MinusLogProbMetric: 27.5051 - val_loss: 28.0808 - val_MinusLogProbMetric: 28.0808 - lr: 2.5000e-04 - 39s/epoch - 198ms/step
Epoch 435/1000
2023-09-29 11:56:36.045 
Epoch 435/1000 
	 loss: 27.5033, MinusLogProbMetric: 27.5033, val_loss: 28.0953, val_MinusLogProbMetric: 28.0953

Epoch 435: val_loss did not improve from 28.03651
196/196 - 39s - loss: 27.5033 - MinusLogProbMetric: 27.5033 - val_loss: 28.0953 - val_MinusLogProbMetric: 28.0953 - lr: 2.5000e-04 - 39s/epoch - 197ms/step
Epoch 436/1000
2023-09-29 11:57:16.025 
Epoch 436/1000 
	 loss: 27.5034, MinusLogProbMetric: 27.5034, val_loss: 28.1029, val_MinusLogProbMetric: 28.1029

Epoch 436: val_loss did not improve from 28.03651
196/196 - 40s - loss: 27.5034 - MinusLogProbMetric: 27.5034 - val_loss: 28.1029 - val_MinusLogProbMetric: 28.1029 - lr: 2.5000e-04 - 40s/epoch - 204ms/step
Epoch 437/1000
2023-09-29 11:57:54.575 
Epoch 437/1000 
	 loss: 27.4932, MinusLogProbMetric: 27.4932, val_loss: 28.0713, val_MinusLogProbMetric: 28.0713

Epoch 437: val_loss did not improve from 28.03651
196/196 - 39s - loss: 27.4932 - MinusLogProbMetric: 27.4932 - val_loss: 28.0713 - val_MinusLogProbMetric: 28.0713 - lr: 2.5000e-04 - 39s/epoch - 197ms/step
Epoch 438/1000
2023-09-29 11:58:33.888 
Epoch 438/1000 
	 loss: 27.4849, MinusLogProbMetric: 27.4849, val_loss: 28.1838, val_MinusLogProbMetric: 28.1838

Epoch 438: val_loss did not improve from 28.03651
196/196 - 39s - loss: 27.4849 - MinusLogProbMetric: 27.4849 - val_loss: 28.1838 - val_MinusLogProbMetric: 28.1838 - lr: 2.5000e-04 - 39s/epoch - 201ms/step
Epoch 439/1000
2023-09-29 11:59:14.493 
Epoch 439/1000 
	 loss: 27.4926, MinusLogProbMetric: 27.4926, val_loss: 28.1138, val_MinusLogProbMetric: 28.1138

Epoch 439: val_loss did not improve from 28.03651
196/196 - 41s - loss: 27.4926 - MinusLogProbMetric: 27.4926 - val_loss: 28.1138 - val_MinusLogProbMetric: 28.1138 - lr: 2.5000e-04 - 41s/epoch - 207ms/step
Epoch 440/1000
2023-09-29 11:59:56.948 
Epoch 440/1000 
	 loss: 27.4893, MinusLogProbMetric: 27.4893, val_loss: 28.0860, val_MinusLogProbMetric: 28.0860

Epoch 440: val_loss did not improve from 28.03651
196/196 - 42s - loss: 27.4893 - MinusLogProbMetric: 27.4893 - val_loss: 28.0860 - val_MinusLogProbMetric: 28.0860 - lr: 2.5000e-04 - 42s/epoch - 217ms/step
Epoch 441/1000
2023-09-29 12:00:36.323 
Epoch 441/1000 
	 loss: 27.4950, MinusLogProbMetric: 27.4950, val_loss: 28.1233, val_MinusLogProbMetric: 28.1233

Epoch 441: val_loss did not improve from 28.03651
196/196 - 39s - loss: 27.4950 - MinusLogProbMetric: 27.4950 - val_loss: 28.1233 - val_MinusLogProbMetric: 28.1233 - lr: 2.5000e-04 - 39s/epoch - 201ms/step
Epoch 442/1000
2023-09-29 12:01:16.320 
Epoch 442/1000 
	 loss: 27.4873, MinusLogProbMetric: 27.4873, val_loss: 28.1046, val_MinusLogProbMetric: 28.1046

Epoch 442: val_loss did not improve from 28.03651
196/196 - 40s - loss: 27.4873 - MinusLogProbMetric: 27.4873 - val_loss: 28.1046 - val_MinusLogProbMetric: 28.1046 - lr: 2.5000e-04 - 40s/epoch - 204ms/step
Epoch 443/1000
2023-09-29 12:01:55.446 
Epoch 443/1000 
	 loss: 27.4977, MinusLogProbMetric: 27.4977, val_loss: 28.0831, val_MinusLogProbMetric: 28.0831

Epoch 443: val_loss did not improve from 28.03651
196/196 - 39s - loss: 27.4977 - MinusLogProbMetric: 27.4977 - val_loss: 28.0831 - val_MinusLogProbMetric: 28.0831 - lr: 2.5000e-04 - 39s/epoch - 200ms/step
Epoch 444/1000
2023-09-29 12:02:35.155 
Epoch 444/1000 
	 loss: 27.4804, MinusLogProbMetric: 27.4804, val_loss: 28.1241, val_MinusLogProbMetric: 28.1241

Epoch 444: val_loss did not improve from 28.03651
196/196 - 40s - loss: 27.4804 - MinusLogProbMetric: 27.4804 - val_loss: 28.1241 - val_MinusLogProbMetric: 28.1241 - lr: 2.5000e-04 - 40s/epoch - 203ms/step
Epoch 445/1000
2023-09-29 12:03:13.883 
Epoch 445/1000 
	 loss: 27.5058, MinusLogProbMetric: 27.5058, val_loss: 28.0874, val_MinusLogProbMetric: 28.0874

Epoch 445: val_loss did not improve from 28.03651
196/196 - 39s - loss: 27.5058 - MinusLogProbMetric: 27.5058 - val_loss: 28.0874 - val_MinusLogProbMetric: 28.0874 - lr: 2.5000e-04 - 39s/epoch - 198ms/step
Epoch 446/1000
2023-09-29 12:03:51.597 
Epoch 446/1000 
	 loss: 27.4901, MinusLogProbMetric: 27.4901, val_loss: 28.1078, val_MinusLogProbMetric: 28.1078

Epoch 446: val_loss did not improve from 28.03651
196/196 - 38s - loss: 27.4901 - MinusLogProbMetric: 27.4901 - val_loss: 28.1078 - val_MinusLogProbMetric: 28.1078 - lr: 2.5000e-04 - 38s/epoch - 192ms/step
Epoch 447/1000
2023-09-29 12:04:31.020 
Epoch 447/1000 
	 loss: 27.5093, MinusLogProbMetric: 27.5093, val_loss: 28.1067, val_MinusLogProbMetric: 28.1067

Epoch 447: val_loss did not improve from 28.03651
196/196 - 39s - loss: 27.5093 - MinusLogProbMetric: 27.5093 - val_loss: 28.1067 - val_MinusLogProbMetric: 28.1067 - lr: 2.5000e-04 - 39s/epoch - 201ms/step
Epoch 448/1000
2023-09-29 12:05:09.863 
Epoch 448/1000 
	 loss: 27.4921, MinusLogProbMetric: 27.4921, val_loss: 28.1686, val_MinusLogProbMetric: 28.1686

Epoch 448: val_loss did not improve from 28.03651
196/196 - 39s - loss: 27.4921 - MinusLogProbMetric: 27.4921 - val_loss: 28.1686 - val_MinusLogProbMetric: 28.1686 - lr: 2.5000e-04 - 39s/epoch - 198ms/step
Epoch 449/1000
2023-09-29 12:05:50.053 
Epoch 449/1000 
	 loss: 27.4984, MinusLogProbMetric: 27.4984, val_loss: 28.1030, val_MinusLogProbMetric: 28.1030

Epoch 449: val_loss did not improve from 28.03651
196/196 - 40s - loss: 27.4984 - MinusLogProbMetric: 27.4984 - val_loss: 28.1030 - val_MinusLogProbMetric: 28.1030 - lr: 2.5000e-04 - 40s/epoch - 205ms/step
Epoch 450/1000
2023-09-29 12:06:28.709 
Epoch 450/1000 
	 loss: 27.4816, MinusLogProbMetric: 27.4816, val_loss: 28.1344, val_MinusLogProbMetric: 28.1344

Epoch 450: val_loss did not improve from 28.03651
196/196 - 39s - loss: 27.4816 - MinusLogProbMetric: 27.4816 - val_loss: 28.1344 - val_MinusLogProbMetric: 28.1344 - lr: 2.5000e-04 - 39s/epoch - 197ms/step
Epoch 451/1000
2023-09-29 12:07:06.806 
Epoch 451/1000 
	 loss: 27.4824, MinusLogProbMetric: 27.4824, val_loss: 28.1164, val_MinusLogProbMetric: 28.1164

Epoch 451: val_loss did not improve from 28.03651
196/196 - 38s - loss: 27.4824 - MinusLogProbMetric: 27.4824 - val_loss: 28.1164 - val_MinusLogProbMetric: 28.1164 - lr: 2.5000e-04 - 38s/epoch - 194ms/step
Epoch 452/1000
2023-09-29 12:07:45.326 
Epoch 452/1000 
	 loss: 27.4796, MinusLogProbMetric: 27.4796, val_loss: 28.1074, val_MinusLogProbMetric: 28.1074

Epoch 452: val_loss did not improve from 28.03651
196/196 - 39s - loss: 27.4796 - MinusLogProbMetric: 27.4796 - val_loss: 28.1074 - val_MinusLogProbMetric: 28.1074 - lr: 2.5000e-04 - 39s/epoch - 197ms/step
Epoch 453/1000
2023-09-29 12:08:23.420 
Epoch 453/1000 
	 loss: 27.4754, MinusLogProbMetric: 27.4754, val_loss: 28.0830, val_MinusLogProbMetric: 28.0830

Epoch 453: val_loss did not improve from 28.03651
196/196 - 38s - loss: 27.4754 - MinusLogProbMetric: 27.4754 - val_loss: 28.0830 - val_MinusLogProbMetric: 28.0830 - lr: 2.5000e-04 - 38s/epoch - 194ms/step
Epoch 454/1000
2023-09-29 12:09:00.916 
Epoch 454/1000 
	 loss: 27.4915, MinusLogProbMetric: 27.4915, val_loss: 28.0797, val_MinusLogProbMetric: 28.0797

Epoch 454: val_loss did not improve from 28.03651
196/196 - 37s - loss: 27.4915 - MinusLogProbMetric: 27.4915 - val_loss: 28.0797 - val_MinusLogProbMetric: 28.0797 - lr: 2.5000e-04 - 37s/epoch - 191ms/step
Epoch 455/1000
2023-09-29 12:09:41.366 
Epoch 455/1000 
	 loss: 27.4790, MinusLogProbMetric: 27.4790, val_loss: 28.1049, val_MinusLogProbMetric: 28.1049

Epoch 455: val_loss did not improve from 28.03651
196/196 - 40s - loss: 27.4790 - MinusLogProbMetric: 27.4790 - val_loss: 28.1049 - val_MinusLogProbMetric: 28.1049 - lr: 2.5000e-04 - 40s/epoch - 206ms/step
Epoch 456/1000
2023-09-29 12:10:19.376 
Epoch 456/1000 
	 loss: 27.4770, MinusLogProbMetric: 27.4770, val_loss: 28.0924, val_MinusLogProbMetric: 28.0924

Epoch 456: val_loss did not improve from 28.03651
196/196 - 38s - loss: 27.4770 - MinusLogProbMetric: 27.4770 - val_loss: 28.0924 - val_MinusLogProbMetric: 28.0924 - lr: 2.5000e-04 - 38s/epoch - 194ms/step
Epoch 457/1000
2023-09-29 12:10:59.486 
Epoch 457/1000 
	 loss: 27.4932, MinusLogProbMetric: 27.4932, val_loss: 28.0805, val_MinusLogProbMetric: 28.0805

Epoch 457: val_loss did not improve from 28.03651
196/196 - 40s - loss: 27.4932 - MinusLogProbMetric: 27.4932 - val_loss: 28.0805 - val_MinusLogProbMetric: 28.0805 - lr: 2.5000e-04 - 40s/epoch - 205ms/step
Epoch 458/1000
2023-09-29 12:11:39.604 
Epoch 458/1000 
	 loss: 27.5103, MinusLogProbMetric: 27.5103, val_loss: 28.1647, val_MinusLogProbMetric: 28.1647

Epoch 458: val_loss did not improve from 28.03651
196/196 - 40s - loss: 27.5103 - MinusLogProbMetric: 27.5103 - val_loss: 28.1647 - val_MinusLogProbMetric: 28.1647 - lr: 2.5000e-04 - 40s/epoch - 205ms/step
Epoch 459/1000
2023-09-29 12:12:19.790 
Epoch 459/1000 
	 loss: 27.4717, MinusLogProbMetric: 27.4717, val_loss: 28.1117, val_MinusLogProbMetric: 28.1117

Epoch 459: val_loss did not improve from 28.03651
196/196 - 40s - loss: 27.4717 - MinusLogProbMetric: 27.4717 - val_loss: 28.1117 - val_MinusLogProbMetric: 28.1117 - lr: 2.5000e-04 - 40s/epoch - 205ms/step
Epoch 460/1000
2023-09-29 12:12:58.068 
Epoch 460/1000 
	 loss: 27.4833, MinusLogProbMetric: 27.4833, val_loss: 28.1539, val_MinusLogProbMetric: 28.1539

Epoch 460: val_loss did not improve from 28.03651
196/196 - 38s - loss: 27.4833 - MinusLogProbMetric: 27.4833 - val_loss: 28.1539 - val_MinusLogProbMetric: 28.1539 - lr: 2.5000e-04 - 38s/epoch - 195ms/step
Epoch 461/1000
2023-09-29 12:13:37.099 
Epoch 461/1000 
	 loss: 27.4910, MinusLogProbMetric: 27.4910, val_loss: 28.0848, val_MinusLogProbMetric: 28.0848

Epoch 461: val_loss did not improve from 28.03651
196/196 - 39s - loss: 27.4910 - MinusLogProbMetric: 27.4910 - val_loss: 28.0848 - val_MinusLogProbMetric: 28.0848 - lr: 2.5000e-04 - 39s/epoch - 199ms/step
Epoch 462/1000
2023-09-29 12:14:14.708 
Epoch 462/1000 
	 loss: 27.4763, MinusLogProbMetric: 27.4763, val_loss: 28.0842, val_MinusLogProbMetric: 28.0842

Epoch 462: val_loss did not improve from 28.03651
196/196 - 38s - loss: 27.4763 - MinusLogProbMetric: 27.4763 - val_loss: 28.0842 - val_MinusLogProbMetric: 28.0842 - lr: 2.5000e-04 - 38s/epoch - 192ms/step
Epoch 463/1000
2023-09-29 12:14:53.418 
Epoch 463/1000 
	 loss: 27.4798, MinusLogProbMetric: 27.4798, val_loss: 28.0515, val_MinusLogProbMetric: 28.0515

Epoch 463: val_loss did not improve from 28.03651
196/196 - 39s - loss: 27.4798 - MinusLogProbMetric: 27.4798 - val_loss: 28.0515 - val_MinusLogProbMetric: 28.0515 - lr: 2.5000e-04 - 39s/epoch - 197ms/step
Epoch 464/1000
2023-09-29 12:15:32.623 
Epoch 464/1000 
	 loss: 27.5120, MinusLogProbMetric: 27.5120, val_loss: 28.1615, val_MinusLogProbMetric: 28.1615

Epoch 464: val_loss did not improve from 28.03651
196/196 - 39s - loss: 27.5120 - MinusLogProbMetric: 27.5120 - val_loss: 28.1615 - val_MinusLogProbMetric: 28.1615 - lr: 2.5000e-04 - 39s/epoch - 200ms/step
Epoch 465/1000
2023-09-29 12:16:10.698 
Epoch 465/1000 
	 loss: 27.4937, MinusLogProbMetric: 27.4937, val_loss: 28.1068, val_MinusLogProbMetric: 28.1068

Epoch 465: val_loss did not improve from 28.03651
196/196 - 38s - loss: 27.4937 - MinusLogProbMetric: 27.4937 - val_loss: 28.1068 - val_MinusLogProbMetric: 28.1068 - lr: 2.5000e-04 - 38s/epoch - 194ms/step
Epoch 466/1000
2023-09-29 12:16:48.261 
Epoch 466/1000 
	 loss: 27.4749, MinusLogProbMetric: 27.4749, val_loss: 28.0700, val_MinusLogProbMetric: 28.0700

Epoch 466: val_loss did not improve from 28.03651
196/196 - 38s - loss: 27.4749 - MinusLogProbMetric: 27.4749 - val_loss: 28.0700 - val_MinusLogProbMetric: 28.0700 - lr: 2.5000e-04 - 38s/epoch - 192ms/step
Epoch 467/1000
2023-09-29 12:17:25.828 
Epoch 467/1000 
	 loss: 27.4757, MinusLogProbMetric: 27.4757, val_loss: 28.2141, val_MinusLogProbMetric: 28.2141

Epoch 467: val_loss did not improve from 28.03651
196/196 - 38s - loss: 27.4757 - MinusLogProbMetric: 27.4757 - val_loss: 28.2141 - val_MinusLogProbMetric: 28.2141 - lr: 2.5000e-04 - 38s/epoch - 192ms/step
Epoch 468/1000
2023-09-29 12:18:03.279 
Epoch 468/1000 
	 loss: 27.4858, MinusLogProbMetric: 27.4858, val_loss: 28.0660, val_MinusLogProbMetric: 28.0660

Epoch 468: val_loss did not improve from 28.03651
196/196 - 37s - loss: 27.4858 - MinusLogProbMetric: 27.4858 - val_loss: 28.0660 - val_MinusLogProbMetric: 28.0660 - lr: 2.5000e-04 - 37s/epoch - 191ms/step
Epoch 469/1000
2023-09-29 12:18:41.146 
Epoch 469/1000 
	 loss: 27.4919, MinusLogProbMetric: 27.4919, val_loss: 28.0819, val_MinusLogProbMetric: 28.0819

Epoch 469: val_loss did not improve from 28.03651
196/196 - 38s - loss: 27.4919 - MinusLogProbMetric: 27.4919 - val_loss: 28.0819 - val_MinusLogProbMetric: 28.0819 - lr: 2.5000e-04 - 38s/epoch - 193ms/step
Epoch 470/1000
2023-09-29 12:19:20.832 
Epoch 470/1000 
	 loss: 27.4757, MinusLogProbMetric: 27.4757, val_loss: 28.0864, val_MinusLogProbMetric: 28.0864

Epoch 470: val_loss did not improve from 28.03651
196/196 - 40s - loss: 27.4757 - MinusLogProbMetric: 27.4757 - val_loss: 28.0864 - val_MinusLogProbMetric: 28.0864 - lr: 2.5000e-04 - 40s/epoch - 203ms/step
Epoch 471/1000
2023-09-29 12:19:59.615 
Epoch 471/1000 
	 loss: 27.4829, MinusLogProbMetric: 27.4829, val_loss: 28.1272, val_MinusLogProbMetric: 28.1272

Epoch 471: val_loss did not improve from 28.03651
196/196 - 39s - loss: 27.4829 - MinusLogProbMetric: 27.4829 - val_loss: 28.1272 - val_MinusLogProbMetric: 28.1272 - lr: 2.5000e-04 - 39s/epoch - 197ms/step
Epoch 472/1000
2023-09-29 12:20:37.116 
Epoch 472/1000 
	 loss: 27.4829, MinusLogProbMetric: 27.4829, val_loss: 28.0879, val_MinusLogProbMetric: 28.0879

Epoch 472: val_loss did not improve from 28.03651
196/196 - 37s - loss: 27.4829 - MinusLogProbMetric: 27.4829 - val_loss: 28.0879 - val_MinusLogProbMetric: 28.0879 - lr: 2.5000e-04 - 37s/epoch - 191ms/step
Epoch 473/1000
2023-09-29 12:21:16.131 
Epoch 473/1000 
	 loss: 27.4842, MinusLogProbMetric: 27.4842, val_loss: 28.1877, val_MinusLogProbMetric: 28.1877

Epoch 473: val_loss did not improve from 28.03651
196/196 - 39s - loss: 27.4842 - MinusLogProbMetric: 27.4842 - val_loss: 28.1877 - val_MinusLogProbMetric: 28.1877 - lr: 2.5000e-04 - 39s/epoch - 199ms/step
Epoch 474/1000
2023-09-29 12:21:56.124 
Epoch 474/1000 
	 loss: 27.4845, MinusLogProbMetric: 27.4845, val_loss: 28.1277, val_MinusLogProbMetric: 28.1277

Epoch 474: val_loss did not improve from 28.03651
196/196 - 40s - loss: 27.4845 - MinusLogProbMetric: 27.4845 - val_loss: 28.1277 - val_MinusLogProbMetric: 28.1277 - lr: 2.5000e-04 - 40s/epoch - 204ms/step
Epoch 475/1000
2023-09-29 12:22:35.920 
Epoch 475/1000 
	 loss: 27.4831, MinusLogProbMetric: 27.4831, val_loss: 28.1522, val_MinusLogProbMetric: 28.1522

Epoch 475: val_loss did not improve from 28.03651
196/196 - 40s - loss: 27.4831 - MinusLogProbMetric: 27.4831 - val_loss: 28.1522 - val_MinusLogProbMetric: 28.1522 - lr: 2.5000e-04 - 40s/epoch - 203ms/step
Epoch 476/1000
2023-09-29 12:23:15.131 
Epoch 476/1000 
	 loss: 27.4901, MinusLogProbMetric: 27.4901, val_loss: 28.1960, val_MinusLogProbMetric: 28.1960

Epoch 476: val_loss did not improve from 28.03651
196/196 - 39s - loss: 27.4901 - MinusLogProbMetric: 27.4901 - val_loss: 28.1960 - val_MinusLogProbMetric: 28.1960 - lr: 2.5000e-04 - 39s/epoch - 200ms/step
Epoch 477/1000
2023-09-29 12:23:55.694 
Epoch 477/1000 
	 loss: 27.4950, MinusLogProbMetric: 27.4950, val_loss: 28.1296, val_MinusLogProbMetric: 28.1296

Epoch 477: val_loss did not improve from 28.03651
196/196 - 41s - loss: 27.4950 - MinusLogProbMetric: 27.4950 - val_loss: 28.1296 - val_MinusLogProbMetric: 28.1296 - lr: 2.5000e-04 - 41s/epoch - 207ms/step
Epoch 478/1000
2023-09-29 12:24:39.152 
Epoch 478/1000 
	 loss: 27.4885, MinusLogProbMetric: 27.4885, val_loss: 28.2032, val_MinusLogProbMetric: 28.2032

Epoch 478: val_loss did not improve from 28.03651
196/196 - 43s - loss: 27.4885 - MinusLogProbMetric: 27.4885 - val_loss: 28.2032 - val_MinusLogProbMetric: 28.2032 - lr: 2.5000e-04 - 43s/epoch - 222ms/step
Epoch 479/1000
2023-09-29 12:25:22.496 
Epoch 479/1000 
	 loss: 27.4925, MinusLogProbMetric: 27.4925, val_loss: 28.1096, val_MinusLogProbMetric: 28.1096

Epoch 479: val_loss did not improve from 28.03651
196/196 - 43s - loss: 27.4925 - MinusLogProbMetric: 27.4925 - val_loss: 28.1096 - val_MinusLogProbMetric: 28.1096 - lr: 2.5000e-04 - 43s/epoch - 221ms/step
Epoch 480/1000
2023-09-29 12:26:05.974 
Epoch 480/1000 
	 loss: 27.4764, MinusLogProbMetric: 27.4764, val_loss: 28.1732, val_MinusLogProbMetric: 28.1732

Epoch 480: val_loss did not improve from 28.03651
196/196 - 43s - loss: 27.4764 - MinusLogProbMetric: 27.4764 - val_loss: 28.1732 - val_MinusLogProbMetric: 28.1732 - lr: 2.5000e-04 - 43s/epoch - 222ms/step
Epoch 481/1000
2023-09-29 12:26:49.828 
Epoch 481/1000 
	 loss: 27.4938, MinusLogProbMetric: 27.4938, val_loss: 28.1114, val_MinusLogProbMetric: 28.1114

Epoch 481: val_loss did not improve from 28.03651
196/196 - 44s - loss: 27.4938 - MinusLogProbMetric: 27.4938 - val_loss: 28.1114 - val_MinusLogProbMetric: 28.1114 - lr: 2.5000e-04 - 44s/epoch - 224ms/step
Epoch 482/1000
2023-09-29 12:27:33.586 
Epoch 482/1000 
	 loss: 27.4735, MinusLogProbMetric: 27.4735, val_loss: 28.0964, val_MinusLogProbMetric: 28.0964

Epoch 482: val_loss did not improve from 28.03651
196/196 - 44s - loss: 27.4735 - MinusLogProbMetric: 27.4735 - val_loss: 28.0964 - val_MinusLogProbMetric: 28.0964 - lr: 2.5000e-04 - 44s/epoch - 223ms/step
Epoch 483/1000
2023-09-29 12:28:17.573 
Epoch 483/1000 
	 loss: 27.4030, MinusLogProbMetric: 27.4030, val_loss: 28.0629, val_MinusLogProbMetric: 28.0629

Epoch 483: val_loss did not improve from 28.03651
196/196 - 44s - loss: 27.4030 - MinusLogProbMetric: 27.4030 - val_loss: 28.0629 - val_MinusLogProbMetric: 28.0629 - lr: 1.2500e-04 - 44s/epoch - 224ms/step
Epoch 484/1000
2023-09-29 12:29:01.131 
Epoch 484/1000 
	 loss: 27.3973, MinusLogProbMetric: 27.3973, val_loss: 28.0421, val_MinusLogProbMetric: 28.0421

Epoch 484: val_loss did not improve from 28.03651
196/196 - 44s - loss: 27.3973 - MinusLogProbMetric: 27.3973 - val_loss: 28.0421 - val_MinusLogProbMetric: 28.0421 - lr: 1.2500e-04 - 44s/epoch - 222ms/step
Epoch 485/1000
2023-09-29 12:29:44.428 
Epoch 485/1000 
	 loss: 27.3937, MinusLogProbMetric: 27.3937, val_loss: 28.0357, val_MinusLogProbMetric: 28.0357

Epoch 485: val_loss improved from 28.03651 to 28.03573, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_332/weights/best_weights.h5
196/196 - 44s - loss: 27.3937 - MinusLogProbMetric: 27.3937 - val_loss: 28.0357 - val_MinusLogProbMetric: 28.0357 - lr: 1.2500e-04 - 44s/epoch - 225ms/step
Epoch 486/1000
2023-09-29 12:30:28.603 
Epoch 486/1000 
	 loss: 27.3976, MinusLogProbMetric: 27.3976, val_loss: 28.0517, val_MinusLogProbMetric: 28.0517

Epoch 486: val_loss did not improve from 28.03573
196/196 - 43s - loss: 27.3976 - MinusLogProbMetric: 27.3976 - val_loss: 28.0517 - val_MinusLogProbMetric: 28.0517 - lr: 1.2500e-04 - 43s/epoch - 221ms/step
Epoch 487/1000
2023-09-29 12:31:11.608 
Epoch 487/1000 
	 loss: 27.3991, MinusLogProbMetric: 27.3991, val_loss: 28.0041, val_MinusLogProbMetric: 28.0041

Epoch 487: val_loss improved from 28.03573 to 28.00410, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_332/weights/best_weights.h5
196/196 - 44s - loss: 27.3991 - MinusLogProbMetric: 27.3991 - val_loss: 28.0041 - val_MinusLogProbMetric: 28.0041 - lr: 1.2500e-04 - 44s/epoch - 223ms/step
Epoch 488/1000
2023-09-29 12:31:53.375 
Epoch 488/1000 
	 loss: 27.3941, MinusLogProbMetric: 27.3941, val_loss: 28.0245, val_MinusLogProbMetric: 28.0245

Epoch 488: val_loss did not improve from 28.00410
196/196 - 41s - loss: 27.3941 - MinusLogProbMetric: 27.3941 - val_loss: 28.0245 - val_MinusLogProbMetric: 28.0245 - lr: 1.2500e-04 - 41s/epoch - 209ms/step
Epoch 489/1000
2023-09-29 12:32:37.779 
Epoch 489/1000 
	 loss: 27.3908, MinusLogProbMetric: 27.3908, val_loss: 28.0405, val_MinusLogProbMetric: 28.0405

Epoch 489: val_loss did not improve from 28.00410
196/196 - 44s - loss: 27.3908 - MinusLogProbMetric: 27.3908 - val_loss: 28.0405 - val_MinusLogProbMetric: 28.0405 - lr: 1.2500e-04 - 44s/epoch - 227ms/step
Epoch 490/1000
2023-09-29 12:33:17.713 
Epoch 490/1000 
	 loss: 27.3921, MinusLogProbMetric: 27.3921, val_loss: 28.0412, val_MinusLogProbMetric: 28.0412

Epoch 490: val_loss did not improve from 28.00410
196/196 - 40s - loss: 27.3921 - MinusLogProbMetric: 27.3921 - val_loss: 28.0412 - val_MinusLogProbMetric: 28.0412 - lr: 1.2500e-04 - 40s/epoch - 204ms/step
Epoch 491/1000
2023-09-29 12:33:58.003 
Epoch 491/1000 
	 loss: 27.3944, MinusLogProbMetric: 27.3944, val_loss: 28.0453, val_MinusLogProbMetric: 28.0453

Epoch 491: val_loss did not improve from 28.00410
196/196 - 40s - loss: 27.3944 - MinusLogProbMetric: 27.3944 - val_loss: 28.0453 - val_MinusLogProbMetric: 28.0453 - lr: 1.2500e-04 - 40s/epoch - 206ms/step
Epoch 492/1000
2023-09-29 12:34:37.667 
Epoch 492/1000 
	 loss: 27.3897, MinusLogProbMetric: 27.3897, val_loss: 28.0521, val_MinusLogProbMetric: 28.0521

Epoch 492: val_loss did not improve from 28.00410
196/196 - 40s - loss: 27.3897 - MinusLogProbMetric: 27.3897 - val_loss: 28.0521 - val_MinusLogProbMetric: 28.0521 - lr: 1.2500e-04 - 40s/epoch - 202ms/step
Epoch 493/1000
2023-09-29 12:35:17.684 
Epoch 493/1000 
	 loss: 27.3960, MinusLogProbMetric: 27.3960, val_loss: 28.0216, val_MinusLogProbMetric: 28.0216

Epoch 493: val_loss did not improve from 28.00410
196/196 - 40s - loss: 27.3960 - MinusLogProbMetric: 27.3960 - val_loss: 28.0216 - val_MinusLogProbMetric: 28.0216 - lr: 1.2500e-04 - 40s/epoch - 204ms/step
Epoch 494/1000
2023-09-29 12:36:00.466 
Epoch 494/1000 
	 loss: 27.3864, MinusLogProbMetric: 27.3864, val_loss: 28.0381, val_MinusLogProbMetric: 28.0381

Epoch 494: val_loss did not improve from 28.00410
196/196 - 43s - loss: 27.3864 - MinusLogProbMetric: 27.3864 - val_loss: 28.0381 - val_MinusLogProbMetric: 28.0381 - lr: 1.2500e-04 - 43s/epoch - 218ms/step
Epoch 495/1000
2023-09-29 12:36:42.796 
Epoch 495/1000 
	 loss: 27.3940, MinusLogProbMetric: 27.3940, val_loss: 28.0138, val_MinusLogProbMetric: 28.0138

Epoch 495: val_loss did not improve from 28.00410
196/196 - 42s - loss: 27.3940 - MinusLogProbMetric: 27.3940 - val_loss: 28.0138 - val_MinusLogProbMetric: 28.0138 - lr: 1.2500e-04 - 42s/epoch - 216ms/step
Epoch 496/1000
2023-09-29 12:37:25.601 
Epoch 496/1000 
	 loss: 27.3930, MinusLogProbMetric: 27.3930, val_loss: 28.0268, val_MinusLogProbMetric: 28.0268

Epoch 496: val_loss did not improve from 28.00410
196/196 - 43s - loss: 27.3930 - MinusLogProbMetric: 27.3930 - val_loss: 28.0268 - val_MinusLogProbMetric: 28.0268 - lr: 1.2500e-04 - 43s/epoch - 218ms/step
Epoch 497/1000
2023-09-29 12:38:09.178 
Epoch 497/1000 
	 loss: 27.3884, MinusLogProbMetric: 27.3884, val_loss: 28.0252, val_MinusLogProbMetric: 28.0252

Epoch 497: val_loss did not improve from 28.00410
196/196 - 44s - loss: 27.3884 - MinusLogProbMetric: 27.3884 - val_loss: 28.0252 - val_MinusLogProbMetric: 28.0252 - lr: 1.2500e-04 - 44s/epoch - 222ms/step
Epoch 498/1000
2023-09-29 12:38:51.198 
Epoch 498/1000 
	 loss: 27.3937, MinusLogProbMetric: 27.3937, val_loss: 28.0170, val_MinusLogProbMetric: 28.0170

Epoch 498: val_loss did not improve from 28.00410
196/196 - 42s - loss: 27.3937 - MinusLogProbMetric: 27.3937 - val_loss: 28.0170 - val_MinusLogProbMetric: 28.0170 - lr: 1.2500e-04 - 42s/epoch - 214ms/step
Epoch 499/1000
2023-09-29 12:39:32.852 
Epoch 499/1000 
	 loss: 27.3905, MinusLogProbMetric: 27.3905, val_loss: 28.0345, val_MinusLogProbMetric: 28.0345

Epoch 499: val_loss did not improve from 28.00410
196/196 - 42s - loss: 27.3905 - MinusLogProbMetric: 27.3905 - val_loss: 28.0345 - val_MinusLogProbMetric: 28.0345 - lr: 1.2500e-04 - 42s/epoch - 213ms/step
Epoch 500/1000
2023-09-29 12:40:17.645 
Epoch 500/1000 
	 loss: 27.3959, MinusLogProbMetric: 27.3959, val_loss: 28.0465, val_MinusLogProbMetric: 28.0465

Epoch 500: val_loss did not improve from 28.00410
196/196 - 45s - loss: 27.3959 - MinusLogProbMetric: 27.3959 - val_loss: 28.0465 - val_MinusLogProbMetric: 28.0465 - lr: 1.2500e-04 - 45s/epoch - 229ms/step
Epoch 501/1000
2023-09-29 12:41:02.182 
Epoch 501/1000 
	 loss: 27.3870, MinusLogProbMetric: 27.3870, val_loss: 28.0267, val_MinusLogProbMetric: 28.0267

Epoch 501: val_loss did not improve from 28.00410
196/196 - 45s - loss: 27.3870 - MinusLogProbMetric: 27.3870 - val_loss: 28.0267 - val_MinusLogProbMetric: 28.0267 - lr: 1.2500e-04 - 45s/epoch - 227ms/step
Epoch 502/1000
2023-09-29 12:41:47.279 
Epoch 502/1000 
	 loss: 27.3863, MinusLogProbMetric: 27.3863, val_loss: 28.0475, val_MinusLogProbMetric: 28.0475

Epoch 502: val_loss did not improve from 28.00410
196/196 - 45s - loss: 27.3863 - MinusLogProbMetric: 27.3863 - val_loss: 28.0475 - val_MinusLogProbMetric: 28.0475 - lr: 1.2500e-04 - 45s/epoch - 230ms/step
Epoch 503/1000
2023-09-29 12:42:31.766 
Epoch 503/1000 
	 loss: 27.3899, MinusLogProbMetric: 27.3899, val_loss: 28.0444, val_MinusLogProbMetric: 28.0444

Epoch 503: val_loss did not improve from 28.00410
196/196 - 44s - loss: 27.3899 - MinusLogProbMetric: 27.3899 - val_loss: 28.0444 - val_MinusLogProbMetric: 28.0444 - lr: 1.2500e-04 - 44s/epoch - 227ms/step
Epoch 504/1000
2023-09-29 12:43:16.530 
Epoch 504/1000 
	 loss: 27.3865, MinusLogProbMetric: 27.3865, val_loss: 28.0367, val_MinusLogProbMetric: 28.0367

Epoch 504: val_loss did not improve from 28.00410
196/196 - 45s - loss: 27.3865 - MinusLogProbMetric: 27.3865 - val_loss: 28.0367 - val_MinusLogProbMetric: 28.0367 - lr: 1.2500e-04 - 45s/epoch - 228ms/step
Epoch 505/1000
2023-09-29 12:44:01.477 
Epoch 505/1000 
	 loss: 27.3954, MinusLogProbMetric: 27.3954, val_loss: 28.0238, val_MinusLogProbMetric: 28.0238

Epoch 505: val_loss did not improve from 28.00410
196/196 - 45s - loss: 27.3954 - MinusLogProbMetric: 27.3954 - val_loss: 28.0238 - val_MinusLogProbMetric: 28.0238 - lr: 1.2500e-04 - 45s/epoch - 229ms/step
Epoch 506/1000
2023-09-29 12:44:45.816 
Epoch 506/1000 
	 loss: 27.3864, MinusLogProbMetric: 27.3864, val_loss: 28.0704, val_MinusLogProbMetric: 28.0704

Epoch 506: val_loss did not improve from 28.00410
196/196 - 44s - loss: 27.3864 - MinusLogProbMetric: 27.3864 - val_loss: 28.0704 - val_MinusLogProbMetric: 28.0704 - lr: 1.2500e-04 - 44s/epoch - 226ms/step
Epoch 507/1000
2023-09-29 12:45:30.627 
Epoch 507/1000 
	 loss: 27.3903, MinusLogProbMetric: 27.3903, val_loss: 28.0290, val_MinusLogProbMetric: 28.0290

Epoch 507: val_loss did not improve from 28.00410
196/196 - 45s - loss: 27.3903 - MinusLogProbMetric: 27.3903 - val_loss: 28.0290 - val_MinusLogProbMetric: 28.0290 - lr: 1.2500e-04 - 45s/epoch - 229ms/step
Epoch 508/1000
2023-09-29 12:46:15.175 
Epoch 508/1000 
	 loss: 27.3868, MinusLogProbMetric: 27.3868, val_loss: 28.0281, val_MinusLogProbMetric: 28.0281

Epoch 508: val_loss did not improve from 28.00410
196/196 - 45s - loss: 27.3868 - MinusLogProbMetric: 27.3868 - val_loss: 28.0281 - val_MinusLogProbMetric: 28.0281 - lr: 1.2500e-04 - 45s/epoch - 227ms/step
Epoch 509/1000
2023-09-29 12:47:00.126 
Epoch 509/1000 
	 loss: 27.3876, MinusLogProbMetric: 27.3876, val_loss: 28.0175, val_MinusLogProbMetric: 28.0175

Epoch 509: val_loss did not improve from 28.00410
196/196 - 45s - loss: 27.3876 - MinusLogProbMetric: 27.3876 - val_loss: 28.0175 - val_MinusLogProbMetric: 28.0175 - lr: 1.2500e-04 - 45s/epoch - 229ms/step
Epoch 510/1000
2023-09-29 12:47:44.823 
Epoch 510/1000 
	 loss: 27.3883, MinusLogProbMetric: 27.3883, val_loss: 28.0271, val_MinusLogProbMetric: 28.0271

Epoch 510: val_loss did not improve from 28.00410
196/196 - 45s - loss: 27.3883 - MinusLogProbMetric: 27.3883 - val_loss: 28.0271 - val_MinusLogProbMetric: 28.0271 - lr: 1.2500e-04 - 45s/epoch - 228ms/step
Epoch 511/1000
2023-09-29 12:48:29.736 
Epoch 511/1000 
	 loss: 27.3837, MinusLogProbMetric: 27.3837, val_loss: 28.0383, val_MinusLogProbMetric: 28.0383

Epoch 511: val_loss did not improve from 28.00410
196/196 - 45s - loss: 27.3837 - MinusLogProbMetric: 27.3837 - val_loss: 28.0383 - val_MinusLogProbMetric: 28.0383 - lr: 1.2500e-04 - 45s/epoch - 229ms/step
Epoch 512/1000
2023-09-29 12:49:14.494 
Epoch 512/1000 
	 loss: 27.3919, MinusLogProbMetric: 27.3919, val_loss: 28.0628, val_MinusLogProbMetric: 28.0628

Epoch 512: val_loss did not improve from 28.00410
196/196 - 45s - loss: 27.3919 - MinusLogProbMetric: 27.3919 - val_loss: 28.0628 - val_MinusLogProbMetric: 28.0628 - lr: 1.2500e-04 - 45s/epoch - 228ms/step
Epoch 513/1000
2023-09-29 12:49:59.086 
Epoch 513/1000 
	 loss: 27.3899, MinusLogProbMetric: 27.3899, val_loss: 28.0648, val_MinusLogProbMetric: 28.0648

Epoch 513: val_loss did not improve from 28.00410
196/196 - 45s - loss: 27.3899 - MinusLogProbMetric: 27.3899 - val_loss: 28.0648 - val_MinusLogProbMetric: 28.0648 - lr: 1.2500e-04 - 45s/epoch - 227ms/step
Epoch 514/1000
2023-09-29 12:50:43.616 
Epoch 514/1000 
	 loss: 27.3796, MinusLogProbMetric: 27.3796, val_loss: 28.0514, val_MinusLogProbMetric: 28.0514

Epoch 514: val_loss did not improve from 28.00410
196/196 - 45s - loss: 27.3796 - MinusLogProbMetric: 27.3796 - val_loss: 28.0514 - val_MinusLogProbMetric: 28.0514 - lr: 1.2500e-04 - 45s/epoch - 227ms/step
Epoch 515/1000
2023-09-29 12:51:28.237 
Epoch 515/1000 
	 loss: 27.3851, MinusLogProbMetric: 27.3851, val_loss: 28.0290, val_MinusLogProbMetric: 28.0290

Epoch 515: val_loss did not improve from 28.00410
196/196 - 45s - loss: 27.3851 - MinusLogProbMetric: 27.3851 - val_loss: 28.0290 - val_MinusLogProbMetric: 28.0290 - lr: 1.2500e-04 - 45s/epoch - 228ms/step
Epoch 516/1000
2023-09-29 12:52:13.085 
Epoch 516/1000 
	 loss: 27.3819, MinusLogProbMetric: 27.3819, val_loss: 28.0249, val_MinusLogProbMetric: 28.0249

Epoch 516: val_loss did not improve from 28.00410
196/196 - 45s - loss: 27.3819 - MinusLogProbMetric: 27.3819 - val_loss: 28.0249 - val_MinusLogProbMetric: 28.0249 - lr: 1.2500e-04 - 45s/epoch - 229ms/step
Epoch 517/1000
2023-09-29 12:52:58.104 
Epoch 517/1000 
	 loss: 27.3898, MinusLogProbMetric: 27.3898, val_loss: 28.0197, val_MinusLogProbMetric: 28.0197

Epoch 517: val_loss did not improve from 28.00410
196/196 - 45s - loss: 27.3898 - MinusLogProbMetric: 27.3898 - val_loss: 28.0197 - val_MinusLogProbMetric: 28.0197 - lr: 1.2500e-04 - 45s/epoch - 230ms/step
Epoch 518/1000
2023-09-29 12:53:42.892 
Epoch 518/1000 
	 loss: 27.3846, MinusLogProbMetric: 27.3846, val_loss: 28.0167, val_MinusLogProbMetric: 28.0167

Epoch 518: val_loss did not improve from 28.00410
196/196 - 45s - loss: 27.3846 - MinusLogProbMetric: 27.3846 - val_loss: 28.0167 - val_MinusLogProbMetric: 28.0167 - lr: 1.2500e-04 - 45s/epoch - 229ms/step
Epoch 519/1000
2023-09-29 12:54:27.718 
Epoch 519/1000 
	 loss: 27.3864, MinusLogProbMetric: 27.3864, val_loss: 28.0325, val_MinusLogProbMetric: 28.0325

Epoch 519: val_loss did not improve from 28.00410
196/196 - 45s - loss: 27.3864 - MinusLogProbMetric: 27.3864 - val_loss: 28.0325 - val_MinusLogProbMetric: 28.0325 - lr: 1.2500e-04 - 45s/epoch - 229ms/step
Epoch 520/1000
2023-09-29 12:55:12.461 
Epoch 520/1000 
	 loss: 27.3774, MinusLogProbMetric: 27.3774, val_loss: 28.0169, val_MinusLogProbMetric: 28.0169

Epoch 520: val_loss did not improve from 28.00410
196/196 - 45s - loss: 27.3774 - MinusLogProbMetric: 27.3774 - val_loss: 28.0169 - val_MinusLogProbMetric: 28.0169 - lr: 1.2500e-04 - 45s/epoch - 228ms/step
Epoch 521/1000
2023-09-29 12:55:56.736 
Epoch 521/1000 
	 loss: 27.3834, MinusLogProbMetric: 27.3834, val_loss: 28.0236, val_MinusLogProbMetric: 28.0236

Epoch 521: val_loss did not improve from 28.00410
196/196 - 44s - loss: 27.3834 - MinusLogProbMetric: 27.3834 - val_loss: 28.0236 - val_MinusLogProbMetric: 28.0236 - lr: 1.2500e-04 - 44s/epoch - 226ms/step
Epoch 522/1000
2023-09-29 12:56:41.404 
Epoch 522/1000 
	 loss: 27.3806, MinusLogProbMetric: 27.3806, val_loss: 28.0245, val_MinusLogProbMetric: 28.0245

Epoch 522: val_loss did not improve from 28.00410
196/196 - 45s - loss: 27.3806 - MinusLogProbMetric: 27.3806 - val_loss: 28.0245 - val_MinusLogProbMetric: 28.0245 - lr: 1.2500e-04 - 45s/epoch - 228ms/step
Epoch 523/1000
2023-09-29 12:57:25.927 
Epoch 523/1000 
	 loss: 27.3848, MinusLogProbMetric: 27.3848, val_loss: 28.0046, val_MinusLogProbMetric: 28.0046

Epoch 523: val_loss did not improve from 28.00410
196/196 - 45s - loss: 27.3848 - MinusLogProbMetric: 27.3848 - val_loss: 28.0046 - val_MinusLogProbMetric: 28.0046 - lr: 1.2500e-04 - 45s/epoch - 227ms/step
Epoch 524/1000
2023-09-29 12:58:10.675 
Epoch 524/1000 
	 loss: 27.3851, MinusLogProbMetric: 27.3851, val_loss: 27.9967, val_MinusLogProbMetric: 27.9967

Epoch 524: val_loss improved from 28.00410 to 27.99666, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_332/weights/best_weights.h5
196/196 - 46s - loss: 27.3851 - MinusLogProbMetric: 27.3851 - val_loss: 27.9967 - val_MinusLogProbMetric: 27.9967 - lr: 1.2500e-04 - 46s/epoch - 234ms/step
Epoch 525/1000
2023-09-29 12:58:56.345 
Epoch 525/1000 
	 loss: 27.3828, MinusLogProbMetric: 27.3828, val_loss: 28.0149, val_MinusLogProbMetric: 28.0149

Epoch 525: val_loss did not improve from 27.99666
196/196 - 45s - loss: 27.3828 - MinusLogProbMetric: 27.3828 - val_loss: 28.0149 - val_MinusLogProbMetric: 28.0149 - lr: 1.2500e-04 - 45s/epoch - 227ms/step
Epoch 526/1000
2023-09-29 12:59:41.240 
Epoch 526/1000 
	 loss: 27.3817, MinusLogProbMetric: 27.3817, val_loss: 28.0161, val_MinusLogProbMetric: 28.0161

Epoch 526: val_loss did not improve from 27.99666
196/196 - 45s - loss: 27.3817 - MinusLogProbMetric: 27.3817 - val_loss: 28.0161 - val_MinusLogProbMetric: 28.0161 - lr: 1.2500e-04 - 45s/epoch - 229ms/step
Epoch 527/1000
2023-09-29 13:00:25.694 
Epoch 527/1000 
	 loss: 27.3851, MinusLogProbMetric: 27.3851, val_loss: 28.0263, val_MinusLogProbMetric: 28.0263

Epoch 527: val_loss did not improve from 27.99666
196/196 - 44s - loss: 27.3851 - MinusLogProbMetric: 27.3851 - val_loss: 28.0263 - val_MinusLogProbMetric: 28.0263 - lr: 1.2500e-04 - 44s/epoch - 227ms/step
Epoch 528/1000
2023-09-29 13:01:10.361 
Epoch 528/1000 
	 loss: 27.3853, MinusLogProbMetric: 27.3853, val_loss: 28.0151, val_MinusLogProbMetric: 28.0151

Epoch 528: val_loss did not improve from 27.99666
196/196 - 45s - loss: 27.3853 - MinusLogProbMetric: 27.3853 - val_loss: 28.0151 - val_MinusLogProbMetric: 28.0151 - lr: 1.2500e-04 - 45s/epoch - 228ms/step
Epoch 529/1000
2023-09-29 13:01:55.069 
Epoch 529/1000 
	 loss: 27.3810, MinusLogProbMetric: 27.3810, val_loss: 28.0241, val_MinusLogProbMetric: 28.0241

Epoch 529: val_loss did not improve from 27.99666
196/196 - 45s - loss: 27.3810 - MinusLogProbMetric: 27.3810 - val_loss: 28.0241 - val_MinusLogProbMetric: 28.0241 - lr: 1.2500e-04 - 45s/epoch - 228ms/step
Epoch 530/1000
2023-09-29 13:02:39.390 
Epoch 530/1000 
	 loss: 27.3834, MinusLogProbMetric: 27.3834, val_loss: 28.0332, val_MinusLogProbMetric: 28.0332

Epoch 530: val_loss did not improve from 27.99666
196/196 - 44s - loss: 27.3834 - MinusLogProbMetric: 27.3834 - val_loss: 28.0332 - val_MinusLogProbMetric: 28.0332 - lr: 1.2500e-04 - 44s/epoch - 226ms/step
Epoch 531/1000
2023-09-29 13:03:24.089 
Epoch 531/1000 
	 loss: 27.3777, MinusLogProbMetric: 27.3777, val_loss: 28.0438, val_MinusLogProbMetric: 28.0438

Epoch 531: val_loss did not improve from 27.99666
196/196 - 45s - loss: 27.3777 - MinusLogProbMetric: 27.3777 - val_loss: 28.0438 - val_MinusLogProbMetric: 28.0438 - lr: 1.2500e-04 - 45s/epoch - 228ms/step
Epoch 532/1000
2023-09-29 13:04:08.505 
Epoch 532/1000 
	 loss: 27.3804, MinusLogProbMetric: 27.3804, val_loss: 28.0020, val_MinusLogProbMetric: 28.0020

Epoch 532: val_loss did not improve from 27.99666
196/196 - 44s - loss: 27.3804 - MinusLogProbMetric: 27.3804 - val_loss: 28.0020 - val_MinusLogProbMetric: 28.0020 - lr: 1.2500e-04 - 44s/epoch - 227ms/step
Epoch 533/1000
2023-09-29 13:04:53.318 
Epoch 533/1000 
	 loss: 27.3811, MinusLogProbMetric: 27.3811, val_loss: 28.0080, val_MinusLogProbMetric: 28.0080

Epoch 533: val_loss did not improve from 27.99666
196/196 - 45s - loss: 27.3811 - MinusLogProbMetric: 27.3811 - val_loss: 28.0080 - val_MinusLogProbMetric: 28.0080 - lr: 1.2500e-04 - 45s/epoch - 229ms/step
Epoch 534/1000
2023-09-29 13:05:38.235 
Epoch 534/1000 
	 loss: 27.3772, MinusLogProbMetric: 27.3772, val_loss: 28.0143, val_MinusLogProbMetric: 28.0143

Epoch 534: val_loss did not improve from 27.99666
196/196 - 45s - loss: 27.3772 - MinusLogProbMetric: 27.3772 - val_loss: 28.0143 - val_MinusLogProbMetric: 28.0143 - lr: 1.2500e-04 - 45s/epoch - 229ms/step
Epoch 535/1000
2023-09-29 13:06:22.878 
Epoch 535/1000 
	 loss: 27.3876, MinusLogProbMetric: 27.3876, val_loss: 28.0296, val_MinusLogProbMetric: 28.0296

Epoch 535: val_loss did not improve from 27.99666
196/196 - 45s - loss: 27.3876 - MinusLogProbMetric: 27.3876 - val_loss: 28.0296 - val_MinusLogProbMetric: 28.0296 - lr: 1.2500e-04 - 45s/epoch - 228ms/step
Epoch 536/1000
2023-09-29 13:07:07.994 
Epoch 536/1000 
	 loss: 27.3816, MinusLogProbMetric: 27.3816, val_loss: 28.0151, val_MinusLogProbMetric: 28.0151

Epoch 536: val_loss did not improve from 27.99666
196/196 - 45s - loss: 27.3816 - MinusLogProbMetric: 27.3816 - val_loss: 28.0151 - val_MinusLogProbMetric: 28.0151 - lr: 1.2500e-04 - 45s/epoch - 230ms/step
Epoch 537/1000
2023-09-29 13:07:53.158 
Epoch 537/1000 
	 loss: 27.3755, MinusLogProbMetric: 27.3755, val_loss: 28.0347, val_MinusLogProbMetric: 28.0347

Epoch 537: val_loss did not improve from 27.99666
196/196 - 45s - loss: 27.3755 - MinusLogProbMetric: 27.3755 - val_loss: 28.0347 - val_MinusLogProbMetric: 28.0347 - lr: 1.2500e-04 - 45s/epoch - 230ms/step
Epoch 538/1000
2023-09-29 13:08:37.811 
Epoch 538/1000 
	 loss: 27.3829, MinusLogProbMetric: 27.3829, val_loss: 28.0329, val_MinusLogProbMetric: 28.0329

Epoch 538: val_loss did not improve from 27.99666
196/196 - 45s - loss: 27.3829 - MinusLogProbMetric: 27.3829 - val_loss: 28.0329 - val_MinusLogProbMetric: 28.0329 - lr: 1.2500e-04 - 45s/epoch - 228ms/step
Epoch 539/1000
2023-09-29 13:09:22.509 
Epoch 539/1000 
	 loss: 27.3783, MinusLogProbMetric: 27.3783, val_loss: 28.0442, val_MinusLogProbMetric: 28.0442

Epoch 539: val_loss did not improve from 27.99666
196/196 - 45s - loss: 27.3783 - MinusLogProbMetric: 27.3783 - val_loss: 28.0442 - val_MinusLogProbMetric: 28.0442 - lr: 1.2500e-04 - 45s/epoch - 228ms/step
Epoch 540/1000
2023-09-29 13:10:06.861 
Epoch 540/1000 
	 loss: 27.3862, MinusLogProbMetric: 27.3862, val_loss: 28.0595, val_MinusLogProbMetric: 28.0595

Epoch 540: val_loss did not improve from 27.99666
196/196 - 44s - loss: 27.3862 - MinusLogProbMetric: 27.3862 - val_loss: 28.0595 - val_MinusLogProbMetric: 28.0595 - lr: 1.2500e-04 - 44s/epoch - 226ms/step
Epoch 541/1000
2023-09-29 13:10:51.241 
Epoch 541/1000 
	 loss: 27.3821, MinusLogProbMetric: 27.3821, val_loss: 28.0272, val_MinusLogProbMetric: 28.0272

Epoch 541: val_loss did not improve from 27.99666
196/196 - 44s - loss: 27.3821 - MinusLogProbMetric: 27.3821 - val_loss: 28.0272 - val_MinusLogProbMetric: 28.0272 - lr: 1.2500e-04 - 44s/epoch - 226ms/step
Epoch 542/1000
2023-09-29 13:11:35.752 
Epoch 542/1000 
	 loss: 27.3716, MinusLogProbMetric: 27.3716, val_loss: 28.0420, val_MinusLogProbMetric: 28.0420

Epoch 542: val_loss did not improve from 27.99666
196/196 - 45s - loss: 27.3716 - MinusLogProbMetric: 27.3716 - val_loss: 28.0420 - val_MinusLogProbMetric: 28.0420 - lr: 1.2500e-04 - 45s/epoch - 227ms/step
Epoch 543/1000
2023-09-29 13:12:20.675 
Epoch 543/1000 
	 loss: 27.3819, MinusLogProbMetric: 27.3819, val_loss: 28.0281, val_MinusLogProbMetric: 28.0281

Epoch 543: val_loss did not improve from 27.99666
196/196 - 45s - loss: 27.3819 - MinusLogProbMetric: 27.3819 - val_loss: 28.0281 - val_MinusLogProbMetric: 28.0281 - lr: 1.2500e-04 - 45s/epoch - 229ms/step
Epoch 544/1000
2023-09-29 13:13:05.468 
Epoch 544/1000 
	 loss: 27.3753, MinusLogProbMetric: 27.3753, val_loss: 28.0286, val_MinusLogProbMetric: 28.0286

Epoch 544: val_loss did not improve from 27.99666
196/196 - 45s - loss: 27.3753 - MinusLogProbMetric: 27.3753 - val_loss: 28.0286 - val_MinusLogProbMetric: 28.0286 - lr: 1.2500e-04 - 45s/epoch - 229ms/step
Epoch 545/1000
2023-09-29 13:13:49.984 
Epoch 545/1000 
	 loss: 27.3828, MinusLogProbMetric: 27.3828, val_loss: 28.0462, val_MinusLogProbMetric: 28.0462

Epoch 545: val_loss did not improve from 27.99666
196/196 - 45s - loss: 27.3828 - MinusLogProbMetric: 27.3828 - val_loss: 28.0462 - val_MinusLogProbMetric: 28.0462 - lr: 1.2500e-04 - 45s/epoch - 227ms/step
Epoch 546/1000
2023-09-29 13:14:34.502 
Epoch 546/1000 
	 loss: 27.3876, MinusLogProbMetric: 27.3876, val_loss: 28.0325, val_MinusLogProbMetric: 28.0325

Epoch 546: val_loss did not improve from 27.99666
196/196 - 45s - loss: 27.3876 - MinusLogProbMetric: 27.3876 - val_loss: 28.0325 - val_MinusLogProbMetric: 28.0325 - lr: 1.2500e-04 - 45s/epoch - 227ms/step
Epoch 547/1000
2023-09-29 13:15:19.280 
Epoch 547/1000 
	 loss: 27.3769, MinusLogProbMetric: 27.3769, val_loss: 28.0536, val_MinusLogProbMetric: 28.0536

Epoch 547: val_loss did not improve from 27.99666
196/196 - 45s - loss: 27.3769 - MinusLogProbMetric: 27.3769 - val_loss: 28.0536 - val_MinusLogProbMetric: 28.0536 - lr: 1.2500e-04 - 45s/epoch - 228ms/step
Epoch 548/1000
2023-09-29 13:16:03.609 
Epoch 548/1000 
	 loss: 27.3799, MinusLogProbMetric: 27.3799, val_loss: 28.0107, val_MinusLogProbMetric: 28.0107

Epoch 548: val_loss did not improve from 27.99666
196/196 - 44s - loss: 27.3799 - MinusLogProbMetric: 27.3799 - val_loss: 28.0107 - val_MinusLogProbMetric: 28.0107 - lr: 1.2500e-04 - 44s/epoch - 226ms/step
Epoch 549/1000
2023-09-29 13:16:48.404 
Epoch 549/1000 
	 loss: 27.3760, MinusLogProbMetric: 27.3760, val_loss: 28.0325, val_MinusLogProbMetric: 28.0325

Epoch 549: val_loss did not improve from 27.99666
196/196 - 45s - loss: 27.3760 - MinusLogProbMetric: 27.3760 - val_loss: 28.0325 - val_MinusLogProbMetric: 28.0325 - lr: 1.2500e-04 - 45s/epoch - 229ms/step
Epoch 550/1000
2023-09-29 13:17:32.611 
Epoch 550/1000 
	 loss: 27.3764, MinusLogProbMetric: 27.3764, val_loss: 28.0269, val_MinusLogProbMetric: 28.0269

Epoch 550: val_loss did not improve from 27.99666
196/196 - 44s - loss: 27.3764 - MinusLogProbMetric: 27.3764 - val_loss: 28.0269 - val_MinusLogProbMetric: 28.0269 - lr: 1.2500e-04 - 44s/epoch - 226ms/step
Epoch 551/1000
2023-09-29 13:18:16.947 
Epoch 551/1000 
	 loss: 27.3811, MinusLogProbMetric: 27.3811, val_loss: 28.0448, val_MinusLogProbMetric: 28.0448

Epoch 551: val_loss did not improve from 27.99666
196/196 - 44s - loss: 27.3811 - MinusLogProbMetric: 27.3811 - val_loss: 28.0448 - val_MinusLogProbMetric: 28.0448 - lr: 1.2500e-04 - 44s/epoch - 226ms/step
Epoch 552/1000
2023-09-29 13:19:01.573 
Epoch 552/1000 
	 loss: 27.3817, MinusLogProbMetric: 27.3817, val_loss: 28.0258, val_MinusLogProbMetric: 28.0258

Epoch 552: val_loss did not improve from 27.99666
196/196 - 45s - loss: 27.3817 - MinusLogProbMetric: 27.3817 - val_loss: 28.0258 - val_MinusLogProbMetric: 28.0258 - lr: 1.2500e-04 - 45s/epoch - 228ms/step
Epoch 553/1000
2023-09-29 13:19:46.280 
Epoch 553/1000 
	 loss: 27.3735, MinusLogProbMetric: 27.3735, val_loss: 28.0169, val_MinusLogProbMetric: 28.0169

Epoch 553: val_loss did not improve from 27.99666
196/196 - 45s - loss: 27.3735 - MinusLogProbMetric: 27.3735 - val_loss: 28.0169 - val_MinusLogProbMetric: 28.0169 - lr: 1.2500e-04 - 45s/epoch - 228ms/step
Epoch 554/1000
2023-09-29 13:20:30.767 
Epoch 554/1000 
	 loss: 27.3855, MinusLogProbMetric: 27.3855, val_loss: 28.0421, val_MinusLogProbMetric: 28.0421

Epoch 554: val_loss did not improve from 27.99666
196/196 - 44s - loss: 27.3855 - MinusLogProbMetric: 27.3855 - val_loss: 28.0421 - val_MinusLogProbMetric: 28.0421 - lr: 1.2500e-04 - 44s/epoch - 227ms/step
Epoch 555/1000
2023-09-29 13:21:15.197 
Epoch 555/1000 
	 loss: 27.3791, MinusLogProbMetric: 27.3791, val_loss: 28.0701, val_MinusLogProbMetric: 28.0701

Epoch 555: val_loss did not improve from 27.99666
196/196 - 44s - loss: 27.3791 - MinusLogProbMetric: 27.3791 - val_loss: 28.0701 - val_MinusLogProbMetric: 28.0701 - lr: 1.2500e-04 - 44s/epoch - 227ms/step
Epoch 556/1000
2023-09-29 13:21:59.697 
Epoch 556/1000 
	 loss: 27.3827, MinusLogProbMetric: 27.3827, val_loss: 28.0164, val_MinusLogProbMetric: 28.0164

Epoch 556: val_loss did not improve from 27.99666
196/196 - 44s - loss: 27.3827 - MinusLogProbMetric: 27.3827 - val_loss: 28.0164 - val_MinusLogProbMetric: 28.0164 - lr: 1.2500e-04 - 44s/epoch - 227ms/step
Epoch 557/1000
2023-09-29 13:22:44.150 
Epoch 557/1000 
	 loss: 27.3807, MinusLogProbMetric: 27.3807, val_loss: 28.0167, val_MinusLogProbMetric: 28.0167

Epoch 557: val_loss did not improve from 27.99666
196/196 - 44s - loss: 27.3807 - MinusLogProbMetric: 27.3807 - val_loss: 28.0167 - val_MinusLogProbMetric: 28.0167 - lr: 1.2500e-04 - 44s/epoch - 227ms/step
Epoch 558/1000
2023-09-29 13:23:28.796 
Epoch 558/1000 
	 loss: 27.3756, MinusLogProbMetric: 27.3756, val_loss: 28.0405, val_MinusLogProbMetric: 28.0405

Epoch 558: val_loss did not improve from 27.99666
196/196 - 45s - loss: 27.3756 - MinusLogProbMetric: 27.3756 - val_loss: 28.0405 - val_MinusLogProbMetric: 28.0405 - lr: 1.2500e-04 - 45s/epoch - 228ms/step
Epoch 559/1000
2023-09-29 13:24:13.509 
Epoch 559/1000 
	 loss: 27.3735, MinusLogProbMetric: 27.3735, val_loss: 28.0345, val_MinusLogProbMetric: 28.0345

Epoch 559: val_loss did not improve from 27.99666
196/196 - 45s - loss: 27.3735 - MinusLogProbMetric: 27.3735 - val_loss: 28.0345 - val_MinusLogProbMetric: 28.0345 - lr: 1.2500e-04 - 45s/epoch - 228ms/step
Epoch 560/1000
2023-09-29 13:24:58.077 
Epoch 560/1000 
	 loss: 27.3769, MinusLogProbMetric: 27.3769, val_loss: 28.0495, val_MinusLogProbMetric: 28.0495

Epoch 560: val_loss did not improve from 27.99666
196/196 - 45s - loss: 27.3769 - MinusLogProbMetric: 27.3769 - val_loss: 28.0495 - val_MinusLogProbMetric: 28.0495 - lr: 1.2500e-04 - 45s/epoch - 227ms/step
Epoch 561/1000
2023-09-29 13:25:42.858 
Epoch 561/1000 
	 loss: 27.3710, MinusLogProbMetric: 27.3710, val_loss: 28.0169, val_MinusLogProbMetric: 28.0169

Epoch 561: val_loss did not improve from 27.99666
196/196 - 45s - loss: 27.3710 - MinusLogProbMetric: 27.3710 - val_loss: 28.0169 - val_MinusLogProbMetric: 28.0169 - lr: 1.2500e-04 - 45s/epoch - 228ms/step
Epoch 562/1000
2023-09-29 13:26:27.446 
Epoch 562/1000 
	 loss: 27.3748, MinusLogProbMetric: 27.3748, val_loss: 28.0082, val_MinusLogProbMetric: 28.0082

Epoch 562: val_loss did not improve from 27.99666
196/196 - 45s - loss: 27.3748 - MinusLogProbMetric: 27.3748 - val_loss: 28.0082 - val_MinusLogProbMetric: 28.0082 - lr: 1.2500e-04 - 45s/epoch - 227ms/step
Epoch 563/1000
2023-09-29 13:27:11.401 
Epoch 563/1000 
	 loss: 27.3763, MinusLogProbMetric: 27.3763, val_loss: 28.0152, val_MinusLogProbMetric: 28.0152

Epoch 563: val_loss did not improve from 27.99666
196/196 - 44s - loss: 27.3763 - MinusLogProbMetric: 27.3763 - val_loss: 28.0152 - val_MinusLogProbMetric: 28.0152 - lr: 1.2500e-04 - 44s/epoch - 224ms/step
Epoch 564/1000
2023-09-29 13:27:55.968 
Epoch 564/1000 
	 loss: 27.3723, MinusLogProbMetric: 27.3723, val_loss: 28.0486, val_MinusLogProbMetric: 28.0486

Epoch 564: val_loss did not improve from 27.99666
196/196 - 45s - loss: 27.3723 - MinusLogProbMetric: 27.3723 - val_loss: 28.0486 - val_MinusLogProbMetric: 28.0486 - lr: 1.2500e-04 - 45s/epoch - 227ms/step
Epoch 565/1000
2023-09-29 13:28:40.768 
Epoch 565/1000 
	 loss: 27.3769, MinusLogProbMetric: 27.3769, val_loss: 28.0462, val_MinusLogProbMetric: 28.0462

Epoch 565: val_loss did not improve from 27.99666
196/196 - 45s - loss: 27.3769 - MinusLogProbMetric: 27.3769 - val_loss: 28.0462 - val_MinusLogProbMetric: 28.0462 - lr: 1.2500e-04 - 45s/epoch - 229ms/step
Epoch 566/1000
2023-09-29 13:29:25.460 
Epoch 566/1000 
	 loss: 27.3739, MinusLogProbMetric: 27.3739, val_loss: 28.0194, val_MinusLogProbMetric: 28.0194

Epoch 566: val_loss did not improve from 27.99666
196/196 - 45s - loss: 27.3739 - MinusLogProbMetric: 27.3739 - val_loss: 28.0194 - val_MinusLogProbMetric: 28.0194 - lr: 1.2500e-04 - 45s/epoch - 228ms/step
Epoch 567/1000
2023-09-29 13:30:10.099 
Epoch 567/1000 
	 loss: 27.3819, MinusLogProbMetric: 27.3819, val_loss: 28.0413, val_MinusLogProbMetric: 28.0413

Epoch 567: val_loss did not improve from 27.99666
196/196 - 45s - loss: 27.3819 - MinusLogProbMetric: 27.3819 - val_loss: 28.0413 - val_MinusLogProbMetric: 28.0413 - lr: 1.2500e-04 - 45s/epoch - 228ms/step
Epoch 568/1000
2023-09-29 13:30:54.789 
Epoch 568/1000 
	 loss: 27.3764, MinusLogProbMetric: 27.3764, val_loss: 28.0289, val_MinusLogProbMetric: 28.0289

Epoch 568: val_loss did not improve from 27.99666
196/196 - 45s - loss: 27.3764 - MinusLogProbMetric: 27.3764 - val_loss: 28.0289 - val_MinusLogProbMetric: 28.0289 - lr: 1.2500e-04 - 45s/epoch - 228ms/step
Epoch 569/1000
2023-09-29 13:31:39.557 
Epoch 569/1000 
	 loss: 27.3818, MinusLogProbMetric: 27.3818, val_loss: 28.0139, val_MinusLogProbMetric: 28.0139

Epoch 569: val_loss did not improve from 27.99666
196/196 - 45s - loss: 27.3818 - MinusLogProbMetric: 27.3818 - val_loss: 28.0139 - val_MinusLogProbMetric: 28.0139 - lr: 1.2500e-04 - 45s/epoch - 228ms/step
Epoch 570/1000
2023-09-29 13:32:24.138 
Epoch 570/1000 
	 loss: 27.3690, MinusLogProbMetric: 27.3690, val_loss: 28.0490, val_MinusLogProbMetric: 28.0490

Epoch 570: val_loss did not improve from 27.99666
196/196 - 45s - loss: 27.3690 - MinusLogProbMetric: 27.3690 - val_loss: 28.0490 - val_MinusLogProbMetric: 28.0490 - lr: 1.2500e-04 - 45s/epoch - 227ms/step
Epoch 571/1000
2023-09-29 13:33:08.771 
Epoch 571/1000 
	 loss: 27.3778, MinusLogProbMetric: 27.3778, val_loss: 28.0522, val_MinusLogProbMetric: 28.0522

Epoch 571: val_loss did not improve from 27.99666
196/196 - 45s - loss: 27.3778 - MinusLogProbMetric: 27.3778 - val_loss: 28.0522 - val_MinusLogProbMetric: 28.0522 - lr: 1.2500e-04 - 45s/epoch - 228ms/step
Epoch 572/1000
2023-09-29 13:33:53.450 
Epoch 572/1000 
	 loss: 27.3790, MinusLogProbMetric: 27.3790, val_loss: 28.0760, val_MinusLogProbMetric: 28.0760

Epoch 572: val_loss did not improve from 27.99666
196/196 - 45s - loss: 27.3790 - MinusLogProbMetric: 27.3790 - val_loss: 28.0760 - val_MinusLogProbMetric: 28.0760 - lr: 1.2500e-04 - 45s/epoch - 228ms/step
Epoch 573/1000
2023-09-29 13:34:38.085 
Epoch 573/1000 
	 loss: 27.3753, MinusLogProbMetric: 27.3753, val_loss: 28.0508, val_MinusLogProbMetric: 28.0508

Epoch 573: val_loss did not improve from 27.99666
196/196 - 45s - loss: 27.3753 - MinusLogProbMetric: 27.3753 - val_loss: 28.0508 - val_MinusLogProbMetric: 28.0508 - lr: 1.2500e-04 - 45s/epoch - 228ms/step
Epoch 574/1000
2023-09-29 13:35:21.893 
Epoch 574/1000 
	 loss: 27.3746, MinusLogProbMetric: 27.3746, val_loss: 28.0610, val_MinusLogProbMetric: 28.0610

Epoch 574: val_loss did not improve from 27.99666
196/196 - 44s - loss: 27.3746 - MinusLogProbMetric: 27.3746 - val_loss: 28.0610 - val_MinusLogProbMetric: 28.0610 - lr: 1.2500e-04 - 44s/epoch - 224ms/step
Epoch 575/1000
2023-09-29 13:36:06.457 
Epoch 575/1000 
	 loss: 27.3374, MinusLogProbMetric: 27.3374, val_loss: 28.0013, val_MinusLogProbMetric: 28.0013

Epoch 575: val_loss did not improve from 27.99666
196/196 - 45s - loss: 27.3374 - MinusLogProbMetric: 27.3374 - val_loss: 28.0013 - val_MinusLogProbMetric: 28.0013 - lr: 6.2500e-05 - 45s/epoch - 227ms/step
Epoch 576/1000
2023-09-29 13:36:51.222 
Epoch 576/1000 
	 loss: 27.3335, MinusLogProbMetric: 27.3335, val_loss: 28.0063, val_MinusLogProbMetric: 28.0063

Epoch 576: val_loss did not improve from 27.99666
196/196 - 45s - loss: 27.3335 - MinusLogProbMetric: 27.3335 - val_loss: 28.0063 - val_MinusLogProbMetric: 28.0063 - lr: 6.2500e-05 - 45s/epoch - 228ms/step
Epoch 577/1000
2023-09-29 13:37:35.567 
Epoch 577/1000 
	 loss: 27.3347, MinusLogProbMetric: 27.3347, val_loss: 28.0016, val_MinusLogProbMetric: 28.0016

Epoch 577: val_loss did not improve from 27.99666
196/196 - 44s - loss: 27.3347 - MinusLogProbMetric: 27.3347 - val_loss: 28.0016 - val_MinusLogProbMetric: 28.0016 - lr: 6.2500e-05 - 44s/epoch - 226ms/step
Epoch 578/1000
2023-09-29 13:38:20.034 
Epoch 578/1000 
	 loss: 27.3315, MinusLogProbMetric: 27.3315, val_loss: 27.9902, val_MinusLogProbMetric: 27.9902

Epoch 578: val_loss improved from 27.99666 to 27.99024, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_332/weights/best_weights.h5
196/196 - 45s - loss: 27.3315 - MinusLogProbMetric: 27.3315 - val_loss: 27.9902 - val_MinusLogProbMetric: 27.9902 - lr: 6.2500e-05 - 45s/epoch - 231ms/step
Epoch 579/1000
2023-09-29 13:39:05.415 
Epoch 579/1000 
	 loss: 27.3359, MinusLogProbMetric: 27.3359, val_loss: 28.0037, val_MinusLogProbMetric: 28.0037

Epoch 579: val_loss did not improve from 27.99024
196/196 - 45s - loss: 27.3359 - MinusLogProbMetric: 27.3359 - val_loss: 28.0037 - val_MinusLogProbMetric: 28.0037 - lr: 6.2500e-05 - 45s/epoch - 228ms/step
Epoch 580/1000
2023-09-29 13:39:45.120 
Epoch 580/1000 
	 loss: 27.3359, MinusLogProbMetric: 27.3359, val_loss: 27.9989, val_MinusLogProbMetric: 27.9989

Epoch 580: val_loss did not improve from 27.99024
196/196 - 40s - loss: 27.3359 - MinusLogProbMetric: 27.3359 - val_loss: 27.9989 - val_MinusLogProbMetric: 27.9989 - lr: 6.2500e-05 - 40s/epoch - 203ms/step
Epoch 581/1000
2023-09-29 13:40:26.204 
Epoch 581/1000 
	 loss: 27.3337, MinusLogProbMetric: 27.3337, val_loss: 28.0068, val_MinusLogProbMetric: 28.0068

Epoch 581: val_loss did not improve from 27.99024
196/196 - 41s - loss: 27.3337 - MinusLogProbMetric: 27.3337 - val_loss: 28.0068 - val_MinusLogProbMetric: 28.0068 - lr: 6.2500e-05 - 41s/epoch - 210ms/step
Epoch 582/1000
2023-09-29 13:41:05.104 
Epoch 582/1000 
	 loss: 27.3356, MinusLogProbMetric: 27.3356, val_loss: 28.0005, val_MinusLogProbMetric: 28.0005

Epoch 582: val_loss did not improve from 27.99024
196/196 - 39s - loss: 27.3356 - MinusLogProbMetric: 27.3356 - val_loss: 28.0005 - val_MinusLogProbMetric: 28.0005 - lr: 6.2500e-05 - 39s/epoch - 198ms/step
Epoch 583/1000
2023-09-29 13:41:42.468 
Epoch 583/1000 
	 loss: 27.3366, MinusLogProbMetric: 27.3366, val_loss: 27.9973, val_MinusLogProbMetric: 27.9973

Epoch 583: val_loss did not improve from 27.99024
196/196 - 37s - loss: 27.3366 - MinusLogProbMetric: 27.3366 - val_loss: 27.9973 - val_MinusLogProbMetric: 27.9973 - lr: 6.2500e-05 - 37s/epoch - 191ms/step
Epoch 584/1000
2023-09-29 13:42:20.570 
Epoch 584/1000 
	 loss: 27.3308, MinusLogProbMetric: 27.3308, val_loss: 28.0058, val_MinusLogProbMetric: 28.0058

Epoch 584: val_loss did not improve from 27.99024
196/196 - 38s - loss: 27.3308 - MinusLogProbMetric: 27.3308 - val_loss: 28.0058 - val_MinusLogProbMetric: 28.0058 - lr: 6.2500e-05 - 38s/epoch - 194ms/step
Epoch 585/1000
2023-09-29 13:42:58.090 
Epoch 585/1000 
	 loss: 27.3362, MinusLogProbMetric: 27.3362, val_loss: 28.0016, val_MinusLogProbMetric: 28.0016

Epoch 585: val_loss did not improve from 27.99024
196/196 - 38s - loss: 27.3362 - MinusLogProbMetric: 27.3362 - val_loss: 28.0016 - val_MinusLogProbMetric: 28.0016 - lr: 6.2500e-05 - 38s/epoch - 191ms/step
Epoch 586/1000
2023-09-29 13:43:35.246 
Epoch 586/1000 
	 loss: 27.3342, MinusLogProbMetric: 27.3342, val_loss: 27.9948, val_MinusLogProbMetric: 27.9948

Epoch 586: val_loss did not improve from 27.99024
196/196 - 37s - loss: 27.3342 - MinusLogProbMetric: 27.3342 - val_loss: 27.9948 - val_MinusLogProbMetric: 27.9948 - lr: 6.2500e-05 - 37s/epoch - 190ms/step
Epoch 587/1000
2023-09-29 13:44:12.747 
Epoch 587/1000 
	 loss: 27.3347, MinusLogProbMetric: 27.3347, val_loss: 28.0031, val_MinusLogProbMetric: 28.0031

Epoch 587: val_loss did not improve from 27.99024
196/196 - 37s - loss: 27.3347 - MinusLogProbMetric: 27.3347 - val_loss: 28.0031 - val_MinusLogProbMetric: 28.0031 - lr: 6.2500e-05 - 37s/epoch - 191ms/step
Epoch 588/1000
2023-09-29 13:44:50.222 
Epoch 588/1000 
	 loss: 27.3326, MinusLogProbMetric: 27.3326, val_loss: 27.9946, val_MinusLogProbMetric: 27.9946

Epoch 588: val_loss did not improve from 27.99024
196/196 - 37s - loss: 27.3326 - MinusLogProbMetric: 27.3326 - val_loss: 27.9946 - val_MinusLogProbMetric: 27.9946 - lr: 6.2500e-05 - 37s/epoch - 191ms/step
Epoch 589/1000
2023-09-29 13:45:29.599 
Epoch 589/1000 
	 loss: 27.3358, MinusLogProbMetric: 27.3358, val_loss: 28.0009, val_MinusLogProbMetric: 28.0009

Epoch 589: val_loss did not improve from 27.99024
196/196 - 39s - loss: 27.3358 - MinusLogProbMetric: 27.3358 - val_loss: 28.0009 - val_MinusLogProbMetric: 28.0009 - lr: 6.2500e-05 - 39s/epoch - 201ms/step
Epoch 590/1000
2023-09-29 13:46:11.479 
Epoch 590/1000 
	 loss: 27.3345, MinusLogProbMetric: 27.3345, val_loss: 28.0005, val_MinusLogProbMetric: 28.0005

Epoch 590: val_loss did not improve from 27.99024
196/196 - 42s - loss: 27.3345 - MinusLogProbMetric: 27.3345 - val_loss: 28.0005 - val_MinusLogProbMetric: 28.0005 - lr: 6.2500e-05 - 42s/epoch - 214ms/step
Epoch 591/1000
2023-09-29 13:46:53.413 
Epoch 591/1000 
	 loss: 27.3333, MinusLogProbMetric: 27.3333, val_loss: 28.0128, val_MinusLogProbMetric: 28.0128

Epoch 591: val_loss did not improve from 27.99024
196/196 - 42s - loss: 27.3333 - MinusLogProbMetric: 27.3333 - val_loss: 28.0128 - val_MinusLogProbMetric: 28.0128 - lr: 6.2500e-05 - 42s/epoch - 214ms/step
Epoch 592/1000
2023-09-29 13:47:32.859 
Epoch 592/1000 
	 loss: 27.3315, MinusLogProbMetric: 27.3315, val_loss: 28.0007, val_MinusLogProbMetric: 28.0007

Epoch 592: val_loss did not improve from 27.99024
196/196 - 39s - loss: 27.3315 - MinusLogProbMetric: 27.3315 - val_loss: 28.0007 - val_MinusLogProbMetric: 28.0007 - lr: 6.2500e-05 - 39s/epoch - 201ms/step
Epoch 593/1000
2023-09-29 13:48:16.204 
Epoch 593/1000 
	 loss: 27.3322, MinusLogProbMetric: 27.3322, val_loss: 28.0029, val_MinusLogProbMetric: 28.0029

Epoch 593: val_loss did not improve from 27.99024
196/196 - 43s - loss: 27.3322 - MinusLogProbMetric: 27.3322 - val_loss: 28.0029 - val_MinusLogProbMetric: 28.0029 - lr: 6.2500e-05 - 43s/epoch - 221ms/step
Epoch 594/1000
2023-09-29 13:48:59.270 
Epoch 594/1000 
	 loss: 27.3325, MinusLogProbMetric: 27.3325, val_loss: 28.0042, val_MinusLogProbMetric: 28.0042

Epoch 594: val_loss did not improve from 27.99024
196/196 - 43s - loss: 27.3325 - MinusLogProbMetric: 27.3325 - val_loss: 28.0042 - val_MinusLogProbMetric: 28.0042 - lr: 6.2500e-05 - 43s/epoch - 220ms/step
Epoch 595/1000
2023-09-29 13:49:40.707 
Epoch 595/1000 
	 loss: 27.3305, MinusLogProbMetric: 27.3305, val_loss: 27.9976, val_MinusLogProbMetric: 27.9976

Epoch 595: val_loss did not improve from 27.99024
196/196 - 41s - loss: 27.3305 - MinusLogProbMetric: 27.3305 - val_loss: 27.9976 - val_MinusLogProbMetric: 27.9976 - lr: 6.2500e-05 - 41s/epoch - 211ms/step
Epoch 596/1000
2023-09-29 13:50:23.333 
Epoch 596/1000 
	 loss: 27.3324, MinusLogProbMetric: 27.3324, val_loss: 28.0032, val_MinusLogProbMetric: 28.0032

Epoch 596: val_loss did not improve from 27.99024
196/196 - 43s - loss: 27.3324 - MinusLogProbMetric: 27.3324 - val_loss: 28.0032 - val_MinusLogProbMetric: 28.0032 - lr: 6.2500e-05 - 43s/epoch - 217ms/step
Epoch 597/1000
2023-09-29 13:51:06.996 
Epoch 597/1000 
	 loss: 27.3333, MinusLogProbMetric: 27.3333, val_loss: 28.0023, val_MinusLogProbMetric: 28.0023

Epoch 597: val_loss did not improve from 27.99024
196/196 - 44s - loss: 27.3333 - MinusLogProbMetric: 27.3333 - val_loss: 28.0023 - val_MinusLogProbMetric: 28.0023 - lr: 6.2500e-05 - 44s/epoch - 223ms/step
Epoch 598/1000
2023-09-29 13:51:46.381 
Epoch 598/1000 
	 loss: 27.3335, MinusLogProbMetric: 27.3335, val_loss: 28.0231, val_MinusLogProbMetric: 28.0231

Epoch 598: val_loss did not improve from 27.99024
196/196 - 39s - loss: 27.3335 - MinusLogProbMetric: 27.3335 - val_loss: 28.0231 - val_MinusLogProbMetric: 28.0231 - lr: 6.2500e-05 - 39s/epoch - 201ms/step
Epoch 599/1000
2023-09-29 13:52:29.004 
Epoch 599/1000 
	 loss: 27.3330, MinusLogProbMetric: 27.3330, val_loss: 28.0075, val_MinusLogProbMetric: 28.0075

Epoch 599: val_loss did not improve from 27.99024
196/196 - 43s - loss: 27.3330 - MinusLogProbMetric: 27.3330 - val_loss: 28.0075 - val_MinusLogProbMetric: 28.0075 - lr: 6.2500e-05 - 43s/epoch - 217ms/step
Epoch 600/1000
2023-09-29 13:53:10.868 
Epoch 600/1000 
	 loss: 27.3340, MinusLogProbMetric: 27.3340, val_loss: 28.0014, val_MinusLogProbMetric: 28.0014

Epoch 600: val_loss did not improve from 27.99024
196/196 - 42s - loss: 27.3340 - MinusLogProbMetric: 27.3340 - val_loss: 28.0014 - val_MinusLogProbMetric: 28.0014 - lr: 6.2500e-05 - 42s/epoch - 214ms/step
Epoch 601/1000
2023-09-29 13:53:53.965 
Epoch 601/1000 
	 loss: 27.3288, MinusLogProbMetric: 27.3288, val_loss: 27.9985, val_MinusLogProbMetric: 27.9985

Epoch 601: val_loss did not improve from 27.99024
196/196 - 43s - loss: 27.3288 - MinusLogProbMetric: 27.3288 - val_loss: 27.9985 - val_MinusLogProbMetric: 27.9985 - lr: 6.2500e-05 - 43s/epoch - 220ms/step
Epoch 602/1000
2023-09-29 13:54:35.778 
Epoch 602/1000 
	 loss: 27.3290, MinusLogProbMetric: 27.3290, val_loss: 27.9965, val_MinusLogProbMetric: 27.9965

Epoch 602: val_loss did not improve from 27.99024
196/196 - 42s - loss: 27.3290 - MinusLogProbMetric: 27.3290 - val_loss: 27.9965 - val_MinusLogProbMetric: 27.9965 - lr: 6.2500e-05 - 42s/epoch - 213ms/step
Epoch 603/1000
2023-09-29 13:55:18.613 
Epoch 603/1000 
	 loss: 27.3318, MinusLogProbMetric: 27.3318, val_loss: 28.0037, val_MinusLogProbMetric: 28.0037

Epoch 603: val_loss did not improve from 27.99024
196/196 - 43s - loss: 27.3318 - MinusLogProbMetric: 27.3318 - val_loss: 28.0037 - val_MinusLogProbMetric: 28.0037 - lr: 6.2500e-05 - 43s/epoch - 219ms/step
Epoch 604/1000
2023-09-29 13:56:01.914 
Epoch 604/1000 
	 loss: 27.3300, MinusLogProbMetric: 27.3300, val_loss: 28.0217, val_MinusLogProbMetric: 28.0217

Epoch 604: val_loss did not improve from 27.99024
196/196 - 43s - loss: 27.3300 - MinusLogProbMetric: 27.3300 - val_loss: 28.0217 - val_MinusLogProbMetric: 28.0217 - lr: 6.2500e-05 - 43s/epoch - 221ms/step
Epoch 605/1000
2023-09-29 13:56:43.552 
Epoch 605/1000 
	 loss: 27.3331, MinusLogProbMetric: 27.3331, val_loss: 27.9922, val_MinusLogProbMetric: 27.9922

Epoch 605: val_loss did not improve from 27.99024
196/196 - 42s - loss: 27.3331 - MinusLogProbMetric: 27.3331 - val_loss: 27.9922 - val_MinusLogProbMetric: 27.9922 - lr: 6.2500e-05 - 42s/epoch - 212ms/step
Epoch 606/1000
2023-09-29 13:57:27.176 
Epoch 606/1000 
	 loss: 27.3320, MinusLogProbMetric: 27.3320, val_loss: 28.0087, val_MinusLogProbMetric: 28.0087

Epoch 606: val_loss did not improve from 27.99024
196/196 - 44s - loss: 27.3320 - MinusLogProbMetric: 27.3320 - val_loss: 28.0087 - val_MinusLogProbMetric: 28.0087 - lr: 6.2500e-05 - 44s/epoch - 223ms/step
Epoch 607/1000
2023-09-29 13:58:08.941 
Epoch 607/1000 
	 loss: 27.3337, MinusLogProbMetric: 27.3337, val_loss: 28.0126, val_MinusLogProbMetric: 28.0126

Epoch 607: val_loss did not improve from 27.99024
196/196 - 42s - loss: 27.3337 - MinusLogProbMetric: 27.3337 - val_loss: 28.0126 - val_MinusLogProbMetric: 28.0126 - lr: 6.2500e-05 - 42s/epoch - 213ms/step
Epoch 608/1000
2023-09-29 13:58:51.494 
Epoch 608/1000 
	 loss: 27.3303, MinusLogProbMetric: 27.3303, val_loss: 27.9904, val_MinusLogProbMetric: 27.9904

Epoch 608: val_loss did not improve from 27.99024
196/196 - 43s - loss: 27.3303 - MinusLogProbMetric: 27.3303 - val_loss: 27.9904 - val_MinusLogProbMetric: 27.9904 - lr: 6.2500e-05 - 43s/epoch - 217ms/step
Epoch 609/1000
2023-09-29 13:59:32.551 
Epoch 609/1000 
	 loss: 27.3275, MinusLogProbMetric: 27.3275, val_loss: 28.0145, val_MinusLogProbMetric: 28.0145

Epoch 609: val_loss did not improve from 27.99024
196/196 - 41s - loss: 27.3275 - MinusLogProbMetric: 27.3275 - val_loss: 28.0145 - val_MinusLogProbMetric: 28.0145 - lr: 6.2500e-05 - 41s/epoch - 209ms/step
Epoch 610/1000
2023-09-29 14:00:11.207 
Epoch 610/1000 
	 loss: 27.3359, MinusLogProbMetric: 27.3359, val_loss: 27.9990, val_MinusLogProbMetric: 27.9990

Epoch 610: val_loss did not improve from 27.99024
196/196 - 39s - loss: 27.3359 - MinusLogProbMetric: 27.3359 - val_loss: 27.9990 - val_MinusLogProbMetric: 27.9990 - lr: 6.2500e-05 - 39s/epoch - 197ms/step
Epoch 611/1000
2023-09-29 14:00:50.963 
Epoch 611/1000 
	 loss: 27.3322, MinusLogProbMetric: 27.3322, val_loss: 28.0040, val_MinusLogProbMetric: 28.0040

Epoch 611: val_loss did not improve from 27.99024
196/196 - 40s - loss: 27.3322 - MinusLogProbMetric: 27.3322 - val_loss: 28.0040 - val_MinusLogProbMetric: 28.0040 - lr: 6.2500e-05 - 40s/epoch - 203ms/step
Epoch 612/1000
2023-09-29 14:01:31.508 
Epoch 612/1000 
	 loss: 27.3292, MinusLogProbMetric: 27.3292, val_loss: 28.0034, val_MinusLogProbMetric: 28.0034

Epoch 612: val_loss did not improve from 27.99024
196/196 - 41s - loss: 27.3292 - MinusLogProbMetric: 27.3292 - val_loss: 28.0034 - val_MinusLogProbMetric: 28.0034 - lr: 6.2500e-05 - 41s/epoch - 207ms/step
Epoch 613/1000
2023-09-29 14:02:12.326 
Epoch 613/1000 
	 loss: 27.3301, MinusLogProbMetric: 27.3301, val_loss: 27.9917, val_MinusLogProbMetric: 27.9917

Epoch 613: val_loss did not improve from 27.99024
196/196 - 41s - loss: 27.3301 - MinusLogProbMetric: 27.3301 - val_loss: 27.9917 - val_MinusLogProbMetric: 27.9917 - lr: 6.2500e-05 - 41s/epoch - 208ms/step
Epoch 614/1000
2023-09-29 14:02:54.574 
Epoch 614/1000 
	 loss: 27.3278, MinusLogProbMetric: 27.3278, val_loss: 27.9994, val_MinusLogProbMetric: 27.9994

Epoch 614: val_loss did not improve from 27.99024
196/196 - 42s - loss: 27.3278 - MinusLogProbMetric: 27.3278 - val_loss: 27.9994 - val_MinusLogProbMetric: 27.9994 - lr: 6.2500e-05 - 42s/epoch - 216ms/step
Epoch 615/1000
2023-09-29 14:03:36.430 
Epoch 615/1000 
	 loss: 27.3284, MinusLogProbMetric: 27.3284, val_loss: 27.9977, val_MinusLogProbMetric: 27.9977

Epoch 615: val_loss did not improve from 27.99024
196/196 - 42s - loss: 27.3284 - MinusLogProbMetric: 27.3284 - val_loss: 27.9977 - val_MinusLogProbMetric: 27.9977 - lr: 6.2500e-05 - 42s/epoch - 214ms/step
Epoch 616/1000
2023-09-29 14:04:19.414 
Epoch 616/1000 
	 loss: 27.3272, MinusLogProbMetric: 27.3272, val_loss: 28.0064, val_MinusLogProbMetric: 28.0064

Epoch 616: val_loss did not improve from 27.99024
196/196 - 43s - loss: 27.3272 - MinusLogProbMetric: 27.3272 - val_loss: 28.0064 - val_MinusLogProbMetric: 28.0064 - lr: 6.2500e-05 - 43s/epoch - 219ms/step
Epoch 617/1000
2023-09-29 14:05:02.227 
Epoch 617/1000 
	 loss: 27.3290, MinusLogProbMetric: 27.3290, val_loss: 27.9993, val_MinusLogProbMetric: 27.9993

Epoch 617: val_loss did not improve from 27.99024
196/196 - 43s - loss: 27.3290 - MinusLogProbMetric: 27.3290 - val_loss: 27.9993 - val_MinusLogProbMetric: 27.9993 - lr: 6.2500e-05 - 43s/epoch - 218ms/step
Epoch 618/1000
2023-09-29 14:05:44.225 
Epoch 618/1000 
	 loss: 27.3324, MinusLogProbMetric: 27.3324, val_loss: 28.0318, val_MinusLogProbMetric: 28.0318

Epoch 618: val_loss did not improve from 27.99024
196/196 - 42s - loss: 27.3324 - MinusLogProbMetric: 27.3324 - val_loss: 28.0318 - val_MinusLogProbMetric: 28.0318 - lr: 6.2500e-05 - 42s/epoch - 214ms/step
Epoch 619/1000
2023-09-29 14:06:23.424 
Epoch 619/1000 
	 loss: 27.3297, MinusLogProbMetric: 27.3297, val_loss: 28.0018, val_MinusLogProbMetric: 28.0018

Epoch 619: val_loss did not improve from 27.99024
196/196 - 39s - loss: 27.3297 - MinusLogProbMetric: 27.3297 - val_loss: 28.0018 - val_MinusLogProbMetric: 28.0018 - lr: 6.2500e-05 - 39s/epoch - 200ms/step
Epoch 620/1000
2023-09-29 14:07:04.126 
Epoch 620/1000 
	 loss: 27.3252, MinusLogProbMetric: 27.3252, val_loss: 27.9973, val_MinusLogProbMetric: 27.9973

Epoch 620: val_loss did not improve from 27.99024
196/196 - 41s - loss: 27.3252 - MinusLogProbMetric: 27.3252 - val_loss: 27.9973 - val_MinusLogProbMetric: 27.9973 - lr: 6.2500e-05 - 41s/epoch - 208ms/step
Epoch 621/1000
2023-09-29 14:07:46.446 
Epoch 621/1000 
	 loss: 27.3298, MinusLogProbMetric: 27.3298, val_loss: 28.0102, val_MinusLogProbMetric: 28.0102

Epoch 621: val_loss did not improve from 27.99024
196/196 - 42s - loss: 27.3298 - MinusLogProbMetric: 27.3298 - val_loss: 28.0102 - val_MinusLogProbMetric: 28.0102 - lr: 6.2500e-05 - 42s/epoch - 216ms/step
Epoch 622/1000
2023-09-29 14:08:26.856 
Epoch 622/1000 
	 loss: 27.3269, MinusLogProbMetric: 27.3269, val_loss: 28.0027, val_MinusLogProbMetric: 28.0027

Epoch 622: val_loss did not improve from 27.99024
196/196 - 40s - loss: 27.3269 - MinusLogProbMetric: 27.3269 - val_loss: 28.0027 - val_MinusLogProbMetric: 28.0027 - lr: 6.2500e-05 - 40s/epoch - 206ms/step
Epoch 623/1000
2023-09-29 14:09:05.366 
Epoch 623/1000 
	 loss: 27.3275, MinusLogProbMetric: 27.3275, val_loss: 28.0011, val_MinusLogProbMetric: 28.0011

Epoch 623: val_loss did not improve from 27.99024
196/196 - 39s - loss: 27.3275 - MinusLogProbMetric: 27.3275 - val_loss: 28.0011 - val_MinusLogProbMetric: 28.0011 - lr: 6.2500e-05 - 39s/epoch - 196ms/step
Epoch 624/1000
2023-09-29 14:09:43.711 
Epoch 624/1000 
	 loss: 27.3336, MinusLogProbMetric: 27.3336, val_loss: 27.9947, val_MinusLogProbMetric: 27.9947

Epoch 624: val_loss did not improve from 27.99024
196/196 - 38s - loss: 27.3336 - MinusLogProbMetric: 27.3336 - val_loss: 27.9947 - val_MinusLogProbMetric: 27.9947 - lr: 6.2500e-05 - 38s/epoch - 196ms/step
Epoch 625/1000
2023-09-29 14:10:24.211 
Epoch 625/1000 
	 loss: 27.3296, MinusLogProbMetric: 27.3296, val_loss: 27.9949, val_MinusLogProbMetric: 27.9949

Epoch 625: val_loss did not improve from 27.99024
196/196 - 40s - loss: 27.3296 - MinusLogProbMetric: 27.3296 - val_loss: 27.9949 - val_MinusLogProbMetric: 27.9949 - lr: 6.2500e-05 - 40s/epoch - 207ms/step
Epoch 626/1000
2023-09-29 14:11:00.622 
Epoch 626/1000 
	 loss: 27.3295, MinusLogProbMetric: 27.3295, val_loss: 28.0151, val_MinusLogProbMetric: 28.0151

Epoch 626: val_loss did not improve from 27.99024
196/196 - 36s - loss: 27.3295 - MinusLogProbMetric: 27.3295 - val_loss: 28.0151 - val_MinusLogProbMetric: 28.0151 - lr: 6.2500e-05 - 36s/epoch - 186ms/step
Epoch 627/1000
2023-09-29 14:11:34.661 
Epoch 627/1000 
	 loss: 27.3318, MinusLogProbMetric: 27.3318, val_loss: 28.0093, val_MinusLogProbMetric: 28.0093

Epoch 627: val_loss did not improve from 27.99024
196/196 - 34s - loss: 27.3318 - MinusLogProbMetric: 27.3318 - val_loss: 28.0093 - val_MinusLogProbMetric: 28.0093 - lr: 6.2500e-05 - 34s/epoch - 174ms/step
Epoch 628/1000
2023-09-29 14:12:08.160 
Epoch 628/1000 
	 loss: 27.3300, MinusLogProbMetric: 27.3300, val_loss: 28.0234, val_MinusLogProbMetric: 28.0234

Epoch 628: val_loss did not improve from 27.99024
196/196 - 33s - loss: 27.3300 - MinusLogProbMetric: 27.3300 - val_loss: 28.0234 - val_MinusLogProbMetric: 28.0234 - lr: 6.2500e-05 - 33s/epoch - 171ms/step
Epoch 629/1000
2023-09-29 14:12:45.318 
Epoch 629/1000 
	 loss: 27.3104, MinusLogProbMetric: 27.3104, val_loss: 27.9866, val_MinusLogProbMetric: 27.9866

Epoch 629: val_loss improved from 27.99024 to 27.98664, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_332/weights/best_weights.h5
196/196 - 39s - loss: 27.3104 - MinusLogProbMetric: 27.3104 - val_loss: 27.9866 - val_MinusLogProbMetric: 27.9866 - lr: 3.1250e-05 - 39s/epoch - 198ms/step
Epoch 630/1000
2023-09-29 14:13:23.017 
Epoch 630/1000 
	 loss: 27.3107, MinusLogProbMetric: 27.3107, val_loss: 27.9842, val_MinusLogProbMetric: 27.9842

Epoch 630: val_loss improved from 27.98664 to 27.98424, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_332/weights/best_weights.h5
196/196 - 37s - loss: 27.3107 - MinusLogProbMetric: 27.3107 - val_loss: 27.9842 - val_MinusLogProbMetric: 27.9842 - lr: 3.1250e-05 - 37s/epoch - 188ms/step
Epoch 631/1000
2023-09-29 14:13:57.645 
Epoch 631/1000 
	 loss: 27.3104, MinusLogProbMetric: 27.3104, val_loss: 27.9862, val_MinusLogProbMetric: 27.9862

Epoch 631: val_loss did not improve from 27.98424
196/196 - 34s - loss: 27.3104 - MinusLogProbMetric: 27.3104 - val_loss: 27.9862 - val_MinusLogProbMetric: 27.9862 - lr: 3.1250e-05 - 34s/epoch - 173ms/step
Epoch 632/1000
2023-09-29 14:14:31.663 
Epoch 632/1000 
	 loss: 27.3105, MinusLogProbMetric: 27.3105, val_loss: 27.9855, val_MinusLogProbMetric: 27.9855

Epoch 632: val_loss did not improve from 27.98424
196/196 - 34s - loss: 27.3105 - MinusLogProbMetric: 27.3105 - val_loss: 27.9855 - val_MinusLogProbMetric: 27.9855 - lr: 3.1250e-05 - 34s/epoch - 174ms/step
Epoch 633/1000
2023-09-29 14:15:05.556 
Epoch 633/1000 
	 loss: 27.3099, MinusLogProbMetric: 27.3099, val_loss: 27.9848, val_MinusLogProbMetric: 27.9848

Epoch 633: val_loss did not improve from 27.98424
196/196 - 34s - loss: 27.3099 - MinusLogProbMetric: 27.3099 - val_loss: 27.9848 - val_MinusLogProbMetric: 27.9848 - lr: 3.1250e-05 - 34s/epoch - 173ms/step
Epoch 634/1000
2023-09-29 14:15:39.305 
Epoch 634/1000 
	 loss: 27.3091, MinusLogProbMetric: 27.3091, val_loss: 27.9860, val_MinusLogProbMetric: 27.9860

Epoch 634: val_loss did not improve from 27.98424
196/196 - 34s - loss: 27.3091 - MinusLogProbMetric: 27.3091 - val_loss: 27.9860 - val_MinusLogProbMetric: 27.9860 - lr: 3.1250e-05 - 34s/epoch - 172ms/step
Epoch 635/1000
2023-09-29 14:16:14.062 
Epoch 635/1000 
	 loss: 27.3078, MinusLogProbMetric: 27.3078, val_loss: 27.9932, val_MinusLogProbMetric: 27.9932

Epoch 635: val_loss did not improve from 27.98424
196/196 - 35s - loss: 27.3078 - MinusLogProbMetric: 27.3078 - val_loss: 27.9932 - val_MinusLogProbMetric: 27.9932 - lr: 3.1250e-05 - 35s/epoch - 177ms/step
Epoch 636/1000
2023-09-29 14:16:50.694 
Epoch 636/1000 
	 loss: 27.3087, MinusLogProbMetric: 27.3087, val_loss: 27.9791, val_MinusLogProbMetric: 27.9791

Epoch 636: val_loss improved from 27.98424 to 27.97906, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_332/weights/best_weights.h5
196/196 - 38s - loss: 27.3087 - MinusLogProbMetric: 27.3087 - val_loss: 27.9791 - val_MinusLogProbMetric: 27.9791 - lr: 3.1250e-05 - 38s/epoch - 195ms/step
Epoch 637/1000
2023-09-29 14:17:27.224 
Epoch 637/1000 
	 loss: 27.3080, MinusLogProbMetric: 27.3080, val_loss: 27.9829, val_MinusLogProbMetric: 27.9829

Epoch 637: val_loss did not improve from 27.97906
196/196 - 35s - loss: 27.3080 - MinusLogProbMetric: 27.3080 - val_loss: 27.9829 - val_MinusLogProbMetric: 27.9829 - lr: 3.1250e-05 - 35s/epoch - 178ms/step
Epoch 638/1000
2023-09-29 14:18:06.455 
Epoch 638/1000 
	 loss: 27.3077, MinusLogProbMetric: 27.3077, val_loss: 27.9868, val_MinusLogProbMetric: 27.9868

Epoch 638: val_loss did not improve from 27.97906
196/196 - 39s - loss: 27.3077 - MinusLogProbMetric: 27.3077 - val_loss: 27.9868 - val_MinusLogProbMetric: 27.9868 - lr: 3.1250e-05 - 39s/epoch - 200ms/step
Epoch 639/1000
2023-09-29 14:18:41.790 
Epoch 639/1000 
	 loss: 27.3118, MinusLogProbMetric: 27.3118, val_loss: 27.9926, val_MinusLogProbMetric: 27.9926

Epoch 639: val_loss did not improve from 27.97906
196/196 - 35s - loss: 27.3118 - MinusLogProbMetric: 27.3118 - val_loss: 27.9926 - val_MinusLogProbMetric: 27.9926 - lr: 3.1250e-05 - 35s/epoch - 180ms/step
Epoch 640/1000
2023-09-29 14:19:16.085 
Epoch 640/1000 
	 loss: 27.3083, MinusLogProbMetric: 27.3083, val_loss: 27.9855, val_MinusLogProbMetric: 27.9855

Epoch 640: val_loss did not improve from 27.97906
196/196 - 34s - loss: 27.3083 - MinusLogProbMetric: 27.3083 - val_loss: 27.9855 - val_MinusLogProbMetric: 27.9855 - lr: 3.1250e-05 - 34s/epoch - 175ms/step
Epoch 641/1000
2023-09-29 14:19:50.821 
Epoch 641/1000 
	 loss: 27.3075, MinusLogProbMetric: 27.3075, val_loss: 27.9920, val_MinusLogProbMetric: 27.9920

Epoch 641: val_loss did not improve from 27.97906
196/196 - 35s - loss: 27.3075 - MinusLogProbMetric: 27.3075 - val_loss: 27.9920 - val_MinusLogProbMetric: 27.9920 - lr: 3.1250e-05 - 35s/epoch - 177ms/step
Epoch 642/1000
2023-09-29 14:20:25.272 
Epoch 642/1000 
	 loss: 27.3088, MinusLogProbMetric: 27.3088, val_loss: 27.9861, val_MinusLogProbMetric: 27.9861

Epoch 642: val_loss did not improve from 27.97906
196/196 - 34s - loss: 27.3088 - MinusLogProbMetric: 27.3088 - val_loss: 27.9861 - val_MinusLogProbMetric: 27.9861 - lr: 3.1250e-05 - 34s/epoch - 176ms/step
Epoch 643/1000
2023-09-29 14:21:01.265 
Epoch 643/1000 
	 loss: 27.3095, MinusLogProbMetric: 27.3095, val_loss: 27.9962, val_MinusLogProbMetric: 27.9962

Epoch 643: val_loss did not improve from 27.97906
196/196 - 36s - loss: 27.3095 - MinusLogProbMetric: 27.3095 - val_loss: 27.9962 - val_MinusLogProbMetric: 27.9962 - lr: 3.1250e-05 - 36s/epoch - 184ms/step
Epoch 644/1000
2023-09-29 14:21:34.915 
Epoch 644/1000 
	 loss: 27.3093, MinusLogProbMetric: 27.3093, val_loss: 27.9851, val_MinusLogProbMetric: 27.9851

Epoch 644: val_loss did not improve from 27.97906
196/196 - 34s - loss: 27.3093 - MinusLogProbMetric: 27.3093 - val_loss: 27.9851 - val_MinusLogProbMetric: 27.9851 - lr: 3.1250e-05 - 34s/epoch - 172ms/step
Epoch 645/1000
2023-09-29 14:22:10.311 
Epoch 645/1000 
	 loss: 27.3082, MinusLogProbMetric: 27.3082, val_loss: 27.9888, val_MinusLogProbMetric: 27.9888

Epoch 645: val_loss did not improve from 27.97906
196/196 - 35s - loss: 27.3082 - MinusLogProbMetric: 27.3082 - val_loss: 27.9888 - val_MinusLogProbMetric: 27.9888 - lr: 3.1250e-05 - 35s/epoch - 181ms/step
Epoch 646/1000
2023-09-29 14:22:45.372 
Epoch 646/1000 
	 loss: 27.3079, MinusLogProbMetric: 27.3079, val_loss: 27.9935, val_MinusLogProbMetric: 27.9935

Epoch 646: val_loss did not improve from 27.97906
196/196 - 35s - loss: 27.3079 - MinusLogProbMetric: 27.3079 - val_loss: 27.9935 - val_MinusLogProbMetric: 27.9935 - lr: 3.1250e-05 - 35s/epoch - 179ms/step
Epoch 647/1000
2023-09-29 14:23:19.131 
Epoch 647/1000 
	 loss: 27.3092, MinusLogProbMetric: 27.3092, val_loss: 27.9919, val_MinusLogProbMetric: 27.9919

Epoch 647: val_loss did not improve from 27.97906
196/196 - 34s - loss: 27.3092 - MinusLogProbMetric: 27.3092 - val_loss: 27.9919 - val_MinusLogProbMetric: 27.9919 - lr: 3.1250e-05 - 34s/epoch - 172ms/step
Epoch 648/1000
2023-09-29 14:23:52.674 
Epoch 648/1000 
	 loss: 27.3115, MinusLogProbMetric: 27.3115, val_loss: 27.9851, val_MinusLogProbMetric: 27.9851

Epoch 648: val_loss did not improve from 27.97906
196/196 - 34s - loss: 27.3115 - MinusLogProbMetric: 27.3115 - val_loss: 27.9851 - val_MinusLogProbMetric: 27.9851 - lr: 3.1250e-05 - 34s/epoch - 171ms/step
Epoch 649/1000
2023-09-29 14:24:26.358 
Epoch 649/1000 
	 loss: 27.3089, MinusLogProbMetric: 27.3089, val_loss: 27.9865, val_MinusLogProbMetric: 27.9865

Epoch 649: val_loss did not improve from 27.97906
196/196 - 34s - loss: 27.3089 - MinusLogProbMetric: 27.3089 - val_loss: 27.9865 - val_MinusLogProbMetric: 27.9865 - lr: 3.1250e-05 - 34s/epoch - 172ms/step
Epoch 650/1000
2023-09-29 14:25:00.386 
Epoch 650/1000 
	 loss: 27.3090, MinusLogProbMetric: 27.3090, val_loss: 27.9826, val_MinusLogProbMetric: 27.9826

Epoch 650: val_loss did not improve from 27.97906
196/196 - 34s - loss: 27.3090 - MinusLogProbMetric: 27.3090 - val_loss: 27.9826 - val_MinusLogProbMetric: 27.9826 - lr: 3.1250e-05 - 34s/epoch - 174ms/step
Epoch 651/1000
2023-09-29 14:25:34.695 
Epoch 651/1000 
	 loss: 27.3085, MinusLogProbMetric: 27.3085, val_loss: 27.9804, val_MinusLogProbMetric: 27.9804

Epoch 651: val_loss did not improve from 27.97906
196/196 - 34s - loss: 27.3085 - MinusLogProbMetric: 27.3085 - val_loss: 27.9804 - val_MinusLogProbMetric: 27.9804 - lr: 3.1250e-05 - 34s/epoch - 175ms/step
Epoch 652/1000
2023-09-29 14:26:08.600 
Epoch 652/1000 
	 loss: 27.3086, MinusLogProbMetric: 27.3086, val_loss: 27.9871, val_MinusLogProbMetric: 27.9871

Epoch 652: val_loss did not improve from 27.97906
196/196 - 34s - loss: 27.3086 - MinusLogProbMetric: 27.3086 - val_loss: 27.9871 - val_MinusLogProbMetric: 27.9871 - lr: 3.1250e-05 - 34s/epoch - 173ms/step
Epoch 653/1000
2023-09-29 14:26:45.164 
Epoch 653/1000 
	 loss: 27.3106, MinusLogProbMetric: 27.3106, val_loss: 27.9854, val_MinusLogProbMetric: 27.9854

Epoch 653: val_loss did not improve from 27.97906
196/196 - 37s - loss: 27.3106 - MinusLogProbMetric: 27.3106 - val_loss: 27.9854 - val_MinusLogProbMetric: 27.9854 - lr: 3.1250e-05 - 37s/epoch - 187ms/step
Epoch 654/1000
2023-09-29 14:27:19.596 
Epoch 654/1000 
	 loss: 27.3102, MinusLogProbMetric: 27.3102, val_loss: 27.9872, val_MinusLogProbMetric: 27.9872

Epoch 654: val_loss did not improve from 27.97906
196/196 - 34s - loss: 27.3102 - MinusLogProbMetric: 27.3102 - val_loss: 27.9872 - val_MinusLogProbMetric: 27.9872 - lr: 3.1250e-05 - 34s/epoch - 176ms/step
Epoch 655/1000
2023-09-29 14:27:56.791 
Epoch 655/1000 
	 loss: 27.3078, MinusLogProbMetric: 27.3078, val_loss: 27.9868, val_MinusLogProbMetric: 27.9868

Epoch 655: val_loss did not improve from 27.97906
196/196 - 37s - loss: 27.3078 - MinusLogProbMetric: 27.3078 - val_loss: 27.9868 - val_MinusLogProbMetric: 27.9868 - lr: 3.1250e-05 - 37s/epoch - 190ms/step
Epoch 656/1000
2023-09-29 14:28:33.078 
Epoch 656/1000 
	 loss: 27.3076, MinusLogProbMetric: 27.3076, val_loss: 27.9885, val_MinusLogProbMetric: 27.9885

Epoch 656: val_loss did not improve from 27.97906
196/196 - 36s - loss: 27.3076 - MinusLogProbMetric: 27.3076 - val_loss: 27.9885 - val_MinusLogProbMetric: 27.9885 - lr: 3.1250e-05 - 36s/epoch - 185ms/step
Epoch 657/1000
2023-09-29 14:29:10.216 
Epoch 657/1000 
	 loss: 27.3093, MinusLogProbMetric: 27.3093, val_loss: 27.9839, val_MinusLogProbMetric: 27.9839

Epoch 657: val_loss did not improve from 27.97906
196/196 - 37s - loss: 27.3093 - MinusLogProbMetric: 27.3093 - val_loss: 27.9839 - val_MinusLogProbMetric: 27.9839 - lr: 3.1250e-05 - 37s/epoch - 189ms/step
Epoch 658/1000
2023-09-29 14:29:46.067 
Epoch 658/1000 
	 loss: 27.3082, MinusLogProbMetric: 27.3082, val_loss: 27.9926, val_MinusLogProbMetric: 27.9926

Epoch 658: val_loss did not improve from 27.97906
196/196 - 36s - loss: 27.3082 - MinusLogProbMetric: 27.3082 - val_loss: 27.9926 - val_MinusLogProbMetric: 27.9926 - lr: 3.1250e-05 - 36s/epoch - 183ms/step
Epoch 659/1000
2023-09-29 14:30:20.335 
Epoch 659/1000 
	 loss: 27.3080, MinusLogProbMetric: 27.3080, val_loss: 27.9846, val_MinusLogProbMetric: 27.9846

Epoch 659: val_loss did not improve from 27.97906
196/196 - 34s - loss: 27.3080 - MinusLogProbMetric: 27.3080 - val_loss: 27.9846 - val_MinusLogProbMetric: 27.9846 - lr: 3.1250e-05 - 34s/epoch - 175ms/step
Epoch 660/1000
2023-09-29 14:30:54.083 
Epoch 660/1000 
	 loss: 27.3067, MinusLogProbMetric: 27.3067, val_loss: 27.9990, val_MinusLogProbMetric: 27.9990

Epoch 660: val_loss did not improve from 27.97906
196/196 - 34s - loss: 27.3067 - MinusLogProbMetric: 27.3067 - val_loss: 27.9990 - val_MinusLogProbMetric: 27.9990 - lr: 3.1250e-05 - 34s/epoch - 172ms/step
Epoch 661/1000
2023-09-29 14:31:30.023 
Epoch 661/1000 
	 loss: 27.3088, MinusLogProbMetric: 27.3088, val_loss: 27.9931, val_MinusLogProbMetric: 27.9931

Epoch 661: val_loss did not improve from 27.97906
196/196 - 36s - loss: 27.3088 - MinusLogProbMetric: 27.3088 - val_loss: 27.9931 - val_MinusLogProbMetric: 27.9931 - lr: 3.1250e-05 - 36s/epoch - 183ms/step
Epoch 662/1000
2023-09-29 14:32:04.031 
Epoch 662/1000 
	 loss: 27.3087, MinusLogProbMetric: 27.3087, val_loss: 27.9851, val_MinusLogProbMetric: 27.9851

Epoch 662: val_loss did not improve from 27.97906
196/196 - 34s - loss: 27.3087 - MinusLogProbMetric: 27.3087 - val_loss: 27.9851 - val_MinusLogProbMetric: 27.9851 - lr: 3.1250e-05 - 34s/epoch - 173ms/step
Epoch 663/1000
2023-09-29 14:32:37.749 
Epoch 663/1000 
	 loss: 27.3077, MinusLogProbMetric: 27.3077, val_loss: 27.9922, val_MinusLogProbMetric: 27.9922

Epoch 663: val_loss did not improve from 27.97906
196/196 - 34s - loss: 27.3077 - MinusLogProbMetric: 27.3077 - val_loss: 27.9922 - val_MinusLogProbMetric: 27.9922 - lr: 3.1250e-05 - 34s/epoch - 172ms/step
Epoch 664/1000
2023-09-29 14:33:13.044 
Epoch 664/1000 
	 loss: 27.3091, MinusLogProbMetric: 27.3091, val_loss: 27.9855, val_MinusLogProbMetric: 27.9855

Epoch 664: val_loss did not improve from 27.97906
196/196 - 35s - loss: 27.3091 - MinusLogProbMetric: 27.3091 - val_loss: 27.9855 - val_MinusLogProbMetric: 27.9855 - lr: 3.1250e-05 - 35s/epoch - 180ms/step
Epoch 665/1000
2023-09-29 14:33:46.529 
Epoch 665/1000 
	 loss: 27.3069, MinusLogProbMetric: 27.3069, val_loss: 27.9807, val_MinusLogProbMetric: 27.9807

Epoch 665: val_loss did not improve from 27.97906
196/196 - 33s - loss: 27.3069 - MinusLogProbMetric: 27.3069 - val_loss: 27.9807 - val_MinusLogProbMetric: 27.9807 - lr: 3.1250e-05 - 33s/epoch - 171ms/step
Epoch 666/1000
2023-09-29 14:34:20.450 
Epoch 666/1000 
	 loss: 27.3055, MinusLogProbMetric: 27.3055, val_loss: 27.9883, val_MinusLogProbMetric: 27.9883

Epoch 666: val_loss did not improve from 27.97906
196/196 - 34s - loss: 27.3055 - MinusLogProbMetric: 27.3055 - val_loss: 27.9883 - val_MinusLogProbMetric: 27.9883 - lr: 3.1250e-05 - 34s/epoch - 173ms/step
Epoch 667/1000
2023-09-29 14:34:56.768 
Epoch 667/1000 
	 loss: 27.3085, MinusLogProbMetric: 27.3085, val_loss: 27.9873, val_MinusLogProbMetric: 27.9873

Epoch 667: val_loss did not improve from 27.97906
196/196 - 36s - loss: 27.3085 - MinusLogProbMetric: 27.3085 - val_loss: 27.9873 - val_MinusLogProbMetric: 27.9873 - lr: 3.1250e-05 - 36s/epoch - 185ms/step
Epoch 668/1000
2023-09-29 14:35:30.263 
Epoch 668/1000 
	 loss: 27.3080, MinusLogProbMetric: 27.3080, val_loss: 27.9809, val_MinusLogProbMetric: 27.9809

Epoch 668: val_loss did not improve from 27.97906
196/196 - 34s - loss: 27.3080 - MinusLogProbMetric: 27.3080 - val_loss: 27.9809 - val_MinusLogProbMetric: 27.9809 - lr: 3.1250e-05 - 34s/epoch - 171ms/step
Epoch 669/1000
2023-09-29 14:36:04.454 
Epoch 669/1000 
	 loss: 27.3077, MinusLogProbMetric: 27.3077, val_loss: 27.9884, val_MinusLogProbMetric: 27.9884

Epoch 669: val_loss did not improve from 27.97906
196/196 - 34s - loss: 27.3077 - MinusLogProbMetric: 27.3077 - val_loss: 27.9884 - val_MinusLogProbMetric: 27.9884 - lr: 3.1250e-05 - 34s/epoch - 174ms/step
Epoch 670/1000
2023-09-29 14:36:38.289 
Epoch 670/1000 
	 loss: 27.3073, MinusLogProbMetric: 27.3073, val_loss: 27.9891, val_MinusLogProbMetric: 27.9891

Epoch 670: val_loss did not improve from 27.97906
196/196 - 34s - loss: 27.3073 - MinusLogProbMetric: 27.3073 - val_loss: 27.9891 - val_MinusLogProbMetric: 27.9891 - lr: 3.1250e-05 - 34s/epoch - 173ms/step
Epoch 671/1000
2023-09-29 14:37:11.906 
Epoch 671/1000 
	 loss: 27.3070, MinusLogProbMetric: 27.3070, val_loss: 27.9909, val_MinusLogProbMetric: 27.9909

Epoch 671: val_loss did not improve from 27.97906
196/196 - 34s - loss: 27.3070 - MinusLogProbMetric: 27.3070 - val_loss: 27.9909 - val_MinusLogProbMetric: 27.9909 - lr: 3.1250e-05 - 34s/epoch - 172ms/step
Epoch 672/1000
2023-09-29 14:37:48.103 
Epoch 672/1000 
	 loss: 27.3069, MinusLogProbMetric: 27.3069, val_loss: 28.0062, val_MinusLogProbMetric: 28.0062

Epoch 672: val_loss did not improve from 27.97906
196/196 - 36s - loss: 27.3069 - MinusLogProbMetric: 27.3069 - val_loss: 28.0062 - val_MinusLogProbMetric: 28.0062 - lr: 3.1250e-05 - 36s/epoch - 185ms/step
Epoch 673/1000
2023-09-29 14:38:24.101 
Epoch 673/1000 
	 loss: 27.3092, MinusLogProbMetric: 27.3092, val_loss: 27.9954, val_MinusLogProbMetric: 27.9954

Epoch 673: val_loss did not improve from 27.97906
196/196 - 36s - loss: 27.3092 - MinusLogProbMetric: 27.3092 - val_loss: 27.9954 - val_MinusLogProbMetric: 27.9954 - lr: 3.1250e-05 - 36s/epoch - 184ms/step
Epoch 674/1000
2023-09-29 14:38:58.591 
Epoch 674/1000 
	 loss: 27.3068, MinusLogProbMetric: 27.3068, val_loss: 27.9912, val_MinusLogProbMetric: 27.9912

Epoch 674: val_loss did not improve from 27.97906
196/196 - 34s - loss: 27.3068 - MinusLogProbMetric: 27.3068 - val_loss: 27.9912 - val_MinusLogProbMetric: 27.9912 - lr: 3.1250e-05 - 34s/epoch - 176ms/step
Epoch 675/1000
2023-09-29 14:39:33.922 
Epoch 675/1000 
	 loss: 27.3060, MinusLogProbMetric: 27.3060, val_loss: 27.9931, val_MinusLogProbMetric: 27.9931

Epoch 675: val_loss did not improve from 27.97906
196/196 - 35s - loss: 27.3060 - MinusLogProbMetric: 27.3060 - val_loss: 27.9931 - val_MinusLogProbMetric: 27.9931 - lr: 3.1250e-05 - 35s/epoch - 180ms/step
Epoch 676/1000
2023-09-29 14:40:11.852 
Epoch 676/1000 
	 loss: 27.3069, MinusLogProbMetric: 27.3069, val_loss: 27.9938, val_MinusLogProbMetric: 27.9938

Epoch 676: val_loss did not improve from 27.97906
196/196 - 38s - loss: 27.3069 - MinusLogProbMetric: 27.3069 - val_loss: 27.9938 - val_MinusLogProbMetric: 27.9938 - lr: 3.1250e-05 - 38s/epoch - 193ms/step
Epoch 677/1000
2023-09-29 14:40:48.286 
Epoch 677/1000 
	 loss: 27.3070, MinusLogProbMetric: 27.3070, val_loss: 27.9892, val_MinusLogProbMetric: 27.9892

Epoch 677: val_loss did not improve from 27.97906
196/196 - 36s - loss: 27.3070 - MinusLogProbMetric: 27.3070 - val_loss: 27.9892 - val_MinusLogProbMetric: 27.9892 - lr: 3.1250e-05 - 36s/epoch - 186ms/step
Epoch 678/1000
2023-09-29 14:41:25.133 
Epoch 678/1000 
	 loss: 27.3079, MinusLogProbMetric: 27.3079, val_loss: 27.9919, val_MinusLogProbMetric: 27.9919

Epoch 678: val_loss did not improve from 27.97906
196/196 - 37s - loss: 27.3079 - MinusLogProbMetric: 27.3079 - val_loss: 27.9919 - val_MinusLogProbMetric: 27.9919 - lr: 3.1250e-05 - 37s/epoch - 188ms/step
Epoch 679/1000
2023-09-29 14:41:59.062 
Epoch 679/1000 
	 loss: 27.3074, MinusLogProbMetric: 27.3074, val_loss: 27.9882, val_MinusLogProbMetric: 27.9882

Epoch 679: val_loss did not improve from 27.97906
196/196 - 34s - loss: 27.3074 - MinusLogProbMetric: 27.3074 - val_loss: 27.9882 - val_MinusLogProbMetric: 27.9882 - lr: 3.1250e-05 - 34s/epoch - 173ms/step
Epoch 680/1000
2023-09-29 14:42:35.568 
Epoch 680/1000 
	 loss: 27.3062, MinusLogProbMetric: 27.3062, val_loss: 27.9841, val_MinusLogProbMetric: 27.9841

Epoch 680: val_loss did not improve from 27.97906
196/196 - 37s - loss: 27.3062 - MinusLogProbMetric: 27.3062 - val_loss: 27.9841 - val_MinusLogProbMetric: 27.9841 - lr: 3.1250e-05 - 37s/epoch - 186ms/step
Epoch 681/1000
2023-09-29 14:43:10.752 
Epoch 681/1000 
	 loss: 27.3071, MinusLogProbMetric: 27.3071, val_loss: 27.9886, val_MinusLogProbMetric: 27.9886

Epoch 681: val_loss did not improve from 27.97906
196/196 - 35s - loss: 27.3071 - MinusLogProbMetric: 27.3071 - val_loss: 27.9886 - val_MinusLogProbMetric: 27.9886 - lr: 3.1250e-05 - 35s/epoch - 179ms/step
Epoch 682/1000
2023-09-29 14:43:45.995 
Epoch 682/1000 
	 loss: 27.3065, MinusLogProbMetric: 27.3065, val_loss: 27.9883, val_MinusLogProbMetric: 27.9883

Epoch 682: val_loss did not improve from 27.97906
196/196 - 35s - loss: 27.3065 - MinusLogProbMetric: 27.3065 - val_loss: 27.9883 - val_MinusLogProbMetric: 27.9883 - lr: 3.1250e-05 - 35s/epoch - 180ms/step
Epoch 683/1000
2023-09-29 14:44:20.820 
Epoch 683/1000 
	 loss: 27.3055, MinusLogProbMetric: 27.3055, val_loss: 27.9911, val_MinusLogProbMetric: 27.9911

Epoch 683: val_loss did not improve from 27.97906
196/196 - 35s - loss: 27.3055 - MinusLogProbMetric: 27.3055 - val_loss: 27.9911 - val_MinusLogProbMetric: 27.9911 - lr: 3.1250e-05 - 35s/epoch - 178ms/step
Epoch 684/1000
2023-09-29 14:44:54.261 
Epoch 684/1000 
	 loss: 27.3042, MinusLogProbMetric: 27.3042, val_loss: 27.9999, val_MinusLogProbMetric: 27.9999

Epoch 684: val_loss did not improve from 27.97906
196/196 - 33s - loss: 27.3042 - MinusLogProbMetric: 27.3042 - val_loss: 27.9999 - val_MinusLogProbMetric: 27.9999 - lr: 3.1250e-05 - 33s/epoch - 171ms/step
Epoch 685/1000
2023-09-29 14:45:27.942 
Epoch 685/1000 
	 loss: 27.3058, MinusLogProbMetric: 27.3058, val_loss: 27.9865, val_MinusLogProbMetric: 27.9865

Epoch 685: val_loss did not improve from 27.97906
196/196 - 34s - loss: 27.3058 - MinusLogProbMetric: 27.3058 - val_loss: 27.9865 - val_MinusLogProbMetric: 27.9865 - lr: 3.1250e-05 - 34s/epoch - 172ms/step
Epoch 686/1000
2023-09-29 14:46:02.933 
Epoch 686/1000 
	 loss: 27.3068, MinusLogProbMetric: 27.3068, val_loss: 27.9813, val_MinusLogProbMetric: 27.9813

Epoch 686: val_loss did not improve from 27.97906
196/196 - 35s - loss: 27.3068 - MinusLogProbMetric: 27.3068 - val_loss: 27.9813 - val_MinusLogProbMetric: 27.9813 - lr: 3.1250e-05 - 35s/epoch - 179ms/step
Epoch 687/1000
2023-09-29 14:46:36.340 
Epoch 687/1000 
	 loss: 27.2963, MinusLogProbMetric: 27.2963, val_loss: 27.9808, val_MinusLogProbMetric: 27.9808

Epoch 687: val_loss did not improve from 27.97906
196/196 - 33s - loss: 27.2963 - MinusLogProbMetric: 27.2963 - val_loss: 27.9808 - val_MinusLogProbMetric: 27.9808 - lr: 1.5625e-05 - 33s/epoch - 170ms/step
Epoch 688/1000
2023-09-29 14:47:10.336 
Epoch 688/1000 
	 loss: 27.2966, MinusLogProbMetric: 27.2966, val_loss: 27.9829, val_MinusLogProbMetric: 27.9829

Epoch 688: val_loss did not improve from 27.97906
196/196 - 34s - loss: 27.2966 - MinusLogProbMetric: 27.2966 - val_loss: 27.9829 - val_MinusLogProbMetric: 27.9829 - lr: 1.5625e-05 - 34s/epoch - 173ms/step
Epoch 689/1000
2023-09-29 14:47:46.167 
Epoch 689/1000 
	 loss: 27.2956, MinusLogProbMetric: 27.2956, val_loss: 27.9787, val_MinusLogProbMetric: 27.9787

Epoch 689: val_loss improved from 27.97906 to 27.97873, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_332/weights/best_weights.h5
196/196 - 37s - loss: 27.2956 - MinusLogProbMetric: 27.2956 - val_loss: 27.9787 - val_MinusLogProbMetric: 27.9787 - lr: 1.5625e-05 - 37s/epoch - 187ms/step
Epoch 690/1000
2023-09-29 14:48:21.676 
Epoch 690/1000 
	 loss: 27.2957, MinusLogProbMetric: 27.2957, val_loss: 27.9768, val_MinusLogProbMetric: 27.9768

Epoch 690: val_loss improved from 27.97873 to 27.97684, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_332/weights/best_weights.h5
196/196 - 36s - loss: 27.2957 - MinusLogProbMetric: 27.2957 - val_loss: 27.9768 - val_MinusLogProbMetric: 27.9768 - lr: 1.5625e-05 - 36s/epoch - 182ms/step
Epoch 691/1000
2023-09-29 14:48:56.303 
Epoch 691/1000 
	 loss: 27.2967, MinusLogProbMetric: 27.2967, val_loss: 27.9875, val_MinusLogProbMetric: 27.9875

Epoch 691: val_loss did not improve from 27.97684
196/196 - 34s - loss: 27.2967 - MinusLogProbMetric: 27.2967 - val_loss: 27.9875 - val_MinusLogProbMetric: 27.9875 - lr: 1.5625e-05 - 34s/epoch - 172ms/step
Epoch 692/1000
2023-09-29 14:49:34.407 
Epoch 692/1000 
	 loss: 27.2952, MinusLogProbMetric: 27.2952, val_loss: 27.9803, val_MinusLogProbMetric: 27.9803

Epoch 692: val_loss did not improve from 27.97684
196/196 - 38s - loss: 27.2952 - MinusLogProbMetric: 27.2952 - val_loss: 27.9803 - val_MinusLogProbMetric: 27.9803 - lr: 1.5625e-05 - 38s/epoch - 194ms/step
Epoch 693/1000
2023-09-29 14:50:10.452 
Epoch 693/1000 
	 loss: 27.2952, MinusLogProbMetric: 27.2952, val_loss: 27.9815, val_MinusLogProbMetric: 27.9815

Epoch 693: val_loss did not improve from 27.97684
196/196 - 36s - loss: 27.2952 - MinusLogProbMetric: 27.2952 - val_loss: 27.9815 - val_MinusLogProbMetric: 27.9815 - lr: 1.5625e-05 - 36s/epoch - 184ms/step
Epoch 694/1000
2023-09-29 14:50:44.338 
Epoch 694/1000 
	 loss: 27.2962, MinusLogProbMetric: 27.2962, val_loss: 27.9875, val_MinusLogProbMetric: 27.9875

Epoch 694: val_loss did not improve from 27.97684
196/196 - 34s - loss: 27.2962 - MinusLogProbMetric: 27.2962 - val_loss: 27.9875 - val_MinusLogProbMetric: 27.9875 - lr: 1.5625e-05 - 34s/epoch - 173ms/step
Epoch 695/1000
2023-09-29 14:51:18.184 
Epoch 695/1000 
	 loss: 27.2958, MinusLogProbMetric: 27.2958, val_loss: 27.9840, val_MinusLogProbMetric: 27.9840

Epoch 695: val_loss did not improve from 27.97684
196/196 - 34s - loss: 27.2958 - MinusLogProbMetric: 27.2958 - val_loss: 27.9840 - val_MinusLogProbMetric: 27.9840 - lr: 1.5625e-05 - 34s/epoch - 173ms/step
Epoch 696/1000
2023-09-29 14:51:51.829 
Epoch 696/1000 
	 loss: 27.2955, MinusLogProbMetric: 27.2955, val_loss: 27.9813, val_MinusLogProbMetric: 27.9813

Epoch 696: val_loss did not improve from 27.97684
196/196 - 34s - loss: 27.2955 - MinusLogProbMetric: 27.2955 - val_loss: 27.9813 - val_MinusLogProbMetric: 27.9813 - lr: 1.5625e-05 - 34s/epoch - 172ms/step
Epoch 697/1000
2023-09-29 14:52:27.295 
Epoch 697/1000 
	 loss: 27.2955, MinusLogProbMetric: 27.2955, val_loss: 27.9851, val_MinusLogProbMetric: 27.9851

Epoch 697: val_loss did not improve from 27.97684
196/196 - 35s - loss: 27.2955 - MinusLogProbMetric: 27.2955 - val_loss: 27.9851 - val_MinusLogProbMetric: 27.9851 - lr: 1.5625e-05 - 35s/epoch - 181ms/step
Epoch 698/1000
2023-09-29 14:53:02.492 
Epoch 698/1000 
	 loss: 27.2955, MinusLogProbMetric: 27.2955, val_loss: 27.9814, val_MinusLogProbMetric: 27.9814

Epoch 698: val_loss did not improve from 27.97684
196/196 - 35s - loss: 27.2955 - MinusLogProbMetric: 27.2955 - val_loss: 27.9814 - val_MinusLogProbMetric: 27.9814 - lr: 1.5625e-05 - 35s/epoch - 180ms/step
Epoch 699/1000
2023-09-29 14:53:39.785 
Epoch 699/1000 
	 loss: 27.2954, MinusLogProbMetric: 27.2954, val_loss: 27.9779, val_MinusLogProbMetric: 27.9779

Epoch 699: val_loss did not improve from 27.97684
196/196 - 37s - loss: 27.2954 - MinusLogProbMetric: 27.2954 - val_loss: 27.9779 - val_MinusLogProbMetric: 27.9779 - lr: 1.5625e-05 - 37s/epoch - 190ms/step
Epoch 700/1000
2023-09-29 14:54:13.514 
Epoch 700/1000 
	 loss: 27.2951, MinusLogProbMetric: 27.2951, val_loss: 27.9817, val_MinusLogProbMetric: 27.9817

Epoch 700: val_loss did not improve from 27.97684
196/196 - 34s - loss: 27.2951 - MinusLogProbMetric: 27.2951 - val_loss: 27.9817 - val_MinusLogProbMetric: 27.9817 - lr: 1.5625e-05 - 34s/epoch - 172ms/step
Epoch 701/1000
2023-09-29 14:54:48.247 
Epoch 701/1000 
	 loss: 27.2949, MinusLogProbMetric: 27.2949, val_loss: 27.9815, val_MinusLogProbMetric: 27.9815

Epoch 701: val_loss did not improve from 27.97684
196/196 - 35s - loss: 27.2949 - MinusLogProbMetric: 27.2949 - val_loss: 27.9815 - val_MinusLogProbMetric: 27.9815 - lr: 1.5625e-05 - 35s/epoch - 177ms/step
Epoch 702/1000
2023-09-29 14:55:24.900 
Epoch 702/1000 
	 loss: 27.2951, MinusLogProbMetric: 27.2951, val_loss: 27.9811, val_MinusLogProbMetric: 27.9811

Epoch 702: val_loss did not improve from 27.97684
196/196 - 37s - loss: 27.2951 - MinusLogProbMetric: 27.2951 - val_loss: 27.9811 - val_MinusLogProbMetric: 27.9811 - lr: 1.5625e-05 - 37s/epoch - 187ms/step
Epoch 703/1000
2023-09-29 14:56:00.685 
Epoch 703/1000 
	 loss: 27.2955, MinusLogProbMetric: 27.2955, val_loss: 27.9765, val_MinusLogProbMetric: 27.9765

Epoch 703: val_loss improved from 27.97684 to 27.97648, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_332/weights/best_weights.h5
196/196 - 37s - loss: 27.2955 - MinusLogProbMetric: 27.2955 - val_loss: 27.9765 - val_MinusLogProbMetric: 27.9765 - lr: 1.5625e-05 - 37s/epoch - 187ms/step
Epoch 704/1000
2023-09-29 14:56:37.416 
Epoch 704/1000 
	 loss: 27.2946, MinusLogProbMetric: 27.2946, val_loss: 27.9783, val_MinusLogProbMetric: 27.9783

Epoch 704: val_loss did not improve from 27.97648
196/196 - 36s - loss: 27.2946 - MinusLogProbMetric: 27.2946 - val_loss: 27.9783 - val_MinusLogProbMetric: 27.9783 - lr: 1.5625e-05 - 36s/epoch - 183ms/step
Epoch 705/1000
2023-09-29 14:57:12.953 
Epoch 705/1000 
	 loss: 27.2953, MinusLogProbMetric: 27.2953, val_loss: 27.9765, val_MinusLogProbMetric: 27.9765

Epoch 705: val_loss improved from 27.97648 to 27.97646, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_332/weights/best_weights.h5
196/196 - 36s - loss: 27.2953 - MinusLogProbMetric: 27.2953 - val_loss: 27.9765 - val_MinusLogProbMetric: 27.9765 - lr: 1.5625e-05 - 36s/epoch - 184ms/step
Epoch 706/1000
2023-09-29 14:57:51.571 
Epoch 706/1000 
	 loss: 27.2955, MinusLogProbMetric: 27.2955, val_loss: 27.9838, val_MinusLogProbMetric: 27.9838

Epoch 706: val_loss did not improve from 27.97646
196/196 - 38s - loss: 27.2955 - MinusLogProbMetric: 27.2955 - val_loss: 27.9838 - val_MinusLogProbMetric: 27.9838 - lr: 1.5625e-05 - 38s/epoch - 194ms/step
Epoch 707/1000
2023-09-29 14:58:27.820 
Epoch 707/1000 
	 loss: 27.2949, MinusLogProbMetric: 27.2949, val_loss: 27.9807, val_MinusLogProbMetric: 27.9807

Epoch 707: val_loss did not improve from 27.97646
196/196 - 36s - loss: 27.2949 - MinusLogProbMetric: 27.2949 - val_loss: 27.9807 - val_MinusLogProbMetric: 27.9807 - lr: 1.5625e-05 - 36s/epoch - 185ms/step
Epoch 708/1000
2023-09-29 14:59:01.305 
Epoch 708/1000 
	 loss: 27.2943, MinusLogProbMetric: 27.2943, val_loss: 27.9817, val_MinusLogProbMetric: 27.9817

Epoch 708: val_loss did not improve from 27.97646
196/196 - 33s - loss: 27.2943 - MinusLogProbMetric: 27.2943 - val_loss: 27.9817 - val_MinusLogProbMetric: 27.9817 - lr: 1.5625e-05 - 33s/epoch - 171ms/step
Epoch 709/1000
2023-09-29 14:59:38.822 
Epoch 709/1000 
	 loss: 27.2947, MinusLogProbMetric: 27.2947, val_loss: 27.9788, val_MinusLogProbMetric: 27.9788

Epoch 709: val_loss did not improve from 27.97646
196/196 - 38s - loss: 27.2947 - MinusLogProbMetric: 27.2947 - val_loss: 27.9788 - val_MinusLogProbMetric: 27.9788 - lr: 1.5625e-05 - 38s/epoch - 191ms/step
Epoch 710/1000
2023-09-29 15:00:13.918 
Epoch 710/1000 
	 loss: 27.2946, MinusLogProbMetric: 27.2946, val_loss: 27.9796, val_MinusLogProbMetric: 27.9796

Epoch 710: val_loss did not improve from 27.97646
196/196 - 35s - loss: 27.2946 - MinusLogProbMetric: 27.2946 - val_loss: 27.9796 - val_MinusLogProbMetric: 27.9796 - lr: 1.5625e-05 - 35s/epoch - 179ms/step
Epoch 711/1000
2023-09-29 15:00:50.608 
Epoch 711/1000 
	 loss: 27.2948, MinusLogProbMetric: 27.2948, val_loss: 27.9818, val_MinusLogProbMetric: 27.9818

Epoch 711: val_loss did not improve from 27.97646
196/196 - 37s - loss: 27.2948 - MinusLogProbMetric: 27.2948 - val_loss: 27.9818 - val_MinusLogProbMetric: 27.9818 - lr: 1.5625e-05 - 37s/epoch - 187ms/step
Epoch 712/1000
2023-09-29 15:01:25.960 
Epoch 712/1000 
	 loss: 27.2947, MinusLogProbMetric: 27.2947, val_loss: 27.9801, val_MinusLogProbMetric: 27.9801

Epoch 712: val_loss did not improve from 27.97646
196/196 - 35s - loss: 27.2947 - MinusLogProbMetric: 27.2947 - val_loss: 27.9801 - val_MinusLogProbMetric: 27.9801 - lr: 1.5625e-05 - 35s/epoch - 180ms/step
Epoch 713/1000
2023-09-29 15:02:00.340 
Epoch 713/1000 
	 loss: 27.2950, MinusLogProbMetric: 27.2950, val_loss: 27.9877, val_MinusLogProbMetric: 27.9877

Epoch 713: val_loss did not improve from 27.97646
196/196 - 34s - loss: 27.2950 - MinusLogProbMetric: 27.2950 - val_loss: 27.9877 - val_MinusLogProbMetric: 27.9877 - lr: 1.5625e-05 - 34s/epoch - 175ms/step
Epoch 714/1000
2023-09-29 15:02:36.710 
Epoch 714/1000 
	 loss: 27.2948, MinusLogProbMetric: 27.2948, val_loss: 27.9814, val_MinusLogProbMetric: 27.9814

Epoch 714: val_loss did not improve from 27.97646
196/196 - 36s - loss: 27.2948 - MinusLogProbMetric: 27.2948 - val_loss: 27.9814 - val_MinusLogProbMetric: 27.9814 - lr: 1.5625e-05 - 36s/epoch - 186ms/step
Epoch 715/1000
2023-09-29 15:03:11.086 
Epoch 715/1000 
	 loss: 27.2946, MinusLogProbMetric: 27.2946, val_loss: 27.9799, val_MinusLogProbMetric: 27.9799

Epoch 715: val_loss did not improve from 27.97646
196/196 - 34s - loss: 27.2946 - MinusLogProbMetric: 27.2946 - val_loss: 27.9799 - val_MinusLogProbMetric: 27.9799 - lr: 1.5625e-05 - 34s/epoch - 175ms/step
Epoch 716/1000
2023-09-29 15:03:53.197 
Epoch 716/1000 
	 loss: 27.2959, MinusLogProbMetric: 27.2959, val_loss: 27.9816, val_MinusLogProbMetric: 27.9816

Epoch 716: val_loss did not improve from 27.97646
196/196 - 42s - loss: 27.2959 - MinusLogProbMetric: 27.2959 - val_loss: 27.9816 - val_MinusLogProbMetric: 27.9816 - lr: 1.5625e-05 - 42s/epoch - 215ms/step
Epoch 717/1000
2023-09-29 15:04:34.049 
Epoch 717/1000 
	 loss: 27.2942, MinusLogProbMetric: 27.2942, val_loss: 27.9841, val_MinusLogProbMetric: 27.9841

Epoch 717: val_loss did not improve from 27.97646
196/196 - 41s - loss: 27.2942 - MinusLogProbMetric: 27.2942 - val_loss: 27.9841 - val_MinusLogProbMetric: 27.9841 - lr: 1.5625e-05 - 41s/epoch - 208ms/step
Epoch 718/1000
2023-09-29 15:05:11.723 
Epoch 718/1000 
	 loss: 27.2958, MinusLogProbMetric: 27.2958, val_loss: 27.9810, val_MinusLogProbMetric: 27.9810

Epoch 718: val_loss did not improve from 27.97646
196/196 - 38s - loss: 27.2958 - MinusLogProbMetric: 27.2958 - val_loss: 27.9810 - val_MinusLogProbMetric: 27.9810 - lr: 1.5625e-05 - 38s/epoch - 192ms/step
Epoch 719/1000
2023-09-29 15:05:50.755 
Epoch 719/1000 
	 loss: 27.2949, MinusLogProbMetric: 27.2949, val_loss: 27.9797, val_MinusLogProbMetric: 27.9797

Epoch 719: val_loss did not improve from 27.97646
196/196 - 39s - loss: 27.2949 - MinusLogProbMetric: 27.2949 - val_loss: 27.9797 - val_MinusLogProbMetric: 27.9797 - lr: 1.5625e-05 - 39s/epoch - 199ms/step
Epoch 720/1000
2023-09-29 15:06:30.892 
Epoch 720/1000 
	 loss: 27.2944, MinusLogProbMetric: 27.2944, val_loss: 27.9796, val_MinusLogProbMetric: 27.9796

Epoch 720: val_loss did not improve from 27.97646
196/196 - 40s - loss: 27.2944 - MinusLogProbMetric: 27.2944 - val_loss: 27.9796 - val_MinusLogProbMetric: 27.9796 - lr: 1.5625e-05 - 40s/epoch - 205ms/step
Epoch 721/1000
2023-09-29 15:07:07.401 
Epoch 721/1000 
	 loss: 27.2960, MinusLogProbMetric: 27.2960, val_loss: 27.9838, val_MinusLogProbMetric: 27.9838

Epoch 721: val_loss did not improve from 27.97646
196/196 - 37s - loss: 27.2960 - MinusLogProbMetric: 27.2960 - val_loss: 27.9838 - val_MinusLogProbMetric: 27.9838 - lr: 1.5625e-05 - 37s/epoch - 186ms/step
Epoch 722/1000
2023-09-29 15:07:46.022 
Epoch 722/1000 
	 loss: 27.2944, MinusLogProbMetric: 27.2944, val_loss: 27.9855, val_MinusLogProbMetric: 27.9855

Epoch 722: val_loss did not improve from 27.97646
196/196 - 39s - loss: 27.2944 - MinusLogProbMetric: 27.2944 - val_loss: 27.9855 - val_MinusLogProbMetric: 27.9855 - lr: 1.5625e-05 - 39s/epoch - 197ms/step
Epoch 723/1000
2023-09-29 15:08:27.519 
Epoch 723/1000 
	 loss: 27.2939, MinusLogProbMetric: 27.2939, val_loss: 27.9784, val_MinusLogProbMetric: 27.9784

Epoch 723: val_loss did not improve from 27.97646
196/196 - 41s - loss: 27.2939 - MinusLogProbMetric: 27.2939 - val_loss: 27.9784 - val_MinusLogProbMetric: 27.9784 - lr: 1.5625e-05 - 41s/epoch - 212ms/step
Epoch 724/1000
2023-09-29 15:09:08.146 
Epoch 724/1000 
	 loss: 27.2948, MinusLogProbMetric: 27.2948, val_loss: 27.9792, val_MinusLogProbMetric: 27.9792

Epoch 724: val_loss did not improve from 27.97646
196/196 - 41s - loss: 27.2948 - MinusLogProbMetric: 27.2948 - val_loss: 27.9792 - val_MinusLogProbMetric: 27.9792 - lr: 1.5625e-05 - 41s/epoch - 207ms/step
Epoch 725/1000
2023-09-29 15:09:45.823 
Epoch 725/1000 
	 loss: 27.2948, MinusLogProbMetric: 27.2948, val_loss: 27.9814, val_MinusLogProbMetric: 27.9814

Epoch 725: val_loss did not improve from 27.97646
196/196 - 38s - loss: 27.2948 - MinusLogProbMetric: 27.2948 - val_loss: 27.9814 - val_MinusLogProbMetric: 27.9814 - lr: 1.5625e-05 - 38s/epoch - 192ms/step
Epoch 726/1000
2023-09-29 15:10:26.195 
Epoch 726/1000 
	 loss: 27.2945, MinusLogProbMetric: 27.2945, val_loss: 27.9794, val_MinusLogProbMetric: 27.9794

Epoch 726: val_loss did not improve from 27.97646
196/196 - 40s - loss: 27.2945 - MinusLogProbMetric: 27.2945 - val_loss: 27.9794 - val_MinusLogProbMetric: 27.9794 - lr: 1.5625e-05 - 40s/epoch - 206ms/step
Epoch 727/1000
2023-09-29 15:11:05.091 
Epoch 727/1000 
	 loss: 27.2945, MinusLogProbMetric: 27.2945, val_loss: 27.9813, val_MinusLogProbMetric: 27.9813

Epoch 727: val_loss did not improve from 27.97646
196/196 - 39s - loss: 27.2945 - MinusLogProbMetric: 27.2945 - val_loss: 27.9813 - val_MinusLogProbMetric: 27.9813 - lr: 1.5625e-05 - 39s/epoch - 198ms/step
Epoch 728/1000
2023-09-29 15:11:46.485 
Epoch 728/1000 
	 loss: 27.2943, MinusLogProbMetric: 27.2943, val_loss: 27.9839, val_MinusLogProbMetric: 27.9839

Epoch 728: val_loss did not improve from 27.97646
196/196 - 41s - loss: 27.2943 - MinusLogProbMetric: 27.2943 - val_loss: 27.9839 - val_MinusLogProbMetric: 27.9839 - lr: 1.5625e-05 - 41s/epoch - 211ms/step
Epoch 729/1000
2023-09-29 15:12:24.936 
Epoch 729/1000 
	 loss: 27.2946, MinusLogProbMetric: 27.2946, val_loss: 27.9805, val_MinusLogProbMetric: 27.9805

Epoch 729: val_loss did not improve from 27.97646
196/196 - 38s - loss: 27.2946 - MinusLogProbMetric: 27.2946 - val_loss: 27.9805 - val_MinusLogProbMetric: 27.9805 - lr: 1.5625e-05 - 38s/epoch - 196ms/step
Epoch 730/1000
2023-09-29 15:13:05.344 
Epoch 730/1000 
	 loss: 27.2941, MinusLogProbMetric: 27.2941, val_loss: 27.9834, val_MinusLogProbMetric: 27.9834

Epoch 730: val_loss did not improve from 27.97646
196/196 - 40s - loss: 27.2941 - MinusLogProbMetric: 27.2941 - val_loss: 27.9834 - val_MinusLogProbMetric: 27.9834 - lr: 1.5625e-05 - 40s/epoch - 206ms/step
Epoch 731/1000
2023-09-29 15:13:44.722 
Epoch 731/1000 
	 loss: 27.2944, MinusLogProbMetric: 27.2944, val_loss: 27.9806, val_MinusLogProbMetric: 27.9806

Epoch 731: val_loss did not improve from 27.97646
196/196 - 39s - loss: 27.2944 - MinusLogProbMetric: 27.2944 - val_loss: 27.9806 - val_MinusLogProbMetric: 27.9806 - lr: 1.5625e-05 - 39s/epoch - 201ms/step
Epoch 732/1000
2023-09-29 15:14:25.344 
Epoch 732/1000 
	 loss: 27.2949, MinusLogProbMetric: 27.2949, val_loss: 27.9817, val_MinusLogProbMetric: 27.9817

Epoch 732: val_loss did not improve from 27.97646
196/196 - 41s - loss: 27.2949 - MinusLogProbMetric: 27.2949 - val_loss: 27.9817 - val_MinusLogProbMetric: 27.9817 - lr: 1.5625e-05 - 41s/epoch - 207ms/step
Epoch 733/1000
2023-09-29 15:15:03.481 
Epoch 733/1000 
	 loss: 27.2944, MinusLogProbMetric: 27.2944, val_loss: 27.9811, val_MinusLogProbMetric: 27.9811

Epoch 733: val_loss did not improve from 27.97646
196/196 - 38s - loss: 27.2944 - MinusLogProbMetric: 27.2944 - val_loss: 27.9811 - val_MinusLogProbMetric: 27.9811 - lr: 1.5625e-05 - 38s/epoch - 195ms/step
Epoch 734/1000
2023-09-29 15:15:37.948 
Epoch 734/1000 
	 loss: 27.2937, MinusLogProbMetric: 27.2937, val_loss: 27.9797, val_MinusLogProbMetric: 27.9797

Epoch 734: val_loss did not improve from 27.97646
196/196 - 34s - loss: 27.2937 - MinusLogProbMetric: 27.2937 - val_loss: 27.9797 - val_MinusLogProbMetric: 27.9797 - lr: 1.5625e-05 - 34s/epoch - 176ms/step
Epoch 735/1000
2023-09-29 15:16:17.135 
Epoch 735/1000 
	 loss: 27.2950, MinusLogProbMetric: 27.2950, val_loss: 27.9815, val_MinusLogProbMetric: 27.9815

Epoch 735: val_loss did not improve from 27.97646
196/196 - 39s - loss: 27.2950 - MinusLogProbMetric: 27.2950 - val_loss: 27.9815 - val_MinusLogProbMetric: 27.9815 - lr: 1.5625e-05 - 39s/epoch - 200ms/step
Epoch 736/1000
2023-09-29 15:16:58.075 
Epoch 736/1000 
	 loss: 27.2958, MinusLogProbMetric: 27.2958, val_loss: 27.9791, val_MinusLogProbMetric: 27.9791

Epoch 736: val_loss did not improve from 27.97646
196/196 - 41s - loss: 27.2958 - MinusLogProbMetric: 27.2958 - val_loss: 27.9791 - val_MinusLogProbMetric: 27.9791 - lr: 1.5625e-05 - 41s/epoch - 209ms/step
Epoch 737/1000
2023-09-29 15:17:36.656 
Epoch 737/1000 
	 loss: 27.2945, MinusLogProbMetric: 27.2945, val_loss: 27.9762, val_MinusLogProbMetric: 27.9762

Epoch 737: val_loss improved from 27.97646 to 27.97618, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_332/weights/best_weights.h5
196/196 - 40s - loss: 27.2945 - MinusLogProbMetric: 27.2945 - val_loss: 27.9762 - val_MinusLogProbMetric: 27.9762 - lr: 1.5625e-05 - 40s/epoch - 205ms/step
Epoch 738/1000
2023-09-29 15:18:21.666 
Epoch 738/1000 
	 loss: 27.2950, MinusLogProbMetric: 27.2950, val_loss: 27.9826, val_MinusLogProbMetric: 27.9826

Epoch 738: val_loss did not improve from 27.97618
196/196 - 43s - loss: 27.2950 - MinusLogProbMetric: 27.2950 - val_loss: 27.9826 - val_MinusLogProbMetric: 27.9826 - lr: 1.5625e-05 - 43s/epoch - 222ms/step
Epoch 739/1000
2023-09-29 15:19:05.113 
Epoch 739/1000 
	 loss: 27.2935, MinusLogProbMetric: 27.2935, val_loss: 27.9840, val_MinusLogProbMetric: 27.9840

Epoch 739: val_loss did not improve from 27.97618
196/196 - 43s - loss: 27.2935 - MinusLogProbMetric: 27.2935 - val_loss: 27.9840 - val_MinusLogProbMetric: 27.9840 - lr: 1.5625e-05 - 43s/epoch - 222ms/step
Epoch 740/1000
2023-09-29 15:19:48.129 
Epoch 740/1000 
	 loss: 27.2947, MinusLogProbMetric: 27.2947, val_loss: 27.9849, val_MinusLogProbMetric: 27.9849

Epoch 740: val_loss did not improve from 27.97618
196/196 - 43s - loss: 27.2947 - MinusLogProbMetric: 27.2947 - val_loss: 27.9849 - val_MinusLogProbMetric: 27.9849 - lr: 1.5625e-05 - 43s/epoch - 219ms/step
Epoch 741/1000
2023-09-29 15:20:31.928 
Epoch 741/1000 
	 loss: 27.2951, MinusLogProbMetric: 27.2951, val_loss: 27.9779, val_MinusLogProbMetric: 27.9779

Epoch 741: val_loss did not improve from 27.97618
196/196 - 44s - loss: 27.2951 - MinusLogProbMetric: 27.2951 - val_loss: 27.9779 - val_MinusLogProbMetric: 27.9779 - lr: 1.5625e-05 - 44s/epoch - 223ms/step
Epoch 742/1000
2023-09-29 15:21:15.380 
Epoch 742/1000 
	 loss: 27.2941, MinusLogProbMetric: 27.2941, val_loss: 27.9829, val_MinusLogProbMetric: 27.9829

Epoch 742: val_loss did not improve from 27.97618
196/196 - 43s - loss: 27.2941 - MinusLogProbMetric: 27.2941 - val_loss: 27.9829 - val_MinusLogProbMetric: 27.9829 - lr: 1.5625e-05 - 43s/epoch - 222ms/step
Epoch 743/1000
2023-09-29 15:21:58.473 
Epoch 743/1000 
	 loss: 27.2942, MinusLogProbMetric: 27.2942, val_loss: 27.9819, val_MinusLogProbMetric: 27.9819

Epoch 743: val_loss did not improve from 27.97618
196/196 - 43s - loss: 27.2942 - MinusLogProbMetric: 27.2942 - val_loss: 27.9819 - val_MinusLogProbMetric: 27.9819 - lr: 1.5625e-05 - 43s/epoch - 220ms/step
Epoch 744/1000
2023-09-29 15:22:41.621 
Epoch 744/1000 
	 loss: 27.2938, MinusLogProbMetric: 27.2938, val_loss: 27.9835, val_MinusLogProbMetric: 27.9835

Epoch 744: val_loss did not improve from 27.97618
196/196 - 43s - loss: 27.2938 - MinusLogProbMetric: 27.2938 - val_loss: 27.9835 - val_MinusLogProbMetric: 27.9835 - lr: 1.5625e-05 - 43s/epoch - 220ms/step
Epoch 745/1000
2023-09-29 15:23:23.569 
Epoch 745/1000 
	 loss: 27.2934, MinusLogProbMetric: 27.2934, val_loss: 27.9778, val_MinusLogProbMetric: 27.9778

Epoch 745: val_loss did not improve from 27.97618
196/196 - 42s - loss: 27.2934 - MinusLogProbMetric: 27.2934 - val_loss: 27.9778 - val_MinusLogProbMetric: 27.9778 - lr: 1.5625e-05 - 42s/epoch - 214ms/step
Epoch 746/1000
2023-09-29 15:24:07.372 
Epoch 746/1000 
	 loss: 27.2933, MinusLogProbMetric: 27.2933, val_loss: 27.9800, val_MinusLogProbMetric: 27.9800

Epoch 746: val_loss did not improve from 27.97618
196/196 - 44s - loss: 27.2933 - MinusLogProbMetric: 27.2933 - val_loss: 27.9800 - val_MinusLogProbMetric: 27.9800 - lr: 1.5625e-05 - 44s/epoch - 223ms/step
Epoch 747/1000
2023-09-29 15:24:50.901 
Epoch 747/1000 
	 loss: 27.2936, MinusLogProbMetric: 27.2936, val_loss: 27.9835, val_MinusLogProbMetric: 27.9835

Epoch 747: val_loss did not improve from 27.97618
196/196 - 44s - loss: 27.2936 - MinusLogProbMetric: 27.2936 - val_loss: 27.9835 - val_MinusLogProbMetric: 27.9835 - lr: 1.5625e-05 - 44s/epoch - 222ms/step
Epoch 748/1000
2023-09-29 15:25:34.547 
Epoch 748/1000 
	 loss: 27.2930, MinusLogProbMetric: 27.2930, val_loss: 27.9815, val_MinusLogProbMetric: 27.9815

Epoch 748: val_loss did not improve from 27.97618
196/196 - 44s - loss: 27.2930 - MinusLogProbMetric: 27.2930 - val_loss: 27.9815 - val_MinusLogProbMetric: 27.9815 - lr: 1.5625e-05 - 44s/epoch - 223ms/step
Epoch 749/1000
2023-09-29 15:26:17.673 
Epoch 749/1000 
	 loss: 27.2938, MinusLogProbMetric: 27.2938, val_loss: 27.9826, val_MinusLogProbMetric: 27.9826

Epoch 749: val_loss did not improve from 27.97618
196/196 - 43s - loss: 27.2938 - MinusLogProbMetric: 27.2938 - val_loss: 27.9826 - val_MinusLogProbMetric: 27.9826 - lr: 1.5625e-05 - 43s/epoch - 220ms/step
Epoch 750/1000
2023-09-29 15:27:01.039 
Epoch 750/1000 
	 loss: 27.2929, MinusLogProbMetric: 27.2929, val_loss: 27.9850, val_MinusLogProbMetric: 27.9850

Epoch 750: val_loss did not improve from 27.97618
196/196 - 43s - loss: 27.2929 - MinusLogProbMetric: 27.2929 - val_loss: 27.9850 - val_MinusLogProbMetric: 27.9850 - lr: 1.5625e-05 - 43s/epoch - 221ms/step
Epoch 751/1000
2023-09-29 15:27:44.606 
Epoch 751/1000 
	 loss: 27.2943, MinusLogProbMetric: 27.2943, val_loss: 27.9814, val_MinusLogProbMetric: 27.9814

Epoch 751: val_loss did not improve from 27.97618
196/196 - 44s - loss: 27.2943 - MinusLogProbMetric: 27.2943 - val_loss: 27.9814 - val_MinusLogProbMetric: 27.9814 - lr: 1.5625e-05 - 44s/epoch - 222ms/step
Epoch 752/1000
2023-09-29 15:28:27.904 
Epoch 752/1000 
	 loss: 27.2935, MinusLogProbMetric: 27.2935, val_loss: 27.9799, val_MinusLogProbMetric: 27.9799

Epoch 752: val_loss did not improve from 27.97618
196/196 - 43s - loss: 27.2935 - MinusLogProbMetric: 27.2935 - val_loss: 27.9799 - val_MinusLogProbMetric: 27.9799 - lr: 1.5625e-05 - 43s/epoch - 221ms/step
Epoch 753/1000
2023-09-29 15:29:11.246 
Epoch 753/1000 
	 loss: 27.2932, MinusLogProbMetric: 27.2932, val_loss: 27.9807, val_MinusLogProbMetric: 27.9807

Epoch 753: val_loss did not improve from 27.97618
196/196 - 43s - loss: 27.2932 - MinusLogProbMetric: 27.2932 - val_loss: 27.9807 - val_MinusLogProbMetric: 27.9807 - lr: 1.5625e-05 - 43s/epoch - 221ms/step
Epoch 754/1000
2023-09-29 15:29:54.519 
Epoch 754/1000 
	 loss: 27.2936, MinusLogProbMetric: 27.2936, val_loss: 27.9812, val_MinusLogProbMetric: 27.9812

Epoch 754: val_loss did not improve from 27.97618
196/196 - 43s - loss: 27.2936 - MinusLogProbMetric: 27.2936 - val_loss: 27.9812 - val_MinusLogProbMetric: 27.9812 - lr: 1.5625e-05 - 43s/epoch - 221ms/step
Epoch 755/1000
2023-09-29 15:30:38.044 
Epoch 755/1000 
	 loss: 27.2939, MinusLogProbMetric: 27.2939, val_loss: 27.9826, val_MinusLogProbMetric: 27.9826

Epoch 755: val_loss did not improve from 27.97618
196/196 - 44s - loss: 27.2939 - MinusLogProbMetric: 27.2939 - val_loss: 27.9826 - val_MinusLogProbMetric: 27.9826 - lr: 1.5625e-05 - 44s/epoch - 222ms/step
Epoch 756/1000
2023-09-29 15:31:21.339 
Epoch 756/1000 
	 loss: 27.2935, MinusLogProbMetric: 27.2935, val_loss: 27.9817, val_MinusLogProbMetric: 27.9817

Epoch 756: val_loss did not improve from 27.97618
196/196 - 43s - loss: 27.2935 - MinusLogProbMetric: 27.2935 - val_loss: 27.9817 - val_MinusLogProbMetric: 27.9817 - lr: 1.5625e-05 - 43s/epoch - 221ms/step
Epoch 757/1000
2023-09-29 15:32:04.383 
Epoch 757/1000 
	 loss: 27.2935, MinusLogProbMetric: 27.2935, val_loss: 27.9799, val_MinusLogProbMetric: 27.9799

Epoch 757: val_loss did not improve from 27.97618
196/196 - 43s - loss: 27.2935 - MinusLogProbMetric: 27.2935 - val_loss: 27.9799 - val_MinusLogProbMetric: 27.9799 - lr: 1.5625e-05 - 43s/epoch - 220ms/step
Epoch 758/1000
2023-09-29 15:32:47.420 
Epoch 758/1000 
	 loss: 27.2935, MinusLogProbMetric: 27.2935, val_loss: 27.9845, val_MinusLogProbMetric: 27.9845

Epoch 758: val_loss did not improve from 27.97618
196/196 - 43s - loss: 27.2935 - MinusLogProbMetric: 27.2935 - val_loss: 27.9845 - val_MinusLogProbMetric: 27.9845 - lr: 1.5625e-05 - 43s/epoch - 220ms/step
Epoch 759/1000
2023-09-29 15:33:29.829 
Epoch 759/1000 
	 loss: 27.2929, MinusLogProbMetric: 27.2929, val_loss: 27.9866, val_MinusLogProbMetric: 27.9866

Epoch 759: val_loss did not improve from 27.97618
196/196 - 42s - loss: 27.2929 - MinusLogProbMetric: 27.2929 - val_loss: 27.9866 - val_MinusLogProbMetric: 27.9866 - lr: 1.5625e-05 - 42s/epoch - 216ms/step
Epoch 760/1000
2023-09-29 15:34:12.302 
Epoch 760/1000 
	 loss: 27.2930, MinusLogProbMetric: 27.2930, val_loss: 27.9832, val_MinusLogProbMetric: 27.9832

Epoch 760: val_loss did not improve from 27.97618
196/196 - 42s - loss: 27.2930 - MinusLogProbMetric: 27.2930 - val_loss: 27.9832 - val_MinusLogProbMetric: 27.9832 - lr: 1.5625e-05 - 42s/epoch - 217ms/step
Epoch 761/1000
2023-09-29 15:34:57.254 
Epoch 761/1000 
	 loss: 27.2940, MinusLogProbMetric: 27.2940, val_loss: 27.9850, val_MinusLogProbMetric: 27.9850

Epoch 761: val_loss did not improve from 27.97618
196/196 - 45s - loss: 27.2940 - MinusLogProbMetric: 27.2940 - val_loss: 27.9850 - val_MinusLogProbMetric: 27.9850 - lr: 1.5625e-05 - 45s/epoch - 229ms/step
Epoch 762/1000
2023-09-29 15:35:42.414 
Epoch 762/1000 
	 loss: 27.2937, MinusLogProbMetric: 27.2937, val_loss: 27.9837, val_MinusLogProbMetric: 27.9837

Epoch 762: val_loss did not improve from 27.97618
196/196 - 45s - loss: 27.2937 - MinusLogProbMetric: 27.2937 - val_loss: 27.9837 - val_MinusLogProbMetric: 27.9837 - lr: 1.5625e-05 - 45s/epoch - 230ms/step
Epoch 763/1000
2023-09-29 15:36:27.988 
Epoch 763/1000 
	 loss: 27.2938, MinusLogProbMetric: 27.2938, val_loss: 27.9815, val_MinusLogProbMetric: 27.9815

Epoch 763: val_loss did not improve from 27.97618
196/196 - 46s - loss: 27.2938 - MinusLogProbMetric: 27.2938 - val_loss: 27.9815 - val_MinusLogProbMetric: 27.9815 - lr: 1.5625e-05 - 46s/epoch - 232ms/step
Epoch 764/1000
2023-09-29 15:37:13.732 
Epoch 764/1000 
	 loss: 27.2933, MinusLogProbMetric: 27.2933, val_loss: 27.9797, val_MinusLogProbMetric: 27.9797

Epoch 764: val_loss did not improve from 27.97618
196/196 - 46s - loss: 27.2933 - MinusLogProbMetric: 27.2933 - val_loss: 27.9797 - val_MinusLogProbMetric: 27.9797 - lr: 1.5625e-05 - 46s/epoch - 233ms/step
Epoch 765/1000
2023-09-29 15:37:58.833 
Epoch 765/1000 
	 loss: 27.2919, MinusLogProbMetric: 27.2919, val_loss: 27.9781, val_MinusLogProbMetric: 27.9781

Epoch 765: val_loss did not improve from 27.97618
196/196 - 45s - loss: 27.2919 - MinusLogProbMetric: 27.2919 - val_loss: 27.9781 - val_MinusLogProbMetric: 27.9781 - lr: 1.5625e-05 - 45s/epoch - 230ms/step
Epoch 766/1000
2023-09-29 15:38:44.174 
Epoch 766/1000 
	 loss: 27.2930, MinusLogProbMetric: 27.2930, val_loss: 27.9907, val_MinusLogProbMetric: 27.9907

Epoch 766: val_loss did not improve from 27.97618
196/196 - 45s - loss: 27.2930 - MinusLogProbMetric: 27.2930 - val_loss: 27.9907 - val_MinusLogProbMetric: 27.9907 - lr: 1.5625e-05 - 45s/epoch - 231ms/step
Epoch 767/1000
2023-09-29 15:39:29.928 
Epoch 767/1000 
	 loss: 27.2936, MinusLogProbMetric: 27.2936, val_loss: 27.9829, val_MinusLogProbMetric: 27.9829

Epoch 767: val_loss did not improve from 27.97618
196/196 - 46s - loss: 27.2936 - MinusLogProbMetric: 27.2936 - val_loss: 27.9829 - val_MinusLogProbMetric: 27.9829 - lr: 1.5625e-05 - 46s/epoch - 233ms/step
Epoch 768/1000
2023-09-29 15:40:15.513 
Epoch 768/1000 
	 loss: 27.2930, MinusLogProbMetric: 27.2930, val_loss: 27.9845, val_MinusLogProbMetric: 27.9845

Epoch 768: val_loss did not improve from 27.97618
196/196 - 46s - loss: 27.2930 - MinusLogProbMetric: 27.2930 - val_loss: 27.9845 - val_MinusLogProbMetric: 27.9845 - lr: 1.5625e-05 - 46s/epoch - 233ms/step
Epoch 769/1000
2023-09-29 15:41:01.101 
Epoch 769/1000 
	 loss: 27.2936, MinusLogProbMetric: 27.2936, val_loss: 27.9785, val_MinusLogProbMetric: 27.9785

Epoch 769: val_loss did not improve from 27.97618
196/196 - 46s - loss: 27.2936 - MinusLogProbMetric: 27.2936 - val_loss: 27.9785 - val_MinusLogProbMetric: 27.9785 - lr: 1.5625e-05 - 46s/epoch - 233ms/step
Epoch 770/1000
2023-09-29 15:41:46.608 
Epoch 770/1000 
	 loss: 27.2931, MinusLogProbMetric: 27.2931, val_loss: 27.9812, val_MinusLogProbMetric: 27.9812

Epoch 770: val_loss did not improve from 27.97618
196/196 - 46s - loss: 27.2931 - MinusLogProbMetric: 27.2931 - val_loss: 27.9812 - val_MinusLogProbMetric: 27.9812 - lr: 1.5625e-05 - 46s/epoch - 232ms/step
Epoch 771/1000
2023-09-29 15:42:32.045 
Epoch 771/1000 
	 loss: 27.2932, MinusLogProbMetric: 27.2932, val_loss: 27.9780, val_MinusLogProbMetric: 27.9780

Epoch 771: val_loss did not improve from 27.97618
196/196 - 45s - loss: 27.2932 - MinusLogProbMetric: 27.2932 - val_loss: 27.9780 - val_MinusLogProbMetric: 27.9780 - lr: 1.5625e-05 - 45s/epoch - 232ms/step
Epoch 772/1000
2023-09-29 15:43:17.741 
Epoch 772/1000 
	 loss: 27.2925, MinusLogProbMetric: 27.2925, val_loss: 27.9809, val_MinusLogProbMetric: 27.9809

Epoch 772: val_loss did not improve from 27.97618
196/196 - 46s - loss: 27.2925 - MinusLogProbMetric: 27.2925 - val_loss: 27.9809 - val_MinusLogProbMetric: 27.9809 - lr: 1.5625e-05 - 46s/epoch - 233ms/step
Epoch 773/1000
2023-09-29 15:44:03.137 
Epoch 773/1000 
	 loss: 27.2930, MinusLogProbMetric: 27.2930, val_loss: 27.9819, val_MinusLogProbMetric: 27.9819

Epoch 773: val_loss did not improve from 27.97618
196/196 - 45s - loss: 27.2930 - MinusLogProbMetric: 27.2930 - val_loss: 27.9819 - val_MinusLogProbMetric: 27.9819 - lr: 1.5625e-05 - 45s/epoch - 232ms/step
Epoch 774/1000
2023-09-29 15:44:48.162 
Epoch 774/1000 
	 loss: 27.2921, MinusLogProbMetric: 27.2921, val_loss: 27.9818, val_MinusLogProbMetric: 27.9818

Epoch 774: val_loss did not improve from 27.97618
196/196 - 45s - loss: 27.2921 - MinusLogProbMetric: 27.2921 - val_loss: 27.9818 - val_MinusLogProbMetric: 27.9818 - lr: 1.5625e-05 - 45s/epoch - 230ms/step
Epoch 775/1000
2023-09-29 15:45:33.934 
Epoch 775/1000 
	 loss: 27.2921, MinusLogProbMetric: 27.2921, val_loss: 27.9818, val_MinusLogProbMetric: 27.9818

Epoch 775: val_loss did not improve from 27.97618
196/196 - 46s - loss: 27.2921 - MinusLogProbMetric: 27.2921 - val_loss: 27.9818 - val_MinusLogProbMetric: 27.9818 - lr: 1.5625e-05 - 46s/epoch - 234ms/step
Epoch 776/1000
2023-09-29 15:46:19.546 
Epoch 776/1000 
	 loss: 27.2924, MinusLogProbMetric: 27.2924, val_loss: 27.9808, val_MinusLogProbMetric: 27.9808

Epoch 776: val_loss did not improve from 27.97618
196/196 - 46s - loss: 27.2924 - MinusLogProbMetric: 27.2924 - val_loss: 27.9808 - val_MinusLogProbMetric: 27.9808 - lr: 1.5625e-05 - 46s/epoch - 233ms/step
Epoch 777/1000
2023-09-29 15:47:04.733 
Epoch 777/1000 
	 loss: 27.2915, MinusLogProbMetric: 27.2915, val_loss: 27.9826, val_MinusLogProbMetric: 27.9826

Epoch 777: val_loss did not improve from 27.97618
196/196 - 45s - loss: 27.2915 - MinusLogProbMetric: 27.2915 - val_loss: 27.9826 - val_MinusLogProbMetric: 27.9826 - lr: 1.5625e-05 - 45s/epoch - 231ms/step
Epoch 778/1000
2023-09-29 15:47:49.863 
Epoch 778/1000 
	 loss: 27.2929, MinusLogProbMetric: 27.2929, val_loss: 27.9797, val_MinusLogProbMetric: 27.9797

Epoch 778: val_loss did not improve from 27.97618
196/196 - 45s - loss: 27.2929 - MinusLogProbMetric: 27.2929 - val_loss: 27.9797 - val_MinusLogProbMetric: 27.9797 - lr: 1.5625e-05 - 45s/epoch - 230ms/step
Epoch 779/1000
2023-09-29 15:48:34.482 
Epoch 779/1000 
	 loss: 27.2919, MinusLogProbMetric: 27.2919, val_loss: 27.9828, val_MinusLogProbMetric: 27.9828

Epoch 779: val_loss did not improve from 27.97618
196/196 - 45s - loss: 27.2919 - MinusLogProbMetric: 27.2919 - val_loss: 27.9828 - val_MinusLogProbMetric: 27.9828 - lr: 1.5625e-05 - 45s/epoch - 228ms/step
Epoch 780/1000
2023-09-29 15:49:19.919 
Epoch 780/1000 
	 loss: 27.2927, MinusLogProbMetric: 27.2927, val_loss: 27.9786, val_MinusLogProbMetric: 27.9786

Epoch 780: val_loss did not improve from 27.97618
196/196 - 45s - loss: 27.2927 - MinusLogProbMetric: 27.2927 - val_loss: 27.9786 - val_MinusLogProbMetric: 27.9786 - lr: 1.5625e-05 - 45s/epoch - 232ms/step
Epoch 781/1000
2023-09-29 15:50:05.177 
Epoch 781/1000 
	 loss: 27.2916, MinusLogProbMetric: 27.2916, val_loss: 27.9794, val_MinusLogProbMetric: 27.9794

Epoch 781: val_loss did not improve from 27.97618
196/196 - 45s - loss: 27.2916 - MinusLogProbMetric: 27.2916 - val_loss: 27.9794 - val_MinusLogProbMetric: 27.9794 - lr: 1.5625e-05 - 45s/epoch - 231ms/step
Epoch 782/1000
2023-09-29 15:50:51.159 
Epoch 782/1000 
	 loss: 27.2926, MinusLogProbMetric: 27.2926, val_loss: 27.9804, val_MinusLogProbMetric: 27.9804

Epoch 782: val_loss did not improve from 27.97618
196/196 - 46s - loss: 27.2926 - MinusLogProbMetric: 27.2926 - val_loss: 27.9804 - val_MinusLogProbMetric: 27.9804 - lr: 1.5625e-05 - 46s/epoch - 235ms/step
Epoch 783/1000
2023-09-29 15:51:36.615 
Epoch 783/1000 
	 loss: 27.2912, MinusLogProbMetric: 27.2912, val_loss: 27.9794, val_MinusLogProbMetric: 27.9794

Epoch 783: val_loss did not improve from 27.97618
196/196 - 45s - loss: 27.2912 - MinusLogProbMetric: 27.2912 - val_loss: 27.9794 - val_MinusLogProbMetric: 27.9794 - lr: 1.5625e-05 - 45s/epoch - 232ms/step
Epoch 784/1000
2023-09-29 15:52:22.075 
Epoch 784/1000 
	 loss: 27.2922, MinusLogProbMetric: 27.2922, val_loss: 27.9802, val_MinusLogProbMetric: 27.9802

Epoch 784: val_loss did not improve from 27.97618
196/196 - 45s - loss: 27.2922 - MinusLogProbMetric: 27.2922 - val_loss: 27.9802 - val_MinusLogProbMetric: 27.9802 - lr: 1.5625e-05 - 45s/epoch - 232ms/step
Epoch 785/1000
2023-09-29 15:53:07.230 
Epoch 785/1000 
	 loss: 27.2925, MinusLogProbMetric: 27.2925, val_loss: 27.9790, val_MinusLogProbMetric: 27.9790

Epoch 785: val_loss did not improve from 27.97618
196/196 - 45s - loss: 27.2925 - MinusLogProbMetric: 27.2925 - val_loss: 27.9790 - val_MinusLogProbMetric: 27.9790 - lr: 1.5625e-05 - 45s/epoch - 230ms/step
Epoch 786/1000
2023-09-29 15:53:52.458 
Epoch 786/1000 
	 loss: 27.2920, MinusLogProbMetric: 27.2920, val_loss: 27.9808, val_MinusLogProbMetric: 27.9808

Epoch 786: val_loss did not improve from 27.97618
196/196 - 45s - loss: 27.2920 - MinusLogProbMetric: 27.2920 - val_loss: 27.9808 - val_MinusLogProbMetric: 27.9808 - lr: 1.5625e-05 - 45s/epoch - 231ms/step
Epoch 787/1000
2023-09-29 15:54:37.794 
Epoch 787/1000 
	 loss: 27.2919, MinusLogProbMetric: 27.2919, val_loss: 27.9847, val_MinusLogProbMetric: 27.9847

Epoch 787: val_loss did not improve from 27.97618
196/196 - 45s - loss: 27.2919 - MinusLogProbMetric: 27.2919 - val_loss: 27.9847 - val_MinusLogProbMetric: 27.9847 - lr: 1.5625e-05 - 45s/epoch - 231ms/step
Epoch 788/1000
2023-09-29 15:55:23.320 
Epoch 788/1000 
	 loss: 27.2877, MinusLogProbMetric: 27.2877, val_loss: 27.9787, val_MinusLogProbMetric: 27.9787

Epoch 788: val_loss did not improve from 27.97618
196/196 - 46s - loss: 27.2877 - MinusLogProbMetric: 27.2877 - val_loss: 27.9787 - val_MinusLogProbMetric: 27.9787 - lr: 7.8125e-06 - 46s/epoch - 232ms/step
Epoch 789/1000
2023-09-29 15:56:08.669 
Epoch 789/1000 
	 loss: 27.2872, MinusLogProbMetric: 27.2872, val_loss: 27.9776, val_MinusLogProbMetric: 27.9776

Epoch 789: val_loss did not improve from 27.97618
196/196 - 45s - loss: 27.2872 - MinusLogProbMetric: 27.2872 - val_loss: 27.9776 - val_MinusLogProbMetric: 27.9776 - lr: 7.8125e-06 - 45s/epoch - 231ms/step
Epoch 790/1000
2023-09-29 15:56:54.011 
Epoch 790/1000 
	 loss: 27.2873, MinusLogProbMetric: 27.2873, val_loss: 27.9801, val_MinusLogProbMetric: 27.9801

Epoch 790: val_loss did not improve from 27.97618
196/196 - 45s - loss: 27.2873 - MinusLogProbMetric: 27.2873 - val_loss: 27.9801 - val_MinusLogProbMetric: 27.9801 - lr: 7.8125e-06 - 45s/epoch - 231ms/step
Epoch 791/1000
2023-09-29 15:57:38.169 
Epoch 791/1000 
	 loss: 27.2870, MinusLogProbMetric: 27.2870, val_loss: 27.9755, val_MinusLogProbMetric: 27.9755

Epoch 791: val_loss improved from 27.97618 to 27.97553, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_332/weights/best_weights.h5
196/196 - 45s - loss: 27.2870 - MinusLogProbMetric: 27.2870 - val_loss: 27.9755 - val_MinusLogProbMetric: 27.9755 - lr: 7.8125e-06 - 45s/epoch - 230ms/step
Epoch 792/1000
2023-09-29 15:58:24.005 
Epoch 792/1000 
	 loss: 27.2871, MinusLogProbMetric: 27.2871, val_loss: 27.9796, val_MinusLogProbMetric: 27.9796

Epoch 792: val_loss did not improve from 27.97553
196/196 - 45s - loss: 27.2871 - MinusLogProbMetric: 27.2871 - val_loss: 27.9796 - val_MinusLogProbMetric: 27.9796 - lr: 7.8125e-06 - 45s/epoch - 229ms/step
Epoch 793/1000
2023-09-29 15:59:08.730 
Epoch 793/1000 
	 loss: 27.2878, MinusLogProbMetric: 27.2878, val_loss: 27.9806, val_MinusLogProbMetric: 27.9806

Epoch 793: val_loss did not improve from 27.97553
196/196 - 45s - loss: 27.2878 - MinusLogProbMetric: 27.2878 - val_loss: 27.9806 - val_MinusLogProbMetric: 27.9806 - lr: 7.8125e-06 - 45s/epoch - 228ms/step
Epoch 794/1000
2023-09-29 15:59:53.801 
Epoch 794/1000 
	 loss: 27.2868, MinusLogProbMetric: 27.2868, val_loss: 27.9783, val_MinusLogProbMetric: 27.9783

Epoch 794: val_loss did not improve from 27.97553
196/196 - 45s - loss: 27.2868 - MinusLogProbMetric: 27.2868 - val_loss: 27.9783 - val_MinusLogProbMetric: 27.9783 - lr: 7.8125e-06 - 45s/epoch - 230ms/step
Epoch 795/1000
2023-09-29 16:00:38.848 
Epoch 795/1000 
	 loss: 27.2872, MinusLogProbMetric: 27.2872, val_loss: 27.9777, val_MinusLogProbMetric: 27.9777

Epoch 795: val_loss did not improve from 27.97553
196/196 - 45s - loss: 27.2872 - MinusLogProbMetric: 27.2872 - val_loss: 27.9777 - val_MinusLogProbMetric: 27.9777 - lr: 7.8125e-06 - 45s/epoch - 230ms/step
Epoch 796/1000
2023-09-29 16:01:24.281 
Epoch 796/1000 
	 loss: 27.2863, MinusLogProbMetric: 27.2863, val_loss: 27.9774, val_MinusLogProbMetric: 27.9774

Epoch 796: val_loss did not improve from 27.97553
196/196 - 45s - loss: 27.2863 - MinusLogProbMetric: 27.2863 - val_loss: 27.9774 - val_MinusLogProbMetric: 27.9774 - lr: 7.8125e-06 - 45s/epoch - 232ms/step
Epoch 797/1000
2023-09-29 16:02:08.555 
Epoch 797/1000 
	 loss: 27.2869, MinusLogProbMetric: 27.2869, val_loss: 27.9783, val_MinusLogProbMetric: 27.9783

Epoch 797: val_loss did not improve from 27.97553
196/196 - 44s - loss: 27.2869 - MinusLogProbMetric: 27.2869 - val_loss: 27.9783 - val_MinusLogProbMetric: 27.9783 - lr: 7.8125e-06 - 44s/epoch - 226ms/step
Epoch 798/1000
2023-09-29 16:02:53.409 
Epoch 798/1000 
	 loss: 27.2871, MinusLogProbMetric: 27.2871, val_loss: 27.9792, val_MinusLogProbMetric: 27.9792

Epoch 798: val_loss did not improve from 27.97553
196/196 - 45s - loss: 27.2871 - MinusLogProbMetric: 27.2871 - val_loss: 27.9792 - val_MinusLogProbMetric: 27.9792 - lr: 7.8125e-06 - 45s/epoch - 229ms/step
Epoch 799/1000
2023-09-29 16:03:38.204 
Epoch 799/1000 
	 loss: 27.2865, MinusLogProbMetric: 27.2865, val_loss: 27.9778, val_MinusLogProbMetric: 27.9778

Epoch 799: val_loss did not improve from 27.97553
196/196 - 45s - loss: 27.2865 - MinusLogProbMetric: 27.2865 - val_loss: 27.9778 - val_MinusLogProbMetric: 27.9778 - lr: 7.8125e-06 - 45s/epoch - 229ms/step
Epoch 800/1000
2023-09-29 16:04:23.714 
Epoch 800/1000 
	 loss: 27.2873, MinusLogProbMetric: 27.2873, val_loss: 27.9811, val_MinusLogProbMetric: 27.9811

Epoch 800: val_loss did not improve from 27.97553
196/196 - 46s - loss: 27.2873 - MinusLogProbMetric: 27.2873 - val_loss: 27.9811 - val_MinusLogProbMetric: 27.9811 - lr: 7.8125e-06 - 46s/epoch - 232ms/step
Epoch 801/1000
2023-09-29 16:05:08.741 
Epoch 801/1000 
	 loss: 27.2869, MinusLogProbMetric: 27.2869, val_loss: 27.9795, val_MinusLogProbMetric: 27.9795

Epoch 801: val_loss did not improve from 27.97553
196/196 - 45s - loss: 27.2869 - MinusLogProbMetric: 27.2869 - val_loss: 27.9795 - val_MinusLogProbMetric: 27.9795 - lr: 7.8125e-06 - 45s/epoch - 230ms/step
Epoch 802/1000
2023-09-29 16:05:53.986 
Epoch 802/1000 
	 loss: 27.2865, MinusLogProbMetric: 27.2865, val_loss: 27.9808, val_MinusLogProbMetric: 27.9808

Epoch 802: val_loss did not improve from 27.97553
196/196 - 45s - loss: 27.2865 - MinusLogProbMetric: 27.2865 - val_loss: 27.9808 - val_MinusLogProbMetric: 27.9808 - lr: 7.8125e-06 - 45s/epoch - 231ms/step
Epoch 803/1000
2023-09-29 16:06:38.918 
Epoch 803/1000 
	 loss: 27.2865, MinusLogProbMetric: 27.2865, val_loss: 27.9777, val_MinusLogProbMetric: 27.9777

Epoch 803: val_loss did not improve from 27.97553
196/196 - 45s - loss: 27.2865 - MinusLogProbMetric: 27.2865 - val_loss: 27.9777 - val_MinusLogProbMetric: 27.9777 - lr: 7.8125e-06 - 45s/epoch - 229ms/step
Epoch 804/1000
2023-09-29 16:07:24.231 
Epoch 804/1000 
	 loss: 27.2870, MinusLogProbMetric: 27.2870, val_loss: 27.9783, val_MinusLogProbMetric: 27.9783

Epoch 804: val_loss did not improve from 27.97553
196/196 - 45s - loss: 27.2870 - MinusLogProbMetric: 27.2870 - val_loss: 27.9783 - val_MinusLogProbMetric: 27.9783 - lr: 7.8125e-06 - 45s/epoch - 231ms/step
Epoch 805/1000
2023-09-29 16:08:09.422 
Epoch 805/1000 
	 loss: 27.2866, MinusLogProbMetric: 27.2866, val_loss: 27.9777, val_MinusLogProbMetric: 27.9777

Epoch 805: val_loss did not improve from 27.97553
196/196 - 45s - loss: 27.2866 - MinusLogProbMetric: 27.2866 - val_loss: 27.9777 - val_MinusLogProbMetric: 27.9777 - lr: 7.8125e-06 - 45s/epoch - 231ms/step
Epoch 806/1000
2023-09-29 16:08:54.578 
Epoch 806/1000 
	 loss: 27.2868, MinusLogProbMetric: 27.2868, val_loss: 27.9784, val_MinusLogProbMetric: 27.9784

Epoch 806: val_loss did not improve from 27.97553
196/196 - 45s - loss: 27.2868 - MinusLogProbMetric: 27.2868 - val_loss: 27.9784 - val_MinusLogProbMetric: 27.9784 - lr: 7.8125e-06 - 45s/epoch - 230ms/step
Epoch 807/1000
2023-09-29 16:09:39.192 
Epoch 807/1000 
	 loss: 27.2871, MinusLogProbMetric: 27.2871, val_loss: 27.9796, val_MinusLogProbMetric: 27.9796

Epoch 807: val_loss did not improve from 27.97553
196/196 - 45s - loss: 27.2871 - MinusLogProbMetric: 27.2871 - val_loss: 27.9796 - val_MinusLogProbMetric: 27.9796 - lr: 7.8125e-06 - 45s/epoch - 228ms/step
Epoch 808/1000
2023-09-29 16:10:24.397 
Epoch 808/1000 
	 loss: 27.2866, MinusLogProbMetric: 27.2866, val_loss: 27.9782, val_MinusLogProbMetric: 27.9782

Epoch 808: val_loss did not improve from 27.97553
196/196 - 45s - loss: 27.2866 - MinusLogProbMetric: 27.2866 - val_loss: 27.9782 - val_MinusLogProbMetric: 27.9782 - lr: 7.8125e-06 - 45s/epoch - 231ms/step
Epoch 809/1000
2023-09-29 16:11:09.794 
Epoch 809/1000 
	 loss: 27.2861, MinusLogProbMetric: 27.2861, val_loss: 27.9783, val_MinusLogProbMetric: 27.9783

Epoch 809: val_loss did not improve from 27.97553
196/196 - 45s - loss: 27.2861 - MinusLogProbMetric: 27.2861 - val_loss: 27.9783 - val_MinusLogProbMetric: 27.9783 - lr: 7.8125e-06 - 45s/epoch - 232ms/step
Epoch 810/1000
2023-09-29 16:11:54.915 
Epoch 810/1000 
	 loss: 27.2861, MinusLogProbMetric: 27.2861, val_loss: 27.9789, val_MinusLogProbMetric: 27.9789

Epoch 810: val_loss did not improve from 27.97553
196/196 - 45s - loss: 27.2861 - MinusLogProbMetric: 27.2861 - val_loss: 27.9789 - val_MinusLogProbMetric: 27.9789 - lr: 7.8125e-06 - 45s/epoch - 230ms/step
Epoch 811/1000
2023-09-29 16:12:39.961 
Epoch 811/1000 
	 loss: 27.2865, MinusLogProbMetric: 27.2865, val_loss: 27.9789, val_MinusLogProbMetric: 27.9789

Epoch 811: val_loss did not improve from 27.97553
196/196 - 45s - loss: 27.2865 - MinusLogProbMetric: 27.2865 - val_loss: 27.9789 - val_MinusLogProbMetric: 27.9789 - lr: 7.8125e-06 - 45s/epoch - 230ms/step
Epoch 812/1000
2023-09-29 16:13:24.847 
Epoch 812/1000 
	 loss: 27.2865, MinusLogProbMetric: 27.2865, val_loss: 27.9780, val_MinusLogProbMetric: 27.9780

Epoch 812: val_loss did not improve from 27.97553
196/196 - 45s - loss: 27.2865 - MinusLogProbMetric: 27.2865 - val_loss: 27.9780 - val_MinusLogProbMetric: 27.9780 - lr: 7.8125e-06 - 45s/epoch - 229ms/step
Epoch 813/1000
2023-09-29 16:14:10.260 
Epoch 813/1000 
	 loss: 27.2861, MinusLogProbMetric: 27.2861, val_loss: 27.9798, val_MinusLogProbMetric: 27.9798

Epoch 813: val_loss did not improve from 27.97553
196/196 - 45s - loss: 27.2861 - MinusLogProbMetric: 27.2861 - val_loss: 27.9798 - val_MinusLogProbMetric: 27.9798 - lr: 7.8125e-06 - 45s/epoch - 232ms/step
Epoch 814/1000
2023-09-29 16:14:55.636 
Epoch 814/1000 
	 loss: 27.2871, MinusLogProbMetric: 27.2871, val_loss: 27.9778, val_MinusLogProbMetric: 27.9778

Epoch 814: val_loss did not improve from 27.97553
196/196 - 45s - loss: 27.2871 - MinusLogProbMetric: 27.2871 - val_loss: 27.9778 - val_MinusLogProbMetric: 27.9778 - lr: 7.8125e-06 - 45s/epoch - 231ms/step
Epoch 815/1000
2023-09-29 16:15:40.812 
Epoch 815/1000 
	 loss: 27.2866, MinusLogProbMetric: 27.2866, val_loss: 27.9788, val_MinusLogProbMetric: 27.9788

Epoch 815: val_loss did not improve from 27.97553
196/196 - 45s - loss: 27.2866 - MinusLogProbMetric: 27.2866 - val_loss: 27.9788 - val_MinusLogProbMetric: 27.9788 - lr: 7.8125e-06 - 45s/epoch - 230ms/step
Epoch 816/1000
2023-09-29 16:16:26.095 
Epoch 816/1000 
	 loss: 27.2859, MinusLogProbMetric: 27.2859, val_loss: 27.9790, val_MinusLogProbMetric: 27.9790

Epoch 816: val_loss did not improve from 27.97553
196/196 - 45s - loss: 27.2859 - MinusLogProbMetric: 27.2859 - val_loss: 27.9790 - val_MinusLogProbMetric: 27.9790 - lr: 7.8125e-06 - 45s/epoch - 231ms/step
Epoch 817/1000
2023-09-29 16:17:11.393 
Epoch 817/1000 
	 loss: 27.2868, MinusLogProbMetric: 27.2868, val_loss: 27.9788, val_MinusLogProbMetric: 27.9788

Epoch 817: val_loss did not improve from 27.97553
196/196 - 45s - loss: 27.2868 - MinusLogProbMetric: 27.2868 - val_loss: 27.9788 - val_MinusLogProbMetric: 27.9788 - lr: 7.8125e-06 - 45s/epoch - 231ms/step
Epoch 818/1000
2023-09-29 16:17:56.319 
Epoch 818/1000 
	 loss: 27.2864, MinusLogProbMetric: 27.2864, val_loss: 27.9815, val_MinusLogProbMetric: 27.9815

Epoch 818: val_loss did not improve from 27.97553
196/196 - 45s - loss: 27.2864 - MinusLogProbMetric: 27.2864 - val_loss: 27.9815 - val_MinusLogProbMetric: 27.9815 - lr: 7.8125e-06 - 45s/epoch - 229ms/step
Epoch 819/1000
2023-09-29 16:18:41.884 
Epoch 819/1000 
	 loss: 27.2865, MinusLogProbMetric: 27.2865, val_loss: 27.9798, val_MinusLogProbMetric: 27.9798

Epoch 819: val_loss did not improve from 27.97553
196/196 - 45s - loss: 27.2865 - MinusLogProbMetric: 27.2865 - val_loss: 27.9798 - val_MinusLogProbMetric: 27.9798 - lr: 7.8125e-06 - 45s/epoch - 232ms/step
Epoch 820/1000
2023-09-29 16:19:27.044 
Epoch 820/1000 
	 loss: 27.2860, MinusLogProbMetric: 27.2860, val_loss: 27.9792, val_MinusLogProbMetric: 27.9792

Epoch 820: val_loss did not improve from 27.97553
196/196 - 45s - loss: 27.2860 - MinusLogProbMetric: 27.2860 - val_loss: 27.9792 - val_MinusLogProbMetric: 27.9792 - lr: 7.8125e-06 - 45s/epoch - 230ms/step
Epoch 821/1000
2023-09-29 16:20:12.017 
Epoch 821/1000 
	 loss: 27.2864, MinusLogProbMetric: 27.2864, val_loss: 27.9800, val_MinusLogProbMetric: 27.9800

Epoch 821: val_loss did not improve from 27.97553
196/196 - 45s - loss: 27.2864 - MinusLogProbMetric: 27.2864 - val_loss: 27.9800 - val_MinusLogProbMetric: 27.9800 - lr: 7.8125e-06 - 45s/epoch - 229ms/step
Epoch 822/1000
2023-09-29 16:20:57.124 
Epoch 822/1000 
	 loss: 27.2862, MinusLogProbMetric: 27.2862, val_loss: 27.9775, val_MinusLogProbMetric: 27.9775

Epoch 822: val_loss did not improve from 27.97553
196/196 - 45s - loss: 27.2862 - MinusLogProbMetric: 27.2862 - val_loss: 27.9775 - val_MinusLogProbMetric: 27.9775 - lr: 7.8125e-06 - 45s/epoch - 230ms/step
Epoch 823/1000
2023-09-29 16:21:42.410 
Epoch 823/1000 
	 loss: 27.2863, MinusLogProbMetric: 27.2863, val_loss: 27.9808, val_MinusLogProbMetric: 27.9808

Epoch 823: val_loss did not improve from 27.97553
196/196 - 45s - loss: 27.2863 - MinusLogProbMetric: 27.2863 - val_loss: 27.9808 - val_MinusLogProbMetric: 27.9808 - lr: 7.8125e-06 - 45s/epoch - 231ms/step
Epoch 824/1000
2023-09-29 16:22:27.626 
Epoch 824/1000 
	 loss: 27.2856, MinusLogProbMetric: 27.2856, val_loss: 27.9786, val_MinusLogProbMetric: 27.9786

Epoch 824: val_loss did not improve from 27.97553
196/196 - 45s - loss: 27.2856 - MinusLogProbMetric: 27.2856 - val_loss: 27.9786 - val_MinusLogProbMetric: 27.9786 - lr: 7.8125e-06 - 45s/epoch - 231ms/step
Epoch 825/1000
2023-09-29 16:23:12.744 
Epoch 825/1000 
	 loss: 27.2860, MinusLogProbMetric: 27.2860, val_loss: 27.9776, val_MinusLogProbMetric: 27.9776

Epoch 825: val_loss did not improve from 27.97553
196/196 - 45s - loss: 27.2860 - MinusLogProbMetric: 27.2860 - val_loss: 27.9776 - val_MinusLogProbMetric: 27.9776 - lr: 7.8125e-06 - 45s/epoch - 230ms/step
Epoch 826/1000
2023-09-29 16:23:57.932 
Epoch 826/1000 
	 loss: 27.2863, MinusLogProbMetric: 27.2863, val_loss: 27.9776, val_MinusLogProbMetric: 27.9776

Epoch 826: val_loss did not improve from 27.97553
196/196 - 45s - loss: 27.2863 - MinusLogProbMetric: 27.2863 - val_loss: 27.9776 - val_MinusLogProbMetric: 27.9776 - lr: 7.8125e-06 - 45s/epoch - 231ms/step
Epoch 827/1000
2023-09-29 16:24:43.383 
Epoch 827/1000 
	 loss: 27.2862, MinusLogProbMetric: 27.2862, val_loss: 27.9780, val_MinusLogProbMetric: 27.9780

Epoch 827: val_loss did not improve from 27.97553
196/196 - 45s - loss: 27.2862 - MinusLogProbMetric: 27.2862 - val_loss: 27.9780 - val_MinusLogProbMetric: 27.9780 - lr: 7.8125e-06 - 45s/epoch - 232ms/step
Epoch 828/1000
2023-09-29 16:25:28.698 
Epoch 828/1000 
	 loss: 27.2862, MinusLogProbMetric: 27.2862, val_loss: 27.9777, val_MinusLogProbMetric: 27.9777

Epoch 828: val_loss did not improve from 27.97553
196/196 - 45s - loss: 27.2862 - MinusLogProbMetric: 27.2862 - val_loss: 27.9777 - val_MinusLogProbMetric: 27.9777 - lr: 7.8125e-06 - 45s/epoch - 231ms/step
Epoch 829/1000
2023-09-29 16:26:13.976 
Epoch 829/1000 
	 loss: 27.2858, MinusLogProbMetric: 27.2858, val_loss: 27.9793, val_MinusLogProbMetric: 27.9793

Epoch 829: val_loss did not improve from 27.97553
196/196 - 45s - loss: 27.2858 - MinusLogProbMetric: 27.2858 - val_loss: 27.9793 - val_MinusLogProbMetric: 27.9793 - lr: 7.8125e-06 - 45s/epoch - 231ms/step
Epoch 830/1000
2023-09-29 16:26:59.422 
Epoch 830/1000 
	 loss: 27.2861, MinusLogProbMetric: 27.2861, val_loss: 27.9786, val_MinusLogProbMetric: 27.9786

Epoch 830: val_loss did not improve from 27.97553
196/196 - 45s - loss: 27.2861 - MinusLogProbMetric: 27.2861 - val_loss: 27.9786 - val_MinusLogProbMetric: 27.9786 - lr: 7.8125e-06 - 45s/epoch - 232ms/step
Epoch 831/1000
2023-09-29 16:27:44.737 
Epoch 831/1000 
	 loss: 27.2862, MinusLogProbMetric: 27.2862, val_loss: 27.9780, val_MinusLogProbMetric: 27.9780

Epoch 831: val_loss did not improve from 27.97553
196/196 - 45s - loss: 27.2862 - MinusLogProbMetric: 27.2862 - val_loss: 27.9780 - val_MinusLogProbMetric: 27.9780 - lr: 7.8125e-06 - 45s/epoch - 231ms/step
Epoch 832/1000
2023-09-29 16:28:29.653 
Epoch 832/1000 
	 loss: 27.2857, MinusLogProbMetric: 27.2857, val_loss: 27.9782, val_MinusLogProbMetric: 27.9782

Epoch 832: val_loss did not improve from 27.97553
196/196 - 45s - loss: 27.2857 - MinusLogProbMetric: 27.2857 - val_loss: 27.9782 - val_MinusLogProbMetric: 27.9782 - lr: 7.8125e-06 - 45s/epoch - 229ms/step
Epoch 833/1000
2023-09-29 16:29:14.968 
Epoch 833/1000 
	 loss: 27.2858, MinusLogProbMetric: 27.2858, val_loss: 27.9790, val_MinusLogProbMetric: 27.9790

Epoch 833: val_loss did not improve from 27.97553
196/196 - 45s - loss: 27.2858 - MinusLogProbMetric: 27.2858 - val_loss: 27.9790 - val_MinusLogProbMetric: 27.9790 - lr: 7.8125e-06 - 45s/epoch - 231ms/step
Epoch 834/1000
2023-09-29 16:30:00.283 
Epoch 834/1000 
	 loss: 27.2863, MinusLogProbMetric: 27.2863, val_loss: 27.9800, val_MinusLogProbMetric: 27.9800

Epoch 834: val_loss did not improve from 27.97553
196/196 - 45s - loss: 27.2863 - MinusLogProbMetric: 27.2863 - val_loss: 27.9800 - val_MinusLogProbMetric: 27.9800 - lr: 7.8125e-06 - 45s/epoch - 231ms/step
Epoch 835/1000
2023-09-29 16:30:45.437 
Epoch 835/1000 
	 loss: 27.2859, MinusLogProbMetric: 27.2859, val_loss: 27.9796, val_MinusLogProbMetric: 27.9796

Epoch 835: val_loss did not improve from 27.97553
196/196 - 45s - loss: 27.2859 - MinusLogProbMetric: 27.2859 - val_loss: 27.9796 - val_MinusLogProbMetric: 27.9796 - lr: 7.8125e-06 - 45s/epoch - 230ms/step
Epoch 836/1000
2023-09-29 16:31:30.674 
Epoch 836/1000 
	 loss: 27.2868, MinusLogProbMetric: 27.2868, val_loss: 27.9768, val_MinusLogProbMetric: 27.9768

Epoch 836: val_loss did not improve from 27.97553
196/196 - 45s - loss: 27.2868 - MinusLogProbMetric: 27.2868 - val_loss: 27.9768 - val_MinusLogProbMetric: 27.9768 - lr: 7.8125e-06 - 45s/epoch - 231ms/step
Epoch 837/1000
2023-09-29 16:32:15.926 
Epoch 837/1000 
	 loss: 27.2865, MinusLogProbMetric: 27.2865, val_loss: 27.9796, val_MinusLogProbMetric: 27.9796

Epoch 837: val_loss did not improve from 27.97553
196/196 - 45s - loss: 27.2865 - MinusLogProbMetric: 27.2865 - val_loss: 27.9796 - val_MinusLogProbMetric: 27.9796 - lr: 7.8125e-06 - 45s/epoch - 231ms/step
Epoch 838/1000
2023-09-29 16:33:01.417 
Epoch 838/1000 
	 loss: 27.2859, MinusLogProbMetric: 27.2859, val_loss: 27.9792, val_MinusLogProbMetric: 27.9792

Epoch 838: val_loss did not improve from 27.97553
196/196 - 45s - loss: 27.2859 - MinusLogProbMetric: 27.2859 - val_loss: 27.9792 - val_MinusLogProbMetric: 27.9792 - lr: 7.8125e-06 - 45s/epoch - 232ms/step
Epoch 839/1000
2023-09-29 16:33:46.829 
Epoch 839/1000 
	 loss: 27.2862, MinusLogProbMetric: 27.2862, val_loss: 27.9779, val_MinusLogProbMetric: 27.9779

Epoch 839: val_loss did not improve from 27.97553
196/196 - 45s - loss: 27.2862 - MinusLogProbMetric: 27.2862 - val_loss: 27.9779 - val_MinusLogProbMetric: 27.9779 - lr: 7.8125e-06 - 45s/epoch - 232ms/step
Epoch 840/1000
2023-09-29 16:34:32.053 
Epoch 840/1000 
	 loss: 27.2865, MinusLogProbMetric: 27.2865, val_loss: 27.9811, val_MinusLogProbMetric: 27.9811

Epoch 840: val_loss did not improve from 27.97553
196/196 - 45s - loss: 27.2865 - MinusLogProbMetric: 27.2865 - val_loss: 27.9811 - val_MinusLogProbMetric: 27.9811 - lr: 7.8125e-06 - 45s/epoch - 231ms/step
Epoch 841/1000
2023-09-29 16:35:15.890 
Epoch 841/1000 
	 loss: 27.2861, MinusLogProbMetric: 27.2861, val_loss: 27.9775, val_MinusLogProbMetric: 27.9775

Epoch 841: val_loss did not improve from 27.97553
196/196 - 44s - loss: 27.2861 - MinusLogProbMetric: 27.2861 - val_loss: 27.9775 - val_MinusLogProbMetric: 27.9775 - lr: 7.8125e-06 - 44s/epoch - 224ms/step
Epoch 842/1000
2023-09-29 16:36:01.158 
Epoch 842/1000 
	 loss: 27.2834, MinusLogProbMetric: 27.2834, val_loss: 27.9763, val_MinusLogProbMetric: 27.9763

Epoch 842: val_loss did not improve from 27.97553
196/196 - 45s - loss: 27.2834 - MinusLogProbMetric: 27.2834 - val_loss: 27.9763 - val_MinusLogProbMetric: 27.9763 - lr: 3.9063e-06 - 45s/epoch - 231ms/step
Epoch 843/1000
2023-09-29 16:36:45.980 
Epoch 843/1000 
	 loss: 27.2834, MinusLogProbMetric: 27.2834, val_loss: 27.9757, val_MinusLogProbMetric: 27.9757

Epoch 843: val_loss did not improve from 27.97553
196/196 - 45s - loss: 27.2834 - MinusLogProbMetric: 27.2834 - val_loss: 27.9757 - val_MinusLogProbMetric: 27.9757 - lr: 3.9063e-06 - 45s/epoch - 229ms/step
Epoch 844/1000
2023-09-29 16:37:30.123 
Epoch 844/1000 
	 loss: 27.2833, MinusLogProbMetric: 27.2833, val_loss: 27.9766, val_MinusLogProbMetric: 27.9766

Epoch 844: val_loss did not improve from 27.97553
196/196 - 44s - loss: 27.2833 - MinusLogProbMetric: 27.2833 - val_loss: 27.9766 - val_MinusLogProbMetric: 27.9766 - lr: 3.9063e-06 - 44s/epoch - 225ms/step
Epoch 845/1000
2023-09-29 16:38:14.512 
Epoch 845/1000 
	 loss: 27.2835, MinusLogProbMetric: 27.2835, val_loss: 27.9757, val_MinusLogProbMetric: 27.9757

Epoch 845: val_loss did not improve from 27.97553
196/196 - 44s - loss: 27.2835 - MinusLogProbMetric: 27.2835 - val_loss: 27.9757 - val_MinusLogProbMetric: 27.9757 - lr: 3.9063e-06 - 44s/epoch - 226ms/step
Epoch 846/1000
2023-09-29 16:38:58.542 
Epoch 846/1000 
	 loss: 27.2833, MinusLogProbMetric: 27.2833, val_loss: 27.9770, val_MinusLogProbMetric: 27.9770

Epoch 846: val_loss did not improve from 27.97553
196/196 - 44s - loss: 27.2833 - MinusLogProbMetric: 27.2833 - val_loss: 27.9770 - val_MinusLogProbMetric: 27.9770 - lr: 3.9063e-06 - 44s/epoch - 225ms/step
Epoch 847/1000
2023-09-29 16:39:42.081 
Epoch 847/1000 
	 loss: 27.2832, MinusLogProbMetric: 27.2832, val_loss: 27.9778, val_MinusLogProbMetric: 27.9778

Epoch 847: val_loss did not improve from 27.97553
196/196 - 44s - loss: 27.2832 - MinusLogProbMetric: 27.2832 - val_loss: 27.9778 - val_MinusLogProbMetric: 27.9778 - lr: 3.9063e-06 - 44s/epoch - 222ms/step
Epoch 848/1000
2023-09-29 16:40:25.919 
Epoch 848/1000 
	 loss: 27.2834, MinusLogProbMetric: 27.2834, val_loss: 27.9762, val_MinusLogProbMetric: 27.9762

Epoch 848: val_loss did not improve from 27.97553
196/196 - 44s - loss: 27.2834 - MinusLogProbMetric: 27.2834 - val_loss: 27.9762 - val_MinusLogProbMetric: 27.9762 - lr: 3.9063e-06 - 44s/epoch - 224ms/step
Epoch 849/1000
2023-09-29 16:41:09.714 
Epoch 849/1000 
	 loss: 27.2830, MinusLogProbMetric: 27.2830, val_loss: 27.9772, val_MinusLogProbMetric: 27.9772

Epoch 849: val_loss did not improve from 27.97553
196/196 - 44s - loss: 27.2830 - MinusLogProbMetric: 27.2830 - val_loss: 27.9772 - val_MinusLogProbMetric: 27.9772 - lr: 3.9063e-06 - 44s/epoch - 223ms/step
Epoch 850/1000
2023-09-29 16:41:53.753 
Epoch 850/1000 
	 loss: 27.2832, MinusLogProbMetric: 27.2832, val_loss: 27.9784, val_MinusLogProbMetric: 27.9784

Epoch 850: val_loss did not improve from 27.97553
196/196 - 44s - loss: 27.2832 - MinusLogProbMetric: 27.2832 - val_loss: 27.9784 - val_MinusLogProbMetric: 27.9784 - lr: 3.9063e-06 - 44s/epoch - 225ms/step
Epoch 851/1000
2023-09-29 16:42:37.097 
Epoch 851/1000 
	 loss: 27.2833, MinusLogProbMetric: 27.2833, val_loss: 27.9778, val_MinusLogProbMetric: 27.9778

Epoch 851: val_loss did not improve from 27.97553
196/196 - 43s - loss: 27.2833 - MinusLogProbMetric: 27.2833 - val_loss: 27.9778 - val_MinusLogProbMetric: 27.9778 - lr: 3.9063e-06 - 43s/epoch - 221ms/step
Epoch 852/1000
2023-09-29 16:43:21.182 
Epoch 852/1000 
	 loss: 27.2830, MinusLogProbMetric: 27.2830, val_loss: 27.9772, val_MinusLogProbMetric: 27.9772

Epoch 852: val_loss did not improve from 27.97553
196/196 - 44s - loss: 27.2830 - MinusLogProbMetric: 27.2830 - val_loss: 27.9772 - val_MinusLogProbMetric: 27.9772 - lr: 3.9063e-06 - 44s/epoch - 225ms/step
Epoch 853/1000
2023-09-29 16:44:03.839 
Epoch 853/1000 
	 loss: 27.2834, MinusLogProbMetric: 27.2834, val_loss: 27.9758, val_MinusLogProbMetric: 27.9758

Epoch 853: val_loss did not improve from 27.97553
196/196 - 43s - loss: 27.2834 - MinusLogProbMetric: 27.2834 - val_loss: 27.9758 - val_MinusLogProbMetric: 27.9758 - lr: 3.9063e-06 - 43s/epoch - 218ms/step
Epoch 854/1000
2023-09-29 16:44:48.282 
Epoch 854/1000 
	 loss: 27.2833, MinusLogProbMetric: 27.2833, val_loss: 27.9768, val_MinusLogProbMetric: 27.9768

Epoch 854: val_loss did not improve from 27.97553
196/196 - 44s - loss: 27.2833 - MinusLogProbMetric: 27.2833 - val_loss: 27.9768 - val_MinusLogProbMetric: 27.9768 - lr: 3.9063e-06 - 44s/epoch - 227ms/step
Epoch 855/1000
2023-09-29 16:45:32.912 
Epoch 855/1000 
	 loss: 27.2832, MinusLogProbMetric: 27.2832, val_loss: 27.9792, val_MinusLogProbMetric: 27.9792

Epoch 855: val_loss did not improve from 27.97553
196/196 - 45s - loss: 27.2832 - MinusLogProbMetric: 27.2832 - val_loss: 27.9792 - val_MinusLogProbMetric: 27.9792 - lr: 3.9063e-06 - 45s/epoch - 228ms/step
Epoch 856/1000
2023-09-29 16:46:15.745 
Epoch 856/1000 
	 loss: 27.2832, MinusLogProbMetric: 27.2832, val_loss: 27.9774, val_MinusLogProbMetric: 27.9774

Epoch 856: val_loss did not improve from 27.97553
196/196 - 43s - loss: 27.2832 - MinusLogProbMetric: 27.2832 - val_loss: 27.9774 - val_MinusLogProbMetric: 27.9774 - lr: 3.9063e-06 - 43s/epoch - 219ms/step
Epoch 857/1000
2023-09-29 16:47:00.208 
Epoch 857/1000 
	 loss: 27.2831, MinusLogProbMetric: 27.2831, val_loss: 27.9778, val_MinusLogProbMetric: 27.9778

Epoch 857: val_loss did not improve from 27.97553
196/196 - 44s - loss: 27.2831 - MinusLogProbMetric: 27.2831 - val_loss: 27.9778 - val_MinusLogProbMetric: 27.9778 - lr: 3.9063e-06 - 44s/epoch - 227ms/step
Epoch 858/1000
2023-09-29 16:47:43.717 
Epoch 858/1000 
	 loss: 27.2831, MinusLogProbMetric: 27.2831, val_loss: 27.9777, val_MinusLogProbMetric: 27.9777

Epoch 858: val_loss did not improve from 27.97553
196/196 - 44s - loss: 27.2831 - MinusLogProbMetric: 27.2831 - val_loss: 27.9777 - val_MinusLogProbMetric: 27.9777 - lr: 3.9063e-06 - 44s/epoch - 222ms/step
Epoch 859/1000
2023-09-29 16:48:28.167 
Epoch 859/1000 
	 loss: 27.2830, MinusLogProbMetric: 27.2830, val_loss: 27.9770, val_MinusLogProbMetric: 27.9770

Epoch 859: val_loss did not improve from 27.97553
196/196 - 44s - loss: 27.2830 - MinusLogProbMetric: 27.2830 - val_loss: 27.9770 - val_MinusLogProbMetric: 27.9770 - lr: 3.9063e-06 - 44s/epoch - 227ms/step
Epoch 860/1000
2023-09-29 16:49:10.176 
Epoch 860/1000 
	 loss: 27.2831, MinusLogProbMetric: 27.2831, val_loss: 27.9783, val_MinusLogProbMetric: 27.9783

Epoch 860: val_loss did not improve from 27.97553
196/196 - 42s - loss: 27.2831 - MinusLogProbMetric: 27.2831 - val_loss: 27.9783 - val_MinusLogProbMetric: 27.9783 - lr: 3.9063e-06 - 42s/epoch - 214ms/step
Epoch 861/1000
2023-09-29 16:49:53.987 
Epoch 861/1000 
	 loss: 27.2833, MinusLogProbMetric: 27.2833, val_loss: 27.9776, val_MinusLogProbMetric: 27.9776

Epoch 861: val_loss did not improve from 27.97553
196/196 - 44s - loss: 27.2833 - MinusLogProbMetric: 27.2833 - val_loss: 27.9776 - val_MinusLogProbMetric: 27.9776 - lr: 3.9063e-06 - 44s/epoch - 224ms/step
Epoch 862/1000
2023-09-29 16:50:38.347 
Epoch 862/1000 
	 loss: 27.2834, MinusLogProbMetric: 27.2834, val_loss: 27.9760, val_MinusLogProbMetric: 27.9760

Epoch 862: val_loss did not improve from 27.97553
196/196 - 44s - loss: 27.2834 - MinusLogProbMetric: 27.2834 - val_loss: 27.9760 - val_MinusLogProbMetric: 27.9760 - lr: 3.9063e-06 - 44s/epoch - 226ms/step
Epoch 863/1000
2023-09-29 16:51:22.740 
Epoch 863/1000 
	 loss: 27.2832, MinusLogProbMetric: 27.2832, val_loss: 27.9770, val_MinusLogProbMetric: 27.9770

Epoch 863: val_loss did not improve from 27.97553
196/196 - 44s - loss: 27.2832 - MinusLogProbMetric: 27.2832 - val_loss: 27.9770 - val_MinusLogProbMetric: 27.9770 - lr: 3.9063e-06 - 44s/epoch - 226ms/step
Epoch 864/1000
2023-09-29 16:52:06.974 
Epoch 864/1000 
	 loss: 27.2829, MinusLogProbMetric: 27.2829, val_loss: 27.9785, val_MinusLogProbMetric: 27.9785

Epoch 864: val_loss did not improve from 27.97553
196/196 - 44s - loss: 27.2829 - MinusLogProbMetric: 27.2829 - val_loss: 27.9785 - val_MinusLogProbMetric: 27.9785 - lr: 3.9063e-06 - 44s/epoch - 226ms/step
Epoch 865/1000
2023-09-29 16:52:49.808 
Epoch 865/1000 
	 loss: 27.2832, MinusLogProbMetric: 27.2832, val_loss: 27.9778, val_MinusLogProbMetric: 27.9778

Epoch 865: val_loss did not improve from 27.97553
196/196 - 43s - loss: 27.2832 - MinusLogProbMetric: 27.2832 - val_loss: 27.9778 - val_MinusLogProbMetric: 27.9778 - lr: 3.9063e-06 - 43s/epoch - 219ms/step
Epoch 866/1000
2023-09-29 16:53:32.202 
Epoch 866/1000 
	 loss: 27.2832, MinusLogProbMetric: 27.2832, val_loss: 27.9766, val_MinusLogProbMetric: 27.9766

Epoch 866: val_loss did not improve from 27.97553
196/196 - 42s - loss: 27.2832 - MinusLogProbMetric: 27.2832 - val_loss: 27.9766 - val_MinusLogProbMetric: 27.9766 - lr: 3.9063e-06 - 42s/epoch - 216ms/step
Epoch 867/1000
2023-09-29 16:54:14.664 
Epoch 867/1000 
	 loss: 27.2832, MinusLogProbMetric: 27.2832, val_loss: 27.9770, val_MinusLogProbMetric: 27.9770

Epoch 867: val_loss did not improve from 27.97553
196/196 - 42s - loss: 27.2832 - MinusLogProbMetric: 27.2832 - val_loss: 27.9770 - val_MinusLogProbMetric: 27.9770 - lr: 3.9063e-06 - 42s/epoch - 217ms/step
Epoch 868/1000
2023-09-29 16:54:58.348 
Epoch 868/1000 
	 loss: 27.2829, MinusLogProbMetric: 27.2829, val_loss: 27.9784, val_MinusLogProbMetric: 27.9784

Epoch 868: val_loss did not improve from 27.97553
196/196 - 44s - loss: 27.2829 - MinusLogProbMetric: 27.2829 - val_loss: 27.9784 - val_MinusLogProbMetric: 27.9784 - lr: 3.9063e-06 - 44s/epoch - 223ms/step
Epoch 869/1000
2023-09-29 16:55:42.417 
Epoch 869/1000 
	 loss: 27.2830, MinusLogProbMetric: 27.2830, val_loss: 27.9776, val_MinusLogProbMetric: 27.9776

Epoch 869: val_loss did not improve from 27.97553
196/196 - 44s - loss: 27.2830 - MinusLogProbMetric: 27.2830 - val_loss: 27.9776 - val_MinusLogProbMetric: 27.9776 - lr: 3.9063e-06 - 44s/epoch - 225ms/step
Epoch 870/1000
2023-09-29 16:56:25.358 
Epoch 870/1000 
	 loss: 27.2831, MinusLogProbMetric: 27.2831, val_loss: 27.9783, val_MinusLogProbMetric: 27.9783

Epoch 870: val_loss did not improve from 27.97553
196/196 - 43s - loss: 27.2831 - MinusLogProbMetric: 27.2831 - val_loss: 27.9783 - val_MinusLogProbMetric: 27.9783 - lr: 3.9063e-06 - 43s/epoch - 219ms/step
Epoch 871/1000
2023-09-29 16:57:09.077 
Epoch 871/1000 
	 loss: 27.2830, MinusLogProbMetric: 27.2830, val_loss: 27.9776, val_MinusLogProbMetric: 27.9776

Epoch 871: val_loss did not improve from 27.97553
196/196 - 44s - loss: 27.2830 - MinusLogProbMetric: 27.2830 - val_loss: 27.9776 - val_MinusLogProbMetric: 27.9776 - lr: 3.9063e-06 - 44s/epoch - 223ms/step
Epoch 872/1000
2023-09-29 16:57:52.615 
Epoch 872/1000 
	 loss: 27.2825, MinusLogProbMetric: 27.2825, val_loss: 27.9769, val_MinusLogProbMetric: 27.9769

Epoch 872: val_loss did not improve from 27.97553
196/196 - 44s - loss: 27.2825 - MinusLogProbMetric: 27.2825 - val_loss: 27.9769 - val_MinusLogProbMetric: 27.9769 - lr: 3.9063e-06 - 44s/epoch - 222ms/step
Epoch 873/1000
2023-09-29 16:58:36.985 
Epoch 873/1000 
	 loss: 27.2828, MinusLogProbMetric: 27.2828, val_loss: 27.9769, val_MinusLogProbMetric: 27.9769

Epoch 873: val_loss did not improve from 27.97553
196/196 - 44s - loss: 27.2828 - MinusLogProbMetric: 27.2828 - val_loss: 27.9769 - val_MinusLogProbMetric: 27.9769 - lr: 3.9063e-06 - 44s/epoch - 226ms/step
Epoch 874/1000
2023-09-29 16:59:20.475 
Epoch 874/1000 
	 loss: 27.2833, MinusLogProbMetric: 27.2833, val_loss: 27.9777, val_MinusLogProbMetric: 27.9777

Epoch 874: val_loss did not improve from 27.97553
196/196 - 43s - loss: 27.2833 - MinusLogProbMetric: 27.2833 - val_loss: 27.9777 - val_MinusLogProbMetric: 27.9777 - lr: 3.9063e-06 - 43s/epoch - 222ms/step
Epoch 875/1000
2023-09-29 17:00:04.362 
Epoch 875/1000 
	 loss: 27.2827, MinusLogProbMetric: 27.2827, val_loss: 27.9777, val_MinusLogProbMetric: 27.9777

Epoch 875: val_loss did not improve from 27.97553
196/196 - 44s - loss: 27.2827 - MinusLogProbMetric: 27.2827 - val_loss: 27.9777 - val_MinusLogProbMetric: 27.9777 - lr: 3.9063e-06 - 44s/epoch - 224ms/step
Epoch 876/1000
2023-09-29 17:00:46.348 
Epoch 876/1000 
	 loss: 27.2829, MinusLogProbMetric: 27.2829, val_loss: 27.9781, val_MinusLogProbMetric: 27.9781

Epoch 876: val_loss did not improve from 27.97553
196/196 - 42s - loss: 27.2829 - MinusLogProbMetric: 27.2829 - val_loss: 27.9781 - val_MinusLogProbMetric: 27.9781 - lr: 3.9063e-06 - 42s/epoch - 214ms/step
Epoch 877/1000
2023-09-29 17:01:30.270 
Epoch 877/1000 
	 loss: 27.2831, MinusLogProbMetric: 27.2831, val_loss: 27.9776, val_MinusLogProbMetric: 27.9776

Epoch 877: val_loss did not improve from 27.97553
196/196 - 44s - loss: 27.2831 - MinusLogProbMetric: 27.2831 - val_loss: 27.9776 - val_MinusLogProbMetric: 27.9776 - lr: 3.9063e-06 - 44s/epoch - 224ms/step
Epoch 878/1000
2023-09-29 17:02:11.776 
Epoch 878/1000 
	 loss: 27.2832, MinusLogProbMetric: 27.2832, val_loss: 27.9764, val_MinusLogProbMetric: 27.9764

Epoch 878: val_loss did not improve from 27.97553
196/196 - 42s - loss: 27.2832 - MinusLogProbMetric: 27.2832 - val_loss: 27.9764 - val_MinusLogProbMetric: 27.9764 - lr: 3.9063e-06 - 42s/epoch - 212ms/step
Epoch 879/1000
2023-09-29 17:02:55.746 
Epoch 879/1000 
	 loss: 27.2826, MinusLogProbMetric: 27.2826, val_loss: 27.9775, val_MinusLogProbMetric: 27.9775

Epoch 879: val_loss did not improve from 27.97553
196/196 - 44s - loss: 27.2826 - MinusLogProbMetric: 27.2826 - val_loss: 27.9775 - val_MinusLogProbMetric: 27.9775 - lr: 3.9063e-06 - 44s/epoch - 224ms/step
Epoch 880/1000
2023-09-29 17:03:39.640 
Epoch 880/1000 
	 loss: 27.2828, MinusLogProbMetric: 27.2828, val_loss: 27.9775, val_MinusLogProbMetric: 27.9775

Epoch 880: val_loss did not improve from 27.97553
196/196 - 44s - loss: 27.2828 - MinusLogProbMetric: 27.2828 - val_loss: 27.9775 - val_MinusLogProbMetric: 27.9775 - lr: 3.9063e-06 - 44s/epoch - 224ms/step
Epoch 881/1000
2023-09-29 17:04:23.848 
Epoch 881/1000 
	 loss: 27.2825, MinusLogProbMetric: 27.2825, val_loss: 27.9769, val_MinusLogProbMetric: 27.9769

Epoch 881: val_loss did not improve from 27.97553
196/196 - 44s - loss: 27.2825 - MinusLogProbMetric: 27.2825 - val_loss: 27.9769 - val_MinusLogProbMetric: 27.9769 - lr: 3.9063e-06 - 44s/epoch - 226ms/step
Epoch 882/1000
2023-09-29 17:05:06.782 
Epoch 882/1000 
	 loss: 27.2831, MinusLogProbMetric: 27.2831, val_loss: 27.9772, val_MinusLogProbMetric: 27.9772

Epoch 882: val_loss did not improve from 27.97553
196/196 - 43s - loss: 27.2831 - MinusLogProbMetric: 27.2831 - val_loss: 27.9772 - val_MinusLogProbMetric: 27.9772 - lr: 3.9063e-06 - 43s/epoch - 219ms/step
Epoch 883/1000
2023-09-29 17:05:51.042 
Epoch 883/1000 
	 loss: 27.2826, MinusLogProbMetric: 27.2826, val_loss: 27.9778, val_MinusLogProbMetric: 27.9778

Epoch 883: val_loss did not improve from 27.97553
196/196 - 44s - loss: 27.2826 - MinusLogProbMetric: 27.2826 - val_loss: 27.9778 - val_MinusLogProbMetric: 27.9778 - lr: 3.9063e-06 - 44s/epoch - 226ms/step
Epoch 884/1000
2023-09-29 17:06:35.107 
Epoch 884/1000 
	 loss: 27.2830, MinusLogProbMetric: 27.2830, val_loss: 27.9801, val_MinusLogProbMetric: 27.9801

Epoch 884: val_loss did not improve from 27.97553
196/196 - 44s - loss: 27.2830 - MinusLogProbMetric: 27.2830 - val_loss: 27.9801 - val_MinusLogProbMetric: 27.9801 - lr: 3.9063e-06 - 44s/epoch - 225ms/step
Epoch 885/1000
2023-09-29 17:07:19.086 
Epoch 885/1000 
	 loss: 27.2831, MinusLogProbMetric: 27.2831, val_loss: 27.9775, val_MinusLogProbMetric: 27.9775

Epoch 885: val_loss did not improve from 27.97553
196/196 - 44s - loss: 27.2831 - MinusLogProbMetric: 27.2831 - val_loss: 27.9775 - val_MinusLogProbMetric: 27.9775 - lr: 3.9063e-06 - 44s/epoch - 224ms/step
Epoch 886/1000
2023-09-29 17:08:02.603 
Epoch 886/1000 
	 loss: 27.2835, MinusLogProbMetric: 27.2835, val_loss: 27.9769, val_MinusLogProbMetric: 27.9769

Epoch 886: val_loss did not improve from 27.97553
196/196 - 44s - loss: 27.2835 - MinusLogProbMetric: 27.2835 - val_loss: 27.9769 - val_MinusLogProbMetric: 27.9769 - lr: 3.9063e-06 - 44s/epoch - 222ms/step
Epoch 887/1000
2023-09-29 17:08:45.996 
Epoch 887/1000 
	 loss: 27.2836, MinusLogProbMetric: 27.2836, val_loss: 27.9765, val_MinusLogProbMetric: 27.9765

Epoch 887: val_loss did not improve from 27.97553
196/196 - 43s - loss: 27.2836 - MinusLogProbMetric: 27.2836 - val_loss: 27.9765 - val_MinusLogProbMetric: 27.9765 - lr: 3.9063e-06 - 43s/epoch - 221ms/step
Epoch 888/1000
2023-09-29 17:09:30.249 
Epoch 888/1000 
	 loss: 27.2829, MinusLogProbMetric: 27.2829, val_loss: 27.9773, val_MinusLogProbMetric: 27.9773

Epoch 888: val_loss did not improve from 27.97553
196/196 - 44s - loss: 27.2829 - MinusLogProbMetric: 27.2829 - val_loss: 27.9773 - val_MinusLogProbMetric: 27.9773 - lr: 3.9063e-06 - 44s/epoch - 226ms/step
Epoch 889/1000
2023-09-29 17:10:13.637 
Epoch 889/1000 
	 loss: 27.2828, MinusLogProbMetric: 27.2828, val_loss: 27.9768, val_MinusLogProbMetric: 27.9768

Epoch 889: val_loss did not improve from 27.97553
196/196 - 43s - loss: 27.2828 - MinusLogProbMetric: 27.2828 - val_loss: 27.9768 - val_MinusLogProbMetric: 27.9768 - lr: 3.9063e-06 - 43s/epoch - 221ms/step
Epoch 890/1000
2023-09-29 17:10:56.853 
Epoch 890/1000 
	 loss: 27.2826, MinusLogProbMetric: 27.2826, val_loss: 27.9764, val_MinusLogProbMetric: 27.9764

Epoch 890: val_loss did not improve from 27.97553
196/196 - 43s - loss: 27.2826 - MinusLogProbMetric: 27.2826 - val_loss: 27.9764 - val_MinusLogProbMetric: 27.9764 - lr: 3.9063e-06 - 43s/epoch - 220ms/step
Epoch 891/1000
2023-09-29 17:11:38.316 
Epoch 891/1000 
	 loss: 27.2826, MinusLogProbMetric: 27.2826, val_loss: 27.9771, val_MinusLogProbMetric: 27.9771

Epoch 891: val_loss did not improve from 27.97553
Restoring model weights from the end of the best epoch: 791.
196/196 - 42s - loss: 27.2826 - MinusLogProbMetric: 27.2826 - val_loss: 27.9771 - val_MinusLogProbMetric: 27.9771 - lr: 3.9063e-06 - 42s/epoch - 214ms/step
Epoch 891: early stopping
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.

------------------------------------------
Starting LR metric calculation...
Running TF LR calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
LR metric calculation completed in 19.35572845104616 seconds.

------------------------------------------
Starting KS tests calculation...
Running TF KS tests...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
KS tests calculation completed in 11.504667193978094 seconds.

------------------------------------------
Starting SWD metric calculation...
Running TF SWD calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
SWD metric calculation completed in 8.159267325070687 seconds.

------------------------------------------
Starting FN metric calculation...
Running TF FN calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
FN metric calculation completed in 8.931698000989854 seconds.
Training succeeded with seed 187.
Model trained in 36608.73 s.

===========
Computing predictions
===========

Computing metrics...
Metrics computed in 49.27 s.
                ===========
                print("===========
Failed to plot
===========
")
                Exception type: ValueError
                Exception message: Number of rows must be a positive integer, not 0
                Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN.py , Line : 469, Func.Name : prediction_function, Message : Plotters.cornerplotter(X_data_test.numpy(),X_data_nf.numpy(),path_to_results,ndims,norm=True) # type: ignore', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/Plotters.py , Line : 156, Func.Name : cornerplotter, Message : figure=corner.corner(target_samples,color=\'red\',bins=n_bins,labels=[r"%s" % s for s in labels],normalize1d=True)', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/corner.py , Line : 235, Func.Name : corner, Message : fig, axes = pl.subplots(K, K, figsize=(dim, dim))', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/pyplot.py , Line : 1502, Func.Name : subplots, Message : axs = fig.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 905, Func.Name : subplots, Message : gs = self.add_gridspec(nrows, ncols, figure=self, **gridspec_kw)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 1527, Func.Name : add_gridspec, Message : gs = GridSpec(nrows=nrows, ncols=ncols, figure=self, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 379, Func.Name : __init__, Message : super().__init__(nrows, ncols,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 49, Func.Name : __init__, Message : raise ValueError(']
                ===========

results.txt saved
results.json saved
Results log saved
Model predictions computed in 49.50 s.
===========
Run 332/720 done in 36665.22 s.
===========

Directory ../../results/CsplineN_new/run_333/ already exists.
Skipping it.
===========
Run 333/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_334/ already exists.
Skipping it.
===========
Run 334/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_335/ already exists.
Skipping it.
===========
Run 335/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_336/ already exists.
Skipping it.
===========
Run 336/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_337/ already exists.
Skipping it.
===========
Run 337/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_338/ already exists.
Skipping it.
===========
Run 338/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_339/ already exists.
Skipping it.
===========
Run 339/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_340/ already exists.
Skipping it.
===========
Run 340/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_341/ already exists.
Skipping it.
===========
Run 341/720 already exists. Skipping it.
===========

===========
Generating train data for run 342.
===========
Train data generated in 0.36 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_342/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_342/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_342/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_342
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.465355   7.122426   6.507682  ...  1.2216942  7.396178   1.4448237]
 [ 1.8683796  2.8760538  9.147447  ...  4.980697  -1.1366768  2.8016024]
 [ 5.6175413  8.642618   5.6734285 ... -0.9440901  7.0561414  1.3082856]
 ...
 [ 6.145307   2.8273122  6.191003  ...  2.964365   6.077158   2.5509653]
 [ 6.5595512  2.759307   6.0778823 ...  3.4800506  4.3951674  2.1941729]
 [ 5.586527   7.3216014  6.2487698 ...  1.292631   5.797888   1.2879843]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_270"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_271 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_25 (LogProbL  (None,)                  3291840   
 ayer)                                                           
                                                                 
=================================================================
Total params: 3,291,840
Trainable params: 3,291,840
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_25/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_25'")
self.model: <keras.engine.functional.Functional object at 0x7fae79a55db0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fb3f01f1960>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fb3f01f1960>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fae794b28c0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fb1728af4c0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fb1728afd30>, <keras.callbacks.ModelCheckpoint object at 0x7fb1728af880>, <keras.callbacks.EarlyStopping object at 0x7fb1728aee00>, <keras.callbacks.ReduceLROnPlateau object at 0x7fb1728af2b0>, <keras.callbacks.TerminateOnNaN object at 0x7fb1728afc10>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_342/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 342/720 with hyperparameters:
timestamp = 2023-09-29 17:12:37.056022
ndims = 64
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 3291840
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 5.465355    7.122426    6.507682    5.369255    4.1136785   6.196197
  4.259451    8.702779    9.329599    4.155665    7.5761204   5.312057
  5.67009     9.304932    0.8909831   1.4673554  -0.03015512  8.003835
  8.704047    8.985184    9.595006    8.103893    4.62255     8.186032
  0.5885266   6.325719    0.92288995  9.5184145   6.6715336   3.3791308
  2.4301429   8.523114    4.619198    5.3705144   0.3660334   5.918427
  6.2292395   7.1572037   9.551745    6.67638     3.2475867   4.406022
  7.1114645   0.65166384  6.9876056   6.6356955   2.3022606   1.2872343
  3.532573    3.8445692   6.0782557   4.5467997   9.491786    0.11065191
  3.0255654   1.2933161   6.592196    2.150623    4.2476006   4.39
  1.805124    1.2216942   7.396178    1.4448237 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 9: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-29 17:14:55.497 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 5339.1611, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 138s - loss: nan - MinusLogProbMetric: 5339.1611 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 138s/epoch - 705ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 0.0003333333333333333.
===========
Generating train data for run 342.
===========
Train data generated in 0.16 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_342/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_342/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_342/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_342
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.465355   7.122426   6.507682  ...  1.2216942  7.396178   1.4448237]
 [ 1.8683796  2.8760538  9.147447  ...  4.980697  -1.1366768  2.8016024]
 [ 5.6175413  8.642618   5.6734285 ... -0.9440901  7.0561414  1.3082856]
 ...
 [ 6.145307   2.8273122  6.191003  ...  2.964365   6.077158   2.5509653]
 [ 6.5595512  2.759307   6.0778823 ...  3.4800506  4.3951674  2.1941729]
 [ 5.586527   7.3216014  6.2487698 ...  1.292631   5.797888   1.2879843]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_281"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_282 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_26 (LogProbL  (None,)                  3291840   
 ayer)                                                           
                                                                 
=================================================================
Total params: 3,291,840
Trainable params: 3,291,840
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_26/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_26'")
self.model: <keras.engine.functional.Functional object at 0x7faeb82de1d0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fb14517d180>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fb14517d180>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fb0ff43ca90>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fae788a6380>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fae788a68f0>, <keras.callbacks.ModelCheckpoint object at 0x7fae788a69b0>, <keras.callbacks.EarlyStopping object at 0x7fae788a6c20>, <keras.callbacks.ReduceLROnPlateau object at 0x7fae788a6c50>, <keras.callbacks.TerminateOnNaN object at 0x7fae788a6890>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_342/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 342/720 with hyperparameters:
timestamp = 2023-09-29 17:15:03.697093
ndims = 64
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 3291840
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 5.465355    7.122426    6.507682    5.369255    4.1136785   6.196197
  4.259451    8.702779    9.329599    4.155665    7.5761204   5.312057
  5.67009     9.304932    0.8909831   1.4673554  -0.03015512  8.003835
  8.704047    8.985184    9.595006    8.103893    4.62255     8.186032
  0.5885266   6.325719    0.92288995  9.5184145   6.6715336   3.3791308
  2.4301429   8.523114    4.619198    5.3705144   0.3660334   5.918427
  6.2292395   7.1572037   9.551745    6.67638     3.2475867   4.406022
  7.1114645   0.65166384  6.9876056   6.6356955   2.3022606   1.2872343
  3.532573    3.8445692   6.0782557   4.5467997   9.491786    0.11065191
  3.0255654   1.2933161   6.592196    2.150623    4.2476006   4.39
  1.805124    1.2216942   7.396178    1.4448237 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 72: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-29 17:17:47.827 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 2983.9954, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 164s - loss: nan - MinusLogProbMetric: 2983.9954 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 164s/epoch - 836ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 0.0001111111111111111.
===========
Generating train data for run 342.
===========
Train data generated in 0.19 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_342/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_342/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_342/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_342
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.465355   7.122426   6.507682  ...  1.2216942  7.396178   1.4448237]
 [ 1.8683796  2.8760538  9.147447  ...  4.980697  -1.1366768  2.8016024]
 [ 5.6175413  8.642618   5.6734285 ... -0.9440901  7.0561414  1.3082856]
 ...
 [ 6.145307   2.8273122  6.191003  ...  2.964365   6.077158   2.5509653]
 [ 6.5595512  2.759307   6.0778823 ...  3.4800506  4.3951674  2.1941729]
 [ 5.586527   7.3216014  6.2487698 ...  1.292631   5.797888   1.2879843]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_292"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_293 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_27 (LogProbL  (None,)                  3291840   
 ayer)                                                           
                                                                 
=================================================================
Total params: 3,291,840
Trainable params: 3,291,840
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_27/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_27'")
self.model: <keras.engine.functional.Functional object at 0x7fb197edbdc0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fb1ce6e25c0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fb1ce6e25c0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fb7fdba3220>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fb83a4c98a0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fb83a4c9e10>, <keras.callbacks.ModelCheckpoint object at 0x7fb83a4c9ed0>, <keras.callbacks.EarlyStopping object at 0x7fb83a4ca140>, <keras.callbacks.ReduceLROnPlateau object at 0x7fb83a4ca170>, <keras.callbacks.TerminateOnNaN object at 0x7fb83a4c9db0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_342/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 342/720 with hyperparameters:
timestamp = 2023-09-29 17:17:56.186113
ndims = 64
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 3291840
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 5.465355    7.122426    6.507682    5.369255    4.1136785   6.196197
  4.259451    8.702779    9.329599    4.155665    7.5761204   5.312057
  5.67009     9.304932    0.8909831   1.4673554  -0.03015512  8.003835
  8.704047    8.985184    9.595006    8.103893    4.62255     8.186032
  0.5885266   6.325719    0.92288995  9.5184145   6.6715336   3.3791308
  2.4301429   8.523114    4.619198    5.3705144   0.3660334   5.918427
  6.2292395   7.1572037   9.551745    6.67638     3.2475867   4.406022
  7.1114645   0.65166384  6.9876056   6.6356955   2.3022606   1.2872343
  3.532573    3.8445692   6.0782557   4.5467997   9.491786    0.11065191
  3.0255654   1.2933161   6.592196    2.150623    4.2476006   4.39
  1.805124    1.2216942   7.396178    1.4448237 ]
Epoch 1/1000
2023-09-29 17:20:50.246 
Epoch 1/1000 
	 loss: 2632.7351, MinusLogProbMetric: 2632.7351, val_loss: 1145.8280, val_MinusLogProbMetric: 1145.8280

Epoch 1: val_loss improved from inf to 1145.82800, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 175s - loss: 2632.7351 - MinusLogProbMetric: 2632.7351 - val_loss: 1145.8280 - val_MinusLogProbMetric: 1145.8280 - lr: 1.1111e-04 - 175s/epoch - 891ms/step
Epoch 2/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 35: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-29 17:21:03.501 
Epoch 2/1000 
	 loss: nan, MinusLogProbMetric: 1179.4333, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 2: val_loss did not improve from 1145.82800
196/196 - 12s - loss: nan - MinusLogProbMetric: 1179.4333 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.1111e-04 - 12s/epoch - 62ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 3.703703703703703e-05.
===========
Generating train data for run 342.
===========
Train data generated in 0.16 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_342/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_342/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_342/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_342
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.465355   7.122426   6.507682  ...  1.2216942  7.396178   1.4448237]
 [ 1.8683796  2.8760538  9.147447  ...  4.980697  -1.1366768  2.8016024]
 [ 5.6175413  8.642618   5.6734285 ... -0.9440901  7.0561414  1.3082856]
 ...
 [ 6.145307   2.8273122  6.191003  ...  2.964365   6.077158   2.5509653]
 [ 6.5595512  2.759307   6.0778823 ...  3.4800506  4.3951674  2.1941729]
 [ 5.586527   7.3216014  6.2487698 ...  1.292631   5.797888   1.2879843]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_303"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_304 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_28 (LogProbL  (None,)                  3291840   
 ayer)                                                           
                                                                 
=================================================================
Total params: 3,291,840
Trainable params: 3,291,840
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_28/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_28'")
self.model: <keras.engine.functional.Functional object at 0x7fb0ff68a710>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fb064346a10>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fb064346a10>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fb1cf115d20>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fb1cf117640>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fb1cf114be0>, <keras.callbacks.ModelCheckpoint object at 0x7fb1cf114dc0>, <keras.callbacks.EarlyStopping object at 0x7fb1cf1149d0>, <keras.callbacks.ReduceLROnPlateau object at 0x7fb1cf116020>, <keras.callbacks.TerminateOnNaN object at 0x7fb1cf116e30>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 342/720 with hyperparameters:
timestamp = 2023-09-29 17:21:16.062584
ndims = 64
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 3291840
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 3.703703703703703e-05...
Train first sample: [ 5.465355    7.122426    6.507682    5.369255    4.1136785   6.196197
  4.259451    8.702779    9.329599    4.155665    7.5761204   5.312057
  5.67009     9.304932    0.8909831   1.4673554  -0.03015512  8.003835
  8.704047    8.985184    9.595006    8.103893    4.62255     8.186032
  0.5885266   6.325719    0.92288995  9.5184145   6.6715336   3.3791308
  2.4301429   8.523114    4.619198    5.3705144   0.3660334   5.918427
  6.2292395   7.1572037   9.551745    6.67638     3.2475867   4.406022
  7.1114645   0.65166384  6.9876056   6.6356955   2.3022606   1.2872343
  3.532573    3.8445692   6.0782557   4.5467997   9.491786    0.11065191
  3.0255654   1.2933161   6.592196    2.150623    4.2476006   4.39
  1.805124    1.2216942   7.396178    1.4448237 ]
Epoch 1/1000
2023-09-29 17:24:17.484 
Epoch 1/1000 
	 loss: 874.2797, MinusLogProbMetric: 874.2797, val_loss: 865.6795, val_MinusLogProbMetric: 865.6795

Epoch 1: val_loss improved from inf to 865.67950, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 182s - loss: 874.2797 - MinusLogProbMetric: 874.2797 - val_loss: 865.6795 - val_MinusLogProbMetric: 865.6795 - lr: 3.7037e-05 - 182s/epoch - 928ms/step
Epoch 2/1000
2023-09-29 17:25:24.779 
Epoch 2/1000 
	 loss: 761.8198, MinusLogProbMetric: 761.8198, val_loss: 644.6166, val_MinusLogProbMetric: 644.6166

Epoch 2: val_loss improved from 865.67950 to 644.61658, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 67s - loss: 761.8198 - MinusLogProbMetric: 761.8198 - val_loss: 644.6166 - val_MinusLogProbMetric: 644.6166 - lr: 3.7037e-05 - 67s/epoch - 343ms/step
Epoch 3/1000
2023-09-29 17:26:27.719 
Epoch 3/1000 
	 loss: 548.0207, MinusLogProbMetric: 548.0207, val_loss: 482.4301, val_MinusLogProbMetric: 482.4301

Epoch 3: val_loss improved from 644.61658 to 482.43015, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 63s - loss: 548.0207 - MinusLogProbMetric: 548.0207 - val_loss: 482.4301 - val_MinusLogProbMetric: 482.4301 - lr: 3.7037e-05 - 63s/epoch - 321ms/step
Epoch 4/1000
2023-09-29 17:27:25.947 
Epoch 4/1000 
	 loss: 486.1716, MinusLogProbMetric: 486.1716, val_loss: 462.9771, val_MinusLogProbMetric: 462.9771

Epoch 4: val_loss improved from 482.43015 to 462.97711, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 58s - loss: 486.1716 - MinusLogProbMetric: 486.1716 - val_loss: 462.9771 - val_MinusLogProbMetric: 462.9771 - lr: 3.7037e-05 - 58s/epoch - 297ms/step
Epoch 5/1000
2023-09-29 17:28:21.642 
Epoch 5/1000 
	 loss: 432.5181, MinusLogProbMetric: 432.5181, val_loss: 418.0260, val_MinusLogProbMetric: 418.0260

Epoch 5: val_loss improved from 462.97711 to 418.02600, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 56s - loss: 432.5181 - MinusLogProbMetric: 432.5181 - val_loss: 418.0260 - val_MinusLogProbMetric: 418.0260 - lr: 3.7037e-05 - 56s/epoch - 284ms/step
Epoch 6/1000
2023-09-29 17:29:15.632 
Epoch 6/1000 
	 loss: 384.6630, MinusLogProbMetric: 384.6630, val_loss: 362.9866, val_MinusLogProbMetric: 362.9866

Epoch 6: val_loss improved from 418.02600 to 362.98663, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 54s - loss: 384.6630 - MinusLogProbMetric: 384.6630 - val_loss: 362.9866 - val_MinusLogProbMetric: 362.9866 - lr: 3.7037e-05 - 54s/epoch - 277ms/step
Epoch 7/1000
2023-09-29 17:30:22.846 
Epoch 7/1000 
	 loss: 337.8554, MinusLogProbMetric: 337.8554, val_loss: 322.7802, val_MinusLogProbMetric: 322.7802

Epoch 7: val_loss improved from 362.98663 to 322.78018, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 67s - loss: 337.8554 - MinusLogProbMetric: 337.8554 - val_loss: 322.7802 - val_MinusLogProbMetric: 322.7802 - lr: 3.7037e-05 - 67s/epoch - 344ms/step
Epoch 8/1000
2023-09-29 17:31:26.999 
Epoch 8/1000 
	 loss: 308.1517, MinusLogProbMetric: 308.1517, val_loss: 304.8425, val_MinusLogProbMetric: 304.8425

Epoch 8: val_loss improved from 322.78018 to 304.84250, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 64s - loss: 308.1517 - MinusLogProbMetric: 308.1517 - val_loss: 304.8425 - val_MinusLogProbMetric: 304.8425 - lr: 3.7037e-05 - 64s/epoch - 324ms/step
Epoch 9/1000
2023-09-29 17:32:31.492 
Epoch 9/1000 
	 loss: 303.4757, MinusLogProbMetric: 303.4757, val_loss: 294.7262, val_MinusLogProbMetric: 294.7262

Epoch 9: val_loss improved from 304.84250 to 294.72617, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 65s - loss: 303.4757 - MinusLogProbMetric: 303.4757 - val_loss: 294.7262 - val_MinusLogProbMetric: 294.7262 - lr: 3.7037e-05 - 65s/epoch - 332ms/step
Epoch 10/1000
2023-09-29 17:33:35.827 
Epoch 10/1000 
	 loss: 277.4359, MinusLogProbMetric: 277.4359, val_loss: 262.9476, val_MinusLogProbMetric: 262.9476

Epoch 10: val_loss improved from 294.72617 to 262.94763, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 64s - loss: 277.4359 - MinusLogProbMetric: 277.4359 - val_loss: 262.9476 - val_MinusLogProbMetric: 262.9476 - lr: 3.7037e-05 - 64s/epoch - 326ms/step
Epoch 11/1000
2023-09-29 17:34:38.333 
Epoch 11/1000 
	 loss: 274.2694, MinusLogProbMetric: 274.2694, val_loss: 258.3496, val_MinusLogProbMetric: 258.3496

Epoch 11: val_loss improved from 262.94763 to 258.34961, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 62s - loss: 274.2694 - MinusLogProbMetric: 274.2694 - val_loss: 258.3496 - val_MinusLogProbMetric: 258.3496 - lr: 3.7037e-05 - 62s/epoch - 318ms/step
Epoch 12/1000
2023-09-29 17:35:42.034 
Epoch 12/1000 
	 loss: 381.6255, MinusLogProbMetric: 381.6255, val_loss: 625.6965, val_MinusLogProbMetric: 625.6965

Epoch 12: val_loss did not improve from 258.34961
196/196 - 63s - loss: 381.6255 - MinusLogProbMetric: 381.6255 - val_loss: 625.6965 - val_MinusLogProbMetric: 625.6965 - lr: 3.7037e-05 - 63s/epoch - 321ms/step
Epoch 13/1000
2023-09-29 17:36:37.917 
Epoch 13/1000 
	 loss: 453.8667, MinusLogProbMetric: 453.8667, val_loss: 421.3715, val_MinusLogProbMetric: 421.3715

Epoch 13: val_loss did not improve from 258.34961
196/196 - 56s - loss: 453.8667 - MinusLogProbMetric: 453.8667 - val_loss: 421.3715 - val_MinusLogProbMetric: 421.3715 - lr: 3.7037e-05 - 56s/epoch - 285ms/step
Epoch 14/1000
2023-09-29 17:37:41.755 
Epoch 14/1000 
	 loss: 469.5022, MinusLogProbMetric: 469.5022, val_loss: 470.4862, val_MinusLogProbMetric: 470.4862

Epoch 14: val_loss did not improve from 258.34961
196/196 - 64s - loss: 469.5022 - MinusLogProbMetric: 469.5022 - val_loss: 470.4862 - val_MinusLogProbMetric: 470.4862 - lr: 3.7037e-05 - 64s/epoch - 326ms/step
Epoch 15/1000
2023-09-29 17:38:45.621 
Epoch 15/1000 
	 loss: 426.0917, MinusLogProbMetric: 426.0917, val_loss: 373.5107, val_MinusLogProbMetric: 373.5107

Epoch 15: val_loss did not improve from 258.34961
196/196 - 64s - loss: 426.0917 - MinusLogProbMetric: 426.0917 - val_loss: 373.5107 - val_MinusLogProbMetric: 373.5107 - lr: 3.7037e-05 - 64s/epoch - 326ms/step
Epoch 16/1000
2023-09-29 17:39:44.570 
Epoch 16/1000 
	 loss: 478.5914, MinusLogProbMetric: 478.5914, val_loss: 382.4322, val_MinusLogProbMetric: 382.4322

Epoch 16: val_loss did not improve from 258.34961
196/196 - 59s - loss: 478.5914 - MinusLogProbMetric: 478.5914 - val_loss: 382.4322 - val_MinusLogProbMetric: 382.4322 - lr: 3.7037e-05 - 59s/epoch - 301ms/step
Epoch 17/1000
2023-09-29 17:40:47.543 
Epoch 17/1000 
	 loss: 370.2854, MinusLogProbMetric: 370.2854, val_loss: 331.7649, val_MinusLogProbMetric: 331.7649

Epoch 17: val_loss did not improve from 258.34961
196/196 - 63s - loss: 370.2854 - MinusLogProbMetric: 370.2854 - val_loss: 331.7649 - val_MinusLogProbMetric: 331.7649 - lr: 3.7037e-05 - 63s/epoch - 321ms/step
Epoch 18/1000
2023-09-29 17:41:51.225 
Epoch 18/1000 
	 loss: 311.4555, MinusLogProbMetric: 311.4555, val_loss: 293.7415, val_MinusLogProbMetric: 293.7415

Epoch 18: val_loss did not improve from 258.34961
196/196 - 64s - loss: 311.4555 - MinusLogProbMetric: 311.4555 - val_loss: 293.7415 - val_MinusLogProbMetric: 293.7415 - lr: 3.7037e-05 - 64s/epoch - 325ms/step
Epoch 19/1000
2023-09-29 17:42:52.351 
Epoch 19/1000 
	 loss: 301.0471, MinusLogProbMetric: 301.0471, val_loss: 351.1194, val_MinusLogProbMetric: 351.1194

Epoch 19: val_loss did not improve from 258.34961
196/196 - 61s - loss: 301.0471 - MinusLogProbMetric: 301.0471 - val_loss: 351.1194 - val_MinusLogProbMetric: 351.1194 - lr: 3.7037e-05 - 61s/epoch - 312ms/step
Epoch 20/1000
2023-09-29 17:43:54.904 
Epoch 20/1000 
	 loss: 292.9559, MinusLogProbMetric: 292.9559, val_loss: 270.4676, val_MinusLogProbMetric: 270.4676

Epoch 20: val_loss did not improve from 258.34961
196/196 - 63s - loss: 292.9559 - MinusLogProbMetric: 292.9559 - val_loss: 270.4676 - val_MinusLogProbMetric: 270.4676 - lr: 3.7037e-05 - 63s/epoch - 319ms/step
Epoch 21/1000
2023-09-29 17:44:53.745 
Epoch 21/1000 
	 loss: 257.7331, MinusLogProbMetric: 257.7331, val_loss: 247.5935, val_MinusLogProbMetric: 247.5935

Epoch 21: val_loss improved from 258.34961 to 247.59354, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 60s - loss: 257.7331 - MinusLogProbMetric: 257.7331 - val_loss: 247.5935 - val_MinusLogProbMetric: 247.5935 - lr: 3.7037e-05 - 60s/epoch - 305ms/step
Epoch 22/1000
2023-09-29 17:45:58.830 
Epoch 22/1000 
	 loss: 239.1694, MinusLogProbMetric: 239.1694, val_loss: 231.1015, val_MinusLogProbMetric: 231.1015

Epoch 22: val_loss improved from 247.59354 to 231.10150, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 65s - loss: 239.1694 - MinusLogProbMetric: 239.1694 - val_loss: 231.1015 - val_MinusLogProbMetric: 231.1015 - lr: 3.7037e-05 - 65s/epoch - 334ms/step
Epoch 23/1000
2023-09-29 17:47:05.482 
Epoch 23/1000 
	 loss: 224.9362, MinusLogProbMetric: 224.9362, val_loss: 221.7590, val_MinusLogProbMetric: 221.7590

Epoch 23: val_loss improved from 231.10150 to 221.75902, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 67s - loss: 224.9362 - MinusLogProbMetric: 224.9362 - val_loss: 221.7590 - val_MinusLogProbMetric: 221.7590 - lr: 3.7037e-05 - 67s/epoch - 340ms/step
Epoch 24/1000
2023-09-29 17:48:13.077 
Epoch 24/1000 
	 loss: 214.6601, MinusLogProbMetric: 214.6601, val_loss: 209.1286, val_MinusLogProbMetric: 209.1286

Epoch 24: val_loss improved from 221.75902 to 209.12863, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 67s - loss: 214.6601 - MinusLogProbMetric: 214.6601 - val_loss: 209.1286 - val_MinusLogProbMetric: 209.1286 - lr: 3.7037e-05 - 67s/epoch - 343ms/step
Epoch 25/1000
2023-09-29 17:49:17.064 
Epoch 25/1000 
	 loss: 204.5379, MinusLogProbMetric: 204.5379, val_loss: 199.8854, val_MinusLogProbMetric: 199.8854

Epoch 25: val_loss improved from 209.12863 to 199.88538, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 64s - loss: 204.5379 - MinusLogProbMetric: 204.5379 - val_loss: 199.8854 - val_MinusLogProbMetric: 199.8854 - lr: 3.7037e-05 - 64s/epoch - 327ms/step
Epoch 26/1000
2023-09-29 17:50:20.202 
Epoch 26/1000 
	 loss: 197.6857, MinusLogProbMetric: 197.6857, val_loss: 193.3873, val_MinusLogProbMetric: 193.3873

Epoch 26: val_loss improved from 199.88538 to 193.38725, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 63s - loss: 197.6857 - MinusLogProbMetric: 197.6857 - val_loss: 193.3873 - val_MinusLogProbMetric: 193.3873 - lr: 3.7037e-05 - 63s/epoch - 321ms/step
Epoch 27/1000
2023-09-29 17:51:23.057 
Epoch 27/1000 
	 loss: 189.8385, MinusLogProbMetric: 189.8385, val_loss: 191.6947, val_MinusLogProbMetric: 191.6947

Epoch 27: val_loss improved from 193.38725 to 191.69466, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 63s - loss: 189.8385 - MinusLogProbMetric: 189.8385 - val_loss: 191.6947 - val_MinusLogProbMetric: 191.6947 - lr: 3.7037e-05 - 63s/epoch - 321ms/step
Epoch 28/1000
2023-09-29 17:52:28.676 
Epoch 28/1000 
	 loss: 185.6880, MinusLogProbMetric: 185.6880, val_loss: 182.5841, val_MinusLogProbMetric: 182.5841

Epoch 28: val_loss improved from 191.69466 to 182.58412, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 66s - loss: 185.6880 - MinusLogProbMetric: 185.6880 - val_loss: 182.5841 - val_MinusLogProbMetric: 182.5841 - lr: 3.7037e-05 - 66s/epoch - 337ms/step
Epoch 29/1000
2023-09-29 17:53:34.750 
Epoch 29/1000 
	 loss: 178.3003, MinusLogProbMetric: 178.3003, val_loss: 176.2342, val_MinusLogProbMetric: 176.2342

Epoch 29: val_loss improved from 182.58412 to 176.23421, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 66s - loss: 178.3003 - MinusLogProbMetric: 178.3003 - val_loss: 176.2342 - val_MinusLogProbMetric: 176.2342 - lr: 3.7037e-05 - 66s/epoch - 335ms/step
Epoch 30/1000
2023-09-29 17:54:42.939 
Epoch 30/1000 
	 loss: 172.7261, MinusLogProbMetric: 172.7261, val_loss: 170.6731, val_MinusLogProbMetric: 170.6731

Epoch 30: val_loss improved from 176.23421 to 170.67308, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 68s - loss: 172.7261 - MinusLogProbMetric: 172.7261 - val_loss: 170.6731 - val_MinusLogProbMetric: 170.6731 - lr: 3.7037e-05 - 68s/epoch - 347ms/step
Epoch 31/1000
2023-09-29 17:55:44.944 
Epoch 31/1000 
	 loss: 168.3586, MinusLogProbMetric: 168.3586, val_loss: 166.6005, val_MinusLogProbMetric: 166.6005

Epoch 31: val_loss improved from 170.67308 to 166.60048, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 63s - loss: 168.3586 - MinusLogProbMetric: 168.3586 - val_loss: 166.6005 - val_MinusLogProbMetric: 166.6005 - lr: 3.7037e-05 - 63s/epoch - 319ms/step
Epoch 32/1000
2023-09-29 17:56:43.604 
Epoch 32/1000 
	 loss: 163.5984, MinusLogProbMetric: 163.5984, val_loss: 162.0792, val_MinusLogProbMetric: 162.0792

Epoch 32: val_loss improved from 166.60048 to 162.07922, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 59s - loss: 163.5984 - MinusLogProbMetric: 163.5984 - val_loss: 162.0792 - val_MinusLogProbMetric: 162.0792 - lr: 3.7037e-05 - 59s/epoch - 300ms/step
Epoch 33/1000
2023-09-29 17:57:40.914 
Epoch 33/1000 
	 loss: 158.7263, MinusLogProbMetric: 158.7263, val_loss: 156.9283, val_MinusLogProbMetric: 156.9283

Epoch 33: val_loss improved from 162.07922 to 156.92834, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 57s - loss: 158.7263 - MinusLogProbMetric: 158.7263 - val_loss: 156.9283 - val_MinusLogProbMetric: 156.9283 - lr: 3.7037e-05 - 57s/epoch - 289ms/step
Epoch 34/1000
2023-09-29 17:58:38.275 
Epoch 34/1000 
	 loss: 154.6832, MinusLogProbMetric: 154.6832, val_loss: 153.1292, val_MinusLogProbMetric: 153.1292

Epoch 34: val_loss improved from 156.92834 to 153.12917, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 57s - loss: 154.6832 - MinusLogProbMetric: 154.6832 - val_loss: 153.1292 - val_MinusLogProbMetric: 153.1292 - lr: 3.7037e-05 - 57s/epoch - 293ms/step
Epoch 35/1000
2023-09-29 17:59:36.153 
Epoch 35/1000 
	 loss: 150.7059, MinusLogProbMetric: 150.7059, val_loss: 149.7374, val_MinusLogProbMetric: 149.7374

Epoch 35: val_loss improved from 153.12917 to 149.73735, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 58s - loss: 150.7059 - MinusLogProbMetric: 150.7059 - val_loss: 149.7374 - val_MinusLogProbMetric: 149.7374 - lr: 3.7037e-05 - 58s/epoch - 295ms/step
Epoch 36/1000
2023-09-29 18:00:35.565 
Epoch 36/1000 
	 loss: 166.8252, MinusLogProbMetric: 166.8252, val_loss: 155.1403, val_MinusLogProbMetric: 155.1403

Epoch 36: val_loss did not improve from 149.73735
196/196 - 59s - loss: 166.8252 - MinusLogProbMetric: 166.8252 - val_loss: 155.1403 - val_MinusLogProbMetric: 155.1403 - lr: 3.7037e-05 - 59s/epoch - 299ms/step
Epoch 37/1000
2023-09-29 18:01:32.507 
Epoch 37/1000 
	 loss: 149.5230, MinusLogProbMetric: 149.5230, val_loss: 146.6521, val_MinusLogProbMetric: 146.6521

Epoch 37: val_loss improved from 149.73735 to 146.65215, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 58s - loss: 149.5230 - MinusLogProbMetric: 149.5230 - val_loss: 146.6521 - val_MinusLogProbMetric: 146.6521 - lr: 3.7037e-05 - 58s/epoch - 295ms/step
Epoch 38/1000
2023-09-29 18:02:28.946 
Epoch 38/1000 
	 loss: 211.1878, MinusLogProbMetric: 211.1878, val_loss: 217.3392, val_MinusLogProbMetric: 217.3392

Epoch 38: val_loss did not improve from 146.65215
196/196 - 56s - loss: 211.1878 - MinusLogProbMetric: 211.1878 - val_loss: 217.3392 - val_MinusLogProbMetric: 217.3392 - lr: 3.7037e-05 - 56s/epoch - 284ms/step
Epoch 39/1000
2023-09-29 18:03:28.491 
Epoch 39/1000 
	 loss: 196.5189, MinusLogProbMetric: 196.5189, val_loss: 182.5307, val_MinusLogProbMetric: 182.5307

Epoch 39: val_loss did not improve from 146.65215
196/196 - 60s - loss: 196.5189 - MinusLogProbMetric: 196.5189 - val_loss: 182.5307 - val_MinusLogProbMetric: 182.5307 - lr: 3.7037e-05 - 60s/epoch - 304ms/step
Epoch 40/1000
2023-09-29 18:04:25.683 
Epoch 40/1000 
	 loss: 174.6156, MinusLogProbMetric: 174.6156, val_loss: 180.6835, val_MinusLogProbMetric: 180.6835

Epoch 40: val_loss did not improve from 146.65215
196/196 - 57s - loss: 174.6156 - MinusLogProbMetric: 174.6156 - val_loss: 180.6835 - val_MinusLogProbMetric: 180.6835 - lr: 3.7037e-05 - 57s/epoch - 292ms/step
Epoch 41/1000
2023-09-29 18:05:21.583 
Epoch 41/1000 
	 loss: 181.9225, MinusLogProbMetric: 181.9225, val_loss: 476.5312, val_MinusLogProbMetric: 476.5312

Epoch 41: val_loss did not improve from 146.65215
196/196 - 56s - loss: 181.9225 - MinusLogProbMetric: 181.9225 - val_loss: 476.5312 - val_MinusLogProbMetric: 476.5312 - lr: 3.7037e-05 - 56s/epoch - 285ms/step
Epoch 42/1000
2023-09-29 18:06:19.369 
Epoch 42/1000 
	 loss: 201.0240, MinusLogProbMetric: 201.0240, val_loss: 166.0457, val_MinusLogProbMetric: 166.0457

Epoch 42: val_loss did not improve from 146.65215
196/196 - 58s - loss: 201.0240 - MinusLogProbMetric: 201.0240 - val_loss: 166.0457 - val_MinusLogProbMetric: 166.0457 - lr: 3.7037e-05 - 58s/epoch - 295ms/step
Epoch 43/1000
2023-09-29 18:07:16.452 
Epoch 43/1000 
	 loss: 198.9391, MinusLogProbMetric: 198.9391, val_loss: 194.2699, val_MinusLogProbMetric: 194.2699

Epoch 43: val_loss did not improve from 146.65215
196/196 - 57s - loss: 198.9391 - MinusLogProbMetric: 198.9391 - val_loss: 194.2699 - val_MinusLogProbMetric: 194.2699 - lr: 3.7037e-05 - 57s/epoch - 291ms/step
Epoch 44/1000
2023-09-29 18:08:12.455 
Epoch 44/1000 
	 loss: 237.2759, MinusLogProbMetric: 237.2759, val_loss: 197.6390, val_MinusLogProbMetric: 197.6390

Epoch 44: val_loss did not improve from 146.65215
196/196 - 56s - loss: 237.2759 - MinusLogProbMetric: 237.2759 - val_loss: 197.6390 - val_MinusLogProbMetric: 197.6390 - lr: 3.7037e-05 - 56s/epoch - 286ms/step
Epoch 45/1000
2023-09-29 18:09:09.574 
Epoch 45/1000 
	 loss: 187.7799, MinusLogProbMetric: 187.7799, val_loss: 174.0053, val_MinusLogProbMetric: 174.0053

Epoch 45: val_loss did not improve from 146.65215
196/196 - 57s - loss: 187.7799 - MinusLogProbMetric: 187.7799 - val_loss: 174.0053 - val_MinusLogProbMetric: 174.0053 - lr: 3.7037e-05 - 57s/epoch - 291ms/step
Epoch 46/1000
2023-09-29 18:10:06.826 
Epoch 46/1000 
	 loss: 167.2314, MinusLogProbMetric: 167.2314, val_loss: 161.4811, val_MinusLogProbMetric: 161.4811

Epoch 46: val_loss did not improve from 146.65215
196/196 - 57s - loss: 167.2314 - MinusLogProbMetric: 167.2314 - val_loss: 161.4811 - val_MinusLogProbMetric: 161.4811 - lr: 3.7037e-05 - 57s/epoch - 292ms/step
Epoch 47/1000
2023-09-29 18:11:03.136 
Epoch 47/1000 
	 loss: 158.9493, MinusLogProbMetric: 158.9493, val_loss: 153.9768, val_MinusLogProbMetric: 153.9768

Epoch 47: val_loss did not improve from 146.65215
196/196 - 56s - loss: 158.9493 - MinusLogProbMetric: 158.9493 - val_loss: 153.9768 - val_MinusLogProbMetric: 153.9768 - lr: 3.7037e-05 - 56s/epoch - 287ms/step
Epoch 48/1000
2023-09-29 18:12:00.039 
Epoch 48/1000 
	 loss: 151.0341, MinusLogProbMetric: 151.0341, val_loss: 148.4343, val_MinusLogProbMetric: 148.4343

Epoch 48: val_loss did not improve from 146.65215
196/196 - 57s - loss: 151.0341 - MinusLogProbMetric: 151.0341 - val_loss: 148.4343 - val_MinusLogProbMetric: 148.4343 - lr: 3.7037e-05 - 57s/epoch - 290ms/step
Epoch 49/1000
2023-09-29 18:12:57.018 
Epoch 49/1000 
	 loss: 145.7362, MinusLogProbMetric: 145.7362, val_loss: 144.9797, val_MinusLogProbMetric: 144.9797

Epoch 49: val_loss improved from 146.65215 to 144.97972, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 58s - loss: 145.7362 - MinusLogProbMetric: 145.7362 - val_loss: 144.9797 - val_MinusLogProbMetric: 144.9797 - lr: 3.7037e-05 - 58s/epoch - 297ms/step
Epoch 50/1000
2023-09-29 18:13:54.583 
Epoch 50/1000 
	 loss: 141.5658, MinusLogProbMetric: 141.5658, val_loss: 138.7775, val_MinusLogProbMetric: 138.7775

Epoch 50: val_loss improved from 144.97972 to 138.77747, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 57s - loss: 141.5658 - MinusLogProbMetric: 141.5658 - val_loss: 138.7775 - val_MinusLogProbMetric: 138.7775 - lr: 3.7037e-05 - 57s/epoch - 291ms/step
Epoch 51/1000
2023-09-29 18:14:51.184 
Epoch 51/1000 
	 loss: 136.8025, MinusLogProbMetric: 136.8025, val_loss: 134.7501, val_MinusLogProbMetric: 134.7501

Epoch 51: val_loss improved from 138.77747 to 134.75014, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 57s - loss: 136.8025 - MinusLogProbMetric: 136.8025 - val_loss: 134.7501 - val_MinusLogProbMetric: 134.7501 - lr: 3.7037e-05 - 57s/epoch - 289ms/step
Epoch 52/1000
2023-09-29 18:15:48.958 
Epoch 52/1000 
	 loss: 133.1878, MinusLogProbMetric: 133.1878, val_loss: 131.5556, val_MinusLogProbMetric: 131.5556

Epoch 52: val_loss improved from 134.75014 to 131.55557, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 58s - loss: 133.1878 - MinusLogProbMetric: 133.1878 - val_loss: 131.5556 - val_MinusLogProbMetric: 131.5556 - lr: 3.7037e-05 - 58s/epoch - 295ms/step
Epoch 53/1000
2023-09-29 18:16:47.394 
Epoch 53/1000 
	 loss: 129.9170, MinusLogProbMetric: 129.9170, val_loss: 128.6255, val_MinusLogProbMetric: 128.6255

Epoch 53: val_loss improved from 131.55557 to 128.62550, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 59s - loss: 129.9170 - MinusLogProbMetric: 129.9170 - val_loss: 128.6255 - val_MinusLogProbMetric: 128.6255 - lr: 3.7037e-05 - 59s/epoch - 299ms/step
Epoch 54/1000
2023-09-29 18:17:44.384 
Epoch 54/1000 
	 loss: 127.2860, MinusLogProbMetric: 127.2860, val_loss: 126.0628, val_MinusLogProbMetric: 126.0628

Epoch 54: val_loss improved from 128.62550 to 126.06276, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 57s - loss: 127.2860 - MinusLogProbMetric: 127.2860 - val_loss: 126.0628 - val_MinusLogProbMetric: 126.0628 - lr: 3.7037e-05 - 57s/epoch - 291ms/step
Epoch 55/1000
2023-09-29 18:18:47.193 
Epoch 55/1000 
	 loss: 125.4518, MinusLogProbMetric: 125.4518, val_loss: 123.5155, val_MinusLogProbMetric: 123.5155

Epoch 55: val_loss improved from 126.06276 to 123.51551, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 63s - loss: 125.4518 - MinusLogProbMetric: 125.4518 - val_loss: 123.5155 - val_MinusLogProbMetric: 123.5155 - lr: 3.7037e-05 - 63s/epoch - 320ms/step
Epoch 56/1000
2023-09-29 18:19:44.936 
Epoch 56/1000 
	 loss: 123.9861, MinusLogProbMetric: 123.9861, val_loss: 123.8696, val_MinusLogProbMetric: 123.8696

Epoch 56: val_loss did not improve from 123.51551
196/196 - 57s - loss: 123.9861 - MinusLogProbMetric: 123.9861 - val_loss: 123.8696 - val_MinusLogProbMetric: 123.8696 - lr: 3.7037e-05 - 57s/epoch - 290ms/step
Epoch 57/1000
2023-09-29 18:20:41.111 
Epoch 57/1000 
	 loss: 121.6438, MinusLogProbMetric: 121.6438, val_loss: 119.4853, val_MinusLogProbMetric: 119.4853

Epoch 57: val_loss improved from 123.51551 to 119.48533, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 57s - loss: 121.6438 - MinusLogProbMetric: 121.6438 - val_loss: 119.4853 - val_MinusLogProbMetric: 119.4853 - lr: 3.7037e-05 - 57s/epoch - 291ms/step
Epoch 58/1000
2023-09-29 18:21:40.786 
Epoch 58/1000 
	 loss: 118.3939, MinusLogProbMetric: 118.3939, val_loss: 117.3406, val_MinusLogProbMetric: 117.3406

Epoch 58: val_loss improved from 119.48533 to 117.34062, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 60s - loss: 118.3939 - MinusLogProbMetric: 118.3939 - val_loss: 117.3406 - val_MinusLogProbMetric: 117.3406 - lr: 3.7037e-05 - 60s/epoch - 306ms/step
Epoch 59/1000
2023-09-29 18:22:40.148 
Epoch 59/1000 
	 loss: 116.7122, MinusLogProbMetric: 116.7122, val_loss: 115.8414, val_MinusLogProbMetric: 115.8414

Epoch 59: val_loss improved from 117.34062 to 115.84143, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 59s - loss: 116.7122 - MinusLogProbMetric: 116.7122 - val_loss: 115.8414 - val_MinusLogProbMetric: 115.8414 - lr: 3.7037e-05 - 59s/epoch - 301ms/step
Epoch 60/1000
2023-09-29 18:23:36.697 
Epoch 60/1000 
	 loss: 118.1759, MinusLogProbMetric: 118.1759, val_loss: 115.7920, val_MinusLogProbMetric: 115.7920

Epoch 60: val_loss improved from 115.84143 to 115.79198, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 56s - loss: 118.1759 - MinusLogProbMetric: 118.1759 - val_loss: 115.7920 - val_MinusLogProbMetric: 115.7920 - lr: 3.7037e-05 - 56s/epoch - 288ms/step
Epoch 61/1000
2023-09-29 18:24:35.104 
Epoch 61/1000 
	 loss: 114.5330, MinusLogProbMetric: 114.5330, val_loss: 113.3138, val_MinusLogProbMetric: 113.3138

Epoch 61: val_loss improved from 115.79198 to 113.31377, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 59s - loss: 114.5330 - MinusLogProbMetric: 114.5330 - val_loss: 113.3138 - val_MinusLogProbMetric: 113.3138 - lr: 3.7037e-05 - 59s/epoch - 300ms/step
Epoch 62/1000
2023-09-29 18:25:37.836 
Epoch 62/1000 
	 loss: 114.5633, MinusLogProbMetric: 114.5633, val_loss: 114.1939, val_MinusLogProbMetric: 114.1939

Epoch 62: val_loss did not improve from 113.31377
196/196 - 62s - loss: 114.5633 - MinusLogProbMetric: 114.5633 - val_loss: 114.1939 - val_MinusLogProbMetric: 114.1939 - lr: 3.7037e-05 - 62s/epoch - 314ms/step
Epoch 63/1000
2023-09-29 18:26:33.646 
Epoch 63/1000 
	 loss: 112.2727, MinusLogProbMetric: 112.2727, val_loss: 109.9064, val_MinusLogProbMetric: 109.9064

Epoch 63: val_loss improved from 113.31377 to 109.90644, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 57s - loss: 112.2727 - MinusLogProbMetric: 112.2727 - val_loss: 109.9064 - val_MinusLogProbMetric: 109.9064 - lr: 3.7037e-05 - 57s/epoch - 289ms/step
Epoch 64/1000
2023-09-29 18:27:31.414 
Epoch 64/1000 
	 loss: 109.4248, MinusLogProbMetric: 109.4248, val_loss: 109.8125, val_MinusLogProbMetric: 109.8125

Epoch 64: val_loss improved from 109.90644 to 109.81252, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 58s - loss: 109.4248 - MinusLogProbMetric: 109.4248 - val_loss: 109.8125 - val_MinusLogProbMetric: 109.8125 - lr: 3.7037e-05 - 58s/epoch - 296ms/step
Epoch 65/1000
2023-09-29 18:28:32.851 
Epoch 65/1000 
	 loss: 107.7823, MinusLogProbMetric: 107.7823, val_loss: 111.7599, val_MinusLogProbMetric: 111.7599

Epoch 65: val_loss did not improve from 109.81252
196/196 - 60s - loss: 107.7823 - MinusLogProbMetric: 107.7823 - val_loss: 111.7599 - val_MinusLogProbMetric: 111.7599 - lr: 3.7037e-05 - 60s/epoch - 308ms/step
Epoch 66/1000
2023-09-29 18:29:30.239 
Epoch 66/1000 
	 loss: 110.7972, MinusLogProbMetric: 110.7972, val_loss: 117.9098, val_MinusLogProbMetric: 117.9098

Epoch 66: val_loss did not improve from 109.81252
196/196 - 57s - loss: 110.7972 - MinusLogProbMetric: 110.7972 - val_loss: 117.9098 - val_MinusLogProbMetric: 117.9098 - lr: 3.7037e-05 - 57s/epoch - 293ms/step
Epoch 67/1000
2023-09-29 18:30:27.007 
Epoch 67/1000 
	 loss: 107.4637, MinusLogProbMetric: 107.4637, val_loss: 104.8700, val_MinusLogProbMetric: 104.8700

Epoch 67: val_loss improved from 109.81252 to 104.87003, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 58s - loss: 107.4637 - MinusLogProbMetric: 107.4637 - val_loss: 104.8700 - val_MinusLogProbMetric: 104.8700 - lr: 3.7037e-05 - 58s/epoch - 294ms/step
Epoch 68/1000
2023-09-29 18:31:32.102 
Epoch 68/1000 
	 loss: 117.8587, MinusLogProbMetric: 117.8587, val_loss: 117.8371, val_MinusLogProbMetric: 117.8371

Epoch 68: val_loss did not improve from 104.87003
196/196 - 64s - loss: 117.8587 - MinusLogProbMetric: 117.8587 - val_loss: 117.8371 - val_MinusLogProbMetric: 117.8371 - lr: 3.7037e-05 - 64s/epoch - 328ms/step
Epoch 69/1000
2023-09-29 18:32:29.089 
Epoch 69/1000 
	 loss: 109.3246, MinusLogProbMetric: 109.3246, val_loss: 105.7801, val_MinusLogProbMetric: 105.7801

Epoch 69: val_loss did not improve from 104.87003
196/196 - 57s - loss: 109.3246 - MinusLogProbMetric: 109.3246 - val_loss: 105.7801 - val_MinusLogProbMetric: 105.7801 - lr: 3.7037e-05 - 57s/epoch - 291ms/step
Epoch 70/1000
2023-09-29 18:33:24.957 
Epoch 70/1000 
	 loss: 104.4431, MinusLogProbMetric: 104.4431, val_loss: 103.1705, val_MinusLogProbMetric: 103.1705

Epoch 70: val_loss improved from 104.87003 to 103.17053, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 57s - loss: 104.4431 - MinusLogProbMetric: 104.4431 - val_loss: 103.1705 - val_MinusLogProbMetric: 103.1705 - lr: 3.7037e-05 - 57s/epoch - 289ms/step
Epoch 71/1000
2023-09-29 18:34:27.099 
Epoch 71/1000 
	 loss: 102.8758, MinusLogProbMetric: 102.8758, val_loss: 101.4941, val_MinusLogProbMetric: 101.4941

Epoch 71: val_loss improved from 103.17053 to 101.49406, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 62s - loss: 102.8758 - MinusLogProbMetric: 102.8758 - val_loss: 101.4941 - val_MinusLogProbMetric: 101.4941 - lr: 3.7037e-05 - 62s/epoch - 319ms/step
Epoch 72/1000
2023-09-29 18:35:27.054 
Epoch 72/1000 
	 loss: 100.7968, MinusLogProbMetric: 100.7968, val_loss: 100.4541, val_MinusLogProbMetric: 100.4541

Epoch 72: val_loss improved from 101.49406 to 100.45412, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 59s - loss: 100.7968 - MinusLogProbMetric: 100.7968 - val_loss: 100.4541 - val_MinusLogProbMetric: 100.4541 - lr: 3.7037e-05 - 59s/epoch - 304ms/step
Epoch 73/1000
2023-09-29 18:36:24.283 
Epoch 73/1000 
	 loss: 105.6557, MinusLogProbMetric: 105.6557, val_loss: 124.9904, val_MinusLogProbMetric: 124.9904

Epoch 73: val_loss did not improve from 100.45412
196/196 - 56s - loss: 105.6557 - MinusLogProbMetric: 105.6557 - val_loss: 124.9904 - val_MinusLogProbMetric: 124.9904 - lr: 3.7037e-05 - 56s/epoch - 288ms/step
Epoch 74/1000
2023-09-29 18:37:23.765 
Epoch 74/1000 
	 loss: 106.3243, MinusLogProbMetric: 106.3243, val_loss: 102.6157, val_MinusLogProbMetric: 102.6157

Epoch 74: val_loss did not improve from 100.45412
196/196 - 59s - loss: 106.3243 - MinusLogProbMetric: 106.3243 - val_loss: 102.6157 - val_MinusLogProbMetric: 102.6157 - lr: 3.7037e-05 - 59s/epoch - 303ms/step
Epoch 75/1000
2023-09-29 18:38:26.410 
Epoch 75/1000 
	 loss: 100.9832, MinusLogProbMetric: 100.9832, val_loss: 99.1625, val_MinusLogProbMetric: 99.1625

Epoch 75: val_loss improved from 100.45412 to 99.16246, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 64s - loss: 100.9832 - MinusLogProbMetric: 100.9832 - val_loss: 99.1625 - val_MinusLogProbMetric: 99.1625 - lr: 3.7037e-05 - 64s/epoch - 324ms/step
Epoch 76/1000
2023-09-29 18:39:27.065 
Epoch 76/1000 
	 loss: 98.7557, MinusLogProbMetric: 98.7557, val_loss: 97.5794, val_MinusLogProbMetric: 97.5794

Epoch 76: val_loss improved from 99.16246 to 97.57941, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 61s - loss: 98.7557 - MinusLogProbMetric: 98.7557 - val_loss: 97.5794 - val_MinusLogProbMetric: 97.5794 - lr: 3.7037e-05 - 61s/epoch - 309ms/step
Epoch 77/1000
2023-09-29 18:40:24.706 
Epoch 77/1000 
	 loss: 98.9165, MinusLogProbMetric: 98.9165, val_loss: 96.6999, val_MinusLogProbMetric: 96.6999

Epoch 77: val_loss improved from 97.57941 to 96.69987, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 58s - loss: 98.9165 - MinusLogProbMetric: 98.9165 - val_loss: 96.6999 - val_MinusLogProbMetric: 96.6999 - lr: 3.7037e-05 - 58s/epoch - 295ms/step
Epoch 78/1000
2023-09-29 18:41:27.152 
Epoch 78/1000 
	 loss: 98.3935, MinusLogProbMetric: 98.3935, val_loss: 96.9185, val_MinusLogProbMetric: 96.9185

Epoch 78: val_loss did not improve from 96.69987
196/196 - 61s - loss: 98.3935 - MinusLogProbMetric: 98.3935 - val_loss: 96.9185 - val_MinusLogProbMetric: 96.9185 - lr: 3.7037e-05 - 61s/epoch - 314ms/step
Epoch 79/1000
2023-09-29 18:42:32.729 
Epoch 79/1000 
	 loss: 95.2881, MinusLogProbMetric: 95.2881, val_loss: 94.6339, val_MinusLogProbMetric: 94.6339

Epoch 79: val_loss improved from 96.69987 to 94.63393, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 67s - loss: 95.2881 - MinusLogProbMetric: 95.2881 - val_loss: 94.6339 - val_MinusLogProbMetric: 94.6339 - lr: 3.7037e-05 - 67s/epoch - 339ms/step
Epoch 80/1000
2023-09-29 18:43:36.660 
Epoch 80/1000 
	 loss: 94.5100, MinusLogProbMetric: 94.5100, val_loss: 94.0682, val_MinusLogProbMetric: 94.0682

Epoch 80: val_loss improved from 94.63393 to 94.06821, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 64s - loss: 94.5100 - MinusLogProbMetric: 94.5100 - val_loss: 94.0682 - val_MinusLogProbMetric: 94.0682 - lr: 3.7037e-05 - 64s/epoch - 326ms/step
Epoch 81/1000
2023-09-29 18:44:36.442 
Epoch 81/1000 
	 loss: 93.1656, MinusLogProbMetric: 93.1656, val_loss: 92.9312, val_MinusLogProbMetric: 92.9312

Epoch 81: val_loss improved from 94.06821 to 92.93120, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 60s - loss: 93.1656 - MinusLogProbMetric: 93.1656 - val_loss: 92.9312 - val_MinusLogProbMetric: 92.9312 - lr: 3.7037e-05 - 60s/epoch - 304ms/step
Epoch 82/1000
2023-09-29 18:45:33.382 
Epoch 82/1000 
	 loss: 94.3021, MinusLogProbMetric: 94.3021, val_loss: 93.3655, val_MinusLogProbMetric: 93.3655

Epoch 82: val_loss did not improve from 92.93120
196/196 - 56s - loss: 94.3021 - MinusLogProbMetric: 94.3021 - val_loss: 93.3655 - val_MinusLogProbMetric: 93.3655 - lr: 3.7037e-05 - 56s/epoch - 286ms/step
Epoch 83/1000
2023-09-29 18:46:37.703 
Epoch 83/1000 
	 loss: 97.1780, MinusLogProbMetric: 97.1780, val_loss: 93.7274, val_MinusLogProbMetric: 93.7274

Epoch 83: val_loss did not improve from 92.93120
196/196 - 64s - loss: 97.1780 - MinusLogProbMetric: 97.1780 - val_loss: 93.7274 - val_MinusLogProbMetric: 93.7274 - lr: 3.7037e-05 - 64s/epoch - 328ms/step
Epoch 84/1000
2023-09-29 18:47:44.429 
Epoch 84/1000 
	 loss: 92.4585, MinusLogProbMetric: 92.4585, val_loss: 91.8826, val_MinusLogProbMetric: 91.8826

Epoch 84: val_loss improved from 92.93120 to 91.88259, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 68s - loss: 92.4585 - MinusLogProbMetric: 92.4585 - val_loss: 91.8826 - val_MinusLogProbMetric: 91.8826 - lr: 3.7037e-05 - 68s/epoch - 346ms/step
Epoch 85/1000
2023-09-29 18:48:50.754 
Epoch 85/1000 
	 loss: 165.9117, MinusLogProbMetric: 165.9117, val_loss: 172.1971, val_MinusLogProbMetric: 172.1971

Epoch 85: val_loss did not improve from 91.88259
196/196 - 65s - loss: 165.9117 - MinusLogProbMetric: 165.9117 - val_loss: 172.1971 - val_MinusLogProbMetric: 172.1971 - lr: 3.7037e-05 - 65s/epoch - 333ms/step
Epoch 86/1000
2023-09-29 18:49:55.208 
Epoch 86/1000 
	 loss: 140.3001, MinusLogProbMetric: 140.3001, val_loss: 121.9045, val_MinusLogProbMetric: 121.9045

Epoch 86: val_loss did not improve from 91.88259
196/196 - 64s - loss: 140.3001 - MinusLogProbMetric: 140.3001 - val_loss: 121.9045 - val_MinusLogProbMetric: 121.9045 - lr: 3.7037e-05 - 64s/epoch - 329ms/step
Epoch 87/1000
2023-09-29 18:51:01.780 
Epoch 87/1000 
	 loss: 115.5318, MinusLogProbMetric: 115.5318, val_loss: 111.5335, val_MinusLogProbMetric: 111.5335

Epoch 87: val_loss did not improve from 91.88259
196/196 - 67s - loss: 115.5318 - MinusLogProbMetric: 115.5318 - val_loss: 111.5335 - val_MinusLogProbMetric: 111.5335 - lr: 3.7037e-05 - 67s/epoch - 340ms/step
Epoch 88/1000
2023-09-29 18:52:08.019 
Epoch 88/1000 
	 loss: 107.8549, MinusLogProbMetric: 107.8549, val_loss: 104.4352, val_MinusLogProbMetric: 104.4352

Epoch 88: val_loss did not improve from 91.88259
196/196 - 66s - loss: 107.8549 - MinusLogProbMetric: 107.8549 - val_loss: 104.4352 - val_MinusLogProbMetric: 104.4352 - lr: 3.7037e-05 - 66s/epoch - 338ms/step
Epoch 89/1000
2023-09-29 18:53:05.670 
Epoch 89/1000 
	 loss: 102.3100, MinusLogProbMetric: 102.3100, val_loss: 100.1917, val_MinusLogProbMetric: 100.1917

Epoch 89: val_loss did not improve from 91.88259
196/196 - 58s - loss: 102.3100 - MinusLogProbMetric: 102.3100 - val_loss: 100.1917 - val_MinusLogProbMetric: 100.1917 - lr: 3.7037e-05 - 58s/epoch - 294ms/step
Epoch 90/1000
2023-09-29 18:54:03.097 
Epoch 90/1000 
	 loss: 98.7149, MinusLogProbMetric: 98.7149, val_loss: 97.0331, val_MinusLogProbMetric: 97.0331

Epoch 90: val_loss did not improve from 91.88259
196/196 - 57s - loss: 98.7149 - MinusLogProbMetric: 98.7149 - val_loss: 97.0331 - val_MinusLogProbMetric: 97.0331 - lr: 3.7037e-05 - 57s/epoch - 293ms/step
Epoch 91/1000
2023-09-29 18:55:08.431 
Epoch 91/1000 
	 loss: 95.2467, MinusLogProbMetric: 95.2467, val_loss: 94.2430, val_MinusLogProbMetric: 94.2430

Epoch 91: val_loss did not improve from 91.88259
196/196 - 65s - loss: 95.2467 - MinusLogProbMetric: 95.2467 - val_loss: 94.2430 - val_MinusLogProbMetric: 94.2430 - lr: 3.7037e-05 - 65s/epoch - 333ms/step
Epoch 92/1000
2023-09-29 18:56:10.839 
Epoch 92/1000 
	 loss: 98.2811, MinusLogProbMetric: 98.2811, val_loss: 96.0159, val_MinusLogProbMetric: 96.0159

Epoch 92: val_loss did not improve from 91.88259
196/196 - 62s - loss: 98.2811 - MinusLogProbMetric: 98.2811 - val_loss: 96.0159 - val_MinusLogProbMetric: 96.0159 - lr: 3.7037e-05 - 62s/epoch - 318ms/step
Epoch 93/1000
2023-09-29 18:57:16.937 
Epoch 93/1000 
	 loss: 92.4772, MinusLogProbMetric: 92.4772, val_loss: 92.6421, val_MinusLogProbMetric: 92.6421

Epoch 93: val_loss did not improve from 91.88259
196/196 - 66s - loss: 92.4772 - MinusLogProbMetric: 92.4772 - val_loss: 92.6421 - val_MinusLogProbMetric: 92.6421 - lr: 3.7037e-05 - 66s/epoch - 337ms/step
Epoch 94/1000
2023-09-29 18:58:24.991 
Epoch 94/1000 
	 loss: 98.9785, MinusLogProbMetric: 98.9785, val_loss: 94.7270, val_MinusLogProbMetric: 94.7270

Epoch 94: val_loss did not improve from 91.88259
196/196 - 68s - loss: 98.9785 - MinusLogProbMetric: 98.9785 - val_loss: 94.7270 - val_MinusLogProbMetric: 94.7270 - lr: 3.7037e-05 - 68s/epoch - 347ms/step
Epoch 95/1000
2023-09-29 18:59:33.493 
Epoch 95/1000 
	 loss: 93.0659, MinusLogProbMetric: 93.0659, val_loss: 90.5789, val_MinusLogProbMetric: 90.5789

Epoch 95: val_loss improved from 91.88259 to 90.57887, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 70s - loss: 93.0659 - MinusLogProbMetric: 93.0659 - val_loss: 90.5789 - val_MinusLogProbMetric: 90.5789 - lr: 3.7037e-05 - 70s/epoch - 357ms/step
Epoch 96/1000
2023-09-29 19:00:43.609 
Epoch 96/1000 
	 loss: 90.5239, MinusLogProbMetric: 90.5239, val_loss: 89.2261, val_MinusLogProbMetric: 89.2261

Epoch 96: val_loss improved from 90.57887 to 89.22608, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 71s - loss: 90.5239 - MinusLogProbMetric: 90.5239 - val_loss: 89.2261 - val_MinusLogProbMetric: 89.2261 - lr: 3.7037e-05 - 71s/epoch - 361ms/step
Epoch 97/1000
2023-09-29 19:01:53.791 
Epoch 97/1000 
	 loss: 93.1560, MinusLogProbMetric: 93.1560, val_loss: 90.5933, val_MinusLogProbMetric: 90.5933

Epoch 97: val_loss did not improve from 89.22608
196/196 - 68s - loss: 93.1560 - MinusLogProbMetric: 93.1560 - val_loss: 90.5933 - val_MinusLogProbMetric: 90.5933 - lr: 3.7037e-05 - 68s/epoch - 347ms/step
Epoch 98/1000
2023-09-29 19:03:01.260 
Epoch 98/1000 
	 loss: 104.2798, MinusLogProbMetric: 104.2798, val_loss: 92.6940, val_MinusLogProbMetric: 92.6940

Epoch 98: val_loss did not improve from 89.22608
196/196 - 67s - loss: 104.2798 - MinusLogProbMetric: 104.2798 - val_loss: 92.6940 - val_MinusLogProbMetric: 92.6940 - lr: 3.7037e-05 - 67s/epoch - 344ms/step
Epoch 99/1000
2023-09-29 19:04:08.835 
Epoch 99/1000 
	 loss: 91.1743, MinusLogProbMetric: 91.1743, val_loss: 89.2163, val_MinusLogProbMetric: 89.2163

Epoch 99: val_loss improved from 89.22608 to 89.21629, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 69s - loss: 91.1743 - MinusLogProbMetric: 91.1743 - val_loss: 89.2163 - val_MinusLogProbMetric: 89.2163 - lr: 3.7037e-05 - 69s/epoch - 351ms/step
Epoch 100/1000
2023-09-29 19:05:17.644 
Epoch 100/1000 
	 loss: 88.5748, MinusLogProbMetric: 88.5748, val_loss: 87.2450, val_MinusLogProbMetric: 87.2450

Epoch 100: val_loss improved from 89.21629 to 87.24503, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 69s - loss: 88.5748 - MinusLogProbMetric: 88.5748 - val_loss: 87.2450 - val_MinusLogProbMetric: 87.2450 - lr: 3.7037e-05 - 69s/epoch - 351ms/step
Epoch 101/1000
2023-09-29 19:06:27.233 
Epoch 101/1000 
	 loss: 86.6870, MinusLogProbMetric: 86.6870, val_loss: 88.2527, val_MinusLogProbMetric: 88.2527

Epoch 101: val_loss did not improve from 87.24503
196/196 - 68s - loss: 86.6870 - MinusLogProbMetric: 86.6870 - val_loss: 88.2527 - val_MinusLogProbMetric: 88.2527 - lr: 3.7037e-05 - 68s/epoch - 349ms/step
Epoch 102/1000
2023-09-29 19:07:35.832 
Epoch 102/1000 
	 loss: 85.3858, MinusLogProbMetric: 85.3858, val_loss: 84.7608, val_MinusLogProbMetric: 84.7608

Epoch 102: val_loss improved from 87.24503 to 84.76076, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 70s - loss: 85.3858 - MinusLogProbMetric: 85.3858 - val_loss: 84.7608 - val_MinusLogProbMetric: 84.7608 - lr: 3.7037e-05 - 70s/epoch - 355ms/step
Epoch 103/1000
2023-09-29 19:08:45.315 
Epoch 103/1000 
	 loss: 84.3618, MinusLogProbMetric: 84.3618, val_loss: 83.3391, val_MinusLogProbMetric: 83.3391

Epoch 103: val_loss improved from 84.76076 to 83.33911, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 70s - loss: 84.3618 - MinusLogProbMetric: 84.3618 - val_loss: 83.3391 - val_MinusLogProbMetric: 83.3391 - lr: 3.7037e-05 - 70s/epoch - 359ms/step
Epoch 104/1000
2023-09-29 19:09:55.331 
Epoch 104/1000 
	 loss: 85.6783, MinusLogProbMetric: 85.6783, val_loss: 83.6173, val_MinusLogProbMetric: 83.6173

Epoch 104: val_loss did not improve from 83.33911
196/196 - 68s - loss: 85.6783 - MinusLogProbMetric: 85.6783 - val_loss: 83.6173 - val_MinusLogProbMetric: 83.6173 - lr: 3.7037e-05 - 68s/epoch - 348ms/step
Epoch 105/1000
2023-09-29 19:11:03.613 
Epoch 105/1000 
	 loss: 82.6236, MinusLogProbMetric: 82.6236, val_loss: 82.1486, val_MinusLogProbMetric: 82.1486

Epoch 105: val_loss improved from 83.33911 to 82.14861, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 70s - loss: 82.6236 - MinusLogProbMetric: 82.6236 - val_loss: 82.1486 - val_MinusLogProbMetric: 82.1486 - lr: 3.7037e-05 - 70s/epoch - 355ms/step
Epoch 106/1000
2023-09-29 19:12:13.317 
Epoch 106/1000 
	 loss: 118.8393, MinusLogProbMetric: 118.8393, val_loss: 369.8024, val_MinusLogProbMetric: 369.8024

Epoch 106: val_loss did not improve from 82.14861
196/196 - 68s - loss: 118.8393 - MinusLogProbMetric: 118.8393 - val_loss: 369.8024 - val_MinusLogProbMetric: 369.8024 - lr: 3.7037e-05 - 68s/epoch - 349ms/step
Epoch 107/1000
2023-09-29 19:13:21.954 
Epoch 107/1000 
	 loss: 188.6157, MinusLogProbMetric: 188.6157, val_loss: 158.7197, val_MinusLogProbMetric: 158.7197

Epoch 107: val_loss did not improve from 82.14861
196/196 - 69s - loss: 188.6157 - MinusLogProbMetric: 188.6157 - val_loss: 158.7197 - val_MinusLogProbMetric: 158.7197 - lr: 3.7037e-05 - 69s/epoch - 350ms/step
Epoch 108/1000
2023-09-29 19:14:30.473 
Epoch 108/1000 
	 loss: 136.7750, MinusLogProbMetric: 136.7750, val_loss: 128.2273, val_MinusLogProbMetric: 128.2273

Epoch 108: val_loss did not improve from 82.14861
196/196 - 69s - loss: 136.7750 - MinusLogProbMetric: 136.7750 - val_loss: 128.2273 - val_MinusLogProbMetric: 128.2273 - lr: 3.7037e-05 - 69s/epoch - 350ms/step
Epoch 109/1000
2023-09-29 19:15:37.872 
Epoch 109/1000 
	 loss: 132.8759, MinusLogProbMetric: 132.8759, val_loss: 122.0797, val_MinusLogProbMetric: 122.0797

Epoch 109: val_loss did not improve from 82.14861
196/196 - 67s - loss: 132.8759 - MinusLogProbMetric: 132.8759 - val_loss: 122.0797 - val_MinusLogProbMetric: 122.0797 - lr: 3.7037e-05 - 67s/epoch - 344ms/step
Epoch 110/1000
2023-09-29 19:16:46.342 
Epoch 110/1000 
	 loss: 118.8869, MinusLogProbMetric: 118.8869, val_loss: 115.0066, val_MinusLogProbMetric: 115.0066

Epoch 110: val_loss did not improve from 82.14861
196/196 - 68s - loss: 118.8869 - MinusLogProbMetric: 118.8869 - val_loss: 115.0066 - val_MinusLogProbMetric: 115.0066 - lr: 3.7037e-05 - 68s/epoch - 349ms/step
Epoch 111/1000
2023-09-29 19:17:55.291 
Epoch 111/1000 
	 loss: 112.2298, MinusLogProbMetric: 112.2298, val_loss: 109.3698, val_MinusLogProbMetric: 109.3698

Epoch 111: val_loss did not improve from 82.14861
196/196 - 69s - loss: 112.2298 - MinusLogProbMetric: 112.2298 - val_loss: 109.3698 - val_MinusLogProbMetric: 109.3698 - lr: 3.7037e-05 - 69s/epoch - 352ms/step
Epoch 112/1000
2023-09-29 19:19:02.668 
Epoch 112/1000 
	 loss: 108.0621, MinusLogProbMetric: 108.0621, val_loss: 107.0435, val_MinusLogProbMetric: 107.0435

Epoch 112: val_loss did not improve from 82.14861
196/196 - 67s - loss: 108.0621 - MinusLogProbMetric: 108.0621 - val_loss: 107.0435 - val_MinusLogProbMetric: 107.0435 - lr: 3.7037e-05 - 67s/epoch - 344ms/step
Epoch 113/1000
2023-09-29 19:20:09.931 
Epoch 113/1000 
	 loss: 110.0868, MinusLogProbMetric: 110.0868, val_loss: 105.5594, val_MinusLogProbMetric: 105.5594

Epoch 113: val_loss did not improve from 82.14861
196/196 - 67s - loss: 110.0868 - MinusLogProbMetric: 110.0868 - val_loss: 105.5594 - val_MinusLogProbMetric: 105.5594 - lr: 3.7037e-05 - 67s/epoch - 343ms/step
Epoch 114/1000
2023-09-29 19:21:17.405 
Epoch 114/1000 
	 loss: 101.8177, MinusLogProbMetric: 101.8177, val_loss: 99.3726, val_MinusLogProbMetric: 99.3726

Epoch 114: val_loss did not improve from 82.14861
196/196 - 67s - loss: 101.8177 - MinusLogProbMetric: 101.8177 - val_loss: 99.3726 - val_MinusLogProbMetric: 99.3726 - lr: 3.7037e-05 - 67s/epoch - 344ms/step
Epoch 115/1000
2023-09-29 19:22:25.073 
Epoch 115/1000 
	 loss: 97.4808, MinusLogProbMetric: 97.4808, val_loss: 98.3382, val_MinusLogProbMetric: 98.3382

Epoch 115: val_loss did not improve from 82.14861
196/196 - 68s - loss: 97.4808 - MinusLogProbMetric: 97.4808 - val_loss: 98.3382 - val_MinusLogProbMetric: 98.3382 - lr: 3.7037e-05 - 68s/epoch - 345ms/step
Epoch 116/1000
2023-09-29 19:23:33.156 
Epoch 116/1000 
	 loss: 96.0385, MinusLogProbMetric: 96.0385, val_loss: 94.1881, val_MinusLogProbMetric: 94.1881

Epoch 116: val_loss did not improve from 82.14861
196/196 - 68s - loss: 96.0385 - MinusLogProbMetric: 96.0385 - val_loss: 94.1881 - val_MinusLogProbMetric: 94.1881 - lr: 3.7037e-05 - 68s/epoch - 347ms/step
Epoch 117/1000
2023-09-29 19:24:40.778 
Epoch 117/1000 
	 loss: 93.3528, MinusLogProbMetric: 93.3528, val_loss: 94.8529, val_MinusLogProbMetric: 94.8529

Epoch 117: val_loss did not improve from 82.14861
196/196 - 68s - loss: 93.3528 - MinusLogProbMetric: 93.3528 - val_loss: 94.8529 - val_MinusLogProbMetric: 94.8529 - lr: 3.7037e-05 - 68s/epoch - 345ms/step
Epoch 118/1000
2023-09-29 19:25:45.251 
Epoch 118/1000 
	 loss: 92.1652, MinusLogProbMetric: 92.1652, val_loss: 91.0285, val_MinusLogProbMetric: 91.0285

Epoch 118: val_loss did not improve from 82.14861
196/196 - 64s - loss: 92.1652 - MinusLogProbMetric: 92.1652 - val_loss: 91.0285 - val_MinusLogProbMetric: 91.0285 - lr: 3.7037e-05 - 64s/epoch - 329ms/step
Epoch 119/1000
2023-09-29 19:26:51.409 
Epoch 119/1000 
	 loss: 90.4693, MinusLogProbMetric: 90.4693, val_loss: 89.3771, val_MinusLogProbMetric: 89.3771

Epoch 119: val_loss did not improve from 82.14861
196/196 - 66s - loss: 90.4693 - MinusLogProbMetric: 90.4693 - val_loss: 89.3771 - val_MinusLogProbMetric: 89.3771 - lr: 3.7037e-05 - 66s/epoch - 338ms/step
Epoch 120/1000
2023-09-29 19:27:56.809 
Epoch 120/1000 
	 loss: 91.6819, MinusLogProbMetric: 91.6819, val_loss: 89.2401, val_MinusLogProbMetric: 89.2401

Epoch 120: val_loss did not improve from 82.14861
196/196 - 65s - loss: 91.6819 - MinusLogProbMetric: 91.6819 - val_loss: 89.2401 - val_MinusLogProbMetric: 89.2401 - lr: 3.7037e-05 - 65s/epoch - 334ms/step
Epoch 121/1000
2023-09-29 19:29:02.312 
Epoch 121/1000 
	 loss: 88.2410, MinusLogProbMetric: 88.2410, val_loss: 87.7698, val_MinusLogProbMetric: 87.7698

Epoch 121: val_loss did not improve from 82.14861
196/196 - 65s - loss: 88.2410 - MinusLogProbMetric: 88.2410 - val_loss: 87.7698 - val_MinusLogProbMetric: 87.7698 - lr: 3.7037e-05 - 65s/epoch - 334ms/step
Epoch 122/1000
2023-09-29 19:30:09.252 
Epoch 122/1000 
	 loss: 86.8990, MinusLogProbMetric: 86.8990, val_loss: 86.4768, val_MinusLogProbMetric: 86.4768

Epoch 122: val_loss did not improve from 82.14861
196/196 - 67s - loss: 86.8990 - MinusLogProbMetric: 86.8990 - val_loss: 86.4768 - val_MinusLogProbMetric: 86.4768 - lr: 3.7037e-05 - 67s/epoch - 342ms/step
Epoch 123/1000
2023-09-29 19:31:12.941 
Epoch 123/1000 
	 loss: 85.7605, MinusLogProbMetric: 85.7605, val_loss: 85.3819, val_MinusLogProbMetric: 85.3819

Epoch 123: val_loss did not improve from 82.14861
196/196 - 64s - loss: 85.7605 - MinusLogProbMetric: 85.7605 - val_loss: 85.3819 - val_MinusLogProbMetric: 85.3819 - lr: 3.7037e-05 - 64s/epoch - 325ms/step
Epoch 124/1000
2023-09-29 19:32:17.701 
Epoch 124/1000 
	 loss: 84.4927, MinusLogProbMetric: 84.4927, val_loss: 84.1308, val_MinusLogProbMetric: 84.1308

Epoch 124: val_loss did not improve from 82.14861
196/196 - 65s - loss: 84.4927 - MinusLogProbMetric: 84.4927 - val_loss: 84.1308 - val_MinusLogProbMetric: 84.1308 - lr: 3.7037e-05 - 65s/epoch - 330ms/step
Epoch 125/1000
2023-09-29 19:33:23.192 
Epoch 125/1000 
	 loss: 83.5101, MinusLogProbMetric: 83.5101, val_loss: 83.1031, val_MinusLogProbMetric: 83.1031

Epoch 125: val_loss did not improve from 82.14861
196/196 - 65s - loss: 83.5101 - MinusLogProbMetric: 83.5101 - val_loss: 83.1031 - val_MinusLogProbMetric: 83.1031 - lr: 3.7037e-05 - 65s/epoch - 334ms/step
Epoch 126/1000
2023-09-29 19:34:31.010 
Epoch 126/1000 
	 loss: 83.6756, MinusLogProbMetric: 83.6756, val_loss: 82.5875, val_MinusLogProbMetric: 82.5875

Epoch 126: val_loss did not improve from 82.14861
196/196 - 68s - loss: 83.6756 - MinusLogProbMetric: 83.6756 - val_loss: 82.5875 - val_MinusLogProbMetric: 82.5875 - lr: 3.7037e-05 - 68s/epoch - 346ms/step
Epoch 127/1000
2023-09-29 19:35:37.867 
Epoch 127/1000 
	 loss: 83.2040, MinusLogProbMetric: 83.2040, val_loss: 82.6672, val_MinusLogProbMetric: 82.6672

Epoch 127: val_loss did not improve from 82.14861
196/196 - 67s - loss: 83.2040 - MinusLogProbMetric: 83.2040 - val_loss: 82.6672 - val_MinusLogProbMetric: 82.6672 - lr: 3.7037e-05 - 67s/epoch - 341ms/step
Epoch 128/1000
2023-09-29 19:36:45.691 
Epoch 128/1000 
	 loss: 81.2680, MinusLogProbMetric: 81.2680, val_loss: 81.2540, val_MinusLogProbMetric: 81.2540

Epoch 128: val_loss improved from 82.14861 to 81.25395, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 69s - loss: 81.2680 - MinusLogProbMetric: 81.2680 - val_loss: 81.2540 - val_MinusLogProbMetric: 81.2540 - lr: 3.7037e-05 - 69s/epoch - 352ms/step
Epoch 129/1000
2023-09-29 19:37:52.182 
Epoch 129/1000 
	 loss: 81.0323, MinusLogProbMetric: 81.0323, val_loss: 81.8610, val_MinusLogProbMetric: 81.8610

Epoch 129: val_loss did not improve from 81.25395
196/196 - 65s - loss: 81.0323 - MinusLogProbMetric: 81.0323 - val_loss: 81.8610 - val_MinusLogProbMetric: 81.8610 - lr: 3.7037e-05 - 65s/epoch - 333ms/step
Epoch 130/1000
2023-09-29 19:39:01.733 
Epoch 130/1000 
	 loss: 80.5454, MinusLogProbMetric: 80.5454, val_loss: 81.0785, val_MinusLogProbMetric: 81.0785

Epoch 130: val_loss improved from 81.25395 to 81.07845, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 70s - loss: 80.5454 - MinusLogProbMetric: 80.5454 - val_loss: 81.0785 - val_MinusLogProbMetric: 81.0785 - lr: 3.7037e-05 - 70s/epoch - 359ms/step
Epoch 131/1000
2023-09-29 19:40:10.902 
Epoch 131/1000 
	 loss: 80.1066, MinusLogProbMetric: 80.1066, val_loss: 78.9709, val_MinusLogProbMetric: 78.9709

Epoch 131: val_loss improved from 81.07845 to 78.97094, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 70s - loss: 80.1066 - MinusLogProbMetric: 80.1066 - val_loss: 78.9709 - val_MinusLogProbMetric: 78.9709 - lr: 3.7037e-05 - 70s/epoch - 355ms/step
Epoch 132/1000
2023-09-29 19:41:18.245 
Epoch 132/1000 
	 loss: 79.2153, MinusLogProbMetric: 79.2153, val_loss: 78.2888, val_MinusLogProbMetric: 78.2888

Epoch 132: val_loss improved from 78.97094 to 78.28878, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 67s - loss: 79.2153 - MinusLogProbMetric: 79.2153 - val_loss: 78.2888 - val_MinusLogProbMetric: 78.2888 - lr: 3.7037e-05 - 67s/epoch - 342ms/step
Epoch 133/1000
2023-09-29 19:42:28.064 
Epoch 133/1000 
	 loss: 77.9776, MinusLogProbMetric: 77.9776, val_loss: 77.6423, val_MinusLogProbMetric: 77.6423

Epoch 133: val_loss improved from 78.28878 to 77.64227, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 70s - loss: 77.9776 - MinusLogProbMetric: 77.9776 - val_loss: 77.6423 - val_MinusLogProbMetric: 77.6423 - lr: 3.7037e-05 - 70s/epoch - 357ms/step
Epoch 134/1000
2023-09-29 19:43:36.889 
Epoch 134/1000 
	 loss: 82.7203, MinusLogProbMetric: 82.7203, val_loss: 96.9930, val_MinusLogProbMetric: 96.9930

Epoch 134: val_loss did not improve from 77.64227
196/196 - 68s - loss: 82.7203 - MinusLogProbMetric: 82.7203 - val_loss: 96.9930 - val_MinusLogProbMetric: 96.9930 - lr: 3.7037e-05 - 68s/epoch - 345ms/step
Epoch 135/1000
2023-09-29 19:44:39.895 
Epoch 135/1000 
	 loss: 79.2988, MinusLogProbMetric: 79.2988, val_loss: 77.8754, val_MinusLogProbMetric: 77.8754

Epoch 135: val_loss did not improve from 77.64227
196/196 - 63s - loss: 79.2988 - MinusLogProbMetric: 79.2988 - val_loss: 77.8754 - val_MinusLogProbMetric: 77.8754 - lr: 3.7037e-05 - 63s/epoch - 321ms/step
Epoch 136/1000
2023-09-29 19:45:42.224 
Epoch 136/1000 
	 loss: 76.5149, MinusLogProbMetric: 76.5149, val_loss: 76.2648, val_MinusLogProbMetric: 76.2648

Epoch 136: val_loss improved from 77.64227 to 76.26485, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 63s - loss: 76.5149 - MinusLogProbMetric: 76.5149 - val_loss: 76.2648 - val_MinusLogProbMetric: 76.2648 - lr: 3.7037e-05 - 63s/epoch - 323ms/step
Epoch 137/1000
2023-09-29 19:46:49.797 
Epoch 137/1000 
	 loss: 75.7830, MinusLogProbMetric: 75.7830, val_loss: 76.0976, val_MinusLogProbMetric: 76.0976

Epoch 137: val_loss improved from 76.26485 to 76.09756, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 68s - loss: 75.7830 - MinusLogProbMetric: 75.7830 - val_loss: 76.0976 - val_MinusLogProbMetric: 76.0976 - lr: 3.7037e-05 - 68s/epoch - 345ms/step
Epoch 138/1000
2023-09-29 19:48:00.194 
Epoch 138/1000 
	 loss: 76.6607, MinusLogProbMetric: 76.6607, val_loss: 75.7129, val_MinusLogProbMetric: 75.7129

Epoch 138: val_loss improved from 76.09756 to 75.71288, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 71s - loss: 76.6607 - MinusLogProbMetric: 76.6607 - val_loss: 75.7129 - val_MinusLogProbMetric: 75.7129 - lr: 3.7037e-05 - 71s/epoch - 360ms/step
Epoch 139/1000
2023-09-29 19:49:06.852 
Epoch 139/1000 
	 loss: 74.8053, MinusLogProbMetric: 74.8053, val_loss: 74.5275, val_MinusLogProbMetric: 74.5275

Epoch 139: val_loss improved from 75.71288 to 74.52755, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 66s - loss: 74.8053 - MinusLogProbMetric: 74.8053 - val_loss: 74.5275 - val_MinusLogProbMetric: 74.5275 - lr: 3.7037e-05 - 66s/epoch - 339ms/step
Epoch 140/1000
2023-09-29 19:50:13.980 
Epoch 140/1000 
	 loss: 76.7077, MinusLogProbMetric: 76.7077, val_loss: 75.0561, val_MinusLogProbMetric: 75.0561

Epoch 140: val_loss did not improve from 74.52755
196/196 - 66s - loss: 76.7077 - MinusLogProbMetric: 76.7077 - val_loss: 75.0561 - val_MinusLogProbMetric: 75.0561 - lr: 3.7037e-05 - 66s/epoch - 337ms/step
Epoch 141/1000
2023-09-29 19:51:20.865 
Epoch 141/1000 
	 loss: 74.0422, MinusLogProbMetric: 74.0422, val_loss: 74.9003, val_MinusLogProbMetric: 74.9003

Epoch 141: val_loss did not improve from 74.52755
196/196 - 67s - loss: 74.0422 - MinusLogProbMetric: 74.0422 - val_loss: 74.9003 - val_MinusLogProbMetric: 74.9003 - lr: 3.7037e-05 - 67s/epoch - 341ms/step
Epoch 142/1000
2023-09-29 19:52:28.767 
Epoch 142/1000 
	 loss: 73.8590, MinusLogProbMetric: 73.8590, val_loss: 74.0089, val_MinusLogProbMetric: 74.0089

Epoch 142: val_loss improved from 74.52755 to 74.00892, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 69s - loss: 73.8590 - MinusLogProbMetric: 73.8590 - val_loss: 74.0089 - val_MinusLogProbMetric: 74.0089 - lr: 3.7037e-05 - 69s/epoch - 352ms/step
Epoch 143/1000
2023-09-29 19:53:36.424 
Epoch 143/1000 
	 loss: 72.8193, MinusLogProbMetric: 72.8193, val_loss: 72.6324, val_MinusLogProbMetric: 72.6324

Epoch 143: val_loss improved from 74.00892 to 72.63241, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 67s - loss: 72.8193 - MinusLogProbMetric: 72.8193 - val_loss: 72.6324 - val_MinusLogProbMetric: 72.6324 - lr: 3.7037e-05 - 67s/epoch - 344ms/step
Epoch 144/1000
2023-09-29 19:54:44.622 
Epoch 144/1000 
	 loss: 72.6638, MinusLogProbMetric: 72.6638, val_loss: 72.1852, val_MinusLogProbMetric: 72.1852

Epoch 144: val_loss improved from 72.63241 to 72.18520, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 68s - loss: 72.6638 - MinusLogProbMetric: 72.6638 - val_loss: 72.1852 - val_MinusLogProbMetric: 72.1852 - lr: 3.7037e-05 - 68s/epoch - 348ms/step
Epoch 145/1000
2023-09-29 19:55:46.057 
Epoch 145/1000 
	 loss: 72.2972, MinusLogProbMetric: 72.2972, val_loss: 71.6263, val_MinusLogProbMetric: 71.6263

Epoch 145: val_loss improved from 72.18520 to 71.62630, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 61s - loss: 72.2972 - MinusLogProbMetric: 72.2972 - val_loss: 71.6263 - val_MinusLogProbMetric: 71.6263 - lr: 3.7037e-05 - 61s/epoch - 313ms/step
Epoch 146/1000
2023-09-29 19:56:49.219 
Epoch 146/1000 
	 loss: 75.8598, MinusLogProbMetric: 75.8598, val_loss: 71.8465, val_MinusLogProbMetric: 71.8465

Epoch 146: val_loss did not improve from 71.62630
196/196 - 62s - loss: 75.8598 - MinusLogProbMetric: 75.8598 - val_loss: 71.8465 - val_MinusLogProbMetric: 71.8465 - lr: 3.7037e-05 - 62s/epoch - 318ms/step
Epoch 147/1000
2023-09-29 19:57:50.205 
Epoch 147/1000 
	 loss: 71.2871, MinusLogProbMetric: 71.2871, val_loss: 71.1353, val_MinusLogProbMetric: 71.1353

Epoch 147: val_loss improved from 71.62630 to 71.13533, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 62s - loss: 71.2871 - MinusLogProbMetric: 71.2871 - val_loss: 71.1353 - val_MinusLogProbMetric: 71.1353 - lr: 3.7037e-05 - 62s/epoch - 317ms/step
Epoch 148/1000
2023-09-29 19:58:53.399 
Epoch 148/1000 
	 loss: 72.7683, MinusLogProbMetric: 72.7683, val_loss: 70.9099, val_MinusLogProbMetric: 70.9099

Epoch 148: val_loss improved from 71.13533 to 70.90988, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 63s - loss: 72.7683 - MinusLogProbMetric: 72.7683 - val_loss: 70.9099 - val_MinusLogProbMetric: 70.9099 - lr: 3.7037e-05 - 63s/epoch - 321ms/step
Epoch 149/1000
2023-09-29 19:59:55.873 
Epoch 149/1000 
	 loss: 70.1381, MinusLogProbMetric: 70.1381, val_loss: 70.1786, val_MinusLogProbMetric: 70.1786

Epoch 149: val_loss improved from 70.90988 to 70.17857, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 63s - loss: 70.1381 - MinusLogProbMetric: 70.1381 - val_loss: 70.1786 - val_MinusLogProbMetric: 70.1786 - lr: 3.7037e-05 - 63s/epoch - 319ms/step
Epoch 150/1000
2023-09-29 20:01:02.148 
Epoch 150/1000 
	 loss: 69.8897, MinusLogProbMetric: 69.8897, val_loss: 69.9208, val_MinusLogProbMetric: 69.9208

Epoch 150: val_loss improved from 70.17857 to 69.92080, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 66s - loss: 69.8897 - MinusLogProbMetric: 69.8897 - val_loss: 69.9208 - val_MinusLogProbMetric: 69.9208 - lr: 3.7037e-05 - 66s/epoch - 338ms/step
Epoch 151/1000
2023-09-29 20:02:05.922 
Epoch 151/1000 
	 loss: 71.6592, MinusLogProbMetric: 71.6592, val_loss: 71.2812, val_MinusLogProbMetric: 71.2812

Epoch 151: val_loss did not improve from 69.92080
196/196 - 63s - loss: 71.6592 - MinusLogProbMetric: 71.6592 - val_loss: 71.2812 - val_MinusLogProbMetric: 71.2812 - lr: 3.7037e-05 - 63s/epoch - 320ms/step
Epoch 152/1000
2023-09-29 20:03:07.744 
Epoch 152/1000 
	 loss: 69.2145, MinusLogProbMetric: 69.2145, val_loss: 69.0614, val_MinusLogProbMetric: 69.0614

Epoch 152: val_loss improved from 69.92080 to 69.06139, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 63s - loss: 69.2145 - MinusLogProbMetric: 69.2145 - val_loss: 69.0614 - val_MinusLogProbMetric: 69.0614 - lr: 3.7037e-05 - 63s/epoch - 321ms/step
Epoch 153/1000
2023-09-29 20:04:11.490 
Epoch 153/1000 
	 loss: 68.7205, MinusLogProbMetric: 68.7205, val_loss: 68.8548, val_MinusLogProbMetric: 68.8548

Epoch 153: val_loss improved from 69.06139 to 68.85483, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 64s - loss: 68.7205 - MinusLogProbMetric: 68.7205 - val_loss: 68.8548 - val_MinusLogProbMetric: 68.8548 - lr: 3.7037e-05 - 64s/epoch - 325ms/step
Epoch 154/1000
2023-09-29 20:05:14.455 
Epoch 154/1000 
	 loss: 74.5907, MinusLogProbMetric: 74.5907, val_loss: 69.9575, val_MinusLogProbMetric: 69.9575

Epoch 154: val_loss did not improve from 68.85483
196/196 - 62s - loss: 74.5907 - MinusLogProbMetric: 74.5907 - val_loss: 69.9575 - val_MinusLogProbMetric: 69.9575 - lr: 3.7037e-05 - 62s/epoch - 316ms/step
Epoch 155/1000
2023-09-29 20:06:15.449 
Epoch 155/1000 
	 loss: 68.4677, MinusLogProbMetric: 68.4677, val_loss: 68.2968, val_MinusLogProbMetric: 68.2968

Epoch 155: val_loss improved from 68.85483 to 68.29679, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 62s - loss: 68.4677 - MinusLogProbMetric: 68.4677 - val_loss: 68.2968 - val_MinusLogProbMetric: 68.2968 - lr: 3.7037e-05 - 62s/epoch - 318ms/step
Epoch 156/1000
2023-09-29 20:07:24.096 
Epoch 156/1000 
	 loss: 69.9424, MinusLogProbMetric: 69.9424, val_loss: 68.6892, val_MinusLogProbMetric: 68.6892

Epoch 156: val_loss did not improve from 68.29679
196/196 - 67s - loss: 69.9424 - MinusLogProbMetric: 69.9424 - val_loss: 68.6892 - val_MinusLogProbMetric: 68.6892 - lr: 3.7037e-05 - 67s/epoch - 344ms/step
Epoch 157/1000
2023-09-29 20:08:24.558 
Epoch 157/1000 
	 loss: 81.0631, MinusLogProbMetric: 81.0631, val_loss: 71.3394, val_MinusLogProbMetric: 71.3394

Epoch 157: val_loss did not improve from 68.29679
196/196 - 60s - loss: 81.0631 - MinusLogProbMetric: 81.0631 - val_loss: 71.3394 - val_MinusLogProbMetric: 71.3394 - lr: 3.7037e-05 - 60s/epoch - 308ms/step
Epoch 158/1000
2023-09-29 20:09:25.099 
Epoch 158/1000 
	 loss: 70.9617, MinusLogProbMetric: 70.9617, val_loss: 69.0550, val_MinusLogProbMetric: 69.0550

Epoch 158: val_loss did not improve from 68.29679
196/196 - 61s - loss: 70.9617 - MinusLogProbMetric: 70.9617 - val_loss: 69.0550 - val_MinusLogProbMetric: 69.0550 - lr: 3.7037e-05 - 61s/epoch - 309ms/step
Epoch 159/1000
2023-09-29 20:10:24.393 
Epoch 159/1000 
	 loss: 68.1671, MinusLogProbMetric: 68.1671, val_loss: 68.1723, val_MinusLogProbMetric: 68.1723

Epoch 159: val_loss improved from 68.29679 to 68.17226, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 60s - loss: 68.1671 - MinusLogProbMetric: 68.1671 - val_loss: 68.1723 - val_MinusLogProbMetric: 68.1723 - lr: 3.7037e-05 - 60s/epoch - 308ms/step
Epoch 160/1000
2023-09-29 20:11:26.326 
Epoch 160/1000 
	 loss: 67.3525, MinusLogProbMetric: 67.3525, val_loss: 67.4642, val_MinusLogProbMetric: 67.4642

Epoch 160: val_loss improved from 68.17226 to 67.46417, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 62s - loss: 67.3525 - MinusLogProbMetric: 67.3525 - val_loss: 67.4642 - val_MinusLogProbMetric: 67.4642 - lr: 3.7037e-05 - 62s/epoch - 316ms/step
Epoch 161/1000
2023-09-29 20:12:28.325 
Epoch 161/1000 
	 loss: 66.8711, MinusLogProbMetric: 66.8711, val_loss: 66.9866, val_MinusLogProbMetric: 66.9866

Epoch 161: val_loss improved from 67.46417 to 66.98663, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 62s - loss: 66.8711 - MinusLogProbMetric: 66.8711 - val_loss: 66.9866 - val_MinusLogProbMetric: 66.9866 - lr: 3.7037e-05 - 62s/epoch - 317ms/step
Epoch 162/1000
2023-09-29 20:13:34.274 
Epoch 162/1000 
	 loss: 67.3188, MinusLogProbMetric: 67.3188, val_loss: 66.6774, val_MinusLogProbMetric: 66.6774

Epoch 162: val_loss improved from 66.98663 to 66.67739, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 66s - loss: 67.3188 - MinusLogProbMetric: 67.3188 - val_loss: 66.6774 - val_MinusLogProbMetric: 66.6774 - lr: 3.7037e-05 - 66s/epoch - 337ms/step
Epoch 163/1000
2023-09-29 20:14:39.350 
Epoch 163/1000 
	 loss: 66.3538, MinusLogProbMetric: 66.3538, val_loss: 68.1849, val_MinusLogProbMetric: 68.1849

Epoch 163: val_loss did not improve from 66.67739
196/196 - 64s - loss: 66.3538 - MinusLogProbMetric: 66.3538 - val_loss: 68.1849 - val_MinusLogProbMetric: 68.1849 - lr: 3.7037e-05 - 64s/epoch - 326ms/step
Epoch 164/1000
2023-09-29 20:15:42.519 
Epoch 164/1000 
	 loss: 65.8623, MinusLogProbMetric: 65.8623, val_loss: 65.8601, val_MinusLogProbMetric: 65.8601

Epoch 164: val_loss improved from 66.67739 to 65.86013, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 65s - loss: 65.8623 - MinusLogProbMetric: 65.8623 - val_loss: 65.8601 - val_MinusLogProbMetric: 65.8601 - lr: 3.7037e-05 - 65s/epoch - 330ms/step
Epoch 165/1000
2023-09-29 20:16:45.778 
Epoch 165/1000 
	 loss: 65.4477, MinusLogProbMetric: 65.4477, val_loss: 65.5393, val_MinusLogProbMetric: 65.5393

Epoch 165: val_loss improved from 65.86013 to 65.53926, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 63s - loss: 65.4477 - MinusLogProbMetric: 65.4477 - val_loss: 65.5393 - val_MinusLogProbMetric: 65.5393 - lr: 3.7037e-05 - 63s/epoch - 320ms/step
Epoch 166/1000
2023-09-29 20:17:47.358 
Epoch 166/1000 
	 loss: 83.5076, MinusLogProbMetric: 83.5076, val_loss: 71.7456, val_MinusLogProbMetric: 71.7456

Epoch 166: val_loss did not improve from 65.53926
196/196 - 61s - loss: 83.5076 - MinusLogProbMetric: 83.5076 - val_loss: 71.7456 - val_MinusLogProbMetric: 71.7456 - lr: 3.7037e-05 - 61s/epoch - 309ms/step
Epoch 167/1000
2023-09-29 20:18:47.121 
Epoch 167/1000 
	 loss: 69.5054, MinusLogProbMetric: 69.5054, val_loss: 69.0419, val_MinusLogProbMetric: 69.0419

Epoch 167: val_loss did not improve from 65.53926
196/196 - 60s - loss: 69.5054 - MinusLogProbMetric: 69.5054 - val_loss: 69.0419 - val_MinusLogProbMetric: 69.0419 - lr: 3.7037e-05 - 60s/epoch - 305ms/step
Epoch 168/1000
2023-09-29 20:19:50.159 
Epoch 168/1000 
	 loss: 70.1624, MinusLogProbMetric: 70.1624, val_loss: 69.3089, val_MinusLogProbMetric: 69.3089

Epoch 168: val_loss did not improve from 65.53926
196/196 - 63s - loss: 70.1624 - MinusLogProbMetric: 70.1624 - val_loss: 69.3089 - val_MinusLogProbMetric: 69.3089 - lr: 3.7037e-05 - 63s/epoch - 322ms/step
Epoch 169/1000
2023-09-29 20:20:54.462 
Epoch 169/1000 
	 loss: 67.6035, MinusLogProbMetric: 67.6035, val_loss: 67.1869, val_MinusLogProbMetric: 67.1869

Epoch 169: val_loss did not improve from 65.53926
196/196 - 64s - loss: 67.6035 - MinusLogProbMetric: 67.6035 - val_loss: 67.1869 - val_MinusLogProbMetric: 67.1869 - lr: 3.7037e-05 - 64s/epoch - 328ms/step
Epoch 170/1000
2023-09-29 20:21:58.861 
Epoch 170/1000 
	 loss: 66.4293, MinusLogProbMetric: 66.4293, val_loss: 66.2161, val_MinusLogProbMetric: 66.2161

Epoch 170: val_loss did not improve from 65.53926
196/196 - 64s - loss: 66.4293 - MinusLogProbMetric: 66.4293 - val_loss: 66.2161 - val_MinusLogProbMetric: 66.2161 - lr: 3.7037e-05 - 64s/epoch - 329ms/step
Epoch 171/1000
2023-09-29 20:22:59.815 
Epoch 171/1000 
	 loss: 65.4152, MinusLogProbMetric: 65.4152, val_loss: 65.5745, val_MinusLogProbMetric: 65.5745

Epoch 171: val_loss did not improve from 65.53926
196/196 - 61s - loss: 65.4152 - MinusLogProbMetric: 65.4152 - val_loss: 65.5745 - val_MinusLogProbMetric: 65.5745 - lr: 3.7037e-05 - 61s/epoch - 311ms/step
Epoch 172/1000
2023-09-29 20:24:00.586 
Epoch 172/1000 
	 loss: 89.5012, MinusLogProbMetric: 89.5012, val_loss: 81.8745, val_MinusLogProbMetric: 81.8745

Epoch 172: val_loss did not improve from 65.53926
196/196 - 61s - loss: 89.5012 - MinusLogProbMetric: 89.5012 - val_loss: 81.8745 - val_MinusLogProbMetric: 81.8745 - lr: 3.7037e-05 - 61s/epoch - 310ms/step
Epoch 173/1000
2023-09-29 20:25:01.856 
Epoch 173/1000 
	 loss: 73.5248, MinusLogProbMetric: 73.5248, val_loss: 69.8134, val_MinusLogProbMetric: 69.8134

Epoch 173: val_loss did not improve from 65.53926
196/196 - 61s - loss: 73.5248 - MinusLogProbMetric: 73.5248 - val_loss: 69.8134 - val_MinusLogProbMetric: 69.8134 - lr: 3.7037e-05 - 61s/epoch - 313ms/step
Epoch 174/1000
2023-09-29 20:26:05.975 
Epoch 174/1000 
	 loss: 68.2626, MinusLogProbMetric: 68.2626, val_loss: 67.7386, val_MinusLogProbMetric: 67.7386

Epoch 174: val_loss did not improve from 65.53926
196/196 - 64s - loss: 68.2626 - MinusLogProbMetric: 68.2626 - val_loss: 67.7386 - val_MinusLogProbMetric: 67.7386 - lr: 3.7037e-05 - 64s/epoch - 327ms/step
Epoch 175/1000
2023-09-29 20:27:08.896 
Epoch 175/1000 
	 loss: 66.8698, MinusLogProbMetric: 66.8698, val_loss: 66.6341, val_MinusLogProbMetric: 66.6341

Epoch 175: val_loss did not improve from 65.53926
196/196 - 63s - loss: 66.8698 - MinusLogProbMetric: 66.8698 - val_loss: 66.6341 - val_MinusLogProbMetric: 66.6341 - lr: 3.7037e-05 - 63s/epoch - 321ms/step
Epoch 176/1000
2023-09-29 20:28:11.137 
Epoch 176/1000 
	 loss: 65.9496, MinusLogProbMetric: 65.9496, val_loss: 65.9887, val_MinusLogProbMetric: 65.9887

Epoch 176: val_loss did not improve from 65.53926
196/196 - 62s - loss: 65.9496 - MinusLogProbMetric: 65.9496 - val_loss: 65.9887 - val_MinusLogProbMetric: 65.9887 - lr: 3.7037e-05 - 62s/epoch - 318ms/step
Epoch 177/1000
2023-09-29 20:29:14.203 
Epoch 177/1000 
	 loss: 65.4864, MinusLogProbMetric: 65.4864, val_loss: 65.7843, val_MinusLogProbMetric: 65.7843

Epoch 177: val_loss did not improve from 65.53926
196/196 - 63s - loss: 65.4864 - MinusLogProbMetric: 65.4864 - val_loss: 65.7843 - val_MinusLogProbMetric: 65.7843 - lr: 3.7037e-05 - 63s/epoch - 322ms/step
Epoch 178/1000
2023-09-29 20:30:18.193 
Epoch 178/1000 
	 loss: 64.8989, MinusLogProbMetric: 64.8989, val_loss: 64.8496, val_MinusLogProbMetric: 64.8496

Epoch 178: val_loss improved from 65.53926 to 64.84961, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 65s - loss: 64.8989 - MinusLogProbMetric: 64.8989 - val_loss: 64.8496 - val_MinusLogProbMetric: 64.8496 - lr: 3.7037e-05 - 65s/epoch - 331ms/step
Epoch 179/1000
2023-09-29 20:31:20.818 
Epoch 179/1000 
	 loss: 64.2389, MinusLogProbMetric: 64.2389, val_loss: 64.3393, val_MinusLogProbMetric: 64.3393

Epoch 179: val_loss improved from 64.84961 to 64.33929, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 63s - loss: 64.2389 - MinusLogProbMetric: 64.2389 - val_loss: 64.3393 - val_MinusLogProbMetric: 64.3393 - lr: 3.7037e-05 - 63s/epoch - 321ms/step
Epoch 180/1000
2023-09-29 20:32:26.733 
Epoch 180/1000 
	 loss: 63.8443, MinusLogProbMetric: 63.8443, val_loss: 64.0847, val_MinusLogProbMetric: 64.0847

Epoch 180: val_loss improved from 64.33929 to 64.08474, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 66s - loss: 63.8443 - MinusLogProbMetric: 63.8443 - val_loss: 64.0847 - val_MinusLogProbMetric: 64.0847 - lr: 3.7037e-05 - 66s/epoch - 336ms/step
Epoch 181/1000
2023-09-29 20:33:28.575 
Epoch 181/1000 
	 loss: 63.6836, MinusLogProbMetric: 63.6836, val_loss: 63.8250, val_MinusLogProbMetric: 63.8250

Epoch 181: val_loss improved from 64.08474 to 63.82500, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 62s - loss: 63.6836 - MinusLogProbMetric: 63.6836 - val_loss: 63.8250 - val_MinusLogProbMetric: 63.8250 - lr: 3.7037e-05 - 62s/epoch - 315ms/step
Epoch 182/1000
2023-09-29 20:34:33.147 
Epoch 182/1000 
	 loss: 74.7382, MinusLogProbMetric: 74.7382, val_loss: 77.9099, val_MinusLogProbMetric: 77.9099

Epoch 182: val_loss did not improve from 63.82500
196/196 - 64s - loss: 74.7382 - MinusLogProbMetric: 74.7382 - val_loss: 77.9099 - val_MinusLogProbMetric: 77.9099 - lr: 3.7037e-05 - 64s/epoch - 324ms/step
Epoch 183/1000
2023-09-29 20:35:34.354 
Epoch 183/1000 
	 loss: 72.4059, MinusLogProbMetric: 72.4059, val_loss: 67.6888, val_MinusLogProbMetric: 67.6888

Epoch 183: val_loss did not improve from 63.82500
196/196 - 61s - loss: 72.4059 - MinusLogProbMetric: 72.4059 - val_loss: 67.6888 - val_MinusLogProbMetric: 67.6888 - lr: 3.7037e-05 - 61s/epoch - 312ms/step
Epoch 184/1000
2023-09-29 20:36:36.148 
Epoch 184/1000 
	 loss: 65.9251, MinusLogProbMetric: 65.9251, val_loss: 65.2780, val_MinusLogProbMetric: 65.2780

Epoch 184: val_loss did not improve from 63.82500
196/196 - 62s - loss: 65.9251 - MinusLogProbMetric: 65.9251 - val_loss: 65.2780 - val_MinusLogProbMetric: 65.2780 - lr: 3.7037e-05 - 62s/epoch - 315ms/step
Epoch 185/1000
2023-09-29 20:37:39.651 
Epoch 185/1000 
	 loss: 64.3295, MinusLogProbMetric: 64.3295, val_loss: 64.1463, val_MinusLogProbMetric: 64.1463

Epoch 185: val_loss did not improve from 63.82500
196/196 - 63s - loss: 64.3295 - MinusLogProbMetric: 64.3295 - val_loss: 64.1463 - val_MinusLogProbMetric: 64.1463 - lr: 3.7037e-05 - 63s/epoch - 324ms/step
Epoch 186/1000
2023-09-29 20:38:45.356 
Epoch 186/1000 
	 loss: 63.8627, MinusLogProbMetric: 63.8627, val_loss: 66.2563, val_MinusLogProbMetric: 66.2563

Epoch 186: val_loss did not improve from 63.82500
196/196 - 66s - loss: 63.8627 - MinusLogProbMetric: 63.8627 - val_loss: 66.2563 - val_MinusLogProbMetric: 66.2563 - lr: 3.7037e-05 - 66s/epoch - 335ms/step
Epoch 187/1000
2023-09-29 20:39:48.651 
Epoch 187/1000 
	 loss: 63.0822, MinusLogProbMetric: 63.0822, val_loss: 62.9995, val_MinusLogProbMetric: 62.9995

Epoch 187: val_loss improved from 63.82500 to 62.99949, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 64s - loss: 63.0822 - MinusLogProbMetric: 63.0822 - val_loss: 62.9995 - val_MinusLogProbMetric: 62.9995 - lr: 3.7037e-05 - 64s/epoch - 328ms/step
Epoch 188/1000
2023-09-29 20:40:53.733 
Epoch 188/1000 
	 loss: 62.3132, MinusLogProbMetric: 62.3132, val_loss: 62.4456, val_MinusLogProbMetric: 62.4456

Epoch 188: val_loss improved from 62.99949 to 62.44564, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 65s - loss: 62.3132 - MinusLogProbMetric: 62.3132 - val_loss: 62.4456 - val_MinusLogProbMetric: 62.4456 - lr: 3.7037e-05 - 65s/epoch - 333ms/step
Epoch 189/1000
2023-09-29 20:42:02.240 
Epoch 189/1000 
	 loss: 61.8469, MinusLogProbMetric: 61.8469, val_loss: 62.0327, val_MinusLogProbMetric: 62.0327

Epoch 189: val_loss improved from 62.44564 to 62.03272, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 69s - loss: 61.8469 - MinusLogProbMetric: 61.8469 - val_loss: 62.0327 - val_MinusLogProbMetric: 62.0327 - lr: 3.7037e-05 - 69s/epoch - 351ms/step
Epoch 190/1000
2023-09-29 20:43:06.681 
Epoch 190/1000 
	 loss: 61.5122, MinusLogProbMetric: 61.5122, val_loss: 65.4755, val_MinusLogProbMetric: 65.4755

Epoch 190: val_loss did not improve from 62.03272
196/196 - 63s - loss: 61.5122 - MinusLogProbMetric: 61.5122 - val_loss: 65.4755 - val_MinusLogProbMetric: 65.4755 - lr: 3.7037e-05 - 63s/epoch - 322ms/step
Epoch 191/1000
2023-09-29 20:44:09.558 
Epoch 191/1000 
	 loss: 61.5382, MinusLogProbMetric: 61.5382, val_loss: 61.3302, val_MinusLogProbMetric: 61.3302

Epoch 191: val_loss improved from 62.03272 to 61.33022, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 64s - loss: 61.5382 - MinusLogProbMetric: 61.5382 - val_loss: 61.3302 - val_MinusLogProbMetric: 61.3302 - lr: 3.7037e-05 - 64s/epoch - 326ms/step
Epoch 192/1000
2023-09-29 20:45:12.134 
Epoch 192/1000 
	 loss: 60.8305, MinusLogProbMetric: 60.8305, val_loss: 61.1295, val_MinusLogProbMetric: 61.1295

Epoch 192: val_loss improved from 61.33022 to 61.12955, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 66s - loss: 60.8305 - MinusLogProbMetric: 60.8305 - val_loss: 61.1295 - val_MinusLogProbMetric: 61.1295 - lr: 3.7037e-05 - 66s/epoch - 337ms/step
Epoch 193/1000
2023-09-29 20:46:18.296 
Epoch 193/1000 
	 loss: 60.5445, MinusLogProbMetric: 60.5445, val_loss: 61.2371, val_MinusLogProbMetric: 61.2371

Epoch 193: val_loss did not improve from 61.12955
196/196 - 62s - loss: 60.5445 - MinusLogProbMetric: 60.5445 - val_loss: 61.2371 - val_MinusLogProbMetric: 61.2371 - lr: 3.7037e-05 - 62s/epoch - 315ms/step
Epoch 194/1000
2023-09-29 20:47:22.058 
Epoch 194/1000 
	 loss: 162.1257, MinusLogProbMetric: 162.1257, val_loss: 332.7919, val_MinusLogProbMetric: 332.7919

Epoch 194: val_loss did not improve from 61.12955
196/196 - 64s - loss: 162.1257 - MinusLogProbMetric: 162.1257 - val_loss: 332.7919 - val_MinusLogProbMetric: 332.7919 - lr: 3.7037e-05 - 64s/epoch - 325ms/step
Epoch 195/1000
2023-09-29 20:48:25.410 
Epoch 195/1000 
	 loss: 217.4019, MinusLogProbMetric: 217.4019, val_loss: 177.6360, val_MinusLogProbMetric: 177.6360

Epoch 195: val_loss did not improve from 61.12955
196/196 - 63s - loss: 217.4019 - MinusLogProbMetric: 217.4019 - val_loss: 177.6360 - val_MinusLogProbMetric: 177.6360 - lr: 3.7037e-05 - 63s/epoch - 323ms/step
Epoch 196/1000
2023-09-29 20:49:26.897 
Epoch 196/1000 
	 loss: 160.9391, MinusLogProbMetric: 160.9391, val_loss: 147.6449, val_MinusLogProbMetric: 147.6449

Epoch 196: val_loss did not improve from 61.12955
196/196 - 61s - loss: 160.9391 - MinusLogProbMetric: 160.9391 - val_loss: 147.6449 - val_MinusLogProbMetric: 147.6449 - lr: 3.7037e-05 - 61s/epoch - 314ms/step
Epoch 197/1000
2023-09-29 20:50:30.556 
Epoch 197/1000 
	 loss: 139.8773, MinusLogProbMetric: 139.8773, val_loss: 132.8779, val_MinusLogProbMetric: 132.8779

Epoch 197: val_loss did not improve from 61.12955
196/196 - 64s - loss: 139.8773 - MinusLogProbMetric: 139.8773 - val_loss: 132.8779 - val_MinusLogProbMetric: 132.8779 - lr: 3.7037e-05 - 64s/epoch - 325ms/step
Epoch 198/1000
2023-09-29 20:51:29.991 
Epoch 198/1000 
	 loss: 128.9157, MinusLogProbMetric: 128.9157, val_loss: 128.0056, val_MinusLogProbMetric: 128.0056

Epoch 198: val_loss did not improve from 61.12955
196/196 - 59s - loss: 128.9157 - MinusLogProbMetric: 128.9157 - val_loss: 128.0056 - val_MinusLogProbMetric: 128.0056 - lr: 3.7037e-05 - 59s/epoch - 303ms/step
Epoch 199/1000
2023-09-29 20:52:34.564 
Epoch 199/1000 
	 loss: 122.4352, MinusLogProbMetric: 122.4352, val_loss: 119.1799, val_MinusLogProbMetric: 119.1799

Epoch 199: val_loss did not improve from 61.12955
196/196 - 65s - loss: 122.4352 - MinusLogProbMetric: 122.4352 - val_loss: 119.1799 - val_MinusLogProbMetric: 119.1799 - lr: 3.7037e-05 - 65s/epoch - 329ms/step
Epoch 200/1000
2023-09-29 20:53:36.379 
Epoch 200/1000 
	 loss: 116.0592, MinusLogProbMetric: 116.0592, val_loss: 113.1481, val_MinusLogProbMetric: 113.1481

Epoch 200: val_loss did not improve from 61.12955
196/196 - 62s - loss: 116.0592 - MinusLogProbMetric: 116.0592 - val_loss: 113.1481 - val_MinusLogProbMetric: 113.1481 - lr: 3.7037e-05 - 62s/epoch - 315ms/step
Epoch 201/1000
2023-09-29 20:54:38.166 
Epoch 201/1000 
	 loss: 109.9880, MinusLogProbMetric: 109.9880, val_loss: 108.3801, val_MinusLogProbMetric: 108.3801

Epoch 201: val_loss did not improve from 61.12955
196/196 - 62s - loss: 109.9880 - MinusLogProbMetric: 109.9880 - val_loss: 108.3801 - val_MinusLogProbMetric: 108.3801 - lr: 3.7037e-05 - 62s/epoch - 315ms/step
Epoch 202/1000
2023-09-29 20:55:40.120 
Epoch 202/1000 
	 loss: 104.8287, MinusLogProbMetric: 104.8287, val_loss: 102.8358, val_MinusLogProbMetric: 102.8358

Epoch 202: val_loss did not improve from 61.12955
196/196 - 62s - loss: 104.8287 - MinusLogProbMetric: 104.8287 - val_loss: 102.8358 - val_MinusLogProbMetric: 102.8358 - lr: 3.7037e-05 - 62s/epoch - 316ms/step
Epoch 203/1000
2023-09-29 20:56:42.443 
Epoch 203/1000 
	 loss: 100.9854, MinusLogProbMetric: 100.9854, val_loss: 99.6146, val_MinusLogProbMetric: 99.6146

Epoch 203: val_loss did not improve from 61.12955
196/196 - 62s - loss: 100.9854 - MinusLogProbMetric: 100.9854 - val_loss: 99.6146 - val_MinusLogProbMetric: 99.6146 - lr: 3.7037e-05 - 62s/epoch - 318ms/step
Epoch 204/1000
2023-09-29 20:57:43.598 
Epoch 204/1000 
	 loss: 98.0650, MinusLogProbMetric: 98.0650, val_loss: 96.9304, val_MinusLogProbMetric: 96.9304

Epoch 204: val_loss did not improve from 61.12955
196/196 - 61s - loss: 98.0650 - MinusLogProbMetric: 98.0650 - val_loss: 96.9304 - val_MinusLogProbMetric: 96.9304 - lr: 3.7037e-05 - 61s/epoch - 312ms/step
Epoch 205/1000
2023-09-29 20:58:47.676 
Epoch 205/1000 
	 loss: 96.1565, MinusLogProbMetric: 96.1565, val_loss: 96.2849, val_MinusLogProbMetric: 96.2849

Epoch 205: val_loss did not improve from 61.12955
196/196 - 64s - loss: 96.1565 - MinusLogProbMetric: 96.1565 - val_loss: 96.2849 - val_MinusLogProbMetric: 96.2849 - lr: 3.7037e-05 - 64s/epoch - 327ms/step
Epoch 206/1000
2023-09-29 20:59:53.640 
Epoch 206/1000 
	 loss: 99.2832, MinusLogProbMetric: 99.2832, val_loss: 94.6707, val_MinusLogProbMetric: 94.6707

Epoch 206: val_loss did not improve from 61.12955
196/196 - 66s - loss: 99.2832 - MinusLogProbMetric: 99.2832 - val_loss: 94.6707 - val_MinusLogProbMetric: 94.6707 - lr: 3.7037e-05 - 66s/epoch - 337ms/step
Epoch 207/1000
2023-09-29 21:01:00.155 
Epoch 207/1000 
	 loss: 92.1330, MinusLogProbMetric: 92.1330, val_loss: 90.3042, val_MinusLogProbMetric: 90.3042

Epoch 207: val_loss did not improve from 61.12955
196/196 - 67s - loss: 92.1330 - MinusLogProbMetric: 92.1330 - val_loss: 90.3042 - val_MinusLogProbMetric: 90.3042 - lr: 3.7037e-05 - 67s/epoch - 339ms/step
Epoch 208/1000
2023-09-29 21:02:02.247 
Epoch 208/1000 
	 loss: 89.2246, MinusLogProbMetric: 89.2246, val_loss: 89.1334, val_MinusLogProbMetric: 89.1334

Epoch 208: val_loss did not improve from 61.12955
196/196 - 62s - loss: 89.2246 - MinusLogProbMetric: 89.2246 - val_loss: 89.1334 - val_MinusLogProbMetric: 89.1334 - lr: 3.7037e-05 - 62s/epoch - 317ms/step
Epoch 209/1000
2023-09-29 21:03:08.225 
Epoch 209/1000 
	 loss: 87.1764, MinusLogProbMetric: 87.1764, val_loss: 86.8528, val_MinusLogProbMetric: 86.8528

Epoch 209: val_loss did not improve from 61.12955
196/196 - 66s - loss: 87.1764 - MinusLogProbMetric: 87.1764 - val_loss: 86.8528 - val_MinusLogProbMetric: 86.8528 - lr: 3.7037e-05 - 66s/epoch - 337ms/step
Epoch 210/1000
2023-09-29 21:04:10.006 
Epoch 210/1000 
	 loss: 85.8210, MinusLogProbMetric: 85.8210, val_loss: 85.2003, val_MinusLogProbMetric: 85.2003

Epoch 210: val_loss did not improve from 61.12955
196/196 - 62s - loss: 85.8210 - MinusLogProbMetric: 85.8210 - val_loss: 85.2003 - val_MinusLogProbMetric: 85.2003 - lr: 3.7037e-05 - 62s/epoch - 315ms/step
Epoch 211/1000
2023-09-29 21:05:16.330 
Epoch 211/1000 
	 loss: 84.1647, MinusLogProbMetric: 84.1647, val_loss: 83.8198, val_MinusLogProbMetric: 83.8198

Epoch 211: val_loss did not improve from 61.12955
196/196 - 66s - loss: 84.1647 - MinusLogProbMetric: 84.1647 - val_loss: 83.8198 - val_MinusLogProbMetric: 83.8198 - lr: 3.7037e-05 - 66s/epoch - 338ms/step
Epoch 212/1000
2023-09-29 21:06:19.508 
Epoch 212/1000 
	 loss: 82.9258, MinusLogProbMetric: 82.9258, val_loss: 82.6985, val_MinusLogProbMetric: 82.6985

Epoch 212: val_loss did not improve from 61.12955
196/196 - 63s - loss: 82.9258 - MinusLogProbMetric: 82.9258 - val_loss: 82.6985 - val_MinusLogProbMetric: 82.6985 - lr: 3.7037e-05 - 63s/epoch - 322ms/step
Epoch 213/1000
2023-09-29 21:07:20.586 
Epoch 213/1000 
	 loss: 81.8998, MinusLogProbMetric: 81.8998, val_loss: 81.6151, val_MinusLogProbMetric: 81.6151

Epoch 213: val_loss did not improve from 61.12955
196/196 - 61s - loss: 81.8998 - MinusLogProbMetric: 81.8998 - val_loss: 81.6151 - val_MinusLogProbMetric: 81.6151 - lr: 3.7037e-05 - 61s/epoch - 312ms/step
Epoch 214/1000
2023-09-29 21:08:23.792 
Epoch 214/1000 
	 loss: 80.6288, MinusLogProbMetric: 80.6288, val_loss: 80.6042, val_MinusLogProbMetric: 80.6042

Epoch 214: val_loss did not improve from 61.12955
196/196 - 63s - loss: 80.6288 - MinusLogProbMetric: 80.6288 - val_loss: 80.6042 - val_MinusLogProbMetric: 80.6042 - lr: 3.7037e-05 - 63s/epoch - 322ms/step
Epoch 215/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 140: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-29 21:09:12.602 
Epoch 215/1000 
	 loss: nan, MinusLogProbMetric: 94.2048, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 215: val_loss did not improve from 61.12955
196/196 - 49s - loss: nan - MinusLogProbMetric: 94.2048 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.7037e-05 - 49s/epoch - 249ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 1.2345679012345677e-05.
===========
Generating train data for run 342.
===========
Train data generated in 0.34 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_342/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_342/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_342/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_342
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.465355   7.122426   6.507682  ...  1.2216942  7.396178   1.4448237]
 [ 1.8683796  2.8760538  9.147447  ...  4.980697  -1.1366768  2.8016024]
 [ 5.6175413  8.642618   5.6734285 ... -0.9440901  7.0561414  1.3082856]
 ...
 [ 6.145307   2.8273122  6.191003  ...  2.964365   6.077158   2.5509653]
 [ 6.5595512  2.759307   6.0778823 ...  3.4800506  4.3951674  2.1941729]
 [ 5.586527   7.3216014  6.2487698 ...  1.292631   5.797888   1.2879843]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_314"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_315 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_29 (LogProbL  (None,)                  3291840   
 ayer)                                                           
                                                                 
=================================================================
Total params: 3,291,840
Trainable params: 3,291,840
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_29/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_29'")
self.model: <keras.engine.functional.Functional object at 0x7fae789bf760>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fb11ca206d0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fb11ca206d0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fae786ab970>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fb03040ca00>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fb03040cf70>, <keras.callbacks.ModelCheckpoint object at 0x7fb03040d030>, <keras.callbacks.EarlyStopping object at 0x7fb03040d2a0>, <keras.callbacks.ReduceLROnPlateau object at 0x7fb03040d2d0>, <keras.callbacks.TerminateOnNaN object at 0x7fb03040cf10>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 342/720 with hyperparameters:
timestamp = 2023-09-29 21:09:23.400394
ndims = 64
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 3291840
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.2345679012345677e-05...
Train first sample: [ 5.465355    7.122426    6.507682    5.369255    4.1136785   6.196197
  4.259451    8.702779    9.329599    4.155665    7.5761204   5.312057
  5.67009     9.304932    0.8909831   1.4673554  -0.03015512  8.003835
  8.704047    8.985184    9.595006    8.103893    4.62255     8.186032
  0.5885266   6.325719    0.92288995  9.5184145   6.6715336   3.3791308
  2.4301429   8.523114    4.619198    5.3705144   0.3660334   5.918427
  6.2292395   7.1572037   9.551745    6.67638     3.2475867   4.406022
  7.1114645   0.65166384  6.9876056   6.6356955   2.3022606   1.2872343
  3.532573    3.8445692   6.0782557   4.5467997   9.491786    0.11065191
  3.0255654   1.2933161   6.592196    2.150623    4.2476006   4.39
  1.805124    1.2216942   7.396178    1.4448237 ]
Epoch 1/1000
2023-09-29 21:12:47.461 
Epoch 1/1000 
	 loss: 73.2919, MinusLogProbMetric: 73.2919, val_loss: 61.6376, val_MinusLogProbMetric: 61.6376

Epoch 1: val_loss improved from inf to 61.63757, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 205s - loss: 73.2919 - MinusLogProbMetric: 73.2919 - val_loss: 61.6376 - val_MinusLogProbMetric: 61.6376 - lr: 1.2346e-05 - 205s/epoch - 1s/step
Epoch 2/1000
2023-09-29 21:13:58.619 
Epoch 2/1000 
	 loss: 60.2393, MinusLogProbMetric: 60.2393, val_loss: 59.7607, val_MinusLogProbMetric: 59.7607

Epoch 2: val_loss improved from 61.63757 to 59.76071, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 71s - loss: 60.2393 - MinusLogProbMetric: 60.2393 - val_loss: 59.7607 - val_MinusLogProbMetric: 59.7607 - lr: 1.2346e-05 - 71s/epoch - 363ms/step
Epoch 3/1000
2023-09-29 21:15:10.276 
Epoch 3/1000 
	 loss: 60.8011, MinusLogProbMetric: 60.8011, val_loss: 58.8956, val_MinusLogProbMetric: 58.8956

Epoch 3: val_loss improved from 59.76071 to 58.89559, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 71s - loss: 60.8011 - MinusLogProbMetric: 60.8011 - val_loss: 58.8956 - val_MinusLogProbMetric: 58.8956 - lr: 1.2346e-05 - 71s/epoch - 364ms/step
Epoch 4/1000
2023-09-29 21:16:20.910 
Epoch 4/1000 
	 loss: 65.4674, MinusLogProbMetric: 65.4674, val_loss: 58.9016, val_MinusLogProbMetric: 58.9016

Epoch 4: val_loss did not improve from 58.89559
196/196 - 70s - loss: 65.4674 - MinusLogProbMetric: 65.4674 - val_loss: 58.9016 - val_MinusLogProbMetric: 58.9016 - lr: 1.2346e-05 - 70s/epoch - 355ms/step
Epoch 5/1000
2023-09-29 21:17:30.678 
Epoch 5/1000 
	 loss: 58.2163, MinusLogProbMetric: 58.2163, val_loss: 61.1357, val_MinusLogProbMetric: 61.1357

Epoch 5: val_loss did not improve from 58.89559
196/196 - 70s - loss: 58.2163 - MinusLogProbMetric: 58.2163 - val_loss: 61.1357 - val_MinusLogProbMetric: 61.1357 - lr: 1.2346e-05 - 70s/epoch - 356ms/step
Epoch 6/1000
2023-09-29 21:18:40.574 
Epoch 6/1000 
	 loss: 110.6572, MinusLogProbMetric: 110.6572, val_loss: 172.4621, val_MinusLogProbMetric: 172.4621

Epoch 6: val_loss did not improve from 58.89559
196/196 - 70s - loss: 110.6572 - MinusLogProbMetric: 110.6572 - val_loss: 172.4621 - val_MinusLogProbMetric: 172.4621 - lr: 1.2346e-05 - 70s/epoch - 357ms/step
Epoch 7/1000
2023-09-29 21:19:50.151 
Epoch 7/1000 
	 loss: 121.3083, MinusLogProbMetric: 121.3083, val_loss: 99.1359, val_MinusLogProbMetric: 99.1359

Epoch 7: val_loss did not improve from 58.89559
196/196 - 70s - loss: 121.3083 - MinusLogProbMetric: 121.3083 - val_loss: 99.1359 - val_MinusLogProbMetric: 99.1359 - lr: 1.2346e-05 - 70s/epoch - 355ms/step
Epoch 8/1000
2023-09-29 21:21:00.039 
Epoch 8/1000 
	 loss: 91.8457, MinusLogProbMetric: 91.8457, val_loss: 85.8461, val_MinusLogProbMetric: 85.8461

Epoch 8: val_loss did not improve from 58.89559
196/196 - 70s - loss: 91.8457 - MinusLogProbMetric: 91.8457 - val_loss: 85.8461 - val_MinusLogProbMetric: 85.8461 - lr: 1.2346e-05 - 70s/epoch - 357ms/step
Epoch 9/1000
2023-09-29 21:22:10.521 
Epoch 9/1000 
	 loss: 83.1732, MinusLogProbMetric: 83.1732, val_loss: 79.1547, val_MinusLogProbMetric: 79.1547

Epoch 9: val_loss did not improve from 58.89559
196/196 - 70s - loss: 83.1732 - MinusLogProbMetric: 83.1732 - val_loss: 79.1547 - val_MinusLogProbMetric: 79.1547 - lr: 1.2346e-05 - 70s/epoch - 360ms/step
Epoch 10/1000
2023-09-29 21:23:19.019 
Epoch 10/1000 
	 loss: 77.8474, MinusLogProbMetric: 77.8474, val_loss: 75.1601, val_MinusLogProbMetric: 75.1601

Epoch 10: val_loss did not improve from 58.89559
196/196 - 68s - loss: 77.8474 - MinusLogProbMetric: 77.8474 - val_loss: 75.1601 - val_MinusLogProbMetric: 75.1601 - lr: 1.2346e-05 - 68s/epoch - 349ms/step
Epoch 11/1000
2023-09-29 21:24:27.733 
Epoch 11/1000 
	 loss: 73.0339, MinusLogProbMetric: 73.0339, val_loss: 71.3544, val_MinusLogProbMetric: 71.3544

Epoch 11: val_loss did not improve from 58.89559
196/196 - 69s - loss: 73.0339 - MinusLogProbMetric: 73.0339 - val_loss: 71.3544 - val_MinusLogProbMetric: 71.3544 - lr: 1.2346e-05 - 69s/epoch - 351ms/step
Epoch 12/1000
2023-09-29 21:25:35.132 
Epoch 12/1000 
	 loss: 71.2844, MinusLogProbMetric: 71.2844, val_loss: 69.7427, val_MinusLogProbMetric: 69.7427

Epoch 12: val_loss did not improve from 58.89559
196/196 - 67s - loss: 71.2844 - MinusLogProbMetric: 71.2844 - val_loss: 69.7427 - val_MinusLogProbMetric: 69.7427 - lr: 1.2346e-05 - 67s/epoch - 344ms/step
Epoch 13/1000
2023-09-29 21:26:40.816 
Epoch 13/1000 
	 loss: 68.4970, MinusLogProbMetric: 68.4970, val_loss: 67.3674, val_MinusLogProbMetric: 67.3674

Epoch 13: val_loss did not improve from 58.89559
196/196 - 66s - loss: 68.4970 - MinusLogProbMetric: 68.4970 - val_loss: 67.3674 - val_MinusLogProbMetric: 67.3674 - lr: 1.2346e-05 - 66s/epoch - 335ms/step
Epoch 14/1000
2023-09-29 21:27:49.194 
Epoch 14/1000 
	 loss: 70.1688, MinusLogProbMetric: 70.1688, val_loss: 66.8818, val_MinusLogProbMetric: 66.8818

Epoch 14: val_loss did not improve from 58.89559
196/196 - 68s - loss: 70.1688 - MinusLogProbMetric: 70.1688 - val_loss: 66.8818 - val_MinusLogProbMetric: 66.8818 - lr: 1.2346e-05 - 68s/epoch - 349ms/step
Epoch 15/1000
2023-09-29 21:28:58.259 
Epoch 15/1000 
	 loss: 65.1004, MinusLogProbMetric: 65.1004, val_loss: 63.8199, val_MinusLogProbMetric: 63.8199

Epoch 15: val_loss did not improve from 58.89559
196/196 - 69s - loss: 65.1004 - MinusLogProbMetric: 65.1004 - val_loss: 63.8199 - val_MinusLogProbMetric: 63.8199 - lr: 1.2346e-05 - 69s/epoch - 352ms/step
Epoch 16/1000
2023-09-29 21:30:08.414 
Epoch 16/1000 
	 loss: 78.6327, MinusLogProbMetric: 78.6327, val_loss: 73.2677, val_MinusLogProbMetric: 73.2677

Epoch 16: val_loss did not improve from 58.89559
196/196 - 70s - loss: 78.6327 - MinusLogProbMetric: 78.6327 - val_loss: 73.2677 - val_MinusLogProbMetric: 73.2677 - lr: 1.2346e-05 - 70s/epoch - 358ms/step
Epoch 17/1000
2023-09-29 21:31:17.003 
Epoch 17/1000 
	 loss: 69.2365, MinusLogProbMetric: 69.2365, val_loss: 66.3076, val_MinusLogProbMetric: 66.3076

Epoch 17: val_loss did not improve from 58.89559
196/196 - 69s - loss: 69.2365 - MinusLogProbMetric: 69.2365 - val_loss: 66.3076 - val_MinusLogProbMetric: 66.3076 - lr: 1.2346e-05 - 69s/epoch - 350ms/step
Epoch 18/1000
2023-09-29 21:32:26.841 
Epoch 18/1000 
	 loss: 84.5179, MinusLogProbMetric: 84.5179, val_loss: 69.2566, val_MinusLogProbMetric: 69.2566

Epoch 18: val_loss did not improve from 58.89559
196/196 - 70s - loss: 84.5179 - MinusLogProbMetric: 84.5179 - val_loss: 69.2566 - val_MinusLogProbMetric: 69.2566 - lr: 1.2346e-05 - 70s/epoch - 356ms/step
Epoch 19/1000
2023-09-29 21:33:36.118 
Epoch 19/1000 
	 loss: 66.6050, MinusLogProbMetric: 66.6050, val_loss: 64.7759, val_MinusLogProbMetric: 64.7759

Epoch 19: val_loss did not improve from 58.89559
196/196 - 69s - loss: 66.6050 - MinusLogProbMetric: 66.6050 - val_loss: 64.7759 - val_MinusLogProbMetric: 64.7759 - lr: 1.2346e-05 - 69s/epoch - 353ms/step
Epoch 20/1000
2023-09-29 21:34:44.541 
Epoch 20/1000 
	 loss: 64.5812, MinusLogProbMetric: 64.5812, val_loss: 62.8685, val_MinusLogProbMetric: 62.8685

Epoch 20: val_loss did not improve from 58.89559
196/196 - 68s - loss: 64.5812 - MinusLogProbMetric: 64.5812 - val_loss: 62.8685 - val_MinusLogProbMetric: 62.8685 - lr: 1.2346e-05 - 68s/epoch - 349ms/step
Epoch 21/1000
2023-09-29 21:35:50.065 
Epoch 21/1000 
	 loss: 96.3595, MinusLogProbMetric: 96.3595, val_loss: 92.5094, val_MinusLogProbMetric: 92.5094

Epoch 21: val_loss did not improve from 58.89559
196/196 - 66s - loss: 96.3595 - MinusLogProbMetric: 96.3595 - val_loss: 92.5094 - val_MinusLogProbMetric: 92.5094 - lr: 1.2346e-05 - 66s/epoch - 334ms/step
Epoch 22/1000
2023-09-29 21:36:55.553 
Epoch 22/1000 
	 loss: 86.5974, MinusLogProbMetric: 86.5974, val_loss: 73.8625, val_MinusLogProbMetric: 73.8625

Epoch 22: val_loss did not improve from 58.89559
196/196 - 65s - loss: 86.5974 - MinusLogProbMetric: 86.5974 - val_loss: 73.8625 - val_MinusLogProbMetric: 73.8625 - lr: 1.2346e-05 - 65s/epoch - 334ms/step
Epoch 23/1000
2023-09-29 21:37:56.698 
Epoch 23/1000 
	 loss: 77.4690, MinusLogProbMetric: 77.4690, val_loss: 68.9487, val_MinusLogProbMetric: 68.9487

Epoch 23: val_loss did not improve from 58.89559
196/196 - 61s - loss: 77.4690 - MinusLogProbMetric: 77.4690 - val_loss: 68.9487 - val_MinusLogProbMetric: 68.9487 - lr: 1.2346e-05 - 61s/epoch - 312ms/step
Epoch 24/1000
2023-09-29 21:38:58.644 
Epoch 24/1000 
	 loss: 66.9954, MinusLogProbMetric: 66.9954, val_loss: 65.5021, val_MinusLogProbMetric: 65.5021

Epoch 24: val_loss did not improve from 58.89559
196/196 - 62s - loss: 66.9954 - MinusLogProbMetric: 66.9954 - val_loss: 65.5021 - val_MinusLogProbMetric: 65.5021 - lr: 1.2346e-05 - 62s/epoch - 316ms/step
Epoch 25/1000
2023-09-29 21:40:04.429 
Epoch 25/1000 
	 loss: 83.9252, MinusLogProbMetric: 83.9252, val_loss: 72.1286, val_MinusLogProbMetric: 72.1286

Epoch 25: val_loss did not improve from 58.89559
196/196 - 66s - loss: 83.9252 - MinusLogProbMetric: 83.9252 - val_loss: 72.1286 - val_MinusLogProbMetric: 72.1286 - lr: 1.2346e-05 - 66s/epoch - 336ms/step
Epoch 26/1000
2023-09-29 21:41:09.796 
Epoch 26/1000 
	 loss: 68.3520, MinusLogProbMetric: 68.3520, val_loss: 65.7179, val_MinusLogProbMetric: 65.7179

Epoch 26: val_loss did not improve from 58.89559
196/196 - 65s - loss: 68.3520 - MinusLogProbMetric: 68.3520 - val_loss: 65.7179 - val_MinusLogProbMetric: 65.7179 - lr: 1.2346e-05 - 65s/epoch - 333ms/step
Epoch 27/1000
2023-09-29 21:42:15.234 
Epoch 27/1000 
	 loss: 63.8331, MinusLogProbMetric: 63.8331, val_loss: 62.5579, val_MinusLogProbMetric: 62.5579

Epoch 27: val_loss did not improve from 58.89559
196/196 - 65s - loss: 63.8331 - MinusLogProbMetric: 63.8331 - val_loss: 62.5579 - val_MinusLogProbMetric: 62.5579 - lr: 1.2346e-05 - 65s/epoch - 334ms/step
Epoch 28/1000
2023-09-29 21:43:17.514 
Epoch 28/1000 
	 loss: 63.3492, MinusLogProbMetric: 63.3492, val_loss: 64.4417, val_MinusLogProbMetric: 64.4417

Epoch 28: val_loss did not improve from 58.89559
196/196 - 62s - loss: 63.3492 - MinusLogProbMetric: 63.3492 - val_loss: 64.4417 - val_MinusLogProbMetric: 64.4417 - lr: 1.2346e-05 - 62s/epoch - 318ms/step
Epoch 29/1000
2023-09-29 21:44:21.983 
Epoch 29/1000 
	 loss: 60.4534, MinusLogProbMetric: 60.4534, val_loss: 60.2747, val_MinusLogProbMetric: 60.2747

Epoch 29: val_loss did not improve from 58.89559
196/196 - 64s - loss: 60.4534 - MinusLogProbMetric: 60.4534 - val_loss: 60.2747 - val_MinusLogProbMetric: 60.2747 - lr: 1.2346e-05 - 64s/epoch - 329ms/step
Epoch 30/1000
2023-09-29 21:45:25.421 
Epoch 30/1000 
	 loss: 58.9187, MinusLogProbMetric: 58.9187, val_loss: 58.3740, val_MinusLogProbMetric: 58.3740

Epoch 30: val_loss improved from 58.89559 to 58.37395, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 64s - loss: 58.9187 - MinusLogProbMetric: 58.9187 - val_loss: 58.3740 - val_MinusLogProbMetric: 58.3740 - lr: 1.2346e-05 - 64s/epoch - 328ms/step
Epoch 31/1000
2023-09-29 21:46:31.403 
Epoch 31/1000 
	 loss: 58.9143, MinusLogProbMetric: 58.9143, val_loss: 57.5014, val_MinusLogProbMetric: 57.5014

Epoch 31: val_loss improved from 58.37395 to 57.50138, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 67s - loss: 58.9143 - MinusLogProbMetric: 58.9143 - val_loss: 57.5014 - val_MinusLogProbMetric: 57.5014 - lr: 1.2346e-05 - 67s/epoch - 340ms/step
Epoch 32/1000
2023-09-29 21:47:33.994 
Epoch 32/1000 
	 loss: 56.8837, MinusLogProbMetric: 56.8837, val_loss: 56.7257, val_MinusLogProbMetric: 56.7257

Epoch 32: val_loss improved from 57.50138 to 56.72567, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 62s - loss: 56.8837 - MinusLogProbMetric: 56.8837 - val_loss: 56.7257 - val_MinusLogProbMetric: 56.7257 - lr: 1.2346e-05 - 62s/epoch - 317ms/step
Epoch 33/1000
2023-09-29 21:48:36.721 
Epoch 33/1000 
	 loss: 56.2535, MinusLogProbMetric: 56.2535, val_loss: 56.0380, val_MinusLogProbMetric: 56.0380

Epoch 33: val_loss improved from 56.72567 to 56.03802, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 63s - loss: 56.2535 - MinusLogProbMetric: 56.2535 - val_loss: 56.0380 - val_MinusLogProbMetric: 56.0380 - lr: 1.2346e-05 - 63s/epoch - 321ms/step
Epoch 34/1000
2023-09-29 21:49:43.188 
Epoch 34/1000 
	 loss: 56.8335, MinusLogProbMetric: 56.8335, val_loss: 55.5868, val_MinusLogProbMetric: 55.5868

Epoch 34: val_loss improved from 56.03802 to 55.58681, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 68s - loss: 56.8335 - MinusLogProbMetric: 56.8335 - val_loss: 55.5868 - val_MinusLogProbMetric: 55.5868 - lr: 1.2346e-05 - 68s/epoch - 345ms/step
Epoch 35/1000
2023-09-29 21:50:49.201 
Epoch 35/1000 
	 loss: 54.9869, MinusLogProbMetric: 54.9869, val_loss: 54.8098, val_MinusLogProbMetric: 54.8098

Epoch 35: val_loss improved from 55.58681 to 54.80976, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 65s - loss: 54.9869 - MinusLogProbMetric: 54.9869 - val_loss: 54.8098 - val_MinusLogProbMetric: 54.8098 - lr: 1.2346e-05 - 65s/epoch - 332ms/step
Epoch 36/1000
2023-09-29 21:51:54.755 
Epoch 36/1000 
	 loss: 54.7635, MinusLogProbMetric: 54.7635, val_loss: 54.7326, val_MinusLogProbMetric: 54.7326

Epoch 36: val_loss improved from 54.80976 to 54.73259, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 65s - loss: 54.7635 - MinusLogProbMetric: 54.7635 - val_loss: 54.7326 - val_MinusLogProbMetric: 54.7326 - lr: 1.2346e-05 - 65s/epoch - 332ms/step
Epoch 37/1000
2023-09-29 21:53:02.983 
Epoch 37/1000 
	 loss: 54.3581, MinusLogProbMetric: 54.3581, val_loss: 54.6969, val_MinusLogProbMetric: 54.6969

Epoch 37: val_loss improved from 54.73259 to 54.69690, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 68s - loss: 54.3581 - MinusLogProbMetric: 54.3581 - val_loss: 54.6969 - val_MinusLogProbMetric: 54.6969 - lr: 1.2346e-05 - 68s/epoch - 348ms/step
Epoch 38/1000
2023-09-29 21:54:07.117 
Epoch 38/1000 
	 loss: 53.7781, MinusLogProbMetric: 53.7781, val_loss: 53.9418, val_MinusLogProbMetric: 53.9418

Epoch 38: val_loss improved from 54.69690 to 53.94183, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 64s - loss: 53.7781 - MinusLogProbMetric: 53.7781 - val_loss: 53.9418 - val_MinusLogProbMetric: 53.9418 - lr: 1.2346e-05 - 64s/epoch - 327ms/step
Epoch 39/1000
2023-09-29 21:55:14.027 
Epoch 39/1000 
	 loss: 60.6721, MinusLogProbMetric: 60.6721, val_loss: 55.2547, val_MinusLogProbMetric: 55.2547

Epoch 39: val_loss did not improve from 53.94183
196/196 - 66s - loss: 60.6721 - MinusLogProbMetric: 60.6721 - val_loss: 55.2547 - val_MinusLogProbMetric: 55.2547 - lr: 1.2346e-05 - 66s/epoch - 337ms/step
Epoch 40/1000
2023-09-29 21:56:19.890 
Epoch 40/1000 
	 loss: 53.8834, MinusLogProbMetric: 53.8834, val_loss: 53.2051, val_MinusLogProbMetric: 53.2051

Epoch 40: val_loss improved from 53.94183 to 53.20506, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 67s - loss: 53.8834 - MinusLogProbMetric: 53.8834 - val_loss: 53.2051 - val_MinusLogProbMetric: 53.2051 - lr: 1.2346e-05 - 67s/epoch - 340ms/step
Epoch 41/1000
2023-09-29 21:57:25.094 
Epoch 41/1000 
	 loss: 53.0008, MinusLogProbMetric: 53.0008, val_loss: 52.8051, val_MinusLogProbMetric: 52.8051

Epoch 41: val_loss improved from 53.20506 to 52.80509, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 66s - loss: 53.0008 - MinusLogProbMetric: 53.0008 - val_loss: 52.8051 - val_MinusLogProbMetric: 52.8051 - lr: 1.2346e-05 - 66s/epoch - 336ms/step
Epoch 42/1000
2023-09-29 21:58:31.372 
Epoch 42/1000 
	 loss: 52.8549, MinusLogProbMetric: 52.8549, val_loss: 52.4547, val_MinusLogProbMetric: 52.4547

Epoch 42: val_loss improved from 52.80509 to 52.45474, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 66s - loss: 52.8549 - MinusLogProbMetric: 52.8549 - val_loss: 52.4547 - val_MinusLogProbMetric: 52.4547 - lr: 1.2346e-05 - 66s/epoch - 335ms/step
Epoch 43/1000
2023-09-29 21:59:37.568 
Epoch 43/1000 
	 loss: 51.9225, MinusLogProbMetric: 51.9225, val_loss: 51.9704, val_MinusLogProbMetric: 51.9704

Epoch 43: val_loss improved from 52.45474 to 51.97036, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 66s - loss: 51.9225 - MinusLogProbMetric: 51.9225 - val_loss: 51.9704 - val_MinusLogProbMetric: 51.9704 - lr: 1.2346e-05 - 66s/epoch - 339ms/step
Epoch 44/1000
2023-09-29 22:00:41.652 
Epoch 44/1000 
	 loss: 51.7513, MinusLogProbMetric: 51.7513, val_loss: 51.6302, val_MinusLogProbMetric: 51.6302

Epoch 44: val_loss improved from 51.97036 to 51.63022, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 64s - loss: 51.7513 - MinusLogProbMetric: 51.7513 - val_loss: 51.6302 - val_MinusLogProbMetric: 51.6302 - lr: 1.2346e-05 - 64s/epoch - 328ms/step
Epoch 45/1000
2023-09-29 22:01:47.056 
Epoch 45/1000 
	 loss: 51.1231, MinusLogProbMetric: 51.1231, val_loss: 51.1497, val_MinusLogProbMetric: 51.1497

Epoch 45: val_loss improved from 51.63022 to 51.14969, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 65s - loss: 51.1231 - MinusLogProbMetric: 51.1231 - val_loss: 51.1497 - val_MinusLogProbMetric: 51.1497 - lr: 1.2346e-05 - 65s/epoch - 334ms/step
Epoch 46/1000
2023-09-29 22:02:54.223 
Epoch 46/1000 
	 loss: 50.6956, MinusLogProbMetric: 50.6956, val_loss: 50.7444, val_MinusLogProbMetric: 50.7444

Epoch 46: val_loss improved from 51.14969 to 50.74443, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 67s - loss: 50.6956 - MinusLogProbMetric: 50.6956 - val_loss: 50.7444 - val_MinusLogProbMetric: 50.7444 - lr: 1.2346e-05 - 67s/epoch - 341ms/step
Epoch 47/1000
2023-09-29 22:04:00.130 
Epoch 47/1000 
	 loss: 50.7514, MinusLogProbMetric: 50.7514, val_loss: 52.1522, val_MinusLogProbMetric: 52.1522

Epoch 47: val_loss did not improve from 50.74443
196/196 - 65s - loss: 50.7514 - MinusLogProbMetric: 50.7514 - val_loss: 52.1522 - val_MinusLogProbMetric: 52.1522 - lr: 1.2346e-05 - 65s/epoch - 331ms/step
Epoch 48/1000
2023-09-29 22:05:03.455 
Epoch 48/1000 
	 loss: 71.0787, MinusLogProbMetric: 71.0787, val_loss: 59.3168, val_MinusLogProbMetric: 59.3168

Epoch 48: val_loss did not improve from 50.74443
196/196 - 63s - loss: 71.0787 - MinusLogProbMetric: 71.0787 - val_loss: 59.3168 - val_MinusLogProbMetric: 59.3168 - lr: 1.2346e-05 - 63s/epoch - 323ms/step
Epoch 49/1000
2023-09-29 22:06:09.394 
Epoch 49/1000 
	 loss: 56.1307, MinusLogProbMetric: 56.1307, val_loss: 54.5772, val_MinusLogProbMetric: 54.5772

Epoch 49: val_loss did not improve from 50.74443
196/196 - 66s - loss: 56.1307 - MinusLogProbMetric: 56.1307 - val_loss: 54.5772 - val_MinusLogProbMetric: 54.5772 - lr: 1.2346e-05 - 66s/epoch - 336ms/step
Epoch 50/1000
2023-09-29 22:07:12.740 
Epoch 50/1000 
	 loss: 54.6469, MinusLogProbMetric: 54.6469, val_loss: 53.9187, val_MinusLogProbMetric: 53.9187

Epoch 50: val_loss did not improve from 50.74443
196/196 - 63s - loss: 54.6469 - MinusLogProbMetric: 54.6469 - val_loss: 53.9187 - val_MinusLogProbMetric: 53.9187 - lr: 1.2346e-05 - 63s/epoch - 323ms/step
Epoch 51/1000
2023-09-29 22:08:21.788 
Epoch 51/1000 
	 loss: 52.5825, MinusLogProbMetric: 52.5825, val_loss: 52.3632, val_MinusLogProbMetric: 52.3632

Epoch 51: val_loss did not improve from 50.74443
196/196 - 69s - loss: 52.5825 - MinusLogProbMetric: 52.5825 - val_loss: 52.3632 - val_MinusLogProbMetric: 52.3632 - lr: 1.2346e-05 - 69s/epoch - 352ms/step
Epoch 52/1000
2023-09-29 22:09:30.409 
Epoch 52/1000 
	 loss: 51.4915, MinusLogProbMetric: 51.4915, val_loss: 51.5069, val_MinusLogProbMetric: 51.5069

Epoch 52: val_loss did not improve from 50.74443
196/196 - 69s - loss: 51.4915 - MinusLogProbMetric: 51.4915 - val_loss: 51.5069 - val_MinusLogProbMetric: 51.5069 - lr: 1.2346e-05 - 69s/epoch - 350ms/step
Epoch 53/1000
2023-09-29 22:10:39.992 
Epoch 53/1000 
	 loss: 50.7400, MinusLogProbMetric: 50.7400, val_loss: 50.7461, val_MinusLogProbMetric: 50.7461

Epoch 53: val_loss did not improve from 50.74443
196/196 - 70s - loss: 50.7400 - MinusLogProbMetric: 50.7400 - val_loss: 50.7461 - val_MinusLogProbMetric: 50.7461 - lr: 1.2346e-05 - 70s/epoch - 355ms/step
Epoch 54/1000
2023-09-29 22:11:50.451 
Epoch 54/1000 
	 loss: 63.5162, MinusLogProbMetric: 63.5162, val_loss: 67.4118, val_MinusLogProbMetric: 67.4118

Epoch 54: val_loss did not improve from 50.74443
196/196 - 70s - loss: 63.5162 - MinusLogProbMetric: 63.5162 - val_loss: 67.4118 - val_MinusLogProbMetric: 67.4118 - lr: 1.2346e-05 - 70s/epoch - 359ms/step
Epoch 55/1000
2023-09-29 22:13:00.243 
Epoch 55/1000 
	 loss: 57.2780, MinusLogProbMetric: 57.2780, val_loss: 53.3408, val_MinusLogProbMetric: 53.3408

Epoch 55: val_loss did not improve from 50.74443
196/196 - 70s - loss: 57.2780 - MinusLogProbMetric: 57.2780 - val_loss: 53.3408 - val_MinusLogProbMetric: 53.3408 - lr: 1.2346e-05 - 70s/epoch - 356ms/step
Epoch 56/1000
2023-09-29 22:14:11.880 
Epoch 56/1000 
	 loss: 52.4037, MinusLogProbMetric: 52.4037, val_loss: 51.9653, val_MinusLogProbMetric: 51.9653

Epoch 56: val_loss did not improve from 50.74443
196/196 - 72s - loss: 52.4037 - MinusLogProbMetric: 52.4037 - val_loss: 51.9653 - val_MinusLogProbMetric: 51.9653 - lr: 1.2346e-05 - 72s/epoch - 365ms/step
Epoch 57/1000
2023-09-29 22:15:22.265 
Epoch 57/1000 
	 loss: 50.8689, MinusLogProbMetric: 50.8689, val_loss: 50.4880, val_MinusLogProbMetric: 50.4880

Epoch 57: val_loss improved from 50.74443 to 50.48805, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 72s - loss: 50.8689 - MinusLogProbMetric: 50.8689 - val_loss: 50.4880 - val_MinusLogProbMetric: 50.4880 - lr: 1.2346e-05 - 72s/epoch - 365ms/step
Epoch 58/1000
2023-09-29 22:16:31.502 
Epoch 58/1000 
	 loss: 52.3418, MinusLogProbMetric: 52.3418, val_loss: 51.6179, val_MinusLogProbMetric: 51.6179

Epoch 58: val_loss did not improve from 50.48805
196/196 - 68s - loss: 52.3418 - MinusLogProbMetric: 52.3418 - val_loss: 51.6179 - val_MinusLogProbMetric: 51.6179 - lr: 1.2346e-05 - 68s/epoch - 347ms/step
Epoch 59/1000
2023-09-29 22:17:40.895 
Epoch 59/1000 
	 loss: 50.6847, MinusLogProbMetric: 50.6847, val_loss: 50.8098, val_MinusLogProbMetric: 50.8098

Epoch 59: val_loss did not improve from 50.48805
196/196 - 69s - loss: 50.6847 - MinusLogProbMetric: 50.6847 - val_loss: 50.8098 - val_MinusLogProbMetric: 50.8098 - lr: 1.2346e-05 - 69s/epoch - 354ms/step
Epoch 60/1000
2023-09-29 22:18:51.129 
Epoch 60/1000 
	 loss: 50.1041, MinusLogProbMetric: 50.1041, val_loss: 49.7753, val_MinusLogProbMetric: 49.7753

Epoch 60: val_loss improved from 50.48805 to 49.77527, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 72s - loss: 50.1041 - MinusLogProbMetric: 50.1041 - val_loss: 49.7753 - val_MinusLogProbMetric: 49.7753 - lr: 1.2346e-05 - 72s/epoch - 369ms/step
Epoch 61/1000
2023-09-29 22:20:04.778 
Epoch 61/1000 
	 loss: 51.5787, MinusLogProbMetric: 51.5787, val_loss: 52.9427, val_MinusLogProbMetric: 52.9427

Epoch 61: val_loss did not improve from 49.77527
196/196 - 72s - loss: 51.5787 - MinusLogProbMetric: 51.5787 - val_loss: 52.9427 - val_MinusLogProbMetric: 52.9427 - lr: 1.2346e-05 - 72s/epoch - 366ms/step
Epoch 62/1000
2023-09-29 22:21:14.393 
Epoch 62/1000 
	 loss: 53.3083, MinusLogProbMetric: 53.3083, val_loss: 83.6179, val_MinusLogProbMetric: 83.6179

Epoch 62: val_loss did not improve from 49.77527
196/196 - 70s - loss: 53.3083 - MinusLogProbMetric: 53.3083 - val_loss: 83.6179 - val_MinusLogProbMetric: 83.6179 - lr: 1.2346e-05 - 70s/epoch - 355ms/step
Epoch 63/1000
2023-09-29 22:22:25.031 
Epoch 63/1000 
	 loss: 58.6689, MinusLogProbMetric: 58.6689, val_loss: 54.3227, val_MinusLogProbMetric: 54.3227

Epoch 63: val_loss did not improve from 49.77527
196/196 - 71s - loss: 58.6689 - MinusLogProbMetric: 58.6689 - val_loss: 54.3227 - val_MinusLogProbMetric: 54.3227 - lr: 1.2346e-05 - 71s/epoch - 360ms/step
Epoch 64/1000
2023-09-29 22:23:34.901 
Epoch 64/1000 
	 loss: 52.9077, MinusLogProbMetric: 52.9077, val_loss: 52.4234, val_MinusLogProbMetric: 52.4234

Epoch 64: val_loss did not improve from 49.77527
196/196 - 70s - loss: 52.9077 - MinusLogProbMetric: 52.9077 - val_loss: 52.4234 - val_MinusLogProbMetric: 52.4234 - lr: 1.2346e-05 - 70s/epoch - 356ms/step
Epoch 65/1000
2023-09-29 22:24:47.510 
Epoch 65/1000 
	 loss: 52.5070, MinusLogProbMetric: 52.5070, val_loss: 51.2684, val_MinusLogProbMetric: 51.2684

Epoch 65: val_loss did not improve from 49.77527
196/196 - 73s - loss: 52.5070 - MinusLogProbMetric: 52.5070 - val_loss: 51.2684 - val_MinusLogProbMetric: 51.2684 - lr: 1.2346e-05 - 73s/epoch - 370ms/step
Epoch 66/1000
2023-09-29 22:25:55.484 
Epoch 66/1000 
	 loss: 55.2264, MinusLogProbMetric: 55.2264, val_loss: 52.6436, val_MinusLogProbMetric: 52.6436

Epoch 66: val_loss did not improve from 49.77527
196/196 - 68s - loss: 55.2264 - MinusLogProbMetric: 55.2264 - val_loss: 52.6436 - val_MinusLogProbMetric: 52.6436 - lr: 1.2346e-05 - 68s/epoch - 347ms/step
Epoch 67/1000
2023-09-29 22:27:03.279 
Epoch 67/1000 
	 loss: 51.1080, MinusLogProbMetric: 51.1080, val_loss: 50.8266, val_MinusLogProbMetric: 50.8266

Epoch 67: val_loss did not improve from 49.77527
196/196 - 68s - loss: 51.1080 - MinusLogProbMetric: 51.1080 - val_loss: 50.8266 - val_MinusLogProbMetric: 50.8266 - lr: 1.2346e-05 - 68s/epoch - 346ms/step
Epoch 68/1000
2023-09-29 22:28:14.557 
Epoch 68/1000 
	 loss: 51.2419, MinusLogProbMetric: 51.2419, val_loss: 50.7254, val_MinusLogProbMetric: 50.7254

Epoch 68: val_loss did not improve from 49.77527
196/196 - 71s - loss: 51.2419 - MinusLogProbMetric: 51.2419 - val_loss: 50.7254 - val_MinusLogProbMetric: 50.7254 - lr: 1.2346e-05 - 71s/epoch - 364ms/step
Epoch 69/1000
2023-09-29 22:29:21.164 
Epoch 69/1000 
	 loss: 49.4795, MinusLogProbMetric: 49.4795, val_loss: 49.4346, val_MinusLogProbMetric: 49.4346

Epoch 69: val_loss improved from 49.77527 to 49.43460, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 68s - loss: 49.4795 - MinusLogProbMetric: 49.4795 - val_loss: 49.4346 - val_MinusLogProbMetric: 49.4346 - lr: 1.2346e-05 - 68s/epoch - 345ms/step
Epoch 70/1000
2023-09-29 22:30:32.082 
Epoch 70/1000 
	 loss: 49.0022, MinusLogProbMetric: 49.0022, val_loss: 49.2525, val_MinusLogProbMetric: 49.2525

Epoch 70: val_loss improved from 49.43460 to 49.25246, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 71s - loss: 49.0022 - MinusLogProbMetric: 49.0022 - val_loss: 49.2525 - val_MinusLogProbMetric: 49.2525 - lr: 1.2346e-05 - 71s/epoch - 361ms/step
Epoch 71/1000
2023-09-29 22:31:40.622 
Epoch 71/1000 
	 loss: 48.6545, MinusLogProbMetric: 48.6545, val_loss: 48.8147, val_MinusLogProbMetric: 48.8147

Epoch 71: val_loss improved from 49.25246 to 48.81466, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 69s - loss: 48.6545 - MinusLogProbMetric: 48.6545 - val_loss: 48.8147 - val_MinusLogProbMetric: 48.8147 - lr: 1.2346e-05 - 69s/epoch - 352ms/step
Epoch 72/1000
2023-09-29 22:32:49.736 
Epoch 72/1000 
	 loss: 49.9847, MinusLogProbMetric: 49.9847, val_loss: 63.4048, val_MinusLogProbMetric: 63.4048

Epoch 72: val_loss did not improve from 48.81466
196/196 - 68s - loss: 49.9847 - MinusLogProbMetric: 49.9847 - val_loss: 63.4048 - val_MinusLogProbMetric: 63.4048 - lr: 1.2346e-05 - 68s/epoch - 346ms/step
Epoch 73/1000
2023-09-29 22:33:51.473 
Epoch 73/1000 
	 loss: 49.7043, MinusLogProbMetric: 49.7043, val_loss: 47.9756, val_MinusLogProbMetric: 47.9756

Epoch 73: val_loss improved from 48.81466 to 47.97563, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 63s - loss: 49.7043 - MinusLogProbMetric: 49.7043 - val_loss: 47.9756 - val_MinusLogProbMetric: 47.9756 - lr: 1.2346e-05 - 63s/epoch - 319ms/step
Epoch 74/1000
2023-09-29 22:34:58.124 
Epoch 74/1000 
	 loss: 83.2235, MinusLogProbMetric: 83.2235, val_loss: 64.9027, val_MinusLogProbMetric: 64.9027

Epoch 74: val_loss did not improve from 47.97563
196/196 - 66s - loss: 83.2235 - MinusLogProbMetric: 83.2235 - val_loss: 64.9027 - val_MinusLogProbMetric: 64.9027 - lr: 1.2346e-05 - 66s/epoch - 336ms/step
Epoch 75/1000
2023-09-29 22:36:02.306 
Epoch 75/1000 
	 loss: 56.9389, MinusLogProbMetric: 56.9389, val_loss: 53.3587, val_MinusLogProbMetric: 53.3587

Epoch 75: val_loss did not improve from 47.97563
196/196 - 64s - loss: 56.9389 - MinusLogProbMetric: 56.9389 - val_loss: 53.3587 - val_MinusLogProbMetric: 53.3587 - lr: 1.2346e-05 - 64s/epoch - 327ms/step
Epoch 76/1000
2023-09-29 22:37:06.709 
Epoch 76/1000 
	 loss: 51.4608, MinusLogProbMetric: 51.4608, val_loss: 50.4992, val_MinusLogProbMetric: 50.4992

Epoch 76: val_loss did not improve from 47.97563
196/196 - 64s - loss: 51.4608 - MinusLogProbMetric: 51.4608 - val_loss: 50.4992 - val_MinusLogProbMetric: 50.4992 - lr: 1.2346e-05 - 64s/epoch - 329ms/step
Epoch 77/1000
2023-09-29 22:38:11.143 
Epoch 77/1000 
	 loss: 56.2971, MinusLogProbMetric: 56.2971, val_loss: 53.7746, val_MinusLogProbMetric: 53.7746

Epoch 77: val_loss did not improve from 47.97563
196/196 - 64s - loss: 56.2971 - MinusLogProbMetric: 56.2971 - val_loss: 53.7746 - val_MinusLogProbMetric: 53.7746 - lr: 1.2346e-05 - 64s/epoch - 329ms/step
Epoch 78/1000
2023-09-29 22:39:16.285 
Epoch 78/1000 
	 loss: 50.1551, MinusLogProbMetric: 50.1551, val_loss: 48.8702, val_MinusLogProbMetric: 48.8702

Epoch 78: val_loss did not improve from 47.97563
196/196 - 65s - loss: 50.1551 - MinusLogProbMetric: 50.1551 - val_loss: 48.8702 - val_MinusLogProbMetric: 48.8702 - lr: 1.2346e-05 - 65s/epoch - 332ms/step
Epoch 79/1000
2023-09-29 22:40:21.140 
Epoch 79/1000 
	 loss: 47.8071, MinusLogProbMetric: 47.8071, val_loss: 47.4493, val_MinusLogProbMetric: 47.4493

Epoch 79: val_loss improved from 47.97563 to 47.44928, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 66s - loss: 47.8071 - MinusLogProbMetric: 47.8071 - val_loss: 47.4493 - val_MinusLogProbMetric: 47.4493 - lr: 1.2346e-05 - 66s/epoch - 336ms/step
Epoch 80/1000
2023-09-29 22:41:25.537 
Epoch 80/1000 
	 loss: 46.8670, MinusLogProbMetric: 46.8670, val_loss: 46.9518, val_MinusLogProbMetric: 46.9518

Epoch 80: val_loss improved from 47.44928 to 46.95177, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 65s - loss: 46.8670 - MinusLogProbMetric: 46.8670 - val_loss: 46.9518 - val_MinusLogProbMetric: 46.9518 - lr: 1.2346e-05 - 65s/epoch - 330ms/step
Epoch 81/1000
2023-09-29 22:42:26.718 
Epoch 81/1000 
	 loss: 46.3374, MinusLogProbMetric: 46.3374, val_loss: 46.4096, val_MinusLogProbMetric: 46.4096

Epoch 81: val_loss improved from 46.95177 to 46.40957, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 61s - loss: 46.3374 - MinusLogProbMetric: 46.3374 - val_loss: 46.4096 - val_MinusLogProbMetric: 46.4096 - lr: 1.2346e-05 - 61s/epoch - 312ms/step
Epoch 82/1000
2023-09-29 22:43:30.495 
Epoch 82/1000 
	 loss: 45.9411, MinusLogProbMetric: 45.9411, val_loss: 46.1494, val_MinusLogProbMetric: 46.1494

Epoch 82: val_loss improved from 46.40957 to 46.14937, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 64s - loss: 45.9411 - MinusLogProbMetric: 45.9411 - val_loss: 46.1494 - val_MinusLogProbMetric: 46.1494 - lr: 1.2346e-05 - 64s/epoch - 327ms/step
Epoch 83/1000
2023-09-29 22:44:40.916 
Epoch 83/1000 
	 loss: 45.6146, MinusLogProbMetric: 45.6146, val_loss: 45.7053, val_MinusLogProbMetric: 45.7053

Epoch 83: val_loss improved from 46.14937 to 45.70532, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 70s - loss: 45.6146 - MinusLogProbMetric: 45.6146 - val_loss: 45.7053 - val_MinusLogProbMetric: 45.7053 - lr: 1.2346e-05 - 70s/epoch - 356ms/step
Epoch 84/1000
2023-09-29 22:45:49.831 
Epoch 84/1000 
	 loss: 46.9473, MinusLogProbMetric: 46.9473, val_loss: 46.0980, val_MinusLogProbMetric: 46.0980

Epoch 84: val_loss did not improve from 45.70532
196/196 - 68s - loss: 46.9473 - MinusLogProbMetric: 46.9473 - val_loss: 46.0980 - val_MinusLogProbMetric: 46.0980 - lr: 1.2346e-05 - 68s/epoch - 348ms/step
Epoch 85/1000
2023-09-29 22:46:57.316 
Epoch 85/1000 
	 loss: 45.5899, MinusLogProbMetric: 45.5899, val_loss: 45.6836, val_MinusLogProbMetric: 45.6836

Epoch 85: val_loss improved from 45.70532 to 45.68355, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 69s - loss: 45.5899 - MinusLogProbMetric: 45.5899 - val_loss: 45.6836 - val_MinusLogProbMetric: 45.6836 - lr: 1.2346e-05 - 69s/epoch - 350ms/step
Epoch 86/1000
2023-09-29 22:48:06.074 
Epoch 86/1000 
	 loss: 45.2752, MinusLogProbMetric: 45.2752, val_loss: 45.5048, val_MinusLogProbMetric: 45.5048

Epoch 86: val_loss improved from 45.68355 to 45.50485, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 69s - loss: 45.2752 - MinusLogProbMetric: 45.2752 - val_loss: 45.5048 - val_MinusLogProbMetric: 45.5048 - lr: 1.2346e-05 - 69s/epoch - 351ms/step
Epoch 87/1000
2023-09-29 22:49:14.890 
Epoch 87/1000 
	 loss: 45.7378, MinusLogProbMetric: 45.7378, val_loss: 62.5112, val_MinusLogProbMetric: 62.5112

Epoch 87: val_loss did not improve from 45.50485
196/196 - 68s - loss: 45.7378 - MinusLogProbMetric: 45.7378 - val_loss: 62.5112 - val_MinusLogProbMetric: 62.5112 - lr: 1.2346e-05 - 68s/epoch - 345ms/step
Epoch 88/1000
2023-09-29 22:50:20.517 
Epoch 88/1000 
	 loss: 45.7867, MinusLogProbMetric: 45.7867, val_loss: 45.0725, val_MinusLogProbMetric: 45.0725

Epoch 88: val_loss improved from 45.50485 to 45.07248, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 67s - loss: 45.7867 - MinusLogProbMetric: 45.7867 - val_loss: 45.0725 - val_MinusLogProbMetric: 45.0725 - lr: 1.2346e-05 - 67s/epoch - 342ms/step
Epoch 89/1000
2023-09-29 22:51:30.239 
Epoch 89/1000 
	 loss: 44.5585, MinusLogProbMetric: 44.5585, val_loss: 44.7675, val_MinusLogProbMetric: 44.7675

Epoch 89: val_loss improved from 45.07248 to 44.76751, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 69s - loss: 44.5585 - MinusLogProbMetric: 44.5585 - val_loss: 44.7675 - val_MinusLogProbMetric: 44.7675 - lr: 1.2346e-05 - 69s/epoch - 354ms/step
Epoch 90/1000
2023-09-29 22:52:39.343 
Epoch 90/1000 
	 loss: 44.4036, MinusLogProbMetric: 44.4036, val_loss: 44.5328, val_MinusLogProbMetric: 44.5328

Epoch 90: val_loss improved from 44.76751 to 44.53280, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 69s - loss: 44.4036 - MinusLogProbMetric: 44.4036 - val_loss: 44.5328 - val_MinusLogProbMetric: 44.5328 - lr: 1.2346e-05 - 69s/epoch - 352ms/step
Epoch 91/1000
2023-09-29 22:53:47.488 
Epoch 91/1000 
	 loss: 44.2139, MinusLogProbMetric: 44.2139, val_loss: 44.3627, val_MinusLogProbMetric: 44.3627

Epoch 91: val_loss improved from 44.53280 to 44.36272, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 68s - loss: 44.2139 - MinusLogProbMetric: 44.2139 - val_loss: 44.3627 - val_MinusLogProbMetric: 44.3627 - lr: 1.2346e-05 - 68s/epoch - 348ms/step
Epoch 92/1000
2023-09-29 22:54:57.148 
Epoch 92/1000 
	 loss: 44.0984, MinusLogProbMetric: 44.0984, val_loss: 44.2372, val_MinusLogProbMetric: 44.2372

Epoch 92: val_loss improved from 44.36272 to 44.23723, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 70s - loss: 44.0984 - MinusLogProbMetric: 44.0984 - val_loss: 44.2372 - val_MinusLogProbMetric: 44.2372 - lr: 1.2346e-05 - 70s/epoch - 355ms/step
Epoch 93/1000
2023-09-29 22:56:06.777 
Epoch 93/1000 
	 loss: 43.9139, MinusLogProbMetric: 43.9139, val_loss: 44.1829, val_MinusLogProbMetric: 44.1829

Epoch 93: val_loss improved from 44.23723 to 44.18289, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 70s - loss: 43.9139 - MinusLogProbMetric: 43.9139 - val_loss: 44.1829 - val_MinusLogProbMetric: 44.1829 - lr: 1.2346e-05 - 70s/epoch - 355ms/step
Epoch 94/1000
2023-09-29 22:57:15.705 
Epoch 94/1000 
	 loss: 43.7246, MinusLogProbMetric: 43.7246, val_loss: 43.8616, val_MinusLogProbMetric: 43.8616

Epoch 94: val_loss improved from 44.18289 to 43.86160, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 69s - loss: 43.7246 - MinusLogProbMetric: 43.7246 - val_loss: 43.8616 - val_MinusLogProbMetric: 43.8616 - lr: 1.2346e-05 - 69s/epoch - 353ms/step
Epoch 95/1000
2023-09-29 22:58:25.621 
Epoch 95/1000 
	 loss: 43.5962, MinusLogProbMetric: 43.5962, val_loss: 43.6979, val_MinusLogProbMetric: 43.6979

Epoch 95: val_loss improved from 43.86160 to 43.69794, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 70s - loss: 43.5962 - MinusLogProbMetric: 43.5962 - val_loss: 43.6979 - val_MinusLogProbMetric: 43.6979 - lr: 1.2346e-05 - 70s/epoch - 356ms/step
Epoch 96/1000
2023-09-29 22:59:35.423 
Epoch 96/1000 
	 loss: 49.7300, MinusLogProbMetric: 49.7300, val_loss: 59.1730, val_MinusLogProbMetric: 59.1730

Epoch 96: val_loss did not improve from 43.69794
196/196 - 69s - loss: 49.7300 - MinusLogProbMetric: 49.7300 - val_loss: 59.1730 - val_MinusLogProbMetric: 59.1730 - lr: 1.2346e-05 - 69s/epoch - 350ms/step
Epoch 97/1000
2023-09-29 23:00:43.904 
Epoch 97/1000 
	 loss: 47.9203, MinusLogProbMetric: 47.9203, val_loss: 45.0040, val_MinusLogProbMetric: 45.0040

Epoch 97: val_loss did not improve from 43.69794
196/196 - 68s - loss: 47.9203 - MinusLogProbMetric: 47.9203 - val_loss: 45.0040 - val_MinusLogProbMetric: 45.0040 - lr: 1.2346e-05 - 68s/epoch - 349ms/step
Epoch 98/1000
2023-09-29 23:01:53.665 
Epoch 98/1000 
	 loss: 43.8617, MinusLogProbMetric: 43.8617, val_loss: 43.8443, val_MinusLogProbMetric: 43.8443

Epoch 98: val_loss did not improve from 43.69794
196/196 - 70s - loss: 43.8617 - MinusLogProbMetric: 43.8617 - val_loss: 43.8443 - val_MinusLogProbMetric: 43.8443 - lr: 1.2346e-05 - 70s/epoch - 356ms/step
Epoch 99/1000
2023-09-29 23:03:02.264 
Epoch 99/1000 
	 loss: 43.4193, MinusLogProbMetric: 43.4193, val_loss: 43.4039, val_MinusLogProbMetric: 43.4039

Epoch 99: val_loss improved from 43.69794 to 43.40391, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 70s - loss: 43.4193 - MinusLogProbMetric: 43.4193 - val_loss: 43.4039 - val_MinusLogProbMetric: 43.4039 - lr: 1.2346e-05 - 70s/epoch - 356ms/step
Epoch 100/1000
2023-09-29 23:04:12.429 
Epoch 100/1000 
	 loss: 44.3370, MinusLogProbMetric: 44.3370, val_loss: 43.3399, val_MinusLogProbMetric: 43.3399

Epoch 100: val_loss improved from 43.40391 to 43.33989, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 70s - loss: 44.3370 - MinusLogProbMetric: 44.3370 - val_loss: 43.3399 - val_MinusLogProbMetric: 43.3399 - lr: 1.2346e-05 - 70s/epoch - 357ms/step
Epoch 101/1000
2023-09-29 23:05:22.200 
Epoch 101/1000 
	 loss: 46.7816, MinusLogProbMetric: 46.7816, val_loss: 45.0048, val_MinusLogProbMetric: 45.0048

Epoch 101: val_loss did not improve from 43.33989
196/196 - 69s - loss: 46.7816 - MinusLogProbMetric: 46.7816 - val_loss: 45.0048 - val_MinusLogProbMetric: 45.0048 - lr: 1.2346e-05 - 69s/epoch - 351ms/step
Epoch 102/1000
2023-09-29 23:06:30.573 
Epoch 102/1000 
	 loss: 43.6538, MinusLogProbMetric: 43.6538, val_loss: 43.5311, val_MinusLogProbMetric: 43.5311

Epoch 102: val_loss did not improve from 43.33989
196/196 - 68s - loss: 43.6538 - MinusLogProbMetric: 43.6538 - val_loss: 43.5311 - val_MinusLogProbMetric: 43.5311 - lr: 1.2346e-05 - 68s/epoch - 349ms/step
Epoch 103/1000
2023-09-29 23:07:38.173 
Epoch 103/1000 
	 loss: 43.0454, MinusLogProbMetric: 43.0454, val_loss: 45.1008, val_MinusLogProbMetric: 45.1008

Epoch 103: val_loss did not improve from 43.33989
196/196 - 68s - loss: 43.0454 - MinusLogProbMetric: 43.0454 - val_loss: 45.1008 - val_MinusLogProbMetric: 45.1008 - lr: 1.2346e-05 - 68s/epoch - 345ms/step
Epoch 104/1000
2023-09-29 23:08:45.420 
Epoch 104/1000 
	 loss: 43.8759, MinusLogProbMetric: 43.8759, val_loss: 44.2628, val_MinusLogProbMetric: 44.2628

Epoch 104: val_loss did not improve from 43.33989
196/196 - 67s - loss: 43.8759 - MinusLogProbMetric: 43.8759 - val_loss: 44.2628 - val_MinusLogProbMetric: 44.2628 - lr: 1.2346e-05 - 67s/epoch - 343ms/step
Epoch 105/1000
2023-09-29 23:09:52.484 
Epoch 105/1000 
	 loss: 47.4935, MinusLogProbMetric: 47.4935, val_loss: 56.1537, val_MinusLogProbMetric: 56.1537

Epoch 105: val_loss did not improve from 43.33989
196/196 - 67s - loss: 47.4935 - MinusLogProbMetric: 47.4935 - val_loss: 56.1537 - val_MinusLogProbMetric: 56.1537 - lr: 1.2346e-05 - 67s/epoch - 342ms/step
Epoch 106/1000
2023-09-29 23:10:59.395 
Epoch 106/1000 
	 loss: 45.8931, MinusLogProbMetric: 45.8931, val_loss: 93.3360, val_MinusLogProbMetric: 93.3360

Epoch 106: val_loss did not improve from 43.33989
196/196 - 67s - loss: 45.8931 - MinusLogProbMetric: 45.8931 - val_loss: 93.3360 - val_MinusLogProbMetric: 93.3360 - lr: 1.2346e-05 - 67s/epoch - 341ms/step
Epoch 107/1000
2023-09-29 23:12:07.369 
Epoch 107/1000 
	 loss: 48.7863, MinusLogProbMetric: 48.7863, val_loss: 43.3543, val_MinusLogProbMetric: 43.3543

Epoch 107: val_loss did not improve from 43.33989
196/196 - 68s - loss: 48.7863 - MinusLogProbMetric: 48.7863 - val_loss: 43.3543 - val_MinusLogProbMetric: 43.3543 - lr: 1.2346e-05 - 68s/epoch - 347ms/step
Epoch 108/1000
2023-09-29 23:13:15.146 
Epoch 108/1000 
	 loss: 43.1490, MinusLogProbMetric: 43.1490, val_loss: 42.8640, val_MinusLogProbMetric: 42.8640

Epoch 108: val_loss improved from 43.33989 to 42.86404, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 69s - loss: 43.1490 - MinusLogProbMetric: 43.1490 - val_loss: 42.8640 - val_MinusLogProbMetric: 42.8640 - lr: 1.2346e-05 - 69s/epoch - 350ms/step
Epoch 109/1000
2023-09-29 23:14:24.672 
Epoch 109/1000 
	 loss: 42.4364, MinusLogProbMetric: 42.4364, val_loss: 42.5577, val_MinusLogProbMetric: 42.5577

Epoch 109: val_loss improved from 42.86404 to 42.55765, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 70s - loss: 42.4364 - MinusLogProbMetric: 42.4364 - val_loss: 42.5577 - val_MinusLogProbMetric: 42.5577 - lr: 1.2346e-05 - 70s/epoch - 355ms/step
Epoch 110/1000
2023-09-29 23:15:34.167 
Epoch 110/1000 
	 loss: 43.5493, MinusLogProbMetric: 43.5493, val_loss: 50.7029, val_MinusLogProbMetric: 50.7029

Epoch 110: val_loss did not improve from 42.55765
196/196 - 69s - loss: 43.5493 - MinusLogProbMetric: 43.5493 - val_loss: 50.7029 - val_MinusLogProbMetric: 50.7029 - lr: 1.2346e-05 - 69s/epoch - 350ms/step
Epoch 111/1000
2023-09-29 23:16:43.193 
Epoch 111/1000 
	 loss: 42.8897, MinusLogProbMetric: 42.8897, val_loss: 42.3949, val_MinusLogProbMetric: 42.3949

Epoch 111: val_loss improved from 42.55765 to 42.39492, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 70s - loss: 42.8897 - MinusLogProbMetric: 42.8897 - val_loss: 42.3949 - val_MinusLogProbMetric: 42.3949 - lr: 1.2346e-05 - 70s/epoch - 358ms/step
Epoch 112/1000
2023-09-29 23:17:53.418 
Epoch 112/1000 
	 loss: 41.8530, MinusLogProbMetric: 41.8530, val_loss: 42.3796, val_MinusLogProbMetric: 42.3796

Epoch 112: val_loss improved from 42.39492 to 42.37960, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 71s - loss: 41.8530 - MinusLogProbMetric: 41.8530 - val_loss: 42.3796 - val_MinusLogProbMetric: 42.3796 - lr: 1.2346e-05 - 71s/epoch - 360ms/step
Epoch 113/1000
2023-09-29 23:19:03.453 
Epoch 113/1000 
	 loss: 41.6508, MinusLogProbMetric: 41.6508, val_loss: 41.8707, val_MinusLogProbMetric: 41.8707

Epoch 113: val_loss improved from 42.37960 to 41.87070, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 70s - loss: 41.6508 - MinusLogProbMetric: 41.6508 - val_loss: 41.8707 - val_MinusLogProbMetric: 41.8707 - lr: 1.2346e-05 - 70s/epoch - 359ms/step
Epoch 114/1000
2023-09-29 23:20:14.553 
Epoch 114/1000 
	 loss: 42.2416, MinusLogProbMetric: 42.2416, val_loss: 41.7744, val_MinusLogProbMetric: 41.7744

Epoch 114: val_loss improved from 41.87070 to 41.77443, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 70s - loss: 42.2416 - MinusLogProbMetric: 42.2416 - val_loss: 41.7744 - val_MinusLogProbMetric: 41.7744 - lr: 1.2346e-05 - 70s/epoch - 359ms/step
Epoch 115/1000
2023-09-29 23:21:23.259 
Epoch 115/1000 
	 loss: 41.3308, MinusLogProbMetric: 41.3308, val_loss: 41.5665, val_MinusLogProbMetric: 41.5665

Epoch 115: val_loss improved from 41.77443 to 41.56653, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 69s - loss: 41.3308 - MinusLogProbMetric: 41.3308 - val_loss: 41.5665 - val_MinusLogProbMetric: 41.5665 - lr: 1.2346e-05 - 69s/epoch - 352ms/step
Epoch 116/1000
2023-09-29 23:22:30.325 
Epoch 116/1000 
	 loss: 41.3390, MinusLogProbMetric: 41.3390, val_loss: 41.2768, val_MinusLogProbMetric: 41.2768

Epoch 116: val_loss improved from 41.56653 to 41.27681, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 67s - loss: 41.3390 - MinusLogProbMetric: 41.3390 - val_loss: 41.2768 - val_MinusLogProbMetric: 41.2768 - lr: 1.2346e-05 - 67s/epoch - 342ms/step
Epoch 117/1000
2023-09-29 23:23:39.297 
Epoch 117/1000 
	 loss: 41.0213, MinusLogProbMetric: 41.0213, val_loss: 41.5241, val_MinusLogProbMetric: 41.5241

Epoch 117: val_loss did not improve from 41.27681
196/196 - 68s - loss: 41.0213 - MinusLogProbMetric: 41.0213 - val_loss: 41.5241 - val_MinusLogProbMetric: 41.5241 - lr: 1.2346e-05 - 68s/epoch - 346ms/step
Epoch 118/1000
2023-09-29 23:24:44.523 
Epoch 118/1000 
	 loss: 40.8684, MinusLogProbMetric: 40.8684, val_loss: 41.0845, val_MinusLogProbMetric: 41.0845

Epoch 118: val_loss improved from 41.27681 to 41.08450, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 66s - loss: 40.8684 - MinusLogProbMetric: 40.8684 - val_loss: 41.0845 - val_MinusLogProbMetric: 41.0845 - lr: 1.2346e-05 - 66s/epoch - 338ms/step
Epoch 119/1000
2023-09-29 23:25:54.426 
Epoch 119/1000 
	 loss: 54.4521, MinusLogProbMetric: 54.4521, val_loss: 55.3545, val_MinusLogProbMetric: 55.3545

Epoch 119: val_loss did not improve from 41.08450
196/196 - 69s - loss: 54.4521 - MinusLogProbMetric: 54.4521 - val_loss: 55.3545 - val_MinusLogProbMetric: 55.3545 - lr: 1.2346e-05 - 69s/epoch - 351ms/step
Epoch 120/1000
2023-09-29 23:27:04.321 
Epoch 120/1000 
	 loss: 48.2663, MinusLogProbMetric: 48.2663, val_loss: 45.0912, val_MinusLogProbMetric: 45.0912

Epoch 120: val_loss did not improve from 41.08450
196/196 - 70s - loss: 48.2663 - MinusLogProbMetric: 48.2663 - val_loss: 45.0912 - val_MinusLogProbMetric: 45.0912 - lr: 1.2346e-05 - 70s/epoch - 357ms/step
Epoch 121/1000
2023-09-29 23:28:13.680 
Epoch 121/1000 
	 loss: 43.9450, MinusLogProbMetric: 43.9450, val_loss: 43.4258, val_MinusLogProbMetric: 43.4258

Epoch 121: val_loss did not improve from 41.08450
196/196 - 69s - loss: 43.9450 - MinusLogProbMetric: 43.9450 - val_loss: 43.4258 - val_MinusLogProbMetric: 43.4258 - lr: 1.2346e-05 - 69s/epoch - 354ms/step
Epoch 122/1000
2023-09-29 23:29:22.583 
Epoch 122/1000 
	 loss: 42.5284, MinusLogProbMetric: 42.5284, val_loss: 42.3656, val_MinusLogProbMetric: 42.3656

Epoch 122: val_loss did not improve from 41.08450
196/196 - 69s - loss: 42.5284 - MinusLogProbMetric: 42.5284 - val_loss: 42.3656 - val_MinusLogProbMetric: 42.3656 - lr: 1.2346e-05 - 69s/epoch - 352ms/step
Epoch 123/1000
2023-09-29 23:30:31.552 
Epoch 123/1000 
	 loss: 41.8091, MinusLogProbMetric: 41.8091, val_loss: 41.8647, val_MinusLogProbMetric: 41.8647

Epoch 123: val_loss did not improve from 41.08450
196/196 - 69s - loss: 41.8091 - MinusLogProbMetric: 41.8091 - val_loss: 41.8647 - val_MinusLogProbMetric: 41.8647 - lr: 1.2346e-05 - 69s/epoch - 352ms/step
Epoch 124/1000
2023-09-29 23:31:40.886 
Epoch 124/1000 
	 loss: 41.7414, MinusLogProbMetric: 41.7414, val_loss: 41.5836, val_MinusLogProbMetric: 41.5836

Epoch 124: val_loss did not improve from 41.08450
196/196 - 69s - loss: 41.7414 - MinusLogProbMetric: 41.7414 - val_loss: 41.5836 - val_MinusLogProbMetric: 41.5836 - lr: 1.2346e-05 - 69s/epoch - 354ms/step
Epoch 125/1000
2023-09-29 23:32:48.961 
Epoch 125/1000 
	 loss: 41.2028, MinusLogProbMetric: 41.2028, val_loss: 41.3250, val_MinusLogProbMetric: 41.3250

Epoch 125: val_loss did not improve from 41.08450
196/196 - 68s - loss: 41.2028 - MinusLogProbMetric: 41.2028 - val_loss: 41.3250 - val_MinusLogProbMetric: 41.3250 - lr: 1.2346e-05 - 68s/epoch - 347ms/step
Epoch 126/1000
2023-09-29 23:33:58.180 
Epoch 126/1000 
	 loss: 40.8994, MinusLogProbMetric: 40.8994, val_loss: 41.1211, val_MinusLogProbMetric: 41.1211

Epoch 126: val_loss did not improve from 41.08450
196/196 - 69s - loss: 40.8994 - MinusLogProbMetric: 40.8994 - val_loss: 41.1211 - val_MinusLogProbMetric: 41.1211 - lr: 1.2346e-05 - 69s/epoch - 353ms/step
Epoch 127/1000
2023-09-29 23:35:07.498 
Epoch 127/1000 
	 loss: 40.6818, MinusLogProbMetric: 40.6818, val_loss: 40.9340, val_MinusLogProbMetric: 40.9340

Epoch 127: val_loss improved from 41.08450 to 40.93397, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 70s - loss: 40.6818 - MinusLogProbMetric: 40.6818 - val_loss: 40.9340 - val_MinusLogProbMetric: 40.9340 - lr: 1.2346e-05 - 70s/epoch - 358ms/step
Epoch 128/1000
2023-09-29 23:36:17.652 
Epoch 128/1000 
	 loss: 40.4462, MinusLogProbMetric: 40.4462, val_loss: 40.5152, val_MinusLogProbMetric: 40.5152

Epoch 128: val_loss improved from 40.93397 to 40.51519, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 70s - loss: 40.4462 - MinusLogProbMetric: 40.4462 - val_loss: 40.5152 - val_MinusLogProbMetric: 40.5152 - lr: 1.2346e-05 - 70s/epoch - 359ms/step
Epoch 129/1000
2023-09-29 23:37:25.877 
Epoch 129/1000 
	 loss: 40.0298, MinusLogProbMetric: 40.0298, val_loss: 40.3046, val_MinusLogProbMetric: 40.3046

Epoch 129: val_loss improved from 40.51519 to 40.30465, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 68s - loss: 40.0298 - MinusLogProbMetric: 40.0298 - val_loss: 40.3046 - val_MinusLogProbMetric: 40.3046 - lr: 1.2346e-05 - 68s/epoch - 346ms/step
Epoch 130/1000
2023-09-29 23:38:37.724 
Epoch 130/1000 
	 loss: 39.7791, MinusLogProbMetric: 39.7791, val_loss: 40.0310, val_MinusLogProbMetric: 40.0310

Epoch 130: val_loss improved from 40.30465 to 40.03098, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 73s - loss: 39.7791 - MinusLogProbMetric: 39.7791 - val_loss: 40.0310 - val_MinusLogProbMetric: 40.0310 - lr: 1.2346e-05 - 73s/epoch - 370ms/step
Epoch 131/1000
2023-09-29 23:39:51.080 
Epoch 131/1000 
	 loss: 39.6370, MinusLogProbMetric: 39.6370, val_loss: 39.9889, val_MinusLogProbMetric: 39.9889

Epoch 131: val_loss improved from 40.03098 to 39.98892, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 73s - loss: 39.6370 - MinusLogProbMetric: 39.6370 - val_loss: 39.9889 - val_MinusLogProbMetric: 39.9889 - lr: 1.2346e-05 - 73s/epoch - 373ms/step
Epoch 132/1000
2023-09-29 23:41:00.808 
Epoch 132/1000 
	 loss: 39.5361, MinusLogProbMetric: 39.5361, val_loss: 39.8335, val_MinusLogProbMetric: 39.8335

Epoch 132: val_loss improved from 39.98892 to 39.83354, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 69s - loss: 39.5361 - MinusLogProbMetric: 39.5361 - val_loss: 39.8335 - val_MinusLogProbMetric: 39.8335 - lr: 1.2346e-05 - 69s/epoch - 354ms/step
Epoch 133/1000
2023-09-29 23:42:12.719 
Epoch 133/1000 
	 loss: 39.4366, MinusLogProbMetric: 39.4366, val_loss: 39.7460, val_MinusLogProbMetric: 39.7460

Epoch 133: val_loss improved from 39.83354 to 39.74601, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 72s - loss: 39.4366 - MinusLogProbMetric: 39.4366 - val_loss: 39.7460 - val_MinusLogProbMetric: 39.7460 - lr: 1.2346e-05 - 72s/epoch - 368ms/step
Epoch 134/1000
2023-09-29 23:43:25.502 
Epoch 134/1000 
	 loss: 39.3617, MinusLogProbMetric: 39.3617, val_loss: 39.7190, val_MinusLogProbMetric: 39.7190

Epoch 134: val_loss improved from 39.74601 to 39.71897, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 73s - loss: 39.3617 - MinusLogProbMetric: 39.3617 - val_loss: 39.7190 - val_MinusLogProbMetric: 39.7190 - lr: 1.2346e-05 - 73s/epoch - 370ms/step
Epoch 135/1000
2023-09-29 23:44:37.673 
Epoch 135/1000 
	 loss: 39.3500, MinusLogProbMetric: 39.3500, val_loss: 39.4613, val_MinusLogProbMetric: 39.4613

Epoch 135: val_loss improved from 39.71897 to 39.46135, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 73s - loss: 39.3500 - MinusLogProbMetric: 39.3500 - val_loss: 39.4613 - val_MinusLogProbMetric: 39.4613 - lr: 1.2346e-05 - 73s/epoch - 370ms/step
Epoch 136/1000
2023-09-29 23:45:49.871 
Epoch 136/1000 
	 loss: 56.0963, MinusLogProbMetric: 56.0963, val_loss: 43.1996, val_MinusLogProbMetric: 43.1996

Epoch 136: val_loss did not improve from 39.46135
196/196 - 71s - loss: 56.0963 - MinusLogProbMetric: 56.0963 - val_loss: 43.1996 - val_MinusLogProbMetric: 43.1996 - lr: 1.2346e-05 - 71s/epoch - 361ms/step
Epoch 137/1000
2023-09-29 23:47:01.632 
Epoch 137/1000 
	 loss: 41.5621, MinusLogProbMetric: 41.5621, val_loss: 41.2229, val_MinusLogProbMetric: 41.2229

Epoch 137: val_loss did not improve from 39.46135
196/196 - 72s - loss: 41.5621 - MinusLogProbMetric: 41.5621 - val_loss: 41.2229 - val_MinusLogProbMetric: 41.2229 - lr: 1.2346e-05 - 72s/epoch - 366ms/step
Epoch 138/1000
2023-09-29 23:48:13.613 
Epoch 138/1000 
	 loss: 40.5671, MinusLogProbMetric: 40.5671, val_loss: 40.6925, val_MinusLogProbMetric: 40.6925

Epoch 138: val_loss did not improve from 39.46135
196/196 - 72s - loss: 40.5671 - MinusLogProbMetric: 40.5671 - val_loss: 40.6925 - val_MinusLogProbMetric: 40.6925 - lr: 1.2346e-05 - 72s/epoch - 367ms/step
Epoch 139/1000
2023-09-29 23:49:25.130 
Epoch 139/1000 
	 loss: 40.1806, MinusLogProbMetric: 40.1806, val_loss: 40.0829, val_MinusLogProbMetric: 40.0829

Epoch 139: val_loss did not improve from 39.46135
196/196 - 72s - loss: 40.1806 - MinusLogProbMetric: 40.1806 - val_loss: 40.0829 - val_MinusLogProbMetric: 40.0829 - lr: 1.2346e-05 - 72s/epoch - 365ms/step
Epoch 140/1000
2023-09-29 23:50:36.287 
Epoch 140/1000 
	 loss: 39.4911, MinusLogProbMetric: 39.4911, val_loss: 39.6540, val_MinusLogProbMetric: 39.6540

Epoch 140: val_loss did not improve from 39.46135
196/196 - 71s - loss: 39.4911 - MinusLogProbMetric: 39.4911 - val_loss: 39.6540 - val_MinusLogProbMetric: 39.6540 - lr: 1.2346e-05 - 71s/epoch - 363ms/step
Epoch 141/1000
2023-09-29 23:51:46.507 
Epoch 141/1000 
	 loss: 39.2839, MinusLogProbMetric: 39.2839, val_loss: 39.6766, val_MinusLogProbMetric: 39.6766

Epoch 141: val_loss did not improve from 39.46135
196/196 - 70s - loss: 39.2839 - MinusLogProbMetric: 39.2839 - val_loss: 39.6766 - val_MinusLogProbMetric: 39.6766 - lr: 1.2346e-05 - 70s/epoch - 358ms/step
Epoch 142/1000
2023-09-29 23:52:58.599 
Epoch 142/1000 
	 loss: 39.0040, MinusLogProbMetric: 39.0040, val_loss: 39.2444, val_MinusLogProbMetric: 39.2444

Epoch 142: val_loss improved from 39.46135 to 39.24438, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 73s - loss: 39.0040 - MinusLogProbMetric: 39.0040 - val_loss: 39.2444 - val_MinusLogProbMetric: 39.2444 - lr: 1.2346e-05 - 73s/epoch - 373ms/step
Epoch 143/1000
2023-09-29 23:54:10.966 
Epoch 143/1000 
	 loss: 38.9132, MinusLogProbMetric: 38.9132, val_loss: 39.0423, val_MinusLogProbMetric: 39.0423

Epoch 143: val_loss improved from 39.24438 to 39.04230, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 72s - loss: 38.9132 - MinusLogProbMetric: 38.9132 - val_loss: 39.0423 - val_MinusLogProbMetric: 39.0423 - lr: 1.2346e-05 - 72s/epoch - 369ms/step
Epoch 144/1000
2023-09-29 23:55:22.939 
Epoch 144/1000 
	 loss: 38.8137, MinusLogProbMetric: 38.8137, val_loss: 39.1968, val_MinusLogProbMetric: 39.1968

Epoch 144: val_loss did not improve from 39.04230
196/196 - 71s - loss: 38.8137 - MinusLogProbMetric: 38.8137 - val_loss: 39.1968 - val_MinusLogProbMetric: 39.1968 - lr: 1.2346e-05 - 71s/epoch - 362ms/step
Epoch 145/1000
2023-09-29 23:56:32.077 
Epoch 145/1000 
	 loss: 38.7383, MinusLogProbMetric: 38.7383, val_loss: 38.9976, val_MinusLogProbMetric: 38.9976

Epoch 145: val_loss improved from 39.04230 to 38.99756, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 70s - loss: 38.7383 - MinusLogProbMetric: 38.7383 - val_loss: 38.9976 - val_MinusLogProbMetric: 38.9976 - lr: 1.2346e-05 - 70s/epoch - 358ms/step
Epoch 146/1000
2023-09-29 23:57:46.550 
Epoch 146/1000 
	 loss: 38.6436, MinusLogProbMetric: 38.6436, val_loss: 38.8757, val_MinusLogProbMetric: 38.8757

Epoch 146: val_loss improved from 38.99756 to 38.87571, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 75s - loss: 38.6436 - MinusLogProbMetric: 38.6436 - val_loss: 38.8757 - val_MinusLogProbMetric: 38.8757 - lr: 1.2346e-05 - 75s/epoch - 381ms/step
Epoch 147/1000
2023-09-29 23:58:59.788 
Epoch 147/1000 
	 loss: 47.0689, MinusLogProbMetric: 47.0689, val_loss: 39.1433, val_MinusLogProbMetric: 39.1433

Epoch 147: val_loss did not improve from 38.87571
196/196 - 72s - loss: 47.0689 - MinusLogProbMetric: 47.0689 - val_loss: 39.1433 - val_MinusLogProbMetric: 39.1433 - lr: 1.2346e-05 - 72s/epoch - 367ms/step
Epoch 148/1000
2023-09-30 00:00:12.248 
Epoch 148/1000 
	 loss: 38.6413, MinusLogProbMetric: 38.6413, val_loss: 38.8863, val_MinusLogProbMetric: 38.8863

Epoch 148: val_loss did not improve from 38.87571
196/196 - 72s - loss: 38.6413 - MinusLogProbMetric: 38.6413 - val_loss: 38.8863 - val_MinusLogProbMetric: 38.8863 - lr: 1.2346e-05 - 72s/epoch - 370ms/step
Epoch 149/1000
2023-09-30 00:01:22.900 
Epoch 149/1000 
	 loss: 38.4800, MinusLogProbMetric: 38.4800, val_loss: 38.7471, val_MinusLogProbMetric: 38.7471

Epoch 149: val_loss improved from 38.87571 to 38.74713, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 71s - loss: 38.4800 - MinusLogProbMetric: 38.4800 - val_loss: 38.7471 - val_MinusLogProbMetric: 38.7471 - lr: 1.2346e-05 - 71s/epoch - 365ms/step
Epoch 150/1000
2023-09-30 00:02:26.436 
Epoch 150/1000 
	 loss: 38.4099, MinusLogProbMetric: 38.4099, val_loss: 38.8931, val_MinusLogProbMetric: 38.8931

Epoch 150: val_loss did not improve from 38.74713
196/196 - 63s - loss: 38.4099 - MinusLogProbMetric: 38.4099 - val_loss: 38.8931 - val_MinusLogProbMetric: 38.8931 - lr: 1.2346e-05 - 63s/epoch - 320ms/step
Epoch 151/1000
2023-09-30 00:03:30.561 
Epoch 151/1000 
	 loss: 38.5909, MinusLogProbMetric: 38.5909, val_loss: 38.6128, val_MinusLogProbMetric: 38.6128

Epoch 151: val_loss improved from 38.74713 to 38.61285, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 65s - loss: 38.5909 - MinusLogProbMetric: 38.5909 - val_loss: 38.6128 - val_MinusLogProbMetric: 38.6128 - lr: 1.2346e-05 - 65s/epoch - 332ms/step
Epoch 152/1000
2023-09-30 00:04:33.658 
Epoch 152/1000 
	 loss: 38.3007, MinusLogProbMetric: 38.3007, val_loss: 38.6733, val_MinusLogProbMetric: 38.6733

Epoch 152: val_loss did not improve from 38.61285
196/196 - 62s - loss: 38.3007 - MinusLogProbMetric: 38.3007 - val_loss: 38.6733 - val_MinusLogProbMetric: 38.6733 - lr: 1.2346e-05 - 62s/epoch - 317ms/step
Epoch 153/1000
2023-09-30 00:05:35.676 
Epoch 153/1000 
	 loss: 38.2143, MinusLogProbMetric: 38.2143, val_loss: 38.4926, val_MinusLogProbMetric: 38.4926

Epoch 153: val_loss improved from 38.61285 to 38.49259, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 63s - loss: 38.2143 - MinusLogProbMetric: 38.2143 - val_loss: 38.4926 - val_MinusLogProbMetric: 38.4926 - lr: 1.2346e-05 - 63s/epoch - 322ms/step
Epoch 154/1000
2023-09-30 00:06:47.205 
Epoch 154/1000 
	 loss: 38.1186, MinusLogProbMetric: 38.1186, val_loss: 38.5613, val_MinusLogProbMetric: 38.5613

Epoch 154: val_loss did not improve from 38.49259
196/196 - 70s - loss: 38.1186 - MinusLogProbMetric: 38.1186 - val_loss: 38.5613 - val_MinusLogProbMetric: 38.5613 - lr: 1.2346e-05 - 70s/epoch - 360ms/step
Epoch 155/1000
2023-09-30 00:07:59.567 
Epoch 155/1000 
	 loss: 41.1533, MinusLogProbMetric: 41.1533, val_loss: 43.9990, val_MinusLogProbMetric: 43.9990

Epoch 155: val_loss did not improve from 38.49259
196/196 - 72s - loss: 41.1533 - MinusLogProbMetric: 41.1533 - val_loss: 43.9990 - val_MinusLogProbMetric: 43.9990 - lr: 1.2346e-05 - 72s/epoch - 369ms/step
Epoch 156/1000
2023-09-30 00:09:07.311 
Epoch 156/1000 
	 loss: 43.3919, MinusLogProbMetric: 43.3919, val_loss: 39.4500, val_MinusLogProbMetric: 39.4500

Epoch 156: val_loss did not improve from 38.49259
196/196 - 68s - loss: 43.3919 - MinusLogProbMetric: 43.3919 - val_loss: 39.4500 - val_MinusLogProbMetric: 39.4500 - lr: 1.2346e-05 - 68s/epoch - 346ms/step
Epoch 157/1000
2023-09-30 00:10:19.840 
Epoch 157/1000 
	 loss: 38.3403, MinusLogProbMetric: 38.3403, val_loss: 38.4714, val_MinusLogProbMetric: 38.4714

Epoch 157: val_loss improved from 38.49259 to 38.47139, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 74s - loss: 38.3403 - MinusLogProbMetric: 38.3403 - val_loss: 38.4714 - val_MinusLogProbMetric: 38.4714 - lr: 1.2346e-05 - 74s/epoch - 376ms/step
Epoch 158/1000
2023-09-30 00:11:31.103 
Epoch 158/1000 
	 loss: 37.9992, MinusLogProbMetric: 37.9992, val_loss: 38.1778, val_MinusLogProbMetric: 38.1778

Epoch 158: val_loss improved from 38.47139 to 38.17780, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 71s - loss: 37.9992 - MinusLogProbMetric: 37.9992 - val_loss: 38.1778 - val_MinusLogProbMetric: 38.1778 - lr: 1.2346e-05 - 71s/epoch - 362ms/step
Epoch 159/1000
2023-09-30 00:12:37.839 
Epoch 159/1000 
	 loss: 37.8798, MinusLogProbMetric: 37.8798, val_loss: 38.1695, val_MinusLogProbMetric: 38.1695

Epoch 159: val_loss improved from 38.17780 to 38.16948, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 67s - loss: 37.8798 - MinusLogProbMetric: 37.8798 - val_loss: 38.1695 - val_MinusLogProbMetric: 38.1695 - lr: 1.2346e-05 - 67s/epoch - 341ms/step
Epoch 160/1000
2023-09-30 00:13:43.710 
Epoch 160/1000 
	 loss: 37.8457, MinusLogProbMetric: 37.8457, val_loss: 38.2454, val_MinusLogProbMetric: 38.2454

Epoch 160: val_loss did not improve from 38.16948
196/196 - 65s - loss: 37.8457 - MinusLogProbMetric: 37.8457 - val_loss: 38.2454 - val_MinusLogProbMetric: 38.2454 - lr: 1.2346e-05 - 65s/epoch - 331ms/step
Epoch 161/1000
2023-09-30 00:14:48.641 
Epoch 161/1000 
	 loss: 37.7627, MinusLogProbMetric: 37.7627, val_loss: 38.1447, val_MinusLogProbMetric: 38.1447

Epoch 161: val_loss improved from 38.16948 to 38.14470, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 66s - loss: 37.7627 - MinusLogProbMetric: 37.7627 - val_loss: 38.1447 - val_MinusLogProbMetric: 38.1447 - lr: 1.2346e-05 - 66s/epoch - 337ms/step
Epoch 162/1000
2023-09-30 00:15:52.850 
Epoch 162/1000 
	 loss: 37.7056, MinusLogProbMetric: 37.7056, val_loss: 38.0803, val_MinusLogProbMetric: 38.0803

Epoch 162: val_loss improved from 38.14470 to 38.08026, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 64s - loss: 37.7056 - MinusLogProbMetric: 37.7056 - val_loss: 38.0803 - val_MinusLogProbMetric: 38.0803 - lr: 1.2346e-05 - 64s/epoch - 328ms/step
Epoch 163/1000
2023-09-30 00:17:05.654 
Epoch 163/1000 
	 loss: 37.6611, MinusLogProbMetric: 37.6611, val_loss: 38.1084, val_MinusLogProbMetric: 38.1084

Epoch 163: val_loss did not improve from 38.08026
196/196 - 72s - loss: 37.6611 - MinusLogProbMetric: 37.6611 - val_loss: 38.1084 - val_MinusLogProbMetric: 38.1084 - lr: 1.2346e-05 - 72s/epoch - 366ms/step
Epoch 164/1000
2023-09-30 00:18:15.220 
Epoch 164/1000 
	 loss: 37.6328, MinusLogProbMetric: 37.6328, val_loss: 38.1563, val_MinusLogProbMetric: 38.1563

Epoch 164: val_loss did not improve from 38.08026
196/196 - 70s - loss: 37.6328 - MinusLogProbMetric: 37.6328 - val_loss: 38.1563 - val_MinusLogProbMetric: 38.1563 - lr: 1.2346e-05 - 70s/epoch - 355ms/step
Epoch 165/1000
2023-09-30 00:19:24.886 
Epoch 165/1000 
	 loss: 37.7747, MinusLogProbMetric: 37.7747, val_loss: 37.8272, val_MinusLogProbMetric: 37.8272

Epoch 165: val_loss improved from 38.08026 to 37.82716, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 71s - loss: 37.7747 - MinusLogProbMetric: 37.7747 - val_loss: 37.8272 - val_MinusLogProbMetric: 37.8272 - lr: 1.2346e-05 - 71s/epoch - 362ms/step
Epoch 166/1000
2023-09-30 00:20:34.619 
Epoch 166/1000 
	 loss: 38.2292, MinusLogProbMetric: 38.2292, val_loss: 38.1753, val_MinusLogProbMetric: 38.1753

Epoch 166: val_loss did not improve from 37.82716
196/196 - 68s - loss: 38.2292 - MinusLogProbMetric: 38.2292 - val_loss: 38.1753 - val_MinusLogProbMetric: 38.1753 - lr: 1.2346e-05 - 68s/epoch - 349ms/step
Epoch 167/1000
2023-09-30 00:21:41.973 
Epoch 167/1000 
	 loss: 37.4825, MinusLogProbMetric: 37.4825, val_loss: 37.8331, val_MinusLogProbMetric: 37.8331

Epoch 167: val_loss did not improve from 37.82716
196/196 - 67s - loss: 37.4825 - MinusLogProbMetric: 37.4825 - val_loss: 37.8331 - val_MinusLogProbMetric: 37.8331 - lr: 1.2346e-05 - 67s/epoch - 344ms/step
Epoch 168/1000
2023-09-30 00:22:46.193 
Epoch 168/1000 
	 loss: 37.3848, MinusLogProbMetric: 37.3848, val_loss: 38.0140, val_MinusLogProbMetric: 38.0140

Epoch 168: val_loss did not improve from 37.82716
196/196 - 64s - loss: 37.3848 - MinusLogProbMetric: 37.3848 - val_loss: 38.0140 - val_MinusLogProbMetric: 38.0140 - lr: 1.2346e-05 - 64s/epoch - 328ms/step
Epoch 169/1000
2023-09-30 00:23:47.101 
Epoch 169/1000 
	 loss: 37.3443, MinusLogProbMetric: 37.3443, val_loss: 37.7054, val_MinusLogProbMetric: 37.7054

Epoch 169: val_loss improved from 37.82716 to 37.70544, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 62s - loss: 37.3443 - MinusLogProbMetric: 37.3443 - val_loss: 37.7054 - val_MinusLogProbMetric: 37.7054 - lr: 1.2346e-05 - 62s/epoch - 317ms/step
Epoch 170/1000
2023-09-30 00:24:55.224 
Epoch 170/1000 
	 loss: 37.2812, MinusLogProbMetric: 37.2812, val_loss: 37.7158, val_MinusLogProbMetric: 37.7158

Epoch 170: val_loss did not improve from 37.70544
196/196 - 67s - loss: 37.2812 - MinusLogProbMetric: 37.2812 - val_loss: 37.7158 - val_MinusLogProbMetric: 37.7158 - lr: 1.2346e-05 - 67s/epoch - 342ms/step
Epoch 171/1000
2023-09-30 00:25:55.917 
Epoch 171/1000 
	 loss: 37.2508, MinusLogProbMetric: 37.2508, val_loss: 37.5978, val_MinusLogProbMetric: 37.5978

Epoch 171: val_loss improved from 37.70544 to 37.59782, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 62s - loss: 37.2508 - MinusLogProbMetric: 37.2508 - val_loss: 37.5978 - val_MinusLogProbMetric: 37.5978 - lr: 1.2346e-05 - 62s/epoch - 315ms/step
Epoch 172/1000
2023-09-30 00:27:04.201 
Epoch 172/1000 
	 loss: 37.1880, MinusLogProbMetric: 37.1880, val_loss: 37.5836, val_MinusLogProbMetric: 37.5836

Epoch 172: val_loss improved from 37.59782 to 37.58357, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 68s - loss: 37.1880 - MinusLogProbMetric: 37.1880 - val_loss: 37.5836 - val_MinusLogProbMetric: 37.5836 - lr: 1.2346e-05 - 68s/epoch - 348ms/step
Epoch 173/1000
2023-09-30 00:28:14.135 
Epoch 173/1000 
	 loss: 37.1915, MinusLogProbMetric: 37.1915, val_loss: 37.4273, val_MinusLogProbMetric: 37.4273

Epoch 173: val_loss improved from 37.58357 to 37.42733, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 70s - loss: 37.1915 - MinusLogProbMetric: 37.1915 - val_loss: 37.4273 - val_MinusLogProbMetric: 37.4273 - lr: 1.2346e-05 - 70s/epoch - 357ms/step
Epoch 174/1000
2023-09-30 00:29:21.963 
Epoch 174/1000 
	 loss: 37.1397, MinusLogProbMetric: 37.1397, val_loss: 37.6533, val_MinusLogProbMetric: 37.6533

Epoch 174: val_loss did not improve from 37.42733
196/196 - 67s - loss: 37.1397 - MinusLogProbMetric: 37.1397 - val_loss: 37.6533 - val_MinusLogProbMetric: 37.6533 - lr: 1.2346e-05 - 67s/epoch - 341ms/step
Epoch 175/1000
2023-09-30 00:30:29.994 
Epoch 175/1000 
	 loss: 37.0839, MinusLogProbMetric: 37.0839, val_loss: 37.5933, val_MinusLogProbMetric: 37.5933

Epoch 175: val_loss did not improve from 37.42733
196/196 - 68s - loss: 37.0839 - MinusLogProbMetric: 37.0839 - val_loss: 37.5933 - val_MinusLogProbMetric: 37.5933 - lr: 1.2346e-05 - 68s/epoch - 347ms/step
Epoch 176/1000
2023-09-30 00:31:37.564 
Epoch 176/1000 
	 loss: 37.0488, MinusLogProbMetric: 37.0488, val_loss: 37.4214, val_MinusLogProbMetric: 37.4214

Epoch 176: val_loss improved from 37.42733 to 37.42141, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 69s - loss: 37.0488 - MinusLogProbMetric: 37.0488 - val_loss: 37.4214 - val_MinusLogProbMetric: 37.4214 - lr: 1.2346e-05 - 69s/epoch - 352ms/step
Epoch 177/1000
2023-09-30 00:32:42.344 
Epoch 177/1000 
	 loss: 37.6898, MinusLogProbMetric: 37.6898, val_loss: 37.5637, val_MinusLogProbMetric: 37.5637

Epoch 177: val_loss did not improve from 37.42141
196/196 - 63s - loss: 37.6898 - MinusLogProbMetric: 37.6898 - val_loss: 37.5637 - val_MinusLogProbMetric: 37.5637 - lr: 1.2346e-05 - 63s/epoch - 323ms/step
Epoch 178/1000
2023-09-30 00:33:47.600 
Epoch 178/1000 
	 loss: 36.9748, MinusLogProbMetric: 36.9748, val_loss: 37.5979, val_MinusLogProbMetric: 37.5979

Epoch 178: val_loss did not improve from 37.42141
196/196 - 65s - loss: 36.9748 - MinusLogProbMetric: 36.9748 - val_loss: 37.5979 - val_MinusLogProbMetric: 37.5979 - lr: 1.2346e-05 - 65s/epoch - 333ms/step
Epoch 179/1000
2023-09-30 00:34:53.958 
Epoch 179/1000 
	 loss: 37.1371, MinusLogProbMetric: 37.1371, val_loss: 37.7904, val_MinusLogProbMetric: 37.7904

Epoch 179: val_loss did not improve from 37.42141
196/196 - 66s - loss: 37.1371 - MinusLogProbMetric: 37.1371 - val_loss: 37.7904 - val_MinusLogProbMetric: 37.7904 - lr: 1.2346e-05 - 66s/epoch - 339ms/step
Epoch 180/1000
2023-09-30 00:35:56.651 
Epoch 180/1000 
	 loss: 37.0568, MinusLogProbMetric: 37.0568, val_loss: 37.3673, val_MinusLogProbMetric: 37.3673

Epoch 180: val_loss improved from 37.42141 to 37.36732, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 64s - loss: 37.0568 - MinusLogProbMetric: 37.0568 - val_loss: 37.3673 - val_MinusLogProbMetric: 37.3673 - lr: 1.2346e-05 - 64s/epoch - 326ms/step
Epoch 181/1000
2023-09-30 00:37:01.121 
Epoch 181/1000 
	 loss: 36.7906, MinusLogProbMetric: 36.7906, val_loss: 37.1898, val_MinusLogProbMetric: 37.1898

Epoch 181: val_loss improved from 37.36732 to 37.18976, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 64s - loss: 36.7906 - MinusLogProbMetric: 36.7906 - val_loss: 37.1898 - val_MinusLogProbMetric: 37.1898 - lr: 1.2346e-05 - 64s/epoch - 328ms/step
Epoch 182/1000
2023-09-30 00:38:07.374 
Epoch 182/1000 
	 loss: 36.8847, MinusLogProbMetric: 36.8847, val_loss: 37.2393, val_MinusLogProbMetric: 37.2393

Epoch 182: val_loss did not improve from 37.18976
196/196 - 65s - loss: 36.8847 - MinusLogProbMetric: 36.8847 - val_loss: 37.2393 - val_MinusLogProbMetric: 37.2393 - lr: 1.2346e-05 - 65s/epoch - 333ms/step
Epoch 183/1000
2023-09-30 00:39:11.562 
Epoch 183/1000 
	 loss: 36.7591, MinusLogProbMetric: 36.7591, val_loss: 37.0564, val_MinusLogProbMetric: 37.0564

Epoch 183: val_loss improved from 37.18976 to 37.05644, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 65s - loss: 36.7591 - MinusLogProbMetric: 36.7591 - val_loss: 37.0564 - val_MinusLogProbMetric: 37.0564 - lr: 1.2346e-05 - 65s/epoch - 333ms/step
Epoch 184/1000
2023-09-30 00:40:20.417 
Epoch 184/1000 
	 loss: 36.7294, MinusLogProbMetric: 36.7294, val_loss: 37.1044, val_MinusLogProbMetric: 37.1044

Epoch 184: val_loss did not improve from 37.05644
196/196 - 68s - loss: 36.7294 - MinusLogProbMetric: 36.7294 - val_loss: 37.1044 - val_MinusLogProbMetric: 37.1044 - lr: 1.2346e-05 - 68s/epoch - 346ms/step
Epoch 185/1000
2023-09-30 00:41:26.988 
Epoch 185/1000 
	 loss: 36.6484, MinusLogProbMetric: 36.6484, val_loss: 37.0467, val_MinusLogProbMetric: 37.0467

Epoch 185: val_loss improved from 37.05644 to 37.04671, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 68s - loss: 36.6484 - MinusLogProbMetric: 36.6484 - val_loss: 37.0467 - val_MinusLogProbMetric: 37.0467 - lr: 1.2346e-05 - 68s/epoch - 345ms/step
Epoch 186/1000
2023-09-30 00:42:30.391 
Epoch 186/1000 
	 loss: 36.6305, MinusLogProbMetric: 36.6305, val_loss: 36.9770, val_MinusLogProbMetric: 36.9770

Epoch 186: val_loss improved from 37.04671 to 36.97697, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 63s - loss: 36.6305 - MinusLogProbMetric: 36.6305 - val_loss: 36.9770 - val_MinusLogProbMetric: 36.9770 - lr: 1.2346e-05 - 63s/epoch - 323ms/step
Epoch 187/1000
2023-09-30 00:43:35.323 
Epoch 187/1000 
	 loss: 36.6102, MinusLogProbMetric: 36.6102, val_loss: 36.8509, val_MinusLogProbMetric: 36.8509

Epoch 187: val_loss improved from 36.97697 to 36.85087, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 67s - loss: 36.6102 - MinusLogProbMetric: 36.6102 - val_loss: 36.8509 - val_MinusLogProbMetric: 36.8509 - lr: 1.2346e-05 - 67s/epoch - 343ms/step
Epoch 188/1000
2023-09-30 00:44:43.455 
Epoch 188/1000 
	 loss: 36.5689, MinusLogProbMetric: 36.5689, val_loss: 37.0269, val_MinusLogProbMetric: 37.0269

Epoch 188: val_loss did not improve from 36.85087
196/196 - 65s - loss: 36.5689 - MinusLogProbMetric: 36.5689 - val_loss: 37.0269 - val_MinusLogProbMetric: 37.0269 - lr: 1.2346e-05 - 65s/epoch - 332ms/step
Epoch 189/1000
2023-09-30 00:45:47.512 
Epoch 189/1000 
	 loss: 36.6048, MinusLogProbMetric: 36.6048, val_loss: 37.1227, val_MinusLogProbMetric: 37.1227

Epoch 189: val_loss did not improve from 36.85087
196/196 - 64s - loss: 36.6048 - MinusLogProbMetric: 36.6048 - val_loss: 37.1227 - val_MinusLogProbMetric: 37.1227 - lr: 1.2346e-05 - 64s/epoch - 327ms/step
Epoch 190/1000
2023-09-30 00:46:50.458 
Epoch 190/1000 
	 loss: 42.2445, MinusLogProbMetric: 42.2445, val_loss: 40.3204, val_MinusLogProbMetric: 40.3204

Epoch 190: val_loss did not improve from 36.85087
196/196 - 63s - loss: 42.2445 - MinusLogProbMetric: 42.2445 - val_loss: 40.3204 - val_MinusLogProbMetric: 40.3204 - lr: 1.2346e-05 - 63s/epoch - 321ms/step
Epoch 191/1000
2023-09-30 00:47:57.419 
Epoch 191/1000 
	 loss: 37.5319, MinusLogProbMetric: 37.5319, val_loss: 37.3492, val_MinusLogProbMetric: 37.3492

Epoch 191: val_loss did not improve from 36.85087
196/196 - 67s - loss: 37.5319 - MinusLogProbMetric: 37.5319 - val_loss: 37.3492 - val_MinusLogProbMetric: 37.3492 - lr: 1.2346e-05 - 67s/epoch - 342ms/step
Epoch 192/1000
2023-09-30 00:49:00.621 
Epoch 192/1000 
	 loss: 36.7139, MinusLogProbMetric: 36.7139, val_loss: 37.3558, val_MinusLogProbMetric: 37.3558

Epoch 192: val_loss did not improve from 36.85087
196/196 - 63s - loss: 36.7139 - MinusLogProbMetric: 36.7139 - val_loss: 37.3558 - val_MinusLogProbMetric: 37.3558 - lr: 1.2346e-05 - 63s/epoch - 322ms/step
Epoch 193/1000
2023-09-30 00:50:03.014 
Epoch 193/1000 
	 loss: 36.5124, MinusLogProbMetric: 36.5124, val_loss: 36.7477, val_MinusLogProbMetric: 36.7477

Epoch 193: val_loss improved from 36.85087 to 36.74769, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 64s - loss: 36.5124 - MinusLogProbMetric: 36.5124 - val_loss: 36.7477 - val_MinusLogProbMetric: 36.7477 - lr: 1.2346e-05 - 64s/epoch - 326ms/step
Epoch 194/1000
2023-09-30 00:51:06.177 
Epoch 194/1000 
	 loss: 36.3850, MinusLogProbMetric: 36.3850, val_loss: 36.9431, val_MinusLogProbMetric: 36.9431

Epoch 194: val_loss did not improve from 36.74769
196/196 - 62s - loss: 36.3850 - MinusLogProbMetric: 36.3850 - val_loss: 36.9431 - val_MinusLogProbMetric: 36.9431 - lr: 1.2346e-05 - 62s/epoch - 315ms/step
Epoch 195/1000
2023-09-30 00:52:06.886 
Epoch 195/1000 
	 loss: 38.1789, MinusLogProbMetric: 38.1789, val_loss: 36.7988, val_MinusLogProbMetric: 36.7988

Epoch 195: val_loss did not improve from 36.74769
196/196 - 61s - loss: 38.1789 - MinusLogProbMetric: 38.1789 - val_loss: 36.7988 - val_MinusLogProbMetric: 36.7988 - lr: 1.2346e-05 - 61s/epoch - 310ms/step
Epoch 196/1000
2023-09-30 00:53:08.515 
Epoch 196/1000 
	 loss: 36.3357, MinusLogProbMetric: 36.3357, val_loss: 36.6652, val_MinusLogProbMetric: 36.6652

Epoch 196: val_loss improved from 36.74769 to 36.66525, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 63s - loss: 36.3357 - MinusLogProbMetric: 36.3357 - val_loss: 36.6652 - val_MinusLogProbMetric: 36.6652 - lr: 1.2346e-05 - 63s/epoch - 320ms/step
Epoch 197/1000
2023-09-30 00:54:09.268 
Epoch 197/1000 
	 loss: 36.2440, MinusLogProbMetric: 36.2440, val_loss: 36.7042, val_MinusLogProbMetric: 36.7042

Epoch 197: val_loss did not improve from 36.66525
196/196 - 60s - loss: 36.2440 - MinusLogProbMetric: 36.2440 - val_loss: 36.7042 - val_MinusLogProbMetric: 36.7042 - lr: 1.2346e-05 - 60s/epoch - 304ms/step
Epoch 198/1000
2023-09-30 00:55:13.044 
Epoch 198/1000 
	 loss: 36.2307, MinusLogProbMetric: 36.2307, val_loss: 36.5946, val_MinusLogProbMetric: 36.5946

Epoch 198: val_loss improved from 36.66525 to 36.59457, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 66s - loss: 36.2307 - MinusLogProbMetric: 36.2307 - val_loss: 36.5946 - val_MinusLogProbMetric: 36.5946 - lr: 1.2346e-05 - 66s/epoch - 337ms/step
Epoch 199/1000
2023-09-30 00:56:14.457 
Epoch 199/1000 
	 loss: 41.2966, MinusLogProbMetric: 41.2966, val_loss: 37.6070, val_MinusLogProbMetric: 37.6070

Epoch 199: val_loss did not improve from 36.59457
196/196 - 59s - loss: 41.2966 - MinusLogProbMetric: 41.2966 - val_loss: 37.6070 - val_MinusLogProbMetric: 37.6070 - lr: 1.2346e-05 - 59s/epoch - 302ms/step
Epoch 200/1000
2023-09-30 00:57:18.118 
Epoch 200/1000 
	 loss: 36.5966, MinusLogProbMetric: 36.5966, val_loss: 36.6543, val_MinusLogProbMetric: 36.6543

Epoch 200: val_loss did not improve from 36.59457
196/196 - 64s - loss: 36.5966 - MinusLogProbMetric: 36.5966 - val_loss: 36.6543 - val_MinusLogProbMetric: 36.6543 - lr: 1.2346e-05 - 64s/epoch - 325ms/step
Epoch 201/1000
2023-09-30 00:58:18.186 
Epoch 201/1000 
	 loss: 36.2206, MinusLogProbMetric: 36.2206, val_loss: 36.8451, val_MinusLogProbMetric: 36.8451

Epoch 201: val_loss did not improve from 36.59457
196/196 - 60s - loss: 36.2206 - MinusLogProbMetric: 36.2206 - val_loss: 36.8451 - val_MinusLogProbMetric: 36.8451 - lr: 1.2346e-05 - 60s/epoch - 306ms/step
Epoch 202/1000
2023-09-30 00:59:17.361 
Epoch 202/1000 
	 loss: 36.1626, MinusLogProbMetric: 36.1626, val_loss: 36.4432, val_MinusLogProbMetric: 36.4432

Epoch 202: val_loss improved from 36.59457 to 36.44317, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 61s - loss: 36.1626 - MinusLogProbMetric: 36.1626 - val_loss: 36.4432 - val_MinusLogProbMetric: 36.4432 - lr: 1.2346e-05 - 61s/epoch - 309ms/step
Epoch 203/1000
2023-09-30 01:00:19.047 
Epoch 203/1000 
	 loss: 36.0292, MinusLogProbMetric: 36.0292, val_loss: 36.3464, val_MinusLogProbMetric: 36.3464

Epoch 203: val_loss improved from 36.44317 to 36.34640, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 65s - loss: 36.0292 - MinusLogProbMetric: 36.0292 - val_loss: 36.3464 - val_MinusLogProbMetric: 36.3464 - lr: 1.2346e-05 - 65s/epoch - 333ms/step
Epoch 204/1000
2023-09-30 01:01:25.541 
Epoch 204/1000 
	 loss: 36.0288, MinusLogProbMetric: 36.0288, val_loss: 36.3491, val_MinusLogProbMetric: 36.3491

Epoch 204: val_loss did not improve from 36.34640
196/196 - 62s - loss: 36.0288 - MinusLogProbMetric: 36.0288 - val_loss: 36.3491 - val_MinusLogProbMetric: 36.3491 - lr: 1.2346e-05 - 62s/epoch - 314ms/step
Epoch 205/1000
2023-09-30 01:02:24.723 
Epoch 205/1000 
	 loss: 35.9787, MinusLogProbMetric: 35.9787, val_loss: 36.5531, val_MinusLogProbMetric: 36.5531

Epoch 205: val_loss did not improve from 36.34640
196/196 - 59s - loss: 35.9787 - MinusLogProbMetric: 35.9787 - val_loss: 36.5531 - val_MinusLogProbMetric: 36.5531 - lr: 1.2346e-05 - 59s/epoch - 302ms/step
Epoch 206/1000
2023-09-30 01:03:26.380 
Epoch 206/1000 
	 loss: 35.9732, MinusLogProbMetric: 35.9732, val_loss: 36.2598, val_MinusLogProbMetric: 36.2598

Epoch 206: val_loss improved from 36.34640 to 36.25977, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 63s - loss: 35.9732 - MinusLogProbMetric: 35.9732 - val_loss: 36.2598 - val_MinusLogProbMetric: 36.2598 - lr: 1.2346e-05 - 63s/epoch - 323ms/step
Epoch 207/1000
2023-09-30 01:04:29.101 
Epoch 207/1000 
	 loss: 35.9227, MinusLogProbMetric: 35.9227, val_loss: 36.2919, val_MinusLogProbMetric: 36.2919

Epoch 207: val_loss did not improve from 36.25977
196/196 - 61s - loss: 35.9227 - MinusLogProbMetric: 35.9227 - val_loss: 36.2919 - val_MinusLogProbMetric: 36.2919 - lr: 1.2346e-05 - 61s/epoch - 312ms/step
Epoch 208/1000
2023-09-30 01:05:28.425 
Epoch 208/1000 
	 loss: 44.8570, MinusLogProbMetric: 44.8570, val_loss: 51.7408, val_MinusLogProbMetric: 51.7408

Epoch 208: val_loss did not improve from 36.25977
196/196 - 59s - loss: 44.8570 - MinusLogProbMetric: 44.8570 - val_loss: 51.7408 - val_MinusLogProbMetric: 51.7408 - lr: 1.2346e-05 - 59s/epoch - 303ms/step
Epoch 209/1000
2023-09-30 01:06:32.358 
Epoch 209/1000 
	 loss: 40.3313, MinusLogProbMetric: 40.3313, val_loss: 37.3206, val_MinusLogProbMetric: 37.3206

Epoch 209: val_loss did not improve from 36.25977
196/196 - 64s - loss: 40.3313 - MinusLogProbMetric: 40.3313 - val_loss: 37.3206 - val_MinusLogProbMetric: 37.3206 - lr: 1.2346e-05 - 64s/epoch - 326ms/step
Epoch 210/1000
2023-09-30 01:07:34.556 
Epoch 210/1000 
	 loss: 36.3691, MinusLogProbMetric: 36.3691, val_loss: 36.4192, val_MinusLogProbMetric: 36.4192

Epoch 210: val_loss did not improve from 36.25977
196/196 - 62s - loss: 36.3691 - MinusLogProbMetric: 36.3691 - val_loss: 36.4192 - val_MinusLogProbMetric: 36.4192 - lr: 1.2346e-05 - 62s/epoch - 317ms/step
Epoch 211/1000
2023-09-30 01:08:43.989 
Epoch 211/1000 
	 loss: 36.0218, MinusLogProbMetric: 36.0218, val_loss: 36.4769, val_MinusLogProbMetric: 36.4769

Epoch 211: val_loss did not improve from 36.25977
196/196 - 69s - loss: 36.0218 - MinusLogProbMetric: 36.0218 - val_loss: 36.4769 - val_MinusLogProbMetric: 36.4769 - lr: 1.2346e-05 - 69s/epoch - 354ms/step
Epoch 212/1000
2023-09-30 01:09:49.444 
Epoch 212/1000 
	 loss: 35.9548, MinusLogProbMetric: 35.9548, val_loss: 36.1598, val_MinusLogProbMetric: 36.1598

Epoch 212: val_loss improved from 36.25977 to 36.15976, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 66s - loss: 35.9548 - MinusLogProbMetric: 35.9548 - val_loss: 36.1598 - val_MinusLogProbMetric: 36.1598 - lr: 1.2346e-05 - 66s/epoch - 338ms/step
Epoch 213/1000
2023-09-30 01:10:51.084 
Epoch 213/1000 
	 loss: 35.8477, MinusLogProbMetric: 35.8477, val_loss: 36.2204, val_MinusLogProbMetric: 36.2204

Epoch 213: val_loss did not improve from 36.15976
196/196 - 61s - loss: 35.8477 - MinusLogProbMetric: 35.8477 - val_loss: 36.2204 - val_MinusLogProbMetric: 36.2204 - lr: 1.2346e-05 - 61s/epoch - 310ms/step
Epoch 214/1000
2023-09-30 01:11:55.040 
Epoch 214/1000 
	 loss: 35.8248, MinusLogProbMetric: 35.8248, val_loss: 36.3012, val_MinusLogProbMetric: 36.3012

Epoch 214: val_loss did not improve from 36.15976
196/196 - 64s - loss: 35.8248 - MinusLogProbMetric: 35.8248 - val_loss: 36.3012 - val_MinusLogProbMetric: 36.3012 - lr: 1.2346e-05 - 64s/epoch - 326ms/step
Epoch 215/1000
2023-09-30 01:12:56.077 
Epoch 215/1000 
	 loss: 35.8153, MinusLogProbMetric: 35.8153, val_loss: 36.2647, val_MinusLogProbMetric: 36.2647

Epoch 215: val_loss did not improve from 36.15976
196/196 - 61s - loss: 35.8153 - MinusLogProbMetric: 35.8153 - val_loss: 36.2647 - val_MinusLogProbMetric: 36.2647 - lr: 1.2346e-05 - 61s/epoch - 311ms/step
Epoch 216/1000
2023-09-30 01:13:58.867 
Epoch 216/1000 
	 loss: 35.7251, MinusLogProbMetric: 35.7251, val_loss: 36.3594, val_MinusLogProbMetric: 36.3594

Epoch 216: val_loss did not improve from 36.15976
196/196 - 63s - loss: 35.7251 - MinusLogProbMetric: 35.7251 - val_loss: 36.3594 - val_MinusLogProbMetric: 36.3594 - lr: 1.2346e-05 - 63s/epoch - 320ms/step
Epoch 217/1000
2023-09-30 01:15:05.969 
Epoch 217/1000 
	 loss: 35.6897, MinusLogProbMetric: 35.6897, val_loss: 36.0848, val_MinusLogProbMetric: 36.0848

Epoch 217: val_loss improved from 36.15976 to 36.08482, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 69s - loss: 35.6897 - MinusLogProbMetric: 35.6897 - val_loss: 36.0848 - val_MinusLogProbMetric: 36.0848 - lr: 1.2346e-05 - 69s/epoch - 350ms/step
Epoch 218/1000
2023-09-30 01:16:14.523 
Epoch 218/1000 
	 loss: 35.6966, MinusLogProbMetric: 35.6966, val_loss: 36.4543, val_MinusLogProbMetric: 36.4543

Epoch 218: val_loss did not improve from 36.08482
196/196 - 67s - loss: 35.6966 - MinusLogProbMetric: 35.6966 - val_loss: 36.4543 - val_MinusLogProbMetric: 36.4543 - lr: 1.2346e-05 - 67s/epoch - 343ms/step
Epoch 219/1000
2023-09-30 01:17:17.856 
Epoch 219/1000 
	 loss: 35.6133, MinusLogProbMetric: 35.6133, val_loss: 36.1063, val_MinusLogProbMetric: 36.1063

Epoch 219: val_loss did not improve from 36.08482
196/196 - 63s - loss: 35.6133 - MinusLogProbMetric: 35.6133 - val_loss: 36.1063 - val_MinusLogProbMetric: 36.1063 - lr: 1.2346e-05 - 63s/epoch - 323ms/step
Epoch 220/1000
2023-09-30 01:18:24.545 
Epoch 220/1000 
	 loss: 35.5816, MinusLogProbMetric: 35.5816, val_loss: 35.8184, val_MinusLogProbMetric: 35.8184

Epoch 220: val_loss improved from 36.08482 to 35.81839, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 68s - loss: 35.5816 - MinusLogProbMetric: 35.5816 - val_loss: 35.8184 - val_MinusLogProbMetric: 35.8184 - lr: 1.2346e-05 - 68s/epoch - 349ms/step
Epoch 221/1000
2023-09-30 01:19:28.674 
Epoch 221/1000 
	 loss: 35.5386, MinusLogProbMetric: 35.5386, val_loss: 36.0202, val_MinusLogProbMetric: 36.0202

Epoch 221: val_loss did not improve from 35.81839
196/196 - 62s - loss: 35.5386 - MinusLogProbMetric: 35.5386 - val_loss: 36.0202 - val_MinusLogProbMetric: 36.0202 - lr: 1.2346e-05 - 62s/epoch - 319ms/step
Epoch 222/1000
2023-09-30 01:20:36.461 
Epoch 222/1000 
	 loss: 35.5138, MinusLogProbMetric: 35.5138, val_loss: 35.8460, val_MinusLogProbMetric: 35.8460

Epoch 222: val_loss did not improve from 35.81839
196/196 - 68s - loss: 35.5138 - MinusLogProbMetric: 35.5138 - val_loss: 35.8460 - val_MinusLogProbMetric: 35.8460 - lr: 1.2346e-05 - 68s/epoch - 346ms/step
Epoch 223/1000
2023-09-30 01:21:42.698 
Epoch 223/1000 
	 loss: 39.4642, MinusLogProbMetric: 39.4642, val_loss: 37.3653, val_MinusLogProbMetric: 37.3653

Epoch 223: val_loss did not improve from 35.81839
196/196 - 66s - loss: 39.4642 - MinusLogProbMetric: 39.4642 - val_loss: 37.3653 - val_MinusLogProbMetric: 37.3653 - lr: 1.2346e-05 - 66s/epoch - 338ms/step
Epoch 224/1000
2023-09-30 01:22:49.328 
Epoch 224/1000 
	 loss: 36.0654, MinusLogProbMetric: 36.0654, val_loss: 36.1794, val_MinusLogProbMetric: 36.1794

Epoch 224: val_loss did not improve from 35.81839
196/196 - 67s - loss: 36.0654 - MinusLogProbMetric: 36.0654 - val_loss: 36.1794 - val_MinusLogProbMetric: 36.1794 - lr: 1.2346e-05 - 67s/epoch - 340ms/step
Epoch 225/1000
2023-09-30 01:23:50.429 
Epoch 225/1000 
	 loss: 35.7104, MinusLogProbMetric: 35.7104, val_loss: 36.2923, val_MinusLogProbMetric: 36.2923

Epoch 225: val_loss did not improve from 35.81839
196/196 - 61s - loss: 35.7104 - MinusLogProbMetric: 35.7104 - val_loss: 36.2923 - val_MinusLogProbMetric: 36.2923 - lr: 1.2346e-05 - 61s/epoch - 312ms/step
Epoch 226/1000
2023-09-30 01:24:52.440 
Epoch 226/1000 
	 loss: 35.5608, MinusLogProbMetric: 35.5608, val_loss: 36.1420, val_MinusLogProbMetric: 36.1420

Epoch 226: val_loss did not improve from 35.81839
196/196 - 62s - loss: 35.5608 - MinusLogProbMetric: 35.5608 - val_loss: 36.1420 - val_MinusLogProbMetric: 36.1420 - lr: 1.2346e-05 - 62s/epoch - 316ms/step
Epoch 227/1000
2023-09-30 01:25:57.343 
Epoch 227/1000 
	 loss: 35.4887, MinusLogProbMetric: 35.4887, val_loss: 35.8924, val_MinusLogProbMetric: 35.8924

Epoch 227: val_loss did not improve from 35.81839
196/196 - 65s - loss: 35.4887 - MinusLogProbMetric: 35.4887 - val_loss: 35.8924 - val_MinusLogProbMetric: 35.8924 - lr: 1.2346e-05 - 65s/epoch - 331ms/step
Epoch 228/1000
2023-09-30 01:27:01.689 
Epoch 228/1000 
	 loss: 35.4946, MinusLogProbMetric: 35.4946, val_loss: 36.0135, val_MinusLogProbMetric: 36.0135

Epoch 228: val_loss did not improve from 35.81839
196/196 - 64s - loss: 35.4946 - MinusLogProbMetric: 35.4946 - val_loss: 36.0135 - val_MinusLogProbMetric: 36.0135 - lr: 1.2346e-05 - 64s/epoch - 328ms/step
Epoch 229/1000
2023-09-30 01:28:09.083 
Epoch 229/1000 
	 loss: 35.3924, MinusLogProbMetric: 35.3924, val_loss: 36.4600, val_MinusLogProbMetric: 36.4600

Epoch 229: val_loss did not improve from 35.81839
196/196 - 67s - loss: 35.3924 - MinusLogProbMetric: 35.3924 - val_loss: 36.4600 - val_MinusLogProbMetric: 36.4600 - lr: 1.2346e-05 - 67s/epoch - 344ms/step
Epoch 230/1000
2023-09-30 01:29:13.430 
Epoch 230/1000 
	 loss: 35.6564, MinusLogProbMetric: 35.6564, val_loss: 35.7686, val_MinusLogProbMetric: 35.7686

Epoch 230: val_loss improved from 35.81839 to 35.76862, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 66s - loss: 35.6564 - MinusLogProbMetric: 35.6564 - val_loss: 35.7686 - val_MinusLogProbMetric: 35.7686 - lr: 1.2346e-05 - 66s/epoch - 336ms/step
Epoch 231/1000
2023-09-30 01:30:18.280 
Epoch 231/1000 
	 loss: 35.3531, MinusLogProbMetric: 35.3531, val_loss: 35.7031, val_MinusLogProbMetric: 35.7031

Epoch 231: val_loss improved from 35.76862 to 35.70314, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 65s - loss: 35.3531 - MinusLogProbMetric: 35.3531 - val_loss: 35.7031 - val_MinusLogProbMetric: 35.7031 - lr: 1.2346e-05 - 65s/epoch - 333ms/step
Epoch 232/1000
2023-09-30 01:31:28.588 
Epoch 232/1000 
	 loss: 35.3017, MinusLogProbMetric: 35.3017, val_loss: 35.6617, val_MinusLogProbMetric: 35.6617

Epoch 232: val_loss improved from 35.70314 to 35.66173, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 70s - loss: 35.3017 - MinusLogProbMetric: 35.3017 - val_loss: 35.6617 - val_MinusLogProbMetric: 35.6617 - lr: 1.2346e-05 - 70s/epoch - 356ms/step
Epoch 233/1000
2023-09-30 01:32:39.854 
Epoch 233/1000 
	 loss: 35.2927, MinusLogProbMetric: 35.2927, val_loss: 35.7773, val_MinusLogProbMetric: 35.7773

Epoch 233: val_loss did not improve from 35.66173
196/196 - 70s - loss: 35.2927 - MinusLogProbMetric: 35.2927 - val_loss: 35.7773 - val_MinusLogProbMetric: 35.7773 - lr: 1.2346e-05 - 70s/epoch - 357ms/step
Epoch 234/1000
2023-09-30 01:33:50.885 
Epoch 234/1000 
	 loss: 35.2456, MinusLogProbMetric: 35.2456, val_loss: 35.5854, val_MinusLogProbMetric: 35.5854

Epoch 234: val_loss improved from 35.66173 to 35.58540, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 72s - loss: 35.2456 - MinusLogProbMetric: 35.2456 - val_loss: 35.5854 - val_MinusLogProbMetric: 35.5854 - lr: 1.2346e-05 - 72s/epoch - 369ms/step
Epoch 235/1000
2023-09-30 01:35:03.399 
Epoch 235/1000 
	 loss: 35.2209, MinusLogProbMetric: 35.2209, val_loss: 35.7218, val_MinusLogProbMetric: 35.7218

Epoch 235: val_loss did not improve from 35.58540
196/196 - 71s - loss: 35.2209 - MinusLogProbMetric: 35.2209 - val_loss: 35.7218 - val_MinusLogProbMetric: 35.7218 - lr: 1.2346e-05 - 71s/epoch - 364ms/step
Epoch 236/1000
2023-09-30 01:36:14.096 
Epoch 236/1000 
	 loss: 35.2267, MinusLogProbMetric: 35.2267, val_loss: 35.5043, val_MinusLogProbMetric: 35.5043

Epoch 236: val_loss improved from 35.58540 to 35.50426, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 72s - loss: 35.2267 - MinusLogProbMetric: 35.2267 - val_loss: 35.5043 - val_MinusLogProbMetric: 35.5043 - lr: 1.2346e-05 - 72s/epoch - 368ms/step
Epoch 237/1000
2023-09-30 01:37:26.207 
Epoch 237/1000 
	 loss: 35.1385, MinusLogProbMetric: 35.1385, val_loss: 35.6096, val_MinusLogProbMetric: 35.6096

Epoch 237: val_loss did not improve from 35.50426
196/196 - 71s - loss: 35.1385 - MinusLogProbMetric: 35.1385 - val_loss: 35.6096 - val_MinusLogProbMetric: 35.6096 - lr: 1.2346e-05 - 71s/epoch - 361ms/step
Epoch 238/1000
2023-09-30 01:38:38.058 
Epoch 238/1000 
	 loss: 35.1349, MinusLogProbMetric: 35.1349, val_loss: 35.6072, val_MinusLogProbMetric: 35.6072

Epoch 238: val_loss did not improve from 35.50426
196/196 - 72s - loss: 35.1349 - MinusLogProbMetric: 35.1349 - val_loss: 35.6072 - val_MinusLogProbMetric: 35.6072 - lr: 1.2346e-05 - 72s/epoch - 367ms/step
Epoch 239/1000
2023-09-30 01:39:48.935 
Epoch 239/1000 
	 loss: 35.0990, MinusLogProbMetric: 35.0990, val_loss: 35.5773, val_MinusLogProbMetric: 35.5773

Epoch 239: val_loss did not improve from 35.50426
196/196 - 71s - loss: 35.0990 - MinusLogProbMetric: 35.0990 - val_loss: 35.5773 - val_MinusLogProbMetric: 35.5773 - lr: 1.2346e-05 - 71s/epoch - 362ms/step
Epoch 240/1000
2023-09-30 01:40:59.798 
Epoch 240/1000 
	 loss: 35.0679, MinusLogProbMetric: 35.0679, val_loss: 35.4533, val_MinusLogProbMetric: 35.4533

Epoch 240: val_loss improved from 35.50426 to 35.45328, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 72s - loss: 35.0679 - MinusLogProbMetric: 35.0679 - val_loss: 35.4533 - val_MinusLogProbMetric: 35.4533 - lr: 1.2346e-05 - 72s/epoch - 369ms/step
Epoch 241/1000
2023-09-30 01:42:13.097 
Epoch 241/1000 
	 loss: 35.0309, MinusLogProbMetric: 35.0309, val_loss: 35.4834, val_MinusLogProbMetric: 35.4834

Epoch 241: val_loss did not improve from 35.45328
196/196 - 72s - loss: 35.0309 - MinusLogProbMetric: 35.0309 - val_loss: 35.4834 - val_MinusLogProbMetric: 35.4834 - lr: 1.2346e-05 - 72s/epoch - 366ms/step
Epoch 242/1000
2023-09-30 01:43:23.305 
Epoch 242/1000 
	 loss: 35.0456, MinusLogProbMetric: 35.0456, val_loss: 35.4831, val_MinusLogProbMetric: 35.4831

Epoch 242: val_loss did not improve from 35.45328
196/196 - 70s - loss: 35.0456 - MinusLogProbMetric: 35.0456 - val_loss: 35.4831 - val_MinusLogProbMetric: 35.4831 - lr: 1.2346e-05 - 70s/epoch - 358ms/step
Epoch 243/1000
2023-09-30 01:44:34.574 
Epoch 243/1000 
	 loss: 34.9896, MinusLogProbMetric: 34.9896, val_loss: 35.6084, val_MinusLogProbMetric: 35.6084

Epoch 243: val_loss did not improve from 35.45328
196/196 - 71s - loss: 34.9896 - MinusLogProbMetric: 34.9896 - val_loss: 35.6084 - val_MinusLogProbMetric: 35.6084 - lr: 1.2346e-05 - 71s/epoch - 364ms/step
Epoch 244/1000
2023-09-30 01:45:45.898 
Epoch 244/1000 
	 loss: 35.0115, MinusLogProbMetric: 35.0115, val_loss: 35.3544, val_MinusLogProbMetric: 35.3544

Epoch 244: val_loss improved from 35.45328 to 35.35437, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 73s - loss: 35.0115 - MinusLogProbMetric: 35.0115 - val_loss: 35.3544 - val_MinusLogProbMetric: 35.3544 - lr: 1.2346e-05 - 73s/epoch - 370ms/step
Epoch 245/1000
2023-09-30 01:46:58.076 
Epoch 245/1000 
	 loss: 34.9488, MinusLogProbMetric: 34.9488, val_loss: 35.3672, val_MinusLogProbMetric: 35.3672

Epoch 245: val_loss did not improve from 35.35437
196/196 - 71s - loss: 34.9488 - MinusLogProbMetric: 34.9488 - val_loss: 35.3672 - val_MinusLogProbMetric: 35.3672 - lr: 1.2346e-05 - 71s/epoch - 362ms/step
Epoch 246/1000
2023-09-30 01:48:09.034 
Epoch 246/1000 
	 loss: 34.9338, MinusLogProbMetric: 34.9338, val_loss: 35.3651, val_MinusLogProbMetric: 35.3651

Epoch 246: val_loss did not improve from 35.35437
196/196 - 71s - loss: 34.9338 - MinusLogProbMetric: 34.9338 - val_loss: 35.3651 - val_MinusLogProbMetric: 35.3651 - lr: 1.2346e-05 - 71s/epoch - 362ms/step
Epoch 247/1000
2023-09-30 01:49:20.773 
Epoch 247/1000 
	 loss: 34.9111, MinusLogProbMetric: 34.9111, val_loss: 35.2681, val_MinusLogProbMetric: 35.2681

Epoch 247: val_loss improved from 35.35437 to 35.26815, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 73s - loss: 34.9111 - MinusLogProbMetric: 34.9111 - val_loss: 35.2681 - val_MinusLogProbMetric: 35.2681 - lr: 1.2346e-05 - 73s/epoch - 373ms/step
Epoch 248/1000
2023-09-30 01:50:33.045 
Epoch 248/1000 
	 loss: 34.8736, MinusLogProbMetric: 34.8736, val_loss: 35.2096, val_MinusLogProbMetric: 35.2096

Epoch 248: val_loss improved from 35.26815 to 35.20962, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 72s - loss: 34.8736 - MinusLogProbMetric: 34.8736 - val_loss: 35.2096 - val_MinusLogProbMetric: 35.2096 - lr: 1.2346e-05 - 72s/epoch - 369ms/step
Epoch 249/1000
2023-09-30 01:51:46.219 
Epoch 249/1000 
	 loss: 34.8718, MinusLogProbMetric: 34.8718, val_loss: 35.1858, val_MinusLogProbMetric: 35.1858

Epoch 249: val_loss improved from 35.20962 to 35.18581, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 73s - loss: 34.8718 - MinusLogProbMetric: 34.8718 - val_loss: 35.1858 - val_MinusLogProbMetric: 35.1858 - lr: 1.2346e-05 - 73s/epoch - 373ms/step
Epoch 250/1000
2023-09-30 01:52:58.631 
Epoch 250/1000 
	 loss: 34.8488, MinusLogProbMetric: 34.8488, val_loss: 35.5340, val_MinusLogProbMetric: 35.5340

Epoch 250: val_loss did not improve from 35.18581
196/196 - 71s - loss: 34.8488 - MinusLogProbMetric: 34.8488 - val_loss: 35.5340 - val_MinusLogProbMetric: 35.5340 - lr: 1.2346e-05 - 71s/epoch - 362ms/step
Epoch 251/1000
2023-09-30 01:54:09.645 
Epoch 251/1000 
	 loss: 34.8138, MinusLogProbMetric: 34.8138, val_loss: 35.3484, val_MinusLogProbMetric: 35.3484

Epoch 251: val_loss did not improve from 35.18581
196/196 - 71s - loss: 34.8138 - MinusLogProbMetric: 34.8138 - val_loss: 35.3484 - val_MinusLogProbMetric: 35.3484 - lr: 1.2346e-05 - 71s/epoch - 362ms/step
Epoch 252/1000
2023-09-30 01:55:20.725 
Epoch 252/1000 
	 loss: 34.8920, MinusLogProbMetric: 34.8920, val_loss: 35.1304, val_MinusLogProbMetric: 35.1304

Epoch 252: val_loss improved from 35.18581 to 35.13042, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 72s - loss: 34.8920 - MinusLogProbMetric: 34.8920 - val_loss: 35.1304 - val_MinusLogProbMetric: 35.1304 - lr: 1.2346e-05 - 72s/epoch - 369ms/step
Epoch 253/1000
2023-09-30 01:56:33.496 
Epoch 253/1000 
	 loss: 34.8045, MinusLogProbMetric: 34.8045, val_loss: 35.3267, val_MinusLogProbMetric: 35.3267

Epoch 253: val_loss did not improve from 35.13042
196/196 - 72s - loss: 34.8045 - MinusLogProbMetric: 34.8045 - val_loss: 35.3267 - val_MinusLogProbMetric: 35.3267 - lr: 1.2346e-05 - 72s/epoch - 365ms/step
Epoch 254/1000
2023-09-30 01:57:44.537 
Epoch 254/1000 
	 loss: 34.7544, MinusLogProbMetric: 34.7544, val_loss: 35.1697, val_MinusLogProbMetric: 35.1697

Epoch 254: val_loss did not improve from 35.13042
196/196 - 71s - loss: 34.7544 - MinusLogProbMetric: 34.7544 - val_loss: 35.1697 - val_MinusLogProbMetric: 35.1697 - lr: 1.2346e-05 - 71s/epoch - 362ms/step
Epoch 255/1000
2023-09-30 01:58:55.193 
Epoch 255/1000 
	 loss: 34.7220, MinusLogProbMetric: 34.7220, val_loss: 35.0968, val_MinusLogProbMetric: 35.0968

Epoch 255: val_loss improved from 35.13042 to 35.09680, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 73s - loss: 34.7220 - MinusLogProbMetric: 34.7220 - val_loss: 35.0968 - val_MinusLogProbMetric: 35.0968 - lr: 1.2346e-05 - 73s/epoch - 371ms/step
Epoch 256/1000
2023-09-30 02:00:08.526 
Epoch 256/1000 
	 loss: 35.0486, MinusLogProbMetric: 35.0486, val_loss: 35.0621, val_MinusLogProbMetric: 35.0621

Epoch 256: val_loss improved from 35.09680 to 35.06213, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 74s - loss: 35.0486 - MinusLogProbMetric: 35.0486 - val_loss: 35.0621 - val_MinusLogProbMetric: 35.0621 - lr: 1.2346e-05 - 74s/epoch - 377ms/step
Epoch 257/1000
2023-09-30 02:01:21.751 
Epoch 257/1000 
	 loss: 34.6571, MinusLogProbMetric: 34.6571, val_loss: 35.2425, val_MinusLogProbMetric: 35.2425

Epoch 257: val_loss did not improve from 35.06213
196/196 - 71s - loss: 34.6571 - MinusLogProbMetric: 34.6571 - val_loss: 35.2425 - val_MinusLogProbMetric: 35.2425 - lr: 1.2346e-05 - 71s/epoch - 361ms/step
Epoch 258/1000
2023-09-30 02:02:32.498 
Epoch 258/1000 
	 loss: 34.6458, MinusLogProbMetric: 34.6458, val_loss: 34.9624, val_MinusLogProbMetric: 34.9624

Epoch 258: val_loss improved from 35.06213 to 34.96244, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 72s - loss: 34.6458 - MinusLogProbMetric: 34.6458 - val_loss: 34.9624 - val_MinusLogProbMetric: 34.9624 - lr: 1.2346e-05 - 72s/epoch - 367ms/step
Epoch 259/1000
2023-09-30 02:03:38.287 
Epoch 259/1000 
	 loss: 34.8011, MinusLogProbMetric: 34.8011, val_loss: 35.2087, val_MinusLogProbMetric: 35.2087

Epoch 259: val_loss did not improve from 34.96244
196/196 - 65s - loss: 34.8011 - MinusLogProbMetric: 34.8011 - val_loss: 35.2087 - val_MinusLogProbMetric: 35.2087 - lr: 1.2346e-05 - 65s/epoch - 329ms/step
Epoch 260/1000
2023-09-30 02:04:43.500 
Epoch 260/1000 
	 loss: 34.6216, MinusLogProbMetric: 34.6216, val_loss: 35.0065, val_MinusLogProbMetric: 35.0065

Epoch 260: val_loss did not improve from 34.96244
196/196 - 65s - loss: 34.6216 - MinusLogProbMetric: 34.6216 - val_loss: 35.0065 - val_MinusLogProbMetric: 35.0065 - lr: 1.2346e-05 - 65s/epoch - 333ms/step
Epoch 261/1000
2023-09-30 02:05:47.469 
Epoch 261/1000 
	 loss: 34.9204, MinusLogProbMetric: 34.9204, val_loss: 36.7714, val_MinusLogProbMetric: 36.7714

Epoch 261: val_loss did not improve from 34.96244
196/196 - 64s - loss: 34.9204 - MinusLogProbMetric: 34.9204 - val_loss: 36.7714 - val_MinusLogProbMetric: 36.7714 - lr: 1.2346e-05 - 64s/epoch - 326ms/step
Epoch 262/1000
2023-09-30 02:06:49.162 
Epoch 262/1000 
	 loss: 34.6709, MinusLogProbMetric: 34.6709, val_loss: 34.9199, val_MinusLogProbMetric: 34.9199

Epoch 262: val_loss improved from 34.96244 to 34.91993, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 63s - loss: 34.6709 - MinusLogProbMetric: 34.6709 - val_loss: 34.9199 - val_MinusLogProbMetric: 34.9199 - lr: 1.2346e-05 - 63s/epoch - 322ms/step
Epoch 263/1000
2023-09-30 02:07:58.566 
Epoch 263/1000 
	 loss: 34.5777, MinusLogProbMetric: 34.5777, val_loss: 34.9085, val_MinusLogProbMetric: 34.9085

Epoch 263: val_loss improved from 34.91993 to 34.90852, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 69s - loss: 34.5777 - MinusLogProbMetric: 34.5777 - val_loss: 34.9085 - val_MinusLogProbMetric: 34.9085 - lr: 1.2346e-05 - 69s/epoch - 353ms/step
Epoch 264/1000
2023-09-30 02:09:04.985 
Epoch 264/1000 
	 loss: 34.5125, MinusLogProbMetric: 34.5125, val_loss: 34.9176, val_MinusLogProbMetric: 34.9176

Epoch 264: val_loss did not improve from 34.90852
196/196 - 65s - loss: 34.5125 - MinusLogProbMetric: 34.5125 - val_loss: 34.9176 - val_MinusLogProbMetric: 34.9176 - lr: 1.2346e-05 - 65s/epoch - 333ms/step
Epoch 265/1000
2023-09-30 02:10:12.469 
Epoch 265/1000 
	 loss: 34.5152, MinusLogProbMetric: 34.5152, val_loss: 34.9864, val_MinusLogProbMetric: 34.9864

Epoch 265: val_loss did not improve from 34.90852
196/196 - 67s - loss: 34.5152 - MinusLogProbMetric: 34.5152 - val_loss: 34.9864 - val_MinusLogProbMetric: 34.9864 - lr: 1.2346e-05 - 67s/epoch - 344ms/step
Epoch 266/1000
2023-09-30 02:11:19.827 
Epoch 266/1000 
	 loss: 34.4717, MinusLogProbMetric: 34.4717, val_loss: 34.9062, val_MinusLogProbMetric: 34.9062

Epoch 266: val_loss improved from 34.90852 to 34.90615, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 68s - loss: 34.4717 - MinusLogProbMetric: 34.4717 - val_loss: 34.9062 - val_MinusLogProbMetric: 34.9062 - lr: 1.2346e-05 - 68s/epoch - 348ms/step
Epoch 267/1000
2023-09-30 02:12:24.050 
Epoch 267/1000 
	 loss: 34.4336, MinusLogProbMetric: 34.4336, val_loss: 34.7373, val_MinusLogProbMetric: 34.7373

Epoch 267: val_loss improved from 34.90615 to 34.73734, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 65s - loss: 34.4336 - MinusLogProbMetric: 34.4336 - val_loss: 34.7373 - val_MinusLogProbMetric: 34.7373 - lr: 1.2346e-05 - 65s/epoch - 330ms/step
Epoch 268/1000
2023-09-30 02:13:28.899 
Epoch 268/1000 
	 loss: 34.4708, MinusLogProbMetric: 34.4708, val_loss: 34.8773, val_MinusLogProbMetric: 34.8773

Epoch 268: val_loss did not improve from 34.73734
196/196 - 63s - loss: 34.4708 - MinusLogProbMetric: 34.4708 - val_loss: 34.8773 - val_MinusLogProbMetric: 34.8773 - lr: 1.2346e-05 - 63s/epoch - 323ms/step
Epoch 269/1000
2023-09-30 02:14:36.110 
Epoch 269/1000 
	 loss: 34.4242, MinusLogProbMetric: 34.4242, val_loss: 34.9206, val_MinusLogProbMetric: 34.9206

Epoch 269: val_loss did not improve from 34.73734
196/196 - 67s - loss: 34.4242 - MinusLogProbMetric: 34.4242 - val_loss: 34.9206 - val_MinusLogProbMetric: 34.9206 - lr: 1.2346e-05 - 67s/epoch - 343ms/step
Epoch 270/1000
2023-09-30 02:15:40.430 
Epoch 270/1000 
	 loss: 34.4400, MinusLogProbMetric: 34.4400, val_loss: 35.0664, val_MinusLogProbMetric: 35.0664

Epoch 270: val_loss did not improve from 34.73734
196/196 - 64s - loss: 34.4400 - MinusLogProbMetric: 34.4400 - val_loss: 35.0664 - val_MinusLogProbMetric: 35.0664 - lr: 1.2346e-05 - 64s/epoch - 328ms/step
Epoch 271/1000
2023-09-30 02:16:46.404 
Epoch 271/1000 
	 loss: 34.4384, MinusLogProbMetric: 34.4384, val_loss: 35.0877, val_MinusLogProbMetric: 35.0877

Epoch 271: val_loss did not improve from 34.73734
196/196 - 66s - loss: 34.4384 - MinusLogProbMetric: 34.4384 - val_loss: 35.0877 - val_MinusLogProbMetric: 35.0877 - lr: 1.2346e-05 - 66s/epoch - 337ms/step
Epoch 272/1000
2023-09-30 02:17:52.540 
Epoch 272/1000 
	 loss: 34.3905, MinusLogProbMetric: 34.3905, val_loss: 34.6870, val_MinusLogProbMetric: 34.6870

Epoch 272: val_loss improved from 34.73734 to 34.68697, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 68s - loss: 34.3905 - MinusLogProbMetric: 34.3905 - val_loss: 34.6870 - val_MinusLogProbMetric: 34.6870 - lr: 1.2346e-05 - 68s/epoch - 344ms/step
Epoch 273/1000
2023-09-30 02:18:59.803 
Epoch 273/1000 
	 loss: 34.3506, MinusLogProbMetric: 34.3506, val_loss: 34.7973, val_MinusLogProbMetric: 34.7973

Epoch 273: val_loss did not improve from 34.68697
196/196 - 66s - loss: 34.3506 - MinusLogProbMetric: 34.3506 - val_loss: 34.7973 - val_MinusLogProbMetric: 34.7973 - lr: 1.2346e-05 - 66s/epoch - 336ms/step
Epoch 274/1000
2023-09-30 02:20:02.042 
Epoch 274/1000 
	 loss: 34.3492, MinusLogProbMetric: 34.3492, val_loss: 34.7395, val_MinusLogProbMetric: 34.7395

Epoch 274: val_loss did not improve from 34.68697
196/196 - 62s - loss: 34.3492 - MinusLogProbMetric: 34.3492 - val_loss: 34.7395 - val_MinusLogProbMetric: 34.7395 - lr: 1.2346e-05 - 62s/epoch - 318ms/step
Epoch 275/1000
2023-09-30 02:21:09.867 
Epoch 275/1000 
	 loss: 34.3067, MinusLogProbMetric: 34.3067, val_loss: 34.6379, val_MinusLogProbMetric: 34.6379

Epoch 275: val_loss improved from 34.68697 to 34.63786, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 69s - loss: 34.3067 - MinusLogProbMetric: 34.3067 - val_loss: 34.6379 - val_MinusLogProbMetric: 34.6379 - lr: 1.2346e-05 - 69s/epoch - 354ms/step
Epoch 276/1000
2023-09-30 02:22:15.169 
Epoch 276/1000 
	 loss: 34.2881, MinusLogProbMetric: 34.2881, val_loss: 34.9411, val_MinusLogProbMetric: 34.9411

Epoch 276: val_loss did not improve from 34.63786
196/196 - 64s - loss: 34.2881 - MinusLogProbMetric: 34.2881 - val_loss: 34.9411 - val_MinusLogProbMetric: 34.9411 - lr: 1.2346e-05 - 64s/epoch - 326ms/step
Epoch 277/1000
2023-09-30 02:23:23.200 
Epoch 277/1000 
	 loss: 34.3654, MinusLogProbMetric: 34.3654, val_loss: 34.6751, val_MinusLogProbMetric: 34.6751

Epoch 277: val_loss did not improve from 34.63786
196/196 - 68s - loss: 34.3654 - MinusLogProbMetric: 34.3654 - val_loss: 34.6751 - val_MinusLogProbMetric: 34.6751 - lr: 1.2346e-05 - 68s/epoch - 347ms/step
Epoch 278/1000
2023-09-30 02:24:29.220 
Epoch 278/1000 
	 loss: 34.2435, MinusLogProbMetric: 34.2435, val_loss: 34.7244, val_MinusLogProbMetric: 34.7244

Epoch 278: val_loss did not improve from 34.63786
196/196 - 66s - loss: 34.2435 - MinusLogProbMetric: 34.2435 - val_loss: 34.7244 - val_MinusLogProbMetric: 34.7244 - lr: 1.2346e-05 - 66s/epoch - 337ms/step
Epoch 279/1000
2023-09-30 02:25:38.595 
Epoch 279/1000 
	 loss: 34.3266, MinusLogProbMetric: 34.3266, val_loss: 34.7173, val_MinusLogProbMetric: 34.7173

Epoch 279: val_loss did not improve from 34.63786
196/196 - 69s - loss: 34.3266 - MinusLogProbMetric: 34.3266 - val_loss: 34.7173 - val_MinusLogProbMetric: 34.7173 - lr: 1.2346e-05 - 69s/epoch - 354ms/step
Epoch 280/1000
2023-09-30 02:26:47.954 
Epoch 280/1000 
	 loss: 34.2515, MinusLogProbMetric: 34.2515, val_loss: 34.6452, val_MinusLogProbMetric: 34.6452

Epoch 280: val_loss did not improve from 34.63786
196/196 - 69s - loss: 34.2515 - MinusLogProbMetric: 34.2515 - val_loss: 34.6452 - val_MinusLogProbMetric: 34.6452 - lr: 1.2346e-05 - 69s/epoch - 354ms/step
Epoch 281/1000
2023-09-30 02:27:57.106 
Epoch 281/1000 
	 loss: 34.2107, MinusLogProbMetric: 34.2107, val_loss: 34.7552, val_MinusLogProbMetric: 34.7552

Epoch 281: val_loss did not improve from 34.63786
196/196 - 69s - loss: 34.2107 - MinusLogProbMetric: 34.2107 - val_loss: 34.7552 - val_MinusLogProbMetric: 34.7552 - lr: 1.2346e-05 - 69s/epoch - 353ms/step
Epoch 282/1000
2023-09-30 02:29:06.708 
Epoch 282/1000 
	 loss: 34.1474, MinusLogProbMetric: 34.1474, val_loss: 34.4543, val_MinusLogProbMetric: 34.4543

Epoch 282: val_loss improved from 34.63786 to 34.45435, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 71s - loss: 34.1474 - MinusLogProbMetric: 34.1474 - val_loss: 34.4543 - val_MinusLogProbMetric: 34.4543 - lr: 1.2346e-05 - 71s/epoch - 361ms/step
Epoch 283/1000
2023-09-30 02:30:16.580 
Epoch 283/1000 
	 loss: 34.1475, MinusLogProbMetric: 34.1475, val_loss: 34.5195, val_MinusLogProbMetric: 34.5195

Epoch 283: val_loss did not improve from 34.45435
196/196 - 69s - loss: 34.1475 - MinusLogProbMetric: 34.1475 - val_loss: 34.5195 - val_MinusLogProbMetric: 34.5195 - lr: 1.2346e-05 - 69s/epoch - 350ms/step
Epoch 284/1000
2023-09-30 02:31:23.563 
Epoch 284/1000 
	 loss: 34.1739, MinusLogProbMetric: 34.1739, val_loss: 34.5404, val_MinusLogProbMetric: 34.5404

Epoch 284: val_loss did not improve from 34.45435
196/196 - 67s - loss: 34.1739 - MinusLogProbMetric: 34.1739 - val_loss: 34.5404 - val_MinusLogProbMetric: 34.5404 - lr: 1.2346e-05 - 67s/epoch - 342ms/step
Epoch 285/1000
2023-09-30 02:32:30.696 
Epoch 285/1000 
	 loss: 34.1844, MinusLogProbMetric: 34.1844, val_loss: 34.5167, val_MinusLogProbMetric: 34.5167

Epoch 285: val_loss did not improve from 34.45435
196/196 - 67s - loss: 34.1844 - MinusLogProbMetric: 34.1844 - val_loss: 34.5167 - val_MinusLogProbMetric: 34.5167 - lr: 1.2346e-05 - 67s/epoch - 342ms/step
Epoch 286/1000
2023-09-30 02:33:36.605 
Epoch 286/1000 
	 loss: 34.1089, MinusLogProbMetric: 34.1089, val_loss: 34.6433, val_MinusLogProbMetric: 34.6433

Epoch 286: val_loss did not improve from 34.45435
196/196 - 66s - loss: 34.1089 - MinusLogProbMetric: 34.1089 - val_loss: 34.6433 - val_MinusLogProbMetric: 34.6433 - lr: 1.2346e-05 - 66s/epoch - 336ms/step
Epoch 287/1000
2023-09-30 02:34:44.956 
Epoch 287/1000 
	 loss: 34.1101, MinusLogProbMetric: 34.1101, val_loss: 34.4087, val_MinusLogProbMetric: 34.4087

Epoch 287: val_loss improved from 34.45435 to 34.40868, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 70s - loss: 34.1101 - MinusLogProbMetric: 34.1101 - val_loss: 34.4087 - val_MinusLogProbMetric: 34.4087 - lr: 1.2346e-05 - 70s/epoch - 356ms/step
Epoch 288/1000
2023-09-30 02:35:55.837 
Epoch 288/1000 
	 loss: 34.0744, MinusLogProbMetric: 34.0744, val_loss: 34.5722, val_MinusLogProbMetric: 34.5722

Epoch 288: val_loss did not improve from 34.40868
196/196 - 70s - loss: 34.0744 - MinusLogProbMetric: 34.0744 - val_loss: 34.5722 - val_MinusLogProbMetric: 34.5722 - lr: 1.2346e-05 - 70s/epoch - 355ms/step
Epoch 289/1000
2023-09-30 02:37:03.025 
Epoch 289/1000 
	 loss: 34.0583, MinusLogProbMetric: 34.0583, val_loss: 34.6444, val_MinusLogProbMetric: 34.6444

Epoch 289: val_loss did not improve from 34.40868
196/196 - 67s - loss: 34.0583 - MinusLogProbMetric: 34.0583 - val_loss: 34.6444 - val_MinusLogProbMetric: 34.6444 - lr: 1.2346e-05 - 67s/epoch - 343ms/step
Epoch 290/1000
2023-09-30 02:38:12.465 
Epoch 290/1000 
	 loss: 34.0368, MinusLogProbMetric: 34.0368, val_loss: 34.6967, val_MinusLogProbMetric: 34.6967

Epoch 290: val_loss did not improve from 34.40868
196/196 - 69s - loss: 34.0368 - MinusLogProbMetric: 34.0368 - val_loss: 34.6967 - val_MinusLogProbMetric: 34.6967 - lr: 1.2346e-05 - 69s/epoch - 354ms/step
Epoch 291/1000
2023-09-30 02:39:18.000 
Epoch 291/1000 
	 loss: 34.0528, MinusLogProbMetric: 34.0528, val_loss: 34.7875, val_MinusLogProbMetric: 34.7875

Epoch 291: val_loss did not improve from 34.40868
196/196 - 66s - loss: 34.0528 - MinusLogProbMetric: 34.0528 - val_loss: 34.7875 - val_MinusLogProbMetric: 34.7875 - lr: 1.2346e-05 - 66s/epoch - 334ms/step
Epoch 292/1000
2023-09-30 02:40:27.204 
Epoch 292/1000 
	 loss: 34.0419, MinusLogProbMetric: 34.0419, val_loss: 34.3590, val_MinusLogProbMetric: 34.3590

Epoch 292: val_loss improved from 34.40868 to 34.35904, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 70s - loss: 34.0419 - MinusLogProbMetric: 34.0419 - val_loss: 34.3590 - val_MinusLogProbMetric: 34.3590 - lr: 1.2346e-05 - 70s/epoch - 358ms/step
Epoch 293/1000
2023-09-30 02:41:35.805 
Epoch 293/1000 
	 loss: 33.9757, MinusLogProbMetric: 33.9757, val_loss: 34.5532, val_MinusLogProbMetric: 34.5532

Epoch 293: val_loss did not improve from 34.35904
196/196 - 68s - loss: 33.9757 - MinusLogProbMetric: 33.9757 - val_loss: 34.5532 - val_MinusLogProbMetric: 34.5532 - lr: 1.2346e-05 - 68s/epoch - 345ms/step
Epoch 294/1000
2023-09-30 02:42:44.994 
Epoch 294/1000 
	 loss: 33.9721, MinusLogProbMetric: 33.9721, val_loss: 34.4007, val_MinusLogProbMetric: 34.4007

Epoch 294: val_loss did not improve from 34.35904
196/196 - 69s - loss: 33.9721 - MinusLogProbMetric: 33.9721 - val_loss: 34.4007 - val_MinusLogProbMetric: 34.4007 - lr: 1.2346e-05 - 69s/epoch - 353ms/step
Epoch 295/1000
2023-09-30 02:43:52.043 
Epoch 295/1000 
	 loss: 34.0008, MinusLogProbMetric: 34.0008, val_loss: 35.1972, val_MinusLogProbMetric: 35.1972

Epoch 295: val_loss did not improve from 34.35904
196/196 - 67s - loss: 34.0008 - MinusLogProbMetric: 34.0008 - val_loss: 35.1972 - val_MinusLogProbMetric: 35.1972 - lr: 1.2346e-05 - 67s/epoch - 342ms/step
Epoch 296/1000
2023-09-30 02:44:57.812 
Epoch 296/1000 
	 loss: 33.9929, MinusLogProbMetric: 33.9929, val_loss: 34.5049, val_MinusLogProbMetric: 34.5049

Epoch 296: val_loss did not improve from 34.35904
196/196 - 66s - loss: 33.9929 - MinusLogProbMetric: 33.9929 - val_loss: 34.5049 - val_MinusLogProbMetric: 34.5049 - lr: 1.2346e-05 - 66s/epoch - 336ms/step
Epoch 297/1000
2023-09-30 02:46:05.853 
Epoch 297/1000 
	 loss: 33.8980, MinusLogProbMetric: 33.8980, val_loss: 34.4488, val_MinusLogProbMetric: 34.4488

Epoch 297: val_loss did not improve from 34.35904
196/196 - 68s - loss: 33.8980 - MinusLogProbMetric: 33.8980 - val_loss: 34.4488 - val_MinusLogProbMetric: 34.4488 - lr: 1.2346e-05 - 68s/epoch - 347ms/step
Epoch 298/1000
2023-09-30 02:47:15.315 
Epoch 298/1000 
	 loss: 33.9257, MinusLogProbMetric: 33.9257, val_loss: 34.4647, val_MinusLogProbMetric: 34.4647

Epoch 298: val_loss did not improve from 34.35904
196/196 - 69s - loss: 33.9257 - MinusLogProbMetric: 33.9257 - val_loss: 34.4647 - val_MinusLogProbMetric: 34.4647 - lr: 1.2346e-05 - 69s/epoch - 354ms/step
Epoch 299/1000
2023-09-30 02:48:22.906 
Epoch 299/1000 
	 loss: 33.9234, MinusLogProbMetric: 33.9234, val_loss: 34.3111, val_MinusLogProbMetric: 34.3111

Epoch 299: val_loss improved from 34.35904 to 34.31107, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 69s - loss: 33.9234 - MinusLogProbMetric: 33.9234 - val_loss: 34.3111 - val_MinusLogProbMetric: 34.3111 - lr: 1.2346e-05 - 69s/epoch - 352ms/step
Epoch 300/1000
2023-09-30 02:49:28.380 
Epoch 300/1000 
	 loss: 33.8592, MinusLogProbMetric: 33.8592, val_loss: 34.1500, val_MinusLogProbMetric: 34.1500

Epoch 300: val_loss improved from 34.31107 to 34.14996, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 65s - loss: 33.8592 - MinusLogProbMetric: 33.8592 - val_loss: 34.1500 - val_MinusLogProbMetric: 34.1500 - lr: 1.2346e-05 - 65s/epoch - 332ms/step
Epoch 301/1000
2023-09-30 02:50:35.808 
Epoch 301/1000 
	 loss: 41.2119, MinusLogProbMetric: 41.2119, val_loss: 35.1156, val_MinusLogProbMetric: 35.1156

Epoch 301: val_loss did not improve from 34.14996
196/196 - 66s - loss: 41.2119 - MinusLogProbMetric: 41.2119 - val_loss: 35.1156 - val_MinusLogProbMetric: 35.1156 - lr: 1.2346e-05 - 66s/epoch - 339ms/step
Epoch 302/1000
2023-09-30 02:51:41.190 
Epoch 302/1000 
	 loss: 34.5259, MinusLogProbMetric: 34.5259, val_loss: 34.7253, val_MinusLogProbMetric: 34.7253

Epoch 302: val_loss did not improve from 34.14996
196/196 - 65s - loss: 34.5259 - MinusLogProbMetric: 34.5259 - val_loss: 34.7253 - val_MinusLogProbMetric: 34.7253 - lr: 1.2346e-05 - 65s/epoch - 334ms/step
Epoch 303/1000
2023-09-30 02:52:46.623 
Epoch 303/1000 
	 loss: 34.1852, MinusLogProbMetric: 34.1852, val_loss: 34.4439, val_MinusLogProbMetric: 34.4439

Epoch 303: val_loss did not improve from 34.14996
196/196 - 65s - loss: 34.1852 - MinusLogProbMetric: 34.1852 - val_loss: 34.4439 - val_MinusLogProbMetric: 34.4439 - lr: 1.2346e-05 - 65s/epoch - 334ms/step
Epoch 304/1000
2023-09-30 02:53:52.096 
Epoch 304/1000 
	 loss: 34.0020, MinusLogProbMetric: 34.0020, val_loss: 34.2454, val_MinusLogProbMetric: 34.2454

Epoch 304: val_loss did not improve from 34.14996
196/196 - 65s - loss: 34.0020 - MinusLogProbMetric: 34.0020 - val_loss: 34.2454 - val_MinusLogProbMetric: 34.2454 - lr: 1.2346e-05 - 65s/epoch - 334ms/step
Epoch 305/1000
2023-09-30 02:54:54.305 
Epoch 305/1000 
	 loss: 33.8629, MinusLogProbMetric: 33.8629, val_loss: 34.5234, val_MinusLogProbMetric: 34.5234

Epoch 305: val_loss did not improve from 34.14996
196/196 - 62s - loss: 33.8629 - MinusLogProbMetric: 33.8629 - val_loss: 34.5234 - val_MinusLogProbMetric: 34.5234 - lr: 1.2346e-05 - 62s/epoch - 317ms/step
Epoch 306/1000
2023-09-30 02:55:58.902 
Epoch 306/1000 
	 loss: 33.8372, MinusLogProbMetric: 33.8372, val_loss: 34.2400, val_MinusLogProbMetric: 34.2400

Epoch 306: val_loss did not improve from 34.14996
196/196 - 65s - loss: 33.8372 - MinusLogProbMetric: 33.8372 - val_loss: 34.2400 - val_MinusLogProbMetric: 34.2400 - lr: 1.2346e-05 - 65s/epoch - 330ms/step
Epoch 307/1000
2023-09-30 02:57:02.531 
Epoch 307/1000 
	 loss: 33.8547, MinusLogProbMetric: 33.8547, val_loss: 34.2126, val_MinusLogProbMetric: 34.2126

Epoch 307: val_loss did not improve from 34.14996
196/196 - 64s - loss: 33.8547 - MinusLogProbMetric: 33.8547 - val_loss: 34.2126 - val_MinusLogProbMetric: 34.2126 - lr: 1.2346e-05 - 64s/epoch - 325ms/step
Epoch 308/1000
2023-09-30 02:58:07.647 
Epoch 308/1000 
	 loss: 33.7943, MinusLogProbMetric: 33.7943, val_loss: 34.2386, val_MinusLogProbMetric: 34.2386

Epoch 308: val_loss did not improve from 34.14996
196/196 - 65s - loss: 33.7943 - MinusLogProbMetric: 33.7943 - val_loss: 34.2386 - val_MinusLogProbMetric: 34.2386 - lr: 1.2346e-05 - 65s/epoch - 332ms/step
Epoch 309/1000
2023-09-30 02:59:09.077 
Epoch 309/1000 
	 loss: 33.7748, MinusLogProbMetric: 33.7748, val_loss: 34.2446, val_MinusLogProbMetric: 34.2446

Epoch 309: val_loss did not improve from 34.14996
196/196 - 61s - loss: 33.7748 - MinusLogProbMetric: 33.7748 - val_loss: 34.2446 - val_MinusLogProbMetric: 34.2446 - lr: 1.2346e-05 - 61s/epoch - 313ms/step
Epoch 310/1000
2023-09-30 03:00:12.938 
Epoch 310/1000 
	 loss: 33.7388, MinusLogProbMetric: 33.7388, val_loss: 34.1541, val_MinusLogProbMetric: 34.1541

Epoch 310: val_loss did not improve from 34.14996
196/196 - 64s - loss: 33.7388 - MinusLogProbMetric: 33.7388 - val_loss: 34.1541 - val_MinusLogProbMetric: 34.1541 - lr: 1.2346e-05 - 64s/epoch - 326ms/step
Epoch 311/1000
2023-09-30 03:01:17.332 
Epoch 311/1000 
	 loss: 33.7361, MinusLogProbMetric: 33.7361, val_loss: 34.1110, val_MinusLogProbMetric: 34.1110

Epoch 311: val_loss improved from 34.14996 to 34.11100, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 66s - loss: 33.7361 - MinusLogProbMetric: 33.7361 - val_loss: 34.1110 - val_MinusLogProbMetric: 34.1110 - lr: 1.2346e-05 - 66s/epoch - 335ms/step
Epoch 312/1000
2023-09-30 03:02:23.536 
Epoch 312/1000 
	 loss: 33.7377, MinusLogProbMetric: 33.7377, val_loss: 34.1316, val_MinusLogProbMetric: 34.1316

Epoch 312: val_loss did not improve from 34.11100
196/196 - 65s - loss: 33.7377 - MinusLogProbMetric: 33.7377 - val_loss: 34.1316 - val_MinusLogProbMetric: 34.1316 - lr: 1.2346e-05 - 65s/epoch - 331ms/step
Epoch 313/1000
2023-09-30 03:03:27.535 
Epoch 313/1000 
	 loss: 33.6810, MinusLogProbMetric: 33.6810, val_loss: 34.1651, val_MinusLogProbMetric: 34.1651

Epoch 313: val_loss did not improve from 34.11100
196/196 - 64s - loss: 33.6810 - MinusLogProbMetric: 33.6810 - val_loss: 34.1651 - val_MinusLogProbMetric: 34.1651 - lr: 1.2346e-05 - 64s/epoch - 326ms/step
Epoch 314/1000
2023-09-30 03:04:30.972 
Epoch 314/1000 
	 loss: 33.6910, MinusLogProbMetric: 33.6910, val_loss: 34.2609, val_MinusLogProbMetric: 34.2609

Epoch 314: val_loss did not improve from 34.11100
196/196 - 63s - loss: 33.6910 - MinusLogProbMetric: 33.6910 - val_loss: 34.2609 - val_MinusLogProbMetric: 34.2609 - lr: 1.2346e-05 - 63s/epoch - 324ms/step
Epoch 315/1000
2023-09-30 03:05:31.852 
Epoch 315/1000 
	 loss: 33.6638, MinusLogProbMetric: 33.6638, val_loss: 34.2117, val_MinusLogProbMetric: 34.2117

Epoch 315: val_loss did not improve from 34.11100
196/196 - 61s - loss: 33.6638 - MinusLogProbMetric: 33.6638 - val_loss: 34.2117 - val_MinusLogProbMetric: 34.2117 - lr: 1.2346e-05 - 61s/epoch - 311ms/step
Epoch 316/1000
2023-09-30 03:06:35.128 
Epoch 316/1000 
	 loss: 33.6459, MinusLogProbMetric: 33.6459, val_loss: 34.0684, val_MinusLogProbMetric: 34.0684

Epoch 316: val_loss improved from 34.11100 to 34.06837, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 65s - loss: 33.6459 - MinusLogProbMetric: 33.6459 - val_loss: 34.0684 - val_MinusLogProbMetric: 34.0684 - lr: 1.2346e-05 - 65s/epoch - 329ms/step
Epoch 317/1000
2023-09-30 03:07:41.721 
Epoch 317/1000 
	 loss: 33.6361, MinusLogProbMetric: 33.6361, val_loss: 33.9457, val_MinusLogProbMetric: 33.9457

Epoch 317: val_loss improved from 34.06837 to 33.94567, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 66s - loss: 33.6361 - MinusLogProbMetric: 33.6361 - val_loss: 33.9457 - val_MinusLogProbMetric: 33.9457 - lr: 1.2346e-05 - 66s/epoch - 339ms/step
Epoch 318/1000
2023-09-30 03:08:47.890 
Epoch 318/1000 
	 loss: 33.6181, MinusLogProbMetric: 33.6181, val_loss: 33.9297, val_MinusLogProbMetric: 33.9297

Epoch 318: val_loss improved from 33.94567 to 33.92973, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 67s - loss: 33.6181 - MinusLogProbMetric: 33.6181 - val_loss: 33.9297 - val_MinusLogProbMetric: 33.9297 - lr: 1.2346e-05 - 67s/epoch - 341ms/step
Epoch 319/1000
2023-09-30 03:09:55.014 
Epoch 319/1000 
	 loss: 33.5712, MinusLogProbMetric: 33.5712, val_loss: 34.0210, val_MinusLogProbMetric: 34.0210

Epoch 319: val_loss did not improve from 33.92973
196/196 - 66s - loss: 33.5712 - MinusLogProbMetric: 33.5712 - val_loss: 34.0210 - val_MinusLogProbMetric: 34.0210 - lr: 1.2346e-05 - 66s/epoch - 334ms/step
Epoch 320/1000
2023-09-30 03:10:57.778 
Epoch 320/1000 
	 loss: 33.6276, MinusLogProbMetric: 33.6276, val_loss: 33.9078, val_MinusLogProbMetric: 33.9078

Epoch 320: val_loss improved from 33.92973 to 33.90776, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 64s - loss: 33.6276 - MinusLogProbMetric: 33.6276 - val_loss: 33.9078 - val_MinusLogProbMetric: 33.9078 - lr: 1.2346e-05 - 64s/epoch - 326ms/step
Epoch 321/1000
2023-09-30 03:12:01.326 
Epoch 321/1000 
	 loss: 33.5755, MinusLogProbMetric: 33.5755, val_loss: 34.0222, val_MinusLogProbMetric: 34.0222

Epoch 321: val_loss did not improve from 33.90776
196/196 - 62s - loss: 33.5755 - MinusLogProbMetric: 33.5755 - val_loss: 34.0222 - val_MinusLogProbMetric: 34.0222 - lr: 1.2346e-05 - 62s/epoch - 318ms/step
Epoch 322/1000
2023-09-30 03:13:04.563 
Epoch 322/1000 
	 loss: 33.5355, MinusLogProbMetric: 33.5355, val_loss: 34.0295, val_MinusLogProbMetric: 34.0295

Epoch 322: val_loss did not improve from 33.90776
196/196 - 63s - loss: 33.5355 - MinusLogProbMetric: 33.5355 - val_loss: 34.0295 - val_MinusLogProbMetric: 34.0295 - lr: 1.2346e-05 - 63s/epoch - 323ms/step
Epoch 323/1000
2023-09-30 03:14:09.680 
Epoch 323/1000 
	 loss: 33.5597, MinusLogProbMetric: 33.5597, val_loss: 33.8982, val_MinusLogProbMetric: 33.8982

Epoch 323: val_loss improved from 33.90776 to 33.89816, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 66s - loss: 33.5597 - MinusLogProbMetric: 33.5597 - val_loss: 33.8982 - val_MinusLogProbMetric: 33.8982 - lr: 1.2346e-05 - 66s/epoch - 338ms/step
Epoch 324/1000
2023-09-30 03:15:17.803 
Epoch 324/1000 
	 loss: 33.5444, MinusLogProbMetric: 33.5444, val_loss: 33.9053, val_MinusLogProbMetric: 33.9053

Epoch 324: val_loss did not improve from 33.89816
196/196 - 67s - loss: 33.5444 - MinusLogProbMetric: 33.5444 - val_loss: 33.9053 - val_MinusLogProbMetric: 33.9053 - lr: 1.2346e-05 - 67s/epoch - 342ms/step
Epoch 325/1000
2023-09-30 03:16:18.624 
Epoch 325/1000 
	 loss: 33.5406, MinusLogProbMetric: 33.5406, val_loss: 34.0372, val_MinusLogProbMetric: 34.0372

Epoch 325: val_loss did not improve from 33.89816
196/196 - 61s - loss: 33.5406 - MinusLogProbMetric: 33.5406 - val_loss: 34.0372 - val_MinusLogProbMetric: 34.0372 - lr: 1.2346e-05 - 61s/epoch - 310ms/step
Epoch 326/1000
2023-09-30 03:17:22.167 
Epoch 326/1000 
	 loss: 33.5127, MinusLogProbMetric: 33.5127, val_loss: 34.2398, val_MinusLogProbMetric: 34.2398

Epoch 326: val_loss did not improve from 33.89816
196/196 - 64s - loss: 33.5127 - MinusLogProbMetric: 33.5127 - val_loss: 34.2398 - val_MinusLogProbMetric: 34.2398 - lr: 1.2346e-05 - 64s/epoch - 324ms/step
Epoch 327/1000
2023-09-30 03:18:24.504 
Epoch 327/1000 
	 loss: 33.4960, MinusLogProbMetric: 33.4960, val_loss: 33.8289, val_MinusLogProbMetric: 33.8289

Epoch 327: val_loss improved from 33.89816 to 33.82888, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 64s - loss: 33.4960 - MinusLogProbMetric: 33.4960 - val_loss: 33.8289 - val_MinusLogProbMetric: 33.8289 - lr: 1.2346e-05 - 64s/epoch - 325ms/step
Epoch 328/1000
2023-09-30 03:19:28.047 
Epoch 328/1000 
	 loss: 33.4787, MinusLogProbMetric: 33.4787, val_loss: 34.0981, val_MinusLogProbMetric: 34.0981

Epoch 328: val_loss did not improve from 33.82888
196/196 - 62s - loss: 33.4787 - MinusLogProbMetric: 33.4787 - val_loss: 34.0981 - val_MinusLogProbMetric: 34.0981 - lr: 1.2346e-05 - 62s/epoch - 317ms/step
Epoch 329/1000
2023-09-30 03:20:31.773 
Epoch 329/1000 
	 loss: 33.4561, MinusLogProbMetric: 33.4561, val_loss: 33.9049, val_MinusLogProbMetric: 33.9049

Epoch 329: val_loss did not improve from 33.82888
196/196 - 64s - loss: 33.4561 - MinusLogProbMetric: 33.4561 - val_loss: 33.9049 - val_MinusLogProbMetric: 33.9049 - lr: 1.2346e-05 - 64s/epoch - 325ms/step
Epoch 330/1000
2023-09-30 03:21:33.881 
Epoch 330/1000 
	 loss: 33.4487, MinusLogProbMetric: 33.4487, val_loss: 34.0007, val_MinusLogProbMetric: 34.0007

Epoch 330: val_loss did not improve from 33.82888
196/196 - 62s - loss: 33.4487 - MinusLogProbMetric: 33.4487 - val_loss: 34.0007 - val_MinusLogProbMetric: 34.0007 - lr: 1.2346e-05 - 62s/epoch - 317ms/step
Epoch 331/1000
2023-09-30 03:22:36.430 
Epoch 331/1000 
	 loss: 33.4346, MinusLogProbMetric: 33.4346, val_loss: 34.0457, val_MinusLogProbMetric: 34.0457

Epoch 331: val_loss did not improve from 33.82888
196/196 - 63s - loss: 33.4346 - MinusLogProbMetric: 33.4346 - val_loss: 34.0457 - val_MinusLogProbMetric: 34.0457 - lr: 1.2346e-05 - 63s/epoch - 319ms/step
Epoch 332/1000
2023-09-30 03:23:43.560 
Epoch 332/1000 
	 loss: 33.4040, MinusLogProbMetric: 33.4040, val_loss: 33.7623, val_MinusLogProbMetric: 33.7623

Epoch 332: val_loss improved from 33.82888 to 33.76234, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 68s - loss: 33.4040 - MinusLogProbMetric: 33.4040 - val_loss: 33.7623 - val_MinusLogProbMetric: 33.7623 - lr: 1.2346e-05 - 68s/epoch - 349ms/step
Epoch 333/1000
2023-09-30 03:24:46.580 
Epoch 333/1000 
	 loss: 34.8818, MinusLogProbMetric: 34.8818, val_loss: 33.8960, val_MinusLogProbMetric: 33.8960

Epoch 333: val_loss did not improve from 33.76234
196/196 - 62s - loss: 34.8818 - MinusLogProbMetric: 34.8818 - val_loss: 33.8960 - val_MinusLogProbMetric: 33.8960 - lr: 1.2346e-05 - 62s/epoch - 315ms/step
Epoch 334/1000
2023-09-30 03:25:51.642 
Epoch 334/1000 
	 loss: 37.1948, MinusLogProbMetric: 37.1948, val_loss: 34.8140, val_MinusLogProbMetric: 34.8140

Epoch 334: val_loss did not improve from 33.76234
196/196 - 65s - loss: 37.1948 - MinusLogProbMetric: 37.1948 - val_loss: 34.8140 - val_MinusLogProbMetric: 34.8140 - lr: 1.2346e-05 - 65s/epoch - 332ms/step
Epoch 335/1000
2023-09-30 03:26:56.674 
Epoch 335/1000 
	 loss: 33.6530, MinusLogProbMetric: 33.6530, val_loss: 33.9767, val_MinusLogProbMetric: 33.9767

Epoch 335: val_loss did not improve from 33.76234
196/196 - 65s - loss: 33.6530 - MinusLogProbMetric: 33.6530 - val_loss: 33.9767 - val_MinusLogProbMetric: 33.9767 - lr: 1.2346e-05 - 65s/epoch - 332ms/step
Epoch 336/1000
2023-09-30 03:28:02.091 
Epoch 336/1000 
	 loss: 33.4641, MinusLogProbMetric: 33.4641, val_loss: 33.8238, val_MinusLogProbMetric: 33.8238

Epoch 336: val_loss did not improve from 33.76234
196/196 - 65s - loss: 33.4641 - MinusLogProbMetric: 33.4641 - val_loss: 33.8238 - val_MinusLogProbMetric: 33.8238 - lr: 1.2346e-05 - 65s/epoch - 334ms/step
Epoch 337/1000
2023-09-30 03:29:07.516 
Epoch 337/1000 
	 loss: 33.4492, MinusLogProbMetric: 33.4492, val_loss: 34.0282, val_MinusLogProbMetric: 34.0282

Epoch 337: val_loss did not improve from 33.76234
196/196 - 65s - loss: 33.4492 - MinusLogProbMetric: 33.4492 - val_loss: 34.0282 - val_MinusLogProbMetric: 34.0282 - lr: 1.2346e-05 - 65s/epoch - 334ms/step
Epoch 338/1000
2023-09-30 03:30:13.746 
Epoch 338/1000 
	 loss: 33.4001, MinusLogProbMetric: 33.4001, val_loss: 33.9027, val_MinusLogProbMetric: 33.9027

Epoch 338: val_loss did not improve from 33.76234
196/196 - 66s - loss: 33.4001 - MinusLogProbMetric: 33.4001 - val_loss: 33.9027 - val_MinusLogProbMetric: 33.9027 - lr: 1.2346e-05 - 66s/epoch - 338ms/step
Epoch 339/1000
2023-09-30 03:31:21.129 
Epoch 339/1000 
	 loss: 33.3407, MinusLogProbMetric: 33.3407, val_loss: 34.0667, val_MinusLogProbMetric: 34.0667

Epoch 339: val_loss did not improve from 33.76234
196/196 - 67s - loss: 33.3407 - MinusLogProbMetric: 33.3407 - val_loss: 34.0667 - val_MinusLogProbMetric: 34.0667 - lr: 1.2346e-05 - 67s/epoch - 344ms/step
Epoch 340/1000
2023-09-30 03:32:24.863 
Epoch 340/1000 
	 loss: 33.3271, MinusLogProbMetric: 33.3271, val_loss: 33.6991, val_MinusLogProbMetric: 33.6991

Epoch 340: val_loss improved from 33.76234 to 33.69912, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 65s - loss: 33.3271 - MinusLogProbMetric: 33.3271 - val_loss: 33.6991 - val_MinusLogProbMetric: 33.6991 - lr: 1.2346e-05 - 65s/epoch - 333ms/step
Epoch 341/1000
2023-09-30 03:33:29.149 
Epoch 341/1000 
	 loss: 33.3561, MinusLogProbMetric: 33.3561, val_loss: 33.6352, val_MinusLogProbMetric: 33.6352

Epoch 341: val_loss improved from 33.69912 to 33.63517, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 64s - loss: 33.3561 - MinusLogProbMetric: 33.3561 - val_loss: 33.6352 - val_MinusLogProbMetric: 33.6352 - lr: 1.2346e-05 - 64s/epoch - 328ms/step
Epoch 342/1000
2023-09-30 03:34:35.628 
Epoch 342/1000 
	 loss: 33.2987, MinusLogProbMetric: 33.2987, val_loss: 33.6678, val_MinusLogProbMetric: 33.6678

Epoch 342: val_loss did not improve from 33.63517
196/196 - 65s - loss: 33.2987 - MinusLogProbMetric: 33.2987 - val_loss: 33.6678 - val_MinusLogProbMetric: 33.6678 - lr: 1.2346e-05 - 65s/epoch - 331ms/step
Epoch 343/1000
2023-09-30 03:35:37.740 
Epoch 343/1000 
	 loss: 33.2846, MinusLogProbMetric: 33.2846, val_loss: 33.7193, val_MinusLogProbMetric: 33.7193

Epoch 343: val_loss did not improve from 33.63517
196/196 - 62s - loss: 33.2846 - MinusLogProbMetric: 33.2846 - val_loss: 33.7193 - val_MinusLogProbMetric: 33.7193 - lr: 1.2346e-05 - 62s/epoch - 317ms/step
Epoch 344/1000
2023-09-30 03:36:46.240 
Epoch 344/1000 
	 loss: 33.2610, MinusLogProbMetric: 33.2610, val_loss: 33.6173, val_MinusLogProbMetric: 33.6173

Epoch 344: val_loss improved from 33.63517 to 33.61728, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 70s - loss: 33.2610 - MinusLogProbMetric: 33.2610 - val_loss: 33.6173 - val_MinusLogProbMetric: 33.6173 - lr: 1.2346e-05 - 70s/epoch - 356ms/step
Epoch 345/1000
2023-09-30 03:37:47.914 
Epoch 345/1000 
	 loss: 33.2716, MinusLogProbMetric: 33.2716, val_loss: 33.5802, val_MinusLogProbMetric: 33.5802

Epoch 345: val_loss improved from 33.61728 to 33.58024, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 62s - loss: 33.2716 - MinusLogProbMetric: 33.2716 - val_loss: 33.5802 - val_MinusLogProbMetric: 33.5802 - lr: 1.2346e-05 - 62s/epoch - 316ms/step
Epoch 346/1000
2023-09-30 03:38:52.629 
Epoch 346/1000 
	 loss: 33.2336, MinusLogProbMetric: 33.2336, val_loss: 33.7269, val_MinusLogProbMetric: 33.7269

Epoch 346: val_loss did not improve from 33.58024
196/196 - 63s - loss: 33.2336 - MinusLogProbMetric: 33.2336 - val_loss: 33.7269 - val_MinusLogProbMetric: 33.7269 - lr: 1.2346e-05 - 63s/epoch - 322ms/step
Epoch 347/1000
2023-09-30 03:39:57.473 
Epoch 347/1000 
	 loss: 33.2446, MinusLogProbMetric: 33.2446, val_loss: 33.7203, val_MinusLogProbMetric: 33.7203

Epoch 347: val_loss did not improve from 33.58024
196/196 - 65s - loss: 33.2446 - MinusLogProbMetric: 33.2446 - val_loss: 33.7203 - val_MinusLogProbMetric: 33.7203 - lr: 1.2346e-05 - 65s/epoch - 331ms/step
Epoch 348/1000
2023-09-30 03:41:01.609 
Epoch 348/1000 
	 loss: 33.1800, MinusLogProbMetric: 33.1800, val_loss: 33.6446, val_MinusLogProbMetric: 33.6446

Epoch 348: val_loss did not improve from 33.58024
196/196 - 64s - loss: 33.1800 - MinusLogProbMetric: 33.1800 - val_loss: 33.6446 - val_MinusLogProbMetric: 33.6446 - lr: 1.2346e-05 - 64s/epoch - 327ms/step
Epoch 349/1000
2023-09-30 03:42:03.602 
Epoch 349/1000 
	 loss: 33.1824, MinusLogProbMetric: 33.1824, val_loss: 33.5841, val_MinusLogProbMetric: 33.5841

Epoch 349: val_loss did not improve from 33.58024
196/196 - 62s - loss: 33.1824 - MinusLogProbMetric: 33.1824 - val_loss: 33.5841 - val_MinusLogProbMetric: 33.5841 - lr: 1.2346e-05 - 62s/epoch - 316ms/step
Epoch 350/1000
2023-09-30 03:43:12.187 
Epoch 350/1000 
	 loss: 33.1654, MinusLogProbMetric: 33.1654, val_loss: 33.6539, val_MinusLogProbMetric: 33.6539

Epoch 350: val_loss did not improve from 33.58024
196/196 - 69s - loss: 33.1654 - MinusLogProbMetric: 33.1654 - val_loss: 33.6539 - val_MinusLogProbMetric: 33.6539 - lr: 1.2346e-05 - 69s/epoch - 350ms/step
Epoch 351/1000
2023-09-30 03:44:20.412 
Epoch 351/1000 
	 loss: 33.9561, MinusLogProbMetric: 33.9561, val_loss: 33.7093, val_MinusLogProbMetric: 33.7093

Epoch 351: val_loss did not improve from 33.58024
196/196 - 68s - loss: 33.9561 - MinusLogProbMetric: 33.9561 - val_loss: 33.7093 - val_MinusLogProbMetric: 33.7093 - lr: 1.2346e-05 - 68s/epoch - 348ms/step
Epoch 352/1000
2023-09-30 03:45:26.840 
Epoch 352/1000 
	 loss: 33.1231, MinusLogProbMetric: 33.1231, val_loss: 33.6127, val_MinusLogProbMetric: 33.6127

Epoch 352: val_loss did not improve from 33.58024
196/196 - 66s - loss: 33.1231 - MinusLogProbMetric: 33.1231 - val_loss: 33.6127 - val_MinusLogProbMetric: 33.6127 - lr: 1.2346e-05 - 66s/epoch - 339ms/step
Epoch 353/1000
2023-09-30 03:46:32.147 
Epoch 353/1000 
	 loss: 33.1940, MinusLogProbMetric: 33.1940, val_loss: 33.5905, val_MinusLogProbMetric: 33.5905

Epoch 353: val_loss did not improve from 33.58024
196/196 - 65s - loss: 33.1940 - MinusLogProbMetric: 33.1940 - val_loss: 33.5905 - val_MinusLogProbMetric: 33.5905 - lr: 1.2346e-05 - 65s/epoch - 333ms/step
Epoch 354/1000
2023-09-30 03:47:36.169 
Epoch 354/1000 
	 loss: 33.0912, MinusLogProbMetric: 33.0912, val_loss: 33.6315, val_MinusLogProbMetric: 33.6315

Epoch 354: val_loss did not improve from 33.58024
196/196 - 64s - loss: 33.0912 - MinusLogProbMetric: 33.0912 - val_loss: 33.6315 - val_MinusLogProbMetric: 33.6315 - lr: 1.2346e-05 - 64s/epoch - 327ms/step
Epoch 355/1000
2023-09-30 03:48:39.882 
Epoch 355/1000 
	 loss: 33.0989, MinusLogProbMetric: 33.0989, val_loss: 33.4023, val_MinusLogProbMetric: 33.4023

Epoch 355: val_loss improved from 33.58024 to 33.40226, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 65s - loss: 33.0989 - MinusLogProbMetric: 33.0989 - val_loss: 33.4023 - val_MinusLogProbMetric: 33.4023 - lr: 1.2346e-05 - 65s/epoch - 330ms/step
Epoch 356/1000
2023-09-30 03:49:45.895 
Epoch 356/1000 
	 loss: 33.0801, MinusLogProbMetric: 33.0801, val_loss: 33.5292, val_MinusLogProbMetric: 33.5292

Epoch 356: val_loss did not improve from 33.40226
196/196 - 65s - loss: 33.0801 - MinusLogProbMetric: 33.0801 - val_loss: 33.5292 - val_MinusLogProbMetric: 33.5292 - lr: 1.2346e-05 - 65s/epoch - 332ms/step
Epoch 357/1000
2023-09-30 03:50:54.284 
Epoch 357/1000 
	 loss: 33.0483, MinusLogProbMetric: 33.0483, val_loss: 33.5949, val_MinusLogProbMetric: 33.5949

Epoch 357: val_loss did not improve from 33.40226
196/196 - 68s - loss: 33.0483 - MinusLogProbMetric: 33.0483 - val_loss: 33.5949 - val_MinusLogProbMetric: 33.5949 - lr: 1.2346e-05 - 68s/epoch - 349ms/step
Epoch 358/1000
2023-09-30 03:51:56.514 
Epoch 358/1000 
	 loss: 33.0613, MinusLogProbMetric: 33.0613, val_loss: 33.4226, val_MinusLogProbMetric: 33.4226

Epoch 358: val_loss did not improve from 33.40226
196/196 - 62s - loss: 33.0613 - MinusLogProbMetric: 33.0613 - val_loss: 33.4226 - val_MinusLogProbMetric: 33.4226 - lr: 1.2346e-05 - 62s/epoch - 317ms/step
Epoch 359/1000
2023-09-30 03:53:04.335 
Epoch 359/1000 
	 loss: 33.0364, MinusLogProbMetric: 33.0364, val_loss: 33.3296, val_MinusLogProbMetric: 33.3296

Epoch 359: val_loss improved from 33.40226 to 33.32959, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 69s - loss: 33.0364 - MinusLogProbMetric: 33.0364 - val_loss: 33.3296 - val_MinusLogProbMetric: 33.3296 - lr: 1.2346e-05 - 69s/epoch - 353ms/step
Epoch 360/1000
2023-09-30 03:54:12.905 
Epoch 360/1000 
	 loss: 33.0601, MinusLogProbMetric: 33.0601, val_loss: 33.5788, val_MinusLogProbMetric: 33.5788

Epoch 360: val_loss did not improve from 33.32959
196/196 - 67s - loss: 33.0601 - MinusLogProbMetric: 33.0601 - val_loss: 33.5788 - val_MinusLogProbMetric: 33.5788 - lr: 1.2346e-05 - 67s/epoch - 343ms/step
Epoch 361/1000
2023-09-30 03:55:13.167 
Epoch 361/1000 
	 loss: 33.0019, MinusLogProbMetric: 33.0019, val_loss: 33.4019, val_MinusLogProbMetric: 33.4019

Epoch 361: val_loss did not improve from 33.32959
196/196 - 60s - loss: 33.0019 - MinusLogProbMetric: 33.0019 - val_loss: 33.4019 - val_MinusLogProbMetric: 33.4019 - lr: 1.2346e-05 - 60s/epoch - 307ms/step
Epoch 362/1000
2023-09-30 03:56:16.083 
Epoch 362/1000 
	 loss: 33.0183, MinusLogProbMetric: 33.0183, val_loss: 33.7122, val_MinusLogProbMetric: 33.7122

Epoch 362: val_loss did not improve from 33.32959
196/196 - 63s - loss: 33.0183 - MinusLogProbMetric: 33.0183 - val_loss: 33.7122 - val_MinusLogProbMetric: 33.7122 - lr: 1.2346e-05 - 63s/epoch - 321ms/step
Epoch 363/1000
2023-09-30 03:57:22.368 
Epoch 363/1000 
	 loss: 33.0171, MinusLogProbMetric: 33.0171, val_loss: 33.4585, val_MinusLogProbMetric: 33.4585

Epoch 363: val_loss did not improve from 33.32959
196/196 - 66s - loss: 33.0171 - MinusLogProbMetric: 33.0171 - val_loss: 33.4585 - val_MinusLogProbMetric: 33.4585 - lr: 1.2346e-05 - 66s/epoch - 338ms/step
Epoch 364/1000
2023-09-30 03:58:30.647 
Epoch 364/1000 
	 loss: 33.0117, MinusLogProbMetric: 33.0117, val_loss: 33.3841, val_MinusLogProbMetric: 33.3841

Epoch 364: val_loss did not improve from 33.32959
196/196 - 68s - loss: 33.0117 - MinusLogProbMetric: 33.0117 - val_loss: 33.3841 - val_MinusLogProbMetric: 33.3841 - lr: 1.2346e-05 - 68s/epoch - 348ms/step
Epoch 365/1000
2023-09-30 03:59:39.238 
Epoch 365/1000 
	 loss: 32.9626, MinusLogProbMetric: 32.9626, val_loss: 33.4261, val_MinusLogProbMetric: 33.4261

Epoch 365: val_loss did not improve from 33.32959
196/196 - 69s - loss: 32.9626 - MinusLogProbMetric: 32.9626 - val_loss: 33.4261 - val_MinusLogProbMetric: 33.4261 - lr: 1.2346e-05 - 69s/epoch - 350ms/step
Epoch 366/1000
2023-09-30 04:00:46.865 
Epoch 366/1000 
	 loss: 33.0174, MinusLogProbMetric: 33.0174, val_loss: 33.4034, val_MinusLogProbMetric: 33.4034

Epoch 366: val_loss did not improve from 33.32959
196/196 - 68s - loss: 33.0174 - MinusLogProbMetric: 33.0174 - val_loss: 33.4034 - val_MinusLogProbMetric: 33.4034 - lr: 1.2346e-05 - 68s/epoch - 345ms/step
Epoch 367/1000
2023-09-30 04:01:47.461 
Epoch 367/1000 
	 loss: 32.9890, MinusLogProbMetric: 32.9890, val_loss: 33.3660, val_MinusLogProbMetric: 33.3660

Epoch 367: val_loss did not improve from 33.32959
196/196 - 61s - loss: 32.9890 - MinusLogProbMetric: 32.9890 - val_loss: 33.3660 - val_MinusLogProbMetric: 33.3660 - lr: 1.2346e-05 - 61s/epoch - 309ms/step
Epoch 368/1000
2023-09-30 04:02:56.571 
Epoch 368/1000 
	 loss: 32.9910, MinusLogProbMetric: 32.9910, val_loss: 33.2641, val_MinusLogProbMetric: 33.2641

Epoch 368: val_loss improved from 33.32959 to 33.26414, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 70s - loss: 32.9910 - MinusLogProbMetric: 32.9910 - val_loss: 33.2641 - val_MinusLogProbMetric: 33.2641 - lr: 1.2346e-05 - 70s/epoch - 359ms/step
Epoch 369/1000
2023-09-30 04:04:03.261 
Epoch 369/1000 
	 loss: 32.9535, MinusLogProbMetric: 32.9535, val_loss: 33.4461, val_MinusLogProbMetric: 33.4461

Epoch 369: val_loss did not improve from 33.26414
196/196 - 65s - loss: 32.9535 - MinusLogProbMetric: 32.9535 - val_loss: 33.4461 - val_MinusLogProbMetric: 33.4461 - lr: 1.2346e-05 - 65s/epoch - 334ms/step
Epoch 370/1000
2023-09-30 04:05:04.773 
Epoch 370/1000 
	 loss: 32.9257, MinusLogProbMetric: 32.9257, val_loss: 33.2127, val_MinusLogProbMetric: 33.2127

Epoch 370: val_loss improved from 33.26414 to 33.21268, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 63s - loss: 32.9257 - MinusLogProbMetric: 32.9257 - val_loss: 33.2127 - val_MinusLogProbMetric: 33.2127 - lr: 1.2346e-05 - 63s/epoch - 319ms/step
Epoch 371/1000
2023-09-30 04:06:12.401 
Epoch 371/1000 
	 loss: 32.9046, MinusLogProbMetric: 32.9046, val_loss: 33.3995, val_MinusLogProbMetric: 33.3995

Epoch 371: val_loss did not improve from 33.21268
196/196 - 67s - loss: 32.9046 - MinusLogProbMetric: 32.9046 - val_loss: 33.3995 - val_MinusLogProbMetric: 33.3995 - lr: 1.2346e-05 - 67s/epoch - 340ms/step
Epoch 372/1000
2023-09-30 04:07:19.303 
Epoch 372/1000 
	 loss: 32.8740, MinusLogProbMetric: 32.8740, val_loss: 33.3480, val_MinusLogProbMetric: 33.3480

Epoch 372: val_loss did not improve from 33.21268
196/196 - 67s - loss: 32.8740 - MinusLogProbMetric: 32.8740 - val_loss: 33.3480 - val_MinusLogProbMetric: 33.3480 - lr: 1.2346e-05 - 67s/epoch - 341ms/step
Epoch 373/1000
2023-09-30 04:08:21.829 
Epoch 373/1000 
	 loss: 32.9082, MinusLogProbMetric: 32.9082, val_loss: 33.2879, val_MinusLogProbMetric: 33.2879

Epoch 373: val_loss did not improve from 33.21268
196/196 - 63s - loss: 32.9082 - MinusLogProbMetric: 32.9082 - val_loss: 33.2879 - val_MinusLogProbMetric: 33.2879 - lr: 1.2346e-05 - 63s/epoch - 319ms/step
Epoch 374/1000
2023-09-30 04:09:29.060 
Epoch 374/1000 
	 loss: 32.8587, MinusLogProbMetric: 32.8587, val_loss: 33.1291, val_MinusLogProbMetric: 33.1291

Epoch 374: val_loss improved from 33.21268 to 33.12908, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 69s - loss: 32.8587 - MinusLogProbMetric: 32.8587 - val_loss: 33.1291 - val_MinusLogProbMetric: 33.1291 - lr: 1.2346e-05 - 69s/epoch - 350ms/step
Epoch 375/1000
2023-09-30 04:10:37.669 
Epoch 375/1000 
	 loss: 32.8929, MinusLogProbMetric: 32.8929, val_loss: 33.4158, val_MinusLogProbMetric: 33.4158

Epoch 375: val_loss did not improve from 33.12908
196/196 - 67s - loss: 32.8929 - MinusLogProbMetric: 32.8929 - val_loss: 33.4158 - val_MinusLogProbMetric: 33.4158 - lr: 1.2346e-05 - 67s/epoch - 343ms/step
Epoch 376/1000
2023-09-30 04:11:45.839 
Epoch 376/1000 
	 loss: 32.8444, MinusLogProbMetric: 32.8444, val_loss: 33.4875, val_MinusLogProbMetric: 33.4875

Epoch 376: val_loss did not improve from 33.12908
196/196 - 68s - loss: 32.8444 - MinusLogProbMetric: 32.8444 - val_loss: 33.4875 - val_MinusLogProbMetric: 33.4875 - lr: 1.2346e-05 - 68s/epoch - 348ms/step
Epoch 377/1000
2023-09-30 04:12:49.632 
Epoch 377/1000 
	 loss: 32.8389, MinusLogProbMetric: 32.8389, val_loss: 33.2050, val_MinusLogProbMetric: 33.2050

Epoch 377: val_loss did not improve from 33.12908
196/196 - 64s - loss: 32.8389 - MinusLogProbMetric: 32.8389 - val_loss: 33.2050 - val_MinusLogProbMetric: 33.2050 - lr: 1.2346e-05 - 64s/epoch - 325ms/step
Epoch 378/1000
2023-09-30 04:13:54.889 
Epoch 378/1000 
	 loss: 32.8640, MinusLogProbMetric: 32.8640, val_loss: 33.3652, val_MinusLogProbMetric: 33.3652

Epoch 378: val_loss did not improve from 33.12908
196/196 - 65s - loss: 32.8640 - MinusLogProbMetric: 32.8640 - val_loss: 33.3652 - val_MinusLogProbMetric: 33.3652 - lr: 1.2346e-05 - 65s/epoch - 333ms/step
Epoch 379/1000
2023-09-30 04:15:01.575 
Epoch 379/1000 
	 loss: 34.8708, MinusLogProbMetric: 34.8708, val_loss: 33.2804, val_MinusLogProbMetric: 33.2804

Epoch 379: val_loss did not improve from 33.12908
196/196 - 67s - loss: 34.8708 - MinusLogProbMetric: 34.8708 - val_loss: 33.2804 - val_MinusLogProbMetric: 33.2804 - lr: 1.2346e-05 - 67s/epoch - 340ms/step
Epoch 380/1000
2023-09-30 04:16:04.596 
Epoch 380/1000 
	 loss: 32.7944, MinusLogProbMetric: 32.7944, val_loss: 33.2531, val_MinusLogProbMetric: 33.2531

Epoch 380: val_loss did not improve from 33.12908
196/196 - 63s - loss: 32.7944 - MinusLogProbMetric: 32.7944 - val_loss: 33.2531 - val_MinusLogProbMetric: 33.2531 - lr: 1.2346e-05 - 63s/epoch - 322ms/step
Epoch 381/1000
2023-09-30 04:17:12.188 
Epoch 381/1000 
	 loss: 32.8223, MinusLogProbMetric: 32.8223, val_loss: 33.2750, val_MinusLogProbMetric: 33.2750

Epoch 381: val_loss did not improve from 33.12908
196/196 - 68s - loss: 32.8223 - MinusLogProbMetric: 32.8223 - val_loss: 33.2750 - val_MinusLogProbMetric: 33.2750 - lr: 1.2346e-05 - 68s/epoch - 345ms/step
Epoch 382/1000
2023-09-30 04:18:16.111 
Epoch 382/1000 
	 loss: 32.8600, MinusLogProbMetric: 32.8600, val_loss: 33.2876, val_MinusLogProbMetric: 33.2876

Epoch 382: val_loss did not improve from 33.12908
196/196 - 64s - loss: 32.8600 - MinusLogProbMetric: 32.8600 - val_loss: 33.2876 - val_MinusLogProbMetric: 33.2876 - lr: 1.2346e-05 - 64s/epoch - 326ms/step
Epoch 383/1000
2023-09-30 04:19:17.445 
Epoch 383/1000 
	 loss: 32.7682, MinusLogProbMetric: 32.7682, val_loss: 33.2739, val_MinusLogProbMetric: 33.2739

Epoch 383: val_loss did not improve from 33.12908
196/196 - 61s - loss: 32.7682 - MinusLogProbMetric: 32.7682 - val_loss: 33.2739 - val_MinusLogProbMetric: 33.2739 - lr: 1.2346e-05 - 61s/epoch - 313ms/step
Epoch 384/1000
2023-09-30 04:20:24.021 
Epoch 384/1000 
	 loss: 32.7975, MinusLogProbMetric: 32.7975, val_loss: 33.0970, val_MinusLogProbMetric: 33.0970

Epoch 384: val_loss improved from 33.12908 to 33.09695, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 68s - loss: 32.7975 - MinusLogProbMetric: 32.7975 - val_loss: 33.0970 - val_MinusLogProbMetric: 33.0970 - lr: 1.2346e-05 - 68s/epoch - 345ms/step
Epoch 385/1000
2023-09-30 04:21:30.555 
Epoch 385/1000 
	 loss: 32.7627, MinusLogProbMetric: 32.7627, val_loss: 33.1709, val_MinusLogProbMetric: 33.1709

Epoch 385: val_loss did not improve from 33.09695
196/196 - 65s - loss: 32.7627 - MinusLogProbMetric: 32.7627 - val_loss: 33.1709 - val_MinusLogProbMetric: 33.1709 - lr: 1.2346e-05 - 65s/epoch - 334ms/step
Epoch 386/1000
2023-09-30 04:22:35.995 
Epoch 386/1000 
	 loss: 32.7191, MinusLogProbMetric: 32.7191, val_loss: 33.1567, val_MinusLogProbMetric: 33.1567

Epoch 386: val_loss did not improve from 33.09695
196/196 - 65s - loss: 32.7191 - MinusLogProbMetric: 32.7191 - val_loss: 33.1567 - val_MinusLogProbMetric: 33.1567 - lr: 1.2346e-05 - 65s/epoch - 334ms/step
Epoch 387/1000
2023-09-30 04:23:36.514 
Epoch 387/1000 
	 loss: 32.7514, MinusLogProbMetric: 32.7514, val_loss: 33.2344, val_MinusLogProbMetric: 33.2344

Epoch 387: val_loss did not improve from 33.09695
196/196 - 61s - loss: 32.7514 - MinusLogProbMetric: 32.7514 - val_loss: 33.2344 - val_MinusLogProbMetric: 33.2344 - lr: 1.2346e-05 - 61s/epoch - 309ms/step
Epoch 388/1000
2023-09-30 04:24:45.546 
Epoch 388/1000 
	 loss: 32.7295, MinusLogProbMetric: 32.7295, val_loss: 33.1786, val_MinusLogProbMetric: 33.1786

Epoch 388: val_loss did not improve from 33.09695
196/196 - 69s - loss: 32.7295 - MinusLogProbMetric: 32.7295 - val_loss: 33.1786 - val_MinusLogProbMetric: 33.1786 - lr: 1.2346e-05 - 69s/epoch - 352ms/step
Epoch 389/1000
2023-09-30 04:25:55.325 
Epoch 389/1000 
	 loss: 32.7274, MinusLogProbMetric: 32.7274, val_loss: 33.1788, val_MinusLogProbMetric: 33.1788

Epoch 389: val_loss did not improve from 33.09695
196/196 - 70s - loss: 32.7274 - MinusLogProbMetric: 32.7274 - val_loss: 33.1788 - val_MinusLogProbMetric: 33.1788 - lr: 1.2346e-05 - 70s/epoch - 356ms/step
Epoch 390/1000
2023-09-30 04:27:04.771 
Epoch 390/1000 
	 loss: 32.6918, MinusLogProbMetric: 32.6918, val_loss: 33.3464, val_MinusLogProbMetric: 33.3464

Epoch 390: val_loss did not improve from 33.09695
196/196 - 69s - loss: 32.6918 - MinusLogProbMetric: 32.6918 - val_loss: 33.3464 - val_MinusLogProbMetric: 33.3464 - lr: 1.2346e-05 - 69s/epoch - 354ms/step
Epoch 391/1000
2023-09-30 04:28:14.676 
Epoch 391/1000 
	 loss: 32.7188, MinusLogProbMetric: 32.7188, val_loss: 33.1284, val_MinusLogProbMetric: 33.1284

Epoch 391: val_loss did not improve from 33.09695
196/196 - 70s - loss: 32.7188 - MinusLogProbMetric: 32.7188 - val_loss: 33.1284 - val_MinusLogProbMetric: 33.1284 - lr: 1.2346e-05 - 70s/epoch - 357ms/step
Epoch 392/1000
2023-09-30 04:29:24.540 
Epoch 392/1000 
	 loss: 32.6959, MinusLogProbMetric: 32.6959, val_loss: 33.1380, val_MinusLogProbMetric: 33.1380

Epoch 392: val_loss did not improve from 33.09695
196/196 - 70s - loss: 32.6959 - MinusLogProbMetric: 32.6959 - val_loss: 33.1380 - val_MinusLogProbMetric: 33.1380 - lr: 1.2346e-05 - 70s/epoch - 356ms/step
Epoch 393/1000
2023-09-30 04:30:32.714 
Epoch 393/1000 
	 loss: 32.6838, MinusLogProbMetric: 32.6838, val_loss: 33.0568, val_MinusLogProbMetric: 33.0568

Epoch 393: val_loss improved from 33.09695 to 33.05684, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 70s - loss: 32.6838 - MinusLogProbMetric: 32.6838 - val_loss: 33.0568 - val_MinusLogProbMetric: 33.0568 - lr: 1.2346e-05 - 70s/epoch - 356ms/step
Epoch 394/1000
2023-09-30 04:31:43.878 
Epoch 394/1000 
	 loss: 32.6731, MinusLogProbMetric: 32.6731, val_loss: 33.1773, val_MinusLogProbMetric: 33.1773

Epoch 394: val_loss did not improve from 33.05684
196/196 - 70s - loss: 32.6731 - MinusLogProbMetric: 32.6731 - val_loss: 33.1773 - val_MinusLogProbMetric: 33.1773 - lr: 1.2346e-05 - 70s/epoch - 355ms/step
Epoch 395/1000
2023-09-30 04:32:53.786 
Epoch 395/1000 
	 loss: 32.6936, MinusLogProbMetric: 32.6936, val_loss: 33.0979, val_MinusLogProbMetric: 33.0979

Epoch 395: val_loss did not improve from 33.05684
196/196 - 70s - loss: 32.6936 - MinusLogProbMetric: 32.6936 - val_loss: 33.0979 - val_MinusLogProbMetric: 33.0979 - lr: 1.2346e-05 - 70s/epoch - 357ms/step
Epoch 396/1000
2023-09-30 04:34:02.161 
Epoch 396/1000 
	 loss: 32.6269, MinusLogProbMetric: 32.6269, val_loss: 33.0387, val_MinusLogProbMetric: 33.0387

Epoch 396: val_loss improved from 33.05684 to 33.03869, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 70s - loss: 32.6269 - MinusLogProbMetric: 32.6269 - val_loss: 33.0387 - val_MinusLogProbMetric: 33.0387 - lr: 1.2346e-05 - 70s/epoch - 355ms/step
Epoch 397/1000
2023-09-30 04:35:12.643 
Epoch 397/1000 
	 loss: 32.6056, MinusLogProbMetric: 32.6056, val_loss: 33.0316, val_MinusLogProbMetric: 33.0316

Epoch 397: val_loss improved from 33.03869 to 33.03156, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 71s - loss: 32.6056 - MinusLogProbMetric: 32.6056 - val_loss: 33.0316 - val_MinusLogProbMetric: 33.0316 - lr: 1.2346e-05 - 71s/epoch - 364ms/step
Epoch 398/1000
2023-09-30 04:36:24.952 
Epoch 398/1000 
	 loss: 32.6381, MinusLogProbMetric: 32.6381, val_loss: 33.0579, val_MinusLogProbMetric: 33.0579

Epoch 398: val_loss did not improve from 33.03156
196/196 - 70s - loss: 32.6381 - MinusLogProbMetric: 32.6381 - val_loss: 33.0579 - val_MinusLogProbMetric: 33.0579 - lr: 1.2346e-05 - 70s/epoch - 358ms/step
Epoch 399/1000
2023-09-30 04:37:33.697 
Epoch 399/1000 
	 loss: 32.7968, MinusLogProbMetric: 32.7968, val_loss: 32.9682, val_MinusLogProbMetric: 32.9682

Epoch 399: val_loss improved from 33.03156 to 32.96819, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 70s - loss: 32.7968 - MinusLogProbMetric: 32.7968 - val_loss: 32.9682 - val_MinusLogProbMetric: 32.9682 - lr: 1.2346e-05 - 70s/epoch - 358ms/step
Epoch 400/1000
2023-09-30 04:38:44.150 
Epoch 400/1000 
	 loss: 32.5879, MinusLogProbMetric: 32.5879, val_loss: 33.0836, val_MinusLogProbMetric: 33.0836

Epoch 400: val_loss did not improve from 32.96819
196/196 - 69s - loss: 32.5879 - MinusLogProbMetric: 32.5879 - val_loss: 33.0836 - val_MinusLogProbMetric: 33.0836 - lr: 1.2346e-05 - 69s/epoch - 352ms/step
Epoch 401/1000
2023-09-30 04:39:52.219 
Epoch 401/1000 
	 loss: 32.6152, MinusLogProbMetric: 32.6152, val_loss: 32.9558, val_MinusLogProbMetric: 32.9558

Epoch 401: val_loss improved from 32.96819 to 32.95584, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 70s - loss: 32.6152 - MinusLogProbMetric: 32.6152 - val_loss: 32.9558 - val_MinusLogProbMetric: 32.9558 - lr: 1.2346e-05 - 70s/epoch - 356ms/step
Epoch 402/1000
2023-09-30 04:41:03.304 
Epoch 402/1000 
	 loss: 32.6757, MinusLogProbMetric: 32.6757, val_loss: 32.9380, val_MinusLogProbMetric: 32.9380

Epoch 402: val_loss improved from 32.95584 to 32.93799, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 71s - loss: 32.6757 - MinusLogProbMetric: 32.6757 - val_loss: 32.9380 - val_MinusLogProbMetric: 32.9380 - lr: 1.2346e-05 - 71s/epoch - 362ms/step
Epoch 403/1000
2023-09-30 04:42:14.346 
Epoch 403/1000 
	 loss: 32.5896, MinusLogProbMetric: 32.5896, val_loss: 32.9793, val_MinusLogProbMetric: 32.9793

Epoch 403: val_loss did not improve from 32.93799
196/196 - 69s - loss: 32.5896 - MinusLogProbMetric: 32.5896 - val_loss: 32.9793 - val_MinusLogProbMetric: 32.9793 - lr: 1.2346e-05 - 69s/epoch - 354ms/step
Epoch 404/1000
2023-09-30 04:43:20.623 
Epoch 404/1000 
	 loss: 32.5599, MinusLogProbMetric: 32.5599, val_loss: 32.9276, val_MinusLogProbMetric: 32.9276

Epoch 404: val_loss improved from 32.93799 to 32.92756, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 67s - loss: 32.5599 - MinusLogProbMetric: 32.5599 - val_loss: 32.9276 - val_MinusLogProbMetric: 32.9276 - lr: 1.2346e-05 - 67s/epoch - 344ms/step
Epoch 405/1000
2023-09-30 04:44:30.546 
Epoch 405/1000 
	 loss: 32.5568, MinusLogProbMetric: 32.5568, val_loss: 33.0684, val_MinusLogProbMetric: 33.0684

Epoch 405: val_loss did not improve from 32.92756
196/196 - 69s - loss: 32.5568 - MinusLogProbMetric: 32.5568 - val_loss: 33.0684 - val_MinusLogProbMetric: 33.0684 - lr: 1.2346e-05 - 69s/epoch - 351ms/step
Epoch 406/1000
2023-09-30 04:45:38.006 
Epoch 406/1000 
	 loss: 32.5548, MinusLogProbMetric: 32.5548, val_loss: 32.9698, val_MinusLogProbMetric: 32.9698

Epoch 406: val_loss did not improve from 32.92756
196/196 - 67s - loss: 32.5548 - MinusLogProbMetric: 32.5548 - val_loss: 32.9698 - val_MinusLogProbMetric: 32.9698 - lr: 1.2346e-05 - 67s/epoch - 344ms/step
Epoch 407/1000
2023-09-30 04:46:46.423 
Epoch 407/1000 
	 loss: 32.5705, MinusLogProbMetric: 32.5705, val_loss: 33.1203, val_MinusLogProbMetric: 33.1203

Epoch 407: val_loss did not improve from 32.92756
196/196 - 68s - loss: 32.5705 - MinusLogProbMetric: 32.5705 - val_loss: 33.1203 - val_MinusLogProbMetric: 33.1203 - lr: 1.2346e-05 - 68s/epoch - 349ms/step
Epoch 408/1000
2023-09-30 04:47:54.464 
Epoch 408/1000 
	 loss: 32.5983, MinusLogProbMetric: 32.5983, val_loss: 33.6650, val_MinusLogProbMetric: 33.6650

Epoch 408: val_loss did not improve from 32.92756
196/196 - 68s - loss: 32.5983 - MinusLogProbMetric: 32.5983 - val_loss: 33.6650 - val_MinusLogProbMetric: 33.6650 - lr: 1.2346e-05 - 68s/epoch - 347ms/step
Epoch 409/1000
2023-09-30 04:49:04.776 
Epoch 409/1000 
	 loss: 32.5692, MinusLogProbMetric: 32.5692, val_loss: 32.9896, val_MinusLogProbMetric: 32.9896

Epoch 409: val_loss did not improve from 32.92756
196/196 - 70s - loss: 32.5692 - MinusLogProbMetric: 32.5692 - val_loss: 32.9896 - val_MinusLogProbMetric: 32.9896 - lr: 1.2346e-05 - 70s/epoch - 359ms/step
Epoch 410/1000
2023-09-30 04:50:11.054 
Epoch 410/1000 
	 loss: 32.5299, MinusLogProbMetric: 32.5299, val_loss: 32.9804, val_MinusLogProbMetric: 32.9804

Epoch 410: val_loss did not improve from 32.92756
196/196 - 66s - loss: 32.5299 - MinusLogProbMetric: 32.5299 - val_loss: 32.9804 - val_MinusLogProbMetric: 32.9804 - lr: 1.2346e-05 - 66s/epoch - 338ms/step
Epoch 411/1000
2023-09-30 04:51:18.335 
Epoch 411/1000 
	 loss: 32.5103, MinusLogProbMetric: 32.5103, val_loss: 32.9896, val_MinusLogProbMetric: 32.9896

Epoch 411: val_loss did not improve from 32.92756
196/196 - 67s - loss: 32.5103 - MinusLogProbMetric: 32.5103 - val_loss: 32.9896 - val_MinusLogProbMetric: 32.9896 - lr: 1.2346e-05 - 67s/epoch - 343ms/step
Epoch 412/1000
2023-09-30 04:52:25.713 
Epoch 412/1000 
	 loss: 32.4987, MinusLogProbMetric: 32.4987, val_loss: 32.8418, val_MinusLogProbMetric: 32.8418

Epoch 412: val_loss improved from 32.92756 to 32.84178, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 70s - loss: 32.4987 - MinusLogProbMetric: 32.4987 - val_loss: 32.8418 - val_MinusLogProbMetric: 32.8418 - lr: 1.2346e-05 - 70s/epoch - 356ms/step
Epoch 413/1000
2023-09-30 04:53:36.454 
Epoch 413/1000 
	 loss: 32.4698, MinusLogProbMetric: 32.4698, val_loss: 32.9905, val_MinusLogProbMetric: 32.9905

Epoch 413: val_loss did not improve from 32.84178
196/196 - 68s - loss: 32.4698 - MinusLogProbMetric: 32.4698 - val_loss: 32.9905 - val_MinusLogProbMetric: 32.9905 - lr: 1.2346e-05 - 68s/epoch - 348ms/step
Epoch 414/1000
2023-09-30 04:54:43.521 
Epoch 414/1000 
	 loss: 32.5548, MinusLogProbMetric: 32.5548, val_loss: 33.1146, val_MinusLogProbMetric: 33.1146

Epoch 414: val_loss did not improve from 32.84178
196/196 - 67s - loss: 32.5548 - MinusLogProbMetric: 32.5548 - val_loss: 33.1146 - val_MinusLogProbMetric: 33.1146 - lr: 1.2346e-05 - 67s/epoch - 342ms/step
Epoch 415/1000
2023-09-30 04:55:51.060 
Epoch 415/1000 
	 loss: 32.4755, MinusLogProbMetric: 32.4755, val_loss: 33.4215, val_MinusLogProbMetric: 33.4215

Epoch 415: val_loss did not improve from 32.84178
196/196 - 68s - loss: 32.4755 - MinusLogProbMetric: 32.4755 - val_loss: 33.4215 - val_MinusLogProbMetric: 33.4215 - lr: 1.2346e-05 - 68s/epoch - 345ms/step
Epoch 416/1000
2023-09-30 04:56:59.794 
Epoch 416/1000 
	 loss: 38.2503, MinusLogProbMetric: 38.2503, val_loss: 36.0575, val_MinusLogProbMetric: 36.0575

Epoch 416: val_loss did not improve from 32.84178
196/196 - 69s - loss: 38.2503 - MinusLogProbMetric: 38.2503 - val_loss: 36.0575 - val_MinusLogProbMetric: 36.0575 - lr: 1.2346e-05 - 69s/epoch - 351ms/step
Epoch 417/1000
2023-09-30 04:58:08.639 
Epoch 417/1000 
	 loss: 33.1494, MinusLogProbMetric: 33.1494, val_loss: 32.9361, val_MinusLogProbMetric: 32.9361

Epoch 417: val_loss did not improve from 32.84178
196/196 - 69s - loss: 33.1494 - MinusLogProbMetric: 33.1494 - val_loss: 32.9361 - val_MinusLogProbMetric: 32.9361 - lr: 1.2346e-05 - 69s/epoch - 351ms/step
Epoch 418/1000
2023-09-30 04:59:16.994 
Epoch 418/1000 
	 loss: 32.5147, MinusLogProbMetric: 32.5147, val_loss: 32.9601, val_MinusLogProbMetric: 32.9601

Epoch 418: val_loss did not improve from 32.84178
196/196 - 68s - loss: 32.5147 - MinusLogProbMetric: 32.5147 - val_loss: 32.9601 - val_MinusLogProbMetric: 32.9601 - lr: 1.2346e-05 - 68s/epoch - 349ms/step
Epoch 419/1000
2023-09-30 05:00:24.295 
Epoch 419/1000 
	 loss: 32.4729, MinusLogProbMetric: 32.4729, val_loss: 32.9870, val_MinusLogProbMetric: 32.9870

Epoch 419: val_loss did not improve from 32.84178
196/196 - 67s - loss: 32.4729 - MinusLogProbMetric: 32.4729 - val_loss: 32.9870 - val_MinusLogProbMetric: 32.9870 - lr: 1.2346e-05 - 67s/epoch - 343ms/step
Epoch 420/1000
2023-09-30 05:01:34.818 
Epoch 420/1000 
	 loss: 32.4589, MinusLogProbMetric: 32.4589, val_loss: 32.8564, val_MinusLogProbMetric: 32.8564

Epoch 420: val_loss did not improve from 32.84178
196/196 - 71s - loss: 32.4589 - MinusLogProbMetric: 32.4589 - val_loss: 32.8564 - val_MinusLogProbMetric: 32.8564 - lr: 1.2346e-05 - 71s/epoch - 360ms/step
Epoch 421/1000
2023-09-30 05:02:40.177 
Epoch 421/1000 
	 loss: 32.4606, MinusLogProbMetric: 32.4606, val_loss: 32.8997, val_MinusLogProbMetric: 32.8997

Epoch 421: val_loss did not improve from 32.84178
196/196 - 65s - loss: 32.4606 - MinusLogProbMetric: 32.4606 - val_loss: 32.8997 - val_MinusLogProbMetric: 32.8997 - lr: 1.2346e-05 - 65s/epoch - 333ms/step
Epoch 422/1000
2023-09-30 05:03:47.510 
Epoch 422/1000 
	 loss: 32.4232, MinusLogProbMetric: 32.4232, val_loss: 32.8953, val_MinusLogProbMetric: 32.8953

Epoch 422: val_loss did not improve from 32.84178
196/196 - 67s - loss: 32.4232 - MinusLogProbMetric: 32.4232 - val_loss: 32.8953 - val_MinusLogProbMetric: 32.8953 - lr: 1.2346e-05 - 67s/epoch - 344ms/step
Epoch 423/1000
2023-09-30 05:04:55.165 
Epoch 423/1000 
	 loss: 32.4008, MinusLogProbMetric: 32.4008, val_loss: 32.7367, val_MinusLogProbMetric: 32.7367

Epoch 423: val_loss improved from 32.84178 to 32.73668, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 69s - loss: 32.4008 - MinusLogProbMetric: 32.4008 - val_loss: 32.7367 - val_MinusLogProbMetric: 32.7367 - lr: 1.2346e-05 - 69s/epoch - 353ms/step
Epoch 424/1000
2023-09-30 05:06:05.688 
Epoch 424/1000 
	 loss: 32.4062, MinusLogProbMetric: 32.4062, val_loss: 32.8741, val_MinusLogProbMetric: 32.8741

Epoch 424: val_loss did not improve from 32.73668
196/196 - 69s - loss: 32.4062 - MinusLogProbMetric: 32.4062 - val_loss: 32.8741 - val_MinusLogProbMetric: 32.8741 - lr: 1.2346e-05 - 69s/epoch - 352ms/step
Epoch 425/1000
2023-09-30 05:07:14.781 
Epoch 425/1000 
	 loss: 32.3855, MinusLogProbMetric: 32.3855, val_loss: 32.8334, val_MinusLogProbMetric: 32.8334

Epoch 425: val_loss did not improve from 32.73668
196/196 - 69s - loss: 32.3855 - MinusLogProbMetric: 32.3855 - val_loss: 32.8334 - val_MinusLogProbMetric: 32.8334 - lr: 1.2346e-05 - 69s/epoch - 353ms/step
Epoch 426/1000
2023-09-30 05:08:20.036 
Epoch 426/1000 
	 loss: 32.3887, MinusLogProbMetric: 32.3887, val_loss: 32.7486, val_MinusLogProbMetric: 32.7486

Epoch 426: val_loss did not improve from 32.73668
196/196 - 65s - loss: 32.3887 - MinusLogProbMetric: 32.3887 - val_loss: 32.7486 - val_MinusLogProbMetric: 32.7486 - lr: 1.2346e-05 - 65s/epoch - 333ms/step
Epoch 427/1000
2023-09-30 05:09:29.737 
Epoch 427/1000 
	 loss: 32.3844, MinusLogProbMetric: 32.3844, val_loss: 32.7972, val_MinusLogProbMetric: 32.7972

Epoch 427: val_loss did not improve from 32.73668
196/196 - 70s - loss: 32.3844 - MinusLogProbMetric: 32.3844 - val_loss: 32.7972 - val_MinusLogProbMetric: 32.7972 - lr: 1.2346e-05 - 70s/epoch - 356ms/step
Epoch 428/1000
2023-09-30 05:10:38.436 
Epoch 428/1000 
	 loss: 32.3770, MinusLogProbMetric: 32.3770, val_loss: 32.9020, val_MinusLogProbMetric: 32.9020

Epoch 428: val_loss did not improve from 32.73668
196/196 - 69s - loss: 32.3770 - MinusLogProbMetric: 32.3770 - val_loss: 32.9020 - val_MinusLogProbMetric: 32.9020 - lr: 1.2346e-05 - 69s/epoch - 350ms/step
Epoch 429/1000
2023-09-30 05:11:48.008 
Epoch 429/1000 
	 loss: 32.3329, MinusLogProbMetric: 32.3329, val_loss: 32.7096, val_MinusLogProbMetric: 32.7096

Epoch 429: val_loss improved from 32.73668 to 32.70964, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 71s - loss: 32.3329 - MinusLogProbMetric: 32.3329 - val_loss: 32.7096 - val_MinusLogProbMetric: 32.7096 - lr: 1.2346e-05 - 71s/epoch - 360ms/step
Epoch 430/1000
2023-09-30 05:12:57.855 
Epoch 430/1000 
	 loss: 32.3581, MinusLogProbMetric: 32.3581, val_loss: 32.6851, val_MinusLogProbMetric: 32.6851

Epoch 430: val_loss improved from 32.70964 to 32.68506, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 70s - loss: 32.3581 - MinusLogProbMetric: 32.3581 - val_loss: 32.6851 - val_MinusLogProbMetric: 32.6851 - lr: 1.2346e-05 - 70s/epoch - 358ms/step
Epoch 431/1000
2023-09-30 05:14:10.410 
Epoch 431/1000 
	 loss: 32.3194, MinusLogProbMetric: 32.3194, val_loss: 32.8259, val_MinusLogProbMetric: 32.8259

Epoch 431: val_loss did not improve from 32.68506
196/196 - 71s - loss: 32.3194 - MinusLogProbMetric: 32.3194 - val_loss: 32.8259 - val_MinusLogProbMetric: 32.8259 - lr: 1.2346e-05 - 71s/epoch - 363ms/step
Epoch 432/1000
2023-09-30 05:15:17.910 
Epoch 432/1000 
	 loss: 32.3437, MinusLogProbMetric: 32.3437, val_loss: 32.7863, val_MinusLogProbMetric: 32.7863

Epoch 432: val_loss did not improve from 32.68506
196/196 - 67s - loss: 32.3437 - MinusLogProbMetric: 32.3437 - val_loss: 32.7863 - val_MinusLogProbMetric: 32.7863 - lr: 1.2346e-05 - 67s/epoch - 344ms/step
Epoch 433/1000
2023-09-30 05:16:29.157 
Epoch 433/1000 
	 loss: 32.3337, MinusLogProbMetric: 32.3337, val_loss: 36.0717, val_MinusLogProbMetric: 36.0717

Epoch 433: val_loss did not improve from 32.68506
196/196 - 71s - loss: 32.3337 - MinusLogProbMetric: 32.3337 - val_loss: 36.0717 - val_MinusLogProbMetric: 36.0717 - lr: 1.2346e-05 - 71s/epoch - 363ms/step
Epoch 434/1000
2023-09-30 05:17:38.696 
Epoch 434/1000 
	 loss: 33.7374, MinusLogProbMetric: 33.7374, val_loss: 33.3422, val_MinusLogProbMetric: 33.3422

Epoch 434: val_loss did not improve from 32.68506
196/196 - 70s - loss: 33.7374 - MinusLogProbMetric: 33.7374 - val_loss: 33.3422 - val_MinusLogProbMetric: 33.3422 - lr: 1.2346e-05 - 70s/epoch - 355ms/step
Epoch 435/1000
2023-09-30 05:18:47.964 
Epoch 435/1000 
	 loss: 32.7698, MinusLogProbMetric: 32.7698, val_loss: 33.0849, val_MinusLogProbMetric: 33.0849

Epoch 435: val_loss did not improve from 32.68506
196/196 - 69s - loss: 32.7698 - MinusLogProbMetric: 32.7698 - val_loss: 33.0849 - val_MinusLogProbMetric: 33.0849 - lr: 1.2346e-05 - 69s/epoch - 353ms/step
Epoch 436/1000
2023-09-30 05:19:57.644 
Epoch 436/1000 
	 loss: 32.5896, MinusLogProbMetric: 32.5896, val_loss: 33.0901, val_MinusLogProbMetric: 33.0901

Epoch 436: val_loss did not improve from 32.68506
196/196 - 70s - loss: 32.5896 - MinusLogProbMetric: 32.5896 - val_loss: 33.0901 - val_MinusLogProbMetric: 33.0901 - lr: 1.2346e-05 - 70s/epoch - 355ms/step
Epoch 437/1000
2023-09-30 05:21:05.684 
Epoch 437/1000 
	 loss: 32.5736, MinusLogProbMetric: 32.5736, val_loss: 32.9826, val_MinusLogProbMetric: 32.9826

Epoch 437: val_loss did not improve from 32.68506
196/196 - 68s - loss: 32.5736 - MinusLogProbMetric: 32.5736 - val_loss: 32.9826 - val_MinusLogProbMetric: 32.9826 - lr: 1.2346e-05 - 68s/epoch - 347ms/step
Epoch 438/1000
2023-09-30 05:22:16.695 
Epoch 438/1000 
	 loss: 32.5006, MinusLogProbMetric: 32.5006, val_loss: 32.9446, val_MinusLogProbMetric: 32.9446

Epoch 438: val_loss did not improve from 32.68506
196/196 - 71s - loss: 32.5006 - MinusLogProbMetric: 32.5006 - val_loss: 32.9446 - val_MinusLogProbMetric: 32.9446 - lr: 1.2346e-05 - 71s/epoch - 362ms/step
Epoch 439/1000
2023-09-30 05:23:22.485 
Epoch 439/1000 
	 loss: 32.4446, MinusLogProbMetric: 32.4446, val_loss: 33.0905, val_MinusLogProbMetric: 33.0905

Epoch 439: val_loss did not improve from 32.68506
196/196 - 66s - loss: 32.4446 - MinusLogProbMetric: 32.4446 - val_loss: 33.0905 - val_MinusLogProbMetric: 33.0905 - lr: 1.2346e-05 - 66s/epoch - 336ms/step
Epoch 440/1000
2023-09-30 05:24:26.993 
Epoch 440/1000 
	 loss: 32.4290, MinusLogProbMetric: 32.4290, val_loss: 33.0383, val_MinusLogProbMetric: 33.0383

Epoch 440: val_loss did not improve from 32.68506
196/196 - 65s - loss: 32.4290 - MinusLogProbMetric: 32.4290 - val_loss: 33.0383 - val_MinusLogProbMetric: 33.0383 - lr: 1.2346e-05 - 65s/epoch - 329ms/step
Epoch 441/1000
2023-09-30 05:25:29.859 
Epoch 441/1000 
	 loss: 32.3872, MinusLogProbMetric: 32.3872, val_loss: 32.7892, val_MinusLogProbMetric: 32.7892

Epoch 441: val_loss did not improve from 32.68506
196/196 - 63s - loss: 32.3872 - MinusLogProbMetric: 32.3872 - val_loss: 32.7892 - val_MinusLogProbMetric: 32.7892 - lr: 1.2346e-05 - 63s/epoch - 321ms/step
Epoch 442/1000
2023-09-30 05:26:37.725 
Epoch 442/1000 
	 loss: 32.3949, MinusLogProbMetric: 32.3949, val_loss: 32.9126, val_MinusLogProbMetric: 32.9126

Epoch 442: val_loss did not improve from 32.68506
196/196 - 68s - loss: 32.3949 - MinusLogProbMetric: 32.3949 - val_loss: 32.9126 - val_MinusLogProbMetric: 32.9126 - lr: 1.2346e-05 - 68s/epoch - 346ms/step
Epoch 443/1000
2023-09-30 05:27:46.049 
Epoch 443/1000 
	 loss: 32.3012, MinusLogProbMetric: 32.3012, val_loss: 32.8918, val_MinusLogProbMetric: 32.8918

Epoch 443: val_loss did not improve from 32.68506
196/196 - 68s - loss: 32.3012 - MinusLogProbMetric: 32.3012 - val_loss: 32.8918 - val_MinusLogProbMetric: 32.8918 - lr: 1.2346e-05 - 68s/epoch - 349ms/step
Epoch 444/1000
2023-09-30 05:28:51.133 
Epoch 444/1000 
	 loss: 32.3051, MinusLogProbMetric: 32.3051, val_loss: 32.7213, val_MinusLogProbMetric: 32.7213

Epoch 444: val_loss did not improve from 32.68506
196/196 - 65s - loss: 32.3051 - MinusLogProbMetric: 32.3051 - val_loss: 32.7213 - val_MinusLogProbMetric: 32.7213 - lr: 1.2346e-05 - 65s/epoch - 332ms/step
Epoch 445/1000
2023-09-30 05:29:56.394 
Epoch 445/1000 
	 loss: 32.3014, MinusLogProbMetric: 32.3014, val_loss: 32.9761, val_MinusLogProbMetric: 32.9761

Epoch 445: val_loss did not improve from 32.68506
196/196 - 65s - loss: 32.3014 - MinusLogProbMetric: 32.3014 - val_loss: 32.9761 - val_MinusLogProbMetric: 32.9761 - lr: 1.2346e-05 - 65s/epoch - 333ms/step
Epoch 446/1000
2023-09-30 05:31:03.745 
Epoch 446/1000 
	 loss: 32.2784, MinusLogProbMetric: 32.2784, val_loss: 32.9338, val_MinusLogProbMetric: 32.9338

Epoch 446: val_loss did not improve from 32.68506
196/196 - 67s - loss: 32.2784 - MinusLogProbMetric: 32.2784 - val_loss: 32.9338 - val_MinusLogProbMetric: 32.9338 - lr: 1.2346e-05 - 67s/epoch - 344ms/step
Epoch 447/1000
2023-09-30 05:32:05.582 
Epoch 447/1000 
	 loss: 32.2647, MinusLogProbMetric: 32.2647, val_loss: 33.0668, val_MinusLogProbMetric: 33.0668

Epoch 447: val_loss did not improve from 32.68506
196/196 - 62s - loss: 32.2647 - MinusLogProbMetric: 32.2647 - val_loss: 33.0668 - val_MinusLogProbMetric: 33.0668 - lr: 1.2346e-05 - 62s/epoch - 315ms/step
Epoch 448/1000
2023-09-30 05:33:10.104 
Epoch 448/1000 
	 loss: 32.2622, MinusLogProbMetric: 32.2622, val_loss: 32.7920, val_MinusLogProbMetric: 32.7920

Epoch 448: val_loss did not improve from 32.68506
196/196 - 65s - loss: 32.2622 - MinusLogProbMetric: 32.2622 - val_loss: 32.7920 - val_MinusLogProbMetric: 32.7920 - lr: 1.2346e-05 - 65s/epoch - 329ms/step
Epoch 449/1000
2023-09-30 05:34:12.608 
Epoch 449/1000 
	 loss: 32.2358, MinusLogProbMetric: 32.2358, val_loss: 32.7926, val_MinusLogProbMetric: 32.7926

Epoch 449: val_loss did not improve from 32.68506
196/196 - 62s - loss: 32.2358 - MinusLogProbMetric: 32.2358 - val_loss: 32.7926 - val_MinusLogProbMetric: 32.7926 - lr: 1.2346e-05 - 62s/epoch - 319ms/step
Epoch 450/1000
2023-09-30 05:35:22.337 
Epoch 450/1000 
	 loss: 32.2666, MinusLogProbMetric: 32.2666, val_loss: 32.8282, val_MinusLogProbMetric: 32.8282

Epoch 450: val_loss did not improve from 32.68506
196/196 - 70s - loss: 32.2666 - MinusLogProbMetric: 32.2666 - val_loss: 32.8282 - val_MinusLogProbMetric: 32.8282 - lr: 1.2346e-05 - 70s/epoch - 356ms/step
Epoch 451/1000
2023-09-30 05:36:27.947 
Epoch 451/1000 
	 loss: 32.2158, MinusLogProbMetric: 32.2158, val_loss: 32.8743, val_MinusLogProbMetric: 32.8743

Epoch 451: val_loss did not improve from 32.68506
196/196 - 66s - loss: 32.2158 - MinusLogProbMetric: 32.2158 - val_loss: 32.8743 - val_MinusLogProbMetric: 32.8743 - lr: 1.2346e-05 - 66s/epoch - 335ms/step
Epoch 452/1000
2023-09-30 05:37:37.706 
Epoch 452/1000 
	 loss: 32.2365, MinusLogProbMetric: 32.2365, val_loss: 32.8491, val_MinusLogProbMetric: 32.8491

Epoch 452: val_loss did not improve from 32.68506
196/196 - 70s - loss: 32.2365 - MinusLogProbMetric: 32.2365 - val_loss: 32.8491 - val_MinusLogProbMetric: 32.8491 - lr: 1.2346e-05 - 70s/epoch - 356ms/step
Epoch 453/1000
2023-09-30 05:38:45.794 
Epoch 453/1000 
	 loss: 32.2143, MinusLogProbMetric: 32.2143, val_loss: 32.7449, val_MinusLogProbMetric: 32.7449

Epoch 453: val_loss did not improve from 32.68506
196/196 - 68s - loss: 32.2143 - MinusLogProbMetric: 32.2143 - val_loss: 32.7449 - val_MinusLogProbMetric: 32.7449 - lr: 1.2346e-05 - 68s/epoch - 347ms/step
Epoch 454/1000
2023-09-30 05:39:48.579 
Epoch 454/1000 
	 loss: 32.1747, MinusLogProbMetric: 32.1747, val_loss: 32.5322, val_MinusLogProbMetric: 32.5322

Epoch 454: val_loss improved from 32.68506 to 32.53220, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 65s - loss: 32.1747 - MinusLogProbMetric: 32.1747 - val_loss: 32.5322 - val_MinusLogProbMetric: 32.5322 - lr: 1.2346e-05 - 65s/epoch - 331ms/step
Epoch 455/1000
2023-09-30 05:40:59.415 
Epoch 455/1000 
	 loss: 32.1998, MinusLogProbMetric: 32.1998, val_loss: 32.6886, val_MinusLogProbMetric: 32.6886

Epoch 455: val_loss did not improve from 32.53220
196/196 - 69s - loss: 32.1998 - MinusLogProbMetric: 32.1998 - val_loss: 32.6886 - val_MinusLogProbMetric: 32.6886 - lr: 1.2346e-05 - 69s/epoch - 351ms/step
Epoch 456/1000
2023-09-30 05:42:03.972 
Epoch 456/1000 
	 loss: 32.1880, MinusLogProbMetric: 32.1880, val_loss: 32.7271, val_MinusLogProbMetric: 32.7271

Epoch 456: val_loss did not improve from 32.53220
196/196 - 65s - loss: 32.1880 - MinusLogProbMetric: 32.1880 - val_loss: 32.7271 - val_MinusLogProbMetric: 32.7271 - lr: 1.2346e-05 - 65s/epoch - 329ms/step
Epoch 457/1000
2023-09-30 05:43:09.455 
Epoch 457/1000 
	 loss: 32.1648, MinusLogProbMetric: 32.1648, val_loss: 32.5773, val_MinusLogProbMetric: 32.5773

Epoch 457: val_loss did not improve from 32.53220
196/196 - 65s - loss: 32.1648 - MinusLogProbMetric: 32.1648 - val_loss: 32.5773 - val_MinusLogProbMetric: 32.5773 - lr: 1.2346e-05 - 65s/epoch - 334ms/step
Epoch 458/1000
2023-09-30 05:44:17.998 
Epoch 458/1000 
	 loss: 32.1709, MinusLogProbMetric: 32.1709, val_loss: 32.5926, val_MinusLogProbMetric: 32.5926

Epoch 458: val_loss did not improve from 32.53220
196/196 - 69s - loss: 32.1709 - MinusLogProbMetric: 32.1709 - val_loss: 32.5926 - val_MinusLogProbMetric: 32.5926 - lr: 1.2346e-05 - 69s/epoch - 350ms/step
Epoch 459/1000
2023-09-30 05:45:21.966 
Epoch 459/1000 
	 loss: 32.1546, MinusLogProbMetric: 32.1546, val_loss: 32.7385, val_MinusLogProbMetric: 32.7385

Epoch 459: val_loss did not improve from 32.53220
196/196 - 64s - loss: 32.1546 - MinusLogProbMetric: 32.1546 - val_loss: 32.7385 - val_MinusLogProbMetric: 32.7385 - lr: 1.2346e-05 - 64s/epoch - 326ms/step
Epoch 460/1000
2023-09-30 05:46:23.420 
Epoch 460/1000 
	 loss: 32.1780, MinusLogProbMetric: 32.1780, val_loss: 32.7461, val_MinusLogProbMetric: 32.7461

Epoch 460: val_loss did not improve from 32.53220
196/196 - 61s - loss: 32.1780 - MinusLogProbMetric: 32.1780 - val_loss: 32.7461 - val_MinusLogProbMetric: 32.7461 - lr: 1.2346e-05 - 61s/epoch - 314ms/step
Epoch 461/1000
2023-09-30 05:47:31.960 
Epoch 461/1000 
	 loss: 32.1501, MinusLogProbMetric: 32.1501, val_loss: 32.6583, val_MinusLogProbMetric: 32.6583

Epoch 461: val_loss did not improve from 32.53220
196/196 - 69s - loss: 32.1501 - MinusLogProbMetric: 32.1501 - val_loss: 32.6583 - val_MinusLogProbMetric: 32.6583 - lr: 1.2346e-05 - 69s/epoch - 350ms/step
Epoch 462/1000
2023-09-30 05:48:37.770 
Epoch 462/1000 
	 loss: 32.1082, MinusLogProbMetric: 32.1082, val_loss: 32.4895, val_MinusLogProbMetric: 32.4895

Epoch 462: val_loss improved from 32.53220 to 32.48947, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 68s - loss: 32.1082 - MinusLogProbMetric: 32.1082 - val_loss: 32.4895 - val_MinusLogProbMetric: 32.4895 - lr: 1.2346e-05 - 68s/epoch - 346ms/step
Epoch 463/1000
2023-09-30 05:49:44.930 
Epoch 463/1000 
	 loss: 32.1296, MinusLogProbMetric: 32.1296, val_loss: 32.7754, val_MinusLogProbMetric: 32.7754

Epoch 463: val_loss did not improve from 32.48947
196/196 - 65s - loss: 32.1296 - MinusLogProbMetric: 32.1296 - val_loss: 32.7754 - val_MinusLogProbMetric: 32.7754 - lr: 1.2346e-05 - 65s/epoch - 332ms/step
Epoch 464/1000
2023-09-30 05:50:52.247 
Epoch 464/1000 
	 loss: 32.1007, MinusLogProbMetric: 32.1007, val_loss: 32.5668, val_MinusLogProbMetric: 32.5668

Epoch 464: val_loss did not improve from 32.48947
196/196 - 67s - loss: 32.1007 - MinusLogProbMetric: 32.1007 - val_loss: 32.5668 - val_MinusLogProbMetric: 32.5668 - lr: 1.2346e-05 - 67s/epoch - 343ms/step
Epoch 465/1000
2023-09-30 05:51:59.987 
Epoch 465/1000 
	 loss: 32.0619, MinusLogProbMetric: 32.0619, val_loss: 32.4580, val_MinusLogProbMetric: 32.4580

Epoch 465: val_loss improved from 32.48947 to 32.45800, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 69s - loss: 32.0619 - MinusLogProbMetric: 32.0619 - val_loss: 32.4580 - val_MinusLogProbMetric: 32.4580 - lr: 1.2346e-05 - 69s/epoch - 353ms/step
Epoch 466/1000
2023-09-30 05:53:06.219 
Epoch 466/1000 
	 loss: 32.0980, MinusLogProbMetric: 32.0980, val_loss: 32.5423, val_MinusLogProbMetric: 32.5423

Epoch 466: val_loss did not improve from 32.45800
196/196 - 65s - loss: 32.0980 - MinusLogProbMetric: 32.0980 - val_loss: 32.5423 - val_MinusLogProbMetric: 32.5423 - lr: 1.2346e-05 - 65s/epoch - 330ms/step
Epoch 467/1000
2023-09-30 05:54:10.055 
Epoch 467/1000 
	 loss: 32.1062, MinusLogProbMetric: 32.1062, val_loss: 32.4465, val_MinusLogProbMetric: 32.4465

Epoch 467: val_loss improved from 32.45800 to 32.44653, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 65s - loss: 32.1062 - MinusLogProbMetric: 32.1062 - val_loss: 32.4465 - val_MinusLogProbMetric: 32.4465 - lr: 1.2346e-05 - 65s/epoch - 333ms/step
Epoch 468/1000
2023-09-30 05:55:13.753 
Epoch 468/1000 
	 loss: 32.0656, MinusLogProbMetric: 32.0656, val_loss: 32.5317, val_MinusLogProbMetric: 32.5317

Epoch 468: val_loss did not improve from 32.44653
196/196 - 62s - loss: 32.0656 - MinusLogProbMetric: 32.0656 - val_loss: 32.5317 - val_MinusLogProbMetric: 32.5317 - lr: 1.2346e-05 - 62s/epoch - 318ms/step
Epoch 469/1000
2023-09-30 05:56:17.055 
Epoch 469/1000 
	 loss: 32.0427, MinusLogProbMetric: 32.0427, val_loss: 32.5181, val_MinusLogProbMetric: 32.5181

Epoch 469: val_loss did not improve from 32.44653
196/196 - 63s - loss: 32.0427 - MinusLogProbMetric: 32.0427 - val_loss: 32.5181 - val_MinusLogProbMetric: 32.5181 - lr: 1.2346e-05 - 63s/epoch - 323ms/step
Epoch 470/1000
2023-09-30 05:57:21.901 
Epoch 470/1000 
	 loss: 32.0637, MinusLogProbMetric: 32.0637, val_loss: 32.3471, val_MinusLogProbMetric: 32.3471

Epoch 470: val_loss improved from 32.44653 to 32.34713, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 66s - loss: 32.0637 - MinusLogProbMetric: 32.0637 - val_loss: 32.3471 - val_MinusLogProbMetric: 32.3471 - lr: 1.2346e-05 - 66s/epoch - 337ms/step
Epoch 471/1000
2023-09-30 05:58:28.921 
Epoch 471/1000 
	 loss: 32.0680, MinusLogProbMetric: 32.0680, val_loss: 32.5628, val_MinusLogProbMetric: 32.5628

Epoch 471: val_loss did not improve from 32.34713
196/196 - 66s - loss: 32.0680 - MinusLogProbMetric: 32.0680 - val_loss: 32.5628 - val_MinusLogProbMetric: 32.5628 - lr: 1.2346e-05 - 66s/epoch - 336ms/step
Epoch 472/1000
2023-09-30 05:59:34.727 
Epoch 472/1000 
	 loss: 32.0606, MinusLogProbMetric: 32.0606, val_loss: 32.3661, val_MinusLogProbMetric: 32.3661

Epoch 472: val_loss did not improve from 32.34713
196/196 - 66s - loss: 32.0606 - MinusLogProbMetric: 32.0606 - val_loss: 32.3661 - val_MinusLogProbMetric: 32.3661 - lr: 1.2346e-05 - 66s/epoch - 336ms/step
Epoch 473/1000
2023-09-30 06:00:41.157 
Epoch 473/1000 
	 loss: 32.0618, MinusLogProbMetric: 32.0618, val_loss: 32.5427, val_MinusLogProbMetric: 32.5427

Epoch 473: val_loss did not improve from 32.34713
196/196 - 66s - loss: 32.0618 - MinusLogProbMetric: 32.0618 - val_loss: 32.5427 - val_MinusLogProbMetric: 32.5427 - lr: 1.2346e-05 - 66s/epoch - 339ms/step
Epoch 474/1000
2023-09-30 06:01:45.218 
Epoch 474/1000 
	 loss: 32.0281, MinusLogProbMetric: 32.0281, val_loss: 33.0020, val_MinusLogProbMetric: 33.0020

Epoch 474: val_loss did not improve from 32.34713
196/196 - 64s - loss: 32.0281 - MinusLogProbMetric: 32.0281 - val_loss: 33.0020 - val_MinusLogProbMetric: 33.0020 - lr: 1.2346e-05 - 64s/epoch - 327ms/step
Epoch 475/1000
2023-09-30 06:02:52.849 
Epoch 475/1000 
	 loss: 32.0211, MinusLogProbMetric: 32.0211, val_loss: 32.6078, val_MinusLogProbMetric: 32.6078

Epoch 475: val_loss did not improve from 32.34713
196/196 - 68s - loss: 32.0211 - MinusLogProbMetric: 32.0211 - val_loss: 32.6078 - val_MinusLogProbMetric: 32.6078 - lr: 1.2346e-05 - 68s/epoch - 345ms/step
Epoch 476/1000
2023-09-30 06:03:57.335 
Epoch 476/1000 
	 loss: 32.0176, MinusLogProbMetric: 32.0176, val_loss: 32.4184, val_MinusLogProbMetric: 32.4184

Epoch 476: val_loss did not improve from 32.34713
196/196 - 64s - loss: 32.0176 - MinusLogProbMetric: 32.0176 - val_loss: 32.4184 - val_MinusLogProbMetric: 32.4184 - lr: 1.2346e-05 - 64s/epoch - 329ms/step
Epoch 477/1000
2023-09-30 06:05:04.408 
Epoch 477/1000 
	 loss: 32.0103, MinusLogProbMetric: 32.0103, val_loss: 32.4505, val_MinusLogProbMetric: 32.4505

Epoch 477: val_loss did not improve from 32.34713
196/196 - 67s - loss: 32.0103 - MinusLogProbMetric: 32.0103 - val_loss: 32.4505 - val_MinusLogProbMetric: 32.4505 - lr: 1.2346e-05 - 67s/epoch - 342ms/step
Epoch 478/1000
2023-09-30 06:06:12.199 
Epoch 478/1000 
	 loss: 32.0051, MinusLogProbMetric: 32.0051, val_loss: 32.3512, val_MinusLogProbMetric: 32.3512

Epoch 478: val_loss did not improve from 32.34713
196/196 - 68s - loss: 32.0051 - MinusLogProbMetric: 32.0051 - val_loss: 32.3512 - val_MinusLogProbMetric: 32.3512 - lr: 1.2346e-05 - 68s/epoch - 346ms/step
Epoch 479/1000
2023-09-30 06:07:17.983 
Epoch 479/1000 
	 loss: 31.9672, MinusLogProbMetric: 31.9672, val_loss: 32.4607, val_MinusLogProbMetric: 32.4607

Epoch 479: val_loss did not improve from 32.34713
196/196 - 66s - loss: 31.9672 - MinusLogProbMetric: 31.9672 - val_loss: 32.4607 - val_MinusLogProbMetric: 32.4607 - lr: 1.2346e-05 - 66s/epoch - 336ms/step
Epoch 480/1000
2023-09-30 06:08:20.717 
Epoch 480/1000 
	 loss: 31.9568, MinusLogProbMetric: 31.9568, val_loss: 32.3844, val_MinusLogProbMetric: 32.3844

Epoch 480: val_loss did not improve from 32.34713
196/196 - 63s - loss: 31.9568 - MinusLogProbMetric: 31.9568 - val_loss: 32.3844 - val_MinusLogProbMetric: 32.3844 - lr: 1.2346e-05 - 63s/epoch - 320ms/step
Epoch 481/1000
2023-09-30 06:09:27.749 
Epoch 481/1000 
	 loss: 31.9700, MinusLogProbMetric: 31.9700, val_loss: 32.4065, val_MinusLogProbMetric: 32.4065

Epoch 481: val_loss did not improve from 32.34713
196/196 - 67s - loss: 31.9700 - MinusLogProbMetric: 31.9700 - val_loss: 32.4065 - val_MinusLogProbMetric: 32.4065 - lr: 1.2346e-05 - 67s/epoch - 342ms/step
Epoch 482/1000
2023-09-30 06:10:30.905 
Epoch 482/1000 
	 loss: 31.9504, MinusLogProbMetric: 31.9504, val_loss: 32.3414, val_MinusLogProbMetric: 32.3414

Epoch 482: val_loss improved from 32.34713 to 32.34138, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 64s - loss: 31.9504 - MinusLogProbMetric: 31.9504 - val_loss: 32.3414 - val_MinusLogProbMetric: 32.3414 - lr: 1.2346e-05 - 64s/epoch - 329ms/step
Epoch 483/1000
2023-09-30 06:11:38.516 
Epoch 483/1000 
	 loss: 31.9513, MinusLogProbMetric: 31.9513, val_loss: 32.4300, val_MinusLogProbMetric: 32.4300

Epoch 483: val_loss did not improve from 32.34138
196/196 - 66s - loss: 31.9513 - MinusLogProbMetric: 31.9513 - val_loss: 32.4300 - val_MinusLogProbMetric: 32.4300 - lr: 1.2346e-05 - 66s/epoch - 339ms/step
Epoch 484/1000
2023-09-30 06:12:41.695 
Epoch 484/1000 
	 loss: 31.9336, MinusLogProbMetric: 31.9336, val_loss: 32.3077, val_MinusLogProbMetric: 32.3077

Epoch 484: val_loss improved from 32.34138 to 32.30767, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 65s - loss: 31.9336 - MinusLogProbMetric: 31.9336 - val_loss: 32.3077 - val_MinusLogProbMetric: 32.3077 - lr: 1.2346e-05 - 65s/epoch - 332ms/step
Epoch 485/1000
2023-09-30 06:13:45.173 
Epoch 485/1000 
	 loss: 31.9096, MinusLogProbMetric: 31.9096, val_loss: 32.3698, val_MinusLogProbMetric: 32.3698

Epoch 485: val_loss did not improve from 32.30767
196/196 - 62s - loss: 31.9096 - MinusLogProbMetric: 31.9096 - val_loss: 32.3698 - val_MinusLogProbMetric: 32.3698 - lr: 1.2346e-05 - 62s/epoch - 314ms/step
Epoch 486/1000
2023-09-30 06:14:47.686 
Epoch 486/1000 
	 loss: 31.9129, MinusLogProbMetric: 31.9129, val_loss: 32.5506, val_MinusLogProbMetric: 32.5506

Epoch 486: val_loss did not improve from 32.30767
196/196 - 63s - loss: 31.9129 - MinusLogProbMetric: 31.9129 - val_loss: 32.5506 - val_MinusLogProbMetric: 32.5506 - lr: 1.2346e-05 - 63s/epoch - 319ms/step
Epoch 487/1000
2023-09-30 06:15:51.467 
Epoch 487/1000 
	 loss: 31.9662, MinusLogProbMetric: 31.9662, val_loss: 32.3767, val_MinusLogProbMetric: 32.3767

Epoch 487: val_loss did not improve from 32.30767
196/196 - 64s - loss: 31.9662 - MinusLogProbMetric: 31.9662 - val_loss: 32.3767 - val_MinusLogProbMetric: 32.3767 - lr: 1.2346e-05 - 64s/epoch - 325ms/step
Epoch 488/1000
2023-09-30 06:16:57.586 
Epoch 488/1000 
	 loss: 31.9379, MinusLogProbMetric: 31.9379, val_loss: 32.5175, val_MinusLogProbMetric: 32.5175

Epoch 488: val_loss did not improve from 32.30767
196/196 - 66s - loss: 31.9379 - MinusLogProbMetric: 31.9379 - val_loss: 32.5175 - val_MinusLogProbMetric: 32.5175 - lr: 1.2346e-05 - 66s/epoch - 337ms/step
Epoch 489/1000
2023-09-30 06:18:01.754 
Epoch 489/1000 
	 loss: 31.9288, MinusLogProbMetric: 31.9288, val_loss: 32.3420, val_MinusLogProbMetric: 32.3420

Epoch 489: val_loss did not improve from 32.30767
196/196 - 64s - loss: 31.9288 - MinusLogProbMetric: 31.9288 - val_loss: 32.3420 - val_MinusLogProbMetric: 32.3420 - lr: 1.2346e-05 - 64s/epoch - 327ms/step
Epoch 490/1000
2023-09-30 06:19:08.721 
Epoch 490/1000 
	 loss: 31.9239, MinusLogProbMetric: 31.9239, val_loss: 32.3367, val_MinusLogProbMetric: 32.3367

Epoch 490: val_loss did not improve from 32.30767
196/196 - 67s - loss: 31.9239 - MinusLogProbMetric: 31.9239 - val_loss: 32.3367 - val_MinusLogProbMetric: 32.3367 - lr: 1.2346e-05 - 67s/epoch - 342ms/step
Epoch 491/1000
2023-09-30 06:20:15.409 
Epoch 491/1000 
	 loss: 31.9004, MinusLogProbMetric: 31.9004, val_loss: 32.4114, val_MinusLogProbMetric: 32.4114

Epoch 491: val_loss did not improve from 32.30767
196/196 - 67s - loss: 31.9004 - MinusLogProbMetric: 31.9004 - val_loss: 32.4114 - val_MinusLogProbMetric: 32.4114 - lr: 1.2346e-05 - 67s/epoch - 340ms/step
Epoch 492/1000
2023-09-30 06:21:20.612 
Epoch 492/1000 
	 loss: 31.9200, MinusLogProbMetric: 31.9200, val_loss: 32.2822, val_MinusLogProbMetric: 32.2822

Epoch 492: val_loss improved from 32.30767 to 32.28224, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 67s - loss: 31.9200 - MinusLogProbMetric: 31.9200 - val_loss: 32.2822 - val_MinusLogProbMetric: 32.2822 - lr: 1.2346e-05 - 67s/epoch - 340ms/step
Epoch 493/1000
2023-09-30 06:22:30.219 
Epoch 493/1000 
	 loss: 31.9186, MinusLogProbMetric: 31.9186, val_loss: 32.3394, val_MinusLogProbMetric: 32.3394

Epoch 493: val_loss did not improve from 32.28224
196/196 - 68s - loss: 31.9186 - MinusLogProbMetric: 31.9186 - val_loss: 32.3394 - val_MinusLogProbMetric: 32.3394 - lr: 1.2346e-05 - 68s/epoch - 347ms/step
Epoch 494/1000
2023-09-30 06:23:35.124 
Epoch 494/1000 
	 loss: 31.9204, MinusLogProbMetric: 31.9204, val_loss: 32.3575, val_MinusLogProbMetric: 32.3575

Epoch 494: val_loss did not improve from 32.28224
196/196 - 65s - loss: 31.9204 - MinusLogProbMetric: 31.9204 - val_loss: 32.3575 - val_MinusLogProbMetric: 32.3575 - lr: 1.2346e-05 - 65s/epoch - 331ms/step
Epoch 495/1000
2023-09-30 06:24:41.955 
Epoch 495/1000 
	 loss: 31.9015, MinusLogProbMetric: 31.9015, val_loss: 32.5673, val_MinusLogProbMetric: 32.5673

Epoch 495: val_loss did not improve from 32.28224
196/196 - 67s - loss: 31.9015 - MinusLogProbMetric: 31.9015 - val_loss: 32.5673 - val_MinusLogProbMetric: 32.5673 - lr: 1.2346e-05 - 67s/epoch - 341ms/step
Epoch 496/1000
2023-09-30 06:25:46.528 
Epoch 496/1000 
	 loss: 31.9268, MinusLogProbMetric: 31.9268, val_loss: 32.2234, val_MinusLogProbMetric: 32.2234

Epoch 496: val_loss improved from 32.28224 to 32.22338, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 66s - loss: 31.9268 - MinusLogProbMetric: 31.9268 - val_loss: 32.2234 - val_MinusLogProbMetric: 32.2234 - lr: 1.2346e-05 - 66s/epoch - 337ms/step
Epoch 497/1000
2023-09-30 06:26:51.722 
Epoch 497/1000 
	 loss: 31.8639, MinusLogProbMetric: 31.8639, val_loss: 32.3260, val_MinusLogProbMetric: 32.3260

Epoch 497: val_loss did not improve from 32.22338
196/196 - 64s - loss: 31.8639 - MinusLogProbMetric: 31.8639 - val_loss: 32.3260 - val_MinusLogProbMetric: 32.3260 - lr: 1.2346e-05 - 64s/epoch - 325ms/step
Epoch 498/1000
2023-09-30 06:27:56.408 
Epoch 498/1000 
	 loss: 31.8567, MinusLogProbMetric: 31.8567, val_loss: 32.3420, val_MinusLogProbMetric: 32.3420

Epoch 498: val_loss did not improve from 32.22338
196/196 - 65s - loss: 31.8567 - MinusLogProbMetric: 31.8567 - val_loss: 32.3420 - val_MinusLogProbMetric: 32.3420 - lr: 1.2346e-05 - 65s/epoch - 330ms/step
Epoch 499/1000
2023-09-30 06:29:03.643 
Epoch 499/1000 
	 loss: 31.8603, MinusLogProbMetric: 31.8603, val_loss: 32.2827, val_MinusLogProbMetric: 32.2827

Epoch 499: val_loss did not improve from 32.22338
196/196 - 67s - loss: 31.8603 - MinusLogProbMetric: 31.8603 - val_loss: 32.2827 - val_MinusLogProbMetric: 32.2827 - lr: 1.2346e-05 - 67s/epoch - 343ms/step
Epoch 500/1000
2023-09-30 06:30:13.579 
Epoch 500/1000 
	 loss: 31.8499, MinusLogProbMetric: 31.8499, val_loss: 32.3062, val_MinusLogProbMetric: 32.3062

Epoch 500: val_loss did not improve from 32.22338
196/196 - 70s - loss: 31.8499 - MinusLogProbMetric: 31.8499 - val_loss: 32.3062 - val_MinusLogProbMetric: 32.3062 - lr: 1.2346e-05 - 70s/epoch - 357ms/step
Epoch 501/1000
2023-09-30 06:31:23.328 
Epoch 501/1000 
	 loss: 31.8784, MinusLogProbMetric: 31.8784, val_loss: 32.2689, val_MinusLogProbMetric: 32.2689

Epoch 501: val_loss did not improve from 32.22338
196/196 - 70s - loss: 31.8784 - MinusLogProbMetric: 31.8784 - val_loss: 32.2689 - val_MinusLogProbMetric: 32.2689 - lr: 1.2346e-05 - 70s/epoch - 356ms/step
Epoch 502/1000
2023-09-30 06:32:31.035 
Epoch 502/1000 
	 loss: 31.8658, MinusLogProbMetric: 31.8658, val_loss: 32.3269, val_MinusLogProbMetric: 32.3269

Epoch 502: val_loss did not improve from 32.22338
196/196 - 68s - loss: 31.8658 - MinusLogProbMetric: 31.8658 - val_loss: 32.3269 - val_MinusLogProbMetric: 32.3269 - lr: 1.2346e-05 - 68s/epoch - 345ms/step
Epoch 503/1000
2023-09-30 06:33:39.352 
Epoch 503/1000 
	 loss: 31.8342, MinusLogProbMetric: 31.8342, val_loss: 32.2209, val_MinusLogProbMetric: 32.2209

Epoch 503: val_loss improved from 32.22338 to 32.22091, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 70s - loss: 31.8342 - MinusLogProbMetric: 31.8342 - val_loss: 32.2209 - val_MinusLogProbMetric: 32.2209 - lr: 1.2346e-05 - 70s/epoch - 355ms/step
Epoch 504/1000
2023-09-30 06:34:45.350 
Epoch 504/1000 
	 loss: 31.8343, MinusLogProbMetric: 31.8343, val_loss: 32.6719, val_MinusLogProbMetric: 32.6719

Epoch 504: val_loss did not improve from 32.22091
196/196 - 65s - loss: 31.8343 - MinusLogProbMetric: 31.8343 - val_loss: 32.6719 - val_MinusLogProbMetric: 32.6719 - lr: 1.2346e-05 - 65s/epoch - 330ms/step
Epoch 505/1000
2023-09-30 06:35:59.896 
Epoch 505/1000 
	 loss: 31.8200, MinusLogProbMetric: 31.8200, val_loss: 32.1197, val_MinusLogProbMetric: 32.1197

Epoch 505: val_loss improved from 32.22091 to 32.11970, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 77s - loss: 31.8200 - MinusLogProbMetric: 31.8200 - val_loss: 32.1197 - val_MinusLogProbMetric: 32.1197 - lr: 1.2346e-05 - 77s/epoch - 392ms/step
Epoch 506/1000
2023-09-30 06:37:19.473 
Epoch 506/1000 
	 loss: 31.8295, MinusLogProbMetric: 31.8295, val_loss: 32.3084, val_MinusLogProbMetric: 32.3084

Epoch 506: val_loss did not improve from 32.11970
196/196 - 77s - loss: 31.8295 - MinusLogProbMetric: 31.8295 - val_loss: 32.3084 - val_MinusLogProbMetric: 32.3084 - lr: 1.2346e-05 - 77s/epoch - 394ms/step
Epoch 507/1000
2023-09-30 06:38:35.356 
Epoch 507/1000 
	 loss: 31.7634, MinusLogProbMetric: 31.7634, val_loss: 32.2027, val_MinusLogProbMetric: 32.2027

Epoch 507: val_loss did not improve from 32.11970
196/196 - 76s - loss: 31.7634 - MinusLogProbMetric: 31.7634 - val_loss: 32.2027 - val_MinusLogProbMetric: 32.2027 - lr: 1.2346e-05 - 76s/epoch - 387ms/step
Epoch 508/1000
2023-09-30 06:39:47.391 
Epoch 508/1000 
	 loss: 31.7873, MinusLogProbMetric: 31.7873, val_loss: 32.2044, val_MinusLogProbMetric: 32.2044

Epoch 508: val_loss did not improve from 32.11970
196/196 - 72s - loss: 31.7873 - MinusLogProbMetric: 31.7873 - val_loss: 32.2044 - val_MinusLogProbMetric: 32.2044 - lr: 1.2346e-05 - 72s/epoch - 368ms/step
Epoch 509/1000
2023-09-30 06:41:02.569 
Epoch 509/1000 
	 loss: 31.7710, MinusLogProbMetric: 31.7710, val_loss: 32.1573, val_MinusLogProbMetric: 32.1573

Epoch 509: val_loss did not improve from 32.11970
196/196 - 75s - loss: 31.7710 - MinusLogProbMetric: 31.7710 - val_loss: 32.1573 - val_MinusLogProbMetric: 32.1573 - lr: 1.2346e-05 - 75s/epoch - 384ms/step
Epoch 510/1000
2023-09-30 06:42:17.318 
Epoch 510/1000 
	 loss: 31.7754, MinusLogProbMetric: 31.7754, val_loss: 32.1386, val_MinusLogProbMetric: 32.1386

Epoch 510: val_loss did not improve from 32.11970
196/196 - 75s - loss: 31.7754 - MinusLogProbMetric: 31.7754 - val_loss: 32.1386 - val_MinusLogProbMetric: 32.1386 - lr: 1.2346e-05 - 75s/epoch - 381ms/step
Epoch 511/1000
2023-09-30 06:43:28.991 
Epoch 511/1000 
	 loss: 34.0045, MinusLogProbMetric: 34.0045, val_loss: 32.1619, val_MinusLogProbMetric: 32.1619

Epoch 511: val_loss did not improve from 32.11970
196/196 - 72s - loss: 34.0045 - MinusLogProbMetric: 34.0045 - val_loss: 32.1619 - val_MinusLogProbMetric: 32.1619 - lr: 1.2346e-05 - 72s/epoch - 366ms/step
Epoch 512/1000
2023-09-30 06:44:42.802 
Epoch 512/1000 
	 loss: 31.7915, MinusLogProbMetric: 31.7915, val_loss: 32.3523, val_MinusLogProbMetric: 32.3523

Epoch 512: val_loss did not improve from 32.11970
196/196 - 74s - loss: 31.7915 - MinusLogProbMetric: 31.7915 - val_loss: 32.3523 - val_MinusLogProbMetric: 32.3523 - lr: 1.2346e-05 - 74s/epoch - 377ms/step
Epoch 513/1000
2023-09-30 06:45:55.835 
Epoch 513/1000 
	 loss: 31.7439, MinusLogProbMetric: 31.7439, val_loss: 32.4408, val_MinusLogProbMetric: 32.4408

Epoch 513: val_loss did not improve from 32.11970
196/196 - 73s - loss: 31.7439 - MinusLogProbMetric: 31.7439 - val_loss: 32.4408 - val_MinusLogProbMetric: 32.4408 - lr: 1.2346e-05 - 73s/epoch - 373ms/step
Epoch 514/1000
2023-09-30 06:47:13.306 
Epoch 514/1000 
	 loss: 31.7537, MinusLogProbMetric: 31.7537, val_loss: 32.1326, val_MinusLogProbMetric: 32.1326

Epoch 514: val_loss did not improve from 32.11970
196/196 - 77s - loss: 31.7537 - MinusLogProbMetric: 31.7537 - val_loss: 32.1326 - val_MinusLogProbMetric: 32.1326 - lr: 1.2346e-05 - 77s/epoch - 395ms/step
Epoch 515/1000
2023-09-30 06:48:28.226 
Epoch 515/1000 
	 loss: 31.7158, MinusLogProbMetric: 31.7158, val_loss: 32.1704, val_MinusLogProbMetric: 32.1704

Epoch 515: val_loss did not improve from 32.11970
196/196 - 75s - loss: 31.7158 - MinusLogProbMetric: 31.7158 - val_loss: 32.1704 - val_MinusLogProbMetric: 32.1704 - lr: 1.2346e-05 - 75s/epoch - 382ms/step
Epoch 516/1000
2023-09-30 06:49:43.400 
Epoch 516/1000 
	 loss: 31.7302, MinusLogProbMetric: 31.7302, val_loss: 32.1114, val_MinusLogProbMetric: 32.1114

Epoch 516: val_loss improved from 32.11970 to 32.11139, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 76s - loss: 31.7302 - MinusLogProbMetric: 31.7302 - val_loss: 32.1114 - val_MinusLogProbMetric: 32.1114 - lr: 1.2346e-05 - 76s/epoch - 389ms/step
Epoch 517/1000
2023-09-30 06:51:00.049 
Epoch 517/1000 
	 loss: 31.7257, MinusLogProbMetric: 31.7257, val_loss: 32.1796, val_MinusLogProbMetric: 32.1796

Epoch 517: val_loss did not improve from 32.11139
196/196 - 75s - loss: 31.7257 - MinusLogProbMetric: 31.7257 - val_loss: 32.1796 - val_MinusLogProbMetric: 32.1796 - lr: 1.2346e-05 - 75s/epoch - 385ms/step
Epoch 518/1000
2023-09-30 06:52:13.328 
Epoch 518/1000 
	 loss: 31.7048, MinusLogProbMetric: 31.7048, val_loss: 32.2759, val_MinusLogProbMetric: 32.2759

Epoch 518: val_loss did not improve from 32.11139
196/196 - 73s - loss: 31.7048 - MinusLogProbMetric: 31.7048 - val_loss: 32.2759 - val_MinusLogProbMetric: 32.2759 - lr: 1.2346e-05 - 73s/epoch - 374ms/step
Epoch 519/1000
2023-09-30 06:53:28.025 
Epoch 519/1000 
	 loss: 31.7211, MinusLogProbMetric: 31.7211, val_loss: 32.0332, val_MinusLogProbMetric: 32.0332

Epoch 519: val_loss improved from 32.11139 to 32.03322, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 77s - loss: 31.7211 - MinusLogProbMetric: 31.7211 - val_loss: 32.0332 - val_MinusLogProbMetric: 32.0332 - lr: 1.2346e-05 - 77s/epoch - 393ms/step
Epoch 520/1000
2023-09-30 06:54:42.761 
Epoch 520/1000 
	 loss: 31.7395, MinusLogProbMetric: 31.7395, val_loss: 32.1703, val_MinusLogProbMetric: 32.1703

Epoch 520: val_loss did not improve from 32.03322
196/196 - 72s - loss: 31.7395 - MinusLogProbMetric: 31.7395 - val_loss: 32.1703 - val_MinusLogProbMetric: 32.1703 - lr: 1.2346e-05 - 72s/epoch - 369ms/step
Epoch 521/1000
2023-09-30 06:55:58.510 
Epoch 521/1000 
	 loss: 31.7095, MinusLogProbMetric: 31.7095, val_loss: 32.1359, val_MinusLogProbMetric: 32.1359

Epoch 521: val_loss did not improve from 32.03322
196/196 - 76s - loss: 31.7095 - MinusLogProbMetric: 31.7095 - val_loss: 32.1359 - val_MinusLogProbMetric: 32.1359 - lr: 1.2346e-05 - 76s/epoch - 387ms/step
Epoch 522/1000
2023-09-30 06:57:12.969 
Epoch 522/1000 
	 loss: 31.6867, MinusLogProbMetric: 31.6867, val_loss: 32.2723, val_MinusLogProbMetric: 32.2723

Epoch 522: val_loss did not improve from 32.03322
196/196 - 74s - loss: 31.6867 - MinusLogProbMetric: 31.6867 - val_loss: 32.2723 - val_MinusLogProbMetric: 32.2723 - lr: 1.2346e-05 - 74s/epoch - 380ms/step
Epoch 523/1000
2023-09-30 06:58:26.220 
Epoch 523/1000 
	 loss: 31.7013, MinusLogProbMetric: 31.7013, val_loss: 32.1833, val_MinusLogProbMetric: 32.1833

Epoch 523: val_loss did not improve from 32.03322
196/196 - 73s - loss: 31.7013 - MinusLogProbMetric: 31.7013 - val_loss: 32.1833 - val_MinusLogProbMetric: 32.1833 - lr: 1.2346e-05 - 73s/epoch - 374ms/step
Epoch 524/1000
2023-09-30 06:59:40.913 
Epoch 524/1000 
	 loss: 31.6752, MinusLogProbMetric: 31.6752, val_loss: 32.2420, val_MinusLogProbMetric: 32.2420

Epoch 524: val_loss did not improve from 32.03322
196/196 - 75s - loss: 31.6752 - MinusLogProbMetric: 31.6752 - val_loss: 32.2420 - val_MinusLogProbMetric: 32.2420 - lr: 1.2346e-05 - 75s/epoch - 381ms/step
Epoch 525/1000
2023-09-30 07:00:52.649 
Epoch 525/1000 
	 loss: 31.6982, MinusLogProbMetric: 31.6982, val_loss: 32.2911, val_MinusLogProbMetric: 32.2911

Epoch 525: val_loss did not improve from 32.03322
196/196 - 72s - loss: 31.6982 - MinusLogProbMetric: 31.6982 - val_loss: 32.2911 - val_MinusLogProbMetric: 32.2911 - lr: 1.2346e-05 - 72s/epoch - 366ms/step
Epoch 526/1000
2023-09-30 07:02:08.523 
Epoch 526/1000 
	 loss: 31.7042, MinusLogProbMetric: 31.7042, val_loss: 32.0110, val_MinusLogProbMetric: 32.0110

Epoch 526: val_loss improved from 32.03322 to 32.01100, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 78s - loss: 31.7042 - MinusLogProbMetric: 31.7042 - val_loss: 32.0110 - val_MinusLogProbMetric: 32.0110 - lr: 1.2346e-05 - 78s/epoch - 400ms/step
Epoch 527/1000
2023-09-30 07:03:24.805 
Epoch 527/1000 
	 loss: 31.6568, MinusLogProbMetric: 31.6568, val_loss: 32.2104, val_MinusLogProbMetric: 32.2104

Epoch 527: val_loss did not improve from 32.01100
196/196 - 74s - loss: 31.6568 - MinusLogProbMetric: 31.6568 - val_loss: 32.2104 - val_MinusLogProbMetric: 32.2104 - lr: 1.2346e-05 - 74s/epoch - 376ms/step
Epoch 528/1000
2023-09-30 07:04:38.950 
Epoch 528/1000 
	 loss: 31.6430, MinusLogProbMetric: 31.6430, val_loss: 32.0418, val_MinusLogProbMetric: 32.0418

Epoch 528: val_loss did not improve from 32.01100
196/196 - 74s - loss: 31.6430 - MinusLogProbMetric: 31.6430 - val_loss: 32.0418 - val_MinusLogProbMetric: 32.0418 - lr: 1.2346e-05 - 74s/epoch - 378ms/step
Epoch 529/1000
2023-09-30 07:05:52.254 
Epoch 529/1000 
	 loss: 31.6291, MinusLogProbMetric: 31.6291, val_loss: 32.0079, val_MinusLogProbMetric: 32.0079

Epoch 529: val_loss improved from 32.01100 to 32.00787, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 75s - loss: 31.6291 - MinusLogProbMetric: 31.6291 - val_loss: 32.0079 - val_MinusLogProbMetric: 32.0079 - lr: 1.2346e-05 - 75s/epoch - 384ms/step
Epoch 530/1000
2023-09-30 07:07:07.126 
Epoch 530/1000 
	 loss: 31.6170, MinusLogProbMetric: 31.6170, val_loss: 32.1409, val_MinusLogProbMetric: 32.1409

Epoch 530: val_loss did not improve from 32.00787
196/196 - 73s - loss: 31.6170 - MinusLogProbMetric: 31.6170 - val_loss: 32.1409 - val_MinusLogProbMetric: 32.1409 - lr: 1.2346e-05 - 73s/epoch - 371ms/step
Epoch 531/1000
2023-09-30 07:08:20.221 
Epoch 531/1000 
	 loss: 31.6519, MinusLogProbMetric: 31.6519, val_loss: 32.3203, val_MinusLogProbMetric: 32.3203

Epoch 531: val_loss did not improve from 32.00787
196/196 - 73s - loss: 31.6519 - MinusLogProbMetric: 31.6519 - val_loss: 32.3203 - val_MinusLogProbMetric: 32.3203 - lr: 1.2346e-05 - 73s/epoch - 373ms/step
Epoch 532/1000
2023-09-30 07:09:34.397 
Epoch 532/1000 
	 loss: 31.6352, MinusLogProbMetric: 31.6352, val_loss: 31.9911, val_MinusLogProbMetric: 31.9911

Epoch 532: val_loss improved from 32.00787 to 31.99115, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 77s - loss: 31.6352 - MinusLogProbMetric: 31.6352 - val_loss: 31.9911 - val_MinusLogProbMetric: 31.9911 - lr: 1.2346e-05 - 77s/epoch - 394ms/step
Epoch 533/1000
2023-09-30 07:10:51.858 
Epoch 533/1000 
	 loss: 31.6288, MinusLogProbMetric: 31.6288, val_loss: 32.4802, val_MinusLogProbMetric: 32.4802

Epoch 533: val_loss did not improve from 31.99115
196/196 - 74s - loss: 31.6288 - MinusLogProbMetric: 31.6288 - val_loss: 32.4802 - val_MinusLogProbMetric: 32.4802 - lr: 1.2346e-05 - 74s/epoch - 379ms/step
Epoch 534/1000
2023-09-30 07:12:06.842 
Epoch 534/1000 
	 loss: 31.6520, MinusLogProbMetric: 31.6520, val_loss: 32.0764, val_MinusLogProbMetric: 32.0764

Epoch 534: val_loss did not improve from 31.99115
196/196 - 75s - loss: 31.6520 - MinusLogProbMetric: 31.6520 - val_loss: 32.0764 - val_MinusLogProbMetric: 32.0764 - lr: 1.2346e-05 - 75s/epoch - 382ms/step
Epoch 535/1000
2023-09-30 07:13:21.351 
Epoch 535/1000 
	 loss: 31.6479, MinusLogProbMetric: 31.6479, val_loss: 32.0521, val_MinusLogProbMetric: 32.0521

Epoch 535: val_loss did not improve from 31.99115
196/196 - 75s - loss: 31.6479 - MinusLogProbMetric: 31.6479 - val_loss: 32.0521 - val_MinusLogProbMetric: 32.0521 - lr: 1.2346e-05 - 75s/epoch - 380ms/step
Epoch 536/1000
2023-09-30 07:14:34.967 
Epoch 536/1000 
	 loss: 31.6582, MinusLogProbMetric: 31.6582, val_loss: 32.4130, val_MinusLogProbMetric: 32.4130

Epoch 536: val_loss did not improve from 31.99115
196/196 - 74s - loss: 31.6582 - MinusLogProbMetric: 31.6582 - val_loss: 32.4130 - val_MinusLogProbMetric: 32.4130 - lr: 1.2346e-05 - 74s/epoch - 376ms/step
Epoch 537/1000
2023-09-30 07:15:49.346 
Epoch 537/1000 
	 loss: 31.6266, MinusLogProbMetric: 31.6266, val_loss: 31.9384, val_MinusLogProbMetric: 31.9384

Epoch 537: val_loss improved from 31.99115 to 31.93844, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 76s - loss: 31.6266 - MinusLogProbMetric: 31.6266 - val_loss: 31.9384 - val_MinusLogProbMetric: 31.9384 - lr: 1.2346e-05 - 76s/epoch - 389ms/step
Epoch 538/1000
2023-09-30 07:17:04.093 
Epoch 538/1000 
	 loss: 31.6174, MinusLogProbMetric: 31.6174, val_loss: 32.3438, val_MinusLogProbMetric: 32.3438

Epoch 538: val_loss did not improve from 31.93844
196/196 - 73s - loss: 31.6174 - MinusLogProbMetric: 31.6174 - val_loss: 32.3438 - val_MinusLogProbMetric: 32.3438 - lr: 1.2346e-05 - 73s/epoch - 372ms/step
Epoch 539/1000
2023-09-30 07:18:17.800 
Epoch 539/1000 
	 loss: 31.6002, MinusLogProbMetric: 31.6002, val_loss: 32.0293, val_MinusLogProbMetric: 32.0293

Epoch 539: val_loss did not improve from 31.93844
196/196 - 74s - loss: 31.6002 - MinusLogProbMetric: 31.6002 - val_loss: 32.0293 - val_MinusLogProbMetric: 32.0293 - lr: 1.2346e-05 - 74s/epoch - 376ms/step
Epoch 540/1000
2023-09-30 07:19:33.007 
Epoch 540/1000 
	 loss: 31.6050, MinusLogProbMetric: 31.6050, val_loss: 32.3089, val_MinusLogProbMetric: 32.3089

Epoch 540: val_loss did not improve from 31.93844
196/196 - 75s - loss: 31.6050 - MinusLogProbMetric: 31.6050 - val_loss: 32.3089 - val_MinusLogProbMetric: 32.3089 - lr: 1.2346e-05 - 75s/epoch - 384ms/step
Epoch 541/1000
2023-09-30 07:20:45.848 
Epoch 541/1000 
	 loss: 31.6134, MinusLogProbMetric: 31.6134, val_loss: 31.9121, val_MinusLogProbMetric: 31.9121

Epoch 541: val_loss improved from 31.93844 to 31.91214, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 74s - loss: 31.6134 - MinusLogProbMetric: 31.6134 - val_loss: 31.9121 - val_MinusLogProbMetric: 31.9121 - lr: 1.2346e-05 - 74s/epoch - 379ms/step
Epoch 542/1000
2023-09-30 07:22:00.598 
Epoch 542/1000 
	 loss: 31.5722, MinusLogProbMetric: 31.5722, val_loss: 31.9842, val_MinusLogProbMetric: 31.9842

Epoch 542: val_loss did not improve from 31.91214
196/196 - 73s - loss: 31.5722 - MinusLogProbMetric: 31.5722 - val_loss: 31.9842 - val_MinusLogProbMetric: 31.9842 - lr: 1.2346e-05 - 73s/epoch - 374ms/step
Epoch 543/1000
2023-09-30 07:23:13.726 
Epoch 543/1000 
	 loss: 31.5734, MinusLogProbMetric: 31.5734, val_loss: 32.1126, val_MinusLogProbMetric: 32.1126

Epoch 543: val_loss did not improve from 31.91214
196/196 - 73s - loss: 31.5734 - MinusLogProbMetric: 31.5734 - val_loss: 32.1126 - val_MinusLogProbMetric: 32.1126 - lr: 1.2346e-05 - 73s/epoch - 373ms/step
Epoch 544/1000
2023-09-30 07:24:27.645 
Epoch 544/1000 
	 loss: 31.5692, MinusLogProbMetric: 31.5692, val_loss: 32.4326, val_MinusLogProbMetric: 32.4326

Epoch 544: val_loss did not improve from 31.91214
196/196 - 74s - loss: 31.5692 - MinusLogProbMetric: 31.5692 - val_loss: 32.4326 - val_MinusLogProbMetric: 32.4326 - lr: 1.2346e-05 - 74s/epoch - 377ms/step
Epoch 545/1000
2023-09-30 07:25:46.733 
Epoch 545/1000 
	 loss: 31.5583, MinusLogProbMetric: 31.5583, val_loss: 31.9743, val_MinusLogProbMetric: 31.9743

Epoch 545: val_loss did not improve from 31.91214
196/196 - 79s - loss: 31.5583 - MinusLogProbMetric: 31.5583 - val_loss: 31.9743 - val_MinusLogProbMetric: 31.9743 - lr: 1.2346e-05 - 79s/epoch - 403ms/step
Epoch 546/1000
2023-09-30 07:27:02.847 
Epoch 546/1000 
	 loss: 31.5763, MinusLogProbMetric: 31.5763, val_loss: 32.0500, val_MinusLogProbMetric: 32.0500

Epoch 546: val_loss did not improve from 31.91214
196/196 - 76s - loss: 31.5763 - MinusLogProbMetric: 31.5763 - val_loss: 32.0500 - val_MinusLogProbMetric: 32.0500 - lr: 1.2346e-05 - 76s/epoch - 388ms/step
Epoch 547/1000
2023-09-30 07:28:18.017 
Epoch 547/1000 
	 loss: 31.5497, MinusLogProbMetric: 31.5497, val_loss: 32.1181, val_MinusLogProbMetric: 32.1181

Epoch 547: val_loss did not improve from 31.91214
196/196 - 75s - loss: 31.5497 - MinusLogProbMetric: 31.5497 - val_loss: 32.1181 - val_MinusLogProbMetric: 32.1181 - lr: 1.2346e-05 - 75s/epoch - 384ms/step
Epoch 548/1000
2023-09-30 07:29:34.744 
Epoch 548/1000 
	 loss: 31.5565, MinusLogProbMetric: 31.5565, val_loss: 32.1354, val_MinusLogProbMetric: 32.1354

Epoch 548: val_loss did not improve from 31.91214
196/196 - 77s - loss: 31.5565 - MinusLogProbMetric: 31.5565 - val_loss: 32.1354 - val_MinusLogProbMetric: 32.1354 - lr: 1.2346e-05 - 77s/epoch - 391ms/step
Epoch 549/1000
2023-09-30 07:30:50.401 
Epoch 549/1000 
	 loss: 31.5458, MinusLogProbMetric: 31.5458, val_loss: 31.9537, val_MinusLogProbMetric: 31.9537

Epoch 549: val_loss did not improve from 31.91214
196/196 - 76s - loss: 31.5458 - MinusLogProbMetric: 31.5458 - val_loss: 31.9537 - val_MinusLogProbMetric: 31.9537 - lr: 1.2346e-05 - 76s/epoch - 386ms/step
Epoch 550/1000
2023-09-30 07:32:07.205 
Epoch 550/1000 
	 loss: 31.5545, MinusLogProbMetric: 31.5545, val_loss: 31.8902, val_MinusLogProbMetric: 31.8902

Epoch 550: val_loss improved from 31.91214 to 31.89024, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 78s - loss: 31.5545 - MinusLogProbMetric: 31.5545 - val_loss: 31.8902 - val_MinusLogProbMetric: 31.8902 - lr: 1.2346e-05 - 78s/epoch - 400ms/step
Epoch 551/1000
2023-09-30 07:33:24.252 
Epoch 551/1000 
	 loss: 31.5390, MinusLogProbMetric: 31.5390, val_loss: 31.9851, val_MinusLogProbMetric: 31.9851

Epoch 551: val_loss did not improve from 31.89024
196/196 - 76s - loss: 31.5390 - MinusLogProbMetric: 31.5390 - val_loss: 31.9851 - val_MinusLogProbMetric: 31.9851 - lr: 1.2346e-05 - 76s/epoch - 385ms/step
Epoch 552/1000
2023-09-30 07:34:40.763 
Epoch 552/1000 
	 loss: 31.5234, MinusLogProbMetric: 31.5234, val_loss: 31.9246, val_MinusLogProbMetric: 31.9246

Epoch 552: val_loss did not improve from 31.89024
196/196 - 76s - loss: 31.5234 - MinusLogProbMetric: 31.5234 - val_loss: 31.9246 - val_MinusLogProbMetric: 31.9246 - lr: 1.2346e-05 - 76s/epoch - 390ms/step
Epoch 553/1000
2023-09-30 07:35:57.184 
Epoch 553/1000 
	 loss: 31.4903, MinusLogProbMetric: 31.4903, val_loss: 32.1080, val_MinusLogProbMetric: 32.1080

Epoch 553: val_loss did not improve from 31.89024
196/196 - 76s - loss: 31.4903 - MinusLogProbMetric: 31.4903 - val_loss: 32.1080 - val_MinusLogProbMetric: 32.1080 - lr: 1.2346e-05 - 76s/epoch - 390ms/step
Epoch 554/1000
2023-09-30 07:37:13.910 
Epoch 554/1000 
	 loss: 31.5229, MinusLogProbMetric: 31.5229, val_loss: 31.9665, val_MinusLogProbMetric: 31.9665

Epoch 554: val_loss did not improve from 31.89024
196/196 - 77s - loss: 31.5229 - MinusLogProbMetric: 31.5229 - val_loss: 31.9665 - val_MinusLogProbMetric: 31.9665 - lr: 1.2346e-05 - 77s/epoch - 391ms/step
Epoch 555/1000
2023-09-30 07:38:30.427 
Epoch 555/1000 
	 loss: 31.5064, MinusLogProbMetric: 31.5064, val_loss: 31.8638, val_MinusLogProbMetric: 31.8638

Epoch 555: val_loss improved from 31.89024 to 31.86376, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 78s - loss: 31.5064 - MinusLogProbMetric: 31.5064 - val_loss: 31.8638 - val_MinusLogProbMetric: 31.8638 - lr: 1.2346e-05 - 78s/epoch - 400ms/step
Epoch 556/1000
2023-09-30 07:39:47.132 
Epoch 556/1000 
	 loss: 31.4968, MinusLogProbMetric: 31.4968, val_loss: 31.9932, val_MinusLogProbMetric: 31.9932

Epoch 556: val_loss did not improve from 31.86376
196/196 - 75s - loss: 31.4968 - MinusLogProbMetric: 31.4968 - val_loss: 31.9932 - val_MinusLogProbMetric: 31.9932 - lr: 1.2346e-05 - 75s/epoch - 381ms/step
Epoch 557/1000
2023-09-30 07:41:04.005 
Epoch 557/1000 
	 loss: 31.4987, MinusLogProbMetric: 31.4987, val_loss: 31.9799, val_MinusLogProbMetric: 31.9799

Epoch 557: val_loss did not improve from 31.86376
196/196 - 77s - loss: 31.4987 - MinusLogProbMetric: 31.4987 - val_loss: 31.9799 - val_MinusLogProbMetric: 31.9799 - lr: 1.2346e-05 - 77s/epoch - 392ms/step
Epoch 558/1000
2023-09-30 07:42:18.031 
Epoch 558/1000 
	 loss: 31.4965, MinusLogProbMetric: 31.4965, val_loss: 32.1643, val_MinusLogProbMetric: 32.1643

Epoch 558: val_loss did not improve from 31.86376
196/196 - 74s - loss: 31.4965 - MinusLogProbMetric: 31.4965 - val_loss: 32.1643 - val_MinusLogProbMetric: 32.1643 - lr: 1.2346e-05 - 74s/epoch - 378ms/step
Epoch 559/1000
2023-09-30 07:43:32.572 
Epoch 559/1000 
	 loss: 31.4869, MinusLogProbMetric: 31.4869, val_loss: 31.9021, val_MinusLogProbMetric: 31.9021

Epoch 559: val_loss did not improve from 31.86376
196/196 - 75s - loss: 31.4869 - MinusLogProbMetric: 31.4869 - val_loss: 31.9021 - val_MinusLogProbMetric: 31.9021 - lr: 1.2346e-05 - 75s/epoch - 380ms/step
Epoch 560/1000
2023-09-30 07:44:49.188 
Epoch 560/1000 
	 loss: 31.4842, MinusLogProbMetric: 31.4842, val_loss: 31.8400, val_MinusLogProbMetric: 31.8400

Epoch 560: val_loss improved from 31.86376 to 31.83995, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 78s - loss: 31.4842 - MinusLogProbMetric: 31.4842 - val_loss: 31.8400 - val_MinusLogProbMetric: 31.8400 - lr: 1.2346e-05 - 78s/epoch - 396ms/step
Epoch 561/1000
2023-09-30 07:46:05.697 
Epoch 561/1000 
	 loss: 31.4505, MinusLogProbMetric: 31.4505, val_loss: 32.0980, val_MinusLogProbMetric: 32.0980

Epoch 561: val_loss did not improve from 31.83995
196/196 - 75s - loss: 31.4505 - MinusLogProbMetric: 31.4505 - val_loss: 32.0980 - val_MinusLogProbMetric: 32.0980 - lr: 1.2346e-05 - 75s/epoch - 385ms/step
Epoch 562/1000
2023-09-30 07:47:22.173 
Epoch 562/1000 
	 loss: 31.4702, MinusLogProbMetric: 31.4702, val_loss: 31.9141, val_MinusLogProbMetric: 31.9141

Epoch 562: val_loss did not improve from 31.83995
196/196 - 76s - loss: 31.4702 - MinusLogProbMetric: 31.4702 - val_loss: 31.9141 - val_MinusLogProbMetric: 31.9141 - lr: 1.2346e-05 - 76s/epoch - 390ms/step
Epoch 563/1000
2023-09-30 07:48:38.422 
Epoch 563/1000 
	 loss: 31.4503, MinusLogProbMetric: 31.4503, val_loss: 32.3000, val_MinusLogProbMetric: 32.3000

Epoch 563: val_loss did not improve from 31.83995
196/196 - 76s - loss: 31.4503 - MinusLogProbMetric: 31.4503 - val_loss: 32.3000 - val_MinusLogProbMetric: 32.3000 - lr: 1.2346e-05 - 76s/epoch - 389ms/step
Epoch 564/1000
2023-09-30 07:49:53.538 
Epoch 564/1000 
	 loss: 31.4669, MinusLogProbMetric: 31.4669, val_loss: 31.8694, val_MinusLogProbMetric: 31.8694

Epoch 564: val_loss did not improve from 31.83995
196/196 - 75s - loss: 31.4669 - MinusLogProbMetric: 31.4669 - val_loss: 31.8694 - val_MinusLogProbMetric: 31.8694 - lr: 1.2346e-05 - 75s/epoch - 383ms/step
Epoch 565/1000
2023-09-30 07:51:11.293 
Epoch 565/1000 
	 loss: 31.8253, MinusLogProbMetric: 31.8253, val_loss: 32.2981, val_MinusLogProbMetric: 32.2981

Epoch 565: val_loss did not improve from 31.83995
196/196 - 78s - loss: 31.8253 - MinusLogProbMetric: 31.8253 - val_loss: 32.2981 - val_MinusLogProbMetric: 32.2981 - lr: 1.2346e-05 - 78s/epoch - 397ms/step
Epoch 566/1000
2023-09-30 07:52:23.267 
Epoch 566/1000 
	 loss: 31.4736, MinusLogProbMetric: 31.4736, val_loss: 31.9991, val_MinusLogProbMetric: 31.9991

Epoch 566: val_loss did not improve from 31.83995
196/196 - 72s - loss: 31.4736 - MinusLogProbMetric: 31.4736 - val_loss: 31.9991 - val_MinusLogProbMetric: 31.9991 - lr: 1.2346e-05 - 72s/epoch - 367ms/step
Epoch 567/1000
2023-09-30 07:53:34.880 
Epoch 567/1000 
	 loss: 31.4340, MinusLogProbMetric: 31.4340, val_loss: 31.8837, val_MinusLogProbMetric: 31.8837

Epoch 567: val_loss did not improve from 31.83995
196/196 - 72s - loss: 31.4340 - MinusLogProbMetric: 31.4340 - val_loss: 31.8837 - val_MinusLogProbMetric: 31.8837 - lr: 1.2346e-05 - 72s/epoch - 365ms/step
Epoch 568/1000
2023-09-30 07:54:46.582 
Epoch 568/1000 
	 loss: 31.4445, MinusLogProbMetric: 31.4445, val_loss: 31.9369, val_MinusLogProbMetric: 31.9369

Epoch 568: val_loss did not improve from 31.83995
196/196 - 72s - loss: 31.4445 - MinusLogProbMetric: 31.4445 - val_loss: 31.9369 - val_MinusLogProbMetric: 31.9369 - lr: 1.2346e-05 - 72s/epoch - 366ms/step
Epoch 569/1000
2023-09-30 07:55:58.273 
Epoch 569/1000 
	 loss: 31.4121, MinusLogProbMetric: 31.4121, val_loss: 31.8999, val_MinusLogProbMetric: 31.8999

Epoch 569: val_loss did not improve from 31.83995
196/196 - 72s - loss: 31.4121 - MinusLogProbMetric: 31.4121 - val_loss: 31.8999 - val_MinusLogProbMetric: 31.8999 - lr: 1.2346e-05 - 72s/epoch - 366ms/step
Epoch 570/1000
2023-09-30 07:57:11.182 
Epoch 570/1000 
	 loss: 31.4412, MinusLogProbMetric: 31.4412, val_loss: 32.0588, val_MinusLogProbMetric: 32.0588

Epoch 570: val_loss did not improve from 31.83995
196/196 - 73s - loss: 31.4412 - MinusLogProbMetric: 31.4412 - val_loss: 32.0588 - val_MinusLogProbMetric: 32.0588 - lr: 1.2346e-05 - 73s/epoch - 372ms/step
Epoch 571/1000
2023-09-30 07:58:22.339 
Epoch 571/1000 
	 loss: 31.4231, MinusLogProbMetric: 31.4231, val_loss: 31.9065, val_MinusLogProbMetric: 31.9065

Epoch 571: val_loss did not improve from 31.83995
196/196 - 71s - loss: 31.4231 - MinusLogProbMetric: 31.4231 - val_loss: 31.9065 - val_MinusLogProbMetric: 31.9065 - lr: 1.2346e-05 - 71s/epoch - 363ms/step
Epoch 572/1000
2023-09-30 07:59:33.496 
Epoch 572/1000 
	 loss: 31.3888, MinusLogProbMetric: 31.3888, val_loss: 31.8207, val_MinusLogProbMetric: 31.8207

Epoch 572: val_loss improved from 31.83995 to 31.82075, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 73s - loss: 31.3888 - MinusLogProbMetric: 31.3888 - val_loss: 31.8207 - val_MinusLogProbMetric: 31.8207 - lr: 1.2346e-05 - 73s/epoch - 372ms/step
Epoch 573/1000
2023-09-30 08:00:46.250 
Epoch 573/1000 
	 loss: 31.4154, MinusLogProbMetric: 31.4154, val_loss: 31.7258, val_MinusLogProbMetric: 31.7258

Epoch 573: val_loss improved from 31.82075 to 31.72577, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 73s - loss: 31.4154 - MinusLogProbMetric: 31.4154 - val_loss: 31.7258 - val_MinusLogProbMetric: 31.7258 - lr: 1.2346e-05 - 73s/epoch - 371ms/step
Epoch 574/1000
2023-09-30 08:01:59.123 
Epoch 574/1000 
	 loss: 31.4172, MinusLogProbMetric: 31.4172, val_loss: 31.8059, val_MinusLogProbMetric: 31.8059

Epoch 574: val_loss did not improve from 31.72577
196/196 - 71s - loss: 31.4172 - MinusLogProbMetric: 31.4172 - val_loss: 31.8059 - val_MinusLogProbMetric: 31.8059 - lr: 1.2346e-05 - 71s/epoch - 363ms/step
Epoch 575/1000
2023-09-30 08:03:10.814 
Epoch 575/1000 
	 loss: 31.3624, MinusLogProbMetric: 31.3624, val_loss: 31.9150, val_MinusLogProbMetric: 31.9150

Epoch 575: val_loss did not improve from 31.72577
196/196 - 72s - loss: 31.3624 - MinusLogProbMetric: 31.3624 - val_loss: 31.9150 - val_MinusLogProbMetric: 31.9150 - lr: 1.2346e-05 - 72s/epoch - 366ms/step
Epoch 576/1000
2023-09-30 08:04:22.461 
Epoch 576/1000 
	 loss: 31.4219, MinusLogProbMetric: 31.4219, val_loss: 31.8811, val_MinusLogProbMetric: 31.8811

Epoch 576: val_loss did not improve from 31.72577
196/196 - 72s - loss: 31.4219 - MinusLogProbMetric: 31.4219 - val_loss: 31.8811 - val_MinusLogProbMetric: 31.8811 - lr: 1.2346e-05 - 72s/epoch - 366ms/step
Epoch 577/1000
2023-09-30 08:05:33.114 
Epoch 577/1000 
	 loss: 31.3650, MinusLogProbMetric: 31.3650, val_loss: 32.0168, val_MinusLogProbMetric: 32.0168

Epoch 577: val_loss did not improve from 31.72577
196/196 - 71s - loss: 31.3650 - MinusLogProbMetric: 31.3650 - val_loss: 32.0168 - val_MinusLogProbMetric: 32.0168 - lr: 1.2346e-05 - 71s/epoch - 360ms/step
Epoch 578/1000
2023-09-30 08:06:44.818 
Epoch 578/1000 
	 loss: 31.4371, MinusLogProbMetric: 31.4371, val_loss: 31.9744, val_MinusLogProbMetric: 31.9744

Epoch 578: val_loss did not improve from 31.72577
196/196 - 72s - loss: 31.4371 - MinusLogProbMetric: 31.4371 - val_loss: 31.9744 - val_MinusLogProbMetric: 31.9744 - lr: 1.2346e-05 - 72s/epoch - 366ms/step
Epoch 579/1000
2023-09-30 08:07:56.078 
Epoch 579/1000 
	 loss: 31.3928, MinusLogProbMetric: 31.3928, val_loss: 31.7930, val_MinusLogProbMetric: 31.7930

Epoch 579: val_loss did not improve from 31.72577
196/196 - 71s - loss: 31.3928 - MinusLogProbMetric: 31.3928 - val_loss: 31.7930 - val_MinusLogProbMetric: 31.7930 - lr: 1.2346e-05 - 71s/epoch - 364ms/step
Epoch 580/1000
2023-09-30 08:09:06.795 
Epoch 580/1000 
	 loss: 31.3871, MinusLogProbMetric: 31.3871, val_loss: 32.0310, val_MinusLogProbMetric: 32.0310

Epoch 580: val_loss did not improve from 31.72577
196/196 - 71s - loss: 31.3871 - MinusLogProbMetric: 31.3871 - val_loss: 32.0310 - val_MinusLogProbMetric: 32.0310 - lr: 1.2346e-05 - 71s/epoch - 361ms/step
Epoch 581/1000
2023-09-30 08:10:17.057 
Epoch 581/1000 
	 loss: 31.3761, MinusLogProbMetric: 31.3761, val_loss: 31.8266, val_MinusLogProbMetric: 31.8266

Epoch 581: val_loss did not improve from 31.72577
196/196 - 70s - loss: 31.3761 - MinusLogProbMetric: 31.3761 - val_loss: 31.8266 - val_MinusLogProbMetric: 31.8266 - lr: 1.2346e-05 - 70s/epoch - 358ms/step
Epoch 582/1000
2023-09-30 08:11:28.886 
Epoch 582/1000 
	 loss: 31.3346, MinusLogProbMetric: 31.3346, val_loss: 31.7535, val_MinusLogProbMetric: 31.7535

Epoch 582: val_loss did not improve from 31.72577
196/196 - 72s - loss: 31.3346 - MinusLogProbMetric: 31.3346 - val_loss: 31.7535 - val_MinusLogProbMetric: 31.7535 - lr: 1.2346e-05 - 72s/epoch - 367ms/step
Epoch 583/1000
2023-09-30 08:12:40.072 
Epoch 583/1000 
	 loss: 31.3384, MinusLogProbMetric: 31.3384, val_loss: 31.8864, val_MinusLogProbMetric: 31.8864

Epoch 583: val_loss did not improve from 31.72577
196/196 - 71s - loss: 31.3384 - MinusLogProbMetric: 31.3384 - val_loss: 31.8864 - val_MinusLogProbMetric: 31.8864 - lr: 1.2346e-05 - 71s/epoch - 363ms/step
Epoch 584/1000
2023-09-30 08:13:50.525 
Epoch 584/1000 
	 loss: 31.3387, MinusLogProbMetric: 31.3387, val_loss: 31.8918, val_MinusLogProbMetric: 31.8918

Epoch 584: val_loss did not improve from 31.72577
196/196 - 70s - loss: 31.3387 - MinusLogProbMetric: 31.3387 - val_loss: 31.8918 - val_MinusLogProbMetric: 31.8918 - lr: 1.2346e-05 - 70s/epoch - 359ms/step
Epoch 585/1000
2023-09-30 08:15:00.759 
Epoch 585/1000 
	 loss: 31.3311, MinusLogProbMetric: 31.3311, val_loss: 31.8102, val_MinusLogProbMetric: 31.8102

Epoch 585: val_loss did not improve from 31.72577
196/196 - 70s - loss: 31.3311 - MinusLogProbMetric: 31.3311 - val_loss: 31.8102 - val_MinusLogProbMetric: 31.8102 - lr: 1.2346e-05 - 70s/epoch - 358ms/step
Epoch 586/1000
2023-09-30 08:16:12.645 
Epoch 586/1000 
	 loss: 31.3396, MinusLogProbMetric: 31.3396, val_loss: 31.7245, val_MinusLogProbMetric: 31.7245

Epoch 586: val_loss improved from 31.72577 to 31.72448, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 73s - loss: 31.3396 - MinusLogProbMetric: 31.3396 - val_loss: 31.7245 - val_MinusLogProbMetric: 31.7245 - lr: 1.2346e-05 - 73s/epoch - 372ms/step
Epoch 587/1000
2023-09-30 08:17:33.429 
Epoch 587/1000 
	 loss: 31.3288, MinusLogProbMetric: 31.3288, val_loss: 31.6793, val_MinusLogProbMetric: 31.6793

Epoch 587: val_loss improved from 31.72448 to 31.67933, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 83s - loss: 31.3288 - MinusLogProbMetric: 31.3288 - val_loss: 31.6793 - val_MinusLogProbMetric: 31.6793 - lr: 1.2346e-05 - 83s/epoch - 421ms/step
Epoch 588/1000
2023-09-30 08:18:49.404 
Epoch 588/1000 
	 loss: 31.3432, MinusLogProbMetric: 31.3432, val_loss: 31.8785, val_MinusLogProbMetric: 31.8785

Epoch 588: val_loss did not improve from 31.67933
196/196 - 73s - loss: 31.3432 - MinusLogProbMetric: 31.3432 - val_loss: 31.8785 - val_MinusLogProbMetric: 31.8785 - lr: 1.2346e-05 - 73s/epoch - 373ms/step
Epoch 589/1000
2023-09-30 08:20:04.250 
Epoch 589/1000 
	 loss: 31.3257, MinusLogProbMetric: 31.3257, val_loss: 32.1853, val_MinusLogProbMetric: 32.1853

Epoch 589: val_loss did not improve from 31.67933
196/196 - 75s - loss: 31.3257 - MinusLogProbMetric: 31.3257 - val_loss: 32.1853 - val_MinusLogProbMetric: 32.1853 - lr: 1.2346e-05 - 75s/epoch - 382ms/step
Epoch 590/1000
2023-09-30 08:21:19.173 
Epoch 590/1000 
	 loss: 31.3495, MinusLogProbMetric: 31.3495, val_loss: 32.0196, val_MinusLogProbMetric: 32.0196

Epoch 590: val_loss did not improve from 31.67933
196/196 - 75s - loss: 31.3495 - MinusLogProbMetric: 31.3495 - val_loss: 32.0196 - val_MinusLogProbMetric: 32.0196 - lr: 1.2346e-05 - 75s/epoch - 382ms/step
Epoch 591/1000
2023-09-30 08:22:35.933 
Epoch 591/1000 
	 loss: 31.3238, MinusLogProbMetric: 31.3238, val_loss: 31.8891, val_MinusLogProbMetric: 31.8891

Epoch 591: val_loss did not improve from 31.67933
196/196 - 77s - loss: 31.3238 - MinusLogProbMetric: 31.3238 - val_loss: 31.8891 - val_MinusLogProbMetric: 31.8891 - lr: 1.2346e-05 - 77s/epoch - 391ms/step
Epoch 592/1000
2023-09-30 08:23:50.414 
Epoch 592/1000 
	 loss: 31.3135, MinusLogProbMetric: 31.3135, val_loss: 31.8107, val_MinusLogProbMetric: 31.8107

Epoch 592: val_loss did not improve from 31.67933
196/196 - 74s - loss: 31.3135 - MinusLogProbMetric: 31.3135 - val_loss: 31.8107 - val_MinusLogProbMetric: 31.8107 - lr: 1.2346e-05 - 74s/epoch - 380ms/step
Epoch 593/1000
2023-09-30 08:25:05.254 
Epoch 593/1000 
	 loss: 31.3222, MinusLogProbMetric: 31.3222, val_loss: 31.8099, val_MinusLogProbMetric: 31.8099

Epoch 593: val_loss did not improve from 31.67933
196/196 - 75s - loss: 31.3222 - MinusLogProbMetric: 31.3222 - val_loss: 31.8099 - val_MinusLogProbMetric: 31.8099 - lr: 1.2346e-05 - 75s/epoch - 382ms/step
Epoch 594/1000
2023-09-30 08:26:18.724 
Epoch 594/1000 
	 loss: 31.3368, MinusLogProbMetric: 31.3368, val_loss: 31.8195, val_MinusLogProbMetric: 31.8195

Epoch 594: val_loss did not improve from 31.67933
196/196 - 73s - loss: 31.3368 - MinusLogProbMetric: 31.3368 - val_loss: 31.8195 - val_MinusLogProbMetric: 31.8195 - lr: 1.2346e-05 - 73s/epoch - 375ms/step
Epoch 595/1000
2023-09-30 08:27:34.318 
Epoch 595/1000 
	 loss: 31.2927, MinusLogProbMetric: 31.2927, val_loss: 31.7618, val_MinusLogProbMetric: 31.7618

Epoch 595: val_loss did not improve from 31.67933
196/196 - 76s - loss: 31.2927 - MinusLogProbMetric: 31.2927 - val_loss: 31.7618 - val_MinusLogProbMetric: 31.7618 - lr: 1.2346e-05 - 76s/epoch - 386ms/step
Epoch 596/1000
2023-09-30 08:28:48.460 
Epoch 596/1000 
	 loss: 31.2835, MinusLogProbMetric: 31.2835, val_loss: 31.8393, val_MinusLogProbMetric: 31.8393

Epoch 596: val_loss did not improve from 31.67933
196/196 - 74s - loss: 31.2835 - MinusLogProbMetric: 31.2835 - val_loss: 31.8393 - val_MinusLogProbMetric: 31.8393 - lr: 1.2346e-05 - 74s/epoch - 378ms/step
Epoch 597/1000
2023-09-30 08:30:02.455 
Epoch 597/1000 
	 loss: 31.2744, MinusLogProbMetric: 31.2744, val_loss: 31.7982, val_MinusLogProbMetric: 31.7982

Epoch 597: val_loss did not improve from 31.67933
196/196 - 74s - loss: 31.2744 - MinusLogProbMetric: 31.2744 - val_loss: 31.7982 - val_MinusLogProbMetric: 31.7982 - lr: 1.2346e-05 - 74s/epoch - 377ms/step
Epoch 598/1000
2023-09-30 08:31:17.880 
Epoch 598/1000 
	 loss: 31.2857, MinusLogProbMetric: 31.2857, val_loss: 31.7742, val_MinusLogProbMetric: 31.7742

Epoch 598: val_loss did not improve from 31.67933
196/196 - 75s - loss: 31.2857 - MinusLogProbMetric: 31.2857 - val_loss: 31.7742 - val_MinusLogProbMetric: 31.7742 - lr: 1.2346e-05 - 75s/epoch - 385ms/step
Epoch 599/1000
2023-09-30 08:32:31.743 
Epoch 599/1000 
	 loss: 31.2849, MinusLogProbMetric: 31.2849, val_loss: 31.9638, val_MinusLogProbMetric: 31.9638

Epoch 599: val_loss did not improve from 31.67933
196/196 - 74s - loss: 31.2849 - MinusLogProbMetric: 31.2849 - val_loss: 31.9638 - val_MinusLogProbMetric: 31.9638 - lr: 1.2346e-05 - 74s/epoch - 377ms/step
Epoch 600/1000
2023-09-30 08:33:45.409 
Epoch 600/1000 
	 loss: 31.2807, MinusLogProbMetric: 31.2807, val_loss: 31.5659, val_MinusLogProbMetric: 31.5659

Epoch 600: val_loss improved from 31.67933 to 31.56588, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 76s - loss: 31.2807 - MinusLogProbMetric: 31.2807 - val_loss: 31.5659 - val_MinusLogProbMetric: 31.5659 - lr: 1.2346e-05 - 76s/epoch - 386ms/step
Epoch 601/1000
2023-09-30 08:35:02.607 
Epoch 601/1000 
	 loss: 31.2795, MinusLogProbMetric: 31.2795, val_loss: 31.8456, val_MinusLogProbMetric: 31.8456

Epoch 601: val_loss did not improve from 31.56588
196/196 - 75s - loss: 31.2795 - MinusLogProbMetric: 31.2795 - val_loss: 31.8456 - val_MinusLogProbMetric: 31.8456 - lr: 1.2346e-05 - 75s/epoch - 384ms/step
Epoch 602/1000
2023-09-30 08:36:17.356 
Epoch 602/1000 
	 loss: 31.2557, MinusLogProbMetric: 31.2557, val_loss: 31.7895, val_MinusLogProbMetric: 31.7895

Epoch 602: val_loss did not improve from 31.56588
196/196 - 75s - loss: 31.2557 - MinusLogProbMetric: 31.2557 - val_loss: 31.7895 - val_MinusLogProbMetric: 31.7895 - lr: 1.2346e-05 - 75s/epoch - 381ms/step
Epoch 603/1000
2023-09-30 08:37:36.015 
Epoch 603/1000 
	 loss: 31.2513, MinusLogProbMetric: 31.2513, val_loss: 31.8012, val_MinusLogProbMetric: 31.8012

Epoch 603: val_loss did not improve from 31.56588
196/196 - 79s - loss: 31.2513 - MinusLogProbMetric: 31.2513 - val_loss: 31.8012 - val_MinusLogProbMetric: 31.8012 - lr: 1.2346e-05 - 79s/epoch - 401ms/step
Epoch 604/1000
2023-09-30 08:39:26.674 
Epoch 604/1000 
	 loss: 31.2811, MinusLogProbMetric: 31.2811, val_loss: 31.8908, val_MinusLogProbMetric: 31.8908

Epoch 604: val_loss did not improve from 31.56588
196/196 - 111s - loss: 31.2811 - MinusLogProbMetric: 31.2811 - val_loss: 31.8908 - val_MinusLogProbMetric: 31.8908 - lr: 1.2346e-05 - 111s/epoch - 565ms/step
Epoch 605/1000
2023-09-30 08:41:19.678 
Epoch 605/1000 
	 loss: 31.2668, MinusLogProbMetric: 31.2668, val_loss: 31.7422, val_MinusLogProbMetric: 31.7422

Epoch 605: val_loss did not improve from 31.56588
196/196 - 113s - loss: 31.2668 - MinusLogProbMetric: 31.2668 - val_loss: 31.7422 - val_MinusLogProbMetric: 31.7422 - lr: 1.2346e-05 - 113s/epoch - 577ms/step
Epoch 606/1000
2023-09-30 08:43:12.937 
Epoch 606/1000 
	 loss: 31.2713, MinusLogProbMetric: 31.2713, val_loss: 31.6217, val_MinusLogProbMetric: 31.6217

Epoch 606: val_loss did not improve from 31.56588
196/196 - 113s - loss: 31.2713 - MinusLogProbMetric: 31.2713 - val_loss: 31.6217 - val_MinusLogProbMetric: 31.6217 - lr: 1.2346e-05 - 113s/epoch - 578ms/step
Epoch 607/1000
2023-09-30 08:45:13.394 
Epoch 607/1000 
	 loss: 31.2434, MinusLogProbMetric: 31.2434, val_loss: 31.7799, val_MinusLogProbMetric: 31.7799

Epoch 607: val_loss did not improve from 31.56588
196/196 - 120s - loss: 31.2434 - MinusLogProbMetric: 31.2434 - val_loss: 31.7799 - val_MinusLogProbMetric: 31.7799 - lr: 1.2346e-05 - 120s/epoch - 615ms/step
Epoch 608/1000
2023-09-30 08:47:06.851 
Epoch 608/1000 
	 loss: 31.2455, MinusLogProbMetric: 31.2455, val_loss: 31.6086, val_MinusLogProbMetric: 31.6086

Epoch 608: val_loss did not improve from 31.56588
196/196 - 113s - loss: 31.2455 - MinusLogProbMetric: 31.2455 - val_loss: 31.6086 - val_MinusLogProbMetric: 31.6086 - lr: 1.2346e-05 - 113s/epoch - 579ms/step
Epoch 609/1000
2023-09-30 08:49:03.514 
Epoch 609/1000 
	 loss: 31.2548, MinusLogProbMetric: 31.2548, val_loss: 31.8750, val_MinusLogProbMetric: 31.8750

Epoch 609: val_loss did not improve from 31.56588
196/196 - 117s - loss: 31.2548 - MinusLogProbMetric: 31.2548 - val_loss: 31.8750 - val_MinusLogProbMetric: 31.8750 - lr: 1.2346e-05 - 117s/epoch - 595ms/step
Epoch 610/1000
2023-09-30 08:50:56.939 
Epoch 610/1000 
	 loss: 31.2565, MinusLogProbMetric: 31.2565, val_loss: 31.8390, val_MinusLogProbMetric: 31.8390

Epoch 610: val_loss did not improve from 31.56588
196/196 - 113s - loss: 31.2565 - MinusLogProbMetric: 31.2565 - val_loss: 31.8390 - val_MinusLogProbMetric: 31.8390 - lr: 1.2346e-05 - 113s/epoch - 578ms/step
Epoch 611/1000
2023-09-30 08:52:53.748 
Epoch 611/1000 
	 loss: 31.2053, MinusLogProbMetric: 31.2053, val_loss: 31.8957, val_MinusLogProbMetric: 31.8957

Epoch 611: val_loss did not improve from 31.56588
196/196 - 117s - loss: 31.2053 - MinusLogProbMetric: 31.2053 - val_loss: 31.8957 - val_MinusLogProbMetric: 31.8957 - lr: 1.2346e-05 - 117s/epoch - 596ms/step
Epoch 612/1000
2023-09-30 08:54:47.636 
Epoch 612/1000 
	 loss: 31.2274, MinusLogProbMetric: 31.2274, val_loss: 31.5690, val_MinusLogProbMetric: 31.5690

Epoch 612: val_loss did not improve from 31.56588
196/196 - 114s - loss: 31.2274 - MinusLogProbMetric: 31.2274 - val_loss: 31.5690 - val_MinusLogProbMetric: 31.5690 - lr: 1.2346e-05 - 114s/epoch - 581ms/step
Epoch 613/1000
2023-09-30 08:56:45.519 
Epoch 613/1000 
	 loss: 31.1914, MinusLogProbMetric: 31.1914, val_loss: 31.5645, val_MinusLogProbMetric: 31.5645

Epoch 613: val_loss improved from 31.56588 to 31.56454, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 119s - loss: 31.1914 - MinusLogProbMetric: 31.1914 - val_loss: 31.5645 - val_MinusLogProbMetric: 31.5645 - lr: 1.2346e-05 - 119s/epoch - 609ms/step
Epoch 614/1000
2023-09-30 08:58:44.964 
Epoch 614/1000 
	 loss: 31.2274, MinusLogProbMetric: 31.2274, val_loss: 31.5912, val_MinusLogProbMetric: 31.5912

Epoch 614: val_loss did not improve from 31.56454
196/196 - 118s - loss: 31.2274 - MinusLogProbMetric: 31.2274 - val_loss: 31.5912 - val_MinusLogProbMetric: 31.5912 - lr: 1.2346e-05 - 118s/epoch - 602ms/step
Epoch 615/1000
2023-09-30 09:00:42.147 
Epoch 615/1000 
	 loss: 31.2073, MinusLogProbMetric: 31.2073, val_loss: 31.9092, val_MinusLogProbMetric: 31.9092

Epoch 615: val_loss did not improve from 31.56454
196/196 - 117s - loss: 31.2073 - MinusLogProbMetric: 31.2073 - val_loss: 31.9092 - val_MinusLogProbMetric: 31.9092 - lr: 1.2346e-05 - 117s/epoch - 598ms/step
Epoch 616/1000
2023-09-30 09:02:31.827 
Epoch 616/1000 
	 loss: 31.2195, MinusLogProbMetric: 31.2195, val_loss: 31.8330, val_MinusLogProbMetric: 31.8330

Epoch 616: val_loss did not improve from 31.56454
196/196 - 110s - loss: 31.2195 - MinusLogProbMetric: 31.2195 - val_loss: 31.8330 - val_MinusLogProbMetric: 31.8330 - lr: 1.2346e-05 - 110s/epoch - 559ms/step
Epoch 617/1000
2023-09-30 09:04:26.084 
Epoch 617/1000 
	 loss: 31.1757, MinusLogProbMetric: 31.1757, val_loss: 31.7559, val_MinusLogProbMetric: 31.7559

Epoch 617: val_loss did not improve from 31.56454
196/196 - 114s - loss: 31.1757 - MinusLogProbMetric: 31.1757 - val_loss: 31.7559 - val_MinusLogProbMetric: 31.7559 - lr: 1.2346e-05 - 114s/epoch - 583ms/step
Epoch 618/1000
2023-09-30 09:06:22.929 
Epoch 618/1000 
	 loss: 31.1664, MinusLogProbMetric: 31.1664, val_loss: 31.5896, val_MinusLogProbMetric: 31.5896

Epoch 618: val_loss did not improve from 31.56454
196/196 - 117s - loss: 31.1664 - MinusLogProbMetric: 31.1664 - val_loss: 31.5896 - val_MinusLogProbMetric: 31.5896 - lr: 1.2346e-05 - 117s/epoch - 596ms/step
Epoch 619/1000
2023-09-30 09:08:13.533 
Epoch 619/1000 
	 loss: 31.1924, MinusLogProbMetric: 31.1924, val_loss: 31.5762, val_MinusLogProbMetric: 31.5762

Epoch 619: val_loss did not improve from 31.56454
196/196 - 111s - loss: 31.1924 - MinusLogProbMetric: 31.1924 - val_loss: 31.5762 - val_MinusLogProbMetric: 31.5762 - lr: 1.2346e-05 - 111s/epoch - 564ms/step
Epoch 620/1000
2023-09-30 09:10:06.545 
Epoch 620/1000 
	 loss: 31.1819, MinusLogProbMetric: 31.1819, val_loss: 31.5729, val_MinusLogProbMetric: 31.5729

Epoch 620: val_loss did not improve from 31.56454
196/196 - 113s - loss: 31.1819 - MinusLogProbMetric: 31.1819 - val_loss: 31.5729 - val_MinusLogProbMetric: 31.5729 - lr: 1.2346e-05 - 113s/epoch - 576ms/step
Epoch 621/1000
2023-09-30 09:11:57.554 
Epoch 621/1000 
	 loss: 31.1846, MinusLogProbMetric: 31.1846, val_loss: 31.5920, val_MinusLogProbMetric: 31.5920

Epoch 621: val_loss did not improve from 31.56454
196/196 - 111s - loss: 31.1846 - MinusLogProbMetric: 31.1846 - val_loss: 31.5920 - val_MinusLogProbMetric: 31.5920 - lr: 1.2346e-05 - 111s/epoch - 567ms/step
Epoch 622/1000
2023-09-30 09:13:52.128 
Epoch 622/1000 
	 loss: 31.1902, MinusLogProbMetric: 31.1902, val_loss: 31.7855, val_MinusLogProbMetric: 31.7855

Epoch 622: val_loss did not improve from 31.56454
196/196 - 114s - loss: 31.1902 - MinusLogProbMetric: 31.1902 - val_loss: 31.7855 - val_MinusLogProbMetric: 31.7855 - lr: 1.2346e-05 - 114s/epoch - 584ms/step
Epoch 623/1000
2023-09-30 09:15:47.358 
Epoch 623/1000 
	 loss: 31.1905, MinusLogProbMetric: 31.1905, val_loss: 31.8241, val_MinusLogProbMetric: 31.8241

Epoch 623: val_loss did not improve from 31.56454
196/196 - 115s - loss: 31.1905 - MinusLogProbMetric: 31.1905 - val_loss: 31.8241 - val_MinusLogProbMetric: 31.8241 - lr: 1.2346e-05 - 115s/epoch - 588ms/step
Epoch 624/1000
2023-09-30 09:17:46.291 
Epoch 624/1000 
	 loss: 31.1472, MinusLogProbMetric: 31.1472, val_loss: 31.5078, val_MinusLogProbMetric: 31.5078

Epoch 624: val_loss improved from 31.56454 to 31.50781, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 124s - loss: 31.1472 - MinusLogProbMetric: 31.1472 - val_loss: 31.5078 - val_MinusLogProbMetric: 31.5078 - lr: 1.2346e-05 - 124s/epoch - 631ms/step
Epoch 625/1000
2023-09-30 09:19:50.674 
Epoch 625/1000 
	 loss: 31.1794, MinusLogProbMetric: 31.1794, val_loss: 31.7404, val_MinusLogProbMetric: 31.7404

Epoch 625: val_loss did not improve from 31.50781
196/196 - 120s - loss: 31.1794 - MinusLogProbMetric: 31.1794 - val_loss: 31.7404 - val_MinusLogProbMetric: 31.7404 - lr: 1.2346e-05 - 120s/epoch - 610ms/step
Epoch 626/1000
2023-09-30 09:21:46.819 
Epoch 626/1000 
	 loss: 31.1461, MinusLogProbMetric: 31.1461, val_loss: 31.6410, val_MinusLogProbMetric: 31.6410

Epoch 626: val_loss did not improve from 31.50781
196/196 - 116s - loss: 31.1461 - MinusLogProbMetric: 31.1461 - val_loss: 31.6410 - val_MinusLogProbMetric: 31.6410 - lr: 1.2346e-05 - 116s/epoch - 593ms/step
Epoch 627/1000
2023-09-30 09:23:42.299 
Epoch 627/1000 
	 loss: 31.1617, MinusLogProbMetric: 31.1617, val_loss: 31.5130, val_MinusLogProbMetric: 31.5130

Epoch 627: val_loss did not improve from 31.50781
196/196 - 115s - loss: 31.1617 - MinusLogProbMetric: 31.1617 - val_loss: 31.5130 - val_MinusLogProbMetric: 31.5130 - lr: 1.2346e-05 - 115s/epoch - 589ms/step
Epoch 628/1000
2023-09-30 09:25:39.651 
Epoch 628/1000 
	 loss: 31.1374, MinusLogProbMetric: 31.1374, val_loss: 31.5966, val_MinusLogProbMetric: 31.5966

Epoch 628: val_loss did not improve from 31.50781
196/196 - 117s - loss: 31.1374 - MinusLogProbMetric: 31.1374 - val_loss: 31.5966 - val_MinusLogProbMetric: 31.5966 - lr: 1.2346e-05 - 117s/epoch - 599ms/step
Epoch 629/1000
2023-09-30 09:27:38.541 
Epoch 629/1000 
	 loss: 31.1270, MinusLogProbMetric: 31.1270, val_loss: 31.6541, val_MinusLogProbMetric: 31.6541

Epoch 629: val_loss did not improve from 31.50781
196/196 - 119s - loss: 31.1270 - MinusLogProbMetric: 31.1270 - val_loss: 31.6541 - val_MinusLogProbMetric: 31.6541 - lr: 1.2346e-05 - 119s/epoch - 607ms/step
Epoch 630/1000
2023-09-30 09:29:34.231 
Epoch 630/1000 
	 loss: 31.1390, MinusLogProbMetric: 31.1390, val_loss: 31.6352, val_MinusLogProbMetric: 31.6352

Epoch 630: val_loss did not improve from 31.50781
196/196 - 116s - loss: 31.1390 - MinusLogProbMetric: 31.1390 - val_loss: 31.6352 - val_MinusLogProbMetric: 31.6352 - lr: 1.2346e-05 - 116s/epoch - 590ms/step
Epoch 631/1000
2023-09-30 09:31:28.631 
Epoch 631/1000 
	 loss: 31.1204, MinusLogProbMetric: 31.1204, val_loss: 31.6126, val_MinusLogProbMetric: 31.6126

Epoch 631: val_loss did not improve from 31.50781
196/196 - 114s - loss: 31.1204 - MinusLogProbMetric: 31.1204 - val_loss: 31.6126 - val_MinusLogProbMetric: 31.6126 - lr: 1.2346e-05 - 114s/epoch - 584ms/step
Epoch 632/1000
2023-09-30 09:33:27.755 
Epoch 632/1000 
	 loss: 31.1110, MinusLogProbMetric: 31.1110, val_loss: 31.8359, val_MinusLogProbMetric: 31.8359

Epoch 632: val_loss did not improve from 31.50781
196/196 - 119s - loss: 31.1110 - MinusLogProbMetric: 31.1110 - val_loss: 31.8359 - val_MinusLogProbMetric: 31.8359 - lr: 1.2346e-05 - 119s/epoch - 608ms/step
Epoch 633/1000
2023-09-30 09:35:29.877 
Epoch 633/1000 
	 loss: 31.1428, MinusLogProbMetric: 31.1428, val_loss: 31.6722, val_MinusLogProbMetric: 31.6722

Epoch 633: val_loss did not improve from 31.50781
196/196 - 122s - loss: 31.1428 - MinusLogProbMetric: 31.1428 - val_loss: 31.6722 - val_MinusLogProbMetric: 31.6722 - lr: 1.2346e-05 - 122s/epoch - 623ms/step
Epoch 634/1000
2023-09-30 09:37:29.545 
Epoch 634/1000 
	 loss: 31.1229, MinusLogProbMetric: 31.1229, val_loss: 31.5366, val_MinusLogProbMetric: 31.5366

Epoch 634: val_loss did not improve from 31.50781
196/196 - 120s - loss: 31.1229 - MinusLogProbMetric: 31.1229 - val_loss: 31.5366 - val_MinusLogProbMetric: 31.5366 - lr: 1.2346e-05 - 120s/epoch - 610ms/step
Epoch 635/1000
2023-09-30 09:39:24.267 
Epoch 635/1000 
	 loss: 31.1537, MinusLogProbMetric: 31.1537, val_loss: 31.6624, val_MinusLogProbMetric: 31.6624

Epoch 635: val_loss did not improve from 31.50781
196/196 - 115s - loss: 31.1537 - MinusLogProbMetric: 31.1537 - val_loss: 31.6624 - val_MinusLogProbMetric: 31.6624 - lr: 1.2346e-05 - 115s/epoch - 585ms/step
Epoch 636/1000
2023-09-30 09:41:21.323 
Epoch 636/1000 
	 loss: 31.1180, MinusLogProbMetric: 31.1180, val_loss: 31.5834, val_MinusLogProbMetric: 31.5834

Epoch 636: val_loss did not improve from 31.50781
196/196 - 117s - loss: 31.1180 - MinusLogProbMetric: 31.1180 - val_loss: 31.5834 - val_MinusLogProbMetric: 31.5834 - lr: 1.2346e-05 - 117s/epoch - 597ms/step
Epoch 637/1000
2023-09-30 09:43:16.504 
Epoch 637/1000 
	 loss: 31.1341, MinusLogProbMetric: 31.1341, val_loss: 31.8794, val_MinusLogProbMetric: 31.8794

Epoch 637: val_loss did not improve from 31.50781
196/196 - 115s - loss: 31.1341 - MinusLogProbMetric: 31.1341 - val_loss: 31.8794 - val_MinusLogProbMetric: 31.8794 - lr: 1.2346e-05 - 115s/epoch - 588ms/step
Epoch 638/1000
2023-09-30 09:45:11.853 
Epoch 638/1000 
	 loss: 31.1201, MinusLogProbMetric: 31.1201, val_loss: 31.8457, val_MinusLogProbMetric: 31.8457

Epoch 638: val_loss did not improve from 31.50781
196/196 - 115s - loss: 31.1201 - MinusLogProbMetric: 31.1201 - val_loss: 31.8457 - val_MinusLogProbMetric: 31.8457 - lr: 1.2346e-05 - 115s/epoch - 589ms/step
Epoch 639/1000
2023-09-30 09:47:11.994 
Epoch 639/1000 
	 loss: 31.1117, MinusLogProbMetric: 31.1117, val_loss: 31.5406, val_MinusLogProbMetric: 31.5406

Epoch 639: val_loss did not improve from 31.50781
196/196 - 120s - loss: 31.1117 - MinusLogProbMetric: 31.1117 - val_loss: 31.5406 - val_MinusLogProbMetric: 31.5406 - lr: 1.2346e-05 - 120s/epoch - 613ms/step
Epoch 640/1000
2023-09-30 09:49:05.863 
Epoch 640/1000 
	 loss: 31.1106, MinusLogProbMetric: 31.1106, val_loss: 31.7604, val_MinusLogProbMetric: 31.7604

Epoch 640: val_loss did not improve from 31.50781
196/196 - 114s - loss: 31.1106 - MinusLogProbMetric: 31.1106 - val_loss: 31.7604 - val_MinusLogProbMetric: 31.7604 - lr: 1.2346e-05 - 114s/epoch - 581ms/step
Epoch 641/1000
2023-09-30 09:51:01.040 
Epoch 641/1000 
	 loss: 31.1082, MinusLogProbMetric: 31.1082, val_loss: 31.6841, val_MinusLogProbMetric: 31.6841

Epoch 641: val_loss did not improve from 31.50781
196/196 - 115s - loss: 31.1082 - MinusLogProbMetric: 31.1082 - val_loss: 31.6841 - val_MinusLogProbMetric: 31.6841 - lr: 1.2346e-05 - 115s/epoch - 588ms/step
Epoch 642/1000
2023-09-30 09:52:55.579 
Epoch 642/1000 
	 loss: 31.1016, MinusLogProbMetric: 31.1016, val_loss: 31.6368, val_MinusLogProbMetric: 31.6368

Epoch 642: val_loss did not improve from 31.50781
196/196 - 115s - loss: 31.1016 - MinusLogProbMetric: 31.1016 - val_loss: 31.6368 - val_MinusLogProbMetric: 31.6368 - lr: 1.2346e-05 - 115s/epoch - 584ms/step
Epoch 643/1000
2023-09-30 09:54:50.674 
Epoch 643/1000 
	 loss: 31.0965, MinusLogProbMetric: 31.0965, val_loss: 31.5133, val_MinusLogProbMetric: 31.5133

Epoch 643: val_loss did not improve from 31.50781
196/196 - 115s - loss: 31.0965 - MinusLogProbMetric: 31.0965 - val_loss: 31.5133 - val_MinusLogProbMetric: 31.5133 - lr: 1.2346e-05 - 115s/epoch - 587ms/step
Epoch 644/1000
2023-09-30 09:56:49.121 
Epoch 644/1000 
	 loss: 31.0809, MinusLogProbMetric: 31.0809, val_loss: 31.6263, val_MinusLogProbMetric: 31.6263

Epoch 644: val_loss did not improve from 31.50781
196/196 - 118s - loss: 31.0809 - MinusLogProbMetric: 31.0809 - val_loss: 31.6263 - val_MinusLogProbMetric: 31.6263 - lr: 1.2346e-05 - 118s/epoch - 604ms/step
Epoch 645/1000
2023-09-30 09:58:45.270 
Epoch 645/1000 
	 loss: 31.0827, MinusLogProbMetric: 31.0827, val_loss: 31.5673, val_MinusLogProbMetric: 31.5673

Epoch 645: val_loss did not improve from 31.50781
196/196 - 116s - loss: 31.0827 - MinusLogProbMetric: 31.0827 - val_loss: 31.5673 - val_MinusLogProbMetric: 31.5673 - lr: 1.2346e-05 - 116s/epoch - 593ms/step
Epoch 646/1000
2023-09-30 10:00:46.496 
Epoch 646/1000 
	 loss: 31.0763, MinusLogProbMetric: 31.0763, val_loss: 31.5643, val_MinusLogProbMetric: 31.5643

Epoch 646: val_loss did not improve from 31.50781
196/196 - 121s - loss: 31.0763 - MinusLogProbMetric: 31.0763 - val_loss: 31.5643 - val_MinusLogProbMetric: 31.5643 - lr: 1.2346e-05 - 121s/epoch - 618ms/step
Epoch 647/1000
2023-09-30 10:02:43.922 
Epoch 647/1000 
	 loss: 31.0628, MinusLogProbMetric: 31.0628, val_loss: 31.4476, val_MinusLogProbMetric: 31.4476

Epoch 647: val_loss improved from 31.50781 to 31.44764, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 121s - loss: 31.0628 - MinusLogProbMetric: 31.0628 - val_loss: 31.4476 - val_MinusLogProbMetric: 31.4476 - lr: 1.2346e-05 - 121s/epoch - 620ms/step
Epoch 648/1000
2023-09-30 10:04:44.249 
Epoch 648/1000 
	 loss: 31.0711, MinusLogProbMetric: 31.0711, val_loss: 31.5192, val_MinusLogProbMetric: 31.5192

Epoch 648: val_loss did not improve from 31.44764
196/196 - 116s - loss: 31.0711 - MinusLogProbMetric: 31.0711 - val_loss: 31.5192 - val_MinusLogProbMetric: 31.5192 - lr: 1.2346e-05 - 116s/epoch - 593ms/step
Epoch 649/1000
2023-09-30 10:06:28.713 
Epoch 649/1000 
	 loss: 31.0813, MinusLogProbMetric: 31.0813, val_loss: 31.6295, val_MinusLogProbMetric: 31.6295

Epoch 649: val_loss did not improve from 31.44764
196/196 - 104s - loss: 31.0813 - MinusLogProbMetric: 31.0813 - val_loss: 31.6295 - val_MinusLogProbMetric: 31.6295 - lr: 1.2346e-05 - 104s/epoch - 533ms/step
Epoch 650/1000
2023-09-30 10:08:19.967 
Epoch 650/1000 
	 loss: 31.0731, MinusLogProbMetric: 31.0731, val_loss: 31.4563, val_MinusLogProbMetric: 31.4563

Epoch 650: val_loss did not improve from 31.44764
196/196 - 111s - loss: 31.0731 - MinusLogProbMetric: 31.0731 - val_loss: 31.4563 - val_MinusLogProbMetric: 31.4563 - lr: 1.2346e-05 - 111s/epoch - 567ms/step
Epoch 651/1000
2023-09-30 10:10:16.993 
Epoch 651/1000 
	 loss: 31.0275, MinusLogProbMetric: 31.0275, val_loss: 32.0507, val_MinusLogProbMetric: 32.0507

Epoch 651: val_loss did not improve from 31.44764
196/196 - 117s - loss: 31.0275 - MinusLogProbMetric: 31.0275 - val_loss: 32.0507 - val_MinusLogProbMetric: 32.0507 - lr: 1.2346e-05 - 117s/epoch - 597ms/step
Epoch 652/1000
2023-09-30 10:12:13.548 
Epoch 652/1000 
	 loss: 31.0654, MinusLogProbMetric: 31.0654, val_loss: 31.5042, val_MinusLogProbMetric: 31.5042

Epoch 652: val_loss did not improve from 31.44764
196/196 - 117s - loss: 31.0654 - MinusLogProbMetric: 31.0654 - val_loss: 31.5042 - val_MinusLogProbMetric: 31.5042 - lr: 1.2346e-05 - 117s/epoch - 595ms/step
Epoch 653/1000
2023-09-30 10:14:08.092 
Epoch 653/1000 
	 loss: 31.0572, MinusLogProbMetric: 31.0572, val_loss: 31.4262, val_MinusLogProbMetric: 31.4262

Epoch 653: val_loss improved from 31.44764 to 31.42622, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 117s - loss: 31.0572 - MinusLogProbMetric: 31.0572 - val_loss: 31.4262 - val_MinusLogProbMetric: 31.4262 - lr: 1.2346e-05 - 117s/epoch - 599ms/step
Epoch 654/1000
2023-09-30 10:16:02.950 
Epoch 654/1000 
	 loss: 31.0534, MinusLogProbMetric: 31.0534, val_loss: 31.4944, val_MinusLogProbMetric: 31.4944

Epoch 654: val_loss did not improve from 31.42622
196/196 - 112s - loss: 31.0534 - MinusLogProbMetric: 31.0534 - val_loss: 31.4944 - val_MinusLogProbMetric: 31.4944 - lr: 1.2346e-05 - 112s/epoch - 571ms/step
Epoch 655/1000
2023-09-30 10:17:58.977 
Epoch 655/1000 
	 loss: 31.0294, MinusLogProbMetric: 31.0294, val_loss: 31.4272, val_MinusLogProbMetric: 31.4272

Epoch 655: val_loss did not improve from 31.42622
196/196 - 116s - loss: 31.0294 - MinusLogProbMetric: 31.0294 - val_loss: 31.4272 - val_MinusLogProbMetric: 31.4272 - lr: 1.2346e-05 - 116s/epoch - 592ms/step
Epoch 656/1000
2023-09-30 10:19:51.886 
Epoch 656/1000 
	 loss: 31.0111, MinusLogProbMetric: 31.0111, val_loss: 31.5500, val_MinusLogProbMetric: 31.5500

Epoch 656: val_loss did not improve from 31.42622
196/196 - 113s - loss: 31.0111 - MinusLogProbMetric: 31.0111 - val_loss: 31.5500 - val_MinusLogProbMetric: 31.5500 - lr: 1.2346e-05 - 113s/epoch - 576ms/step
Epoch 657/1000
2023-09-30 10:21:45.448 
Epoch 657/1000 
	 loss: 31.0151, MinusLogProbMetric: 31.0151, val_loss: 31.6653, val_MinusLogProbMetric: 31.6653

Epoch 657: val_loss did not improve from 31.42622
196/196 - 113s - loss: 31.0151 - MinusLogProbMetric: 31.0151 - val_loss: 31.6653 - val_MinusLogProbMetric: 31.6653 - lr: 1.2346e-05 - 113s/epoch - 579ms/step
Epoch 658/1000
2023-09-30 10:23:37.474 
Epoch 658/1000 
	 loss: 31.0761, MinusLogProbMetric: 31.0761, val_loss: 31.6155, val_MinusLogProbMetric: 31.6155

Epoch 658: val_loss did not improve from 31.42622
196/196 - 112s - loss: 31.0761 - MinusLogProbMetric: 31.0761 - val_loss: 31.6155 - val_MinusLogProbMetric: 31.6155 - lr: 1.2346e-05 - 112s/epoch - 572ms/step
Epoch 659/1000
2023-09-30 10:25:29.510 
Epoch 659/1000 
	 loss: 30.9914, MinusLogProbMetric: 30.9914, val_loss: 31.5404, val_MinusLogProbMetric: 31.5404

Epoch 659: val_loss did not improve from 31.42622
196/196 - 112s - loss: 30.9914 - MinusLogProbMetric: 30.9914 - val_loss: 31.5404 - val_MinusLogProbMetric: 31.5404 - lr: 1.2346e-05 - 112s/epoch - 571ms/step
Epoch 660/1000
2023-09-30 10:27:24.084 
Epoch 660/1000 
	 loss: 30.9988, MinusLogProbMetric: 30.9988, val_loss: 31.3574, val_MinusLogProbMetric: 31.3574

Epoch 660: val_loss improved from 31.42622 to 31.35739, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 116s - loss: 30.9988 - MinusLogProbMetric: 30.9988 - val_loss: 31.3574 - val_MinusLogProbMetric: 31.3574 - lr: 1.2346e-05 - 116s/epoch - 592ms/step
Epoch 661/1000
2023-09-30 10:29:20.410 
Epoch 661/1000 
	 loss: 31.0334, MinusLogProbMetric: 31.0334, val_loss: 31.4819, val_MinusLogProbMetric: 31.4819

Epoch 661: val_loss did not improve from 31.35739
196/196 - 115s - loss: 31.0334 - MinusLogProbMetric: 31.0334 - val_loss: 31.4819 - val_MinusLogProbMetric: 31.4819 - lr: 1.2346e-05 - 115s/epoch - 586ms/step
Epoch 662/1000
2023-09-30 10:31:17.981 
Epoch 662/1000 
	 loss: 30.9924, MinusLogProbMetric: 30.9924, val_loss: 31.5265, val_MinusLogProbMetric: 31.5265

Epoch 662: val_loss did not improve from 31.35739
196/196 - 118s - loss: 30.9924 - MinusLogProbMetric: 30.9924 - val_loss: 31.5265 - val_MinusLogProbMetric: 31.5265 - lr: 1.2346e-05 - 118s/epoch - 600ms/step
Epoch 663/1000
2023-09-30 10:33:11.217 
Epoch 663/1000 
	 loss: 31.0044, MinusLogProbMetric: 31.0044, val_loss: 31.5379, val_MinusLogProbMetric: 31.5379

Epoch 663: val_loss did not improve from 31.35739
196/196 - 113s - loss: 31.0044 - MinusLogProbMetric: 31.0044 - val_loss: 31.5379 - val_MinusLogProbMetric: 31.5379 - lr: 1.2346e-05 - 113s/epoch - 578ms/step
Epoch 664/1000
2023-09-30 10:35:05.176 
Epoch 664/1000 
	 loss: 30.9887, MinusLogProbMetric: 30.9887, val_loss: 31.4294, val_MinusLogProbMetric: 31.4294

Epoch 664: val_loss did not improve from 31.35739
196/196 - 114s - loss: 30.9887 - MinusLogProbMetric: 30.9887 - val_loss: 31.4294 - val_MinusLogProbMetric: 31.4294 - lr: 1.2346e-05 - 114s/epoch - 581ms/step
Epoch 665/1000
2023-09-30 10:36:58.943 
Epoch 665/1000 
	 loss: 31.0176, MinusLogProbMetric: 31.0176, val_loss: 31.5169, val_MinusLogProbMetric: 31.5169

Epoch 665: val_loss did not improve from 31.35739
196/196 - 114s - loss: 31.0176 - MinusLogProbMetric: 31.0176 - val_loss: 31.5169 - val_MinusLogProbMetric: 31.5169 - lr: 1.2346e-05 - 114s/epoch - 580ms/step
Epoch 666/1000
2023-09-30 10:38:57.325 
Epoch 666/1000 
	 loss: 30.9830, MinusLogProbMetric: 30.9830, val_loss: 31.5599, val_MinusLogProbMetric: 31.5599

Epoch 666: val_loss did not improve from 31.35739
196/196 - 118s - loss: 30.9830 - MinusLogProbMetric: 30.9830 - val_loss: 31.5599 - val_MinusLogProbMetric: 31.5599 - lr: 1.2346e-05 - 118s/epoch - 604ms/step
Epoch 667/1000
2023-09-30 10:40:49.578 
Epoch 667/1000 
	 loss: 30.9717, MinusLogProbMetric: 30.9717, val_loss: 31.3462, val_MinusLogProbMetric: 31.3462

Epoch 667: val_loss improved from 31.35739 to 31.34617, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 114s - loss: 30.9717 - MinusLogProbMetric: 30.9717 - val_loss: 31.3462 - val_MinusLogProbMetric: 31.3462 - lr: 1.2346e-05 - 114s/epoch - 583ms/step
Epoch 668/1000
2023-09-30 10:42:44.908 
Epoch 668/1000 
	 loss: 31.0013, MinusLogProbMetric: 31.0013, val_loss: 31.5229, val_MinusLogProbMetric: 31.5229

Epoch 668: val_loss did not improve from 31.34617
196/196 - 113s - loss: 31.0013 - MinusLogProbMetric: 31.0013 - val_loss: 31.5229 - val_MinusLogProbMetric: 31.5229 - lr: 1.2346e-05 - 113s/epoch - 578ms/step
Epoch 669/1000
2023-09-30 10:44:35.527 
Epoch 669/1000 
	 loss: 31.0313, MinusLogProbMetric: 31.0313, val_loss: 31.5687, val_MinusLogProbMetric: 31.5687

Epoch 669: val_loss did not improve from 31.34617
196/196 - 111s - loss: 31.0313 - MinusLogProbMetric: 31.0313 - val_loss: 31.5687 - val_MinusLogProbMetric: 31.5687 - lr: 1.2346e-05 - 111s/epoch - 564ms/step
Epoch 670/1000
2023-09-30 10:46:28.108 
Epoch 670/1000 
	 loss: 30.9591, MinusLogProbMetric: 30.9591, val_loss: 31.4420, val_MinusLogProbMetric: 31.4420

Epoch 670: val_loss did not improve from 31.34617
196/196 - 113s - loss: 30.9591 - MinusLogProbMetric: 30.9591 - val_loss: 31.4420 - val_MinusLogProbMetric: 31.4420 - lr: 1.2346e-05 - 113s/epoch - 574ms/step
Epoch 671/1000
2023-09-30 10:48:22.010 
Epoch 671/1000 
	 loss: 30.9824, MinusLogProbMetric: 30.9824, val_loss: 31.5321, val_MinusLogProbMetric: 31.5321

Epoch 671: val_loss did not improve from 31.34617
196/196 - 114s - loss: 30.9824 - MinusLogProbMetric: 30.9824 - val_loss: 31.5321 - val_MinusLogProbMetric: 31.5321 - lr: 1.2346e-05 - 114s/epoch - 581ms/step
Epoch 672/1000
2023-09-30 10:50:16.219 
Epoch 672/1000 
	 loss: 30.9824, MinusLogProbMetric: 30.9824, val_loss: 31.6174, val_MinusLogProbMetric: 31.6174

Epoch 672: val_loss did not improve from 31.34617
196/196 - 114s - loss: 30.9824 - MinusLogProbMetric: 30.9824 - val_loss: 31.6174 - val_MinusLogProbMetric: 31.6174 - lr: 1.2346e-05 - 114s/epoch - 583ms/step
Epoch 673/1000
2023-09-30 10:52:10.541 
Epoch 673/1000 
	 loss: 31.0127, MinusLogProbMetric: 31.0127, val_loss: 31.5397, val_MinusLogProbMetric: 31.5397

Epoch 673: val_loss did not improve from 31.34617
196/196 - 114s - loss: 31.0127 - MinusLogProbMetric: 31.0127 - val_loss: 31.5397 - val_MinusLogProbMetric: 31.5397 - lr: 1.2346e-05 - 114s/epoch - 583ms/step
Epoch 674/1000
2023-09-30 10:54:01.669 
Epoch 674/1000 
	 loss: 30.9624, MinusLogProbMetric: 30.9624, val_loss: 31.4837, val_MinusLogProbMetric: 31.4837

Epoch 674: val_loss did not improve from 31.34617
196/196 - 111s - loss: 30.9624 - MinusLogProbMetric: 30.9624 - val_loss: 31.4837 - val_MinusLogProbMetric: 31.4837 - lr: 1.2346e-05 - 111s/epoch - 567ms/step
Epoch 675/1000
2023-09-30 10:55:56.453 
Epoch 675/1000 
	 loss: 30.9578, MinusLogProbMetric: 30.9578, val_loss: 31.5317, val_MinusLogProbMetric: 31.5317

Epoch 675: val_loss did not improve from 31.34617
196/196 - 115s - loss: 30.9578 - MinusLogProbMetric: 30.9578 - val_loss: 31.5317 - val_MinusLogProbMetric: 31.5317 - lr: 1.2346e-05 - 115s/epoch - 586ms/step
Epoch 676/1000
2023-09-30 10:57:47.789 
Epoch 676/1000 
	 loss: 31.0601, MinusLogProbMetric: 31.0601, val_loss: 31.7544, val_MinusLogProbMetric: 31.7544

Epoch 676: val_loss did not improve from 31.34617
196/196 - 111s - loss: 31.0601 - MinusLogProbMetric: 31.0601 - val_loss: 31.7544 - val_MinusLogProbMetric: 31.7544 - lr: 1.2346e-05 - 111s/epoch - 568ms/step
Epoch 677/1000
2023-09-30 10:59:43.832 
Epoch 677/1000 
	 loss: 30.9819, MinusLogProbMetric: 30.9819, val_loss: 31.3763, val_MinusLogProbMetric: 31.3763

Epoch 677: val_loss did not improve from 31.34617
196/196 - 116s - loss: 30.9819 - MinusLogProbMetric: 30.9819 - val_loss: 31.3763 - val_MinusLogProbMetric: 31.3763 - lr: 1.2346e-05 - 116s/epoch - 592ms/step
Epoch 678/1000
2023-09-30 11:01:37.525 
Epoch 678/1000 
	 loss: 30.9684, MinusLogProbMetric: 30.9684, val_loss: 31.3083, val_MinusLogProbMetric: 31.3083

Epoch 678: val_loss improved from 31.34617 to 31.30825, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 117s - loss: 30.9684 - MinusLogProbMetric: 30.9684 - val_loss: 31.3083 - val_MinusLogProbMetric: 31.3083 - lr: 1.2346e-05 - 117s/epoch - 596ms/step
Epoch 679/1000
2023-09-30 11:03:30.719 
Epoch 679/1000 
	 loss: 30.9214, MinusLogProbMetric: 30.9214, val_loss: 31.6501, val_MinusLogProbMetric: 31.6501

Epoch 679: val_loss did not improve from 31.30825
196/196 - 110s - loss: 30.9214 - MinusLogProbMetric: 30.9214 - val_loss: 31.6501 - val_MinusLogProbMetric: 31.6501 - lr: 1.2346e-05 - 110s/epoch - 562ms/step
Epoch 680/1000
2023-09-30 11:05:21.953 
Epoch 680/1000 
	 loss: 30.9546, MinusLogProbMetric: 30.9546, val_loss: 31.3669, val_MinusLogProbMetric: 31.3669

Epoch 680: val_loss did not improve from 31.30825
196/196 - 111s - loss: 30.9546 - MinusLogProbMetric: 30.9546 - val_loss: 31.3669 - val_MinusLogProbMetric: 31.3669 - lr: 1.2346e-05 - 111s/epoch - 568ms/step
Epoch 681/1000
2023-09-30 11:07:15.888 
Epoch 681/1000 
	 loss: 30.9160, MinusLogProbMetric: 30.9160, val_loss: 31.2737, val_MinusLogProbMetric: 31.2737

Epoch 681: val_loss improved from 31.30825 to 31.27369, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 116s - loss: 30.9160 - MinusLogProbMetric: 30.9160 - val_loss: 31.2737 - val_MinusLogProbMetric: 31.2737 - lr: 1.2346e-05 - 116s/epoch - 592ms/step
Epoch 682/1000
2023-09-30 11:09:12.994 
Epoch 682/1000 
	 loss: 30.9192, MinusLogProbMetric: 30.9192, val_loss: 31.3222, val_MinusLogProbMetric: 31.3222

Epoch 682: val_loss did not improve from 31.27369
196/196 - 115s - loss: 30.9192 - MinusLogProbMetric: 30.9192 - val_loss: 31.3222 - val_MinusLogProbMetric: 31.3222 - lr: 1.2346e-05 - 115s/epoch - 587ms/step
Epoch 683/1000
2023-09-30 11:10:59.130 
Epoch 683/1000 
	 loss: 30.9049, MinusLogProbMetric: 30.9049, val_loss: 31.3964, val_MinusLogProbMetric: 31.3964

Epoch 683: val_loss did not improve from 31.27369
196/196 - 106s - loss: 30.9049 - MinusLogProbMetric: 30.9049 - val_loss: 31.3964 - val_MinusLogProbMetric: 31.3964 - lr: 1.2346e-05 - 106s/epoch - 542ms/step
Epoch 684/1000
2023-09-30 11:12:54.243 
Epoch 684/1000 
	 loss: 30.9220, MinusLogProbMetric: 30.9220, val_loss: 31.4046, val_MinusLogProbMetric: 31.4046

Epoch 684: val_loss did not improve from 31.27369
196/196 - 115s - loss: 30.9220 - MinusLogProbMetric: 30.9220 - val_loss: 31.4046 - val_MinusLogProbMetric: 31.4046 - lr: 1.2346e-05 - 115s/epoch - 587ms/step
Epoch 685/1000
2023-09-30 11:14:44.199 
Epoch 685/1000 
	 loss: 30.9116, MinusLogProbMetric: 30.9116, val_loss: 31.5526, val_MinusLogProbMetric: 31.5526

Epoch 685: val_loss did not improve from 31.27369
196/196 - 110s - loss: 30.9116 - MinusLogProbMetric: 30.9116 - val_loss: 31.5526 - val_MinusLogProbMetric: 31.5526 - lr: 1.2346e-05 - 110s/epoch - 561ms/step
Epoch 686/1000
2023-09-30 11:16:38.807 
Epoch 686/1000 
	 loss: 30.9405, MinusLogProbMetric: 30.9405, val_loss: 31.3602, val_MinusLogProbMetric: 31.3602

Epoch 686: val_loss did not improve from 31.27369
196/196 - 115s - loss: 30.9405 - MinusLogProbMetric: 30.9405 - val_loss: 31.3602 - val_MinusLogProbMetric: 31.3602 - lr: 1.2346e-05 - 115s/epoch - 585ms/step
Epoch 687/1000
2023-09-30 11:18:26.551 
Epoch 687/1000 
	 loss: 30.9065, MinusLogProbMetric: 30.9065, val_loss: 31.4264, val_MinusLogProbMetric: 31.4264

Epoch 687: val_loss did not improve from 31.27369
196/196 - 108s - loss: 30.9065 - MinusLogProbMetric: 30.9065 - val_loss: 31.4264 - val_MinusLogProbMetric: 31.4264 - lr: 1.2346e-05 - 108s/epoch - 550ms/step
Epoch 688/1000
2023-09-30 11:20:18.649 
Epoch 688/1000 
	 loss: 30.9088, MinusLogProbMetric: 30.9088, val_loss: 31.3593, val_MinusLogProbMetric: 31.3593

Epoch 688: val_loss did not improve from 31.27369
196/196 - 112s - loss: 30.9088 - MinusLogProbMetric: 30.9088 - val_loss: 31.3593 - val_MinusLogProbMetric: 31.3593 - lr: 1.2346e-05 - 112s/epoch - 572ms/step
Epoch 689/1000
2023-09-30 11:22:11.188 
Epoch 689/1000 
	 loss: 30.9110, MinusLogProbMetric: 30.9110, val_loss: 31.3474, val_MinusLogProbMetric: 31.3474

Epoch 689: val_loss did not improve from 31.27369
196/196 - 113s - loss: 30.9110 - MinusLogProbMetric: 30.9110 - val_loss: 31.3474 - val_MinusLogProbMetric: 31.3474 - lr: 1.2346e-05 - 113s/epoch - 574ms/step
Epoch 690/1000
2023-09-30 11:24:02.405 
Epoch 690/1000 
	 loss: 30.9141, MinusLogProbMetric: 30.9141, val_loss: 31.3201, val_MinusLogProbMetric: 31.3201

Epoch 690: val_loss did not improve from 31.27369
196/196 - 111s - loss: 30.9141 - MinusLogProbMetric: 30.9141 - val_loss: 31.3201 - val_MinusLogProbMetric: 31.3201 - lr: 1.2346e-05 - 111s/epoch - 567ms/step
Epoch 691/1000
2023-09-30 11:25:55.046 
Epoch 691/1000 
	 loss: 30.8864, MinusLogProbMetric: 30.8864, val_loss: 31.3398, val_MinusLogProbMetric: 31.3398

Epoch 691: val_loss did not improve from 31.27369
196/196 - 113s - loss: 30.8864 - MinusLogProbMetric: 30.8864 - val_loss: 31.3398 - val_MinusLogProbMetric: 31.3398 - lr: 1.2346e-05 - 113s/epoch - 574ms/step
Epoch 692/1000
2023-09-30 11:27:45.608 
Epoch 692/1000 
	 loss: 30.9224, MinusLogProbMetric: 30.9224, val_loss: 31.3034, val_MinusLogProbMetric: 31.3034

Epoch 692: val_loss did not improve from 31.27369
196/196 - 111s - loss: 30.9224 - MinusLogProbMetric: 30.9224 - val_loss: 31.3034 - val_MinusLogProbMetric: 31.3034 - lr: 1.2346e-05 - 111s/epoch - 564ms/step
Epoch 693/1000
2023-09-30 11:29:35.032 
Epoch 693/1000 
	 loss: 30.8929, MinusLogProbMetric: 30.8929, val_loss: 31.3233, val_MinusLogProbMetric: 31.3233

Epoch 693: val_loss did not improve from 31.27369
196/196 - 109s - loss: 30.8929 - MinusLogProbMetric: 30.8929 - val_loss: 31.3233 - val_MinusLogProbMetric: 31.3233 - lr: 1.2346e-05 - 109s/epoch - 558ms/step
Epoch 694/1000
2023-09-30 11:31:28.031 
Epoch 694/1000 
	 loss: 30.8994, MinusLogProbMetric: 30.8994, val_loss: 31.1863, val_MinusLogProbMetric: 31.1863

Epoch 694: val_loss improved from 31.27369 to 31.18634, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 117s - loss: 30.8994 - MinusLogProbMetric: 30.8994 - val_loss: 31.1863 - val_MinusLogProbMetric: 31.1863 - lr: 1.2346e-05 - 117s/epoch - 596ms/step
Epoch 695/1000
2023-09-30 11:33:30.543 
Epoch 695/1000 
	 loss: 30.8693, MinusLogProbMetric: 30.8693, val_loss: 31.3674, val_MinusLogProbMetric: 31.3674

Epoch 695: val_loss did not improve from 31.18634
196/196 - 119s - loss: 30.8693 - MinusLogProbMetric: 30.8693 - val_loss: 31.3674 - val_MinusLogProbMetric: 31.3674 - lr: 1.2346e-05 - 119s/epoch - 605ms/step
Epoch 696/1000
2023-09-30 11:36:13.818 
Epoch 696/1000 
	 loss: 30.8796, MinusLogProbMetric: 30.8796, val_loss: 31.3831, val_MinusLogProbMetric: 31.3831

Epoch 696: val_loss did not improve from 31.18634
196/196 - 163s - loss: 30.8796 - MinusLogProbMetric: 30.8796 - val_loss: 31.3831 - val_MinusLogProbMetric: 31.3831 - lr: 1.2346e-05 - 163s/epoch - 833ms/step
Epoch 697/1000
2023-09-30 11:38:38.696 
Epoch 697/1000 
	 loss: 30.8845, MinusLogProbMetric: 30.8845, val_loss: 31.5474, val_MinusLogProbMetric: 31.5474

Epoch 697: val_loss did not improve from 31.18634
196/196 - 145s - loss: 30.8845 - MinusLogProbMetric: 30.8845 - val_loss: 31.5474 - val_MinusLogProbMetric: 31.5474 - lr: 1.2346e-05 - 145s/epoch - 739ms/step
Epoch 698/1000
2023-09-30 11:40:58.985 
Epoch 698/1000 
	 loss: 30.9062, MinusLogProbMetric: 30.9062, val_loss: 31.6286, val_MinusLogProbMetric: 31.6286

Epoch 698: val_loss did not improve from 31.18634
196/196 - 140s - loss: 30.9062 - MinusLogProbMetric: 30.9062 - val_loss: 31.6286 - val_MinusLogProbMetric: 31.6286 - lr: 1.2346e-05 - 140s/epoch - 716ms/step
Epoch 699/1000
2023-09-30 11:43:12.781 
Epoch 699/1000 
	 loss: 30.9006, MinusLogProbMetric: 30.9006, val_loss: 31.4375, val_MinusLogProbMetric: 31.4375

Epoch 699: val_loss did not improve from 31.18634
196/196 - 134s - loss: 30.9006 - MinusLogProbMetric: 30.9006 - val_loss: 31.4375 - val_MinusLogProbMetric: 31.4375 - lr: 1.2346e-05 - 134s/epoch - 683ms/step
Epoch 700/1000
2023-09-30 11:45:28.853 
Epoch 700/1000 
	 loss: 30.8945, MinusLogProbMetric: 30.8945, val_loss: 31.6080, val_MinusLogProbMetric: 31.6080

Epoch 700: val_loss did not improve from 31.18634
196/196 - 136s - loss: 30.8945 - MinusLogProbMetric: 30.8945 - val_loss: 31.6080 - val_MinusLogProbMetric: 31.6080 - lr: 1.2346e-05 - 136s/epoch - 694ms/step
Epoch 701/1000
2023-09-30 11:47:49.094 
Epoch 701/1000 
	 loss: 30.9040, MinusLogProbMetric: 30.9040, val_loss: 31.4577, val_MinusLogProbMetric: 31.4577

Epoch 701: val_loss did not improve from 31.18634
196/196 - 140s - loss: 30.9040 - MinusLogProbMetric: 30.9040 - val_loss: 31.4577 - val_MinusLogProbMetric: 31.4577 - lr: 1.2346e-05 - 140s/epoch - 715ms/step
Epoch 702/1000
2023-09-30 11:50:16.616 
Epoch 702/1000 
	 loss: 30.8473, MinusLogProbMetric: 30.8473, val_loss: 31.3571, val_MinusLogProbMetric: 31.3571

Epoch 702: val_loss did not improve from 31.18634
196/196 - 148s - loss: 30.8473 - MinusLogProbMetric: 30.8473 - val_loss: 31.3571 - val_MinusLogProbMetric: 31.3571 - lr: 1.2346e-05 - 148s/epoch - 753ms/step
Epoch 703/1000
2023-09-30 11:52:41.056 
Epoch 703/1000 
	 loss: 30.8389, MinusLogProbMetric: 30.8389, val_loss: 31.6954, val_MinusLogProbMetric: 31.6954

Epoch 703: val_loss did not improve from 31.18634
196/196 - 144s - loss: 30.8389 - MinusLogProbMetric: 30.8389 - val_loss: 31.6954 - val_MinusLogProbMetric: 31.6954 - lr: 1.2346e-05 - 144s/epoch - 737ms/step
Epoch 704/1000
2023-09-30 11:55:02.547 
Epoch 704/1000 
	 loss: 30.8643, MinusLogProbMetric: 30.8643, val_loss: 31.2703, val_MinusLogProbMetric: 31.2703

Epoch 704: val_loss did not improve from 31.18634
196/196 - 141s - loss: 30.8643 - MinusLogProbMetric: 30.8643 - val_loss: 31.2703 - val_MinusLogProbMetric: 31.2703 - lr: 1.2346e-05 - 141s/epoch - 722ms/step
Epoch 705/1000
2023-09-30 11:57:22.425 
Epoch 705/1000 
	 loss: 30.8569, MinusLogProbMetric: 30.8569, val_loss: 31.3646, val_MinusLogProbMetric: 31.3646

Epoch 705: val_loss did not improve from 31.18634
196/196 - 140s - loss: 30.8569 - MinusLogProbMetric: 30.8569 - val_loss: 31.3646 - val_MinusLogProbMetric: 31.3646 - lr: 1.2346e-05 - 140s/epoch - 714ms/step
Epoch 706/1000
2023-09-30 11:59:47.749 
Epoch 706/1000 
	 loss: 30.8170, MinusLogProbMetric: 30.8170, val_loss: 31.3347, val_MinusLogProbMetric: 31.3347

Epoch 706: val_loss did not improve from 31.18634
196/196 - 145s - loss: 30.8170 - MinusLogProbMetric: 30.8170 - val_loss: 31.3347 - val_MinusLogProbMetric: 31.3347 - lr: 1.2346e-05 - 145s/epoch - 741ms/step
Epoch 707/1000
2023-09-30 12:02:10.217 
Epoch 707/1000 
	 loss: 30.8741, MinusLogProbMetric: 30.8741, val_loss: 31.6564, val_MinusLogProbMetric: 31.6564

Epoch 707: val_loss did not improve from 31.18634
196/196 - 142s - loss: 30.8741 - MinusLogProbMetric: 30.8741 - val_loss: 31.6564 - val_MinusLogProbMetric: 31.6564 - lr: 1.2346e-05 - 142s/epoch - 727ms/step
Epoch 708/1000
2023-09-30 12:04:30.568 
Epoch 708/1000 
	 loss: 30.8553, MinusLogProbMetric: 30.8553, val_loss: 31.5228, val_MinusLogProbMetric: 31.5228

Epoch 708: val_loss did not improve from 31.18634
196/196 - 140s - loss: 30.8553 - MinusLogProbMetric: 30.8553 - val_loss: 31.5228 - val_MinusLogProbMetric: 31.5228 - lr: 1.2346e-05 - 140s/epoch - 716ms/step
Epoch 709/1000
2023-09-30 12:06:56.932 
Epoch 709/1000 
	 loss: 31.0932, MinusLogProbMetric: 31.0932, val_loss: 31.3147, val_MinusLogProbMetric: 31.3147

Epoch 709: val_loss did not improve from 31.18634
196/196 - 146s - loss: 31.0932 - MinusLogProbMetric: 31.0932 - val_loss: 31.3147 - val_MinusLogProbMetric: 31.3147 - lr: 1.2346e-05 - 146s/epoch - 747ms/step
Epoch 710/1000
2023-09-30 12:09:16.273 
Epoch 710/1000 
	 loss: 30.8493, MinusLogProbMetric: 30.8493, val_loss: 31.2616, val_MinusLogProbMetric: 31.2616

Epoch 710: val_loss did not improve from 31.18634
196/196 - 139s - loss: 30.8493 - MinusLogProbMetric: 30.8493 - val_loss: 31.2616 - val_MinusLogProbMetric: 31.2616 - lr: 1.2346e-05 - 139s/epoch - 711ms/step
Epoch 711/1000
2023-09-30 12:11:34.228 
Epoch 711/1000 
	 loss: 30.8394, MinusLogProbMetric: 30.8394, val_loss: 31.2026, val_MinusLogProbMetric: 31.2026

Epoch 711: val_loss did not improve from 31.18634
196/196 - 138s - loss: 30.8394 - MinusLogProbMetric: 30.8394 - val_loss: 31.2026 - val_MinusLogProbMetric: 31.2026 - lr: 1.2346e-05 - 138s/epoch - 703ms/step
Epoch 712/1000
2023-09-30 12:14:02.712 
Epoch 712/1000 
	 loss: 30.7970, MinusLogProbMetric: 30.7970, val_loss: 31.2442, val_MinusLogProbMetric: 31.2442

Epoch 712: val_loss did not improve from 31.18634
196/196 - 148s - loss: 30.7970 - MinusLogProbMetric: 30.7970 - val_loss: 31.2442 - val_MinusLogProbMetric: 31.2442 - lr: 1.2346e-05 - 148s/epoch - 757ms/step
Epoch 713/1000
2023-09-30 12:16:33.758 
Epoch 713/1000 
	 loss: 30.8149, MinusLogProbMetric: 30.8149, val_loss: 31.4675, val_MinusLogProbMetric: 31.4675

Epoch 713: val_loss did not improve from 31.18634
196/196 - 151s - loss: 30.8149 - MinusLogProbMetric: 30.8149 - val_loss: 31.4675 - val_MinusLogProbMetric: 31.4675 - lr: 1.2346e-05 - 151s/epoch - 771ms/step
Epoch 714/1000
2023-09-30 12:18:54.913 
Epoch 714/1000 
	 loss: 30.8002, MinusLogProbMetric: 30.8002, val_loss: 31.2024, val_MinusLogProbMetric: 31.2024

Epoch 714: val_loss did not improve from 31.18634
196/196 - 141s - loss: 30.8002 - MinusLogProbMetric: 30.8002 - val_loss: 31.2024 - val_MinusLogProbMetric: 31.2024 - lr: 1.2346e-05 - 141s/epoch - 720ms/step
Epoch 715/1000
2023-09-30 12:21:13.553 
Epoch 715/1000 
	 loss: 30.7988, MinusLogProbMetric: 30.7988, val_loss: 31.3343, val_MinusLogProbMetric: 31.3343

Epoch 715: val_loss did not improve from 31.18634
196/196 - 139s - loss: 30.7988 - MinusLogProbMetric: 30.7988 - val_loss: 31.3343 - val_MinusLogProbMetric: 31.3343 - lr: 1.2346e-05 - 139s/epoch - 707ms/step
Epoch 716/1000
2023-09-30 12:23:36.630 
Epoch 716/1000 
	 loss: 30.8205, MinusLogProbMetric: 30.8205, val_loss: 31.4021, val_MinusLogProbMetric: 31.4021

Epoch 716: val_loss did not improve from 31.18634
196/196 - 143s - loss: 30.8205 - MinusLogProbMetric: 30.8205 - val_loss: 31.4021 - val_MinusLogProbMetric: 31.4021 - lr: 1.2346e-05 - 143s/epoch - 730ms/step
Epoch 717/1000
2023-09-30 12:25:56.405 
Epoch 717/1000 
	 loss: 30.8207, MinusLogProbMetric: 30.8207, val_loss: 31.2433, val_MinusLogProbMetric: 31.2433

Epoch 717: val_loss did not improve from 31.18634
196/196 - 140s - loss: 30.8207 - MinusLogProbMetric: 30.8207 - val_loss: 31.2433 - val_MinusLogProbMetric: 31.2433 - lr: 1.2346e-05 - 140s/epoch - 713ms/step
Epoch 718/1000
2023-09-30 12:28:08.888 
Epoch 718/1000 
	 loss: 30.7880, MinusLogProbMetric: 30.7880, val_loss: 31.3485, val_MinusLogProbMetric: 31.3485

Epoch 718: val_loss did not improve from 31.18634
196/196 - 132s - loss: 30.7880 - MinusLogProbMetric: 30.7880 - val_loss: 31.3485 - val_MinusLogProbMetric: 31.3485 - lr: 1.2346e-05 - 132s/epoch - 676ms/step
Epoch 719/1000
2023-09-30 12:30:19.594 
Epoch 719/1000 
	 loss: 30.7752, MinusLogProbMetric: 30.7752, val_loss: 31.2869, val_MinusLogProbMetric: 31.2869

Epoch 719: val_loss did not improve from 31.18634
196/196 - 131s - loss: 30.7752 - MinusLogProbMetric: 30.7752 - val_loss: 31.2869 - val_MinusLogProbMetric: 31.2869 - lr: 1.2346e-05 - 131s/epoch - 667ms/step
Epoch 720/1000
2023-09-30 12:32:34.877 
Epoch 720/1000 
	 loss: 30.8028, MinusLogProbMetric: 30.8028, val_loss: 31.2189, val_MinusLogProbMetric: 31.2189

Epoch 720: val_loss did not improve from 31.18634
196/196 - 135s - loss: 30.8028 - MinusLogProbMetric: 30.8028 - val_loss: 31.2189 - val_MinusLogProbMetric: 31.2189 - lr: 1.2346e-05 - 135s/epoch - 690ms/step
Epoch 721/1000
2023-09-30 12:34:44.411 
Epoch 721/1000 
	 loss: 30.8033, MinusLogProbMetric: 30.8033, val_loss: 31.3378, val_MinusLogProbMetric: 31.3378

Epoch 721: val_loss did not improve from 31.18634
196/196 - 130s - loss: 30.8033 - MinusLogProbMetric: 30.8033 - val_loss: 31.3378 - val_MinusLogProbMetric: 31.3378 - lr: 1.2346e-05 - 130s/epoch - 661ms/step
Epoch 722/1000
2023-09-30 12:36:49.937 
Epoch 722/1000 
	 loss: 30.7812, MinusLogProbMetric: 30.7812, val_loss: 31.2742, val_MinusLogProbMetric: 31.2742

Epoch 722: val_loss did not improve from 31.18634
196/196 - 126s - loss: 30.7812 - MinusLogProbMetric: 30.7812 - val_loss: 31.2742 - val_MinusLogProbMetric: 31.2742 - lr: 1.2346e-05 - 126s/epoch - 640ms/step
Epoch 723/1000
2023-09-30 12:39:05.505 
Epoch 723/1000 
	 loss: 30.7916, MinusLogProbMetric: 30.7916, val_loss: 31.4694, val_MinusLogProbMetric: 31.4694

Epoch 723: val_loss did not improve from 31.18634
196/196 - 136s - loss: 30.7916 - MinusLogProbMetric: 30.7916 - val_loss: 31.4694 - val_MinusLogProbMetric: 31.4694 - lr: 1.2346e-05 - 136s/epoch - 692ms/step
Epoch 724/1000
2023-09-30 12:41:16.089 
Epoch 724/1000 
	 loss: 30.8097, MinusLogProbMetric: 30.8097, val_loss: 31.3033, val_MinusLogProbMetric: 31.3033

Epoch 724: val_loss did not improve from 31.18634
196/196 - 131s - loss: 30.8097 - MinusLogProbMetric: 30.8097 - val_loss: 31.3033 - val_MinusLogProbMetric: 31.3033 - lr: 1.2346e-05 - 131s/epoch - 666ms/step
Epoch 725/1000
2023-09-30 12:43:26.575 
Epoch 725/1000 
	 loss: 30.7751, MinusLogProbMetric: 30.7751, val_loss: 31.1624, val_MinusLogProbMetric: 31.1624

Epoch 725: val_loss improved from 31.18634 to 31.16244, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 135s - loss: 30.7751 - MinusLogProbMetric: 30.7751 - val_loss: 31.1624 - val_MinusLogProbMetric: 31.1624 - lr: 1.2346e-05 - 135s/epoch - 691ms/step
Epoch 726/1000
2023-09-30 12:45:43.214 
Epoch 726/1000 
	 loss: 30.7513, MinusLogProbMetric: 30.7513, val_loss: 31.2519, val_MinusLogProbMetric: 31.2519

Epoch 726: val_loss did not improve from 31.16244
196/196 - 132s - loss: 30.7513 - MinusLogProbMetric: 30.7513 - val_loss: 31.2519 - val_MinusLogProbMetric: 31.2519 - lr: 1.2346e-05 - 132s/epoch - 672ms/step
Epoch 727/1000
2023-09-30 12:47:55.984 
Epoch 727/1000 
	 loss: 30.7604, MinusLogProbMetric: 30.7604, val_loss: 31.3773, val_MinusLogProbMetric: 31.3773

Epoch 727: val_loss did not improve from 31.16244
196/196 - 133s - loss: 30.7604 - MinusLogProbMetric: 30.7604 - val_loss: 31.3773 - val_MinusLogProbMetric: 31.3773 - lr: 1.2346e-05 - 133s/epoch - 678ms/step
Epoch 728/1000
2023-09-30 12:50:09.363 
Epoch 728/1000 
	 loss: 30.7793, MinusLogProbMetric: 30.7793, val_loss: 31.2624, val_MinusLogProbMetric: 31.2624

Epoch 728: val_loss did not improve from 31.16244
196/196 - 133s - loss: 30.7793 - MinusLogProbMetric: 30.7793 - val_loss: 31.2624 - val_MinusLogProbMetric: 31.2624 - lr: 1.2346e-05 - 133s/epoch - 680ms/step
Epoch 729/1000
2023-09-30 12:52:19.965 
Epoch 729/1000 
	 loss: 30.7934, MinusLogProbMetric: 30.7934, val_loss: 31.4159, val_MinusLogProbMetric: 31.4159

Epoch 729: val_loss did not improve from 31.16244
196/196 - 131s - loss: 30.7934 - MinusLogProbMetric: 30.7934 - val_loss: 31.4159 - val_MinusLogProbMetric: 31.4159 - lr: 1.2346e-05 - 131s/epoch - 666ms/step
Epoch 730/1000
2023-09-30 12:54:31.936 
Epoch 730/1000 
	 loss: 30.7847, MinusLogProbMetric: 30.7847, val_loss: 31.2487, val_MinusLogProbMetric: 31.2487

Epoch 730: val_loss did not improve from 31.16244
196/196 - 132s - loss: 30.7847 - MinusLogProbMetric: 30.7847 - val_loss: 31.2487 - val_MinusLogProbMetric: 31.2487 - lr: 1.2346e-05 - 132s/epoch - 673ms/step
Epoch 731/1000
2023-09-30 12:56:35.219 
Epoch 731/1000 
	 loss: 30.7780, MinusLogProbMetric: 30.7780, val_loss: 31.2050, val_MinusLogProbMetric: 31.2050

Epoch 731: val_loss did not improve from 31.16244
196/196 - 123s - loss: 30.7780 - MinusLogProbMetric: 30.7780 - val_loss: 31.2050 - val_MinusLogProbMetric: 31.2050 - lr: 1.2346e-05 - 123s/epoch - 629ms/step
Epoch 732/1000
2023-09-30 12:58:43.109 
Epoch 732/1000 
	 loss: 30.7785, MinusLogProbMetric: 30.7785, val_loss: 31.2571, val_MinusLogProbMetric: 31.2571

Epoch 732: val_loss did not improve from 31.16244
196/196 - 128s - loss: 30.7785 - MinusLogProbMetric: 30.7785 - val_loss: 31.2571 - val_MinusLogProbMetric: 31.2571 - lr: 1.2346e-05 - 128s/epoch - 653ms/step
Epoch 733/1000
2023-09-30 13:00:45.271 
Epoch 733/1000 
	 loss: 30.7636, MinusLogProbMetric: 30.7636, val_loss: 31.1704, val_MinusLogProbMetric: 31.1704

Epoch 733: val_loss did not improve from 31.16244
196/196 - 122s - loss: 30.7636 - MinusLogProbMetric: 30.7636 - val_loss: 31.1704 - val_MinusLogProbMetric: 31.1704 - lr: 1.2346e-05 - 122s/epoch - 623ms/step
Epoch 734/1000
2023-09-30 13:02:49.691 
Epoch 734/1000 
	 loss: 30.7556, MinusLogProbMetric: 30.7556, val_loss: 31.2284, val_MinusLogProbMetric: 31.2284

Epoch 734: val_loss did not improve from 31.16244
196/196 - 124s - loss: 30.7556 - MinusLogProbMetric: 30.7556 - val_loss: 31.2284 - val_MinusLogProbMetric: 31.2284 - lr: 1.2346e-05 - 124s/epoch - 635ms/step
Epoch 735/1000
2023-09-30 13:04:52.097 
Epoch 735/1000 
	 loss: 30.7466, MinusLogProbMetric: 30.7466, val_loss: 31.4506, val_MinusLogProbMetric: 31.4506

Epoch 735: val_loss did not improve from 31.16244
196/196 - 122s - loss: 30.7466 - MinusLogProbMetric: 30.7466 - val_loss: 31.4506 - val_MinusLogProbMetric: 31.4506 - lr: 1.2346e-05 - 122s/epoch - 625ms/step
Epoch 736/1000
2023-09-30 13:06:54.247 
Epoch 736/1000 
	 loss: 30.7705, MinusLogProbMetric: 30.7705, val_loss: 31.2751, val_MinusLogProbMetric: 31.2751

Epoch 736: val_loss did not improve from 31.16244
196/196 - 122s - loss: 30.7705 - MinusLogProbMetric: 30.7705 - val_loss: 31.2751 - val_MinusLogProbMetric: 31.2751 - lr: 1.2346e-05 - 122s/epoch - 623ms/step
Epoch 737/1000
2023-09-30 13:09:00.819 
Epoch 737/1000 
	 loss: 30.7518, MinusLogProbMetric: 30.7518, val_loss: 31.3802, val_MinusLogProbMetric: 31.3802

Epoch 737: val_loss did not improve from 31.16244
196/196 - 127s - loss: 30.7518 - MinusLogProbMetric: 30.7518 - val_loss: 31.3802 - val_MinusLogProbMetric: 31.3802 - lr: 1.2346e-05 - 127s/epoch - 646ms/step
Epoch 738/1000
2023-09-30 13:11:07.288 
Epoch 738/1000 
	 loss: 30.7256, MinusLogProbMetric: 30.7256, val_loss: 31.2990, val_MinusLogProbMetric: 31.2990

Epoch 738: val_loss did not improve from 31.16244
196/196 - 126s - loss: 30.7256 - MinusLogProbMetric: 30.7256 - val_loss: 31.2990 - val_MinusLogProbMetric: 31.2990 - lr: 1.2346e-05 - 126s/epoch - 645ms/step
Epoch 739/1000
2023-09-30 13:13:08.660 
Epoch 739/1000 
	 loss: 30.7420, MinusLogProbMetric: 30.7420, val_loss: 31.1460, val_MinusLogProbMetric: 31.1460

Epoch 739: val_loss improved from 31.16244 to 31.14601, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 125s - loss: 30.7420 - MinusLogProbMetric: 30.7420 - val_loss: 31.1460 - val_MinusLogProbMetric: 31.1460 - lr: 1.2346e-05 - 125s/epoch - 638ms/step
Epoch 740/1000
2023-09-30 13:15:14.724 
Epoch 740/1000 
	 loss: 30.7211, MinusLogProbMetric: 30.7211, val_loss: 31.3009, val_MinusLogProbMetric: 31.3009

Epoch 740: val_loss did not improve from 31.14601
196/196 - 122s - loss: 30.7211 - MinusLogProbMetric: 30.7211 - val_loss: 31.3009 - val_MinusLogProbMetric: 31.3009 - lr: 1.2346e-05 - 122s/epoch - 624ms/step
Epoch 741/1000
2023-09-30 13:17:14.001 
Epoch 741/1000 
	 loss: 30.7396, MinusLogProbMetric: 30.7396, val_loss: 31.2728, val_MinusLogProbMetric: 31.2728

Epoch 741: val_loss did not improve from 31.14601
196/196 - 119s - loss: 30.7396 - MinusLogProbMetric: 30.7396 - val_loss: 31.2728 - val_MinusLogProbMetric: 31.2728 - lr: 1.2346e-05 - 119s/epoch - 609ms/step
Epoch 742/1000
2023-09-30 13:19:11.867 
Epoch 742/1000 
	 loss: 59.7359, MinusLogProbMetric: 59.7359, val_loss: 59.1105, val_MinusLogProbMetric: 59.1105

Epoch 742: val_loss did not improve from 31.14601
196/196 - 118s - loss: 59.7359 - MinusLogProbMetric: 59.7359 - val_loss: 59.1105 - val_MinusLogProbMetric: 59.1105 - lr: 1.2346e-05 - 118s/epoch - 601ms/step
Epoch 743/1000
2023-09-30 13:21:11.868 
Epoch 743/1000 
	 loss: 43.9271, MinusLogProbMetric: 43.9271, val_loss: 40.5453, val_MinusLogProbMetric: 40.5453

Epoch 743: val_loss did not improve from 31.14601
196/196 - 120s - loss: 43.9271 - MinusLogProbMetric: 43.9271 - val_loss: 40.5453 - val_MinusLogProbMetric: 40.5453 - lr: 1.2346e-05 - 120s/epoch - 612ms/step
Epoch 744/1000
2023-09-30 13:23:10.235 
Epoch 744/1000 
	 loss: 37.6332, MinusLogProbMetric: 37.6332, val_loss: 36.4711, val_MinusLogProbMetric: 36.4711

Epoch 744: val_loss did not improve from 31.14601
196/196 - 118s - loss: 37.6332 - MinusLogProbMetric: 37.6332 - val_loss: 36.4711 - val_MinusLogProbMetric: 36.4711 - lr: 1.2346e-05 - 118s/epoch - 604ms/step
Epoch 745/1000
2023-09-30 13:25:11.624 
Epoch 745/1000 
	 loss: 35.2686, MinusLogProbMetric: 35.2686, val_loss: 35.2525, val_MinusLogProbMetric: 35.2525

Epoch 745: val_loss did not improve from 31.14601
196/196 - 121s - loss: 35.2686 - MinusLogProbMetric: 35.2686 - val_loss: 35.2525 - val_MinusLogProbMetric: 35.2525 - lr: 1.2346e-05 - 121s/epoch - 620ms/step
Epoch 746/1000
2023-09-30 13:27:12.130 
Epoch 746/1000 
	 loss: 71.3892, MinusLogProbMetric: 71.3892, val_loss: 167.6693, val_MinusLogProbMetric: 167.6693

Epoch 746: val_loss did not improve from 31.14601
196/196 - 120s - loss: 71.3892 - MinusLogProbMetric: 71.3892 - val_loss: 167.6693 - val_MinusLogProbMetric: 167.6693 - lr: 1.2346e-05 - 120s/epoch - 614ms/step
Epoch 747/1000
2023-09-30 13:29:09.928 
Epoch 747/1000 
	 loss: 85.2178, MinusLogProbMetric: 85.2178, val_loss: 570.8440, val_MinusLogProbMetric: 570.8440

Epoch 747: val_loss did not improve from 31.14601
196/196 - 118s - loss: 85.2178 - MinusLogProbMetric: 85.2178 - val_loss: 570.8440 - val_MinusLogProbMetric: 570.8440 - lr: 1.2346e-05 - 118s/epoch - 601ms/step
Epoch 748/1000
2023-09-30 13:31:20.155 
Epoch 748/1000 
	 loss: 142.4035, MinusLogProbMetric: 142.4035, val_loss: 89.3094, val_MinusLogProbMetric: 89.3094

Epoch 748: val_loss did not improve from 31.14601
196/196 - 130s - loss: 142.4035 - MinusLogProbMetric: 142.4035 - val_loss: 89.3094 - val_MinusLogProbMetric: 89.3094 - lr: 1.2346e-05 - 130s/epoch - 664ms/step
Epoch 749/1000
2023-09-30 13:33:20.550 
Epoch 749/1000 
	 loss: 78.3754, MinusLogProbMetric: 78.3754, val_loss: 72.1517, val_MinusLogProbMetric: 72.1517

Epoch 749: val_loss did not improve from 31.14601
196/196 - 120s - loss: 78.3754 - MinusLogProbMetric: 78.3754 - val_loss: 72.1517 - val_MinusLogProbMetric: 72.1517 - lr: 1.2346e-05 - 120s/epoch - 614ms/step
Epoch 750/1000
2023-09-30 13:35:21.666 
Epoch 750/1000 
	 loss: 68.3295, MinusLogProbMetric: 68.3295, val_loss: 65.0627, val_MinusLogProbMetric: 65.0627

Epoch 750: val_loss did not improve from 31.14601
196/196 - 121s - loss: 68.3295 - MinusLogProbMetric: 68.3295 - val_loss: 65.0627 - val_MinusLogProbMetric: 65.0627 - lr: 1.2346e-05 - 121s/epoch - 618ms/step
Epoch 751/1000
2023-09-30 13:37:28.647 
Epoch 751/1000 
	 loss: 61.7360, MinusLogProbMetric: 61.7360, val_loss: 59.4853, val_MinusLogProbMetric: 59.4853

Epoch 751: val_loss did not improve from 31.14601
196/196 - 127s - loss: 61.7360 - MinusLogProbMetric: 61.7360 - val_loss: 59.4853 - val_MinusLogProbMetric: 59.4853 - lr: 1.2346e-05 - 127s/epoch - 648ms/step
Epoch 752/1000
2023-09-30 13:39:28.250 
Epoch 752/1000 
	 loss: 58.7020, MinusLogProbMetric: 58.7020, val_loss: 56.3314, val_MinusLogProbMetric: 56.3314

Epoch 752: val_loss did not improve from 31.14601
196/196 - 120s - loss: 58.7020 - MinusLogProbMetric: 58.7020 - val_loss: 56.3314 - val_MinusLogProbMetric: 56.3314 - lr: 1.2346e-05 - 120s/epoch - 610ms/step
Epoch 753/1000
2023-09-30 13:41:33.482 
Epoch 753/1000 
	 loss: 54.9907, MinusLogProbMetric: 54.9907, val_loss: 53.9242, val_MinusLogProbMetric: 53.9242

Epoch 753: val_loss did not improve from 31.14601
196/196 - 125s - loss: 54.9907 - MinusLogProbMetric: 54.9907 - val_loss: 53.9242 - val_MinusLogProbMetric: 53.9242 - lr: 1.2346e-05 - 125s/epoch - 639ms/step
Epoch 754/1000
2023-09-30 13:43:36.951 
Epoch 754/1000 
	 loss: 52.9180, MinusLogProbMetric: 52.9180, val_loss: 52.2663, val_MinusLogProbMetric: 52.2663

Epoch 754: val_loss did not improve from 31.14601
196/196 - 123s - loss: 52.9180 - MinusLogProbMetric: 52.9180 - val_loss: 52.2663 - val_MinusLogProbMetric: 52.2663 - lr: 1.2346e-05 - 123s/epoch - 630ms/step
Epoch 755/1000
2023-09-30 13:45:39.000 
Epoch 755/1000 
	 loss: 51.3465, MinusLogProbMetric: 51.3465, val_loss: 50.8474, val_MinusLogProbMetric: 50.8474

Epoch 755: val_loss did not improve from 31.14601
196/196 - 122s - loss: 51.3465 - MinusLogProbMetric: 51.3465 - val_loss: 50.8474 - val_MinusLogProbMetric: 50.8474 - lr: 1.2346e-05 - 122s/epoch - 623ms/step
Epoch 756/1000
2023-09-30 13:47:36.144 
Epoch 756/1000 
	 loss: 49.8526, MinusLogProbMetric: 49.8526, val_loss: 49.2366, val_MinusLogProbMetric: 49.2366

Epoch 756: val_loss did not improve from 31.14601
196/196 - 117s - loss: 49.8526 - MinusLogProbMetric: 49.8526 - val_loss: 49.2366 - val_MinusLogProbMetric: 49.2366 - lr: 1.2346e-05 - 117s/epoch - 598ms/step
Epoch 757/1000
2023-09-30 13:49:37.421 
Epoch 757/1000 
	 loss: 66.6848, MinusLogProbMetric: 66.6848, val_loss: 78.7202, val_MinusLogProbMetric: 78.7202

Epoch 757: val_loss did not improve from 31.14601
196/196 - 121s - loss: 66.6848 - MinusLogProbMetric: 66.6848 - val_loss: 78.7202 - val_MinusLogProbMetric: 78.7202 - lr: 1.2346e-05 - 121s/epoch - 619ms/step
Epoch 758/1000
2023-09-30 13:51:39.713 
Epoch 758/1000 
	 loss: 63.0605, MinusLogProbMetric: 63.0605, val_loss: 56.3135, val_MinusLogProbMetric: 56.3135

Epoch 758: val_loss did not improve from 31.14601
196/196 - 122s - loss: 63.0605 - MinusLogProbMetric: 63.0605 - val_loss: 56.3135 - val_MinusLogProbMetric: 56.3135 - lr: 1.2346e-05 - 122s/epoch - 624ms/step
Epoch 759/1000
2023-09-30 13:53:44.956 
Epoch 759/1000 
	 loss: 58.6571, MinusLogProbMetric: 58.6571, val_loss: 57.1445, val_MinusLogProbMetric: 57.1445

Epoch 759: val_loss did not improve from 31.14601
196/196 - 125s - loss: 58.6571 - MinusLogProbMetric: 58.6571 - val_loss: 57.1445 - val_MinusLogProbMetric: 57.1445 - lr: 1.2346e-05 - 125s/epoch - 639ms/step
Epoch 760/1000
2023-09-30 13:55:51.282 
Epoch 760/1000 
	 loss: 51.6196, MinusLogProbMetric: 51.6196, val_loss: 49.4618, val_MinusLogProbMetric: 49.4618

Epoch 760: val_loss did not improve from 31.14601
196/196 - 126s - loss: 51.6196 - MinusLogProbMetric: 51.6196 - val_loss: 49.4618 - val_MinusLogProbMetric: 49.4618 - lr: 1.2346e-05 - 126s/epoch - 645ms/step
Epoch 761/1000
2023-09-30 13:57:56.823 
Epoch 761/1000 
	 loss: 48.5520, MinusLogProbMetric: 48.5520, val_loss: 48.0350, val_MinusLogProbMetric: 48.0350

Epoch 761: val_loss did not improve from 31.14601
196/196 - 125s - loss: 48.5520 - MinusLogProbMetric: 48.5520 - val_loss: 48.0350 - val_MinusLogProbMetric: 48.0350 - lr: 1.2346e-05 - 125s/epoch - 640ms/step
Epoch 762/1000
2023-09-30 13:59:57.594 
Epoch 762/1000 
	 loss: 47.3178, MinusLogProbMetric: 47.3178, val_loss: 47.2438, val_MinusLogProbMetric: 47.2438

Epoch 762: val_loss did not improve from 31.14601
196/196 - 121s - loss: 47.3178 - MinusLogProbMetric: 47.3178 - val_loss: 47.2438 - val_MinusLogProbMetric: 47.2438 - lr: 1.2346e-05 - 121s/epoch - 616ms/step
Epoch 763/1000
2023-09-30 14:02:07.180 
Epoch 763/1000 
	 loss: 46.3934, MinusLogProbMetric: 46.3934, val_loss: 46.0552, val_MinusLogProbMetric: 46.0552

Epoch 763: val_loss did not improve from 31.14601
196/196 - 130s - loss: 46.3934 - MinusLogProbMetric: 46.3934 - val_loss: 46.0552 - val_MinusLogProbMetric: 46.0552 - lr: 1.2346e-05 - 130s/epoch - 661ms/step
Epoch 764/1000
2023-09-30 14:04:15.233 
Epoch 764/1000 
	 loss: 45.5334, MinusLogProbMetric: 45.5334, val_loss: 45.3270, val_MinusLogProbMetric: 45.3270

Epoch 764: val_loss did not improve from 31.14601
196/196 - 128s - loss: 45.5334 - MinusLogProbMetric: 45.5334 - val_loss: 45.3270 - val_MinusLogProbMetric: 45.3270 - lr: 1.2346e-05 - 128s/epoch - 653ms/step
Epoch 765/1000
2023-09-30 14:06:12.183 
Epoch 765/1000 
	 loss: 44.8679, MinusLogProbMetric: 44.8679, val_loss: 45.0256, val_MinusLogProbMetric: 45.0256

Epoch 765: val_loss did not improve from 31.14601
196/196 - 117s - loss: 44.8679 - MinusLogProbMetric: 44.8679 - val_loss: 45.0256 - val_MinusLogProbMetric: 45.0256 - lr: 1.2346e-05 - 117s/epoch - 597ms/step
Epoch 766/1000
2023-09-30 14:08:13.641 
Epoch 766/1000 
	 loss: 44.3084, MinusLogProbMetric: 44.3084, val_loss: 44.3599, val_MinusLogProbMetric: 44.3599

Epoch 766: val_loss did not improve from 31.14601
196/196 - 121s - loss: 44.3084 - MinusLogProbMetric: 44.3084 - val_loss: 44.3599 - val_MinusLogProbMetric: 44.3599 - lr: 1.2346e-05 - 121s/epoch - 619ms/step
Epoch 767/1000
2023-09-30 14:10:16.141 
Epoch 767/1000 
	 loss: 43.8047, MinusLogProbMetric: 43.8047, val_loss: 43.8306, val_MinusLogProbMetric: 43.8306

Epoch 767: val_loss did not improve from 31.14601
196/196 - 123s - loss: 43.8047 - MinusLogProbMetric: 43.8047 - val_loss: 43.8306 - val_MinusLogProbMetric: 43.8306 - lr: 1.2346e-05 - 123s/epoch - 625ms/step
Epoch 768/1000
2023-09-30 14:12:12.564 
Epoch 768/1000 
	 loss: 43.3600, MinusLogProbMetric: 43.3600, val_loss: 43.5557, val_MinusLogProbMetric: 43.5557

Epoch 768: val_loss did not improve from 31.14601
196/196 - 116s - loss: 43.3600 - MinusLogProbMetric: 43.3600 - val_loss: 43.5557 - val_MinusLogProbMetric: 43.5557 - lr: 1.2346e-05 - 116s/epoch - 594ms/step
Epoch 769/1000
2023-09-30 14:14:05.619 
Epoch 769/1000 
	 loss: 42.9710, MinusLogProbMetric: 42.9710, val_loss: 42.9229, val_MinusLogProbMetric: 42.9229

Epoch 769: val_loss did not improve from 31.14601
196/196 - 113s - loss: 42.9710 - MinusLogProbMetric: 42.9710 - val_loss: 42.9229 - val_MinusLogProbMetric: 42.9229 - lr: 1.2346e-05 - 113s/epoch - 577ms/step
Epoch 770/1000
2023-09-30 14:16:02.586 
Epoch 770/1000 
	 loss: 42.5241, MinusLogProbMetric: 42.5241, val_loss: 42.6972, val_MinusLogProbMetric: 42.6972

Epoch 770: val_loss did not improve from 31.14601
196/196 - 117s - loss: 42.5241 - MinusLogProbMetric: 42.5241 - val_loss: 42.6972 - val_MinusLogProbMetric: 42.6972 - lr: 1.2346e-05 - 117s/epoch - 597ms/step
Epoch 771/1000
2023-09-30 14:17:57.357 
Epoch 771/1000 
	 loss: 42.1914, MinusLogProbMetric: 42.1914, val_loss: 42.2113, val_MinusLogProbMetric: 42.2113

Epoch 771: val_loss did not improve from 31.14601
196/196 - 115s - loss: 42.1914 - MinusLogProbMetric: 42.1914 - val_loss: 42.2113 - val_MinusLogProbMetric: 42.2113 - lr: 1.2346e-05 - 115s/epoch - 586ms/step
Epoch 772/1000
2023-09-30 14:19:50.508 
Epoch 772/1000 
	 loss: 41.8537, MinusLogProbMetric: 41.8537, val_loss: 41.8060, val_MinusLogProbMetric: 41.8060

Epoch 772: val_loss did not improve from 31.14601
196/196 - 113s - loss: 41.8537 - MinusLogProbMetric: 41.8537 - val_loss: 41.8060 - val_MinusLogProbMetric: 41.8060 - lr: 1.2346e-05 - 113s/epoch - 577ms/step
Epoch 773/1000
2023-09-30 14:21:46.102 
Epoch 773/1000 
	 loss: 41.4461, MinusLogProbMetric: 41.4461, val_loss: 41.5406, val_MinusLogProbMetric: 41.5406

Epoch 773: val_loss did not improve from 31.14601
196/196 - 116s - loss: 41.4461 - MinusLogProbMetric: 41.4461 - val_loss: 41.5406 - val_MinusLogProbMetric: 41.5406 - lr: 1.2346e-05 - 116s/epoch - 589ms/step
Epoch 774/1000
2023-09-30 14:23:42.398 
Epoch 774/1000 
	 loss: 40.9247, MinusLogProbMetric: 40.9247, val_loss: 40.8629, val_MinusLogProbMetric: 40.8629

Epoch 774: val_loss did not improve from 31.14601
196/196 - 116s - loss: 40.9247 - MinusLogProbMetric: 40.9247 - val_loss: 40.8629 - val_MinusLogProbMetric: 40.8629 - lr: 1.2346e-05 - 116s/epoch - 593ms/step
Epoch 775/1000
2023-09-30 14:25:35.460 
Epoch 775/1000 
	 loss: 40.2965, MinusLogProbMetric: 40.2965, val_loss: 40.3750, val_MinusLogProbMetric: 40.3750

Epoch 775: val_loss did not improve from 31.14601
196/196 - 113s - loss: 40.2965 - MinusLogProbMetric: 40.2965 - val_loss: 40.3750 - val_MinusLogProbMetric: 40.3750 - lr: 1.2346e-05 - 113s/epoch - 577ms/step
Epoch 776/1000
2023-09-30 14:27:33.643 
Epoch 776/1000 
	 loss: 39.9218, MinusLogProbMetric: 39.9218, val_loss: 40.0871, val_MinusLogProbMetric: 40.0871

Epoch 776: val_loss did not improve from 31.14601
196/196 - 118s - loss: 39.9218 - MinusLogProbMetric: 39.9218 - val_loss: 40.0871 - val_MinusLogProbMetric: 40.0871 - lr: 1.2346e-05 - 118s/epoch - 603ms/step
Epoch 777/1000
2023-09-30 14:29:28.111 
Epoch 777/1000 
	 loss: 39.6963, MinusLogProbMetric: 39.6963, val_loss: 39.7527, val_MinusLogProbMetric: 39.7527

Epoch 777: val_loss did not improve from 31.14601
196/196 - 114s - loss: 39.6963 - MinusLogProbMetric: 39.6963 - val_loss: 39.7527 - val_MinusLogProbMetric: 39.7527 - lr: 1.2346e-05 - 114s/epoch - 584ms/step
Epoch 778/1000
2023-09-30 14:31:20.455 
Epoch 778/1000 
	 loss: 39.4083, MinusLogProbMetric: 39.4083, val_loss: 39.5837, val_MinusLogProbMetric: 39.5837

Epoch 778: val_loss did not improve from 31.14601
196/196 - 112s - loss: 39.4083 - MinusLogProbMetric: 39.4083 - val_loss: 39.5837 - val_MinusLogProbMetric: 39.5837 - lr: 1.2346e-05 - 112s/epoch - 573ms/step
Epoch 779/1000
2023-09-30 14:33:14.684 
Epoch 779/1000 
	 loss: 39.0406, MinusLogProbMetric: 39.0406, val_loss: 39.1448, val_MinusLogProbMetric: 39.1448

Epoch 779: val_loss did not improve from 31.14601
196/196 - 114s - loss: 39.0406 - MinusLogProbMetric: 39.0406 - val_loss: 39.1448 - val_MinusLogProbMetric: 39.1448 - lr: 1.2346e-05 - 114s/epoch - 583ms/step
Epoch 780/1000
2023-09-30 14:35:08.653 
Epoch 780/1000 
	 loss: 38.7809, MinusLogProbMetric: 38.7809, val_loss: 38.9006, val_MinusLogProbMetric: 38.9006

Epoch 780: val_loss did not improve from 31.14601
196/196 - 114s - loss: 38.7809 - MinusLogProbMetric: 38.7809 - val_loss: 38.9006 - val_MinusLogProbMetric: 38.9006 - lr: 1.2346e-05 - 114s/epoch - 582ms/step
Epoch 781/1000
2023-09-30 14:37:05.712 
Epoch 781/1000 
	 loss: 38.5923, MinusLogProbMetric: 38.5923, val_loss: 38.6865, val_MinusLogProbMetric: 38.6865

Epoch 781: val_loss did not improve from 31.14601
196/196 - 117s - loss: 38.5923 - MinusLogProbMetric: 38.5923 - val_loss: 38.6865 - val_MinusLogProbMetric: 38.6865 - lr: 1.2346e-05 - 117s/epoch - 597ms/step
Epoch 782/1000
2023-09-30 14:39:04.109 
Epoch 782/1000 
	 loss: 38.2371, MinusLogProbMetric: 38.2371, val_loss: 38.4401, val_MinusLogProbMetric: 38.4401

Epoch 782: val_loss did not improve from 31.14601
196/196 - 118s - loss: 38.2371 - MinusLogProbMetric: 38.2371 - val_loss: 38.4401 - val_MinusLogProbMetric: 38.4401 - lr: 1.2346e-05 - 118s/epoch - 604ms/step
Epoch 783/1000
2023-09-30 14:41:01.641 
Epoch 783/1000 
	 loss: 38.0158, MinusLogProbMetric: 38.0158, val_loss: 38.1854, val_MinusLogProbMetric: 38.1854

Epoch 783: val_loss did not improve from 31.14601
196/196 - 118s - loss: 38.0158 - MinusLogProbMetric: 38.0158 - val_loss: 38.1854 - val_MinusLogProbMetric: 38.1854 - lr: 1.2346e-05 - 118s/epoch - 600ms/step
Epoch 784/1000
2023-09-30 14:42:52.676 
Epoch 784/1000 
	 loss: 37.5026, MinusLogProbMetric: 37.5026, val_loss: 37.5271, val_MinusLogProbMetric: 37.5271

Epoch 784: val_loss did not improve from 31.14601
196/196 - 111s - loss: 37.5026 - MinusLogProbMetric: 37.5026 - val_loss: 37.5271 - val_MinusLogProbMetric: 37.5271 - lr: 1.2346e-05 - 111s/epoch - 566ms/step
Epoch 785/1000
2023-09-30 14:44:48.604 
Epoch 785/1000 
	 loss: 37.0232, MinusLogProbMetric: 37.0232, val_loss: 37.0782, val_MinusLogProbMetric: 37.0782

Epoch 785: val_loss did not improve from 31.14601
196/196 - 116s - loss: 37.0232 - MinusLogProbMetric: 37.0232 - val_loss: 37.0782 - val_MinusLogProbMetric: 37.0782 - lr: 1.2346e-05 - 116s/epoch - 591ms/step
Epoch 786/1000
2023-09-30 14:46:43.704 
Epoch 786/1000 
	 loss: 36.7593, MinusLogProbMetric: 36.7593, val_loss: 36.9007, val_MinusLogProbMetric: 36.9007

Epoch 786: val_loss did not improve from 31.14601
196/196 - 115s - loss: 36.7593 - MinusLogProbMetric: 36.7593 - val_loss: 36.9007 - val_MinusLogProbMetric: 36.9007 - lr: 1.2346e-05 - 115s/epoch - 587ms/step
Epoch 787/1000
2023-09-30 14:48:33.396 
Epoch 787/1000 
	 loss: 36.5863, MinusLogProbMetric: 36.5863, val_loss: 36.7903, val_MinusLogProbMetric: 36.7903

Epoch 787: val_loss did not improve from 31.14601
196/196 - 110s - loss: 36.5863 - MinusLogProbMetric: 36.5863 - val_loss: 36.7903 - val_MinusLogProbMetric: 36.7903 - lr: 1.2346e-05 - 110s/epoch - 560ms/step
Epoch 788/1000
2023-09-30 14:50:29.248 
Epoch 788/1000 
	 loss: 36.3880, MinusLogProbMetric: 36.3880, val_loss: 36.6083, val_MinusLogProbMetric: 36.6083

Epoch 788: val_loss did not improve from 31.14601
196/196 - 116s - loss: 36.3880 - MinusLogProbMetric: 36.3880 - val_loss: 36.6083 - val_MinusLogProbMetric: 36.6083 - lr: 1.2346e-05 - 116s/epoch - 591ms/step
Epoch 789/1000
2023-09-30 14:52:25.680 
Epoch 789/1000 
	 loss: 36.2797, MinusLogProbMetric: 36.2797, val_loss: 36.5445, val_MinusLogProbMetric: 36.5445

Epoch 789: val_loss did not improve from 31.14601
196/196 - 116s - loss: 36.2797 - MinusLogProbMetric: 36.2797 - val_loss: 36.5445 - val_MinusLogProbMetric: 36.5445 - lr: 1.2346e-05 - 116s/epoch - 594ms/step
Epoch 790/1000
2023-09-30 14:54:20.879 
Epoch 790/1000 
	 loss: 35.9889, MinusLogProbMetric: 35.9889, val_loss: 36.3118, val_MinusLogProbMetric: 36.3118

Epoch 790: val_loss did not improve from 31.14601
196/196 - 115s - loss: 35.9889 - MinusLogProbMetric: 35.9889 - val_loss: 36.3118 - val_MinusLogProbMetric: 36.3118 - lr: 6.1728e-06 - 115s/epoch - 588ms/step
Epoch 791/1000
2023-09-30 14:56:23.156 
Epoch 791/1000 
	 loss: 36.1155, MinusLogProbMetric: 36.1155, val_loss: 36.2598, val_MinusLogProbMetric: 36.2598

Epoch 791: val_loss did not improve from 31.14601
196/196 - 122s - loss: 36.1155 - MinusLogProbMetric: 36.1155 - val_loss: 36.2598 - val_MinusLogProbMetric: 36.2598 - lr: 6.1728e-06 - 122s/epoch - 624ms/step
Epoch 792/1000
2023-09-30 14:58:23.618 
Epoch 792/1000 
	 loss: 35.8558, MinusLogProbMetric: 35.8558, val_loss: 36.1380, val_MinusLogProbMetric: 36.1380

Epoch 792: val_loss did not improve from 31.14601
196/196 - 120s - loss: 35.8558 - MinusLogProbMetric: 35.8558 - val_loss: 36.1380 - val_MinusLogProbMetric: 36.1380 - lr: 6.1728e-06 - 120s/epoch - 615ms/step
Epoch 793/1000
2023-09-30 15:00:25.218 
Epoch 793/1000 
	 loss: 35.7941, MinusLogProbMetric: 35.7941, val_loss: 36.0831, val_MinusLogProbMetric: 36.0831

Epoch 793: val_loss did not improve from 31.14601
196/196 - 122s - loss: 35.7941 - MinusLogProbMetric: 35.7941 - val_loss: 36.0831 - val_MinusLogProbMetric: 36.0831 - lr: 6.1728e-06 - 122s/epoch - 620ms/step
Epoch 794/1000
2023-09-30 15:02:27.209 
Epoch 794/1000 
	 loss: 35.7255, MinusLogProbMetric: 35.7255, val_loss: 36.0421, val_MinusLogProbMetric: 36.0421

Epoch 794: val_loss did not improve from 31.14601
196/196 - 122s - loss: 35.7255 - MinusLogProbMetric: 35.7255 - val_loss: 36.0421 - val_MinusLogProbMetric: 36.0421 - lr: 6.1728e-06 - 122s/epoch - 622ms/step
Epoch 795/1000
2023-09-30 15:04:25.808 
Epoch 795/1000 
	 loss: 35.6651, MinusLogProbMetric: 35.6651, val_loss: 35.9567, val_MinusLogProbMetric: 35.9567

Epoch 795: val_loss did not improve from 31.14601
196/196 - 119s - loss: 35.6651 - MinusLogProbMetric: 35.6651 - val_loss: 35.9567 - val_MinusLogProbMetric: 35.9567 - lr: 6.1728e-06 - 119s/epoch - 605ms/step
Epoch 796/1000
2023-09-30 15:06:27.698 
Epoch 796/1000 
	 loss: 35.6536, MinusLogProbMetric: 35.6536, val_loss: 35.9172, val_MinusLogProbMetric: 35.9172

Epoch 796: val_loss did not improve from 31.14601
196/196 - 122s - loss: 35.6536 - MinusLogProbMetric: 35.6536 - val_loss: 35.9172 - val_MinusLogProbMetric: 35.9172 - lr: 6.1728e-06 - 122s/epoch - 622ms/step
Epoch 797/1000
2023-09-30 15:08:22.246 
Epoch 797/1000 
	 loss: 35.5546, MinusLogProbMetric: 35.5546, val_loss: 35.8534, val_MinusLogProbMetric: 35.8534

Epoch 797: val_loss did not improve from 31.14601
196/196 - 115s - loss: 35.5546 - MinusLogProbMetric: 35.5546 - val_loss: 35.8534 - val_MinusLogProbMetric: 35.8534 - lr: 6.1728e-06 - 115s/epoch - 584ms/step
Epoch 798/1000
2023-09-30 15:10:17.233 
Epoch 798/1000 
	 loss: 35.4869, MinusLogProbMetric: 35.4869, val_loss: 35.8372, val_MinusLogProbMetric: 35.8372

Epoch 798: val_loss did not improve from 31.14601
196/196 - 115s - loss: 35.4869 - MinusLogProbMetric: 35.4869 - val_loss: 35.8372 - val_MinusLogProbMetric: 35.8372 - lr: 6.1728e-06 - 115s/epoch - 586ms/step
Epoch 799/1000
2023-09-30 15:12:11.494 
Epoch 799/1000 
	 loss: 35.4300, MinusLogProbMetric: 35.4300, val_loss: 35.7883, val_MinusLogProbMetric: 35.7883

Epoch 799: val_loss did not improve from 31.14601
196/196 - 114s - loss: 35.4300 - MinusLogProbMetric: 35.4300 - val_loss: 35.7883 - val_MinusLogProbMetric: 35.7883 - lr: 6.1728e-06 - 114s/epoch - 583ms/step
Epoch 800/1000
2023-09-30 15:14:08.480 
Epoch 800/1000 
	 loss: 35.4061, MinusLogProbMetric: 35.4061, val_loss: 35.7603, val_MinusLogProbMetric: 35.7603

Epoch 800: val_loss did not improve from 31.14601
196/196 - 117s - loss: 35.4061 - MinusLogProbMetric: 35.4061 - val_loss: 35.7603 - val_MinusLogProbMetric: 35.7603 - lr: 6.1728e-06 - 117s/epoch - 597ms/step
Epoch 801/1000
2023-09-30 15:16:04.246 
Epoch 801/1000 
	 loss: 35.3333, MinusLogProbMetric: 35.3333, val_loss: 35.7410, val_MinusLogProbMetric: 35.7410

Epoch 801: val_loss did not improve from 31.14601
196/196 - 116s - loss: 35.3333 - MinusLogProbMetric: 35.3333 - val_loss: 35.7410 - val_MinusLogProbMetric: 35.7410 - lr: 6.1728e-06 - 116s/epoch - 591ms/step
Epoch 802/1000
2023-09-30 15:17:59.953 
Epoch 802/1000 
	 loss: 35.2738, MinusLogProbMetric: 35.2738, val_loss: 35.6070, val_MinusLogProbMetric: 35.6070

Epoch 802: val_loss did not improve from 31.14601
196/196 - 116s - loss: 35.2738 - MinusLogProbMetric: 35.2738 - val_loss: 35.6070 - val_MinusLogProbMetric: 35.6070 - lr: 6.1728e-06 - 116s/epoch - 590ms/step
Epoch 803/1000
2023-09-30 15:19:54.393 
Epoch 803/1000 
	 loss: 35.2410, MinusLogProbMetric: 35.2410, val_loss: 35.5854, val_MinusLogProbMetric: 35.5854

Epoch 803: val_loss did not improve from 31.14601
196/196 - 114s - loss: 35.2410 - MinusLogProbMetric: 35.2410 - val_loss: 35.5854 - val_MinusLogProbMetric: 35.5854 - lr: 6.1728e-06 - 114s/epoch - 584ms/step
Epoch 804/1000
2023-09-30 15:21:47.694 
Epoch 804/1000 
	 loss: 35.1948, MinusLogProbMetric: 35.1948, val_loss: 35.5658, val_MinusLogProbMetric: 35.5658

Epoch 804: val_loss did not improve from 31.14601
196/196 - 113s - loss: 35.1948 - MinusLogProbMetric: 35.1948 - val_loss: 35.5658 - val_MinusLogProbMetric: 35.5658 - lr: 6.1728e-06 - 113s/epoch - 578ms/step
Epoch 805/1000
2023-09-30 15:23:44.530 
Epoch 805/1000 
	 loss: 35.1641, MinusLogProbMetric: 35.1641, val_loss: 35.5876, val_MinusLogProbMetric: 35.5876

Epoch 805: val_loss did not improve from 31.14601
196/196 - 117s - loss: 35.1641 - MinusLogProbMetric: 35.1641 - val_loss: 35.5876 - val_MinusLogProbMetric: 35.5876 - lr: 6.1728e-06 - 117s/epoch - 596ms/step
Epoch 806/1000
2023-09-30 15:25:43.932 
Epoch 806/1000 
	 loss: 35.1192, MinusLogProbMetric: 35.1192, val_loss: 35.4079, val_MinusLogProbMetric: 35.4079

Epoch 806: val_loss did not improve from 31.14601
196/196 - 120s - loss: 35.1192 - MinusLogProbMetric: 35.1192 - val_loss: 35.4079 - val_MinusLogProbMetric: 35.4079 - lr: 6.1728e-06 - 120s/epoch - 610ms/step
Epoch 807/1000
2023-09-30 15:27:45.729 
Epoch 807/1000 
	 loss: 35.0501, MinusLogProbMetric: 35.0501, val_loss: 35.3881, val_MinusLogProbMetric: 35.3881

Epoch 807: val_loss did not improve from 31.14601
196/196 - 122s - loss: 35.0501 - MinusLogProbMetric: 35.0501 - val_loss: 35.3881 - val_MinusLogProbMetric: 35.3881 - lr: 6.1728e-06 - 122s/epoch - 621ms/step
Epoch 808/1000
2023-09-30 15:29:43.320 
Epoch 808/1000 
	 loss: 35.0078, MinusLogProbMetric: 35.0078, val_loss: 35.4505, val_MinusLogProbMetric: 35.4505

Epoch 808: val_loss did not improve from 31.14601
196/196 - 118s - loss: 35.0078 - MinusLogProbMetric: 35.0078 - val_loss: 35.4505 - val_MinusLogProbMetric: 35.4505 - lr: 6.1728e-06 - 118s/epoch - 600ms/step
Epoch 809/1000
2023-09-30 15:31:38.772 
Epoch 809/1000 
	 loss: 34.9711, MinusLogProbMetric: 34.9711, val_loss: 35.2616, val_MinusLogProbMetric: 35.2616

Epoch 809: val_loss did not improve from 31.14601
196/196 - 115s - loss: 34.9711 - MinusLogProbMetric: 34.9711 - val_loss: 35.2616 - val_MinusLogProbMetric: 35.2616 - lr: 6.1728e-06 - 115s/epoch - 589ms/step
Epoch 810/1000
2023-09-30 15:33:38.674 
Epoch 810/1000 
	 loss: 34.9326, MinusLogProbMetric: 34.9326, val_loss: 35.2311, val_MinusLogProbMetric: 35.2311

Epoch 810: val_loss did not improve from 31.14601
196/196 - 120s - loss: 34.9326 - MinusLogProbMetric: 34.9326 - val_loss: 35.2311 - val_MinusLogProbMetric: 35.2311 - lr: 6.1728e-06 - 120s/epoch - 612ms/step
Epoch 811/1000
2023-09-30 15:35:35.145 
Epoch 811/1000 
	 loss: 34.8849, MinusLogProbMetric: 34.8849, val_loss: 35.2475, val_MinusLogProbMetric: 35.2475

Epoch 811: val_loss did not improve from 31.14601
196/196 - 116s - loss: 34.8849 - MinusLogProbMetric: 34.8849 - val_loss: 35.2475 - val_MinusLogProbMetric: 35.2475 - lr: 6.1728e-06 - 116s/epoch - 594ms/step
Epoch 812/1000
2023-09-30 15:37:30.900 
Epoch 812/1000 
	 loss: 34.8514, MinusLogProbMetric: 34.8514, val_loss: 35.1621, val_MinusLogProbMetric: 35.1621

Epoch 812: val_loss did not improve from 31.14601
196/196 - 116s - loss: 34.8514 - MinusLogProbMetric: 34.8514 - val_loss: 35.1621 - val_MinusLogProbMetric: 35.1621 - lr: 6.1728e-06 - 116s/epoch - 591ms/step
Epoch 813/1000
2023-09-30 15:39:23.928 
Epoch 813/1000 
	 loss: 34.7981, MinusLogProbMetric: 34.7981, val_loss: 35.1431, val_MinusLogProbMetric: 35.1431

Epoch 813: val_loss did not improve from 31.14601
196/196 - 113s - loss: 34.7981 - MinusLogProbMetric: 34.7981 - val_loss: 35.1431 - val_MinusLogProbMetric: 35.1431 - lr: 6.1728e-06 - 113s/epoch - 577ms/step
Epoch 814/1000
2023-09-30 15:41:21.784 
Epoch 814/1000 
	 loss: 34.7680, MinusLogProbMetric: 34.7680, val_loss: 35.0887, val_MinusLogProbMetric: 35.0887

Epoch 814: val_loss did not improve from 31.14601
196/196 - 118s - loss: 34.7680 - MinusLogProbMetric: 34.7680 - val_loss: 35.0887 - val_MinusLogProbMetric: 35.0887 - lr: 6.1728e-06 - 118s/epoch - 601ms/step
Epoch 815/1000
2023-09-30 15:43:18.776 
Epoch 815/1000 
	 loss: 108.0091, MinusLogProbMetric: 108.0091, val_loss: 109.7654, val_MinusLogProbMetric: 109.7654

Epoch 815: val_loss did not improve from 31.14601
196/196 - 117s - loss: 108.0091 - MinusLogProbMetric: 108.0091 - val_loss: 109.7654 - val_MinusLogProbMetric: 109.7654 - lr: 6.1728e-06 - 117s/epoch - 597ms/step
Epoch 816/1000
2023-09-30 15:45:19.169 
Epoch 816/1000 
	 loss: 114.8185, MinusLogProbMetric: 114.8185, val_loss: 80.6501, val_MinusLogProbMetric: 80.6501

Epoch 816: val_loss did not improve from 31.14601
196/196 - 120s - loss: 114.8185 - MinusLogProbMetric: 114.8185 - val_loss: 80.6501 - val_MinusLogProbMetric: 80.6501 - lr: 6.1728e-06 - 120s/epoch - 614ms/step
Epoch 817/1000
2023-09-30 15:47:18.833 
Epoch 817/1000 
	 loss: 62.0927, MinusLogProbMetric: 62.0927, val_loss: 54.6665, val_MinusLogProbMetric: 54.6665

Epoch 817: val_loss did not improve from 31.14601
196/196 - 120s - loss: 62.0927 - MinusLogProbMetric: 62.0927 - val_loss: 54.6665 - val_MinusLogProbMetric: 54.6665 - lr: 6.1728e-06 - 120s/epoch - 611ms/step
Epoch 818/1000
2023-09-30 15:49:15.503 
Epoch 818/1000 
	 loss: 52.2605, MinusLogProbMetric: 52.2605, val_loss: 50.2802, val_MinusLogProbMetric: 50.2802

Epoch 818: val_loss did not improve from 31.14601
196/196 - 117s - loss: 52.2605 - MinusLogProbMetric: 52.2605 - val_loss: 50.2802 - val_MinusLogProbMetric: 50.2802 - lr: 6.1728e-06 - 117s/epoch - 595ms/step
Epoch 819/1000
2023-09-30 15:51:20.185 
Epoch 819/1000 
	 loss: 49.0532, MinusLogProbMetric: 49.0532, val_loss: 47.9764, val_MinusLogProbMetric: 47.9764

Epoch 819: val_loss did not improve from 31.14601
196/196 - 125s - loss: 49.0532 - MinusLogProbMetric: 49.0532 - val_loss: 47.9764 - val_MinusLogProbMetric: 47.9764 - lr: 6.1728e-06 - 125s/epoch - 636ms/step
Epoch 820/1000
2023-09-30 15:53:16.458 
Epoch 820/1000 
	 loss: 47.0888, MinusLogProbMetric: 47.0888, val_loss: 46.3334, val_MinusLogProbMetric: 46.3334

Epoch 820: val_loss did not improve from 31.14601
196/196 - 116s - loss: 47.0888 - MinusLogProbMetric: 47.0888 - val_loss: 46.3334 - val_MinusLogProbMetric: 46.3334 - lr: 6.1728e-06 - 116s/epoch - 593ms/step
Epoch 821/1000
2023-09-30 15:55:14.978 
Epoch 821/1000 
	 loss: 45.5511, MinusLogProbMetric: 45.5511, val_loss: 45.0209, val_MinusLogProbMetric: 45.0209

Epoch 821: val_loss did not improve from 31.14601
196/196 - 119s - loss: 45.5511 - MinusLogProbMetric: 45.5511 - val_loss: 45.0209 - val_MinusLogProbMetric: 45.0209 - lr: 6.1728e-06 - 119s/epoch - 605ms/step
Epoch 822/1000
2023-09-30 15:57:17.115 
Epoch 822/1000 
	 loss: 44.3776, MinusLogProbMetric: 44.3776, val_loss: 43.9145, val_MinusLogProbMetric: 43.9145

Epoch 822: val_loss did not improve from 31.14601
196/196 - 122s - loss: 44.3776 - MinusLogProbMetric: 44.3776 - val_loss: 43.9145 - val_MinusLogProbMetric: 43.9145 - lr: 6.1728e-06 - 122s/epoch - 623ms/step
Epoch 823/1000
2023-09-30 15:59:18.535 
Epoch 823/1000 
	 loss: 43.2832, MinusLogProbMetric: 43.2832, val_loss: 42.9554, val_MinusLogProbMetric: 42.9554

Epoch 823: val_loss did not improve from 31.14601
196/196 - 121s - loss: 43.2832 - MinusLogProbMetric: 43.2832 - val_loss: 42.9554 - val_MinusLogProbMetric: 42.9554 - lr: 6.1728e-06 - 121s/epoch - 619ms/step
Epoch 824/1000
2023-09-30 16:01:17.562 
Epoch 824/1000 
	 loss: 42.3573, MinusLogProbMetric: 42.3573, val_loss: 42.0713, val_MinusLogProbMetric: 42.0713

Epoch 824: val_loss did not improve from 31.14601
196/196 - 119s - loss: 42.3573 - MinusLogProbMetric: 42.3573 - val_loss: 42.0713 - val_MinusLogProbMetric: 42.0713 - lr: 6.1728e-06 - 119s/epoch - 607ms/step
Epoch 825/1000
2023-09-30 16:03:14.805 
Epoch 825/1000 
	 loss: 41.4967, MinusLogProbMetric: 41.4967, val_loss: 41.1053, val_MinusLogProbMetric: 41.1053

Epoch 825: val_loss did not improve from 31.14601
196/196 - 117s - loss: 41.4967 - MinusLogProbMetric: 41.4967 - val_loss: 41.1053 - val_MinusLogProbMetric: 41.1053 - lr: 6.1728e-06 - 117s/epoch - 598ms/step
Epoch 826/1000
2023-09-30 16:05:14.340 
Epoch 826/1000 
	 loss: 40.4667, MinusLogProbMetric: 40.4667, val_loss: 40.0588, val_MinusLogProbMetric: 40.0588

Epoch 826: val_loss did not improve from 31.14601
196/196 - 120s - loss: 40.4667 - MinusLogProbMetric: 40.4667 - val_loss: 40.0588 - val_MinusLogProbMetric: 40.0588 - lr: 6.1728e-06 - 120s/epoch - 610ms/step
Epoch 827/1000
2023-09-30 16:07:16.347 
Epoch 827/1000 
	 loss: 39.0459, MinusLogProbMetric: 39.0459, val_loss: 38.9097, val_MinusLogProbMetric: 38.9097

Epoch 827: val_loss did not improve from 31.14601
196/196 - 122s - loss: 39.0459 - MinusLogProbMetric: 39.0459 - val_loss: 38.9097 - val_MinusLogProbMetric: 38.9097 - lr: 6.1728e-06 - 122s/epoch - 623ms/step
Epoch 828/1000
2023-09-30 16:09:16.399 
Epoch 828/1000 
	 loss: 38.4730, MinusLogProbMetric: 38.4730, val_loss: 38.4876, val_MinusLogProbMetric: 38.4876

Epoch 828: val_loss did not improve from 31.14601
196/196 - 120s - loss: 38.4730 - MinusLogProbMetric: 38.4730 - val_loss: 38.4876 - val_MinusLogProbMetric: 38.4876 - lr: 6.1728e-06 - 120s/epoch - 612ms/step
Epoch 829/1000
2023-09-30 16:11:14.162 
Epoch 829/1000 
	 loss: 38.0684, MinusLogProbMetric: 38.0684, val_loss: 38.1182, val_MinusLogProbMetric: 38.1182

Epoch 829: val_loss did not improve from 31.14601
196/196 - 118s - loss: 38.0684 - MinusLogProbMetric: 38.0684 - val_loss: 38.1182 - val_MinusLogProbMetric: 38.1182 - lr: 6.1728e-06 - 118s/epoch - 601ms/step
Epoch 830/1000
2023-09-30 16:13:07.089 
Epoch 830/1000 
	 loss: 37.7249, MinusLogProbMetric: 37.7249, val_loss: 37.8604, val_MinusLogProbMetric: 37.8604

Epoch 830: val_loss did not improve from 31.14601
196/196 - 113s - loss: 37.7249 - MinusLogProbMetric: 37.7249 - val_loss: 37.8604 - val_MinusLogProbMetric: 37.8604 - lr: 6.1728e-06 - 113s/epoch - 576ms/step
Epoch 831/1000
2023-09-30 16:15:01.883 
Epoch 831/1000 
	 loss: 37.4603, MinusLogProbMetric: 37.4603, val_loss: 37.5558, val_MinusLogProbMetric: 37.5558

Epoch 831: val_loss did not improve from 31.14601
196/196 - 115s - loss: 37.4603 - MinusLogProbMetric: 37.4603 - val_loss: 37.5558 - val_MinusLogProbMetric: 37.5558 - lr: 6.1728e-06 - 115s/epoch - 586ms/step
Epoch 832/1000
2023-09-30 16:16:56.099 
Epoch 832/1000 
	 loss: 37.1893, MinusLogProbMetric: 37.1893, val_loss: 37.3152, val_MinusLogProbMetric: 37.3152

Epoch 832: val_loss did not improve from 31.14601
196/196 - 114s - loss: 37.1893 - MinusLogProbMetric: 37.1893 - val_loss: 37.3152 - val_MinusLogProbMetric: 37.3152 - lr: 6.1728e-06 - 114s/epoch - 583ms/step
Epoch 833/1000
2023-09-30 16:18:49.258 
Epoch 833/1000 
	 loss: 36.9781, MinusLogProbMetric: 36.9781, val_loss: 37.1619, val_MinusLogProbMetric: 37.1619

Epoch 833: val_loss did not improve from 31.14601
196/196 - 113s - loss: 36.9781 - MinusLogProbMetric: 36.9781 - val_loss: 37.1619 - val_MinusLogProbMetric: 37.1619 - lr: 6.1728e-06 - 113s/epoch - 577ms/step
Epoch 834/1000
2023-09-30 16:20:45.183 
Epoch 834/1000 
	 loss: 36.8681, MinusLogProbMetric: 36.8681, val_loss: 36.9527, val_MinusLogProbMetric: 36.9527

Epoch 834: val_loss did not improve from 31.14601
196/196 - 116s - loss: 36.8681 - MinusLogProbMetric: 36.8681 - val_loss: 36.9527 - val_MinusLogProbMetric: 36.9527 - lr: 6.1728e-06 - 116s/epoch - 591ms/step
Epoch 835/1000
2023-09-30 16:22:39.097 
Epoch 835/1000 
	 loss: 36.6108, MinusLogProbMetric: 36.6108, val_loss: 36.7660, val_MinusLogProbMetric: 36.7660

Epoch 835: val_loss did not improve from 31.14601
196/196 - 114s - loss: 36.6108 - MinusLogProbMetric: 36.6108 - val_loss: 36.7660 - val_MinusLogProbMetric: 36.7660 - lr: 6.1728e-06 - 114s/epoch - 582ms/step
Epoch 836/1000
2023-09-30 16:24:36.654 
Epoch 836/1000 
	 loss: 36.4377, MinusLogProbMetric: 36.4377, val_loss: 36.6039, val_MinusLogProbMetric: 36.6039

Epoch 836: val_loss did not improve from 31.14601
196/196 - 117s - loss: 36.4377 - MinusLogProbMetric: 36.4377 - val_loss: 36.6039 - val_MinusLogProbMetric: 36.6039 - lr: 6.1728e-06 - 117s/epoch - 599ms/step
Epoch 837/1000
2023-09-30 16:26:30.509 
Epoch 837/1000 
	 loss: 36.2973, MinusLogProbMetric: 36.2973, val_loss: 36.4935, val_MinusLogProbMetric: 36.4935

Epoch 837: val_loss did not improve from 31.14601
196/196 - 114s - loss: 36.2973 - MinusLogProbMetric: 36.2973 - val_loss: 36.4935 - val_MinusLogProbMetric: 36.4935 - lr: 6.1728e-06 - 114s/epoch - 581ms/step
Epoch 838/1000
2023-09-30 16:28:30.097 
Epoch 838/1000 
	 loss: 36.1687, MinusLogProbMetric: 36.1687, val_loss: 36.3814, val_MinusLogProbMetric: 36.3814

Epoch 838: val_loss did not improve from 31.14601
196/196 - 120s - loss: 36.1687 - MinusLogProbMetric: 36.1687 - val_loss: 36.3814 - val_MinusLogProbMetric: 36.3814 - lr: 6.1728e-06 - 120s/epoch - 610ms/step
Epoch 839/1000
2023-09-30 16:30:27.885 
Epoch 839/1000 
	 loss: 36.0582, MinusLogProbMetric: 36.0582, val_loss: 36.2515, val_MinusLogProbMetric: 36.2515

Epoch 839: val_loss did not improve from 31.14601
Restoring model weights from the end of the best epoch: 739.
196/196 - 119s - loss: 36.0582 - MinusLogProbMetric: 36.0582 - val_loss: 36.2515 - val_MinusLogProbMetric: 36.2515 - lr: 6.1728e-06 - 119s/epoch - 608ms/step
Epoch 839: early stopping
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.

------------------------------------------
Starting LR metric calculation...
Running TF LR calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
Warning: Removed a fraction of alternative samples due to non-finite log probabilities.
Warning: Removed a fraction of alternative samples due to non-finite log probabilities.
Warning: Removed a fraction of alternative samples due to non-finite log probabilities.
Warning: Removed a fraction of alternative samples due to non-finite log probabilities.
Warning: Removed a fraction of alternative samples due to non-finite log probabilities.
Warning: Removed a fraction of alternative samples due to non-finite log probabilities.
Warning: Removed a fraction of alternative samples due to non-finite log probabilities.
Warning: Removed a fraction of alternative samples due to non-finite log probabilities.
Warning: Removed a fraction of alternative samples due to non-finite log probabilities.
Warning: Removed a fraction of alternative samples due to non-finite log probabilities.
LR metric calculation completed in 77.21664092096034 seconds.

------------------------------------------
Starting KS tests calculation...
Running TF KS tests...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
KS tests calculation completed in 39.4049097399693 seconds.

------------------------------------------
Starting SWD metric calculation...
Running TF SWD calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
SWD metric calculation completed in 30.984455291065387 seconds.

------------------------------------------
Starting FN metric calculation...
Running TF FN calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
FN metric calculation completed in 35.55935037904419 seconds.
Training succeeded with seed 377.
Model trained in 69666.18 s.

===========
Computing predictions
===========

Computing metrics...
Metrics computed in 190.91 s.
                ===========
                print("===========
Failed to plot
===========
")
                Exception type: ValueError
                Exception message: Number of rows must be a positive integer, not 0
                Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN.py , Line : 469, Func.Name : prediction_function, Message : Plotters.cornerplotter(X_data_test.numpy(),X_data_nf.numpy(),path_to_results,ndims,norm=True) # type: ignore', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/Plotters.py , Line : 156, Func.Name : cornerplotter, Message : figure=corner.corner(target_samples,color=\'red\',bins=n_bins,labels=[r"%s" % s for s in labels],normalize1d=True)', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/corner.py , Line : 235, Func.Name : corner, Message : fig, axes = pl.subplots(K, K, figsize=(dim, dim))', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/pyplot.py , Line : 1502, Func.Name : subplots, Message : axs = fig.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 905, Func.Name : subplots, Message : gs = self.add_gridspec(nrows, ncols, figure=self, **gridspec_kw)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 1527, Func.Name : add_gridspec, Message : gs = GridSpec(nrows=nrows, ncols=ncols, figure=self, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 379, Func.Name : __init__, Message : super().__init__(nrows, ncols,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 49, Func.Name : __init__, Message : raise ValueError(']
                ===========

results.txt saved
results.json saved
Results log saved
Model predictions computed in 192.37 s.
===========
Run 342/720 done in 84073.85 s.
===========

Directory ../../results/CsplineN_new/run_343/ already exists.
Skipping it.
===========
Run 343/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_344/ already exists.
Skipping it.
===========
Run 344/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_345/ already exists.
Skipping it.
===========
Run 345/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_346/ already exists.
Skipping it.
===========
Run 346/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_347/ already exists.
Skipping it.
===========
Run 347/720 already exists. Skipping it.
===========

===========
Generating train data for run 348.
===========
Train data generated in 0.21 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_348/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_348/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_348/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_348
self.data_kwargs: {'seed': 440}
self.x_data: [[ 2.0547452   3.776408    6.9601746  ...  5.8637457  -0.4179731
   2.747595  ]
 [ 6.405085    2.999549    6.1033454  ...  2.8107233   3.0569289
   2.003619  ]
 [ 2.0047038   4.064143    6.2048492  ...  5.6568336   0.71395487
   2.6926432 ]
 ...
 [ 5.645455    7.0971727   6.202566   ...  0.61970913  6.238492
   1.3539525 ]
 [ 6.475603    3.1151166   6.2248073  ...  2.8990176   3.6724448
   2.289971  ]
 [ 2.2290072   4.101192    7.1413527  ...  6.717821   -2.0046902
   2.3999293 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_320"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_321 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_30 (LogProbL  (None,)                  2139360   
 ayer)                                                           
                                                                 
=================================================================
Total params: 2,139,360
Trainable params: 2,139,360
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_30/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_30'")
self.model: <keras.engine.functional.Functional object at 0x7fb171533bb0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7faeb91cee00>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7faeb91cee00>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fae79d0c370>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fb1714eebc0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_348/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fb1714ef130>, <keras.callbacks.ModelCheckpoint object at 0x7fb1714ef1f0>, <keras.callbacks.EarlyStopping object at 0x7fb1714ef460>, <keras.callbacks.ReduceLROnPlateau object at 0x7fb1714ef490>, <keras.callbacks.TerminateOnNaN object at 0x7fb1714ef0d0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_348/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 348/720 with hyperparameters:
timestamp = 2023-09-30 16:33:56.986438
ndims = 64
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 2139360
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 2.0547452   3.776408    6.9601746   1.2163222   8.644804    1.0450234
  9.756204    5.9333344   9.3759165   6.2056174   7.6663713   0.8787037
  2.7781603   1.7675867   3.9287794   0.8286338   3.6572614   4.846633
  0.68292063  6.317236    6.48625     3.843101    5.88724     2.0270782
  5.1297107   7.7579265   2.6054757   6.6726594   1.0543873   7.123391
  3.5526195   2.046467    5.620422   -0.869851    8.648443    0.02500496
  7.4141846   2.6777635   7.6790566   9.76465     2.481489    6.2672224
  5.7068043   4.504637    1.1161722   9.691578    4.0927534   8.503739
  6.9964066   3.1729455   8.086372    4.0078015   8.096493    5.813139
  8.181514    6.97417     7.246071    5.0315676  10.666024    6.2678714
  3.6855178   5.8637457  -0.4179731   2.747595  ]
Epoch 1/1000
2023-09-30 16:39:32.412 
Epoch 1/1000 
	 loss: 361.2928, MinusLogProbMetric: 361.2928, val_loss: 77.6952, val_MinusLogProbMetric: 77.6952

Epoch 1: val_loss improved from inf to 77.69524, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_348/weights/best_weights.h5
196/196 - 337s - loss: 361.2928 - MinusLogProbMetric: 361.2928 - val_loss: 77.6952 - val_MinusLogProbMetric: 77.6952 - lr: 0.0010 - 337s/epoch - 2s/step
Epoch 2/1000
2023-09-30 16:40:44.560 
Epoch 2/1000 
	 loss: 62.5002, MinusLogProbMetric: 62.5002, val_loss: 52.7566, val_MinusLogProbMetric: 52.7566

Epoch 2: val_loss improved from 77.69524 to 52.75664, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_348/weights/best_weights.h5
196/196 - 71s - loss: 62.5002 - MinusLogProbMetric: 62.5002 - val_loss: 52.7566 - val_MinusLogProbMetric: 52.7566 - lr: 0.0010 - 71s/epoch - 361ms/step
Epoch 3/1000
2023-09-30 16:41:57.510 
Epoch 3/1000 
	 loss: 48.9668, MinusLogProbMetric: 48.9668, val_loss: 48.5978, val_MinusLogProbMetric: 48.5978

Epoch 3: val_loss improved from 52.75664 to 48.59780, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_348/weights/best_weights.h5
196/196 - 74s - loss: 48.9668 - MinusLogProbMetric: 48.9668 - val_loss: 48.5978 - val_MinusLogProbMetric: 48.5978 - lr: 0.0010 - 74s/epoch - 376ms/step
Epoch 4/1000
2023-09-30 16:43:09.377 
Epoch 4/1000 
	 loss: 43.6912, MinusLogProbMetric: 43.6912, val_loss: 42.2538, val_MinusLogProbMetric: 42.2538

Epoch 4: val_loss improved from 48.59780 to 42.25376, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_348/weights/best_weights.h5
196/196 - 72s - loss: 43.6912 - MinusLogProbMetric: 43.6912 - val_loss: 42.2538 - val_MinusLogProbMetric: 42.2538 - lr: 0.0010 - 72s/epoch - 366ms/step
Epoch 5/1000
2023-09-30 16:44:20.828 
Epoch 5/1000 
	 loss: 40.7961, MinusLogProbMetric: 40.7961, val_loss: 39.5908, val_MinusLogProbMetric: 39.5908

Epoch 5: val_loss improved from 42.25376 to 39.59078, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_348/weights/best_weights.h5
196/196 - 72s - loss: 40.7961 - MinusLogProbMetric: 40.7961 - val_loss: 39.5908 - val_MinusLogProbMetric: 39.5908 - lr: 0.0010 - 72s/epoch - 367ms/step
Epoch 6/1000
2023-09-30 16:45:35.730 
Epoch 6/1000 
	 loss: 39.1863, MinusLogProbMetric: 39.1863, val_loss: 38.2649, val_MinusLogProbMetric: 38.2649

Epoch 6: val_loss improved from 39.59078 to 38.26493, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_348/weights/best_weights.h5
196/196 - 76s - loss: 39.1863 - MinusLogProbMetric: 39.1863 - val_loss: 38.2649 - val_MinusLogProbMetric: 38.2649 - lr: 0.0010 - 76s/epoch - 388ms/step
Epoch 7/1000
2023-09-30 16:46:51.276 
Epoch 7/1000 
	 loss: 37.8143, MinusLogProbMetric: 37.8143, val_loss: 39.1023, val_MinusLogProbMetric: 39.1023

Epoch 7: val_loss did not improve from 38.26493
196/196 - 72s - loss: 37.8143 - MinusLogProbMetric: 37.8143 - val_loss: 39.1023 - val_MinusLogProbMetric: 39.1023 - lr: 0.0010 - 72s/epoch - 367ms/step
Epoch 8/1000
2023-09-30 16:47:59.767 
Epoch 8/1000 
	 loss: 37.2953, MinusLogProbMetric: 37.2953, val_loss: 37.6343, val_MinusLogProbMetric: 37.6343

Epoch 8: val_loss improved from 38.26493 to 37.63427, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_348/weights/best_weights.h5
196/196 - 69s - loss: 37.2953 - MinusLogProbMetric: 37.2953 - val_loss: 37.6343 - val_MinusLogProbMetric: 37.6343 - lr: 0.0010 - 69s/epoch - 354ms/step
Epoch 9/1000
2023-09-30 16:49:14.785 
Epoch 9/1000 
	 loss: 36.3241, MinusLogProbMetric: 36.3241, val_loss: 36.6656, val_MinusLogProbMetric: 36.6656

Epoch 9: val_loss improved from 37.63427 to 36.66562, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_348/weights/best_weights.h5
196/196 - 77s - loss: 36.3241 - MinusLogProbMetric: 36.3241 - val_loss: 36.6656 - val_MinusLogProbMetric: 36.6656 - lr: 0.0010 - 77s/epoch - 391ms/step
Epoch 10/1000
2023-09-30 16:50:26.212 
Epoch 10/1000 
	 loss: 35.6239, MinusLogProbMetric: 35.6239, val_loss: 36.5026, val_MinusLogProbMetric: 36.5026

Epoch 10: val_loss improved from 36.66562 to 36.50257, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_348/weights/best_weights.h5
196/196 - 70s - loss: 35.6239 - MinusLogProbMetric: 35.6239 - val_loss: 36.5026 - val_MinusLogProbMetric: 36.5026 - lr: 0.0010 - 70s/epoch - 359ms/step
Epoch 11/1000
2023-09-30 16:51:38.547 
Epoch 11/1000 
	 loss: 34.7134, MinusLogProbMetric: 34.7134, val_loss: 34.4648, val_MinusLogProbMetric: 34.4648

Epoch 11: val_loss improved from 36.50257 to 34.46476, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_348/weights/best_weights.h5
196/196 - 74s - loss: 34.7134 - MinusLogProbMetric: 34.7134 - val_loss: 34.4648 - val_MinusLogProbMetric: 34.4648 - lr: 0.0010 - 74s/epoch - 375ms/step
Epoch 12/1000
2023-09-30 16:52:53.921 
Epoch 12/1000 
	 loss: 34.1622, MinusLogProbMetric: 34.1622, val_loss: 34.6180, val_MinusLogProbMetric: 34.6180

Epoch 12: val_loss did not improve from 34.46476
196/196 - 73s - loss: 34.1622 - MinusLogProbMetric: 34.1622 - val_loss: 34.6180 - val_MinusLogProbMetric: 34.6180 - lr: 0.0010 - 73s/epoch - 371ms/step
Epoch 13/1000
2023-09-30 16:54:02.438 
Epoch 13/1000 
	 loss: 33.7573, MinusLogProbMetric: 33.7573, val_loss: 34.4218, val_MinusLogProbMetric: 34.4218

Epoch 13: val_loss improved from 34.46476 to 34.42178, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_348/weights/best_weights.h5
196/196 - 70s - loss: 33.7573 - MinusLogProbMetric: 33.7573 - val_loss: 34.4218 - val_MinusLogProbMetric: 34.4218 - lr: 0.0010 - 70s/epoch - 360ms/step
Epoch 14/1000
2023-09-30 16:55:12.416 
Epoch 14/1000 
	 loss: 33.6336, MinusLogProbMetric: 33.6336, val_loss: 34.1903, val_MinusLogProbMetric: 34.1903

Epoch 14: val_loss improved from 34.42178 to 34.19027, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_348/weights/best_weights.h5
196/196 - 69s - loss: 33.6336 - MinusLogProbMetric: 33.6336 - val_loss: 34.1903 - val_MinusLogProbMetric: 34.1903 - lr: 0.0010 - 69s/epoch - 353ms/step
Epoch 15/1000
2023-09-30 16:56:20.892 
Epoch 15/1000 
	 loss: 32.9645, MinusLogProbMetric: 32.9645, val_loss: 32.9781, val_MinusLogProbMetric: 32.9781

Epoch 15: val_loss improved from 34.19027 to 32.97815, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_348/weights/best_weights.h5
196/196 - 69s - loss: 32.9645 - MinusLogProbMetric: 32.9645 - val_loss: 32.9781 - val_MinusLogProbMetric: 32.9781 - lr: 0.0010 - 69s/epoch - 350ms/step
Epoch 16/1000
2023-09-30 16:57:28.044 
Epoch 16/1000 
	 loss: 32.6785, MinusLogProbMetric: 32.6785, val_loss: 32.8870, val_MinusLogProbMetric: 32.8870

Epoch 16: val_loss improved from 32.97815 to 32.88698, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_348/weights/best_weights.h5
196/196 - 68s - loss: 32.6785 - MinusLogProbMetric: 32.6785 - val_loss: 32.8870 - val_MinusLogProbMetric: 32.8870 - lr: 0.0010 - 68s/epoch - 347ms/step
Epoch 17/1000
2023-09-30 16:58:36.359 
Epoch 17/1000 
	 loss: 32.4878, MinusLogProbMetric: 32.4878, val_loss: 33.2685, val_MinusLogProbMetric: 33.2685

Epoch 17: val_loss did not improve from 32.88698
196/196 - 66s - loss: 32.4878 - MinusLogProbMetric: 32.4878 - val_loss: 33.2685 - val_MinusLogProbMetric: 33.2685 - lr: 0.0010 - 66s/epoch - 338ms/step
Epoch 18/1000
2023-09-30 16:59:44.813 
Epoch 18/1000 
	 loss: 32.3155, MinusLogProbMetric: 32.3155, val_loss: 33.0884, val_MinusLogProbMetric: 33.0884

Epoch 18: val_loss did not improve from 32.88698
196/196 - 68s - loss: 32.3155 - MinusLogProbMetric: 32.3155 - val_loss: 33.0884 - val_MinusLogProbMetric: 33.0884 - lr: 0.0010 - 68s/epoch - 349ms/step
Epoch 19/1000
2023-09-30 17:00:52.210 
Epoch 19/1000 
	 loss: 32.1224, MinusLogProbMetric: 32.1224, val_loss: 33.7209, val_MinusLogProbMetric: 33.7209

Epoch 19: val_loss did not improve from 32.88698
196/196 - 67s - loss: 32.1224 - MinusLogProbMetric: 32.1224 - val_loss: 33.7209 - val_MinusLogProbMetric: 33.7209 - lr: 0.0010 - 67s/epoch - 344ms/step
Epoch 20/1000
2023-09-30 17:01:58.680 
Epoch 20/1000 
	 loss: 32.0666, MinusLogProbMetric: 32.0666, val_loss: 32.6432, val_MinusLogProbMetric: 32.6432

Epoch 20: val_loss improved from 32.88698 to 32.64320, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_348/weights/best_weights.h5
196/196 - 68s - loss: 32.0666 - MinusLogProbMetric: 32.0666 - val_loss: 32.6432 - val_MinusLogProbMetric: 32.6432 - lr: 0.0010 - 68s/epoch - 345ms/step
Epoch 21/1000
2023-09-30 17:03:04.832 
Epoch 21/1000 
	 loss: 31.8407, MinusLogProbMetric: 31.8407, val_loss: 32.9576, val_MinusLogProbMetric: 32.9576

Epoch 21: val_loss did not improve from 32.64320
196/196 - 65s - loss: 31.8407 - MinusLogProbMetric: 31.8407 - val_loss: 32.9576 - val_MinusLogProbMetric: 32.9576 - lr: 0.0010 - 65s/epoch - 332ms/step
Epoch 22/1000
2023-09-30 17:04:11.935 
Epoch 22/1000 
	 loss: 31.6194, MinusLogProbMetric: 31.6194, val_loss: 31.0343, val_MinusLogProbMetric: 31.0343

Epoch 22: val_loss improved from 32.64320 to 31.03433, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_348/weights/best_weights.h5
196/196 - 69s - loss: 31.6194 - MinusLogProbMetric: 31.6194 - val_loss: 31.0343 - val_MinusLogProbMetric: 31.0343 - lr: 0.0010 - 69s/epoch - 354ms/step
Epoch 23/1000
2023-09-30 17:05:21.788 
Epoch 23/1000 
	 loss: 31.5356, MinusLogProbMetric: 31.5356, val_loss: 32.1659, val_MinusLogProbMetric: 32.1659

Epoch 23: val_loss did not improve from 31.03433
196/196 - 68s - loss: 31.5356 - MinusLogProbMetric: 31.5356 - val_loss: 32.1659 - val_MinusLogProbMetric: 32.1659 - lr: 0.0010 - 68s/epoch - 345ms/step
Epoch 24/1000
2023-09-30 17:06:28.347 
Epoch 24/1000 
	 loss: 31.6980, MinusLogProbMetric: 31.6980, val_loss: 31.6546, val_MinusLogProbMetric: 31.6546

Epoch 24: val_loss did not improve from 31.03433
196/196 - 67s - loss: 31.6980 - MinusLogProbMetric: 31.6980 - val_loss: 31.6546 - val_MinusLogProbMetric: 31.6546 - lr: 0.0010 - 67s/epoch - 340ms/step
Epoch 25/1000
2023-09-30 17:07:32.815 
Epoch 25/1000 
	 loss: 31.3537, MinusLogProbMetric: 31.3537, val_loss: 31.2392, val_MinusLogProbMetric: 31.2392

Epoch 25: val_loss did not improve from 31.03433
196/196 - 64s - loss: 31.3537 - MinusLogProbMetric: 31.3537 - val_loss: 31.2392 - val_MinusLogProbMetric: 31.2392 - lr: 0.0010 - 64s/epoch - 329ms/step
Epoch 26/1000
2023-09-30 17:08:37.588 
Epoch 26/1000 
	 loss: 31.1710, MinusLogProbMetric: 31.1710, val_loss: 32.6547, val_MinusLogProbMetric: 32.6547

Epoch 26: val_loss did not improve from 31.03433
196/196 - 65s - loss: 31.1710 - MinusLogProbMetric: 31.1710 - val_loss: 32.6547 - val_MinusLogProbMetric: 32.6547 - lr: 0.0010 - 65s/epoch - 330ms/step
Epoch 27/1000
2023-09-30 17:09:43.912 
Epoch 27/1000 
	 loss: 31.2115, MinusLogProbMetric: 31.2115, val_loss: 32.3185, val_MinusLogProbMetric: 32.3185

Epoch 27: val_loss did not improve from 31.03433
196/196 - 66s - loss: 31.2115 - MinusLogProbMetric: 31.2115 - val_loss: 32.3185 - val_MinusLogProbMetric: 32.3185 - lr: 0.0010 - 66s/epoch - 338ms/step
Epoch 28/1000
2023-09-30 17:10:46.630 
Epoch 28/1000 
	 loss: 31.0588, MinusLogProbMetric: 31.0588, val_loss: 31.0591, val_MinusLogProbMetric: 31.0591

Epoch 28: val_loss did not improve from 31.03433
196/196 - 63s - loss: 31.0588 - MinusLogProbMetric: 31.0588 - val_loss: 31.0591 - val_MinusLogProbMetric: 31.0591 - lr: 0.0010 - 63s/epoch - 320ms/step
Epoch 29/1000
2023-09-30 17:11:52.590 
Epoch 29/1000 
	 loss: 31.0089, MinusLogProbMetric: 31.0089, val_loss: 31.4569, val_MinusLogProbMetric: 31.4569

Epoch 29: val_loss did not improve from 31.03433
196/196 - 66s - loss: 31.0089 - MinusLogProbMetric: 31.0089 - val_loss: 31.4569 - val_MinusLogProbMetric: 31.4569 - lr: 0.0010 - 66s/epoch - 336ms/step
Epoch 30/1000
2023-09-30 17:12:57.718 
Epoch 30/1000 
	 loss: 30.9818, MinusLogProbMetric: 30.9818, val_loss: 30.8684, val_MinusLogProbMetric: 30.8684

Epoch 30: val_loss improved from 31.03433 to 30.86842, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_348/weights/best_weights.h5
196/196 - 67s - loss: 30.9818 - MinusLogProbMetric: 30.9818 - val_loss: 30.8684 - val_MinusLogProbMetric: 30.8684 - lr: 0.0010 - 67s/epoch - 343ms/step
Epoch 31/1000
2023-09-30 17:14:06.933 
Epoch 31/1000 
	 loss: 30.9589, MinusLogProbMetric: 30.9589, val_loss: 31.7831, val_MinusLogProbMetric: 31.7831

Epoch 31: val_loss did not improve from 30.86842
196/196 - 67s - loss: 30.9589 - MinusLogProbMetric: 30.9589 - val_loss: 31.7831 - val_MinusLogProbMetric: 31.7831 - lr: 0.0010 - 67s/epoch - 342ms/step
Epoch 32/1000
2023-09-30 17:15:13.090 
Epoch 32/1000 
	 loss: 30.7528, MinusLogProbMetric: 30.7528, val_loss: 30.8523, val_MinusLogProbMetric: 30.8523

Epoch 32: val_loss improved from 30.86842 to 30.85233, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_348/weights/best_weights.h5
196/196 - 69s - loss: 30.7528 - MinusLogProbMetric: 30.7528 - val_loss: 30.8523 - val_MinusLogProbMetric: 30.8523 - lr: 0.0010 - 69s/epoch - 350ms/step
Epoch 33/1000
2023-09-30 17:16:22.869 
Epoch 33/1000 
	 loss: 30.5199, MinusLogProbMetric: 30.5199, val_loss: 30.6813, val_MinusLogProbMetric: 30.6813

Epoch 33: val_loss improved from 30.85233 to 30.68125, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_348/weights/best_weights.h5
196/196 - 70s - loss: 30.5199 - MinusLogProbMetric: 30.5199 - val_loss: 30.6813 - val_MinusLogProbMetric: 30.6813 - lr: 0.0010 - 70s/epoch - 356ms/step
Epoch 34/1000
2023-09-30 17:17:33.766 
Epoch 34/1000 
	 loss: 30.7596, MinusLogProbMetric: 30.7596, val_loss: 31.0342, val_MinusLogProbMetric: 31.0342

Epoch 34: val_loss did not improve from 30.68125
196/196 - 68s - loss: 30.7596 - MinusLogProbMetric: 30.7596 - val_loss: 31.0342 - val_MinusLogProbMetric: 31.0342 - lr: 0.0010 - 68s/epoch - 349ms/step
Epoch 35/1000
2023-09-30 17:18:43.769 
Epoch 35/1000 
	 loss: 30.4838, MinusLogProbMetric: 30.4838, val_loss: 31.1599, val_MinusLogProbMetric: 31.1599

Epoch 35: val_loss did not improve from 30.68125
196/196 - 70s - loss: 30.4838 - MinusLogProbMetric: 30.4838 - val_loss: 31.1599 - val_MinusLogProbMetric: 31.1599 - lr: 0.0010 - 70s/epoch - 357ms/step
Epoch 36/1000
2023-09-30 17:19:51.569 
Epoch 36/1000 
	 loss: 30.3638, MinusLogProbMetric: 30.3638, val_loss: 30.9401, val_MinusLogProbMetric: 30.9401

Epoch 36: val_loss did not improve from 30.68125
196/196 - 68s - loss: 30.3638 - MinusLogProbMetric: 30.3638 - val_loss: 30.9401 - val_MinusLogProbMetric: 30.9401 - lr: 0.0010 - 68s/epoch - 346ms/step
Epoch 37/1000
2023-09-30 17:20:57.869 
Epoch 37/1000 
	 loss: 30.5237, MinusLogProbMetric: 30.5237, val_loss: 30.9206, val_MinusLogProbMetric: 30.9206

Epoch 37: val_loss did not improve from 30.68125
196/196 - 66s - loss: 30.5237 - MinusLogProbMetric: 30.5237 - val_loss: 30.9206 - val_MinusLogProbMetric: 30.9206 - lr: 0.0010 - 66s/epoch - 338ms/step
Epoch 38/1000
2023-09-30 17:22:05.713 
Epoch 38/1000 
	 loss: 30.3303, MinusLogProbMetric: 30.3303, val_loss: 30.9494, val_MinusLogProbMetric: 30.9494

Epoch 38: val_loss did not improve from 30.68125
196/196 - 68s - loss: 30.3303 - MinusLogProbMetric: 30.3303 - val_loss: 30.9494 - val_MinusLogProbMetric: 30.9494 - lr: 0.0010 - 68s/epoch - 346ms/step
Epoch 39/1000
2023-09-30 17:23:12.973 
Epoch 39/1000 
	 loss: 30.2973, MinusLogProbMetric: 30.2973, val_loss: 30.9093, val_MinusLogProbMetric: 30.9093

Epoch 39: val_loss did not improve from 30.68125
196/196 - 67s - loss: 30.2973 - MinusLogProbMetric: 30.2973 - val_loss: 30.9093 - val_MinusLogProbMetric: 30.9093 - lr: 0.0010 - 67s/epoch - 343ms/step
Epoch 40/1000
2023-09-30 17:24:22.910 
Epoch 40/1000 
	 loss: 30.3936, MinusLogProbMetric: 30.3936, val_loss: 30.4688, val_MinusLogProbMetric: 30.4688

Epoch 40: val_loss improved from 30.68125 to 30.46877, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_348/weights/best_weights.h5
196/196 - 71s - loss: 30.3936 - MinusLogProbMetric: 30.3936 - val_loss: 30.4688 - val_MinusLogProbMetric: 30.4688 - lr: 0.0010 - 71s/epoch - 365ms/step
Epoch 41/1000
2023-09-30 17:25:32.001 
Epoch 41/1000 
	 loss: 30.2036, MinusLogProbMetric: 30.2036, val_loss: 31.2396, val_MinusLogProbMetric: 31.2396

Epoch 41: val_loss did not improve from 30.46877
196/196 - 68s - loss: 30.2036 - MinusLogProbMetric: 30.2036 - val_loss: 31.2396 - val_MinusLogProbMetric: 31.2396 - lr: 0.0010 - 68s/epoch - 345ms/step
Epoch 42/1000
2023-09-30 17:26:41.384 
Epoch 42/1000 
	 loss: 30.2067, MinusLogProbMetric: 30.2067, val_loss: 30.5845, val_MinusLogProbMetric: 30.5845

Epoch 42: val_loss did not improve from 30.46877
196/196 - 69s - loss: 30.2067 - MinusLogProbMetric: 30.2067 - val_loss: 30.5845 - val_MinusLogProbMetric: 30.5845 - lr: 0.0010 - 69s/epoch - 354ms/step
Epoch 43/1000
2023-09-30 17:27:48.166 
Epoch 43/1000 
	 loss: 30.2372, MinusLogProbMetric: 30.2372, val_loss: 30.4854, val_MinusLogProbMetric: 30.4854

Epoch 43: val_loss did not improve from 30.46877
196/196 - 67s - loss: 30.2372 - MinusLogProbMetric: 30.2372 - val_loss: 30.4854 - val_MinusLogProbMetric: 30.4854 - lr: 0.0010 - 67s/epoch - 341ms/step
Epoch 44/1000
2023-09-30 17:28:53.540 
Epoch 44/1000 
	 loss: 30.0391, MinusLogProbMetric: 30.0391, val_loss: 30.2077, val_MinusLogProbMetric: 30.2077

Epoch 44: val_loss improved from 30.46877 to 30.20770, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_348/weights/best_weights.h5
196/196 - 68s - loss: 30.0391 - MinusLogProbMetric: 30.0391 - val_loss: 30.2077 - val_MinusLogProbMetric: 30.2077 - lr: 0.0010 - 68s/epoch - 345ms/step
Epoch 45/1000
2023-09-30 17:30:03.945 
Epoch 45/1000 
	 loss: 30.1192, MinusLogProbMetric: 30.1192, val_loss: 29.9671, val_MinusLogProbMetric: 29.9671

Epoch 45: val_loss improved from 30.20770 to 29.96709, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_348/weights/best_weights.h5
196/196 - 70s - loss: 30.1192 - MinusLogProbMetric: 30.1192 - val_loss: 29.9671 - val_MinusLogProbMetric: 29.9671 - lr: 0.0010 - 70s/epoch - 357ms/step
Epoch 46/1000
2023-09-30 17:31:12.102 
Epoch 46/1000 
	 loss: 29.9426, MinusLogProbMetric: 29.9426, val_loss: 30.1087, val_MinusLogProbMetric: 30.1087

Epoch 46: val_loss did not improve from 29.96709
196/196 - 66s - loss: 29.9426 - MinusLogProbMetric: 29.9426 - val_loss: 30.1087 - val_MinusLogProbMetric: 30.1087 - lr: 0.0010 - 66s/epoch - 339ms/step
Epoch 47/1000
2023-09-30 17:32:21.949 
Epoch 47/1000 
	 loss: 30.0020, MinusLogProbMetric: 30.0020, val_loss: 30.3973, val_MinusLogProbMetric: 30.3973

Epoch 47: val_loss did not improve from 29.96709
196/196 - 70s - loss: 30.0020 - MinusLogProbMetric: 30.0020 - val_loss: 30.3973 - val_MinusLogProbMetric: 30.3973 - lr: 0.0010 - 70s/epoch - 356ms/step
Epoch 48/1000
2023-09-30 17:33:27.997 
Epoch 48/1000 
	 loss: 29.9241, MinusLogProbMetric: 29.9241, val_loss: 30.1321, val_MinusLogProbMetric: 30.1321

Epoch 48: val_loss did not improve from 29.96709
196/196 - 66s - loss: 29.9241 - MinusLogProbMetric: 29.9241 - val_loss: 30.1321 - val_MinusLogProbMetric: 30.1321 - lr: 0.0010 - 66s/epoch - 337ms/step
Epoch 49/1000
2023-09-30 17:34:40.068 
Epoch 49/1000 
	 loss: 29.8765, MinusLogProbMetric: 29.8765, val_loss: 29.7486, val_MinusLogProbMetric: 29.7486

Epoch 49: val_loss improved from 29.96709 to 29.74855, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_348/weights/best_weights.h5
196/196 - 74s - loss: 29.8765 - MinusLogProbMetric: 29.8765 - val_loss: 29.7486 - val_MinusLogProbMetric: 29.7486 - lr: 0.0010 - 74s/epoch - 377ms/step
Epoch 50/1000
2023-09-30 17:35:50.852 
Epoch 50/1000 
	 loss: 29.8441, MinusLogProbMetric: 29.8441, val_loss: 29.6346, val_MinusLogProbMetric: 29.6346

Epoch 50: val_loss improved from 29.74855 to 29.63458, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_348/weights/best_weights.h5
196/196 - 70s - loss: 29.8441 - MinusLogProbMetric: 29.8441 - val_loss: 29.6346 - val_MinusLogProbMetric: 29.6346 - lr: 0.0010 - 70s/epoch - 356ms/step
Epoch 51/1000
2023-09-30 17:37:00.837 
Epoch 51/1000 
	 loss: 29.7970, MinusLogProbMetric: 29.7970, val_loss: 29.8655, val_MinusLogProbMetric: 29.8655

Epoch 51: val_loss did not improve from 29.63458
196/196 - 69s - loss: 29.7970 - MinusLogProbMetric: 29.7970 - val_loss: 29.8655 - val_MinusLogProbMetric: 29.8655 - lr: 0.0010 - 69s/epoch - 352ms/step
Epoch 52/1000
2023-09-30 17:38:10.795 
Epoch 52/1000 
	 loss: 29.6733, MinusLogProbMetric: 29.6733, val_loss: 30.3504, val_MinusLogProbMetric: 30.3504

Epoch 52: val_loss did not improve from 29.63458
196/196 - 70s - loss: 29.6733 - MinusLogProbMetric: 29.6733 - val_loss: 30.3504 - val_MinusLogProbMetric: 30.3504 - lr: 0.0010 - 70s/epoch - 357ms/step
Epoch 53/1000
2023-09-30 17:39:21.893 
Epoch 53/1000 
	 loss: 29.6898, MinusLogProbMetric: 29.6898, val_loss: 29.7398, val_MinusLogProbMetric: 29.7398

Epoch 53: val_loss did not improve from 29.63458
196/196 - 71s - loss: 29.6898 - MinusLogProbMetric: 29.6898 - val_loss: 29.7398 - val_MinusLogProbMetric: 29.7398 - lr: 0.0010 - 71s/epoch - 363ms/step
Epoch 54/1000
2023-09-30 17:40:28.673 
Epoch 54/1000 
	 loss: 29.6590, MinusLogProbMetric: 29.6590, val_loss: 30.3660, val_MinusLogProbMetric: 30.3660

Epoch 54: val_loss did not improve from 29.63458
196/196 - 67s - loss: 29.6590 - MinusLogProbMetric: 29.6590 - val_loss: 30.3660 - val_MinusLogProbMetric: 30.3660 - lr: 0.0010 - 67s/epoch - 341ms/step
Epoch 55/1000
2023-09-30 17:41:40.713 
Epoch 55/1000 
	 loss: 29.6814, MinusLogProbMetric: 29.6814, val_loss: 29.6880, val_MinusLogProbMetric: 29.6880

Epoch 55: val_loss did not improve from 29.63458
196/196 - 72s - loss: 29.6814 - MinusLogProbMetric: 29.6814 - val_loss: 29.6880 - val_MinusLogProbMetric: 29.6880 - lr: 0.0010 - 72s/epoch - 368ms/step
Epoch 56/1000
2023-09-30 17:42:48.053 
Epoch 56/1000 
	 loss: 29.7110, MinusLogProbMetric: 29.7110, val_loss: 29.7638, val_MinusLogProbMetric: 29.7638

Epoch 56: val_loss did not improve from 29.63458
196/196 - 67s - loss: 29.7110 - MinusLogProbMetric: 29.7110 - val_loss: 29.7638 - val_MinusLogProbMetric: 29.7638 - lr: 0.0010 - 67s/epoch - 343ms/step
Epoch 57/1000
2023-09-30 17:43:57.559 
Epoch 57/1000 
	 loss: 29.5940, MinusLogProbMetric: 29.5940, val_loss: 29.5305, val_MinusLogProbMetric: 29.5305

Epoch 57: val_loss improved from 29.63458 to 29.53055, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_348/weights/best_weights.h5
196/196 - 70s - loss: 29.5940 - MinusLogProbMetric: 29.5940 - val_loss: 29.5305 - val_MinusLogProbMetric: 29.5305 - lr: 0.0010 - 70s/epoch - 359ms/step
Epoch 58/1000
2023-09-30 17:45:08.100 
Epoch 58/1000 
	 loss: 29.5429, MinusLogProbMetric: 29.5429, val_loss: 29.8139, val_MinusLogProbMetric: 29.8139

Epoch 58: val_loss did not improve from 29.53055
196/196 - 70s - loss: 29.5429 - MinusLogProbMetric: 29.5429 - val_loss: 29.8139 - val_MinusLogProbMetric: 29.8139 - lr: 0.0010 - 70s/epoch - 355ms/step
Epoch 59/1000
2023-09-30 17:46:13.966 
Epoch 59/1000 
	 loss: 29.4743, MinusLogProbMetric: 29.4743, val_loss: 29.3232, val_MinusLogProbMetric: 29.3232

Epoch 59: val_loss improved from 29.53055 to 29.32318, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_348/weights/best_weights.h5
196/196 - 69s - loss: 29.4743 - MinusLogProbMetric: 29.4743 - val_loss: 29.3232 - val_MinusLogProbMetric: 29.3232 - lr: 0.0010 - 69s/epoch - 351ms/step
Epoch 60/1000
2023-09-30 17:47:29.829 
Epoch 60/1000 
	 loss: 29.4404, MinusLogProbMetric: 29.4404, val_loss: 30.2509, val_MinusLogProbMetric: 30.2509

Epoch 60: val_loss did not improve from 29.32318
196/196 - 73s - loss: 29.4404 - MinusLogProbMetric: 29.4404 - val_loss: 30.2509 - val_MinusLogProbMetric: 30.2509 - lr: 0.0010 - 73s/epoch - 372ms/step
Epoch 61/1000
2023-09-30 17:48:38.130 
Epoch 61/1000 
	 loss: 29.6088, MinusLogProbMetric: 29.6088, val_loss: 29.8542, val_MinusLogProbMetric: 29.8542

Epoch 61: val_loss did not improve from 29.32318
196/196 - 68s - loss: 29.6088 - MinusLogProbMetric: 29.6088 - val_loss: 29.8542 - val_MinusLogProbMetric: 29.8542 - lr: 0.0010 - 68s/epoch - 348ms/step
Epoch 62/1000
2023-09-30 17:49:49.505 
Epoch 62/1000 
	 loss: 29.4044, MinusLogProbMetric: 29.4044, val_loss: 29.8086, val_MinusLogProbMetric: 29.8086

Epoch 62: val_loss did not improve from 29.32318
196/196 - 71s - loss: 29.4044 - MinusLogProbMetric: 29.4044 - val_loss: 29.8086 - val_MinusLogProbMetric: 29.8086 - lr: 0.0010 - 71s/epoch - 364ms/step
Epoch 63/1000
2023-09-30 17:51:01.399 
Epoch 63/1000 
	 loss: 29.4350, MinusLogProbMetric: 29.4350, val_loss: 30.0241, val_MinusLogProbMetric: 30.0241

Epoch 63: val_loss did not improve from 29.32318
196/196 - 72s - loss: 29.4350 - MinusLogProbMetric: 29.4350 - val_loss: 30.0241 - val_MinusLogProbMetric: 30.0241 - lr: 0.0010 - 72s/epoch - 367ms/step
Epoch 64/1000
2023-09-30 17:52:14.862 
Epoch 64/1000 
	 loss: 29.3848, MinusLogProbMetric: 29.3848, val_loss: 30.6220, val_MinusLogProbMetric: 30.6220

Epoch 64: val_loss did not improve from 29.32318
196/196 - 73s - loss: 29.3848 - MinusLogProbMetric: 29.3848 - val_loss: 30.6220 - val_MinusLogProbMetric: 30.6220 - lr: 0.0010 - 73s/epoch - 375ms/step
Epoch 65/1000
2023-09-30 17:53:22.599 
Epoch 65/1000 
	 loss: 29.5132, MinusLogProbMetric: 29.5132, val_loss: 29.5170, val_MinusLogProbMetric: 29.5170

Epoch 65: val_loss did not improve from 29.32318
196/196 - 68s - loss: 29.5132 - MinusLogProbMetric: 29.5132 - val_loss: 29.5170 - val_MinusLogProbMetric: 29.5170 - lr: 0.0010 - 68s/epoch - 346ms/step
Epoch 66/1000
2023-09-30 17:54:29.679 
Epoch 66/1000 
	 loss: 29.3476, MinusLogProbMetric: 29.3476, val_loss: 30.5907, val_MinusLogProbMetric: 30.5907

Epoch 66: val_loss did not improve from 29.32318
196/196 - 67s - loss: 29.3476 - MinusLogProbMetric: 29.3476 - val_loss: 30.5907 - val_MinusLogProbMetric: 30.5907 - lr: 0.0010 - 67s/epoch - 342ms/step
Epoch 67/1000
2023-09-30 17:55:41.006 
Epoch 67/1000 
	 loss: 29.2974, MinusLogProbMetric: 29.2974, val_loss: 29.8945, val_MinusLogProbMetric: 29.8945

Epoch 67: val_loss did not improve from 29.32318
196/196 - 71s - loss: 29.2974 - MinusLogProbMetric: 29.2974 - val_loss: 29.8945 - val_MinusLogProbMetric: 29.8945 - lr: 0.0010 - 71s/epoch - 364ms/step
Epoch 68/1000
2023-09-30 17:56:49.550 
Epoch 68/1000 
	 loss: 29.4014, MinusLogProbMetric: 29.4014, val_loss: 29.0622, val_MinusLogProbMetric: 29.0622

Epoch 68: val_loss improved from 29.32318 to 29.06219, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_348/weights/best_weights.h5
196/196 - 71s - loss: 29.4014 - MinusLogProbMetric: 29.4014 - val_loss: 29.0622 - val_MinusLogProbMetric: 29.0622 - lr: 0.0010 - 71s/epoch - 361ms/step
Epoch 69/1000
2023-09-30 17:57:58.548 
Epoch 69/1000 
	 loss: 29.4210, MinusLogProbMetric: 29.4210, val_loss: 29.6496, val_MinusLogProbMetric: 29.6496

Epoch 69: val_loss did not improve from 29.06219
196/196 - 67s - loss: 29.4210 - MinusLogProbMetric: 29.4210 - val_loss: 29.6496 - val_MinusLogProbMetric: 29.6496 - lr: 0.0010 - 67s/epoch - 340ms/step
Epoch 70/1000
2023-09-30 17:59:05.752 
Epoch 70/1000 
	 loss: 29.2332, MinusLogProbMetric: 29.2332, val_loss: 29.1497, val_MinusLogProbMetric: 29.1497

Epoch 70: val_loss did not improve from 29.06219
196/196 - 67s - loss: 29.2332 - MinusLogProbMetric: 29.2332 - val_loss: 29.1497 - val_MinusLogProbMetric: 29.1497 - lr: 0.0010 - 67s/epoch - 343ms/step
Epoch 71/1000
2023-09-30 18:00:14.292 
Epoch 71/1000 
	 loss: 29.3673, MinusLogProbMetric: 29.3673, val_loss: 29.5611, val_MinusLogProbMetric: 29.5611

Epoch 71: val_loss did not improve from 29.06219
196/196 - 69s - loss: 29.3673 - MinusLogProbMetric: 29.3673 - val_loss: 29.5611 - val_MinusLogProbMetric: 29.5611 - lr: 0.0010 - 69s/epoch - 350ms/step
Epoch 72/1000
2023-09-30 18:01:21.488 
Epoch 72/1000 
	 loss: 29.2080, MinusLogProbMetric: 29.2080, val_loss: 29.7959, val_MinusLogProbMetric: 29.7959

Epoch 72: val_loss did not improve from 29.06219
196/196 - 67s - loss: 29.2080 - MinusLogProbMetric: 29.2080 - val_loss: 29.7959 - val_MinusLogProbMetric: 29.7959 - lr: 0.0010 - 67s/epoch - 343ms/step
Epoch 73/1000
2023-09-30 18:02:27.792 
Epoch 73/1000 
	 loss: 29.2836, MinusLogProbMetric: 29.2836, val_loss: 29.4113, val_MinusLogProbMetric: 29.4113

Epoch 73: val_loss did not improve from 29.06219
196/196 - 66s - loss: 29.2836 - MinusLogProbMetric: 29.2836 - val_loss: 29.4113 - val_MinusLogProbMetric: 29.4113 - lr: 0.0010 - 66s/epoch - 338ms/step
Epoch 74/1000
2023-09-30 18:03:35.957 
Epoch 74/1000 
	 loss: 29.1185, MinusLogProbMetric: 29.1185, val_loss: 29.4381, val_MinusLogProbMetric: 29.4381

Epoch 74: val_loss did not improve from 29.06219
196/196 - 68s - loss: 29.1185 - MinusLogProbMetric: 29.1185 - val_loss: 29.4381 - val_MinusLogProbMetric: 29.4381 - lr: 0.0010 - 68s/epoch - 348ms/step
Epoch 75/1000
2023-09-30 18:04:45.765 
Epoch 75/1000 
	 loss: 29.1742, MinusLogProbMetric: 29.1742, val_loss: 29.6165, val_MinusLogProbMetric: 29.6165

Epoch 75: val_loss did not improve from 29.06219
196/196 - 70s - loss: 29.1742 - MinusLogProbMetric: 29.1742 - val_loss: 29.6165 - val_MinusLogProbMetric: 29.6165 - lr: 0.0010 - 70s/epoch - 356ms/step
Epoch 76/1000
2023-09-30 18:05:53.862 
Epoch 76/1000 
	 loss: 29.1739, MinusLogProbMetric: 29.1739, val_loss: 29.7030, val_MinusLogProbMetric: 29.7030

Epoch 76: val_loss did not improve from 29.06219
196/196 - 68s - loss: 29.1739 - MinusLogProbMetric: 29.1739 - val_loss: 29.7030 - val_MinusLogProbMetric: 29.7030 - lr: 0.0010 - 68s/epoch - 347ms/step
Epoch 77/1000
2023-09-30 18:07:00.853 
Epoch 77/1000 
	 loss: 29.1746, MinusLogProbMetric: 29.1746, val_loss: 29.4136, val_MinusLogProbMetric: 29.4136

Epoch 77: val_loss did not improve from 29.06219
196/196 - 67s - loss: 29.1746 - MinusLogProbMetric: 29.1746 - val_loss: 29.4136 - val_MinusLogProbMetric: 29.4136 - lr: 0.0010 - 67s/epoch - 342ms/step
Epoch 78/1000
2023-09-30 18:08:08.449 
Epoch 78/1000 
	 loss: 29.1033, MinusLogProbMetric: 29.1033, val_loss: 29.4189, val_MinusLogProbMetric: 29.4189

Epoch 78: val_loss did not improve from 29.06219
196/196 - 68s - loss: 29.1033 - MinusLogProbMetric: 29.1033 - val_loss: 29.4189 - val_MinusLogProbMetric: 29.4189 - lr: 0.0010 - 68s/epoch - 345ms/step
Epoch 79/1000
2023-09-30 18:09:15.629 
Epoch 79/1000 
	 loss: 29.3031, MinusLogProbMetric: 29.3031, val_loss: 30.0291, val_MinusLogProbMetric: 30.0291

Epoch 79: val_loss did not improve from 29.06219
196/196 - 67s - loss: 29.3031 - MinusLogProbMetric: 29.3031 - val_loss: 30.0291 - val_MinusLogProbMetric: 30.0291 - lr: 0.0010 - 67s/epoch - 343ms/step
Epoch 80/1000
2023-09-30 18:10:29.122 
Epoch 80/1000 
	 loss: 29.2579, MinusLogProbMetric: 29.2579, val_loss: 30.4277, val_MinusLogProbMetric: 30.4277

Epoch 80: val_loss did not improve from 29.06219
196/196 - 73s - loss: 29.2579 - MinusLogProbMetric: 29.2579 - val_loss: 30.4277 - val_MinusLogProbMetric: 30.4277 - lr: 0.0010 - 73s/epoch - 375ms/step
Epoch 81/1000
2023-09-30 18:11:36.886 
Epoch 81/1000 
	 loss: 30.2465, MinusLogProbMetric: 30.2465, val_loss: 29.8239, val_MinusLogProbMetric: 29.8239

Epoch 81: val_loss did not improve from 29.06219
196/196 - 68s - loss: 30.2465 - MinusLogProbMetric: 30.2465 - val_loss: 29.8239 - val_MinusLogProbMetric: 29.8239 - lr: 0.0010 - 68s/epoch - 346ms/step
Epoch 82/1000
2023-09-30 18:12:42.833 
Epoch 82/1000 
	 loss: 29.3732, MinusLogProbMetric: 29.3732, val_loss: 29.8072, val_MinusLogProbMetric: 29.8072

Epoch 82: val_loss did not improve from 29.06219
196/196 - 66s - loss: 29.3732 - MinusLogProbMetric: 29.3732 - val_loss: 29.8072 - val_MinusLogProbMetric: 29.8072 - lr: 0.0010 - 66s/epoch - 336ms/step
Epoch 83/1000
2023-09-30 18:13:49.373 
Epoch 83/1000 
	 loss: 29.1954, MinusLogProbMetric: 29.1954, val_loss: 29.5806, val_MinusLogProbMetric: 29.5806

Epoch 83: val_loss did not improve from 29.06219
196/196 - 67s - loss: 29.1954 - MinusLogProbMetric: 29.1954 - val_loss: 29.5806 - val_MinusLogProbMetric: 29.5806 - lr: 0.0010 - 67s/epoch - 339ms/step
Epoch 84/1000
2023-09-30 18:14:55.983 
Epoch 84/1000 
	 loss: 29.1642, MinusLogProbMetric: 29.1642, val_loss: 29.6623, val_MinusLogProbMetric: 29.6623

Epoch 84: val_loss did not improve from 29.06219
196/196 - 67s - loss: 29.1642 - MinusLogProbMetric: 29.1642 - val_loss: 29.6623 - val_MinusLogProbMetric: 29.6623 - lr: 0.0010 - 67s/epoch - 340ms/step
Epoch 85/1000
2023-09-30 18:16:05.023 
Epoch 85/1000 
	 loss: 29.3405, MinusLogProbMetric: 29.3405, val_loss: 29.7107, val_MinusLogProbMetric: 29.7107

Epoch 85: val_loss did not improve from 29.06219
196/196 - 69s - loss: 29.3405 - MinusLogProbMetric: 29.3405 - val_loss: 29.7107 - val_MinusLogProbMetric: 29.7107 - lr: 0.0010 - 69s/epoch - 352ms/step
Epoch 86/1000
2023-09-30 18:17:14.744 
Epoch 86/1000 
	 loss: 29.2267, MinusLogProbMetric: 29.2267, val_loss: 30.8284, val_MinusLogProbMetric: 30.8284

Epoch 86: val_loss did not improve from 29.06219
196/196 - 70s - loss: 29.2267 - MinusLogProbMetric: 29.2267 - val_loss: 30.8284 - val_MinusLogProbMetric: 30.8284 - lr: 0.0010 - 70s/epoch - 356ms/step
Epoch 87/1000
2023-09-30 18:18:24.923 
Epoch 87/1000 
	 loss: 29.3490, MinusLogProbMetric: 29.3490, val_loss: 29.3375, val_MinusLogProbMetric: 29.3375

Epoch 87: val_loss did not improve from 29.06219
196/196 - 70s - loss: 29.3490 - MinusLogProbMetric: 29.3490 - val_loss: 29.3375 - val_MinusLogProbMetric: 29.3375 - lr: 0.0010 - 70s/epoch - 358ms/step
Epoch 88/1000
2023-09-30 18:19:32.297 
Epoch 88/1000 
	 loss: 29.0631, MinusLogProbMetric: 29.0631, val_loss: 29.9247, val_MinusLogProbMetric: 29.9247

Epoch 88: val_loss did not improve from 29.06219
196/196 - 67s - loss: 29.0631 - MinusLogProbMetric: 29.0631 - val_loss: 29.9247 - val_MinusLogProbMetric: 29.9247 - lr: 0.0010 - 67s/epoch - 344ms/step
Epoch 89/1000
2023-09-30 18:20:40.294 
Epoch 89/1000 
	 loss: 29.3259, MinusLogProbMetric: 29.3259, val_loss: 30.1859, val_MinusLogProbMetric: 30.1859

Epoch 89: val_loss did not improve from 29.06219
196/196 - 68s - loss: 29.3259 - MinusLogProbMetric: 29.3259 - val_loss: 30.1859 - val_MinusLogProbMetric: 30.1859 - lr: 0.0010 - 68s/epoch - 347ms/step
Epoch 90/1000
2023-09-30 18:21:49.405 
Epoch 90/1000 
	 loss: 29.1149, MinusLogProbMetric: 29.1149, val_loss: 30.2721, val_MinusLogProbMetric: 30.2721

Epoch 90: val_loss did not improve from 29.06219
196/196 - 69s - loss: 29.1149 - MinusLogProbMetric: 29.1149 - val_loss: 30.2721 - val_MinusLogProbMetric: 30.2721 - lr: 0.0010 - 69s/epoch - 352ms/step
Epoch 91/1000
2023-09-30 18:22:58.953 
Epoch 91/1000 
	 loss: 29.0253, MinusLogProbMetric: 29.0253, val_loss: 29.1877, val_MinusLogProbMetric: 29.1877

Epoch 91: val_loss did not improve from 29.06219
196/196 - 70s - loss: 29.0253 - MinusLogProbMetric: 29.0253 - val_loss: 29.1877 - val_MinusLogProbMetric: 29.1877 - lr: 0.0010 - 70s/epoch - 355ms/step
Epoch 92/1000
2023-09-30 18:24:06.809 
Epoch 92/1000 
	 loss: 29.0647, MinusLogProbMetric: 29.0647, val_loss: 30.0628, val_MinusLogProbMetric: 30.0628

Epoch 92: val_loss did not improve from 29.06219
196/196 - 68s - loss: 29.0647 - MinusLogProbMetric: 29.0647 - val_loss: 30.0628 - val_MinusLogProbMetric: 30.0628 - lr: 0.0010 - 68s/epoch - 346ms/step
Epoch 93/1000
2023-09-30 18:25:13.840 
Epoch 93/1000 
	 loss: 29.0783, MinusLogProbMetric: 29.0783, val_loss: 29.5460, val_MinusLogProbMetric: 29.5460

Epoch 93: val_loss did not improve from 29.06219
196/196 - 67s - loss: 29.0783 - MinusLogProbMetric: 29.0783 - val_loss: 29.5460 - val_MinusLogProbMetric: 29.5460 - lr: 0.0010 - 67s/epoch - 342ms/step
Epoch 94/1000
2023-09-30 18:26:21.008 
Epoch 94/1000 
	 loss: 28.9686, MinusLogProbMetric: 28.9686, val_loss: 29.7337, val_MinusLogProbMetric: 29.7337

Epoch 94: val_loss did not improve from 29.06219
196/196 - 67s - loss: 28.9686 - MinusLogProbMetric: 28.9686 - val_loss: 29.7337 - val_MinusLogProbMetric: 29.7337 - lr: 0.0010 - 67s/epoch - 343ms/step
Epoch 95/1000
2023-09-30 18:27:28.059 
Epoch 95/1000 
	 loss: 29.2387, MinusLogProbMetric: 29.2387, val_loss: 30.9018, val_MinusLogProbMetric: 30.9018

Epoch 95: val_loss did not improve from 29.06219
196/196 - 67s - loss: 29.2387 - MinusLogProbMetric: 29.2387 - val_loss: 30.9018 - val_MinusLogProbMetric: 30.9018 - lr: 0.0010 - 67s/epoch - 342ms/step
Epoch 96/1000
2023-09-30 18:28:34.882 
Epoch 96/1000 
	 loss: 29.1149, MinusLogProbMetric: 29.1149, val_loss: 29.7323, val_MinusLogProbMetric: 29.7323

Epoch 96: val_loss did not improve from 29.06219
196/196 - 67s - loss: 29.1149 - MinusLogProbMetric: 29.1149 - val_loss: 29.7323 - val_MinusLogProbMetric: 29.7323 - lr: 0.0010 - 67s/epoch - 341ms/step
Epoch 97/1000
2023-09-30 18:29:43.073 
Epoch 97/1000 
	 loss: 29.0968, MinusLogProbMetric: 29.0968, val_loss: 30.1219, val_MinusLogProbMetric: 30.1219

Epoch 97: val_loss did not improve from 29.06219
196/196 - 68s - loss: 29.0968 - MinusLogProbMetric: 29.0968 - val_loss: 30.1219 - val_MinusLogProbMetric: 30.1219 - lr: 0.0010 - 68s/epoch - 348ms/step
Epoch 98/1000
2023-09-30 18:30:50.403 
Epoch 98/1000 
	 loss: 29.0616, MinusLogProbMetric: 29.0616, val_loss: 29.3470, val_MinusLogProbMetric: 29.3470

Epoch 98: val_loss did not improve from 29.06219
196/196 - 67s - loss: 29.0616 - MinusLogProbMetric: 29.0616 - val_loss: 29.3470 - val_MinusLogProbMetric: 29.3470 - lr: 0.0010 - 67s/epoch - 343ms/step
Epoch 99/1000
2023-09-30 18:31:57.675 
Epoch 99/1000 
	 loss: 29.0581, MinusLogProbMetric: 29.0581, val_loss: 29.1590, val_MinusLogProbMetric: 29.1590

Epoch 99: val_loss did not improve from 29.06219
196/196 - 67s - loss: 29.0581 - MinusLogProbMetric: 29.0581 - val_loss: 29.1590 - val_MinusLogProbMetric: 29.1590 - lr: 0.0010 - 67s/epoch - 343ms/step
Epoch 100/1000
2023-09-30 18:33:08.150 
Epoch 100/1000 
	 loss: 29.0085, MinusLogProbMetric: 29.0085, val_loss: 29.2766, val_MinusLogProbMetric: 29.2766

Epoch 100: val_loss did not improve from 29.06219
196/196 - 70s - loss: 29.0085 - MinusLogProbMetric: 29.0085 - val_loss: 29.2766 - val_MinusLogProbMetric: 29.2766 - lr: 0.0010 - 70s/epoch - 360ms/step
Epoch 101/1000
2023-09-30 18:34:15.409 
Epoch 101/1000 
	 loss: 29.0115, MinusLogProbMetric: 29.0115, val_loss: 29.6881, val_MinusLogProbMetric: 29.6881

Epoch 101: val_loss did not improve from 29.06219
196/196 - 67s - loss: 29.0115 - MinusLogProbMetric: 29.0115 - val_loss: 29.6881 - val_MinusLogProbMetric: 29.6881 - lr: 0.0010 - 67s/epoch - 343ms/step
Epoch 102/1000
2023-09-30 18:35:21.927 
Epoch 102/1000 
	 loss: 29.0035, MinusLogProbMetric: 29.0035, val_loss: 29.6017, val_MinusLogProbMetric: 29.6017

Epoch 102: val_loss did not improve from 29.06219
196/196 - 66s - loss: 29.0035 - MinusLogProbMetric: 29.0035 - val_loss: 29.6017 - val_MinusLogProbMetric: 29.6017 - lr: 0.0010 - 66s/epoch - 339ms/step
Epoch 103/1000
2023-09-30 18:36:31.333 
Epoch 103/1000 
	 loss: 28.8672, MinusLogProbMetric: 28.8672, val_loss: 29.6280, val_MinusLogProbMetric: 29.6280

Epoch 103: val_loss did not improve from 29.06219
196/196 - 69s - loss: 28.8672 - MinusLogProbMetric: 28.8672 - val_loss: 29.6280 - val_MinusLogProbMetric: 29.6280 - lr: 0.0010 - 69s/epoch - 354ms/step
Epoch 104/1000
2023-09-30 18:37:39.792 
Epoch 104/1000 
	 loss: 29.0510, MinusLogProbMetric: 29.0510, val_loss: 29.0632, val_MinusLogProbMetric: 29.0632

Epoch 104: val_loss did not improve from 29.06219
196/196 - 68s - loss: 29.0510 - MinusLogProbMetric: 29.0510 - val_loss: 29.0632 - val_MinusLogProbMetric: 29.0632 - lr: 0.0010 - 68s/epoch - 349ms/step
Epoch 105/1000
2023-09-30 18:38:49.607 
Epoch 105/1000 
	 loss: 28.9606, MinusLogProbMetric: 28.9606, val_loss: 29.4301, val_MinusLogProbMetric: 29.4301

Epoch 105: val_loss did not improve from 29.06219
196/196 - 70s - loss: 28.9606 - MinusLogProbMetric: 28.9606 - val_loss: 29.4301 - val_MinusLogProbMetric: 29.4301 - lr: 0.0010 - 70s/epoch - 356ms/step
Epoch 106/1000
2023-09-30 18:39:59.314 
Epoch 106/1000 
	 loss: 28.9692, MinusLogProbMetric: 28.9692, val_loss: 29.3154, val_MinusLogProbMetric: 29.3154

Epoch 106: val_loss did not improve from 29.06219
196/196 - 70s - loss: 28.9692 - MinusLogProbMetric: 28.9692 - val_loss: 29.3154 - val_MinusLogProbMetric: 29.3154 - lr: 0.0010 - 70s/epoch - 356ms/step
Epoch 107/1000
2023-09-30 18:41:34.342 
Epoch 107/1000 
	 loss: 29.1137, MinusLogProbMetric: 29.1137, val_loss: 29.0197, val_MinusLogProbMetric: 29.0197

Epoch 107: val_loss improved from 29.06219 to 29.01967, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_348/weights/best_weights.h5
196/196 - 97s - loss: 29.1137 - MinusLogProbMetric: 29.1137 - val_loss: 29.0197 - val_MinusLogProbMetric: 29.0197 - lr: 0.0010 - 97s/epoch - 495ms/step
Epoch 108/1000
2023-09-30 18:43:12.265 
Epoch 108/1000 
	 loss: 28.7700, MinusLogProbMetric: 28.7700, val_loss: 29.4527, val_MinusLogProbMetric: 29.4527

Epoch 108: val_loss did not improve from 29.01967
196/196 - 96s - loss: 28.7700 - MinusLogProbMetric: 28.7700 - val_loss: 29.4527 - val_MinusLogProbMetric: 29.4527 - lr: 0.0010 - 96s/epoch - 490ms/step
Epoch 109/1000
2023-09-30 18:44:53.201 
Epoch 109/1000 
	 loss: 28.7390, MinusLogProbMetric: 28.7390, val_loss: 30.6506, val_MinusLogProbMetric: 30.6506

Epoch 109: val_loss did not improve from 29.01967
196/196 - 101s - loss: 28.7390 - MinusLogProbMetric: 28.7390 - val_loss: 30.6506 - val_MinusLogProbMetric: 30.6506 - lr: 0.0010 - 101s/epoch - 515ms/step
Epoch 110/1000
2023-09-30 18:46:29.360 
Epoch 110/1000 
	 loss: 29.0201, MinusLogProbMetric: 29.0201, val_loss: 29.4479, val_MinusLogProbMetric: 29.4479

Epoch 110: val_loss did not improve from 29.01967
196/196 - 96s - loss: 29.0201 - MinusLogProbMetric: 29.0201 - val_loss: 29.4479 - val_MinusLogProbMetric: 29.4479 - lr: 0.0010 - 96s/epoch - 491ms/step
Epoch 111/1000
2023-09-30 18:48:07.700 
Epoch 111/1000 
	 loss: 28.9257, MinusLogProbMetric: 28.9257, val_loss: 29.5372, val_MinusLogProbMetric: 29.5372

Epoch 111: val_loss did not improve from 29.01967
196/196 - 98s - loss: 28.9257 - MinusLogProbMetric: 28.9257 - val_loss: 29.5372 - val_MinusLogProbMetric: 29.5372 - lr: 0.0010 - 98s/epoch - 502ms/step
Epoch 112/1000
2023-09-30 18:49:51.632 
Epoch 112/1000 
	 loss: 28.7791, MinusLogProbMetric: 28.7791, val_loss: 29.3165, val_MinusLogProbMetric: 29.3165

Epoch 112: val_loss did not improve from 29.01967
196/196 - 104s - loss: 28.7791 - MinusLogProbMetric: 28.7791 - val_loss: 29.3165 - val_MinusLogProbMetric: 29.3165 - lr: 0.0010 - 104s/epoch - 530ms/step
Epoch 113/1000
2023-09-30 18:51:25.558 
Epoch 113/1000 
	 loss: 28.7730, MinusLogProbMetric: 28.7730, val_loss: 30.0992, val_MinusLogProbMetric: 30.0992

Epoch 113: val_loss did not improve from 29.01967
196/196 - 94s - loss: 28.7730 - MinusLogProbMetric: 28.7730 - val_loss: 30.0992 - val_MinusLogProbMetric: 30.0992 - lr: 0.0010 - 94s/epoch - 479ms/step
Epoch 114/1000
2023-09-30 18:53:05.508 
Epoch 114/1000 
	 loss: 28.8821, MinusLogProbMetric: 28.8821, val_loss: 30.1431, val_MinusLogProbMetric: 30.1431

Epoch 114: val_loss did not improve from 29.01967
196/196 - 100s - loss: 28.8821 - MinusLogProbMetric: 28.8821 - val_loss: 30.1431 - val_MinusLogProbMetric: 30.1431 - lr: 0.0010 - 100s/epoch - 510ms/step
Epoch 115/1000
2023-09-30 18:54:40.355 
Epoch 115/1000 
	 loss: 28.8426, MinusLogProbMetric: 28.8426, val_loss: 30.0308, val_MinusLogProbMetric: 30.0308

Epoch 115: val_loss did not improve from 29.01967
196/196 - 95s - loss: 28.8426 - MinusLogProbMetric: 28.8426 - val_loss: 30.0308 - val_MinusLogProbMetric: 30.0308 - lr: 0.0010 - 95s/epoch - 484ms/step
Epoch 116/1000
2023-09-30 18:56:16.783 
Epoch 116/1000 
	 loss: 28.7529, MinusLogProbMetric: 28.7529, val_loss: 30.2109, val_MinusLogProbMetric: 30.2109

Epoch 116: val_loss did not improve from 29.01967
196/196 - 96s - loss: 28.7529 - MinusLogProbMetric: 28.7529 - val_loss: 30.2109 - val_MinusLogProbMetric: 30.2109 - lr: 0.0010 - 96s/epoch - 492ms/step
Epoch 117/1000
2023-09-30 18:57:50.307 
Epoch 117/1000 
	 loss: 28.9037, MinusLogProbMetric: 28.9037, val_loss: 29.4761, val_MinusLogProbMetric: 29.4761

Epoch 117: val_loss did not improve from 29.01967
196/196 - 94s - loss: 28.9037 - MinusLogProbMetric: 28.9037 - val_loss: 29.4761 - val_MinusLogProbMetric: 29.4761 - lr: 0.0010 - 94s/epoch - 477ms/step
Epoch 118/1000
2023-09-30 18:59:29.292 
Epoch 118/1000 
	 loss: 28.8119, MinusLogProbMetric: 28.8119, val_loss: 29.2283, val_MinusLogProbMetric: 29.2283

Epoch 118: val_loss did not improve from 29.01967
196/196 - 99s - loss: 28.8119 - MinusLogProbMetric: 28.8119 - val_loss: 29.2283 - val_MinusLogProbMetric: 29.2283 - lr: 0.0010 - 99s/epoch - 505ms/step
Epoch 119/1000
2023-09-30 19:01:04.883 
Epoch 119/1000 
	 loss: 28.7278, MinusLogProbMetric: 28.7278, val_loss: 29.7111, val_MinusLogProbMetric: 29.7111

Epoch 119: val_loss did not improve from 29.01967
196/196 - 96s - loss: 28.7278 - MinusLogProbMetric: 28.7278 - val_loss: 29.7111 - val_MinusLogProbMetric: 29.7111 - lr: 0.0010 - 96s/epoch - 488ms/step
Epoch 120/1000
2023-09-30 19:02:36.448 
Epoch 120/1000 
	 loss: 28.7330, MinusLogProbMetric: 28.7330, val_loss: 29.7180, val_MinusLogProbMetric: 29.7180

Epoch 120: val_loss did not improve from 29.01967
196/196 - 92s - loss: 28.7330 - MinusLogProbMetric: 28.7330 - val_loss: 29.7180 - val_MinusLogProbMetric: 29.7180 - lr: 0.0010 - 92s/epoch - 467ms/step
Epoch 121/1000
2023-09-30 19:04:08.432 
Epoch 121/1000 
	 loss: 28.8014, MinusLogProbMetric: 28.8014, val_loss: 28.9333, val_MinusLogProbMetric: 28.9333

Epoch 121: val_loss improved from 29.01967 to 28.93331, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_348/weights/best_weights.h5
196/196 - 94s - loss: 28.8014 - MinusLogProbMetric: 28.8014 - val_loss: 28.9333 - val_MinusLogProbMetric: 28.9333 - lr: 0.0010 - 94s/epoch - 480ms/step
Epoch 122/1000
2023-09-30 19:05:42.678 
Epoch 122/1000 
	 loss: 28.7015, MinusLogProbMetric: 28.7015, val_loss: 29.4671, val_MinusLogProbMetric: 29.4671

Epoch 122: val_loss did not improve from 28.93331
196/196 - 92s - loss: 28.7015 - MinusLogProbMetric: 28.7015 - val_loss: 29.4671 - val_MinusLogProbMetric: 29.4671 - lr: 0.0010 - 92s/epoch - 470ms/step
Epoch 123/1000
2023-09-30 19:07:15.080 
Epoch 123/1000 
	 loss: 28.6289, MinusLogProbMetric: 28.6289, val_loss: 29.4342, val_MinusLogProbMetric: 29.4342

Epoch 123: val_loss did not improve from 28.93331
196/196 - 92s - loss: 28.6289 - MinusLogProbMetric: 28.6289 - val_loss: 29.4342 - val_MinusLogProbMetric: 29.4342 - lr: 0.0010 - 92s/epoch - 471ms/step
Epoch 124/1000
2023-09-30 19:08:48.037 
Epoch 124/1000 
	 loss: 28.6969, MinusLogProbMetric: 28.6969, val_loss: 29.2238, val_MinusLogProbMetric: 29.2238

Epoch 124: val_loss did not improve from 28.93331
196/196 - 93s - loss: 28.6969 - MinusLogProbMetric: 28.6969 - val_loss: 29.2238 - val_MinusLogProbMetric: 29.2238 - lr: 0.0010 - 93s/epoch - 474ms/step
Epoch 125/1000
2023-09-30 19:10:21.461 
Epoch 125/1000 
	 loss: 28.8625, MinusLogProbMetric: 28.8625, val_loss: 29.8315, val_MinusLogProbMetric: 29.8315

Epoch 125: val_loss did not improve from 28.93331
196/196 - 93s - loss: 28.8625 - MinusLogProbMetric: 28.8625 - val_loss: 29.8315 - val_MinusLogProbMetric: 29.8315 - lr: 0.0010 - 93s/epoch - 477ms/step
Epoch 126/1000
2023-09-30 19:11:54.106 
Epoch 126/1000 
	 loss: 28.7178, MinusLogProbMetric: 28.7178, val_loss: 29.0508, val_MinusLogProbMetric: 29.0508

Epoch 126: val_loss did not improve from 28.93331
196/196 - 93s - loss: 28.7178 - MinusLogProbMetric: 28.7178 - val_loss: 29.0508 - val_MinusLogProbMetric: 29.0508 - lr: 0.0010 - 93s/epoch - 473ms/step
Epoch 127/1000
2023-09-30 19:13:29.603 
Epoch 127/1000 
	 loss: 28.6610, MinusLogProbMetric: 28.6610, val_loss: 29.8055, val_MinusLogProbMetric: 29.8055

Epoch 127: val_loss did not improve from 28.93331
196/196 - 95s - loss: 28.6610 - MinusLogProbMetric: 28.6610 - val_loss: 29.8055 - val_MinusLogProbMetric: 29.8055 - lr: 0.0010 - 95s/epoch - 487ms/step
Epoch 128/1000
2023-09-30 19:15:05.677 
Epoch 128/1000 
	 loss: 28.6048, MinusLogProbMetric: 28.6048, val_loss: 29.5662, val_MinusLogProbMetric: 29.5662

Epoch 128: val_loss did not improve from 28.93331
196/196 - 96s - loss: 28.6048 - MinusLogProbMetric: 28.6048 - val_loss: 29.5662 - val_MinusLogProbMetric: 29.5662 - lr: 0.0010 - 96s/epoch - 490ms/step
Epoch 129/1000
2023-09-30 19:16:38.692 
Epoch 129/1000 
	 loss: 28.8043, MinusLogProbMetric: 28.8043, val_loss: 29.4101, val_MinusLogProbMetric: 29.4101

Epoch 129: val_loss did not improve from 28.93331
196/196 - 93s - loss: 28.8043 - MinusLogProbMetric: 28.8043 - val_loss: 29.4101 - val_MinusLogProbMetric: 29.4101 - lr: 0.0010 - 93s/epoch - 475ms/step
Epoch 130/1000
2023-09-30 19:18:11.644 
Epoch 130/1000 
	 loss: 28.6000, MinusLogProbMetric: 28.6000, val_loss: 29.2161, val_MinusLogProbMetric: 29.2161

Epoch 130: val_loss did not improve from 28.93331
196/196 - 93s - loss: 28.6000 - MinusLogProbMetric: 28.6000 - val_loss: 29.2161 - val_MinusLogProbMetric: 29.2161 - lr: 0.0010 - 93s/epoch - 474ms/step
Epoch 131/1000
2023-09-30 19:19:46.961 
Epoch 131/1000 
	 loss: 28.8590, MinusLogProbMetric: 28.8590, val_loss: 28.9799, val_MinusLogProbMetric: 28.9799

Epoch 131: val_loss did not improve from 28.93331
196/196 - 95s - loss: 28.8590 - MinusLogProbMetric: 28.8590 - val_loss: 28.9799 - val_MinusLogProbMetric: 28.9799 - lr: 0.0010 - 95s/epoch - 486ms/step
Epoch 132/1000
2023-09-30 19:21:19.085 
Epoch 132/1000 
	 loss: 28.6280, MinusLogProbMetric: 28.6280, val_loss: 28.9304, val_MinusLogProbMetric: 28.9304

Epoch 132: val_loss improved from 28.93331 to 28.93038, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_348/weights/best_weights.h5
196/196 - 95s - loss: 28.6280 - MinusLogProbMetric: 28.6280 - val_loss: 28.9304 - val_MinusLogProbMetric: 28.9304 - lr: 0.0010 - 95s/epoch - 483ms/step
Epoch 133/1000
2023-09-30 19:22:55.788 
Epoch 133/1000 
	 loss: 28.5673, MinusLogProbMetric: 28.5673, val_loss: 29.0704, val_MinusLogProbMetric: 29.0704

Epoch 133: val_loss did not improve from 28.93038
196/196 - 94s - loss: 28.5673 - MinusLogProbMetric: 28.5673 - val_loss: 29.0704 - val_MinusLogProbMetric: 29.0704 - lr: 0.0010 - 94s/epoch - 480ms/step
Epoch 134/1000
2023-09-30 19:24:28.272 
Epoch 134/1000 
	 loss: 28.6478, MinusLogProbMetric: 28.6478, val_loss: 29.4294, val_MinusLogProbMetric: 29.4294

Epoch 134: val_loss did not improve from 28.93038
196/196 - 92s - loss: 28.6478 - MinusLogProbMetric: 28.6478 - val_loss: 29.4294 - val_MinusLogProbMetric: 29.4294 - lr: 0.0010 - 92s/epoch - 472ms/step
Epoch 135/1000
2023-09-30 19:26:01.976 
Epoch 135/1000 
	 loss: 28.6555, MinusLogProbMetric: 28.6555, val_loss: 29.5446, val_MinusLogProbMetric: 29.5446

Epoch 135: val_loss did not improve from 28.93038
196/196 - 94s - loss: 28.6555 - MinusLogProbMetric: 28.6555 - val_loss: 29.5446 - val_MinusLogProbMetric: 29.5446 - lr: 0.0010 - 94s/epoch - 478ms/step
Epoch 136/1000
2023-09-30 19:27:37.065 
Epoch 136/1000 
	 loss: 28.6440, MinusLogProbMetric: 28.6440, val_loss: 29.0312, val_MinusLogProbMetric: 29.0312

Epoch 136: val_loss did not improve from 28.93038
196/196 - 95s - loss: 28.6440 - MinusLogProbMetric: 28.6440 - val_loss: 29.0312 - val_MinusLogProbMetric: 29.0312 - lr: 0.0010 - 95s/epoch - 485ms/step
Epoch 137/1000
2023-09-30 19:29:12.473 
Epoch 137/1000 
	 loss: 28.6733, MinusLogProbMetric: 28.6733, val_loss: 29.3861, val_MinusLogProbMetric: 29.3861

Epoch 137: val_loss did not improve from 28.93038
196/196 - 95s - loss: 28.6733 - MinusLogProbMetric: 28.6733 - val_loss: 29.3861 - val_MinusLogProbMetric: 29.3861 - lr: 0.0010 - 95s/epoch - 487ms/step
Epoch 138/1000
2023-09-30 19:30:47.449 
Epoch 138/1000 
	 loss: 28.6558, MinusLogProbMetric: 28.6558, val_loss: 28.9896, val_MinusLogProbMetric: 28.9896

Epoch 138: val_loss did not improve from 28.93038
196/196 - 95s - loss: 28.6558 - MinusLogProbMetric: 28.6558 - val_loss: 28.9896 - val_MinusLogProbMetric: 28.9896 - lr: 0.0010 - 95s/epoch - 485ms/step
Epoch 139/1000
2023-09-30 19:32:22.890 
Epoch 139/1000 
	 loss: 28.5169, MinusLogProbMetric: 28.5169, val_loss: 29.2736, val_MinusLogProbMetric: 29.2736

Epoch 139: val_loss did not improve from 28.93038
196/196 - 95s - loss: 28.5169 - MinusLogProbMetric: 28.5169 - val_loss: 29.2736 - val_MinusLogProbMetric: 29.2736 - lr: 0.0010 - 95s/epoch - 487ms/step
Epoch 140/1000
2023-09-30 19:33:56.912 
Epoch 140/1000 
	 loss: 28.4861, MinusLogProbMetric: 28.4861, val_loss: 29.3181, val_MinusLogProbMetric: 29.3181

Epoch 140: val_loss did not improve from 28.93038
196/196 - 94s - loss: 28.4861 - MinusLogProbMetric: 28.4861 - val_loss: 29.3181 - val_MinusLogProbMetric: 29.3181 - lr: 0.0010 - 94s/epoch - 480ms/step
Epoch 141/1000
2023-09-30 19:35:34.949 
Epoch 141/1000 
	 loss: 28.6030, MinusLogProbMetric: 28.6030, val_loss: 29.7854, val_MinusLogProbMetric: 29.7854

Epoch 141: val_loss did not improve from 28.93038
196/196 - 98s - loss: 28.6030 - MinusLogProbMetric: 28.6030 - val_loss: 29.7854 - val_MinusLogProbMetric: 29.7854 - lr: 0.0010 - 98s/epoch - 500ms/step
Epoch 142/1000
2023-09-30 19:37:11.685 
Epoch 142/1000 
	 loss: 28.6511, MinusLogProbMetric: 28.6511, val_loss: 29.3212, val_MinusLogProbMetric: 29.3212

Epoch 142: val_loss did not improve from 28.93038
196/196 - 97s - loss: 28.6511 - MinusLogProbMetric: 28.6511 - val_loss: 29.3212 - val_MinusLogProbMetric: 29.3212 - lr: 0.0010 - 97s/epoch - 494ms/step
Epoch 143/1000
2023-09-30 19:38:47.587 
Epoch 143/1000 
	 loss: 28.5600, MinusLogProbMetric: 28.5600, val_loss: 28.8361, val_MinusLogProbMetric: 28.8361

Epoch 143: val_loss improved from 28.93038 to 28.83609, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_348/weights/best_weights.h5
196/196 - 97s - loss: 28.5600 - MinusLogProbMetric: 28.5600 - val_loss: 28.8361 - val_MinusLogProbMetric: 28.8361 - lr: 0.0010 - 97s/epoch - 495ms/step
Epoch 144/1000
2023-09-30 19:40:24.646 
Epoch 144/1000 
	 loss: 28.5031, MinusLogProbMetric: 28.5031, val_loss: 28.8919, val_MinusLogProbMetric: 28.8919

Epoch 144: val_loss did not improve from 28.83609
196/196 - 96s - loss: 28.5031 - MinusLogProbMetric: 28.5031 - val_loss: 28.8919 - val_MinusLogProbMetric: 28.8919 - lr: 0.0010 - 96s/epoch - 489ms/step
Epoch 145/1000
2023-09-30 19:41:59.433 
Epoch 145/1000 
	 loss: 28.5969, MinusLogProbMetric: 28.5969, val_loss: 29.1727, val_MinusLogProbMetric: 29.1727

Epoch 145: val_loss did not improve from 28.83609
196/196 - 95s - loss: 28.5969 - MinusLogProbMetric: 28.5969 - val_loss: 29.1727 - val_MinusLogProbMetric: 29.1727 - lr: 0.0010 - 95s/epoch - 484ms/step
Epoch 146/1000
2023-09-30 19:43:33.525 
Epoch 146/1000 
	 loss: 28.5238, MinusLogProbMetric: 28.5238, val_loss: 28.8147, val_MinusLogProbMetric: 28.8147

Epoch 146: val_loss improved from 28.83609 to 28.81474, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_348/weights/best_weights.h5
196/196 - 95s - loss: 28.5238 - MinusLogProbMetric: 28.5238 - val_loss: 28.8147 - val_MinusLogProbMetric: 28.8147 - lr: 0.0010 - 95s/epoch - 485ms/step
Epoch 147/1000
2023-09-30 19:45:09.215 
Epoch 147/1000 
	 loss: 28.5549, MinusLogProbMetric: 28.5549, val_loss: 29.1786, val_MinusLogProbMetric: 29.1786

Epoch 147: val_loss did not improve from 28.81474
196/196 - 95s - loss: 28.5549 - MinusLogProbMetric: 28.5549 - val_loss: 29.1786 - val_MinusLogProbMetric: 29.1786 - lr: 0.0010 - 95s/epoch - 483ms/step
Epoch 148/1000
2023-09-30 19:46:49.879 
Epoch 148/1000 
	 loss: 28.4836, MinusLogProbMetric: 28.4836, val_loss: 28.9563, val_MinusLogProbMetric: 28.9563

Epoch 148: val_loss did not improve from 28.81474
196/196 - 101s - loss: 28.4836 - MinusLogProbMetric: 28.4836 - val_loss: 28.9563 - val_MinusLogProbMetric: 28.9563 - lr: 0.0010 - 101s/epoch - 514ms/step
Epoch 149/1000
2023-09-30 19:48:27.986 
Epoch 149/1000 
	 loss: 28.4773, MinusLogProbMetric: 28.4773, val_loss: 28.7382, val_MinusLogProbMetric: 28.7382

Epoch 149: val_loss improved from 28.81474 to 28.73822, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_348/weights/best_weights.h5
196/196 - 101s - loss: 28.4773 - MinusLogProbMetric: 28.4773 - val_loss: 28.7382 - val_MinusLogProbMetric: 28.7382 - lr: 0.0010 - 101s/epoch - 517ms/step
Epoch 150/1000
2023-09-30 19:50:13.375 
Epoch 150/1000 
	 loss: 28.4672, MinusLogProbMetric: 28.4672, val_loss: 29.1122, val_MinusLogProbMetric: 29.1122

Epoch 150: val_loss did not improve from 28.73822
196/196 - 102s - loss: 28.4672 - MinusLogProbMetric: 28.4672 - val_loss: 29.1122 - val_MinusLogProbMetric: 29.1122 - lr: 0.0010 - 102s/epoch - 521ms/step
Epoch 151/1000
2023-09-30 19:51:56.931 
Epoch 151/1000 
	 loss: 28.5780, MinusLogProbMetric: 28.5780, val_loss: 28.7459, val_MinusLogProbMetric: 28.7459

Epoch 151: val_loss did not improve from 28.73822
196/196 - 104s - loss: 28.5780 - MinusLogProbMetric: 28.5780 - val_loss: 28.7459 - val_MinusLogProbMetric: 28.7459 - lr: 0.0010 - 104s/epoch - 528ms/step
Epoch 152/1000
2023-09-30 19:53:28.728 
Epoch 152/1000 
	 loss: 28.4734, MinusLogProbMetric: 28.4734, val_loss: 29.1658, val_MinusLogProbMetric: 29.1658

Epoch 152: val_loss did not improve from 28.73822
196/196 - 92s - loss: 28.4734 - MinusLogProbMetric: 28.4734 - val_loss: 29.1658 - val_MinusLogProbMetric: 29.1658 - lr: 0.0010 - 92s/epoch - 468ms/step
Epoch 153/1000
2023-09-30 19:55:03.634 
Epoch 153/1000 
	 loss: 28.4055, MinusLogProbMetric: 28.4055, val_loss: 30.2957, val_MinusLogProbMetric: 30.2957

Epoch 153: val_loss did not improve from 28.73822
196/196 - 95s - loss: 28.4055 - MinusLogProbMetric: 28.4055 - val_loss: 30.2957 - val_MinusLogProbMetric: 30.2957 - lr: 0.0010 - 95s/epoch - 484ms/step
Epoch 154/1000
2023-09-30 19:56:45.349 
Epoch 154/1000 
	 loss: 28.4805, MinusLogProbMetric: 28.4805, val_loss: 29.5648, val_MinusLogProbMetric: 29.5648

Epoch 154: val_loss did not improve from 28.73822
196/196 - 102s - loss: 28.4805 - MinusLogProbMetric: 28.4805 - val_loss: 29.5648 - val_MinusLogProbMetric: 29.5648 - lr: 0.0010 - 102s/epoch - 519ms/step
Epoch 155/1000
2023-09-30 19:58:26.425 
Epoch 155/1000 
	 loss: 28.4480, MinusLogProbMetric: 28.4480, val_loss: 29.1226, val_MinusLogProbMetric: 29.1226

Epoch 155: val_loss did not improve from 28.73822
196/196 - 101s - loss: 28.4480 - MinusLogProbMetric: 28.4480 - val_loss: 29.1226 - val_MinusLogProbMetric: 29.1226 - lr: 0.0010 - 101s/epoch - 515ms/step
Epoch 156/1000
2023-09-30 19:59:59.347 
Epoch 156/1000 
	 loss: 28.5208, MinusLogProbMetric: 28.5208, val_loss: 29.5747, val_MinusLogProbMetric: 29.5747

Epoch 156: val_loss did not improve from 28.73822
196/196 - 93s - loss: 28.5208 - MinusLogProbMetric: 28.5208 - val_loss: 29.5747 - val_MinusLogProbMetric: 29.5747 - lr: 0.0010 - 93s/epoch - 474ms/step
Epoch 157/1000
2023-09-30 20:01:40.504 
Epoch 157/1000 
	 loss: 28.3998, MinusLogProbMetric: 28.3998, val_loss: 29.1431, val_MinusLogProbMetric: 29.1431

Epoch 157: val_loss did not improve from 28.73822
196/196 - 101s - loss: 28.3998 - MinusLogProbMetric: 28.3998 - val_loss: 29.1431 - val_MinusLogProbMetric: 29.1431 - lr: 0.0010 - 101s/epoch - 516ms/step
Epoch 158/1000
2023-09-30 20:03:22.774 
Epoch 158/1000 
	 loss: 28.4876, MinusLogProbMetric: 28.4876, val_loss: 28.8365, val_MinusLogProbMetric: 28.8365

Epoch 158: val_loss did not improve from 28.73822
196/196 - 102s - loss: 28.4876 - MinusLogProbMetric: 28.4876 - val_loss: 28.8365 - val_MinusLogProbMetric: 28.8365 - lr: 0.0010 - 102s/epoch - 522ms/step
Epoch 159/1000
2023-09-30 20:05:07.662 
Epoch 159/1000 
	 loss: 28.5264, MinusLogProbMetric: 28.5264, val_loss: 28.8076, val_MinusLogProbMetric: 28.8076

Epoch 159: val_loss did not improve from 28.73822
196/196 - 105s - loss: 28.5264 - MinusLogProbMetric: 28.5264 - val_loss: 28.8076 - val_MinusLogProbMetric: 28.8076 - lr: 0.0010 - 105s/epoch - 535ms/step
Epoch 160/1000
2023-09-30 20:06:49.832 
Epoch 160/1000 
	 loss: 28.4299, MinusLogProbMetric: 28.4299, val_loss: 28.6776, val_MinusLogProbMetric: 28.6776

Epoch 160: val_loss improved from 28.73822 to 28.67763, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_348/weights/best_weights.h5
196/196 - 106s - loss: 28.4299 - MinusLogProbMetric: 28.4299 - val_loss: 28.6776 - val_MinusLogProbMetric: 28.6776 - lr: 0.0010 - 106s/epoch - 541ms/step
Epoch 161/1000
2023-09-30 20:08:37.256 
Epoch 161/1000 
	 loss: 28.2686, MinusLogProbMetric: 28.2686, val_loss: 30.5547, val_MinusLogProbMetric: 30.5547

Epoch 161: val_loss did not improve from 28.67763
196/196 - 104s - loss: 28.2686 - MinusLogProbMetric: 28.2686 - val_loss: 30.5547 - val_MinusLogProbMetric: 30.5547 - lr: 0.0010 - 104s/epoch - 529ms/step
Epoch 162/1000
2023-09-30 20:10:23.435 
Epoch 162/1000 
	 loss: 28.5413, MinusLogProbMetric: 28.5413, val_loss: 29.0552, val_MinusLogProbMetric: 29.0552

Epoch 162: val_loss did not improve from 28.67763
196/196 - 106s - loss: 28.5413 - MinusLogProbMetric: 28.5413 - val_loss: 29.0552 - val_MinusLogProbMetric: 29.0552 - lr: 0.0010 - 106s/epoch - 542ms/step
Epoch 163/1000
2023-09-30 20:12:03.164 
Epoch 163/1000 
	 loss: 28.2722, MinusLogProbMetric: 28.2722, val_loss: 29.3251, val_MinusLogProbMetric: 29.3251

Epoch 163: val_loss did not improve from 28.67763
196/196 - 100s - loss: 28.2722 - MinusLogProbMetric: 28.2722 - val_loss: 29.3251 - val_MinusLogProbMetric: 29.3251 - lr: 0.0010 - 100s/epoch - 509ms/step
Epoch 164/1000
2023-09-30 20:13:48.807 
Epoch 164/1000 
	 loss: 28.3341, MinusLogProbMetric: 28.3341, val_loss: 28.8611, val_MinusLogProbMetric: 28.8611

Epoch 164: val_loss did not improve from 28.67763
196/196 - 106s - loss: 28.3341 - MinusLogProbMetric: 28.3341 - val_loss: 28.8611 - val_MinusLogProbMetric: 28.8611 - lr: 0.0010 - 106s/epoch - 539ms/step
Epoch 165/1000
2023-09-30 20:15:34.276 
Epoch 165/1000 
	 loss: 28.3676, MinusLogProbMetric: 28.3676, val_loss: 29.0896, val_MinusLogProbMetric: 29.0896

Epoch 165: val_loss did not improve from 28.67763
196/196 - 105s - loss: 28.3676 - MinusLogProbMetric: 28.3676 - val_loss: 29.0896 - val_MinusLogProbMetric: 29.0896 - lr: 0.0010 - 105s/epoch - 538ms/step
Epoch 166/1000
2023-09-30 20:17:13.053 
Epoch 166/1000 
	 loss: 28.4555, MinusLogProbMetric: 28.4555, val_loss: 29.4066, val_MinusLogProbMetric: 29.4066

Epoch 166: val_loss did not improve from 28.67763
196/196 - 99s - loss: 28.4555 - MinusLogProbMetric: 28.4555 - val_loss: 29.4066 - val_MinusLogProbMetric: 29.4066 - lr: 0.0010 - 99s/epoch - 504ms/step
Epoch 167/1000
2023-09-30 20:18:52.689 
Epoch 167/1000 
	 loss: 28.3026, MinusLogProbMetric: 28.3026, val_loss: 29.1539, val_MinusLogProbMetric: 29.1539

Epoch 167: val_loss did not improve from 28.67763
196/196 - 100s - loss: 28.3026 - MinusLogProbMetric: 28.3026 - val_loss: 29.1539 - val_MinusLogProbMetric: 29.1539 - lr: 0.0010 - 100s/epoch - 508ms/step
Epoch 168/1000
2023-09-30 20:20:38.138 
Epoch 168/1000 
	 loss: 28.2936, MinusLogProbMetric: 28.2936, val_loss: 30.0037, val_MinusLogProbMetric: 30.0037

Epoch 168: val_loss did not improve from 28.67763
196/196 - 105s - loss: 28.2936 - MinusLogProbMetric: 28.2936 - val_loss: 30.0037 - val_MinusLogProbMetric: 30.0037 - lr: 0.0010 - 105s/epoch - 538ms/step
Epoch 169/1000
2023-09-30 20:22:23.586 
Epoch 169/1000 
	 loss: 28.4673, MinusLogProbMetric: 28.4673, val_loss: 29.2877, val_MinusLogProbMetric: 29.2877

Epoch 169: val_loss did not improve from 28.67763
196/196 - 105s - loss: 28.4673 - MinusLogProbMetric: 28.4673 - val_loss: 29.2877 - val_MinusLogProbMetric: 29.2877 - lr: 0.0010 - 105s/epoch - 538ms/step
Epoch 170/1000
2023-09-30 20:24:10.228 
Epoch 170/1000 
	 loss: 28.2704, MinusLogProbMetric: 28.2704, val_loss: 28.8641, val_MinusLogProbMetric: 28.8641

Epoch 170: val_loss did not improve from 28.67763
196/196 - 107s - loss: 28.2704 - MinusLogProbMetric: 28.2704 - val_loss: 28.8641 - val_MinusLogProbMetric: 28.8641 - lr: 0.0010 - 107s/epoch - 544ms/step
Epoch 171/1000
2023-09-30 20:25:54.449 
Epoch 171/1000 
	 loss: 28.3783, MinusLogProbMetric: 28.3783, val_loss: 29.1292, val_MinusLogProbMetric: 29.1292

Epoch 171: val_loss did not improve from 28.67763
196/196 - 104s - loss: 28.3783 - MinusLogProbMetric: 28.3783 - val_loss: 29.1292 - val_MinusLogProbMetric: 29.1292 - lr: 0.0010 - 104s/epoch - 532ms/step
Epoch 172/1000
2023-09-30 20:27:35.573 
Epoch 172/1000 
	 loss: 28.3534, MinusLogProbMetric: 28.3534, val_loss: 28.7110, val_MinusLogProbMetric: 28.7110

Epoch 172: val_loss did not improve from 28.67763
196/196 - 101s - loss: 28.3534 - MinusLogProbMetric: 28.3534 - val_loss: 28.7110 - val_MinusLogProbMetric: 28.7110 - lr: 0.0010 - 101s/epoch - 516ms/step
Epoch 173/1000
2023-09-30 20:29:19.612 
Epoch 173/1000 
	 loss: 28.3869, MinusLogProbMetric: 28.3869, val_loss: 28.9237, val_MinusLogProbMetric: 28.9237

Epoch 173: val_loss did not improve from 28.67763
196/196 - 104s - loss: 28.3869 - MinusLogProbMetric: 28.3869 - val_loss: 28.9237 - val_MinusLogProbMetric: 28.9237 - lr: 0.0010 - 104s/epoch - 530ms/step
Epoch 174/1000
2023-09-30 20:31:05.376 
Epoch 174/1000 
	 loss: 28.3391, MinusLogProbMetric: 28.3391, val_loss: 28.8369, val_MinusLogProbMetric: 28.8369

Epoch 174: val_loss did not improve from 28.67763
196/196 - 106s - loss: 28.3391 - MinusLogProbMetric: 28.3391 - val_loss: 28.8369 - val_MinusLogProbMetric: 28.8369 - lr: 0.0010 - 106s/epoch - 540ms/step
Epoch 175/1000
2023-09-30 20:32:47.933 
Epoch 175/1000 
	 loss: 28.2634, MinusLogProbMetric: 28.2634, val_loss: 29.9636, val_MinusLogProbMetric: 29.9636

Epoch 175: val_loss did not improve from 28.67763
196/196 - 103s - loss: 28.2634 - MinusLogProbMetric: 28.2634 - val_loss: 29.9636 - val_MinusLogProbMetric: 29.9636 - lr: 0.0010 - 103s/epoch - 523ms/step
Epoch 176/1000
2023-09-30 20:34:34.534 
Epoch 176/1000 
	 loss: 28.4237, MinusLogProbMetric: 28.4237, val_loss: 29.2362, val_MinusLogProbMetric: 29.2362

Epoch 176: val_loss did not improve from 28.67763
196/196 - 107s - loss: 28.4237 - MinusLogProbMetric: 28.4237 - val_loss: 29.2362 - val_MinusLogProbMetric: 29.2362 - lr: 0.0010 - 107s/epoch - 544ms/step
Epoch 177/1000
2023-09-30 20:36:08.346 
Epoch 177/1000 
	 loss: 28.2740, MinusLogProbMetric: 28.2740, val_loss: 28.9310, val_MinusLogProbMetric: 28.9310

Epoch 177: val_loss did not improve from 28.67763
196/196 - 94s - loss: 28.2740 - MinusLogProbMetric: 28.2740 - val_loss: 28.9310 - val_MinusLogProbMetric: 28.9310 - lr: 0.0010 - 94s/epoch - 479ms/step
Epoch 178/1000
2023-09-30 20:37:47.465 
Epoch 178/1000 
	 loss: 28.2140, MinusLogProbMetric: 28.2140, val_loss: 29.0126, val_MinusLogProbMetric: 29.0126

Epoch 178: val_loss did not improve from 28.67763
196/196 - 99s - loss: 28.2140 - MinusLogProbMetric: 28.2140 - val_loss: 29.0126 - val_MinusLogProbMetric: 29.0126 - lr: 0.0010 - 99s/epoch - 506ms/step
Epoch 179/1000
2023-09-30 20:39:23.388 
Epoch 179/1000 
	 loss: 28.2516, MinusLogProbMetric: 28.2516, val_loss: 28.7265, val_MinusLogProbMetric: 28.7265

Epoch 179: val_loss did not improve from 28.67763
196/196 - 96s - loss: 28.2516 - MinusLogProbMetric: 28.2516 - val_loss: 28.7265 - val_MinusLogProbMetric: 28.7265 - lr: 0.0010 - 96s/epoch - 489ms/step
Epoch 180/1000
2023-09-30 20:40:59.994 
Epoch 180/1000 
	 loss: 28.4298, MinusLogProbMetric: 28.4298, val_loss: 28.8852, val_MinusLogProbMetric: 28.8852

Epoch 180: val_loss did not improve from 28.67763
196/196 - 97s - loss: 28.4298 - MinusLogProbMetric: 28.4298 - val_loss: 28.8852 - val_MinusLogProbMetric: 28.8852 - lr: 0.0010 - 97s/epoch - 493ms/step
Epoch 181/1000
2023-09-30 20:42:45.271 
Epoch 181/1000 
	 loss: 28.2188, MinusLogProbMetric: 28.2188, val_loss: 28.9649, val_MinusLogProbMetric: 28.9649

Epoch 181: val_loss did not improve from 28.67763
196/196 - 105s - loss: 28.2188 - MinusLogProbMetric: 28.2188 - val_loss: 28.9649 - val_MinusLogProbMetric: 28.9649 - lr: 0.0010 - 105s/epoch - 537ms/step
Epoch 182/1000
2023-09-30 20:44:31.185 
Epoch 182/1000 
	 loss: 28.2688, MinusLogProbMetric: 28.2688, val_loss: 28.7410, val_MinusLogProbMetric: 28.7410

Epoch 182: val_loss did not improve from 28.67763
196/196 - 106s - loss: 28.2688 - MinusLogProbMetric: 28.2688 - val_loss: 28.7410 - val_MinusLogProbMetric: 28.7410 - lr: 0.0010 - 106s/epoch - 540ms/step
Epoch 183/1000
2023-09-30 20:46:08.726 
Epoch 183/1000 
	 loss: 28.2228, MinusLogProbMetric: 28.2228, val_loss: 28.9273, val_MinusLogProbMetric: 28.9273

Epoch 183: val_loss did not improve from 28.67763
196/196 - 98s - loss: 28.2228 - MinusLogProbMetric: 28.2228 - val_loss: 28.9273 - val_MinusLogProbMetric: 28.9273 - lr: 0.0010 - 98s/epoch - 498ms/step
Epoch 184/1000
2023-09-30 20:47:42.960 
Epoch 184/1000 
	 loss: 28.3234, MinusLogProbMetric: 28.3234, val_loss: 29.3434, val_MinusLogProbMetric: 29.3434

Epoch 184: val_loss did not improve from 28.67763
196/196 - 94s - loss: 28.3234 - MinusLogProbMetric: 28.3234 - val_loss: 29.3434 - val_MinusLogProbMetric: 29.3434 - lr: 0.0010 - 94s/epoch - 481ms/step
Epoch 185/1000
2023-09-30 20:49:17.020 
Epoch 185/1000 
	 loss: 28.3050, MinusLogProbMetric: 28.3050, val_loss: 28.9553, val_MinusLogProbMetric: 28.9553

Epoch 185: val_loss did not improve from 28.67763
196/196 - 94s - loss: 28.3050 - MinusLogProbMetric: 28.3050 - val_loss: 28.9553 - val_MinusLogProbMetric: 28.9553 - lr: 0.0010 - 94s/epoch - 480ms/step
Epoch 186/1000
2023-09-30 20:50:58.414 
Epoch 186/1000 
	 loss: 28.2766, MinusLogProbMetric: 28.2766, val_loss: 29.0625, val_MinusLogProbMetric: 29.0625

Epoch 186: val_loss did not improve from 28.67763
196/196 - 101s - loss: 28.2766 - MinusLogProbMetric: 28.2766 - val_loss: 29.0625 - val_MinusLogProbMetric: 29.0625 - lr: 0.0010 - 101s/epoch - 517ms/step
Epoch 187/1000
2023-09-30 20:52:41.212 
Epoch 187/1000 
	 loss: 28.2003, MinusLogProbMetric: 28.2003, val_loss: 29.0886, val_MinusLogProbMetric: 29.0886

Epoch 187: val_loss did not improve from 28.67763
196/196 - 103s - loss: 28.2003 - MinusLogProbMetric: 28.2003 - val_loss: 29.0886 - val_MinusLogProbMetric: 29.0886 - lr: 0.0010 - 103s/epoch - 524ms/step
Epoch 188/1000
2023-09-30 20:54:24.394 
Epoch 188/1000 
	 loss: 28.3262, MinusLogProbMetric: 28.3262, val_loss: 29.3878, val_MinusLogProbMetric: 29.3878

Epoch 188: val_loss did not improve from 28.67763
196/196 - 103s - loss: 28.3262 - MinusLogProbMetric: 28.3262 - val_loss: 29.3878 - val_MinusLogProbMetric: 29.3878 - lr: 0.0010 - 103s/epoch - 526ms/step
Epoch 189/1000
2023-09-30 20:55:59.761 
Epoch 189/1000 
	 loss: 28.2107, MinusLogProbMetric: 28.2107, val_loss: 29.1764, val_MinusLogProbMetric: 29.1764

Epoch 189: val_loss did not improve from 28.67763
196/196 - 95s - loss: 28.2107 - MinusLogProbMetric: 28.2107 - val_loss: 29.1764 - val_MinusLogProbMetric: 29.1764 - lr: 0.0010 - 95s/epoch - 486ms/step
Epoch 190/1000
2023-09-30 20:57:37.413 
Epoch 190/1000 
	 loss: 28.2327, MinusLogProbMetric: 28.2327, val_loss: 28.8875, val_MinusLogProbMetric: 28.8875

Epoch 190: val_loss did not improve from 28.67763
196/196 - 98s - loss: 28.2327 - MinusLogProbMetric: 28.2327 - val_loss: 28.8875 - val_MinusLogProbMetric: 28.8875 - lr: 0.0010 - 98s/epoch - 498ms/step
Epoch 191/1000
2023-09-30 20:59:20.153 
Epoch 191/1000 
	 loss: 28.1963, MinusLogProbMetric: 28.1963, val_loss: 28.7731, val_MinusLogProbMetric: 28.7731

Epoch 191: val_loss did not improve from 28.67763
196/196 - 103s - loss: 28.1963 - MinusLogProbMetric: 28.1963 - val_loss: 28.7731 - val_MinusLogProbMetric: 28.7731 - lr: 0.0010 - 103s/epoch - 524ms/step
Epoch 192/1000
2023-09-30 21:00:55.522 
Epoch 192/1000 
	 loss: 28.1996, MinusLogProbMetric: 28.1996, val_loss: 29.1834, val_MinusLogProbMetric: 29.1834

Epoch 192: val_loss did not improve from 28.67763
196/196 - 95s - loss: 28.1996 - MinusLogProbMetric: 28.1996 - val_loss: 29.1834 - val_MinusLogProbMetric: 29.1834 - lr: 0.0010 - 95s/epoch - 486ms/step
Epoch 193/1000
2023-09-30 21:02:32.260 
Epoch 193/1000 
	 loss: 28.0963, MinusLogProbMetric: 28.0963, val_loss: 28.7473, val_MinusLogProbMetric: 28.7473

Epoch 193: val_loss did not improve from 28.67763
196/196 - 97s - loss: 28.0963 - MinusLogProbMetric: 28.0963 - val_loss: 28.7473 - val_MinusLogProbMetric: 28.7473 - lr: 0.0010 - 97s/epoch - 493ms/step
Epoch 194/1000
2023-09-30 21:04:11.399 
Epoch 194/1000 
	 loss: 28.1865, MinusLogProbMetric: 28.1865, val_loss: 29.2358, val_MinusLogProbMetric: 29.2358

Epoch 194: val_loss did not improve from 28.67763
196/196 - 99s - loss: 28.1865 - MinusLogProbMetric: 28.1865 - val_loss: 29.2358 - val_MinusLogProbMetric: 29.2358 - lr: 0.0010 - 99s/epoch - 506ms/step
Epoch 195/1000
2023-09-30 21:05:47.274 
Epoch 195/1000 
	 loss: 28.1506, MinusLogProbMetric: 28.1506, val_loss: 29.2191, val_MinusLogProbMetric: 29.2191

Epoch 195: val_loss did not improve from 28.67763
196/196 - 96s - loss: 28.1506 - MinusLogProbMetric: 28.1506 - val_loss: 29.2191 - val_MinusLogProbMetric: 29.2191 - lr: 0.0010 - 96s/epoch - 489ms/step
Epoch 196/1000
2023-09-30 21:07:25.278 
Epoch 196/1000 
	 loss: 28.1773, MinusLogProbMetric: 28.1773, val_loss: 28.8718, val_MinusLogProbMetric: 28.8718

Epoch 196: val_loss did not improve from 28.67763
196/196 - 98s - loss: 28.1773 - MinusLogProbMetric: 28.1773 - val_loss: 28.8718 - val_MinusLogProbMetric: 28.8718 - lr: 0.0010 - 98s/epoch - 500ms/step
Epoch 197/1000
2023-09-30 21:08:59.737 
Epoch 197/1000 
	 loss: 28.1952, MinusLogProbMetric: 28.1952, val_loss: 29.1312, val_MinusLogProbMetric: 29.1312

Epoch 197: val_loss did not improve from 28.67763
196/196 - 94s - loss: 28.1952 - MinusLogProbMetric: 28.1952 - val_loss: 29.1312 - val_MinusLogProbMetric: 29.1312 - lr: 0.0010 - 94s/epoch - 482ms/step
Epoch 198/1000
2023-09-30 21:10:35.640 
Epoch 198/1000 
	 loss: 28.2537, MinusLogProbMetric: 28.2537, val_loss: 28.9378, val_MinusLogProbMetric: 28.9378

Epoch 198: val_loss did not improve from 28.67763
196/196 - 96s - loss: 28.2537 - MinusLogProbMetric: 28.2537 - val_loss: 28.9378 - val_MinusLogProbMetric: 28.9378 - lr: 0.0010 - 96s/epoch - 489ms/step
Epoch 199/1000
2023-09-30 21:12:03.673 
Epoch 199/1000 
	 loss: 28.0778, MinusLogProbMetric: 28.0778, val_loss: 28.7215, val_MinusLogProbMetric: 28.7215

Epoch 199: val_loss did not improve from 28.67763
196/196 - 88s - loss: 28.0778 - MinusLogProbMetric: 28.0778 - val_loss: 28.7215 - val_MinusLogProbMetric: 28.7215 - lr: 0.0010 - 88s/epoch - 449ms/step
Epoch 200/1000
2023-09-30 21:13:43.726 
Epoch 200/1000 
	 loss: 28.1189, MinusLogProbMetric: 28.1189, val_loss: 28.7634, val_MinusLogProbMetric: 28.7634

Epoch 200: val_loss did not improve from 28.67763
196/196 - 100s - loss: 28.1189 - MinusLogProbMetric: 28.1189 - val_loss: 28.7634 - val_MinusLogProbMetric: 28.7634 - lr: 0.0010 - 100s/epoch - 510ms/step
Epoch 201/1000
2023-09-30 21:15:29.233 
Epoch 201/1000 
	 loss: 28.2640, MinusLogProbMetric: 28.2640, val_loss: 28.8619, val_MinusLogProbMetric: 28.8619

Epoch 201: val_loss did not improve from 28.67763
196/196 - 106s - loss: 28.2640 - MinusLogProbMetric: 28.2640 - val_loss: 28.8619 - val_MinusLogProbMetric: 28.8619 - lr: 0.0010 - 106s/epoch - 538ms/step
Epoch 202/1000
2023-09-30 21:17:11.153 
Epoch 202/1000 
	 loss: 28.1854, MinusLogProbMetric: 28.1854, val_loss: 28.6805, val_MinusLogProbMetric: 28.6805

Epoch 202: val_loss did not improve from 28.67763
196/196 - 102s - loss: 28.1854 - MinusLogProbMetric: 28.1854 - val_loss: 28.6805 - val_MinusLogProbMetric: 28.6805 - lr: 0.0010 - 102s/epoch - 520ms/step
Epoch 203/1000
2023-09-30 21:18:56.391 
Epoch 203/1000 
	 loss: 28.2079, MinusLogProbMetric: 28.2079, val_loss: 28.7721, val_MinusLogProbMetric: 28.7721

Epoch 203: val_loss did not improve from 28.67763
196/196 - 105s - loss: 28.2079 - MinusLogProbMetric: 28.2079 - val_loss: 28.7721 - val_MinusLogProbMetric: 28.7721 - lr: 0.0010 - 105s/epoch - 537ms/step
Epoch 204/1000
2023-09-30 21:20:32.100 
Epoch 204/1000 
	 loss: 28.0853, MinusLogProbMetric: 28.0853, val_loss: 28.7685, val_MinusLogProbMetric: 28.7685

Epoch 204: val_loss did not improve from 28.67763
196/196 - 96s - loss: 28.0853 - MinusLogProbMetric: 28.0853 - val_loss: 28.7685 - val_MinusLogProbMetric: 28.7685 - lr: 0.0010 - 96s/epoch - 488ms/step
Epoch 205/1000
2023-09-30 21:22:18.347 
Epoch 205/1000 
	 loss: 28.0797, MinusLogProbMetric: 28.0797, val_loss: 28.9440, val_MinusLogProbMetric: 28.9440

Epoch 205: val_loss did not improve from 28.67763
196/196 - 106s - loss: 28.0797 - MinusLogProbMetric: 28.0797 - val_loss: 28.9440 - val_MinusLogProbMetric: 28.9440 - lr: 0.0010 - 106s/epoch - 542ms/step
Epoch 206/1000
2023-09-30 21:23:54.638 
Epoch 206/1000 
	 loss: 28.0275, MinusLogProbMetric: 28.0275, val_loss: 29.0663, val_MinusLogProbMetric: 29.0663

Epoch 206: val_loss did not improve from 28.67763
196/196 - 96s - loss: 28.0275 - MinusLogProbMetric: 28.0275 - val_loss: 29.0663 - val_MinusLogProbMetric: 29.0663 - lr: 0.0010 - 96s/epoch - 491ms/step
Epoch 207/1000
2023-09-30 21:25:39.910 
Epoch 207/1000 
	 loss: 28.0897, MinusLogProbMetric: 28.0897, val_loss: 28.8369, val_MinusLogProbMetric: 28.8369

Epoch 207: val_loss did not improve from 28.67763
196/196 - 105s - loss: 28.0897 - MinusLogProbMetric: 28.0897 - val_loss: 28.8369 - val_MinusLogProbMetric: 28.8369 - lr: 0.0010 - 105s/epoch - 537ms/step
Epoch 208/1000
2023-09-30 21:27:16.873 
Epoch 208/1000 
	 loss: 28.0371, MinusLogProbMetric: 28.0371, val_loss: 29.7719, val_MinusLogProbMetric: 29.7719

Epoch 208: val_loss did not improve from 28.67763
196/196 - 97s - loss: 28.0371 - MinusLogProbMetric: 28.0371 - val_loss: 29.7719 - val_MinusLogProbMetric: 29.7719 - lr: 0.0010 - 97s/epoch - 495ms/step
Epoch 209/1000
2023-09-30 21:29:02.441 
Epoch 209/1000 
	 loss: 28.2395, MinusLogProbMetric: 28.2395, val_loss: 29.1433, val_MinusLogProbMetric: 29.1433

Epoch 209: val_loss did not improve from 28.67763
196/196 - 106s - loss: 28.2395 - MinusLogProbMetric: 28.2395 - val_loss: 29.1433 - val_MinusLogProbMetric: 29.1433 - lr: 0.0010 - 106s/epoch - 538ms/step
Epoch 210/1000
2023-09-30 21:30:42.543 
Epoch 210/1000 
	 loss: 28.0758, MinusLogProbMetric: 28.0758, val_loss: 28.8484, val_MinusLogProbMetric: 28.8484

Epoch 210: val_loss did not improve from 28.67763
196/196 - 100s - loss: 28.0758 - MinusLogProbMetric: 28.0758 - val_loss: 28.8484 - val_MinusLogProbMetric: 28.8484 - lr: 0.0010 - 100s/epoch - 511ms/step
Epoch 211/1000
2023-09-30 21:32:21.813 
Epoch 211/1000 
	 loss: 27.5755, MinusLogProbMetric: 27.5755, val_loss: 28.4051, val_MinusLogProbMetric: 28.4051

Epoch 211: val_loss improved from 28.67763 to 28.40507, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_348/weights/best_weights.h5
196/196 - 101s - loss: 27.5755 - MinusLogProbMetric: 27.5755 - val_loss: 28.4051 - val_MinusLogProbMetric: 28.4051 - lr: 5.0000e-04 - 101s/epoch - 515ms/step
Epoch 212/1000
2023-09-30 21:34:07.177 
Epoch 212/1000 
	 loss: 27.5603, MinusLogProbMetric: 27.5603, val_loss: 28.4122, val_MinusLogProbMetric: 28.4122

Epoch 212: val_loss did not improve from 28.40507
196/196 - 104s - loss: 27.5603 - MinusLogProbMetric: 27.5603 - val_loss: 28.4122 - val_MinusLogProbMetric: 28.4122 - lr: 5.0000e-04 - 104s/epoch - 529ms/step
Epoch 213/1000
2023-09-30 21:35:55.397 
Epoch 213/1000 
	 loss: 27.5526, MinusLogProbMetric: 27.5526, val_loss: 28.3489, val_MinusLogProbMetric: 28.3489

Epoch 213: val_loss improved from 28.40507 to 28.34895, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_348/weights/best_weights.h5
196/196 - 111s - loss: 27.5526 - MinusLogProbMetric: 27.5526 - val_loss: 28.3489 - val_MinusLogProbMetric: 28.3489 - lr: 5.0000e-04 - 111s/epoch - 564ms/step
Epoch 214/1000
2023-09-30 21:37:35.625 
Epoch 214/1000 
	 loss: 27.5473, MinusLogProbMetric: 27.5473, val_loss: 28.4119, val_MinusLogProbMetric: 28.4119

Epoch 214: val_loss did not improve from 28.34895
196/196 - 98s - loss: 27.5473 - MinusLogProbMetric: 27.5473 - val_loss: 28.4119 - val_MinusLogProbMetric: 28.4119 - lr: 5.0000e-04 - 98s/epoch - 500ms/step
Epoch 215/1000
2023-09-30 21:39:14.184 
Epoch 215/1000 
	 loss: 27.5625, MinusLogProbMetric: 27.5625, val_loss: 28.4113, val_MinusLogProbMetric: 28.4113

Epoch 215: val_loss did not improve from 28.34895
196/196 - 99s - loss: 27.5625 - MinusLogProbMetric: 27.5625 - val_loss: 28.4113 - val_MinusLogProbMetric: 28.4113 - lr: 5.0000e-04 - 99s/epoch - 503ms/step
Epoch 216/1000
2023-09-30 21:40:58.773 
Epoch 216/1000 
	 loss: 27.5467, MinusLogProbMetric: 27.5467, val_loss: 28.4532, val_MinusLogProbMetric: 28.4532

Epoch 216: val_loss did not improve from 28.34895
196/196 - 105s - loss: 27.5467 - MinusLogProbMetric: 27.5467 - val_loss: 28.4532 - val_MinusLogProbMetric: 28.4532 - lr: 5.0000e-04 - 105s/epoch - 533ms/step
Epoch 217/1000
2023-09-30 21:42:39.180 
Epoch 217/1000 
	 loss: 27.5545, MinusLogProbMetric: 27.5545, val_loss: 28.4955, val_MinusLogProbMetric: 28.4955

Epoch 217: val_loss did not improve from 28.34895
196/196 - 100s - loss: 27.5545 - MinusLogProbMetric: 27.5545 - val_loss: 28.4955 - val_MinusLogProbMetric: 28.4955 - lr: 5.0000e-04 - 100s/epoch - 512ms/step
Epoch 218/1000
2023-09-30 21:44:24.987 
Epoch 218/1000 
	 loss: 27.5564, MinusLogProbMetric: 27.5564, val_loss: 28.3421, val_MinusLogProbMetric: 28.3421

Epoch 218: val_loss improved from 28.34895 to 28.34208, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_348/weights/best_weights.h5
196/196 - 108s - loss: 27.5564 - MinusLogProbMetric: 27.5564 - val_loss: 28.3421 - val_MinusLogProbMetric: 28.3421 - lr: 5.0000e-04 - 108s/epoch - 551ms/step
Epoch 219/1000
2023-09-30 21:46:01.734 
Epoch 219/1000 
	 loss: 27.5463, MinusLogProbMetric: 27.5463, val_loss: 28.3334, val_MinusLogProbMetric: 28.3334

Epoch 219: val_loss improved from 28.34208 to 28.33344, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_348/weights/best_weights.h5
196/196 - 96s - loss: 27.5463 - MinusLogProbMetric: 27.5463 - val_loss: 28.3334 - val_MinusLogProbMetric: 28.3334 - lr: 5.0000e-04 - 96s/epoch - 492ms/step
Epoch 220/1000
2023-09-30 21:47:40.305 
Epoch 220/1000 
	 loss: 27.5459, MinusLogProbMetric: 27.5459, val_loss: 28.4111, val_MinusLogProbMetric: 28.4111

Epoch 220: val_loss did not improve from 28.33344
196/196 - 97s - loss: 27.5459 - MinusLogProbMetric: 27.5459 - val_loss: 28.4111 - val_MinusLogProbMetric: 28.4111 - lr: 5.0000e-04 - 97s/epoch - 493ms/step
Epoch 221/1000
2023-09-30 21:49:20.450 
Epoch 221/1000 
	 loss: 27.5346, MinusLogProbMetric: 27.5346, val_loss: 28.3617, val_MinusLogProbMetric: 28.3617

Epoch 221: val_loss did not improve from 28.33344
196/196 - 100s - loss: 27.5346 - MinusLogProbMetric: 27.5346 - val_loss: 28.3617 - val_MinusLogProbMetric: 28.3617 - lr: 5.0000e-04 - 100s/epoch - 511ms/step
Epoch 222/1000
2023-09-30 21:51:10.808 
Epoch 222/1000 
	 loss: 27.5521, MinusLogProbMetric: 27.5521, val_loss: 28.4449, val_MinusLogProbMetric: 28.4449

Epoch 222: val_loss did not improve from 28.33344
196/196 - 110s - loss: 27.5521 - MinusLogProbMetric: 27.5521 - val_loss: 28.4449 - val_MinusLogProbMetric: 28.4449 - lr: 5.0000e-04 - 110s/epoch - 563ms/step
Epoch 223/1000
2023-09-30 21:52:54.819 
Epoch 223/1000 
	 loss: 27.5680, MinusLogProbMetric: 27.5680, val_loss: 28.4783, val_MinusLogProbMetric: 28.4783

Epoch 223: val_loss did not improve from 28.33344
196/196 - 104s - loss: 27.5680 - MinusLogProbMetric: 27.5680 - val_loss: 28.4783 - val_MinusLogProbMetric: 28.4783 - lr: 5.0000e-04 - 104s/epoch - 531ms/step
Epoch 224/1000
2023-09-30 21:54:32.609 
Epoch 224/1000 
	 loss: 27.5197, MinusLogProbMetric: 27.5197, val_loss: 28.4221, val_MinusLogProbMetric: 28.4221

Epoch 224: val_loss did not improve from 28.33344
196/196 - 98s - loss: 27.5197 - MinusLogProbMetric: 27.5197 - val_loss: 28.4221 - val_MinusLogProbMetric: 28.4221 - lr: 5.0000e-04 - 98s/epoch - 499ms/step
Epoch 225/1000
2023-09-30 21:56:25.234 
Epoch 225/1000 
	 loss: 27.5459, MinusLogProbMetric: 27.5459, val_loss: 28.5681, val_MinusLogProbMetric: 28.5681

Epoch 225: val_loss did not improve from 28.33344
196/196 - 113s - loss: 27.5459 - MinusLogProbMetric: 27.5459 - val_loss: 28.5681 - val_MinusLogProbMetric: 28.5681 - lr: 5.0000e-04 - 113s/epoch - 575ms/step
Epoch 226/1000
2023-09-30 21:58:12.314 
Epoch 226/1000 
	 loss: 27.5374, MinusLogProbMetric: 27.5374, val_loss: 28.5023, val_MinusLogProbMetric: 28.5023

Epoch 226: val_loss did not improve from 28.33344
196/196 - 107s - loss: 27.5374 - MinusLogProbMetric: 27.5374 - val_loss: 28.5023 - val_MinusLogProbMetric: 28.5023 - lr: 5.0000e-04 - 107s/epoch - 546ms/step
Epoch 227/1000
2023-09-30 21:59:59.065 
Epoch 227/1000 
	 loss: 27.5268, MinusLogProbMetric: 27.5268, val_loss: 28.3862, val_MinusLogProbMetric: 28.3862

Epoch 227: val_loss did not improve from 28.33344
196/196 - 107s - loss: 27.5268 - MinusLogProbMetric: 27.5268 - val_loss: 28.3862 - val_MinusLogProbMetric: 28.3862 - lr: 5.0000e-04 - 107s/epoch - 544ms/step
Epoch 228/1000
2023-09-30 22:01:44.410 
Epoch 228/1000 
	 loss: 27.5212, MinusLogProbMetric: 27.5212, val_loss: 28.7571, val_MinusLogProbMetric: 28.7571

Epoch 228: val_loss did not improve from 28.33344
196/196 - 105s - loss: 27.5212 - MinusLogProbMetric: 27.5212 - val_loss: 28.7571 - val_MinusLogProbMetric: 28.7571 - lr: 5.0000e-04 - 105s/epoch - 538ms/step
Epoch 229/1000
2023-09-30 22:03:30.267 
Epoch 229/1000 
	 loss: 27.5427, MinusLogProbMetric: 27.5427, val_loss: 28.6167, val_MinusLogProbMetric: 28.6167

Epoch 229: val_loss did not improve from 28.33344
196/196 - 106s - loss: 27.5427 - MinusLogProbMetric: 27.5427 - val_loss: 28.6167 - val_MinusLogProbMetric: 28.6167 - lr: 5.0000e-04 - 106s/epoch - 540ms/step
Epoch 230/1000
2023-09-30 22:05:11.832 
Epoch 230/1000 
	 loss: 27.5405, MinusLogProbMetric: 27.5405, val_loss: 28.2984, val_MinusLogProbMetric: 28.2984

Epoch 230: val_loss improved from 28.33344 to 28.29838, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_348/weights/best_weights.h5
196/196 - 104s - loss: 27.5405 - MinusLogProbMetric: 27.5405 - val_loss: 28.2984 - val_MinusLogProbMetric: 28.2984 - lr: 5.0000e-04 - 104s/epoch - 528ms/step
Epoch 231/1000
2023-09-30 22:07:02.982 
Epoch 231/1000 
	 loss: 27.5119, MinusLogProbMetric: 27.5119, val_loss: 28.4413, val_MinusLogProbMetric: 28.4413

Epoch 231: val_loss did not improve from 28.29838
196/196 - 109s - loss: 27.5119 - MinusLogProbMetric: 27.5119 - val_loss: 28.4413 - val_MinusLogProbMetric: 28.4413 - lr: 5.0000e-04 - 109s/epoch - 557ms/step
Epoch 232/1000
2023-09-30 22:08:50.393 
Epoch 232/1000 
	 loss: 27.4972, MinusLogProbMetric: 27.4972, val_loss: 28.3650, val_MinusLogProbMetric: 28.3650

Epoch 232: val_loss did not improve from 28.29838
196/196 - 107s - loss: 27.4972 - MinusLogProbMetric: 27.4972 - val_loss: 28.3650 - val_MinusLogProbMetric: 28.3650 - lr: 5.0000e-04 - 107s/epoch - 548ms/step
Epoch 233/1000
2023-09-30 22:10:36.767 
Epoch 233/1000 
	 loss: 27.5226, MinusLogProbMetric: 27.5226, val_loss: 28.6240, val_MinusLogProbMetric: 28.6240

Epoch 233: val_loss did not improve from 28.29838
196/196 - 106s - loss: 27.5226 - MinusLogProbMetric: 27.5226 - val_loss: 28.6240 - val_MinusLogProbMetric: 28.6240 - lr: 5.0000e-04 - 106s/epoch - 543ms/step
Epoch 234/1000
2023-09-30 22:12:19.764 
Epoch 234/1000 
	 loss: 27.5068, MinusLogProbMetric: 27.5068, val_loss: 28.3960, val_MinusLogProbMetric: 28.3960

Epoch 234: val_loss did not improve from 28.29838
196/196 - 103s - loss: 27.5068 - MinusLogProbMetric: 27.5068 - val_loss: 28.3960 - val_MinusLogProbMetric: 28.3960 - lr: 5.0000e-04 - 103s/epoch - 525ms/step
Epoch 235/1000
2023-09-30 22:14:08.902 
Epoch 235/1000 
	 loss: 27.5626, MinusLogProbMetric: 27.5626, val_loss: 28.3765, val_MinusLogProbMetric: 28.3765

Epoch 235: val_loss did not improve from 28.29838
196/196 - 109s - loss: 27.5626 - MinusLogProbMetric: 27.5626 - val_loss: 28.3765 - val_MinusLogProbMetric: 28.3765 - lr: 5.0000e-04 - 109s/epoch - 557ms/step
Epoch 236/1000
2023-09-30 22:15:57.868 
Epoch 236/1000 
	 loss: 27.5467, MinusLogProbMetric: 27.5467, val_loss: 28.3163, val_MinusLogProbMetric: 28.3163

Epoch 236: val_loss did not improve from 28.29838
196/196 - 109s - loss: 27.5467 - MinusLogProbMetric: 27.5467 - val_loss: 28.3163 - val_MinusLogProbMetric: 28.3163 - lr: 5.0000e-04 - 109s/epoch - 556ms/step
Epoch 237/1000
2023-09-30 22:17:51.735 
Epoch 237/1000 
	 loss: 27.5100, MinusLogProbMetric: 27.5100, val_loss: 28.4894, val_MinusLogProbMetric: 28.4894

Epoch 237: val_loss did not improve from 28.29838
196/196 - 114s - loss: 27.5100 - MinusLogProbMetric: 27.5100 - val_loss: 28.4894 - val_MinusLogProbMetric: 28.4894 - lr: 5.0000e-04 - 114s/epoch - 581ms/step
Epoch 238/1000
2023-09-30 22:19:40.723 
Epoch 238/1000 
	 loss: 27.5022, MinusLogProbMetric: 27.5022, val_loss: 28.3781, val_MinusLogProbMetric: 28.3781

Epoch 238: val_loss did not improve from 28.29838
196/196 - 109s - loss: 27.5022 - MinusLogProbMetric: 27.5022 - val_loss: 28.3781 - val_MinusLogProbMetric: 28.3781 - lr: 5.0000e-04 - 109s/epoch - 556ms/step
Epoch 239/1000
2023-09-30 22:21:27.653 
Epoch 239/1000 
	 loss: 27.5140, MinusLogProbMetric: 27.5140, val_loss: 28.3728, val_MinusLogProbMetric: 28.3728

Epoch 239: val_loss did not improve from 28.29838
196/196 - 107s - loss: 27.5140 - MinusLogProbMetric: 27.5140 - val_loss: 28.3728 - val_MinusLogProbMetric: 28.3728 - lr: 5.0000e-04 - 107s/epoch - 546ms/step
Epoch 240/1000
2023-09-30 22:23:01.281 
Epoch 240/1000 
	 loss: 27.4820, MinusLogProbMetric: 27.4820, val_loss: 28.5113, val_MinusLogProbMetric: 28.5113

Epoch 240: val_loss did not improve from 28.29838
196/196 - 94s - loss: 27.4820 - MinusLogProbMetric: 27.4820 - val_loss: 28.5113 - val_MinusLogProbMetric: 28.5113 - lr: 5.0000e-04 - 94s/epoch - 478ms/step
Epoch 241/1000
2023-09-30 22:24:43.800 
Epoch 241/1000 
	 loss: 27.4969, MinusLogProbMetric: 27.4969, val_loss: 28.4659, val_MinusLogProbMetric: 28.4659

Epoch 241: val_loss did not improve from 28.29838
196/196 - 103s - loss: 27.4969 - MinusLogProbMetric: 27.4969 - val_loss: 28.4659 - val_MinusLogProbMetric: 28.4659 - lr: 5.0000e-04 - 103s/epoch - 523ms/step
Epoch 242/1000
2023-09-30 22:26:35.753 
Epoch 242/1000 
	 loss: 27.4866, MinusLogProbMetric: 27.4866, val_loss: 28.5721, val_MinusLogProbMetric: 28.5721

Epoch 242: val_loss did not improve from 28.29838
196/196 - 112s - loss: 27.4866 - MinusLogProbMetric: 27.4866 - val_loss: 28.5721 - val_MinusLogProbMetric: 28.5721 - lr: 5.0000e-04 - 112s/epoch - 571ms/step
Epoch 243/1000
2023-09-30 22:28:12.264 
Epoch 243/1000 
	 loss: 27.5497, MinusLogProbMetric: 27.5497, val_loss: 28.3505, val_MinusLogProbMetric: 28.3505

Epoch 243: val_loss did not improve from 28.29838
196/196 - 97s - loss: 27.5497 - MinusLogProbMetric: 27.5497 - val_loss: 28.3505 - val_MinusLogProbMetric: 28.3505 - lr: 5.0000e-04 - 97s/epoch - 492ms/step
Epoch 244/1000
2023-09-30 22:29:59.418 
Epoch 244/1000 
	 loss: 27.5406, MinusLogProbMetric: 27.5406, val_loss: 28.3903, val_MinusLogProbMetric: 28.3903

Epoch 244: val_loss did not improve from 28.29838
196/196 - 107s - loss: 27.5406 - MinusLogProbMetric: 27.5406 - val_loss: 28.3903 - val_MinusLogProbMetric: 28.3903 - lr: 5.0000e-04 - 107s/epoch - 547ms/step
Epoch 245/1000
2023-09-30 22:31:43.932 
Epoch 245/1000 
	 loss: 27.4658, MinusLogProbMetric: 27.4658, val_loss: 28.3995, val_MinusLogProbMetric: 28.3995

Epoch 245: val_loss did not improve from 28.29838
196/196 - 105s - loss: 27.4658 - MinusLogProbMetric: 27.4658 - val_loss: 28.3995 - val_MinusLogProbMetric: 28.3995 - lr: 5.0000e-04 - 105s/epoch - 533ms/step
Epoch 246/1000
2023-09-30 22:33:27.213 
Epoch 246/1000 
	 loss: 27.4769, MinusLogProbMetric: 27.4769, val_loss: 28.5906, val_MinusLogProbMetric: 28.5906

Epoch 246: val_loss did not improve from 28.29838
196/196 - 103s - loss: 27.4769 - MinusLogProbMetric: 27.4769 - val_loss: 28.5906 - val_MinusLogProbMetric: 28.5906 - lr: 5.0000e-04 - 103s/epoch - 527ms/step
Epoch 247/1000
2023-09-30 22:35:09.388 
Epoch 247/1000 
	 loss: 27.4905, MinusLogProbMetric: 27.4905, val_loss: 28.4228, val_MinusLogProbMetric: 28.4228

Epoch 247: val_loss did not improve from 28.29838
196/196 - 102s - loss: 27.4905 - MinusLogProbMetric: 27.4905 - val_loss: 28.4228 - val_MinusLogProbMetric: 28.4228 - lr: 5.0000e-04 - 102s/epoch - 521ms/step
Epoch 248/1000
2023-09-30 22:36:53.745 
Epoch 248/1000 
	 loss: 27.4966, MinusLogProbMetric: 27.4966, val_loss: 28.3391, val_MinusLogProbMetric: 28.3391

Epoch 248: val_loss did not improve from 28.29838
196/196 - 104s - loss: 27.4966 - MinusLogProbMetric: 27.4966 - val_loss: 28.3391 - val_MinusLogProbMetric: 28.3391 - lr: 5.0000e-04 - 104s/epoch - 532ms/step
Epoch 249/1000
2023-09-30 22:38:36.738 
Epoch 249/1000 
	 loss: 27.4781, MinusLogProbMetric: 27.4781, val_loss: 28.4384, val_MinusLogProbMetric: 28.4384

Epoch 249: val_loss did not improve from 28.29838
196/196 - 103s - loss: 27.4781 - MinusLogProbMetric: 27.4781 - val_loss: 28.4384 - val_MinusLogProbMetric: 28.4384 - lr: 5.0000e-04 - 103s/epoch - 525ms/step
Epoch 250/1000
2023-09-30 22:40:17.117 
Epoch 250/1000 
	 loss: 27.4993, MinusLogProbMetric: 27.4993, val_loss: 28.4433, val_MinusLogProbMetric: 28.4433

Epoch 250: val_loss did not improve from 28.29838
196/196 - 100s - loss: 27.4993 - MinusLogProbMetric: 27.4993 - val_loss: 28.4433 - val_MinusLogProbMetric: 28.4433 - lr: 5.0000e-04 - 100s/epoch - 512ms/step
Epoch 251/1000
2023-09-30 22:41:53.559 
Epoch 251/1000 
	 loss: 27.4783, MinusLogProbMetric: 27.4783, val_loss: 28.6555, val_MinusLogProbMetric: 28.6555

Epoch 251: val_loss did not improve from 28.29838
196/196 - 96s - loss: 27.4783 - MinusLogProbMetric: 27.4783 - val_loss: 28.6555 - val_MinusLogProbMetric: 28.6555 - lr: 5.0000e-04 - 96s/epoch - 492ms/step
Epoch 252/1000
2023-09-30 22:43:33.491 
Epoch 252/1000 
	 loss: 27.4531, MinusLogProbMetric: 27.4531, val_loss: 28.4272, val_MinusLogProbMetric: 28.4272

Epoch 252: val_loss did not improve from 28.29838
196/196 - 100s - loss: 27.4531 - MinusLogProbMetric: 27.4531 - val_loss: 28.4272 - val_MinusLogProbMetric: 28.4272 - lr: 5.0000e-04 - 100s/epoch - 510ms/step
Epoch 253/1000
2023-09-30 22:45:16.611 
Epoch 253/1000 
	 loss: 27.4859, MinusLogProbMetric: 27.4859, val_loss: 28.4027, val_MinusLogProbMetric: 28.4027

Epoch 253: val_loss did not improve from 28.29838
196/196 - 103s - loss: 27.4859 - MinusLogProbMetric: 27.4859 - val_loss: 28.4027 - val_MinusLogProbMetric: 28.4027 - lr: 5.0000e-04 - 103s/epoch - 526ms/step
Epoch 254/1000
2023-09-30 22:46:59.777 
Epoch 254/1000 
	 loss: 27.4870, MinusLogProbMetric: 27.4870, val_loss: 28.5192, val_MinusLogProbMetric: 28.5192

Epoch 254: val_loss did not improve from 28.29838
196/196 - 103s - loss: 27.4870 - MinusLogProbMetric: 27.4870 - val_loss: 28.5192 - val_MinusLogProbMetric: 28.5192 - lr: 5.0000e-04 - 103s/epoch - 526ms/step
Epoch 255/1000
2023-09-30 22:48:37.272 
Epoch 255/1000 
	 loss: 27.4826, MinusLogProbMetric: 27.4826, val_loss: 28.5219, val_MinusLogProbMetric: 28.5219

Epoch 255: val_loss did not improve from 28.29838
196/196 - 97s - loss: 27.4826 - MinusLogProbMetric: 27.4826 - val_loss: 28.5219 - val_MinusLogProbMetric: 28.5219 - lr: 5.0000e-04 - 97s/epoch - 497ms/step
Epoch 256/1000
2023-09-30 22:50:12.911 
Epoch 256/1000 
	 loss: 27.4467, MinusLogProbMetric: 27.4467, val_loss: 28.3723, val_MinusLogProbMetric: 28.3723

Epoch 256: val_loss did not improve from 28.29838
196/196 - 96s - loss: 27.4467 - MinusLogProbMetric: 27.4467 - val_loss: 28.3723 - val_MinusLogProbMetric: 28.3723 - lr: 5.0000e-04 - 96s/epoch - 488ms/step
Epoch 257/1000
2023-09-30 22:51:50.347 
Epoch 257/1000 
	 loss: 27.4419, MinusLogProbMetric: 27.4419, val_loss: 28.4099, val_MinusLogProbMetric: 28.4099

Epoch 257: val_loss did not improve from 28.29838
196/196 - 97s - loss: 27.4419 - MinusLogProbMetric: 27.4419 - val_loss: 28.4099 - val_MinusLogProbMetric: 28.4099 - lr: 5.0000e-04 - 97s/epoch - 497ms/step
Epoch 258/1000
2023-09-30 22:53:35.915 
Epoch 258/1000 
	 loss: 27.4405, MinusLogProbMetric: 27.4405, val_loss: 28.4846, val_MinusLogProbMetric: 28.4846

Epoch 258: val_loss did not improve from 28.29838
196/196 - 106s - loss: 27.4405 - MinusLogProbMetric: 27.4405 - val_loss: 28.4846 - val_MinusLogProbMetric: 28.4846 - lr: 5.0000e-04 - 106s/epoch - 539ms/step
Epoch 259/1000
2023-09-30 22:55:10.128 
Epoch 259/1000 
	 loss: 27.4743, MinusLogProbMetric: 27.4743, val_loss: 28.4572, val_MinusLogProbMetric: 28.4572

Epoch 259: val_loss did not improve from 28.29838
196/196 - 94s - loss: 27.4743 - MinusLogProbMetric: 27.4743 - val_loss: 28.4572 - val_MinusLogProbMetric: 28.4572 - lr: 5.0000e-04 - 94s/epoch - 481ms/step
Epoch 260/1000
2023-09-30 22:56:55.990 
Epoch 260/1000 
	 loss: 27.4743, MinusLogProbMetric: 27.4743, val_loss: 28.3788, val_MinusLogProbMetric: 28.3788

Epoch 260: val_loss did not improve from 28.29838
196/196 - 106s - loss: 27.4743 - MinusLogProbMetric: 27.4743 - val_loss: 28.3788 - val_MinusLogProbMetric: 28.3788 - lr: 5.0000e-04 - 106s/epoch - 540ms/step
Epoch 261/1000
2023-09-30 22:58:40.348 
Epoch 261/1000 
	 loss: 27.4298, MinusLogProbMetric: 27.4298, val_loss: 28.3869, val_MinusLogProbMetric: 28.3869

Epoch 261: val_loss did not improve from 28.29838
196/196 - 104s - loss: 27.4298 - MinusLogProbMetric: 27.4298 - val_loss: 28.3869 - val_MinusLogProbMetric: 28.3869 - lr: 5.0000e-04 - 104s/epoch - 532ms/step
Epoch 262/1000
2023-09-30 23:00:26.339 
Epoch 262/1000 
	 loss: 27.4533, MinusLogProbMetric: 27.4533, val_loss: 28.4376, val_MinusLogProbMetric: 28.4376

Epoch 262: val_loss did not improve from 28.29838
196/196 - 106s - loss: 27.4533 - MinusLogProbMetric: 27.4533 - val_loss: 28.4376 - val_MinusLogProbMetric: 28.4376 - lr: 5.0000e-04 - 106s/epoch - 541ms/step
Epoch 263/1000
2023-09-30 23:02:12.561 
Epoch 263/1000 
	 loss: 27.5277, MinusLogProbMetric: 27.5277, val_loss: 28.3943, val_MinusLogProbMetric: 28.3943

Epoch 263: val_loss did not improve from 28.29838
196/196 - 106s - loss: 27.5277 - MinusLogProbMetric: 27.5277 - val_loss: 28.3943 - val_MinusLogProbMetric: 28.3943 - lr: 5.0000e-04 - 106s/epoch - 542ms/step
Epoch 264/1000
2023-09-30 23:03:56.828 
Epoch 264/1000 
	 loss: 27.4905, MinusLogProbMetric: 27.4905, val_loss: 28.3952, val_MinusLogProbMetric: 28.3952

Epoch 264: val_loss did not improve from 28.29838
196/196 - 104s - loss: 27.4905 - MinusLogProbMetric: 27.4905 - val_loss: 28.3952 - val_MinusLogProbMetric: 28.3952 - lr: 5.0000e-04 - 104s/epoch - 532ms/step
Epoch 265/1000
2023-09-30 23:05:42.762 
Epoch 265/1000 
	 loss: 27.4509, MinusLogProbMetric: 27.4509, val_loss: 28.4202, val_MinusLogProbMetric: 28.4202

Epoch 265: val_loss did not improve from 28.29838
196/196 - 106s - loss: 27.4509 - MinusLogProbMetric: 27.4509 - val_loss: 28.4202 - val_MinusLogProbMetric: 28.4202 - lr: 5.0000e-04 - 106s/epoch - 540ms/step
Epoch 266/1000
2023-09-30 23:07:33.918 
Epoch 266/1000 
	 loss: 27.4612, MinusLogProbMetric: 27.4612, val_loss: 28.3237, val_MinusLogProbMetric: 28.3237

Epoch 266: val_loss did not improve from 28.29838
196/196 - 111s - loss: 27.4612 - MinusLogProbMetric: 27.4612 - val_loss: 28.3237 - val_MinusLogProbMetric: 28.3237 - lr: 5.0000e-04 - 111s/epoch - 567ms/step
Epoch 267/1000
2023-09-30 23:09:19.709 
Epoch 267/1000 
	 loss: 27.4297, MinusLogProbMetric: 27.4297, val_loss: 28.5298, val_MinusLogProbMetric: 28.5298

Epoch 267: val_loss did not improve from 28.29838
196/196 - 106s - loss: 27.4297 - MinusLogProbMetric: 27.4297 - val_loss: 28.5298 - val_MinusLogProbMetric: 28.5298 - lr: 5.0000e-04 - 106s/epoch - 540ms/step
Epoch 268/1000
2023-09-30 23:11:00.955 
Epoch 268/1000 
	 loss: 27.4663, MinusLogProbMetric: 27.4663, val_loss: 28.4652, val_MinusLogProbMetric: 28.4652

Epoch 268: val_loss did not improve from 28.29838
196/196 - 101s - loss: 27.4663 - MinusLogProbMetric: 27.4663 - val_loss: 28.4652 - val_MinusLogProbMetric: 28.4652 - lr: 5.0000e-04 - 101s/epoch - 517ms/step
Epoch 269/1000
2023-09-30 23:12:41.010 
Epoch 269/1000 
	 loss: 27.4315, MinusLogProbMetric: 27.4315, val_loss: 28.4428, val_MinusLogProbMetric: 28.4428

Epoch 269: val_loss did not improve from 28.29838
196/196 - 100s - loss: 27.4315 - MinusLogProbMetric: 27.4315 - val_loss: 28.4428 - val_MinusLogProbMetric: 28.4428 - lr: 5.0000e-04 - 100s/epoch - 510ms/step
Epoch 270/1000
2023-09-30 23:14:26.473 
Epoch 270/1000 
	 loss: 27.4179, MinusLogProbMetric: 27.4179, val_loss: 28.6164, val_MinusLogProbMetric: 28.6164

Epoch 270: val_loss did not improve from 28.29838
196/196 - 105s - loss: 27.4179 - MinusLogProbMetric: 27.4179 - val_loss: 28.6164 - val_MinusLogProbMetric: 28.6164 - lr: 5.0000e-04 - 105s/epoch - 538ms/step
Epoch 271/1000
2023-09-30 23:16:12.893 
Epoch 271/1000 
	 loss: 27.4405, MinusLogProbMetric: 27.4405, val_loss: 28.4160, val_MinusLogProbMetric: 28.4160

Epoch 271: val_loss did not improve from 28.29838
196/196 - 106s - loss: 27.4405 - MinusLogProbMetric: 27.4405 - val_loss: 28.4160 - val_MinusLogProbMetric: 28.4160 - lr: 5.0000e-04 - 106s/epoch - 543ms/step
Epoch 272/1000
2023-09-30 23:17:59.082 
Epoch 272/1000 
	 loss: 27.5259, MinusLogProbMetric: 27.5259, val_loss: 28.4791, val_MinusLogProbMetric: 28.4791

Epoch 272: val_loss did not improve from 28.29838
196/196 - 106s - loss: 27.5259 - MinusLogProbMetric: 27.5259 - val_loss: 28.4791 - val_MinusLogProbMetric: 28.4791 - lr: 5.0000e-04 - 106s/epoch - 542ms/step
Epoch 273/1000
2023-09-30 23:19:41.218 
Epoch 273/1000 
	 loss: 27.4314, MinusLogProbMetric: 27.4314, val_loss: 28.4551, val_MinusLogProbMetric: 28.4551

Epoch 273: val_loss did not improve from 28.29838
196/196 - 102s - loss: 27.4314 - MinusLogProbMetric: 27.4314 - val_loss: 28.4551 - val_MinusLogProbMetric: 28.4551 - lr: 5.0000e-04 - 102s/epoch - 521ms/step
Epoch 274/1000
2023-09-30 23:21:27.432 
Epoch 274/1000 
	 loss: 27.4888, MinusLogProbMetric: 27.4888, val_loss: 28.5513, val_MinusLogProbMetric: 28.5513

Epoch 274: val_loss did not improve from 28.29838
196/196 - 106s - loss: 27.4888 - MinusLogProbMetric: 27.4888 - val_loss: 28.5513 - val_MinusLogProbMetric: 28.5513 - lr: 5.0000e-04 - 106s/epoch - 542ms/step
Epoch 275/1000
2023-09-30 23:23:17.967 
Epoch 275/1000 
	 loss: 27.4334, MinusLogProbMetric: 27.4334, val_loss: 28.3649, val_MinusLogProbMetric: 28.3649

Epoch 275: val_loss did not improve from 28.29838
196/196 - 111s - loss: 27.4334 - MinusLogProbMetric: 27.4334 - val_loss: 28.3649 - val_MinusLogProbMetric: 28.3649 - lr: 5.0000e-04 - 111s/epoch - 564ms/step
Epoch 276/1000
2023-09-30 23:25:12.936 
Epoch 276/1000 
	 loss: 27.4220, MinusLogProbMetric: 27.4220, val_loss: 28.4048, val_MinusLogProbMetric: 28.4048

Epoch 276: val_loss did not improve from 28.29838
196/196 - 115s - loss: 27.4220 - MinusLogProbMetric: 27.4220 - val_loss: 28.4048 - val_MinusLogProbMetric: 28.4048 - lr: 5.0000e-04 - 115s/epoch - 587ms/step
Epoch 277/1000
2023-09-30 23:27:02.784 
Epoch 277/1000 
	 loss: 27.4660, MinusLogProbMetric: 27.4660, val_loss: 28.4051, val_MinusLogProbMetric: 28.4051

Epoch 277: val_loss did not improve from 28.29838
196/196 - 110s - loss: 27.4660 - MinusLogProbMetric: 27.4660 - val_loss: 28.4051 - val_MinusLogProbMetric: 28.4051 - lr: 5.0000e-04 - 110s/epoch - 560ms/step
Epoch 278/1000
2023-09-30 23:28:52.270 
Epoch 278/1000 
	 loss: 27.4619, MinusLogProbMetric: 27.4619, val_loss: 28.4562, val_MinusLogProbMetric: 28.4562

Epoch 278: val_loss did not improve from 28.29838
196/196 - 109s - loss: 27.4619 - MinusLogProbMetric: 27.4619 - val_loss: 28.4562 - val_MinusLogProbMetric: 28.4562 - lr: 5.0000e-04 - 109s/epoch - 558ms/step
Epoch 279/1000
2023-09-30 23:30:45.451 
Epoch 279/1000 
	 loss: 27.4445, MinusLogProbMetric: 27.4445, val_loss: 28.3343, val_MinusLogProbMetric: 28.3343

Epoch 279: val_loss did not improve from 28.29838
196/196 - 113s - loss: 27.4445 - MinusLogProbMetric: 27.4445 - val_loss: 28.3343 - val_MinusLogProbMetric: 28.3343 - lr: 5.0000e-04 - 113s/epoch - 577ms/step
Epoch 280/1000
2023-09-30 23:32:31.411 
Epoch 280/1000 
	 loss: 27.4382, MinusLogProbMetric: 27.4382, val_loss: 28.3741, val_MinusLogProbMetric: 28.3741

Epoch 280: val_loss did not improve from 28.29838
196/196 - 106s - loss: 27.4382 - MinusLogProbMetric: 27.4382 - val_loss: 28.3741 - val_MinusLogProbMetric: 28.3741 - lr: 5.0000e-04 - 106s/epoch - 541ms/step
Epoch 281/1000
2023-09-30 23:34:18.307 
Epoch 281/1000 
	 loss: 27.2503, MinusLogProbMetric: 27.2503, val_loss: 28.2886, val_MinusLogProbMetric: 28.2886

Epoch 281: val_loss improved from 28.29838 to 28.28856, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_348/weights/best_weights.h5
196/196 - 108s - loss: 27.2503 - MinusLogProbMetric: 27.2503 - val_loss: 28.2886 - val_MinusLogProbMetric: 28.2886 - lr: 2.5000e-04 - 108s/epoch - 553ms/step
Epoch 282/1000
2023-09-30 23:35:58.713 
Epoch 282/1000 
	 loss: 27.2469, MinusLogProbMetric: 27.2469, val_loss: 28.2715, val_MinusLogProbMetric: 28.2715

Epoch 282: val_loss improved from 28.28856 to 28.27154, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_348/weights/best_weights.h5
196/196 - 104s - loss: 27.2469 - MinusLogProbMetric: 27.2469 - val_loss: 28.2715 - val_MinusLogProbMetric: 28.2715 - lr: 2.5000e-04 - 104s/epoch - 531ms/step
Epoch 283/1000
2023-09-30 23:37:43.829 
Epoch 283/1000 
	 loss: 27.2365, MinusLogProbMetric: 27.2365, val_loss: 28.2609, val_MinusLogProbMetric: 28.2609

Epoch 283: val_loss improved from 28.27154 to 28.26094, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_348/weights/best_weights.h5
196/196 - 102s - loss: 27.2365 - MinusLogProbMetric: 27.2365 - val_loss: 28.2609 - val_MinusLogProbMetric: 28.2609 - lr: 2.5000e-04 - 102s/epoch - 522ms/step
Epoch 284/1000
2023-09-30 23:39:36.748 
Epoch 284/1000 
	 loss: 27.2393, MinusLogProbMetric: 27.2393, val_loss: 28.2979, val_MinusLogProbMetric: 28.2979

Epoch 284: val_loss did not improve from 28.26094
196/196 - 111s - loss: 27.2393 - MinusLogProbMetric: 27.2393 - val_loss: 28.2979 - val_MinusLogProbMetric: 28.2979 - lr: 2.5000e-04 - 111s/epoch - 564ms/step
Epoch 285/1000
2023-09-30 23:41:24.393 
Epoch 285/1000 
	 loss: 27.2442, MinusLogProbMetric: 27.2442, val_loss: 28.2751, val_MinusLogProbMetric: 28.2751

Epoch 285: val_loss did not improve from 28.26094
196/196 - 108s - loss: 27.2442 - MinusLogProbMetric: 27.2442 - val_loss: 28.2751 - val_MinusLogProbMetric: 28.2751 - lr: 2.5000e-04 - 108s/epoch - 549ms/step
Epoch 286/1000
2023-09-30 23:43:17.133 
Epoch 286/1000 
	 loss: 27.2298, MinusLogProbMetric: 27.2298, val_loss: 28.2933, val_MinusLogProbMetric: 28.2933

Epoch 286: val_loss did not improve from 28.26094
196/196 - 113s - loss: 27.2298 - MinusLogProbMetric: 27.2298 - val_loss: 28.2933 - val_MinusLogProbMetric: 28.2933 - lr: 2.5000e-04 - 113s/epoch - 575ms/step
Epoch 287/1000
2023-09-30 23:45:07.888 
Epoch 287/1000 
	 loss: 27.2380, MinusLogProbMetric: 27.2380, val_loss: 28.2416, val_MinusLogProbMetric: 28.2416

Epoch 287: val_loss improved from 28.26094 to 28.24159, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_348/weights/best_weights.h5
196/196 - 113s - loss: 27.2380 - MinusLogProbMetric: 27.2380 - val_loss: 28.2416 - val_MinusLogProbMetric: 28.2416 - lr: 2.5000e-04 - 113s/epoch - 577ms/step
Epoch 288/1000
2023-09-30 23:46:55.125 
Epoch 288/1000 
	 loss: 27.2305, MinusLogProbMetric: 27.2305, val_loss: 28.2717, val_MinusLogProbMetric: 28.2717

Epoch 288: val_loss did not improve from 28.24159
196/196 - 105s - loss: 27.2305 - MinusLogProbMetric: 27.2305 - val_loss: 28.2717 - val_MinusLogProbMetric: 28.2717 - lr: 2.5000e-04 - 105s/epoch - 535ms/step
Epoch 289/1000
2023-09-30 23:48:35.770 
Epoch 289/1000 
	 loss: 27.2299, MinusLogProbMetric: 27.2299, val_loss: 28.2773, val_MinusLogProbMetric: 28.2773

Epoch 289: val_loss did not improve from 28.24159
196/196 - 101s - loss: 27.2299 - MinusLogProbMetric: 27.2299 - val_loss: 28.2773 - val_MinusLogProbMetric: 28.2773 - lr: 2.5000e-04 - 101s/epoch - 514ms/step
Epoch 290/1000
2023-09-30 23:50:16.729 
Epoch 290/1000 
	 loss: 27.2317, MinusLogProbMetric: 27.2317, val_loss: 28.2463, val_MinusLogProbMetric: 28.2463

Epoch 290: val_loss did not improve from 28.24159
196/196 - 101s - loss: 27.2317 - MinusLogProbMetric: 27.2317 - val_loss: 28.2463 - val_MinusLogProbMetric: 28.2463 - lr: 2.5000e-04 - 101s/epoch - 515ms/step
Epoch 291/1000
2023-09-30 23:52:08.005 
Epoch 291/1000 
	 loss: 27.2382, MinusLogProbMetric: 27.2382, val_loss: 28.2978, val_MinusLogProbMetric: 28.2978

Epoch 291: val_loss did not improve from 28.24159
196/196 - 111s - loss: 27.2382 - MinusLogProbMetric: 27.2382 - val_loss: 28.2978 - val_MinusLogProbMetric: 28.2978 - lr: 2.5000e-04 - 111s/epoch - 568ms/step
Epoch 292/1000
2023-09-30 23:53:54.962 
Epoch 292/1000 
	 loss: 27.2411, MinusLogProbMetric: 27.2411, val_loss: 28.3336, val_MinusLogProbMetric: 28.3336

Epoch 292: val_loss did not improve from 28.24159
196/196 - 107s - loss: 27.2411 - MinusLogProbMetric: 27.2411 - val_loss: 28.3336 - val_MinusLogProbMetric: 28.3336 - lr: 2.5000e-04 - 107s/epoch - 546ms/step
Epoch 293/1000
2023-09-30 23:55:39.201 
Epoch 293/1000 
	 loss: 27.2339, MinusLogProbMetric: 27.2339, val_loss: 28.2422, val_MinusLogProbMetric: 28.2422

Epoch 293: val_loss did not improve from 28.24159
196/196 - 104s - loss: 27.2339 - MinusLogProbMetric: 27.2339 - val_loss: 28.2422 - val_MinusLogProbMetric: 28.2422 - lr: 2.5000e-04 - 104s/epoch - 532ms/step
Epoch 294/1000
2023-09-30 23:57:21.850 
Epoch 294/1000 
	 loss: 27.2437, MinusLogProbMetric: 27.2437, val_loss: 28.2733, val_MinusLogProbMetric: 28.2733

Epoch 294: val_loss did not improve from 28.24159
196/196 - 103s - loss: 27.2437 - MinusLogProbMetric: 27.2437 - val_loss: 28.2733 - val_MinusLogProbMetric: 28.2733 - lr: 2.5000e-04 - 103s/epoch - 524ms/step
Epoch 295/1000
2023-09-30 23:59:04.966 
Epoch 295/1000 
	 loss: 27.2293, MinusLogProbMetric: 27.2293, val_loss: 28.2823, val_MinusLogProbMetric: 28.2823

Epoch 295: val_loss did not improve from 28.24159
196/196 - 103s - loss: 27.2293 - MinusLogProbMetric: 27.2293 - val_loss: 28.2823 - val_MinusLogProbMetric: 28.2823 - lr: 2.5000e-04 - 103s/epoch - 526ms/step
Epoch 296/1000
2023-10-01 00:00:44.944 
Epoch 296/1000 
	 loss: 27.2296, MinusLogProbMetric: 27.2296, val_loss: 28.2893, val_MinusLogProbMetric: 28.2893

Epoch 296: val_loss did not improve from 28.24159
196/196 - 100s - loss: 27.2296 - MinusLogProbMetric: 27.2296 - val_loss: 28.2893 - val_MinusLogProbMetric: 28.2893 - lr: 2.5000e-04 - 100s/epoch - 510ms/step
Epoch 297/1000
2023-10-01 00:02:20.514 
Epoch 297/1000 
	 loss: 27.2342, MinusLogProbMetric: 27.2342, val_loss: 28.3083, val_MinusLogProbMetric: 28.3083

Epoch 297: val_loss did not improve from 28.24159
196/196 - 96s - loss: 27.2342 - MinusLogProbMetric: 27.2342 - val_loss: 28.3083 - val_MinusLogProbMetric: 28.3083 - lr: 2.5000e-04 - 96s/epoch - 488ms/step
Epoch 298/1000
2023-10-01 00:03:52.960 
Epoch 298/1000 
	 loss: 27.2189, MinusLogProbMetric: 27.2189, val_loss: 28.3593, val_MinusLogProbMetric: 28.3593

Epoch 298: val_loss did not improve from 28.24159
196/196 - 92s - loss: 27.2189 - MinusLogProbMetric: 27.2189 - val_loss: 28.3593 - val_MinusLogProbMetric: 28.3593 - lr: 2.5000e-04 - 92s/epoch - 472ms/step
Epoch 299/1000
2023-10-01 00:05:38.539 
Epoch 299/1000 
	 loss: 27.2319, MinusLogProbMetric: 27.2319, val_loss: 28.2528, val_MinusLogProbMetric: 28.2528

Epoch 299: val_loss did not improve from 28.24159
196/196 - 106s - loss: 27.2319 - MinusLogProbMetric: 27.2319 - val_loss: 28.2528 - val_MinusLogProbMetric: 28.2528 - lr: 2.5000e-04 - 106s/epoch - 539ms/step
Epoch 300/1000
2023-10-01 00:07:17.689 
Epoch 300/1000 
	 loss: 27.2181, MinusLogProbMetric: 27.2181, val_loss: 28.3347, val_MinusLogProbMetric: 28.3347

Epoch 300: val_loss did not improve from 28.24159
196/196 - 99s - loss: 27.2181 - MinusLogProbMetric: 27.2181 - val_loss: 28.3347 - val_MinusLogProbMetric: 28.3347 - lr: 2.5000e-04 - 99s/epoch - 506ms/step
Epoch 301/1000
2023-10-01 00:08:51.514 
Epoch 301/1000 
	 loss: 27.2290, MinusLogProbMetric: 27.2290, val_loss: 28.2931, val_MinusLogProbMetric: 28.2931

Epoch 301: val_loss did not improve from 28.24159
196/196 - 94s - loss: 27.2290 - MinusLogProbMetric: 27.2290 - val_loss: 28.2931 - val_MinusLogProbMetric: 28.2931 - lr: 2.5000e-04 - 94s/epoch - 479ms/step
Epoch 302/1000
2023-10-01 00:10:22.872 
Epoch 302/1000 
	 loss: 27.2305, MinusLogProbMetric: 27.2305, val_loss: 28.2985, val_MinusLogProbMetric: 28.2985

Epoch 302: val_loss did not improve from 28.24159
196/196 - 91s - loss: 27.2305 - MinusLogProbMetric: 27.2305 - val_loss: 28.2985 - val_MinusLogProbMetric: 28.2985 - lr: 2.5000e-04 - 91s/epoch - 465ms/step
Epoch 303/1000
2023-10-01 00:11:50.149 
Epoch 303/1000 
	 loss: 27.2134, MinusLogProbMetric: 27.2134, val_loss: 28.2407, val_MinusLogProbMetric: 28.2407

Epoch 303: val_loss improved from 28.24159 to 28.24070, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_348/weights/best_weights.h5
196/196 - 92s - loss: 27.2134 - MinusLogProbMetric: 27.2134 - val_loss: 28.2407 - val_MinusLogProbMetric: 28.2407 - lr: 2.5000e-04 - 92s/epoch - 467ms/step
Epoch 304/1000
2023-10-01 00:13:27.435 
Epoch 304/1000 
	 loss: 27.2366, MinusLogProbMetric: 27.2366, val_loss: 28.3549, val_MinusLogProbMetric: 28.3549

Epoch 304: val_loss did not improve from 28.24070
196/196 - 93s - loss: 27.2366 - MinusLogProbMetric: 27.2366 - val_loss: 28.3549 - val_MinusLogProbMetric: 28.3549 - lr: 2.5000e-04 - 93s/epoch - 475ms/step
Epoch 305/1000
2023-10-01 00:15:00.144 
Epoch 305/1000 
	 loss: 27.2149, MinusLogProbMetric: 27.2149, val_loss: 28.3200, val_MinusLogProbMetric: 28.3200

Epoch 305: val_loss did not improve from 28.24070
196/196 - 93s - loss: 27.2149 - MinusLogProbMetric: 27.2149 - val_loss: 28.3200 - val_MinusLogProbMetric: 28.3200 - lr: 2.5000e-04 - 93s/epoch - 473ms/step
Epoch 306/1000
2023-10-01 00:16:31.722 
Epoch 306/1000 
	 loss: 27.2209, MinusLogProbMetric: 27.2209, val_loss: 28.2840, val_MinusLogProbMetric: 28.2840

Epoch 306: val_loss did not improve from 28.24070
196/196 - 92s - loss: 27.2209 - MinusLogProbMetric: 27.2209 - val_loss: 28.2840 - val_MinusLogProbMetric: 28.2840 - lr: 2.5000e-04 - 92s/epoch - 467ms/step
Epoch 307/1000
2023-10-01 00:18:13.526 
Epoch 307/1000 
	 loss: 27.2166, MinusLogProbMetric: 27.2166, val_loss: 28.3017, val_MinusLogProbMetric: 28.3017

Epoch 307: val_loss did not improve from 28.24070
196/196 - 102s - loss: 27.2166 - MinusLogProbMetric: 27.2166 - val_loss: 28.3017 - val_MinusLogProbMetric: 28.3017 - lr: 2.5000e-04 - 102s/epoch - 519ms/step
Epoch 308/1000
2023-10-01 00:19:53.830 
Epoch 308/1000 
	 loss: 27.2138, MinusLogProbMetric: 27.2138, val_loss: 28.2811, val_MinusLogProbMetric: 28.2811

Epoch 308: val_loss did not improve from 28.24070
196/196 - 100s - loss: 27.2138 - MinusLogProbMetric: 27.2138 - val_loss: 28.2811 - val_MinusLogProbMetric: 28.2811 - lr: 2.5000e-04 - 100s/epoch - 511ms/step
Epoch 309/1000
2023-10-01 00:21:29.004 
Epoch 309/1000 
	 loss: 27.2233, MinusLogProbMetric: 27.2233, val_loss: 28.2455, val_MinusLogProbMetric: 28.2455

Epoch 309: val_loss did not improve from 28.24070
196/196 - 95s - loss: 27.2233 - MinusLogProbMetric: 27.2233 - val_loss: 28.2455 - val_MinusLogProbMetric: 28.2455 - lr: 2.5000e-04 - 95s/epoch - 486ms/step
Epoch 310/1000
2023-10-01 00:23:00.739 
Epoch 310/1000 
	 loss: 27.2234, MinusLogProbMetric: 27.2234, val_loss: 28.3676, val_MinusLogProbMetric: 28.3676

Epoch 310: val_loss did not improve from 28.24070
196/196 - 92s - loss: 27.2234 - MinusLogProbMetric: 27.2234 - val_loss: 28.3676 - val_MinusLogProbMetric: 28.3676 - lr: 2.5000e-04 - 92s/epoch - 468ms/step
Epoch 311/1000
2023-10-01 00:24:31.128 
Epoch 311/1000 
	 loss: 27.2102, MinusLogProbMetric: 27.2102, val_loss: 28.2550, val_MinusLogProbMetric: 28.2550

Epoch 311: val_loss did not improve from 28.24070
196/196 - 90s - loss: 27.2102 - MinusLogProbMetric: 27.2102 - val_loss: 28.2550 - val_MinusLogProbMetric: 28.2550 - lr: 2.5000e-04 - 90s/epoch - 461ms/step
Epoch 312/1000
2023-10-01 00:26:00.498 
Epoch 312/1000 
	 loss: 27.2124, MinusLogProbMetric: 27.2124, val_loss: 28.3035, val_MinusLogProbMetric: 28.3035

Epoch 312: val_loss did not improve from 28.24070
196/196 - 89s - loss: 27.2124 - MinusLogProbMetric: 27.2124 - val_loss: 28.3035 - val_MinusLogProbMetric: 28.3035 - lr: 2.5000e-04 - 89s/epoch - 456ms/step
Epoch 313/1000
2023-10-01 00:27:31.140 
Epoch 313/1000 
	 loss: 27.2087, MinusLogProbMetric: 27.2087, val_loss: 28.2546, val_MinusLogProbMetric: 28.2546

Epoch 313: val_loss did not improve from 28.24070
196/196 - 91s - loss: 27.2087 - MinusLogProbMetric: 27.2087 - val_loss: 28.2546 - val_MinusLogProbMetric: 28.2546 - lr: 2.5000e-04 - 91s/epoch - 463ms/step
Epoch 314/1000
2023-10-01 00:28:57.953 
Epoch 314/1000 
	 loss: 27.2156, MinusLogProbMetric: 27.2156, val_loss: 28.3116, val_MinusLogProbMetric: 28.3116

Epoch 314: val_loss did not improve from 28.24070
196/196 - 87s - loss: 27.2156 - MinusLogProbMetric: 27.2156 - val_loss: 28.3116 - val_MinusLogProbMetric: 28.3116 - lr: 2.5000e-04 - 87s/epoch - 442ms/step
Epoch 315/1000
2023-10-01 00:30:28.192 
Epoch 315/1000 
	 loss: 27.2115, MinusLogProbMetric: 27.2115, val_loss: 28.2875, val_MinusLogProbMetric: 28.2875

Epoch 315: val_loss did not improve from 28.24070
196/196 - 90s - loss: 27.2115 - MinusLogProbMetric: 27.2115 - val_loss: 28.2875 - val_MinusLogProbMetric: 28.2875 - lr: 2.5000e-04 - 90s/epoch - 460ms/step
Epoch 316/1000
2023-10-01 00:31:58.170 
Epoch 316/1000 
	 loss: 27.2186, MinusLogProbMetric: 27.2186, val_loss: 28.2974, val_MinusLogProbMetric: 28.2974

Epoch 316: val_loss did not improve from 28.24070
196/196 - 90s - loss: 27.2186 - MinusLogProbMetric: 27.2186 - val_loss: 28.2974 - val_MinusLogProbMetric: 28.2974 - lr: 2.5000e-04 - 90s/epoch - 459ms/step
Epoch 317/1000
2023-10-01 00:33:28.893 
Epoch 317/1000 
	 loss: 27.2080, MinusLogProbMetric: 27.2080, val_loss: 28.3063, val_MinusLogProbMetric: 28.3063

Epoch 317: val_loss did not improve from 28.24070
196/196 - 91s - loss: 27.2080 - MinusLogProbMetric: 27.2080 - val_loss: 28.3063 - val_MinusLogProbMetric: 28.3063 - lr: 2.5000e-04 - 91s/epoch - 463ms/step
Epoch 318/1000
2023-10-01 00:34:57.831 
Epoch 318/1000 
	 loss: 27.2028, MinusLogProbMetric: 27.2028, val_loss: 28.2740, val_MinusLogProbMetric: 28.2740

Epoch 318: val_loss did not improve from 28.24070
196/196 - 89s - loss: 27.2028 - MinusLogProbMetric: 27.2028 - val_loss: 28.2740 - val_MinusLogProbMetric: 28.2740 - lr: 2.5000e-04 - 89s/epoch - 453ms/step
Epoch 319/1000
2023-10-01 00:36:31.333 
Epoch 319/1000 
	 loss: 27.1958, MinusLogProbMetric: 27.1958, val_loss: 28.2612, val_MinusLogProbMetric: 28.2612

Epoch 319: val_loss did not improve from 28.24070
196/196 - 94s - loss: 27.1958 - MinusLogProbMetric: 27.1958 - val_loss: 28.2612 - val_MinusLogProbMetric: 28.2612 - lr: 2.5000e-04 - 94s/epoch - 477ms/step
Epoch 320/1000
2023-10-01 00:38:04.966 
Epoch 320/1000 
	 loss: 27.1956, MinusLogProbMetric: 27.1956, val_loss: 28.3120, val_MinusLogProbMetric: 28.3120

Epoch 320: val_loss did not improve from 28.24070
196/196 - 94s - loss: 27.1956 - MinusLogProbMetric: 27.1956 - val_loss: 28.3120 - val_MinusLogProbMetric: 28.3120 - lr: 2.5000e-04 - 94s/epoch - 478ms/step
Epoch 321/1000
2023-10-01 00:39:35.613 
Epoch 321/1000 
	 loss: 27.2058, MinusLogProbMetric: 27.2058, val_loss: 28.3149, val_MinusLogProbMetric: 28.3149

Epoch 321: val_loss did not improve from 28.24070
196/196 - 91s - loss: 27.2058 - MinusLogProbMetric: 27.2058 - val_loss: 28.3149 - val_MinusLogProbMetric: 28.3149 - lr: 2.5000e-04 - 91s/epoch - 462ms/step
Epoch 322/1000
2023-10-01 00:41:04.955 
Epoch 322/1000 
	 loss: 27.2043, MinusLogProbMetric: 27.2043, val_loss: 28.2816, val_MinusLogProbMetric: 28.2816

Epoch 322: val_loss did not improve from 28.24070
196/196 - 89s - loss: 27.2043 - MinusLogProbMetric: 27.2043 - val_loss: 28.2816 - val_MinusLogProbMetric: 28.2816 - lr: 2.5000e-04 - 89s/epoch - 456ms/step
Epoch 323/1000
2023-10-01 00:42:42.076 
Epoch 323/1000 
	 loss: 27.1955, MinusLogProbMetric: 27.1955, val_loss: 28.3326, val_MinusLogProbMetric: 28.3326

Epoch 323: val_loss did not improve from 28.24070
196/196 - 97s - loss: 27.1955 - MinusLogProbMetric: 27.1955 - val_loss: 28.3326 - val_MinusLogProbMetric: 28.3326 - lr: 2.5000e-04 - 97s/epoch - 496ms/step
Epoch 324/1000
2023-10-01 00:44:11.033 
Epoch 324/1000 
	 loss: 27.2119, MinusLogProbMetric: 27.2119, val_loss: 28.3146, val_MinusLogProbMetric: 28.3146

Epoch 324: val_loss did not improve from 28.24070
196/196 - 89s - loss: 27.2119 - MinusLogProbMetric: 27.2119 - val_loss: 28.3146 - val_MinusLogProbMetric: 28.3146 - lr: 2.5000e-04 - 89s/epoch - 454ms/step
Epoch 325/1000
2023-10-01 00:45:50.380 
Epoch 325/1000 
	 loss: 27.2084, MinusLogProbMetric: 27.2084, val_loss: 28.3128, val_MinusLogProbMetric: 28.3128

Epoch 325: val_loss did not improve from 28.24070
196/196 - 99s - loss: 27.2084 - MinusLogProbMetric: 27.2084 - val_loss: 28.3128 - val_MinusLogProbMetric: 28.3128 - lr: 2.5000e-04 - 99s/epoch - 507ms/step
Epoch 326/1000
2023-10-01 00:47:27.961 
Epoch 326/1000 
	 loss: 27.2043, MinusLogProbMetric: 27.2043, val_loss: 28.3072, val_MinusLogProbMetric: 28.3072

Epoch 326: val_loss did not improve from 28.24070
196/196 - 98s - loss: 27.2043 - MinusLogProbMetric: 27.2043 - val_loss: 28.3072 - val_MinusLogProbMetric: 28.3072 - lr: 2.5000e-04 - 98s/epoch - 498ms/step
Epoch 327/1000
2023-10-01 00:49:06.536 
Epoch 327/1000 
	 loss: 27.1863, MinusLogProbMetric: 27.1863, val_loss: 28.3460, val_MinusLogProbMetric: 28.3460

Epoch 327: val_loss did not improve from 28.24070
196/196 - 99s - loss: 27.1863 - MinusLogProbMetric: 27.1863 - val_loss: 28.3460 - val_MinusLogProbMetric: 28.3460 - lr: 2.5000e-04 - 99s/epoch - 503ms/step
Epoch 328/1000
2023-10-01 00:50:42.931 
Epoch 328/1000 
	 loss: 27.1891, MinusLogProbMetric: 27.1891, val_loss: 28.3097, val_MinusLogProbMetric: 28.3097

Epoch 328: val_loss did not improve from 28.24070
196/196 - 96s - loss: 27.1891 - MinusLogProbMetric: 27.1891 - val_loss: 28.3097 - val_MinusLogProbMetric: 28.3097 - lr: 2.5000e-04 - 96s/epoch - 492ms/step
Epoch 329/1000
2023-10-01 00:52:19.335 
Epoch 329/1000 
	 loss: 27.1979, MinusLogProbMetric: 27.1979, val_loss: 28.3211, val_MinusLogProbMetric: 28.3211

Epoch 329: val_loss did not improve from 28.24070
196/196 - 96s - loss: 27.1979 - MinusLogProbMetric: 27.1979 - val_loss: 28.3211 - val_MinusLogProbMetric: 28.3211 - lr: 2.5000e-04 - 96s/epoch - 492ms/step
Epoch 330/1000
2023-10-01 00:54:03.443 
Epoch 330/1000 
	 loss: 27.1867, MinusLogProbMetric: 27.1867, val_loss: 28.2706, val_MinusLogProbMetric: 28.2706

Epoch 330: val_loss did not improve from 28.24070
196/196 - 104s - loss: 27.1867 - MinusLogProbMetric: 27.1867 - val_loss: 28.2706 - val_MinusLogProbMetric: 28.2706 - lr: 2.5000e-04 - 104s/epoch - 530ms/step
Epoch 331/1000
2023-10-01 00:55:43.887 
Epoch 331/1000 
	 loss: 27.2002, MinusLogProbMetric: 27.2002, val_loss: 28.2625, val_MinusLogProbMetric: 28.2625

Epoch 331: val_loss did not improve from 28.24070
196/196 - 100s - loss: 27.2002 - MinusLogProbMetric: 27.2002 - val_loss: 28.2625 - val_MinusLogProbMetric: 28.2625 - lr: 2.5000e-04 - 100s/epoch - 513ms/step
Epoch 332/1000
2023-10-01 00:57:23.908 
Epoch 332/1000 
	 loss: 27.2017, MinusLogProbMetric: 27.2017, val_loss: 28.3127, val_MinusLogProbMetric: 28.3127

Epoch 332: val_loss did not improve from 28.24070
196/196 - 100s - loss: 27.2017 - MinusLogProbMetric: 27.2017 - val_loss: 28.3127 - val_MinusLogProbMetric: 28.3127 - lr: 2.5000e-04 - 100s/epoch - 510ms/step
Epoch 333/1000
2023-10-01 00:58:57.995 
Epoch 333/1000 
	 loss: 27.1800, MinusLogProbMetric: 27.1800, val_loss: 28.3051, val_MinusLogProbMetric: 28.3051

Epoch 333: val_loss did not improve from 28.24070
196/196 - 94s - loss: 27.1800 - MinusLogProbMetric: 27.1800 - val_loss: 28.3051 - val_MinusLogProbMetric: 28.3051 - lr: 2.5000e-04 - 94s/epoch - 480ms/step
Epoch 334/1000
2023-10-01 01:00:42.117 
Epoch 334/1000 
	 loss: 27.1958, MinusLogProbMetric: 27.1958, val_loss: 28.2802, val_MinusLogProbMetric: 28.2802

Epoch 334: val_loss did not improve from 28.24070
196/196 - 104s - loss: 27.1958 - MinusLogProbMetric: 27.1958 - val_loss: 28.2802 - val_MinusLogProbMetric: 28.2802 - lr: 2.5000e-04 - 104s/epoch - 531ms/step
Epoch 335/1000
2023-10-01 01:02:22.502 
Epoch 335/1000 
	 loss: 27.1844, MinusLogProbMetric: 27.1844, val_loss: 28.2811, val_MinusLogProbMetric: 28.2811

Epoch 335: val_loss did not improve from 28.24070
196/196 - 100s - loss: 27.1844 - MinusLogProbMetric: 27.1844 - val_loss: 28.2811 - val_MinusLogProbMetric: 28.2811 - lr: 2.5000e-04 - 100s/epoch - 512ms/step
Epoch 336/1000
2023-10-01 01:04:06.732 
Epoch 336/1000 
	 loss: 27.1900, MinusLogProbMetric: 27.1900, val_loss: 28.2900, val_MinusLogProbMetric: 28.2900

Epoch 336: val_loss did not improve from 28.24070
196/196 - 104s - loss: 27.1900 - MinusLogProbMetric: 27.1900 - val_loss: 28.2900 - val_MinusLogProbMetric: 28.2900 - lr: 2.5000e-04 - 104s/epoch - 532ms/step
Epoch 337/1000
2023-10-01 01:05:52.515 
Epoch 337/1000 
	 loss: 27.1867, MinusLogProbMetric: 27.1867, val_loss: 28.2948, val_MinusLogProbMetric: 28.2948

Epoch 337: val_loss did not improve from 28.24070
196/196 - 106s - loss: 27.1867 - MinusLogProbMetric: 27.1867 - val_loss: 28.2948 - val_MinusLogProbMetric: 28.2948 - lr: 2.5000e-04 - 106s/epoch - 540ms/step
Epoch 338/1000
2023-10-01 01:07:27.473 
Epoch 338/1000 
	 loss: 27.1886, MinusLogProbMetric: 27.1886, val_loss: 28.3230, val_MinusLogProbMetric: 28.3230

Epoch 338: val_loss did not improve from 28.24070
196/196 - 95s - loss: 27.1886 - MinusLogProbMetric: 27.1886 - val_loss: 28.3230 - val_MinusLogProbMetric: 28.3230 - lr: 2.5000e-04 - 95s/epoch - 484ms/step
Epoch 339/1000
2023-10-01 01:08:59.494 
Epoch 339/1000 
	 loss: 27.1952, MinusLogProbMetric: 27.1952, val_loss: 28.2826, val_MinusLogProbMetric: 28.2826

Epoch 339: val_loss did not improve from 28.24070
196/196 - 92s - loss: 27.1952 - MinusLogProbMetric: 27.1952 - val_loss: 28.2826 - val_MinusLogProbMetric: 28.2826 - lr: 2.5000e-04 - 92s/epoch - 469ms/step
Epoch 340/1000
2023-10-01 01:10:37.809 
Epoch 340/1000 
	 loss: 27.1763, MinusLogProbMetric: 27.1763, val_loss: 28.3826, val_MinusLogProbMetric: 28.3826

Epoch 340: val_loss did not improve from 28.24070
196/196 - 98s - loss: 27.1763 - MinusLogProbMetric: 27.1763 - val_loss: 28.3826 - val_MinusLogProbMetric: 28.3826 - lr: 2.5000e-04 - 98s/epoch - 502ms/step
Epoch 341/1000
2023-10-01 01:12:20.267 
Epoch 341/1000 
	 loss: 27.1866, MinusLogProbMetric: 27.1866, val_loss: 28.3444, val_MinusLogProbMetric: 28.3444

Epoch 341: val_loss did not improve from 28.24070
196/196 - 102s - loss: 27.1866 - MinusLogProbMetric: 27.1866 - val_loss: 28.3444 - val_MinusLogProbMetric: 28.3444 - lr: 2.5000e-04 - 102s/epoch - 522ms/step
Epoch 342/1000
2023-10-01 01:13:55.433 
Epoch 342/1000 
	 loss: 27.1805, MinusLogProbMetric: 27.1805, val_loss: 28.2964, val_MinusLogProbMetric: 28.2964

Epoch 342: val_loss did not improve from 28.24070
196/196 - 95s - loss: 27.1805 - MinusLogProbMetric: 27.1805 - val_loss: 28.2964 - val_MinusLogProbMetric: 28.2964 - lr: 2.5000e-04 - 95s/epoch - 486ms/step
Epoch 343/1000
2023-10-01 01:15:35.344 
Epoch 343/1000 
	 loss: 27.1804, MinusLogProbMetric: 27.1804, val_loss: 28.3043, val_MinusLogProbMetric: 28.3043

Epoch 343: val_loss did not improve from 28.24070
196/196 - 100s - loss: 27.1804 - MinusLogProbMetric: 27.1804 - val_loss: 28.3043 - val_MinusLogProbMetric: 28.3043 - lr: 2.5000e-04 - 100s/epoch - 510ms/step
Epoch 344/1000
2023-10-01 01:17:12.500 
Epoch 344/1000 
	 loss: 27.1862, MinusLogProbMetric: 27.1862, val_loss: 28.4050, val_MinusLogProbMetric: 28.4050

Epoch 344: val_loss did not improve from 28.24070
196/196 - 97s - loss: 27.1862 - MinusLogProbMetric: 27.1862 - val_loss: 28.4050 - val_MinusLogProbMetric: 28.4050 - lr: 2.5000e-04 - 97s/epoch - 496ms/step
Epoch 345/1000
2023-10-01 01:18:49.124 
Epoch 345/1000 
	 loss: 27.1822, MinusLogProbMetric: 27.1822, val_loss: 28.2588, val_MinusLogProbMetric: 28.2588

Epoch 345: val_loss did not improve from 28.24070
196/196 - 97s - loss: 27.1822 - MinusLogProbMetric: 27.1822 - val_loss: 28.2588 - val_MinusLogProbMetric: 28.2588 - lr: 2.5000e-04 - 97s/epoch - 493ms/step
Epoch 346/1000
2023-10-01 01:20:37.040 
Epoch 346/1000 
	 loss: 27.1766, MinusLogProbMetric: 27.1766, val_loss: 28.2660, val_MinusLogProbMetric: 28.2660

Epoch 346: val_loss did not improve from 28.24070
196/196 - 108s - loss: 27.1766 - MinusLogProbMetric: 27.1766 - val_loss: 28.2660 - val_MinusLogProbMetric: 28.2660 - lr: 2.5000e-04 - 108s/epoch - 551ms/step
Epoch 347/1000
2023-10-01 01:22:18.693 
Epoch 347/1000 
	 loss: 27.1749, MinusLogProbMetric: 27.1749, val_loss: 28.2628, val_MinusLogProbMetric: 28.2628

Epoch 347: val_loss did not improve from 28.24070
196/196 - 102s - loss: 27.1749 - MinusLogProbMetric: 27.1749 - val_loss: 28.2628 - val_MinusLogProbMetric: 28.2628 - lr: 2.5000e-04 - 102s/epoch - 519ms/step
Epoch 348/1000
2023-10-01 01:23:57.593 
Epoch 348/1000 
	 loss: 27.1949, MinusLogProbMetric: 27.1949, val_loss: 28.3179, val_MinusLogProbMetric: 28.3179

Epoch 348: val_loss did not improve from 28.24070
196/196 - 99s - loss: 27.1949 - MinusLogProbMetric: 27.1949 - val_loss: 28.3179 - val_MinusLogProbMetric: 28.3179 - lr: 2.5000e-04 - 99s/epoch - 505ms/step
Epoch 349/1000
2023-10-01 01:25:43.692 
Epoch 349/1000 
	 loss: 27.1741, MinusLogProbMetric: 27.1741, val_loss: 28.3228, val_MinusLogProbMetric: 28.3228

Epoch 349: val_loss did not improve from 28.24070
196/196 - 106s - loss: 27.1741 - MinusLogProbMetric: 27.1741 - val_loss: 28.3228 - val_MinusLogProbMetric: 28.3228 - lr: 2.5000e-04 - 106s/epoch - 541ms/step
Epoch 350/1000
2023-10-01 01:27:30.886 
Epoch 350/1000 
	 loss: 27.1766, MinusLogProbMetric: 27.1766, val_loss: 28.3341, val_MinusLogProbMetric: 28.3341

Epoch 350: val_loss did not improve from 28.24070
196/196 - 107s - loss: 27.1766 - MinusLogProbMetric: 27.1766 - val_loss: 28.3341 - val_MinusLogProbMetric: 28.3341 - lr: 2.5000e-04 - 107s/epoch - 547ms/step
Epoch 351/1000
2023-10-01 01:29:17.472 
Epoch 351/1000 
	 loss: 27.1772, MinusLogProbMetric: 27.1772, val_loss: 28.2877, val_MinusLogProbMetric: 28.2877

Epoch 351: val_loss did not improve from 28.24070
196/196 - 107s - loss: 27.1772 - MinusLogProbMetric: 27.1772 - val_loss: 28.2877 - val_MinusLogProbMetric: 28.2877 - lr: 2.5000e-04 - 107s/epoch - 544ms/step
Epoch 352/1000
2023-10-01 01:31:01.511 
Epoch 352/1000 
	 loss: 27.1705, MinusLogProbMetric: 27.1705, val_loss: 28.2721, val_MinusLogProbMetric: 28.2721

Epoch 352: val_loss did not improve from 28.24070
196/196 - 104s - loss: 27.1705 - MinusLogProbMetric: 27.1705 - val_loss: 28.2721 - val_MinusLogProbMetric: 28.2721 - lr: 2.5000e-04 - 104s/epoch - 531ms/step
Epoch 353/1000
2023-10-01 01:32:51.145 
Epoch 353/1000 
	 loss: 27.1673, MinusLogProbMetric: 27.1673, val_loss: 28.3693, val_MinusLogProbMetric: 28.3693

Epoch 353: val_loss did not improve from 28.24070
196/196 - 110s - loss: 27.1673 - MinusLogProbMetric: 27.1673 - val_loss: 28.3693 - val_MinusLogProbMetric: 28.3693 - lr: 2.5000e-04 - 110s/epoch - 559ms/step
Epoch 354/1000
2023-10-01 01:34:31.916 
Epoch 354/1000 
	 loss: 27.1030, MinusLogProbMetric: 27.1030, val_loss: 28.2473, val_MinusLogProbMetric: 28.2473

Epoch 354: val_loss did not improve from 28.24070
196/196 - 101s - loss: 27.1030 - MinusLogProbMetric: 27.1030 - val_loss: 28.2473 - val_MinusLogProbMetric: 28.2473 - lr: 1.2500e-04 - 101s/epoch - 514ms/step
Epoch 355/1000
2023-10-01 01:36:14.059 
Epoch 355/1000 
	 loss: 27.0991, MinusLogProbMetric: 27.0991, val_loss: 28.2233, val_MinusLogProbMetric: 28.2233

Epoch 355: val_loss improved from 28.24070 to 28.22332, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_348/weights/best_weights.h5
196/196 - 104s - loss: 27.0991 - MinusLogProbMetric: 27.0991 - val_loss: 28.2233 - val_MinusLogProbMetric: 28.2233 - lr: 1.2500e-04 - 104s/epoch - 532ms/step
Epoch 356/1000
2023-10-01 01:38:06.505 
Epoch 356/1000 
	 loss: 27.0995, MinusLogProbMetric: 27.0995, val_loss: 28.2475, val_MinusLogProbMetric: 28.2475

Epoch 356: val_loss did not improve from 28.22332
196/196 - 110s - loss: 27.0995 - MinusLogProbMetric: 27.0995 - val_loss: 28.2475 - val_MinusLogProbMetric: 28.2475 - lr: 1.2500e-04 - 110s/epoch - 562ms/step
Epoch 357/1000
2023-10-01 01:39:53.403 
Epoch 357/1000 
	 loss: 27.0968, MinusLogProbMetric: 27.0968, val_loss: 28.2825, val_MinusLogProbMetric: 28.2825

Epoch 357: val_loss did not improve from 28.22332
196/196 - 107s - loss: 27.0968 - MinusLogProbMetric: 27.0968 - val_loss: 28.2825 - val_MinusLogProbMetric: 28.2825 - lr: 1.2500e-04 - 107s/epoch - 546ms/step
Epoch 358/1000
2023-10-01 01:41:38.232 
Epoch 358/1000 
	 loss: 27.0988, MinusLogProbMetric: 27.0988, val_loss: 28.2523, val_MinusLogProbMetric: 28.2523

Epoch 358: val_loss did not improve from 28.22332
196/196 - 105s - loss: 27.0988 - MinusLogProbMetric: 27.0988 - val_loss: 28.2523 - val_MinusLogProbMetric: 28.2523 - lr: 1.2500e-04 - 105s/epoch - 535ms/step
Epoch 359/1000
2023-10-01 01:43:18.226 
Epoch 359/1000 
	 loss: 27.1024, MinusLogProbMetric: 27.1024, val_loss: 28.2423, val_MinusLogProbMetric: 28.2423

Epoch 359: val_loss did not improve from 28.22332
196/196 - 100s - loss: 27.1024 - MinusLogProbMetric: 27.1024 - val_loss: 28.2423 - val_MinusLogProbMetric: 28.2423 - lr: 1.2500e-04 - 100s/epoch - 510ms/step
Epoch 360/1000
2023-10-01 01:45:06.394 
Epoch 360/1000 
	 loss: 27.0962, MinusLogProbMetric: 27.0962, val_loss: 28.2420, val_MinusLogProbMetric: 28.2420

Epoch 360: val_loss did not improve from 28.22332
196/196 - 108s - loss: 27.0962 - MinusLogProbMetric: 27.0962 - val_loss: 28.2420 - val_MinusLogProbMetric: 28.2420 - lr: 1.2500e-04 - 108s/epoch - 552ms/step
Epoch 361/1000
2023-10-01 01:46:51.374 
Epoch 361/1000 
	 loss: 27.0912, MinusLogProbMetric: 27.0912, val_loss: 28.2428, val_MinusLogProbMetric: 28.2428

Epoch 361: val_loss did not improve from 28.22332
196/196 - 105s - loss: 27.0912 - MinusLogProbMetric: 27.0912 - val_loss: 28.2428 - val_MinusLogProbMetric: 28.2428 - lr: 1.2500e-04 - 105s/epoch - 536ms/step
Epoch 362/1000
2023-10-01 01:48:31.289 
Epoch 362/1000 
	 loss: 27.0945, MinusLogProbMetric: 27.0945, val_loss: 28.2796, val_MinusLogProbMetric: 28.2796

Epoch 362: val_loss did not improve from 28.22332
196/196 - 100s - loss: 27.0945 - MinusLogProbMetric: 27.0945 - val_loss: 28.2796 - val_MinusLogProbMetric: 28.2796 - lr: 1.2500e-04 - 100s/epoch - 510ms/step
Epoch 363/1000
2023-10-01 01:50:11.317 
Epoch 363/1000 
	 loss: 27.0928, MinusLogProbMetric: 27.0928, val_loss: 28.2484, val_MinusLogProbMetric: 28.2484

Epoch 363: val_loss did not improve from 28.22332
196/196 - 100s - loss: 27.0928 - MinusLogProbMetric: 27.0928 - val_loss: 28.2484 - val_MinusLogProbMetric: 28.2484 - lr: 1.2500e-04 - 100s/epoch - 510ms/step
Epoch 364/1000
2023-10-01 01:51:50.678 
Epoch 364/1000 
	 loss: 27.0935, MinusLogProbMetric: 27.0935, val_loss: 28.2481, val_MinusLogProbMetric: 28.2481

Epoch 364: val_loss did not improve from 28.22332
196/196 - 99s - loss: 27.0935 - MinusLogProbMetric: 27.0935 - val_loss: 28.2481 - val_MinusLogProbMetric: 28.2481 - lr: 1.2500e-04 - 99s/epoch - 507ms/step
Epoch 365/1000
2023-10-01 01:53:33.571 
Epoch 365/1000 
	 loss: 27.0911, MinusLogProbMetric: 27.0911, val_loss: 28.2275, val_MinusLogProbMetric: 28.2275

Epoch 365: val_loss did not improve from 28.22332
196/196 - 103s - loss: 27.0911 - MinusLogProbMetric: 27.0911 - val_loss: 28.2275 - val_MinusLogProbMetric: 28.2275 - lr: 1.2500e-04 - 103s/epoch - 525ms/step
Epoch 366/1000
2023-10-01 01:55:20.790 
Epoch 366/1000 
	 loss: 27.0932, MinusLogProbMetric: 27.0932, val_loss: 28.2529, val_MinusLogProbMetric: 28.2529

Epoch 366: val_loss did not improve from 28.22332
196/196 - 107s - loss: 27.0932 - MinusLogProbMetric: 27.0932 - val_loss: 28.2529 - val_MinusLogProbMetric: 28.2529 - lr: 1.2500e-04 - 107s/epoch - 547ms/step
Epoch 367/1000
2023-10-01 01:57:08.775 
Epoch 367/1000 
	 loss: 27.0914, MinusLogProbMetric: 27.0914, val_loss: 28.2424, val_MinusLogProbMetric: 28.2424

Epoch 367: val_loss did not improve from 28.22332
196/196 - 108s - loss: 27.0914 - MinusLogProbMetric: 27.0914 - val_loss: 28.2424 - val_MinusLogProbMetric: 28.2424 - lr: 1.2500e-04 - 108s/epoch - 551ms/step
Epoch 368/1000
2023-10-01 01:58:45.898 
Epoch 368/1000 
	 loss: 27.0873, MinusLogProbMetric: 27.0873, val_loss: 28.2530, val_MinusLogProbMetric: 28.2530

Epoch 368: val_loss did not improve from 28.22332
196/196 - 97s - loss: 27.0873 - MinusLogProbMetric: 27.0873 - val_loss: 28.2530 - val_MinusLogProbMetric: 28.2530 - lr: 1.2500e-04 - 97s/epoch - 496ms/step
Epoch 369/1000
2023-10-01 02:00:27.373 
Epoch 369/1000 
	 loss: 27.0842, MinusLogProbMetric: 27.0842, val_loss: 28.2453, val_MinusLogProbMetric: 28.2453

Epoch 369: val_loss did not improve from 28.22332
196/196 - 101s - loss: 27.0842 - MinusLogProbMetric: 27.0842 - val_loss: 28.2453 - val_MinusLogProbMetric: 28.2453 - lr: 1.2500e-04 - 101s/epoch - 518ms/step
Epoch 370/1000
2023-10-01 02:02:16.606 
Epoch 370/1000 
	 loss: 27.0870, MinusLogProbMetric: 27.0870, val_loss: 28.2448, val_MinusLogProbMetric: 28.2448

Epoch 370: val_loss did not improve from 28.22332
196/196 - 109s - loss: 27.0870 - MinusLogProbMetric: 27.0870 - val_loss: 28.2448 - val_MinusLogProbMetric: 28.2448 - lr: 1.2500e-04 - 109s/epoch - 557ms/step
Epoch 371/1000
2023-10-01 02:04:02.891 
Epoch 371/1000 
	 loss: 27.0857, MinusLogProbMetric: 27.0857, val_loss: 28.2489, val_MinusLogProbMetric: 28.2489

Epoch 371: val_loss did not improve from 28.22332
196/196 - 106s - loss: 27.0857 - MinusLogProbMetric: 27.0857 - val_loss: 28.2489 - val_MinusLogProbMetric: 28.2489 - lr: 1.2500e-04 - 106s/epoch - 542ms/step
Epoch 372/1000
2023-10-01 02:05:51.845 
Epoch 372/1000 
	 loss: 27.0911, MinusLogProbMetric: 27.0911, val_loss: 28.2592, val_MinusLogProbMetric: 28.2592

Epoch 372: val_loss did not improve from 28.22332
196/196 - 109s - loss: 27.0911 - MinusLogProbMetric: 27.0911 - val_loss: 28.2592 - val_MinusLogProbMetric: 28.2592 - lr: 1.2500e-04 - 109s/epoch - 556ms/step
Epoch 373/1000
2023-10-01 02:07:33.885 
Epoch 373/1000 
	 loss: 27.0864, MinusLogProbMetric: 27.0864, val_loss: 28.2583, val_MinusLogProbMetric: 28.2583

Epoch 373: val_loss did not improve from 28.22332
196/196 - 102s - loss: 27.0864 - MinusLogProbMetric: 27.0864 - val_loss: 28.2583 - val_MinusLogProbMetric: 28.2583 - lr: 1.2500e-04 - 102s/epoch - 521ms/step
Epoch 374/1000
2023-10-01 02:09:22.110 
Epoch 374/1000 
	 loss: 27.0942, MinusLogProbMetric: 27.0942, val_loss: 28.2599, val_MinusLogProbMetric: 28.2599

Epoch 374: val_loss did not improve from 28.22332
196/196 - 108s - loss: 27.0942 - MinusLogProbMetric: 27.0942 - val_loss: 28.2599 - val_MinusLogProbMetric: 28.2599 - lr: 1.2500e-04 - 108s/epoch - 552ms/step
Epoch 375/1000
2023-10-01 02:11:06.743 
Epoch 375/1000 
	 loss: 27.0876, MinusLogProbMetric: 27.0876, val_loss: 28.2483, val_MinusLogProbMetric: 28.2483

Epoch 375: val_loss did not improve from 28.22332
196/196 - 105s - loss: 27.0876 - MinusLogProbMetric: 27.0876 - val_loss: 28.2483 - val_MinusLogProbMetric: 28.2483 - lr: 1.2500e-04 - 105s/epoch - 534ms/step
Epoch 376/1000
2023-10-01 02:12:59.976 
Epoch 376/1000 
	 loss: 27.0848, MinusLogProbMetric: 27.0848, val_loss: 28.2716, val_MinusLogProbMetric: 28.2716

Epoch 376: val_loss did not improve from 28.22332
196/196 - 113s - loss: 27.0848 - MinusLogProbMetric: 27.0848 - val_loss: 28.2716 - val_MinusLogProbMetric: 28.2716 - lr: 1.2500e-04 - 113s/epoch - 578ms/step
Epoch 377/1000
2023-10-01 02:14:50.359 
Epoch 377/1000 
	 loss: 27.0884, MinusLogProbMetric: 27.0884, val_loss: 28.2650, val_MinusLogProbMetric: 28.2650

Epoch 377: val_loss did not improve from 28.22332
196/196 - 110s - loss: 27.0884 - MinusLogProbMetric: 27.0884 - val_loss: 28.2650 - val_MinusLogProbMetric: 28.2650 - lr: 1.2500e-04 - 110s/epoch - 563ms/step
Epoch 378/1000
2023-10-01 02:16:32.268 
Epoch 378/1000 
	 loss: 27.0865, MinusLogProbMetric: 27.0865, val_loss: 28.2465, val_MinusLogProbMetric: 28.2465

Epoch 378: val_loss did not improve from 28.22332
196/196 - 102s - loss: 27.0865 - MinusLogProbMetric: 27.0865 - val_loss: 28.2465 - val_MinusLogProbMetric: 28.2465 - lr: 1.2500e-04 - 102s/epoch - 520ms/step
Epoch 379/1000
2023-10-01 02:18:12.836 
Epoch 379/1000 
	 loss: 27.0848, MinusLogProbMetric: 27.0848, val_loss: 28.2284, val_MinusLogProbMetric: 28.2284

Epoch 379: val_loss did not improve from 28.22332
196/196 - 101s - loss: 27.0848 - MinusLogProbMetric: 27.0848 - val_loss: 28.2284 - val_MinusLogProbMetric: 28.2284 - lr: 1.2500e-04 - 101s/epoch - 513ms/step
Epoch 380/1000
2023-10-01 02:19:53.677 
Epoch 380/1000 
	 loss: 27.0804, MinusLogProbMetric: 27.0804, val_loss: 28.2485, val_MinusLogProbMetric: 28.2485

Epoch 380: val_loss did not improve from 28.22332
196/196 - 101s - loss: 27.0804 - MinusLogProbMetric: 27.0804 - val_loss: 28.2485 - val_MinusLogProbMetric: 28.2485 - lr: 1.2500e-04 - 101s/epoch - 515ms/step
Epoch 381/1000
2023-10-01 02:21:30.427 
Epoch 381/1000 
	 loss: 27.0842, MinusLogProbMetric: 27.0842, val_loss: 28.2587, val_MinusLogProbMetric: 28.2587

Epoch 381: val_loss did not improve from 28.22332
196/196 - 97s - loss: 27.0842 - MinusLogProbMetric: 27.0842 - val_loss: 28.2587 - val_MinusLogProbMetric: 28.2587 - lr: 1.2500e-04 - 97s/epoch - 494ms/step
Epoch 382/1000
2023-10-01 02:23:08.270 
Epoch 382/1000 
	 loss: 27.0855, MinusLogProbMetric: 27.0855, val_loss: 28.2401, val_MinusLogProbMetric: 28.2401

Epoch 382: val_loss did not improve from 28.22332
196/196 - 98s - loss: 27.0855 - MinusLogProbMetric: 27.0855 - val_loss: 28.2401 - val_MinusLogProbMetric: 28.2401 - lr: 1.2500e-04 - 98s/epoch - 499ms/step
Epoch 383/1000
2023-10-01 02:24:53.357 
Epoch 383/1000 
	 loss: 27.0785, MinusLogProbMetric: 27.0785, val_loss: 28.2425, val_MinusLogProbMetric: 28.2425

Epoch 383: val_loss did not improve from 28.22332
196/196 - 105s - loss: 27.0785 - MinusLogProbMetric: 27.0785 - val_loss: 28.2425 - val_MinusLogProbMetric: 28.2425 - lr: 1.2500e-04 - 105s/epoch - 536ms/step
Epoch 384/1000
2023-10-01 02:26:37.613 
Epoch 384/1000 
	 loss: 27.0803, MinusLogProbMetric: 27.0803, val_loss: 28.2687, val_MinusLogProbMetric: 28.2687

Epoch 384: val_loss did not improve from 28.22332
196/196 - 104s - loss: 27.0803 - MinusLogProbMetric: 27.0803 - val_loss: 28.2687 - val_MinusLogProbMetric: 28.2687 - lr: 1.2500e-04 - 104s/epoch - 532ms/step
Epoch 385/1000
2023-10-01 02:28:23.761 
Epoch 385/1000 
	 loss: 27.0829, MinusLogProbMetric: 27.0829, val_loss: 28.2846, val_MinusLogProbMetric: 28.2846

Epoch 385: val_loss did not improve from 28.22332
196/196 - 106s - loss: 27.0829 - MinusLogProbMetric: 27.0829 - val_loss: 28.2846 - val_MinusLogProbMetric: 28.2846 - lr: 1.2500e-04 - 106s/epoch - 541ms/step
Epoch 386/1000
2023-10-01 02:30:08.752 
Epoch 386/1000 
	 loss: 27.0796, MinusLogProbMetric: 27.0796, val_loss: 28.2359, val_MinusLogProbMetric: 28.2359

Epoch 386: val_loss did not improve from 28.22332
196/196 - 105s - loss: 27.0796 - MinusLogProbMetric: 27.0796 - val_loss: 28.2359 - val_MinusLogProbMetric: 28.2359 - lr: 1.2500e-04 - 105s/epoch - 536ms/step
Epoch 387/1000
2023-10-01 02:31:55.067 
Epoch 387/1000 
	 loss: 27.0851, MinusLogProbMetric: 27.0851, val_loss: 28.2557, val_MinusLogProbMetric: 28.2557

Epoch 387: val_loss did not improve from 28.22332
196/196 - 106s - loss: 27.0851 - MinusLogProbMetric: 27.0851 - val_loss: 28.2557 - val_MinusLogProbMetric: 28.2557 - lr: 1.2500e-04 - 106s/epoch - 543ms/step
Epoch 388/1000
2023-10-01 02:33:34.456 
Epoch 388/1000 
	 loss: 27.0800, MinusLogProbMetric: 27.0800, val_loss: 28.2430, val_MinusLogProbMetric: 28.2430

Epoch 388: val_loss did not improve from 28.22332
196/196 - 99s - loss: 27.0800 - MinusLogProbMetric: 27.0800 - val_loss: 28.2430 - val_MinusLogProbMetric: 28.2430 - lr: 1.2500e-04 - 99s/epoch - 507ms/step
Epoch 389/1000
2023-10-01 02:35:13.945 
Epoch 389/1000 
	 loss: 27.0749, MinusLogProbMetric: 27.0749, val_loss: 28.2547, val_MinusLogProbMetric: 28.2547

Epoch 389: val_loss did not improve from 28.22332
196/196 - 100s - loss: 27.0749 - MinusLogProbMetric: 27.0749 - val_loss: 28.2547 - val_MinusLogProbMetric: 28.2547 - lr: 1.2500e-04 - 100s/epoch - 508ms/step
Epoch 390/1000
2023-10-01 02:36:59.421 
Epoch 390/1000 
	 loss: 27.0771, MinusLogProbMetric: 27.0771, val_loss: 28.2618, val_MinusLogProbMetric: 28.2618

Epoch 390: val_loss did not improve from 28.22332
196/196 - 105s - loss: 27.0771 - MinusLogProbMetric: 27.0771 - val_loss: 28.2618 - val_MinusLogProbMetric: 28.2618 - lr: 1.2500e-04 - 105s/epoch - 538ms/step
Epoch 391/1000
2023-10-01 02:38:46.372 
Epoch 391/1000 
	 loss: 27.0819, MinusLogProbMetric: 27.0819, val_loss: 28.2421, val_MinusLogProbMetric: 28.2421

Epoch 391: val_loss did not improve from 28.22332
196/196 - 107s - loss: 27.0819 - MinusLogProbMetric: 27.0819 - val_loss: 28.2421 - val_MinusLogProbMetric: 28.2421 - lr: 1.2500e-04 - 107s/epoch - 546ms/step
Epoch 392/1000
2023-10-01 02:40:25.970 
Epoch 392/1000 
	 loss: 27.0778, MinusLogProbMetric: 27.0778, val_loss: 28.2443, val_MinusLogProbMetric: 28.2443

Epoch 392: val_loss did not improve from 28.22332
196/196 - 100s - loss: 27.0778 - MinusLogProbMetric: 27.0778 - val_loss: 28.2443 - val_MinusLogProbMetric: 28.2443 - lr: 1.2500e-04 - 100s/epoch - 508ms/step
Epoch 393/1000
2023-10-01 02:42:05.351 
Epoch 393/1000 
	 loss: 27.0821, MinusLogProbMetric: 27.0821, val_loss: 28.2589, val_MinusLogProbMetric: 28.2589

Epoch 393: val_loss did not improve from 28.22332
196/196 - 99s - loss: 27.0821 - MinusLogProbMetric: 27.0821 - val_loss: 28.2589 - val_MinusLogProbMetric: 28.2589 - lr: 1.2500e-04 - 99s/epoch - 507ms/step
Epoch 394/1000
2023-10-01 02:43:46.982 
Epoch 394/1000 
	 loss: 27.0797, MinusLogProbMetric: 27.0797, val_loss: 28.2425, val_MinusLogProbMetric: 28.2425

Epoch 394: val_loss did not improve from 28.22332
196/196 - 102s - loss: 27.0797 - MinusLogProbMetric: 27.0797 - val_loss: 28.2425 - val_MinusLogProbMetric: 28.2425 - lr: 1.2500e-04 - 102s/epoch - 519ms/step
Epoch 395/1000
2023-10-01 02:45:33.234 
Epoch 395/1000 
	 loss: 27.0785, MinusLogProbMetric: 27.0785, val_loss: 28.2436, val_MinusLogProbMetric: 28.2436

Epoch 395: val_loss did not improve from 28.22332
196/196 - 106s - loss: 27.0785 - MinusLogProbMetric: 27.0785 - val_loss: 28.2436 - val_MinusLogProbMetric: 28.2436 - lr: 1.2500e-04 - 106s/epoch - 542ms/step
Epoch 396/1000
2023-10-01 02:47:26.074 
Epoch 396/1000 
	 loss: 27.0738, MinusLogProbMetric: 27.0738, val_loss: 28.2519, val_MinusLogProbMetric: 28.2519

Epoch 396: val_loss did not improve from 28.22332
196/196 - 113s - loss: 27.0738 - MinusLogProbMetric: 27.0738 - val_loss: 28.2519 - val_MinusLogProbMetric: 28.2519 - lr: 1.2500e-04 - 113s/epoch - 576ms/step
Epoch 397/1000
2023-10-01 02:49:14.254 
Epoch 397/1000 
	 loss: 27.0742, MinusLogProbMetric: 27.0742, val_loss: 28.2576, val_MinusLogProbMetric: 28.2576

Epoch 397: val_loss did not improve from 28.22332
196/196 - 108s - loss: 27.0742 - MinusLogProbMetric: 27.0742 - val_loss: 28.2576 - val_MinusLogProbMetric: 28.2576 - lr: 1.2500e-04 - 108s/epoch - 552ms/step
Epoch 398/1000
2023-10-01 02:50:57.173 
Epoch 398/1000 
	 loss: 27.0764, MinusLogProbMetric: 27.0764, val_loss: 28.2479, val_MinusLogProbMetric: 28.2479

Epoch 398: val_loss did not improve from 28.22332
196/196 - 103s - loss: 27.0764 - MinusLogProbMetric: 27.0764 - val_loss: 28.2479 - val_MinusLogProbMetric: 28.2479 - lr: 1.2500e-04 - 103s/epoch - 525ms/step
Epoch 399/1000
2023-10-01 02:52:43.602 
Epoch 399/1000 
	 loss: 27.0770, MinusLogProbMetric: 27.0770, val_loss: 28.2498, val_MinusLogProbMetric: 28.2498

Epoch 399: val_loss did not improve from 28.22332
196/196 - 106s - loss: 27.0770 - MinusLogProbMetric: 27.0770 - val_loss: 28.2498 - val_MinusLogProbMetric: 28.2498 - lr: 1.2500e-04 - 106s/epoch - 543ms/step
Epoch 400/1000
2023-10-01 02:54:36.176 
Epoch 400/1000 
	 loss: 27.0684, MinusLogProbMetric: 27.0684, val_loss: 28.2570, val_MinusLogProbMetric: 28.2570

Epoch 400: val_loss did not improve from 28.22332
196/196 - 113s - loss: 27.0684 - MinusLogProbMetric: 27.0684 - val_loss: 28.2570 - val_MinusLogProbMetric: 28.2570 - lr: 1.2500e-04 - 113s/epoch - 574ms/step
Epoch 401/1000
2023-10-01 02:56:23.794 
Epoch 401/1000 
	 loss: 27.0778, MinusLogProbMetric: 27.0778, val_loss: 28.2545, val_MinusLogProbMetric: 28.2545

Epoch 401: val_loss did not improve from 28.22332
196/196 - 108s - loss: 27.0778 - MinusLogProbMetric: 27.0778 - val_loss: 28.2545 - val_MinusLogProbMetric: 28.2545 - lr: 1.2500e-04 - 108s/epoch - 549ms/step
Epoch 402/1000
2023-10-01 02:58:03.319 
Epoch 402/1000 
	 loss: 27.0721, MinusLogProbMetric: 27.0721, val_loss: 28.2517, val_MinusLogProbMetric: 28.2517

Epoch 402: val_loss did not improve from 28.22332
196/196 - 100s - loss: 27.0721 - MinusLogProbMetric: 27.0721 - val_loss: 28.2517 - val_MinusLogProbMetric: 28.2517 - lr: 1.2500e-04 - 100s/epoch - 508ms/step
Epoch 403/1000
2023-10-01 02:59:52.926 
Epoch 403/1000 
	 loss: 27.0693, MinusLogProbMetric: 27.0693, val_loss: 28.2515, val_MinusLogProbMetric: 28.2515

Epoch 403: val_loss did not improve from 28.22332
196/196 - 110s - loss: 27.0693 - MinusLogProbMetric: 27.0693 - val_loss: 28.2515 - val_MinusLogProbMetric: 28.2515 - lr: 1.2500e-04 - 110s/epoch - 559ms/step
Epoch 404/1000
2023-10-01 03:01:42.286 
Epoch 404/1000 
	 loss: 27.0747, MinusLogProbMetric: 27.0747, val_loss: 28.2581, val_MinusLogProbMetric: 28.2581

Epoch 404: val_loss did not improve from 28.22332
196/196 - 109s - loss: 27.0747 - MinusLogProbMetric: 27.0747 - val_loss: 28.2581 - val_MinusLogProbMetric: 28.2581 - lr: 1.2500e-04 - 109s/epoch - 558ms/step
Epoch 405/1000
2023-10-01 03:03:19.484 
Epoch 405/1000 
	 loss: 27.0687, MinusLogProbMetric: 27.0687, val_loss: 28.2705, val_MinusLogProbMetric: 28.2705

Epoch 405: val_loss did not improve from 28.22332
196/196 - 97s - loss: 27.0687 - MinusLogProbMetric: 27.0687 - val_loss: 28.2705 - val_MinusLogProbMetric: 28.2705 - lr: 1.2500e-04 - 97s/epoch - 496ms/step
Epoch 406/1000
2023-10-01 03:05:03.730 
Epoch 406/1000 
	 loss: 27.0364, MinusLogProbMetric: 27.0364, val_loss: 28.2319, val_MinusLogProbMetric: 28.2319

Epoch 406: val_loss did not improve from 28.22332
196/196 - 104s - loss: 27.0364 - MinusLogProbMetric: 27.0364 - val_loss: 28.2319 - val_MinusLogProbMetric: 28.2319 - lr: 6.2500e-05 - 104s/epoch - 532ms/step
Epoch 407/1000
2023-10-01 03:06:48.459 
Epoch 407/1000 
	 loss: 27.0349, MinusLogProbMetric: 27.0349, val_loss: 28.2367, val_MinusLogProbMetric: 28.2367

Epoch 407: val_loss did not improve from 28.22332
196/196 - 105s - loss: 27.0349 - MinusLogProbMetric: 27.0349 - val_loss: 28.2367 - val_MinusLogProbMetric: 28.2367 - lr: 6.2500e-05 - 105s/epoch - 534ms/step
Epoch 408/1000
2023-10-01 03:08:33.229 
Epoch 408/1000 
	 loss: 27.0379, MinusLogProbMetric: 27.0379, val_loss: 28.2292, val_MinusLogProbMetric: 28.2292

Epoch 408: val_loss did not improve from 28.22332
196/196 - 105s - loss: 27.0379 - MinusLogProbMetric: 27.0379 - val_loss: 28.2292 - val_MinusLogProbMetric: 28.2292 - lr: 6.2500e-05 - 105s/epoch - 535ms/step
Epoch 409/1000
2023-10-01 03:10:21.496 
Epoch 409/1000 
	 loss: 27.0343, MinusLogProbMetric: 27.0343, val_loss: 28.2379, val_MinusLogProbMetric: 28.2379

Epoch 409: val_loss did not improve from 28.22332
196/196 - 108s - loss: 27.0343 - MinusLogProbMetric: 27.0343 - val_loss: 28.2379 - val_MinusLogProbMetric: 28.2379 - lr: 6.2500e-05 - 108s/epoch - 552ms/step
Epoch 410/1000
2023-10-01 03:12:11.683 
Epoch 410/1000 
	 loss: 27.0347, MinusLogProbMetric: 27.0347, val_loss: 28.2370, val_MinusLogProbMetric: 28.2370

Epoch 410: val_loss did not improve from 28.22332
196/196 - 110s - loss: 27.0347 - MinusLogProbMetric: 27.0347 - val_loss: 28.2370 - val_MinusLogProbMetric: 28.2370 - lr: 6.2500e-05 - 110s/epoch - 562ms/step
Epoch 411/1000
2023-10-01 03:13:45.691 
Epoch 411/1000 
	 loss: 27.0348, MinusLogProbMetric: 27.0348, val_loss: 28.2402, val_MinusLogProbMetric: 28.2402

Epoch 411: val_loss did not improve from 28.22332
196/196 - 94s - loss: 27.0348 - MinusLogProbMetric: 27.0348 - val_loss: 28.2402 - val_MinusLogProbMetric: 28.2402 - lr: 6.2500e-05 - 94s/epoch - 480ms/step
Epoch 412/1000
2023-10-01 03:15:16.068 
Epoch 412/1000 
	 loss: 27.0352, MinusLogProbMetric: 27.0352, val_loss: 28.2451, val_MinusLogProbMetric: 28.2451

Epoch 412: val_loss did not improve from 28.22332
196/196 - 90s - loss: 27.0352 - MinusLogProbMetric: 27.0352 - val_loss: 28.2451 - val_MinusLogProbMetric: 28.2451 - lr: 6.2500e-05 - 90s/epoch - 461ms/step
Epoch 413/1000
2023-10-01 03:16:46.808 
Epoch 413/1000 
	 loss: 27.0324, MinusLogProbMetric: 27.0324, val_loss: 28.2364, val_MinusLogProbMetric: 28.2364

Epoch 413: val_loss did not improve from 28.22332
196/196 - 91s - loss: 27.0324 - MinusLogProbMetric: 27.0324 - val_loss: 28.2364 - val_MinusLogProbMetric: 28.2364 - lr: 6.2500e-05 - 91s/epoch - 463ms/step
Epoch 414/1000
2023-10-01 03:18:16.639 
Epoch 414/1000 
	 loss: 27.0321, MinusLogProbMetric: 27.0321, val_loss: 28.2505, val_MinusLogProbMetric: 28.2505

Epoch 414: val_loss did not improve from 28.22332
196/196 - 90s - loss: 27.0321 - MinusLogProbMetric: 27.0321 - val_loss: 28.2505 - val_MinusLogProbMetric: 28.2505 - lr: 6.2500e-05 - 90s/epoch - 458ms/step
Epoch 415/1000
2023-10-01 03:19:41.106 
Epoch 415/1000 
	 loss: 27.0333, MinusLogProbMetric: 27.0333, val_loss: 28.2383, val_MinusLogProbMetric: 28.2383

Epoch 415: val_loss did not improve from 28.22332
196/196 - 84s - loss: 27.0333 - MinusLogProbMetric: 27.0333 - val_loss: 28.2383 - val_MinusLogProbMetric: 28.2383 - lr: 6.2500e-05 - 84s/epoch - 431ms/step
Epoch 416/1000
2023-10-01 03:21:09.641 
Epoch 416/1000 
	 loss: 27.0344, MinusLogProbMetric: 27.0344, val_loss: 28.2413, val_MinusLogProbMetric: 28.2413

Epoch 416: val_loss did not improve from 28.22332
196/196 - 89s - loss: 27.0344 - MinusLogProbMetric: 27.0344 - val_loss: 28.2413 - val_MinusLogProbMetric: 28.2413 - lr: 6.2500e-05 - 89s/epoch - 452ms/step
Epoch 417/1000
2023-10-01 03:22:41.454 
Epoch 417/1000 
	 loss: 27.0337, MinusLogProbMetric: 27.0337, val_loss: 28.2563, val_MinusLogProbMetric: 28.2563

Epoch 417: val_loss did not improve from 28.22332
196/196 - 92s - loss: 27.0337 - MinusLogProbMetric: 27.0337 - val_loss: 28.2563 - val_MinusLogProbMetric: 28.2563 - lr: 6.2500e-05 - 92s/epoch - 468ms/step
Epoch 418/1000
2023-10-01 03:24:14.874 
Epoch 418/1000 
	 loss: 27.0334, MinusLogProbMetric: 27.0334, val_loss: 28.2400, val_MinusLogProbMetric: 28.2400

Epoch 418: val_loss did not improve from 28.22332
196/196 - 93s - loss: 27.0334 - MinusLogProbMetric: 27.0334 - val_loss: 28.2400 - val_MinusLogProbMetric: 28.2400 - lr: 6.2500e-05 - 93s/epoch - 477ms/step
Epoch 419/1000
2023-10-01 03:25:46.733 
Epoch 419/1000 
	 loss: 27.0338, MinusLogProbMetric: 27.0338, val_loss: 28.2363, val_MinusLogProbMetric: 28.2363

Epoch 419: val_loss did not improve from 28.22332
196/196 - 92s - loss: 27.0338 - MinusLogProbMetric: 27.0338 - val_loss: 28.2363 - val_MinusLogProbMetric: 28.2363 - lr: 6.2500e-05 - 92s/epoch - 468ms/step
Epoch 420/1000
2023-10-01 03:27:17.703 
Epoch 420/1000 
	 loss: 27.0293, MinusLogProbMetric: 27.0293, val_loss: 28.2458, val_MinusLogProbMetric: 28.2458

Epoch 420: val_loss did not improve from 28.22332
196/196 - 91s - loss: 27.0293 - MinusLogProbMetric: 27.0293 - val_loss: 28.2458 - val_MinusLogProbMetric: 28.2458 - lr: 6.2500e-05 - 91s/epoch - 464ms/step
Epoch 421/1000
2023-10-01 03:28:47.433 
Epoch 421/1000 
	 loss: 27.0329, MinusLogProbMetric: 27.0329, val_loss: 28.2499, val_MinusLogProbMetric: 28.2499

Epoch 421: val_loss did not improve from 28.22332
196/196 - 90s - loss: 27.0329 - MinusLogProbMetric: 27.0329 - val_loss: 28.2499 - val_MinusLogProbMetric: 28.2499 - lr: 6.2500e-05 - 90s/epoch - 458ms/step
Epoch 422/1000
2023-10-01 03:30:17.472 
Epoch 422/1000 
	 loss: 27.0333, MinusLogProbMetric: 27.0333, val_loss: 28.2433, val_MinusLogProbMetric: 28.2433

Epoch 422: val_loss did not improve from 28.22332
196/196 - 90s - loss: 27.0333 - MinusLogProbMetric: 27.0333 - val_loss: 28.2433 - val_MinusLogProbMetric: 28.2433 - lr: 6.2500e-05 - 90s/epoch - 459ms/step
Epoch 423/1000
2023-10-01 03:31:57.726 
Epoch 423/1000 
	 loss: 27.0318, MinusLogProbMetric: 27.0318, val_loss: 28.2396, val_MinusLogProbMetric: 28.2396

Epoch 423: val_loss did not improve from 28.22332
196/196 - 100s - loss: 27.0318 - MinusLogProbMetric: 27.0318 - val_loss: 28.2396 - val_MinusLogProbMetric: 28.2396 - lr: 6.2500e-05 - 100s/epoch - 511ms/step
Epoch 424/1000
2023-10-01 03:33:29.833 
Epoch 424/1000 
	 loss: 27.0300, MinusLogProbMetric: 27.0300, val_loss: 28.2661, val_MinusLogProbMetric: 28.2661

Epoch 424: val_loss did not improve from 28.22332
196/196 - 92s - loss: 27.0300 - MinusLogProbMetric: 27.0300 - val_loss: 28.2661 - val_MinusLogProbMetric: 28.2661 - lr: 6.2500e-05 - 92s/epoch - 470ms/step
Epoch 425/1000
2023-10-01 03:35:11.571 
Epoch 425/1000 
	 loss: 27.0310, MinusLogProbMetric: 27.0310, val_loss: 28.2403, val_MinusLogProbMetric: 28.2403

Epoch 425: val_loss did not improve from 28.22332
196/196 - 102s - loss: 27.0310 - MinusLogProbMetric: 27.0310 - val_loss: 28.2403 - val_MinusLogProbMetric: 28.2403 - lr: 6.2500e-05 - 102s/epoch - 519ms/step
Epoch 426/1000
2023-10-01 03:36:43.031 
Epoch 426/1000 
	 loss: 27.0278, MinusLogProbMetric: 27.0278, val_loss: 28.2347, val_MinusLogProbMetric: 28.2347

Epoch 426: val_loss did not improve from 28.22332
196/196 - 91s - loss: 27.0278 - MinusLogProbMetric: 27.0278 - val_loss: 28.2347 - val_MinusLogProbMetric: 28.2347 - lr: 6.2500e-05 - 91s/epoch - 467ms/step
Epoch 427/1000
2023-10-01 03:38:12.979 
Epoch 427/1000 
	 loss: 27.0304, MinusLogProbMetric: 27.0304, val_loss: 28.2484, val_MinusLogProbMetric: 28.2484

Epoch 427: val_loss did not improve from 28.22332
196/196 - 90s - loss: 27.0304 - MinusLogProbMetric: 27.0304 - val_loss: 28.2484 - val_MinusLogProbMetric: 28.2484 - lr: 6.2500e-05 - 90s/epoch - 459ms/step
Epoch 428/1000
2023-10-01 03:39:41.393 
Epoch 428/1000 
	 loss: 27.0311, MinusLogProbMetric: 27.0311, val_loss: 28.2379, val_MinusLogProbMetric: 28.2379

Epoch 428: val_loss did not improve from 28.22332
196/196 - 88s - loss: 27.0311 - MinusLogProbMetric: 27.0311 - val_loss: 28.2379 - val_MinusLogProbMetric: 28.2379 - lr: 6.2500e-05 - 88s/epoch - 451ms/step
Epoch 429/1000
2023-10-01 03:41:08.735 
Epoch 429/1000 
	 loss: 27.0288, MinusLogProbMetric: 27.0288, val_loss: 28.2444, val_MinusLogProbMetric: 28.2444

Epoch 429: val_loss did not improve from 28.22332
196/196 - 87s - loss: 27.0288 - MinusLogProbMetric: 27.0288 - val_loss: 28.2444 - val_MinusLogProbMetric: 28.2444 - lr: 6.2500e-05 - 87s/epoch - 446ms/step
Epoch 430/1000
2023-10-01 03:42:41.383 
Epoch 430/1000 
	 loss: 27.0300, MinusLogProbMetric: 27.0300, val_loss: 28.2421, val_MinusLogProbMetric: 28.2421

Epoch 430: val_loss did not improve from 28.22332
196/196 - 93s - loss: 27.0300 - MinusLogProbMetric: 27.0300 - val_loss: 28.2421 - val_MinusLogProbMetric: 28.2421 - lr: 6.2500e-05 - 93s/epoch - 472ms/step
Epoch 431/1000
2023-10-01 03:44:20.287 
Epoch 431/1000 
	 loss: 27.0270, MinusLogProbMetric: 27.0270, val_loss: 28.2559, val_MinusLogProbMetric: 28.2559

Epoch 431: val_loss did not improve from 28.22332
196/196 - 99s - loss: 27.0270 - MinusLogProbMetric: 27.0270 - val_loss: 28.2559 - val_MinusLogProbMetric: 28.2559 - lr: 6.2500e-05 - 99s/epoch - 505ms/step
Epoch 432/1000
2023-10-01 03:45:51.714 
Epoch 432/1000 
	 loss: 27.0276, MinusLogProbMetric: 27.0276, val_loss: 28.2417, val_MinusLogProbMetric: 28.2417

Epoch 432: val_loss did not improve from 28.22332
196/196 - 91s - loss: 27.0276 - MinusLogProbMetric: 27.0276 - val_loss: 28.2417 - val_MinusLogProbMetric: 28.2417 - lr: 6.2500e-05 - 91s/epoch - 466ms/step
Epoch 433/1000
2023-10-01 03:47:31.686 
Epoch 433/1000 
	 loss: 27.0294, MinusLogProbMetric: 27.0294, val_loss: 28.2447, val_MinusLogProbMetric: 28.2447

Epoch 433: val_loss did not improve from 28.22332
196/196 - 100s - loss: 27.0294 - MinusLogProbMetric: 27.0294 - val_loss: 28.2447 - val_MinusLogProbMetric: 28.2447 - lr: 6.2500e-05 - 100s/epoch - 510ms/step
Epoch 434/1000
2023-10-01 03:49:04.133 
Epoch 434/1000 
	 loss: 27.0274, MinusLogProbMetric: 27.0274, val_loss: 28.2425, val_MinusLogProbMetric: 28.2425

Epoch 434: val_loss did not improve from 28.22332
196/196 - 92s - loss: 27.0274 - MinusLogProbMetric: 27.0274 - val_loss: 28.2425 - val_MinusLogProbMetric: 28.2425 - lr: 6.2500e-05 - 92s/epoch - 472ms/step
Epoch 435/1000
2023-10-01 03:50:32.550 
Epoch 435/1000 
	 loss: 27.0284, MinusLogProbMetric: 27.0284, val_loss: 28.2465, val_MinusLogProbMetric: 28.2465

Epoch 435: val_loss did not improve from 28.22332
196/196 - 88s - loss: 27.0284 - MinusLogProbMetric: 27.0284 - val_loss: 28.2465 - val_MinusLogProbMetric: 28.2465 - lr: 6.2500e-05 - 88s/epoch - 451ms/step
Epoch 436/1000
2023-10-01 03:52:03.675 
Epoch 436/1000 
	 loss: 27.0292, MinusLogProbMetric: 27.0292, val_loss: 28.2481, val_MinusLogProbMetric: 28.2481

Epoch 436: val_loss did not improve from 28.22332
196/196 - 91s - loss: 27.0292 - MinusLogProbMetric: 27.0292 - val_loss: 28.2481 - val_MinusLogProbMetric: 28.2481 - lr: 6.2500e-05 - 91s/epoch - 465ms/step
Epoch 437/1000
2023-10-01 03:53:34.217 
Epoch 437/1000 
	 loss: 27.0264, MinusLogProbMetric: 27.0264, val_loss: 28.2421, val_MinusLogProbMetric: 28.2421

Epoch 437: val_loss did not improve from 28.22332
196/196 - 91s - loss: 27.0264 - MinusLogProbMetric: 27.0264 - val_loss: 28.2421 - val_MinusLogProbMetric: 28.2421 - lr: 6.2500e-05 - 91s/epoch - 462ms/step
Epoch 438/1000
2023-10-01 03:55:08.248 
Epoch 438/1000 
	 loss: 27.0302, MinusLogProbMetric: 27.0302, val_loss: 28.2484, val_MinusLogProbMetric: 28.2484

Epoch 438: val_loss did not improve from 28.22332
196/196 - 94s - loss: 27.0302 - MinusLogProbMetric: 27.0302 - val_loss: 28.2484 - val_MinusLogProbMetric: 28.2484 - lr: 6.2500e-05 - 94s/epoch - 480ms/step
Epoch 439/1000
2023-10-01 03:56:38.125 
Epoch 439/1000 
	 loss: 27.0281, MinusLogProbMetric: 27.0281, val_loss: 28.2445, val_MinusLogProbMetric: 28.2445

Epoch 439: val_loss did not improve from 28.22332
196/196 - 90s - loss: 27.0281 - MinusLogProbMetric: 27.0281 - val_loss: 28.2445 - val_MinusLogProbMetric: 28.2445 - lr: 6.2500e-05 - 90s/epoch - 459ms/step
Epoch 440/1000
2023-10-01 03:58:02.616 
Epoch 440/1000 
	 loss: 27.0261, MinusLogProbMetric: 27.0261, val_loss: 28.2486, val_MinusLogProbMetric: 28.2486

Epoch 440: val_loss did not improve from 28.22332
196/196 - 84s - loss: 27.0261 - MinusLogProbMetric: 27.0261 - val_loss: 28.2486 - val_MinusLogProbMetric: 28.2486 - lr: 6.2500e-05 - 84s/epoch - 431ms/step
Epoch 441/1000
2023-10-01 03:59:34.907 
Epoch 441/1000 
	 loss: 27.0283, MinusLogProbMetric: 27.0283, val_loss: 28.2514, val_MinusLogProbMetric: 28.2514

Epoch 441: val_loss did not improve from 28.22332
196/196 - 92s - loss: 27.0283 - MinusLogProbMetric: 27.0283 - val_loss: 28.2514 - val_MinusLogProbMetric: 28.2514 - lr: 6.2500e-05 - 92s/epoch - 471ms/step
Epoch 442/1000
2023-10-01 04:01:12.940 
Epoch 442/1000 
	 loss: 27.0269, MinusLogProbMetric: 27.0269, val_loss: 28.2467, val_MinusLogProbMetric: 28.2467

Epoch 442: val_loss did not improve from 28.22332
196/196 - 98s - loss: 27.0269 - MinusLogProbMetric: 27.0269 - val_loss: 28.2467 - val_MinusLogProbMetric: 28.2467 - lr: 6.2500e-05 - 98s/epoch - 500ms/step
Epoch 443/1000
2023-10-01 04:02:42.113 
Epoch 443/1000 
	 loss: 27.0241, MinusLogProbMetric: 27.0241, val_loss: 28.2421, val_MinusLogProbMetric: 28.2421

Epoch 443: val_loss did not improve from 28.22332
196/196 - 89s - loss: 27.0241 - MinusLogProbMetric: 27.0241 - val_loss: 28.2421 - val_MinusLogProbMetric: 28.2421 - lr: 6.2500e-05 - 89s/epoch - 455ms/step
Epoch 444/1000
2023-10-01 04:04:07.447 
Epoch 444/1000 
	 loss: 27.0248, MinusLogProbMetric: 27.0248, val_loss: 28.2365, val_MinusLogProbMetric: 28.2365

Epoch 444: val_loss did not improve from 28.22332
196/196 - 85s - loss: 27.0248 - MinusLogProbMetric: 27.0248 - val_loss: 28.2365 - val_MinusLogProbMetric: 28.2365 - lr: 6.2500e-05 - 85s/epoch - 435ms/step
Epoch 445/1000
2023-10-01 04:05:36.590 
Epoch 445/1000 
	 loss: 27.0259, MinusLogProbMetric: 27.0259, val_loss: 28.2442, val_MinusLogProbMetric: 28.2442

Epoch 445: val_loss did not improve from 28.22332
196/196 - 89s - loss: 27.0259 - MinusLogProbMetric: 27.0259 - val_loss: 28.2442 - val_MinusLogProbMetric: 28.2442 - lr: 6.2500e-05 - 89s/epoch - 455ms/step
Epoch 446/1000
2023-10-01 04:07:03.966 
Epoch 446/1000 
	 loss: 27.0272, MinusLogProbMetric: 27.0272, val_loss: 28.2429, val_MinusLogProbMetric: 28.2429

Epoch 446: val_loss did not improve from 28.22332
196/196 - 87s - loss: 27.0272 - MinusLogProbMetric: 27.0272 - val_loss: 28.2429 - val_MinusLogProbMetric: 28.2429 - lr: 6.2500e-05 - 87s/epoch - 445ms/step
Epoch 447/1000
2023-10-01 04:08:36.127 
Epoch 447/1000 
	 loss: 27.0242, MinusLogProbMetric: 27.0242, val_loss: 28.2701, val_MinusLogProbMetric: 28.2701

Epoch 447: val_loss did not improve from 28.22332
196/196 - 92s - loss: 27.0242 - MinusLogProbMetric: 27.0242 - val_loss: 28.2701 - val_MinusLogProbMetric: 28.2701 - lr: 6.2500e-05 - 92s/epoch - 470ms/step
Epoch 448/1000
2023-10-01 04:10:17.047 
Epoch 448/1000 
	 loss: 27.0250, MinusLogProbMetric: 27.0250, val_loss: 28.2401, val_MinusLogProbMetric: 28.2401

Epoch 448: val_loss did not improve from 28.22332
196/196 - 101s - loss: 27.0250 - MinusLogProbMetric: 27.0250 - val_loss: 28.2401 - val_MinusLogProbMetric: 28.2401 - lr: 6.2500e-05 - 101s/epoch - 515ms/step
Epoch 449/1000
2023-10-01 04:11:50.422 
Epoch 449/1000 
	 loss: 27.0254, MinusLogProbMetric: 27.0254, val_loss: 28.2553, val_MinusLogProbMetric: 28.2553

Epoch 449: val_loss did not improve from 28.22332
196/196 - 93s - loss: 27.0254 - MinusLogProbMetric: 27.0254 - val_loss: 28.2553 - val_MinusLogProbMetric: 28.2553 - lr: 6.2500e-05 - 93s/epoch - 476ms/step
Epoch 450/1000
2023-10-01 04:13:19.632 
Epoch 450/1000 
	 loss: 27.0240, MinusLogProbMetric: 27.0240, val_loss: 28.2440, val_MinusLogProbMetric: 28.2440

Epoch 450: val_loss did not improve from 28.22332
196/196 - 89s - loss: 27.0240 - MinusLogProbMetric: 27.0240 - val_loss: 28.2440 - val_MinusLogProbMetric: 28.2440 - lr: 6.2500e-05 - 89s/epoch - 455ms/step
Epoch 451/1000
2023-10-01 04:14:53.134 
Epoch 451/1000 
	 loss: 27.0246, MinusLogProbMetric: 27.0246, val_loss: 28.2562, val_MinusLogProbMetric: 28.2562

Epoch 451: val_loss did not improve from 28.22332
196/196 - 94s - loss: 27.0246 - MinusLogProbMetric: 27.0246 - val_loss: 28.2562 - val_MinusLogProbMetric: 28.2562 - lr: 6.2500e-05 - 94s/epoch - 477ms/step
Epoch 452/1000
2023-10-01 04:16:24.171 
Epoch 452/1000 
	 loss: 27.0245, MinusLogProbMetric: 27.0245, val_loss: 28.2501, val_MinusLogProbMetric: 28.2501

Epoch 452: val_loss did not improve from 28.22332
196/196 - 91s - loss: 27.0245 - MinusLogProbMetric: 27.0245 - val_loss: 28.2501 - val_MinusLogProbMetric: 28.2501 - lr: 6.2500e-05 - 91s/epoch - 464ms/step
Epoch 453/1000
2023-10-01 04:17:57.257 
Epoch 453/1000 
	 loss: 27.0235, MinusLogProbMetric: 27.0235, val_loss: 28.2564, val_MinusLogProbMetric: 28.2564

Epoch 453: val_loss did not improve from 28.22332
196/196 - 93s - loss: 27.0235 - MinusLogProbMetric: 27.0235 - val_loss: 28.2564 - val_MinusLogProbMetric: 28.2564 - lr: 6.2500e-05 - 93s/epoch - 475ms/step
Epoch 454/1000
2023-10-01 04:19:31.053 
Epoch 454/1000 
	 loss: 27.0231, MinusLogProbMetric: 27.0231, val_loss: 28.2531, val_MinusLogProbMetric: 28.2531

Epoch 454: val_loss did not improve from 28.22332
196/196 - 94s - loss: 27.0231 - MinusLogProbMetric: 27.0231 - val_loss: 28.2531 - val_MinusLogProbMetric: 28.2531 - lr: 6.2500e-05 - 94s/epoch - 479ms/step
Epoch 455/1000
2023-10-01 04:21:03.230 
Epoch 455/1000 
	 loss: 27.0251, MinusLogProbMetric: 27.0251, val_loss: 28.2471, val_MinusLogProbMetric: 28.2471

Epoch 455: val_loss did not improve from 28.22332
Restoring model weights from the end of the best epoch: 355.
196/196 - 92s - loss: 27.0251 - MinusLogProbMetric: 27.0251 - val_loss: 28.2471 - val_MinusLogProbMetric: 28.2471 - lr: 6.2500e-05 - 92s/epoch - 472ms/step
Epoch 455: early stopping
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.

------------------------------------------
Starting LR metric calculation...
Running TF LR calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
LR metric calculation completed in 54.885692159994505 seconds.

------------------------------------------
Starting KS tests calculation...
Running TF KS tests...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
KS tests calculation completed in 33.60094646504149 seconds.

------------------------------------------
Starting SWD metric calculation...
Running TF SWD calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
SWD metric calculation completed in 22.36899688001722 seconds.

------------------------------------------
Starting FN metric calculation...
Running TF FN calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
FN metric calculation completed in 28.7605821529869 seconds.
Training succeeded with seed 440.
Model trained in 42426.83 s.

===========
Computing predictions
===========

Computing metrics...
Metrics computed in 148.41 s.
                ===========
                print("===========
Failed to plot
===========
")
                Exception type: ValueError
                Exception message: Number of rows must be a positive integer, not 0
                Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN.py , Line : 469, Func.Name : prediction_function, Message : Plotters.cornerplotter(X_data_test.numpy(),X_data_nf.numpy(),path_to_results,ndims,norm=True) # type: ignore', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/Plotters.py , Line : 156, Func.Name : cornerplotter, Message : figure=corner.corner(target_samples,color=\'red\',bins=n_bins,labels=[r"%s" % s for s in labels],normalize1d=True)', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/corner.py , Line : 235, Func.Name : corner, Message : fig, axes = pl.subplots(K, K, figsize=(dim, dim))', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/pyplot.py , Line : 1502, Func.Name : subplots, Message : axs = fig.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 905, Func.Name : subplots, Message : gs = self.add_gridspec(nrows, ncols, figure=self, **gridspec_kw)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 1527, Func.Name : add_gridspec, Message : gs = GridSpec(nrows=nrows, ncols=ncols, figure=self, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 379, Func.Name : __init__, Message : super().__init__(nrows, ncols,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 49, Func.Name : __init__, Message : raise ValueError(']
                ===========

results.txt saved
results.json saved
Results log saved
Model predictions computed in 149.92 s.
===========
Run 348/720 done in 42591.01 s.
===========

Directory ../../results/CsplineN_new/run_349/ already exists.
Skipping it.
===========
Run 349/720 already exists. Skipping it.
===========

===========
Generating train data for run 350.
===========
Train data generated in 1.24 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_350/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_350/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_350/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_350
self.data_kwargs: {'seed': 440}
self.x_data: [[ 2.0547452   3.776408    6.9601746  ...  5.8637457  -0.4179731
   2.747595  ]
 [ 6.405085    2.999549    6.1033454  ...  2.8107233   3.0569289
   2.003619  ]
 [ 2.0047038   4.064143    6.2048492  ...  5.6568336   0.71395487
   2.6926432 ]
 ...
 [ 5.645455    7.0971727   6.202566   ...  0.61970913  6.238492
   1.3539525 ]
 [ 6.475603    3.1151166   6.2248073  ...  2.8990176   3.6724448
   2.289971  ]
 [ 2.2290072   4.101192    7.1413527  ...  6.717821   -2.0046902
   2.3999293 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_331"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_332 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_31 (LogProbL  (None,)                  3291840   
 ayer)                                                           
                                                                 
=================================================================
Total params: 3,291,840
Trainable params: 3,291,840
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_31/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_31'")
self.model: <keras.engine.functional.Functional object at 0x7fb0fd05bca0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fb11d3b3190>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fb11d3b3190>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fb0fc980ee0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fb86ca0ace0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fb86ca0b250>, <keras.callbacks.ModelCheckpoint object at 0x7fb86ca0b310>, <keras.callbacks.EarlyStopping object at 0x7fb86ca0b580>, <keras.callbacks.ReduceLROnPlateau object at 0x7fb86ca0b5b0>, <keras.callbacks.TerminateOnNaN object at 0x7fb86ca0b1f0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_350/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 350/720 with hyperparameters:
timestamp = 2023-10-01 04:24:04.181522
ndims = 64
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 3291840
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 2.0547452   3.776408    6.9601746   1.2163222   8.644804    1.0450234
  9.756204    5.9333344   9.3759165   6.2056174   7.6663713   0.8787037
  2.7781603   1.7675867   3.9287794   0.8286338   3.6572614   4.846633
  0.68292063  6.317236    6.48625     3.843101    5.88724     2.0270782
  5.1297107   7.7579265   2.6054757   6.6726594   1.0543873   7.123391
  3.5526195   2.046467    5.620422   -0.869851    8.648443    0.02500496
  7.4141846   2.6777635   7.6790566   9.76465     2.481489    6.2672224
  5.7068043   4.504637    1.1161722   9.691578    4.0927534   8.503739
  6.9964066   3.1729455   8.086372    4.0078015   8.096493    5.813139
  8.181514    6.97417     7.246071    5.0315676  10.666024    6.2678714
  3.6855178   5.8637457  -0.4179731   2.747595  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 6: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-01 04:35:38.874 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6647.8794, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 694s - loss: nan - MinusLogProbMetric: 6647.8794 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 694s/epoch - 4s/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 0.0003333333333333333.
===========
Generating train data for run 350.
===========
Train data generated in 0.22 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_350/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_350/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_350/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_350
self.data_kwargs: {'seed': 440}
self.x_data: [[ 2.0547452   3.776408    6.9601746  ...  5.8637457  -0.4179731
   2.747595  ]
 [ 6.405085    2.999549    6.1033454  ...  2.8107233   3.0569289
   2.003619  ]
 [ 2.0047038   4.064143    6.2048492  ...  5.6568336   0.71395487
   2.6926432 ]
 ...
 [ 5.645455    7.0971727   6.202566   ...  0.61970913  6.238492
   1.3539525 ]
 [ 6.475603    3.1151166   6.2248073  ...  2.8990176   3.6724448
   2.289971  ]
 [ 2.2290072   4.101192    7.1413527  ...  6.717821   -2.0046902
   2.3999293 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_342"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_343 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_32 (LogProbL  (None,)                  3291840   
 ayer)                                                           
                                                                 
=================================================================
Total params: 3,291,840
Trainable params: 3,291,840
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_32/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_32'")
self.model: <keras.engine.functional.Functional object at 0x7fafe060bfd0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fb0306ec490>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fb0306ec490>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fb054fd5000>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fb054bdc940>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fb054bdceb0>, <keras.callbacks.ModelCheckpoint object at 0x7fb054bdcf70>, <keras.callbacks.EarlyStopping object at 0x7fb054bdd1e0>, <keras.callbacks.ReduceLROnPlateau object at 0x7fb054bdd210>, <keras.callbacks.TerminateOnNaN object at 0x7fb054bdce50>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_350/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 350/720 with hyperparameters:
timestamp = 2023-10-01 04:36:08.829648
ndims = 64
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 3291840
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 2.0547452   3.776408    6.9601746   1.2163222   8.644804    1.0450234
  9.756204    5.9333344   9.3759165   6.2056174   7.6663713   0.8787037
  2.7781603   1.7675867   3.9287794   0.8286338   3.6572614   4.846633
  0.68292063  6.317236    6.48625     3.843101    5.88724     2.0270782
  5.1297107   7.7579265   2.6054757   6.6726594   1.0543873   7.123391
  3.5526195   2.046467    5.620422   -0.869851    8.648443    0.02500496
  7.4141846   2.6777635   7.6790566   9.76465     2.481489    6.2672224
  5.7068043   4.504637    1.1161722   9.691578    4.0927534   8.503739
  6.9964066   3.1729455   8.086372    4.0078015   8.096493    5.813139
  8.181514    6.97417     7.246071    5.0315676  10.666024    6.2678714
  3.6855178   5.8637457  -0.4179731   2.747595  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 2: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-01 04:46:44.620 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6804.6235, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 636s - loss: nan - MinusLogProbMetric: 6804.6235 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 636s/epoch - 3s/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 0.0001111111111111111.
===========
Generating train data for run 350.
===========
Train data generated in 0.14 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_350/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_350/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_350/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_350
self.data_kwargs: {'seed': 440}
self.x_data: [[ 2.0547452   3.776408    6.9601746  ...  5.8637457  -0.4179731
   2.747595  ]
 [ 6.405085    2.999549    6.1033454  ...  2.8107233   3.0569289
   2.003619  ]
 [ 2.0047038   4.064143    6.2048492  ...  5.6568336   0.71395487
   2.6926432 ]
 ...
 [ 5.645455    7.0971727   6.202566   ...  0.61970913  6.238492
   1.3539525 ]
 [ 6.475603    3.1151166   6.2248073  ...  2.8990176   3.6724448
   2.289971  ]
 [ 2.2290072   4.101192    7.1413527  ...  6.717821   -2.0046902
   2.3999293 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_353"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_354 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_33 (LogProbL  (None,)                  3291840   
 ayer)                                                           
                                                                 
=================================================================
Total params: 3,291,840
Trainable params: 3,291,840
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_33/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_33'")
self.model: <keras.engine.functional.Functional object at 0x7fad0116bca0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fad2848ff40>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fad2848ff40>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7facfaf96b30>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fad00e03520>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fad00e03a90>, <keras.callbacks.ModelCheckpoint object at 0x7fad00e03b50>, <keras.callbacks.EarlyStopping object at 0x7fad00e03dc0>, <keras.callbacks.ReduceLROnPlateau object at 0x7fad00e03df0>, <keras.callbacks.TerminateOnNaN object at 0x7fad00e03a30>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_350/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 350/720 with hyperparameters:
timestamp = 2023-10-01 04:47:14.199695
ndims = 64
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 3291840
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 2.0547452   3.776408    6.9601746   1.2163222   8.644804    1.0450234
  9.756204    5.9333344   9.3759165   6.2056174   7.6663713   0.8787037
  2.7781603   1.7675867   3.9287794   0.8286338   3.6572614   4.846633
  0.68292063  6.317236    6.48625     3.843101    5.88724     2.0270782
  5.1297107   7.7579265   2.6054757   6.6726594   1.0543873   7.123391
  3.5526195   2.046467    5.620422   -0.869851    8.648443    0.02500496
  7.4141846   2.6777635   7.6790566   9.76465     2.481489    6.2672224
  5.7068043   4.504637    1.1161722   9.691578    4.0927534   8.503739
  6.9964066   3.1729455   8.086372    4.0078015   8.096493    5.813139
  8.181514    6.97417     7.246071    5.0315676  10.666024    6.2678714
  3.6855178   5.8637457  -0.4179731   2.747595  ]
Epoch 1/1000
2023-10-01 05:01:33.428 
Epoch 1/1000 
	 loss: 3490.4111, MinusLogProbMetric: 3490.4111, val_loss: 1384.7465, val_MinusLogProbMetric: 1384.7465

Epoch 1: val_loss improved from inf to 1384.74646, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 856s - loss: 3490.4111 - MinusLogProbMetric: 3490.4111 - val_loss: 1384.7465 - val_MinusLogProbMetric: 1384.7465 - lr: 1.1111e-04 - 856s/epoch - 4s/step
Epoch 2/1000
2023-10-01 05:04:15.176 
Epoch 2/1000 
	 loss: 1444.9397, MinusLogProbMetric: 1444.9397, val_loss: 1741.4432, val_MinusLogProbMetric: 1741.4432

Epoch 2: val_loss did not improve from 1384.74646
196/196 - 160s - loss: 1444.9397 - MinusLogProbMetric: 1444.9397 - val_loss: 1741.4432 - val_MinusLogProbMetric: 1741.4432 - lr: 1.1111e-04 - 160s/epoch - 814ms/step
Epoch 3/1000
2023-10-01 05:06:47.766 
Epoch 3/1000 
	 loss: 1041.4927, MinusLogProbMetric: 1041.4927, val_loss: 744.2280, val_MinusLogProbMetric: 744.2280

Epoch 3: val_loss improved from 1384.74646 to 744.22803, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 157s - loss: 1041.4927 - MinusLogProbMetric: 1041.4927 - val_loss: 744.2280 - val_MinusLogProbMetric: 744.2280 - lr: 1.1111e-04 - 157s/epoch - 799ms/step
Epoch 4/1000
2023-10-01 05:09:23.931 
Epoch 4/1000 
	 loss: 909.9951, MinusLogProbMetric: 909.9951, val_loss: 1006.5729, val_MinusLogProbMetric: 1006.5729

Epoch 4: val_loss did not improve from 744.22803
196/196 - 152s - loss: 909.9951 - MinusLogProbMetric: 909.9951 - val_loss: 1006.5729 - val_MinusLogProbMetric: 1006.5729 - lr: 1.1111e-04 - 152s/epoch - 776ms/step
Epoch 5/1000
2023-10-01 05:12:02.220 
Epoch 5/1000 
	 loss: 765.3713, MinusLogProbMetric: 765.3713, val_loss: 656.7100, val_MinusLogProbMetric: 656.7100

Epoch 5: val_loss improved from 744.22803 to 656.70996, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 162s - loss: 765.3713 - MinusLogProbMetric: 765.3713 - val_loss: 656.7100 - val_MinusLogProbMetric: 656.7100 - lr: 1.1111e-04 - 162s/epoch - 825ms/step
Epoch 6/1000
2023-10-01 05:14:41.052 
Epoch 6/1000 
	 loss: 608.5893, MinusLogProbMetric: 608.5893, val_loss: 574.7625, val_MinusLogProbMetric: 574.7625

Epoch 6: val_loss improved from 656.70996 to 574.76251, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 158s - loss: 608.5893 - MinusLogProbMetric: 608.5893 - val_loss: 574.7625 - val_MinusLogProbMetric: 574.7625 - lr: 1.1111e-04 - 158s/epoch - 805ms/step
Epoch 7/1000
2023-10-01 05:17:18.058 
Epoch 7/1000 
	 loss: 524.9641, MinusLogProbMetric: 524.9641, val_loss: 506.3366, val_MinusLogProbMetric: 506.3366

Epoch 7: val_loss improved from 574.76251 to 506.33661, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 157s - loss: 524.9641 - MinusLogProbMetric: 524.9641 - val_loss: 506.3366 - val_MinusLogProbMetric: 506.3366 - lr: 1.1111e-04 - 157s/epoch - 799ms/step
Epoch 8/1000
2023-10-01 05:19:56.888 
Epoch 8/1000 
	 loss: 485.5200, MinusLogProbMetric: 485.5200, val_loss: 457.4850, val_MinusLogProbMetric: 457.4850

Epoch 8: val_loss improved from 506.33661 to 457.48505, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 159s - loss: 485.5200 - MinusLogProbMetric: 485.5200 - val_loss: 457.4850 - val_MinusLogProbMetric: 457.4850 - lr: 1.1111e-04 - 159s/epoch - 814ms/step
Epoch 9/1000
2023-10-01 05:22:32.208 
Epoch 9/1000 
	 loss: 490.7676, MinusLogProbMetric: 490.7676, val_loss: 455.9147, val_MinusLogProbMetric: 455.9147

Epoch 9: val_loss improved from 457.48505 to 455.91473, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 154s - loss: 490.7676 - MinusLogProbMetric: 490.7676 - val_loss: 455.9147 - val_MinusLogProbMetric: 455.9147 - lr: 1.1111e-04 - 154s/epoch - 787ms/step
Epoch 10/1000
2023-10-01 05:25:09.672 
Epoch 10/1000 
	 loss: 439.6455, MinusLogProbMetric: 439.6455, val_loss: 416.3205, val_MinusLogProbMetric: 416.3205

Epoch 10: val_loss improved from 455.91473 to 416.32053, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 157s - loss: 439.6455 - MinusLogProbMetric: 439.6455 - val_loss: 416.3205 - val_MinusLogProbMetric: 416.3205 - lr: 1.1111e-04 - 157s/epoch - 802ms/step
Epoch 11/1000
2023-10-01 05:27:44.732 
Epoch 11/1000 
	 loss: 407.3219, MinusLogProbMetric: 407.3219, val_loss: 392.6873, val_MinusLogProbMetric: 392.6873

Epoch 11: val_loss improved from 416.32053 to 392.68735, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 155s - loss: 407.3219 - MinusLogProbMetric: 407.3219 - val_loss: 392.6873 - val_MinusLogProbMetric: 392.6873 - lr: 1.1111e-04 - 155s/epoch - 792ms/step
Epoch 12/1000
2023-10-01 05:30:26.581 
Epoch 12/1000 
	 loss: 386.4203, MinusLogProbMetric: 386.4203, val_loss: 370.7138, val_MinusLogProbMetric: 370.7138

Epoch 12: val_loss improved from 392.68735 to 370.71378, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 164s - loss: 386.4203 - MinusLogProbMetric: 386.4203 - val_loss: 370.7138 - val_MinusLogProbMetric: 370.7138 - lr: 1.1111e-04 - 164s/epoch - 836ms/step
Epoch 13/1000
2023-10-01 05:32:58.753 
Epoch 13/1000 
	 loss: 366.9388, MinusLogProbMetric: 366.9388, val_loss: 353.8890, val_MinusLogProbMetric: 353.8890

Epoch 13: val_loss improved from 370.71378 to 353.88898, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 154s - loss: 366.9388 - MinusLogProbMetric: 366.9388 - val_loss: 353.8890 - val_MinusLogProbMetric: 353.8890 - lr: 1.1111e-04 - 154s/epoch - 785ms/step
Epoch 14/1000
2023-10-01 05:35:39.240 
Epoch 14/1000 
	 loss: 345.7783, MinusLogProbMetric: 345.7783, val_loss: 336.7100, val_MinusLogProbMetric: 336.7100

Epoch 14: val_loss improved from 353.88898 to 336.70996, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 158s - loss: 345.7783 - MinusLogProbMetric: 345.7783 - val_loss: 336.7100 - val_MinusLogProbMetric: 336.7100 - lr: 1.1111e-04 - 158s/epoch - 806ms/step
Epoch 15/1000
2023-10-01 05:38:03.227 
Epoch 15/1000 
	 loss: 341.1985, MinusLogProbMetric: 341.1985, val_loss: 336.6736, val_MinusLogProbMetric: 336.6736

Epoch 15: val_loss improved from 336.70996 to 336.67355, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 143s - loss: 341.1985 - MinusLogProbMetric: 341.1985 - val_loss: 336.6736 - val_MinusLogProbMetric: 336.6736 - lr: 1.1111e-04 - 143s/epoch - 729ms/step
Epoch 16/1000
2023-10-01 05:40:07.697 
Epoch 16/1000 
	 loss: 354.0748, MinusLogProbMetric: 354.0748, val_loss: 328.9364, val_MinusLogProbMetric: 328.9364

Epoch 16: val_loss improved from 336.67355 to 328.93643, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 125s - loss: 354.0748 - MinusLogProbMetric: 354.0748 - val_loss: 328.9364 - val_MinusLogProbMetric: 328.9364 - lr: 1.1111e-04 - 125s/epoch - 639ms/step
Epoch 17/1000
2023-10-01 05:42:16.471 
Epoch 17/1000 
	 loss: 323.7308, MinusLogProbMetric: 323.7308, val_loss: 314.6159, val_MinusLogProbMetric: 314.6159

Epoch 17: val_loss improved from 328.93643 to 314.61591, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 129s - loss: 323.7308 - MinusLogProbMetric: 323.7308 - val_loss: 314.6159 - val_MinusLogProbMetric: 314.6159 - lr: 1.1111e-04 - 129s/epoch - 660ms/step
Epoch 18/1000
2023-10-01 05:44:23.818 
Epoch 18/1000 
	 loss: 307.1331, MinusLogProbMetric: 307.1331, val_loss: 301.9999, val_MinusLogProbMetric: 301.9999

Epoch 18: val_loss improved from 314.61591 to 301.99991, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 126s - loss: 307.1331 - MinusLogProbMetric: 307.1331 - val_loss: 301.9999 - val_MinusLogProbMetric: 301.9999 - lr: 1.1111e-04 - 126s/epoch - 643ms/step
Epoch 19/1000
2023-10-01 05:46:27.690 
Epoch 19/1000 
	 loss: 294.2612, MinusLogProbMetric: 294.2612, val_loss: 291.6810, val_MinusLogProbMetric: 291.6810

Epoch 19: val_loss improved from 301.99991 to 291.68100, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 125s - loss: 294.2612 - MinusLogProbMetric: 294.2612 - val_loss: 291.6810 - val_MinusLogProbMetric: 291.6810 - lr: 1.1111e-04 - 125s/epoch - 637ms/step
Epoch 20/1000
2023-10-01 05:48:38.047 
Epoch 20/1000 
	 loss: 284.2348, MinusLogProbMetric: 284.2348, val_loss: 282.1712, val_MinusLogProbMetric: 282.1712

Epoch 20: val_loss improved from 291.68100 to 282.17117, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 130s - loss: 284.2348 - MinusLogProbMetric: 284.2348 - val_loss: 282.1712 - val_MinusLogProbMetric: 282.1712 - lr: 1.1111e-04 - 130s/epoch - 662ms/step
Epoch 21/1000
2023-10-01 05:50:41.863 
Epoch 21/1000 
	 loss: 276.4667, MinusLogProbMetric: 276.4667, val_loss: 272.3039, val_MinusLogProbMetric: 272.3039

Epoch 21: val_loss improved from 282.17117 to 272.30386, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 123s - loss: 276.4667 - MinusLogProbMetric: 276.4667 - val_loss: 272.3039 - val_MinusLogProbMetric: 272.3039 - lr: 1.1111e-04 - 123s/epoch - 628ms/step
Epoch 22/1000
2023-10-01 05:52:46.624 
Epoch 22/1000 
	 loss: 268.7318, MinusLogProbMetric: 268.7318, val_loss: 264.1988, val_MinusLogProbMetric: 264.1988

Epoch 22: val_loss improved from 272.30386 to 264.19882, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 127s - loss: 268.7318 - MinusLogProbMetric: 268.7318 - val_loss: 264.1988 - val_MinusLogProbMetric: 264.1988 - lr: 1.1111e-04 - 127s/epoch - 646ms/step
Epoch 23/1000
2023-10-01 05:54:55.382 
Epoch 23/1000 
	 loss: 259.5035, MinusLogProbMetric: 259.5035, val_loss: 255.5391, val_MinusLogProbMetric: 255.5391

Epoch 23: val_loss improved from 264.19882 to 255.53909, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 127s - loss: 259.5035 - MinusLogProbMetric: 259.5035 - val_loss: 255.5391 - val_MinusLogProbMetric: 255.5391 - lr: 1.1111e-04 - 127s/epoch - 647ms/step
Epoch 24/1000
2023-10-01 05:57:01.593 
Epoch 24/1000 
	 loss: 253.7875, MinusLogProbMetric: 253.7875, val_loss: 253.9518, val_MinusLogProbMetric: 253.9518

Epoch 24: val_loss improved from 255.53909 to 253.95180, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 127s - loss: 253.7875 - MinusLogProbMetric: 253.7875 - val_loss: 253.9518 - val_MinusLogProbMetric: 253.9518 - lr: 1.1111e-04 - 127s/epoch - 650ms/step
Epoch 25/1000
2023-10-01 05:59:05.130 
Epoch 25/1000 
	 loss: 245.1100, MinusLogProbMetric: 245.1100, val_loss: 240.4420, val_MinusLogProbMetric: 240.4420

Epoch 25: val_loss improved from 253.95180 to 240.44203, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 123s - loss: 245.1100 - MinusLogProbMetric: 245.1100 - val_loss: 240.4420 - val_MinusLogProbMetric: 240.4420 - lr: 1.1111e-04 - 123s/epoch - 629ms/step
Epoch 26/1000
2023-10-01 06:01:16.917 
Epoch 26/1000 
	 loss: 237.3776, MinusLogProbMetric: 237.3776, val_loss: 234.5791, val_MinusLogProbMetric: 234.5791

Epoch 26: val_loss improved from 240.44203 to 234.57906, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 132s - loss: 237.3776 - MinusLogProbMetric: 237.3776 - val_loss: 234.5791 - val_MinusLogProbMetric: 234.5791 - lr: 1.1111e-04 - 132s/epoch - 673ms/step
Epoch 27/1000
2023-10-01 06:03:23.485 
Epoch 27/1000 
	 loss: 230.3836, MinusLogProbMetric: 230.3836, val_loss: 225.0348, val_MinusLogProbMetric: 225.0348

Epoch 27: val_loss improved from 234.57906 to 225.03484, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 125s - loss: 230.3836 - MinusLogProbMetric: 230.3836 - val_loss: 225.0348 - val_MinusLogProbMetric: 225.0348 - lr: 1.1111e-04 - 125s/epoch - 640ms/step
Epoch 28/1000
2023-10-01 06:05:26.575 
Epoch 28/1000 
	 loss: 224.0949, MinusLogProbMetric: 224.0949, val_loss: 220.1584, val_MinusLogProbMetric: 220.1584

Epoch 28: val_loss improved from 225.03484 to 220.15837, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 124s - loss: 224.0949 - MinusLogProbMetric: 224.0949 - val_loss: 220.1584 - val_MinusLogProbMetric: 220.1584 - lr: 1.1111e-04 - 124s/epoch - 630ms/step
Epoch 29/1000
2023-10-01 06:07:27.453 
Epoch 29/1000 
	 loss: 219.7525, MinusLogProbMetric: 219.7525, val_loss: 217.3251, val_MinusLogProbMetric: 217.3251

Epoch 29: val_loss improved from 220.15837 to 217.32513, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 122s - loss: 219.7525 - MinusLogProbMetric: 219.7525 - val_loss: 217.3251 - val_MinusLogProbMetric: 217.3251 - lr: 1.1111e-04 - 122s/epoch - 620ms/step
Epoch 30/1000
2023-10-01 06:09:30.024 
Epoch 30/1000 
	 loss: 216.1330, MinusLogProbMetric: 216.1330, val_loss: 212.2797, val_MinusLogProbMetric: 212.2797

Epoch 30: val_loss improved from 217.32513 to 212.27968, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 123s - loss: 216.1330 - MinusLogProbMetric: 216.1330 - val_loss: 212.2797 - val_MinusLogProbMetric: 212.2797 - lr: 1.1111e-04 - 123s/epoch - 628ms/step
Epoch 31/1000
2023-10-01 06:11:33.402 
Epoch 31/1000 
	 loss: 209.2460, MinusLogProbMetric: 209.2460, val_loss: 205.6913, val_MinusLogProbMetric: 205.6913

Epoch 31: val_loss improved from 212.27968 to 205.69127, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 123s - loss: 209.2460 - MinusLogProbMetric: 209.2460 - val_loss: 205.6913 - val_MinusLogProbMetric: 205.6913 - lr: 1.1111e-04 - 123s/epoch - 629ms/step
Epoch 32/1000
2023-10-01 06:13:41.425 
Epoch 32/1000 
	 loss: 215.3672, MinusLogProbMetric: 215.3672, val_loss: 211.1005, val_MinusLogProbMetric: 211.1005

Epoch 32: val_loss did not improve from 205.69127
196/196 - 125s - loss: 215.3672 - MinusLogProbMetric: 215.3672 - val_loss: 211.1005 - val_MinusLogProbMetric: 211.1005 - lr: 1.1111e-04 - 125s/epoch - 637ms/step
Epoch 33/1000
2023-10-01 06:15:44.273 
Epoch 33/1000 
	 loss: 206.7271, MinusLogProbMetric: 206.7271, val_loss: 201.9345, val_MinusLogProbMetric: 201.9345

Epoch 33: val_loss improved from 205.69127 to 201.93454, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 124s - loss: 206.7271 - MinusLogProbMetric: 206.7271 - val_loss: 201.9345 - val_MinusLogProbMetric: 201.9345 - lr: 1.1111e-04 - 124s/epoch - 634ms/step
Epoch 34/1000
2023-10-01 06:17:49.091 
Epoch 34/1000 
	 loss: 200.0075, MinusLogProbMetric: 200.0075, val_loss: 196.8454, val_MinusLogProbMetric: 196.8454

Epoch 34: val_loss improved from 201.93454 to 196.84537, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 126s - loss: 200.0075 - MinusLogProbMetric: 200.0075 - val_loss: 196.8454 - val_MinusLogProbMetric: 196.8454 - lr: 1.1111e-04 - 126s/epoch - 641ms/step
Epoch 35/1000
2023-10-01 06:19:53.759 
Epoch 35/1000 
	 loss: 193.9082, MinusLogProbMetric: 193.9082, val_loss: 192.6758, val_MinusLogProbMetric: 192.6758

Epoch 35: val_loss improved from 196.84537 to 192.67577, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 125s - loss: 193.9082 - MinusLogProbMetric: 193.9082 - val_loss: 192.6758 - val_MinusLogProbMetric: 192.6758 - lr: 1.1111e-04 - 125s/epoch - 640ms/step
Epoch 36/1000
2023-10-01 06:21:57.067 
Epoch 36/1000 
	 loss: 190.2030, MinusLogProbMetric: 190.2030, val_loss: 187.2214, val_MinusLogProbMetric: 187.2214

Epoch 36: val_loss improved from 192.67577 to 187.22144, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 123s - loss: 190.2030 - MinusLogProbMetric: 190.2030 - val_loss: 187.2214 - val_MinusLogProbMetric: 187.2214 - lr: 1.1111e-04 - 123s/epoch - 627ms/step
Epoch 37/1000
2023-10-01 06:23:57.514 
Epoch 37/1000 
	 loss: 186.4237, MinusLogProbMetric: 186.4237, val_loss: 183.8502, val_MinusLogProbMetric: 183.8502

Epoch 37: val_loss improved from 187.22144 to 183.85019, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 120s - loss: 186.4237 - MinusLogProbMetric: 186.4237 - val_loss: 183.8502 - val_MinusLogProbMetric: 183.8502 - lr: 1.1111e-04 - 120s/epoch - 611ms/step
Epoch 38/1000
2023-10-01 06:25:55.746 
Epoch 38/1000 
	 loss: 184.7491, MinusLogProbMetric: 184.7491, val_loss: 185.2348, val_MinusLogProbMetric: 185.2348

Epoch 38: val_loss did not improve from 183.85019
196/196 - 116s - loss: 184.7491 - MinusLogProbMetric: 184.7491 - val_loss: 185.2348 - val_MinusLogProbMetric: 185.2348 - lr: 1.1111e-04 - 116s/epoch - 593ms/step
Epoch 39/1000
2023-10-01 06:27:54.637 
Epoch 39/1000 
	 loss: 181.7544, MinusLogProbMetric: 181.7544, val_loss: 182.4840, val_MinusLogProbMetric: 182.4840

Epoch 39: val_loss improved from 183.85019 to 182.48402, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 122s - loss: 181.7544 - MinusLogProbMetric: 181.7544 - val_loss: 182.4840 - val_MinusLogProbMetric: 182.4840 - lr: 1.1111e-04 - 122s/epoch - 623ms/step
Epoch 40/1000
2023-10-01 06:29:55.982 
Epoch 40/1000 
	 loss: 177.8745, MinusLogProbMetric: 177.8745, val_loss: 176.1407, val_MinusLogProbMetric: 176.1407

Epoch 40: val_loss improved from 182.48402 to 176.14066, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 120s - loss: 177.8745 - MinusLogProbMetric: 177.8745 - val_loss: 176.1407 - val_MinusLogProbMetric: 176.1407 - lr: 1.1111e-04 - 120s/epoch - 611ms/step
Epoch 41/1000
2023-10-01 06:31:56.878 
Epoch 41/1000 
	 loss: 174.7209, MinusLogProbMetric: 174.7209, val_loss: 171.9888, val_MinusLogProbMetric: 171.9888

Epoch 41: val_loss improved from 176.14066 to 171.98877, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 120s - loss: 174.7209 - MinusLogProbMetric: 174.7209 - val_loss: 171.9888 - val_MinusLogProbMetric: 171.9888 - lr: 1.1111e-04 - 120s/epoch - 614ms/step
Epoch 42/1000
2023-10-01 06:34:00.500 
Epoch 42/1000 
	 loss: 229.5231, MinusLogProbMetric: 229.5231, val_loss: 217.7091, val_MinusLogProbMetric: 217.7091

Epoch 42: val_loss did not improve from 171.98877
196/196 - 122s - loss: 229.5231 - MinusLogProbMetric: 229.5231 - val_loss: 217.7091 - val_MinusLogProbMetric: 217.7091 - lr: 1.1111e-04 - 122s/epoch - 625ms/step
Epoch 43/1000
2023-10-01 06:35:58.738 
Epoch 43/1000 
	 loss: 192.4217, MinusLogProbMetric: 192.4217, val_loss: 181.4629, val_MinusLogProbMetric: 181.4629

Epoch 43: val_loss did not improve from 171.98877
196/196 - 118s - loss: 192.4217 - MinusLogProbMetric: 192.4217 - val_loss: 181.4629 - val_MinusLogProbMetric: 181.4629 - lr: 1.1111e-04 - 118s/epoch - 603ms/step
Epoch 44/1000
2023-10-01 06:37:56.734 
Epoch 44/1000 
	 loss: 177.9249, MinusLogProbMetric: 177.9249, val_loss: 174.2816, val_MinusLogProbMetric: 174.2816

Epoch 44: val_loss did not improve from 171.98877
196/196 - 118s - loss: 177.9249 - MinusLogProbMetric: 177.9249 - val_loss: 174.2816 - val_MinusLogProbMetric: 174.2816 - lr: 1.1111e-04 - 118s/epoch - 602ms/step
Epoch 45/1000
2023-10-01 06:39:53.853 
Epoch 45/1000 
	 loss: 171.2049, MinusLogProbMetric: 171.2049, val_loss: 168.8529, val_MinusLogProbMetric: 168.8529

Epoch 45: val_loss improved from 171.98877 to 168.85291, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 119s - loss: 171.2049 - MinusLogProbMetric: 171.2049 - val_loss: 168.8529 - val_MinusLogProbMetric: 168.8529 - lr: 1.1111e-04 - 119s/epoch - 608ms/step
Epoch 46/1000
2023-10-01 06:41:52.896 
Epoch 46/1000 
	 loss: 168.1430, MinusLogProbMetric: 168.1430, val_loss: 166.2966, val_MinusLogProbMetric: 166.2966

Epoch 46: val_loss improved from 168.85291 to 166.29662, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 118s - loss: 168.1430 - MinusLogProbMetric: 168.1430 - val_loss: 166.2966 - val_MinusLogProbMetric: 166.2966 - lr: 1.1111e-04 - 118s/epoch - 602ms/step
Epoch 47/1000
2023-10-01 06:43:47.491 
Epoch 47/1000 
	 loss: 164.4418, MinusLogProbMetric: 164.4418, val_loss: 238.1284, val_MinusLogProbMetric: 238.1284

Epoch 47: val_loss did not improve from 166.29662
196/196 - 113s - loss: 164.4418 - MinusLogProbMetric: 164.4418 - val_loss: 238.1284 - val_MinusLogProbMetric: 238.1284 - lr: 1.1111e-04 - 113s/epoch - 579ms/step
Epoch 48/1000
2023-10-01 06:45:45.995 
Epoch 48/1000 
	 loss: 170.0800, MinusLogProbMetric: 170.0800, val_loss: 160.9441, val_MinusLogProbMetric: 160.9441

Epoch 48: val_loss improved from 166.29662 to 160.94406, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 120s - loss: 170.0800 - MinusLogProbMetric: 170.0800 - val_loss: 160.9441 - val_MinusLogProbMetric: 160.9441 - lr: 1.1111e-04 - 120s/epoch - 612ms/step
Epoch 49/1000
2023-10-01 06:47:45.083 
Epoch 49/1000 
	 loss: 186.5193, MinusLogProbMetric: 186.5193, val_loss: 195.8079, val_MinusLogProbMetric: 195.8079

Epoch 49: val_loss did not improve from 160.94406
196/196 - 118s - loss: 186.5193 - MinusLogProbMetric: 186.5193 - val_loss: 195.8079 - val_MinusLogProbMetric: 195.8079 - lr: 1.1111e-04 - 118s/epoch - 600ms/step
Epoch 50/1000
2023-10-01 06:49:45.999 
Epoch 50/1000 
	 loss: 181.3292, MinusLogProbMetric: 181.3292, val_loss: 172.2919, val_MinusLogProbMetric: 172.2919

Epoch 50: val_loss did not improve from 160.94406
196/196 - 121s - loss: 181.3292 - MinusLogProbMetric: 181.3292 - val_loss: 172.2919 - val_MinusLogProbMetric: 172.2919 - lr: 1.1111e-04 - 121s/epoch - 617ms/step
Epoch 51/1000
2023-10-01 06:51:42.619 
Epoch 51/1000 
	 loss: 170.8794, MinusLogProbMetric: 170.8794, val_loss: 166.7039, val_MinusLogProbMetric: 166.7039

Epoch 51: val_loss did not improve from 160.94406
196/196 - 117s - loss: 170.8794 - MinusLogProbMetric: 170.8794 - val_loss: 166.7039 - val_MinusLogProbMetric: 166.7039 - lr: 1.1111e-04 - 117s/epoch - 595ms/step
Epoch 52/1000
2023-10-01 06:53:41.111 
Epoch 52/1000 
	 loss: 163.9968, MinusLogProbMetric: 163.9968, val_loss: 161.2542, val_MinusLogProbMetric: 161.2542

Epoch 52: val_loss did not improve from 160.94406
196/196 - 118s - loss: 163.9968 - MinusLogProbMetric: 163.9968 - val_loss: 161.2542 - val_MinusLogProbMetric: 161.2542 - lr: 1.1111e-04 - 118s/epoch - 604ms/step
Epoch 53/1000
2023-10-01 06:55:36.822 
Epoch 53/1000 
	 loss: 159.5590, MinusLogProbMetric: 159.5590, val_loss: 160.9908, val_MinusLogProbMetric: 160.9908

Epoch 53: val_loss did not improve from 160.94406
196/196 - 116s - loss: 159.5590 - MinusLogProbMetric: 159.5590 - val_loss: 160.9908 - val_MinusLogProbMetric: 160.9908 - lr: 1.1111e-04 - 116s/epoch - 590ms/step
Epoch 54/1000
2023-10-01 06:57:32.972 
Epoch 54/1000 
	 loss: 159.3634, MinusLogProbMetric: 159.3634, val_loss: 155.6436, val_MinusLogProbMetric: 155.6436

Epoch 54: val_loss improved from 160.94406 to 155.64357, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 118s - loss: 159.3634 - MinusLogProbMetric: 159.3634 - val_loss: 155.6436 - val_MinusLogProbMetric: 155.6436 - lr: 1.1111e-04 - 118s/epoch - 603ms/step
Epoch 55/1000
2023-10-01 06:59:29.029 
Epoch 55/1000 
	 loss: 164.6015, MinusLogProbMetric: 164.6015, val_loss: 229.9762, val_MinusLogProbMetric: 229.9762

Epoch 55: val_loss did not improve from 155.64357
196/196 - 114s - loss: 164.6015 - MinusLogProbMetric: 164.6015 - val_loss: 229.9762 - val_MinusLogProbMetric: 229.9762 - lr: 1.1111e-04 - 114s/epoch - 582ms/step
Epoch 56/1000
2023-10-01 07:01:22.495 
Epoch 56/1000 
	 loss: 176.9299, MinusLogProbMetric: 176.9299, val_loss: 165.3807, val_MinusLogProbMetric: 165.3807

Epoch 56: val_loss did not improve from 155.64357
196/196 - 113s - loss: 176.9299 - MinusLogProbMetric: 176.9299 - val_loss: 165.3807 - val_MinusLogProbMetric: 165.3807 - lr: 1.1111e-04 - 113s/epoch - 579ms/step
Epoch 57/1000
2023-10-01 07:03:20.743 
Epoch 57/1000 
	 loss: 160.6212, MinusLogProbMetric: 160.6212, val_loss: 157.8291, val_MinusLogProbMetric: 157.8291

Epoch 57: val_loss did not improve from 155.64357
196/196 - 118s - loss: 160.6212 - MinusLogProbMetric: 160.6212 - val_loss: 157.8291 - val_MinusLogProbMetric: 157.8291 - lr: 1.1111e-04 - 118s/epoch - 603ms/step
Epoch 58/1000
2023-10-01 07:05:19.118 
Epoch 58/1000 
	 loss: 155.9630, MinusLogProbMetric: 155.9630, val_loss: 154.1763, val_MinusLogProbMetric: 154.1763

Epoch 58: val_loss improved from 155.64357 to 154.17635, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 121s - loss: 155.9630 - MinusLogProbMetric: 155.9630 - val_loss: 154.1763 - val_MinusLogProbMetric: 154.1763 - lr: 1.1111e-04 - 121s/epoch - 619ms/step
Epoch 59/1000
2023-10-01 07:07:20.737 
Epoch 59/1000 
	 loss: 152.3864, MinusLogProbMetric: 152.3864, val_loss: 150.8849, val_MinusLogProbMetric: 150.8849

Epoch 59: val_loss improved from 154.17635 to 150.88487, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 121s - loss: 152.3864 - MinusLogProbMetric: 152.3864 - val_loss: 150.8849 - val_MinusLogProbMetric: 150.8849 - lr: 1.1111e-04 - 121s/epoch - 617ms/step
Epoch 60/1000
2023-10-01 07:09:19.756 
Epoch 60/1000 
	 loss: 155.5528, MinusLogProbMetric: 155.5528, val_loss: 295.1125, val_MinusLogProbMetric: 295.1125

Epoch 60: val_loss did not improve from 150.88487
196/196 - 117s - loss: 155.5528 - MinusLogProbMetric: 155.5528 - val_loss: 295.1125 - val_MinusLogProbMetric: 295.1125 - lr: 1.1111e-04 - 117s/epoch - 596ms/step
Epoch 61/1000
2023-10-01 07:11:14.484 
Epoch 61/1000 
	 loss: 230.5231, MinusLogProbMetric: 230.5231, val_loss: 201.3130, val_MinusLogProbMetric: 201.3130

Epoch 61: val_loss did not improve from 150.88487
196/196 - 115s - loss: 230.5231 - MinusLogProbMetric: 230.5231 - val_loss: 201.3130 - val_MinusLogProbMetric: 201.3130 - lr: 1.1111e-04 - 115s/epoch - 585ms/step
Epoch 62/1000
2023-10-01 07:13:16.344 
Epoch 62/1000 
	 loss: 194.7439, MinusLogProbMetric: 194.7439, val_loss: 182.2486, val_MinusLogProbMetric: 182.2486

Epoch 62: val_loss did not improve from 150.88487
196/196 - 122s - loss: 194.7439 - MinusLogProbMetric: 194.7439 - val_loss: 182.2486 - val_MinusLogProbMetric: 182.2486 - lr: 1.1111e-04 - 122s/epoch - 622ms/step
Epoch 63/1000
2023-10-01 07:15:16.929 
Epoch 63/1000 
	 loss: 183.5939, MinusLogProbMetric: 183.5939, val_loss: 181.2253, val_MinusLogProbMetric: 181.2253

Epoch 63: val_loss did not improve from 150.88487
196/196 - 121s - loss: 183.5939 - MinusLogProbMetric: 183.5939 - val_loss: 181.2253 - val_MinusLogProbMetric: 181.2253 - lr: 1.1111e-04 - 121s/epoch - 615ms/step
Epoch 64/1000
2023-10-01 07:17:15.894 
Epoch 64/1000 
	 loss: 176.7311, MinusLogProbMetric: 176.7311, val_loss: 173.2625, val_MinusLogProbMetric: 173.2625

Epoch 64: val_loss did not improve from 150.88487
196/196 - 119s - loss: 176.7311 - MinusLogProbMetric: 176.7311 - val_loss: 173.2625 - val_MinusLogProbMetric: 173.2625 - lr: 1.1111e-04 - 119s/epoch - 607ms/step
Epoch 65/1000
2023-10-01 07:19:19.213 
Epoch 65/1000 
	 loss: 167.6713, MinusLogProbMetric: 167.6713, val_loss: 163.7453, val_MinusLogProbMetric: 163.7453

Epoch 65: val_loss did not improve from 150.88487
196/196 - 123s - loss: 167.6713 - MinusLogProbMetric: 167.6713 - val_loss: 163.7453 - val_MinusLogProbMetric: 163.7453 - lr: 1.1111e-04 - 123s/epoch - 629ms/step
Epoch 66/1000
2023-10-01 07:21:19.338 
Epoch 66/1000 
	 loss: 162.2632, MinusLogProbMetric: 162.2632, val_loss: 159.9898, val_MinusLogProbMetric: 159.9898

Epoch 66: val_loss did not improve from 150.88487
196/196 - 120s - loss: 162.2632 - MinusLogProbMetric: 162.2632 - val_loss: 159.9898 - val_MinusLogProbMetric: 159.9898 - lr: 1.1111e-04 - 120s/epoch - 613ms/step
Epoch 67/1000
2023-10-01 07:23:15.357 
Epoch 67/1000 
	 loss: 158.1576, MinusLogProbMetric: 158.1576, val_loss: 156.3142, val_MinusLogProbMetric: 156.3142

Epoch 67: val_loss did not improve from 150.88487
196/196 - 116s - loss: 158.1576 - MinusLogProbMetric: 158.1576 - val_loss: 156.3142 - val_MinusLogProbMetric: 156.3142 - lr: 1.1111e-04 - 116s/epoch - 592ms/step
Epoch 68/1000
2023-10-01 07:25:12.206 
Epoch 68/1000 
	 loss: 154.6912, MinusLogProbMetric: 154.6912, val_loss: 153.6150, val_MinusLogProbMetric: 153.6150

Epoch 68: val_loss did not improve from 150.88487
196/196 - 117s - loss: 154.6912 - MinusLogProbMetric: 154.6912 - val_loss: 153.6150 - val_MinusLogProbMetric: 153.6150 - lr: 1.1111e-04 - 117s/epoch - 596ms/step
Epoch 69/1000
2023-10-01 07:27:10.090 
Epoch 69/1000 
	 loss: 152.1343, MinusLogProbMetric: 152.1343, val_loss: 151.0012, val_MinusLogProbMetric: 151.0012

Epoch 69: val_loss did not improve from 150.88487
196/196 - 118s - loss: 152.1343 - MinusLogProbMetric: 152.1343 - val_loss: 151.0012 - val_MinusLogProbMetric: 151.0012 - lr: 1.1111e-04 - 118s/epoch - 601ms/step
Epoch 70/1000
2023-10-01 07:29:08.119 
Epoch 70/1000 
	 loss: 149.6060, MinusLogProbMetric: 149.6060, val_loss: 148.6003, val_MinusLogProbMetric: 148.6003

Epoch 70: val_loss improved from 150.88487 to 148.60025, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 121s - loss: 149.6060 - MinusLogProbMetric: 149.6060 - val_loss: 148.6003 - val_MinusLogProbMetric: 148.6003 - lr: 1.1111e-04 - 121s/epoch - 617ms/step
Epoch 71/1000
2023-10-01 07:31:08.748 
Epoch 71/1000 
	 loss: 147.4162, MinusLogProbMetric: 147.4162, val_loss: 146.8039, val_MinusLogProbMetric: 146.8039

Epoch 71: val_loss improved from 148.60025 to 146.80391, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 119s - loss: 147.4162 - MinusLogProbMetric: 147.4162 - val_loss: 146.8039 - val_MinusLogProbMetric: 146.8039 - lr: 1.1111e-04 - 119s/epoch - 608ms/step
Epoch 72/1000
2023-10-01 07:33:07.890 
Epoch 72/1000 
	 loss: 145.7413, MinusLogProbMetric: 145.7413, val_loss: 145.5734, val_MinusLogProbMetric: 145.5734

Epoch 72: val_loss improved from 146.80391 to 145.57336, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 119s - loss: 145.7413 - MinusLogProbMetric: 145.7413 - val_loss: 145.5734 - val_MinusLogProbMetric: 145.5734 - lr: 1.1111e-04 - 119s/epoch - 607ms/step
Epoch 73/1000
2023-10-01 07:35:11.549 
Epoch 73/1000 
	 loss: 265.8596, MinusLogProbMetric: 265.8596, val_loss: 324.2654, val_MinusLogProbMetric: 324.2654

Epoch 73: val_loss did not improve from 145.57336
196/196 - 122s - loss: 265.8596 - MinusLogProbMetric: 265.8596 - val_loss: 324.2654 - val_MinusLogProbMetric: 324.2654 - lr: 1.1111e-04 - 122s/epoch - 624ms/step
Epoch 74/1000
2023-10-01 07:37:11.442 
Epoch 74/1000 
	 loss: 282.1140, MinusLogProbMetric: 282.1140, val_loss: 249.1164, val_MinusLogProbMetric: 249.1164

Epoch 74: val_loss did not improve from 145.57336
196/196 - 120s - loss: 282.1140 - MinusLogProbMetric: 282.1140 - val_loss: 249.1164 - val_MinusLogProbMetric: 249.1164 - lr: 1.1111e-04 - 120s/epoch - 612ms/step
Epoch 75/1000
2023-10-01 07:39:09.459 
Epoch 75/1000 
	 loss: 232.1305, MinusLogProbMetric: 232.1305, val_loss: 219.4785, val_MinusLogProbMetric: 219.4785

Epoch 75: val_loss did not improve from 145.57336
196/196 - 118s - loss: 232.1305 - MinusLogProbMetric: 232.1305 - val_loss: 219.4785 - val_MinusLogProbMetric: 219.4785 - lr: 1.1111e-04 - 118s/epoch - 602ms/step
Epoch 76/1000
2023-10-01 07:41:07.970 
Epoch 76/1000 
	 loss: 211.6640, MinusLogProbMetric: 211.6640, val_loss: 203.0712, val_MinusLogProbMetric: 203.0712

Epoch 76: val_loss did not improve from 145.57336
196/196 - 119s - loss: 211.6640 - MinusLogProbMetric: 211.6640 - val_loss: 203.0712 - val_MinusLogProbMetric: 203.0712 - lr: 1.1111e-04 - 119s/epoch - 605ms/step
Epoch 77/1000
2023-10-01 07:43:07.049 
Epoch 77/1000 
	 loss: 202.6118, MinusLogProbMetric: 202.6118, val_loss: 193.3456, val_MinusLogProbMetric: 193.3456

Epoch 77: val_loss did not improve from 145.57336
196/196 - 119s - loss: 202.6118 - MinusLogProbMetric: 202.6118 - val_loss: 193.3456 - val_MinusLogProbMetric: 193.3456 - lr: 1.1111e-04 - 119s/epoch - 607ms/step
Epoch 78/1000
2023-10-01 07:45:06.154 
Epoch 78/1000 
	 loss: 193.2687, MinusLogProbMetric: 193.2687, val_loss: 186.1386, val_MinusLogProbMetric: 186.1386

Epoch 78: val_loss did not improve from 145.57336
196/196 - 119s - loss: 193.2687 - MinusLogProbMetric: 193.2687 - val_loss: 186.1386 - val_MinusLogProbMetric: 186.1386 - lr: 1.1111e-04 - 119s/epoch - 608ms/step
Epoch 79/1000
2023-10-01 07:47:05.054 
Epoch 79/1000 
	 loss: 205.3866, MinusLogProbMetric: 205.3866, val_loss: 192.5123, val_MinusLogProbMetric: 192.5123

Epoch 79: val_loss did not improve from 145.57336
196/196 - 119s - loss: 205.3866 - MinusLogProbMetric: 205.3866 - val_loss: 192.5123 - val_MinusLogProbMetric: 192.5123 - lr: 1.1111e-04 - 119s/epoch - 607ms/step
Epoch 80/1000
2023-10-01 07:49:02.587 
Epoch 80/1000 
	 loss: 185.4929, MinusLogProbMetric: 185.4929, val_loss: 180.4112, val_MinusLogProbMetric: 180.4112

Epoch 80: val_loss did not improve from 145.57336
196/196 - 118s - loss: 185.4929 - MinusLogProbMetric: 185.4929 - val_loss: 180.4112 - val_MinusLogProbMetric: 180.4112 - lr: 1.1111e-04 - 118s/epoch - 600ms/step
Epoch 81/1000
2023-10-01 07:50:59.978 
Epoch 81/1000 
	 loss: 177.5707, MinusLogProbMetric: 177.5707, val_loss: 172.9146, val_MinusLogProbMetric: 172.9146

Epoch 81: val_loss did not improve from 145.57336
196/196 - 117s - loss: 177.5707 - MinusLogProbMetric: 177.5707 - val_loss: 172.9146 - val_MinusLogProbMetric: 172.9146 - lr: 1.1111e-04 - 117s/epoch - 599ms/step
Epoch 82/1000
2023-10-01 07:52:57.209 
Epoch 82/1000 
	 loss: 169.3782, MinusLogProbMetric: 169.3782, val_loss: 166.4597, val_MinusLogProbMetric: 166.4597

Epoch 82: val_loss did not improve from 145.57336
196/196 - 117s - loss: 169.3782 - MinusLogProbMetric: 169.3782 - val_loss: 166.4597 - val_MinusLogProbMetric: 166.4597 - lr: 1.1111e-04 - 117s/epoch - 598ms/step
Epoch 83/1000
2023-10-01 07:54:54.790 
Epoch 83/1000 
	 loss: 165.1438, MinusLogProbMetric: 165.1438, val_loss: 163.5419, val_MinusLogProbMetric: 163.5419

Epoch 83: val_loss did not improve from 145.57336
196/196 - 118s - loss: 165.1438 - MinusLogProbMetric: 165.1438 - val_loss: 163.5419 - val_MinusLogProbMetric: 163.5419 - lr: 1.1111e-04 - 118s/epoch - 600ms/step
Epoch 84/1000
2023-10-01 07:56:53.167 
Epoch 84/1000 
	 loss: 176.4008, MinusLogProbMetric: 176.4008, val_loss: 170.1370, val_MinusLogProbMetric: 170.1370

Epoch 84: val_loss did not improve from 145.57336
196/196 - 118s - loss: 176.4008 - MinusLogProbMetric: 176.4008 - val_loss: 170.1370 - val_MinusLogProbMetric: 170.1370 - lr: 1.1111e-04 - 118s/epoch - 604ms/step
Epoch 85/1000
2023-10-01 07:58:47.251 
Epoch 85/1000 
	 loss: 165.5094, MinusLogProbMetric: 165.5094, val_loss: 162.3148, val_MinusLogProbMetric: 162.3148

Epoch 85: val_loss did not improve from 145.57336
196/196 - 114s - loss: 165.5094 - MinusLogProbMetric: 165.5094 - val_loss: 162.3148 - val_MinusLogProbMetric: 162.3148 - lr: 1.1111e-04 - 114s/epoch - 582ms/step
Epoch 86/1000
2023-10-01 08:00:44.362 
Epoch 86/1000 
	 loss: 160.3000, MinusLogProbMetric: 160.3000, val_loss: 157.8295, val_MinusLogProbMetric: 157.8295

Epoch 86: val_loss did not improve from 145.57336
196/196 - 117s - loss: 160.3000 - MinusLogProbMetric: 160.3000 - val_loss: 157.8295 - val_MinusLogProbMetric: 157.8295 - lr: 1.1111e-04 - 117s/epoch - 597ms/step
Epoch 87/1000
2023-10-01 08:02:39.165 
Epoch 87/1000 
	 loss: 175.2164, MinusLogProbMetric: 175.2164, val_loss: 183.8927, val_MinusLogProbMetric: 183.8927

Epoch 87: val_loss did not improve from 145.57336
196/196 - 115s - loss: 175.2164 - MinusLogProbMetric: 175.2164 - val_loss: 183.8927 - val_MinusLogProbMetric: 183.8927 - lr: 1.1111e-04 - 115s/epoch - 586ms/step
Epoch 88/1000
2023-10-01 08:04:32.853 
Epoch 88/1000 
	 loss: 180.5266, MinusLogProbMetric: 180.5266, val_loss: 178.0087, val_MinusLogProbMetric: 178.0087

Epoch 88: val_loss did not improve from 145.57336
196/196 - 114s - loss: 180.5266 - MinusLogProbMetric: 180.5266 - val_loss: 178.0087 - val_MinusLogProbMetric: 178.0087 - lr: 1.1111e-04 - 114s/epoch - 580ms/step
Epoch 89/1000
2023-10-01 08:06:26.741 
Epoch 89/1000 
	 loss: 176.1247, MinusLogProbMetric: 176.1247, val_loss: 174.5554, val_MinusLogProbMetric: 174.5554

Epoch 89: val_loss did not improve from 145.57336
196/196 - 114s - loss: 176.1247 - MinusLogProbMetric: 176.1247 - val_loss: 174.5554 - val_MinusLogProbMetric: 174.5554 - lr: 1.1111e-04 - 114s/epoch - 581ms/step
Epoch 90/1000
2023-10-01 08:08:23.549 
Epoch 90/1000 
	 loss: 173.7209, MinusLogProbMetric: 173.7209, val_loss: 171.9856, val_MinusLogProbMetric: 171.9856

Epoch 90: val_loss did not improve from 145.57336
196/196 - 117s - loss: 173.7209 - MinusLogProbMetric: 173.7209 - val_loss: 171.9856 - val_MinusLogProbMetric: 171.9856 - lr: 1.1111e-04 - 117s/epoch - 596ms/step
Epoch 91/1000
2023-10-01 08:10:19.342 
Epoch 91/1000 
	 loss: 171.2260, MinusLogProbMetric: 171.2260, val_loss: 169.5211, val_MinusLogProbMetric: 169.5211

Epoch 91: val_loss did not improve from 145.57336
196/196 - 116s - loss: 171.2260 - MinusLogProbMetric: 171.2260 - val_loss: 169.5211 - val_MinusLogProbMetric: 169.5211 - lr: 1.1111e-04 - 116s/epoch - 591ms/step
Epoch 92/1000
2023-10-01 08:12:14.538 
Epoch 92/1000 
	 loss: 168.6334, MinusLogProbMetric: 168.6334, val_loss: 167.9092, val_MinusLogProbMetric: 167.9092

Epoch 92: val_loss did not improve from 145.57336
196/196 - 115s - loss: 168.6334 - MinusLogProbMetric: 168.6334 - val_loss: 167.9092 - val_MinusLogProbMetric: 167.9092 - lr: 1.1111e-04 - 115s/epoch - 588ms/step
Epoch 93/1000
2023-10-01 08:14:12.013 
Epoch 93/1000 
	 loss: 170.8630, MinusLogProbMetric: 170.8630, val_loss: 169.6752, val_MinusLogProbMetric: 169.6752

Epoch 93: val_loss did not improve from 145.57336
196/196 - 117s - loss: 170.8630 - MinusLogProbMetric: 170.8630 - val_loss: 169.6752 - val_MinusLogProbMetric: 169.6752 - lr: 1.1111e-04 - 117s/epoch - 599ms/step
Epoch 94/1000
2023-10-01 08:16:09.799 
Epoch 94/1000 
	 loss: 166.8076, MinusLogProbMetric: 166.8076, val_loss: 166.2596, val_MinusLogProbMetric: 166.2596

Epoch 94: val_loss did not improve from 145.57336
196/196 - 118s - loss: 166.8076 - MinusLogProbMetric: 166.8076 - val_loss: 166.2596 - val_MinusLogProbMetric: 166.2596 - lr: 1.1111e-04 - 118s/epoch - 601ms/step
Epoch 95/1000
2023-10-01 08:18:06.962 
Epoch 95/1000 
	 loss: 162.8507, MinusLogProbMetric: 162.8507, val_loss: 163.0988, val_MinusLogProbMetric: 163.0988

Epoch 95: val_loss did not improve from 145.57336
196/196 - 117s - loss: 162.8507 - MinusLogProbMetric: 162.8507 - val_loss: 163.0988 - val_MinusLogProbMetric: 163.0988 - lr: 1.1111e-04 - 117s/epoch - 598ms/step
Epoch 96/1000
2023-10-01 08:20:05.646 
Epoch 96/1000 
	 loss: 171.1264, MinusLogProbMetric: 171.1264, val_loss: 177.4828, val_MinusLogProbMetric: 177.4828

Epoch 96: val_loss did not improve from 145.57336
196/196 - 119s - loss: 171.1264 - MinusLogProbMetric: 171.1264 - val_loss: 177.4828 - val_MinusLogProbMetric: 177.4828 - lr: 1.1111e-04 - 119s/epoch - 606ms/step
Epoch 97/1000
2023-10-01 08:22:01.632 
Epoch 97/1000 
	 loss: 175.5179, MinusLogProbMetric: 175.5179, val_loss: 174.2387, val_MinusLogProbMetric: 174.2387

Epoch 97: val_loss did not improve from 145.57336
196/196 - 116s - loss: 175.5179 - MinusLogProbMetric: 175.5179 - val_loss: 174.2387 - val_MinusLogProbMetric: 174.2387 - lr: 1.1111e-04 - 116s/epoch - 592ms/step
Epoch 98/1000
2023-10-01 08:23:56.849 
Epoch 98/1000 
	 loss: 172.1770, MinusLogProbMetric: 172.1770, val_loss: 170.7195, val_MinusLogProbMetric: 170.7195

Epoch 98: val_loss did not improve from 145.57336
196/196 - 115s - loss: 172.1770 - MinusLogProbMetric: 172.1770 - val_loss: 170.7195 - val_MinusLogProbMetric: 170.7195 - lr: 1.1111e-04 - 115s/epoch - 588ms/step
Epoch 99/1000
2023-10-01 08:25:51.734 
Epoch 99/1000 
	 loss: 172.6110, MinusLogProbMetric: 172.6110, val_loss: 156.9151, val_MinusLogProbMetric: 156.9151

Epoch 99: val_loss did not improve from 145.57336
196/196 - 115s - loss: 172.6110 - MinusLogProbMetric: 172.6110 - val_loss: 156.9151 - val_MinusLogProbMetric: 156.9151 - lr: 1.1111e-04 - 115s/epoch - 586ms/step
Epoch 100/1000
2023-10-01 08:27:50.972 
Epoch 100/1000 
	 loss: 160.0683, MinusLogProbMetric: 160.0683, val_loss: 234.7739, val_MinusLogProbMetric: 234.7739

Epoch 100: val_loss did not improve from 145.57336
196/196 - 119s - loss: 160.0683 - MinusLogProbMetric: 160.0683 - val_loss: 234.7739 - val_MinusLogProbMetric: 234.7739 - lr: 1.1111e-04 - 119s/epoch - 608ms/step
Epoch 101/1000
2023-10-01 08:29:49.700 
Epoch 101/1000 
	 loss: 267.0990, MinusLogProbMetric: 267.0990, val_loss: 222.4843, val_MinusLogProbMetric: 222.4843

Epoch 101: val_loss did not improve from 145.57336
196/196 - 119s - loss: 267.0990 - MinusLogProbMetric: 267.0990 - val_loss: 222.4843 - val_MinusLogProbMetric: 222.4843 - lr: 1.1111e-04 - 119s/epoch - 606ms/step
Epoch 102/1000
2023-10-01 08:31:46.683 
Epoch 102/1000 
	 loss: 212.9378, MinusLogProbMetric: 212.9378, val_loss: 204.3790, val_MinusLogProbMetric: 204.3790

Epoch 102: val_loss did not improve from 145.57336
196/196 - 117s - loss: 212.9378 - MinusLogProbMetric: 212.9378 - val_loss: 204.3790 - val_MinusLogProbMetric: 204.3790 - lr: 1.1111e-04 - 117s/epoch - 597ms/step
Epoch 103/1000
2023-10-01 08:33:41.024 
Epoch 103/1000 
	 loss: 244.3920, MinusLogProbMetric: 244.3920, val_loss: 253.3409, val_MinusLogProbMetric: 253.3409

Epoch 103: val_loss did not improve from 145.57336
196/196 - 114s - loss: 244.3920 - MinusLogProbMetric: 244.3920 - val_loss: 253.3409 - val_MinusLogProbMetric: 253.3409 - lr: 1.1111e-04 - 114s/epoch - 583ms/step
Epoch 104/1000
2023-10-01 08:35:37.465 
Epoch 104/1000 
	 loss: 213.5781, MinusLogProbMetric: 213.5781, val_loss: 196.5316, val_MinusLogProbMetric: 196.5316

Epoch 104: val_loss did not improve from 145.57336
196/196 - 116s - loss: 213.5781 - MinusLogProbMetric: 213.5781 - val_loss: 196.5316 - val_MinusLogProbMetric: 196.5316 - lr: 1.1111e-04 - 116s/epoch - 594ms/step
Epoch 105/1000
2023-10-01 08:37:34.783 
Epoch 105/1000 
	 loss: 195.8749, MinusLogProbMetric: 195.8749, val_loss: 187.6033, val_MinusLogProbMetric: 187.6033

Epoch 105: val_loss did not improve from 145.57336
196/196 - 117s - loss: 195.8749 - MinusLogProbMetric: 195.8749 - val_loss: 187.6033 - val_MinusLogProbMetric: 187.6033 - lr: 1.1111e-04 - 117s/epoch - 599ms/step
Epoch 106/1000
2023-10-01 08:40:01.887 
Epoch 106/1000 
	 loss: 184.5782, MinusLogProbMetric: 184.5782, val_loss: 181.9745, val_MinusLogProbMetric: 181.9745

Epoch 106: val_loss did not improve from 145.57336
196/196 - 147s - loss: 184.5782 - MinusLogProbMetric: 184.5782 - val_loss: 181.9745 - val_MinusLogProbMetric: 181.9745 - lr: 1.1111e-04 - 147s/epoch - 750ms/step
Epoch 107/1000
2023-10-01 08:42:26.912 
Epoch 107/1000 
	 loss: 179.7754, MinusLogProbMetric: 179.7754, val_loss: 178.3046, val_MinusLogProbMetric: 178.3046

Epoch 107: val_loss did not improve from 145.57336
196/196 - 145s - loss: 179.7754 - MinusLogProbMetric: 179.7754 - val_loss: 178.3046 - val_MinusLogProbMetric: 178.3046 - lr: 1.1111e-04 - 145s/epoch - 740ms/step
Epoch 108/1000
2023-10-01 08:45:00.653 
Epoch 108/1000 
	 loss: 174.7480, MinusLogProbMetric: 174.7480, val_loss: 172.0961, val_MinusLogProbMetric: 172.0961

Epoch 108: val_loss did not improve from 145.57336
196/196 - 154s - loss: 174.7480 - MinusLogProbMetric: 174.7480 - val_loss: 172.0961 - val_MinusLogProbMetric: 172.0961 - lr: 1.1111e-04 - 154s/epoch - 784ms/step
Epoch 109/1000
2023-10-01 08:47:22.446 
Epoch 109/1000 
	 loss: 170.8904, MinusLogProbMetric: 170.8904, val_loss: 169.8568, val_MinusLogProbMetric: 169.8568

Epoch 109: val_loss did not improve from 145.57336
196/196 - 142s - loss: 170.8904 - MinusLogProbMetric: 170.8904 - val_loss: 169.8568 - val_MinusLogProbMetric: 169.8568 - lr: 1.1111e-04 - 142s/epoch - 723ms/step
Epoch 110/1000
2023-10-01 08:49:39.274 
Epoch 110/1000 
	 loss: 172.6860, MinusLogProbMetric: 172.6860, val_loss: 172.7030, val_MinusLogProbMetric: 172.7030

Epoch 110: val_loss did not improve from 145.57336
196/196 - 137s - loss: 172.6860 - MinusLogProbMetric: 172.6860 - val_loss: 172.7030 - val_MinusLogProbMetric: 172.7030 - lr: 1.1111e-04 - 137s/epoch - 698ms/step
Epoch 111/1000
2023-10-01 08:51:40.950 
Epoch 111/1000 
	 loss: 169.2679, MinusLogProbMetric: 169.2679, val_loss: 168.0766, val_MinusLogProbMetric: 168.0766

Epoch 111: val_loss did not improve from 145.57336
196/196 - 122s - loss: 169.2679 - MinusLogProbMetric: 169.2679 - val_loss: 168.0766 - val_MinusLogProbMetric: 168.0766 - lr: 1.1111e-04 - 122s/epoch - 621ms/step
Epoch 112/1000
2023-10-01 08:53:36.852 
Epoch 112/1000 
	 loss: 167.6787, MinusLogProbMetric: 167.6787, val_loss: 165.8710, val_MinusLogProbMetric: 165.8710

Epoch 112: val_loss did not improve from 145.57336
196/196 - 116s - loss: 167.6787 - MinusLogProbMetric: 167.6787 - val_loss: 165.8710 - val_MinusLogProbMetric: 165.8710 - lr: 1.1111e-04 - 116s/epoch - 591ms/step
Epoch 113/1000
2023-10-01 08:55:33.843 
Epoch 113/1000 
	 loss: 164.5116, MinusLogProbMetric: 164.5116, val_loss: 162.8522, val_MinusLogProbMetric: 162.8522

Epoch 113: val_loss did not improve from 145.57336
196/196 - 117s - loss: 164.5116 - MinusLogProbMetric: 164.5116 - val_loss: 162.8522 - val_MinusLogProbMetric: 162.8522 - lr: 1.1111e-04 - 117s/epoch - 597ms/step
Epoch 114/1000
2023-10-01 08:57:28.404 
Epoch 114/1000 
	 loss: 161.7403, MinusLogProbMetric: 161.7403, val_loss: 161.1908, val_MinusLogProbMetric: 161.1908

Epoch 114: val_loss did not improve from 145.57336
196/196 - 115s - loss: 161.7403 - MinusLogProbMetric: 161.7403 - val_loss: 161.1908 - val_MinusLogProbMetric: 161.1908 - lr: 1.1111e-04 - 115s/epoch - 585ms/step
Epoch 115/1000
2023-10-01 08:59:22.033 
Epoch 115/1000 
	 loss: 160.4271, MinusLogProbMetric: 160.4271, val_loss: 159.9391, val_MinusLogProbMetric: 159.9391

Epoch 115: val_loss did not improve from 145.57336
196/196 - 114s - loss: 160.4271 - MinusLogProbMetric: 160.4271 - val_loss: 159.9391 - val_MinusLogProbMetric: 159.9391 - lr: 1.1111e-04 - 114s/epoch - 580ms/step
Epoch 116/1000
2023-10-01 09:01:16.291 
Epoch 116/1000 
	 loss: 159.8800, MinusLogProbMetric: 159.8800, val_loss: 158.6069, val_MinusLogProbMetric: 158.6069

Epoch 116: val_loss did not improve from 145.57336
196/196 - 114s - loss: 159.8800 - MinusLogProbMetric: 159.8800 - val_loss: 158.6069 - val_MinusLogProbMetric: 158.6069 - lr: 1.1111e-04 - 114s/epoch - 583ms/step
Epoch 117/1000
2023-10-01 09:03:11.745 
Epoch 117/1000 
	 loss: 158.1171, MinusLogProbMetric: 158.1171, val_loss: 157.3496, val_MinusLogProbMetric: 157.3496

Epoch 117: val_loss did not improve from 145.57336
196/196 - 115s - loss: 158.1171 - MinusLogProbMetric: 158.1171 - val_loss: 157.3496 - val_MinusLogProbMetric: 157.3496 - lr: 1.1111e-04 - 115s/epoch - 589ms/step
Epoch 118/1000
2023-10-01 09:05:04.437 
Epoch 118/1000 
	 loss: 156.4355, MinusLogProbMetric: 156.4355, val_loss: 155.8239, val_MinusLogProbMetric: 155.8239

Epoch 118: val_loss did not improve from 145.57336
196/196 - 113s - loss: 156.4355 - MinusLogProbMetric: 156.4355 - val_loss: 155.8239 - val_MinusLogProbMetric: 155.8239 - lr: 1.1111e-04 - 113s/epoch - 575ms/step
Epoch 119/1000
2023-10-01 09:06:58.700 
Epoch 119/1000 
	 loss: 155.7259, MinusLogProbMetric: 155.7259, val_loss: 155.0030, val_MinusLogProbMetric: 155.0030

Epoch 119: val_loss did not improve from 145.57336
196/196 - 114s - loss: 155.7259 - MinusLogProbMetric: 155.7259 - val_loss: 155.0030 - val_MinusLogProbMetric: 155.0030 - lr: 1.1111e-04 - 114s/epoch - 583ms/step
Epoch 120/1000
2023-10-01 09:08:51.879 
Epoch 120/1000 
	 loss: 154.1927, MinusLogProbMetric: 154.1927, val_loss: 153.7029, val_MinusLogProbMetric: 153.7029

Epoch 120: val_loss did not improve from 145.57336
196/196 - 113s - loss: 154.1927 - MinusLogProbMetric: 154.1927 - val_loss: 153.7029 - val_MinusLogProbMetric: 153.7029 - lr: 1.1111e-04 - 113s/epoch - 577ms/step
Epoch 121/1000
2023-10-01 09:10:47.165 
Epoch 121/1000 
	 loss: 152.6866, MinusLogProbMetric: 152.6866, val_loss: 152.0154, val_MinusLogProbMetric: 152.0154

Epoch 121: val_loss did not improve from 145.57336
196/196 - 115s - loss: 152.6866 - MinusLogProbMetric: 152.6866 - val_loss: 152.0154 - val_MinusLogProbMetric: 152.0154 - lr: 1.1111e-04 - 115s/epoch - 588ms/step
Epoch 122/1000
2023-10-01 09:12:40.400 
Epoch 122/1000 
	 loss: 151.6633, MinusLogProbMetric: 151.6633, val_loss: 152.1585, val_MinusLogProbMetric: 152.1585

Epoch 122: val_loss did not improve from 145.57336
196/196 - 113s - loss: 151.6633 - MinusLogProbMetric: 151.6633 - val_loss: 152.1585 - val_MinusLogProbMetric: 152.1585 - lr: 1.1111e-04 - 113s/epoch - 578ms/step
Epoch 123/1000
2023-10-01 09:14:31.483 
Epoch 123/1000 
	 loss: 150.4762, MinusLogProbMetric: 150.4762, val_loss: 150.1444, val_MinusLogProbMetric: 150.1444

Epoch 123: val_loss did not improve from 145.57336
196/196 - 111s - loss: 150.4762 - MinusLogProbMetric: 150.4762 - val_loss: 150.1444 - val_MinusLogProbMetric: 150.1444 - lr: 5.5556e-05 - 111s/epoch - 567ms/step
Epoch 124/1000
2023-10-01 09:16:26.676 
Epoch 124/1000 
	 loss: 150.1284, MinusLogProbMetric: 150.1284, val_loss: 149.7495, val_MinusLogProbMetric: 149.7495

Epoch 124: val_loss did not improve from 145.57336
196/196 - 115s - loss: 150.1284 - MinusLogProbMetric: 150.1284 - val_loss: 149.7495 - val_MinusLogProbMetric: 149.7495 - lr: 5.5556e-05 - 115s/epoch - 588ms/step
Epoch 125/1000
2023-10-01 09:18:19.554 
Epoch 125/1000 
	 loss: 149.3503, MinusLogProbMetric: 149.3503, val_loss: 149.0702, val_MinusLogProbMetric: 149.0702

Epoch 125: val_loss did not improve from 145.57336
196/196 - 113s - loss: 149.3503 - MinusLogProbMetric: 149.3503 - val_loss: 149.0702 - val_MinusLogProbMetric: 149.0702 - lr: 5.5556e-05 - 113s/epoch - 576ms/step
Epoch 126/1000
2023-10-01 09:20:15.448 
Epoch 126/1000 
	 loss: 148.7645, MinusLogProbMetric: 148.7645, val_loss: 148.6013, val_MinusLogProbMetric: 148.6013

Epoch 126: val_loss did not improve from 145.57336
196/196 - 116s - loss: 148.7645 - MinusLogProbMetric: 148.7645 - val_loss: 148.6013 - val_MinusLogProbMetric: 148.6013 - lr: 5.5556e-05 - 116s/epoch - 591ms/step
Epoch 127/1000
2023-10-01 09:22:07.283 
Epoch 127/1000 
	 loss: 148.1935, MinusLogProbMetric: 148.1935, val_loss: 148.1236, val_MinusLogProbMetric: 148.1236

Epoch 127: val_loss did not improve from 145.57336
196/196 - 112s - loss: 148.1935 - MinusLogProbMetric: 148.1935 - val_loss: 148.1236 - val_MinusLogProbMetric: 148.1236 - lr: 5.5556e-05 - 112s/epoch - 571ms/step
Epoch 128/1000
2023-10-01 09:23:59.446 
Epoch 128/1000 
	 loss: 147.7128, MinusLogProbMetric: 147.7128, val_loss: 147.4588, val_MinusLogProbMetric: 147.4588

Epoch 128: val_loss did not improve from 145.57336
196/196 - 112s - loss: 147.7128 - MinusLogProbMetric: 147.7128 - val_loss: 147.4588 - val_MinusLogProbMetric: 147.4588 - lr: 5.5556e-05 - 112s/epoch - 572ms/step
Epoch 129/1000
2023-10-01 09:25:51.779 
Epoch 129/1000 
	 loss: 148.0733, MinusLogProbMetric: 148.0733, val_loss: 147.8389, val_MinusLogProbMetric: 147.8389

Epoch 129: val_loss did not improve from 145.57336
196/196 - 112s - loss: 148.0733 - MinusLogProbMetric: 148.0733 - val_loss: 147.8389 - val_MinusLogProbMetric: 147.8389 - lr: 5.5556e-05 - 112s/epoch - 573ms/step
Epoch 130/1000
2023-10-01 09:27:44.587 
Epoch 130/1000 
	 loss: 148.1307, MinusLogProbMetric: 148.1307, val_loss: 146.9374, val_MinusLogProbMetric: 146.9374

Epoch 130: val_loss did not improve from 145.57336
196/196 - 113s - loss: 148.1307 - MinusLogProbMetric: 148.1307 - val_loss: 146.9374 - val_MinusLogProbMetric: 146.9374 - lr: 5.5556e-05 - 113s/epoch - 576ms/step
Epoch 131/1000
2023-10-01 09:29:40.667 
Epoch 131/1000 
	 loss: 146.8353, MinusLogProbMetric: 146.8353, val_loss: 146.8046, val_MinusLogProbMetric: 146.8046

Epoch 131: val_loss did not improve from 145.57336
196/196 - 116s - loss: 146.8353 - MinusLogProbMetric: 146.8353 - val_loss: 146.8046 - val_MinusLogProbMetric: 146.8046 - lr: 5.5556e-05 - 116s/epoch - 592ms/step
Epoch 132/1000
2023-10-01 09:31:32.888 
Epoch 132/1000 
	 loss: 146.4019, MinusLogProbMetric: 146.4019, val_loss: 146.1790, val_MinusLogProbMetric: 146.1790

Epoch 132: val_loss did not improve from 145.57336
196/196 - 112s - loss: 146.4019 - MinusLogProbMetric: 146.4019 - val_loss: 146.1790 - val_MinusLogProbMetric: 146.1790 - lr: 5.5556e-05 - 112s/epoch - 572ms/step
Epoch 133/1000
2023-10-01 09:33:24.646 
Epoch 133/1000 
	 loss: 145.8777, MinusLogProbMetric: 145.8777, val_loss: 145.5473, val_MinusLogProbMetric: 145.5473

Epoch 133: val_loss improved from 145.57336 to 145.54733, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 115s - loss: 145.8777 - MinusLogProbMetric: 145.8777 - val_loss: 145.5473 - val_MinusLogProbMetric: 145.5473 - lr: 5.5556e-05 - 115s/epoch - 587ms/step
Epoch 134/1000
2023-10-01 09:35:20.289 
Epoch 134/1000 
	 loss: 145.0090, MinusLogProbMetric: 145.0090, val_loss: 145.0111, val_MinusLogProbMetric: 145.0111

Epoch 134: val_loss improved from 145.54733 to 145.01109, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 114s - loss: 145.0090 - MinusLogProbMetric: 145.0090 - val_loss: 145.0111 - val_MinusLogProbMetric: 145.0111 - lr: 5.5556e-05 - 114s/epoch - 582ms/step
Epoch 135/1000
2023-10-01 09:37:12.994 
Epoch 135/1000 
	 loss: 144.7272, MinusLogProbMetric: 144.7272, val_loss: 144.5497, val_MinusLogProbMetric: 144.5497

Epoch 135: val_loss improved from 145.01109 to 144.54970, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 115s - loss: 144.7272 - MinusLogProbMetric: 144.7272 - val_loss: 144.5497 - val_MinusLogProbMetric: 144.5497 - lr: 5.5556e-05 - 115s/epoch - 585ms/step
Epoch 136/1000
2023-10-01 09:39:06.543 
Epoch 136/1000 
	 loss: 143.9508, MinusLogProbMetric: 143.9508, val_loss: 143.9392, val_MinusLogProbMetric: 143.9392

Epoch 136: val_loss improved from 144.54970 to 143.93918, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 112s - loss: 143.9508 - MinusLogProbMetric: 143.9508 - val_loss: 143.9392 - val_MinusLogProbMetric: 143.9392 - lr: 5.5556e-05 - 112s/epoch - 572ms/step
Epoch 137/1000
2023-10-01 09:41:01.057 
Epoch 137/1000 
	 loss: 143.5027, MinusLogProbMetric: 143.5027, val_loss: 143.3364, val_MinusLogProbMetric: 143.3364

Epoch 137: val_loss improved from 143.93918 to 143.33638, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 117s - loss: 143.5027 - MinusLogProbMetric: 143.5027 - val_loss: 143.3364 - val_MinusLogProbMetric: 143.3364 - lr: 5.5556e-05 - 117s/epoch - 595ms/step
Epoch 138/1000
2023-10-01 09:42:57.909 
Epoch 138/1000 
	 loss: 142.9416, MinusLogProbMetric: 142.9416, val_loss: 143.2112, val_MinusLogProbMetric: 143.2112

Epoch 138: val_loss improved from 143.33638 to 143.21120, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 114s - loss: 142.9416 - MinusLogProbMetric: 142.9416 - val_loss: 143.2112 - val_MinusLogProbMetric: 143.2112 - lr: 5.5556e-05 - 114s/epoch - 581ms/step
Epoch 139/1000
2023-10-01 09:44:53.919 
Epoch 139/1000 
	 loss: 142.5621, MinusLogProbMetric: 142.5621, val_loss: 142.9425, val_MinusLogProbMetric: 142.9425

Epoch 139: val_loss improved from 143.21120 to 142.94246, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 116s - loss: 142.5621 - MinusLogProbMetric: 142.5621 - val_loss: 142.9425 - val_MinusLogProbMetric: 142.9425 - lr: 5.5556e-05 - 116s/epoch - 593ms/step
Epoch 140/1000
2023-10-01 09:46:50.539 
Epoch 140/1000 
	 loss: 142.0629, MinusLogProbMetric: 142.0629, val_loss: 141.9094, val_MinusLogProbMetric: 141.9094

Epoch 140: val_loss improved from 142.94246 to 141.90939, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 118s - loss: 142.0629 - MinusLogProbMetric: 142.0629 - val_loss: 141.9094 - val_MinusLogProbMetric: 141.9094 - lr: 5.5556e-05 - 118s/epoch - 602ms/step
Epoch 141/1000
2023-10-01 09:48:46.367 
Epoch 141/1000 
	 loss: 141.5032, MinusLogProbMetric: 141.5032, val_loss: 141.4267, val_MinusLogProbMetric: 141.4267

Epoch 141: val_loss improved from 141.90939 to 141.42667, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 114s - loss: 141.5032 - MinusLogProbMetric: 141.5032 - val_loss: 141.4267 - val_MinusLogProbMetric: 141.4267 - lr: 5.5556e-05 - 114s/epoch - 584ms/step
Epoch 142/1000
2023-10-01 09:50:39.322 
Epoch 142/1000 
	 loss: 141.0258, MinusLogProbMetric: 141.0258, val_loss: 140.8902, val_MinusLogProbMetric: 140.8902

Epoch 142: val_loss improved from 141.42667 to 140.89023, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 115s - loss: 141.0258 - MinusLogProbMetric: 141.0258 - val_loss: 140.8902 - val_MinusLogProbMetric: 140.8902 - lr: 5.5556e-05 - 115s/epoch - 588ms/step
Epoch 143/1000
2023-10-01 09:52:34.625 
Epoch 143/1000 
	 loss: 140.6400, MinusLogProbMetric: 140.6400, val_loss: 140.4855, val_MinusLogProbMetric: 140.4855

Epoch 143: val_loss improved from 140.89023 to 140.48552, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 113s - loss: 140.6400 - MinusLogProbMetric: 140.6400 - val_loss: 140.4855 - val_MinusLogProbMetric: 140.4855 - lr: 5.5556e-05 - 113s/epoch - 578ms/step
Epoch 144/1000
2023-10-01 09:54:27.853 
Epoch 144/1000 
	 loss: 140.0988, MinusLogProbMetric: 140.0988, val_loss: 139.8407, val_MinusLogProbMetric: 139.8407

Epoch 144: val_loss improved from 140.48552 to 139.84067, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 115s - loss: 140.0988 - MinusLogProbMetric: 140.0988 - val_loss: 139.8407 - val_MinusLogProbMetric: 139.8407 - lr: 5.5556e-05 - 115s/epoch - 586ms/step
Epoch 145/1000
2023-10-01 09:56:23.109 
Epoch 145/1000 
	 loss: 139.6570, MinusLogProbMetric: 139.6570, val_loss: 139.4897, val_MinusLogProbMetric: 139.4897

Epoch 145: val_loss improved from 139.84067 to 139.48972, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 113s - loss: 139.6570 - MinusLogProbMetric: 139.6570 - val_loss: 139.4897 - val_MinusLogProbMetric: 139.4897 - lr: 5.5556e-05 - 113s/epoch - 577ms/step
Epoch 146/1000
2023-10-01 09:58:14.866 
Epoch 146/1000 
	 loss: 139.2034, MinusLogProbMetric: 139.2034, val_loss: 139.1877, val_MinusLogProbMetric: 139.1877

Epoch 146: val_loss improved from 139.48972 to 139.18770, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 115s - loss: 139.2034 - MinusLogProbMetric: 139.2034 - val_loss: 139.1877 - val_MinusLogProbMetric: 139.1877 - lr: 5.5556e-05 - 115s/epoch - 585ms/step
Epoch 147/1000
2023-10-01 10:00:11.081 
Epoch 147/1000 
	 loss: 138.8252, MinusLogProbMetric: 138.8252, val_loss: 138.7975, val_MinusLogProbMetric: 138.7975

Epoch 147: val_loss improved from 139.18770 to 138.79745, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 115s - loss: 138.8252 - MinusLogProbMetric: 138.8252 - val_loss: 138.7975 - val_MinusLogProbMetric: 138.7975 - lr: 5.5556e-05 - 115s/epoch - 588ms/step
Epoch 148/1000
2023-10-01 10:02:05.348 
Epoch 148/1000 
	 loss: 138.4389, MinusLogProbMetric: 138.4389, val_loss: 140.0002, val_MinusLogProbMetric: 140.0002

Epoch 148: val_loss did not improve from 138.79745
196/196 - 111s - loss: 138.4389 - MinusLogProbMetric: 138.4389 - val_loss: 140.0002 - val_MinusLogProbMetric: 140.0002 - lr: 5.5556e-05 - 111s/epoch - 568ms/step
Epoch 149/1000
2023-10-01 10:03:57.886 
Epoch 149/1000 
	 loss: 138.4192, MinusLogProbMetric: 138.4192, val_loss: 138.1381, val_MinusLogProbMetric: 138.1381

Epoch 149: val_loss improved from 138.79745 to 138.13808, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 114s - loss: 138.4192 - MinusLogProbMetric: 138.4192 - val_loss: 138.1381 - val_MinusLogProbMetric: 138.1381 - lr: 5.5556e-05 - 114s/epoch - 583ms/step
Epoch 150/1000
2023-10-01 10:05:57.677 
Epoch 150/1000 
	 loss: 137.8795, MinusLogProbMetric: 137.8795, val_loss: 137.7019, val_MinusLogProbMetric: 137.7019

Epoch 150: val_loss improved from 138.13808 to 137.70189, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 120s - loss: 137.8795 - MinusLogProbMetric: 137.8795 - val_loss: 137.7019 - val_MinusLogProbMetric: 137.7019 - lr: 5.5556e-05 - 120s/epoch - 610ms/step
Epoch 151/1000
2023-10-01 10:07:52.627 
Epoch 151/1000 
	 loss: 138.1517, MinusLogProbMetric: 138.1517, val_loss: 175.6402, val_MinusLogProbMetric: 175.6402

Epoch 151: val_loss did not improve from 137.70189
196/196 - 113s - loss: 138.1517 - MinusLogProbMetric: 138.1517 - val_loss: 175.6402 - val_MinusLogProbMetric: 175.6402 - lr: 5.5556e-05 - 113s/epoch - 579ms/step
Epoch 152/1000
2023-10-01 10:09:44.815 
Epoch 152/1000 
	 loss: 141.2784, MinusLogProbMetric: 141.2784, val_loss: 137.4307, val_MinusLogProbMetric: 137.4307

Epoch 152: val_loss improved from 137.70189 to 137.43071, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 115s - loss: 141.2784 - MinusLogProbMetric: 141.2784 - val_loss: 137.4307 - val_MinusLogProbMetric: 137.4307 - lr: 5.5556e-05 - 115s/epoch - 586ms/step
Epoch 153/1000
2023-10-01 10:11:41.025 
Epoch 153/1000 
	 loss: 136.8329, MinusLogProbMetric: 136.8329, val_loss: 136.6310, val_MinusLogProbMetric: 136.6310

Epoch 153: val_loss improved from 137.43071 to 136.63103, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 115s - loss: 136.8329 - MinusLogProbMetric: 136.8329 - val_loss: 136.6310 - val_MinusLogProbMetric: 136.6310 - lr: 5.5556e-05 - 115s/epoch - 588ms/step
Epoch 154/1000
2023-10-01 10:13:35.233 
Epoch 154/1000 
	 loss: 136.3757, MinusLogProbMetric: 136.3757, val_loss: 136.3399, val_MinusLogProbMetric: 136.3399

Epoch 154: val_loss improved from 136.63103 to 136.33989, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 115s - loss: 136.3757 - MinusLogProbMetric: 136.3757 - val_loss: 136.3399 - val_MinusLogProbMetric: 136.3399 - lr: 5.5556e-05 - 115s/epoch - 586ms/step
Epoch 155/1000
2023-10-01 10:15:29.734 
Epoch 155/1000 
	 loss: 135.9506, MinusLogProbMetric: 135.9506, val_loss: 135.9049, val_MinusLogProbMetric: 135.9049

Epoch 155: val_loss improved from 136.33989 to 135.90486, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 115s - loss: 135.9506 - MinusLogProbMetric: 135.9506 - val_loss: 135.9049 - val_MinusLogProbMetric: 135.9049 - lr: 5.5556e-05 - 115s/epoch - 585ms/step
Epoch 156/1000
2023-10-01 10:17:25.894 
Epoch 156/1000 
	 loss: 135.5246, MinusLogProbMetric: 135.5246, val_loss: 135.7416, val_MinusLogProbMetric: 135.7416

Epoch 156: val_loss improved from 135.90486 to 135.74158, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 115s - loss: 135.5246 - MinusLogProbMetric: 135.5246 - val_loss: 135.7416 - val_MinusLogProbMetric: 135.7416 - lr: 5.5556e-05 - 115s/epoch - 588ms/step
Epoch 157/1000
2023-10-01 10:19:18.752 
Epoch 157/1000 
	 loss: 135.1519, MinusLogProbMetric: 135.1519, val_loss: 135.1363, val_MinusLogProbMetric: 135.1363

Epoch 157: val_loss improved from 135.74158 to 135.13631, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 115s - loss: 135.1519 - MinusLogProbMetric: 135.1519 - val_loss: 135.1363 - val_MinusLogProbMetric: 135.1363 - lr: 5.5556e-05 - 115s/epoch - 589ms/step
Epoch 158/1000
2023-10-01 10:21:13.744 
Epoch 158/1000 
	 loss: 134.8729, MinusLogProbMetric: 134.8729, val_loss: 134.5911, val_MinusLogProbMetric: 134.5911

Epoch 158: val_loss improved from 135.13631 to 134.59114, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 112s - loss: 134.8729 - MinusLogProbMetric: 134.8729 - val_loss: 134.5911 - val_MinusLogProbMetric: 134.5911 - lr: 5.5556e-05 - 112s/epoch - 573ms/step
Epoch 159/1000
2023-10-01 10:23:06.583 
Epoch 159/1000 
	 loss: 134.3341, MinusLogProbMetric: 134.3341, val_loss: 134.0628, val_MinusLogProbMetric: 134.0628

Epoch 159: val_loss improved from 134.59114 to 134.06279, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 115s - loss: 134.3341 - MinusLogProbMetric: 134.3341 - val_loss: 134.0628 - val_MinusLogProbMetric: 134.0628 - lr: 5.5556e-05 - 115s/epoch - 586ms/step
Epoch 160/1000
2023-10-01 10:25:01.979 
Epoch 160/1000 
	 loss: 134.0065, MinusLogProbMetric: 134.0065, val_loss: 134.4962, val_MinusLogProbMetric: 134.4962

Epoch 160: val_loss did not improve from 134.06279
196/196 - 112s - loss: 134.0065 - MinusLogProbMetric: 134.0065 - val_loss: 134.4962 - val_MinusLogProbMetric: 134.4962 - lr: 5.5556e-05 - 112s/epoch - 570ms/step
Epoch 161/1000
2023-10-01 10:26:52.894 
Epoch 161/1000 
	 loss: 133.8980, MinusLogProbMetric: 133.8980, val_loss: 133.7401, val_MinusLogProbMetric: 133.7401

Epoch 161: val_loss improved from 134.06279 to 133.74010, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 113s - loss: 133.8980 - MinusLogProbMetric: 133.8980 - val_loss: 133.7401 - val_MinusLogProbMetric: 133.7401 - lr: 5.5556e-05 - 113s/epoch - 575ms/step
Epoch 162/1000
2023-10-01 10:28:45.351 
Epoch 162/1000 
	 loss: 133.4664, MinusLogProbMetric: 133.4664, val_loss: 133.2599, val_MinusLogProbMetric: 133.2599

Epoch 162: val_loss improved from 133.74010 to 133.25987, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 114s - loss: 133.4664 - MinusLogProbMetric: 133.4664 - val_loss: 133.2599 - val_MinusLogProbMetric: 133.2599 - lr: 5.5556e-05 - 114s/epoch - 582ms/step
Epoch 163/1000
2023-10-01 10:30:38.447 
Epoch 163/1000 
	 loss: 133.2488, MinusLogProbMetric: 133.2488, val_loss: 133.2396, val_MinusLogProbMetric: 133.2396

Epoch 163: val_loss improved from 133.25987 to 133.23962, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 111s - loss: 133.2488 - MinusLogProbMetric: 133.2488 - val_loss: 133.2396 - val_MinusLogProbMetric: 133.2396 - lr: 5.5556e-05 - 111s/epoch - 567ms/step
Epoch 164/1000
2023-10-01 10:32:30.050 
Epoch 164/1000 
	 loss: 132.8688, MinusLogProbMetric: 132.8688, val_loss: 132.6817, val_MinusLogProbMetric: 132.6817

Epoch 164: val_loss improved from 133.23962 to 132.68169, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 114s - loss: 132.8688 - MinusLogProbMetric: 132.8688 - val_loss: 132.6817 - val_MinusLogProbMetric: 132.6817 - lr: 5.5556e-05 - 114s/epoch - 583ms/step
Epoch 165/1000
2023-10-01 10:34:24.432 
Epoch 165/1000 
	 loss: 132.5387, MinusLogProbMetric: 132.5387, val_loss: 132.7939, val_MinusLogProbMetric: 132.7939

Epoch 165: val_loss did not improve from 132.68169
196/196 - 110s - loss: 132.5387 - MinusLogProbMetric: 132.5387 - val_loss: 132.7939 - val_MinusLogProbMetric: 132.7939 - lr: 5.5556e-05 - 110s/epoch - 564ms/step
Epoch 166/1000
2023-10-01 10:36:15.944 
Epoch 166/1000 
	 loss: 132.2474, MinusLogProbMetric: 132.2474, val_loss: 132.1363, val_MinusLogProbMetric: 132.1363

Epoch 166: val_loss improved from 132.68169 to 132.13626, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 113s - loss: 132.2474 - MinusLogProbMetric: 132.2474 - val_loss: 132.1363 - val_MinusLogProbMetric: 132.1363 - lr: 5.5556e-05 - 113s/epoch - 578ms/step
Epoch 167/1000
2023-10-01 10:38:08.044 
Epoch 167/1000 
	 loss: 131.7561, MinusLogProbMetric: 131.7561, val_loss: 131.7658, val_MinusLogProbMetric: 131.7658

Epoch 167: val_loss improved from 132.13626 to 131.76579, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 112s - loss: 131.7561 - MinusLogProbMetric: 131.7561 - val_loss: 131.7658 - val_MinusLogProbMetric: 131.7658 - lr: 5.5556e-05 - 112s/epoch - 570ms/step
Epoch 168/1000
2023-10-01 10:40:04.600 
Epoch 168/1000 
	 loss: 131.7844, MinusLogProbMetric: 131.7844, val_loss: 131.6556, val_MinusLogProbMetric: 131.6556

Epoch 168: val_loss improved from 131.76579 to 131.65562, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 117s - loss: 131.7844 - MinusLogProbMetric: 131.7844 - val_loss: 131.6556 - val_MinusLogProbMetric: 131.6556 - lr: 5.5556e-05 - 117s/epoch - 596ms/step
Epoch 169/1000
2023-10-01 10:41:56.599 
Epoch 169/1000 
	 loss: 131.2784, MinusLogProbMetric: 131.2784, val_loss: 131.1561, val_MinusLogProbMetric: 131.1561

Epoch 169: val_loss improved from 131.65562 to 131.15614, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 114s - loss: 131.2784 - MinusLogProbMetric: 131.2784 - val_loss: 131.1561 - val_MinusLogProbMetric: 131.1561 - lr: 5.5556e-05 - 114s/epoch - 583ms/step
Epoch 170/1000
2023-10-01 10:43:52.121 
Epoch 170/1000 
	 loss: 131.1084, MinusLogProbMetric: 131.1084, val_loss: 131.4569, val_MinusLogProbMetric: 131.4569

Epoch 170: val_loss did not improve from 131.15614
196/196 - 112s - loss: 131.1084 - MinusLogProbMetric: 131.1084 - val_loss: 131.4569 - val_MinusLogProbMetric: 131.4569 - lr: 5.5556e-05 - 112s/epoch - 569ms/step
Epoch 171/1000
2023-10-01 10:45:42.162 
Epoch 171/1000 
	 loss: 130.7929, MinusLogProbMetric: 130.7929, val_loss: 131.1578, val_MinusLogProbMetric: 131.1578

Epoch 171: val_loss did not improve from 131.15614
196/196 - 110s - loss: 130.7929 - MinusLogProbMetric: 130.7929 - val_loss: 131.1578 - val_MinusLogProbMetric: 131.1578 - lr: 5.5556e-05 - 110s/epoch - 561ms/step
Epoch 172/1000
2023-10-01 10:47:33.541 
Epoch 172/1000 
	 loss: 131.0650, MinusLogProbMetric: 131.0650, val_loss: 131.0615, val_MinusLogProbMetric: 131.0615

Epoch 172: val_loss improved from 131.15614 to 131.06154, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 112s - loss: 131.0650 - MinusLogProbMetric: 131.0650 - val_loss: 131.0615 - val_MinusLogProbMetric: 131.0615 - lr: 5.5556e-05 - 112s/epoch - 573ms/step
Epoch 173/1000
2023-10-01 10:49:25.751 
Epoch 173/1000 
	 loss: 130.6758, MinusLogProbMetric: 130.6758, val_loss: 130.8153, val_MinusLogProbMetric: 130.8153

Epoch 173: val_loss improved from 131.06154 to 130.81528, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 113s - loss: 130.6758 - MinusLogProbMetric: 130.6758 - val_loss: 130.8153 - val_MinusLogProbMetric: 130.8153 - lr: 5.5556e-05 - 113s/epoch - 575ms/step
Epoch 174/1000
2023-10-01 10:51:17.480 
Epoch 174/1000 
	 loss: 130.5840, MinusLogProbMetric: 130.5840, val_loss: 130.0930, val_MinusLogProbMetric: 130.0930

Epoch 174: val_loss improved from 130.81528 to 130.09303, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 114s - loss: 130.5840 - MinusLogProbMetric: 130.5840 - val_loss: 130.0930 - val_MinusLogProbMetric: 130.0930 - lr: 5.5556e-05 - 114s/epoch - 582ms/step
Epoch 175/1000
2023-10-01 10:53:11.852 
Epoch 175/1000 
	 loss: 129.6793, MinusLogProbMetric: 129.6793, val_loss: 130.4818, val_MinusLogProbMetric: 130.4818

Epoch 175: val_loss did not improve from 130.09303
196/196 - 110s - loss: 129.6793 - MinusLogProbMetric: 129.6793 - val_loss: 130.4818 - val_MinusLogProbMetric: 130.4818 - lr: 5.5556e-05 - 110s/epoch - 564ms/step
Epoch 176/1000
2023-10-01 10:55:02.147 
Epoch 176/1000 
	 loss: 129.3999, MinusLogProbMetric: 129.3999, val_loss: 129.6388, val_MinusLogProbMetric: 129.6388

Epoch 176: val_loss improved from 130.09303 to 129.63878, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 112s - loss: 129.3999 - MinusLogProbMetric: 129.3999 - val_loss: 129.6388 - val_MinusLogProbMetric: 129.6388 - lr: 5.5556e-05 - 112s/epoch - 570ms/step
Epoch 177/1000
2023-10-01 10:56:54.284 
Epoch 177/1000 
	 loss: 129.3670, MinusLogProbMetric: 129.3670, val_loss: 129.3377, val_MinusLogProbMetric: 129.3377

Epoch 177: val_loss improved from 129.63878 to 129.33766, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 112s - loss: 129.3670 - MinusLogProbMetric: 129.3670 - val_loss: 129.3377 - val_MinusLogProbMetric: 129.3377 - lr: 5.5556e-05 - 112s/epoch - 572ms/step
Epoch 178/1000
2023-10-01 10:58:49.921 
Epoch 178/1000 
	 loss: 128.8634, MinusLogProbMetric: 128.8634, val_loss: 128.8914, val_MinusLogProbMetric: 128.8914

Epoch 178: val_loss improved from 129.33766 to 128.89145, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 116s - loss: 128.8634 - MinusLogProbMetric: 128.8634 - val_loss: 128.8914 - val_MinusLogProbMetric: 128.8914 - lr: 5.5556e-05 - 116s/epoch - 592ms/step
Epoch 179/1000
2023-10-01 11:00:42.957 
Epoch 179/1000 
	 loss: 129.5910, MinusLogProbMetric: 129.5910, val_loss: 128.5569, val_MinusLogProbMetric: 128.5569

Epoch 179: val_loss improved from 128.89145 to 128.55688, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 114s - loss: 129.5910 - MinusLogProbMetric: 129.5910 - val_loss: 128.5569 - val_MinusLogProbMetric: 128.5569 - lr: 5.5556e-05 - 114s/epoch - 583ms/step
Epoch 180/1000
2023-10-01 11:02:33.969 
Epoch 180/1000 
	 loss: 128.5772, MinusLogProbMetric: 128.5772, val_loss: 128.3283, val_MinusLogProbMetric: 128.3283

Epoch 180: val_loss improved from 128.55688 to 128.32828, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 112s - loss: 128.5772 - MinusLogProbMetric: 128.5772 - val_loss: 128.3283 - val_MinusLogProbMetric: 128.3283 - lr: 5.5556e-05 - 112s/epoch - 572ms/step
Epoch 181/1000
2023-10-01 11:04:28.998 
Epoch 181/1000 
	 loss: 137.0946, MinusLogProbMetric: 137.0946, val_loss: 128.5216, val_MinusLogProbMetric: 128.5216

Epoch 181: val_loss did not improve from 128.32828
196/196 - 111s - loss: 137.0946 - MinusLogProbMetric: 137.0946 - val_loss: 128.5216 - val_MinusLogProbMetric: 128.5216 - lr: 5.5556e-05 - 111s/epoch - 565ms/step
Epoch 182/1000
2023-10-01 11:06:19.675 
Epoch 182/1000 
	 loss: 129.1725, MinusLogProbMetric: 129.1725, val_loss: 129.8849, val_MinusLogProbMetric: 129.8849

Epoch 182: val_loss did not improve from 128.32828
196/196 - 111s - loss: 129.1725 - MinusLogProbMetric: 129.1725 - val_loss: 129.8849 - val_MinusLogProbMetric: 129.8849 - lr: 5.5556e-05 - 111s/epoch - 565ms/step
Epoch 183/1000
2023-10-01 11:08:10.323 
Epoch 183/1000 
	 loss: 129.0523, MinusLogProbMetric: 129.0523, val_loss: 128.6796, val_MinusLogProbMetric: 128.6796

Epoch 183: val_loss did not improve from 128.32828
196/196 - 111s - loss: 129.0523 - MinusLogProbMetric: 129.0523 - val_loss: 128.6796 - val_MinusLogProbMetric: 128.6796 - lr: 5.5556e-05 - 111s/epoch - 564ms/step
Epoch 184/1000
2023-10-01 11:09:59.831 
Epoch 184/1000 
	 loss: 128.2943, MinusLogProbMetric: 128.2943, val_loss: 128.1416, val_MinusLogProbMetric: 128.1416

Epoch 184: val_loss improved from 128.32828 to 128.14165, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 111s - loss: 128.2943 - MinusLogProbMetric: 128.2943 - val_loss: 128.1416 - val_MinusLogProbMetric: 128.1416 - lr: 5.5556e-05 - 111s/epoch - 565ms/step
Epoch 185/1000
2023-10-01 11:11:51.380 
Epoch 185/1000 
	 loss: 127.6509, MinusLogProbMetric: 127.6509, val_loss: 127.9160, val_MinusLogProbMetric: 127.9160

Epoch 185: val_loss improved from 128.14165 to 127.91602, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 115s - loss: 127.6509 - MinusLogProbMetric: 127.6509 - val_loss: 127.9160 - val_MinusLogProbMetric: 127.9160 - lr: 5.5556e-05 - 115s/epoch - 584ms/step
Epoch 186/1000
2023-10-01 11:13:47.095 
Epoch 186/1000 
	 loss: 127.2827, MinusLogProbMetric: 127.2827, val_loss: 127.2035, val_MinusLogProbMetric: 127.2035

Epoch 186: val_loss improved from 127.91602 to 127.20348, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 113s - loss: 127.2827 - MinusLogProbMetric: 127.2827 - val_loss: 127.2035 - val_MinusLogProbMetric: 127.2035 - lr: 5.5556e-05 - 113s/epoch - 577ms/step
Epoch 187/1000
2023-10-01 11:15:40.869 
Epoch 187/1000 
	 loss: 126.7747, MinusLogProbMetric: 126.7747, val_loss: 127.6185, val_MinusLogProbMetric: 127.6185

Epoch 187: val_loss did not improve from 127.20348
196/196 - 112s - loss: 126.7747 - MinusLogProbMetric: 126.7747 - val_loss: 127.6185 - val_MinusLogProbMetric: 127.6185 - lr: 5.5556e-05 - 112s/epoch - 572ms/step
Epoch 188/1000
2023-10-01 11:17:33.248 
Epoch 188/1000 
	 loss: 127.0811, MinusLogProbMetric: 127.0811, val_loss: 126.9051, val_MinusLogProbMetric: 126.9051

Epoch 188: val_loss improved from 127.20348 to 126.90505, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 114s - loss: 127.0811 - MinusLogProbMetric: 127.0811 - val_loss: 126.9051 - val_MinusLogProbMetric: 126.9051 - lr: 5.5556e-05 - 114s/epoch - 582ms/step
Epoch 189/1000
2023-10-01 11:19:24.733 
Epoch 189/1000 
	 loss: 126.6898, MinusLogProbMetric: 126.6898, val_loss: 126.5693, val_MinusLogProbMetric: 126.5693

Epoch 189: val_loss improved from 126.90505 to 126.56927, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 111s - loss: 126.6898 - MinusLogProbMetric: 126.6898 - val_loss: 126.5693 - val_MinusLogProbMetric: 126.5693 - lr: 5.5556e-05 - 111s/epoch - 568ms/step
Epoch 190/1000
2023-10-01 11:21:16.401 
Epoch 190/1000 
	 loss: 126.3127, MinusLogProbMetric: 126.3127, val_loss: 126.1561, val_MinusLogProbMetric: 126.1561

Epoch 190: val_loss improved from 126.56927 to 126.15607, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 113s - loss: 126.3127 - MinusLogProbMetric: 126.3127 - val_loss: 126.1561 - val_MinusLogProbMetric: 126.1561 - lr: 5.5556e-05 - 113s/epoch - 579ms/step
Epoch 191/1000
2023-10-01 11:23:11.769 
Epoch 191/1000 
	 loss: 127.2385, MinusLogProbMetric: 127.2385, val_loss: 126.1294, val_MinusLogProbMetric: 126.1294

Epoch 191: val_loss improved from 126.15607 to 126.12936, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 113s - loss: 127.2385 - MinusLogProbMetric: 127.2385 - val_loss: 126.1294 - val_MinusLogProbMetric: 126.1294 - lr: 5.5556e-05 - 113s/epoch - 578ms/step
Epoch 192/1000
2023-10-01 11:25:06.832 
Epoch 192/1000 
	 loss: 125.8785, MinusLogProbMetric: 125.8785, val_loss: 126.6593, val_MinusLogProbMetric: 126.6593

Epoch 192: val_loss did not improve from 126.12936
196/196 - 114s - loss: 125.8785 - MinusLogProbMetric: 125.8785 - val_loss: 126.6593 - val_MinusLogProbMetric: 126.6593 - lr: 5.5556e-05 - 114s/epoch - 581ms/step
Epoch 193/1000
2023-10-01 11:26:58.544 
Epoch 193/1000 
	 loss: 125.7630, MinusLogProbMetric: 125.7630, val_loss: 125.5023, val_MinusLogProbMetric: 125.5023

Epoch 193: val_loss improved from 126.12936 to 125.50234, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 114s - loss: 125.7630 - MinusLogProbMetric: 125.7630 - val_loss: 125.5023 - val_MinusLogProbMetric: 125.5023 - lr: 5.5556e-05 - 114s/epoch - 580ms/step
Epoch 194/1000
2023-10-01 11:28:51.559 
Epoch 194/1000 
	 loss: 125.2600, MinusLogProbMetric: 125.2600, val_loss: 125.3556, val_MinusLogProbMetric: 125.3556

Epoch 194: val_loss improved from 125.50234 to 125.35564, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 115s - loss: 125.2600 - MinusLogProbMetric: 125.2600 - val_loss: 125.3556 - val_MinusLogProbMetric: 125.3556 - lr: 5.5556e-05 - 115s/epoch - 587ms/step
Epoch 195/1000
2023-10-01 11:30:47.460 
Epoch 195/1000 
	 loss: 125.1494, MinusLogProbMetric: 125.1494, val_loss: 125.1331, val_MinusLogProbMetric: 125.1331

Epoch 195: val_loss improved from 125.35564 to 125.13307, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 114s - loss: 125.1494 - MinusLogProbMetric: 125.1494 - val_loss: 125.1331 - val_MinusLogProbMetric: 125.1331 - lr: 5.5556e-05 - 114s/epoch - 580ms/step
Epoch 196/1000
2023-10-01 11:32:39.277 
Epoch 196/1000 
	 loss: 124.6230, MinusLogProbMetric: 124.6230, val_loss: 125.1291, val_MinusLogProbMetric: 125.1291

Epoch 196: val_loss improved from 125.13307 to 125.12907, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 113s - loss: 124.6230 - MinusLogProbMetric: 124.6230 - val_loss: 125.1291 - val_MinusLogProbMetric: 125.1291 - lr: 5.5556e-05 - 113s/epoch - 579ms/step
Epoch 197/1000
2023-10-01 11:34:34.474 
Epoch 197/1000 
	 loss: 124.3704, MinusLogProbMetric: 124.3704, val_loss: 124.2398, val_MinusLogProbMetric: 124.2398

Epoch 197: val_loss improved from 125.12907 to 124.23985, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 113s - loss: 124.3704 - MinusLogProbMetric: 124.3704 - val_loss: 124.2398 - val_MinusLogProbMetric: 124.2398 - lr: 5.5556e-05 - 113s/epoch - 578ms/step
Epoch 198/1000
2023-10-01 11:36:26.231 
Epoch 198/1000 
	 loss: 124.1377, MinusLogProbMetric: 124.1377, val_loss: 124.1902, val_MinusLogProbMetric: 124.1902

Epoch 198: val_loss improved from 124.23985 to 124.19019, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 114s - loss: 124.1377 - MinusLogProbMetric: 124.1377 - val_loss: 124.1902 - val_MinusLogProbMetric: 124.1902 - lr: 5.5556e-05 - 114s/epoch - 583ms/step
Epoch 199/1000
2023-10-01 11:38:21.645 
Epoch 199/1000 
	 loss: 123.7729, MinusLogProbMetric: 123.7729, val_loss: 123.8635, val_MinusLogProbMetric: 123.8635

Epoch 199: val_loss improved from 124.19019 to 123.86349, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 113s - loss: 123.7729 - MinusLogProbMetric: 123.7729 - val_loss: 123.8635 - val_MinusLogProbMetric: 123.8635 - lr: 5.5556e-05 - 113s/epoch - 577ms/step
Epoch 200/1000
2023-10-01 11:40:13.535 
Epoch 200/1000 
	 loss: 148.7189, MinusLogProbMetric: 148.7189, val_loss: 142.0966, val_MinusLogProbMetric: 142.0966

Epoch 200: val_loss did not improve from 123.86349
196/196 - 110s - loss: 148.7189 - MinusLogProbMetric: 148.7189 - val_loss: 142.0966 - val_MinusLogProbMetric: 142.0966 - lr: 5.5556e-05 - 110s/epoch - 563ms/step
Epoch 201/1000
2023-10-01 11:42:04.086 
Epoch 201/1000 
	 loss: 136.8684, MinusLogProbMetric: 136.8684, val_loss: 133.4904, val_MinusLogProbMetric: 133.4904

Epoch 201: val_loss did not improve from 123.86349
196/196 - 111s - loss: 136.8684 - MinusLogProbMetric: 136.8684 - val_loss: 133.4904 - val_MinusLogProbMetric: 133.4904 - lr: 5.5556e-05 - 111s/epoch - 564ms/step
Epoch 202/1000
2023-10-01 11:43:56.209 
Epoch 202/1000 
	 loss: 132.0287, MinusLogProbMetric: 132.0287, val_loss: 131.4079, val_MinusLogProbMetric: 131.4079

Epoch 202: val_loss did not improve from 123.86349
196/196 - 112s - loss: 132.0287 - MinusLogProbMetric: 132.0287 - val_loss: 131.4079 - val_MinusLogProbMetric: 131.4079 - lr: 5.5556e-05 - 112s/epoch - 572ms/step
Epoch 203/1000
2023-10-01 11:45:49.788 
Epoch 203/1000 
	 loss: 129.8958, MinusLogProbMetric: 129.8958, val_loss: 128.8196, val_MinusLogProbMetric: 128.8196

Epoch 203: val_loss did not improve from 123.86349
196/196 - 114s - loss: 129.8958 - MinusLogProbMetric: 129.8958 - val_loss: 128.8196 - val_MinusLogProbMetric: 128.8196 - lr: 5.5556e-05 - 114s/epoch - 579ms/step
Epoch 204/1000
2023-10-01 11:47:37.493 
Epoch 204/1000 
	 loss: 128.2811, MinusLogProbMetric: 128.2811, val_loss: 127.8173, val_MinusLogProbMetric: 127.8173

Epoch 204: val_loss did not improve from 123.86349
196/196 - 108s - loss: 128.2811 - MinusLogProbMetric: 128.2811 - val_loss: 127.8173 - val_MinusLogProbMetric: 127.8173 - lr: 5.5556e-05 - 108s/epoch - 550ms/step
Epoch 205/1000
2023-10-01 11:49:28.453 
Epoch 205/1000 
	 loss: 127.3105, MinusLogProbMetric: 127.3105, val_loss: 126.9723, val_MinusLogProbMetric: 126.9723

Epoch 205: val_loss did not improve from 123.86349
196/196 - 111s - loss: 127.3105 - MinusLogProbMetric: 127.3105 - val_loss: 126.9723 - val_MinusLogProbMetric: 126.9723 - lr: 5.5556e-05 - 111s/epoch - 566ms/step
Epoch 206/1000
2023-10-01 11:51:20.570 
Epoch 206/1000 
	 loss: 126.6206, MinusLogProbMetric: 126.6206, val_loss: 126.0478, val_MinusLogProbMetric: 126.0478

Epoch 206: val_loss did not improve from 123.86349
196/196 - 112s - loss: 126.6206 - MinusLogProbMetric: 126.6206 - val_loss: 126.0478 - val_MinusLogProbMetric: 126.0478 - lr: 5.5556e-05 - 112s/epoch - 572ms/step
Epoch 207/1000
2023-10-01 11:53:09.933 
Epoch 207/1000 
	 loss: 125.7510, MinusLogProbMetric: 125.7510, val_loss: 125.3862, val_MinusLogProbMetric: 125.3862

Epoch 207: val_loss did not improve from 123.86349
196/196 - 109s - loss: 125.7510 - MinusLogProbMetric: 125.7510 - val_loss: 125.3862 - val_MinusLogProbMetric: 125.3862 - lr: 5.5556e-05 - 109s/epoch - 558ms/step
Epoch 208/1000
2023-10-01 11:55:01.046 
Epoch 208/1000 
	 loss: 124.8361, MinusLogProbMetric: 124.8361, val_loss: 124.8282, val_MinusLogProbMetric: 124.8282

Epoch 208: val_loss did not improve from 123.86349
196/196 - 111s - loss: 124.8361 - MinusLogProbMetric: 124.8361 - val_loss: 124.8282 - val_MinusLogProbMetric: 124.8282 - lr: 5.5556e-05 - 111s/epoch - 567ms/step
Epoch 209/1000
2023-10-01 11:57:03.925 
Epoch 209/1000 
	 loss: 124.5774, MinusLogProbMetric: 124.5774, val_loss: 124.4217, val_MinusLogProbMetric: 124.4217

Epoch 209: val_loss did not improve from 123.86349
196/196 - 123s - loss: 124.5774 - MinusLogProbMetric: 124.5774 - val_loss: 124.4217 - val_MinusLogProbMetric: 124.4217 - lr: 5.5556e-05 - 123s/epoch - 627ms/step
Epoch 210/1000
2023-10-01 12:00:18.893 
Epoch 210/1000 
	 loss: 134.7164, MinusLogProbMetric: 134.7164, val_loss: 133.1954, val_MinusLogProbMetric: 133.1954

Epoch 210: val_loss did not improve from 123.86349
196/196 - 195s - loss: 134.7164 - MinusLogProbMetric: 134.7164 - val_loss: 133.1954 - val_MinusLogProbMetric: 133.1954 - lr: 5.5556e-05 - 195s/epoch - 995ms/step
Epoch 211/1000
2023-10-01 12:03:37.731 
Epoch 211/1000 
	 loss: 150.7912, MinusLogProbMetric: 150.7912, val_loss: 149.0167, val_MinusLogProbMetric: 149.0167

Epoch 211: val_loss did not improve from 123.86349
196/196 - 199s - loss: 150.7912 - MinusLogProbMetric: 150.7912 - val_loss: 149.0167 - val_MinusLogProbMetric: 149.0167 - lr: 5.5556e-05 - 199s/epoch - 1s/step
Epoch 212/1000
2023-10-01 12:06:46.653 
Epoch 212/1000 
	 loss: 134.3546, MinusLogProbMetric: 134.3546, val_loss: 124.9342, val_MinusLogProbMetric: 124.9342

Epoch 212: val_loss did not improve from 123.86349
196/196 - 189s - loss: 134.3546 - MinusLogProbMetric: 134.3546 - val_loss: 124.9342 - val_MinusLogProbMetric: 124.9342 - lr: 5.5556e-05 - 189s/epoch - 964ms/step
Epoch 213/1000
2023-10-01 12:09:34.459 
Epoch 213/1000 
	 loss: 122.0462, MinusLogProbMetric: 122.0462, val_loss: 120.1648, val_MinusLogProbMetric: 120.1648

Epoch 213: val_loss improved from 123.86349 to 120.16477, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 170s - loss: 122.0462 - MinusLogProbMetric: 122.0462 - val_loss: 120.1648 - val_MinusLogProbMetric: 120.1648 - lr: 5.5556e-05 - 170s/epoch - 870ms/step
Epoch 214/1000
2023-10-01 12:12:37.802 
Epoch 214/1000 
	 loss: 117.8819, MinusLogProbMetric: 117.8819, val_loss: 114.4113, val_MinusLogProbMetric: 114.4113

Epoch 214: val_loss improved from 120.16477 to 114.41131, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 185s - loss: 117.8819 - MinusLogProbMetric: 117.8819 - val_loss: 114.4113 - val_MinusLogProbMetric: 114.4113 - lr: 5.5556e-05 - 185s/epoch - 942ms/step
Epoch 215/1000
2023-10-01 12:15:42.103 
Epoch 215/1000 
	 loss: 112.4441, MinusLogProbMetric: 112.4441, val_loss: 110.3235, val_MinusLogProbMetric: 110.3235

Epoch 215: val_loss improved from 114.41131 to 110.32346, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 184s - loss: 112.4441 - MinusLogProbMetric: 112.4441 - val_loss: 110.3235 - val_MinusLogProbMetric: 110.3235 - lr: 5.5556e-05 - 184s/epoch - 939ms/step
Epoch 216/1000
2023-10-01 12:18:48.945 
Epoch 216/1000 
	 loss: 108.5356, MinusLogProbMetric: 108.5356, val_loss: 107.4464, val_MinusLogProbMetric: 107.4464

Epoch 216: val_loss improved from 110.32346 to 107.44637, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 185s - loss: 108.5356 - MinusLogProbMetric: 108.5356 - val_loss: 107.4464 - val_MinusLogProbMetric: 107.4464 - lr: 5.5556e-05 - 185s/epoch - 943ms/step
Epoch 217/1000
2023-10-01 12:21:51.591 
Epoch 217/1000 
	 loss: 106.8247, MinusLogProbMetric: 106.8247, val_loss: 106.6045, val_MinusLogProbMetric: 106.6045

Epoch 217: val_loss improved from 107.44637 to 106.60450, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 184s - loss: 106.8247 - MinusLogProbMetric: 106.8247 - val_loss: 106.6045 - val_MinusLogProbMetric: 106.6045 - lr: 5.5556e-05 - 184s/epoch - 941ms/step
Epoch 218/1000
2023-10-01 12:24:49.975 
Epoch 218/1000 
	 loss: 105.6159, MinusLogProbMetric: 105.6159, val_loss: 104.8935, val_MinusLogProbMetric: 104.8935

Epoch 218: val_loss improved from 106.60450 to 104.89353, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 178s - loss: 105.6159 - MinusLogProbMetric: 105.6159 - val_loss: 104.8935 - val_MinusLogProbMetric: 104.8935 - lr: 5.5556e-05 - 178s/epoch - 907ms/step
Epoch 219/1000
2023-10-01 12:27:57.722 
Epoch 219/1000 
	 loss: 103.8749, MinusLogProbMetric: 103.8749, val_loss: 101.2774, val_MinusLogProbMetric: 101.2774

Epoch 219: val_loss improved from 104.89353 to 101.27740, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 188s - loss: 103.8749 - MinusLogProbMetric: 103.8749 - val_loss: 101.2774 - val_MinusLogProbMetric: 101.2774 - lr: 5.5556e-05 - 188s/epoch - 958ms/step
Epoch 220/1000
2023-10-01 12:31:00.728 
Epoch 220/1000 
	 loss: 100.2310, MinusLogProbMetric: 100.2310, val_loss: 102.0974, val_MinusLogProbMetric: 102.0974

Epoch 220: val_loss did not improve from 101.27740
196/196 - 180s - loss: 100.2310 - MinusLogProbMetric: 100.2310 - val_loss: 102.0974 - val_MinusLogProbMetric: 102.0974 - lr: 5.5556e-05 - 180s/epoch - 919ms/step
Epoch 221/1000
2023-10-01 12:34:10.053 
Epoch 221/1000 
	 loss: 110.9394, MinusLogProbMetric: 110.9394, val_loss: 106.3069, val_MinusLogProbMetric: 106.3069

Epoch 221: val_loss did not improve from 101.27740
196/196 - 189s - loss: 110.9394 - MinusLogProbMetric: 110.9394 - val_loss: 106.3069 - val_MinusLogProbMetric: 106.3069 - lr: 5.5556e-05 - 189s/epoch - 966ms/step
Epoch 222/1000
2023-10-01 12:37:06.064 
Epoch 222/1000 
	 loss: 104.0680, MinusLogProbMetric: 104.0680, val_loss: 102.8093, val_MinusLogProbMetric: 102.8093

Epoch 222: val_loss did not improve from 101.27740
196/196 - 176s - loss: 104.0680 - MinusLogProbMetric: 104.0680 - val_loss: 102.8093 - val_MinusLogProbMetric: 102.8093 - lr: 5.5556e-05 - 176s/epoch - 898ms/step
Epoch 223/1000
2023-10-01 12:40:14.648 
Epoch 223/1000 
	 loss: 101.6390, MinusLogProbMetric: 101.6390, val_loss: 100.8018, val_MinusLogProbMetric: 100.8018

Epoch 223: val_loss improved from 101.27740 to 100.80185, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 191s - loss: 101.6390 - MinusLogProbMetric: 101.6390 - val_loss: 100.8018 - val_MinusLogProbMetric: 100.8018 - lr: 5.5556e-05 - 191s/epoch - 975ms/step
Epoch 224/1000
2023-10-01 12:43:16.573 
Epoch 224/1000 
	 loss: 99.8525, MinusLogProbMetric: 99.8525, val_loss: 99.1279, val_MinusLogProbMetric: 99.1279

Epoch 224: val_loss improved from 100.80185 to 99.12790, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 184s - loss: 99.8525 - MinusLogProbMetric: 99.8525 - val_loss: 99.1279 - val_MinusLogProbMetric: 99.1279 - lr: 5.5556e-05 - 184s/epoch - 939ms/step
Epoch 225/1000
2023-10-01 12:46:28.130 
Epoch 225/1000 
	 loss: 98.4340, MinusLogProbMetric: 98.4340, val_loss: 98.1146, val_MinusLogProbMetric: 98.1146

Epoch 225: val_loss improved from 99.12790 to 98.11461, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 189s - loss: 98.4340 - MinusLogProbMetric: 98.4340 - val_loss: 98.1146 - val_MinusLogProbMetric: 98.1146 - lr: 5.5556e-05 - 189s/epoch - 967ms/step
Epoch 226/1000
2023-10-01 12:49:26.601 
Epoch 226/1000 
	 loss: 99.5937, MinusLogProbMetric: 99.5937, val_loss: 97.6979, val_MinusLogProbMetric: 97.6979

Epoch 226: val_loss improved from 98.11461 to 97.69792, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 179s - loss: 99.5937 - MinusLogProbMetric: 99.5937 - val_loss: 97.6979 - val_MinusLogProbMetric: 97.6979 - lr: 5.5556e-05 - 179s/epoch - 912ms/step
Epoch 227/1000
2023-10-01 12:52:18.204 
Epoch 227/1000 
	 loss: 97.3035, MinusLogProbMetric: 97.3035, val_loss: 96.6586, val_MinusLogProbMetric: 96.6586

Epoch 227: val_loss improved from 97.69792 to 96.65856, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 171s - loss: 97.3035 - MinusLogProbMetric: 97.3035 - val_loss: 96.6586 - val_MinusLogProbMetric: 96.6586 - lr: 5.5556e-05 - 171s/epoch - 875ms/step
Epoch 228/1000
2023-10-01 12:55:18.383 
Epoch 228/1000 
	 loss: 98.6815, MinusLogProbMetric: 98.6815, val_loss: 97.8549, val_MinusLogProbMetric: 97.8549

Epoch 228: val_loss did not improve from 96.65856
196/196 - 178s - loss: 98.6815 - MinusLogProbMetric: 98.6815 - val_loss: 97.8549 - val_MinusLogProbMetric: 97.8549 - lr: 5.5556e-05 - 178s/epoch - 906ms/step
Epoch 229/1000
2023-10-01 12:58:13.646 
Epoch 229/1000 
	 loss: 96.9534, MinusLogProbMetric: 96.9534, val_loss: 96.3593, val_MinusLogProbMetric: 96.3593

Epoch 229: val_loss improved from 96.65856 to 96.35931, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 178s - loss: 96.9534 - MinusLogProbMetric: 96.9534 - val_loss: 96.3593 - val_MinusLogProbMetric: 96.3593 - lr: 5.5556e-05 - 178s/epoch - 909ms/step
Epoch 230/1000
2023-10-01 13:01:05.939 
Epoch 230/1000 
	 loss: 95.6820, MinusLogProbMetric: 95.6820, val_loss: 95.4775, val_MinusLogProbMetric: 95.4775

Epoch 230: val_loss improved from 96.35931 to 95.47754, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 173s - loss: 95.6820 - MinusLogProbMetric: 95.6820 - val_loss: 95.4775 - val_MinusLogProbMetric: 95.4775 - lr: 5.5556e-05 - 173s/epoch - 880ms/step
Epoch 231/1000
2023-10-01 13:04:02.593 
Epoch 231/1000 
	 loss: 95.6608, MinusLogProbMetric: 95.6608, val_loss: 97.9576, val_MinusLogProbMetric: 97.9576

Epoch 231: val_loss did not improve from 95.47754
196/196 - 174s - loss: 95.6608 - MinusLogProbMetric: 95.6608 - val_loss: 97.9576 - val_MinusLogProbMetric: 97.9576 - lr: 5.5556e-05 - 174s/epoch - 885ms/step
Epoch 232/1000
2023-10-01 13:06:51.332 
Epoch 232/1000 
	 loss: 95.6911, MinusLogProbMetric: 95.6911, val_loss: 95.2178, val_MinusLogProbMetric: 95.2178

Epoch 232: val_loss improved from 95.47754 to 95.21776, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 172s - loss: 95.6911 - MinusLogProbMetric: 95.6911 - val_loss: 95.2178 - val_MinusLogProbMetric: 95.2178 - lr: 5.5556e-05 - 172s/epoch - 876ms/step
Epoch 233/1000
2023-10-01 13:09:49.261 
Epoch 233/1000 
	 loss: 94.8936, MinusLogProbMetric: 94.8936, val_loss: 94.5213, val_MinusLogProbMetric: 94.5213

Epoch 233: val_loss improved from 95.21776 to 94.52135, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 178s - loss: 94.8936 - MinusLogProbMetric: 94.8936 - val_loss: 94.5213 - val_MinusLogProbMetric: 94.5213 - lr: 5.5556e-05 - 178s/epoch - 906ms/step
Epoch 234/1000
2023-10-01 13:12:39.219 
Epoch 234/1000 
	 loss: 94.2455, MinusLogProbMetric: 94.2455, val_loss: 94.0380, val_MinusLogProbMetric: 94.0380

Epoch 234: val_loss improved from 94.52135 to 94.03802, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 170s - loss: 94.2455 - MinusLogProbMetric: 94.2455 - val_loss: 94.0380 - val_MinusLogProbMetric: 94.0380 - lr: 5.5556e-05 - 170s/epoch - 870ms/step
Epoch 235/1000
2023-10-01 13:15:38.226 
Epoch 235/1000 
	 loss: 93.6274, MinusLogProbMetric: 93.6274, val_loss: 93.8998, val_MinusLogProbMetric: 93.8998

Epoch 235: val_loss improved from 94.03802 to 93.89977, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 180s - loss: 93.6274 - MinusLogProbMetric: 93.6274 - val_loss: 93.8998 - val_MinusLogProbMetric: 93.8998 - lr: 5.5556e-05 - 180s/epoch - 918ms/step
Epoch 236/1000
2023-10-01 13:18:36.952 
Epoch 236/1000 
	 loss: 93.1395, MinusLogProbMetric: 93.1395, val_loss: 93.0868, val_MinusLogProbMetric: 93.0868

Epoch 236: val_loss improved from 93.89977 to 93.08685, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 178s - loss: 93.1395 - MinusLogProbMetric: 93.1395 - val_loss: 93.0868 - val_MinusLogProbMetric: 93.0868 - lr: 5.5556e-05 - 178s/epoch - 910ms/step
Epoch 237/1000
2023-10-01 13:21:38.272 
Epoch 237/1000 
	 loss: 93.2042, MinusLogProbMetric: 93.2042, val_loss: 92.9956, val_MinusLogProbMetric: 92.9956

Epoch 237: val_loss improved from 93.08685 to 92.99557, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 181s - loss: 93.2042 - MinusLogProbMetric: 93.2042 - val_loss: 92.9956 - val_MinusLogProbMetric: 92.9956 - lr: 5.5556e-05 - 181s/epoch - 923ms/step
Epoch 238/1000
2023-10-01 13:24:42.273 
Epoch 238/1000 
	 loss: 92.4637, MinusLogProbMetric: 92.4637, val_loss: 92.7093, val_MinusLogProbMetric: 92.7093

Epoch 238: val_loss improved from 92.99557 to 92.70925, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 184s - loss: 92.4637 - MinusLogProbMetric: 92.4637 - val_loss: 92.7093 - val_MinusLogProbMetric: 92.7093 - lr: 5.5556e-05 - 184s/epoch - 938ms/step
Epoch 239/1000
2023-10-01 13:27:40.612 
Epoch 239/1000 
	 loss: 92.3446, MinusLogProbMetric: 92.3446, val_loss: 92.2549, val_MinusLogProbMetric: 92.2549

Epoch 239: val_loss improved from 92.70925 to 92.25494, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 178s - loss: 92.3446 - MinusLogProbMetric: 92.3446 - val_loss: 92.2549 - val_MinusLogProbMetric: 92.2549 - lr: 5.5556e-05 - 178s/epoch - 907ms/step
Epoch 240/1000
2023-10-01 13:30:38.690 
Epoch 240/1000 
	 loss: 91.8020, MinusLogProbMetric: 91.8020, val_loss: 92.0917, val_MinusLogProbMetric: 92.0917

Epoch 240: val_loss improved from 92.25494 to 92.09167, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 179s - loss: 91.8020 - MinusLogProbMetric: 91.8020 - val_loss: 92.0917 - val_MinusLogProbMetric: 92.0917 - lr: 5.5556e-05 - 179s/epoch - 911ms/step
Epoch 241/1000
2023-10-01 13:33:38.655 
Epoch 241/1000 
	 loss: 91.6405, MinusLogProbMetric: 91.6405, val_loss: 91.5513, val_MinusLogProbMetric: 91.5513

Epoch 241: val_loss improved from 92.09167 to 91.55132, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 180s - loss: 91.6405 - MinusLogProbMetric: 91.6405 - val_loss: 91.5513 - val_MinusLogProbMetric: 91.5513 - lr: 5.5556e-05 - 180s/epoch - 917ms/step
Epoch 242/1000
2023-10-01 13:36:40.930 
Epoch 242/1000 
	 loss: 91.1122, MinusLogProbMetric: 91.1122, val_loss: 91.3783, val_MinusLogProbMetric: 91.3783

Epoch 242: val_loss improved from 91.55132 to 91.37826, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 183s - loss: 91.1122 - MinusLogProbMetric: 91.1122 - val_loss: 91.3783 - val_MinusLogProbMetric: 91.3783 - lr: 5.5556e-05 - 183s/epoch - 936ms/step
Epoch 243/1000
2023-10-01 13:39:44.939 
Epoch 243/1000 
	 loss: 90.7250, MinusLogProbMetric: 90.7250, val_loss: 90.9756, val_MinusLogProbMetric: 90.9756

Epoch 243: val_loss improved from 91.37826 to 90.97562, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 182s - loss: 90.7250 - MinusLogProbMetric: 90.7250 - val_loss: 90.9756 - val_MinusLogProbMetric: 90.9756 - lr: 5.5556e-05 - 182s/epoch - 931ms/step
Epoch 244/1000
2023-10-01 13:42:48.533 
Epoch 244/1000 
	 loss: 90.3269, MinusLogProbMetric: 90.3269, val_loss: 90.3538, val_MinusLogProbMetric: 90.3538

Epoch 244: val_loss improved from 90.97562 to 90.35381, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 185s - loss: 90.3269 - MinusLogProbMetric: 90.3269 - val_loss: 90.3538 - val_MinusLogProbMetric: 90.3538 - lr: 5.5556e-05 - 185s/epoch - 944ms/step
Epoch 245/1000
2023-10-01 13:45:45.029 
Epoch 245/1000 
	 loss: 90.0779, MinusLogProbMetric: 90.0779, val_loss: 89.8301, val_MinusLogProbMetric: 89.8301

Epoch 245: val_loss improved from 90.35381 to 89.83012, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 176s - loss: 90.0779 - MinusLogProbMetric: 90.0779 - val_loss: 89.8301 - val_MinusLogProbMetric: 89.8301 - lr: 5.5556e-05 - 176s/epoch - 899ms/step
Epoch 246/1000
2023-10-01 13:48:46.101 
Epoch 246/1000 
	 loss: 89.7444, MinusLogProbMetric: 89.7444, val_loss: 89.5814, val_MinusLogProbMetric: 89.5814

Epoch 246: val_loss improved from 89.83012 to 89.58136, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 183s - loss: 89.7444 - MinusLogProbMetric: 89.7444 - val_loss: 89.5814 - val_MinusLogProbMetric: 89.5814 - lr: 5.5556e-05 - 183s/epoch - 932ms/step
Epoch 247/1000
2023-10-01 13:51:46.560 
Epoch 247/1000 
	 loss: 90.5265, MinusLogProbMetric: 90.5265, val_loss: 89.7391, val_MinusLogProbMetric: 89.7391

Epoch 247: val_loss did not improve from 89.58136
196/196 - 176s - loss: 90.5265 - MinusLogProbMetric: 90.5265 - val_loss: 89.7391 - val_MinusLogProbMetric: 89.7391 - lr: 5.5556e-05 - 176s/epoch - 895ms/step
Epoch 248/1000
2023-10-01 13:54:35.372 
Epoch 248/1000 
	 loss: 89.2339, MinusLogProbMetric: 89.2339, val_loss: 89.1150, val_MinusLogProbMetric: 89.1150

Epoch 248: val_loss improved from 89.58136 to 89.11496, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 171s - loss: 89.2339 - MinusLogProbMetric: 89.2339 - val_loss: 89.1150 - val_MinusLogProbMetric: 89.1150 - lr: 5.5556e-05 - 171s/epoch - 872ms/step
Epoch 249/1000
2023-10-01 13:57:30.350 
Epoch 249/1000 
	 loss: 89.0568, MinusLogProbMetric: 89.0568, val_loss: 89.1937, val_MinusLogProbMetric: 89.1937

Epoch 249: val_loss did not improve from 89.11496
196/196 - 173s - loss: 89.0568 - MinusLogProbMetric: 89.0568 - val_loss: 89.1937 - val_MinusLogProbMetric: 89.1937 - lr: 5.5556e-05 - 173s/epoch - 882ms/step
Epoch 250/1000
2023-10-01 14:00:18.668 
Epoch 250/1000 
	 loss: 88.9092, MinusLogProbMetric: 88.9092, val_loss: 89.0479, val_MinusLogProbMetric: 89.0479

Epoch 250: val_loss improved from 89.11496 to 89.04789, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 170s - loss: 88.9092 - MinusLogProbMetric: 88.9092 - val_loss: 89.0479 - val_MinusLogProbMetric: 89.0479 - lr: 5.5556e-05 - 170s/epoch - 869ms/step
Epoch 251/1000
2023-10-01 14:03:03.690 
Epoch 251/1000 
	 loss: 88.6392, MinusLogProbMetric: 88.6392, val_loss: 88.2931, val_MinusLogProbMetric: 88.2931

Epoch 251: val_loss improved from 89.04789 to 88.29307, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 167s - loss: 88.6392 - MinusLogProbMetric: 88.6392 - val_loss: 88.2931 - val_MinusLogProbMetric: 88.2931 - lr: 5.5556e-05 - 167s/epoch - 852ms/step
Epoch 252/1000
2023-10-01 14:05:54.772 
Epoch 252/1000 
	 loss: 88.2361, MinusLogProbMetric: 88.2361, val_loss: 88.1164, val_MinusLogProbMetric: 88.1164

Epoch 252: val_loss improved from 88.29307 to 88.11642, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 171s - loss: 88.2361 - MinusLogProbMetric: 88.2361 - val_loss: 88.1164 - val_MinusLogProbMetric: 88.1164 - lr: 5.5556e-05 - 171s/epoch - 875ms/step
Epoch 253/1000
2023-10-01 14:08:45.264 
Epoch 253/1000 
	 loss: 87.8470, MinusLogProbMetric: 87.8470, val_loss: 87.9234, val_MinusLogProbMetric: 87.9234

Epoch 253: val_loss improved from 88.11642 to 87.92345, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 169s - loss: 87.8470 - MinusLogProbMetric: 87.8470 - val_loss: 87.9234 - val_MinusLogProbMetric: 87.9234 - lr: 5.5556e-05 - 169s/epoch - 861ms/step
Epoch 254/1000
2023-10-01 14:11:37.742 
Epoch 254/1000 
	 loss: 87.6284, MinusLogProbMetric: 87.6284, val_loss: 87.4313, val_MinusLogProbMetric: 87.4313

Epoch 254: val_loss improved from 87.92345 to 87.43126, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 172s - loss: 87.6284 - MinusLogProbMetric: 87.6284 - val_loss: 87.4313 - val_MinusLogProbMetric: 87.4313 - lr: 5.5556e-05 - 172s/epoch - 879ms/step
Epoch 255/1000
2023-10-01 14:14:28.752 
Epoch 255/1000 
	 loss: 88.1331, MinusLogProbMetric: 88.1331, val_loss: 87.5092, val_MinusLogProbMetric: 87.5092

Epoch 255: val_loss did not improve from 87.43126
196/196 - 169s - loss: 88.1331 - MinusLogProbMetric: 88.1331 - val_loss: 87.5092 - val_MinusLogProbMetric: 87.5092 - lr: 5.5556e-05 - 169s/epoch - 860ms/step
Epoch 256/1000
2023-10-01 14:17:15.151 
Epoch 256/1000 
	 loss: 87.2769, MinusLogProbMetric: 87.2769, val_loss: 87.4032, val_MinusLogProbMetric: 87.4032

Epoch 256: val_loss improved from 87.43126 to 87.40318, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 171s - loss: 87.2769 - MinusLogProbMetric: 87.2769 - val_loss: 87.4032 - val_MinusLogProbMetric: 87.4032 - lr: 5.5556e-05 - 171s/epoch - 872ms/step
Epoch 257/1000
2023-10-01 14:20:08.879 
Epoch 257/1000 
	 loss: 88.0607, MinusLogProbMetric: 88.0607, val_loss: 87.3027, val_MinusLogProbMetric: 87.3027

Epoch 257: val_loss improved from 87.40318 to 87.30273, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 172s - loss: 88.0607 - MinusLogProbMetric: 88.0607 - val_loss: 87.3027 - val_MinusLogProbMetric: 87.3027 - lr: 5.5556e-05 - 172s/epoch - 876ms/step
Epoch 258/1000
2023-10-01 14:23:02.990 
Epoch 258/1000 
	 loss: 86.7317, MinusLogProbMetric: 86.7317, val_loss: 86.6067, val_MinusLogProbMetric: 86.6067

Epoch 258: val_loss improved from 87.30273 to 86.60666, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 175s - loss: 86.7317 - MinusLogProbMetric: 86.7317 - val_loss: 86.6067 - val_MinusLogProbMetric: 86.6067 - lr: 5.5556e-05 - 175s/epoch - 895ms/step
Epoch 259/1000
2023-10-01 14:25:55.284 
Epoch 259/1000 
	 loss: 86.4344, MinusLogProbMetric: 86.4344, val_loss: 86.5570, val_MinusLogProbMetric: 86.5570

Epoch 259: val_loss improved from 86.60666 to 86.55697, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 171s - loss: 86.4344 - MinusLogProbMetric: 86.4344 - val_loss: 86.5570 - val_MinusLogProbMetric: 86.5570 - lr: 5.5556e-05 - 171s/epoch - 873ms/step
Epoch 260/1000
2023-10-01 14:28:44.931 
Epoch 260/1000 
	 loss: 86.2418, MinusLogProbMetric: 86.2418, val_loss: 86.4756, val_MinusLogProbMetric: 86.4756

Epoch 260: val_loss improved from 86.55697 to 86.47559, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 171s - loss: 86.2418 - MinusLogProbMetric: 86.2418 - val_loss: 86.4756 - val_MinusLogProbMetric: 86.4756 - lr: 5.5556e-05 - 171s/epoch - 870ms/step
Epoch 261/1000
2023-10-01 14:31:40.026 
Epoch 261/1000 
	 loss: 85.9563, MinusLogProbMetric: 85.9563, val_loss: 85.9885, val_MinusLogProbMetric: 85.9885

Epoch 261: val_loss improved from 86.47559 to 85.98853, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 174s - loss: 85.9563 - MinusLogProbMetric: 85.9563 - val_loss: 85.9885 - val_MinusLogProbMetric: 85.9885 - lr: 5.5556e-05 - 174s/epoch - 890ms/step
Epoch 262/1000
2023-10-01 14:34:26.693 
Epoch 262/1000 
	 loss: 85.8859, MinusLogProbMetric: 85.8859, val_loss: 85.7443, val_MinusLogProbMetric: 85.7443

Epoch 262: val_loss improved from 85.98853 to 85.74429, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 168s - loss: 85.8859 - MinusLogProbMetric: 85.8859 - val_loss: 85.7443 - val_MinusLogProbMetric: 85.7443 - lr: 5.5556e-05 - 168s/epoch - 859ms/step
Epoch 263/1000
2023-10-01 14:37:20.220 
Epoch 263/1000 
	 loss: 85.5076, MinusLogProbMetric: 85.5076, val_loss: 85.5430, val_MinusLogProbMetric: 85.5430

Epoch 263: val_loss improved from 85.74429 to 85.54303, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 171s - loss: 85.5076 - MinusLogProbMetric: 85.5076 - val_loss: 85.5430 - val_MinusLogProbMetric: 85.5430 - lr: 5.5556e-05 - 171s/epoch - 875ms/step
Epoch 264/1000
2023-10-01 14:40:21.069 
Epoch 264/1000 
	 loss: 86.9073, MinusLogProbMetric: 86.9073, val_loss: 85.2830, val_MinusLogProbMetric: 85.2830

Epoch 264: val_loss improved from 85.54303 to 85.28304, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 182s - loss: 86.9073 - MinusLogProbMetric: 86.9073 - val_loss: 85.2830 - val_MinusLogProbMetric: 85.2830 - lr: 5.5556e-05 - 182s/epoch - 926ms/step
Epoch 265/1000
2023-10-01 14:43:12.696 
Epoch 265/1000 
	 loss: 85.0700, MinusLogProbMetric: 85.0700, val_loss: 84.9742, val_MinusLogProbMetric: 84.9742

Epoch 265: val_loss improved from 85.28304 to 84.97419, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 173s - loss: 85.0700 - MinusLogProbMetric: 85.0700 - val_loss: 84.9742 - val_MinusLogProbMetric: 84.9742 - lr: 5.5556e-05 - 173s/epoch - 883ms/step
Epoch 266/1000
2023-10-01 14:46:07.964 
Epoch 266/1000 
	 loss: 84.7249, MinusLogProbMetric: 84.7249, val_loss: 84.8837, val_MinusLogProbMetric: 84.8837

Epoch 266: val_loss improved from 84.97419 to 84.88367, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 173s - loss: 84.7249 - MinusLogProbMetric: 84.7249 - val_loss: 84.8837 - val_MinusLogProbMetric: 84.8837 - lr: 5.5556e-05 - 173s/epoch - 882ms/step
Epoch 267/1000
2023-10-01 14:49:02.394 
Epoch 267/1000 
	 loss: 84.6009, MinusLogProbMetric: 84.6009, val_loss: 84.3109, val_MinusLogProbMetric: 84.3109

Epoch 267: val_loss improved from 84.88367 to 84.31094, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 175s - loss: 84.6009 - MinusLogProbMetric: 84.6009 - val_loss: 84.3109 - val_MinusLogProbMetric: 84.3109 - lr: 5.5556e-05 - 175s/epoch - 891ms/step
Epoch 268/1000
2023-10-01 14:51:56.672 
Epoch 268/1000 
	 loss: 84.3578, MinusLogProbMetric: 84.3578, val_loss: 84.3458, val_MinusLogProbMetric: 84.3458

Epoch 268: val_loss did not improve from 84.31094
196/196 - 172s - loss: 84.3578 - MinusLogProbMetric: 84.3578 - val_loss: 84.3458 - val_MinusLogProbMetric: 84.3458 - lr: 5.5556e-05 - 172s/epoch - 876ms/step
Epoch 269/1000
2023-10-01 14:54:58.009 
Epoch 269/1000 
	 loss: 84.2719, MinusLogProbMetric: 84.2719, val_loss: 84.5891, val_MinusLogProbMetric: 84.5891

Epoch 269: val_loss did not improve from 84.31094
196/196 - 181s - loss: 84.2719 - MinusLogProbMetric: 84.2719 - val_loss: 84.5891 - val_MinusLogProbMetric: 84.5891 - lr: 5.5556e-05 - 181s/epoch - 925ms/step
Epoch 270/1000
2023-10-01 14:57:52.238 
Epoch 270/1000 
	 loss: 84.1663, MinusLogProbMetric: 84.1663, val_loss: 84.1286, val_MinusLogProbMetric: 84.1286

Epoch 270: val_loss improved from 84.31094 to 84.12861, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 176s - loss: 84.1663 - MinusLogProbMetric: 84.1663 - val_loss: 84.1286 - val_MinusLogProbMetric: 84.1286 - lr: 5.5556e-05 - 176s/epoch - 898ms/step
Epoch 271/1000
2023-10-01 15:00:47.228 
Epoch 271/1000 
	 loss: 83.7947, MinusLogProbMetric: 83.7947, val_loss: 83.7686, val_MinusLogProbMetric: 83.7686

Epoch 271: val_loss improved from 84.12861 to 83.76858, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 177s - loss: 83.7947 - MinusLogProbMetric: 83.7947 - val_loss: 83.7686 - val_MinusLogProbMetric: 83.7686 - lr: 5.5556e-05 - 177s/epoch - 903ms/step
Epoch 272/1000
2023-10-01 15:03:45.334 
Epoch 272/1000 
	 loss: 84.4923, MinusLogProbMetric: 84.4923, val_loss: 84.1434, val_MinusLogProbMetric: 84.1434

Epoch 272: val_loss did not improve from 83.76858
196/196 - 174s - loss: 84.4923 - MinusLogProbMetric: 84.4923 - val_loss: 84.1434 - val_MinusLogProbMetric: 84.1434 - lr: 5.5556e-05 - 174s/epoch - 889ms/step
Epoch 273/1000
2023-10-01 15:06:36.015 
Epoch 273/1000 
	 loss: 83.4659, MinusLogProbMetric: 83.4659, val_loss: 83.4102, val_MinusLogProbMetric: 83.4102

Epoch 273: val_loss improved from 83.76858 to 83.41016, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 176s - loss: 83.4659 - MinusLogProbMetric: 83.4659 - val_loss: 83.4102 - val_MinusLogProbMetric: 83.4102 - lr: 5.5556e-05 - 176s/epoch - 898ms/step
Epoch 274/1000
2023-10-01 15:09:38.749 
Epoch 274/1000 
	 loss: 83.4801, MinusLogProbMetric: 83.4801, val_loss: 83.8011, val_MinusLogProbMetric: 83.8011

Epoch 274: val_loss did not improve from 83.41016
196/196 - 177s - loss: 83.4801 - MinusLogProbMetric: 83.4801 - val_loss: 83.8011 - val_MinusLogProbMetric: 83.8011 - lr: 5.5556e-05 - 177s/epoch - 905ms/step
Epoch 275/1000
2023-10-01 15:12:24.259 
Epoch 275/1000 
	 loss: 83.1918, MinusLogProbMetric: 83.1918, val_loss: 84.3239, val_MinusLogProbMetric: 84.3239

Epoch 275: val_loss did not improve from 83.41016
196/196 - 165s - loss: 83.1918 - MinusLogProbMetric: 83.1918 - val_loss: 84.3239 - val_MinusLogProbMetric: 84.3239 - lr: 5.5556e-05 - 165s/epoch - 844ms/step
Epoch 276/1000
2023-10-01 15:15:17.626 
Epoch 276/1000 
	 loss: 83.5711, MinusLogProbMetric: 83.5711, val_loss: 82.9352, val_MinusLogProbMetric: 82.9352

Epoch 276: val_loss improved from 83.41016 to 82.93516, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 176s - loss: 83.5711 - MinusLogProbMetric: 83.5711 - val_loss: 82.9352 - val_MinusLogProbMetric: 82.9352 - lr: 5.5556e-05 - 176s/epoch - 899ms/step
Epoch 277/1000
2023-10-01 15:18:11.982 
Epoch 277/1000 
	 loss: 83.1036, MinusLogProbMetric: 83.1036, val_loss: 82.8583, val_MinusLogProbMetric: 82.8583

Epoch 277: val_loss improved from 82.93516 to 82.85826, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 175s - loss: 83.1036 - MinusLogProbMetric: 83.1036 - val_loss: 82.8583 - val_MinusLogProbMetric: 82.8583 - lr: 5.5556e-05 - 175s/epoch - 894ms/step
Epoch 278/1000
2023-10-01 15:21:05.866 
Epoch 278/1000 
	 loss: 82.6982, MinusLogProbMetric: 82.6982, val_loss: 82.9115, val_MinusLogProbMetric: 82.9115

Epoch 278: val_loss did not improve from 82.85826
196/196 - 170s - loss: 82.6982 - MinusLogProbMetric: 82.6982 - val_loss: 82.9115 - val_MinusLogProbMetric: 82.9115 - lr: 5.5556e-05 - 170s/epoch - 868ms/step
Epoch 279/1000
2023-10-01 15:24:00.648 
Epoch 279/1000 
	 loss: 82.4072, MinusLogProbMetric: 82.4072, val_loss: 82.4923, val_MinusLogProbMetric: 82.4923

Epoch 279: val_loss improved from 82.85826 to 82.49229, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 178s - loss: 82.4072 - MinusLogProbMetric: 82.4072 - val_loss: 82.4923 - val_MinusLogProbMetric: 82.4923 - lr: 5.5556e-05 - 178s/epoch - 908ms/step
Epoch 280/1000
2023-10-01 15:26:55.336 
Epoch 280/1000 
	 loss: 82.5039, MinusLogProbMetric: 82.5039, val_loss: 82.2371, val_MinusLogProbMetric: 82.2371

Epoch 280: val_loss improved from 82.49229 to 82.23712, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 175s - loss: 82.5039 - MinusLogProbMetric: 82.5039 - val_loss: 82.2371 - val_MinusLogProbMetric: 82.2371 - lr: 5.5556e-05 - 175s/epoch - 890ms/step
Epoch 281/1000
2023-10-01 15:29:46.760 
Epoch 281/1000 
	 loss: 82.2998, MinusLogProbMetric: 82.2998, val_loss: 81.8113, val_MinusLogProbMetric: 81.8113

Epoch 281: val_loss improved from 82.23712 to 81.81134, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 171s - loss: 82.2998 - MinusLogProbMetric: 82.2998 - val_loss: 81.8113 - val_MinusLogProbMetric: 81.8113 - lr: 5.5556e-05 - 171s/epoch - 874ms/step
Epoch 282/1000
2023-10-01 15:32:46.304 
Epoch 282/1000 
	 loss: 81.8551, MinusLogProbMetric: 81.8551, val_loss: 81.7538, val_MinusLogProbMetric: 81.7538

Epoch 282: val_loss improved from 81.81134 to 81.75380, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 180s - loss: 81.8551 - MinusLogProbMetric: 81.8551 - val_loss: 81.7538 - val_MinusLogProbMetric: 81.7538 - lr: 5.5556e-05 - 180s/epoch - 920ms/step
Epoch 283/1000
2023-10-01 15:35:42.125 
Epoch 283/1000 
	 loss: 83.0031, MinusLogProbMetric: 83.0031, val_loss: 82.5180, val_MinusLogProbMetric: 82.5180

Epoch 283: val_loss did not improve from 81.75380
196/196 - 172s - loss: 83.0031 - MinusLogProbMetric: 83.0031 - val_loss: 82.5180 - val_MinusLogProbMetric: 82.5180 - lr: 5.5556e-05 - 172s/epoch - 878ms/step
Epoch 284/1000
2023-10-01 15:38:34.580 
Epoch 284/1000 
	 loss: 81.6814, MinusLogProbMetric: 81.6814, val_loss: 81.5072, val_MinusLogProbMetric: 81.5072

Epoch 284: val_loss improved from 81.75380 to 81.50719, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 176s - loss: 81.6814 - MinusLogProbMetric: 81.6814 - val_loss: 81.5072 - val_MinusLogProbMetric: 81.5072 - lr: 5.5556e-05 - 176s/epoch - 898ms/step
Epoch 285/1000
2023-10-01 15:41:22.630 
Epoch 285/1000 
	 loss: 81.5467, MinusLogProbMetric: 81.5467, val_loss: 81.6580, val_MinusLogProbMetric: 81.6580

Epoch 285: val_loss did not improve from 81.50719
196/196 - 165s - loss: 81.5467 - MinusLogProbMetric: 81.5467 - val_loss: 81.6580 - val_MinusLogProbMetric: 81.6580 - lr: 5.5556e-05 - 165s/epoch - 839ms/step
Epoch 286/1000
2023-10-01 15:44:14.189 
Epoch 286/1000 
	 loss: 82.1846, MinusLogProbMetric: 82.1846, val_loss: 82.0636, val_MinusLogProbMetric: 82.0636

Epoch 286: val_loss did not improve from 81.50719
196/196 - 172s - loss: 82.1846 - MinusLogProbMetric: 82.1846 - val_loss: 82.0636 - val_MinusLogProbMetric: 82.0636 - lr: 5.5556e-05 - 172s/epoch - 875ms/step
Epoch 287/1000
2023-10-01 15:47:05.377 
Epoch 287/1000 
	 loss: 81.4872, MinusLogProbMetric: 81.4872, val_loss: 81.5210, val_MinusLogProbMetric: 81.5210

Epoch 287: val_loss did not improve from 81.50719
196/196 - 171s - loss: 81.4872 - MinusLogProbMetric: 81.4872 - val_loss: 81.5210 - val_MinusLogProbMetric: 81.5210 - lr: 5.5556e-05 - 171s/epoch - 873ms/step
Epoch 288/1000
2023-10-01 15:49:56.173 
Epoch 288/1000 
	 loss: 81.1125, MinusLogProbMetric: 81.1125, val_loss: 81.4315, val_MinusLogProbMetric: 81.4315

Epoch 288: val_loss improved from 81.50719 to 81.43154, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 173s - loss: 81.1125 - MinusLogProbMetric: 81.1125 - val_loss: 81.4315 - val_MinusLogProbMetric: 81.4315 - lr: 5.5556e-05 - 173s/epoch - 883ms/step
Epoch 289/1000
2023-10-01 15:52:56.014 
Epoch 289/1000 
	 loss: 81.0004, MinusLogProbMetric: 81.0004, val_loss: 81.5691, val_MinusLogProbMetric: 81.5691

Epoch 289: val_loss did not improve from 81.43154
196/196 - 177s - loss: 81.0004 - MinusLogProbMetric: 81.0004 - val_loss: 81.5691 - val_MinusLogProbMetric: 81.5691 - lr: 5.5556e-05 - 177s/epoch - 905ms/step
Epoch 290/1000
2023-10-01 15:55:50.006 
Epoch 290/1000 
	 loss: 80.7316, MinusLogProbMetric: 80.7316, val_loss: 81.0242, val_MinusLogProbMetric: 81.0242

Epoch 290: val_loss improved from 81.43154 to 81.02423, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 178s - loss: 80.7316 - MinusLogProbMetric: 80.7316 - val_loss: 81.0242 - val_MinusLogProbMetric: 81.0242 - lr: 5.5556e-05 - 178s/epoch - 907ms/step
Epoch 291/1000
2023-10-01 15:58:50.813 
Epoch 291/1000 
	 loss: 80.5628, MinusLogProbMetric: 80.5628, val_loss: 80.4365, val_MinusLogProbMetric: 80.4365

Epoch 291: val_loss improved from 81.02423 to 80.43647, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 180s - loss: 80.5628 - MinusLogProbMetric: 80.5628 - val_loss: 80.4365 - val_MinusLogProbMetric: 80.4365 - lr: 5.5556e-05 - 180s/epoch - 919ms/step
Epoch 292/1000
2023-10-01 16:01:56.615 
Epoch 292/1000 
	 loss: 80.3666, MinusLogProbMetric: 80.3666, val_loss: 80.7365, val_MinusLogProbMetric: 80.7365

Epoch 292: val_loss did not improve from 80.43647
196/196 - 183s - loss: 80.3666 - MinusLogProbMetric: 80.3666 - val_loss: 80.7365 - val_MinusLogProbMetric: 80.7365 - lr: 5.5556e-05 - 183s/epoch - 932ms/step
Epoch 293/1000
2023-10-01 16:04:51.517 
Epoch 293/1000 
	 loss: 80.1471, MinusLogProbMetric: 80.1471, val_loss: 80.3419, val_MinusLogProbMetric: 80.3419

Epoch 293: val_loss improved from 80.43647 to 80.34193, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 178s - loss: 80.1471 - MinusLogProbMetric: 80.1471 - val_loss: 80.3419 - val_MinusLogProbMetric: 80.3419 - lr: 5.5556e-05 - 178s/epoch - 909ms/step
Epoch 294/1000
2023-10-01 16:07:59.864 
Epoch 294/1000 
	 loss: 80.1177, MinusLogProbMetric: 80.1177, val_loss: 80.1886, val_MinusLogProbMetric: 80.1886

Epoch 294: val_loss improved from 80.34193 to 80.18859, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 188s - loss: 80.1177 - MinusLogProbMetric: 80.1177 - val_loss: 80.1886 - val_MinusLogProbMetric: 80.1886 - lr: 5.5556e-05 - 188s/epoch - 959ms/step
Epoch 295/1000
2023-10-01 16:11:02.212 
Epoch 295/1000 
	 loss: 79.8650, MinusLogProbMetric: 79.8650, val_loss: 79.8128, val_MinusLogProbMetric: 79.8128

Epoch 295: val_loss improved from 80.18859 to 79.81279, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 183s - loss: 79.8650 - MinusLogProbMetric: 79.8650 - val_loss: 79.8128 - val_MinusLogProbMetric: 79.8128 - lr: 5.5556e-05 - 183s/epoch - 933ms/step
Epoch 296/1000
2023-10-01 16:13:54.521 
Epoch 296/1000 
	 loss: 80.5348, MinusLogProbMetric: 80.5348, val_loss: 79.8085, val_MinusLogProbMetric: 79.8085

Epoch 296: val_loss improved from 79.81279 to 79.80854, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 173s - loss: 80.5348 - MinusLogProbMetric: 80.5348 - val_loss: 79.8085 - val_MinusLogProbMetric: 79.8085 - lr: 5.5556e-05 - 173s/epoch - 881ms/step
Epoch 297/1000
2023-10-01 16:17:07.255 
Epoch 297/1000 
	 loss: 79.6042, MinusLogProbMetric: 79.6042, val_loss: 79.6712, val_MinusLogProbMetric: 79.6712

Epoch 297: val_loss improved from 79.80854 to 79.67124, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 193s - loss: 79.6042 - MinusLogProbMetric: 79.6042 - val_loss: 79.6712 - val_MinusLogProbMetric: 79.6712 - lr: 5.5556e-05 - 193s/epoch - 983ms/step
Epoch 298/1000
2023-10-01 16:20:12.208 
Epoch 298/1000 
	 loss: 79.3766, MinusLogProbMetric: 79.3766, val_loss: 79.8628, val_MinusLogProbMetric: 79.8628

Epoch 298: val_loss did not improve from 79.67124
196/196 - 181s - loss: 79.3766 - MinusLogProbMetric: 79.3766 - val_loss: 79.8628 - val_MinusLogProbMetric: 79.8628 - lr: 5.5556e-05 - 181s/epoch - 925ms/step
Epoch 299/1000
2023-10-01 16:23:19.116 
Epoch 299/1000 
	 loss: 79.3884, MinusLogProbMetric: 79.3884, val_loss: 79.4295, val_MinusLogProbMetric: 79.4295

Epoch 299: val_loss improved from 79.67124 to 79.42953, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 191s - loss: 79.3884 - MinusLogProbMetric: 79.3884 - val_loss: 79.4295 - val_MinusLogProbMetric: 79.4295 - lr: 5.5556e-05 - 191s/epoch - 974ms/step
Epoch 300/1000
2023-10-01 16:26:23.974 
Epoch 300/1000 
	 loss: 80.2582, MinusLogProbMetric: 80.2582, val_loss: 80.3058, val_MinusLogProbMetric: 80.3058

Epoch 300: val_loss did not improve from 79.42953
196/196 - 181s - loss: 80.2582 - MinusLogProbMetric: 80.2582 - val_loss: 80.3058 - val_MinusLogProbMetric: 80.3058 - lr: 5.5556e-05 - 181s/epoch - 923ms/step
Epoch 301/1000
2023-10-01 16:29:26.859 
Epoch 301/1000 
	 loss: 79.6927, MinusLogProbMetric: 79.6927, val_loss: 80.8727, val_MinusLogProbMetric: 80.8727

Epoch 301: val_loss did not improve from 79.42953
196/196 - 183s - loss: 79.6927 - MinusLogProbMetric: 79.6927 - val_loss: 80.8727 - val_MinusLogProbMetric: 80.8727 - lr: 5.5556e-05 - 183s/epoch - 933ms/step
Epoch 302/1000
2023-10-01 16:32:42.951 
Epoch 302/1000 
	 loss: 79.0480, MinusLogProbMetric: 79.0480, val_loss: 78.7503, val_MinusLogProbMetric: 78.7503

Epoch 302: val_loss improved from 79.42953 to 78.75033, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 201s - loss: 79.0480 - MinusLogProbMetric: 79.0480 - val_loss: 78.7503 - val_MinusLogProbMetric: 78.7503 - lr: 5.5556e-05 - 201s/epoch - 1s/step
Epoch 303/1000
2023-10-01 16:35:50.796 
Epoch 303/1000 
	 loss: 78.8082, MinusLogProbMetric: 78.8082, val_loss: 78.9315, val_MinusLogProbMetric: 78.9315

Epoch 303: val_loss did not improve from 78.75033
196/196 - 183s - loss: 78.8082 - MinusLogProbMetric: 78.8082 - val_loss: 78.9315 - val_MinusLogProbMetric: 78.9315 - lr: 5.5556e-05 - 183s/epoch - 934ms/step
Epoch 304/1000
2023-10-01 16:38:52.254 
Epoch 304/1000 
	 loss: 78.4394, MinusLogProbMetric: 78.4394, val_loss: 78.3359, val_MinusLogProbMetric: 78.3359

Epoch 304: val_loss improved from 78.75033 to 78.33586, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 185s - loss: 78.4394 - MinusLogProbMetric: 78.4394 - val_loss: 78.3359 - val_MinusLogProbMetric: 78.3359 - lr: 5.5556e-05 - 185s/epoch - 943ms/step
Epoch 305/1000
2023-10-01 16:41:53.094 
Epoch 305/1000 
	 loss: 85.0620, MinusLogProbMetric: 85.0620, val_loss: 90.4021, val_MinusLogProbMetric: 90.4021

Epoch 305: val_loss did not improve from 78.33586
196/196 - 177s - loss: 85.0620 - MinusLogProbMetric: 85.0620 - val_loss: 90.4021 - val_MinusLogProbMetric: 90.4021 - lr: 5.5556e-05 - 177s/epoch - 905ms/step
Epoch 306/1000
2023-10-01 16:44:48.126 
Epoch 306/1000 
	 loss: 82.7055, MinusLogProbMetric: 82.7055, val_loss: 81.4619, val_MinusLogProbMetric: 81.4619

Epoch 306: val_loss did not improve from 78.33586
196/196 - 175s - loss: 82.7055 - MinusLogProbMetric: 82.7055 - val_loss: 81.4619 - val_MinusLogProbMetric: 81.4619 - lr: 5.5556e-05 - 175s/epoch - 893ms/step
Epoch 307/1000
2023-10-01 16:47:39.892 
Epoch 307/1000 
	 loss: 80.9452, MinusLogProbMetric: 80.9452, val_loss: 82.3948, val_MinusLogProbMetric: 82.3948

Epoch 307: val_loss did not improve from 78.33586
196/196 - 172s - loss: 80.9452 - MinusLogProbMetric: 80.9452 - val_loss: 82.3948 - val_MinusLogProbMetric: 82.3948 - lr: 5.5556e-05 - 172s/epoch - 876ms/step
Epoch 308/1000
2023-10-01 16:50:38.134 
Epoch 308/1000 
	 loss: 80.6133, MinusLogProbMetric: 80.6133, val_loss: 80.3369, val_MinusLogProbMetric: 80.3369

Epoch 308: val_loss did not improve from 78.33586
196/196 - 178s - loss: 80.6133 - MinusLogProbMetric: 80.6133 - val_loss: 80.3369 - val_MinusLogProbMetric: 80.3369 - lr: 5.5556e-05 - 178s/epoch - 909ms/step
Epoch 309/1000
2023-10-01 16:53:35.050 
Epoch 309/1000 
	 loss: 80.6089, MinusLogProbMetric: 80.6089, val_loss: 84.9329, val_MinusLogProbMetric: 84.9329

Epoch 309: val_loss did not improve from 78.33586
196/196 - 177s - loss: 80.6089 - MinusLogProbMetric: 80.6089 - val_loss: 84.9329 - val_MinusLogProbMetric: 84.9329 - lr: 5.5556e-05 - 177s/epoch - 903ms/step
Epoch 310/1000
2023-10-01 16:56:29.774 
Epoch 310/1000 
	 loss: 80.3630, MinusLogProbMetric: 80.3630, val_loss: 80.1437, val_MinusLogProbMetric: 80.1437

Epoch 310: val_loss did not improve from 78.33586
196/196 - 175s - loss: 80.3630 - MinusLogProbMetric: 80.3630 - val_loss: 80.1437 - val_MinusLogProbMetric: 80.1437 - lr: 5.5556e-05 - 175s/epoch - 892ms/step
Epoch 311/1000
2023-10-01 16:59:22.091 
Epoch 311/1000 
	 loss: 79.9733, MinusLogProbMetric: 79.9733, val_loss: 80.0390, val_MinusLogProbMetric: 80.0390

Epoch 311: val_loss did not improve from 78.33586
196/196 - 172s - loss: 79.9733 - MinusLogProbMetric: 79.9733 - val_loss: 80.0390 - val_MinusLogProbMetric: 80.0390 - lr: 5.5556e-05 - 172s/epoch - 879ms/step
Epoch 312/1000
2023-10-01 17:02:09.010 
Epoch 312/1000 
	 loss: 80.3530, MinusLogProbMetric: 80.3530, val_loss: 80.0554, val_MinusLogProbMetric: 80.0554

Epoch 312: val_loss did not improve from 78.33586
196/196 - 167s - loss: 80.3530 - MinusLogProbMetric: 80.3530 - val_loss: 80.0554 - val_MinusLogProbMetric: 80.0554 - lr: 5.5556e-05 - 167s/epoch - 852ms/step
Epoch 313/1000
2023-10-01 17:04:59.262 
Epoch 313/1000 
	 loss: 79.4630, MinusLogProbMetric: 79.4630, val_loss: 79.7473, val_MinusLogProbMetric: 79.7473

Epoch 313: val_loss did not improve from 78.33586
196/196 - 170s - loss: 79.4630 - MinusLogProbMetric: 79.4630 - val_loss: 79.7473 - val_MinusLogProbMetric: 79.7473 - lr: 5.5556e-05 - 170s/epoch - 869ms/step
Epoch 314/1000
2023-10-01 17:08:00.221 
Epoch 314/1000 
	 loss: 79.5348, MinusLogProbMetric: 79.5348, val_loss: 80.4904, val_MinusLogProbMetric: 80.4904

Epoch 314: val_loss did not improve from 78.33586
196/196 - 181s - loss: 79.5348 - MinusLogProbMetric: 79.5348 - val_loss: 80.4904 - val_MinusLogProbMetric: 80.4904 - lr: 5.5556e-05 - 181s/epoch - 923ms/step
Epoch 315/1000
2023-10-01 17:10:56.623 
Epoch 315/1000 
	 loss: 79.1255, MinusLogProbMetric: 79.1255, val_loss: 78.9666, val_MinusLogProbMetric: 78.9666

Epoch 315: val_loss did not improve from 78.33586
196/196 - 176s - loss: 79.1255 - MinusLogProbMetric: 79.1255 - val_loss: 78.9666 - val_MinusLogProbMetric: 78.9666 - lr: 5.5556e-05 - 176s/epoch - 900ms/step
Epoch 316/1000
2023-10-01 17:13:40.789 
Epoch 316/1000 
	 loss: 78.9182, MinusLogProbMetric: 78.9182, val_loss: 78.7879, val_MinusLogProbMetric: 78.7879

Epoch 316: val_loss did not improve from 78.33586
196/196 - 164s - loss: 78.9182 - MinusLogProbMetric: 78.9182 - val_loss: 78.7879 - val_MinusLogProbMetric: 78.7879 - lr: 5.5556e-05 - 164s/epoch - 838ms/step
Epoch 317/1000
2023-10-01 17:16:36.054 
Epoch 317/1000 
	 loss: 78.8634, MinusLogProbMetric: 78.8634, val_loss: 78.9907, val_MinusLogProbMetric: 78.9907

Epoch 317: val_loss did not improve from 78.33586
196/196 - 175s - loss: 78.8634 - MinusLogProbMetric: 78.8634 - val_loss: 78.9907 - val_MinusLogProbMetric: 78.9907 - lr: 5.5556e-05 - 175s/epoch - 894ms/step
Epoch 318/1000
2023-10-01 17:19:37.093 
Epoch 318/1000 
	 loss: 78.8946, MinusLogProbMetric: 78.8946, val_loss: 78.7502, val_MinusLogProbMetric: 78.7502

Epoch 318: val_loss did not improve from 78.33586
196/196 - 181s - loss: 78.8946 - MinusLogProbMetric: 78.8946 - val_loss: 78.7502 - val_MinusLogProbMetric: 78.7502 - lr: 5.5556e-05 - 181s/epoch - 924ms/step
Epoch 319/1000
2023-10-01 17:22:19.037 
Epoch 319/1000 
	 loss: 78.8702, MinusLogProbMetric: 78.8702, val_loss: 78.8123, val_MinusLogProbMetric: 78.8123

Epoch 319: val_loss did not improve from 78.33586
196/196 - 162s - loss: 78.8702 - MinusLogProbMetric: 78.8702 - val_loss: 78.8123 - val_MinusLogProbMetric: 78.8123 - lr: 5.5556e-05 - 162s/epoch - 826ms/step
Epoch 320/1000
2023-10-01 17:25:22.164 
Epoch 320/1000 
	 loss: 78.4764, MinusLogProbMetric: 78.4764, val_loss: 78.4871, val_MinusLogProbMetric: 78.4871

Epoch 320: val_loss did not improve from 78.33586
196/196 - 183s - loss: 78.4764 - MinusLogProbMetric: 78.4764 - val_loss: 78.4871 - val_MinusLogProbMetric: 78.4871 - lr: 5.5556e-05 - 183s/epoch - 934ms/step
Epoch 321/1000
2023-10-01 17:28:15.722 
Epoch 321/1000 
	 loss: 78.3994, MinusLogProbMetric: 78.3994, val_loss: 78.5065, val_MinusLogProbMetric: 78.5065

Epoch 321: val_loss did not improve from 78.33586
196/196 - 174s - loss: 78.3994 - MinusLogProbMetric: 78.3994 - val_loss: 78.5065 - val_MinusLogProbMetric: 78.5065 - lr: 5.5556e-05 - 174s/epoch - 885ms/step
Epoch 322/1000
2023-10-01 17:31:08.217 
Epoch 322/1000 
	 loss: 78.1447, MinusLogProbMetric: 78.1447, val_loss: 78.3169, val_MinusLogProbMetric: 78.3169

Epoch 322: val_loss improved from 78.33586 to 78.31689, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 176s - loss: 78.1447 - MinusLogProbMetric: 78.1447 - val_loss: 78.3169 - val_MinusLogProbMetric: 78.3169 - lr: 5.5556e-05 - 176s/epoch - 900ms/step
Epoch 323/1000
2023-10-01 17:34:02.253 
Epoch 323/1000 
	 loss: 78.2770, MinusLogProbMetric: 78.2770, val_loss: 77.9505, val_MinusLogProbMetric: 77.9505

Epoch 323: val_loss improved from 78.31689 to 77.95049, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 175s - loss: 78.2770 - MinusLogProbMetric: 78.2770 - val_loss: 77.9505 - val_MinusLogProbMetric: 77.9505 - lr: 5.5556e-05 - 175s/epoch - 891ms/step
Epoch 324/1000
2023-10-01 17:36:56.170 
Epoch 324/1000 
	 loss: 77.9647, MinusLogProbMetric: 77.9647, val_loss: 78.0938, val_MinusLogProbMetric: 78.0938

Epoch 324: val_loss did not improve from 77.95049
196/196 - 169s - loss: 77.9647 - MinusLogProbMetric: 77.9647 - val_loss: 78.0938 - val_MinusLogProbMetric: 78.0938 - lr: 5.5556e-05 - 169s/epoch - 864ms/step
Epoch 325/1000
2023-10-01 17:39:53.275 
Epoch 325/1000 
	 loss: 77.8698, MinusLogProbMetric: 77.8698, val_loss: 78.5830, val_MinusLogProbMetric: 78.5830

Epoch 325: val_loss did not improve from 77.95049
196/196 - 177s - loss: 77.8698 - MinusLogProbMetric: 77.8698 - val_loss: 78.5830 - val_MinusLogProbMetric: 78.5830 - lr: 5.5556e-05 - 177s/epoch - 903ms/step
Epoch 326/1000
2023-10-01 17:42:46.944 
Epoch 326/1000 
	 loss: 77.5547, MinusLogProbMetric: 77.5547, val_loss: 77.5859, val_MinusLogProbMetric: 77.5859

Epoch 326: val_loss improved from 77.95049 to 77.58587, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 178s - loss: 77.5547 - MinusLogProbMetric: 77.5547 - val_loss: 77.5859 - val_MinusLogProbMetric: 77.5859 - lr: 5.5556e-05 - 178s/epoch - 910ms/step
Epoch 327/1000
2023-10-01 17:45:42.945 
Epoch 327/1000 
	 loss: 77.3908, MinusLogProbMetric: 77.3908, val_loss: 78.3870, val_MinusLogProbMetric: 78.3870

Epoch 327: val_loss did not improve from 77.58587
196/196 - 171s - loss: 77.3908 - MinusLogProbMetric: 77.3908 - val_loss: 78.3870 - val_MinusLogProbMetric: 78.3870 - lr: 5.5556e-05 - 171s/epoch - 874ms/step
Epoch 328/1000
2023-10-01 17:48:31.908 
Epoch 328/1000 
	 loss: 77.3506, MinusLogProbMetric: 77.3506, val_loss: 77.4133, val_MinusLogProbMetric: 77.4133

Epoch 328: val_loss improved from 77.58587 to 77.41328, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 172s - loss: 77.3506 - MinusLogProbMetric: 77.3506 - val_loss: 77.4133 - val_MinusLogProbMetric: 77.4133 - lr: 5.5556e-05 - 172s/epoch - 876ms/step
Epoch 329/1000
2023-10-01 17:51:30.025 
Epoch 329/1000 
	 loss: 77.3836, MinusLogProbMetric: 77.3836, val_loss: 78.0804, val_MinusLogProbMetric: 78.0804

Epoch 329: val_loss did not improve from 77.41328
196/196 - 175s - loss: 77.3836 - MinusLogProbMetric: 77.3836 - val_loss: 78.0804 - val_MinusLogProbMetric: 78.0804 - lr: 5.5556e-05 - 175s/epoch - 895ms/step
Epoch 330/1000
2023-10-01 17:54:26.553 
Epoch 330/1000 
	 loss: 77.1357, MinusLogProbMetric: 77.1357, val_loss: 77.1072, val_MinusLogProbMetric: 77.1072

Epoch 330: val_loss improved from 77.41328 to 77.10715, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 181s - loss: 77.1357 - MinusLogProbMetric: 77.1357 - val_loss: 77.1072 - val_MinusLogProbMetric: 77.1072 - lr: 5.5556e-05 - 181s/epoch - 921ms/step
Epoch 331/1000
2023-10-01 17:57:24.210 
Epoch 331/1000 
	 loss: 77.9228, MinusLogProbMetric: 77.9228, val_loss: 77.9103, val_MinusLogProbMetric: 77.9103

Epoch 331: val_loss did not improve from 77.10715
196/196 - 174s - loss: 77.9228 - MinusLogProbMetric: 77.9228 - val_loss: 77.9103 - val_MinusLogProbMetric: 77.9103 - lr: 5.5556e-05 - 174s/epoch - 886ms/step
Epoch 332/1000
2023-10-01 18:00:12.311 
Epoch 332/1000 
	 loss: 77.0955, MinusLogProbMetric: 77.0955, val_loss: 77.2571, val_MinusLogProbMetric: 77.2571

Epoch 332: val_loss did not improve from 77.10715
196/196 - 168s - loss: 77.0955 - MinusLogProbMetric: 77.0955 - val_loss: 77.2571 - val_MinusLogProbMetric: 77.2571 - lr: 5.5556e-05 - 168s/epoch - 858ms/step
Epoch 333/1000
2023-10-01 18:03:01.313 
Epoch 333/1000 
	 loss: 76.9272, MinusLogProbMetric: 76.9272, val_loss: 76.7099, val_MinusLogProbMetric: 76.7099

Epoch 333: val_loss improved from 77.10715 to 76.70990, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 172s - loss: 76.9272 - MinusLogProbMetric: 76.9272 - val_loss: 76.7099 - val_MinusLogProbMetric: 76.7099 - lr: 5.5556e-05 - 172s/epoch - 877ms/step
Epoch 334/1000
2023-10-01 18:06:04.353 
Epoch 334/1000 
	 loss: 76.5861, MinusLogProbMetric: 76.5861, val_loss: 76.8454, val_MinusLogProbMetric: 76.8454

Epoch 334: val_loss did not improve from 76.70990
196/196 - 180s - loss: 76.5861 - MinusLogProbMetric: 76.5861 - val_loss: 76.8454 - val_MinusLogProbMetric: 76.8454 - lr: 5.5556e-05 - 180s/epoch - 919ms/step
Epoch 335/1000
2023-10-01 18:08:58.980 
Epoch 335/1000 
	 loss: 76.6191, MinusLogProbMetric: 76.6191, val_loss: 76.6072, val_MinusLogProbMetric: 76.6072

Epoch 335: val_loss improved from 76.70990 to 76.60720, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 178s - loss: 76.6191 - MinusLogProbMetric: 76.6191 - val_loss: 76.6072 - val_MinusLogProbMetric: 76.6072 - lr: 5.5556e-05 - 178s/epoch - 906ms/step
Epoch 336/1000
2023-10-01 18:11:53.586 
Epoch 336/1000 
	 loss: 76.5397, MinusLogProbMetric: 76.5397, val_loss: 76.9176, val_MinusLogProbMetric: 76.9176

Epoch 336: val_loss did not improve from 76.60720
196/196 - 172s - loss: 76.5397 - MinusLogProbMetric: 76.5397 - val_loss: 76.9176 - val_MinusLogProbMetric: 76.9176 - lr: 5.5556e-05 - 172s/epoch - 875ms/step
Epoch 337/1000
2023-10-01 18:14:50.108 
Epoch 337/1000 
	 loss: 76.6142, MinusLogProbMetric: 76.6142, val_loss: 76.2066, val_MinusLogProbMetric: 76.2066

Epoch 337: val_loss improved from 76.60720 to 76.20659, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 179s - loss: 76.6142 - MinusLogProbMetric: 76.6142 - val_loss: 76.2066 - val_MinusLogProbMetric: 76.2066 - lr: 5.5556e-05 - 179s/epoch - 913ms/step
Epoch 338/1000
2023-10-01 18:17:36.141 
Epoch 338/1000 
	 loss: 76.2197, MinusLogProbMetric: 76.2197, val_loss: 76.3112, val_MinusLogProbMetric: 76.3112

Epoch 338: val_loss did not improve from 76.20659
196/196 - 164s - loss: 76.2197 - MinusLogProbMetric: 76.2197 - val_loss: 76.3112 - val_MinusLogProbMetric: 76.3112 - lr: 5.5556e-05 - 164s/epoch - 835ms/step
Epoch 339/1000
2023-10-01 18:20:40.848 
Epoch 339/1000 
	 loss: 75.9857, MinusLogProbMetric: 75.9857, val_loss: 76.3635, val_MinusLogProbMetric: 76.3635

Epoch 339: val_loss did not improve from 76.20659
196/196 - 185s - loss: 75.9857 - MinusLogProbMetric: 75.9857 - val_loss: 76.3635 - val_MinusLogProbMetric: 76.3635 - lr: 5.5556e-05 - 185s/epoch - 942ms/step
Epoch 340/1000
2023-10-01 18:23:27.527 
Epoch 340/1000 
	 loss: 75.8283, MinusLogProbMetric: 75.8283, val_loss: 75.8197, val_MinusLogProbMetric: 75.8197

Epoch 340: val_loss improved from 76.20659 to 75.81966, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 170s - loss: 75.8283 - MinusLogProbMetric: 75.8283 - val_loss: 75.8197 - val_MinusLogProbMetric: 75.8197 - lr: 5.5556e-05 - 170s/epoch - 867ms/step
Epoch 341/1000
2023-10-01 18:26:27.573 
Epoch 341/1000 
	 loss: 76.0683, MinusLogProbMetric: 76.0683, val_loss: 75.8590, val_MinusLogProbMetric: 75.8590

Epoch 341: val_loss did not improve from 75.81966
196/196 - 177s - loss: 76.0683 - MinusLogProbMetric: 76.0683 - val_loss: 75.8590 - val_MinusLogProbMetric: 75.8590 - lr: 5.5556e-05 - 177s/epoch - 902ms/step
Epoch 342/1000
2023-10-01 18:29:23.225 
Epoch 342/1000 
	 loss: 75.7796, MinusLogProbMetric: 75.7796, val_loss: 75.4231, val_MinusLogProbMetric: 75.4231

Epoch 342: val_loss improved from 75.81966 to 75.42315, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 179s - loss: 75.7796 - MinusLogProbMetric: 75.7796 - val_loss: 75.4231 - val_MinusLogProbMetric: 75.4231 - lr: 5.5556e-05 - 179s/epoch - 912ms/step
Epoch 343/1000
2023-10-01 18:32:18.182 
Epoch 343/1000 
	 loss: 75.4314, MinusLogProbMetric: 75.4314, val_loss: 75.4035, val_MinusLogProbMetric: 75.4035

Epoch 343: val_loss improved from 75.42315 to 75.40345, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 173s - loss: 75.4314 - MinusLogProbMetric: 75.4314 - val_loss: 75.4035 - val_MinusLogProbMetric: 75.4035 - lr: 5.5556e-05 - 173s/epoch - 883ms/step
Epoch 344/1000
2023-10-01 18:35:06.521 
Epoch 344/1000 
	 loss: 75.3308, MinusLogProbMetric: 75.3308, val_loss: 75.5221, val_MinusLogProbMetric: 75.5221

Epoch 344: val_loss did not improve from 75.40345
196/196 - 167s - loss: 75.3308 - MinusLogProbMetric: 75.3308 - val_loss: 75.5221 - val_MinusLogProbMetric: 75.5221 - lr: 5.5556e-05 - 167s/epoch - 853ms/step
Epoch 345/1000
2023-10-01 18:37:48.847 
Epoch 345/1000 
	 loss: 75.1584, MinusLogProbMetric: 75.1584, val_loss: 75.9591, val_MinusLogProbMetric: 75.9591

Epoch 345: val_loss did not improve from 75.40345
196/196 - 162s - loss: 75.1584 - MinusLogProbMetric: 75.1584 - val_loss: 75.9591 - val_MinusLogProbMetric: 75.9591 - lr: 5.5556e-05 - 162s/epoch - 828ms/step
Epoch 346/1000
2023-10-01 18:40:34.537 
Epoch 346/1000 
	 loss: 75.1305, MinusLogProbMetric: 75.1305, val_loss: 75.6078, val_MinusLogProbMetric: 75.6078

Epoch 346: val_loss did not improve from 75.40345
196/196 - 166s - loss: 75.1305 - MinusLogProbMetric: 75.1305 - val_loss: 75.6078 - val_MinusLogProbMetric: 75.6078 - lr: 5.5556e-05 - 166s/epoch - 845ms/step
Epoch 347/1000
2023-10-01 18:43:22.719 
Epoch 347/1000 
	 loss: 74.9160, MinusLogProbMetric: 74.9160, val_loss: 75.1131, val_MinusLogProbMetric: 75.1131

Epoch 347: val_loss improved from 75.40345 to 75.11310, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 173s - loss: 74.9160 - MinusLogProbMetric: 74.9160 - val_loss: 75.1131 - val_MinusLogProbMetric: 75.1131 - lr: 5.5556e-05 - 173s/epoch - 883ms/step
Epoch 348/1000
2023-10-01 18:46:12.984 
Epoch 348/1000 
	 loss: 80.5398, MinusLogProbMetric: 80.5398, val_loss: 78.0136, val_MinusLogProbMetric: 78.0136

Epoch 348: val_loss did not improve from 75.11310
196/196 - 165s - loss: 80.5398 - MinusLogProbMetric: 80.5398 - val_loss: 78.0136 - val_MinusLogProbMetric: 78.0136 - lr: 5.5556e-05 - 165s/epoch - 844ms/step
Epoch 349/1000
2023-10-01 18:49:04.743 
Epoch 349/1000 
	 loss: 76.0839, MinusLogProbMetric: 76.0839, val_loss: 75.4643, val_MinusLogProbMetric: 75.4643

Epoch 349: val_loss did not improve from 75.11310
196/196 - 172s - loss: 76.0839 - MinusLogProbMetric: 76.0839 - val_loss: 75.4643 - val_MinusLogProbMetric: 75.4643 - lr: 5.5556e-05 - 172s/epoch - 876ms/step
Epoch 350/1000
2023-10-01 18:51:50.224 
Epoch 350/1000 
	 loss: 75.0873, MinusLogProbMetric: 75.0873, val_loss: 75.2135, val_MinusLogProbMetric: 75.2135

Epoch 350: val_loss did not improve from 75.11310
196/196 - 166s - loss: 75.0873 - MinusLogProbMetric: 75.0873 - val_loss: 75.2135 - val_MinusLogProbMetric: 75.2135 - lr: 5.5556e-05 - 166s/epoch - 844ms/step
Epoch 351/1000
2023-10-01 18:54:31.793 
Epoch 351/1000 
	 loss: 74.8819, MinusLogProbMetric: 74.8819, val_loss: 75.9359, val_MinusLogProbMetric: 75.9359

Epoch 351: val_loss did not improve from 75.11310
196/196 - 162s - loss: 74.8819 - MinusLogProbMetric: 74.8819 - val_loss: 75.9359 - val_MinusLogProbMetric: 75.9359 - lr: 5.5556e-05 - 162s/epoch - 824ms/step
Epoch 352/1000
2023-10-01 18:57:24.481 
Epoch 352/1000 
	 loss: 74.6877, MinusLogProbMetric: 74.6877, val_loss: 74.4831, val_MinusLogProbMetric: 74.4831

Epoch 352: val_loss improved from 75.11310 to 74.48311, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 175s - loss: 74.6877 - MinusLogProbMetric: 74.6877 - val_loss: 74.4831 - val_MinusLogProbMetric: 74.4831 - lr: 5.5556e-05 - 175s/epoch - 894ms/step
Epoch 353/1000
2023-10-01 19:00:27.532 
Epoch 353/1000 
	 loss: 74.5130, MinusLogProbMetric: 74.5130, val_loss: 75.4051, val_MinusLogProbMetric: 75.4051

Epoch 353: val_loss did not improve from 74.48311
196/196 - 180s - loss: 74.5130 - MinusLogProbMetric: 74.5130 - val_loss: 75.4051 - val_MinusLogProbMetric: 75.4051 - lr: 5.5556e-05 - 180s/epoch - 921ms/step
Epoch 354/1000
2023-10-01 19:03:19.427 
Epoch 354/1000 
	 loss: 74.4797, MinusLogProbMetric: 74.4797, val_loss: 74.6094, val_MinusLogProbMetric: 74.6094

Epoch 354: val_loss did not improve from 74.48311
196/196 - 172s - loss: 74.4797 - MinusLogProbMetric: 74.4797 - val_loss: 74.6094 - val_MinusLogProbMetric: 74.6094 - lr: 5.5556e-05 - 172s/epoch - 877ms/step
Epoch 355/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-01 19:06:10.943 
Epoch 355/1000 
	 loss: 74.3843, MinusLogProbMetric: 74.3843, val_loss: 74.3073, val_MinusLogProbMetric: 74.3073

Epoch 355: val_loss improved from 74.48311 to 74.30729, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 174s - loss: 74.3843 - MinusLogProbMetric: 74.3843 - val_loss: 74.3073 - val_MinusLogProbMetric: 74.3073 - lr: 5.5556e-05 - 174s/epoch - 887ms/step
Epoch 356/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-01 19:09:22.477 
Epoch 356/1000 
	 loss: 74.1961, MinusLogProbMetric: 74.1961, val_loss: 78.7349, val_MinusLogProbMetric: 78.7349

Epoch 356: val_loss did not improve from 74.30729
196/196 - 189s - loss: 74.1961 - MinusLogProbMetric: 74.1961 - val_loss: 78.7349 - val_MinusLogProbMetric: 78.7349 - lr: 5.5556e-05 - 189s/epoch - 965ms/step
Epoch 357/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-01 19:12:11.686 
Epoch 357/1000 
	 loss: 74.3469, MinusLogProbMetric: 74.3469, val_loss: 74.1612, val_MinusLogProbMetric: 74.1612

Epoch 357: val_loss improved from 74.30729 to 74.16123, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 172s - loss: 74.3469 - MinusLogProbMetric: 74.3469 - val_loss: 74.1612 - val_MinusLogProbMetric: 74.1612 - lr: 5.5556e-05 - 172s/epoch - 880ms/step
Epoch 358/1000
2023-10-01 19:15:10.151 
Epoch 358/1000 
	 loss: 74.7415, MinusLogProbMetric: 74.7415, val_loss: 74.9969, val_MinusLogProbMetric: 74.9969

Epoch 358: val_loss did not improve from 74.16123
196/196 - 175s - loss: 74.7415 - MinusLogProbMetric: 74.7415 - val_loss: 74.9969 - val_MinusLogProbMetric: 74.9969 - lr: 5.5556e-05 - 175s/epoch - 894ms/step
Epoch 359/1000
2023-10-01 19:17:53.002 
Epoch 359/1000 
	 loss: 74.5906, MinusLogProbMetric: 74.5906, val_loss: 74.5191, val_MinusLogProbMetric: 74.5191

Epoch 359: val_loss did not improve from 74.16123
196/196 - 163s - loss: 74.5906 - MinusLogProbMetric: 74.5906 - val_loss: 74.5191 - val_MinusLogProbMetric: 74.5191 - lr: 5.5556e-05 - 163s/epoch - 831ms/step
Epoch 360/1000
2023-10-01 19:20:44.543 
Epoch 360/1000 
	 loss: 73.8721, MinusLogProbMetric: 73.8721, val_loss: 74.3176, val_MinusLogProbMetric: 74.3176

Epoch 360: val_loss did not improve from 74.16123
196/196 - 172s - loss: 73.8721 - MinusLogProbMetric: 73.8721 - val_loss: 74.3176 - val_MinusLogProbMetric: 74.3176 - lr: 5.5556e-05 - 172s/epoch - 875ms/step
Epoch 361/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-01 19:23:31.611 
Epoch 361/1000 
	 loss: 73.5392, MinusLogProbMetric: 73.5392, val_loss: 73.6978, val_MinusLogProbMetric: 73.6978

Epoch 361: val_loss improved from 74.16123 to 73.69781, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 169s - loss: 73.5392 - MinusLogProbMetric: 73.5392 - val_loss: 73.6978 - val_MinusLogProbMetric: 73.6978 - lr: 5.5556e-05 - 169s/epoch - 864ms/step
Epoch 362/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-01 19:26:31.636 
Epoch 362/1000 
	 loss: 73.5373, MinusLogProbMetric: 73.5373, val_loss: 73.5968, val_MinusLogProbMetric: 73.5968

Epoch 362: val_loss improved from 73.69781 to 73.59681, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 180s - loss: 73.5373 - MinusLogProbMetric: 73.5373 - val_loss: 73.5968 - val_MinusLogProbMetric: 73.5968 - lr: 5.5556e-05 - 180s/epoch - 919ms/step
Epoch 363/1000
2023-10-01 19:29:23.586 
Epoch 363/1000 
	 loss: 73.4112, MinusLogProbMetric: 73.4112, val_loss: 73.4581, val_MinusLogProbMetric: 73.4581

Epoch 363: val_loss improved from 73.59681 to 73.45811, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 172s - loss: 73.4112 - MinusLogProbMetric: 73.4112 - val_loss: 73.4581 - val_MinusLogProbMetric: 73.4581 - lr: 5.5556e-05 - 172s/epoch - 878ms/step
Epoch 364/1000
2023-10-01 19:32:18.463 
Epoch 364/1000 
	 loss: 73.2062, MinusLogProbMetric: 73.2062, val_loss: 73.7376, val_MinusLogProbMetric: 73.7376

Epoch 364: val_loss did not improve from 73.45811
196/196 - 172s - loss: 73.2062 - MinusLogProbMetric: 73.2062 - val_loss: 73.7376 - val_MinusLogProbMetric: 73.7376 - lr: 5.5556e-05 - 172s/epoch - 879ms/step
Epoch 365/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-01 19:35:02.701 
Epoch 365/1000 
	 loss: 73.1902, MinusLogProbMetric: 73.1902, val_loss: 73.7043, val_MinusLogProbMetric: 73.7043

Epoch 365: val_loss did not improve from 73.45811
196/196 - 164s - loss: 73.1902 - MinusLogProbMetric: 73.1902 - val_loss: 73.7043 - val_MinusLogProbMetric: 73.7043 - lr: 5.5556e-05 - 164s/epoch - 838ms/step
Epoch 366/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-01 19:37:48.393 
Epoch 366/1000 
	 loss: 73.0318, MinusLogProbMetric: 73.0318, val_loss: 73.4993, val_MinusLogProbMetric: 73.4993

Epoch 366: val_loss did not improve from 73.45811
196/196 - 166s - loss: 73.0318 - MinusLogProbMetric: 73.0318 - val_loss: 73.4993 - val_MinusLogProbMetric: 73.4993 - lr: 5.5556e-05 - 166s/epoch - 845ms/step
Epoch 367/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-01 19:40:26.693 
Epoch 367/1000 
	 loss: 72.9682, MinusLogProbMetric: 72.9682, val_loss: 73.2876, val_MinusLogProbMetric: 73.2876

Epoch 367: val_loss improved from 73.45811 to 73.28758, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 163s - loss: 72.9682 - MinusLogProbMetric: 72.9682 - val_loss: 73.2876 - val_MinusLogProbMetric: 73.2876 - lr: 5.5556e-05 - 163s/epoch - 834ms/step
Epoch 368/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-01 19:43:17.161 
Epoch 368/1000 
	 loss: 72.8984, MinusLogProbMetric: 72.8984, val_loss: 73.3390, val_MinusLogProbMetric: 73.3390

Epoch 368: val_loss did not improve from 73.28758
196/196 - 165s - loss: 72.8984 - MinusLogProbMetric: 72.8984 - val_loss: 73.3390 - val_MinusLogProbMetric: 73.3390 - lr: 5.5556e-05 - 165s/epoch - 843ms/step
Epoch 369/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-01 19:46:00.460 
Epoch 369/1000 
	 loss: 72.9565, MinusLogProbMetric: 72.9565, val_loss: 73.1423, val_MinusLogProbMetric: 73.1423

Epoch 369: val_loss improved from 73.28758 to 73.14233, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 167s - loss: 72.9565 - MinusLogProbMetric: 72.9565 - val_loss: 73.1423 - val_MinusLogProbMetric: 73.1423 - lr: 5.5556e-05 - 167s/epoch - 851ms/step
Epoch 370/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-01 19:48:51.196 
Epoch 370/1000 
	 loss: 72.6368, MinusLogProbMetric: 72.6368, val_loss: 72.9758, val_MinusLogProbMetric: 72.9758

Epoch 370: val_loss improved from 73.14233 to 72.97583, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 169s - loss: 72.6368 - MinusLogProbMetric: 72.6368 - val_loss: 72.9758 - val_MinusLogProbMetric: 72.9758 - lr: 5.5556e-05 - 169s/epoch - 861ms/step
Epoch 371/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-01 19:51:39.484 
Epoch 371/1000 
	 loss: 72.6671, MinusLogProbMetric: 72.6671, val_loss: 72.7129, val_MinusLogProbMetric: 72.7128

Epoch 371: val_loss improved from 72.97583 to 72.71286, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 172s - loss: 72.6671 - MinusLogProbMetric: 72.6671 - val_loss: 72.7129 - val_MinusLogProbMetric: 72.7128 - lr: 5.5556e-05 - 172s/epoch - 875ms/step
Epoch 372/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-01 19:54:31.592 
Epoch 372/1000 
	 loss: 72.4540, MinusLogProbMetric: 72.4540, val_loss: 73.0102, val_MinusLogProbMetric: 73.0102

Epoch 372: val_loss did not improve from 72.71286
196/196 - 167s - loss: 72.4540 - MinusLogProbMetric: 72.4540 - val_loss: 73.0102 - val_MinusLogProbMetric: 73.0102 - lr: 5.5556e-05 - 167s/epoch - 853ms/step
Epoch 373/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-01 19:57:14.699 
Epoch 373/1000 
	 loss: 72.5332, MinusLogProbMetric: 72.5332, val_loss: 73.2168, val_MinusLogProbMetric: 73.2167

Epoch 373: val_loss did not improve from 72.71286
196/196 - 163s - loss: 72.5332 - MinusLogProbMetric: 72.5332 - val_loss: 73.2168 - val_MinusLogProbMetric: 73.2167 - lr: 5.5556e-05 - 163s/epoch - 832ms/step
Epoch 374/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-01 19:59:54.752 
Epoch 374/1000 
	 loss: 72.2458, MinusLogProbMetric: 72.2458, val_loss: 72.8049, val_MinusLogProbMetric: 72.8049

Epoch 374: val_loss did not improve from 72.71286
196/196 - 160s - loss: 72.2458 - MinusLogProbMetric: 72.2458 - val_loss: 72.8049 - val_MinusLogProbMetric: 72.8049 - lr: 5.5556e-05 - 160s/epoch - 817ms/step
Epoch 375/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-01 20:02:43.243 
Epoch 375/1000 
	 loss: 72.0513, MinusLogProbMetric: 72.0513, val_loss: 72.3208, val_MinusLogProbMetric: 72.3207

Epoch 375: val_loss improved from 72.71286 to 72.32076, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 171s - loss: 72.0513 - MinusLogProbMetric: 72.0513 - val_loss: 72.3208 - val_MinusLogProbMetric: 72.3207 - lr: 5.5556e-05 - 171s/epoch - 872ms/step
Epoch 376/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-01 20:05:33.461 
Epoch 376/1000 
	 loss: 72.0973, MinusLogProbMetric: 72.0973, val_loss: 72.0659, val_MinusLogProbMetric: 72.0659

Epoch 376: val_loss improved from 72.32076 to 72.06588, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 172s - loss: 72.0973 - MinusLogProbMetric: 72.0973 - val_loss: 72.0659 - val_MinusLogProbMetric: 72.0659 - lr: 5.5556e-05 - 172s/epoch - 879ms/step
Epoch 377/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-01 20:08:23.784 
Epoch 377/1000 
	 loss: 71.9452, MinusLogProbMetric: 71.9452, val_loss: 72.4449, val_MinusLogProbMetric: 72.4449

Epoch 377: val_loss did not improve from 72.06588
196/196 - 166s - loss: 71.9452 - MinusLogProbMetric: 71.9452 - val_loss: 72.4449 - val_MinusLogProbMetric: 72.4449 - lr: 5.5556e-05 - 166s/epoch - 846ms/step
Epoch 378/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-01 20:11:04.073 
Epoch 378/1000 
	 loss: 71.7122, MinusLogProbMetric: 71.7122, val_loss: 71.8986, val_MinusLogProbMetric: 71.8985

Epoch 378: val_loss improved from 72.06588 to 71.89857, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 163s - loss: 71.7122 - MinusLogProbMetric: 71.7122 - val_loss: 71.8986 - val_MinusLogProbMetric: 71.8985 - lr: 5.5556e-05 - 163s/epoch - 831ms/step
Epoch 379/1000
2023-10-01 20:13:43.094 
Epoch 379/1000 
	 loss: 71.7671, MinusLogProbMetric: 71.7671, val_loss: 71.7862, val_MinusLogProbMetric: 71.7862

Epoch 379: val_loss improved from 71.89857 to 71.78625, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 162s - loss: 71.7671 - MinusLogProbMetric: 71.7671 - val_loss: 71.7862 - val_MinusLogProbMetric: 71.7862 - lr: 5.5556e-05 - 162s/epoch - 828ms/step
Epoch 380/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-01 20:16:36.798 
Epoch 380/1000 
	 loss: 71.5067, MinusLogProbMetric: 71.5067, val_loss: 71.6751, val_MinusLogProbMetric: 71.6751

Epoch 380: val_loss improved from 71.78625 to 71.67506, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 172s - loss: 71.5067 - MinusLogProbMetric: 71.5067 - val_loss: 71.6751 - val_MinusLogProbMetric: 71.6751 - lr: 5.5556e-05 - 172s/epoch - 880ms/step
Epoch 381/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-01 20:19:25.359 
Epoch 381/1000 
	 loss: 71.4986, MinusLogProbMetric: 71.4986, val_loss: 71.8259, val_MinusLogProbMetric: 71.8259

Epoch 381: val_loss did not improve from 71.67506
196/196 - 164s - loss: 71.4986 - MinusLogProbMetric: 71.4986 - val_loss: 71.8259 - val_MinusLogProbMetric: 71.8259 - lr: 5.5556e-05 - 164s/epoch - 836ms/step
Epoch 382/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-01 20:22:07.683 
Epoch 382/1000 
	 loss: 71.3426, MinusLogProbMetric: 71.3426, val_loss: 71.4127, val_MinusLogProbMetric: 71.4127

Epoch 382: val_loss improved from 71.67506 to 71.41267, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 167s - loss: 71.3426 - MinusLogProbMetric: 71.3426 - val_loss: 71.4127 - val_MinusLogProbMetric: 71.4127 - lr: 5.5556e-05 - 167s/epoch - 850ms/step
Epoch 383/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-01 20:24:52.212 
Epoch 383/1000 
	 loss: 71.3968, MinusLogProbMetric: 71.3968, val_loss: 72.7223, val_MinusLogProbMetric: 72.7223

Epoch 383: val_loss did not improve from 71.41267
196/196 - 160s - loss: 71.3968 - MinusLogProbMetric: 71.3968 - val_loss: 72.7223 - val_MinusLogProbMetric: 72.7223 - lr: 5.5556e-05 - 160s/epoch - 818ms/step
Epoch 384/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-01 20:27:30.947 
Epoch 384/1000 
	 loss: 71.1642, MinusLogProbMetric: 71.1642, val_loss: 71.6460, val_MinusLogProbMetric: 71.6460

Epoch 384: val_loss did not improve from 71.41267
196/196 - 159s - loss: 71.1642 - MinusLogProbMetric: 71.1642 - val_loss: 71.6460 - val_MinusLogProbMetric: 71.6460 - lr: 5.5556e-05 - 159s/epoch - 810ms/step
Epoch 385/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-01 20:30:10.313 
Epoch 385/1000 
	 loss: 71.5099, MinusLogProbMetric: 71.5099, val_loss: 73.9830, val_MinusLogProbMetric: 73.9830

Epoch 385: val_loss did not improve from 71.41267
196/196 - 159s - loss: 71.5099 - MinusLogProbMetric: 71.5099 - val_loss: 73.9830 - val_MinusLogProbMetric: 73.9830 - lr: 5.5556e-05 - 159s/epoch - 813ms/step
Epoch 386/1000
2023-10-01 20:32:47.109 
Epoch 386/1000 
	 loss: 71.1539, MinusLogProbMetric: 71.1539, val_loss: 71.9108, val_MinusLogProbMetric: 71.9108

Epoch 386: val_loss did not improve from 71.41267
196/196 - 157s - loss: 71.1539 - MinusLogProbMetric: 71.1539 - val_loss: 71.9108 - val_MinusLogProbMetric: 71.9108 - lr: 5.5556e-05 - 157s/epoch - 800ms/step
Epoch 387/1000
2023-10-01 20:35:37.896 
Epoch 387/1000 
	 loss: 71.2880, MinusLogProbMetric: 71.2880, val_loss: 71.3488, val_MinusLogProbMetric: 71.3488

Epoch 387: val_loss improved from 71.41267 to 71.34881, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 173s - loss: 71.2880 - MinusLogProbMetric: 71.2880 - val_loss: 71.3488 - val_MinusLogProbMetric: 71.3488 - lr: 5.5556e-05 - 173s/epoch - 881ms/step
Epoch 388/1000
2023-10-01 20:38:18.963 
Epoch 388/1000 
	 loss: 70.8892, MinusLogProbMetric: 70.8892, val_loss: 72.3759, val_MinusLogProbMetric: 72.3759

Epoch 388: val_loss did not improve from 71.34881
196/196 - 159s - loss: 70.8892 - MinusLogProbMetric: 70.8892 - val_loss: 72.3759 - val_MinusLogProbMetric: 72.3759 - lr: 5.5556e-05 - 159s/epoch - 812ms/step
Epoch 389/1000
2023-10-01 20:41:00.736 
Epoch 389/1000 
	 loss: 75.3513, MinusLogProbMetric: 75.3513, val_loss: 74.0673, val_MinusLogProbMetric: 74.0673

Epoch 389: val_loss did not improve from 71.34881
196/196 - 162s - loss: 75.3513 - MinusLogProbMetric: 75.3513 - val_loss: 74.0673 - val_MinusLogProbMetric: 74.0673 - lr: 5.5556e-05 - 162s/epoch - 825ms/step
Epoch 390/1000
2023-10-01 20:43:42.321 
Epoch 390/1000 
	 loss: 75.9652, MinusLogProbMetric: 75.9652, val_loss: 73.2067, val_MinusLogProbMetric: 73.2067

Epoch 390: val_loss did not improve from 71.34881
196/196 - 162s - loss: 75.9652 - MinusLogProbMetric: 75.9652 - val_loss: 73.2067 - val_MinusLogProbMetric: 73.2067 - lr: 5.5556e-05 - 162s/epoch - 825ms/step
Epoch 391/1000
2023-10-01 20:46:29.459 
Epoch 391/1000 
	 loss: 71.8513, MinusLogProbMetric: 71.8513, val_loss: 70.9805, val_MinusLogProbMetric: 70.9805

Epoch 391: val_loss improved from 71.34881 to 70.98045, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 170s - loss: 71.8513 - MinusLogProbMetric: 71.8513 - val_loss: 70.9805 - val_MinusLogProbMetric: 70.9805 - lr: 5.5556e-05 - 170s/epoch - 869ms/step
Epoch 392/1000
2023-10-01 20:49:12.385 
Epoch 392/1000 
	 loss: 71.3163, MinusLogProbMetric: 71.3163, val_loss: 73.4371, val_MinusLogProbMetric: 73.4371

Epoch 392: val_loss did not improve from 70.98045
196/196 - 160s - loss: 71.3163 - MinusLogProbMetric: 71.3163 - val_loss: 73.4371 - val_MinusLogProbMetric: 73.4371 - lr: 5.5556e-05 - 160s/epoch - 815ms/step
Epoch 393/1000
2023-10-01 20:51:56.533 
Epoch 393/1000 
	 loss: 72.3898, MinusLogProbMetric: 72.3898, val_loss: 70.7834, val_MinusLogProbMetric: 70.7834

Epoch 393: val_loss improved from 70.98045 to 70.78340, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 168s - loss: 72.3898 - MinusLogProbMetric: 72.3898 - val_loss: 70.7834 - val_MinusLogProbMetric: 70.7834 - lr: 5.5556e-05 - 168s/epoch - 858ms/step
Epoch 394/1000
2023-10-01 20:54:50.406 
Epoch 394/1000 
	 loss: 70.3733, MinusLogProbMetric: 70.3733, val_loss: 70.4094, val_MinusLogProbMetric: 70.4094

Epoch 394: val_loss improved from 70.78340 to 70.40943, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 173s - loss: 70.3733 - MinusLogProbMetric: 70.3733 - val_loss: 70.4094 - val_MinusLogProbMetric: 70.4094 - lr: 5.5556e-05 - 173s/epoch - 882ms/step
Epoch 395/1000
2023-10-01 20:57:32.788 
Epoch 395/1000 
	 loss: 70.0466, MinusLogProbMetric: 70.0466, val_loss: 70.4766, val_MinusLogProbMetric: 70.4766

Epoch 395: val_loss did not improve from 70.40943
196/196 - 159s - loss: 70.0466 - MinusLogProbMetric: 70.0466 - val_loss: 70.4766 - val_MinusLogProbMetric: 70.4766 - lr: 5.5556e-05 - 159s/epoch - 812ms/step
Epoch 396/1000
2023-10-01 21:00:26.976 
Epoch 396/1000 
	 loss: 70.0202, MinusLogProbMetric: 70.0202, val_loss: 72.5178, val_MinusLogProbMetric: 72.5178

Epoch 396: val_loss did not improve from 70.40943
196/196 - 174s - loss: 70.0202 - MinusLogProbMetric: 70.0202 - val_loss: 72.5178 - val_MinusLogProbMetric: 72.5178 - lr: 5.5556e-05 - 174s/epoch - 889ms/step
Epoch 397/1000
2023-10-01 21:03:15.198 
Epoch 397/1000 
	 loss: 70.0223, MinusLogProbMetric: 70.0223, val_loss: 70.3041, val_MinusLogProbMetric: 70.3041

Epoch 397: val_loss improved from 70.40943 to 70.30410, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 170s - loss: 70.0223 - MinusLogProbMetric: 70.0223 - val_loss: 70.3041 - val_MinusLogProbMetric: 70.3041 - lr: 5.5556e-05 - 170s/epoch - 870ms/step
Epoch 398/1000
2023-10-01 21:06:09.115 
Epoch 398/1000 
	 loss: 73.1423, MinusLogProbMetric: 73.1423, val_loss: 76.0565, val_MinusLogProbMetric: 76.0565

Epoch 398: val_loss did not improve from 70.30410
196/196 - 172s - loss: 73.1423 - MinusLogProbMetric: 73.1423 - val_loss: 76.0565 - val_MinusLogProbMetric: 76.0565 - lr: 5.5556e-05 - 172s/epoch - 875ms/step
Epoch 399/1000
2023-10-01 21:09:00.188 
Epoch 399/1000 
	 loss: 70.2445, MinusLogProbMetric: 70.2445, val_loss: 69.2603, val_MinusLogProbMetric: 69.2603

Epoch 399: val_loss improved from 70.30410 to 69.26025, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 174s - loss: 70.2445 - MinusLogProbMetric: 70.2445 - val_loss: 69.2603 - val_MinusLogProbMetric: 69.2603 - lr: 5.5556e-05 - 174s/epoch - 886ms/step
Epoch 400/1000
2023-10-01 21:11:49.969 
Epoch 400/1000 
	 loss: 69.3446, MinusLogProbMetric: 69.3446, val_loss: 69.3401, val_MinusLogProbMetric: 69.3401

Epoch 400: val_loss did not improve from 69.26025
196/196 - 167s - loss: 69.3446 - MinusLogProbMetric: 69.3446 - val_loss: 69.3401 - val_MinusLogProbMetric: 69.3401 - lr: 5.5556e-05 - 167s/epoch - 853ms/step
Epoch 401/1000
2023-10-01 21:14:42.092 
Epoch 401/1000 
	 loss: 69.3902, MinusLogProbMetric: 69.3902, val_loss: 69.1060, val_MinusLogProbMetric: 69.1060

Epoch 401: val_loss improved from 69.26025 to 69.10603, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 175s - loss: 69.3902 - MinusLogProbMetric: 69.3902 - val_loss: 69.1060 - val_MinusLogProbMetric: 69.1060 - lr: 5.5556e-05 - 175s/epoch - 894ms/step
Epoch 402/1000
2023-10-01 21:17:31.315 
Epoch 402/1000 
	 loss: 68.7117, MinusLogProbMetric: 68.7117, val_loss: 68.8733, val_MinusLogProbMetric: 68.8733

Epoch 402: val_loss improved from 69.10603 to 68.87327, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 168s - loss: 68.7117 - MinusLogProbMetric: 68.7117 - val_loss: 68.8733 - val_MinusLogProbMetric: 68.8733 - lr: 5.5556e-05 - 168s/epoch - 859ms/step
Epoch 403/1000
2023-10-01 21:20:13.966 
Epoch 403/1000 
	 loss: 68.6356, MinusLogProbMetric: 68.6356, val_loss: 68.6141, val_MinusLogProbMetric: 68.6141

Epoch 403: val_loss improved from 68.87327 to 68.61407, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 163s - loss: 68.6356 - MinusLogProbMetric: 68.6356 - val_loss: 68.6141 - val_MinusLogProbMetric: 68.6141 - lr: 5.5556e-05 - 163s/epoch - 833ms/step
Epoch 404/1000
2023-10-01 21:23:03.494 
Epoch 404/1000 
	 loss: 68.5055, MinusLogProbMetric: 68.5055, val_loss: 68.3536, val_MinusLogProbMetric: 68.3536

Epoch 404: val_loss improved from 68.61407 to 68.35359, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 170s - loss: 68.5055 - MinusLogProbMetric: 68.5055 - val_loss: 68.3536 - val_MinusLogProbMetric: 68.3536 - lr: 5.5556e-05 - 170s/epoch - 868ms/step
Epoch 405/1000
2023-10-01 21:25:56.054 
Epoch 405/1000 
	 loss: 68.8956, MinusLogProbMetric: 68.8956, val_loss: 68.6953, val_MinusLogProbMetric: 68.6953

Epoch 405: val_loss did not improve from 68.35359
196/196 - 169s - loss: 68.8956 - MinusLogProbMetric: 68.8956 - val_loss: 68.6953 - val_MinusLogProbMetric: 68.6953 - lr: 5.5556e-05 - 169s/epoch - 863ms/step
Epoch 406/1000
2023-10-01 21:28:42.898 
Epoch 406/1000 
	 loss: 68.2580, MinusLogProbMetric: 68.2580, val_loss: 68.4114, val_MinusLogProbMetric: 68.4114

Epoch 406: val_loss did not improve from 68.35359
196/196 - 167s - loss: 68.2580 - MinusLogProbMetric: 68.2580 - val_loss: 68.4114 - val_MinusLogProbMetric: 68.4114 - lr: 5.5556e-05 - 167s/epoch - 851ms/step
Epoch 407/1000
2023-10-01 21:31:24.646 
Epoch 407/1000 
	 loss: 68.1557, MinusLogProbMetric: 68.1557, val_loss: 68.1328, val_MinusLogProbMetric: 68.1328

Epoch 407: val_loss improved from 68.35359 to 68.13276, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 164s - loss: 68.1557 - MinusLogProbMetric: 68.1557 - val_loss: 68.1328 - val_MinusLogProbMetric: 68.1328 - lr: 5.5556e-05 - 164s/epoch - 835ms/step
Epoch 408/1000
2023-10-01 21:34:09.192 
Epoch 408/1000 
	 loss: 68.1475, MinusLogProbMetric: 68.1475, val_loss: 68.5185, val_MinusLogProbMetric: 68.5185

Epoch 408: val_loss did not improve from 68.13276
196/196 - 163s - loss: 68.1475 - MinusLogProbMetric: 68.1475 - val_loss: 68.5185 - val_MinusLogProbMetric: 68.5185 - lr: 5.5556e-05 - 163s/epoch - 829ms/step
Epoch 409/1000
2023-10-01 21:36:52.274 
Epoch 409/1000 
	 loss: 68.0947, MinusLogProbMetric: 68.0947, val_loss: 68.6890, val_MinusLogProbMetric: 68.6890

Epoch 409: val_loss did not improve from 68.13276
196/196 - 163s - loss: 68.0947 - MinusLogProbMetric: 68.0947 - val_loss: 68.6890 - val_MinusLogProbMetric: 68.6890 - lr: 5.5556e-05 - 163s/epoch - 832ms/step
Epoch 410/1000
2023-10-01 21:39:45.441 
Epoch 410/1000 
	 loss: 68.0389, MinusLogProbMetric: 68.0389, val_loss: 68.5621, val_MinusLogProbMetric: 68.5621

Epoch 410: val_loss did not improve from 68.13276
196/196 - 173s - loss: 68.0389 - MinusLogProbMetric: 68.0389 - val_loss: 68.5621 - val_MinusLogProbMetric: 68.5621 - lr: 5.5556e-05 - 173s/epoch - 883ms/step
Epoch 411/1000
2023-10-01 21:42:42.362 
Epoch 411/1000 
	 loss: 67.7424, MinusLogProbMetric: 67.7424, val_loss: 67.9113, val_MinusLogProbMetric: 67.9113

Epoch 411: val_loss improved from 68.13276 to 67.91134, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 180s - loss: 67.7424 - MinusLogProbMetric: 67.7424 - val_loss: 67.9113 - val_MinusLogProbMetric: 67.9113 - lr: 5.5556e-05 - 180s/epoch - 917ms/step
Epoch 412/1000
2023-10-01 21:45:30.771 
Epoch 412/1000 
	 loss: 67.7098, MinusLogProbMetric: 67.7098, val_loss: 67.7835, val_MinusLogProbMetric: 67.7835

Epoch 412: val_loss improved from 67.91134 to 67.78352, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 169s - loss: 67.7098 - MinusLogProbMetric: 67.7098 - val_loss: 67.7835 - val_MinusLogProbMetric: 67.7835 - lr: 5.5556e-05 - 169s/epoch - 860ms/step
Epoch 413/1000
2023-10-01 21:48:18.149 
Epoch 413/1000 
	 loss: 67.8192, MinusLogProbMetric: 67.8192, val_loss: 68.1606, val_MinusLogProbMetric: 68.1606

Epoch 413: val_loss did not improve from 67.78352
196/196 - 164s - loss: 67.8192 - MinusLogProbMetric: 67.8192 - val_loss: 68.1606 - val_MinusLogProbMetric: 68.1606 - lr: 5.5556e-05 - 164s/epoch - 839ms/step
Epoch 414/1000
2023-10-01 21:51:03.494 
Epoch 414/1000 
	 loss: 67.7418, MinusLogProbMetric: 67.7418, val_loss: 68.6454, val_MinusLogProbMetric: 68.6454

Epoch 414: val_loss did not improve from 67.78352
196/196 - 165s - loss: 67.7418 - MinusLogProbMetric: 67.7418 - val_loss: 68.6454 - val_MinusLogProbMetric: 68.6454 - lr: 5.5556e-05 - 165s/epoch - 844ms/step
Epoch 415/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-01 21:53:47.946 
Epoch 415/1000 
	 loss: 67.4942, MinusLogProbMetric: 67.4942, val_loss: 67.9657, val_MinusLogProbMetric: 67.9657

Epoch 415: val_loss did not improve from 67.78352
196/196 - 164s - loss: 67.4942 - MinusLogProbMetric: 67.4942 - val_loss: 67.9657 - val_MinusLogProbMetric: 67.9657 - lr: 5.5556e-05 - 164s/epoch - 839ms/step
Epoch 416/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-01 21:56:45.125 
Epoch 416/1000 
	 loss: 67.7292, MinusLogProbMetric: 67.7292, val_loss: 67.8477, val_MinusLogProbMetric: 67.8477

Epoch 416: val_loss did not improve from 67.78352
196/196 - 177s - loss: 67.7292 - MinusLogProbMetric: 67.7292 - val_loss: 67.8477 - val_MinusLogProbMetric: 67.8477 - lr: 5.5556e-05 - 177s/epoch - 904ms/step
Epoch 417/1000
2023-10-01 21:59:33.955 
Epoch 417/1000 
	 loss: 67.3087, MinusLogProbMetric: 67.3087, val_loss: 67.2660, val_MinusLogProbMetric: 67.2660

Epoch 417: val_loss improved from 67.78352 to 67.26601, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 172s - loss: 67.3087 - MinusLogProbMetric: 67.3087 - val_loss: 67.2660 - val_MinusLogProbMetric: 67.2660 - lr: 5.5556e-05 - 172s/epoch - 878ms/step
Epoch 418/1000
2023-10-01 22:02:39.436 
Epoch 418/1000 
	 loss: 67.2340, MinusLogProbMetric: 67.2340, val_loss: 69.1889, val_MinusLogProbMetric: 69.1889

Epoch 418: val_loss did not improve from 67.26601
196/196 - 182s - loss: 67.2340 - MinusLogProbMetric: 67.2340 - val_loss: 69.1889 - val_MinusLogProbMetric: 69.1889 - lr: 5.5556e-05 - 182s/epoch - 929ms/step
Epoch 419/1000
2023-10-01 22:05:25.324 
Epoch 419/1000 
	 loss: 67.2412, MinusLogProbMetric: 67.2412, val_loss: 67.2187, val_MinusLogProbMetric: 67.2187

Epoch 419: val_loss improved from 67.26601 to 67.21873, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 169s - loss: 67.2412 - MinusLogProbMetric: 67.2412 - val_loss: 67.2187 - val_MinusLogProbMetric: 67.2187 - lr: 5.5556e-05 - 169s/epoch - 864ms/step
Epoch 420/1000
2023-10-01 22:08:19.267 
Epoch 420/1000 
	 loss: 66.9907, MinusLogProbMetric: 66.9907, val_loss: 67.0395, val_MinusLogProbMetric: 67.0395

Epoch 420: val_loss improved from 67.21873 to 67.03948, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 173s - loss: 66.9907 - MinusLogProbMetric: 66.9907 - val_loss: 67.0395 - val_MinusLogProbMetric: 67.0395 - lr: 5.5556e-05 - 173s/epoch - 885ms/step
Epoch 421/1000
2023-10-01 22:11:14.471 
Epoch 421/1000 
	 loss: 67.0669, MinusLogProbMetric: 67.0669, val_loss: 67.3416, val_MinusLogProbMetric: 67.3416

Epoch 421: val_loss did not improve from 67.03948
196/196 - 172s - loss: 67.0669 - MinusLogProbMetric: 67.0669 - val_loss: 67.3416 - val_MinusLogProbMetric: 67.3416 - lr: 5.5556e-05 - 172s/epoch - 878ms/step
Epoch 422/1000
2023-10-01 22:14:23.304 
Epoch 422/1000 
	 loss: 66.8178, MinusLogProbMetric: 66.8178, val_loss: 67.1568, val_MinusLogProbMetric: 67.1568

Epoch 422: val_loss did not improve from 67.03948
196/196 - 189s - loss: 66.8178 - MinusLogProbMetric: 66.8178 - val_loss: 67.1568 - val_MinusLogProbMetric: 67.1568 - lr: 5.5556e-05 - 189s/epoch - 963ms/step
Epoch 423/1000
2023-10-01 22:17:04.461 
Epoch 423/1000 
	 loss: 66.7849, MinusLogProbMetric: 66.7849, val_loss: 66.8118, val_MinusLogProbMetric: 66.8118

Epoch 423: val_loss improved from 67.03948 to 66.81176, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 164s - loss: 66.7849 - MinusLogProbMetric: 66.7849 - val_loss: 66.8118 - val_MinusLogProbMetric: 66.8118 - lr: 5.5556e-05 - 164s/epoch - 836ms/step
Epoch 424/1000
2023-10-01 22:19:54.140 
Epoch 424/1000 
	 loss: 66.8120, MinusLogProbMetric: 66.8120, val_loss: 66.4430, val_MinusLogProbMetric: 66.4430

Epoch 424: val_loss improved from 66.81176 to 66.44298, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 170s - loss: 66.8120 - MinusLogProbMetric: 66.8120 - val_loss: 66.4430 - val_MinusLogProbMetric: 66.4430 - lr: 5.5556e-05 - 170s/epoch - 865ms/step
Epoch 425/1000
2023-10-01 22:22:39.853 
Epoch 425/1000 
	 loss: 66.4592, MinusLogProbMetric: 66.4592, val_loss: 66.4999, val_MinusLogProbMetric: 66.4999

Epoch 425: val_loss did not improve from 66.44298
196/196 - 163s - loss: 66.4592 - MinusLogProbMetric: 66.4592 - val_loss: 66.4999 - val_MinusLogProbMetric: 66.4999 - lr: 5.5556e-05 - 163s/epoch - 831ms/step
Epoch 426/1000
2023-10-01 22:25:26.017 
Epoch 426/1000 
	 loss: 66.4039, MinusLogProbMetric: 66.4039, val_loss: 66.6279, val_MinusLogProbMetric: 66.6279

Epoch 426: val_loss did not improve from 66.44298
196/196 - 166s - loss: 66.4039 - MinusLogProbMetric: 66.4039 - val_loss: 66.6279 - val_MinusLogProbMetric: 66.6279 - lr: 5.5556e-05 - 166s/epoch - 848ms/step
Epoch 427/1000
2023-10-01 22:28:16.551 
Epoch 427/1000 
	 loss: 66.7065, MinusLogProbMetric: 66.7065, val_loss: 66.8474, val_MinusLogProbMetric: 66.8474

Epoch 427: val_loss did not improve from 66.44298
196/196 - 171s - loss: 66.7065 - MinusLogProbMetric: 66.7065 - val_loss: 66.8474 - val_MinusLogProbMetric: 66.8474 - lr: 5.5556e-05 - 171s/epoch - 870ms/step
Epoch 428/1000
2023-10-01 22:31:09.769 
Epoch 428/1000 
	 loss: 66.6868, MinusLogProbMetric: 66.6868, val_loss: 66.5898, val_MinusLogProbMetric: 66.5898

Epoch 428: val_loss did not improve from 66.44298
196/196 - 173s - loss: 66.6868 - MinusLogProbMetric: 66.6868 - val_loss: 66.5898 - val_MinusLogProbMetric: 66.5898 - lr: 5.5556e-05 - 173s/epoch - 884ms/step
Epoch 429/1000
2023-10-01 22:34:00.168 
Epoch 429/1000 
	 loss: 66.2395, MinusLogProbMetric: 66.2395, val_loss: 66.2747, val_MinusLogProbMetric: 66.2747

Epoch 429: val_loss improved from 66.44298 to 66.27470, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 173s - loss: 66.2395 - MinusLogProbMetric: 66.2395 - val_loss: 66.2747 - val_MinusLogProbMetric: 66.2747 - lr: 5.5556e-05 - 173s/epoch - 881ms/step
Epoch 430/1000
2023-10-01 22:36:54.793 
Epoch 430/1000 
	 loss: 66.0991, MinusLogProbMetric: 66.0991, val_loss: 66.4862, val_MinusLogProbMetric: 66.4862

Epoch 430: val_loss did not improve from 66.27470
196/196 - 172s - loss: 66.0991 - MinusLogProbMetric: 66.0991 - val_loss: 66.4862 - val_MinusLogProbMetric: 66.4862 - lr: 5.5556e-05 - 172s/epoch - 879ms/step
Epoch 431/1000
2023-10-01 22:39:35.432 
Epoch 431/1000 
	 loss: 66.1100, MinusLogProbMetric: 66.1100, val_loss: 66.6696, val_MinusLogProbMetric: 66.6696

Epoch 431: val_loss did not improve from 66.27470
196/196 - 161s - loss: 66.1100 - MinusLogProbMetric: 66.1100 - val_loss: 66.6696 - val_MinusLogProbMetric: 66.6696 - lr: 5.5556e-05 - 161s/epoch - 819ms/step
Epoch 432/1000
2023-10-01 22:42:26.373 
Epoch 432/1000 
	 loss: 66.1944, MinusLogProbMetric: 66.1944, val_loss: 66.8790, val_MinusLogProbMetric: 66.8790

Epoch 432: val_loss did not improve from 66.27470
196/196 - 171s - loss: 66.1944 - MinusLogProbMetric: 66.1944 - val_loss: 66.8790 - val_MinusLogProbMetric: 66.8790 - lr: 5.5556e-05 - 171s/epoch - 872ms/step
Epoch 433/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-01 22:45:21.364 
Epoch 433/1000 
	 loss: 66.4235, MinusLogProbMetric: 66.4235, val_loss: 65.9611, val_MinusLogProbMetric: 65.9611

Epoch 433: val_loss improved from 66.27470 to 65.96114, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 178s - loss: 66.4235 - MinusLogProbMetric: 66.4235 - val_loss: 65.9611 - val_MinusLogProbMetric: 65.9611 - lr: 5.5556e-05 - 178s/epoch - 907ms/step
Epoch 434/1000
2023-10-01 22:48:13.422 
Epoch 434/1000 
	 loss: 65.8769, MinusLogProbMetric: 65.8769, val_loss: 66.4848, val_MinusLogProbMetric: 66.4848

Epoch 434: val_loss did not improve from 65.96114
196/196 - 169s - loss: 65.8769 - MinusLogProbMetric: 65.8769 - val_loss: 66.4848 - val_MinusLogProbMetric: 66.4848 - lr: 5.5556e-05 - 169s/epoch - 864ms/step
Epoch 435/1000
2023-10-01 22:50:59.280 
Epoch 435/1000 
	 loss: 65.6315, MinusLogProbMetric: 65.6315, val_loss: 66.7900, val_MinusLogProbMetric: 66.7900

Epoch 435: val_loss did not improve from 65.96114
196/196 - 166s - loss: 65.6315 - MinusLogProbMetric: 65.6315 - val_loss: 66.7900 - val_MinusLogProbMetric: 66.7900 - lr: 5.5556e-05 - 166s/epoch - 846ms/step
Epoch 436/1000
2023-10-01 22:53:45.013 
Epoch 436/1000 
	 loss: 65.8418, MinusLogProbMetric: 65.8418, val_loss: 65.7888, val_MinusLogProbMetric: 65.7888

Epoch 436: val_loss improved from 65.96114 to 65.78881, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 171s - loss: 65.8418 - MinusLogProbMetric: 65.8418 - val_loss: 65.7888 - val_MinusLogProbMetric: 65.7888 - lr: 5.5556e-05 - 171s/epoch - 871ms/step
Epoch 437/1000
2023-10-01 22:56:34.131 
Epoch 437/1000 
	 loss: 65.6823, MinusLogProbMetric: 65.6823, val_loss: 66.3951, val_MinusLogProbMetric: 66.3951

Epoch 437: val_loss did not improve from 65.78881
196/196 - 164s - loss: 65.6823 - MinusLogProbMetric: 65.6823 - val_loss: 66.3951 - val_MinusLogProbMetric: 66.3951 - lr: 5.5556e-05 - 164s/epoch - 838ms/step
Epoch 438/1000
2023-10-01 22:59:19.617 
Epoch 438/1000 
	 loss: 65.5623, MinusLogProbMetric: 65.5623, val_loss: 65.3776, val_MinusLogProbMetric: 65.3776

Epoch 438: val_loss improved from 65.78881 to 65.37762, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 168s - loss: 65.5623 - MinusLogProbMetric: 65.5623 - val_loss: 65.3776 - val_MinusLogProbMetric: 65.3776 - lr: 5.5556e-05 - 168s/epoch - 859ms/step
Epoch 439/1000
2023-10-01 23:02:08.468 
Epoch 439/1000 
	 loss: 65.4855, MinusLogProbMetric: 65.4855, val_loss: 65.7855, val_MinusLogProbMetric: 65.7855

Epoch 439: val_loss did not improve from 65.37762
196/196 - 166s - loss: 65.4855 - MinusLogProbMetric: 65.4855 - val_loss: 65.7855 - val_MinusLogProbMetric: 65.7855 - lr: 5.5556e-05 - 166s/epoch - 847ms/step
Epoch 440/1000
2023-10-01 23:05:00.314 
Epoch 440/1000 
	 loss: 65.3084, MinusLogProbMetric: 65.3084, val_loss: 66.6211, val_MinusLogProbMetric: 66.6211

Epoch 440: val_loss did not improve from 65.37762
196/196 - 172s - loss: 65.3084 - MinusLogProbMetric: 65.3084 - val_loss: 66.6211 - val_MinusLogProbMetric: 66.6211 - lr: 5.5556e-05 - 172s/epoch - 877ms/step
Epoch 441/1000
2023-10-01 23:08:00.498 
Epoch 441/1000 
	 loss: 65.1641, MinusLogProbMetric: 65.1641, val_loss: 66.0068, val_MinusLogProbMetric: 66.0068

Epoch 441: val_loss did not improve from 65.37762
196/196 - 180s - loss: 65.1641 - MinusLogProbMetric: 65.1641 - val_loss: 66.0068 - val_MinusLogProbMetric: 66.0068 - lr: 5.5556e-05 - 180s/epoch - 919ms/step
Epoch 442/1000
2023-10-01 23:11:03.337 
Epoch 442/1000 
	 loss: 65.0988, MinusLogProbMetric: 65.0988, val_loss: 65.5563, val_MinusLogProbMetric: 65.5563

Epoch 442: val_loss did not improve from 65.37762
196/196 - 183s - loss: 65.0988 - MinusLogProbMetric: 65.0988 - val_loss: 65.5563 - val_MinusLogProbMetric: 65.5563 - lr: 5.5556e-05 - 183s/epoch - 933ms/step
Epoch 443/1000
2023-10-01 23:13:51.893 
Epoch 443/1000 
	 loss: 65.1038, MinusLogProbMetric: 65.1038, val_loss: 65.2611, val_MinusLogProbMetric: 65.2611

Epoch 443: val_loss improved from 65.37762 to 65.26106, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 171s - loss: 65.1038 - MinusLogProbMetric: 65.1038 - val_loss: 65.2611 - val_MinusLogProbMetric: 65.2611 - lr: 5.5556e-05 - 171s/epoch - 873ms/step
Epoch 444/1000
2023-10-01 23:16:49.880 
Epoch 444/1000 
	 loss: 64.9262, MinusLogProbMetric: 64.9262, val_loss: 71.7253, val_MinusLogProbMetric: 71.7253

Epoch 444: val_loss did not improve from 65.26106
196/196 - 175s - loss: 64.9262 - MinusLogProbMetric: 64.9262 - val_loss: 71.7253 - val_MinusLogProbMetric: 71.7253 - lr: 5.5556e-05 - 175s/epoch - 895ms/step
Epoch 445/1000
2023-10-01 23:19:59.050 
Epoch 445/1000 
	 loss: 65.6358, MinusLogProbMetric: 65.6358, val_loss: 65.5279, val_MinusLogProbMetric: 65.5279

Epoch 445: val_loss did not improve from 65.26106
196/196 - 189s - loss: 65.6358 - MinusLogProbMetric: 65.6358 - val_loss: 65.5279 - val_MinusLogProbMetric: 65.5279 - lr: 5.5556e-05 - 189s/epoch - 965ms/step
Epoch 446/1000
2023-10-01 23:23:10.483 
Epoch 446/1000 
	 loss: 65.5664, MinusLogProbMetric: 65.5664, val_loss: 65.7093, val_MinusLogProbMetric: 65.7093

Epoch 446: val_loss did not improve from 65.26106
196/196 - 191s - loss: 65.5664 - MinusLogProbMetric: 65.5664 - val_loss: 65.7093 - val_MinusLogProbMetric: 65.7093 - lr: 5.5556e-05 - 191s/epoch - 977ms/step
Epoch 447/1000
2023-10-01 23:26:30.846 
Epoch 447/1000 
	 loss: 65.2489, MinusLogProbMetric: 65.2489, val_loss: 66.8924, val_MinusLogProbMetric: 66.8924

Epoch 447: val_loss did not improve from 65.26106
196/196 - 200s - loss: 65.2489 - MinusLogProbMetric: 65.2489 - val_loss: 66.8924 - val_MinusLogProbMetric: 66.8924 - lr: 5.5556e-05 - 200s/epoch - 1s/step
Epoch 448/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 95: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-01 23:28:12.192 
Epoch 448/1000 
	 loss: nan, MinusLogProbMetric: 74.1224, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 448: val_loss did not improve from 65.26106
196/196 - 101s - loss: nan - MinusLogProbMetric: 74.1224 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 5.5556e-05 - 101s/epoch - 517ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 3.703703703703703e-05.
===========
Generating train data for run 350.
===========
Train data generated in 1.63 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_350/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_350/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_350/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_350
self.data_kwargs: {'seed': 440}
self.x_data: [[ 2.0547452   3.776408    6.9601746  ...  5.8637457  -0.4179731
   2.747595  ]
 [ 6.405085    2.999549    6.1033454  ...  2.8107233   3.0569289
   2.003619  ]
 [ 2.0047038   4.064143    6.2048492  ...  5.6568336   0.71395487
   2.6926432 ]
 ...
 [ 5.645455    7.0971727   6.202566   ...  0.61970913  6.238492
   1.3539525 ]
 [ 6.475603    3.1151166   6.2248073  ...  2.8990176   3.6724448
   2.289971  ]
 [ 2.2290072   4.101192    7.1413527  ...  6.717821   -2.0046902
   2.3999293 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_364"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_365 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_34 (LogProbL  (None,)                  3291840   
 ayer)                                                           
                                                                 
=================================================================
Total params: 3,291,840
Trainable params: 3,291,840
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_34/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_34'")
self.model: <keras.engine.functional.Functional object at 0x7fae788b5e10>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7faeb0e5d270>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7faeb0e5d270>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fb86c94b910>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fb144383a30>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fb144383fa0>, <keras.callbacks.ModelCheckpoint object at 0x7fb144383f40>, <keras.callbacks.EarlyStopping object at 0x7fb144383f70>, <keras.callbacks.ReduceLROnPlateau object at 0x7fb14438c130>, <keras.callbacks.TerminateOnNaN object at 0x7fb14438c310>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 350/720 with hyperparameters:
timestamp = 2023-10-01 23:29:03.179347
ndims = 64
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 3291840
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 3.703703703703703e-05...
Train first sample: [ 2.0547452   3.776408    6.9601746   1.2163222   8.644804    1.0450234
  9.756204    5.9333344   9.3759165   6.2056174   7.6663713   0.8787037
  2.7781603   1.7675867   3.9287794   0.8286338   3.6572614   4.846633
  0.68292063  6.317236    6.48625     3.843101    5.88724     2.0270782
  5.1297107   7.7579265   2.6054757   6.6726594   1.0543873   7.123391
  3.5526195   2.046467    5.620422   -0.869851    8.648443    0.02500496
  7.4141846   2.6777635   7.6790566   9.76465     2.481489    6.2672224
  5.7068043   4.504637    1.1161722   9.691578    4.0927534   8.503739
  6.9964066   3.1729455   8.086372    4.0078015   8.096493    5.813139
  8.181514    6.97417     7.246071    5.0315676  10.666024    6.2678714
  3.6855178   5.8637457  -0.4179731   2.747595  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 7: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-01 23:43:23.596 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 544.3489, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 858s - loss: nan - MinusLogProbMetric: 544.3489 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.7037e-05 - 858s/epoch - 4s/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 1.2345679012345677e-05.
===========
Generating train data for run 350.
===========
Train data generated in 1.64 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_350/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_350/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_350/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_350
self.data_kwargs: {'seed': 440}
self.x_data: [[ 2.0547452   3.776408    6.9601746  ...  5.8637457  -0.4179731
   2.747595  ]
 [ 6.405085    2.999549    6.1033454  ...  2.8107233   3.0569289
   2.003619  ]
 [ 2.0047038   4.064143    6.2048492  ...  5.6568336   0.71395487
   2.6926432 ]
 ...
 [ 5.645455    7.0971727   6.202566   ...  0.61970913  6.238492
   1.3539525 ]
 [ 6.475603    3.1151166   6.2248073  ...  2.8990176   3.6724448
   2.289971  ]
 [ 2.2290072   4.101192    7.1413527  ...  6.717821   -2.0046902
   2.3999293 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_375"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_376 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_35 (LogProbL  (None,)                  3291840   
 ayer)                                                           
                                                                 
=================================================================
Total params: 3,291,840
Trainable params: 3,291,840
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_35/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_35'")
self.model: <keras.engine.functional.Functional object at 0x7fad0a247760>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fb064f89750>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fb064f89750>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fad4830b6a0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fb064ea2950>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fb064ea2ec0>, <keras.callbacks.ModelCheckpoint object at 0x7fb064ea2f80>, <keras.callbacks.EarlyStopping object at 0x7fb064ea31f0>, <keras.callbacks.ReduceLROnPlateau object at 0x7fb064ea3220>, <keras.callbacks.TerminateOnNaN object at 0x7fb064ea2e60>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 350/720 with hyperparameters:
timestamp = 2023-10-01 23:44:08.989529
ndims = 64
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 3291840
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.2345679012345677e-05...
Train first sample: [ 2.0547452   3.776408    6.9601746   1.2163222   8.644804    1.0450234
  9.756204    5.9333344   9.3759165   6.2056174   7.6663713   0.8787037
  2.7781603   1.7675867   3.9287794   0.8286338   3.6572614   4.846633
  0.68292063  6.317236    6.48625     3.843101    5.88724     2.0270782
  5.1297107   7.7579265   2.6054757   6.6726594   1.0543873   7.123391
  3.5526195   2.046467    5.620422   -0.869851    8.648443    0.02500496
  7.4141846   2.6777635   7.6790566   9.76465     2.481489    6.2672224
  5.7068043   4.504637    1.1161722   9.691578    4.0927534   8.503739
  6.9964066   3.1729455   8.086372    4.0078015   8.096493    5.813139
  8.181514    6.97417     7.246071    5.0315676  10.666024    6.2678714
  3.6855178   5.8637457  -0.4179731   2.747595  ]
Epoch 1/1000
2023-10-02 00:03:29.727 
Epoch 1/1000 
	 loss: 77.8346, MinusLogProbMetric: 77.8346, val_loss: 69.3887, val_MinusLogProbMetric: 69.3887

Epoch 1: val_loss improved from inf to 69.38867, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 1162s - loss: 77.8346 - MinusLogProbMetric: 77.8346 - val_loss: 69.3887 - val_MinusLogProbMetric: 69.3887 - lr: 1.2346e-05 - 1162s/epoch - 6s/step
Epoch 2/1000
2023-10-02 00:05:52.975 
Epoch 2/1000 
	 loss: 66.8857, MinusLogProbMetric: 66.8857, val_loss: 65.8899, val_MinusLogProbMetric: 65.8899

Epoch 2: val_loss improved from 69.38867 to 65.88988, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 142s - loss: 66.8857 - MinusLogProbMetric: 66.8857 - val_loss: 65.8899 - val_MinusLogProbMetric: 65.8899 - lr: 1.2346e-05 - 142s/epoch - 727ms/step
Epoch 3/1000
2023-10-02 00:07:39.448 
Epoch 3/1000 
	 loss: 64.8461, MinusLogProbMetric: 64.8461, val_loss: 64.4852, val_MinusLogProbMetric: 64.4852

Epoch 3: val_loss improved from 65.88988 to 64.48520, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 105s - loss: 64.8461 - MinusLogProbMetric: 64.8461 - val_loss: 64.4852 - val_MinusLogProbMetric: 64.4852 - lr: 1.2346e-05 - 105s/epoch - 537ms/step
Epoch 4/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 61: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-02 00:08:19.377 
Epoch 4/1000 
	 loss: nan, MinusLogProbMetric: 68.7069, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 4: val_loss did not improve from 64.48520
196/196 - 39s - loss: nan - MinusLogProbMetric: 68.7069 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.2346e-05 - 39s/epoch - 197ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 4.115226337448558e-06.
===========
Generating train data for run 350.
===========
Train data generated in 0.62 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_350/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_350/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_350/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_350
self.data_kwargs: {'seed': 440}
self.x_data: [[ 2.0547452   3.776408    6.9601746  ...  5.8637457  -0.4179731
   2.747595  ]
 [ 6.405085    2.999549    6.1033454  ...  2.8107233   3.0569289
   2.003619  ]
 [ 2.0047038   4.064143    6.2048492  ...  5.6568336   0.71395487
   2.6926432 ]
 ...
 [ 5.645455    7.0971727   6.202566   ...  0.61970913  6.238492
   1.3539525 ]
 [ 6.475603    3.1151166   6.2248073  ...  2.8990176   3.6724448
   2.289971  ]
 [ 2.2290072   4.101192    7.1413527  ...  6.717821   -2.0046902
   2.3999293 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_386"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_387 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_36 (LogProbL  (None,)                  3291840   
 ayer)                                                           
                                                                 
=================================================================
Total params: 3,291,840
Trainable params: 3,291,840
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_36/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_36'")
self.model: <keras.engine.functional.Functional object at 0x7facfb483c10>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fb2d4655060>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fb2d4655060>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fb1ed5c6dd0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7facfbcc88e0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7facfbcc8e50>, <keras.callbacks.ModelCheckpoint object at 0x7facfbcc8f10>, <keras.callbacks.EarlyStopping object at 0x7facfbcc9180>, <keras.callbacks.ReduceLROnPlateau object at 0x7facfbcc91b0>, <keras.callbacks.TerminateOnNaN object at 0x7facfbcc8df0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 350/720 with hyperparameters:
timestamp = 2023-10-02 00:08:43.545080
ndims = 64
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 3291840
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.115226337448558e-06...
Train first sample: [ 2.0547452   3.776408    6.9601746   1.2163222   8.644804    1.0450234
  9.756204    5.9333344   9.3759165   6.2056174   7.6663713   0.8787037
  2.7781603   1.7675867   3.9287794   0.8286338   3.6572614   4.846633
  0.68292063  6.317236    6.48625     3.843101    5.88724     2.0270782
  5.1297107   7.7579265   2.6054757   6.6726594   1.0543873   7.123391
  3.5526195   2.046467    5.620422   -0.869851    8.648443    0.02500496
  7.4141846   2.6777635   7.6790566   9.76465     2.481489    6.2672224
  5.7068043   4.504637    1.1161722   9.691578    4.0927534   8.503739
  6.9964066   3.1729455   8.086372    4.0078015   8.096493    5.813139
  8.181514    6.97417     7.246071    5.0315676  10.666024    6.2678714
  3.6855178   5.8637457  -0.4179731   2.747595  ]
Epoch 1/1000
2023-10-02 00:17:03.211 
Epoch 1/1000 
	 loss: 64.5709, MinusLogProbMetric: 64.5709, val_loss: 62.8630, val_MinusLogProbMetric: 62.8630

Epoch 1: val_loss improved from inf to 62.86303, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 501s - loss: 64.5709 - MinusLogProbMetric: 64.5709 - val_loss: 62.8630 - val_MinusLogProbMetric: 62.8630 - lr: 4.1152e-06 - 501s/epoch - 3s/step
Epoch 2/1000
2023-10-02 00:18:40.535 
Epoch 2/1000 
	 loss: 62.4396, MinusLogProbMetric: 62.4396, val_loss: 62.1290, val_MinusLogProbMetric: 62.1290

Epoch 2: val_loss improved from 62.86303 to 62.12899, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 96s - loss: 62.4396 - MinusLogProbMetric: 62.4396 - val_loss: 62.1290 - val_MinusLogProbMetric: 62.1290 - lr: 4.1152e-06 - 96s/epoch - 492ms/step
Epoch 3/1000
2023-10-02 00:20:16.107 
Epoch 3/1000 
	 loss: 61.8605, MinusLogProbMetric: 61.8605, val_loss: 62.0049, val_MinusLogProbMetric: 62.0049

Epoch 3: val_loss improved from 62.12899 to 62.00488, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 95s - loss: 61.8605 - MinusLogProbMetric: 61.8605 - val_loss: 62.0049 - val_MinusLogProbMetric: 62.0049 - lr: 4.1152e-06 - 95s/epoch - 486ms/step
Epoch 4/1000
2023-10-02 00:21:52.811 
Epoch 4/1000 
	 loss: 61.4627, MinusLogProbMetric: 61.4627, val_loss: 61.4073, val_MinusLogProbMetric: 61.4073

Epoch 4: val_loss improved from 62.00488 to 61.40731, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 97s - loss: 61.4627 - MinusLogProbMetric: 61.4627 - val_loss: 61.4073 - val_MinusLogProbMetric: 61.4073 - lr: 4.1152e-06 - 97s/epoch - 493ms/step
Epoch 5/1000
2023-10-02 00:23:27.737 
Epoch 5/1000 
	 loss: 61.1496, MinusLogProbMetric: 61.1496, val_loss: 61.0968, val_MinusLogProbMetric: 61.0968

Epoch 5: val_loss improved from 61.40731 to 61.09679, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 95s - loss: 61.1496 - MinusLogProbMetric: 61.1496 - val_loss: 61.0968 - val_MinusLogProbMetric: 61.0968 - lr: 4.1152e-06 - 95s/epoch - 486ms/step
Epoch 6/1000
2023-10-02 00:25:02.033 
Epoch 6/1000 
	 loss: 60.7080, MinusLogProbMetric: 60.7080, val_loss: 61.8525, val_MinusLogProbMetric: 61.8525

Epoch 6: val_loss did not improve from 61.09679
196/196 - 93s - loss: 60.7080 - MinusLogProbMetric: 60.7080 - val_loss: 61.8525 - val_MinusLogProbMetric: 61.8525 - lr: 4.1152e-06 - 93s/epoch - 474ms/step
Epoch 7/1000
2023-10-02 00:26:35.186 
Epoch 7/1000 
	 loss: 60.6163, MinusLogProbMetric: 60.6163, val_loss: 60.2933, val_MinusLogProbMetric: 60.2933

Epoch 7: val_loss improved from 61.09679 to 60.29328, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 96s - loss: 60.6163 - MinusLogProbMetric: 60.6163 - val_loss: 60.2933 - val_MinusLogProbMetric: 60.2933 - lr: 4.1152e-06 - 96s/epoch - 489ms/step
Epoch 8/1000
2023-10-02 00:28:10.625 
Epoch 8/1000 
	 loss: 60.1477, MinusLogProbMetric: 60.1477, val_loss: 60.8574, val_MinusLogProbMetric: 60.8574

Epoch 8: val_loss did not improve from 60.29328
196/196 - 93s - loss: 60.1477 - MinusLogProbMetric: 60.1477 - val_loss: 60.8574 - val_MinusLogProbMetric: 60.8574 - lr: 4.1152e-06 - 93s/epoch - 473ms/step
Epoch 9/1000
2023-10-02 00:29:41.039 
Epoch 9/1000 
	 loss: 59.6475, MinusLogProbMetric: 59.6475, val_loss: 59.7627, val_MinusLogProbMetric: 59.7627

Epoch 9: val_loss improved from 60.29328 to 59.76272, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 92s - loss: 59.6475 - MinusLogProbMetric: 59.6475 - val_loss: 59.7627 - val_MinusLogProbMetric: 59.7627 - lr: 4.1152e-06 - 92s/epoch - 470ms/step
Epoch 10/1000
2023-10-02 00:31:15.188 
Epoch 10/1000 
	 loss: 60.0160, MinusLogProbMetric: 60.0160, val_loss: 60.0217, val_MinusLogProbMetric: 60.0217

Epoch 10: val_loss did not improve from 59.76272
196/196 - 92s - loss: 60.0160 - MinusLogProbMetric: 60.0160 - val_loss: 60.0217 - val_MinusLogProbMetric: 60.0217 - lr: 4.1152e-06 - 92s/epoch - 472ms/step
Epoch 11/1000
2023-10-02 00:32:47.961 
Epoch 11/1000 
	 loss: 59.2192, MinusLogProbMetric: 59.2192, val_loss: 59.4917, val_MinusLogProbMetric: 59.4917

Epoch 11: val_loss improved from 59.76272 to 59.49170, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 95s - loss: 59.2192 - MinusLogProbMetric: 59.2192 - val_loss: 59.4917 - val_MinusLogProbMetric: 59.4917 - lr: 4.1152e-06 - 95s/epoch - 484ms/step
Epoch 12/1000
2023-10-02 00:34:24.693 
Epoch 12/1000 
	 loss: 58.9564, MinusLogProbMetric: 58.9564, val_loss: 59.1078, val_MinusLogProbMetric: 59.1078

Epoch 12: val_loss improved from 59.49170 to 59.10784, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 96s - loss: 58.9564 - MinusLogProbMetric: 58.9564 - val_loss: 59.1078 - val_MinusLogProbMetric: 59.1078 - lr: 4.1152e-06 - 96s/epoch - 492ms/step
Epoch 13/1000
2023-10-02 00:35:55.487 
Epoch 13/1000 
	 loss: 58.8062, MinusLogProbMetric: 58.8062, val_loss: 58.7681, val_MinusLogProbMetric: 58.7681

Epoch 13: val_loss improved from 59.10784 to 58.76810, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 91s - loss: 58.8062 - MinusLogProbMetric: 58.8062 - val_loss: 58.7681 - val_MinusLogProbMetric: 58.7681 - lr: 4.1152e-06 - 91s/epoch - 464ms/step
Epoch 14/1000
2023-10-02 00:37:23.734 
Epoch 14/1000 
	 loss: 58.2822, MinusLogProbMetric: 58.2822, val_loss: 58.3835, val_MinusLogProbMetric: 58.3835

Epoch 14: val_loss improved from 58.76810 to 58.38352, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 87s - loss: 58.2822 - MinusLogProbMetric: 58.2822 - val_loss: 58.3835 - val_MinusLogProbMetric: 58.3835 - lr: 4.1152e-06 - 87s/epoch - 446ms/step
Epoch 15/1000
2023-10-02 00:38:59.912 
Epoch 15/1000 
	 loss: 58.2470, MinusLogProbMetric: 58.2470, val_loss: 58.5441, val_MinusLogProbMetric: 58.5441

Epoch 15: val_loss did not improve from 58.38352
196/196 - 95s - loss: 58.2470 - MinusLogProbMetric: 58.2470 - val_loss: 58.5441 - val_MinusLogProbMetric: 58.5441 - lr: 4.1152e-06 - 95s/epoch - 485ms/step
Epoch 16/1000
2023-10-02 00:40:30.865 
Epoch 16/1000 
	 loss: 57.7504, MinusLogProbMetric: 57.7504, val_loss: 59.6904, val_MinusLogProbMetric: 59.6904

Epoch 16: val_loss did not improve from 58.38352
196/196 - 91s - loss: 57.7504 - MinusLogProbMetric: 57.7504 - val_loss: 59.6904 - val_MinusLogProbMetric: 59.6904 - lr: 4.1152e-06 - 91s/epoch - 464ms/step
Epoch 17/1000
2023-10-02 00:42:01.641 
Epoch 17/1000 
	 loss: 57.5028, MinusLogProbMetric: 57.5028, val_loss: 57.7989, val_MinusLogProbMetric: 57.7989

Epoch 17: val_loss improved from 58.38352 to 57.79890, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 93s - loss: 57.5028 - MinusLogProbMetric: 57.5028 - val_loss: 57.7989 - val_MinusLogProbMetric: 57.7989 - lr: 4.1152e-06 - 93s/epoch - 475ms/step
Epoch 18/1000
2023-10-02 00:43:34.033 
Epoch 18/1000 
	 loss: 58.3429, MinusLogProbMetric: 58.3429, val_loss: 57.8185, val_MinusLogProbMetric: 57.8185

Epoch 18: val_loss did not improve from 57.79890
196/196 - 90s - loss: 58.3429 - MinusLogProbMetric: 58.3429 - val_loss: 57.8185 - val_MinusLogProbMetric: 57.8185 - lr: 4.1152e-06 - 90s/epoch - 459ms/step
Epoch 19/1000
2023-10-02 00:44:59.447 
Epoch 19/1000 
	 loss: 57.3395, MinusLogProbMetric: 57.3395, val_loss: 57.5139, val_MinusLogProbMetric: 57.5139

Epoch 19: val_loss improved from 57.79890 to 57.51386, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 88s - loss: 57.3395 - MinusLogProbMetric: 57.3395 - val_loss: 57.5139 - val_MinusLogProbMetric: 57.5139 - lr: 4.1152e-06 - 88s/epoch - 450ms/step
Epoch 20/1000
2023-10-02 00:46:34.484 
Epoch 20/1000 
	 loss: 57.0848, MinusLogProbMetric: 57.0848, val_loss: 57.6745, val_MinusLogProbMetric: 57.6745

Epoch 20: val_loss did not improve from 57.51386
196/196 - 92s - loss: 57.0848 - MinusLogProbMetric: 57.0848 - val_loss: 57.6745 - val_MinusLogProbMetric: 57.6745 - lr: 4.1152e-06 - 92s/epoch - 470ms/step
Epoch 21/1000
2023-10-02 00:48:06.472 
Epoch 21/1000 
	 loss: 57.2023, MinusLogProbMetric: 57.2023, val_loss: 57.3855, val_MinusLogProbMetric: 57.3855

Epoch 21: val_loss improved from 57.51386 to 57.38553, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 94s - loss: 57.2023 - MinusLogProbMetric: 57.2023 - val_loss: 57.3855 - val_MinusLogProbMetric: 57.3855 - lr: 4.1152e-06 - 94s/epoch - 481ms/step
Epoch 22/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 18: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-02 00:48:22.973 
Epoch 22/1000 
	 loss: nan, MinusLogProbMetric: 56.7108, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 22: val_loss did not improve from 57.38553
196/196 - 14s - loss: nan - MinusLogProbMetric: 56.7108 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.1152e-06 - 14s/epoch - 73ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 1.3717421124828526e-06.
===========
Generating train data for run 350.
===========
Train data generated in 1.12 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_350/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_350/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_350/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_350
self.data_kwargs: {'seed': 440}
self.x_data: [[ 2.0547452   3.776408    6.9601746  ...  5.8637457  -0.4179731
   2.747595  ]
 [ 6.405085    2.999549    6.1033454  ...  2.8107233   3.0569289
   2.003619  ]
 [ 2.0047038   4.064143    6.2048492  ...  5.6568336   0.71395487
   2.6926432 ]
 ...
 [ 5.645455    7.0971727   6.202566   ...  0.61970913  6.238492
   1.3539525 ]
 [ 6.475603    3.1151166   6.2248073  ...  2.8990176   3.6724448
   2.289971  ]
 [ 2.2290072   4.101192    7.1413527  ...  6.717821   -2.0046902
   2.3999293 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_397"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_398 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_37 (LogProbL  (None,)                  3291840   
 ayer)                                                           
                                                                 
=================================================================
Total params: 3,291,840
Trainable params: 3,291,840
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_37/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_37'")
self.model: <keras.engine.functional.Functional object at 0x7fad185b9900>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fad0aa806d0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fad0aa806d0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fad02da16c0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fad02f999f0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fad02f99f60>, <keras.callbacks.ModelCheckpoint object at 0x7fad02f9a020>, <keras.callbacks.EarlyStopping object at 0x7fad02f9a290>, <keras.callbacks.ReduceLROnPlateau object at 0x7fad02f9a2c0>, <keras.callbacks.TerminateOnNaN object at 0x7fad02f99f00>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 350/720 with hyperparameters:
timestamp = 2023-10-02 00:48:43.032295
ndims = 64
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 3291840
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.3717421124828526e-06...
Train first sample: [ 2.0547452   3.776408    6.9601746   1.2163222   8.644804    1.0450234
  9.756204    5.9333344   9.3759165   6.2056174   7.6663713   0.8787037
  2.7781603   1.7675867   3.9287794   0.8286338   3.6572614   4.846633
  0.68292063  6.317236    6.48625     3.843101    5.88724     2.0270782
  5.1297107   7.7579265   2.6054757   6.6726594   1.0543873   7.123391
  3.5526195   2.046467    5.620422   -0.869851    8.648443    0.02500496
  7.4141846   2.6777635   7.6790566   9.76465     2.481489    6.2672224
  5.7068043   4.504637    1.1161722   9.691578    4.0927534   8.503739
  6.9964066   3.1729455   8.086372    4.0078015   8.096493    5.813139
  8.181514    6.97417     7.246071    5.0315676  10.666024    6.2678714
  3.6855178   5.8637457  -0.4179731   2.747595  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 25: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-02 00:56:06.324 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 57.3074, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 442s - loss: nan - MinusLogProbMetric: 57.3074 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.3717e-06 - 442s/epoch - 2s/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 4.572473708276175e-07.
===========
Generating train data for run 350.
===========
Train data generated in 0.20 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_350/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_350/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_350/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_350
self.data_kwargs: {'seed': 440}
self.x_data: [[ 2.0547452   3.776408    6.9601746  ...  5.8637457  -0.4179731
   2.747595  ]
 [ 6.405085    2.999549    6.1033454  ...  2.8107233   3.0569289
   2.003619  ]
 [ 2.0047038   4.064143    6.2048492  ...  5.6568336   0.71395487
   2.6926432 ]
 ...
 [ 5.645455    7.0971727   6.202566   ...  0.61970913  6.238492
   1.3539525 ]
 [ 6.475603    3.1151166   6.2248073  ...  2.8990176   3.6724448
   2.289971  ]
 [ 2.2290072   4.101192    7.1413527  ...  6.717821   -2.0046902
   2.3999293 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_408"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_409 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_38 (LogProbL  (None,)                  3291840   
 ayer)                                                           
                                                                 
=================================================================
Total params: 3,291,840
Trainable params: 3,291,840
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_38/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_38'")
self.model: <keras.engine.functional.Functional object at 0x7fb1cf07d150>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fb054fd44f0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fb054fd44f0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fb1ecdc2aa0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fb86c937eb0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fb86c936920>, <keras.callbacks.ModelCheckpoint object at 0x7fb86c934ac0>, <keras.callbacks.EarlyStopping object at 0x7fb86c935930>, <keras.callbacks.ReduceLROnPlateau object at 0x7fb86c937fd0>, <keras.callbacks.TerminateOnNaN object at 0x7fb86c9351b0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 350/720 with hyperparameters:
timestamp = 2023-10-02 00:56:27.367472
ndims = 64
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 3291840
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.572473708276175e-07...
Train first sample: [ 2.0547452   3.776408    6.9601746   1.2163222   8.644804    1.0450234
  9.756204    5.9333344   9.3759165   6.2056174   7.6663713   0.8787037
  2.7781603   1.7675867   3.9287794   0.8286338   3.6572614   4.846633
  0.68292063  6.317236    6.48625     3.843101    5.88724     2.0270782
  5.1297107   7.7579265   2.6054757   6.6726594   1.0543873   7.123391
  3.5526195   2.046467    5.620422   -0.869851    8.648443    0.02500496
  7.4141846   2.6777635   7.6790566   9.76465     2.481489    6.2672224
  5.7068043   4.504637    1.1161722   9.691578    4.0927534   8.503739
  6.9964066   3.1729455   8.086372    4.0078015   8.096493    5.813139
  8.181514    6.97417     7.246071    5.0315676  10.666024    6.2678714
  3.6855178   5.8637457  -0.4179731   2.747595  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 25: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-02 01:02:26.361 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 56.4247, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 358s - loss: nan - MinusLogProbMetric: 56.4247 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.5725e-07 - 358s/epoch - 2s/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 1.524157902758725e-07.
===========
Generating train data for run 350.
===========
Train data generated in 0.74 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_350/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_350/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_350/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_350
self.data_kwargs: {'seed': 440}
self.x_data: [[ 2.0547452   3.776408    6.9601746  ...  5.8637457  -0.4179731
   2.747595  ]
 [ 6.405085    2.999549    6.1033454  ...  2.8107233   3.0569289
   2.003619  ]
 [ 2.0047038   4.064143    6.2048492  ...  5.6568336   0.71395487
   2.6926432 ]
 ...
 [ 5.645455    7.0971727   6.202566   ...  0.61970913  6.238492
   1.3539525 ]
 [ 6.475603    3.1151166   6.2248073  ...  2.8990176   3.6724448
   2.289971  ]
 [ 2.2290072   4.101192    7.1413527  ...  6.717821   -2.0046902
   2.3999293 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_419"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_420 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_39 (LogProbL  (None,)                  3291840   
 ayer)                                                           
                                                                 
=================================================================
Total params: 3,291,840
Trainable params: 3,291,840
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_39/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_39'")
self.model: <keras.engine.functional.Functional object at 0x7facfa2c8970>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fad0a983250>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fad0a983250>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fad0ad7df60>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7facf9ca6800>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7facf9ca6d70>, <keras.callbacks.ModelCheckpoint object at 0x7facf9ca6e30>, <keras.callbacks.EarlyStopping object at 0x7facf9ca70a0>, <keras.callbacks.ReduceLROnPlateau object at 0x7facf9ca70d0>, <keras.callbacks.TerminateOnNaN object at 0x7facf9ca6d10>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 350/720 with hyperparameters:
timestamp = 2023-10-02 01:02:45.625994
ndims = 64
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 3291840
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.524157902758725e-07...
Train first sample: [ 2.0547452   3.776408    6.9601746   1.2163222   8.644804    1.0450234
  9.756204    5.9333344   9.3759165   6.2056174   7.6663713   0.8787037
  2.7781603   1.7675867   3.9287794   0.8286338   3.6572614   4.846633
  0.68292063  6.317236    6.48625     3.843101    5.88724     2.0270782
  5.1297107   7.7579265   2.6054757   6.6726594   1.0543873   7.123391
  3.5526195   2.046467    5.620422   -0.869851    8.648443    0.02500496
  7.4141846   2.6777635   7.6790566   9.76465     2.481489    6.2672224
  5.7068043   4.504637    1.1161722   9.691578    4.0927534   8.503739
  6.9964066   3.1729455   8.086372    4.0078015   8.096493    5.813139
  8.181514    6.97417     7.246071    5.0315676  10.666024    6.2678714
  3.6855178   5.8637457  -0.4179731   2.747595  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 25: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-02 01:09:49.449 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 56.3955, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 423s - loss: nan - MinusLogProbMetric: 56.3955 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.5242e-07 - 423s/epoch - 2s/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 5.0805263425290834e-08.
===========
Generating train data for run 350.
===========
Train data generated in 0.91 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_350/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_350/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_350/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_350
self.data_kwargs: {'seed': 440}
self.x_data: [[ 2.0547452   3.776408    6.9601746  ...  5.8637457  -0.4179731
   2.747595  ]
 [ 6.405085    2.999549    6.1033454  ...  2.8107233   3.0569289
   2.003619  ]
 [ 2.0047038   4.064143    6.2048492  ...  5.6568336   0.71395487
   2.6926432 ]
 ...
 [ 5.645455    7.0971727   6.202566   ...  0.61970913  6.238492
   1.3539525 ]
 [ 6.475603    3.1151166   6.2248073  ...  2.8990176   3.6724448
   2.289971  ]
 [ 2.2290072   4.101192    7.1413527  ...  6.717821   -2.0046902
   2.3999293 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_430"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_431 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_40 (LogProbL  (None,)                  3291840   
 ayer)                                                           
                                                                 
=================================================================
Total params: 3,291,840
Trainable params: 3,291,840
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_40/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_40'")
self.model: <keras.engine.functional.Functional object at 0x7fb1965793c0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7facf0d687f0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7facf0d687f0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fad28193f10>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fad401ed2d0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fad401ee140>, <keras.callbacks.ModelCheckpoint object at 0x7fad401ee290>, <keras.callbacks.EarlyStopping object at 0x7fad401ee7a0>, <keras.callbacks.ReduceLROnPlateau object at 0x7fad401ee650>, <keras.callbacks.TerminateOnNaN object at 0x7fad401ede70>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 350/720 with hyperparameters:
timestamp = 2023-10-02 01:10:08.668830
ndims = 64
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 3291840
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 5.0805263425290834e-08...
Train first sample: [ 2.0547452   3.776408    6.9601746   1.2163222   8.644804    1.0450234
  9.756204    5.9333344   9.3759165   6.2056174   7.6663713   0.8787037
  2.7781603   1.7675867   3.9287794   0.8286338   3.6572614   4.846633
  0.68292063  6.317236    6.48625     3.843101    5.88724     2.0270782
  5.1297107   7.7579265   2.6054757   6.6726594   1.0543873   7.123391
  3.5526195   2.046467    5.620422   -0.869851    8.648443    0.02500496
  7.4141846   2.6777635   7.6790566   9.76465     2.481489    6.2672224
  5.7068043   4.504637    1.1161722   9.691578    4.0927534   8.503739
  6.9964066   3.1729455   8.086372    4.0078015   8.096493    5.813139
  8.181514    6.97417     7.246071    5.0315676  10.666024    6.2678714
  3.6855178   5.8637457  -0.4179731   2.747595  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 25: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-02 01:16:19.817 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 56.4429, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 371s - loss: nan - MinusLogProbMetric: 56.4429 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 5.0805e-08 - 371s/epoch - 2s/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 1.6935087808430278e-08.
===========
Generating train data for run 350.
===========
Train data generated in 0.22 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_350/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_350/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_350/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_350
self.data_kwargs: {'seed': 440}
self.x_data: [[ 2.0547452   3.776408    6.9601746  ...  5.8637457  -0.4179731
   2.747595  ]
 [ 6.405085    2.999549    6.1033454  ...  2.8107233   3.0569289
   2.003619  ]
 [ 2.0047038   4.064143    6.2048492  ...  5.6568336   0.71395487
   2.6926432 ]
 ...
 [ 5.645455    7.0971727   6.202566   ...  0.61970913  6.238492
   1.3539525 ]
 [ 6.475603    3.1151166   6.2248073  ...  2.8990176   3.6724448
   2.289971  ]
 [ 2.2290072   4.101192    7.1413527  ...  6.717821   -2.0046902
   2.3999293 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_441"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_442 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_41 (LogProbL  (None,)                  3291840   
 ayer)                                                           
                                                                 
=================================================================
Total params: 3,291,840
Trainable params: 3,291,840
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_41/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_41'")
self.model: <keras.engine.functional.Functional object at 0x7fad0239f490>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fad094a57e0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fad094a57e0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7faceb68d390>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fad10e32050>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fad10e325c0>, <keras.callbacks.ModelCheckpoint object at 0x7fad10e32680>, <keras.callbacks.EarlyStopping object at 0x7fad10e328f0>, <keras.callbacks.ReduceLROnPlateau object at 0x7fad10e32920>, <keras.callbacks.TerminateOnNaN object at 0x7fad10e32560>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 350/720 with hyperparameters:
timestamp = 2023-10-02 01:16:39.043763
ndims = 64
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 3291840
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.6935087808430278e-08...
Train first sample: [ 2.0547452   3.776408    6.9601746   1.2163222   8.644804    1.0450234
  9.756204    5.9333344   9.3759165   6.2056174   7.6663713   0.8787037
  2.7781603   1.7675867   3.9287794   0.8286338   3.6572614   4.846633
  0.68292063  6.317236    6.48625     3.843101    5.88724     2.0270782
  5.1297107   7.7579265   2.6054757   6.6726594   1.0543873   7.123391
  3.5526195   2.046467    5.620422   -0.869851    8.648443    0.02500496
  7.4141846   2.6777635   7.6790566   9.76465     2.481489    6.2672224
  5.7068043   4.504637    1.1161722   9.691578    4.0927534   8.503739
  6.9964066   3.1729455   8.086372    4.0078015   8.096493    5.813139
  8.181514    6.97417     7.246071    5.0315676  10.666024    6.2678714
  3.6855178   5.8637457  -0.4179731   2.747595  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 25: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-02 01:23:39.092 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 56.6868, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 419s - loss: nan - MinusLogProbMetric: 56.6868 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.6935e-08 - 419s/epoch - 2s/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 5.645029269476759e-09.
===========
Run 350/720 failed.
Exception type: Exception
Exception message: Training failed for the maximum number of retry.
Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN.py , Line : 637, Func.Name : <module>, Message : hyperparams_dict, NFObject, seed_train, training_time = train_function(seeds = [seed_train, seed_test, seed_dist, seed_metrics],', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN.py , Line : 313, Func.Name : train_function, Message : raise Exception("Training failed for the maximum number of retry.")']
===========

Directory ../../results/CsplineN_new/run_351/ already exists.
Skipping it.
===========
Run 351/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_352/ already exists.
Skipping it.
===========
Run 352/720 already exists. Skipping it.
===========

===========
Generating train data for run 353.
===========
Train data generated in 0.66 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_353/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_353/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_353/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_353
self.data_kwargs: {'seed': 520}
self.x_data: [[2.1731846  3.8937397  6.9821544  ... 5.669328   0.68163395 3.5624375 ]
 [6.7535415  2.6749182  6.1859965  ... 3.3760262  4.638645   2.336814  ]
 [6.1686287  2.7568936  6.2012696  ... 2.5904071  3.339636   2.9827425 ]
 ...
 [6.6194353  2.9398146  6.1845684  ... 3.0365293  3.5444417  1.3393478 ]
 [5.613061   6.348512   5.164087   ... 1.829057   7.7712007  1.4071141 ]
 [6.893006   3.0565336  6.057088   ... 2.613742   4.4370584  2.0332472 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_447"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_448 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_42 (LogProbL  (None,)                  660960    
 ayer)                                                           
                                                                 
=================================================================
Total params: 660,960
Trainable params: 660,960
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_42/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_42'")
self.model: <keras.engine.functional.Functional object at 0x7facfbc18d90>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fb0ff29b910>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fb0ff29b910>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7facdb38c760>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fad09e4afb0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_353/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fb14cde6740>, <keras.callbacks.ModelCheckpoint object at 0x7fb14cde4640>, <keras.callbacks.EarlyStopping object at 0x7fb14cde7250>, <keras.callbacks.ReduceLROnPlateau object at 0x7fb14cde4b80>, <keras.callbacks.TerminateOnNaN object at 0x7fb14cde5240>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_353/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 353/720 with hyperparameters:
timestamp = 2023-10-02 01:23:50.424108
ndims = 64
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 660960
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 2.1731846   3.8937397   6.9821544   1.4459838   8.670284    0.996012
  9.747296    5.3488703  10.829596    5.798153    7.4385934  -0.39963475
  2.887594    1.6816981   1.9483483   2.4950166   3.377345    6.4428034
  0.7434967  10.024268    5.9064217   3.9731474   5.977063    2.0677786
  5.421294    7.7411385   2.3877835   6.667169    1.0165741   6.879494
  2.333092    1.6771841   5.2052946  -0.01726444  8.224516   -0.06788193
  7.1896877   2.6926644   7.7422037   9.618431    2.508284    5.3540187
  8.24927     4.96435     2.6443048   8.933116    4.0272393   8.583376
  6.9591885   2.9305875   8.131322    3.9143033   8.096406    5.8134246
  9.931866    7.011071    7.2540216   5.1431737  10.5702715   6.340821
  4.2660613   5.669328    0.68163395  3.5624375 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-02 01:26:50.886 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6323.5312, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 179s - loss: nan - MinusLogProbMetric: 6323.5312 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 179s/epoch - 914ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 0.0003333333333333333.
===========
Generating train data for run 353.
===========
Train data generated in 0.45 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_353/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_353/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_353/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_353
self.data_kwargs: {'seed': 520}
self.x_data: [[2.1731846  3.8937397  6.9821544  ... 5.669328   0.68163395 3.5624375 ]
 [6.7535415  2.6749182  6.1859965  ... 3.3760262  4.638645   2.336814  ]
 [6.1686287  2.7568936  6.2012696  ... 2.5904071  3.339636   2.9827425 ]
 ...
 [6.6194353  2.9398146  6.1845684  ... 3.0365293  3.5444417  1.3393478 ]
 [5.613061   6.348512   5.164087   ... 1.829057   7.7712007  1.4071141 ]
 [6.893006   3.0565336  6.057088   ... 2.613742   4.4370584  2.0332472 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_453"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_454 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_43 (LogProbL  (None,)                  660960    
 ayer)                                                           
                                                                 
=================================================================
Total params: 660,960
Trainable params: 660,960
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_43/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_43'")
self.model: <keras.engine.functional.Functional object at 0x7fad307f4ca0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7facf9afbc40>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7facf9afbc40>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fb842c681c0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7faeb8694100>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_353/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7faeb8694be0>, <keras.callbacks.ModelCheckpoint object at 0x7faeb8694e50>, <keras.callbacks.EarlyStopping object at 0x7faeb8695120>, <keras.callbacks.ReduceLROnPlateau object at 0x7faeb8695cc0>, <keras.callbacks.TerminateOnNaN object at 0x7faeb8694b80>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_353/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 353/720 with hyperparameters:
timestamp = 2023-10-02 01:27:00.127648
ndims = 64
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 660960
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 2.1731846   3.8937397   6.9821544   1.4459838   8.670284    0.996012
  9.747296    5.3488703  10.829596    5.798153    7.4385934  -0.39963475
  2.887594    1.6816981   1.9483483   2.4950166   3.377345    6.4428034
  0.7434967  10.024268    5.9064217   3.9731474   5.977063    2.0677786
  5.421294    7.7411385   2.3877835   6.667169    1.0165741   6.879494
  2.333092    1.6771841   5.2052946  -0.01726444  8.224516   -0.06788193
  7.1896877   2.6926644   7.7422037   9.618431    2.508284    5.3540187
  8.24927     4.96435     2.6443048   8.933116    4.0272393   8.583376
  6.9591885   2.9305875   8.131322    3.9143033   8.096406    5.8134246
  9.931866    7.011071    7.2540216   5.1431737  10.5702715   6.340821
  4.2660613   5.669328    0.68163395  3.5624375 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-02 01:29:46.642 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6323.5312, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 166s - loss: nan - MinusLogProbMetric: 6323.5312 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 166s/epoch - 846ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 0.0001111111111111111.
===========
Generating train data for run 353.
===========
Train data generated in 0.31 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_353/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_353/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_353/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_353
self.data_kwargs: {'seed': 520}
self.x_data: [[2.1731846  3.8937397  6.9821544  ... 5.669328   0.68163395 3.5624375 ]
 [6.7535415  2.6749182  6.1859965  ... 3.3760262  4.638645   2.336814  ]
 [6.1686287  2.7568936  6.2012696  ... 2.5904071  3.339636   2.9827425 ]
 ...
 [6.6194353  2.9398146  6.1845684  ... 3.0365293  3.5444417  1.3393478 ]
 [5.613061   6.348512   5.164087   ... 1.829057   7.7712007  1.4071141 ]
 [6.893006   3.0565336  6.057088   ... 2.613742   4.4370584  2.0332472 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_459"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_460 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_44 (LogProbL  (None,)                  660960    
 ayer)                                                           
                                                                 
=================================================================
Total params: 660,960
Trainable params: 660,960
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_44/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_44'")
self.model: <keras.engine.functional.Functional object at 0x7facd9ff3940>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7facea25bc40>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7facea25bc40>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7faceae1fc40>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7facea274df0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_353/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7facea275360>, <keras.callbacks.ModelCheckpoint object at 0x7facea275420>, <keras.callbacks.EarlyStopping object at 0x7facea275690>, <keras.callbacks.ReduceLROnPlateau object at 0x7facea2756c0>, <keras.callbacks.TerminateOnNaN object at 0x7facea275300>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_353/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 353/720 with hyperparameters:
timestamp = 2023-10-02 01:29:55.813118
ndims = 64
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 660960
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 2.1731846   3.8937397   6.9821544   1.4459838   8.670284    0.996012
  9.747296    5.3488703  10.829596    5.798153    7.4385934  -0.39963475
  2.887594    1.6816981   1.9483483   2.4950166   3.377345    6.4428034
  0.7434967  10.024268    5.9064217   3.9731474   5.977063    2.0677786
  5.421294    7.7411385   2.3877835   6.667169    1.0165741   6.879494
  2.333092    1.6771841   5.2052946  -0.01726444  8.224516   -0.06788193
  7.1896877   2.6926644   7.7422037   9.618431    2.508284    5.3540187
  8.24927     4.96435     2.6443048   8.933116    4.0272393   8.583376
  6.9591885   2.9305875   8.131322    3.9143033   8.096406    5.8134246
  9.931866    7.011071    7.2540216   5.1431737  10.5702715   6.340821
  4.2660613   5.669328    0.68163395  3.5624375 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-02 01:32:36.710 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6323.5312, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 160s - loss: nan - MinusLogProbMetric: 6323.5312 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.1111e-04 - 160s/epoch - 818ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 3.703703703703703e-05.
===========
Generating train data for run 353.
===========
Train data generated in 0.76 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_353/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_353/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_353/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_353
self.data_kwargs: {'seed': 520}
self.x_data: [[2.1731846  3.8937397  6.9821544  ... 5.669328   0.68163395 3.5624375 ]
 [6.7535415  2.6749182  6.1859965  ... 3.3760262  4.638645   2.336814  ]
 [6.1686287  2.7568936  6.2012696  ... 2.5904071  3.339636   2.9827425 ]
 ...
 [6.6194353  2.9398146  6.1845684  ... 3.0365293  3.5444417  1.3393478 ]
 [5.613061   6.348512   5.164087   ... 1.829057   7.7712007  1.4071141 ]
 [6.893006   3.0565336  6.057088   ... 2.613742   4.4370584  2.0332472 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_465"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_466 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_45 (LogProbL  (None,)                  660960    
 ayer)                                                           
                                                                 
=================================================================
Total params: 660,960
Trainable params: 660,960
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_45/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_45'")
self.model: <keras.engine.functional.Functional object at 0x7fb055e86fe0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fb055937a00>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fb055937a00>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fb055ee4580>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fb0557c4a60>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_353/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fb0557c4fd0>, <keras.callbacks.ModelCheckpoint object at 0x7fb0557c5090>, <keras.callbacks.EarlyStopping object at 0x7fb0557c5300>, <keras.callbacks.ReduceLROnPlateau object at 0x7fb0557c5330>, <keras.callbacks.TerminateOnNaN object at 0x7fb0557c4f70>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_353/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 353/720 with hyperparameters:
timestamp = 2023-10-02 01:32:47.362861
ndims = 64
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 660960
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 3.703703703703703e-05...
Train first sample: [ 2.1731846   3.8937397   6.9821544   1.4459838   8.670284    0.996012
  9.747296    5.3488703  10.829596    5.798153    7.4385934  -0.39963475
  2.887594    1.6816981   1.9483483   2.4950166   3.377345    6.4428034
  0.7434967  10.024268    5.9064217   3.9731474   5.977063    2.0677786
  5.421294    7.7411385   2.3877835   6.667169    1.0165741   6.879494
  2.333092    1.6771841   5.2052946  -0.01726444  8.224516   -0.06788193
  7.1896877   2.6926644   7.7422037   9.618431    2.508284    5.3540187
  8.24927     4.96435     2.6443048   8.933116    4.0272393   8.583376
  6.9591885   2.9305875   8.131322    3.9143033   8.096406    5.8134246
  9.931866    7.011071    7.2540216   5.1431737  10.5702715   6.340821
  4.2660613   5.669328    0.68163395  3.5624375 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-02 01:35:38.557 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6323.5312, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 172s - loss: nan - MinusLogProbMetric: 6323.5312 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.7037e-05 - 172s/epoch - 876ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 1.2345679012345677e-05.
===========
Generating train data for run 353.
===========
Train data generated in 25.22 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_353/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_353/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_353/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_353
self.data_kwargs: {'seed': 520}
self.x_data: [[2.1731846  3.8937397  6.9821544  ... 5.669328   0.68163395 3.5624375 ]
 [6.7535415  2.6749182  6.1859965  ... 3.3760262  4.638645   2.336814  ]
 [6.1686287  2.7568936  6.2012696  ... 2.5904071  3.339636   2.9827425 ]
 ...
 [6.6194353  2.9398146  6.1845684  ... 3.0365293  3.5444417  1.3393478 ]
 [5.613061   6.348512   5.164087   ... 1.829057   7.7712007  1.4071141 ]
 [6.893006   3.0565336  6.057088   ... 2.613742   4.4370584  2.0332472 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_471"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_472 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_46 (LogProbL  (None,)                  660960    
 ayer)                                                           
                                                                 
=================================================================
Total params: 660,960
Trainable params: 660,960
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_46/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_46'")
self.model: <keras.engine.functional.Functional object at 0x7fb04dd572e0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fb04d906aa0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fb04d906aa0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fb04d73c460>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fb04d7dc8b0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_353/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fb04d7dce20>, <keras.callbacks.ModelCheckpoint object at 0x7fb04d7dcee0>, <keras.callbacks.EarlyStopping object at 0x7fb04d7dd150>, <keras.callbacks.ReduceLROnPlateau object at 0x7fb04d7dd180>, <keras.callbacks.TerminateOnNaN object at 0x7fb04d7dcdc0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_353/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 353/720 with hyperparameters:
timestamp = 2023-10-02 01:36:11.482227
ndims = 64
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 660960
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.2345679012345677e-05...
Train first sample: [ 2.1731846   3.8937397   6.9821544   1.4459838   8.670284    0.996012
  9.747296    5.3488703  10.829596    5.798153    7.4385934  -0.39963475
  2.887594    1.6816981   1.9483483   2.4950166   3.377345    6.4428034
  0.7434967  10.024268    5.9064217   3.9731474   5.977063    2.0677786
  5.421294    7.7411385   2.3877835   6.667169    1.0165741   6.879494
  2.333092    1.6771841   5.2052946  -0.01726444  8.224516   -0.06788193
  7.1896877   2.6926644   7.7422037   9.618431    2.508284    5.3540187
  8.24927     4.96435     2.6443048   8.933116    4.0272393   8.583376
  6.9591885   2.9305875   8.131322    3.9143033   8.096406    5.8134246
  9.931866    7.011071    7.2540216   5.1431737  10.5702715   6.340821
  4.2660613   5.669328    0.68163395  3.5624375 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-02 01:40:16.935 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6323.5312, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 245s - loss: nan - MinusLogProbMetric: 6323.5312 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.2346e-05 - 245s/epoch - 1s/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 4.115226337448558e-06.
===========
Generating train data for run 353.
===========
Train data generated in 0.25 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_353/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_353/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_353/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_353
self.data_kwargs: {'seed': 520}
self.x_data: [[2.1731846  3.8937397  6.9821544  ... 5.669328   0.68163395 3.5624375 ]
 [6.7535415  2.6749182  6.1859965  ... 3.3760262  4.638645   2.336814  ]
 [6.1686287  2.7568936  6.2012696  ... 2.5904071  3.339636   2.9827425 ]
 ...
 [6.6194353  2.9398146  6.1845684  ... 3.0365293  3.5444417  1.3393478 ]
 [5.613061   6.348512   5.164087   ... 1.829057   7.7712007  1.4071141 ]
 [6.893006   3.0565336  6.057088   ... 2.613742   4.4370584  2.0332472 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_477"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_478 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_47 (LogProbL  (None,)                  660960    
 ayer)                                                           
                                                                 
=================================================================
Total params: 660,960
Trainable params: 660,960
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_47/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_47'")
self.model: <keras.engine.functional.Functional object at 0x7fb328355720>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fb4283cb4f0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fb4283cb4f0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fb0ffb6f460>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fb38c14ee00>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_353/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fb38c14e500>, <keras.callbacks.ModelCheckpoint object at 0x7fb38c14d510>, <keras.callbacks.EarlyStopping object at 0x7fb38c14d270>, <keras.callbacks.ReduceLROnPlateau object at 0x7fb38c14c6a0>, <keras.callbacks.TerminateOnNaN object at 0x7fb38c14c8b0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_353/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 353/720 with hyperparameters:
timestamp = 2023-10-02 01:40:26.194174
ndims = 64
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 660960
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.115226337448558e-06...
Train first sample: [ 2.1731846   3.8937397   6.9821544   1.4459838   8.670284    0.996012
  9.747296    5.3488703  10.829596    5.798153    7.4385934  -0.39963475
  2.887594    1.6816981   1.9483483   2.4950166   3.377345    6.4428034
  0.7434967  10.024268    5.9064217   3.9731474   5.977063    2.0677786
  5.421294    7.7411385   2.3877835   6.667169    1.0165741   6.879494
  2.333092    1.6771841   5.2052946  -0.01726444  8.224516   -0.06788193
  7.1896877   2.6926644   7.7422037   9.618431    2.508284    5.3540187
  8.24927     4.96435     2.6443048   8.933116    4.0272393   8.583376
  6.9591885   2.9305875   8.131322    3.9143033   8.096406    5.8134246
  9.931866    7.011071    7.2540216   5.1431737  10.5702715   6.340821
  4.2660613   5.669328    0.68163395  3.5624375 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-02 01:43:27.799 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6323.5312, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 181s - loss: nan - MinusLogProbMetric: 6323.5312 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.1152e-06 - 181s/epoch - 923ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 1.3717421124828526e-06.
===========
Generating train data for run 353.
===========
Train data generated in 0.83 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_353/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_353/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_353/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_353
self.data_kwargs: {'seed': 520}
self.x_data: [[2.1731846  3.8937397  6.9821544  ... 5.669328   0.68163395 3.5624375 ]
 [6.7535415  2.6749182  6.1859965  ... 3.3760262  4.638645   2.336814  ]
 [6.1686287  2.7568936  6.2012696  ... 2.5904071  3.339636   2.9827425 ]
 ...
 [6.6194353  2.9398146  6.1845684  ... 3.0365293  3.5444417  1.3393478 ]
 [5.613061   6.348512   5.164087   ... 1.829057   7.7712007  1.4071141 ]
 [6.893006   3.0565336  6.057088   ... 2.613742   4.4370584  2.0332472 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_483"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_484 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_48 (LogProbL  (None,)                  660960    
 ayer)                                                           
                                                                 
=================================================================
Total params: 660,960
Trainable params: 660,960
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_48/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_48'")
self.model: <keras.engine.functional.Functional object at 0x7fb328544d90>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fb1df9f4910>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fb1df9f4910>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fb044a44490>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fb43054db40>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_353/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fb43054e0b0>, <keras.callbacks.ModelCheckpoint object at 0x7fb43054e170>, <keras.callbacks.EarlyStopping object at 0x7fb43054e3e0>, <keras.callbacks.ReduceLROnPlateau object at 0x7fb43054e410>, <keras.callbacks.TerminateOnNaN object at 0x7fb43054e050>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_353/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 353/720 with hyperparameters:
timestamp = 2023-10-02 01:43:37.224925
ndims = 64
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 660960
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.3717421124828526e-06...
Train first sample: [ 2.1731846   3.8937397   6.9821544   1.4459838   8.670284    0.996012
  9.747296    5.3488703  10.829596    5.798153    7.4385934  -0.39963475
  2.887594    1.6816981   1.9483483   2.4950166   3.377345    6.4428034
  0.7434967  10.024268    5.9064217   3.9731474   5.977063    2.0677786
  5.421294    7.7411385   2.3877835   6.667169    1.0165741   6.879494
  2.333092    1.6771841   5.2052946  -0.01726444  8.224516   -0.06788193
  7.1896877   2.6926644   7.7422037   9.618431    2.508284    5.3540187
  8.24927     4.96435     2.6443048   8.933116    4.0272393   8.583376
  6.9591885   2.9305875   8.131322    3.9143033   8.096406    5.8134246
  9.931866    7.011071    7.2540216   5.1431737  10.5702715   6.340821
  4.2660613   5.669328    0.68163395  3.5624375 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-02 01:46:35.889 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6323.5312, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 178s - loss: nan - MinusLogProbMetric: 6323.5312 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.3717e-06 - 178s/epoch - 907ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 4.572473708276175e-07.
===========
Generating train data for run 353.
===========
Train data generated in 0.13 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_353/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_353/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_353/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_353
self.data_kwargs: {'seed': 520}
self.x_data: [[2.1731846  3.8937397  6.9821544  ... 5.669328   0.68163395 3.5624375 ]
 [6.7535415  2.6749182  6.1859965  ... 3.3760262  4.638645   2.336814  ]
 [6.1686287  2.7568936  6.2012696  ... 2.5904071  3.339636   2.9827425 ]
 ...
 [6.6194353  2.9398146  6.1845684  ... 3.0365293  3.5444417  1.3393478 ]
 [5.613061   6.348512   5.164087   ... 1.829057   7.7712007  1.4071141 ]
 [6.893006   3.0565336  6.057088   ... 2.613742   4.4370584  2.0332472 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_489"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_490 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_49 (LogProbL  (None,)                  660960    
 ayer)                                                           
                                                                 
=================================================================
Total params: 660,960
Trainable params: 660,960
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_49/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_49'")
self.model: <keras.engine.functional.Functional object at 0x7facaaf139d0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7facaafc43d0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7facaafc43d0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7facaafd2a10>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7faca81ee9e0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_353/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7faca81eef50>, <keras.callbacks.ModelCheckpoint object at 0x7faca81ef010>, <keras.callbacks.EarlyStopping object at 0x7faca81ef280>, <keras.callbacks.ReduceLROnPlateau object at 0x7faca81ef2b0>, <keras.callbacks.TerminateOnNaN object at 0x7faca81eeef0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_353/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 353/720 with hyperparameters:
timestamp = 2023-10-02 01:46:44.392869
ndims = 64
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 660960
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.572473708276175e-07...
Train first sample: [ 2.1731846   3.8937397   6.9821544   1.4459838   8.670284    0.996012
  9.747296    5.3488703  10.829596    5.798153    7.4385934  -0.39963475
  2.887594    1.6816981   1.9483483   2.4950166   3.377345    6.4428034
  0.7434967  10.024268    5.9064217   3.9731474   5.977063    2.0677786
  5.421294    7.7411385   2.3877835   6.667169    1.0165741   6.879494
  2.333092    1.6771841   5.2052946  -0.01726444  8.224516   -0.06788193
  7.1896877   2.6926644   7.7422037   9.618431    2.508284    5.3540187
  8.24927     4.96435     2.6443048   8.933116    4.0272393   8.583376
  6.9591885   2.9305875   8.131322    3.9143033   8.096406    5.8134246
  9.931866    7.011071    7.2540216   5.1431737  10.5702715   6.340821
  4.2660613   5.669328    0.68163395  3.5624375 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-02 01:49:27.185 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6323.5312, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 162s - loss: nan - MinusLogProbMetric: 6323.5312 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.5725e-07 - 162s/epoch - 825ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 1.524157902758725e-07.
===========
Generating train data for run 353.
===========
Train data generated in 0.74 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_353/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_353/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_353/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_353
self.data_kwargs: {'seed': 520}
self.x_data: [[2.1731846  3.8937397  6.9821544  ... 5.669328   0.68163395 3.5624375 ]
 [6.7535415  2.6749182  6.1859965  ... 3.3760262  4.638645   2.336814  ]
 [6.1686287  2.7568936  6.2012696  ... 2.5904071  3.339636   2.9827425 ]
 ...
 [6.6194353  2.9398146  6.1845684  ... 3.0365293  3.5444417  1.3393478 ]
 [5.613061   6.348512   5.164087   ... 1.829057   7.7712007  1.4071141 ]
 [6.893006   3.0565336  6.057088   ... 2.613742   4.4370584  2.0332472 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_495"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_496 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_50 (LogProbL  (None,)                  660960    
 ayer)                                                           
                                                                 
=================================================================
Total params: 660,960
Trainable params: 660,960
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_50/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_50'")
self.model: <keras.engine.functional.Functional object at 0x7facb8f212d0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7facaba1ed10>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7facaba1ed10>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7facba3f5ab0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7facd86e2710>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_353/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7facd86e2c80>, <keras.callbacks.ModelCheckpoint object at 0x7facd86e2d40>, <keras.callbacks.EarlyStopping object at 0x7facd86e2fb0>, <keras.callbacks.ReduceLROnPlateau object at 0x7facd86e2fe0>, <keras.callbacks.TerminateOnNaN object at 0x7facd86e2c20>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_353/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 353/720 with hyperparameters:
timestamp = 2023-10-02 01:49:36.475248
ndims = 64
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 660960
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.524157902758725e-07...
Train first sample: [ 2.1731846   3.8937397   6.9821544   1.4459838   8.670284    0.996012
  9.747296    5.3488703  10.829596    5.798153    7.4385934  -0.39963475
  2.887594    1.6816981   1.9483483   2.4950166   3.377345    6.4428034
  0.7434967  10.024268    5.9064217   3.9731474   5.977063    2.0677786
  5.421294    7.7411385   2.3877835   6.667169    1.0165741   6.879494
  2.333092    1.6771841   5.2052946  -0.01726444  8.224516   -0.06788193
  7.1896877   2.6926644   7.7422037   9.618431    2.508284    5.3540187
  8.24927     4.96435     2.6443048   8.933116    4.0272393   8.583376
  6.9591885   2.9305875   8.131322    3.9143033   8.096406    5.8134246
  9.931866    7.011071    7.2540216   5.1431737  10.5702715   6.340821
  4.2660613   5.669328    0.68163395  3.5624375 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-02 01:52:17.311 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6323.5312, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 161s - loss: nan - MinusLogProbMetric: 6323.5312 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.5242e-07 - 161s/epoch - 819ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 5.0805263425290834e-08.
===========
Generating train data for run 353.
===========
Train data generated in 0.37 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_353/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_353/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_353/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_353
self.data_kwargs: {'seed': 520}
self.x_data: [[2.1731846  3.8937397  6.9821544  ... 5.669328   0.68163395 3.5624375 ]
 [6.7535415  2.6749182  6.1859965  ... 3.3760262  4.638645   2.336814  ]
 [6.1686287  2.7568936  6.2012696  ... 2.5904071  3.339636   2.9827425 ]
 ...
 [6.6194353  2.9398146  6.1845684  ... 3.0365293  3.5444417  1.3393478 ]
 [5.613061   6.348512   5.164087   ... 1.829057   7.7712007  1.4071141 ]
 [6.893006   3.0565336  6.057088   ... 2.613742   4.4370584  2.0332472 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_501"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_502 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_51 (LogProbL  (None,)                  660960    
 ayer)                                                           
                                                                 
=================================================================
Total params: 660,960
Trainable params: 660,960
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_51/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_51'")
self.model: <keras.engine.functional.Functional object at 0x7facb95579d0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7facb859dc30>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7facb859dc30>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7facda7731f0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7face9cb64a0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_353/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7face9cb6a10>, <keras.callbacks.ModelCheckpoint object at 0x7face9cb6ad0>, <keras.callbacks.EarlyStopping object at 0x7face9cb6d40>, <keras.callbacks.ReduceLROnPlateau object at 0x7face9cb6d70>, <keras.callbacks.TerminateOnNaN object at 0x7face9cb69b0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_353/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 353/720 with hyperparameters:
timestamp = 2023-10-02 01:52:27.595191
ndims = 64
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 660960
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 5.0805263425290834e-08...
Train first sample: [ 2.1731846   3.8937397   6.9821544   1.4459838   8.670284    0.996012
  9.747296    5.3488703  10.829596    5.798153    7.4385934  -0.39963475
  2.887594    1.6816981   1.9483483   2.4950166   3.377345    6.4428034
  0.7434967  10.024268    5.9064217   3.9731474   5.977063    2.0677786
  5.421294    7.7411385   2.3877835   6.667169    1.0165741   6.879494
  2.333092    1.6771841   5.2052946  -0.01726444  8.224516   -0.06788193
  7.1896877   2.6926644   7.7422037   9.618431    2.508284    5.3540187
  8.24927     4.96435     2.6443048   8.933116    4.0272393   8.583376
  6.9591885   2.9305875   8.131322    3.9143033   8.096406    5.8134246
  9.931866    7.011071    7.2540216   5.1431737  10.5702715   6.340821
  4.2660613   5.669328    0.68163395  3.5624375 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-02 01:56:18.709 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6323.5312, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 231s - loss: nan - MinusLogProbMetric: 6323.5312 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 5.0805e-08 - 231s/epoch - 1s/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 1.6935087808430278e-08.
===========
Generating train data for run 353.
===========
Train data generated in 0.58 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_353/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_353/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_353/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_353
self.data_kwargs: {'seed': 520}
self.x_data: [[2.1731846  3.8937397  6.9821544  ... 5.669328   0.68163395 3.5624375 ]
 [6.7535415  2.6749182  6.1859965  ... 3.3760262  4.638645   2.336814  ]
 [6.1686287  2.7568936  6.2012696  ... 2.5904071  3.339636   2.9827425 ]
 ...
 [6.6194353  2.9398146  6.1845684  ... 3.0365293  3.5444417  1.3393478 ]
 [5.613061   6.348512   5.164087   ... 1.829057   7.7712007  1.4071141 ]
 [6.893006   3.0565336  6.057088   ... 2.613742   4.4370584  2.0332472 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_507"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_508 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_52 (LogProbL  (None,)                  660960    
 ayer)                                                           
                                                                 
=================================================================
Total params: 660,960
Trainable params: 660,960
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_52/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_52'")
self.model: <keras.engine.functional.Functional object at 0x7faeb842fa60>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fb0444f68c0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fb0444f68c0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fb1df70a860>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fad0b4664d0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_353/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fad0b464040>, <keras.callbacks.ModelCheckpoint object at 0x7fad0b467730>, <keras.callbacks.EarlyStopping object at 0x7fad0b4641f0>, <keras.callbacks.ReduceLROnPlateau object at 0x7fad0b464490>, <keras.callbacks.TerminateOnNaN object at 0x7fad0b4662f0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_353/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 353/720 with hyperparameters:
timestamp = 2023-10-02 01:56:27.586453
ndims = 64
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 660960
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.6935087808430278e-08...
Train first sample: [ 2.1731846   3.8937397   6.9821544   1.4459838   8.670284    0.996012
  9.747296    5.3488703  10.829596    5.798153    7.4385934  -0.39963475
  2.887594    1.6816981   1.9483483   2.4950166   3.377345    6.4428034
  0.7434967  10.024268    5.9064217   3.9731474   5.977063    2.0677786
  5.421294    7.7411385   2.3877835   6.667169    1.0165741   6.879494
  2.333092    1.6771841   5.2052946  -0.01726444  8.224516   -0.06788193
  7.1896877   2.6926644   7.7422037   9.618431    2.508284    5.3540187
  8.24927     4.96435     2.6443048   8.933116    4.0272393   8.583376
  6.9591885   2.9305875   8.131322    3.9143033   8.096406    5.8134246
  9.931866    7.011071    7.2540216   5.1431737  10.5702715   6.340821
  4.2660613   5.669328    0.68163395  3.5624375 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-02 01:59:18.268 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6323.5312, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 170s - loss: nan - MinusLogProbMetric: 6323.5312 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.6935e-08 - 170s/epoch - 867ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 5.645029269476759e-09.
===========
Run 353/720 failed.
Exception type: Exception
Exception message: Training failed for the maximum number of retry.
Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN.py , Line : 637, Func.Name : <module>, Message : hyperparams_dict, NFObject, seed_train, training_time = train_function(seeds = [seed_train, seed_test, seed_dist, seed_metrics],', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN.py , Line : 313, Func.Name : train_function, Message : raise Exception("Training failed for the maximum number of retry.")']
===========

===========
Generating train data for run 354.
===========
Train data generated in 1.34 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_354/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_354/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_354/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_354
self.data_kwargs: {'seed': 520}
self.x_data: [[2.1731846  3.8937397  6.9821544  ... 5.669328   0.68163395 3.5624375 ]
 [6.7535415  2.6749182  6.1859965  ... 3.3760262  4.638645   2.336814  ]
 [6.1686287  2.7568936  6.2012696  ... 2.5904071  3.339636   2.9827425 ]
 ...
 [6.6194353  2.9398146  6.1845684  ... 3.0365293  3.5444417  1.3393478 ]
 [5.613061   6.348512   5.164087   ... 1.829057   7.7712007  1.4071141 ]
 [6.893006   3.0565336  6.057088   ... 2.613742   4.4370584  2.0332472 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_513"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_514 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_53 (LogProbL  (None,)                  1645920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,645,920
Trainable params: 1,645,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_53/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_53'")
self.model: <keras.engine.functional.Functional object at 0x7fad2836ba60>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fad20228e20>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fad20228e20>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fb14c8f59f0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fb330599570>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_354/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fb33059b310>, <keras.callbacks.ModelCheckpoint object at 0x7fb33059b190>, <keras.callbacks.EarlyStopping object at 0x7fb33059bd60>, <keras.callbacks.ReduceLROnPlateau object at 0x7fb33059bbe0>, <keras.callbacks.TerminateOnNaN object at 0x7fb33059b3d0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_354/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 354/720 with hyperparameters:
timestamp = 2023-10-02 01:59:28.090407
ndims = 64
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 1645920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 2.1731846   3.8937397   6.9821544   1.4459838   8.670284    0.996012
  9.747296    5.3488703  10.829596    5.798153    7.4385934  -0.39963475
  2.887594    1.6816981   1.9483483   2.4950166   3.377345    6.4428034
  0.7434967  10.024268    5.9064217   3.9731474   5.977063    2.0677786
  5.421294    7.7411385   2.3877835   6.667169    1.0165741   6.879494
  2.333092    1.6771841   5.2052946  -0.01726444  8.224516   -0.06788193
  7.1896877   2.6926644   7.7422037   9.618431    2.508284    5.3540187
  8.24927     4.96435     2.6443048   8.933116    4.0272393   8.583376
  6.9591885   2.9305875   8.131322    3.9143033   8.096406    5.8134246
  9.931866    7.011071    7.2540216   5.1431737  10.5702715   6.340821
  4.2660613   5.669328    0.68163395  3.5624375 ]
Epoch 1/1000
2023-10-02 02:02:52.163 
Epoch 1/1000 
	 loss: 454.8204, MinusLogProbMetric: 454.8204, val_loss: 141.2573, val_MinusLogProbMetric: 141.2573

Epoch 1: val_loss improved from inf to 141.25728, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_354/weights/best_weights.h5
196/196 - 206s - loss: 454.8204 - MinusLogProbMetric: 454.8204 - val_loss: 141.2573 - val_MinusLogProbMetric: 141.2573 - lr: 0.0010 - 206s/epoch - 1s/step
Epoch 2/1000
2023-10-02 02:03:41.112 
Epoch 2/1000 
	 loss: 112.4068, MinusLogProbMetric: 112.4068, val_loss: 81.6273, val_MinusLogProbMetric: 81.6273

Epoch 2: val_loss improved from 141.25728 to 81.62733, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_354/weights/best_weights.h5
196/196 - 48s - loss: 112.4068 - MinusLogProbMetric: 112.4068 - val_loss: 81.6273 - val_MinusLogProbMetric: 81.6273 - lr: 0.0010 - 48s/epoch - 244ms/step
Epoch 3/1000
2023-10-02 02:04:27.879 
Epoch 3/1000 
	 loss: 70.5022, MinusLogProbMetric: 70.5022, val_loss: 60.5351, val_MinusLogProbMetric: 60.5351

Epoch 3: val_loss improved from 81.62733 to 60.53514, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_354/weights/best_weights.h5
196/196 - 46s - loss: 70.5022 - MinusLogProbMetric: 70.5022 - val_loss: 60.5351 - val_MinusLogProbMetric: 60.5351 - lr: 0.0010 - 46s/epoch - 235ms/step
Epoch 4/1000
2023-10-02 02:05:13.003 
Epoch 4/1000 
	 loss: 56.3664, MinusLogProbMetric: 56.3664, val_loss: 56.3004, val_MinusLogProbMetric: 56.3004

Epoch 4: val_loss improved from 60.53514 to 56.30045, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_354/weights/best_weights.h5
196/196 - 46s - loss: 56.3664 - MinusLogProbMetric: 56.3664 - val_loss: 56.3004 - val_MinusLogProbMetric: 56.3004 - lr: 0.0010 - 46s/epoch - 234ms/step
Epoch 5/1000
2023-10-02 02:05:59.513 
Epoch 5/1000 
	 loss: 55.0282, MinusLogProbMetric: 55.0282, val_loss: 68.0254, val_MinusLogProbMetric: 68.0254

Epoch 5: val_loss did not improve from 56.30045
196/196 - 45s - loss: 55.0282 - MinusLogProbMetric: 55.0282 - val_loss: 68.0254 - val_MinusLogProbMetric: 68.0254 - lr: 0.0010 - 45s/epoch - 231ms/step
Epoch 6/1000
2023-10-02 02:06:43.286 
Epoch 6/1000 
	 loss: 47.9448, MinusLogProbMetric: 47.9448, val_loss: 47.7764, val_MinusLogProbMetric: 47.7764

Epoch 6: val_loss improved from 56.30045 to 47.77639, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_354/weights/best_weights.h5
196/196 - 46s - loss: 47.9448 - MinusLogProbMetric: 47.9448 - val_loss: 47.7764 - val_MinusLogProbMetric: 47.7764 - lr: 0.0010 - 46s/epoch - 235ms/step
Epoch 7/1000
2023-10-02 02:07:30.467 
Epoch 7/1000 
	 loss: 44.3607, MinusLogProbMetric: 44.3607, val_loss: 46.6457, val_MinusLogProbMetric: 46.6457

Epoch 7: val_loss improved from 47.77639 to 46.64573, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_354/weights/best_weights.h5
196/196 - 46s - loss: 44.3607 - MinusLogProbMetric: 44.3607 - val_loss: 46.6457 - val_MinusLogProbMetric: 46.6457 - lr: 0.0010 - 46s/epoch - 235ms/step
Epoch 8/1000
2023-10-02 02:08:18.792 
Epoch 8/1000 
	 loss: 47.7511, MinusLogProbMetric: 47.7511, val_loss: 46.3464, val_MinusLogProbMetric: 46.3464

Epoch 8: val_loss improved from 46.64573 to 46.34637, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_354/weights/best_weights.h5
196/196 - 48s - loss: 47.7511 - MinusLogProbMetric: 47.7511 - val_loss: 46.3464 - val_MinusLogProbMetric: 46.3464 - lr: 0.0010 - 48s/epoch - 245ms/step
Epoch 9/1000
2023-10-02 02:09:04.277 
Epoch 9/1000 
	 loss: 41.6705, MinusLogProbMetric: 41.6705, val_loss: 39.8360, val_MinusLogProbMetric: 39.8360

Epoch 9: val_loss improved from 46.34637 to 39.83602, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_354/weights/best_weights.h5
196/196 - 46s - loss: 41.6705 - MinusLogProbMetric: 41.6705 - val_loss: 39.8360 - val_MinusLogProbMetric: 39.8360 - lr: 0.0010 - 46s/epoch - 233ms/step
Epoch 10/1000
2023-10-02 02:09:52.287 
Epoch 10/1000 
	 loss: 40.2140, MinusLogProbMetric: 40.2140, val_loss: 47.1479, val_MinusLogProbMetric: 47.1479

Epoch 10: val_loss did not improve from 39.83602
196/196 - 47s - loss: 40.2140 - MinusLogProbMetric: 40.2140 - val_loss: 47.1479 - val_MinusLogProbMetric: 47.1479 - lr: 0.0010 - 47s/epoch - 239ms/step
Epoch 11/1000
2023-10-02 02:10:37.777 
Epoch 11/1000 
	 loss: 39.8018, MinusLogProbMetric: 39.8018, val_loss: 37.8028, val_MinusLogProbMetric: 37.8028

Epoch 11: val_loss improved from 39.83602 to 37.80278, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_354/weights/best_weights.h5
196/196 - 46s - loss: 39.8018 - MinusLogProbMetric: 39.8018 - val_loss: 37.8028 - val_MinusLogProbMetric: 37.8028 - lr: 0.0010 - 46s/epoch - 234ms/step
Epoch 12/1000
2023-10-02 02:11:24.001 
Epoch 12/1000 
	 loss: 40.6596, MinusLogProbMetric: 40.6596, val_loss: 40.7518, val_MinusLogProbMetric: 40.7518

Epoch 12: val_loss did not improve from 37.80278
196/196 - 46s - loss: 40.6596 - MinusLogProbMetric: 40.6596 - val_loss: 40.7518 - val_MinusLogProbMetric: 40.7518 - lr: 0.0010 - 46s/epoch - 234ms/step
Epoch 13/1000
2023-10-02 02:12:09.065 
Epoch 13/1000 
	 loss: 38.2235, MinusLogProbMetric: 38.2235, val_loss: 36.6020, val_MinusLogProbMetric: 36.6020

Epoch 13: val_loss improved from 37.80278 to 36.60199, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_354/weights/best_weights.h5
196/196 - 47s - loss: 38.2235 - MinusLogProbMetric: 38.2235 - val_loss: 36.6020 - val_MinusLogProbMetric: 36.6020 - lr: 0.0010 - 47s/epoch - 239ms/step
Epoch 14/1000
2023-10-02 02:12:56.857 
Epoch 14/1000 
	 loss: 37.9601, MinusLogProbMetric: 37.9601, val_loss: 37.1180, val_MinusLogProbMetric: 37.1180

Epoch 14: val_loss did not improve from 36.60199
196/196 - 46s - loss: 37.9601 - MinusLogProbMetric: 37.9601 - val_loss: 37.1180 - val_MinusLogProbMetric: 37.1180 - lr: 0.0010 - 46s/epoch - 235ms/step
Epoch 15/1000
2023-10-02 02:13:40.875 
Epoch 15/1000 
	 loss: 37.1042, MinusLogProbMetric: 37.1042, val_loss: 37.1101, val_MinusLogProbMetric: 37.1101

Epoch 15: val_loss did not improve from 36.60199
196/196 - 44s - loss: 37.1042 - MinusLogProbMetric: 37.1042 - val_loss: 37.1101 - val_MinusLogProbMetric: 37.1101 - lr: 0.0010 - 44s/epoch - 225ms/step
Epoch 16/1000
2023-10-02 02:14:26.901 
Epoch 16/1000 
	 loss: 37.5676, MinusLogProbMetric: 37.5676, val_loss: 37.5316, val_MinusLogProbMetric: 37.5316

Epoch 16: val_loss did not improve from 36.60199
196/196 - 46s - loss: 37.5676 - MinusLogProbMetric: 37.5676 - val_loss: 37.5316 - val_MinusLogProbMetric: 37.5316 - lr: 0.0010 - 46s/epoch - 235ms/step
Epoch 17/1000
2023-10-02 02:15:11.301 
Epoch 17/1000 
	 loss: 36.2875, MinusLogProbMetric: 36.2875, val_loss: 35.6504, val_MinusLogProbMetric: 35.6504

Epoch 17: val_loss improved from 36.60199 to 35.65042, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_354/weights/best_weights.h5
196/196 - 45s - loss: 36.2875 - MinusLogProbMetric: 36.2875 - val_loss: 35.6504 - val_MinusLogProbMetric: 35.6504 - lr: 0.0010 - 45s/epoch - 228ms/step
Epoch 18/1000
2023-10-02 02:15:57.253 
Epoch 18/1000 
	 loss: 36.1660, MinusLogProbMetric: 36.1660, val_loss: 35.8491, val_MinusLogProbMetric: 35.8491

Epoch 18: val_loss did not improve from 35.65042
196/196 - 46s - loss: 36.1660 - MinusLogProbMetric: 36.1660 - val_loss: 35.8491 - val_MinusLogProbMetric: 35.8491 - lr: 0.0010 - 46s/epoch - 233ms/step
Epoch 19/1000
2023-10-02 02:16:43.070 
Epoch 19/1000 
	 loss: 35.3339, MinusLogProbMetric: 35.3339, val_loss: 34.8204, val_MinusLogProbMetric: 34.8204

Epoch 19: val_loss improved from 35.65042 to 34.82036, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_354/weights/best_weights.h5
196/196 - 46s - loss: 35.3339 - MinusLogProbMetric: 35.3339 - val_loss: 34.8204 - val_MinusLogProbMetric: 34.8204 - lr: 0.0010 - 46s/epoch - 237ms/step
Epoch 20/1000
2023-10-02 02:17:30.462 
Epoch 20/1000 
	 loss: 35.1061, MinusLogProbMetric: 35.1061, val_loss: 36.4800, val_MinusLogProbMetric: 36.4800

Epoch 20: val_loss did not improve from 34.82036
196/196 - 47s - loss: 35.1061 - MinusLogProbMetric: 35.1061 - val_loss: 36.4800 - val_MinusLogProbMetric: 36.4800 - lr: 0.0010 - 47s/epoch - 239ms/step
Epoch 21/1000
2023-10-02 02:18:14.592 
Epoch 21/1000 
	 loss: 35.2237, MinusLogProbMetric: 35.2237, val_loss: 35.8978, val_MinusLogProbMetric: 35.8978

Epoch 21: val_loss did not improve from 34.82036
196/196 - 44s - loss: 35.2237 - MinusLogProbMetric: 35.2237 - val_loss: 35.8978 - val_MinusLogProbMetric: 35.8978 - lr: 0.0010 - 44s/epoch - 225ms/step
Epoch 22/1000
2023-10-02 02:18:58.844 
Epoch 22/1000 
	 loss: 34.6039, MinusLogProbMetric: 34.6039, val_loss: 33.8099, val_MinusLogProbMetric: 33.8099

Epoch 22: val_loss improved from 34.82036 to 33.80994, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_354/weights/best_weights.h5
196/196 - 48s - loss: 34.6039 - MinusLogProbMetric: 34.6039 - val_loss: 33.8099 - val_MinusLogProbMetric: 33.8099 - lr: 0.0010 - 48s/epoch - 243ms/step
Epoch 23/1000
2023-10-02 02:19:44.852 
Epoch 23/1000 
	 loss: 34.2283, MinusLogProbMetric: 34.2283, val_loss: 34.8881, val_MinusLogProbMetric: 34.8881

Epoch 23: val_loss did not improve from 33.80994
196/196 - 43s - loss: 34.2283 - MinusLogProbMetric: 34.2283 - val_loss: 34.8881 - val_MinusLogProbMetric: 34.8881 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 24/1000
2023-10-02 02:20:29.685 
Epoch 24/1000 
	 loss: 34.5272, MinusLogProbMetric: 34.5272, val_loss: 35.3994, val_MinusLogProbMetric: 35.3994

Epoch 24: val_loss did not improve from 33.80994
196/196 - 45s - loss: 34.5272 - MinusLogProbMetric: 34.5272 - val_loss: 35.3994 - val_MinusLogProbMetric: 35.3994 - lr: 0.0010 - 45s/epoch - 229ms/step
Epoch 25/1000
2023-10-02 02:21:17.658 
Epoch 25/1000 
	 loss: 33.5819, MinusLogProbMetric: 33.5819, val_loss: 33.2902, val_MinusLogProbMetric: 33.2902

Epoch 25: val_loss improved from 33.80994 to 33.29019, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_354/weights/best_weights.h5
196/196 - 49s - loss: 33.5819 - MinusLogProbMetric: 33.5819 - val_loss: 33.2902 - val_MinusLogProbMetric: 33.2902 - lr: 0.0010 - 49s/epoch - 250ms/step
Epoch 26/1000
2023-10-02 02:22:06.232 
Epoch 26/1000 
	 loss: 34.4735, MinusLogProbMetric: 34.4735, val_loss: 34.1521, val_MinusLogProbMetric: 34.1521

Epoch 26: val_loss did not improve from 33.29019
196/196 - 48s - loss: 34.4735 - MinusLogProbMetric: 34.4735 - val_loss: 34.1521 - val_MinusLogProbMetric: 34.1521 - lr: 0.0010 - 48s/epoch - 242ms/step
Epoch 27/1000
2023-10-02 02:22:49.489 
Epoch 27/1000 
	 loss: 33.5545, MinusLogProbMetric: 33.5545, val_loss: 33.5744, val_MinusLogProbMetric: 33.5744

Epoch 27: val_loss did not improve from 33.29019
196/196 - 43s - loss: 33.5545 - MinusLogProbMetric: 33.5545 - val_loss: 33.5744 - val_MinusLogProbMetric: 33.5744 - lr: 0.0010 - 43s/epoch - 221ms/step
Epoch 28/1000
2023-10-02 02:23:33.588 
Epoch 28/1000 
	 loss: 33.6971, MinusLogProbMetric: 33.6971, val_loss: 33.8389, val_MinusLogProbMetric: 33.8389

Epoch 28: val_loss did not improve from 33.29019
196/196 - 44s - loss: 33.6971 - MinusLogProbMetric: 33.6971 - val_loss: 33.8389 - val_MinusLogProbMetric: 33.8389 - lr: 0.0010 - 44s/epoch - 225ms/step
Epoch 29/1000
2023-10-02 02:24:20.509 
Epoch 29/1000 
	 loss: 33.3258, MinusLogProbMetric: 33.3258, val_loss: 32.6961, val_MinusLogProbMetric: 32.6961

Epoch 29: val_loss improved from 33.29019 to 32.69615, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_354/weights/best_weights.h5
196/196 - 48s - loss: 33.3258 - MinusLogProbMetric: 33.3258 - val_loss: 32.6961 - val_MinusLogProbMetric: 32.6961 - lr: 0.0010 - 48s/epoch - 243ms/step
Epoch 30/1000
2023-10-02 02:25:06.639 
Epoch 30/1000 
	 loss: 33.1163, MinusLogProbMetric: 33.1163, val_loss: 34.4653, val_MinusLogProbMetric: 34.4653

Epoch 30: val_loss did not improve from 32.69615
196/196 - 46s - loss: 33.1163 - MinusLogProbMetric: 33.1163 - val_loss: 34.4653 - val_MinusLogProbMetric: 34.4653 - lr: 0.0010 - 46s/epoch - 232ms/step
Epoch 31/1000
2023-10-02 02:25:52.522 
Epoch 31/1000 
	 loss: 33.3609, MinusLogProbMetric: 33.3609, val_loss: 32.4150, val_MinusLogProbMetric: 32.4150

Epoch 31: val_loss improved from 32.69615 to 32.41496, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_354/weights/best_weights.h5
196/196 - 47s - loss: 33.3609 - MinusLogProbMetric: 33.3609 - val_loss: 32.4150 - val_MinusLogProbMetric: 32.4150 - lr: 0.0010 - 47s/epoch - 239ms/step
Epoch 32/1000
2023-10-02 02:26:38.412 
Epoch 32/1000 
	 loss: 32.9743, MinusLogProbMetric: 32.9743, val_loss: 33.3846, val_MinusLogProbMetric: 33.3846

Epoch 32: val_loss did not improve from 32.41496
196/196 - 45s - loss: 32.9743 - MinusLogProbMetric: 32.9743 - val_loss: 33.3846 - val_MinusLogProbMetric: 33.3846 - lr: 0.0010 - 45s/epoch - 229ms/step
Epoch 33/1000
2023-10-02 02:27:22.025 
Epoch 33/1000 
	 loss: 32.9107, MinusLogProbMetric: 32.9107, val_loss: 33.4531, val_MinusLogProbMetric: 33.4531

Epoch 33: val_loss did not improve from 32.41496
196/196 - 44s - loss: 32.9107 - MinusLogProbMetric: 32.9107 - val_loss: 33.4531 - val_MinusLogProbMetric: 33.4531 - lr: 0.0010 - 44s/epoch - 222ms/step
Epoch 34/1000
2023-10-02 02:28:07.635 
Epoch 34/1000 
	 loss: 32.6668, MinusLogProbMetric: 32.6668, val_loss: 32.7465, val_MinusLogProbMetric: 32.7465

Epoch 34: val_loss did not improve from 32.41496
196/196 - 46s - loss: 32.6668 - MinusLogProbMetric: 32.6668 - val_loss: 32.7465 - val_MinusLogProbMetric: 32.7465 - lr: 0.0010 - 46s/epoch - 233ms/step
Epoch 35/1000
2023-10-02 02:28:51.669 
Epoch 35/1000 
	 loss: 32.5925, MinusLogProbMetric: 32.5925, val_loss: 31.8128, val_MinusLogProbMetric: 31.8128

Epoch 35: val_loss improved from 32.41496 to 31.81281, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_354/weights/best_weights.h5
196/196 - 45s - loss: 32.5925 - MinusLogProbMetric: 32.5925 - val_loss: 31.8128 - val_MinusLogProbMetric: 31.8128 - lr: 0.0010 - 45s/epoch - 229ms/step
Epoch 36/1000
2023-10-02 02:29:36.865 
Epoch 36/1000 
	 loss: 32.5719, MinusLogProbMetric: 32.5719, val_loss: 32.7340, val_MinusLogProbMetric: 32.7340

Epoch 36: val_loss did not improve from 31.81281
196/196 - 44s - loss: 32.5719 - MinusLogProbMetric: 32.5719 - val_loss: 32.7340 - val_MinusLogProbMetric: 32.7340 - lr: 0.0010 - 44s/epoch - 226ms/step
Epoch 37/1000
2023-10-02 02:30:21.792 
Epoch 37/1000 
	 loss: 32.3265, MinusLogProbMetric: 32.3265, val_loss: 33.8239, val_MinusLogProbMetric: 33.8239

Epoch 37: val_loss did not improve from 31.81281
196/196 - 45s - loss: 32.3265 - MinusLogProbMetric: 32.3265 - val_loss: 33.8239 - val_MinusLogProbMetric: 33.8239 - lr: 0.0010 - 45s/epoch - 229ms/step
Epoch 38/1000
2023-10-02 02:31:07.705 
Epoch 38/1000 
	 loss: 32.6729, MinusLogProbMetric: 32.6729, val_loss: 32.4377, val_MinusLogProbMetric: 32.4377

Epoch 38: val_loss did not improve from 31.81281
196/196 - 46s - loss: 32.6729 - MinusLogProbMetric: 32.6729 - val_loss: 32.4377 - val_MinusLogProbMetric: 32.4377 - lr: 0.0010 - 46s/epoch - 234ms/step
Epoch 39/1000
2023-10-02 02:31:52.808 
Epoch 39/1000 
	 loss: 32.3323, MinusLogProbMetric: 32.3323, val_loss: 32.2781, val_MinusLogProbMetric: 32.2781

Epoch 39: val_loss did not improve from 31.81281
196/196 - 45s - loss: 32.3323 - MinusLogProbMetric: 32.3323 - val_loss: 32.2781 - val_MinusLogProbMetric: 32.2781 - lr: 0.0010 - 45s/epoch - 230ms/step
Epoch 40/1000
2023-10-02 02:32:36.404 
Epoch 40/1000 
	 loss: 32.3068, MinusLogProbMetric: 32.3068, val_loss: 32.1562, val_MinusLogProbMetric: 32.1562

Epoch 40: val_loss did not improve from 31.81281
196/196 - 44s - loss: 32.3068 - MinusLogProbMetric: 32.3068 - val_loss: 32.1562 - val_MinusLogProbMetric: 32.1562 - lr: 0.0010 - 44s/epoch - 222ms/step
Epoch 41/1000
2023-10-02 02:33:19.331 
Epoch 41/1000 
	 loss: 32.1433, MinusLogProbMetric: 32.1433, val_loss: 32.8708, val_MinusLogProbMetric: 32.8708

Epoch 41: val_loss did not improve from 31.81281
196/196 - 43s - loss: 32.1433 - MinusLogProbMetric: 32.1433 - val_loss: 32.8708 - val_MinusLogProbMetric: 32.8708 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 42/1000
2023-10-02 02:34:04.386 
Epoch 42/1000 
	 loss: 32.0901, MinusLogProbMetric: 32.0901, val_loss: 33.5938, val_MinusLogProbMetric: 33.5938

Epoch 42: val_loss did not improve from 31.81281
196/196 - 45s - loss: 32.0901 - MinusLogProbMetric: 32.0901 - val_loss: 33.5938 - val_MinusLogProbMetric: 33.5938 - lr: 0.0010 - 45s/epoch - 230ms/step
Epoch 43/1000
2023-10-02 02:34:47.890 
Epoch 43/1000 
	 loss: 31.8643, MinusLogProbMetric: 31.8643, val_loss: 32.0334, val_MinusLogProbMetric: 32.0334

Epoch 43: val_loss did not improve from 31.81281
196/196 - 43s - loss: 31.8643 - MinusLogProbMetric: 31.8643 - val_loss: 32.0334 - val_MinusLogProbMetric: 32.0334 - lr: 0.0010 - 43s/epoch - 222ms/step
Epoch 44/1000
2023-10-02 02:35:31.313 
Epoch 44/1000 
	 loss: 31.8210, MinusLogProbMetric: 31.8210, val_loss: 33.5974, val_MinusLogProbMetric: 33.5974

Epoch 44: val_loss did not improve from 31.81281
196/196 - 43s - loss: 31.8210 - MinusLogProbMetric: 31.8210 - val_loss: 33.5974 - val_MinusLogProbMetric: 33.5974 - lr: 0.0010 - 43s/epoch - 222ms/step
Epoch 45/1000
2023-10-02 02:36:15.371 
Epoch 45/1000 
	 loss: 32.1662, MinusLogProbMetric: 32.1662, val_loss: 32.1815, val_MinusLogProbMetric: 32.1815

Epoch 45: val_loss did not improve from 31.81281
196/196 - 44s - loss: 32.1662 - MinusLogProbMetric: 32.1662 - val_loss: 32.1815 - val_MinusLogProbMetric: 32.1815 - lr: 0.0010 - 44s/epoch - 225ms/step
Epoch 46/1000
2023-10-02 02:37:00.572 
Epoch 46/1000 
	 loss: 31.5561, MinusLogProbMetric: 31.5561, val_loss: 31.1896, val_MinusLogProbMetric: 31.1896

Epoch 46: val_loss improved from 31.81281 to 31.18957, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_354/weights/best_weights.h5
196/196 - 47s - loss: 31.5561 - MinusLogProbMetric: 31.5561 - val_loss: 31.1896 - val_MinusLogProbMetric: 31.1896 - lr: 0.0010 - 47s/epoch - 238ms/step
Epoch 47/1000
2023-10-02 02:37:45.592 
Epoch 47/1000 
	 loss: 31.8503, MinusLogProbMetric: 31.8503, val_loss: 32.5261, val_MinusLogProbMetric: 32.5261

Epoch 47: val_loss did not improve from 31.18957
196/196 - 44s - loss: 31.8503 - MinusLogProbMetric: 31.8503 - val_loss: 32.5261 - val_MinusLogProbMetric: 32.5261 - lr: 0.0010 - 44s/epoch - 223ms/step
Epoch 48/1000
2023-10-02 02:38:29.824 
Epoch 48/1000 
	 loss: 31.8562, MinusLogProbMetric: 31.8562, val_loss: 34.6857, val_MinusLogProbMetric: 34.6857

Epoch 48: val_loss did not improve from 31.18957
196/196 - 44s - loss: 31.8562 - MinusLogProbMetric: 31.8562 - val_loss: 34.6857 - val_MinusLogProbMetric: 34.6857 - lr: 0.0010 - 44s/epoch - 225ms/step
Epoch 49/1000
2023-10-02 02:39:14.647 
Epoch 49/1000 
	 loss: 32.1622, MinusLogProbMetric: 32.1622, val_loss: 32.0156, val_MinusLogProbMetric: 32.0156

Epoch 49: val_loss did not improve from 31.18957
196/196 - 45s - loss: 32.1622 - MinusLogProbMetric: 32.1622 - val_loss: 32.0156 - val_MinusLogProbMetric: 32.0156 - lr: 0.0010 - 45s/epoch - 228ms/step
Epoch 50/1000
2023-10-02 02:39:58.987 
Epoch 50/1000 
	 loss: 31.7709, MinusLogProbMetric: 31.7709, val_loss: 33.3445, val_MinusLogProbMetric: 33.3445

Epoch 50: val_loss did not improve from 31.18957
196/196 - 44s - loss: 31.7709 - MinusLogProbMetric: 31.7709 - val_loss: 33.3445 - val_MinusLogProbMetric: 33.3445 - lr: 0.0010 - 44s/epoch - 226ms/step
Epoch 51/1000
2023-10-02 02:40:44.675 
Epoch 51/1000 
	 loss: 31.7218, MinusLogProbMetric: 31.7218, val_loss: 32.2160, val_MinusLogProbMetric: 32.2160

Epoch 51: val_loss did not improve from 31.18957
196/196 - 46s - loss: 31.7218 - MinusLogProbMetric: 31.7218 - val_loss: 32.2160 - val_MinusLogProbMetric: 32.2160 - lr: 0.0010 - 46s/epoch - 233ms/step
Epoch 52/1000
2023-10-02 02:41:27.191 
Epoch 52/1000 
	 loss: 31.6348, MinusLogProbMetric: 31.6348, val_loss: 31.3941, val_MinusLogProbMetric: 31.3941

Epoch 52: val_loss did not improve from 31.18957
196/196 - 43s - loss: 31.6348 - MinusLogProbMetric: 31.6348 - val_loss: 31.3941 - val_MinusLogProbMetric: 31.3941 - lr: 0.0010 - 43s/epoch - 217ms/step
Epoch 53/1000
2023-10-02 02:42:10.350 
Epoch 53/1000 
	 loss: 31.4058, MinusLogProbMetric: 31.4058, val_loss: 31.6937, val_MinusLogProbMetric: 31.6937

Epoch 53: val_loss did not improve from 31.18957
196/196 - 43s - loss: 31.4058 - MinusLogProbMetric: 31.4058 - val_loss: 31.6937 - val_MinusLogProbMetric: 31.6937 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 54/1000
2023-10-02 02:42:55.120 
Epoch 54/1000 
	 loss: 31.4110, MinusLogProbMetric: 31.4110, val_loss: 33.7791, val_MinusLogProbMetric: 33.7791

Epoch 54: val_loss did not improve from 31.18957
196/196 - 45s - loss: 31.4110 - MinusLogProbMetric: 31.4110 - val_loss: 33.7791 - val_MinusLogProbMetric: 33.7791 - lr: 0.0010 - 45s/epoch - 228ms/step
Epoch 55/1000
2023-10-02 02:43:39.050 
Epoch 55/1000 
	 loss: 31.5219, MinusLogProbMetric: 31.5219, val_loss: 32.0697, val_MinusLogProbMetric: 32.0697

Epoch 55: val_loss did not improve from 31.18957
196/196 - 44s - loss: 31.5219 - MinusLogProbMetric: 31.5219 - val_loss: 32.0697 - val_MinusLogProbMetric: 32.0697 - lr: 0.0010 - 44s/epoch - 224ms/step
Epoch 56/1000
2023-10-02 02:44:22.216 
Epoch 56/1000 
	 loss: 31.7210, MinusLogProbMetric: 31.7210, val_loss: 31.3733, val_MinusLogProbMetric: 31.3733

Epoch 56: val_loss did not improve from 31.18957
196/196 - 43s - loss: 31.7210 - MinusLogProbMetric: 31.7210 - val_loss: 31.3733 - val_MinusLogProbMetric: 31.3733 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 57/1000
2023-10-02 02:45:06.354 
Epoch 57/1000 
	 loss: 31.4982, MinusLogProbMetric: 31.4982, val_loss: 31.1761, val_MinusLogProbMetric: 31.1761

Epoch 57: val_loss improved from 31.18957 to 31.17607, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_354/weights/best_weights.h5
196/196 - 45s - loss: 31.4982 - MinusLogProbMetric: 31.4982 - val_loss: 31.1761 - val_MinusLogProbMetric: 31.1761 - lr: 0.0010 - 45s/epoch - 232ms/step
Epoch 58/1000
2023-10-02 02:45:53.560 
Epoch 58/1000 
	 loss: 31.3365, MinusLogProbMetric: 31.3365, val_loss: 31.6739, val_MinusLogProbMetric: 31.6739

Epoch 58: val_loss did not improve from 31.17607
196/196 - 46s - loss: 31.3365 - MinusLogProbMetric: 31.3365 - val_loss: 31.6739 - val_MinusLogProbMetric: 31.6739 - lr: 0.0010 - 46s/epoch - 234ms/step
Epoch 59/1000
2023-10-02 02:46:37.347 
Epoch 59/1000 
	 loss: 31.2211, MinusLogProbMetric: 31.2211, val_loss: 34.4792, val_MinusLogProbMetric: 34.4792

Epoch 59: val_loss did not improve from 31.17607
196/196 - 44s - loss: 31.2211 - MinusLogProbMetric: 31.2211 - val_loss: 34.4792 - val_MinusLogProbMetric: 34.4792 - lr: 0.0010 - 44s/epoch - 223ms/step
Epoch 60/1000
2023-10-02 02:47:20.159 
Epoch 60/1000 
	 loss: 31.2178, MinusLogProbMetric: 31.2178, val_loss: 30.7762, val_MinusLogProbMetric: 30.7762

Epoch 60: val_loss improved from 31.17607 to 30.77615, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_354/weights/best_weights.h5
196/196 - 44s - loss: 31.2178 - MinusLogProbMetric: 31.2178 - val_loss: 30.7762 - val_MinusLogProbMetric: 30.7762 - lr: 0.0010 - 44s/epoch - 224ms/step
Epoch 61/1000
2023-10-02 02:48:04.458 
Epoch 61/1000 
	 loss: 31.2500, MinusLogProbMetric: 31.2500, val_loss: 31.2590, val_MinusLogProbMetric: 31.2590

Epoch 61: val_loss did not improve from 30.77615
196/196 - 43s - loss: 31.2500 - MinusLogProbMetric: 31.2500 - val_loss: 31.2590 - val_MinusLogProbMetric: 31.2590 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 62/1000
2023-10-02 02:48:50.499 
Epoch 62/1000 
	 loss: 30.9635, MinusLogProbMetric: 30.9635, val_loss: 31.6555, val_MinusLogProbMetric: 31.6555

Epoch 62: val_loss did not improve from 30.77615
196/196 - 46s - loss: 30.9635 - MinusLogProbMetric: 30.9635 - val_loss: 31.6555 - val_MinusLogProbMetric: 31.6555 - lr: 0.0010 - 46s/epoch - 235ms/step
Epoch 63/1000
2023-10-02 02:49:35.521 
Epoch 63/1000 
	 loss: 31.1062, MinusLogProbMetric: 31.1062, val_loss: 31.3895, val_MinusLogProbMetric: 31.3895

Epoch 63: val_loss did not improve from 30.77615
196/196 - 45s - loss: 31.1062 - MinusLogProbMetric: 31.1062 - val_loss: 31.3895 - val_MinusLogProbMetric: 31.3895 - lr: 0.0010 - 45s/epoch - 229ms/step
Epoch 64/1000
2023-10-02 02:50:18.473 
Epoch 64/1000 
	 loss: 31.2025, MinusLogProbMetric: 31.2025, val_loss: 31.1685, val_MinusLogProbMetric: 31.1685

Epoch 64: val_loss did not improve from 30.77615
196/196 - 43s - loss: 31.2025 - MinusLogProbMetric: 31.2025 - val_loss: 31.1685 - val_MinusLogProbMetric: 31.1685 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 65/1000
2023-10-02 02:51:04.586 
Epoch 65/1000 
	 loss: 31.0886, MinusLogProbMetric: 31.0886, val_loss: 31.2308, val_MinusLogProbMetric: 31.2308

Epoch 65: val_loss did not improve from 30.77615
196/196 - 46s - loss: 31.0886 - MinusLogProbMetric: 31.0886 - val_loss: 31.2308 - val_MinusLogProbMetric: 31.2308 - lr: 0.0010 - 46s/epoch - 235ms/step
Epoch 66/1000
2023-10-02 02:51:48.857 
Epoch 66/1000 
	 loss: 31.3063, MinusLogProbMetric: 31.3063, val_loss: 31.7108, val_MinusLogProbMetric: 31.7108

Epoch 66: val_loss did not improve from 30.77615
196/196 - 44s - loss: 31.3063 - MinusLogProbMetric: 31.3063 - val_loss: 31.7108 - val_MinusLogProbMetric: 31.7108 - lr: 0.0010 - 44s/epoch - 226ms/step
Epoch 67/1000
2023-10-02 02:52:34.002 
Epoch 67/1000 
	 loss: 31.0292, MinusLogProbMetric: 31.0292, val_loss: 31.4245, val_MinusLogProbMetric: 31.4245

Epoch 67: val_loss did not improve from 30.77615
196/196 - 45s - loss: 31.0292 - MinusLogProbMetric: 31.0292 - val_loss: 31.4245 - val_MinusLogProbMetric: 31.4245 - lr: 0.0010 - 45s/epoch - 230ms/step
Epoch 68/1000
2023-10-02 02:53:17.924 
Epoch 68/1000 
	 loss: 30.9203, MinusLogProbMetric: 30.9203, val_loss: 31.2046, val_MinusLogProbMetric: 31.2046

Epoch 68: val_loss did not improve from 30.77615
196/196 - 44s - loss: 30.9203 - MinusLogProbMetric: 30.9203 - val_loss: 31.2046 - val_MinusLogProbMetric: 31.2046 - lr: 0.0010 - 44s/epoch - 224ms/step
Epoch 69/1000
2023-10-02 02:54:03.976 
Epoch 69/1000 
	 loss: 30.9596, MinusLogProbMetric: 30.9596, val_loss: 31.4942, val_MinusLogProbMetric: 31.4942

Epoch 69: val_loss did not improve from 30.77615
196/196 - 46s - loss: 30.9596 - MinusLogProbMetric: 30.9596 - val_loss: 31.4942 - val_MinusLogProbMetric: 31.4942 - lr: 0.0010 - 46s/epoch - 235ms/step
Epoch 70/1000
2023-10-02 02:54:46.685 
Epoch 70/1000 
	 loss: 30.9209, MinusLogProbMetric: 30.9209, val_loss: 31.7463, val_MinusLogProbMetric: 31.7463

Epoch 70: val_loss did not improve from 30.77615
196/196 - 43s - loss: 30.9209 - MinusLogProbMetric: 30.9209 - val_loss: 31.7463 - val_MinusLogProbMetric: 31.7463 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 71/1000
2023-10-02 02:55:32.001 
Epoch 71/1000 
	 loss: 30.7405, MinusLogProbMetric: 30.7405, val_loss: 31.2331, val_MinusLogProbMetric: 31.2331

Epoch 71: val_loss did not improve from 30.77615
196/196 - 45s - loss: 30.7405 - MinusLogProbMetric: 30.7405 - val_loss: 31.2331 - val_MinusLogProbMetric: 31.2331 - lr: 0.0010 - 45s/epoch - 231ms/step
Epoch 72/1000
2023-10-02 02:56:15.295 
Epoch 72/1000 
	 loss: 30.7675, MinusLogProbMetric: 30.7675, val_loss: 31.9298, val_MinusLogProbMetric: 31.9298

Epoch 72: val_loss did not improve from 30.77615
196/196 - 43s - loss: 30.7675 - MinusLogProbMetric: 30.7675 - val_loss: 31.9298 - val_MinusLogProbMetric: 31.9298 - lr: 0.0010 - 43s/epoch - 221ms/step
Epoch 73/1000
2023-10-02 02:56:59.341 
Epoch 73/1000 
	 loss: 30.8339, MinusLogProbMetric: 30.8339, val_loss: 31.6034, val_MinusLogProbMetric: 31.6034

Epoch 73: val_loss did not improve from 30.77615
196/196 - 44s - loss: 30.8339 - MinusLogProbMetric: 30.8339 - val_loss: 31.6034 - val_MinusLogProbMetric: 31.6034 - lr: 0.0010 - 44s/epoch - 225ms/step
Epoch 74/1000
2023-10-02 02:57:41.473 
Epoch 74/1000 
	 loss: 30.9049, MinusLogProbMetric: 30.9049, val_loss: 32.8252, val_MinusLogProbMetric: 32.8252

Epoch 74: val_loss did not improve from 30.77615
196/196 - 42s - loss: 30.9049 - MinusLogProbMetric: 30.9049 - val_loss: 32.8252 - val_MinusLogProbMetric: 32.8252 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 75/1000
2023-10-02 02:58:26.075 
Epoch 75/1000 
	 loss: 30.6745, MinusLogProbMetric: 30.6745, val_loss: 30.9286, val_MinusLogProbMetric: 30.9286

Epoch 75: val_loss did not improve from 30.77615
196/196 - 45s - loss: 30.6745 - MinusLogProbMetric: 30.6745 - val_loss: 30.9286 - val_MinusLogProbMetric: 30.9286 - lr: 0.0010 - 45s/epoch - 227ms/step
Epoch 76/1000
2023-10-02 02:59:09.966 
Epoch 76/1000 
	 loss: 30.9157, MinusLogProbMetric: 30.9157, val_loss: 31.9051, val_MinusLogProbMetric: 31.9051

Epoch 76: val_loss did not improve from 30.77615
196/196 - 44s - loss: 30.9157 - MinusLogProbMetric: 30.9157 - val_loss: 31.9051 - val_MinusLogProbMetric: 31.9051 - lr: 0.0010 - 44s/epoch - 224ms/step
Epoch 77/1000
2023-10-02 02:59:52.905 
Epoch 77/1000 
	 loss: 30.8311, MinusLogProbMetric: 30.8311, val_loss: 31.2900, val_MinusLogProbMetric: 31.2900

Epoch 77: val_loss did not improve from 30.77615
196/196 - 43s - loss: 30.8311 - MinusLogProbMetric: 30.8311 - val_loss: 31.2900 - val_MinusLogProbMetric: 31.2900 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 78/1000
2023-10-02 03:00:37.860 
Epoch 78/1000 
	 loss: 30.6897, MinusLogProbMetric: 30.6897, val_loss: 31.2217, val_MinusLogProbMetric: 31.2217

Epoch 78: val_loss did not improve from 30.77615
196/196 - 45s - loss: 30.6897 - MinusLogProbMetric: 30.6897 - val_loss: 31.2217 - val_MinusLogProbMetric: 31.2217 - lr: 0.0010 - 45s/epoch - 229ms/step
Epoch 79/1000
2023-10-02 03:01:19.344 
Epoch 79/1000 
	 loss: 30.6806, MinusLogProbMetric: 30.6806, val_loss: 30.6606, val_MinusLogProbMetric: 30.6606

Epoch 79: val_loss improved from 30.77615 to 30.66065, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_354/weights/best_weights.h5
196/196 - 43s - loss: 30.6806 - MinusLogProbMetric: 30.6806 - val_loss: 30.6606 - val_MinusLogProbMetric: 30.6606 - lr: 0.0010 - 43s/epoch - 221ms/step
Epoch 80/1000
2023-10-02 03:02:04.173 
Epoch 80/1000 
	 loss: 30.6604, MinusLogProbMetric: 30.6604, val_loss: 31.4743, val_MinusLogProbMetric: 31.4743

Epoch 80: val_loss did not improve from 30.66065
196/196 - 43s - loss: 30.6604 - MinusLogProbMetric: 30.6604 - val_loss: 31.4743 - val_MinusLogProbMetric: 31.4743 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 81/1000
2023-10-02 03:02:48.774 
Epoch 81/1000 
	 loss: 30.6257, MinusLogProbMetric: 30.6257, val_loss: 31.0149, val_MinusLogProbMetric: 31.0149

Epoch 81: val_loss did not improve from 30.66065
196/196 - 45s - loss: 30.6257 - MinusLogProbMetric: 30.6257 - val_loss: 31.0149 - val_MinusLogProbMetric: 31.0149 - lr: 0.0010 - 45s/epoch - 228ms/step
Epoch 82/1000
2023-10-02 03:03:30.698 
Epoch 82/1000 
	 loss: 30.4841, MinusLogProbMetric: 30.4841, val_loss: 30.8103, val_MinusLogProbMetric: 30.8103

Epoch 82: val_loss did not improve from 30.66065
196/196 - 42s - loss: 30.4841 - MinusLogProbMetric: 30.4841 - val_loss: 30.8103 - val_MinusLogProbMetric: 30.8103 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 83/1000
2023-10-02 03:04:15.769 
Epoch 83/1000 
	 loss: 30.4336, MinusLogProbMetric: 30.4336, val_loss: 30.0343, val_MinusLogProbMetric: 30.0343

Epoch 83: val_loss improved from 30.66065 to 30.03432, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_354/weights/best_weights.h5
196/196 - 46s - loss: 30.4336 - MinusLogProbMetric: 30.4336 - val_loss: 30.0343 - val_MinusLogProbMetric: 30.0343 - lr: 0.0010 - 46s/epoch - 235ms/step
Epoch 84/1000
2023-10-02 03:05:01.532 
Epoch 84/1000 
	 loss: 30.5822, MinusLogProbMetric: 30.5822, val_loss: 30.5615, val_MinusLogProbMetric: 30.5615

Epoch 84: val_loss did not improve from 30.03432
196/196 - 45s - loss: 30.5822 - MinusLogProbMetric: 30.5822 - val_loss: 30.5615 - val_MinusLogProbMetric: 30.5615 - lr: 0.0010 - 45s/epoch - 228ms/step
Epoch 85/1000
2023-10-02 03:05:44.781 
Epoch 85/1000 
	 loss: 30.3734, MinusLogProbMetric: 30.3734, val_loss: 33.0659, val_MinusLogProbMetric: 33.0659

Epoch 85: val_loss did not improve from 30.03432
196/196 - 43s - loss: 30.3734 - MinusLogProbMetric: 30.3734 - val_loss: 33.0659 - val_MinusLogProbMetric: 33.0659 - lr: 0.0010 - 43s/epoch - 221ms/step
Epoch 86/1000
2023-10-02 03:06:27.345 
Epoch 86/1000 
	 loss: 30.4395, MinusLogProbMetric: 30.4395, val_loss: 30.3807, val_MinusLogProbMetric: 30.3807

Epoch 86: val_loss did not improve from 30.03432
196/196 - 43s - loss: 30.4395 - MinusLogProbMetric: 30.4395 - val_loss: 30.3807 - val_MinusLogProbMetric: 30.3807 - lr: 0.0010 - 43s/epoch - 217ms/step
Epoch 87/1000
2023-10-02 03:07:10.597 
Epoch 87/1000 
	 loss: 30.2675, MinusLogProbMetric: 30.2675, val_loss: 31.6557, val_MinusLogProbMetric: 31.6557

Epoch 87: val_loss did not improve from 30.03432
196/196 - 43s - loss: 30.2675 - MinusLogProbMetric: 30.2675 - val_loss: 31.6557 - val_MinusLogProbMetric: 31.6557 - lr: 0.0010 - 43s/epoch - 221ms/step
Epoch 88/1000
2023-10-02 03:07:53.432 
Epoch 88/1000 
	 loss: 30.3560, MinusLogProbMetric: 30.3560, val_loss: 30.5690, val_MinusLogProbMetric: 30.5690

Epoch 88: val_loss did not improve from 30.03432
196/196 - 43s - loss: 30.3560 - MinusLogProbMetric: 30.3560 - val_loss: 30.5690 - val_MinusLogProbMetric: 30.5690 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 89/1000
2023-10-02 03:08:37.532 
Epoch 89/1000 
	 loss: 30.5790, MinusLogProbMetric: 30.5790, val_loss: 32.2670, val_MinusLogProbMetric: 32.2670

Epoch 89: val_loss did not improve from 30.03432
196/196 - 44s - loss: 30.5790 - MinusLogProbMetric: 30.5790 - val_loss: 32.2670 - val_MinusLogProbMetric: 32.2670 - lr: 0.0010 - 44s/epoch - 225ms/step
Epoch 90/1000
2023-10-02 03:09:19.685 
Epoch 90/1000 
	 loss: 30.2987, MinusLogProbMetric: 30.2987, val_loss: 30.8567, val_MinusLogProbMetric: 30.8567

Epoch 90: val_loss did not improve from 30.03432
196/196 - 42s - loss: 30.2987 - MinusLogProbMetric: 30.2987 - val_loss: 30.8567 - val_MinusLogProbMetric: 30.8567 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 91/1000
2023-10-02 03:10:02.872 
Epoch 91/1000 
	 loss: 30.2309, MinusLogProbMetric: 30.2309, val_loss: 30.4607, val_MinusLogProbMetric: 30.4607

Epoch 91: val_loss did not improve from 30.03432
196/196 - 43s - loss: 30.2309 - MinusLogProbMetric: 30.2309 - val_loss: 30.4607 - val_MinusLogProbMetric: 30.4607 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 92/1000
2023-10-02 03:10:46.540 
Epoch 92/1000 
	 loss: 30.2692, MinusLogProbMetric: 30.2692, val_loss: 30.2062, val_MinusLogProbMetric: 30.2062

Epoch 92: val_loss did not improve from 30.03432
196/196 - 44s - loss: 30.2692 - MinusLogProbMetric: 30.2692 - val_loss: 30.2062 - val_MinusLogProbMetric: 30.2062 - lr: 0.0010 - 44s/epoch - 223ms/step
Epoch 93/1000
2023-10-02 03:11:29.989 
Epoch 93/1000 
	 loss: 30.2819, MinusLogProbMetric: 30.2819, val_loss: 30.6000, val_MinusLogProbMetric: 30.6000

Epoch 93: val_loss did not improve from 30.03432
196/196 - 43s - loss: 30.2819 - MinusLogProbMetric: 30.2819 - val_loss: 30.6000 - val_MinusLogProbMetric: 30.6000 - lr: 0.0010 - 43s/epoch - 222ms/step
Epoch 94/1000
2023-10-02 03:12:11.806 
Epoch 94/1000 
	 loss: 30.1493, MinusLogProbMetric: 30.1493, val_loss: 30.7795, val_MinusLogProbMetric: 30.7795

Epoch 94: val_loss did not improve from 30.03432
196/196 - 42s - loss: 30.1493 - MinusLogProbMetric: 30.1493 - val_loss: 30.7795 - val_MinusLogProbMetric: 30.7795 - lr: 0.0010 - 42s/epoch - 213ms/step
Epoch 95/1000
2023-10-02 03:12:56.249 
Epoch 95/1000 
	 loss: 30.1940, MinusLogProbMetric: 30.1940, val_loss: 29.9585, val_MinusLogProbMetric: 29.9585

Epoch 95: val_loss improved from 30.03432 to 29.95846, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_354/weights/best_weights.h5
196/196 - 46s - loss: 30.1940 - MinusLogProbMetric: 30.1940 - val_loss: 29.9585 - val_MinusLogProbMetric: 29.9585 - lr: 0.0010 - 46s/epoch - 234ms/step
Epoch 96/1000
2023-10-02 03:13:41.488 
Epoch 96/1000 
	 loss: 30.3356, MinusLogProbMetric: 30.3356, val_loss: 30.8641, val_MinusLogProbMetric: 30.8641

Epoch 96: val_loss did not improve from 29.95846
196/196 - 44s - loss: 30.3356 - MinusLogProbMetric: 30.3356 - val_loss: 30.8641 - val_MinusLogProbMetric: 30.8641 - lr: 0.0010 - 44s/epoch - 224ms/step
Epoch 97/1000
2023-10-02 03:14:23.416 
Epoch 97/1000 
	 loss: 30.1284, MinusLogProbMetric: 30.1284, val_loss: 31.6077, val_MinusLogProbMetric: 31.6077

Epoch 97: val_loss did not improve from 29.95846
196/196 - 42s - loss: 30.1284 - MinusLogProbMetric: 30.1284 - val_loss: 31.6077 - val_MinusLogProbMetric: 31.6077 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 98/1000
2023-10-02 03:15:08.580 
Epoch 98/1000 
	 loss: 30.1690, MinusLogProbMetric: 30.1690, val_loss: 30.4250, val_MinusLogProbMetric: 30.4250

Epoch 98: val_loss did not improve from 29.95846
196/196 - 45s - loss: 30.1690 - MinusLogProbMetric: 30.1690 - val_loss: 30.4250 - val_MinusLogProbMetric: 30.4250 - lr: 0.0010 - 45s/epoch - 230ms/step
Epoch 99/1000
2023-10-02 03:15:50.925 
Epoch 99/1000 
	 loss: 30.2623, MinusLogProbMetric: 30.2623, val_loss: 30.7618, val_MinusLogProbMetric: 30.7618

Epoch 99: val_loss did not improve from 29.95846
196/196 - 42s - loss: 30.2623 - MinusLogProbMetric: 30.2623 - val_loss: 30.7618 - val_MinusLogProbMetric: 30.7618 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 100/1000
2023-10-02 03:16:34.588 
Epoch 100/1000 
	 loss: 30.2553, MinusLogProbMetric: 30.2553, val_loss: 30.4857, val_MinusLogProbMetric: 30.4857

Epoch 100: val_loss did not improve from 29.95846
196/196 - 44s - loss: 30.2553 - MinusLogProbMetric: 30.2553 - val_loss: 30.4857 - val_MinusLogProbMetric: 30.4857 - lr: 0.0010 - 44s/epoch - 223ms/step
Epoch 101/1000
2023-10-02 03:17:18.810 
Epoch 101/1000 
	 loss: 30.0931, MinusLogProbMetric: 30.0931, val_loss: 29.9350, val_MinusLogProbMetric: 29.9350

Epoch 101: val_loss improved from 29.95846 to 29.93504, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_354/weights/best_weights.h5
196/196 - 46s - loss: 30.0931 - MinusLogProbMetric: 30.0931 - val_loss: 29.9350 - val_MinusLogProbMetric: 29.9350 - lr: 0.0010 - 46s/epoch - 232ms/step
Epoch 102/1000
2023-10-02 03:18:04.727 
Epoch 102/1000 
	 loss: 30.0547, MinusLogProbMetric: 30.0547, val_loss: 29.8597, val_MinusLogProbMetric: 29.8597

Epoch 102: val_loss improved from 29.93504 to 29.85971, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_354/weights/best_weights.h5
196/196 - 45s - loss: 30.0547 - MinusLogProbMetric: 30.0547 - val_loss: 29.8597 - val_MinusLogProbMetric: 29.8597 - lr: 0.0010 - 45s/epoch - 230ms/step
Epoch 103/1000
2023-10-02 03:18:49.740 
Epoch 103/1000 
	 loss: 30.0939, MinusLogProbMetric: 30.0939, val_loss: 30.4789, val_MinusLogProbMetric: 30.4789

Epoch 103: val_loss did not improve from 29.85971
196/196 - 44s - loss: 30.0939 - MinusLogProbMetric: 30.0939 - val_loss: 30.4789 - val_MinusLogProbMetric: 30.4789 - lr: 0.0010 - 44s/epoch - 227ms/step
Epoch 104/1000
2023-10-02 03:19:32.363 
Epoch 104/1000 
	 loss: 30.1170, MinusLogProbMetric: 30.1170, val_loss: 30.4417, val_MinusLogProbMetric: 30.4417

Epoch 104: val_loss did not improve from 29.85971
196/196 - 43s - loss: 30.1170 - MinusLogProbMetric: 30.1170 - val_loss: 30.4417 - val_MinusLogProbMetric: 30.4417 - lr: 0.0010 - 43s/epoch - 217ms/step
Epoch 105/1000
2023-10-02 03:20:16.324 
Epoch 105/1000 
	 loss: 30.3359, MinusLogProbMetric: 30.3359, val_loss: 30.4482, val_MinusLogProbMetric: 30.4482

Epoch 105: val_loss did not improve from 29.85971
196/196 - 44s - loss: 30.3359 - MinusLogProbMetric: 30.3359 - val_loss: 30.4482 - val_MinusLogProbMetric: 30.4482 - lr: 0.0010 - 44s/epoch - 224ms/step
Epoch 106/1000
2023-10-02 03:20:58.414 
Epoch 106/1000 
	 loss: 30.1774, MinusLogProbMetric: 30.1774, val_loss: 29.8189, val_MinusLogProbMetric: 29.8189

Epoch 106: val_loss improved from 29.85971 to 29.81887, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_354/weights/best_weights.h5
196/196 - 43s - loss: 30.1774 - MinusLogProbMetric: 30.1774 - val_loss: 29.8189 - val_MinusLogProbMetric: 29.8189 - lr: 0.0010 - 43s/epoch - 221ms/step
Epoch 107/1000
2023-10-02 03:21:41.972 
Epoch 107/1000 
	 loss: 30.1640, MinusLogProbMetric: 30.1640, val_loss: 30.3229, val_MinusLogProbMetric: 30.3229

Epoch 107: val_loss did not improve from 29.81887
196/196 - 42s - loss: 30.1640 - MinusLogProbMetric: 30.1640 - val_loss: 30.3229 - val_MinusLogProbMetric: 30.3229 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 108/1000
2023-10-02 03:22:24.952 
Epoch 108/1000 
	 loss: 30.0001, MinusLogProbMetric: 30.0001, val_loss: 32.1654, val_MinusLogProbMetric: 32.1654

Epoch 108: val_loss did not improve from 29.81887
196/196 - 43s - loss: 30.0001 - MinusLogProbMetric: 30.0001 - val_loss: 32.1654 - val_MinusLogProbMetric: 32.1654 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 109/1000
2023-10-02 03:23:09.667 
Epoch 109/1000 
	 loss: 30.1529, MinusLogProbMetric: 30.1529, val_loss: 31.5886, val_MinusLogProbMetric: 31.5886

Epoch 109: val_loss did not improve from 29.81887
196/196 - 45s - loss: 30.1529 - MinusLogProbMetric: 30.1529 - val_loss: 31.5886 - val_MinusLogProbMetric: 31.5886 - lr: 0.0010 - 45s/epoch - 228ms/step
Epoch 110/1000
2023-10-02 03:23:52.926 
Epoch 110/1000 
	 loss: 29.9208, MinusLogProbMetric: 29.9208, val_loss: 31.6253, val_MinusLogProbMetric: 31.6253

Epoch 110: val_loss did not improve from 29.81887
196/196 - 43s - loss: 29.9208 - MinusLogProbMetric: 29.9208 - val_loss: 31.6253 - val_MinusLogProbMetric: 31.6253 - lr: 0.0010 - 43s/epoch - 221ms/step
Epoch 111/1000
2023-10-02 03:24:35.314 
Epoch 111/1000 
	 loss: 29.9174, MinusLogProbMetric: 29.9174, val_loss: 29.7841, val_MinusLogProbMetric: 29.7841

Epoch 111: val_loss improved from 29.81887 to 29.78408, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_354/weights/best_weights.h5
196/196 - 44s - loss: 29.9174 - MinusLogProbMetric: 29.9174 - val_loss: 29.7841 - val_MinusLogProbMetric: 29.7841 - lr: 0.0010 - 44s/epoch - 224ms/step
Epoch 112/1000
2023-10-02 03:25:18.956 
Epoch 112/1000 
	 loss: 29.8819, MinusLogProbMetric: 29.8819, val_loss: 29.8365, val_MinusLogProbMetric: 29.8365

Epoch 112: val_loss did not improve from 29.78408
196/196 - 42s - loss: 29.8819 - MinusLogProbMetric: 29.8819 - val_loss: 29.8365 - val_MinusLogProbMetric: 29.8365 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 113/1000
2023-10-02 03:26:02.709 
Epoch 113/1000 
	 loss: 29.8508, MinusLogProbMetric: 29.8508, val_loss: 30.2251, val_MinusLogProbMetric: 30.2251

Epoch 113: val_loss did not improve from 29.78408
196/196 - 44s - loss: 29.8508 - MinusLogProbMetric: 29.8508 - val_loss: 30.2251 - val_MinusLogProbMetric: 30.2251 - lr: 0.0010 - 44s/epoch - 223ms/step
Epoch 114/1000
2023-10-02 03:26:45.944 
Epoch 114/1000 
	 loss: 30.0091, MinusLogProbMetric: 30.0091, val_loss: 30.1449, val_MinusLogProbMetric: 30.1449

Epoch 114: val_loss did not improve from 29.78408
196/196 - 43s - loss: 30.0091 - MinusLogProbMetric: 30.0091 - val_loss: 30.1449 - val_MinusLogProbMetric: 30.1449 - lr: 0.0010 - 43s/epoch - 221ms/step
Epoch 115/1000
2023-10-02 03:27:30.011 
Epoch 115/1000 
	 loss: 29.9449, MinusLogProbMetric: 29.9449, val_loss: 31.1108, val_MinusLogProbMetric: 31.1108

Epoch 115: val_loss did not improve from 29.78408
196/196 - 44s - loss: 29.9449 - MinusLogProbMetric: 29.9449 - val_loss: 31.1108 - val_MinusLogProbMetric: 31.1108 - lr: 0.0010 - 44s/epoch - 225ms/step
Epoch 116/1000
2023-10-02 03:28:14.982 
Epoch 116/1000 
	 loss: 29.9822, MinusLogProbMetric: 29.9822, val_loss: 29.7655, val_MinusLogProbMetric: 29.7655

Epoch 116: val_loss improved from 29.78408 to 29.76546, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_354/weights/best_weights.h5
196/196 - 46s - loss: 29.9822 - MinusLogProbMetric: 29.9822 - val_loss: 29.7655 - val_MinusLogProbMetric: 29.7655 - lr: 0.0010 - 46s/epoch - 233ms/step
Epoch 117/1000
2023-10-02 03:29:02.844 
Epoch 117/1000 
	 loss: 29.9453, MinusLogProbMetric: 29.9453, val_loss: 29.8725, val_MinusLogProbMetric: 29.8725

Epoch 117: val_loss did not improve from 29.76546
196/196 - 47s - loss: 29.9453 - MinusLogProbMetric: 29.9453 - val_loss: 29.8725 - val_MinusLogProbMetric: 29.8725 - lr: 0.0010 - 47s/epoch - 240ms/step
Epoch 118/1000
2023-10-02 03:29:49.085 
Epoch 118/1000 
	 loss: 29.7709, MinusLogProbMetric: 29.7709, val_loss: 29.9405, val_MinusLogProbMetric: 29.9405

Epoch 118: val_loss did not improve from 29.76546
196/196 - 46s - loss: 29.7709 - MinusLogProbMetric: 29.7709 - val_loss: 29.9405 - val_MinusLogProbMetric: 29.9405 - lr: 0.0010 - 46s/epoch - 236ms/step
Epoch 119/1000
2023-10-02 03:30:30.819 
Epoch 119/1000 
	 loss: 30.0175, MinusLogProbMetric: 30.0175, val_loss: 29.7738, val_MinusLogProbMetric: 29.7738

Epoch 119: val_loss did not improve from 29.76546
196/196 - 42s - loss: 30.0175 - MinusLogProbMetric: 30.0175 - val_loss: 29.7738 - val_MinusLogProbMetric: 29.7738 - lr: 0.0010 - 42s/epoch - 213ms/step
Epoch 120/1000
2023-10-02 03:31:14.645 
Epoch 120/1000 
	 loss: 29.7697, MinusLogProbMetric: 29.7697, val_loss: 30.1603, val_MinusLogProbMetric: 30.1603

Epoch 120: val_loss did not improve from 29.76546
196/196 - 44s - loss: 29.7697 - MinusLogProbMetric: 29.7697 - val_loss: 30.1603 - val_MinusLogProbMetric: 30.1603 - lr: 0.0010 - 44s/epoch - 224ms/step
Epoch 121/1000
2023-10-02 03:31:59.590 
Epoch 121/1000 
	 loss: 29.8074, MinusLogProbMetric: 29.8074, val_loss: 29.4156, val_MinusLogProbMetric: 29.4156

Epoch 121: val_loss improved from 29.76546 to 29.41564, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_354/weights/best_weights.h5
196/196 - 46s - loss: 29.8074 - MinusLogProbMetric: 29.8074 - val_loss: 29.4156 - val_MinusLogProbMetric: 29.4156 - lr: 0.0010 - 46s/epoch - 235ms/step
Epoch 122/1000
2023-10-02 03:32:45.312 
Epoch 122/1000 
	 loss: 29.8428, MinusLogProbMetric: 29.8428, val_loss: 30.4394, val_MinusLogProbMetric: 30.4394

Epoch 122: val_loss did not improve from 29.41564
196/196 - 45s - loss: 29.8428 - MinusLogProbMetric: 29.8428 - val_loss: 30.4394 - val_MinusLogProbMetric: 30.4394 - lr: 0.0010 - 45s/epoch - 227ms/step
Epoch 123/1000
2023-10-02 03:33:29.550 
Epoch 123/1000 
	 loss: 29.8277, MinusLogProbMetric: 29.8277, val_loss: 29.8848, val_MinusLogProbMetric: 29.8848

Epoch 123: val_loss did not improve from 29.41564
196/196 - 44s - loss: 29.8277 - MinusLogProbMetric: 29.8277 - val_loss: 29.8848 - val_MinusLogProbMetric: 29.8848 - lr: 0.0010 - 44s/epoch - 226ms/step
Epoch 124/1000
2023-10-02 03:34:12.044 
Epoch 124/1000 
	 loss: 29.8068, MinusLogProbMetric: 29.8068, val_loss: 29.9459, val_MinusLogProbMetric: 29.9459

Epoch 124: val_loss did not improve from 29.41564
196/196 - 42s - loss: 29.8068 - MinusLogProbMetric: 29.8068 - val_loss: 29.9459 - val_MinusLogProbMetric: 29.9459 - lr: 0.0010 - 42s/epoch - 217ms/step
Epoch 125/1000
2023-10-02 03:34:55.744 
Epoch 125/1000 
	 loss: 29.8218, MinusLogProbMetric: 29.8218, val_loss: 30.2011, val_MinusLogProbMetric: 30.2011

Epoch 125: val_loss did not improve from 29.41564
196/196 - 44s - loss: 29.8218 - MinusLogProbMetric: 29.8218 - val_loss: 30.2011 - val_MinusLogProbMetric: 30.2011 - lr: 0.0010 - 44s/epoch - 223ms/step
Epoch 126/1000
2023-10-02 03:35:37.793 
Epoch 126/1000 
	 loss: 29.6172, MinusLogProbMetric: 29.6172, val_loss: 29.7223, val_MinusLogProbMetric: 29.7223

Epoch 126: val_loss did not improve from 29.41564
196/196 - 42s - loss: 29.6172 - MinusLogProbMetric: 29.6172 - val_loss: 29.7223 - val_MinusLogProbMetric: 29.7223 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 127/1000
2023-10-02 03:36:23.686 
Epoch 127/1000 
	 loss: 29.8306, MinusLogProbMetric: 29.8306, val_loss: 29.6701, val_MinusLogProbMetric: 29.6701

Epoch 127: val_loss did not improve from 29.41564
196/196 - 46s - loss: 29.8306 - MinusLogProbMetric: 29.8306 - val_loss: 29.6701 - val_MinusLogProbMetric: 29.6701 - lr: 0.0010 - 46s/epoch - 234ms/step
Epoch 128/1000
2023-10-02 03:37:07.520 
Epoch 128/1000 
	 loss: 29.9453, MinusLogProbMetric: 29.9453, val_loss: 30.5666, val_MinusLogProbMetric: 30.5666

Epoch 128: val_loss did not improve from 29.41564
196/196 - 44s - loss: 29.9453 - MinusLogProbMetric: 29.9453 - val_loss: 30.5666 - val_MinusLogProbMetric: 30.5666 - lr: 0.0010 - 44s/epoch - 224ms/step
Epoch 129/1000
2023-10-02 03:37:53.452 
Epoch 129/1000 
	 loss: 29.6671, MinusLogProbMetric: 29.6671, val_loss: 30.3556, val_MinusLogProbMetric: 30.3556

Epoch 129: val_loss did not improve from 29.41564
196/196 - 46s - loss: 29.6671 - MinusLogProbMetric: 29.6671 - val_loss: 30.3556 - val_MinusLogProbMetric: 30.3556 - lr: 0.0010 - 46s/epoch - 234ms/step
Epoch 130/1000
2023-10-02 03:38:35.687 
Epoch 130/1000 
	 loss: 29.7343, MinusLogProbMetric: 29.7343, val_loss: 29.7263, val_MinusLogProbMetric: 29.7263

Epoch 130: val_loss did not improve from 29.41564
196/196 - 42s - loss: 29.7343 - MinusLogProbMetric: 29.7343 - val_loss: 29.7263 - val_MinusLogProbMetric: 29.7263 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 131/1000
2023-10-02 03:39:20.748 
Epoch 131/1000 
	 loss: 29.6451, MinusLogProbMetric: 29.6451, val_loss: 30.5112, val_MinusLogProbMetric: 30.5112

Epoch 131: val_loss did not improve from 29.41564
196/196 - 45s - loss: 29.6451 - MinusLogProbMetric: 29.6451 - val_loss: 30.5112 - val_MinusLogProbMetric: 30.5112 - lr: 0.0010 - 45s/epoch - 230ms/step
Epoch 132/1000
2023-10-02 03:40:05.629 
Epoch 132/1000 
	 loss: 29.8792, MinusLogProbMetric: 29.8792, val_loss: 29.8806, val_MinusLogProbMetric: 29.8806

Epoch 132: val_loss did not improve from 29.41564
196/196 - 45s - loss: 29.8792 - MinusLogProbMetric: 29.8792 - val_loss: 29.8806 - val_MinusLogProbMetric: 29.8806 - lr: 0.0010 - 45s/epoch - 229ms/step
Epoch 133/1000
2023-10-02 03:40:49.232 
Epoch 133/1000 
	 loss: 29.7253, MinusLogProbMetric: 29.7253, val_loss: 31.0592, val_MinusLogProbMetric: 31.0592

Epoch 133: val_loss did not improve from 29.41564
196/196 - 44s - loss: 29.7253 - MinusLogProbMetric: 29.7253 - val_loss: 31.0592 - val_MinusLogProbMetric: 31.0592 - lr: 0.0010 - 44s/epoch - 222ms/step
Epoch 134/1000
2023-10-02 03:41:33.639 
Epoch 134/1000 
	 loss: 29.4732, MinusLogProbMetric: 29.4732, val_loss: 29.5765, val_MinusLogProbMetric: 29.5765

Epoch 134: val_loss did not improve from 29.41564
196/196 - 44s - loss: 29.4732 - MinusLogProbMetric: 29.4732 - val_loss: 29.5765 - val_MinusLogProbMetric: 29.5765 - lr: 0.0010 - 44s/epoch - 227ms/step
Epoch 135/1000
2023-10-02 03:42:18.611 
Epoch 135/1000 
	 loss: 29.7653, MinusLogProbMetric: 29.7653, val_loss: 30.6959, val_MinusLogProbMetric: 30.6959

Epoch 135: val_loss did not improve from 29.41564
196/196 - 45s - loss: 29.7653 - MinusLogProbMetric: 29.7653 - val_loss: 30.6959 - val_MinusLogProbMetric: 30.6959 - lr: 0.0010 - 45s/epoch - 229ms/step
Epoch 136/1000
2023-10-02 03:43:02.176 
Epoch 136/1000 
	 loss: 29.6933, MinusLogProbMetric: 29.6933, val_loss: 30.6074, val_MinusLogProbMetric: 30.6074

Epoch 136: val_loss did not improve from 29.41564
196/196 - 44s - loss: 29.6933 - MinusLogProbMetric: 29.6933 - val_loss: 30.6074 - val_MinusLogProbMetric: 30.6074 - lr: 0.0010 - 44s/epoch - 222ms/step
Epoch 137/1000
2023-10-02 03:43:44.707 
Epoch 137/1000 
	 loss: 29.7763, MinusLogProbMetric: 29.7763, val_loss: 31.3082, val_MinusLogProbMetric: 31.3082

Epoch 137: val_loss did not improve from 29.41564
196/196 - 43s - loss: 29.7763 - MinusLogProbMetric: 29.7763 - val_loss: 31.3082 - val_MinusLogProbMetric: 31.3082 - lr: 0.0010 - 43s/epoch - 217ms/step
Epoch 138/1000
2023-10-02 03:44:27.902 
Epoch 138/1000 
	 loss: 29.6849, MinusLogProbMetric: 29.6849, val_loss: 30.4722, val_MinusLogProbMetric: 30.4722

Epoch 138: val_loss did not improve from 29.41564
196/196 - 43s - loss: 29.6849 - MinusLogProbMetric: 29.6849 - val_loss: 30.4722 - val_MinusLogProbMetric: 30.4722 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 139/1000
2023-10-02 03:45:11.705 
Epoch 139/1000 
	 loss: 29.4403, MinusLogProbMetric: 29.4403, val_loss: 29.9569, val_MinusLogProbMetric: 29.9569

Epoch 139: val_loss did not improve from 29.41564
196/196 - 44s - loss: 29.4403 - MinusLogProbMetric: 29.4403 - val_loss: 29.9569 - val_MinusLogProbMetric: 29.9569 - lr: 0.0010 - 44s/epoch - 223ms/step
Epoch 140/1000
2023-10-02 03:45:55.587 
Epoch 140/1000 
	 loss: 29.6119, MinusLogProbMetric: 29.6119, val_loss: 29.9072, val_MinusLogProbMetric: 29.9072

Epoch 140: val_loss did not improve from 29.41564
196/196 - 44s - loss: 29.6119 - MinusLogProbMetric: 29.6119 - val_loss: 29.9072 - val_MinusLogProbMetric: 29.9072 - lr: 0.0010 - 44s/epoch - 224ms/step
Epoch 141/1000
2023-10-02 03:46:39.784 
Epoch 141/1000 
	 loss: 29.4955, MinusLogProbMetric: 29.4955, val_loss: 29.7527, val_MinusLogProbMetric: 29.7527

Epoch 141: val_loss did not improve from 29.41564
196/196 - 44s - loss: 29.4955 - MinusLogProbMetric: 29.4955 - val_loss: 29.7527 - val_MinusLogProbMetric: 29.7527 - lr: 0.0010 - 44s/epoch - 225ms/step
Epoch 142/1000
2023-10-02 03:47:22.232 
Epoch 142/1000 
	 loss: 29.6748, MinusLogProbMetric: 29.6748, val_loss: 29.9959, val_MinusLogProbMetric: 29.9959

Epoch 142: val_loss did not improve from 29.41564
196/196 - 42s - loss: 29.6748 - MinusLogProbMetric: 29.6748 - val_loss: 29.9959 - val_MinusLogProbMetric: 29.9959 - lr: 0.0010 - 42s/epoch - 217ms/step
Epoch 143/1000
2023-10-02 03:48:05.847 
Epoch 143/1000 
	 loss: 29.5786, MinusLogProbMetric: 29.5786, val_loss: 29.3342, val_MinusLogProbMetric: 29.3342

Epoch 143: val_loss improved from 29.41564 to 29.33422, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_354/weights/best_weights.h5
196/196 - 44s - loss: 29.5786 - MinusLogProbMetric: 29.5786 - val_loss: 29.3342 - val_MinusLogProbMetric: 29.3342 - lr: 0.0010 - 44s/epoch - 225ms/step
Epoch 144/1000
2023-10-02 03:48:50.630 
Epoch 144/1000 
	 loss: 29.6795, MinusLogProbMetric: 29.6795, val_loss: 29.4513, val_MinusLogProbMetric: 29.4513

Epoch 144: val_loss did not improve from 29.33422
196/196 - 44s - loss: 29.6795 - MinusLogProbMetric: 29.6795 - val_loss: 29.4513 - val_MinusLogProbMetric: 29.4513 - lr: 0.0010 - 44s/epoch - 226ms/step
Epoch 145/1000
2023-10-02 03:49:32.704 
Epoch 145/1000 
	 loss: 29.5544, MinusLogProbMetric: 29.5544, val_loss: 30.5740, val_MinusLogProbMetric: 30.5740

Epoch 145: val_loss did not improve from 29.33422
196/196 - 42s - loss: 29.5544 - MinusLogProbMetric: 29.5544 - val_loss: 30.5740 - val_MinusLogProbMetric: 30.5740 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 146/1000
2023-10-02 03:50:15.112 
Epoch 146/1000 
	 loss: 29.5019, MinusLogProbMetric: 29.5019, val_loss: 29.7699, val_MinusLogProbMetric: 29.7699

Epoch 146: val_loss did not improve from 29.33422
196/196 - 42s - loss: 29.5019 - MinusLogProbMetric: 29.5019 - val_loss: 29.7699 - val_MinusLogProbMetric: 29.7699 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 147/1000
2023-10-02 03:50:57.056 
Epoch 147/1000 
	 loss: 29.6602, MinusLogProbMetric: 29.6602, val_loss: 29.7758, val_MinusLogProbMetric: 29.7758

Epoch 147: val_loss did not improve from 29.33422
196/196 - 42s - loss: 29.6602 - MinusLogProbMetric: 29.6602 - val_loss: 29.7758 - val_MinusLogProbMetric: 29.7758 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 148/1000
2023-10-02 03:51:38.952 
Epoch 148/1000 
	 loss: 29.4035, MinusLogProbMetric: 29.4035, val_loss: 29.9203, val_MinusLogProbMetric: 29.9203

Epoch 148: val_loss did not improve from 29.33422
196/196 - 42s - loss: 29.4035 - MinusLogProbMetric: 29.4035 - val_loss: 29.9203 - val_MinusLogProbMetric: 29.9203 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 149/1000
2023-10-02 03:52:24.943 
Epoch 149/1000 
	 loss: 29.5718, MinusLogProbMetric: 29.5718, val_loss: 29.5171, val_MinusLogProbMetric: 29.5171

Epoch 149: val_loss did not improve from 29.33422
196/196 - 46s - loss: 29.5718 - MinusLogProbMetric: 29.5718 - val_loss: 29.5171 - val_MinusLogProbMetric: 29.5171 - lr: 0.0010 - 46s/epoch - 235ms/step
Epoch 150/1000
2023-10-02 03:53:07.680 
Epoch 150/1000 
	 loss: 29.6067, MinusLogProbMetric: 29.6067, val_loss: 29.9410, val_MinusLogProbMetric: 29.9410

Epoch 150: val_loss did not improve from 29.33422
196/196 - 43s - loss: 29.6067 - MinusLogProbMetric: 29.6067 - val_loss: 29.9410 - val_MinusLogProbMetric: 29.9410 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 151/1000
2023-10-02 03:53:52.687 
Epoch 151/1000 
	 loss: 29.5118, MinusLogProbMetric: 29.5118, val_loss: 29.8823, val_MinusLogProbMetric: 29.8823

Epoch 151: val_loss did not improve from 29.33422
196/196 - 45s - loss: 29.5118 - MinusLogProbMetric: 29.5118 - val_loss: 29.8823 - val_MinusLogProbMetric: 29.8823 - lr: 0.0010 - 45s/epoch - 230ms/step
Epoch 152/1000
2023-10-02 03:54:35.771 
Epoch 152/1000 
	 loss: 29.4963, MinusLogProbMetric: 29.4963, val_loss: 29.6239, val_MinusLogProbMetric: 29.6239

Epoch 152: val_loss did not improve from 29.33422
196/196 - 43s - loss: 29.4963 - MinusLogProbMetric: 29.4963 - val_loss: 29.6239 - val_MinusLogProbMetric: 29.6239 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 153/1000
2023-10-02 03:55:21.223 
Epoch 153/1000 
	 loss: 29.4545, MinusLogProbMetric: 29.4545, val_loss: 29.7738, val_MinusLogProbMetric: 29.7738

Epoch 153: val_loss did not improve from 29.33422
196/196 - 45s - loss: 29.4545 - MinusLogProbMetric: 29.4545 - val_loss: 29.7738 - val_MinusLogProbMetric: 29.7738 - lr: 0.0010 - 45s/epoch - 232ms/step
Epoch 154/1000
2023-10-02 03:56:06.716 
Epoch 154/1000 
	 loss: 29.5516, MinusLogProbMetric: 29.5516, val_loss: 29.3906, val_MinusLogProbMetric: 29.3906

Epoch 154: val_loss did not improve from 29.33422
196/196 - 45s - loss: 29.5516 - MinusLogProbMetric: 29.5516 - val_loss: 29.3906 - val_MinusLogProbMetric: 29.3906 - lr: 0.0010 - 45s/epoch - 232ms/step
Epoch 155/1000
2023-10-02 03:56:50.965 
Epoch 155/1000 
	 loss: 29.5904, MinusLogProbMetric: 29.5904, val_loss: 30.6045, val_MinusLogProbMetric: 30.6045

Epoch 155: val_loss did not improve from 29.33422
196/196 - 44s - loss: 29.5904 - MinusLogProbMetric: 29.5904 - val_loss: 30.6045 - val_MinusLogProbMetric: 30.6045 - lr: 0.0010 - 44s/epoch - 226ms/step
Epoch 156/1000
2023-10-02 03:57:33.719 
Epoch 156/1000 
	 loss: 29.4225, MinusLogProbMetric: 29.4225, val_loss: 30.2740, val_MinusLogProbMetric: 30.2740

Epoch 156: val_loss did not improve from 29.33422
196/196 - 43s - loss: 29.4225 - MinusLogProbMetric: 29.4225 - val_loss: 30.2740 - val_MinusLogProbMetric: 30.2740 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 157/1000
2023-10-02 03:58:18.082 
Epoch 157/1000 
	 loss: 29.4683, MinusLogProbMetric: 29.4683, val_loss: 30.1410, val_MinusLogProbMetric: 30.1410

Epoch 157: val_loss did not improve from 29.33422
196/196 - 44s - loss: 29.4683 - MinusLogProbMetric: 29.4683 - val_loss: 30.1410 - val_MinusLogProbMetric: 30.1410 - lr: 0.0010 - 44s/epoch - 226ms/step
Epoch 158/1000
2023-10-02 03:59:03.159 
Epoch 158/1000 
	 loss: 29.4587, MinusLogProbMetric: 29.4587, val_loss: 30.8930, val_MinusLogProbMetric: 30.8930

Epoch 158: val_loss did not improve from 29.33422
196/196 - 45s - loss: 29.4587 - MinusLogProbMetric: 29.4587 - val_loss: 30.8930 - val_MinusLogProbMetric: 30.8930 - lr: 0.0010 - 45s/epoch - 230ms/step
Epoch 159/1000
2023-10-02 03:59:45.766 
Epoch 159/1000 
	 loss: 29.4994, MinusLogProbMetric: 29.4994, val_loss: 30.4807, val_MinusLogProbMetric: 30.4807

Epoch 159: val_loss did not improve from 29.33422
196/196 - 43s - loss: 29.4994 - MinusLogProbMetric: 29.4994 - val_loss: 30.4807 - val_MinusLogProbMetric: 30.4807 - lr: 0.0010 - 43s/epoch - 217ms/step
Epoch 160/1000
2023-10-02 04:00:31.067 
Epoch 160/1000 
	 loss: 29.4338, MinusLogProbMetric: 29.4338, val_loss: 30.5728, val_MinusLogProbMetric: 30.5728

Epoch 160: val_loss did not improve from 29.33422
196/196 - 45s - loss: 29.4338 - MinusLogProbMetric: 29.4338 - val_loss: 30.5728 - val_MinusLogProbMetric: 30.5728 - lr: 0.0010 - 45s/epoch - 231ms/step
Epoch 161/1000
2023-10-02 04:01:14.958 
Epoch 161/1000 
	 loss: 29.3783, MinusLogProbMetric: 29.3783, val_loss: 29.6970, val_MinusLogProbMetric: 29.6970

Epoch 161: val_loss did not improve from 29.33422
196/196 - 44s - loss: 29.3783 - MinusLogProbMetric: 29.3783 - val_loss: 29.6970 - val_MinusLogProbMetric: 29.6970 - lr: 0.0010 - 44s/epoch - 224ms/step
Epoch 162/1000
2023-10-02 04:02:00.751 
Epoch 162/1000 
	 loss: 29.1959, MinusLogProbMetric: 29.1959, val_loss: 29.6477, val_MinusLogProbMetric: 29.6477

Epoch 162: val_loss did not improve from 29.33422
196/196 - 46s - loss: 29.1959 - MinusLogProbMetric: 29.1959 - val_loss: 29.6477 - val_MinusLogProbMetric: 29.6477 - lr: 0.0010 - 46s/epoch - 234ms/step
Epoch 163/1000
2023-10-02 04:02:45.745 
Epoch 163/1000 
	 loss: 29.4670, MinusLogProbMetric: 29.4670, val_loss: 29.9078, val_MinusLogProbMetric: 29.9078

Epoch 163: val_loss did not improve from 29.33422
196/196 - 45s - loss: 29.4670 - MinusLogProbMetric: 29.4670 - val_loss: 29.9078 - val_MinusLogProbMetric: 29.9078 - lr: 0.0010 - 45s/epoch - 229ms/step
Epoch 164/1000
2023-10-02 04:03:30.212 
Epoch 164/1000 
	 loss: 29.5016, MinusLogProbMetric: 29.5016, val_loss: 29.7746, val_MinusLogProbMetric: 29.7746

Epoch 164: val_loss did not improve from 29.33422
196/196 - 44s - loss: 29.5016 - MinusLogProbMetric: 29.5016 - val_loss: 29.7746 - val_MinusLogProbMetric: 29.7746 - lr: 0.0010 - 44s/epoch - 227ms/step
Epoch 165/1000
2023-10-02 04:04:14.944 
Epoch 165/1000 
	 loss: 29.3123, MinusLogProbMetric: 29.3123, val_loss: 30.3206, val_MinusLogProbMetric: 30.3206

Epoch 165: val_loss did not improve from 29.33422
196/196 - 45s - loss: 29.3123 - MinusLogProbMetric: 29.3123 - val_loss: 30.3206 - val_MinusLogProbMetric: 30.3206 - lr: 0.0010 - 45s/epoch - 228ms/step
Epoch 166/1000
2023-10-02 04:04:59.550 
Epoch 166/1000 
	 loss: 29.3966, MinusLogProbMetric: 29.3966, val_loss: 30.0716, val_MinusLogProbMetric: 30.0716

Epoch 166: val_loss did not improve from 29.33422
196/196 - 45s - loss: 29.3966 - MinusLogProbMetric: 29.3966 - val_loss: 30.0716 - val_MinusLogProbMetric: 30.0716 - lr: 0.0010 - 45s/epoch - 227ms/step
Epoch 167/1000
2023-10-02 04:05:45.208 
Epoch 167/1000 
	 loss: 29.3266, MinusLogProbMetric: 29.3266, val_loss: 31.0788, val_MinusLogProbMetric: 31.0788

Epoch 167: val_loss did not improve from 29.33422
196/196 - 46s - loss: 29.3266 - MinusLogProbMetric: 29.3266 - val_loss: 31.0788 - val_MinusLogProbMetric: 31.0788 - lr: 0.0010 - 46s/epoch - 233ms/step
Epoch 168/1000
2023-10-02 04:06:28.867 
Epoch 168/1000 
	 loss: 29.3096, MinusLogProbMetric: 29.3096, val_loss: 29.7523, val_MinusLogProbMetric: 29.7523

Epoch 168: val_loss did not improve from 29.33422
196/196 - 44s - loss: 29.3096 - MinusLogProbMetric: 29.3096 - val_loss: 29.7523 - val_MinusLogProbMetric: 29.7523 - lr: 0.0010 - 44s/epoch - 223ms/step
Epoch 169/1000
2023-10-02 04:07:12.468 
Epoch 169/1000 
	 loss: 29.4113, MinusLogProbMetric: 29.4113, val_loss: 30.3446, val_MinusLogProbMetric: 30.3446

Epoch 169: val_loss did not improve from 29.33422
196/196 - 44s - loss: 29.4113 - MinusLogProbMetric: 29.4113 - val_loss: 30.3446 - val_MinusLogProbMetric: 30.3446 - lr: 0.0010 - 44s/epoch - 222ms/step
Epoch 170/1000
2023-10-02 04:07:56.781 
Epoch 170/1000 
	 loss: 29.3799, MinusLogProbMetric: 29.3799, val_loss: 29.8541, val_MinusLogProbMetric: 29.8541

Epoch 170: val_loss did not improve from 29.33422
196/196 - 44s - loss: 29.3799 - MinusLogProbMetric: 29.3799 - val_loss: 29.8541 - val_MinusLogProbMetric: 29.8541 - lr: 0.0010 - 44s/epoch - 226ms/step
Epoch 171/1000
2023-10-02 04:08:48.722 
Epoch 171/1000 
	 loss: 29.4175, MinusLogProbMetric: 29.4175, val_loss: 29.8727, val_MinusLogProbMetric: 29.8727

Epoch 171: val_loss did not improve from 29.33422
196/196 - 52s - loss: 29.4175 - MinusLogProbMetric: 29.4175 - val_loss: 29.8727 - val_MinusLogProbMetric: 29.8727 - lr: 0.0010 - 52s/epoch - 265ms/step
Epoch 172/1000
2023-10-02 04:09:32.137 
Epoch 172/1000 
	 loss: 29.1845, MinusLogProbMetric: 29.1845, val_loss: 30.6314, val_MinusLogProbMetric: 30.6314

Epoch 172: val_loss did not improve from 29.33422
196/196 - 43s - loss: 29.1845 - MinusLogProbMetric: 29.1845 - val_loss: 30.6314 - val_MinusLogProbMetric: 30.6314 - lr: 0.0010 - 43s/epoch - 222ms/step
Epoch 173/1000
2023-10-02 04:10:16.140 
Epoch 173/1000 
	 loss: 29.3422, MinusLogProbMetric: 29.3422, val_loss: 30.5298, val_MinusLogProbMetric: 30.5298

Epoch 173: val_loss did not improve from 29.33422
196/196 - 44s - loss: 29.3422 - MinusLogProbMetric: 29.3422 - val_loss: 30.5298 - val_MinusLogProbMetric: 30.5298 - lr: 0.0010 - 44s/epoch - 224ms/step
Epoch 174/1000
2023-10-02 04:11:00.887 
Epoch 174/1000 
	 loss: 29.4544, MinusLogProbMetric: 29.4544, val_loss: 29.3433, val_MinusLogProbMetric: 29.3433

Epoch 174: val_loss did not improve from 29.33422
196/196 - 45s - loss: 29.4544 - MinusLogProbMetric: 29.4544 - val_loss: 29.3433 - val_MinusLogProbMetric: 29.3433 - lr: 0.0010 - 45s/epoch - 228ms/step
Epoch 175/1000
2023-10-02 04:11:45.540 
Epoch 175/1000 
	 loss: 29.2693, MinusLogProbMetric: 29.2693, val_loss: 29.8674, val_MinusLogProbMetric: 29.8674

Epoch 175: val_loss did not improve from 29.33422
196/196 - 45s - loss: 29.2693 - MinusLogProbMetric: 29.2693 - val_loss: 29.8674 - val_MinusLogProbMetric: 29.8674 - lr: 0.0010 - 45s/epoch - 228ms/step
Epoch 176/1000
2023-10-02 04:12:29.336 
Epoch 176/1000 
	 loss: 29.2447, MinusLogProbMetric: 29.2447, val_loss: 29.5294, val_MinusLogProbMetric: 29.5294

Epoch 176: val_loss did not improve from 29.33422
196/196 - 44s - loss: 29.2447 - MinusLogProbMetric: 29.2447 - val_loss: 29.5294 - val_MinusLogProbMetric: 29.5294 - lr: 0.0010 - 44s/epoch - 223ms/step
Epoch 177/1000
2023-10-02 04:13:15.399 
Epoch 177/1000 
	 loss: 29.3333, MinusLogProbMetric: 29.3333, val_loss: 29.8687, val_MinusLogProbMetric: 29.8687

Epoch 177: val_loss did not improve from 29.33422
196/196 - 46s - loss: 29.3333 - MinusLogProbMetric: 29.3333 - val_loss: 29.8687 - val_MinusLogProbMetric: 29.8687 - lr: 0.0010 - 46s/epoch - 235ms/step
Epoch 178/1000
2023-10-02 04:14:00.801 
Epoch 178/1000 
	 loss: 29.3372, MinusLogProbMetric: 29.3372, val_loss: 30.6742, val_MinusLogProbMetric: 30.6742

Epoch 178: val_loss did not improve from 29.33422
196/196 - 45s - loss: 29.3372 - MinusLogProbMetric: 29.3372 - val_loss: 30.6742 - val_MinusLogProbMetric: 30.6742 - lr: 0.0010 - 45s/epoch - 232ms/step
Epoch 179/1000
2023-10-02 04:14:46.641 
Epoch 179/1000 
	 loss: 29.3585, MinusLogProbMetric: 29.3585, val_loss: 30.5039, val_MinusLogProbMetric: 30.5039

Epoch 179: val_loss did not improve from 29.33422
196/196 - 46s - loss: 29.3585 - MinusLogProbMetric: 29.3585 - val_loss: 30.5039 - val_MinusLogProbMetric: 30.5039 - lr: 0.0010 - 46s/epoch - 234ms/step
Epoch 180/1000
2023-10-02 04:15:31.035 
Epoch 180/1000 
	 loss: 29.3367, MinusLogProbMetric: 29.3367, val_loss: 29.5535, val_MinusLogProbMetric: 29.5535

Epoch 180: val_loss did not improve from 29.33422
196/196 - 44s - loss: 29.3367 - MinusLogProbMetric: 29.3367 - val_loss: 29.5535 - val_MinusLogProbMetric: 29.5535 - lr: 0.0010 - 44s/epoch - 226ms/step
Epoch 181/1000
2023-10-02 04:16:16.952 
Epoch 181/1000 
	 loss: 29.2255, MinusLogProbMetric: 29.2255, val_loss: 29.3173, val_MinusLogProbMetric: 29.3173

Epoch 181: val_loss improved from 29.33422 to 29.31730, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_354/weights/best_weights.h5
196/196 - 47s - loss: 29.2255 - MinusLogProbMetric: 29.2255 - val_loss: 29.3173 - val_MinusLogProbMetric: 29.3173 - lr: 0.0010 - 47s/epoch - 239ms/step
Epoch 182/1000
2023-10-02 04:17:02.712 
Epoch 182/1000 
	 loss: 29.2266, MinusLogProbMetric: 29.2266, val_loss: 29.7478, val_MinusLogProbMetric: 29.7478

Epoch 182: val_loss did not improve from 29.31730
196/196 - 45s - loss: 29.2266 - MinusLogProbMetric: 29.2266 - val_loss: 29.7478 - val_MinusLogProbMetric: 29.7478 - lr: 0.0010 - 45s/epoch - 228ms/step
Epoch 183/1000
2023-10-02 04:17:46.296 
Epoch 183/1000 
	 loss: 29.2527, MinusLogProbMetric: 29.2527, val_loss: 30.3136, val_MinusLogProbMetric: 30.3136

Epoch 183: val_loss did not improve from 29.31730
196/196 - 44s - loss: 29.2527 - MinusLogProbMetric: 29.2527 - val_loss: 30.3136 - val_MinusLogProbMetric: 30.3136 - lr: 0.0010 - 44s/epoch - 222ms/step
Epoch 184/1000
2023-10-02 04:18:30.163 
Epoch 184/1000 
	 loss: 29.3101, MinusLogProbMetric: 29.3101, val_loss: 29.8894, val_MinusLogProbMetric: 29.8894

Epoch 184: val_loss did not improve from 29.31730
196/196 - 44s - loss: 29.3101 - MinusLogProbMetric: 29.3101 - val_loss: 29.8894 - val_MinusLogProbMetric: 29.8894 - lr: 0.0010 - 44s/epoch - 224ms/step
Epoch 185/1000
2023-10-02 04:19:12.711 
Epoch 185/1000 
	 loss: 29.3277, MinusLogProbMetric: 29.3277, val_loss: 29.2921, val_MinusLogProbMetric: 29.2921

Epoch 185: val_loss improved from 29.31730 to 29.29211, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_354/weights/best_weights.h5
196/196 - 44s - loss: 29.3277 - MinusLogProbMetric: 29.3277 - val_loss: 29.2921 - val_MinusLogProbMetric: 29.2921 - lr: 0.0010 - 44s/epoch - 225ms/step
Epoch 186/1000
2023-10-02 04:19:58.798 
Epoch 186/1000 
	 loss: 29.1399, MinusLogProbMetric: 29.1399, val_loss: 29.3960, val_MinusLogProbMetric: 29.3960

Epoch 186: val_loss did not improve from 29.29211
196/196 - 45s - loss: 29.1399 - MinusLogProbMetric: 29.1399 - val_loss: 29.3960 - val_MinusLogProbMetric: 29.3960 - lr: 0.0010 - 45s/epoch - 227ms/step
Epoch 187/1000
2023-10-02 04:20:43.964 
Epoch 187/1000 
	 loss: 29.2302, MinusLogProbMetric: 29.2302, val_loss: 30.2045, val_MinusLogProbMetric: 30.2045

Epoch 187: val_loss did not improve from 29.29211
196/196 - 45s - loss: 29.2302 - MinusLogProbMetric: 29.2302 - val_loss: 30.2045 - val_MinusLogProbMetric: 30.2045 - lr: 0.0010 - 45s/epoch - 230ms/step
Epoch 188/1000
2023-10-02 04:21:26.644 
Epoch 188/1000 
	 loss: 29.2691, MinusLogProbMetric: 29.2691, val_loss: 29.1260, val_MinusLogProbMetric: 29.1260

Epoch 188: val_loss improved from 29.29211 to 29.12603, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_354/weights/best_weights.h5
196/196 - 44s - loss: 29.2691 - MinusLogProbMetric: 29.2691 - val_loss: 29.1260 - val_MinusLogProbMetric: 29.1260 - lr: 0.0010 - 44s/epoch - 222ms/step
Epoch 189/1000
2023-10-02 04:22:12.062 
Epoch 189/1000 
	 loss: 29.2997, MinusLogProbMetric: 29.2997, val_loss: 29.8171, val_MinusLogProbMetric: 29.8171

Epoch 189: val_loss did not improve from 29.12603
196/196 - 45s - loss: 29.2997 - MinusLogProbMetric: 29.2997 - val_loss: 29.8171 - val_MinusLogProbMetric: 29.8171 - lr: 0.0010 - 45s/epoch - 227ms/step
Epoch 190/1000
2023-10-02 04:22:54.837 
Epoch 190/1000 
	 loss: 29.1602, MinusLogProbMetric: 29.1602, val_loss: 29.9385, val_MinusLogProbMetric: 29.9385

Epoch 190: val_loss did not improve from 29.12603
196/196 - 43s - loss: 29.1602 - MinusLogProbMetric: 29.1602 - val_loss: 29.9385 - val_MinusLogProbMetric: 29.9385 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 191/1000
2023-10-02 04:23:38.246 
Epoch 191/1000 
	 loss: 29.3028, MinusLogProbMetric: 29.3028, val_loss: 29.7306, val_MinusLogProbMetric: 29.7306

Epoch 191: val_loss did not improve from 29.12603
196/196 - 43s - loss: 29.3028 - MinusLogProbMetric: 29.3028 - val_loss: 29.7306 - val_MinusLogProbMetric: 29.7306 - lr: 0.0010 - 43s/epoch - 221ms/step
Epoch 192/1000
2023-10-02 04:24:21.634 
Epoch 192/1000 
	 loss: 29.1759, MinusLogProbMetric: 29.1759, val_loss: 29.8658, val_MinusLogProbMetric: 29.8658

Epoch 192: val_loss did not improve from 29.12603
196/196 - 43s - loss: 29.1759 - MinusLogProbMetric: 29.1759 - val_loss: 29.8658 - val_MinusLogProbMetric: 29.8658 - lr: 0.0010 - 43s/epoch - 221ms/step
Epoch 193/1000
2023-10-02 04:25:06.425 
Epoch 193/1000 
	 loss: 29.2555, MinusLogProbMetric: 29.2555, val_loss: 29.5246, val_MinusLogProbMetric: 29.5246

Epoch 193: val_loss did not improve from 29.12603
196/196 - 45s - loss: 29.2555 - MinusLogProbMetric: 29.2555 - val_loss: 29.5246 - val_MinusLogProbMetric: 29.5246 - lr: 0.0010 - 45s/epoch - 229ms/step
Epoch 194/1000
2023-10-02 04:25:51.356 
Epoch 194/1000 
	 loss: 29.1749, MinusLogProbMetric: 29.1749, val_loss: 29.6616, val_MinusLogProbMetric: 29.6616

Epoch 194: val_loss did not improve from 29.12603
196/196 - 45s - loss: 29.1749 - MinusLogProbMetric: 29.1749 - val_loss: 29.6616 - val_MinusLogProbMetric: 29.6616 - lr: 0.0010 - 45s/epoch - 229ms/step
Epoch 195/1000
2023-10-02 04:26:33.677 
Epoch 195/1000 
	 loss: 29.2997, MinusLogProbMetric: 29.2997, val_loss: 31.4994, val_MinusLogProbMetric: 31.4994

Epoch 195: val_loss did not improve from 29.12603
196/196 - 42s - loss: 29.2997 - MinusLogProbMetric: 29.2997 - val_loss: 31.4994 - val_MinusLogProbMetric: 31.4994 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 196/1000
2023-10-02 04:27:17.152 
Epoch 196/1000 
	 loss: 29.2709, MinusLogProbMetric: 29.2709, val_loss: 29.8614, val_MinusLogProbMetric: 29.8614

Epoch 196: val_loss did not improve from 29.12603
196/196 - 43s - loss: 29.2709 - MinusLogProbMetric: 29.2709 - val_loss: 29.8614 - val_MinusLogProbMetric: 29.8614 - lr: 0.0010 - 43s/epoch - 222ms/step
Epoch 197/1000
2023-10-02 04:28:01.696 
Epoch 197/1000 
	 loss: 29.1651, MinusLogProbMetric: 29.1651, val_loss: 29.8506, val_MinusLogProbMetric: 29.8506

Epoch 197: val_loss did not improve from 29.12603
196/196 - 45s - loss: 29.1651 - MinusLogProbMetric: 29.1651 - val_loss: 29.8506 - val_MinusLogProbMetric: 29.8506 - lr: 0.0010 - 45s/epoch - 227ms/step
Epoch 198/1000
2023-10-02 04:28:44.880 
Epoch 198/1000 
	 loss: 29.0970, MinusLogProbMetric: 29.0970, val_loss: 30.1721, val_MinusLogProbMetric: 30.1721

Epoch 198: val_loss did not improve from 29.12603
196/196 - 43s - loss: 29.0970 - MinusLogProbMetric: 29.0970 - val_loss: 30.1721 - val_MinusLogProbMetric: 30.1721 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 199/1000
2023-10-02 04:29:28.557 
Epoch 199/1000 
	 loss: 29.1133, MinusLogProbMetric: 29.1133, val_loss: 29.9958, val_MinusLogProbMetric: 29.9958

Epoch 199: val_loss did not improve from 29.12603
196/196 - 44s - loss: 29.1133 - MinusLogProbMetric: 29.1133 - val_loss: 29.9958 - val_MinusLogProbMetric: 29.9958 - lr: 0.0010 - 44s/epoch - 223ms/step
Epoch 200/1000
2023-10-02 04:30:10.691 
Epoch 200/1000 
	 loss: 29.2820, MinusLogProbMetric: 29.2820, val_loss: 29.5803, val_MinusLogProbMetric: 29.5803

Epoch 200: val_loss did not improve from 29.12603
196/196 - 42s - loss: 29.2820 - MinusLogProbMetric: 29.2820 - val_loss: 29.5803 - val_MinusLogProbMetric: 29.5803 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 201/1000
2023-10-02 04:30:55.705 
Epoch 201/1000 
	 loss: 29.1528, MinusLogProbMetric: 29.1528, val_loss: 29.2943, val_MinusLogProbMetric: 29.2943

Epoch 201: val_loss did not improve from 29.12603
196/196 - 45s - loss: 29.1528 - MinusLogProbMetric: 29.1528 - val_loss: 29.2943 - val_MinusLogProbMetric: 29.2943 - lr: 0.0010 - 45s/epoch - 229ms/step
Epoch 202/1000
2023-10-02 04:31:42.399 
Epoch 202/1000 
	 loss: 29.0160, MinusLogProbMetric: 29.0160, val_loss: 29.3550, val_MinusLogProbMetric: 29.3550

Epoch 202: val_loss did not improve from 29.12603
196/196 - 47s - loss: 29.0160 - MinusLogProbMetric: 29.0160 - val_loss: 29.3550 - val_MinusLogProbMetric: 29.3550 - lr: 0.0010 - 47s/epoch - 238ms/step
Epoch 203/1000
2023-10-02 04:32:31.305 
Epoch 203/1000 
	 loss: 29.0605, MinusLogProbMetric: 29.0605, val_loss: 29.6410, val_MinusLogProbMetric: 29.6410

Epoch 203: val_loss did not improve from 29.12603
196/196 - 49s - loss: 29.0605 - MinusLogProbMetric: 29.0605 - val_loss: 29.6410 - val_MinusLogProbMetric: 29.6410 - lr: 0.0010 - 49s/epoch - 250ms/step
Epoch 204/1000
2023-10-02 04:33:12.845 
Epoch 204/1000 
	 loss: 29.1206, MinusLogProbMetric: 29.1206, val_loss: 29.5538, val_MinusLogProbMetric: 29.5538

Epoch 204: val_loss did not improve from 29.12603
196/196 - 42s - loss: 29.1206 - MinusLogProbMetric: 29.1206 - val_loss: 29.5538 - val_MinusLogProbMetric: 29.5538 - lr: 0.0010 - 42s/epoch - 212ms/step
Epoch 205/1000
2023-10-02 04:33:56.935 
Epoch 205/1000 
	 loss: 29.1290, MinusLogProbMetric: 29.1290, val_loss: 30.0003, val_MinusLogProbMetric: 30.0003

Epoch 205: val_loss did not improve from 29.12603
196/196 - 44s - loss: 29.1290 - MinusLogProbMetric: 29.1290 - val_loss: 30.0003 - val_MinusLogProbMetric: 30.0003 - lr: 0.0010 - 44s/epoch - 225ms/step
Epoch 206/1000
2023-10-02 04:34:41.629 
Epoch 206/1000 
	 loss: 29.0483, MinusLogProbMetric: 29.0483, val_loss: 29.7412, val_MinusLogProbMetric: 29.7412

Epoch 206: val_loss did not improve from 29.12603
196/196 - 45s - loss: 29.0483 - MinusLogProbMetric: 29.0483 - val_loss: 29.7412 - val_MinusLogProbMetric: 29.7412 - lr: 0.0010 - 45s/epoch - 228ms/step
Epoch 207/1000
2023-10-02 04:35:25.039 
Epoch 207/1000 
	 loss: 29.0917, MinusLogProbMetric: 29.0917, val_loss: 30.1782, val_MinusLogProbMetric: 30.1782

Epoch 207: val_loss did not improve from 29.12603
196/196 - 43s - loss: 29.0917 - MinusLogProbMetric: 29.0917 - val_loss: 30.1782 - val_MinusLogProbMetric: 30.1782 - lr: 0.0010 - 43s/epoch - 221ms/step
Epoch 208/1000
2023-10-02 04:36:08.981 
Epoch 208/1000 
	 loss: 28.9803, MinusLogProbMetric: 28.9803, val_loss: 29.3846, val_MinusLogProbMetric: 29.3846

Epoch 208: val_loss did not improve from 29.12603
196/196 - 44s - loss: 28.9803 - MinusLogProbMetric: 28.9803 - val_loss: 29.3846 - val_MinusLogProbMetric: 29.3846 - lr: 0.0010 - 44s/epoch - 224ms/step
Epoch 209/1000
2023-10-02 04:36:56.761 
Epoch 209/1000 
	 loss: 29.2565, MinusLogProbMetric: 29.2565, val_loss: 29.6840, val_MinusLogProbMetric: 29.6840

Epoch 209: val_loss did not improve from 29.12603
196/196 - 48s - loss: 29.2565 - MinusLogProbMetric: 29.2565 - val_loss: 29.6840 - val_MinusLogProbMetric: 29.6840 - lr: 0.0010 - 48s/epoch - 244ms/step
Epoch 210/1000
2023-10-02 04:37:37.611 
Epoch 210/1000 
	 loss: 29.1025, MinusLogProbMetric: 29.1025, val_loss: 29.2761, val_MinusLogProbMetric: 29.2761

Epoch 210: val_loss did not improve from 29.12603
196/196 - 41s - loss: 29.1025 - MinusLogProbMetric: 29.1025 - val_loss: 29.2761 - val_MinusLogProbMetric: 29.2761 - lr: 0.0010 - 41s/epoch - 208ms/step
Epoch 211/1000
2023-10-02 04:38:23.051 
Epoch 211/1000 
	 loss: 28.9308, MinusLogProbMetric: 28.9308, val_loss: 30.6847, val_MinusLogProbMetric: 30.6847

Epoch 211: val_loss did not improve from 29.12603
196/196 - 45s - loss: 28.9308 - MinusLogProbMetric: 28.9308 - val_loss: 30.6847 - val_MinusLogProbMetric: 30.6847 - lr: 0.0010 - 45s/epoch - 232ms/step
Epoch 212/1000
2023-10-02 04:39:06.542 
Epoch 212/1000 
	 loss: 29.0715, MinusLogProbMetric: 29.0715, val_loss: 29.9766, val_MinusLogProbMetric: 29.9766

Epoch 212: val_loss did not improve from 29.12603
196/196 - 43s - loss: 29.0715 - MinusLogProbMetric: 29.0715 - val_loss: 29.9766 - val_MinusLogProbMetric: 29.9766 - lr: 0.0010 - 43s/epoch - 222ms/step
Epoch 213/1000
2023-10-02 04:39:50.845 
Epoch 213/1000 
	 loss: 28.9607, MinusLogProbMetric: 28.9607, val_loss: 30.5891, val_MinusLogProbMetric: 30.5891

Epoch 213: val_loss did not improve from 29.12603
196/196 - 44s - loss: 28.9607 - MinusLogProbMetric: 28.9607 - val_loss: 30.5891 - val_MinusLogProbMetric: 30.5891 - lr: 0.0010 - 44s/epoch - 226ms/step
Epoch 214/1000
2023-10-02 04:40:35.491 
Epoch 214/1000 
	 loss: 29.1185, MinusLogProbMetric: 29.1185, val_loss: 29.7830, val_MinusLogProbMetric: 29.7830

Epoch 214: val_loss did not improve from 29.12603
196/196 - 45s - loss: 29.1185 - MinusLogProbMetric: 29.1185 - val_loss: 29.7830 - val_MinusLogProbMetric: 29.7830 - lr: 0.0010 - 45s/epoch - 228ms/step
Epoch 215/1000
2023-10-02 04:41:20.593 
Epoch 215/1000 
	 loss: 28.9279, MinusLogProbMetric: 28.9279, val_loss: 30.2170, val_MinusLogProbMetric: 30.2170

Epoch 215: val_loss did not improve from 29.12603
196/196 - 45s - loss: 28.9279 - MinusLogProbMetric: 28.9279 - val_loss: 30.2170 - val_MinusLogProbMetric: 30.2170 - lr: 0.0010 - 45s/epoch - 230ms/step
Epoch 216/1000
2023-10-02 04:42:03.738 
Epoch 216/1000 
	 loss: 29.1149, MinusLogProbMetric: 29.1149, val_loss: 29.5581, val_MinusLogProbMetric: 29.5581

Epoch 216: val_loss did not improve from 29.12603
196/196 - 43s - loss: 29.1149 - MinusLogProbMetric: 29.1149 - val_loss: 29.5581 - val_MinusLogProbMetric: 29.5581 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 217/1000
2023-10-02 04:42:47.873 
Epoch 217/1000 
	 loss: 29.1641, MinusLogProbMetric: 29.1641, val_loss: 30.0487, val_MinusLogProbMetric: 30.0487

Epoch 217: val_loss did not improve from 29.12603
196/196 - 44s - loss: 29.1641 - MinusLogProbMetric: 29.1641 - val_loss: 30.0487 - val_MinusLogProbMetric: 30.0487 - lr: 0.0010 - 44s/epoch - 225ms/step
Epoch 218/1000
2023-10-02 04:43:30.406 
Epoch 218/1000 
	 loss: 28.9000, MinusLogProbMetric: 28.9000, val_loss: 29.6642, val_MinusLogProbMetric: 29.6642

Epoch 218: val_loss did not improve from 29.12603
196/196 - 43s - loss: 28.9000 - MinusLogProbMetric: 28.9000 - val_loss: 29.6642 - val_MinusLogProbMetric: 29.6642 - lr: 0.0010 - 43s/epoch - 217ms/step
Epoch 219/1000
2023-10-02 04:44:16.584 
Epoch 219/1000 
	 loss: 29.1213, MinusLogProbMetric: 29.1213, val_loss: 29.5578, val_MinusLogProbMetric: 29.5578

Epoch 219: val_loss did not improve from 29.12603
196/196 - 46s - loss: 29.1213 - MinusLogProbMetric: 29.1213 - val_loss: 29.5578 - val_MinusLogProbMetric: 29.5578 - lr: 0.0010 - 46s/epoch - 236ms/step
Epoch 220/1000
2023-10-02 04:45:01.644 
Epoch 220/1000 
	 loss: 28.9493, MinusLogProbMetric: 28.9493, val_loss: 29.2036, val_MinusLogProbMetric: 29.2036

Epoch 220: val_loss did not improve from 29.12603
196/196 - 45s - loss: 28.9493 - MinusLogProbMetric: 28.9493 - val_loss: 29.2036 - val_MinusLogProbMetric: 29.2036 - lr: 0.0010 - 45s/epoch - 230ms/step
Epoch 221/1000
2023-10-02 04:45:46.887 
Epoch 221/1000 
	 loss: 28.9155, MinusLogProbMetric: 28.9155, val_loss: 29.3768, val_MinusLogProbMetric: 29.3768

Epoch 221: val_loss did not improve from 29.12603
196/196 - 45s - loss: 28.9155 - MinusLogProbMetric: 28.9155 - val_loss: 29.3768 - val_MinusLogProbMetric: 29.3768 - lr: 0.0010 - 45s/epoch - 231ms/step
Epoch 222/1000
2023-10-02 04:46:32.029 
Epoch 222/1000 
	 loss: 29.1233, MinusLogProbMetric: 29.1233, val_loss: 31.0277, val_MinusLogProbMetric: 31.0277

Epoch 222: val_loss did not improve from 29.12603
196/196 - 45s - loss: 29.1233 - MinusLogProbMetric: 29.1233 - val_loss: 31.0277 - val_MinusLogProbMetric: 31.0277 - lr: 0.0010 - 45s/epoch - 230ms/step
Epoch 223/1000
2023-10-02 04:47:14.985 
Epoch 223/1000 
	 loss: 28.8759, MinusLogProbMetric: 28.8759, val_loss: 29.7241, val_MinusLogProbMetric: 29.7241

Epoch 223: val_loss did not improve from 29.12603
196/196 - 43s - loss: 28.8759 - MinusLogProbMetric: 28.8759 - val_loss: 29.7241 - val_MinusLogProbMetric: 29.7241 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 224/1000
2023-10-02 04:47:59.150 
Epoch 224/1000 
	 loss: 29.0831, MinusLogProbMetric: 29.0831, val_loss: 29.6128, val_MinusLogProbMetric: 29.6128

Epoch 224: val_loss did not improve from 29.12603
196/196 - 44s - loss: 29.0831 - MinusLogProbMetric: 29.0831 - val_loss: 29.6128 - val_MinusLogProbMetric: 29.6128 - lr: 0.0010 - 44s/epoch - 225ms/step
Epoch 225/1000
2023-10-02 04:48:43.675 
Epoch 225/1000 
	 loss: 29.0340, MinusLogProbMetric: 29.0340, val_loss: 29.1681, val_MinusLogProbMetric: 29.1681

Epoch 225: val_loss did not improve from 29.12603
196/196 - 45s - loss: 29.0340 - MinusLogProbMetric: 29.0340 - val_loss: 29.1681 - val_MinusLogProbMetric: 29.1681 - lr: 0.0010 - 45s/epoch - 227ms/step
Epoch 226/1000
2023-10-02 04:49:27.427 
Epoch 226/1000 
	 loss: 29.0414, MinusLogProbMetric: 29.0414, val_loss: 29.2724, val_MinusLogProbMetric: 29.2724

Epoch 226: val_loss did not improve from 29.12603
196/196 - 44s - loss: 29.0414 - MinusLogProbMetric: 29.0414 - val_loss: 29.2724 - val_MinusLogProbMetric: 29.2724 - lr: 0.0010 - 44s/epoch - 223ms/step
Epoch 227/1000
2023-10-02 04:50:11.086 
Epoch 227/1000 
	 loss: 29.1148, MinusLogProbMetric: 29.1148, val_loss: 29.9045, val_MinusLogProbMetric: 29.9045

Epoch 227: val_loss did not improve from 29.12603
196/196 - 44s - loss: 29.1148 - MinusLogProbMetric: 29.1148 - val_loss: 29.9045 - val_MinusLogProbMetric: 29.9045 - lr: 0.0010 - 44s/epoch - 223ms/step
Epoch 228/1000
2023-10-02 04:50:55.030 
Epoch 228/1000 
	 loss: 28.9551, MinusLogProbMetric: 28.9551, val_loss: 30.6185, val_MinusLogProbMetric: 30.6185

Epoch 228: val_loss did not improve from 29.12603
196/196 - 44s - loss: 28.9551 - MinusLogProbMetric: 28.9551 - val_loss: 30.6185 - val_MinusLogProbMetric: 30.6185 - lr: 0.0010 - 44s/epoch - 224ms/step
Epoch 229/1000
2023-10-02 04:51:41.053 
Epoch 229/1000 
	 loss: 29.0007, MinusLogProbMetric: 29.0007, val_loss: 29.3320, val_MinusLogProbMetric: 29.3320

Epoch 229: val_loss did not improve from 29.12603
196/196 - 46s - loss: 29.0007 - MinusLogProbMetric: 29.0007 - val_loss: 29.3320 - val_MinusLogProbMetric: 29.3320 - lr: 0.0010 - 46s/epoch - 235ms/step
Epoch 230/1000
2023-10-02 04:52:24.388 
Epoch 230/1000 
	 loss: 28.9318, MinusLogProbMetric: 28.9318, val_loss: 29.4538, val_MinusLogProbMetric: 29.4538

Epoch 230: val_loss did not improve from 29.12603
196/196 - 43s - loss: 28.9318 - MinusLogProbMetric: 28.9318 - val_loss: 29.4538 - val_MinusLogProbMetric: 29.4538 - lr: 0.0010 - 43s/epoch - 222ms/step
Epoch 231/1000
2023-10-02 04:53:08.767 
Epoch 231/1000 
	 loss: 28.9730, MinusLogProbMetric: 28.9730, val_loss: 29.7527, val_MinusLogProbMetric: 29.7527

Epoch 231: val_loss did not improve from 29.12603
196/196 - 44s - loss: 28.9730 - MinusLogProbMetric: 28.9730 - val_loss: 29.7527 - val_MinusLogProbMetric: 29.7527 - lr: 0.0010 - 44s/epoch - 226ms/step
Epoch 232/1000
2023-10-02 04:53:52.558 
Epoch 232/1000 
	 loss: 28.9447, MinusLogProbMetric: 28.9447, val_loss: 28.9808, val_MinusLogProbMetric: 28.9808

Epoch 232: val_loss improved from 29.12603 to 28.98079, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_354/weights/best_weights.h5
196/196 - 45s - loss: 28.9447 - MinusLogProbMetric: 28.9447 - val_loss: 28.9808 - val_MinusLogProbMetric: 28.9808 - lr: 0.0010 - 45s/epoch - 228ms/step
Epoch 233/1000
2023-10-02 04:54:38.778 
Epoch 233/1000 
	 loss: 29.0056, MinusLogProbMetric: 29.0056, val_loss: 29.1001, val_MinusLogProbMetric: 29.1001

Epoch 233: val_loss did not improve from 28.98079
196/196 - 45s - loss: 29.0056 - MinusLogProbMetric: 29.0056 - val_loss: 29.1001 - val_MinusLogProbMetric: 29.1001 - lr: 0.0010 - 45s/epoch - 231ms/step
Epoch 234/1000
2023-10-02 04:55:23.885 
Epoch 234/1000 
	 loss: 28.9772, MinusLogProbMetric: 28.9772, val_loss: 30.2894, val_MinusLogProbMetric: 30.2894

Epoch 234: val_loss did not improve from 28.98079
196/196 - 45s - loss: 28.9772 - MinusLogProbMetric: 28.9772 - val_loss: 30.2894 - val_MinusLogProbMetric: 30.2894 - lr: 0.0010 - 45s/epoch - 230ms/step
Epoch 235/1000
2023-10-02 04:56:06.391 
Epoch 235/1000 
	 loss: 28.8516, MinusLogProbMetric: 28.8516, val_loss: 29.8166, val_MinusLogProbMetric: 29.8166

Epoch 235: val_loss did not improve from 28.98079
196/196 - 43s - loss: 28.8516 - MinusLogProbMetric: 28.8516 - val_loss: 29.8166 - val_MinusLogProbMetric: 29.8166 - lr: 0.0010 - 43s/epoch - 217ms/step
Epoch 236/1000
2023-10-02 04:56:51.313 
Epoch 236/1000 
	 loss: 28.9402, MinusLogProbMetric: 28.9402, val_loss: 32.2286, val_MinusLogProbMetric: 32.2286

Epoch 236: val_loss did not improve from 28.98079
196/196 - 45s - loss: 28.9402 - MinusLogProbMetric: 28.9402 - val_loss: 32.2286 - val_MinusLogProbMetric: 32.2286 - lr: 0.0010 - 45s/epoch - 229ms/step
Epoch 237/1000
2023-10-02 04:57:34.336 
Epoch 237/1000 
	 loss: 28.9021, MinusLogProbMetric: 28.9021, val_loss: 29.3951, val_MinusLogProbMetric: 29.3951

Epoch 237: val_loss did not improve from 28.98079
196/196 - 43s - loss: 28.9021 - MinusLogProbMetric: 28.9021 - val_loss: 29.3951 - val_MinusLogProbMetric: 29.3951 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 238/1000
2023-10-02 04:58:20.232 
Epoch 238/1000 
	 loss: 28.9071, MinusLogProbMetric: 28.9071, val_loss: 30.0680, val_MinusLogProbMetric: 30.0680

Epoch 238: val_loss did not improve from 28.98079
196/196 - 46s - loss: 28.9071 - MinusLogProbMetric: 28.9071 - val_loss: 30.0680 - val_MinusLogProbMetric: 30.0680 - lr: 0.0010 - 46s/epoch - 234ms/step
Epoch 239/1000
2023-10-02 04:59:03.943 
Epoch 239/1000 
	 loss: 28.9855, MinusLogProbMetric: 28.9855, val_loss: 29.1549, val_MinusLogProbMetric: 29.1549

Epoch 239: val_loss did not improve from 28.98079
196/196 - 44s - loss: 28.9855 - MinusLogProbMetric: 28.9855 - val_loss: 29.1549 - val_MinusLogProbMetric: 29.1549 - lr: 0.0010 - 44s/epoch - 223ms/step
Epoch 240/1000
2023-10-02 04:59:48.005 
Epoch 240/1000 
	 loss: 28.8855, MinusLogProbMetric: 28.8855, val_loss: 30.4589, val_MinusLogProbMetric: 30.4589

Epoch 240: val_loss did not improve from 28.98079
196/196 - 44s - loss: 28.8855 - MinusLogProbMetric: 28.8855 - val_loss: 30.4589 - val_MinusLogProbMetric: 30.4589 - lr: 0.0010 - 44s/epoch - 225ms/step
Epoch 241/1000
2023-10-02 05:00:30.873 
Epoch 241/1000 
	 loss: 28.7937, MinusLogProbMetric: 28.7937, val_loss: 29.4253, val_MinusLogProbMetric: 29.4253

Epoch 241: val_loss did not improve from 28.98079
196/196 - 43s - loss: 28.7937 - MinusLogProbMetric: 28.7937 - val_loss: 29.4253 - val_MinusLogProbMetric: 29.4253 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 242/1000
2023-10-02 05:01:15.693 
Epoch 242/1000 
	 loss: 28.8941, MinusLogProbMetric: 28.8941, val_loss: 29.1782, val_MinusLogProbMetric: 29.1782

Epoch 242: val_loss did not improve from 28.98079
196/196 - 45s - loss: 28.8941 - MinusLogProbMetric: 28.8941 - val_loss: 29.1782 - val_MinusLogProbMetric: 29.1782 - lr: 0.0010 - 45s/epoch - 229ms/step
Epoch 243/1000
2023-10-02 05:02:01.400 
Epoch 243/1000 
	 loss: 28.9249, MinusLogProbMetric: 28.9249, val_loss: 29.3758, val_MinusLogProbMetric: 29.3758

Epoch 243: val_loss did not improve from 28.98079
196/196 - 46s - loss: 28.9249 - MinusLogProbMetric: 28.9249 - val_loss: 29.3758 - val_MinusLogProbMetric: 29.3758 - lr: 0.0010 - 46s/epoch - 233ms/step
Epoch 244/1000
2023-10-02 05:02:45.731 
Epoch 244/1000 
	 loss: 28.9577, MinusLogProbMetric: 28.9577, val_loss: 29.6119, val_MinusLogProbMetric: 29.6119

Epoch 244: val_loss did not improve from 28.98079
196/196 - 44s - loss: 28.9577 - MinusLogProbMetric: 28.9577 - val_loss: 29.6119 - val_MinusLogProbMetric: 29.6119 - lr: 0.0010 - 44s/epoch - 226ms/step
Epoch 245/1000
2023-10-02 05:03:30.257 
Epoch 245/1000 
	 loss: 28.8763, MinusLogProbMetric: 28.8763, val_loss: 29.1312, val_MinusLogProbMetric: 29.1312

Epoch 245: val_loss did not improve from 28.98079
196/196 - 45s - loss: 28.8763 - MinusLogProbMetric: 28.8763 - val_loss: 29.1312 - val_MinusLogProbMetric: 29.1312 - lr: 0.0010 - 45s/epoch - 227ms/step
Epoch 246/1000
2023-10-02 05:04:13.425 
Epoch 246/1000 
	 loss: 28.8440, MinusLogProbMetric: 28.8440, val_loss: 30.0450, val_MinusLogProbMetric: 30.0450

Epoch 246: val_loss did not improve from 28.98079
196/196 - 43s - loss: 28.8440 - MinusLogProbMetric: 28.8440 - val_loss: 30.0450 - val_MinusLogProbMetric: 30.0450 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 247/1000
2023-10-02 05:04:59.060 
Epoch 247/1000 
	 loss: 28.9429, MinusLogProbMetric: 28.9429, val_loss: 29.5977, val_MinusLogProbMetric: 29.5977

Epoch 247: val_loss did not improve from 28.98079
196/196 - 46s - loss: 28.9429 - MinusLogProbMetric: 28.9429 - val_loss: 29.5977 - val_MinusLogProbMetric: 29.5977 - lr: 0.0010 - 46s/epoch - 233ms/step
Epoch 248/1000
2023-10-02 05:05:41.636 
Epoch 248/1000 
	 loss: 29.0485, MinusLogProbMetric: 29.0485, val_loss: 29.7772, val_MinusLogProbMetric: 29.7772

Epoch 248: val_loss did not improve from 28.98079
196/196 - 43s - loss: 29.0485 - MinusLogProbMetric: 29.0485 - val_loss: 29.7772 - val_MinusLogProbMetric: 29.7772 - lr: 0.0010 - 43s/epoch - 217ms/step
Epoch 249/1000
2023-10-02 05:06:26.214 
Epoch 249/1000 
	 loss: 28.9484, MinusLogProbMetric: 28.9484, val_loss: 29.8492, val_MinusLogProbMetric: 29.8492

Epoch 249: val_loss did not improve from 28.98079
196/196 - 45s - loss: 28.9484 - MinusLogProbMetric: 28.9484 - val_loss: 29.8492 - val_MinusLogProbMetric: 29.8492 - lr: 0.0010 - 45s/epoch - 227ms/step
Epoch 250/1000
2023-10-02 05:07:11.973 
Epoch 250/1000 
	 loss: 28.9237, MinusLogProbMetric: 28.9237, val_loss: 29.0928, val_MinusLogProbMetric: 29.0928

Epoch 250: val_loss did not improve from 28.98079
196/196 - 46s - loss: 28.9237 - MinusLogProbMetric: 28.9237 - val_loss: 29.0928 - val_MinusLogProbMetric: 29.0928 - lr: 0.0010 - 46s/epoch - 233ms/step
Epoch 251/1000
2023-10-02 05:07:55.409 
Epoch 251/1000 
	 loss: 28.9054, MinusLogProbMetric: 28.9054, val_loss: 29.5277, val_MinusLogProbMetric: 29.5277

Epoch 251: val_loss did not improve from 28.98079
196/196 - 43s - loss: 28.9054 - MinusLogProbMetric: 28.9054 - val_loss: 29.5277 - val_MinusLogProbMetric: 29.5277 - lr: 0.0010 - 43s/epoch - 222ms/step
Epoch 252/1000
2023-10-02 05:08:40.239 
Epoch 252/1000 
	 loss: 28.7893, MinusLogProbMetric: 28.7893, val_loss: 29.4942, val_MinusLogProbMetric: 29.4942

Epoch 252: val_loss did not improve from 28.98079
196/196 - 45s - loss: 28.7893 - MinusLogProbMetric: 28.7893 - val_loss: 29.4942 - val_MinusLogProbMetric: 29.4942 - lr: 0.0010 - 45s/epoch - 229ms/step
Epoch 253/1000
2023-10-02 05:09:24.828 
Epoch 253/1000 
	 loss: 28.8431, MinusLogProbMetric: 28.8431, val_loss: 29.4499, val_MinusLogProbMetric: 29.4499

Epoch 253: val_loss did not improve from 28.98079
196/196 - 45s - loss: 28.8431 - MinusLogProbMetric: 28.8431 - val_loss: 29.4499 - val_MinusLogProbMetric: 29.4499 - lr: 0.0010 - 45s/epoch - 227ms/step
Epoch 254/1000
2023-10-02 05:10:09.864 
Epoch 254/1000 
	 loss: 28.7765, MinusLogProbMetric: 28.7765, val_loss: 29.2410, val_MinusLogProbMetric: 29.2410

Epoch 254: val_loss did not improve from 28.98079
196/196 - 45s - loss: 28.7765 - MinusLogProbMetric: 28.7765 - val_loss: 29.2410 - val_MinusLogProbMetric: 29.2410 - lr: 0.0010 - 45s/epoch - 230ms/step
Epoch 255/1000
2023-10-02 05:10:54.778 
Epoch 255/1000 
	 loss: 28.9691, MinusLogProbMetric: 28.9691, val_loss: 29.5122, val_MinusLogProbMetric: 29.5122

Epoch 255: val_loss did not improve from 28.98079
196/196 - 45s - loss: 28.9691 - MinusLogProbMetric: 28.9691 - val_loss: 29.5122 - val_MinusLogProbMetric: 29.5122 - lr: 0.0010 - 45s/epoch - 229ms/step
Epoch 256/1000
2023-10-02 05:11:38.906 
Epoch 256/1000 
	 loss: 28.8529, MinusLogProbMetric: 28.8529, val_loss: 29.3456, val_MinusLogProbMetric: 29.3456

Epoch 256: val_loss did not improve from 28.98079
196/196 - 44s - loss: 28.8529 - MinusLogProbMetric: 28.8529 - val_loss: 29.3456 - val_MinusLogProbMetric: 29.3456 - lr: 0.0010 - 44s/epoch - 225ms/step
Epoch 257/1000
2023-10-02 05:12:22.744 
Epoch 257/1000 
	 loss: 28.7909, MinusLogProbMetric: 28.7909, val_loss: 30.9692, val_MinusLogProbMetric: 30.9692

Epoch 257: val_loss did not improve from 28.98079
196/196 - 44s - loss: 28.7909 - MinusLogProbMetric: 28.7909 - val_loss: 30.9692 - val_MinusLogProbMetric: 30.9692 - lr: 0.0010 - 44s/epoch - 224ms/step
Epoch 258/1000
2023-10-02 05:13:10.468 
Epoch 258/1000 
	 loss: 28.8964, MinusLogProbMetric: 28.8964, val_loss: 29.0923, val_MinusLogProbMetric: 29.0923

Epoch 258: val_loss did not improve from 28.98079
196/196 - 48s - loss: 28.8964 - MinusLogProbMetric: 28.8964 - val_loss: 29.0923 - val_MinusLogProbMetric: 29.0923 - lr: 0.0010 - 48s/epoch - 243ms/step
Epoch 259/1000
2023-10-02 05:13:55.019 
Epoch 259/1000 
	 loss: 28.8375, MinusLogProbMetric: 28.8375, val_loss: 29.2038, val_MinusLogProbMetric: 29.2038

Epoch 259: val_loss did not improve from 28.98079
196/196 - 45s - loss: 28.8375 - MinusLogProbMetric: 28.8375 - val_loss: 29.2038 - val_MinusLogProbMetric: 29.2038 - lr: 0.0010 - 45s/epoch - 227ms/step
Epoch 260/1000
2023-10-02 05:14:40.159 
Epoch 260/1000 
	 loss: 28.7622, MinusLogProbMetric: 28.7622, val_loss: 29.2030, val_MinusLogProbMetric: 29.2030

Epoch 260: val_loss did not improve from 28.98079
196/196 - 45s - loss: 28.7622 - MinusLogProbMetric: 28.7622 - val_loss: 29.2030 - val_MinusLogProbMetric: 29.2030 - lr: 0.0010 - 45s/epoch - 230ms/step
Epoch 261/1000
2023-10-02 05:15:25.776 
Epoch 261/1000 
	 loss: 29.0744, MinusLogProbMetric: 29.0744, val_loss: 29.3566, val_MinusLogProbMetric: 29.3566

Epoch 261: val_loss did not improve from 28.98079
196/196 - 46s - loss: 29.0744 - MinusLogProbMetric: 29.0744 - val_loss: 29.3566 - val_MinusLogProbMetric: 29.3566 - lr: 0.0010 - 46s/epoch - 233ms/step
Epoch 262/1000
2023-10-02 05:16:11.503 
Epoch 262/1000 
	 loss: 28.7674, MinusLogProbMetric: 28.7674, val_loss: 29.3269, val_MinusLogProbMetric: 29.3269

Epoch 262: val_loss did not improve from 28.98079
196/196 - 46s - loss: 28.7674 - MinusLogProbMetric: 28.7674 - val_loss: 29.3269 - val_MinusLogProbMetric: 29.3269 - lr: 0.0010 - 46s/epoch - 233ms/step
Epoch 263/1000
2023-10-02 05:16:55.059 
Epoch 263/1000 
	 loss: 28.7295, MinusLogProbMetric: 28.7295, val_loss: 29.2052, val_MinusLogProbMetric: 29.2052

Epoch 263: val_loss did not improve from 28.98079
196/196 - 44s - loss: 28.7295 - MinusLogProbMetric: 28.7295 - val_loss: 29.2052 - val_MinusLogProbMetric: 29.2052 - lr: 0.0010 - 44s/epoch - 222ms/step
Epoch 264/1000
2023-10-02 05:17:41.080 
Epoch 264/1000 
	 loss: 28.9693, MinusLogProbMetric: 28.9693, val_loss: 29.1631, val_MinusLogProbMetric: 29.1631

Epoch 264: val_loss did not improve from 28.98079
196/196 - 46s - loss: 28.9693 - MinusLogProbMetric: 28.9693 - val_loss: 29.1631 - val_MinusLogProbMetric: 29.1631 - lr: 0.0010 - 46s/epoch - 235ms/step
Epoch 265/1000
2023-10-02 05:18:26.449 
Epoch 265/1000 
	 loss: 28.9554, MinusLogProbMetric: 28.9554, val_loss: 29.5044, val_MinusLogProbMetric: 29.5044

Epoch 265: val_loss did not improve from 28.98079
196/196 - 45s - loss: 28.9554 - MinusLogProbMetric: 28.9554 - val_loss: 29.5044 - val_MinusLogProbMetric: 29.5044 - lr: 0.0010 - 45s/epoch - 232ms/step
Epoch 266/1000
2023-10-02 05:19:13.793 
Epoch 266/1000 
	 loss: 28.8516, MinusLogProbMetric: 28.8516, val_loss: 29.5676, val_MinusLogProbMetric: 29.5676

Epoch 266: val_loss did not improve from 28.98079
196/196 - 47s - loss: 28.8516 - MinusLogProbMetric: 28.8516 - val_loss: 29.5676 - val_MinusLogProbMetric: 29.5676 - lr: 0.0010 - 47s/epoch - 241ms/step
Epoch 267/1000
2023-10-02 05:19:58.232 
Epoch 267/1000 
	 loss: 28.8207, MinusLogProbMetric: 28.8207, val_loss: 29.2733, val_MinusLogProbMetric: 29.2733

Epoch 267: val_loss did not improve from 28.98079
196/196 - 44s - loss: 28.8207 - MinusLogProbMetric: 28.8207 - val_loss: 29.2733 - val_MinusLogProbMetric: 29.2733 - lr: 0.0010 - 44s/epoch - 227ms/step
Epoch 268/1000
2023-10-02 05:20:42.520 
Epoch 268/1000 
	 loss: 28.7308, MinusLogProbMetric: 28.7308, val_loss: 29.5509, val_MinusLogProbMetric: 29.5509

Epoch 268: val_loss did not improve from 28.98079
196/196 - 44s - loss: 28.7308 - MinusLogProbMetric: 28.7308 - val_loss: 29.5509 - val_MinusLogProbMetric: 29.5509 - lr: 0.0010 - 44s/epoch - 226ms/step
Epoch 269/1000
2023-10-02 05:21:30.096 
Epoch 269/1000 
	 loss: 28.9132, MinusLogProbMetric: 28.9132, val_loss: 30.1141, val_MinusLogProbMetric: 30.1141

Epoch 269: val_loss did not improve from 28.98079
196/196 - 48s - loss: 28.9132 - MinusLogProbMetric: 28.9132 - val_loss: 30.1141 - val_MinusLogProbMetric: 30.1141 - lr: 0.0010 - 48s/epoch - 243ms/step
Epoch 270/1000
2023-10-02 05:22:14.134 
Epoch 270/1000 
	 loss: 28.9239, MinusLogProbMetric: 28.9239, val_loss: 29.1105, val_MinusLogProbMetric: 29.1105

Epoch 270: val_loss did not improve from 28.98079
196/196 - 44s - loss: 28.9239 - MinusLogProbMetric: 28.9239 - val_loss: 29.1105 - val_MinusLogProbMetric: 29.1105 - lr: 0.0010 - 44s/epoch - 225ms/step
Epoch 271/1000
2023-10-02 05:23:00.701 
Epoch 271/1000 
	 loss: 28.7661, MinusLogProbMetric: 28.7661, val_loss: 29.1544, val_MinusLogProbMetric: 29.1544

Epoch 271: val_loss did not improve from 28.98079
196/196 - 47s - loss: 28.7661 - MinusLogProbMetric: 28.7661 - val_loss: 29.1544 - val_MinusLogProbMetric: 29.1544 - lr: 0.0010 - 47s/epoch - 238ms/step
Epoch 272/1000
2023-10-02 05:23:46.632 
Epoch 272/1000 
	 loss: 28.8545, MinusLogProbMetric: 28.8545, val_loss: 29.0771, val_MinusLogProbMetric: 29.0771

Epoch 272: val_loss did not improve from 28.98079
196/196 - 46s - loss: 28.8545 - MinusLogProbMetric: 28.8545 - val_loss: 29.0771 - val_MinusLogProbMetric: 29.0771 - lr: 0.0010 - 46s/epoch - 234ms/step
Epoch 273/1000
2023-10-02 05:24:31.954 
Epoch 273/1000 
	 loss: 28.6818, MinusLogProbMetric: 28.6818, val_loss: 30.0488, val_MinusLogProbMetric: 30.0488

Epoch 273: val_loss did not improve from 28.98079
196/196 - 45s - loss: 28.6818 - MinusLogProbMetric: 28.6818 - val_loss: 30.0488 - val_MinusLogProbMetric: 30.0488 - lr: 0.0010 - 45s/epoch - 231ms/step
Epoch 274/1000
2023-10-02 05:25:15.514 
Epoch 274/1000 
	 loss: 28.9322, MinusLogProbMetric: 28.9322, val_loss: 29.4483, val_MinusLogProbMetric: 29.4483

Epoch 274: val_loss did not improve from 28.98079
196/196 - 44s - loss: 28.9322 - MinusLogProbMetric: 28.9322 - val_loss: 29.4483 - val_MinusLogProbMetric: 29.4483 - lr: 0.0010 - 44s/epoch - 222ms/step
Epoch 275/1000
2023-10-02 05:25:59.630 
Epoch 275/1000 
	 loss: 28.6069, MinusLogProbMetric: 28.6069, val_loss: 30.1883, val_MinusLogProbMetric: 30.1883

Epoch 275: val_loss did not improve from 28.98079
196/196 - 44s - loss: 28.6069 - MinusLogProbMetric: 28.6069 - val_loss: 30.1883 - val_MinusLogProbMetric: 30.1883 - lr: 0.0010 - 44s/epoch - 225ms/step
Epoch 276/1000
2023-10-02 05:26:46.695 
Epoch 276/1000 
	 loss: 28.6751, MinusLogProbMetric: 28.6751, val_loss: 30.2907, val_MinusLogProbMetric: 30.2907

Epoch 276: val_loss did not improve from 28.98079
196/196 - 47s - loss: 28.6751 - MinusLogProbMetric: 28.6751 - val_loss: 30.2907 - val_MinusLogProbMetric: 30.2907 - lr: 0.0010 - 47s/epoch - 240ms/step
Epoch 277/1000
2023-10-02 05:27:31.724 
Epoch 277/1000 
	 loss: 28.7108, MinusLogProbMetric: 28.7108, val_loss: 29.1421, val_MinusLogProbMetric: 29.1421

Epoch 277: val_loss did not improve from 28.98079
196/196 - 45s - loss: 28.7108 - MinusLogProbMetric: 28.7108 - val_loss: 29.1421 - val_MinusLogProbMetric: 29.1421 - lr: 0.0010 - 45s/epoch - 230ms/step
Epoch 278/1000
2023-10-02 05:28:19.407 
Epoch 278/1000 
	 loss: 28.8041, MinusLogProbMetric: 28.8041, val_loss: 29.6471, val_MinusLogProbMetric: 29.6471

Epoch 278: val_loss did not improve from 28.98079
196/196 - 48s - loss: 28.8041 - MinusLogProbMetric: 28.8041 - val_loss: 29.6471 - val_MinusLogProbMetric: 29.6471 - lr: 0.0010 - 48s/epoch - 243ms/step
Epoch 279/1000
2023-10-02 05:29:04.470 
Epoch 279/1000 
	 loss: 28.5151, MinusLogProbMetric: 28.5151, val_loss: 29.0571, val_MinusLogProbMetric: 29.0571

Epoch 279: val_loss did not improve from 28.98079
196/196 - 45s - loss: 28.5151 - MinusLogProbMetric: 28.5151 - val_loss: 29.0571 - val_MinusLogProbMetric: 29.0571 - lr: 0.0010 - 45s/epoch - 230ms/step
Epoch 280/1000
2023-10-02 05:29:48.995 
Epoch 280/1000 
	 loss: 28.5769, MinusLogProbMetric: 28.5769, val_loss: 29.3535, val_MinusLogProbMetric: 29.3535

Epoch 280: val_loss did not improve from 28.98079
196/196 - 45s - loss: 28.5769 - MinusLogProbMetric: 28.5769 - val_loss: 29.3535 - val_MinusLogProbMetric: 29.3535 - lr: 0.0010 - 45s/epoch - 227ms/step
Epoch 281/1000
2023-10-02 05:30:33.012 
Epoch 281/1000 
	 loss: 28.8375, MinusLogProbMetric: 28.8375, val_loss: 29.1445, val_MinusLogProbMetric: 29.1445

Epoch 281: val_loss did not improve from 28.98079
196/196 - 44s - loss: 28.8375 - MinusLogProbMetric: 28.8375 - val_loss: 29.1445 - val_MinusLogProbMetric: 29.1445 - lr: 0.0010 - 44s/epoch - 224ms/step
Epoch 282/1000
2023-10-02 05:31:17.982 
Epoch 282/1000 
	 loss: 28.8382, MinusLogProbMetric: 28.8382, val_loss: 29.2535, val_MinusLogProbMetric: 29.2535

Epoch 282: val_loss did not improve from 28.98079
196/196 - 45s - loss: 28.8382 - MinusLogProbMetric: 28.8382 - val_loss: 29.2535 - val_MinusLogProbMetric: 29.2535 - lr: 0.0010 - 45s/epoch - 229ms/step
Epoch 283/1000
2023-10-02 05:32:04.057 
Epoch 283/1000 
	 loss: 27.9372, MinusLogProbMetric: 27.9372, val_loss: 28.8539, val_MinusLogProbMetric: 28.8539

Epoch 283: val_loss improved from 28.98079 to 28.85394, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_354/weights/best_weights.h5
196/196 - 46s - loss: 27.9372 - MinusLogProbMetric: 27.9372 - val_loss: 28.8539 - val_MinusLogProbMetric: 28.8539 - lr: 5.0000e-04 - 46s/epoch - 236ms/step
Epoch 284/1000
2023-10-02 05:32:48.690 
Epoch 284/1000 
	 loss: 27.8841, MinusLogProbMetric: 27.8841, val_loss: 28.5510, val_MinusLogProbMetric: 28.5510

Epoch 284: val_loss improved from 28.85394 to 28.55098, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_354/weights/best_weights.h5
196/196 - 45s - loss: 27.8841 - MinusLogProbMetric: 27.8841 - val_loss: 28.5510 - val_MinusLogProbMetric: 28.5510 - lr: 5.0000e-04 - 45s/epoch - 229ms/step
Epoch 285/1000
2023-10-02 05:33:34.478 
Epoch 285/1000 
	 loss: 28.0037, MinusLogProbMetric: 28.0037, val_loss: 28.7256, val_MinusLogProbMetric: 28.7256

Epoch 285: val_loss did not improve from 28.55098
196/196 - 45s - loss: 28.0037 - MinusLogProbMetric: 28.0037 - val_loss: 28.7256 - val_MinusLogProbMetric: 28.7256 - lr: 5.0000e-04 - 45s/epoch - 231ms/step
Epoch 286/1000
2023-10-02 05:34:19.691 
Epoch 286/1000 
	 loss: 27.9263, MinusLogProbMetric: 27.9263, val_loss: 28.7690, val_MinusLogProbMetric: 28.7690

Epoch 286: val_loss did not improve from 28.55098
196/196 - 45s - loss: 27.9263 - MinusLogProbMetric: 27.9263 - val_loss: 28.7690 - val_MinusLogProbMetric: 28.7690 - lr: 5.0000e-04 - 45s/epoch - 231ms/step
Epoch 287/1000
2023-10-02 05:35:05.715 
Epoch 287/1000 
	 loss: 28.3350, MinusLogProbMetric: 28.3350, val_loss: 28.7067, val_MinusLogProbMetric: 28.7067

Epoch 287: val_loss did not improve from 28.55098
196/196 - 46s - loss: 28.3350 - MinusLogProbMetric: 28.3350 - val_loss: 28.7067 - val_MinusLogProbMetric: 28.7067 - lr: 5.0000e-04 - 46s/epoch - 235ms/step
Epoch 288/1000
2023-10-02 05:35:51.571 
Epoch 288/1000 
	 loss: 28.1330, MinusLogProbMetric: 28.1330, val_loss: 29.0362, val_MinusLogProbMetric: 29.0362

Epoch 288: val_loss did not improve from 28.55098
196/196 - 46s - loss: 28.1330 - MinusLogProbMetric: 28.1330 - val_loss: 29.0362 - val_MinusLogProbMetric: 29.0362 - lr: 5.0000e-04 - 46s/epoch - 234ms/step
Epoch 289/1000
2023-10-02 05:36:37.268 
Epoch 289/1000 
	 loss: 28.0302, MinusLogProbMetric: 28.0302, val_loss: 28.5916, val_MinusLogProbMetric: 28.5916

Epoch 289: val_loss did not improve from 28.55098
196/196 - 46s - loss: 28.0302 - MinusLogProbMetric: 28.0302 - val_loss: 28.5916 - val_MinusLogProbMetric: 28.5916 - lr: 5.0000e-04 - 46s/epoch - 233ms/step
Epoch 290/1000
2023-10-02 05:37:23.612 
Epoch 290/1000 
	 loss: 28.0132, MinusLogProbMetric: 28.0132, val_loss: 28.5501, val_MinusLogProbMetric: 28.5501

Epoch 290: val_loss improved from 28.55098 to 28.55013, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_354/weights/best_weights.h5
196/196 - 47s - loss: 28.0132 - MinusLogProbMetric: 28.0132 - val_loss: 28.5501 - val_MinusLogProbMetric: 28.5501 - lr: 5.0000e-04 - 47s/epoch - 240ms/step
Epoch 291/1000
2023-10-02 05:38:09.874 
Epoch 291/1000 
	 loss: 27.9834, MinusLogProbMetric: 27.9834, val_loss: 28.8314, val_MinusLogProbMetric: 28.8314

Epoch 291: val_loss did not improve from 28.55013
196/196 - 46s - loss: 27.9834 - MinusLogProbMetric: 27.9834 - val_loss: 28.8314 - val_MinusLogProbMetric: 28.8314 - lr: 5.0000e-04 - 46s/epoch - 232ms/step
Epoch 292/1000
2023-10-02 05:38:57.069 
Epoch 292/1000 
	 loss: 27.9503, MinusLogProbMetric: 27.9503, val_loss: 28.9049, val_MinusLogProbMetric: 28.9049

Epoch 292: val_loss did not improve from 28.55013
196/196 - 47s - loss: 27.9503 - MinusLogProbMetric: 27.9503 - val_loss: 28.9049 - val_MinusLogProbMetric: 28.9049 - lr: 5.0000e-04 - 47s/epoch - 241ms/step
Epoch 293/1000
2023-10-02 05:39:43.945 
Epoch 293/1000 
	 loss: 28.0329, MinusLogProbMetric: 28.0329, val_loss: 28.7802, val_MinusLogProbMetric: 28.7802

Epoch 293: val_loss did not improve from 28.55013
196/196 - 47s - loss: 28.0329 - MinusLogProbMetric: 28.0329 - val_loss: 28.7802 - val_MinusLogProbMetric: 28.7802 - lr: 5.0000e-04 - 47s/epoch - 239ms/step
Epoch 294/1000
2023-10-02 05:40:27.740 
Epoch 294/1000 
	 loss: 28.3150, MinusLogProbMetric: 28.3150, val_loss: 28.7931, val_MinusLogProbMetric: 28.7931

Epoch 294: val_loss did not improve from 28.55013
196/196 - 44s - loss: 28.3150 - MinusLogProbMetric: 28.3150 - val_loss: 28.7931 - val_MinusLogProbMetric: 28.7931 - lr: 5.0000e-04 - 44s/epoch - 223ms/step
Epoch 295/1000
2023-10-02 05:41:13.173 
Epoch 295/1000 
	 loss: 28.3091, MinusLogProbMetric: 28.3091, val_loss: 28.9255, val_MinusLogProbMetric: 28.9255

Epoch 295: val_loss did not improve from 28.55013
196/196 - 45s - loss: 28.3091 - MinusLogProbMetric: 28.3091 - val_loss: 28.9255 - val_MinusLogProbMetric: 28.9255 - lr: 5.0000e-04 - 45s/epoch - 232ms/step
Epoch 296/1000
2023-10-02 05:41:59.131 
Epoch 296/1000 
	 loss: 28.1107, MinusLogProbMetric: 28.1107, val_loss: 28.7903, val_MinusLogProbMetric: 28.7903

Epoch 296: val_loss did not improve from 28.55013
196/196 - 46s - loss: 28.1107 - MinusLogProbMetric: 28.1107 - val_loss: 28.7903 - val_MinusLogProbMetric: 28.7903 - lr: 5.0000e-04 - 46s/epoch - 235ms/step
Epoch 297/1000
2023-10-02 05:42:44.368 
Epoch 297/1000 
	 loss: 28.2366, MinusLogProbMetric: 28.2366, val_loss: 28.5333, val_MinusLogProbMetric: 28.5333

Epoch 297: val_loss improved from 28.55013 to 28.53328, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_354/weights/best_weights.h5
196/196 - 46s - loss: 28.2366 - MinusLogProbMetric: 28.2366 - val_loss: 28.5333 - val_MinusLogProbMetric: 28.5333 - lr: 5.0000e-04 - 46s/epoch - 235ms/step
Epoch 298/1000
2023-10-02 05:43:30.307 
Epoch 298/1000 
	 loss: 28.0685, MinusLogProbMetric: 28.0685, val_loss: 28.7452, val_MinusLogProbMetric: 28.7452

Epoch 298: val_loss did not improve from 28.53328
196/196 - 45s - loss: 28.0685 - MinusLogProbMetric: 28.0685 - val_loss: 28.7452 - val_MinusLogProbMetric: 28.7452 - lr: 5.0000e-04 - 45s/epoch - 230ms/step
Epoch 299/1000
2023-10-02 05:44:15.345 
Epoch 299/1000 
	 loss: 27.9902, MinusLogProbMetric: 27.9902, val_loss: 29.1533, val_MinusLogProbMetric: 29.1533

Epoch 299: val_loss did not improve from 28.53328
196/196 - 45s - loss: 27.9902 - MinusLogProbMetric: 27.9902 - val_loss: 29.1533 - val_MinusLogProbMetric: 29.1533 - lr: 5.0000e-04 - 45s/epoch - 230ms/step
Epoch 300/1000
2023-10-02 05:45:01.451 
Epoch 300/1000 
	 loss: 27.9400, MinusLogProbMetric: 27.9400, val_loss: 29.0347, val_MinusLogProbMetric: 29.0347

Epoch 300: val_loss did not improve from 28.53328
196/196 - 46s - loss: 27.9400 - MinusLogProbMetric: 27.9400 - val_loss: 29.0347 - val_MinusLogProbMetric: 29.0347 - lr: 5.0000e-04 - 46s/epoch - 235ms/step
Epoch 301/1000
2023-10-02 05:45:46.752 
Epoch 301/1000 
	 loss: 28.1209, MinusLogProbMetric: 28.1209, val_loss: 29.3214, val_MinusLogProbMetric: 29.3214

Epoch 301: val_loss did not improve from 28.53328
196/196 - 45s - loss: 28.1209 - MinusLogProbMetric: 28.1209 - val_loss: 29.3214 - val_MinusLogProbMetric: 29.3214 - lr: 5.0000e-04 - 45s/epoch - 231ms/step
Epoch 302/1000
2023-10-02 05:46:32.493 
Epoch 302/1000 
	 loss: 28.0812, MinusLogProbMetric: 28.0812, val_loss: 28.8601, val_MinusLogProbMetric: 28.8601

Epoch 302: val_loss did not improve from 28.53328
196/196 - 46s - loss: 28.0812 - MinusLogProbMetric: 28.0812 - val_loss: 28.8601 - val_MinusLogProbMetric: 28.8601 - lr: 5.0000e-04 - 46s/epoch - 233ms/step
Epoch 303/1000
2023-10-02 05:47:16.374 
Epoch 303/1000 
	 loss: 27.9490, MinusLogProbMetric: 27.9490, val_loss: 28.8044, val_MinusLogProbMetric: 28.8044

Epoch 303: val_loss did not improve from 28.53328
196/196 - 44s - loss: 27.9490 - MinusLogProbMetric: 27.9490 - val_loss: 28.8044 - val_MinusLogProbMetric: 28.8044 - lr: 5.0000e-04 - 44s/epoch - 224ms/step
Epoch 304/1000
2023-10-02 05:48:02.565 
Epoch 304/1000 
	 loss: 28.1371, MinusLogProbMetric: 28.1371, val_loss: 28.6205, val_MinusLogProbMetric: 28.6205

Epoch 304: val_loss did not improve from 28.53328
196/196 - 46s - loss: 28.1371 - MinusLogProbMetric: 28.1371 - val_loss: 28.6205 - val_MinusLogProbMetric: 28.6205 - lr: 5.0000e-04 - 46s/epoch - 236ms/step
Epoch 305/1000
2023-10-02 05:48:47.004 
Epoch 305/1000 
	 loss: 28.1912, MinusLogProbMetric: 28.1912, val_loss: 28.6805, val_MinusLogProbMetric: 28.6805

Epoch 305: val_loss did not improve from 28.53328
196/196 - 44s - loss: 28.1912 - MinusLogProbMetric: 28.1912 - val_loss: 28.6805 - val_MinusLogProbMetric: 28.6805 - lr: 5.0000e-04 - 44s/epoch - 227ms/step
Epoch 306/1000
2023-10-02 05:49:29.742 
Epoch 306/1000 
	 loss: 27.8558, MinusLogProbMetric: 27.8558, val_loss: 28.7692, val_MinusLogProbMetric: 28.7692

Epoch 306: val_loss did not improve from 28.53328
196/196 - 43s - loss: 27.8558 - MinusLogProbMetric: 27.8558 - val_loss: 28.7692 - val_MinusLogProbMetric: 28.7692 - lr: 5.0000e-04 - 43s/epoch - 218ms/step
Epoch 307/1000
2023-10-02 05:50:15.644 
Epoch 307/1000 
	 loss: 27.9749, MinusLogProbMetric: 27.9749, val_loss: 28.7757, val_MinusLogProbMetric: 28.7757

Epoch 307: val_loss did not improve from 28.53328
196/196 - 46s - loss: 27.9749 - MinusLogProbMetric: 27.9749 - val_loss: 28.7757 - val_MinusLogProbMetric: 28.7757 - lr: 5.0000e-04 - 46s/epoch - 234ms/step
Epoch 308/1000
2023-10-02 05:51:02.323 
Epoch 308/1000 
	 loss: 28.0226, MinusLogProbMetric: 28.0226, val_loss: 29.3621, val_MinusLogProbMetric: 29.3621

Epoch 308: val_loss did not improve from 28.53328
196/196 - 47s - loss: 28.0226 - MinusLogProbMetric: 28.0226 - val_loss: 29.3621 - val_MinusLogProbMetric: 29.3621 - lr: 5.0000e-04 - 47s/epoch - 238ms/step
Epoch 309/1000
2023-10-02 05:51:46.664 
Epoch 309/1000 
	 loss: 28.0150, MinusLogProbMetric: 28.0150, val_loss: 28.7579, val_MinusLogProbMetric: 28.7579

Epoch 309: val_loss did not improve from 28.53328
196/196 - 44s - loss: 28.0150 - MinusLogProbMetric: 28.0150 - val_loss: 28.7579 - val_MinusLogProbMetric: 28.7579 - lr: 5.0000e-04 - 44s/epoch - 226ms/step
Epoch 310/1000
2023-10-02 05:52:32.251 
Epoch 310/1000 
	 loss: 28.1166, MinusLogProbMetric: 28.1166, val_loss: 28.6986, val_MinusLogProbMetric: 28.6986

Epoch 310: val_loss did not improve from 28.53328
196/196 - 46s - loss: 28.1166 - MinusLogProbMetric: 28.1166 - val_loss: 28.6986 - val_MinusLogProbMetric: 28.6986 - lr: 5.0000e-04 - 46s/epoch - 233ms/step
Epoch 311/1000
2023-10-02 05:53:17.797 
Epoch 311/1000 
	 loss: 27.8657, MinusLogProbMetric: 27.8657, val_loss: 28.5998, val_MinusLogProbMetric: 28.5998

Epoch 311: val_loss did not improve from 28.53328
196/196 - 46s - loss: 27.8657 - MinusLogProbMetric: 27.8657 - val_loss: 28.5998 - val_MinusLogProbMetric: 28.5998 - lr: 5.0000e-04 - 46s/epoch - 232ms/step
Epoch 312/1000
2023-10-02 05:54:00.841 
Epoch 312/1000 
	 loss: 28.0129, MinusLogProbMetric: 28.0129, val_loss: 29.1330, val_MinusLogProbMetric: 29.1330

Epoch 312: val_loss did not improve from 28.53328
196/196 - 43s - loss: 28.0129 - MinusLogProbMetric: 28.0129 - val_loss: 29.1330 - val_MinusLogProbMetric: 29.1330 - lr: 5.0000e-04 - 43s/epoch - 220ms/step
Epoch 313/1000
2023-10-02 05:54:47.342 
Epoch 313/1000 
	 loss: 28.0753, MinusLogProbMetric: 28.0753, val_loss: 28.6414, val_MinusLogProbMetric: 28.6414

Epoch 313: val_loss did not improve from 28.53328
196/196 - 46s - loss: 28.0753 - MinusLogProbMetric: 28.0753 - val_loss: 28.6414 - val_MinusLogProbMetric: 28.6414 - lr: 5.0000e-04 - 46s/epoch - 237ms/step
Epoch 314/1000
2023-10-02 05:55:32.237 
Epoch 314/1000 
	 loss: 28.0466, MinusLogProbMetric: 28.0466, val_loss: 29.0481, val_MinusLogProbMetric: 29.0481

Epoch 314: val_loss did not improve from 28.53328
196/196 - 45s - loss: 28.0466 - MinusLogProbMetric: 28.0466 - val_loss: 29.0481 - val_MinusLogProbMetric: 29.0481 - lr: 5.0000e-04 - 45s/epoch - 229ms/step
Epoch 315/1000
2023-10-02 05:56:14.884 
Epoch 315/1000 
	 loss: 28.0203, MinusLogProbMetric: 28.0203, val_loss: 28.5404, val_MinusLogProbMetric: 28.5404

Epoch 315: val_loss did not improve from 28.53328
196/196 - 43s - loss: 28.0203 - MinusLogProbMetric: 28.0203 - val_loss: 28.5404 - val_MinusLogProbMetric: 28.5404 - lr: 5.0000e-04 - 43s/epoch - 218ms/step
Epoch 316/1000
2023-10-02 05:57:01.990 
Epoch 316/1000 
	 loss: 27.9443, MinusLogProbMetric: 27.9443, val_loss: 28.9853, val_MinusLogProbMetric: 28.9853

Epoch 316: val_loss did not improve from 28.53328
196/196 - 47s - loss: 27.9443 - MinusLogProbMetric: 27.9443 - val_loss: 28.9853 - val_MinusLogProbMetric: 28.9853 - lr: 5.0000e-04 - 47s/epoch - 240ms/step
Epoch 317/1000
2023-10-02 05:57:47.497 
Epoch 317/1000 
	 loss: 27.9100, MinusLogProbMetric: 27.9100, val_loss: 28.9920, val_MinusLogProbMetric: 28.9920

Epoch 317: val_loss did not improve from 28.53328
196/196 - 46s - loss: 27.9100 - MinusLogProbMetric: 27.9100 - val_loss: 28.9920 - val_MinusLogProbMetric: 28.9920 - lr: 5.0000e-04 - 46s/epoch - 232ms/step
Epoch 318/1000
2023-10-02 05:58:30.025 
Epoch 318/1000 
	 loss: 28.0058, MinusLogProbMetric: 28.0058, val_loss: 28.6057, val_MinusLogProbMetric: 28.6057

Epoch 318: val_loss did not improve from 28.53328
196/196 - 43s - loss: 28.0058 - MinusLogProbMetric: 28.0058 - val_loss: 28.6057 - val_MinusLogProbMetric: 28.6057 - lr: 5.0000e-04 - 43s/epoch - 217ms/step
Epoch 319/1000
2023-10-02 05:59:16.175 
Epoch 319/1000 
	 loss: 27.9736, MinusLogProbMetric: 27.9736, val_loss: 28.9732, val_MinusLogProbMetric: 28.9732

Epoch 319: val_loss did not improve from 28.53328
196/196 - 46s - loss: 27.9736 - MinusLogProbMetric: 27.9736 - val_loss: 28.9732 - val_MinusLogProbMetric: 28.9732 - lr: 5.0000e-04 - 46s/epoch - 235ms/step
Epoch 320/1000
2023-10-02 06:00:01.638 
Epoch 320/1000 
	 loss: 28.2735, MinusLogProbMetric: 28.2735, val_loss: 29.6273, val_MinusLogProbMetric: 29.6273

Epoch 320: val_loss did not improve from 28.53328
196/196 - 45s - loss: 28.2735 - MinusLogProbMetric: 28.2735 - val_loss: 29.6273 - val_MinusLogProbMetric: 29.6273 - lr: 5.0000e-04 - 45s/epoch - 232ms/step
Epoch 321/1000
2023-10-02 06:00:47.193 
Epoch 321/1000 
	 loss: 27.8682, MinusLogProbMetric: 27.8682, val_loss: 28.7525, val_MinusLogProbMetric: 28.7525

Epoch 321: val_loss did not improve from 28.53328
196/196 - 46s - loss: 27.8682 - MinusLogProbMetric: 27.8682 - val_loss: 28.7525 - val_MinusLogProbMetric: 28.7525 - lr: 5.0000e-04 - 46s/epoch - 232ms/step
Epoch 322/1000
2023-10-02 06:01:33.353 
Epoch 322/1000 
	 loss: 28.0077, MinusLogProbMetric: 28.0077, val_loss: 28.6413, val_MinusLogProbMetric: 28.6413

Epoch 322: val_loss did not improve from 28.53328
196/196 - 46s - loss: 28.0077 - MinusLogProbMetric: 28.0077 - val_loss: 28.6413 - val_MinusLogProbMetric: 28.6413 - lr: 5.0000e-04 - 46s/epoch - 236ms/step
Epoch 323/1000
2023-10-02 06:02:19.718 
Epoch 323/1000 
	 loss: 28.0300, MinusLogProbMetric: 28.0300, val_loss: 29.8626, val_MinusLogProbMetric: 29.8626

Epoch 323: val_loss did not improve from 28.53328
196/196 - 46s - loss: 28.0300 - MinusLogProbMetric: 28.0300 - val_loss: 29.8626 - val_MinusLogProbMetric: 29.8626 - lr: 5.0000e-04 - 46s/epoch - 237ms/step
Epoch 324/1000
2023-10-02 06:03:04.451 
Epoch 324/1000 
	 loss: 28.2476, MinusLogProbMetric: 28.2476, val_loss: 28.9171, val_MinusLogProbMetric: 28.9171

Epoch 324: val_loss did not improve from 28.53328
196/196 - 45s - loss: 28.2476 - MinusLogProbMetric: 28.2476 - val_loss: 28.9171 - val_MinusLogProbMetric: 28.9171 - lr: 5.0000e-04 - 45s/epoch - 228ms/step
Epoch 325/1000
2023-10-02 06:03:48.720 
Epoch 325/1000 
	 loss: 28.2600, MinusLogProbMetric: 28.2600, val_loss: 28.9648, val_MinusLogProbMetric: 28.9648

Epoch 325: val_loss did not improve from 28.53328
196/196 - 44s - loss: 28.2600 - MinusLogProbMetric: 28.2600 - val_loss: 28.9648 - val_MinusLogProbMetric: 28.9648 - lr: 5.0000e-04 - 44s/epoch - 226ms/step
Epoch 326/1000
2023-10-02 06:04:34.011 
Epoch 326/1000 
	 loss: 28.3687, MinusLogProbMetric: 28.3687, val_loss: 28.8235, val_MinusLogProbMetric: 28.8235

Epoch 326: val_loss did not improve from 28.53328
196/196 - 45s - loss: 28.3687 - MinusLogProbMetric: 28.3687 - val_loss: 28.8235 - val_MinusLogProbMetric: 28.8235 - lr: 5.0000e-04 - 45s/epoch - 231ms/step
Epoch 327/1000
2023-10-02 06:05:18.581 
Epoch 327/1000 
	 loss: 28.3565, MinusLogProbMetric: 28.3565, val_loss: 32.9187, val_MinusLogProbMetric: 32.9187

Epoch 327: val_loss did not improve from 28.53328
196/196 - 45s - loss: 28.3565 - MinusLogProbMetric: 28.3565 - val_loss: 32.9187 - val_MinusLogProbMetric: 32.9187 - lr: 5.0000e-04 - 45s/epoch - 227ms/step
Epoch 328/1000
2023-10-02 06:06:04.703 
Epoch 328/1000 
	 loss: 28.3760, MinusLogProbMetric: 28.3760, val_loss: 28.9869, val_MinusLogProbMetric: 28.9869

Epoch 328: val_loss did not improve from 28.53328
196/196 - 46s - loss: 28.3760 - MinusLogProbMetric: 28.3760 - val_loss: 28.9869 - val_MinusLogProbMetric: 28.9869 - lr: 5.0000e-04 - 46s/epoch - 235ms/step
Epoch 329/1000
2023-10-02 06:06:47.897 
Epoch 329/1000 
	 loss: 28.2979, MinusLogProbMetric: 28.2979, val_loss: 28.8354, val_MinusLogProbMetric: 28.8354

Epoch 329: val_loss did not improve from 28.53328
196/196 - 43s - loss: 28.2979 - MinusLogProbMetric: 28.2979 - val_loss: 28.8354 - val_MinusLogProbMetric: 28.8354 - lr: 5.0000e-04 - 43s/epoch - 220ms/step
Epoch 330/1000
2023-10-02 06:07:32.632 
Epoch 330/1000 
	 loss: 28.1309, MinusLogProbMetric: 28.1309, val_loss: 28.6370, val_MinusLogProbMetric: 28.6370

Epoch 330: val_loss did not improve from 28.53328
196/196 - 45s - loss: 28.1309 - MinusLogProbMetric: 28.1309 - val_loss: 28.6370 - val_MinusLogProbMetric: 28.6370 - lr: 5.0000e-04 - 45s/epoch - 228ms/step
Epoch 331/1000
2023-10-02 06:08:16.952 
Epoch 331/1000 
	 loss: 28.0272, MinusLogProbMetric: 28.0272, val_loss: 28.9966, val_MinusLogProbMetric: 28.9966

Epoch 331: val_loss did not improve from 28.53328
196/196 - 44s - loss: 28.0272 - MinusLogProbMetric: 28.0272 - val_loss: 28.9966 - val_MinusLogProbMetric: 28.9966 - lr: 5.0000e-04 - 44s/epoch - 226ms/step
Epoch 332/1000
2023-10-02 06:09:00.525 
Epoch 332/1000 
	 loss: 28.0649, MinusLogProbMetric: 28.0649, val_loss: 28.8453, val_MinusLogProbMetric: 28.8453

Epoch 332: val_loss did not improve from 28.53328
196/196 - 44s - loss: 28.0649 - MinusLogProbMetric: 28.0649 - val_loss: 28.8453 - val_MinusLogProbMetric: 28.8453 - lr: 5.0000e-04 - 44s/epoch - 222ms/step
Epoch 333/1000
2023-10-02 06:09:44.573 
Epoch 333/1000 
	 loss: 28.0194, MinusLogProbMetric: 28.0194, val_loss: 29.7953, val_MinusLogProbMetric: 29.7953

Epoch 333: val_loss did not improve from 28.53328
196/196 - 44s - loss: 28.0194 - MinusLogProbMetric: 28.0194 - val_loss: 29.7953 - val_MinusLogProbMetric: 29.7953 - lr: 5.0000e-04 - 44s/epoch - 225ms/step
Epoch 334/1000
2023-10-02 06:10:29.856 
Epoch 334/1000 
	 loss: 27.9649, MinusLogProbMetric: 27.9649, val_loss: 30.0617, val_MinusLogProbMetric: 30.0617

Epoch 334: val_loss did not improve from 28.53328
196/196 - 45s - loss: 27.9649 - MinusLogProbMetric: 27.9649 - val_loss: 30.0617 - val_MinusLogProbMetric: 30.0617 - lr: 5.0000e-04 - 45s/epoch - 231ms/step
Epoch 335/1000
2023-10-02 06:11:13.229 
Epoch 335/1000 
	 loss: 28.1168, MinusLogProbMetric: 28.1168, val_loss: 28.5852, val_MinusLogProbMetric: 28.5852

Epoch 335: val_loss did not improve from 28.53328
196/196 - 43s - loss: 28.1168 - MinusLogProbMetric: 28.1168 - val_loss: 28.5852 - val_MinusLogProbMetric: 28.5852 - lr: 5.0000e-04 - 43s/epoch - 221ms/step
Epoch 336/1000
2023-10-02 06:11:57.525 
Epoch 336/1000 
	 loss: 27.9013, MinusLogProbMetric: 27.9013, val_loss: 28.7372, val_MinusLogProbMetric: 28.7372

Epoch 336: val_loss did not improve from 28.53328
196/196 - 44s - loss: 27.9013 - MinusLogProbMetric: 27.9013 - val_loss: 28.7372 - val_MinusLogProbMetric: 28.7372 - lr: 5.0000e-04 - 44s/epoch - 226ms/step
Epoch 337/1000
2023-10-02 06:12:42.428 
Epoch 337/1000 
	 loss: 28.0650, MinusLogProbMetric: 28.0650, val_loss: 28.6880, val_MinusLogProbMetric: 28.6880

Epoch 337: val_loss did not improve from 28.53328
196/196 - 45s - loss: 28.0650 - MinusLogProbMetric: 28.0650 - val_loss: 28.6880 - val_MinusLogProbMetric: 28.6880 - lr: 5.0000e-04 - 45s/epoch - 229ms/step
Epoch 338/1000
2023-10-02 06:13:25.654 
Epoch 338/1000 
	 loss: 27.9555, MinusLogProbMetric: 27.9555, val_loss: 28.6635, val_MinusLogProbMetric: 28.6635

Epoch 338: val_loss did not improve from 28.53328
196/196 - 43s - loss: 27.9555 - MinusLogProbMetric: 27.9555 - val_loss: 28.6635 - val_MinusLogProbMetric: 28.6635 - lr: 5.0000e-04 - 43s/epoch - 221ms/step
Epoch 339/1000
2023-10-02 06:14:10.142 
Epoch 339/1000 
	 loss: 28.0541, MinusLogProbMetric: 28.0541, val_loss: 28.9985, val_MinusLogProbMetric: 28.9985

Epoch 339: val_loss did not improve from 28.53328
196/196 - 44s - loss: 28.0541 - MinusLogProbMetric: 28.0541 - val_loss: 28.9985 - val_MinusLogProbMetric: 28.9985 - lr: 5.0000e-04 - 44s/epoch - 227ms/step
Epoch 340/1000
2023-10-02 06:14:54.983 
Epoch 340/1000 
	 loss: 27.9695, MinusLogProbMetric: 27.9695, val_loss: 28.5732, val_MinusLogProbMetric: 28.5732

Epoch 340: val_loss did not improve from 28.53328
196/196 - 45s - loss: 27.9695 - MinusLogProbMetric: 27.9695 - val_loss: 28.5732 - val_MinusLogProbMetric: 28.5732 - lr: 5.0000e-04 - 45s/epoch - 229ms/step
Epoch 341/1000
2023-10-02 06:15:38.403 
Epoch 341/1000 
	 loss: 27.9320, MinusLogProbMetric: 27.9320, val_loss: 28.6263, val_MinusLogProbMetric: 28.6263

Epoch 341: val_loss did not improve from 28.53328
196/196 - 43s - loss: 27.9320 - MinusLogProbMetric: 27.9320 - val_loss: 28.6263 - val_MinusLogProbMetric: 28.6263 - lr: 5.0000e-04 - 43s/epoch - 222ms/step
Epoch 342/1000
2023-10-02 06:16:27.837 
Epoch 342/1000 
	 loss: 28.2674, MinusLogProbMetric: 28.2674, val_loss: 29.3991, val_MinusLogProbMetric: 29.3991

Epoch 342: val_loss did not improve from 28.53328
196/196 - 49s - loss: 28.2674 - MinusLogProbMetric: 28.2674 - val_loss: 29.3991 - val_MinusLogProbMetric: 29.3991 - lr: 5.0000e-04 - 49s/epoch - 252ms/step
Epoch 343/1000
2023-10-02 06:17:11.471 
Epoch 343/1000 
	 loss: 28.2373, MinusLogProbMetric: 28.2373, val_loss: 29.1346, val_MinusLogProbMetric: 29.1346

Epoch 343: val_loss did not improve from 28.53328
196/196 - 44s - loss: 28.2373 - MinusLogProbMetric: 28.2373 - val_loss: 29.1346 - val_MinusLogProbMetric: 29.1346 - lr: 5.0000e-04 - 44s/epoch - 223ms/step
Epoch 344/1000
2023-10-02 06:17:56.730 
Epoch 344/1000 
	 loss: 28.1950, MinusLogProbMetric: 28.1950, val_loss: 28.7733, val_MinusLogProbMetric: 28.7733

Epoch 344: val_loss did not improve from 28.53328
196/196 - 45s - loss: 28.1950 - MinusLogProbMetric: 28.1950 - val_loss: 28.7733 - val_MinusLogProbMetric: 28.7733 - lr: 5.0000e-04 - 45s/epoch - 231ms/step
Epoch 345/1000
2023-10-02 06:18:41.412 
Epoch 345/1000 
	 loss: 28.3679, MinusLogProbMetric: 28.3679, val_loss: 28.8720, val_MinusLogProbMetric: 28.8720

Epoch 345: val_loss did not improve from 28.53328
196/196 - 45s - loss: 28.3679 - MinusLogProbMetric: 28.3679 - val_loss: 28.8720 - val_MinusLogProbMetric: 28.8720 - lr: 5.0000e-04 - 45s/epoch - 228ms/step
Epoch 346/1000
2023-10-02 06:19:25.160 
Epoch 346/1000 
	 loss: 28.1471, MinusLogProbMetric: 28.1471, val_loss: 28.9915, val_MinusLogProbMetric: 28.9915

Epoch 346: val_loss did not improve from 28.53328
196/196 - 44s - loss: 28.1471 - MinusLogProbMetric: 28.1471 - val_loss: 28.9915 - val_MinusLogProbMetric: 28.9915 - lr: 5.0000e-04 - 44s/epoch - 223ms/step
Epoch 347/1000
2023-10-02 06:20:10.618 
Epoch 347/1000 
	 loss: 28.5380, MinusLogProbMetric: 28.5380, val_loss: 28.8123, val_MinusLogProbMetric: 28.8123

Epoch 347: val_loss did not improve from 28.53328
196/196 - 45s - loss: 28.5380 - MinusLogProbMetric: 28.5380 - val_loss: 28.8123 - val_MinusLogProbMetric: 28.8123 - lr: 5.0000e-04 - 45s/epoch - 232ms/step
Epoch 348/1000
2023-10-02 06:20:53.442 
Epoch 348/1000 
	 loss: 27.6196, MinusLogProbMetric: 27.6196, val_loss: 28.4107, val_MinusLogProbMetric: 28.4107

Epoch 348: val_loss improved from 28.53328 to 28.41066, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_354/weights/best_weights.h5
196/196 - 44s - loss: 27.6196 - MinusLogProbMetric: 27.6196 - val_loss: 28.4107 - val_MinusLogProbMetric: 28.4107 - lr: 2.5000e-04 - 44s/epoch - 226ms/step
Epoch 349/1000
2023-10-02 06:21:41.354 
Epoch 349/1000 
	 loss: 27.5336, MinusLogProbMetric: 27.5336, val_loss: 28.4396, val_MinusLogProbMetric: 28.4396

Epoch 349: val_loss did not improve from 28.41066
196/196 - 46s - loss: 27.5336 - MinusLogProbMetric: 27.5336 - val_loss: 28.4396 - val_MinusLogProbMetric: 28.4396 - lr: 2.5000e-04 - 46s/epoch - 237ms/step
Epoch 350/1000
2023-10-02 06:22:25.754 
Epoch 350/1000 
	 loss: 27.5275, MinusLogProbMetric: 27.5275, val_loss: 28.4383, val_MinusLogProbMetric: 28.4383

Epoch 350: val_loss did not improve from 28.41066
196/196 - 44s - loss: 27.5275 - MinusLogProbMetric: 27.5275 - val_loss: 28.4383 - val_MinusLogProbMetric: 28.4383 - lr: 2.5000e-04 - 44s/epoch - 227ms/step
Epoch 351/1000
2023-10-02 06:23:09.986 
Epoch 351/1000 
	 loss: 27.6823, MinusLogProbMetric: 27.6823, val_loss: 28.4396, val_MinusLogProbMetric: 28.4396

Epoch 351: val_loss did not improve from 28.41066
196/196 - 44s - loss: 27.6823 - MinusLogProbMetric: 27.6823 - val_loss: 28.4396 - val_MinusLogProbMetric: 28.4396 - lr: 2.5000e-04 - 44s/epoch - 226ms/step
Epoch 352/1000
2023-10-02 06:23:53.352 
Epoch 352/1000 
	 loss: 27.5188, MinusLogProbMetric: 27.5188, val_loss: 28.4949, val_MinusLogProbMetric: 28.4949

Epoch 352: val_loss did not improve from 28.41066
196/196 - 43s - loss: 27.5188 - MinusLogProbMetric: 27.5188 - val_loss: 28.4949 - val_MinusLogProbMetric: 28.4949 - lr: 2.5000e-04 - 43s/epoch - 221ms/step
Epoch 353/1000
2023-10-02 06:24:37.640 
Epoch 353/1000 
	 loss: 27.7345, MinusLogProbMetric: 27.7345, val_loss: 28.6163, val_MinusLogProbMetric: 28.6163

Epoch 353: val_loss did not improve from 28.41066
196/196 - 44s - loss: 27.7345 - MinusLogProbMetric: 27.7345 - val_loss: 28.6163 - val_MinusLogProbMetric: 28.6163 - lr: 2.5000e-04 - 44s/epoch - 226ms/step
Epoch 354/1000
2023-10-02 06:25:21.891 
Epoch 354/1000 
	 loss: 27.5417, MinusLogProbMetric: 27.5417, val_loss: 28.3621, val_MinusLogProbMetric: 28.3621

Epoch 354: val_loss improved from 28.41066 to 28.36215, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_354/weights/best_weights.h5
196/196 - 45s - loss: 27.5417 - MinusLogProbMetric: 27.5417 - val_loss: 28.3621 - val_MinusLogProbMetric: 28.3621 - lr: 2.5000e-04 - 45s/epoch - 231ms/step
Epoch 355/1000
2023-10-02 06:26:06.694 
Epoch 355/1000 
	 loss: 27.5261, MinusLogProbMetric: 27.5261, val_loss: 28.4948, val_MinusLogProbMetric: 28.4948

Epoch 355: val_loss did not improve from 28.36215
196/196 - 44s - loss: 27.5261 - MinusLogProbMetric: 27.5261 - val_loss: 28.4948 - val_MinusLogProbMetric: 28.4948 - lr: 2.5000e-04 - 44s/epoch - 223ms/step
Epoch 356/1000
2023-10-02 06:26:51.659 
Epoch 356/1000 
	 loss: 27.7423, MinusLogProbMetric: 27.7423, val_loss: 28.3495, val_MinusLogProbMetric: 28.3495

Epoch 356: val_loss improved from 28.36215 to 28.34949, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_354/weights/best_weights.h5
196/196 - 46s - loss: 27.7423 - MinusLogProbMetric: 27.7423 - val_loss: 28.3495 - val_MinusLogProbMetric: 28.3495 - lr: 2.5000e-04 - 46s/epoch - 235ms/step
Epoch 357/1000
2023-10-02 06:27:35.151 
Epoch 357/1000 
	 loss: 27.5361, MinusLogProbMetric: 27.5361, val_loss: 28.3763, val_MinusLogProbMetric: 28.3763

Epoch 357: val_loss did not improve from 28.34949
196/196 - 42s - loss: 27.5361 - MinusLogProbMetric: 27.5361 - val_loss: 28.3763 - val_MinusLogProbMetric: 28.3763 - lr: 2.5000e-04 - 42s/epoch - 217ms/step
Epoch 358/1000
2023-10-02 06:28:19.962 
Epoch 358/1000 
	 loss: 27.5474, MinusLogProbMetric: 27.5474, val_loss: 28.5356, val_MinusLogProbMetric: 28.5356

Epoch 358: val_loss did not improve from 28.34949
196/196 - 45s - loss: 27.5474 - MinusLogProbMetric: 27.5474 - val_loss: 28.5356 - val_MinusLogProbMetric: 28.5356 - lr: 2.5000e-04 - 45s/epoch - 229ms/step
Epoch 359/1000
2023-10-02 06:29:03.290 
Epoch 359/1000 
	 loss: 27.5039, MinusLogProbMetric: 27.5039, val_loss: 28.4839, val_MinusLogProbMetric: 28.4839

Epoch 359: val_loss did not improve from 28.34949
196/196 - 43s - loss: 27.5039 - MinusLogProbMetric: 27.5039 - val_loss: 28.4839 - val_MinusLogProbMetric: 28.4839 - lr: 2.5000e-04 - 43s/epoch - 221ms/step
Epoch 360/1000
2023-10-02 06:29:46.330 
Epoch 360/1000 
	 loss: 27.7974, MinusLogProbMetric: 27.7974, val_loss: 28.4022, val_MinusLogProbMetric: 28.4022

Epoch 360: val_loss did not improve from 28.34949
196/196 - 43s - loss: 27.7974 - MinusLogProbMetric: 27.7974 - val_loss: 28.4022 - val_MinusLogProbMetric: 28.4022 - lr: 2.5000e-04 - 43s/epoch - 220ms/step
Epoch 361/1000
2023-10-02 06:30:29.399 
Epoch 361/1000 
	 loss: 27.5003, MinusLogProbMetric: 27.5003, val_loss: 28.3323, val_MinusLogProbMetric: 28.3323

Epoch 361: val_loss improved from 28.34949 to 28.33235, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_354/weights/best_weights.h5
196/196 - 45s - loss: 27.5003 - MinusLogProbMetric: 27.5003 - val_loss: 28.3323 - val_MinusLogProbMetric: 28.3323 - lr: 2.5000e-04 - 45s/epoch - 228ms/step
Epoch 362/1000
2023-10-02 06:31:15.849 
Epoch 362/1000 
	 loss: 27.6464, MinusLogProbMetric: 27.6464, val_loss: 28.5112, val_MinusLogProbMetric: 28.5112

Epoch 362: val_loss did not improve from 28.33235
196/196 - 45s - loss: 27.6464 - MinusLogProbMetric: 27.6464 - val_loss: 28.5112 - val_MinusLogProbMetric: 28.5112 - lr: 2.5000e-04 - 45s/epoch - 229ms/step
Epoch 363/1000
2023-10-02 06:31:59.420 
Epoch 363/1000 
	 loss: 27.5612, MinusLogProbMetric: 27.5612, val_loss: 28.3780, val_MinusLogProbMetric: 28.3780

Epoch 363: val_loss did not improve from 28.33235
196/196 - 44s - loss: 27.5612 - MinusLogProbMetric: 27.5612 - val_loss: 28.3780 - val_MinusLogProbMetric: 28.3780 - lr: 2.5000e-04 - 44s/epoch - 222ms/step
Epoch 364/1000
2023-10-02 06:32:45.086 
Epoch 364/1000 
	 loss: 27.7252, MinusLogProbMetric: 27.7252, val_loss: 28.4331, val_MinusLogProbMetric: 28.4331

Epoch 364: val_loss did not improve from 28.33235
196/196 - 46s - loss: 27.7252 - MinusLogProbMetric: 27.7252 - val_loss: 28.4331 - val_MinusLogProbMetric: 28.4331 - lr: 2.5000e-04 - 46s/epoch - 233ms/step
Epoch 365/1000
2023-10-02 06:33:27.031 
Epoch 365/1000 
	 loss: 27.5160, MinusLogProbMetric: 27.5160, val_loss: 28.3851, val_MinusLogProbMetric: 28.3851

Epoch 365: val_loss did not improve from 28.33235
196/196 - 42s - loss: 27.5160 - MinusLogProbMetric: 27.5160 - val_loss: 28.3851 - val_MinusLogProbMetric: 28.3851 - lr: 2.5000e-04 - 42s/epoch - 214ms/step
Epoch 366/1000
2023-10-02 06:34:11.706 
Epoch 366/1000 
	 loss: 27.5126, MinusLogProbMetric: 27.5126, val_loss: 28.5278, val_MinusLogProbMetric: 28.5278

Epoch 366: val_loss did not improve from 28.33235
196/196 - 45s - loss: 27.5126 - MinusLogProbMetric: 27.5126 - val_loss: 28.5278 - val_MinusLogProbMetric: 28.5278 - lr: 2.5000e-04 - 45s/epoch - 228ms/step
Epoch 367/1000
2023-10-02 06:34:54.685 
Epoch 367/1000 
	 loss: 27.5967, MinusLogProbMetric: 27.5967, val_loss: 28.7116, val_MinusLogProbMetric: 28.7116

Epoch 367: val_loss did not improve from 28.33235
196/196 - 43s - loss: 27.5967 - MinusLogProbMetric: 27.5967 - val_loss: 28.7116 - val_MinusLogProbMetric: 28.7116 - lr: 2.5000e-04 - 43s/epoch - 219ms/step
Epoch 368/1000
2023-10-02 06:35:40.009 
Epoch 368/1000 
	 loss: 27.7099, MinusLogProbMetric: 27.7099, val_loss: 28.8604, val_MinusLogProbMetric: 28.8604

Epoch 368: val_loss did not improve from 28.33235
196/196 - 45s - loss: 27.7099 - MinusLogProbMetric: 27.7099 - val_loss: 28.8604 - val_MinusLogProbMetric: 28.8604 - lr: 2.5000e-04 - 45s/epoch - 231ms/step
Epoch 369/1000
2023-10-02 06:36:23.802 
Epoch 369/1000 
	 loss: 27.5096, MinusLogProbMetric: 27.5096, val_loss: 28.6896, val_MinusLogProbMetric: 28.6896

Epoch 369: val_loss did not improve from 28.33235
196/196 - 44s - loss: 27.5096 - MinusLogProbMetric: 27.5096 - val_loss: 28.6896 - val_MinusLogProbMetric: 28.6896 - lr: 2.5000e-04 - 44s/epoch - 223ms/step
Epoch 370/1000
2023-10-02 06:37:08.336 
Epoch 370/1000 
	 loss: 27.9360, MinusLogProbMetric: 27.9360, val_loss: 28.4531, val_MinusLogProbMetric: 28.4531

Epoch 370: val_loss did not improve from 28.33235
196/196 - 45s - loss: 27.9360 - MinusLogProbMetric: 27.9360 - val_loss: 28.4531 - val_MinusLogProbMetric: 28.4531 - lr: 2.5000e-04 - 45s/epoch - 227ms/step
Epoch 371/1000
2023-10-02 06:37:52.418 
Epoch 371/1000 
	 loss: 27.5178, MinusLogProbMetric: 27.5178, val_loss: 28.3559, val_MinusLogProbMetric: 28.3559

Epoch 371: val_loss did not improve from 28.33235
196/196 - 44s - loss: 27.5178 - MinusLogProbMetric: 27.5178 - val_loss: 28.3559 - val_MinusLogProbMetric: 28.3559 - lr: 2.5000e-04 - 44s/epoch - 225ms/step
Epoch 372/1000
2023-10-02 06:38:36.517 
Epoch 372/1000 
	 loss: 27.7525, MinusLogProbMetric: 27.7525, val_loss: 28.4334, val_MinusLogProbMetric: 28.4334

Epoch 372: val_loss did not improve from 28.33235
196/196 - 44s - loss: 27.7525 - MinusLogProbMetric: 27.7525 - val_loss: 28.4334 - val_MinusLogProbMetric: 28.4334 - lr: 2.5000e-04 - 44s/epoch - 225ms/step
Epoch 373/1000
2023-10-02 06:39:21.451 
Epoch 373/1000 
	 loss: 27.6532, MinusLogProbMetric: 27.6532, val_loss: 28.3687, val_MinusLogProbMetric: 28.3687

Epoch 373: val_loss did not improve from 28.33235
196/196 - 45s - loss: 27.6532 - MinusLogProbMetric: 27.6532 - val_loss: 28.3687 - val_MinusLogProbMetric: 28.3687 - lr: 2.5000e-04 - 45s/epoch - 229ms/step
Epoch 374/1000
2023-10-02 06:40:04.176 
Epoch 374/1000 
	 loss: 27.4794, MinusLogProbMetric: 27.4794, val_loss: 28.3616, val_MinusLogProbMetric: 28.3616

Epoch 374: val_loss did not improve from 28.33235
196/196 - 43s - loss: 27.4794 - MinusLogProbMetric: 27.4794 - val_loss: 28.3616 - val_MinusLogProbMetric: 28.3616 - lr: 2.5000e-04 - 43s/epoch - 218ms/step
Epoch 375/1000
2023-10-02 06:40:50.418 
Epoch 375/1000 
	 loss: 27.8157, MinusLogProbMetric: 27.8157, val_loss: 28.3920, val_MinusLogProbMetric: 28.3920

Epoch 375: val_loss did not improve from 28.33235
196/196 - 46s - loss: 27.8157 - MinusLogProbMetric: 27.8157 - val_loss: 28.3920 - val_MinusLogProbMetric: 28.3920 - lr: 2.5000e-04 - 46s/epoch - 236ms/step
Epoch 376/1000
2023-10-02 06:41:33.582 
Epoch 376/1000 
	 loss: 27.4952, MinusLogProbMetric: 27.4952, val_loss: 28.3837, val_MinusLogProbMetric: 28.3837

Epoch 376: val_loss did not improve from 28.33235
196/196 - 43s - loss: 27.4952 - MinusLogProbMetric: 27.4952 - val_loss: 28.3837 - val_MinusLogProbMetric: 28.3837 - lr: 2.5000e-04 - 43s/epoch - 220ms/step
Epoch 377/1000
2023-10-02 06:42:14.837 
Epoch 377/1000 
	 loss: 27.5984, MinusLogProbMetric: 27.5984, val_loss: 28.6530, val_MinusLogProbMetric: 28.6530

Epoch 377: val_loss did not improve from 28.33235
196/196 - 41s - loss: 27.5984 - MinusLogProbMetric: 27.5984 - val_loss: 28.6530 - val_MinusLogProbMetric: 28.6530 - lr: 2.5000e-04 - 41s/epoch - 210ms/step
Epoch 378/1000
2023-10-02 06:43:00.425 
Epoch 378/1000 
	 loss: 27.7679, MinusLogProbMetric: 27.7679, val_loss: 28.4408, val_MinusLogProbMetric: 28.4408

Epoch 378: val_loss did not improve from 28.33235
196/196 - 46s - loss: 27.7679 - MinusLogProbMetric: 27.7679 - val_loss: 28.4408 - val_MinusLogProbMetric: 28.4408 - lr: 2.5000e-04 - 46s/epoch - 233ms/step
Epoch 379/1000
2023-10-02 06:43:44.480 
Epoch 379/1000 
	 loss: 27.5137, MinusLogProbMetric: 27.5137, val_loss: 28.3337, val_MinusLogProbMetric: 28.3337

Epoch 379: val_loss did not improve from 28.33235
196/196 - 44s - loss: 27.5137 - MinusLogProbMetric: 27.5137 - val_loss: 28.3337 - val_MinusLogProbMetric: 28.3337 - lr: 2.5000e-04 - 44s/epoch - 225ms/step
Epoch 380/1000
2023-10-02 06:44:29.122 
Epoch 380/1000 
	 loss: 27.6787, MinusLogProbMetric: 27.6787, val_loss: 28.5094, val_MinusLogProbMetric: 28.5094

Epoch 380: val_loss did not improve from 28.33235
196/196 - 45s - loss: 27.6787 - MinusLogProbMetric: 27.6787 - val_loss: 28.5094 - val_MinusLogProbMetric: 28.5094 - lr: 2.5000e-04 - 45s/epoch - 228ms/step
Epoch 381/1000
2023-10-02 06:45:14.265 
Epoch 381/1000 
	 loss: 27.5491, MinusLogProbMetric: 27.5491, val_loss: 28.4516, val_MinusLogProbMetric: 28.4516

Epoch 381: val_loss did not improve from 28.33235
196/196 - 45s - loss: 27.5491 - MinusLogProbMetric: 27.5491 - val_loss: 28.4516 - val_MinusLogProbMetric: 28.4516 - lr: 2.5000e-04 - 45s/epoch - 230ms/step
Epoch 382/1000
2023-10-02 06:45:57.623 
Epoch 382/1000 
	 loss: 27.6952, MinusLogProbMetric: 27.6952, val_loss: 28.3583, val_MinusLogProbMetric: 28.3583

Epoch 382: val_loss did not improve from 28.33235
196/196 - 43s - loss: 27.6952 - MinusLogProbMetric: 27.6952 - val_loss: 28.3583 - val_MinusLogProbMetric: 28.3583 - lr: 2.5000e-04 - 43s/epoch - 221ms/step
Epoch 383/1000
2023-10-02 06:46:43.168 
Epoch 383/1000 
	 loss: 27.4629, MinusLogProbMetric: 27.4629, val_loss: 28.3426, val_MinusLogProbMetric: 28.3426

Epoch 383: val_loss did not improve from 28.33235
196/196 - 46s - loss: 27.4629 - MinusLogProbMetric: 27.4629 - val_loss: 28.3426 - val_MinusLogProbMetric: 28.3426 - lr: 2.5000e-04 - 46s/epoch - 232ms/step
Epoch 384/1000
2023-10-02 06:47:25.754 
Epoch 384/1000 
	 loss: 27.5774, MinusLogProbMetric: 27.5774, val_loss: 28.5983, val_MinusLogProbMetric: 28.5983

Epoch 384: val_loss did not improve from 28.33235
196/196 - 43s - loss: 27.5774 - MinusLogProbMetric: 27.5774 - val_loss: 28.5983 - val_MinusLogProbMetric: 28.5983 - lr: 2.5000e-04 - 43s/epoch - 217ms/step
Epoch 385/1000
2023-10-02 06:48:10.135 
Epoch 385/1000 
	 loss: 27.5420, MinusLogProbMetric: 27.5420, val_loss: 28.6396, val_MinusLogProbMetric: 28.6396

Epoch 385: val_loss did not improve from 28.33235
196/196 - 44s - loss: 27.5420 - MinusLogProbMetric: 27.5420 - val_loss: 28.6396 - val_MinusLogProbMetric: 28.6396 - lr: 2.5000e-04 - 44s/epoch - 226ms/step
Epoch 386/1000
2023-10-02 06:48:53.407 
Epoch 386/1000 
	 loss: 27.7104, MinusLogProbMetric: 27.7104, val_loss: 28.8740, val_MinusLogProbMetric: 28.8740

Epoch 386: val_loss did not improve from 28.33235
196/196 - 43s - loss: 27.7104 - MinusLogProbMetric: 27.7104 - val_loss: 28.8740 - val_MinusLogProbMetric: 28.8740 - lr: 2.5000e-04 - 43s/epoch - 221ms/step
Epoch 387/1000
2023-10-02 06:49:38.268 
Epoch 387/1000 
	 loss: 27.4835, MinusLogProbMetric: 27.4835, val_loss: 28.6937, val_MinusLogProbMetric: 28.6937

Epoch 387: val_loss did not improve from 28.33235
196/196 - 45s - loss: 27.4835 - MinusLogProbMetric: 27.4835 - val_loss: 28.6937 - val_MinusLogProbMetric: 28.6937 - lr: 2.5000e-04 - 45s/epoch - 229ms/step
Epoch 388/1000
2023-10-02 06:50:22.862 
Epoch 388/1000 
	 loss: 27.4976, MinusLogProbMetric: 27.4976, val_loss: 28.5578, val_MinusLogProbMetric: 28.5578

Epoch 388: val_loss did not improve from 28.33235
196/196 - 45s - loss: 27.4976 - MinusLogProbMetric: 27.4976 - val_loss: 28.5578 - val_MinusLogProbMetric: 28.5578 - lr: 2.5000e-04 - 45s/epoch - 228ms/step
Epoch 389/1000
2023-10-02 06:51:06.959 
Epoch 389/1000 
	 loss: 27.6778, MinusLogProbMetric: 27.6778, val_loss: 29.4720, val_MinusLogProbMetric: 29.4720

Epoch 389: val_loss did not improve from 28.33235
196/196 - 44s - loss: 27.6778 - MinusLogProbMetric: 27.6778 - val_loss: 29.4720 - val_MinusLogProbMetric: 29.4720 - lr: 2.5000e-04 - 44s/epoch - 225ms/step
Epoch 390/1000
2023-10-02 06:51:50.074 
Epoch 390/1000 
	 loss: 27.6745, MinusLogProbMetric: 27.6745, val_loss: 28.4649, val_MinusLogProbMetric: 28.4649

Epoch 390: val_loss did not improve from 28.33235
196/196 - 43s - loss: 27.6745 - MinusLogProbMetric: 27.6745 - val_loss: 28.4649 - val_MinusLogProbMetric: 28.4649 - lr: 2.5000e-04 - 43s/epoch - 220ms/step
Epoch 391/1000
2023-10-02 06:52:35.072 
Epoch 391/1000 
	 loss: 27.6553, MinusLogProbMetric: 27.6553, val_loss: 29.0977, val_MinusLogProbMetric: 29.0977

Epoch 391: val_loss did not improve from 28.33235
196/196 - 45s - loss: 27.6553 - MinusLogProbMetric: 27.6553 - val_loss: 29.0977 - val_MinusLogProbMetric: 29.0977 - lr: 2.5000e-04 - 45s/epoch - 229ms/step
Epoch 392/1000
2023-10-02 06:53:20.337 
Epoch 392/1000 
	 loss: 27.9067, MinusLogProbMetric: 27.9067, val_loss: 28.6661, val_MinusLogProbMetric: 28.6661

Epoch 392: val_loss did not improve from 28.33235
196/196 - 45s - loss: 27.9067 - MinusLogProbMetric: 27.9067 - val_loss: 28.6661 - val_MinusLogProbMetric: 28.6661 - lr: 2.5000e-04 - 45s/epoch - 231ms/step
Epoch 393/1000
2023-10-02 06:54:07.122 
Epoch 393/1000 
	 loss: 27.6557, MinusLogProbMetric: 27.6557, val_loss: 28.4996, val_MinusLogProbMetric: 28.4996

Epoch 393: val_loss did not improve from 28.33235
196/196 - 47s - loss: 27.6557 - MinusLogProbMetric: 27.6557 - val_loss: 28.4996 - val_MinusLogProbMetric: 28.4996 - lr: 2.5000e-04 - 47s/epoch - 239ms/step
Epoch 394/1000
2023-10-02 06:54:52.409 
Epoch 394/1000 
	 loss: 27.6048, MinusLogProbMetric: 27.6048, val_loss: 28.4674, val_MinusLogProbMetric: 28.4674

Epoch 394: val_loss did not improve from 28.33235
196/196 - 45s - loss: 27.6048 - MinusLogProbMetric: 27.6048 - val_loss: 28.4674 - val_MinusLogProbMetric: 28.4674 - lr: 2.5000e-04 - 45s/epoch - 231ms/step
Epoch 395/1000
2023-10-02 06:55:35.775 
Epoch 395/1000 
	 loss: 27.5934, MinusLogProbMetric: 27.5934, val_loss: 28.3569, val_MinusLogProbMetric: 28.3569

Epoch 395: val_loss did not improve from 28.33235
196/196 - 43s - loss: 27.5934 - MinusLogProbMetric: 27.5934 - val_loss: 28.3569 - val_MinusLogProbMetric: 28.3569 - lr: 2.5000e-04 - 43s/epoch - 221ms/step
Epoch 396/1000
2023-10-02 06:56:20.967 
Epoch 396/1000 
	 loss: 27.5871, MinusLogProbMetric: 27.5871, val_loss: 28.4401, val_MinusLogProbMetric: 28.4401

Epoch 396: val_loss did not improve from 28.33235
196/196 - 45s - loss: 27.5871 - MinusLogProbMetric: 27.5871 - val_loss: 28.4401 - val_MinusLogProbMetric: 28.4401 - lr: 2.5000e-04 - 45s/epoch - 231ms/step
Epoch 397/1000
2023-10-02 06:57:05.505 
Epoch 397/1000 
	 loss: 27.4981, MinusLogProbMetric: 27.4981, val_loss: 28.3294, val_MinusLogProbMetric: 28.3294

Epoch 397: val_loss improved from 28.33235 to 28.32941, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_354/weights/best_weights.h5
196/196 - 45s - loss: 27.4981 - MinusLogProbMetric: 27.4981 - val_loss: 28.3294 - val_MinusLogProbMetric: 28.3294 - lr: 2.5000e-04 - 45s/epoch - 230ms/step
Epoch 398/1000
2023-10-02 06:57:50.696 
Epoch 398/1000 
	 loss: 27.6280, MinusLogProbMetric: 27.6280, val_loss: 28.4251, val_MinusLogProbMetric: 28.4251

Epoch 398: val_loss did not improve from 28.32941
196/196 - 45s - loss: 27.6280 - MinusLogProbMetric: 27.6280 - val_loss: 28.4251 - val_MinusLogProbMetric: 28.4251 - lr: 2.5000e-04 - 45s/epoch - 228ms/step
Epoch 399/1000
2023-10-02 06:58:34.763 
Epoch 399/1000 
	 loss: 27.4892, MinusLogProbMetric: 27.4892, val_loss: 28.4135, val_MinusLogProbMetric: 28.4135

Epoch 399: val_loss did not improve from 28.32941
196/196 - 44s - loss: 27.4892 - MinusLogProbMetric: 27.4892 - val_loss: 28.4135 - val_MinusLogProbMetric: 28.4135 - lr: 2.5000e-04 - 44s/epoch - 225ms/step
Epoch 400/1000
2023-10-02 06:59:19.287 
Epoch 400/1000 
	 loss: 27.6286, MinusLogProbMetric: 27.6286, val_loss: 29.7797, val_MinusLogProbMetric: 29.7797

Epoch 400: val_loss did not improve from 28.32941
196/196 - 45s - loss: 27.6286 - MinusLogProbMetric: 27.6286 - val_loss: 29.7797 - val_MinusLogProbMetric: 29.7797 - lr: 2.5000e-04 - 45s/epoch - 227ms/step
Epoch 401/1000
2023-10-02 07:00:02.509 
Epoch 401/1000 
	 loss: 27.8580, MinusLogProbMetric: 27.8580, val_loss: 28.6181, val_MinusLogProbMetric: 28.6181

Epoch 401: val_loss did not improve from 28.32941
196/196 - 43s - loss: 27.8580 - MinusLogProbMetric: 27.8580 - val_loss: 28.6181 - val_MinusLogProbMetric: 28.6181 - lr: 2.5000e-04 - 43s/epoch - 221ms/step
Epoch 402/1000
2023-10-02 07:00:46.887 
Epoch 402/1000 
	 loss: 27.5233, MinusLogProbMetric: 27.5233, val_loss: 28.8408, val_MinusLogProbMetric: 28.8408

Epoch 402: val_loss did not improve from 28.32941
196/196 - 44s - loss: 27.5233 - MinusLogProbMetric: 27.5233 - val_loss: 28.8408 - val_MinusLogProbMetric: 28.8408 - lr: 2.5000e-04 - 44s/epoch - 226ms/step
Epoch 403/1000
2023-10-02 07:01:31.116 
Epoch 403/1000 
	 loss: 27.5490, MinusLogProbMetric: 27.5490, val_loss: 28.3659, val_MinusLogProbMetric: 28.3659

Epoch 403: val_loss did not improve from 28.32941
196/196 - 44s - loss: 27.5490 - MinusLogProbMetric: 27.5490 - val_loss: 28.3659 - val_MinusLogProbMetric: 28.3659 - lr: 2.5000e-04 - 44s/epoch - 226ms/step
Epoch 404/1000
2023-10-02 07:02:17.092 
Epoch 404/1000 
	 loss: 27.9392, MinusLogProbMetric: 27.9392, val_loss: 29.1237, val_MinusLogProbMetric: 29.1237

Epoch 404: val_loss did not improve from 28.32941
196/196 - 46s - loss: 27.9392 - MinusLogProbMetric: 27.9392 - val_loss: 29.1237 - val_MinusLogProbMetric: 29.1237 - lr: 2.5000e-04 - 46s/epoch - 235ms/step
Epoch 405/1000
2023-10-02 07:03:02.350 
Epoch 405/1000 
	 loss: 27.6515, MinusLogProbMetric: 27.6515, val_loss: 28.5137, val_MinusLogProbMetric: 28.5137

Epoch 405: val_loss did not improve from 28.32941
196/196 - 45s - loss: 27.6515 - MinusLogProbMetric: 27.6515 - val_loss: 28.5137 - val_MinusLogProbMetric: 28.5137 - lr: 2.5000e-04 - 45s/epoch - 231ms/step
Epoch 406/1000
2023-10-02 07:03:46.309 
Epoch 406/1000 
	 loss: 27.6574, MinusLogProbMetric: 27.6574, val_loss: 28.8180, val_MinusLogProbMetric: 28.8180

Epoch 406: val_loss did not improve from 28.32941
196/196 - 44s - loss: 27.6574 - MinusLogProbMetric: 27.6574 - val_loss: 28.8180 - val_MinusLogProbMetric: 28.8180 - lr: 2.5000e-04 - 44s/epoch - 224ms/step
Epoch 407/1000
2023-10-02 07:04:30.226 
Epoch 407/1000 
	 loss: 27.7521, MinusLogProbMetric: 27.7521, val_loss: 28.5454, val_MinusLogProbMetric: 28.5454

Epoch 407: val_loss did not improve from 28.32941
196/196 - 44s - loss: 27.7521 - MinusLogProbMetric: 27.7521 - val_loss: 28.5454 - val_MinusLogProbMetric: 28.5454 - lr: 2.5000e-04 - 44s/epoch - 224ms/step
Epoch 408/1000
2023-10-02 07:05:15.567 
Epoch 408/1000 
	 loss: 27.4963, MinusLogProbMetric: 27.4963, val_loss: 28.3586, val_MinusLogProbMetric: 28.3586

Epoch 408: val_loss did not improve from 28.32941
196/196 - 45s - loss: 27.4963 - MinusLogProbMetric: 27.4963 - val_loss: 28.3586 - val_MinusLogProbMetric: 28.3586 - lr: 2.5000e-04 - 45s/epoch - 231ms/step
Epoch 409/1000
2023-10-02 07:06:00.155 
Epoch 409/1000 
	 loss: 27.6486, MinusLogProbMetric: 27.6486, val_loss: 28.4563, val_MinusLogProbMetric: 28.4563

Epoch 409: val_loss did not improve from 28.32941
196/196 - 45s - loss: 27.6486 - MinusLogProbMetric: 27.6486 - val_loss: 28.4563 - val_MinusLogProbMetric: 28.4563 - lr: 2.5000e-04 - 45s/epoch - 228ms/step
Epoch 410/1000
2023-10-02 07:06:44.951 
Epoch 410/1000 
	 loss: 27.6649, MinusLogProbMetric: 27.6649, val_loss: 28.4062, val_MinusLogProbMetric: 28.4062

Epoch 410: val_loss did not improve from 28.32941
196/196 - 45s - loss: 27.6649 - MinusLogProbMetric: 27.6649 - val_loss: 28.4062 - val_MinusLogProbMetric: 28.4062 - lr: 2.5000e-04 - 45s/epoch - 228ms/step
Epoch 411/1000
2023-10-02 07:07:27.613 
Epoch 411/1000 
	 loss: 27.4488, MinusLogProbMetric: 27.4488, val_loss: 28.4729, val_MinusLogProbMetric: 28.4729

Epoch 411: val_loss did not improve from 28.32941
196/196 - 43s - loss: 27.4488 - MinusLogProbMetric: 27.4488 - val_loss: 28.4729 - val_MinusLogProbMetric: 28.4729 - lr: 2.5000e-04 - 43s/epoch - 218ms/step
Epoch 412/1000
2023-10-02 07:08:11.268 
Epoch 412/1000 
	 loss: 27.4653, MinusLogProbMetric: 27.4653, val_loss: 28.3531, val_MinusLogProbMetric: 28.3531

Epoch 412: val_loss did not improve from 28.32941
196/196 - 44s - loss: 27.4653 - MinusLogProbMetric: 27.4653 - val_loss: 28.3531 - val_MinusLogProbMetric: 28.3531 - lr: 2.5000e-04 - 44s/epoch - 223ms/step
Epoch 413/1000
2023-10-02 07:08:54.793 
Epoch 413/1000 
	 loss: 27.7701, MinusLogProbMetric: 27.7701, val_loss: 28.4002, val_MinusLogProbMetric: 28.4002

Epoch 413: val_loss did not improve from 28.32941
196/196 - 44s - loss: 27.7701 - MinusLogProbMetric: 27.7701 - val_loss: 28.4002 - val_MinusLogProbMetric: 28.4002 - lr: 2.5000e-04 - 44s/epoch - 222ms/step
Epoch 414/1000
2023-10-02 07:09:39.812 
Epoch 414/1000 
	 loss: 27.5156, MinusLogProbMetric: 27.5156, val_loss: 29.0245, val_MinusLogProbMetric: 29.0245

Epoch 414: val_loss did not improve from 28.32941
196/196 - 45s - loss: 27.5156 - MinusLogProbMetric: 27.5156 - val_loss: 29.0245 - val_MinusLogProbMetric: 29.0245 - lr: 2.5000e-04 - 45s/epoch - 230ms/step
Epoch 415/1000
2023-10-02 07:10:25.215 
Epoch 415/1000 
	 loss: 27.5628, MinusLogProbMetric: 27.5628, val_loss: 28.3338, val_MinusLogProbMetric: 28.3338

Epoch 415: val_loss did not improve from 28.32941
196/196 - 45s - loss: 27.5628 - MinusLogProbMetric: 27.5628 - val_loss: 28.3338 - val_MinusLogProbMetric: 28.3338 - lr: 2.5000e-04 - 45s/epoch - 232ms/step
Epoch 416/1000
2023-10-02 07:11:08.973 
Epoch 416/1000 
	 loss: 27.5734, MinusLogProbMetric: 27.5734, val_loss: 28.4534, val_MinusLogProbMetric: 28.4534

Epoch 416: val_loss did not improve from 28.32941
196/196 - 44s - loss: 27.5734 - MinusLogProbMetric: 27.5734 - val_loss: 28.4534 - val_MinusLogProbMetric: 28.4534 - lr: 2.5000e-04 - 44s/epoch - 223ms/step
Epoch 417/1000
2023-10-02 07:11:53.441 
Epoch 417/1000 
	 loss: 27.4431, MinusLogProbMetric: 27.4431, val_loss: 28.4077, val_MinusLogProbMetric: 28.4077

Epoch 417: val_loss did not improve from 28.32941
196/196 - 44s - loss: 27.4431 - MinusLogProbMetric: 27.4431 - val_loss: 28.4077 - val_MinusLogProbMetric: 28.4077 - lr: 2.5000e-04 - 44s/epoch - 227ms/step
Epoch 418/1000
2023-10-02 07:12:37.130 
Epoch 418/1000 
	 loss: 27.6502, MinusLogProbMetric: 27.6502, val_loss: 29.2705, val_MinusLogProbMetric: 29.2705

Epoch 418: val_loss did not improve from 28.32941
196/196 - 44s - loss: 27.6502 - MinusLogProbMetric: 27.6502 - val_loss: 29.2705 - val_MinusLogProbMetric: 29.2705 - lr: 2.5000e-04 - 44s/epoch - 223ms/step
Epoch 419/1000
2023-10-02 07:13:21.898 
Epoch 419/1000 
	 loss: 27.5310, MinusLogProbMetric: 27.5310, val_loss: 28.3580, val_MinusLogProbMetric: 28.3580

Epoch 419: val_loss did not improve from 28.32941
196/196 - 45s - loss: 27.5310 - MinusLogProbMetric: 27.5310 - val_loss: 28.3580 - val_MinusLogProbMetric: 28.3580 - lr: 2.5000e-04 - 45s/epoch - 228ms/step
Epoch 420/1000
2023-10-02 07:14:05.464 
Epoch 420/1000 
	 loss: 27.7225, MinusLogProbMetric: 27.7225, val_loss: 28.8077, val_MinusLogProbMetric: 28.8077

Epoch 420: val_loss did not improve from 28.32941
196/196 - 44s - loss: 27.7225 - MinusLogProbMetric: 27.7225 - val_loss: 28.8077 - val_MinusLogProbMetric: 28.8077 - lr: 2.5000e-04 - 44s/epoch - 222ms/step
Epoch 421/1000
2023-10-02 07:14:50.307 
Epoch 421/1000 
	 loss: 27.5838, MinusLogProbMetric: 27.5838, val_loss: 29.1475, val_MinusLogProbMetric: 29.1475

Epoch 421: val_loss did not improve from 28.32941
196/196 - 45s - loss: 27.5838 - MinusLogProbMetric: 27.5838 - val_loss: 29.1475 - val_MinusLogProbMetric: 29.1475 - lr: 2.5000e-04 - 45s/epoch - 229ms/step
Epoch 422/1000
2023-10-02 07:15:34.432 
Epoch 422/1000 
	 loss: 27.5258, MinusLogProbMetric: 27.5258, val_loss: 28.6681, val_MinusLogProbMetric: 28.6681

Epoch 422: val_loss did not improve from 28.32941
196/196 - 44s - loss: 27.5258 - MinusLogProbMetric: 27.5258 - val_loss: 28.6681 - val_MinusLogProbMetric: 28.6681 - lr: 2.5000e-04 - 44s/epoch - 225ms/step
Epoch 423/1000
2023-10-02 07:16:18.989 
Epoch 423/1000 
	 loss: 27.6843, MinusLogProbMetric: 27.6843, val_loss: 28.4676, val_MinusLogProbMetric: 28.4676

Epoch 423: val_loss did not improve from 28.32941
196/196 - 44s - loss: 27.6843 - MinusLogProbMetric: 27.6843 - val_loss: 28.4676 - val_MinusLogProbMetric: 28.4676 - lr: 2.5000e-04 - 44s/epoch - 227ms/step
Epoch 424/1000
2023-10-02 07:17:02.094 
Epoch 424/1000 
	 loss: 27.4822, MinusLogProbMetric: 27.4822, val_loss: 28.4304, val_MinusLogProbMetric: 28.4304

Epoch 424: val_loss did not improve from 28.32941
196/196 - 43s - loss: 27.4822 - MinusLogProbMetric: 27.4822 - val_loss: 28.4304 - val_MinusLogProbMetric: 28.4304 - lr: 2.5000e-04 - 43s/epoch - 220ms/step
Epoch 425/1000
2023-10-02 07:17:47.087 
Epoch 425/1000 
	 loss: 27.6855, MinusLogProbMetric: 27.6855, val_loss: 28.4637, val_MinusLogProbMetric: 28.4637

Epoch 425: val_loss did not improve from 28.32941
196/196 - 45s - loss: 27.6855 - MinusLogProbMetric: 27.6855 - val_loss: 28.4637 - val_MinusLogProbMetric: 28.4637 - lr: 2.5000e-04 - 45s/epoch - 230ms/step
Epoch 426/1000
2023-10-02 07:18:31.416 
Epoch 426/1000 
	 loss: 27.4774, MinusLogProbMetric: 27.4774, val_loss: 28.4252, val_MinusLogProbMetric: 28.4252

Epoch 426: val_loss did not improve from 28.32941
196/196 - 44s - loss: 27.4774 - MinusLogProbMetric: 27.4774 - val_loss: 28.4252 - val_MinusLogProbMetric: 28.4252 - lr: 2.5000e-04 - 44s/epoch - 226ms/step
Epoch 427/1000
2023-10-02 07:19:15.571 
Epoch 427/1000 
	 loss: 27.4777, MinusLogProbMetric: 27.4777, val_loss: 28.7497, val_MinusLogProbMetric: 28.7497

Epoch 427: val_loss did not improve from 28.32941
196/196 - 44s - loss: 27.4777 - MinusLogProbMetric: 27.4777 - val_loss: 28.7497 - val_MinusLogProbMetric: 28.7497 - lr: 2.5000e-04 - 44s/epoch - 225ms/step
Epoch 428/1000
2023-10-02 07:20:00.191 
Epoch 428/1000 
	 loss: 27.5840, MinusLogProbMetric: 27.5840, val_loss: 28.9562, val_MinusLogProbMetric: 28.9562

Epoch 428: val_loss did not improve from 28.32941
196/196 - 45s - loss: 27.5840 - MinusLogProbMetric: 27.5840 - val_loss: 28.9562 - val_MinusLogProbMetric: 28.9562 - lr: 2.5000e-04 - 45s/epoch - 228ms/step
Epoch 429/1000
2023-10-02 07:20:43.849 
Epoch 429/1000 
	 loss: 27.5459, MinusLogProbMetric: 27.5459, val_loss: 28.5177, val_MinusLogProbMetric: 28.5177

Epoch 429: val_loss did not improve from 28.32941
196/196 - 44s - loss: 27.5459 - MinusLogProbMetric: 27.5459 - val_loss: 28.5177 - val_MinusLogProbMetric: 28.5177 - lr: 2.5000e-04 - 44s/epoch - 223ms/step
Epoch 430/1000
2023-10-02 07:21:28.854 
Epoch 430/1000 
	 loss: 27.7632, MinusLogProbMetric: 27.7632, val_loss: 29.5792, val_MinusLogProbMetric: 29.5792

Epoch 430: val_loss did not improve from 28.32941
196/196 - 45s - loss: 27.7632 - MinusLogProbMetric: 27.7632 - val_loss: 29.5792 - val_MinusLogProbMetric: 29.5792 - lr: 2.5000e-04 - 45s/epoch - 230ms/step
Epoch 431/1000
2023-10-02 07:22:15.804 
Epoch 431/1000 
	 loss: 27.5317, MinusLogProbMetric: 27.5317, val_loss: 28.3841, val_MinusLogProbMetric: 28.3841

Epoch 431: val_loss did not improve from 28.32941
196/196 - 47s - loss: 27.5317 - MinusLogProbMetric: 27.5317 - val_loss: 28.3841 - val_MinusLogProbMetric: 28.3841 - lr: 2.5000e-04 - 47s/epoch - 240ms/step
Epoch 432/1000
2023-10-02 07:23:00.829 
Epoch 432/1000 
	 loss: 27.4535, MinusLogProbMetric: 27.4535, val_loss: 28.4400, val_MinusLogProbMetric: 28.4400

Epoch 432: val_loss did not improve from 28.32941
196/196 - 45s - loss: 27.4535 - MinusLogProbMetric: 27.4535 - val_loss: 28.4400 - val_MinusLogProbMetric: 28.4400 - lr: 2.5000e-04 - 45s/epoch - 230ms/step
Epoch 433/1000
2023-10-02 07:23:47.413 
Epoch 433/1000 
	 loss: 28.0061, MinusLogProbMetric: 28.0061, val_loss: 28.6898, val_MinusLogProbMetric: 28.6898

Epoch 433: val_loss did not improve from 28.32941
196/196 - 47s - loss: 28.0061 - MinusLogProbMetric: 28.0061 - val_loss: 28.6898 - val_MinusLogProbMetric: 28.6898 - lr: 2.5000e-04 - 47s/epoch - 238ms/step
Epoch 434/1000
2023-10-02 07:24:36.419 
Epoch 434/1000 
	 loss: 27.7602, MinusLogProbMetric: 27.7602, val_loss: 28.6639, val_MinusLogProbMetric: 28.6639

Epoch 434: val_loss did not improve from 28.32941
196/196 - 49s - loss: 27.7602 - MinusLogProbMetric: 27.7602 - val_loss: 28.6639 - val_MinusLogProbMetric: 28.6639 - lr: 2.5000e-04 - 49s/epoch - 250ms/step
Epoch 435/1000
2023-10-02 07:25:23.887 
Epoch 435/1000 
	 loss: 27.5215, MinusLogProbMetric: 27.5215, val_loss: 28.3821, val_MinusLogProbMetric: 28.3821

Epoch 435: val_loss did not improve from 28.32941
196/196 - 47s - loss: 27.5215 - MinusLogProbMetric: 27.5215 - val_loss: 28.3821 - val_MinusLogProbMetric: 28.3821 - lr: 2.5000e-04 - 47s/epoch - 242ms/step
Epoch 436/1000
2023-10-02 07:26:10.686 
Epoch 436/1000 
	 loss: 27.4663, MinusLogProbMetric: 27.4663, val_loss: 28.4763, val_MinusLogProbMetric: 28.4763

Epoch 436: val_loss did not improve from 28.32941
196/196 - 47s - loss: 27.4663 - MinusLogProbMetric: 27.4663 - val_loss: 28.4763 - val_MinusLogProbMetric: 28.4763 - lr: 2.5000e-04 - 47s/epoch - 239ms/step
Epoch 437/1000
2023-10-02 07:26:54.844 
Epoch 437/1000 
	 loss: 27.6619, MinusLogProbMetric: 27.6619, val_loss: 28.4864, val_MinusLogProbMetric: 28.4864

Epoch 437: val_loss did not improve from 28.32941
196/196 - 44s - loss: 27.6619 - MinusLogProbMetric: 27.6619 - val_loss: 28.4864 - val_MinusLogProbMetric: 28.4864 - lr: 2.5000e-04 - 44s/epoch - 225ms/step
Epoch 438/1000
2023-10-02 07:27:42.216 
Epoch 438/1000 
	 loss: 27.4300, MinusLogProbMetric: 27.4300, val_loss: 28.3920, val_MinusLogProbMetric: 28.3920

Epoch 438: val_loss did not improve from 28.32941
196/196 - 47s - loss: 27.4300 - MinusLogProbMetric: 27.4300 - val_loss: 28.3920 - val_MinusLogProbMetric: 28.3920 - lr: 2.5000e-04 - 47s/epoch - 242ms/step
Epoch 439/1000
2023-10-02 07:28:29.411 
Epoch 439/1000 
	 loss: 27.6537, MinusLogProbMetric: 27.6537, val_loss: 28.4272, val_MinusLogProbMetric: 28.4272

Epoch 439: val_loss did not improve from 28.32941
196/196 - 47s - loss: 27.6537 - MinusLogProbMetric: 27.6537 - val_loss: 28.4272 - val_MinusLogProbMetric: 28.4272 - lr: 2.5000e-04 - 47s/epoch - 241ms/step
Epoch 440/1000
2023-10-02 07:29:22.098 
Epoch 440/1000 
	 loss: 27.5748, MinusLogProbMetric: 27.5748, val_loss: 28.5783, val_MinusLogProbMetric: 28.5783

Epoch 440: val_loss did not improve from 28.32941
196/196 - 53s - loss: 27.5748 - MinusLogProbMetric: 27.5748 - val_loss: 28.5783 - val_MinusLogProbMetric: 28.5783 - lr: 2.5000e-04 - 53s/epoch - 269ms/step
Epoch 441/1000
2023-10-02 07:30:09.049 
Epoch 441/1000 
	 loss: 27.4770, MinusLogProbMetric: 27.4770, val_loss: 29.3488, val_MinusLogProbMetric: 29.3488

Epoch 441: val_loss did not improve from 28.32941
196/196 - 47s - loss: 27.4770 - MinusLogProbMetric: 27.4770 - val_loss: 29.3488 - val_MinusLogProbMetric: 29.3488 - lr: 2.5000e-04 - 47s/epoch - 240ms/step
Epoch 442/1000
2023-10-02 07:30:53.082 
Epoch 442/1000 
	 loss: 27.6886, MinusLogProbMetric: 27.6886, val_loss: 28.5522, val_MinusLogProbMetric: 28.5522

Epoch 442: val_loss did not improve from 28.32941
196/196 - 44s - loss: 27.6886 - MinusLogProbMetric: 27.6886 - val_loss: 28.5522 - val_MinusLogProbMetric: 28.5522 - lr: 2.5000e-04 - 44s/epoch - 225ms/step
Epoch 443/1000
2023-10-02 07:31:38.611 
Epoch 443/1000 
	 loss: 27.5737, MinusLogProbMetric: 27.5737, val_loss: 28.3982, val_MinusLogProbMetric: 28.3982

Epoch 443: val_loss did not improve from 28.32941
196/196 - 46s - loss: 27.5737 - MinusLogProbMetric: 27.5737 - val_loss: 28.3982 - val_MinusLogProbMetric: 28.3982 - lr: 2.5000e-04 - 46s/epoch - 232ms/step
Epoch 444/1000
2023-10-02 07:32:26.028 
Epoch 444/1000 
	 loss: 27.6341, MinusLogProbMetric: 27.6341, val_loss: 28.4846, val_MinusLogProbMetric: 28.4846

Epoch 444: val_loss did not improve from 28.32941
196/196 - 47s - loss: 27.6341 - MinusLogProbMetric: 27.6341 - val_loss: 28.4846 - val_MinusLogProbMetric: 28.4846 - lr: 2.5000e-04 - 47s/epoch - 242ms/step
Epoch 445/1000
2023-10-02 07:33:11.271 
Epoch 445/1000 
	 loss: 27.5433, MinusLogProbMetric: 27.5433, val_loss: 29.2677, val_MinusLogProbMetric: 29.2677

Epoch 445: val_loss did not improve from 28.32941
196/196 - 45s - loss: 27.5433 - MinusLogProbMetric: 27.5433 - val_loss: 29.2677 - val_MinusLogProbMetric: 29.2677 - lr: 2.5000e-04 - 45s/epoch - 231ms/step
Epoch 446/1000
2023-10-02 07:33:56.402 
Epoch 446/1000 
	 loss: 27.4952, MinusLogProbMetric: 27.4952, val_loss: 28.5813, val_MinusLogProbMetric: 28.5813

Epoch 446: val_loss did not improve from 28.32941
196/196 - 45s - loss: 27.4952 - MinusLogProbMetric: 27.4952 - val_loss: 28.5813 - val_MinusLogProbMetric: 28.5813 - lr: 2.5000e-04 - 45s/epoch - 230ms/step
Epoch 447/1000
2023-10-02 07:34:42.362 
Epoch 447/1000 
	 loss: 27.4891, MinusLogProbMetric: 27.4891, val_loss: 28.5145, val_MinusLogProbMetric: 28.5145

Epoch 447: val_loss did not improve from 28.32941
196/196 - 46s - loss: 27.4891 - MinusLogProbMetric: 27.4891 - val_loss: 28.5145 - val_MinusLogProbMetric: 28.5145 - lr: 2.5000e-04 - 46s/epoch - 234ms/step
Epoch 448/1000
2023-10-02 07:35:28.588 
Epoch 448/1000 
	 loss: 27.3272, MinusLogProbMetric: 27.3272, val_loss: 28.2922, val_MinusLogProbMetric: 28.2922

Epoch 448: val_loss improved from 28.32941 to 28.29218, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_354/weights/best_weights.h5
196/196 - 47s - loss: 27.3272 - MinusLogProbMetric: 27.3272 - val_loss: 28.2922 - val_MinusLogProbMetric: 28.2922 - lr: 1.2500e-04 - 47s/epoch - 240ms/step
Epoch 449/1000
2023-10-02 07:36:14.434 
Epoch 449/1000 
	 loss: 27.3098, MinusLogProbMetric: 27.3098, val_loss: 28.3388, val_MinusLogProbMetric: 28.3388

Epoch 449: val_loss did not improve from 28.29218
196/196 - 45s - loss: 27.3098 - MinusLogProbMetric: 27.3098 - val_loss: 28.3388 - val_MinusLogProbMetric: 28.3388 - lr: 1.2500e-04 - 45s/epoch - 230ms/step
Epoch 450/1000
2023-10-02 07:37:00.677 
Epoch 450/1000 
	 loss: 27.3075, MinusLogProbMetric: 27.3075, val_loss: 28.3013, val_MinusLogProbMetric: 28.3013

Epoch 450: val_loss did not improve from 28.29218
196/196 - 46s - loss: 27.3075 - MinusLogProbMetric: 27.3075 - val_loss: 28.3013 - val_MinusLogProbMetric: 28.3013 - lr: 1.2500e-04 - 46s/epoch - 236ms/step
Epoch 451/1000
2023-10-02 07:37:47.311 
Epoch 451/1000 
	 loss: 27.3007, MinusLogProbMetric: 27.3007, val_loss: 28.2707, val_MinusLogProbMetric: 28.2707

Epoch 451: val_loss improved from 28.29218 to 28.27071, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_354/weights/best_weights.h5
196/196 - 47s - loss: 27.3007 - MinusLogProbMetric: 27.3007 - val_loss: 28.2707 - val_MinusLogProbMetric: 28.2707 - lr: 1.2500e-04 - 47s/epoch - 241ms/step
Epoch 452/1000
2023-10-02 07:38:32.453 
Epoch 452/1000 
	 loss: 27.3175, MinusLogProbMetric: 27.3175, val_loss: 28.4615, val_MinusLogProbMetric: 28.4615

Epoch 452: val_loss did not improve from 28.27071
196/196 - 44s - loss: 27.3175 - MinusLogProbMetric: 27.3175 - val_loss: 28.4615 - val_MinusLogProbMetric: 28.4615 - lr: 1.2500e-04 - 44s/epoch - 227ms/step
Epoch 453/1000
2023-10-02 07:39:18.718 
Epoch 453/1000 
	 loss: 27.3378, MinusLogProbMetric: 27.3378, val_loss: 28.8346, val_MinusLogProbMetric: 28.8346

Epoch 453: val_loss did not improve from 28.27071
196/196 - 46s - loss: 27.3378 - MinusLogProbMetric: 27.3378 - val_loss: 28.8346 - val_MinusLogProbMetric: 28.8346 - lr: 1.2500e-04 - 46s/epoch - 236ms/step
Epoch 454/1000
2023-10-02 07:40:03.061 
Epoch 454/1000 
	 loss: 27.3285, MinusLogProbMetric: 27.3285, val_loss: 28.3250, val_MinusLogProbMetric: 28.3250

Epoch 454: val_loss did not improve from 28.27071
196/196 - 44s - loss: 27.3285 - MinusLogProbMetric: 27.3285 - val_loss: 28.3250 - val_MinusLogProbMetric: 28.3250 - lr: 1.2500e-04 - 44s/epoch - 226ms/step
Epoch 455/1000
2023-10-02 07:40:45.884 
Epoch 455/1000 
	 loss: 27.3562, MinusLogProbMetric: 27.3562, val_loss: 28.3682, val_MinusLogProbMetric: 28.3682

Epoch 455: val_loss did not improve from 28.27071
196/196 - 43s - loss: 27.3562 - MinusLogProbMetric: 27.3562 - val_loss: 28.3682 - val_MinusLogProbMetric: 28.3682 - lr: 1.2500e-04 - 43s/epoch - 219ms/step
Epoch 456/1000
2023-10-02 07:41:32.544 
Epoch 456/1000 
	 loss: 27.3056, MinusLogProbMetric: 27.3056, val_loss: 28.2956, val_MinusLogProbMetric: 28.2956

Epoch 456: val_loss did not improve from 28.27071
196/196 - 47s - loss: 27.3056 - MinusLogProbMetric: 27.3056 - val_loss: 28.2956 - val_MinusLogProbMetric: 28.2956 - lr: 1.2500e-04 - 47s/epoch - 238ms/step
Epoch 457/1000
2023-10-02 07:42:18.495 
Epoch 457/1000 
	 loss: 27.2944, MinusLogProbMetric: 27.2944, val_loss: 28.2676, val_MinusLogProbMetric: 28.2676

Epoch 457: val_loss improved from 28.27071 to 28.26759, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_354/weights/best_weights.h5
196/196 - 48s - loss: 27.2944 - MinusLogProbMetric: 27.2944 - val_loss: 28.2676 - val_MinusLogProbMetric: 28.2676 - lr: 1.2500e-04 - 48s/epoch - 244ms/step
Epoch 458/1000
2023-10-02 07:43:03.959 
Epoch 458/1000 
	 loss: 27.3064, MinusLogProbMetric: 27.3064, val_loss: 28.2783, val_MinusLogProbMetric: 28.2783

Epoch 458: val_loss did not improve from 28.26759
196/196 - 44s - loss: 27.3064 - MinusLogProbMetric: 27.3064 - val_loss: 28.2783 - val_MinusLogProbMetric: 28.2783 - lr: 1.2500e-04 - 44s/epoch - 223ms/step
Epoch 459/1000
2023-10-02 07:43:48.387 
Epoch 459/1000 
	 loss: 27.3096, MinusLogProbMetric: 27.3096, val_loss: 28.3154, val_MinusLogProbMetric: 28.3154

Epoch 459: val_loss did not improve from 28.26759
196/196 - 44s - loss: 27.3096 - MinusLogProbMetric: 27.3096 - val_loss: 28.3154 - val_MinusLogProbMetric: 28.3154 - lr: 1.2500e-04 - 44s/epoch - 227ms/step
Epoch 460/1000
2023-10-02 07:44:34.424 
Epoch 460/1000 
	 loss: 27.2984, MinusLogProbMetric: 27.2984, val_loss: 28.5085, val_MinusLogProbMetric: 28.5085

Epoch 460: val_loss did not improve from 28.26759
196/196 - 46s - loss: 27.2984 - MinusLogProbMetric: 27.2984 - val_loss: 28.5085 - val_MinusLogProbMetric: 28.5085 - lr: 1.2500e-04 - 46s/epoch - 235ms/step
Epoch 461/1000
2023-10-02 07:45:19.930 
Epoch 461/1000 
	 loss: 27.3096, MinusLogProbMetric: 27.3096, val_loss: 28.2587, val_MinusLogProbMetric: 28.2587

Epoch 461: val_loss improved from 28.26759 to 28.25871, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_354/weights/best_weights.h5
196/196 - 46s - loss: 27.3096 - MinusLogProbMetric: 27.3096 - val_loss: 28.2587 - val_MinusLogProbMetric: 28.2587 - lr: 1.2500e-04 - 46s/epoch - 235ms/step
Epoch 462/1000
2023-10-02 07:46:06.237 
Epoch 462/1000 
	 loss: 27.3045, MinusLogProbMetric: 27.3045, val_loss: 28.2556, val_MinusLogProbMetric: 28.2556

Epoch 462: val_loss improved from 28.25871 to 28.25558, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_354/weights/best_weights.h5
196/196 - 47s - loss: 27.3045 - MinusLogProbMetric: 27.3045 - val_loss: 28.2556 - val_MinusLogProbMetric: 28.2556 - lr: 1.2500e-04 - 47s/epoch - 238ms/step
Epoch 463/1000
2023-10-02 07:46:52.111 
Epoch 463/1000 
	 loss: 27.3029, MinusLogProbMetric: 27.3029, val_loss: 28.2809, val_MinusLogProbMetric: 28.2809

Epoch 463: val_loss did not improve from 28.25558
196/196 - 45s - loss: 27.3029 - MinusLogProbMetric: 27.3029 - val_loss: 28.2809 - val_MinusLogProbMetric: 28.2809 - lr: 1.2500e-04 - 45s/epoch - 229ms/step
Epoch 464/1000
2023-10-02 07:47:37.128 
Epoch 464/1000 
	 loss: 27.3293, MinusLogProbMetric: 27.3293, val_loss: 28.3281, val_MinusLogProbMetric: 28.3281

Epoch 464: val_loss did not improve from 28.25558
196/196 - 45s - loss: 27.3293 - MinusLogProbMetric: 27.3293 - val_loss: 28.3281 - val_MinusLogProbMetric: 28.3281 - lr: 1.2500e-04 - 45s/epoch - 230ms/step
Epoch 465/1000
2023-10-02 07:48:23.323 
Epoch 465/1000 
	 loss: 27.2939, MinusLogProbMetric: 27.2939, val_loss: 28.2837, val_MinusLogProbMetric: 28.2837

Epoch 465: val_loss did not improve from 28.25558
196/196 - 46s - loss: 27.2939 - MinusLogProbMetric: 27.2939 - val_loss: 28.2837 - val_MinusLogProbMetric: 28.2837 - lr: 1.2500e-04 - 46s/epoch - 236ms/step
Epoch 466/1000
2023-10-02 07:49:07.500 
Epoch 466/1000 
	 loss: 27.3041, MinusLogProbMetric: 27.3041, val_loss: 28.5650, val_MinusLogProbMetric: 28.5650

Epoch 466: val_loss did not improve from 28.25558
196/196 - 44s - loss: 27.3041 - MinusLogProbMetric: 27.3041 - val_loss: 28.5650 - val_MinusLogProbMetric: 28.5650 - lr: 1.2500e-04 - 44s/epoch - 225ms/step
Epoch 467/1000
2023-10-02 07:49:52.245 
Epoch 467/1000 
	 loss: 27.7045, MinusLogProbMetric: 27.7045, val_loss: 28.4555, val_MinusLogProbMetric: 28.4555

Epoch 467: val_loss did not improve from 28.25558
196/196 - 45s - loss: 27.7045 - MinusLogProbMetric: 27.7045 - val_loss: 28.4555 - val_MinusLogProbMetric: 28.4555 - lr: 1.2500e-04 - 45s/epoch - 228ms/step
Epoch 468/1000
2023-10-02 07:50:36.838 
Epoch 468/1000 
	 loss: 27.7664, MinusLogProbMetric: 27.7664, val_loss: 28.5265, val_MinusLogProbMetric: 28.5265

Epoch 468: val_loss did not improve from 28.25558
196/196 - 45s - loss: 27.7664 - MinusLogProbMetric: 27.7664 - val_loss: 28.5265 - val_MinusLogProbMetric: 28.5265 - lr: 1.2500e-04 - 45s/epoch - 228ms/step
Epoch 469/1000
2023-10-02 07:51:21.344 
Epoch 469/1000 
	 loss: 27.3560, MinusLogProbMetric: 27.3560, val_loss: 28.2758, val_MinusLogProbMetric: 28.2758

Epoch 469: val_loss did not improve from 28.25558
196/196 - 44s - loss: 27.3560 - MinusLogProbMetric: 27.3560 - val_loss: 28.2758 - val_MinusLogProbMetric: 28.2758 - lr: 1.2500e-04 - 44s/epoch - 227ms/step
Epoch 470/1000
2023-10-02 07:52:04.459 
Epoch 470/1000 
	 loss: 27.3393, MinusLogProbMetric: 27.3393, val_loss: 28.3217, val_MinusLogProbMetric: 28.3217

Epoch 470: val_loss did not improve from 28.25558
196/196 - 43s - loss: 27.3393 - MinusLogProbMetric: 27.3393 - val_loss: 28.3217 - val_MinusLogProbMetric: 28.3217 - lr: 1.2500e-04 - 43s/epoch - 220ms/step
Epoch 471/1000
2023-10-02 07:52:50.104 
Epoch 471/1000 
	 loss: 27.2887, MinusLogProbMetric: 27.2887, val_loss: 28.2901, val_MinusLogProbMetric: 28.2901

Epoch 471: val_loss did not improve from 28.25558
196/196 - 46s - loss: 27.2887 - MinusLogProbMetric: 27.2887 - val_loss: 28.2901 - val_MinusLogProbMetric: 28.2901 - lr: 1.2500e-04 - 46s/epoch - 233ms/step
Epoch 472/1000
2023-10-02 07:53:34.355 
Epoch 472/1000 
	 loss: 27.3006, MinusLogProbMetric: 27.3006, val_loss: 28.4965, val_MinusLogProbMetric: 28.4965

Epoch 472: val_loss did not improve from 28.25558
196/196 - 44s - loss: 27.3006 - MinusLogProbMetric: 27.3006 - val_loss: 28.4965 - val_MinusLogProbMetric: 28.4965 - lr: 1.2500e-04 - 44s/epoch - 226ms/step
Epoch 473/1000
2023-10-02 07:54:17.509 
Epoch 473/1000 
	 loss: 27.2826, MinusLogProbMetric: 27.2826, val_loss: 28.4549, val_MinusLogProbMetric: 28.4549

Epoch 473: val_loss did not improve from 28.25558
196/196 - 43s - loss: 27.2826 - MinusLogProbMetric: 27.2826 - val_loss: 28.4549 - val_MinusLogProbMetric: 28.4549 - lr: 1.2500e-04 - 43s/epoch - 220ms/step
Epoch 474/1000
2023-10-02 07:55:03.362 
Epoch 474/1000 
	 loss: 27.3002, MinusLogProbMetric: 27.3002, val_loss: 28.2695, val_MinusLogProbMetric: 28.2695

Epoch 474: val_loss did not improve from 28.25558
196/196 - 46s - loss: 27.3002 - MinusLogProbMetric: 27.3002 - val_loss: 28.2695 - val_MinusLogProbMetric: 28.2695 - lr: 1.2500e-04 - 46s/epoch - 234ms/step
Epoch 475/1000
2023-10-02 07:55:48.036 
Epoch 475/1000 
	 loss: 27.3169, MinusLogProbMetric: 27.3169, val_loss: 28.2575, val_MinusLogProbMetric: 28.2575

Epoch 475: val_loss did not improve from 28.25558
196/196 - 45s - loss: 27.3169 - MinusLogProbMetric: 27.3169 - val_loss: 28.2575 - val_MinusLogProbMetric: 28.2575 - lr: 1.2500e-04 - 45s/epoch - 228ms/step
Epoch 476/1000
2023-10-02 07:56:35.904 
Epoch 476/1000 
	 loss: 27.4035, MinusLogProbMetric: 27.4035, val_loss: 28.2776, val_MinusLogProbMetric: 28.2776

Epoch 476: val_loss did not improve from 28.25558
196/196 - 48s - loss: 27.4035 - MinusLogProbMetric: 27.4035 - val_loss: 28.2776 - val_MinusLogProbMetric: 28.2776 - lr: 1.2500e-04 - 48s/epoch - 244ms/step
Epoch 477/1000
2023-10-02 07:57:20.393 
Epoch 477/1000 
	 loss: 27.2811, MinusLogProbMetric: 27.2811, val_loss: 28.3727, val_MinusLogProbMetric: 28.3727

Epoch 477: val_loss did not improve from 28.25558
196/196 - 45s - loss: 27.2811 - MinusLogProbMetric: 27.2811 - val_loss: 28.3727 - val_MinusLogProbMetric: 28.3727 - lr: 1.2500e-04 - 45s/epoch - 227ms/step
Epoch 478/1000
2023-10-02 07:58:04.851 
Epoch 478/1000 
	 loss: 27.3401, MinusLogProbMetric: 27.3401, val_loss: 28.7164, val_MinusLogProbMetric: 28.7164

Epoch 478: val_loss did not improve from 28.25558
196/196 - 44s - loss: 27.3401 - MinusLogProbMetric: 27.3401 - val_loss: 28.7164 - val_MinusLogProbMetric: 28.7164 - lr: 1.2500e-04 - 44s/epoch - 227ms/step
Epoch 479/1000
2023-10-02 07:58:49.721 
Epoch 479/1000 
	 loss: 27.4214, MinusLogProbMetric: 27.4214, val_loss: 28.3177, val_MinusLogProbMetric: 28.3177

Epoch 479: val_loss did not improve from 28.25558
196/196 - 45s - loss: 27.4214 - MinusLogProbMetric: 27.4214 - val_loss: 28.3177 - val_MinusLogProbMetric: 28.3177 - lr: 1.2500e-04 - 45s/epoch - 229ms/step
Epoch 480/1000
2023-10-02 07:59:36.117 
Epoch 480/1000 
	 loss: 27.3114, MinusLogProbMetric: 27.3114, val_loss: 28.2649, val_MinusLogProbMetric: 28.2649

Epoch 480: val_loss did not improve from 28.25558
196/196 - 46s - loss: 27.3114 - MinusLogProbMetric: 27.3114 - val_loss: 28.2649 - val_MinusLogProbMetric: 28.2649 - lr: 1.2500e-04 - 46s/epoch - 237ms/step
Epoch 481/1000
2023-10-02 08:00:19.697 
Epoch 481/1000 
	 loss: 27.3040, MinusLogProbMetric: 27.3040, val_loss: 28.4337, val_MinusLogProbMetric: 28.4337

Epoch 481: val_loss did not improve from 28.25558
196/196 - 44s - loss: 27.3040 - MinusLogProbMetric: 27.3040 - val_loss: 28.4337 - val_MinusLogProbMetric: 28.4337 - lr: 1.2500e-04 - 44s/epoch - 222ms/step
Epoch 482/1000
2023-10-02 08:01:07.155 
Epoch 482/1000 
	 loss: 27.3331, MinusLogProbMetric: 27.3331, val_loss: 28.3609, val_MinusLogProbMetric: 28.3609

Epoch 482: val_loss did not improve from 28.25558
196/196 - 47s - loss: 27.3331 - MinusLogProbMetric: 27.3331 - val_loss: 28.3609 - val_MinusLogProbMetric: 28.3609 - lr: 1.2500e-04 - 47s/epoch - 242ms/step
Epoch 483/1000
2023-10-02 08:01:51.854 
Epoch 483/1000 
	 loss: 27.2843, MinusLogProbMetric: 27.2843, val_loss: 28.4842, val_MinusLogProbMetric: 28.4842

Epoch 483: val_loss did not improve from 28.25558
196/196 - 45s - loss: 27.2843 - MinusLogProbMetric: 27.2843 - val_loss: 28.4842 - val_MinusLogProbMetric: 28.4842 - lr: 1.2500e-04 - 45s/epoch - 228ms/step
Epoch 484/1000
2023-10-02 08:02:37.832 
Epoch 484/1000 
	 loss: 27.3312, MinusLogProbMetric: 27.3312, val_loss: 28.2464, val_MinusLogProbMetric: 28.2464

Epoch 484: val_loss improved from 28.25558 to 28.24639, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_354/weights/best_weights.h5
196/196 - 47s - loss: 27.3312 - MinusLogProbMetric: 27.3312 - val_loss: 28.2464 - val_MinusLogProbMetric: 28.2464 - lr: 1.2500e-04 - 47s/epoch - 240ms/step
Epoch 485/1000
2023-10-02 08:03:23.460 
Epoch 485/1000 
	 loss: 27.3362, MinusLogProbMetric: 27.3362, val_loss: 28.3089, val_MinusLogProbMetric: 28.3089

Epoch 485: val_loss did not improve from 28.24639
196/196 - 45s - loss: 27.3362 - MinusLogProbMetric: 27.3362 - val_loss: 28.3089 - val_MinusLogProbMetric: 28.3089 - lr: 1.2500e-04 - 45s/epoch - 228ms/step
Epoch 486/1000
2023-10-02 08:04:09.343 
Epoch 486/1000 
	 loss: 27.2931, MinusLogProbMetric: 27.2931, val_loss: 28.2512, val_MinusLogProbMetric: 28.2512

Epoch 486: val_loss did not improve from 28.24639
196/196 - 46s - loss: 27.2931 - MinusLogProbMetric: 27.2931 - val_loss: 28.2512 - val_MinusLogProbMetric: 28.2512 - lr: 1.2500e-04 - 46s/epoch - 234ms/step
Epoch 487/1000
2023-10-02 08:04:54.548 
Epoch 487/1000 
	 loss: 27.3494, MinusLogProbMetric: 27.3494, val_loss: 28.3196, val_MinusLogProbMetric: 28.3196

Epoch 487: val_loss did not improve from 28.24639
196/196 - 45s - loss: 27.3494 - MinusLogProbMetric: 27.3494 - val_loss: 28.3196 - val_MinusLogProbMetric: 28.3196 - lr: 1.2500e-04 - 45s/epoch - 231ms/step
Epoch 488/1000
2023-10-02 08:05:39.316 
Epoch 488/1000 
	 loss: 27.2858, MinusLogProbMetric: 27.2858, val_loss: 28.2999, val_MinusLogProbMetric: 28.2999

Epoch 488: val_loss did not improve from 28.24639
196/196 - 45s - loss: 27.2858 - MinusLogProbMetric: 27.2858 - val_loss: 28.2999 - val_MinusLogProbMetric: 28.2999 - lr: 1.2500e-04 - 45s/epoch - 228ms/step
Epoch 489/1000
2023-10-02 08:06:23.786 
Epoch 489/1000 
	 loss: 27.3351, MinusLogProbMetric: 27.3351, val_loss: 28.2452, val_MinusLogProbMetric: 28.2452

Epoch 489: val_loss improved from 28.24639 to 28.24521, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_354/weights/best_weights.h5
196/196 - 46s - loss: 27.3351 - MinusLogProbMetric: 27.3351 - val_loss: 28.2452 - val_MinusLogProbMetric: 28.2452 - lr: 1.2500e-04 - 46s/epoch - 234ms/step
Epoch 490/1000
2023-10-02 08:07:13.612 
Epoch 490/1000 
	 loss: 27.3130, MinusLogProbMetric: 27.3130, val_loss: 29.6774, val_MinusLogProbMetric: 29.6774

Epoch 490: val_loss did not improve from 28.24521
196/196 - 49s - loss: 27.3130 - MinusLogProbMetric: 27.3130 - val_loss: 29.6774 - val_MinusLogProbMetric: 29.6774 - lr: 1.2500e-04 - 49s/epoch - 248ms/step
Epoch 491/1000
2023-10-02 08:08:00.625 
Epoch 491/1000 
	 loss: 27.4251, MinusLogProbMetric: 27.4251, val_loss: 28.2825, val_MinusLogProbMetric: 28.2825

Epoch 491: val_loss did not improve from 28.24521
196/196 - 47s - loss: 27.4251 - MinusLogProbMetric: 27.4251 - val_loss: 28.2825 - val_MinusLogProbMetric: 28.2825 - lr: 1.2500e-04 - 47s/epoch - 240ms/step
Epoch 492/1000
2023-10-02 08:08:46.872 
Epoch 492/1000 
	 loss: 27.2803, MinusLogProbMetric: 27.2803, val_loss: 28.3373, val_MinusLogProbMetric: 28.3373

Epoch 492: val_loss did not improve from 28.24521
196/196 - 46s - loss: 27.2803 - MinusLogProbMetric: 27.2803 - val_loss: 28.3373 - val_MinusLogProbMetric: 28.3373 - lr: 1.2500e-04 - 46s/epoch - 236ms/step
Epoch 493/1000
2023-10-02 08:09:33.063 
Epoch 493/1000 
	 loss: 27.2777, MinusLogProbMetric: 27.2777, val_loss: 28.2653, val_MinusLogProbMetric: 28.2653

Epoch 493: val_loss did not improve from 28.24521
196/196 - 46s - loss: 27.2777 - MinusLogProbMetric: 27.2777 - val_loss: 28.2653 - val_MinusLogProbMetric: 28.2653 - lr: 1.2500e-04 - 46s/epoch - 235ms/step
Epoch 494/1000
2023-10-02 08:10:18.409 
Epoch 494/1000 
	 loss: 27.3176, MinusLogProbMetric: 27.3176, val_loss: 28.3563, val_MinusLogProbMetric: 28.3563

Epoch 494: val_loss did not improve from 28.24521
196/196 - 45s - loss: 27.3176 - MinusLogProbMetric: 27.3176 - val_loss: 28.3563 - val_MinusLogProbMetric: 28.3563 - lr: 1.2500e-04 - 45s/epoch - 231ms/step
Epoch 495/1000
2023-10-02 08:11:05.372 
Epoch 495/1000 
	 loss: 27.3102, MinusLogProbMetric: 27.3102, val_loss: 28.3873, val_MinusLogProbMetric: 28.3873

Epoch 495: val_loss did not improve from 28.24521
196/196 - 47s - loss: 27.3102 - MinusLogProbMetric: 27.3102 - val_loss: 28.3873 - val_MinusLogProbMetric: 28.3873 - lr: 1.2500e-04 - 47s/epoch - 240ms/step
Epoch 496/1000
2023-10-02 08:11:52.769 
Epoch 496/1000 
	 loss: 27.2762, MinusLogProbMetric: 27.2762, val_loss: 28.4047, val_MinusLogProbMetric: 28.4047

Epoch 496: val_loss did not improve from 28.24521
196/196 - 47s - loss: 27.2762 - MinusLogProbMetric: 27.2762 - val_loss: 28.4047 - val_MinusLogProbMetric: 28.4047 - lr: 1.2500e-04 - 47s/epoch - 242ms/step
Epoch 497/1000
2023-10-02 08:12:39.647 
Epoch 497/1000 
	 loss: 27.2746, MinusLogProbMetric: 27.2746, val_loss: 28.2477, val_MinusLogProbMetric: 28.2477

Epoch 497: val_loss did not improve from 28.24521
196/196 - 47s - loss: 27.2746 - MinusLogProbMetric: 27.2746 - val_loss: 28.2477 - val_MinusLogProbMetric: 28.2477 - lr: 1.2500e-04 - 47s/epoch - 239ms/step
Epoch 498/1000
2023-10-02 08:13:26.776 
Epoch 498/1000 
	 loss: 27.2808, MinusLogProbMetric: 27.2808, val_loss: 28.3016, val_MinusLogProbMetric: 28.3016

Epoch 498: val_loss did not improve from 28.24521
196/196 - 47s - loss: 27.2808 - MinusLogProbMetric: 27.2808 - val_loss: 28.3016 - val_MinusLogProbMetric: 28.3016 - lr: 1.2500e-04 - 47s/epoch - 240ms/step
Epoch 499/1000
2023-10-02 08:14:12.998 
Epoch 499/1000 
	 loss: 27.2963, MinusLogProbMetric: 27.2963, val_loss: 28.2627, val_MinusLogProbMetric: 28.2627

Epoch 499: val_loss did not improve from 28.24521
196/196 - 46s - loss: 27.2963 - MinusLogProbMetric: 27.2963 - val_loss: 28.2627 - val_MinusLogProbMetric: 28.2627 - lr: 1.2500e-04 - 46s/epoch - 236ms/step
Epoch 500/1000
2023-10-02 08:14:59.378 
Epoch 500/1000 
	 loss: 27.2773, MinusLogProbMetric: 27.2773, val_loss: 28.3003, val_MinusLogProbMetric: 28.3003

Epoch 500: val_loss did not improve from 28.24521
196/196 - 46s - loss: 27.2773 - MinusLogProbMetric: 27.2773 - val_loss: 28.3003 - val_MinusLogProbMetric: 28.3003 - lr: 1.2500e-04 - 46s/epoch - 237ms/step
Epoch 501/1000
2023-10-02 08:15:45.899 
Epoch 501/1000 
	 loss: 27.3203, MinusLogProbMetric: 27.3203, val_loss: 28.3821, val_MinusLogProbMetric: 28.3821

Epoch 501: val_loss did not improve from 28.24521
196/196 - 47s - loss: 27.3203 - MinusLogProbMetric: 27.3203 - val_loss: 28.3821 - val_MinusLogProbMetric: 28.3821 - lr: 1.2500e-04 - 47s/epoch - 237ms/step
Epoch 502/1000
2023-10-02 08:16:32.993 
Epoch 502/1000 
	 loss: 27.3077, MinusLogProbMetric: 27.3077, val_loss: 28.2928, val_MinusLogProbMetric: 28.2928

Epoch 502: val_loss did not improve from 28.24521
196/196 - 47s - loss: 27.3077 - MinusLogProbMetric: 27.3077 - val_loss: 28.2928 - val_MinusLogProbMetric: 28.2928 - lr: 1.2500e-04 - 47s/epoch - 240ms/step
Epoch 503/1000
2023-10-02 08:17:18.293 
Epoch 503/1000 
	 loss: 27.4130, MinusLogProbMetric: 27.4130, val_loss: 28.4581, val_MinusLogProbMetric: 28.4581

Epoch 503: val_loss did not improve from 28.24521
196/196 - 45s - loss: 27.4130 - MinusLogProbMetric: 27.4130 - val_loss: 28.4581 - val_MinusLogProbMetric: 28.4581 - lr: 1.2500e-04 - 45s/epoch - 231ms/step
Epoch 504/1000
2023-10-02 08:18:05.461 
Epoch 504/1000 
	 loss: 27.3245, MinusLogProbMetric: 27.3245, val_loss: 28.3374, val_MinusLogProbMetric: 28.3374

Epoch 504: val_loss did not improve from 28.24521
196/196 - 47s - loss: 27.3245 - MinusLogProbMetric: 27.3245 - val_loss: 28.3374 - val_MinusLogProbMetric: 28.3374 - lr: 1.2500e-04 - 47s/epoch - 241ms/step
Epoch 505/1000
2023-10-02 08:18:50.842 
Epoch 505/1000 
	 loss: 27.2825, MinusLogProbMetric: 27.2825, val_loss: 28.3371, val_MinusLogProbMetric: 28.3371

Epoch 505: val_loss did not improve from 28.24521
196/196 - 45s - loss: 27.2825 - MinusLogProbMetric: 27.2825 - val_loss: 28.3371 - val_MinusLogProbMetric: 28.3371 - lr: 1.2500e-04 - 45s/epoch - 231ms/step
Epoch 506/1000
2023-10-02 08:19:39.913 
Epoch 506/1000 
	 loss: 27.3385, MinusLogProbMetric: 27.3385, val_loss: 28.2847, val_MinusLogProbMetric: 28.2847

Epoch 506: val_loss did not improve from 28.24521
196/196 - 49s - loss: 27.3385 - MinusLogProbMetric: 27.3385 - val_loss: 28.2847 - val_MinusLogProbMetric: 28.2847 - lr: 1.2500e-04 - 49s/epoch - 251ms/step
Epoch 507/1000
2023-10-02 08:20:25.477 
Epoch 507/1000 
	 loss: 27.2863, MinusLogProbMetric: 27.2863, val_loss: 28.2843, val_MinusLogProbMetric: 28.2843

Epoch 507: val_loss did not improve from 28.24521
196/196 - 46s - loss: 27.2863 - MinusLogProbMetric: 27.2863 - val_loss: 28.2843 - val_MinusLogProbMetric: 28.2843 - lr: 1.2500e-04 - 46s/epoch - 232ms/step
Epoch 508/1000
2023-10-02 08:21:13.147 
Epoch 508/1000 
	 loss: 27.3840, MinusLogProbMetric: 27.3840, val_loss: 28.2780, val_MinusLogProbMetric: 28.2780

Epoch 508: val_loss did not improve from 28.24521
196/196 - 48s - loss: 27.3840 - MinusLogProbMetric: 27.3840 - val_loss: 28.2780 - val_MinusLogProbMetric: 28.2780 - lr: 1.2500e-04 - 48s/epoch - 243ms/step
Epoch 509/1000
2023-10-02 08:21:59.512 
Epoch 509/1000 
	 loss: 27.2981, MinusLogProbMetric: 27.2981, val_loss: 28.3505, val_MinusLogProbMetric: 28.3505

Epoch 509: val_loss did not improve from 28.24521
196/196 - 46s - loss: 27.2981 - MinusLogProbMetric: 27.2981 - val_loss: 28.3505 - val_MinusLogProbMetric: 28.3505 - lr: 1.2500e-04 - 46s/epoch - 237ms/step
Epoch 510/1000
2023-10-02 08:22:44.772 
Epoch 510/1000 
	 loss: 27.3018, MinusLogProbMetric: 27.3018, val_loss: 28.2928, val_MinusLogProbMetric: 28.2928

Epoch 510: val_loss did not improve from 28.24521
196/196 - 45s - loss: 27.3018 - MinusLogProbMetric: 27.3018 - val_loss: 28.2928 - val_MinusLogProbMetric: 28.2928 - lr: 1.2500e-04 - 45s/epoch - 231ms/step
Epoch 511/1000
2023-10-02 08:23:26.928 
Epoch 511/1000 
	 loss: 27.4511, MinusLogProbMetric: 27.4511, val_loss: 28.5071, val_MinusLogProbMetric: 28.5071

Epoch 511: val_loss did not improve from 28.24521
196/196 - 42s - loss: 27.4511 - MinusLogProbMetric: 27.4511 - val_loss: 28.5071 - val_MinusLogProbMetric: 28.5071 - lr: 1.2500e-04 - 42s/epoch - 215ms/step
Epoch 512/1000
2023-10-02 08:24:12.148 
Epoch 512/1000 
	 loss: 27.3274, MinusLogProbMetric: 27.3274, val_loss: 28.2803, val_MinusLogProbMetric: 28.2803

Epoch 512: val_loss did not improve from 28.24521
196/196 - 45s - loss: 27.3274 - MinusLogProbMetric: 27.3274 - val_loss: 28.2803 - val_MinusLogProbMetric: 28.2803 - lr: 1.2500e-04 - 45s/epoch - 231ms/step
Epoch 513/1000
2023-10-02 08:24:58.078 
Epoch 513/1000 
	 loss: 27.2878, MinusLogProbMetric: 27.2878, val_loss: 28.3520, val_MinusLogProbMetric: 28.3520

Epoch 513: val_loss did not improve from 28.24521
196/196 - 46s - loss: 27.2878 - MinusLogProbMetric: 27.2878 - val_loss: 28.3520 - val_MinusLogProbMetric: 28.3520 - lr: 1.2500e-04 - 46s/epoch - 234ms/step
Epoch 514/1000
2023-10-02 08:25:41.349 
Epoch 514/1000 
	 loss: 27.2693, MinusLogProbMetric: 27.2693, val_loss: 28.2706, val_MinusLogProbMetric: 28.2706

Epoch 514: val_loss did not improve from 28.24521
196/196 - 43s - loss: 27.2693 - MinusLogProbMetric: 27.2693 - val_loss: 28.2706 - val_MinusLogProbMetric: 28.2706 - lr: 1.2500e-04 - 43s/epoch - 221ms/step
Epoch 515/1000
2023-10-02 08:26:25.588 
Epoch 515/1000 
	 loss: 27.3145, MinusLogProbMetric: 27.3145, val_loss: 28.3361, val_MinusLogProbMetric: 28.3361

Epoch 515: val_loss did not improve from 28.24521
196/196 - 44s - loss: 27.3145 - MinusLogProbMetric: 27.3145 - val_loss: 28.3361 - val_MinusLogProbMetric: 28.3361 - lr: 1.2500e-04 - 44s/epoch - 226ms/step
Epoch 516/1000
2023-10-02 08:27:14.652 
Epoch 516/1000 
	 loss: 27.3098, MinusLogProbMetric: 27.3098, val_loss: 28.2631, val_MinusLogProbMetric: 28.2631

Epoch 516: val_loss did not improve from 28.24521
196/196 - 49s - loss: 27.3098 - MinusLogProbMetric: 27.3098 - val_loss: 28.2631 - val_MinusLogProbMetric: 28.2631 - lr: 1.2500e-04 - 49s/epoch - 250ms/step
Epoch 517/1000
2023-10-02 08:28:02.897 
Epoch 517/1000 
	 loss: 27.2758, MinusLogProbMetric: 27.2758, val_loss: 28.2793, val_MinusLogProbMetric: 28.2793

Epoch 517: val_loss did not improve from 28.24521
196/196 - 48s - loss: 27.2758 - MinusLogProbMetric: 27.2758 - val_loss: 28.2793 - val_MinusLogProbMetric: 28.2793 - lr: 1.2500e-04 - 48s/epoch - 246ms/step
Epoch 518/1000
2023-10-02 08:28:46.070 
Epoch 518/1000 
	 loss: 27.2722, MinusLogProbMetric: 27.2722, val_loss: 28.3294, val_MinusLogProbMetric: 28.3294

Epoch 518: val_loss did not improve from 28.24521
196/196 - 43s - loss: 27.2722 - MinusLogProbMetric: 27.2722 - val_loss: 28.3294 - val_MinusLogProbMetric: 28.3294 - lr: 1.2500e-04 - 43s/epoch - 220ms/step
Epoch 519/1000
2023-10-02 08:29:31.194 
Epoch 519/1000 
	 loss: 27.3173, MinusLogProbMetric: 27.3173, val_loss: 28.2821, val_MinusLogProbMetric: 28.2821

Epoch 519: val_loss did not improve from 28.24521
196/196 - 45s - loss: 27.3173 - MinusLogProbMetric: 27.3173 - val_loss: 28.2821 - val_MinusLogProbMetric: 28.2821 - lr: 1.2500e-04 - 45s/epoch - 230ms/step
Epoch 520/1000
2023-10-02 08:30:15.439 
Epoch 520/1000 
	 loss: 27.3482, MinusLogProbMetric: 27.3482, val_loss: 28.2769, val_MinusLogProbMetric: 28.2769

Epoch 520: val_loss did not improve from 28.24521
196/196 - 44s - loss: 27.3482 - MinusLogProbMetric: 27.3482 - val_loss: 28.2769 - val_MinusLogProbMetric: 28.2769 - lr: 1.2500e-04 - 44s/epoch - 226ms/step
Epoch 521/1000
2023-10-02 08:30:59.436 
Epoch 521/1000 
	 loss: 27.4029, MinusLogProbMetric: 27.4029, val_loss: 28.3650, val_MinusLogProbMetric: 28.3650

Epoch 521: val_loss did not improve from 28.24521
196/196 - 44s - loss: 27.4029 - MinusLogProbMetric: 27.4029 - val_loss: 28.3650 - val_MinusLogProbMetric: 28.3650 - lr: 1.2500e-04 - 44s/epoch - 224ms/step
Epoch 522/1000
2023-10-02 08:31:42.319 
Epoch 522/1000 
	 loss: 27.2617, MinusLogProbMetric: 27.2617, val_loss: 28.2952, val_MinusLogProbMetric: 28.2952

Epoch 522: val_loss did not improve from 28.24521
196/196 - 43s - loss: 27.2617 - MinusLogProbMetric: 27.2617 - val_loss: 28.2952 - val_MinusLogProbMetric: 28.2952 - lr: 1.2500e-04 - 43s/epoch - 219ms/step
Epoch 523/1000
2023-10-02 08:32:25.068 
Epoch 523/1000 
	 loss: 27.3317, MinusLogProbMetric: 27.3317, val_loss: 28.2980, val_MinusLogProbMetric: 28.2980

Epoch 523: val_loss did not improve from 28.24521
196/196 - 43s - loss: 27.3317 - MinusLogProbMetric: 27.3317 - val_loss: 28.2980 - val_MinusLogProbMetric: 28.2980 - lr: 1.2500e-04 - 43s/epoch - 218ms/step
Epoch 524/1000
2023-10-02 08:33:09.071 
Epoch 524/1000 
	 loss: 27.2745, MinusLogProbMetric: 27.2745, val_loss: 28.2613, val_MinusLogProbMetric: 28.2613

Epoch 524: val_loss did not improve from 28.24521
196/196 - 44s - loss: 27.2745 - MinusLogProbMetric: 27.2745 - val_loss: 28.2613 - val_MinusLogProbMetric: 28.2613 - lr: 1.2500e-04 - 44s/epoch - 224ms/step
Epoch 525/1000
2023-10-02 08:33:50.872 
Epoch 525/1000 
	 loss: 27.3371, MinusLogProbMetric: 27.3371, val_loss: 28.2849, val_MinusLogProbMetric: 28.2849

Epoch 525: val_loss did not improve from 28.24521
196/196 - 42s - loss: 27.3371 - MinusLogProbMetric: 27.3371 - val_loss: 28.2849 - val_MinusLogProbMetric: 28.2849 - lr: 1.2500e-04 - 42s/epoch - 213ms/step
Epoch 526/1000
2023-10-02 08:34:38.454 
Epoch 526/1000 
	 loss: 27.2651, MinusLogProbMetric: 27.2651, val_loss: 28.3254, val_MinusLogProbMetric: 28.3254

Epoch 526: val_loss did not improve from 28.24521
196/196 - 48s - loss: 27.2651 - MinusLogProbMetric: 27.2651 - val_loss: 28.3254 - val_MinusLogProbMetric: 28.3254 - lr: 1.2500e-04 - 48s/epoch - 243ms/step
Epoch 527/1000
2023-10-02 08:35:25.470 
Epoch 527/1000 
	 loss: 27.2657, MinusLogProbMetric: 27.2657, val_loss: 28.2547, val_MinusLogProbMetric: 28.2547

Epoch 527: val_loss did not improve from 28.24521
196/196 - 47s - loss: 27.2657 - MinusLogProbMetric: 27.2657 - val_loss: 28.2547 - val_MinusLogProbMetric: 28.2547 - lr: 1.2500e-04 - 47s/epoch - 240ms/step
Epoch 528/1000
2023-10-02 08:36:09.100 
Epoch 528/1000 
	 loss: 27.2757, MinusLogProbMetric: 27.2757, val_loss: 28.4296, val_MinusLogProbMetric: 28.4296

Epoch 528: val_loss did not improve from 28.24521
196/196 - 44s - loss: 27.2757 - MinusLogProbMetric: 27.2757 - val_loss: 28.4296 - val_MinusLogProbMetric: 28.4296 - lr: 1.2500e-04 - 44s/epoch - 223ms/step
Epoch 529/1000
2023-10-02 08:36:53.635 
Epoch 529/1000 
	 loss: 27.3683, MinusLogProbMetric: 27.3683, val_loss: 28.5167, val_MinusLogProbMetric: 28.5167

Epoch 529: val_loss did not improve from 28.24521
196/196 - 45s - loss: 27.3683 - MinusLogProbMetric: 27.3683 - val_loss: 28.5167 - val_MinusLogProbMetric: 28.5167 - lr: 1.2500e-04 - 45s/epoch - 227ms/step
Epoch 530/1000
2023-10-02 08:37:37.383 
Epoch 530/1000 
	 loss: 27.4262, MinusLogProbMetric: 27.4262, val_loss: 28.5483, val_MinusLogProbMetric: 28.5483

Epoch 530: val_loss did not improve from 28.24521
196/196 - 44s - loss: 27.4262 - MinusLogProbMetric: 27.4262 - val_loss: 28.5483 - val_MinusLogProbMetric: 28.5483 - lr: 1.2500e-04 - 44s/epoch - 223ms/step
Epoch 531/1000
2023-10-02 08:38:21.888 
Epoch 531/1000 
	 loss: 27.3030, MinusLogProbMetric: 27.3030, val_loss: 28.3459, val_MinusLogProbMetric: 28.3459

Epoch 531: val_loss did not improve from 28.24521
196/196 - 45s - loss: 27.3030 - MinusLogProbMetric: 27.3030 - val_loss: 28.3459 - val_MinusLogProbMetric: 28.3459 - lr: 1.2500e-04 - 45s/epoch - 227ms/step
Epoch 532/1000
2023-10-02 08:39:03.814 
Epoch 532/1000 
	 loss: 27.3565, MinusLogProbMetric: 27.3565, val_loss: 28.7221, val_MinusLogProbMetric: 28.7221

Epoch 532: val_loss did not improve from 28.24521
196/196 - 42s - loss: 27.3565 - MinusLogProbMetric: 27.3565 - val_loss: 28.7221 - val_MinusLogProbMetric: 28.7221 - lr: 1.2500e-04 - 42s/epoch - 214ms/step
Epoch 533/1000
2023-10-02 08:39:47.636 
Epoch 533/1000 
	 loss: 27.3256, MinusLogProbMetric: 27.3256, val_loss: 28.4386, val_MinusLogProbMetric: 28.4386

Epoch 533: val_loss did not improve from 28.24521
196/196 - 44s - loss: 27.3256 - MinusLogProbMetric: 27.3256 - val_loss: 28.4386 - val_MinusLogProbMetric: 28.4386 - lr: 1.2500e-04 - 44s/epoch - 224ms/step
Epoch 534/1000
2023-10-02 08:40:30.027 
Epoch 534/1000 
	 loss: 27.3713, MinusLogProbMetric: 27.3713, val_loss: 28.4152, val_MinusLogProbMetric: 28.4152

Epoch 534: val_loss did not improve from 28.24521
196/196 - 42s - loss: 27.3713 - MinusLogProbMetric: 27.3713 - val_loss: 28.4152 - val_MinusLogProbMetric: 28.4152 - lr: 1.2500e-04 - 42s/epoch - 216ms/step
Epoch 535/1000
2023-10-02 08:41:13.092 
Epoch 535/1000 
	 loss: 27.3296, MinusLogProbMetric: 27.3296, val_loss: 28.3148, val_MinusLogProbMetric: 28.3148

Epoch 535: val_loss did not improve from 28.24521
196/196 - 43s - loss: 27.3296 - MinusLogProbMetric: 27.3296 - val_loss: 28.3148 - val_MinusLogProbMetric: 28.3148 - lr: 1.2500e-04 - 43s/epoch - 220ms/step
Epoch 536/1000
2023-10-02 08:41:57.915 
Epoch 536/1000 
	 loss: 27.2663, MinusLogProbMetric: 27.2663, val_loss: 28.3397, val_MinusLogProbMetric: 28.3397

Epoch 536: val_loss did not improve from 28.24521
196/196 - 45s - loss: 27.2663 - MinusLogProbMetric: 27.2663 - val_loss: 28.3397 - val_MinusLogProbMetric: 28.3397 - lr: 1.2500e-04 - 45s/epoch - 229ms/step
Epoch 537/1000
2023-10-02 08:42:45.673 
Epoch 537/1000 
	 loss: 27.2742, MinusLogProbMetric: 27.2742, val_loss: 28.2555, val_MinusLogProbMetric: 28.2555

Epoch 537: val_loss did not improve from 28.24521
196/196 - 48s - loss: 27.2742 - MinusLogProbMetric: 27.2742 - val_loss: 28.2555 - val_MinusLogProbMetric: 28.2555 - lr: 1.2500e-04 - 48s/epoch - 244ms/step
Epoch 538/1000
2023-10-02 08:43:30.295 
Epoch 538/1000 
	 loss: 27.3331, MinusLogProbMetric: 27.3331, val_loss: 28.2638, val_MinusLogProbMetric: 28.2638

Epoch 538: val_loss did not improve from 28.24521
196/196 - 45s - loss: 27.3331 - MinusLogProbMetric: 27.3331 - val_loss: 28.2638 - val_MinusLogProbMetric: 28.2638 - lr: 1.2500e-04 - 45s/epoch - 228ms/step
Epoch 539/1000
2023-10-02 08:44:15.722 
Epoch 539/1000 
	 loss: 27.3183, MinusLogProbMetric: 27.3183, val_loss: 28.3391, val_MinusLogProbMetric: 28.3391

Epoch 539: val_loss did not improve from 28.24521
196/196 - 45s - loss: 27.3183 - MinusLogProbMetric: 27.3183 - val_loss: 28.3391 - val_MinusLogProbMetric: 28.3391 - lr: 1.2500e-04 - 45s/epoch - 232ms/step
Epoch 540/1000
2023-10-02 08:44:57.942 
Epoch 540/1000 
	 loss: 27.2035, MinusLogProbMetric: 27.2035, val_loss: 28.2456, val_MinusLogProbMetric: 28.2456

Epoch 540: val_loss did not improve from 28.24521
196/196 - 42s - loss: 27.2035 - MinusLogProbMetric: 27.2035 - val_loss: 28.2456 - val_MinusLogProbMetric: 28.2456 - lr: 6.2500e-05 - 42s/epoch - 215ms/step
Epoch 541/1000
2023-10-02 08:45:42.599 
Epoch 541/1000 
	 loss: 27.2009, MinusLogProbMetric: 27.2009, val_loss: 28.2804, val_MinusLogProbMetric: 28.2804

Epoch 541: val_loss did not improve from 28.24521
196/196 - 45s - loss: 27.2009 - MinusLogProbMetric: 27.2009 - val_loss: 28.2804 - val_MinusLogProbMetric: 28.2804 - lr: 6.2500e-05 - 45s/epoch - 228ms/step
Epoch 542/1000
2023-10-02 08:46:27.416 
Epoch 542/1000 
	 loss: 27.1999, MinusLogProbMetric: 27.1999, val_loss: 28.2744, val_MinusLogProbMetric: 28.2744

Epoch 542: val_loss did not improve from 28.24521
196/196 - 45s - loss: 27.1999 - MinusLogProbMetric: 27.1999 - val_loss: 28.2744 - val_MinusLogProbMetric: 28.2744 - lr: 6.2500e-05 - 45s/epoch - 229ms/step
Epoch 543/1000
2023-10-02 08:47:15.849 
Epoch 543/1000 
	 loss: 27.1979, MinusLogProbMetric: 27.1979, val_loss: 28.2336, val_MinusLogProbMetric: 28.2336

Epoch 543: val_loss improved from 28.24521 to 28.23357, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_354/weights/best_weights.h5
196/196 - 49s - loss: 27.1979 - MinusLogProbMetric: 27.1979 - val_loss: 28.2336 - val_MinusLogProbMetric: 28.2336 - lr: 6.2500e-05 - 49s/epoch - 251ms/step
Epoch 544/1000
2023-10-02 08:48:01.299 
Epoch 544/1000 
	 loss: 27.2067, MinusLogProbMetric: 27.2067, val_loss: 28.2428, val_MinusLogProbMetric: 28.2428

Epoch 544: val_loss did not improve from 28.23357
196/196 - 45s - loss: 27.2067 - MinusLogProbMetric: 27.2067 - val_loss: 28.2428 - val_MinusLogProbMetric: 28.2428 - lr: 6.2500e-05 - 45s/epoch - 228ms/step
Epoch 545/1000
2023-10-02 08:48:46.661 
Epoch 545/1000 
	 loss: 27.2026, MinusLogProbMetric: 27.2026, val_loss: 28.2986, val_MinusLogProbMetric: 28.2986

Epoch 545: val_loss did not improve from 28.23357
196/196 - 45s - loss: 27.2026 - MinusLogProbMetric: 27.2026 - val_loss: 28.2986 - val_MinusLogProbMetric: 28.2986 - lr: 6.2500e-05 - 45s/epoch - 231ms/step
Epoch 546/1000
2023-10-02 08:49:29.629 
Epoch 546/1000 
	 loss: 27.2106, MinusLogProbMetric: 27.2106, val_loss: 28.2419, val_MinusLogProbMetric: 28.2419

Epoch 546: val_loss did not improve from 28.23357
196/196 - 43s - loss: 27.2106 - MinusLogProbMetric: 27.2106 - val_loss: 28.2419 - val_MinusLogProbMetric: 28.2419 - lr: 6.2500e-05 - 43s/epoch - 219ms/step
Epoch 547/1000
2023-10-02 08:50:15.435 
Epoch 547/1000 
	 loss: 27.1923, MinusLogProbMetric: 27.1923, val_loss: 28.2521, val_MinusLogProbMetric: 28.2521

Epoch 547: val_loss did not improve from 28.23357
196/196 - 46s - loss: 27.1923 - MinusLogProbMetric: 27.1923 - val_loss: 28.2521 - val_MinusLogProbMetric: 28.2521 - lr: 6.2500e-05 - 46s/epoch - 234ms/step
Epoch 548/1000
2023-10-02 08:51:01.009 
Epoch 548/1000 
	 loss: 27.1980, MinusLogProbMetric: 27.1980, val_loss: 28.2325, val_MinusLogProbMetric: 28.2325

Epoch 548: val_loss improved from 28.23357 to 28.23253, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_354/weights/best_weights.h5
196/196 - 46s - loss: 27.1980 - MinusLogProbMetric: 27.1980 - val_loss: 28.2325 - val_MinusLogProbMetric: 28.2325 - lr: 6.2500e-05 - 46s/epoch - 237ms/step
Epoch 549/1000
2023-10-02 08:51:44.966 
Epoch 549/1000 
	 loss: 27.1923, MinusLogProbMetric: 27.1923, val_loss: 28.3537, val_MinusLogProbMetric: 28.3537

Epoch 549: val_loss did not improve from 28.23253
196/196 - 43s - loss: 27.1923 - MinusLogProbMetric: 27.1923 - val_loss: 28.3537 - val_MinusLogProbMetric: 28.3537 - lr: 6.2500e-05 - 43s/epoch - 220ms/step
Epoch 550/1000
2023-10-02 08:52:29.069 
Epoch 550/1000 
	 loss: 27.2008, MinusLogProbMetric: 27.2008, val_loss: 28.3084, val_MinusLogProbMetric: 28.3084

Epoch 550: val_loss did not improve from 28.23253
196/196 - 44s - loss: 27.2008 - MinusLogProbMetric: 27.2008 - val_loss: 28.3084 - val_MinusLogProbMetric: 28.3084 - lr: 6.2500e-05 - 44s/epoch - 225ms/step
Epoch 551/1000
2023-10-02 08:53:14.131 
Epoch 551/1000 
	 loss: 27.1934, MinusLogProbMetric: 27.1934, val_loss: 28.2473, val_MinusLogProbMetric: 28.2473

Epoch 551: val_loss did not improve from 28.23253
196/196 - 45s - loss: 27.1934 - MinusLogProbMetric: 27.1934 - val_loss: 28.2473 - val_MinusLogProbMetric: 28.2473 - lr: 6.2500e-05 - 45s/epoch - 230ms/step
Epoch 552/1000
2023-10-02 08:53:58.660 
Epoch 552/1000 
	 loss: 27.1941, MinusLogProbMetric: 27.1941, val_loss: 28.2471, val_MinusLogProbMetric: 28.2471

Epoch 552: val_loss did not improve from 28.23253
196/196 - 44s - loss: 27.1941 - MinusLogProbMetric: 27.1941 - val_loss: 28.2471 - val_MinusLogProbMetric: 28.2471 - lr: 6.2500e-05 - 44s/epoch - 227ms/step
Epoch 553/1000
2023-10-02 08:54:40.772 
Epoch 553/1000 
	 loss: 27.2011, MinusLogProbMetric: 27.2011, val_loss: 28.2397, val_MinusLogProbMetric: 28.2397

Epoch 553: val_loss did not improve from 28.23253
196/196 - 42s - loss: 27.2011 - MinusLogProbMetric: 27.2011 - val_loss: 28.2397 - val_MinusLogProbMetric: 28.2397 - lr: 6.2500e-05 - 42s/epoch - 215ms/step
Epoch 554/1000
2023-10-02 08:55:30.143 
Epoch 554/1000 
	 loss: 27.1958, MinusLogProbMetric: 27.1958, val_loss: 28.2435, val_MinusLogProbMetric: 28.2435

Epoch 554: val_loss did not improve from 28.23253
196/196 - 49s - loss: 27.1958 - MinusLogProbMetric: 27.1958 - val_loss: 28.2435 - val_MinusLogProbMetric: 28.2435 - lr: 6.2500e-05 - 49s/epoch - 252ms/step
Epoch 555/1000
2023-10-02 08:56:14.946 
Epoch 555/1000 
	 loss: 27.1886, MinusLogProbMetric: 27.1886, val_loss: 28.2639, val_MinusLogProbMetric: 28.2639

Epoch 555: val_loss did not improve from 28.23253
196/196 - 45s - loss: 27.1886 - MinusLogProbMetric: 27.1886 - val_loss: 28.2639 - val_MinusLogProbMetric: 28.2639 - lr: 6.2500e-05 - 45s/epoch - 228ms/step
Epoch 556/1000
2023-10-02 08:57:02.157 
Epoch 556/1000 
	 loss: 27.1861, MinusLogProbMetric: 27.1861, val_loss: 28.2631, val_MinusLogProbMetric: 28.2631

Epoch 556: val_loss did not improve from 28.23253
196/196 - 47s - loss: 27.1861 - MinusLogProbMetric: 27.1861 - val_loss: 28.2631 - val_MinusLogProbMetric: 28.2631 - lr: 6.2500e-05 - 47s/epoch - 241ms/step
Epoch 557/1000
2023-10-02 08:57:48.302 
Epoch 557/1000 
	 loss: 27.1895, MinusLogProbMetric: 27.1895, val_loss: 28.2821, val_MinusLogProbMetric: 28.2821

Epoch 557: val_loss did not improve from 28.23253
196/196 - 46s - loss: 27.1895 - MinusLogProbMetric: 27.1895 - val_loss: 28.2821 - val_MinusLogProbMetric: 28.2821 - lr: 6.2500e-05 - 46s/epoch - 235ms/step
Epoch 558/1000
2023-10-02 08:58:35.980 
Epoch 558/1000 
	 loss: 27.2012, MinusLogProbMetric: 27.2012, val_loss: 28.2592, val_MinusLogProbMetric: 28.2592

Epoch 558: val_loss did not improve from 28.23253
196/196 - 48s - loss: 27.2012 - MinusLogProbMetric: 27.2012 - val_loss: 28.2592 - val_MinusLogProbMetric: 28.2592 - lr: 6.2500e-05 - 48s/epoch - 243ms/step
Epoch 559/1000
2023-10-02 08:59:21.565 
Epoch 559/1000 
	 loss: 27.2014, MinusLogProbMetric: 27.2014, val_loss: 28.3120, val_MinusLogProbMetric: 28.3120

Epoch 559: val_loss did not improve from 28.23253
196/196 - 46s - loss: 27.2014 - MinusLogProbMetric: 27.2014 - val_loss: 28.3120 - val_MinusLogProbMetric: 28.3120 - lr: 6.2500e-05 - 46s/epoch - 233ms/step
Epoch 560/1000
2023-10-02 09:00:06.044 
Epoch 560/1000 
	 loss: 27.2056, MinusLogProbMetric: 27.2056, val_loss: 28.2270, val_MinusLogProbMetric: 28.2270

Epoch 560: val_loss improved from 28.23253 to 28.22703, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_354/weights/best_weights.h5
196/196 - 45s - loss: 27.2056 - MinusLogProbMetric: 27.2056 - val_loss: 28.2270 - val_MinusLogProbMetric: 28.2270 - lr: 6.2500e-05 - 45s/epoch - 231ms/step
Epoch 561/1000
2023-10-02 09:00:53.017 
Epoch 561/1000 
	 loss: 27.1870, MinusLogProbMetric: 27.1870, val_loss: 28.2391, val_MinusLogProbMetric: 28.2391

Epoch 561: val_loss did not improve from 28.22703
196/196 - 46s - loss: 27.1870 - MinusLogProbMetric: 27.1870 - val_loss: 28.2391 - val_MinusLogProbMetric: 28.2391 - lr: 6.2500e-05 - 46s/epoch - 236ms/step
Epoch 562/1000
2023-10-02 09:01:38.965 
Epoch 562/1000 
	 loss: 27.1893, MinusLogProbMetric: 27.1893, val_loss: 28.2879, val_MinusLogProbMetric: 28.2879

Epoch 562: val_loss did not improve from 28.22703
196/196 - 46s - loss: 27.1893 - MinusLogProbMetric: 27.1893 - val_loss: 28.2879 - val_MinusLogProbMetric: 28.2879 - lr: 6.2500e-05 - 46s/epoch - 234ms/step
Epoch 563/1000
2023-10-02 09:02:24.883 
Epoch 563/1000 
	 loss: 27.1905, MinusLogProbMetric: 27.1905, val_loss: 28.3057, val_MinusLogProbMetric: 28.3057

Epoch 563: val_loss did not improve from 28.22703
196/196 - 46s - loss: 27.1905 - MinusLogProbMetric: 27.1905 - val_loss: 28.3057 - val_MinusLogProbMetric: 28.3057 - lr: 6.2500e-05 - 46s/epoch - 234ms/step
Epoch 564/1000
2023-10-02 09:03:10.831 
Epoch 564/1000 
	 loss: 27.1977, MinusLogProbMetric: 27.1977, val_loss: 28.2297, val_MinusLogProbMetric: 28.2297

Epoch 564: val_loss did not improve from 28.22703
196/196 - 46s - loss: 27.1977 - MinusLogProbMetric: 27.1977 - val_loss: 28.2297 - val_MinusLogProbMetric: 28.2297 - lr: 6.2500e-05 - 46s/epoch - 234ms/step
Epoch 565/1000
2023-10-02 09:03:55.827 
Epoch 565/1000 
	 loss: 27.1911, MinusLogProbMetric: 27.1911, val_loss: 28.2514, val_MinusLogProbMetric: 28.2514

Epoch 565: val_loss did not improve from 28.22703
196/196 - 45s - loss: 27.1911 - MinusLogProbMetric: 27.1911 - val_loss: 28.2514 - val_MinusLogProbMetric: 28.2514 - lr: 6.2500e-05 - 45s/epoch - 230ms/step
Epoch 566/1000
2023-10-02 09:04:40.572 
Epoch 566/1000 
	 loss: 27.1941, MinusLogProbMetric: 27.1941, val_loss: 28.2798, val_MinusLogProbMetric: 28.2798

Epoch 566: val_loss did not improve from 28.22703
196/196 - 45s - loss: 27.1941 - MinusLogProbMetric: 27.1941 - val_loss: 28.2798 - val_MinusLogProbMetric: 28.2798 - lr: 6.2500e-05 - 45s/epoch - 228ms/step
Epoch 567/1000
2023-10-02 09:05:26.515 
Epoch 567/1000 
	 loss: 27.1906, MinusLogProbMetric: 27.1906, val_loss: 28.2632, val_MinusLogProbMetric: 28.2632

Epoch 567: val_loss did not improve from 28.22703
196/196 - 46s - loss: 27.1906 - MinusLogProbMetric: 27.1906 - val_loss: 28.2632 - val_MinusLogProbMetric: 28.2632 - lr: 6.2500e-05 - 46s/epoch - 234ms/step
Epoch 568/1000
2023-10-02 09:06:15.098 
Epoch 568/1000 
	 loss: 27.1957, MinusLogProbMetric: 27.1957, val_loss: 28.2522, val_MinusLogProbMetric: 28.2522

Epoch 568: val_loss did not improve from 28.22703
196/196 - 49s - loss: 27.1957 - MinusLogProbMetric: 27.1957 - val_loss: 28.2522 - val_MinusLogProbMetric: 28.2522 - lr: 6.2500e-05 - 49s/epoch - 248ms/step
Epoch 569/1000
2023-10-02 09:07:03.437 
Epoch 569/1000 
	 loss: 27.1895, MinusLogProbMetric: 27.1895, val_loss: 28.2440, val_MinusLogProbMetric: 28.2440

Epoch 569: val_loss did not improve from 28.22703
196/196 - 48s - loss: 27.1895 - MinusLogProbMetric: 27.1895 - val_loss: 28.2440 - val_MinusLogProbMetric: 28.2440 - lr: 6.2500e-05 - 48s/epoch - 247ms/step
Epoch 570/1000
2023-10-02 09:07:51.155 
Epoch 570/1000 
	 loss: 27.2116, MinusLogProbMetric: 27.2116, val_loss: 28.3424, val_MinusLogProbMetric: 28.3424

Epoch 570: val_loss did not improve from 28.22703
196/196 - 48s - loss: 27.2116 - MinusLogProbMetric: 27.2116 - val_loss: 28.3424 - val_MinusLogProbMetric: 28.3424 - lr: 6.2500e-05 - 48s/epoch - 243ms/step
Epoch 571/1000
2023-10-02 09:08:40.228 
Epoch 571/1000 
	 loss: 27.1926, MinusLogProbMetric: 27.1926, val_loss: 28.2454, val_MinusLogProbMetric: 28.2454

Epoch 571: val_loss did not improve from 28.22703
196/196 - 49s - loss: 27.1926 - MinusLogProbMetric: 27.1926 - val_loss: 28.2454 - val_MinusLogProbMetric: 28.2454 - lr: 6.2500e-05 - 49s/epoch - 250ms/step
Epoch 572/1000
2023-10-02 09:09:29.128 
Epoch 572/1000 
	 loss: 27.2004, MinusLogProbMetric: 27.2004, val_loss: 28.2697, val_MinusLogProbMetric: 28.2697

Epoch 572: val_loss did not improve from 28.22703
196/196 - 49s - loss: 27.2004 - MinusLogProbMetric: 27.2004 - val_loss: 28.2697 - val_MinusLogProbMetric: 28.2697 - lr: 6.2500e-05 - 49s/epoch - 250ms/step
Epoch 573/1000
2023-10-02 09:10:18.716 
Epoch 573/1000 
	 loss: 27.2077, MinusLogProbMetric: 27.2077, val_loss: 28.2784, val_MinusLogProbMetric: 28.2784

Epoch 573: val_loss did not improve from 28.22703
196/196 - 50s - loss: 27.2077 - MinusLogProbMetric: 27.2077 - val_loss: 28.2784 - val_MinusLogProbMetric: 28.2784 - lr: 6.2500e-05 - 50s/epoch - 253ms/step
Epoch 574/1000
2023-10-02 09:11:07.882 
Epoch 574/1000 
	 loss: 27.1910, MinusLogProbMetric: 27.1910, val_loss: 28.2682, val_MinusLogProbMetric: 28.2682

Epoch 574: val_loss did not improve from 28.22703
196/196 - 49s - loss: 27.1910 - MinusLogProbMetric: 27.1910 - val_loss: 28.2682 - val_MinusLogProbMetric: 28.2682 - lr: 6.2500e-05 - 49s/epoch - 251ms/step
Epoch 575/1000
2023-10-02 09:11:58.037 
Epoch 575/1000 
	 loss: 27.1940, MinusLogProbMetric: 27.1940, val_loss: 28.2235, val_MinusLogProbMetric: 28.2235

Epoch 575: val_loss improved from 28.22703 to 28.22353, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_354/weights/best_weights.h5
196/196 - 51s - loss: 27.1940 - MinusLogProbMetric: 27.1940 - val_loss: 28.2235 - val_MinusLogProbMetric: 28.2235 - lr: 6.2500e-05 - 51s/epoch - 260ms/step
Epoch 576/1000
2023-10-02 09:12:45.207 
Epoch 576/1000 
	 loss: 27.2030, MinusLogProbMetric: 27.2030, val_loss: 28.4677, val_MinusLogProbMetric: 28.4677

Epoch 576: val_loss did not improve from 28.22353
196/196 - 46s - loss: 27.2030 - MinusLogProbMetric: 27.2030 - val_loss: 28.4677 - val_MinusLogProbMetric: 28.4677 - lr: 6.2500e-05 - 46s/epoch - 237ms/step
Epoch 577/1000
2023-10-02 09:13:34.540 
Epoch 577/1000 
	 loss: 27.2004, MinusLogProbMetric: 27.2004, val_loss: 28.2197, val_MinusLogProbMetric: 28.2197

Epoch 577: val_loss improved from 28.22353 to 28.21968, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_354/weights/best_weights.h5
196/196 - 50s - loss: 27.2004 - MinusLogProbMetric: 27.2004 - val_loss: 28.2197 - val_MinusLogProbMetric: 28.2197 - lr: 6.2500e-05 - 50s/epoch - 253ms/step
Epoch 578/1000
2023-10-02 09:14:23.836 
Epoch 578/1000 
	 loss: 27.1920, MinusLogProbMetric: 27.1920, val_loss: 28.2462, val_MinusLogProbMetric: 28.2462

Epoch 578: val_loss did not improve from 28.21968
196/196 - 49s - loss: 27.1920 - MinusLogProbMetric: 27.1920 - val_loss: 28.2462 - val_MinusLogProbMetric: 28.2462 - lr: 6.2500e-05 - 49s/epoch - 249ms/step
Epoch 579/1000
2023-10-02 09:15:12.205 
Epoch 579/1000 
	 loss: 27.1866, MinusLogProbMetric: 27.1866, val_loss: 28.2344, val_MinusLogProbMetric: 28.2344

Epoch 579: val_loss did not improve from 28.21968
196/196 - 48s - loss: 27.1866 - MinusLogProbMetric: 27.1866 - val_loss: 28.2344 - val_MinusLogProbMetric: 28.2344 - lr: 6.2500e-05 - 48s/epoch - 247ms/step
Epoch 580/1000
2023-10-02 09:15:59.033 
Epoch 580/1000 
	 loss: 27.1885, MinusLogProbMetric: 27.1885, val_loss: 28.2602, val_MinusLogProbMetric: 28.2602

Epoch 580: val_loss did not improve from 28.21968
196/196 - 47s - loss: 27.1885 - MinusLogProbMetric: 27.1885 - val_loss: 28.2602 - val_MinusLogProbMetric: 28.2602 - lr: 6.2500e-05 - 47s/epoch - 239ms/step
Epoch 581/1000
2023-10-02 09:16:48.446 
Epoch 581/1000 
	 loss: 27.1748, MinusLogProbMetric: 27.1748, val_loss: 28.3917, val_MinusLogProbMetric: 28.3917

Epoch 581: val_loss did not improve from 28.21968
196/196 - 49s - loss: 27.1748 - MinusLogProbMetric: 27.1748 - val_loss: 28.3917 - val_MinusLogProbMetric: 28.3917 - lr: 6.2500e-05 - 49s/epoch - 252ms/step
Epoch 582/1000
2023-10-02 09:17:35.061 
Epoch 582/1000 
	 loss: 27.2015, MinusLogProbMetric: 27.2015, val_loss: 28.2308, val_MinusLogProbMetric: 28.2308

Epoch 582: val_loss did not improve from 28.21968
196/196 - 47s - loss: 27.2015 - MinusLogProbMetric: 27.2015 - val_loss: 28.2308 - val_MinusLogProbMetric: 28.2308 - lr: 6.2500e-05 - 47s/epoch - 238ms/step
Epoch 583/1000
2023-10-02 09:18:23.358 
Epoch 583/1000 
	 loss: 27.1892, MinusLogProbMetric: 27.1892, val_loss: 28.2496, val_MinusLogProbMetric: 28.2496

Epoch 583: val_loss did not improve from 28.21968
196/196 - 48s - loss: 27.1892 - MinusLogProbMetric: 27.1892 - val_loss: 28.2496 - val_MinusLogProbMetric: 28.2496 - lr: 6.2500e-05 - 48s/epoch - 246ms/step
Epoch 584/1000
2023-10-02 09:19:11.165 
Epoch 584/1000 
	 loss: 27.2002, MinusLogProbMetric: 27.2002, val_loss: 28.2580, val_MinusLogProbMetric: 28.2580

Epoch 584: val_loss did not improve from 28.21968
196/196 - 48s - loss: 27.2002 - MinusLogProbMetric: 27.2002 - val_loss: 28.2580 - val_MinusLogProbMetric: 28.2580 - lr: 6.2500e-05 - 48s/epoch - 244ms/step
Epoch 585/1000
2023-10-02 09:19:56.383 
Epoch 585/1000 
	 loss: 27.1895, MinusLogProbMetric: 27.1895, val_loss: 28.2660, val_MinusLogProbMetric: 28.2660

Epoch 585: val_loss did not improve from 28.21968
196/196 - 45s - loss: 27.1895 - MinusLogProbMetric: 27.1895 - val_loss: 28.2660 - val_MinusLogProbMetric: 28.2660 - lr: 6.2500e-05 - 45s/epoch - 231ms/step
Epoch 586/1000
2023-10-02 09:20:45.683 
Epoch 586/1000 
	 loss: 27.1916, MinusLogProbMetric: 27.1916, val_loss: 28.3006, val_MinusLogProbMetric: 28.3006

Epoch 586: val_loss did not improve from 28.21968
196/196 - 49s - loss: 27.1916 - MinusLogProbMetric: 27.1916 - val_loss: 28.3006 - val_MinusLogProbMetric: 28.3006 - lr: 6.2500e-05 - 49s/epoch - 252ms/step
Epoch 587/1000
2023-10-02 09:21:31.651 
Epoch 587/1000 
	 loss: 27.2000, MinusLogProbMetric: 27.2000, val_loss: 28.2402, val_MinusLogProbMetric: 28.2402

Epoch 587: val_loss did not improve from 28.21968
196/196 - 46s - loss: 27.2000 - MinusLogProbMetric: 27.2000 - val_loss: 28.2402 - val_MinusLogProbMetric: 28.2402 - lr: 6.2500e-05 - 46s/epoch - 234ms/step
Epoch 588/1000
2023-10-02 09:22:23.255 
Epoch 588/1000 
	 loss: 27.1866, MinusLogProbMetric: 27.1866, val_loss: 28.2326, val_MinusLogProbMetric: 28.2326

Epoch 588: val_loss did not improve from 28.21968
196/196 - 52s - loss: 27.1866 - MinusLogProbMetric: 27.1866 - val_loss: 28.2326 - val_MinusLogProbMetric: 28.2326 - lr: 6.2500e-05 - 52s/epoch - 263ms/step
Epoch 589/1000
2023-10-02 09:23:18.184 
Epoch 589/1000 
	 loss: 27.1915, MinusLogProbMetric: 27.1915, val_loss: 28.2345, val_MinusLogProbMetric: 28.2345

Epoch 589: val_loss did not improve from 28.21968
196/196 - 55s - loss: 27.1915 - MinusLogProbMetric: 27.1915 - val_loss: 28.2345 - val_MinusLogProbMetric: 28.2345 - lr: 6.2500e-05 - 55s/epoch - 280ms/step
Epoch 590/1000
2023-10-02 09:24:10.063 
Epoch 590/1000 
	 loss: 27.1928, MinusLogProbMetric: 27.1928, val_loss: 28.4184, val_MinusLogProbMetric: 28.4184

Epoch 590: val_loss did not improve from 28.21968
196/196 - 52s - loss: 27.1928 - MinusLogProbMetric: 27.1928 - val_loss: 28.4184 - val_MinusLogProbMetric: 28.4184 - lr: 6.2500e-05 - 52s/epoch - 265ms/step
Epoch 591/1000
2023-10-02 09:25:03.534 
Epoch 591/1000 
	 loss: 27.2004, MinusLogProbMetric: 27.2004, val_loss: 28.2708, val_MinusLogProbMetric: 28.2708

Epoch 591: val_loss did not improve from 28.21968
196/196 - 53s - loss: 27.2004 - MinusLogProbMetric: 27.2004 - val_loss: 28.2708 - val_MinusLogProbMetric: 28.2708 - lr: 6.2500e-05 - 53s/epoch - 273ms/step
Epoch 592/1000
2023-10-02 09:25:56.397 
Epoch 592/1000 
	 loss: 27.1944, MinusLogProbMetric: 27.1944, val_loss: 28.2757, val_MinusLogProbMetric: 28.2757

Epoch 592: val_loss did not improve from 28.21968
196/196 - 53s - loss: 27.1944 - MinusLogProbMetric: 27.1944 - val_loss: 28.2757 - val_MinusLogProbMetric: 28.2757 - lr: 6.2500e-05 - 53s/epoch - 270ms/step
Epoch 593/1000
2023-10-02 09:26:49.466 
Epoch 593/1000 
	 loss: 27.1907, MinusLogProbMetric: 27.1907, val_loss: 28.2206, val_MinusLogProbMetric: 28.2206

Epoch 593: val_loss did not improve from 28.21968
196/196 - 53s - loss: 27.1907 - MinusLogProbMetric: 27.1907 - val_loss: 28.2206 - val_MinusLogProbMetric: 28.2206 - lr: 6.2500e-05 - 53s/epoch - 271ms/step
Epoch 594/1000
2023-10-02 09:27:37.861 
Epoch 594/1000 
	 loss: 27.2041, MinusLogProbMetric: 27.2041, val_loss: 28.2548, val_MinusLogProbMetric: 28.2548

Epoch 594: val_loss did not improve from 28.21968
196/196 - 48s - loss: 27.2041 - MinusLogProbMetric: 27.2041 - val_loss: 28.2548 - val_MinusLogProbMetric: 28.2548 - lr: 6.2500e-05 - 48s/epoch - 247ms/step
Epoch 595/1000
2023-10-02 09:28:30.072 
Epoch 595/1000 
	 loss: 27.1809, MinusLogProbMetric: 27.1809, val_loss: 28.2279, val_MinusLogProbMetric: 28.2279

Epoch 595: val_loss did not improve from 28.21968
196/196 - 52s - loss: 27.1809 - MinusLogProbMetric: 27.1809 - val_loss: 28.2279 - val_MinusLogProbMetric: 28.2279 - lr: 6.2500e-05 - 52s/epoch - 266ms/step
Epoch 596/1000
2023-10-02 09:29:17.948 
Epoch 596/1000 
	 loss: 27.1872, MinusLogProbMetric: 27.1872, val_loss: 28.2621, val_MinusLogProbMetric: 28.2621

Epoch 596: val_loss did not improve from 28.21968
196/196 - 48s - loss: 27.1872 - MinusLogProbMetric: 27.1872 - val_loss: 28.2621 - val_MinusLogProbMetric: 28.2621 - lr: 6.2500e-05 - 48s/epoch - 244ms/step
Epoch 597/1000
2023-10-02 09:30:11.337 
Epoch 597/1000 
	 loss: 27.2045, MinusLogProbMetric: 27.2045, val_loss: 28.2315, val_MinusLogProbMetric: 28.2315

Epoch 597: val_loss did not improve from 28.21968
196/196 - 53s - loss: 27.2045 - MinusLogProbMetric: 27.2045 - val_loss: 28.2315 - val_MinusLogProbMetric: 28.2315 - lr: 6.2500e-05 - 53s/epoch - 273ms/step
Epoch 598/1000
2023-10-02 09:31:01.292 
Epoch 598/1000 
	 loss: 27.1935, MinusLogProbMetric: 27.1935, val_loss: 28.2974, val_MinusLogProbMetric: 28.2974

Epoch 598: val_loss did not improve from 28.21968
196/196 - 50s - loss: 27.1935 - MinusLogProbMetric: 27.1935 - val_loss: 28.2974 - val_MinusLogProbMetric: 28.2974 - lr: 6.2500e-05 - 50s/epoch - 255ms/step
Epoch 599/1000
2023-10-02 09:31:54.312 
Epoch 599/1000 
	 loss: 27.1877, MinusLogProbMetric: 27.1877, val_loss: 28.2256, val_MinusLogProbMetric: 28.2256

Epoch 599: val_loss did not improve from 28.21968
196/196 - 53s - loss: 27.1877 - MinusLogProbMetric: 27.1877 - val_loss: 28.2256 - val_MinusLogProbMetric: 28.2256 - lr: 6.2500e-05 - 53s/epoch - 270ms/step
Epoch 600/1000
2023-10-02 09:32:44.662 
Epoch 600/1000 
	 loss: 27.1810, MinusLogProbMetric: 27.1810, val_loss: 28.2580, val_MinusLogProbMetric: 28.2580

Epoch 600: val_loss did not improve from 28.21968
196/196 - 50s - loss: 27.1810 - MinusLogProbMetric: 27.1810 - val_loss: 28.2580 - val_MinusLogProbMetric: 28.2580 - lr: 6.2500e-05 - 50s/epoch - 257ms/step
Epoch 601/1000
2023-10-02 09:33:38.784 
Epoch 601/1000 
	 loss: 27.1781, MinusLogProbMetric: 27.1781, val_loss: 28.2246, val_MinusLogProbMetric: 28.2246

Epoch 601: val_loss did not improve from 28.21968
196/196 - 54s - loss: 27.1781 - MinusLogProbMetric: 27.1781 - val_loss: 28.2246 - val_MinusLogProbMetric: 28.2246 - lr: 6.2500e-05 - 54s/epoch - 276ms/step
Epoch 602/1000
2023-10-02 09:34:32.873 
Epoch 602/1000 
	 loss: 27.1814, MinusLogProbMetric: 27.1814, val_loss: 28.2618, val_MinusLogProbMetric: 28.2618

Epoch 602: val_loss did not improve from 28.21968
196/196 - 54s - loss: 27.1814 - MinusLogProbMetric: 27.1814 - val_loss: 28.2618 - val_MinusLogProbMetric: 28.2618 - lr: 6.2500e-05 - 54s/epoch - 276ms/step
Epoch 603/1000
2023-10-02 09:35:24.915 
Epoch 603/1000 
	 loss: 27.1804, MinusLogProbMetric: 27.1804, val_loss: 28.2338, val_MinusLogProbMetric: 28.2338

Epoch 603: val_loss did not improve from 28.21968
196/196 - 52s - loss: 27.1804 - MinusLogProbMetric: 27.1804 - val_loss: 28.2338 - val_MinusLogProbMetric: 28.2338 - lr: 6.2500e-05 - 52s/epoch - 265ms/step
Epoch 604/1000
2023-10-02 09:36:17.300 
Epoch 604/1000 
	 loss: 27.1990, MinusLogProbMetric: 27.1990, val_loss: 28.2366, val_MinusLogProbMetric: 28.2366

Epoch 604: val_loss did not improve from 28.21968
196/196 - 52s - loss: 27.1990 - MinusLogProbMetric: 27.1990 - val_loss: 28.2366 - val_MinusLogProbMetric: 28.2366 - lr: 6.2500e-05 - 52s/epoch - 267ms/step
Epoch 605/1000
2023-10-02 09:37:07.391 
Epoch 605/1000 
	 loss: 27.1879, MinusLogProbMetric: 27.1879, val_loss: 28.2750, val_MinusLogProbMetric: 28.2750

Epoch 605: val_loss did not improve from 28.21968
196/196 - 50s - loss: 27.1879 - MinusLogProbMetric: 27.1879 - val_loss: 28.2750 - val_MinusLogProbMetric: 28.2750 - lr: 6.2500e-05 - 50s/epoch - 256ms/step
Epoch 606/1000
2023-10-02 09:37:59.243 
Epoch 606/1000 
	 loss: 27.1773, MinusLogProbMetric: 27.1773, val_loss: 28.2350, val_MinusLogProbMetric: 28.2350

Epoch 606: val_loss did not improve from 28.21968
196/196 - 52s - loss: 27.1773 - MinusLogProbMetric: 27.1773 - val_loss: 28.2350 - val_MinusLogProbMetric: 28.2350 - lr: 6.2500e-05 - 52s/epoch - 264ms/step
Epoch 607/1000
2023-10-02 09:38:48.099 
Epoch 607/1000 
	 loss: 27.1791, MinusLogProbMetric: 27.1791, val_loss: 28.2358, val_MinusLogProbMetric: 28.2358

Epoch 607: val_loss did not improve from 28.21968
196/196 - 49s - loss: 27.1791 - MinusLogProbMetric: 27.1791 - val_loss: 28.2358 - val_MinusLogProbMetric: 28.2358 - lr: 6.2500e-05 - 49s/epoch - 249ms/step
Epoch 608/1000
2023-10-02 09:39:38.985 
Epoch 608/1000 
	 loss: 27.1882, MinusLogProbMetric: 27.1882, val_loss: 28.2463, val_MinusLogProbMetric: 28.2463

Epoch 608: val_loss did not improve from 28.21968
196/196 - 51s - loss: 27.1882 - MinusLogProbMetric: 27.1882 - val_loss: 28.2463 - val_MinusLogProbMetric: 28.2463 - lr: 6.2500e-05 - 51s/epoch - 260ms/step
Epoch 609/1000
2023-10-02 09:40:28.871 
Epoch 609/1000 
	 loss: 27.2048, MinusLogProbMetric: 27.2048, val_loss: 28.2446, val_MinusLogProbMetric: 28.2446

Epoch 609: val_loss did not improve from 28.21968
196/196 - 50s - loss: 27.2048 - MinusLogProbMetric: 27.2048 - val_loss: 28.2446 - val_MinusLogProbMetric: 28.2446 - lr: 6.2500e-05 - 50s/epoch - 254ms/step
Epoch 610/1000
2023-10-02 09:41:18.372 
Epoch 610/1000 
	 loss: 27.1823, MinusLogProbMetric: 27.1823, val_loss: 28.3286, val_MinusLogProbMetric: 28.3286

Epoch 610: val_loss did not improve from 28.21968
196/196 - 50s - loss: 27.1823 - MinusLogProbMetric: 27.1823 - val_loss: 28.3286 - val_MinusLogProbMetric: 28.3286 - lr: 6.2500e-05 - 50s/epoch - 253ms/step
Epoch 611/1000
2023-10-02 09:42:10.861 
Epoch 611/1000 
	 loss: 27.1860, MinusLogProbMetric: 27.1860, val_loss: 28.2317, val_MinusLogProbMetric: 28.2317

Epoch 611: val_loss did not improve from 28.21968
196/196 - 52s - loss: 27.1860 - MinusLogProbMetric: 27.1860 - val_loss: 28.2317 - val_MinusLogProbMetric: 28.2317 - lr: 6.2500e-05 - 52s/epoch - 268ms/step
Epoch 612/1000
2023-10-02 09:43:03.611 
Epoch 612/1000 
	 loss: 27.1896, MinusLogProbMetric: 27.1896, val_loss: 28.3545, val_MinusLogProbMetric: 28.3545

Epoch 612: val_loss did not improve from 28.21968
196/196 - 53s - loss: 27.1896 - MinusLogProbMetric: 27.1896 - val_loss: 28.3545 - val_MinusLogProbMetric: 28.3545 - lr: 6.2500e-05 - 53s/epoch - 269ms/step
Epoch 613/1000
2023-10-02 09:43:56.878 
Epoch 613/1000 
	 loss: 27.1809, MinusLogProbMetric: 27.1809, val_loss: 28.2261, val_MinusLogProbMetric: 28.2261

Epoch 613: val_loss did not improve from 28.21968
196/196 - 53s - loss: 27.1809 - MinusLogProbMetric: 27.1809 - val_loss: 28.2261 - val_MinusLogProbMetric: 28.2261 - lr: 6.2500e-05 - 53s/epoch - 272ms/step
Epoch 614/1000
2023-10-02 09:44:49.306 
Epoch 614/1000 
	 loss: 27.1782, MinusLogProbMetric: 27.1782, val_loss: 28.2552, val_MinusLogProbMetric: 28.2552

Epoch 614: val_loss did not improve from 28.21968
196/196 - 52s - loss: 27.1782 - MinusLogProbMetric: 27.1782 - val_loss: 28.2552 - val_MinusLogProbMetric: 28.2552 - lr: 6.2500e-05 - 52s/epoch - 267ms/step
Epoch 615/1000
2023-10-02 09:45:44.542 
Epoch 615/1000 
	 loss: 27.1880, MinusLogProbMetric: 27.1880, val_loss: 28.3064, val_MinusLogProbMetric: 28.3064

Epoch 615: val_loss did not improve from 28.21968
196/196 - 55s - loss: 27.1880 - MinusLogProbMetric: 27.1880 - val_loss: 28.3064 - val_MinusLogProbMetric: 28.3064 - lr: 6.2500e-05 - 55s/epoch - 282ms/step
Epoch 616/1000
2023-10-02 09:46:36.724 
Epoch 616/1000 
	 loss: 27.1829, MinusLogProbMetric: 27.1829, val_loss: 28.2280, val_MinusLogProbMetric: 28.2280

Epoch 616: val_loss did not improve from 28.21968
196/196 - 52s - loss: 27.1829 - MinusLogProbMetric: 27.1829 - val_loss: 28.2280 - val_MinusLogProbMetric: 28.2280 - lr: 6.2500e-05 - 52s/epoch - 266ms/step
Epoch 617/1000
2023-10-02 09:47:32.711 
Epoch 617/1000 
	 loss: 27.1891, MinusLogProbMetric: 27.1891, val_loss: 28.2463, val_MinusLogProbMetric: 28.2463

Epoch 617: val_loss did not improve from 28.21968
196/196 - 56s - loss: 27.1891 - MinusLogProbMetric: 27.1891 - val_loss: 28.2463 - val_MinusLogProbMetric: 28.2463 - lr: 6.2500e-05 - 56s/epoch - 286ms/step
Epoch 618/1000
2023-10-02 09:48:25.550 
Epoch 618/1000 
	 loss: 27.1752, MinusLogProbMetric: 27.1752, val_loss: 28.4104, val_MinusLogProbMetric: 28.4104

Epoch 618: val_loss did not improve from 28.21968
196/196 - 53s - loss: 27.1752 - MinusLogProbMetric: 27.1752 - val_loss: 28.4104 - val_MinusLogProbMetric: 28.4104 - lr: 6.2500e-05 - 53s/epoch - 270ms/step
Epoch 619/1000
2023-10-02 09:49:18.015 
Epoch 619/1000 
	 loss: 27.2134, MinusLogProbMetric: 27.2134, val_loss: 28.2299, val_MinusLogProbMetric: 28.2299

Epoch 619: val_loss did not improve from 28.21968
196/196 - 52s - loss: 27.2134 - MinusLogProbMetric: 27.2134 - val_loss: 28.2299 - val_MinusLogProbMetric: 28.2299 - lr: 6.2500e-05 - 52s/epoch - 268ms/step
Epoch 620/1000
2023-10-02 09:50:14.058 
Epoch 620/1000 
	 loss: 27.1880, MinusLogProbMetric: 27.1880, val_loss: 28.2211, val_MinusLogProbMetric: 28.2211

Epoch 620: val_loss did not improve from 28.21968
196/196 - 56s - loss: 27.1880 - MinusLogProbMetric: 27.1880 - val_loss: 28.2211 - val_MinusLogProbMetric: 28.2211 - lr: 6.2500e-05 - 56s/epoch - 286ms/step
Epoch 621/1000
2023-10-02 09:51:09.201 
Epoch 621/1000 
	 loss: 27.1823, MinusLogProbMetric: 27.1823, val_loss: 28.2345, val_MinusLogProbMetric: 28.2345

Epoch 621: val_loss did not improve from 28.21968
196/196 - 55s - loss: 27.1823 - MinusLogProbMetric: 27.1823 - val_loss: 28.2345 - val_MinusLogProbMetric: 28.2345 - lr: 6.2500e-05 - 55s/epoch - 281ms/step
Epoch 622/1000
2023-10-02 09:52:01.901 
Epoch 622/1000 
	 loss: 27.1833, MinusLogProbMetric: 27.1833, val_loss: 28.2337, val_MinusLogProbMetric: 28.2337

Epoch 622: val_loss did not improve from 28.21968
196/196 - 53s - loss: 27.1833 - MinusLogProbMetric: 27.1833 - val_loss: 28.2337 - val_MinusLogProbMetric: 28.2337 - lr: 6.2500e-05 - 53s/epoch - 269ms/step
Epoch 623/1000
2023-10-02 09:52:57.773 
Epoch 623/1000 
	 loss: 27.1782, MinusLogProbMetric: 27.1782, val_loss: 28.2432, val_MinusLogProbMetric: 28.2432

Epoch 623: val_loss did not improve from 28.21968
196/196 - 56s - loss: 27.1782 - MinusLogProbMetric: 27.1782 - val_loss: 28.2432 - val_MinusLogProbMetric: 28.2432 - lr: 6.2500e-05 - 56s/epoch - 285ms/step
Epoch 624/1000
2023-10-02 09:53:52.606 
Epoch 624/1000 
	 loss: 27.1775, MinusLogProbMetric: 27.1775, val_loss: 28.2855, val_MinusLogProbMetric: 28.2855

Epoch 624: val_loss did not improve from 28.21968
196/196 - 55s - loss: 27.1775 - MinusLogProbMetric: 27.1775 - val_loss: 28.2855 - val_MinusLogProbMetric: 28.2855 - lr: 6.2500e-05 - 55s/epoch - 280ms/step
Epoch 625/1000
2023-10-02 09:54:48.074 
Epoch 625/1000 
	 loss: 27.1868, MinusLogProbMetric: 27.1868, val_loss: 28.2217, val_MinusLogProbMetric: 28.2217

Epoch 625: val_loss did not improve from 28.21968
196/196 - 55s - loss: 27.1868 - MinusLogProbMetric: 27.1868 - val_loss: 28.2217 - val_MinusLogProbMetric: 28.2217 - lr: 6.2500e-05 - 55s/epoch - 283ms/step
Epoch 626/1000
2023-10-02 09:55:42.449 
Epoch 626/1000 
	 loss: 27.1924, MinusLogProbMetric: 27.1924, val_loss: 28.3264, val_MinusLogProbMetric: 28.3264

Epoch 626: val_loss did not improve from 28.21968
196/196 - 54s - loss: 27.1924 - MinusLogProbMetric: 27.1924 - val_loss: 28.3264 - val_MinusLogProbMetric: 28.3264 - lr: 6.2500e-05 - 54s/epoch - 277ms/step
Epoch 627/1000
2023-10-02 09:56:37.242 
Epoch 627/1000 
	 loss: 27.1849, MinusLogProbMetric: 27.1849, val_loss: 28.2337, val_MinusLogProbMetric: 28.2337

Epoch 627: val_loss did not improve from 28.21968
196/196 - 55s - loss: 27.1849 - MinusLogProbMetric: 27.1849 - val_loss: 28.2337 - val_MinusLogProbMetric: 28.2337 - lr: 6.2500e-05 - 55s/epoch - 280ms/step
Epoch 628/1000
2023-10-02 09:57:31.443 
Epoch 628/1000 
	 loss: 27.1550, MinusLogProbMetric: 27.1550, val_loss: 28.2373, val_MinusLogProbMetric: 28.2373

Epoch 628: val_loss did not improve from 28.21968
196/196 - 54s - loss: 27.1550 - MinusLogProbMetric: 27.1550 - val_loss: 28.2373 - val_MinusLogProbMetric: 28.2373 - lr: 3.1250e-05 - 54s/epoch - 276ms/step
Epoch 629/1000
2023-10-02 09:58:25.451 
Epoch 629/1000 
	 loss: 27.1500, MinusLogProbMetric: 27.1500, val_loss: 28.2276, val_MinusLogProbMetric: 28.2276

Epoch 629: val_loss did not improve from 28.21968
196/196 - 54s - loss: 27.1500 - MinusLogProbMetric: 27.1500 - val_loss: 28.2276 - val_MinusLogProbMetric: 28.2276 - lr: 3.1250e-05 - 54s/epoch - 276ms/step
Epoch 630/1000
2023-10-02 09:59:17.765 
Epoch 630/1000 
	 loss: 27.1490, MinusLogProbMetric: 27.1490, val_loss: 28.2274, val_MinusLogProbMetric: 28.2274

Epoch 630: val_loss did not improve from 28.21968
196/196 - 52s - loss: 27.1490 - MinusLogProbMetric: 27.1490 - val_loss: 28.2274 - val_MinusLogProbMetric: 28.2274 - lr: 3.1250e-05 - 52s/epoch - 267ms/step
Epoch 631/1000
