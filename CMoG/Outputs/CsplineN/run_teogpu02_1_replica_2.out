2023-10-23 12:40:16.207971: Importing os...
2023-10-23 12:40:16.208051: Importing sys...
2023-10-23 12:40:16.208070: Importing and initializing argparse...
Visible devices: [1]
2023-10-23 12:40:16.233286: Importing timer from timeit...
2023-10-23 12:40:16.234172: Setting env variables for tf import (only device [1] will be available)...
2023-10-23 12:40:16.234241: Importing numpy...
2023-10-23 12:40:16.391685: Importing pandas...
2023-10-23 12:40:16.596890: Importing shutil...
2023-10-23 12:40:16.596921: Importing subprocess...
2023-10-23 12:40:16.596930: Importing tensorflow...
Tensorflow version: 2.12.0
2023-10-23 12:40:18.664285: Importing tensorflow_probability...
Tensorflow probability version: 0.20.1
2023-10-23 12:40:19.047574: Importing textwrap...
2023-10-23 12:40:19.047609: Importing timeit...
2023-10-23 12:40:19.047620: Importing traceback...
2023-10-23 12:40:19.047627: Importing typing...
2023-10-23 12:40:19.047638: Setting tf configs...
2023-10-23 12:40:19.243726: Importing custom module...
Successfully loaded GPU model: NVIDIA A40
2023-10-23 12:40:20.693835: All modues imported successfully.
Directory ../../results/CsplineN_new/ already exists.
Directory ../../results/CsplineN_new/run_1/ already exists.
Skipping it.
===========
Run 1/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_2/ already exists.
Skipping it.
===========
Run 2/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_3/ already exists.
Skipping it.
===========
Run 3/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_4/ already exists.
Skipping it.
===========
Run 4/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_5/ already exists.
Skipping it.
===========
Run 5/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_6/ already exists.
Skipping it.
===========
Run 6/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_7/ already exists.
Skipping it.
===========
Run 7/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_8/ already exists.
Skipping it.
===========
Run 8/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_9/ already exists.
Skipping it.
===========
Run 9/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_10/ already exists.
Skipping it.
===========
Run 10/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_11/ already exists.
Skipping it.
===========
Run 11/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_12/ already exists.
Skipping it.
===========
Run 12/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_13/ already exists.
Skipping it.
===========
Run 13/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_14/ already exists.
Skipping it.
===========
Run 14/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_15/ already exists.
Skipping it.
===========
Run 15/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_16/ already exists.
Skipping it.
===========
Run 16/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_17/ already exists.
Skipping it.
===========
Run 17/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_18/ already exists.
Skipping it.
===========
Run 18/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_19/ already exists.
Skipping it.
===========
Run 19/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_20/ already exists.
Skipping it.
===========
Run 20/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_21/ already exists.
Skipping it.
===========
Run 21/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_22/ already exists.
Skipping it.
===========
Run 22/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_23/ already exists.
Skipping it.
===========
Run 23/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_24/ already exists.
Skipping it.
===========
Run 24/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_25/ already exists.
Skipping it.
===========
Run 25/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_26/ already exists.
Skipping it.
===========
Run 26/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_27/ already exists.
Skipping it.
===========
Run 27/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_28/ already exists.
Skipping it.
===========
Run 28/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_29/ already exists.
Skipping it.
===========
Run 29/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_30/ already exists.
Skipping it.
===========
Run 30/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_31/ already exists.
Skipping it.
===========
Run 31/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_32/ already exists.
Skipping it.
===========
Run 32/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_33/ already exists.
Skipping it.
===========
Run 33/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_34/ already exists.
Skipping it.
===========
Run 34/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_35/ already exists.
Skipping it.
===========
Run 35/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_36/ already exists.
Skipping it.
===========
Run 36/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_37/ already exists.
Skipping it.
===========
Run 37/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_38/ already exists.
Skipping it.
===========
Run 38/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_39/ already exists.
Skipping it.
===========
Run 39/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_40/ already exists.
Skipping it.
===========
Run 40/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_41/ already exists.
Skipping it.
===========
Run 41/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_42/ already exists.
Skipping it.
===========
Run 42/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_43/ already exists.
Skipping it.
===========
Run 43/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_44/ already exists.
Skipping it.
===========
Run 44/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_45/ already exists.
Skipping it.
===========
Run 45/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_46/ already exists.
Skipping it.
===========
Run 46/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_47/ already exists.
Skipping it.
===========
Run 47/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_48/ already exists.
Skipping it.
===========
Run 48/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_49/ already exists.
Skipping it.
===========
Run 49/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_50/ already exists.
Skipping it.
===========
Run 50/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_51/ already exists.
Skipping it.
===========
Run 51/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_52/ already exists.
Skipping it.
===========
Run 52/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_53/ already exists.
Skipping it.
===========
Run 53/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_54/ already exists.
Skipping it.
===========
Run 54/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_55/ already exists.
Skipping it.
===========
Run 55/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_56/ already exists.
Skipping it.
===========
Run 56/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_57/ already exists.
Skipping it.
===========
Run 57/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_58/ already exists.
Skipping it.
===========
Run 58/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_59/ already exists.
Skipping it.
===========
Run 59/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_60/ already exists.
Skipping it.
===========
Run 60/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_61/ already exists.
Skipping it.
===========
Run 61/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_62/ already exists.
Skipping it.
===========
Run 62/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_63/ already exists.
Skipping it.
===========
Run 63/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_64/ already exists.
Skipping it.
===========
Run 64/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_65/ already exists.
Skipping it.
===========
Run 65/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_66/ already exists.
Skipping it.
===========
Run 66/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_67/ already exists.
Skipping it.
===========
Run 67/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_68/ already exists.
Skipping it.
===========
Run 68/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_69/ already exists.
Skipping it.
===========
Run 69/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_70/ already exists.
Skipping it.
===========
Run 70/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_71/ already exists.
Skipping it.
===========
Run 71/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_72/ already exists.
Skipping it.
===========
Run 72/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_73/ already exists.
Skipping it.
===========
Run 73/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_74/ already exists.
Skipping it.
===========
Run 74/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_75/ already exists.
Skipping it.
===========
Run 75/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_76/ already exists.
Skipping it.
===========
Run 76/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_77/ already exists.
Skipping it.
===========
Run 77/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_78/ already exists.
Skipping it.
===========
Run 78/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_79/ already exists.
Skipping it.
===========
Run 79/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_80/ already exists.
Skipping it.
===========
Run 80/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_81/ already exists.
Skipping it.
===========
Run 81/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_82/ already exists.
Skipping it.
===========
Run 82/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_83/ already exists.
Skipping it.
===========
Run 83/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_84/ already exists.
Skipping it.
===========
Run 84/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_85/ already exists.
Skipping it.
===========
Run 85/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_86/ already exists.
Skipping it.
===========
Run 86/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_87/ already exists.
Skipping it.
===========
Run 87/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_88/ already exists.
Skipping it.
===========
Run 88/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_89/ already exists.
Skipping it.
===========
Run 89/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_90/ already exists.
Skipping it.
===========
Run 90/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_91/ already exists.
Skipping it.
===========
Run 91/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_92/ already exists.
Skipping it.
===========
Run 92/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_93/ already exists.
Skipping it.
===========
Run 93/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_94/ already exists.
Skipping it.
===========
Run 94/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_95/ already exists.
Skipping it.
===========
Run 95/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_96/ already exists.
Skipping it.
===========
Run 96/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_97/ already exists.
Skipping it.
===========
Run 97/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_98/ already exists.
Skipping it.
===========
Run 98/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_99/ already exists.
Skipping it.
===========
Run 99/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_100/ already exists.
Skipping it.
===========
Run 100/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_101/ already exists.
Skipping it.
===========
Run 101/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_102/ already exists.
Skipping it.
===========
Run 102/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_103/ already exists.
Skipping it.
===========
Run 103/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_104/ already exists.
Skipping it.
===========
Run 104/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_105/ already exists.
Skipping it.
===========
Run 105/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_106/ already exists.
Skipping it.
===========
Run 106/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_107/ already exists.
Skipping it.
===========
Run 107/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_108/ already exists.
Skipping it.
===========
Run 108/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_109/ already exists.
Skipping it.
===========
Run 109/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_110/ already exists.
Skipping it.
===========
Run 110/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_111/ already exists.
Skipping it.
===========
Run 111/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_112/ already exists.
Skipping it.
===========
Run 112/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_113/ already exists.
Skipping it.
===========
Run 113/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_114/ already exists.
Skipping it.
===========
Run 114/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_115/ already exists.
Skipping it.
===========
Run 115/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_116/ already exists.
Skipping it.
===========
Run 116/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_117/ already exists.
Skipping it.
===========
Run 117/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_118/ already exists.
Skipping it.
===========
Run 118/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_119/ already exists.
Skipping it.
===========
Run 119/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_120/ already exists.
Skipping it.
===========
Run 120/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_121/ already exists.
Skipping it.
===========
Run 121/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_122/ already exists.
Skipping it.
===========
Run 122/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_123/ already exists.
Skipping it.
===========
Run 123/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_124/ already exists.
Skipping it.
===========
Run 124/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_125/ already exists.
Skipping it.
===========
Run 125/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_126/ already exists.
Skipping it.
===========
Run 126/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_127/ already exists.
Skipping it.
===========
Run 127/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_128/ already exists.
Skipping it.
===========
Run 128/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_129/ already exists.
Skipping it.
===========
Run 129/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_130/ already exists.
Skipping it.
===========
Run 130/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_131/ already exists.
Skipping it.
===========
Run 131/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_132/ already exists.
Skipping it.
===========
Run 132/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_133/ already exists.
Skipping it.
===========
Run 133/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_134/ already exists.
Skipping it.
===========
Run 134/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_135/ already exists.
Skipping it.
===========
Run 135/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_136/ already exists.
Skipping it.
===========
Run 136/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_137/ already exists.
Skipping it.
===========
Run 137/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_138/ already exists.
Skipping it.
===========
Run 138/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_139/ already exists.
Skipping it.
===========
Run 139/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_140/ already exists.
Skipping it.
===========
Run 140/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_141/ already exists.
Skipping it.
===========
Run 141/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_142/ already exists.
Skipping it.
===========
Run 142/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_143/ already exists.
Skipping it.
===========
Run 143/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_144/ already exists.
Skipping it.
===========
Run 144/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_145/ already exists.
Skipping it.
===========
Run 145/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_146/ already exists.
Skipping it.
===========
Run 146/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_147/ already exists.
Skipping it.
===========
Run 147/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_148/ already exists.
Skipping it.
===========
Run 148/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_149/ already exists.
Skipping it.
===========
Run 149/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_150/ already exists.
Skipping it.
===========
Run 150/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_151/ already exists.
Skipping it.
===========
Run 151/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_152/ already exists.
Skipping it.
===========
Run 152/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_153/ already exists.
Skipping it.
===========
Run 153/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_154/ already exists.
Skipping it.
===========
Run 154/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_155/ already exists.
Skipping it.
===========
Run 155/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_156/ already exists.
Skipping it.
===========
Run 156/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_157/ already exists.
Skipping it.
===========
Run 157/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_158/ already exists.
Skipping it.
===========
Run 158/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_159/ already exists.
Skipping it.
===========
Run 159/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_160/ already exists.
Skipping it.
===========
Run 160/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_161/ already exists.
Skipping it.
===========
Run 161/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_162/ already exists.
Skipping it.
===========
Run 162/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_163/ already exists.
Skipping it.
===========
Run 163/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_164/ already exists.
Skipping it.
===========
Run 164/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_165/ already exists.
Skipping it.
===========
Run 165/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_166/ already exists.
Skipping it.
===========
Run 166/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_167/ already exists.
Skipping it.
===========
Run 167/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_168/ already exists.
Skipping it.
===========
Run 168/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_169/ already exists.
Skipping it.
===========
Run 169/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_170/ already exists.
Skipping it.
===========
Run 170/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_171/ already exists.
Skipping it.
===========
Run 171/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_172/ already exists.
Skipping it.
===========
Run 172/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_173/ already exists.
Skipping it.
===========
Run 173/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_174/ already exists.
Skipping it.
===========
Run 174/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_175/ already exists.
Skipping it.
===========
Run 175/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_176/ already exists.
Skipping it.
===========
Run 176/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_177/ already exists.
Skipping it.
===========
Run 177/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_178/ already exists.
Skipping it.
===========
Run 178/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_179/ already exists.
Skipping it.
===========
Run 179/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_180/ already exists.
Skipping it.
===========
Run 180/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_181/ already exists.
Skipping it.
===========
Run 181/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_182/ already exists.
Skipping it.
===========
Run 182/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_183/ already exists.
Skipping it.
===========
Run 183/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_184/ already exists.
Skipping it.
===========
Run 184/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_185/ already exists.
Skipping it.
===========
Run 185/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_186/ already exists.
Skipping it.
===========
Run 186/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_187/ already exists.
Skipping it.
===========
Run 187/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_188/ already exists.
Skipping it.
===========
Run 188/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_189/ already exists.
Skipping it.
===========
Run 189/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_190/ already exists.
Skipping it.
===========
Run 190/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_191/ already exists.
Skipping it.
===========
Run 191/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_192/ already exists.
Skipping it.
===========
Run 192/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_193/ already exists.
Skipping it.
===========
Run 193/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_194/ already exists.
Skipping it.
===========
Run 194/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_195/ already exists.
Skipping it.
===========
Run 195/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_196/ already exists.
Skipping it.
===========
Run 196/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_197/ already exists.
Skipping it.
===========
Run 197/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_198/ already exists.
Skipping it.
===========
Run 198/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_199/ already exists.
Skipping it.
===========
Run 199/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_200/ already exists.
Skipping it.
===========
Run 200/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_201/ already exists.
Skipping it.
===========
Run 201/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_202/ already exists.
Skipping it.
===========
Run 202/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_203/ already exists.
Skipping it.
===========
Run 203/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_204/ already exists.
Skipping it.
===========
Run 204/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_205/ already exists.
Skipping it.
===========
Run 205/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_206/ already exists.
Skipping it.
===========
Run 206/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_207/ already exists.
Skipping it.
===========
Run 207/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_208/ already exists.
Skipping it.
===========
Run 208/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_209/ already exists.
Skipping it.
===========
Run 209/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_210/ already exists.
Skipping it.
===========
Run 210/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_211/ already exists.
Skipping it.
===========
Run 211/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_212/ already exists.
Skipping it.
===========
Run 212/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_213/ already exists.
Skipping it.
===========
Run 213/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_214/ already exists.
Skipping it.
===========
Run 214/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_215/ already exists.
Skipping it.
===========
Run 215/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_216/ already exists.
Skipping it.
===========
Run 216/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_217/ already exists.
Skipping it.
===========
Run 217/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_218/ already exists.
Skipping it.
===========
Run 218/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_219/ already exists.
Skipping it.
===========
Run 219/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_220/ already exists.
Skipping it.
===========
Run 220/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_221/ already exists.
Skipping it.
===========
Run 221/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_222/ already exists.
Skipping it.
===========
Run 222/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_223/ already exists.
Skipping it.
===========
Run 223/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_224/ already exists.
Skipping it.
===========
Run 224/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_225/ already exists.
Skipping it.
===========
Run 225/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_226/ already exists.
Skipping it.
===========
Run 226/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_227/ already exists.
Skipping it.
===========
Run 227/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_228/ already exists.
Skipping it.
===========
Run 228/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_229/ already exists.
Skipping it.
===========
Run 229/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_230/ already exists.
Skipping it.
===========
Run 230/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_231/ already exists.
Skipping it.
===========
Run 231/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_232/ already exists.
Skipping it.
===========
Run 232/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_233/ already exists.
Skipping it.
===========
Run 233/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_234/ already exists.
Skipping it.
===========
Run 234/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_235/ already exists.
Skipping it.
===========
Run 235/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_236/ already exists.
Skipping it.
===========
Run 236/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_237/ already exists.
Skipping it.
===========
Run 237/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_238/ already exists.
Skipping it.
===========
Run 238/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_239/ already exists.
Skipping it.
===========
Run 239/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_240/ already exists.
Skipping it.
===========
Run 240/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_241/ already exists.
Skipping it.
===========
Run 241/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_242/ already exists.
Skipping it.
===========
Run 242/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_243/ already exists.
Skipping it.
===========
Run 243/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_244/ already exists.
Skipping it.
===========
Run 244/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_245/ already exists.
Skipping it.
===========
Run 245/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_246/ already exists.
Skipping it.
===========
Run 246/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_247/ already exists.
Skipping it.
===========
Run 247/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_248/ already exists.
Skipping it.
===========
Run 248/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_249/ already exists.
Skipping it.
===========
Run 249/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_250/ already exists.
Skipping it.
===========
Run 250/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_251/ already exists.
Skipping it.
===========
Run 251/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_252/ already exists.
Skipping it.
===========
Run 252/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_253/ already exists.
Skipping it.
===========
Run 253/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_254/ already exists.
Skipping it.
===========
Run 254/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_255/ already exists.
Skipping it.
===========
Run 255/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_256/ already exists.
Skipping it.
===========
Run 256/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_257/ already exists.
Skipping it.
===========
Run 257/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_258/ already exists.
Skipping it.
===========
Run 258/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_259/ already exists.
Skipping it.
===========
Run 259/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_260/ already exists.
Skipping it.
===========
Run 260/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_261/ already exists.
Skipping it.
===========
Run 261/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_262/ already exists.
Skipping it.
===========
Run 262/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_263/ already exists.
Skipping it.
===========
Run 263/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_264/ already exists.
Skipping it.
===========
Run 264/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_265/ already exists.
Skipping it.
===========
Run 265/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_266/ already exists.
Skipping it.
===========
Run 266/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_267/ already exists.
Skipping it.
===========
Run 267/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_268/ already exists.
Skipping it.
===========
Run 268/720 already exists. Skipping it.
===========

===========
Generating train data for run 269.
===========
Train data generated in 0.40 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_269/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_269/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.569926  ,  5.931914  ,  1.0340854 , ...,  1.178907  ,
         6.4472156 ,  1.3327898 ],
       [ 6.2850404 ,  7.153691  ,  4.423192  , ...,  3.3972363 ,
         2.667123  ,  7.632936  ],
       [ 3.9655547 ,  5.522517  ,  0.75010407, ...,  1.7247348 ,
         6.692078  ,  1.3383894 ],
       ...,
       [ 4.8108435 ,  5.787278  , -0.11691105, ...,  0.15166378,
         6.433085  ,  1.4475136 ],
       [ 1.3107729 ,  3.3536706 ,  8.5924425 , ...,  6.826266  ,
         2.8685987 ,  1.5047977 ],
       [ 2.0164819 ,  3.0697684 ,  7.39879   , ...,  6.933044  ,
         2.57938   ,  2.0720963 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_269/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_269
self.data_kwargs: {'seed': 440}
self.x_data: [[ 4.667814    5.728588   -0.61503386 ...  0.95054436  6.8364124
   1.4128516 ]
 [ 2.3392496   3.3370924   7.9916444  ...  7.2833347   3.313804
   1.7574413 ]
 [ 5.6111684   5.037572   -0.7357439  ...  2.1567051   6.6985016
   1.3668824 ]
 ...
 [ 2.8990598   4.2636833   8.734106   ...  7.059716    2.8529127
   1.343578  ]
 [ 2.64989     3.1235895   8.579933   ...  6.7448006   2.6647887
   2.1982503 ]
 [ 3.925435    5.6032453  -0.7142817  ...  0.51811683  6.7432346
   1.4061358 ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_10"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_11 (InputLayer)       [(None, 32)]              0         
                                                                 
 log_prob_layer (LogProbLaye  (None,)                  826720    
 r)                                                              
                                                                 
=================================================================
Total params: 826,720
Trainable params: 826,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer'")
self.model: <keras.engine.functional.Functional object at 0x7f5c687f2ef0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f5c44161f90>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f5c44161f90>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f5c687f2140>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f5c1c7dbc10>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_269/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f5c1c4fc1c0>, <keras.callbacks.ModelCheckpoint object at 0x7f5c1c4fc310>, <keras.callbacks.EarlyStopping object at 0x7f5c1c4fc520>, <keras.callbacks.ReduceLROnPlateau object at 0x7f5c1c4fc550>, <keras.callbacks.TerminateOnNaN object at 0x7f5c1c4fc280>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.569926  ,  5.931914  ,  1.0340854 , ...,  1.178907  ,
         6.4472156 ,  1.3327898 ],
       [ 6.2850404 ,  7.153691  ,  4.423192  , ...,  3.3972363 ,
         2.667123  ,  7.632936  ],
       [ 3.9655547 ,  5.522517  ,  0.75010407, ...,  1.7247348 ,
         6.692078  ,  1.3383894 ],
       ...,
       [ 4.8108435 ,  5.787278  , -0.11691105, ...,  0.15166378,
         6.433085  ,  1.4475136 ],
       [ 1.3107729 ,  3.3536706 ,  8.5924425 , ...,  6.826266  ,
         2.8685987 ,  1.5047977 ],
       [ 2.0164819 ,  3.0697684 ,  7.39879   , ...,  6.933044  ,
         2.57938   ,  2.0720963 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_269/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 269/720 with hyperparameters:
timestamp = 2023-10-23 12:40:31.495381
ndims = 32
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 826720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 4.667814    5.728588   -0.61503386  6.5363817   6.270228    6.41928
  9.1463785   7.6107693   3.2592764   4.501069    7.6099105   0.9604095
  5.968021    7.1344857   2.535179    0.9243109   4.2925086   4.152452
  5.719896    3.7976222  10.506067    2.5067234   2.243979    2.6931024
  6.181856    1.4198786   4.5468354   2.3517177   0.97684515  0.95054436
  6.8364124   1.4128516 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-23 12:42:33.630 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 3449.4939, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 122s - loss: nan - MinusLogProbMetric: 3449.4939 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 122s/epoch - 622ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 0.0003333333333333333.
===========
Generating train data for run 269.
===========
Train data generated in 0.19 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_269/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_269/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.569926  ,  5.931914  ,  1.0340854 , ...,  1.178907  ,
         6.4472156 ,  1.3327898 ],
       [ 6.2850404 ,  7.153691  ,  4.423192  , ...,  3.3972363 ,
         2.667123  ,  7.632936  ],
       [ 3.9655547 ,  5.522517  ,  0.75010407, ...,  1.7247348 ,
         6.692078  ,  1.3383894 ],
       ...,
       [ 4.8108435 ,  5.787278  , -0.11691105, ...,  0.15166378,
         6.433085  ,  1.4475136 ],
       [ 1.3107729 ,  3.3536706 ,  8.5924425 , ...,  6.826266  ,
         2.8685987 ,  1.5047977 ],
       [ 2.0164819 ,  3.0697684 ,  7.39879   , ...,  6.933044  ,
         2.57938   ,  2.0720963 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_269/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_269
self.data_kwargs: {'seed': 440}
self.x_data: [[ 4.667814    5.728588   -0.61503386 ...  0.95054436  6.8364124
   1.4128516 ]
 [ 2.3392496   3.3370924   7.9916444  ...  7.2833347   3.313804
   1.7574413 ]
 [ 5.6111684   5.037572   -0.7357439  ...  2.1567051   6.6985016
   1.3668824 ]
 ...
 [ 2.8990598   4.2636833   8.734106   ...  7.059716    2.8529127
   1.343578  ]
 [ 2.64989     3.1235895   8.579933   ...  6.7448006   2.6647887
   2.1982503 ]
 [ 3.925435    5.6032453  -0.7142817  ...  0.51811683  6.7432346
   1.4061358 ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_21"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_22 (InputLayer)       [(None, 32)]              0         
                                                                 
 log_prob_layer_1 (LogProbLa  (None,)                  826720    
 yer)                                                            
                                                                 
=================================================================
Total params: 826,720
Trainable params: 826,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_1/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_1'")
self.model: <keras.engine.functional.Functional object at 0x7f602879e320>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f60283afc40>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f60283afc40>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f60280d2d10>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f6027e939a0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_269/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f6027e93f10>, <keras.callbacks.ModelCheckpoint object at 0x7f6027e93fd0>, <keras.callbacks.EarlyStopping object at 0x7f6027e93ee0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f6027e93eb0>, <keras.callbacks.TerminateOnNaN object at 0x7f6027ee0280>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.569926  ,  5.931914  ,  1.0340854 , ...,  1.178907  ,
         6.4472156 ,  1.3327898 ],
       [ 6.2850404 ,  7.153691  ,  4.423192  , ...,  3.3972363 ,
         2.667123  ,  7.632936  ],
       [ 3.9655547 ,  5.522517  ,  0.75010407, ...,  1.7247348 ,
         6.692078  ,  1.3383894 ],
       ...,
       [ 4.8108435 ,  5.787278  , -0.11691105, ...,  0.15166378,
         6.433085  ,  1.4475136 ],
       [ 1.3107729 ,  3.3536706 ,  8.5924425 , ...,  6.826266  ,
         2.8685987 ,  1.5047977 ],
       [ 2.0164819 ,  3.0697684 ,  7.39879   , ...,  6.933044  ,
         2.57938   ,  2.0720963 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_269/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 269/720 with hyperparameters:
timestamp = 2023-10-23 12:42:41.682321
ndims = 32
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 826720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 4.667814    5.728588   -0.61503386  6.5363817   6.270228    6.41928
  9.1463785   7.6107693   3.2592764   4.501069    7.6099105   0.9604095
  5.968021    7.1344857   2.535179    0.9243109   4.2925086   4.152452
  5.719896    3.7976222  10.506067    2.5067234   2.243979    2.6931024
  6.181856    1.4198786   4.5468354   2.3517177   0.97684515  0.95054436
  6.8364124   1.4128516 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-23 12:44:47.409 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 3449.4939, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 125s - loss: nan - MinusLogProbMetric: 3449.4939 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 125s/epoch - 640ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 0.0001111111111111111.
===========
Generating train data for run 269.
===========
Train data generated in 0.22 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_269/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_269/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.569926  ,  5.931914  ,  1.0340854 , ...,  1.178907  ,
         6.4472156 ,  1.3327898 ],
       [ 6.2850404 ,  7.153691  ,  4.423192  , ...,  3.3972363 ,
         2.667123  ,  7.632936  ],
       [ 3.9655547 ,  5.522517  ,  0.75010407, ...,  1.7247348 ,
         6.692078  ,  1.3383894 ],
       ...,
       [ 4.8108435 ,  5.787278  , -0.11691105, ...,  0.15166378,
         6.433085  ,  1.4475136 ],
       [ 1.3107729 ,  3.3536706 ,  8.5924425 , ...,  6.826266  ,
         2.8685987 ,  1.5047977 ],
       [ 2.0164819 ,  3.0697684 ,  7.39879   , ...,  6.933044  ,
         2.57938   ,  2.0720963 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_269/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_269
self.data_kwargs: {'seed': 440}
self.x_data: [[ 4.667814    5.728588   -0.61503386 ...  0.95054436  6.8364124
   1.4128516 ]
 [ 2.3392496   3.3370924   7.9916444  ...  7.2833347   3.313804
   1.7574413 ]
 [ 5.6111684   5.037572   -0.7357439  ...  2.1567051   6.6985016
   1.3668824 ]
 ...
 [ 2.8990598   4.2636833   8.734106   ...  7.059716    2.8529127
   1.343578  ]
 [ 2.64989     3.1235895   8.579933   ...  6.7448006   2.6647887
   2.1982503 ]
 [ 3.925435    5.6032453  -0.7142817  ...  0.51811683  6.7432346
   1.4061358 ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_32"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_33 (InputLayer)       [(None, 32)]              0         
                                                                 
 log_prob_layer_2 (LogProbLa  (None,)                  826720    
 yer)                                                            
                                                                 
=================================================================
Total params: 826,720
Trainable params: 826,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_2/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_2'")
self.model: <keras.engine.functional.Functional object at 0x7f59782bf430>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f5a1043bf10>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f5a1043bf10>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f5bb06c8400>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f5a101d4f10>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_269/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f5a101d5480>, <keras.callbacks.ModelCheckpoint object at 0x7f5a101d5540>, <keras.callbacks.EarlyStopping object at 0x7f5a101d57b0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f5a101d57e0>, <keras.callbacks.TerminateOnNaN object at 0x7f5a101d5420>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.569926  ,  5.931914  ,  1.0340854 , ...,  1.178907  ,
         6.4472156 ,  1.3327898 ],
       [ 6.2850404 ,  7.153691  ,  4.423192  , ...,  3.3972363 ,
         2.667123  ,  7.632936  ],
       [ 3.9655547 ,  5.522517  ,  0.75010407, ...,  1.7247348 ,
         6.692078  ,  1.3383894 ],
       ...,
       [ 4.8108435 ,  5.787278  , -0.11691105, ...,  0.15166378,
         6.433085  ,  1.4475136 ],
       [ 1.3107729 ,  3.3536706 ,  8.5924425 , ...,  6.826266  ,
         2.8685987 ,  1.5047977 ],
       [ 2.0164819 ,  3.0697684 ,  7.39879   , ...,  6.933044  ,
         2.57938   ,  2.0720963 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_269/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 269/720 with hyperparameters:
timestamp = 2023-10-23 12:44:56.182320
ndims = 32
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 826720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 4.667814    5.728588   -0.61503386  6.5363817   6.270228    6.41928
  9.1463785   7.6107693   3.2592764   4.501069    7.6099105   0.9604095
  5.968021    7.1344857   2.535179    0.9243109   4.2925086   4.152452
  5.719896    3.7976222  10.506067    2.5067234   2.243979    2.6931024
  6.181856    1.4198786   4.5468354   2.3517177   0.97684515  0.95054436
  6.8364124   1.4128516 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-23 12:47:26.871 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 3449.4939, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 150s - loss: nan - MinusLogProbMetric: 3449.4939 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.1111e-04 - 150s/epoch - 768ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 3.703703703703703e-05.
===========
Generating train data for run 269.
===========
Train data generated in 0.20 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_269/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_269/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.569926  ,  5.931914  ,  1.0340854 , ...,  1.178907  ,
         6.4472156 ,  1.3327898 ],
       [ 6.2850404 ,  7.153691  ,  4.423192  , ...,  3.3972363 ,
         2.667123  ,  7.632936  ],
       [ 3.9655547 ,  5.522517  ,  0.75010407, ...,  1.7247348 ,
         6.692078  ,  1.3383894 ],
       ...,
       [ 4.8108435 ,  5.787278  , -0.11691105, ...,  0.15166378,
         6.433085  ,  1.4475136 ],
       [ 1.3107729 ,  3.3536706 ,  8.5924425 , ...,  6.826266  ,
         2.8685987 ,  1.5047977 ],
       [ 2.0164819 ,  3.0697684 ,  7.39879   , ...,  6.933044  ,
         2.57938   ,  2.0720963 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_269/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_269
self.data_kwargs: {'seed': 440}
self.x_data: [[ 4.667814    5.728588   -0.61503386 ...  0.95054436  6.8364124
   1.4128516 ]
 [ 2.3392496   3.3370924   7.9916444  ...  7.2833347   3.313804
   1.7574413 ]
 [ 5.6111684   5.037572   -0.7357439  ...  2.1567051   6.6985016
   1.3668824 ]
 ...
 [ 2.8990598   4.2636833   8.734106   ...  7.059716    2.8529127
   1.343578  ]
 [ 2.64989     3.1235895   8.579933   ...  6.7448006   2.6647887
   2.1982503 ]
 [ 3.925435    5.6032453  -0.7142817  ...  0.51811683  6.7432346
   1.4061358 ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_43"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_44 (InputLayer)       [(None, 32)]              0         
                                                                 
 log_prob_layer_3 (LogProbLa  (None,)                  826720    
 yer)                                                            
                                                                 
=================================================================
Total params: 826,720
Trainable params: 826,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_3/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_3'")
self.model: <keras.engine.functional.Functional object at 0x7f59791d7c10>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f6026cb5900>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f6026cb5900>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f5af0631120>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f6027d2ce20>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_269/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f6027d2d390>, <keras.callbacks.ModelCheckpoint object at 0x7f6027d2d450>, <keras.callbacks.EarlyStopping object at 0x7f6027d2d6c0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f6027d2d6f0>, <keras.callbacks.TerminateOnNaN object at 0x7f6027d2d330>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.569926  ,  5.931914  ,  1.0340854 , ...,  1.178907  ,
         6.4472156 ,  1.3327898 ],
       [ 6.2850404 ,  7.153691  ,  4.423192  , ...,  3.3972363 ,
         2.667123  ,  7.632936  ],
       [ 3.9655547 ,  5.522517  ,  0.75010407, ...,  1.7247348 ,
         6.692078  ,  1.3383894 ],
       ...,
       [ 4.8108435 ,  5.787278  , -0.11691105, ...,  0.15166378,
         6.433085  ,  1.4475136 ],
       [ 1.3107729 ,  3.3536706 ,  8.5924425 , ...,  6.826266  ,
         2.8685987 ,  1.5047977 ],
       [ 2.0164819 ,  3.0697684 ,  7.39879   , ...,  6.933044  ,
         2.57938   ,  2.0720963 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_269/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 269/720 with hyperparameters:
timestamp = 2023-10-23 12:47:35.675096
ndims = 32
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 826720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 3.703703703703703e-05...
Train first sample: [ 4.667814    5.728588   -0.61503386  6.5363817   6.270228    6.41928
  9.1463785   7.6107693   3.2592764   4.501069    7.6099105   0.9604095
  5.968021    7.1344857   2.535179    0.9243109   4.2925086   4.152452
  5.719896    3.7976222  10.506067    2.5067234   2.243979    2.6931024
  6.181856    1.4198786   4.5468354   2.3517177   0.97684515  0.95054436
  6.8364124   1.4128516 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-23 12:49:52.756 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 3449.4939, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 137s - loss: nan - MinusLogProbMetric: 3449.4939 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.7037e-05 - 137s/epoch - 698ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 1.2345679012345677e-05.
===========
Generating train data for run 269.
===========
Train data generated in 0.24 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_269/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_269/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.569926  ,  5.931914  ,  1.0340854 , ...,  1.178907  ,
         6.4472156 ,  1.3327898 ],
       [ 6.2850404 ,  7.153691  ,  4.423192  , ...,  3.3972363 ,
         2.667123  ,  7.632936  ],
       [ 3.9655547 ,  5.522517  ,  0.75010407, ...,  1.7247348 ,
         6.692078  ,  1.3383894 ],
       ...,
       [ 4.8108435 ,  5.787278  , -0.11691105, ...,  0.15166378,
         6.433085  ,  1.4475136 ],
       [ 1.3107729 ,  3.3536706 ,  8.5924425 , ...,  6.826266  ,
         2.8685987 ,  1.5047977 ],
       [ 2.0164819 ,  3.0697684 ,  7.39879   , ...,  6.933044  ,
         2.57938   ,  2.0720963 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_269/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_269
self.data_kwargs: {'seed': 440}
self.x_data: [[ 4.667814    5.728588   -0.61503386 ...  0.95054436  6.8364124
   1.4128516 ]
 [ 2.3392496   3.3370924   7.9916444  ...  7.2833347   3.313804
   1.7574413 ]
 [ 5.6111684   5.037572   -0.7357439  ...  2.1567051   6.6985016
   1.3668824 ]
 ...
 [ 2.8990598   4.2636833   8.734106   ...  7.059716    2.8529127
   1.343578  ]
 [ 2.64989     3.1235895   8.579933   ...  6.7448006   2.6647887
   2.1982503 ]
 [ 3.925435    5.6032453  -0.7142817  ...  0.51811683  6.7432346
   1.4061358 ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_54"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_55 (InputLayer)       [(None, 32)]              0         
                                                                 
 log_prob_layer_4 (LogProbLa  (None,)                  826720    
 yer)                                                            
                                                                 
=================================================================
Total params: 826,720
Trainable params: 826,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_4/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_4'")
self.model: <keras.engine.functional.Functional object at 0x7f5bb85f6740>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f5b02c28190>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f5b02c28190>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f5a941358a0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f597886b1f0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_269/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f597886b760>, <keras.callbacks.ModelCheckpoint object at 0x7f597886b820>, <keras.callbacks.EarlyStopping object at 0x7f597886ba90>, <keras.callbacks.ReduceLROnPlateau object at 0x7f597886bac0>, <keras.callbacks.TerminateOnNaN object at 0x7f597886b700>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.569926  ,  5.931914  ,  1.0340854 , ...,  1.178907  ,
         6.4472156 ,  1.3327898 ],
       [ 6.2850404 ,  7.153691  ,  4.423192  , ...,  3.3972363 ,
         2.667123  ,  7.632936  ],
       [ 3.9655547 ,  5.522517  ,  0.75010407, ...,  1.7247348 ,
         6.692078  ,  1.3383894 ],
       ...,
       [ 4.8108435 ,  5.787278  , -0.11691105, ...,  0.15166378,
         6.433085  ,  1.4475136 ],
       [ 1.3107729 ,  3.3536706 ,  8.5924425 , ...,  6.826266  ,
         2.8685987 ,  1.5047977 ],
       [ 2.0164819 ,  3.0697684 ,  7.39879   , ...,  6.933044  ,
         2.57938   ,  2.0720963 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_269/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 269/720 with hyperparameters:
timestamp = 2023-10-23 12:50:02.484749
ndims = 32
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 826720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.2345679012345677e-05...
Train first sample: [ 4.667814    5.728588   -0.61503386  6.5363817   6.270228    6.41928
  9.1463785   7.6107693   3.2592764   4.501069    7.6099105   0.9604095
  5.968021    7.1344857   2.535179    0.9243109   4.2925086   4.152452
  5.719896    3.7976222  10.506067    2.5067234   2.243979    2.6931024
  6.181856    1.4198786   4.5468354   2.3517177   0.97684515  0.95054436
  6.8364124   1.4128516 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_train_function.<locals>.train_function at 0x7f5aa43140d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-23 12:52:21.166 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 3449.4939, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 138s - loss: nan - MinusLogProbMetric: 3449.4939 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.2346e-05 - 138s/epoch - 706ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 4.115226337448558e-06.
===========
Generating train data for run 269.
===========
Train data generated in 0.20 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_269/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_269/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.569926  ,  5.931914  ,  1.0340854 , ...,  1.178907  ,
         6.4472156 ,  1.3327898 ],
       [ 6.2850404 ,  7.153691  ,  4.423192  , ...,  3.3972363 ,
         2.667123  ,  7.632936  ],
       [ 3.9655547 ,  5.522517  ,  0.75010407, ...,  1.7247348 ,
         6.692078  ,  1.3383894 ],
       ...,
       [ 4.8108435 ,  5.787278  , -0.11691105, ...,  0.15166378,
         6.433085  ,  1.4475136 ],
       [ 1.3107729 ,  3.3536706 ,  8.5924425 , ...,  6.826266  ,
         2.8685987 ,  1.5047977 ],
       [ 2.0164819 ,  3.0697684 ,  7.39879   , ...,  6.933044  ,
         2.57938   ,  2.0720963 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_269/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_269
self.data_kwargs: {'seed': 440}
self.x_data: [[ 4.667814    5.728588   -0.61503386 ...  0.95054436  6.8364124
   1.4128516 ]
 [ 2.3392496   3.3370924   7.9916444  ...  7.2833347   3.313804
   1.7574413 ]
 [ 5.6111684   5.037572   -0.7357439  ...  2.1567051   6.6985016
   1.3668824 ]
 ...
 [ 2.8990598   4.2636833   8.734106   ...  7.059716    2.8529127
   1.343578  ]
 [ 2.64989     3.1235895   8.579933   ...  6.7448006   2.6647887
   2.1982503 ]
 [ 3.925435    5.6032453  -0.7142817  ...  0.51811683  6.7432346
   1.4061358 ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_65"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_66 (InputLayer)       [(None, 32)]              0         
                                                                 
 log_prob_layer_5 (LogProbLa  (None,)                  826720    
 yer)                                                            
                                                                 
=================================================================
Total params: 826,720
Trainable params: 826,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_5/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_5'")
self.model: <keras.engine.functional.Functional object at 0x7f5a002878b0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f5bd8707a60>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f5bd8707a60>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f5a947d6020>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f5c1c3a4310>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_269/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f5c1c3a4880>, <keras.callbacks.ModelCheckpoint object at 0x7f5c1c3a4940>, <keras.callbacks.EarlyStopping object at 0x7f5c1c3a4bb0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f5c1c3a4be0>, <keras.callbacks.TerminateOnNaN object at 0x7f5c1c3a4820>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.569926  ,  5.931914  ,  1.0340854 , ...,  1.178907  ,
         6.4472156 ,  1.3327898 ],
       [ 6.2850404 ,  7.153691  ,  4.423192  , ...,  3.3972363 ,
         2.667123  ,  7.632936  ],
       [ 3.9655547 ,  5.522517  ,  0.75010407, ...,  1.7247348 ,
         6.692078  ,  1.3383894 ],
       ...,
       [ 4.8108435 ,  5.787278  , -0.11691105, ...,  0.15166378,
         6.433085  ,  1.4475136 ],
       [ 1.3107729 ,  3.3536706 ,  8.5924425 , ...,  6.826266  ,
         2.8685987 ,  1.5047977 ],
       [ 2.0164819 ,  3.0697684 ,  7.39879   , ...,  6.933044  ,
         2.57938   ,  2.0720963 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_269/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 269/720 with hyperparameters:
timestamp = 2023-10-23 12:52:30.736739
ndims = 32
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 826720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.115226337448558e-06...
Train first sample: [ 4.667814    5.728588   -0.61503386  6.5363817   6.270228    6.41928
  9.1463785   7.6107693   3.2592764   4.501069    7.6099105   0.9604095
  5.968021    7.1344857   2.535179    0.9243109   4.2925086   4.152452
  5.719896    3.7976222  10.506067    2.5067234   2.243979    2.6931024
  6.181856    1.4198786   4.5468354   2.3517177   0.97684515  0.95054436
  6.8364124   1.4128516 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_train_function.<locals>.train_function at 0x7f5aa45eacb0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-23 12:54:52.906 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 3449.4939, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 142s - loss: nan - MinusLogProbMetric: 3449.4939 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.1152e-06 - 142s/epoch - 724ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 1.3717421124828526e-06.
===========
Generating train data for run 269.
===========
Train data generated in 0.21 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_269/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_269/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.569926  ,  5.931914  ,  1.0340854 , ...,  1.178907  ,
         6.4472156 ,  1.3327898 ],
       [ 6.2850404 ,  7.153691  ,  4.423192  , ...,  3.3972363 ,
         2.667123  ,  7.632936  ],
       [ 3.9655547 ,  5.522517  ,  0.75010407, ...,  1.7247348 ,
         6.692078  ,  1.3383894 ],
       ...,
       [ 4.8108435 ,  5.787278  , -0.11691105, ...,  0.15166378,
         6.433085  ,  1.4475136 ],
       [ 1.3107729 ,  3.3536706 ,  8.5924425 , ...,  6.826266  ,
         2.8685987 ,  1.5047977 ],
       [ 2.0164819 ,  3.0697684 ,  7.39879   , ...,  6.933044  ,
         2.57938   ,  2.0720963 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_269/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_269
self.data_kwargs: {'seed': 440}
self.x_data: [[ 4.667814    5.728588   -0.61503386 ...  0.95054436  6.8364124
   1.4128516 ]
 [ 2.3392496   3.3370924   7.9916444  ...  7.2833347   3.313804
   1.7574413 ]
 [ 5.6111684   5.037572   -0.7357439  ...  2.1567051   6.6985016
   1.3668824 ]
 ...
 [ 2.8990598   4.2636833   8.734106   ...  7.059716    2.8529127
   1.343578  ]
 [ 2.64989     3.1235895   8.579933   ...  6.7448006   2.6647887
   2.1982503 ]
 [ 3.925435    5.6032453  -0.7142817  ...  0.51811683  6.7432346
   1.4061358 ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_76"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_77 (InputLayer)       [(None, 32)]              0         
                                                                 
 log_prob_layer_6 (LogProbLa  (None,)                  826720    
 yer)                                                            
                                                                 
=================================================================
Total params: 826,720
Trainable params: 826,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_6/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_6'")
self.model: <keras.engine.functional.Functional object at 0x7f5971f2d690>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f5b24365990>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f5b24365990>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f60271c8a00>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f595d775360>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_269/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f595d7758d0>, <keras.callbacks.ModelCheckpoint object at 0x7f595d775990>, <keras.callbacks.EarlyStopping object at 0x7f595d775c00>, <keras.callbacks.ReduceLROnPlateau object at 0x7f595d775c30>, <keras.callbacks.TerminateOnNaN object at 0x7f595d775870>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.569926  ,  5.931914  ,  1.0340854 , ...,  1.178907  ,
         6.4472156 ,  1.3327898 ],
       [ 6.2850404 ,  7.153691  ,  4.423192  , ...,  3.3972363 ,
         2.667123  ,  7.632936  ],
       [ 3.9655547 ,  5.522517  ,  0.75010407, ...,  1.7247348 ,
         6.692078  ,  1.3383894 ],
       ...,
       [ 4.8108435 ,  5.787278  , -0.11691105, ...,  0.15166378,
         6.433085  ,  1.4475136 ],
       [ 1.3107729 ,  3.3536706 ,  8.5924425 , ...,  6.826266  ,
         2.8685987 ,  1.5047977 ],
       [ 2.0164819 ,  3.0697684 ,  7.39879   , ...,  6.933044  ,
         2.57938   ,  2.0720963 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_269/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 269/720 with hyperparameters:
timestamp = 2023-10-23 12:55:01.772585
ndims = 32
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 826720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.3717421124828526e-06...
Train first sample: [ 4.667814    5.728588   -0.61503386  6.5363817   6.270228    6.41928
  9.1463785   7.6107693   3.2592764   4.501069    7.6099105   0.9604095
  5.968021    7.1344857   2.535179    0.9243109   4.2925086   4.152452
  5.719896    3.7976222  10.506067    2.5067234   2.243979    2.6931024
  6.181856    1.4198786   4.5468354   2.3517177   0.97684515  0.95054436
  6.8364124   1.4128516 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-23 12:57:23.196 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 3449.4939, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 141s - loss: nan - MinusLogProbMetric: 3449.4939 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.3717e-06 - 141s/epoch - 720ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 4.572473708276175e-07.
===========
Generating train data for run 269.
===========
Train data generated in 0.20 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_269/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_269/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.569926  ,  5.931914  ,  1.0340854 , ...,  1.178907  ,
         6.4472156 ,  1.3327898 ],
       [ 6.2850404 ,  7.153691  ,  4.423192  , ...,  3.3972363 ,
         2.667123  ,  7.632936  ],
       [ 3.9655547 ,  5.522517  ,  0.75010407, ...,  1.7247348 ,
         6.692078  ,  1.3383894 ],
       ...,
       [ 4.8108435 ,  5.787278  , -0.11691105, ...,  0.15166378,
         6.433085  ,  1.4475136 ],
       [ 1.3107729 ,  3.3536706 ,  8.5924425 , ...,  6.826266  ,
         2.8685987 ,  1.5047977 ],
       [ 2.0164819 ,  3.0697684 ,  7.39879   , ...,  6.933044  ,
         2.57938   ,  2.0720963 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_269/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_269
self.data_kwargs: {'seed': 440}
self.x_data: [[ 4.667814    5.728588   -0.61503386 ...  0.95054436  6.8364124
   1.4128516 ]
 [ 2.3392496   3.3370924   7.9916444  ...  7.2833347   3.313804
   1.7574413 ]
 [ 5.6111684   5.037572   -0.7357439  ...  2.1567051   6.6985016
   1.3668824 ]
 ...
 [ 2.8990598   4.2636833   8.734106   ...  7.059716    2.8529127
   1.343578  ]
 [ 2.64989     3.1235895   8.579933   ...  6.7448006   2.6647887
   2.1982503 ]
 [ 3.925435    5.6032453  -0.7142817  ...  0.51811683  6.7432346
   1.4061358 ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_87"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_88 (InputLayer)       [(None, 32)]              0         
                                                                 
 log_prob_layer_7 (LogProbLa  (None,)                  826720    
 yer)                                                            
                                                                 
=================================================================
Total params: 826,720
Trainable params: 826,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_7/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_7'")
self.model: <keras.engine.functional.Functional object at 0x7f5c00568a30>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f5a00197700>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f5a00197700>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f5a001757b0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f597065a710>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_269/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f597065ac80>, <keras.callbacks.ModelCheckpoint object at 0x7f597065ad40>, <keras.callbacks.EarlyStopping object at 0x7f597065afb0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f597065afe0>, <keras.callbacks.TerminateOnNaN object at 0x7f597065ac20>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.569926  ,  5.931914  ,  1.0340854 , ...,  1.178907  ,
         6.4472156 ,  1.3327898 ],
       [ 6.2850404 ,  7.153691  ,  4.423192  , ...,  3.3972363 ,
         2.667123  ,  7.632936  ],
       [ 3.9655547 ,  5.522517  ,  0.75010407, ...,  1.7247348 ,
         6.692078  ,  1.3383894 ],
       ...,
       [ 4.8108435 ,  5.787278  , -0.11691105, ...,  0.15166378,
         6.433085  ,  1.4475136 ],
       [ 1.3107729 ,  3.3536706 ,  8.5924425 , ...,  6.826266  ,
         2.8685987 ,  1.5047977 ],
       [ 2.0164819 ,  3.0697684 ,  7.39879   , ...,  6.933044  ,
         2.57938   ,  2.0720963 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_269/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 269/720 with hyperparameters:
timestamp = 2023-10-23 12:57:32.076912
ndims = 32
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 826720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.572473708276175e-07...
Train first sample: [ 4.667814    5.728588   -0.61503386  6.5363817   6.270228    6.41928
  9.1463785   7.6107693   3.2592764   4.501069    7.6099105   0.9604095
  5.968021    7.1344857   2.535179    0.9243109   4.2925086   4.152452
  5.719896    3.7976222  10.506067    2.5067234   2.243979    2.6931024
  6.181856    1.4198786   4.5468354   2.3517177   0.97684515  0.95054436
  6.8364124   1.4128516 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-23 12:59:53.030 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 3449.4939, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 141s - loss: nan - MinusLogProbMetric: 3449.4939 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.5725e-07 - 141s/epoch - 719ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 1.524157902758725e-07.
===========
Generating train data for run 269.
===========
Train data generated in 0.17 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_269/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_269/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.569926  ,  5.931914  ,  1.0340854 , ...,  1.178907  ,
         6.4472156 ,  1.3327898 ],
       [ 6.2850404 ,  7.153691  ,  4.423192  , ...,  3.3972363 ,
         2.667123  ,  7.632936  ],
       [ 3.9655547 ,  5.522517  ,  0.75010407, ...,  1.7247348 ,
         6.692078  ,  1.3383894 ],
       ...,
       [ 4.8108435 ,  5.787278  , -0.11691105, ...,  0.15166378,
         6.433085  ,  1.4475136 ],
       [ 1.3107729 ,  3.3536706 ,  8.5924425 , ...,  6.826266  ,
         2.8685987 ,  1.5047977 ],
       [ 2.0164819 ,  3.0697684 ,  7.39879   , ...,  6.933044  ,
         2.57938   ,  2.0720963 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_269/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_269
self.data_kwargs: {'seed': 440}
self.x_data: [[ 4.667814    5.728588   -0.61503386 ...  0.95054436  6.8364124
   1.4128516 ]
 [ 2.3392496   3.3370924   7.9916444  ...  7.2833347   3.313804
   1.7574413 ]
 [ 5.6111684   5.037572   -0.7357439  ...  2.1567051   6.6985016
   1.3668824 ]
 ...
 [ 2.8990598   4.2636833   8.734106   ...  7.059716    2.8529127
   1.343578  ]
 [ 2.64989     3.1235895   8.579933   ...  6.7448006   2.6647887
   2.1982503 ]
 [ 3.925435    5.6032453  -0.7142817  ...  0.51811683  6.7432346
   1.4061358 ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_98"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_99 (InputLayer)       [(None, 32)]              0         
                                                                 
 log_prob_layer_8 (LogProbLa  (None,)                  826720    
 yer)                                                            
                                                                 
=================================================================
Total params: 826,720
Trainable params: 826,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_8/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_8'")
self.model: <keras.engine.functional.Functional object at 0x7f594d88bfa0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f594cb973a0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f594cb973a0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f595debda20>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f594d8c7790>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_269/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f594d8c7d00>, <keras.callbacks.ModelCheckpoint object at 0x7f594d8c7dc0>, <keras.callbacks.EarlyStopping object at 0x7f594d8c7fd0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f594d8c7cd0>, <keras.callbacks.TerminateOnNaN object at 0x7f594d8c7f70>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.569926  ,  5.931914  ,  1.0340854 , ...,  1.178907  ,
         6.4472156 ,  1.3327898 ],
       [ 6.2850404 ,  7.153691  ,  4.423192  , ...,  3.3972363 ,
         2.667123  ,  7.632936  ],
       [ 3.9655547 ,  5.522517  ,  0.75010407, ...,  1.7247348 ,
         6.692078  ,  1.3383894 ],
       ...,
       [ 4.8108435 ,  5.787278  , -0.11691105, ...,  0.15166378,
         6.433085  ,  1.4475136 ],
       [ 1.3107729 ,  3.3536706 ,  8.5924425 , ...,  6.826266  ,
         2.8685987 ,  1.5047977 ],
       [ 2.0164819 ,  3.0697684 ,  7.39879   , ...,  6.933044  ,
         2.57938   ,  2.0720963 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_269/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 269/720 with hyperparameters:
timestamp = 2023-10-23 13:00:02.969022
ndims = 32
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 826720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.524157902758725e-07...
Train first sample: [ 4.667814    5.728588   -0.61503386  6.5363817   6.270228    6.41928
  9.1463785   7.6107693   3.2592764   4.501069    7.6099105   0.9604095
  5.968021    7.1344857   2.535179    0.9243109   4.2925086   4.152452
  5.719896    3.7976222  10.506067    2.5067234   2.243979    2.6931024
  6.181856    1.4198786   4.5468354   2.3517177   0.97684515  0.95054436
  6.8364124   1.4128516 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-23 13:02:27.983 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 3449.4939, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 145s - loss: nan - MinusLogProbMetric: 3449.4939 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.5242e-07 - 145s/epoch - 738ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 5.0805263425290834e-08.
===========
Generating train data for run 269.
===========
Train data generated in 0.21 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_269/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_269/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.569926  ,  5.931914  ,  1.0340854 , ...,  1.178907  ,
         6.4472156 ,  1.3327898 ],
       [ 6.2850404 ,  7.153691  ,  4.423192  , ...,  3.3972363 ,
         2.667123  ,  7.632936  ],
       [ 3.9655547 ,  5.522517  ,  0.75010407, ...,  1.7247348 ,
         6.692078  ,  1.3383894 ],
       ...,
       [ 4.8108435 ,  5.787278  , -0.11691105, ...,  0.15166378,
         6.433085  ,  1.4475136 ],
       [ 1.3107729 ,  3.3536706 ,  8.5924425 , ...,  6.826266  ,
         2.8685987 ,  1.5047977 ],
       [ 2.0164819 ,  3.0697684 ,  7.39879   , ...,  6.933044  ,
         2.57938   ,  2.0720963 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_269/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_269
self.data_kwargs: {'seed': 440}
self.x_data: [[ 4.667814    5.728588   -0.61503386 ...  0.95054436  6.8364124
   1.4128516 ]
 [ 2.3392496   3.3370924   7.9916444  ...  7.2833347   3.313804
   1.7574413 ]
 [ 5.6111684   5.037572   -0.7357439  ...  2.1567051   6.6985016
   1.3668824 ]
 ...
 [ 2.8990598   4.2636833   8.734106   ...  7.059716    2.8529127
   1.343578  ]
 [ 2.64989     3.1235895   8.579933   ...  6.7448006   2.6647887
   2.1982503 ]
 [ 3.925435    5.6032453  -0.7142817  ...  0.51811683  6.7432346
   1.4061358 ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_109"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_110 (InputLayer)      [(None, 32)]              0         
                                                                 
 log_prob_layer_9 (LogProbLa  (None,)                  826720    
 yer)                                                            
                                                                 
=================================================================
Total params: 826,720
Trainable params: 826,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_9/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_9'")
self.model: <keras.engine.functional.Functional object at 0x7f5bb819fbe0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f5b5c5d7d30>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f5b5c5d7d30>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f5b5c66f550>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f599c5638b0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_269/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f599c563e20>, <keras.callbacks.ModelCheckpoint object at 0x7f599c563ee0>, <keras.callbacks.EarlyStopping object at 0x7f599c563fa0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f599c563dc0>, <keras.callbacks.TerminateOnNaN object at 0x7f599c563df0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.569926  ,  5.931914  ,  1.0340854 , ...,  1.178907  ,
         6.4472156 ,  1.3327898 ],
       [ 6.2850404 ,  7.153691  ,  4.423192  , ...,  3.3972363 ,
         2.667123  ,  7.632936  ],
       [ 3.9655547 ,  5.522517  ,  0.75010407, ...,  1.7247348 ,
         6.692078  ,  1.3383894 ],
       ...,
       [ 4.8108435 ,  5.787278  , -0.11691105, ...,  0.15166378,
         6.433085  ,  1.4475136 ],
       [ 1.3107729 ,  3.3536706 ,  8.5924425 , ...,  6.826266  ,
         2.8685987 ,  1.5047977 ],
       [ 2.0164819 ,  3.0697684 ,  7.39879   , ...,  6.933044  ,
         2.57938   ,  2.0720963 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_269/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 269/720 with hyperparameters:
timestamp = 2023-10-23 13:02:37.328802
ndims = 32
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 826720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 5.0805263425290834e-08...
Train first sample: [ 4.667814    5.728588   -0.61503386  6.5363817   6.270228    6.41928
  9.1463785   7.6107693   3.2592764   4.501069    7.6099105   0.9604095
  5.968021    7.1344857   2.535179    0.9243109   4.2925086   4.152452
  5.719896    3.7976222  10.506067    2.5067234   2.243979    2.6931024
  6.181856    1.4198786   4.5468354   2.3517177   0.97684515  0.95054436
  6.8364124   1.4128516 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-23 13:04:47.222 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 3449.4939, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 130s - loss: nan - MinusLogProbMetric: 3449.4939 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 5.0805e-08 - 130s/epoch - 661ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 1.6935087808430278e-08.
===========
Generating train data for run 269.
===========
Train data generated in 0.21 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_269/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_269/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.569926  ,  5.931914  ,  1.0340854 , ...,  1.178907  ,
         6.4472156 ,  1.3327898 ],
       [ 6.2850404 ,  7.153691  ,  4.423192  , ...,  3.3972363 ,
         2.667123  ,  7.632936  ],
       [ 3.9655547 ,  5.522517  ,  0.75010407, ...,  1.7247348 ,
         6.692078  ,  1.3383894 ],
       ...,
       [ 4.8108435 ,  5.787278  , -0.11691105, ...,  0.15166378,
         6.433085  ,  1.4475136 ],
       [ 1.3107729 ,  3.3536706 ,  8.5924425 , ...,  6.826266  ,
         2.8685987 ,  1.5047977 ],
       [ 2.0164819 ,  3.0697684 ,  7.39879   , ...,  6.933044  ,
         2.57938   ,  2.0720963 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_269/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_269
self.data_kwargs: {'seed': 440}
self.x_data: [[ 4.667814    5.728588   -0.61503386 ...  0.95054436  6.8364124
   1.4128516 ]
 [ 2.3392496   3.3370924   7.9916444  ...  7.2833347   3.313804
   1.7574413 ]
 [ 5.6111684   5.037572   -0.7357439  ...  2.1567051   6.6985016
   1.3668824 ]
 ...
 [ 2.8990598   4.2636833   8.734106   ...  7.059716    2.8529127
   1.343578  ]
 [ 2.64989     3.1235895   8.579933   ...  6.7448006   2.6647887
   2.1982503 ]
 [ 3.925435    5.6032453  -0.7142817  ...  0.51811683  6.7432346
   1.4061358 ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_120"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_121 (InputLayer)      [(None, 32)]              0         
                                                                 
 log_prob_layer_10 (LogProbL  (None,)                  826720    
 ayer)                                                           
                                                                 
=================================================================
Total params: 826,720
Trainable params: 826,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_10/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_10'")
self.model: <keras.engine.functional.Functional object at 0x7f594d36aa70>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f594d3341f0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f594d3341f0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f5aa45b7400>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f594d3d5f30>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_269/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f594d3d64a0>, <keras.callbacks.ModelCheckpoint object at 0x7f594d3d6560>, <keras.callbacks.EarlyStopping object at 0x7f594d3d67d0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f594d3d6800>, <keras.callbacks.TerminateOnNaN object at 0x7f594d3d6440>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.569926  ,  5.931914  ,  1.0340854 , ...,  1.178907  ,
         6.4472156 ,  1.3327898 ],
       [ 6.2850404 ,  7.153691  ,  4.423192  , ...,  3.3972363 ,
         2.667123  ,  7.632936  ],
       [ 3.9655547 ,  5.522517  ,  0.75010407, ...,  1.7247348 ,
         6.692078  ,  1.3383894 ],
       ...,
       [ 4.8108435 ,  5.787278  , -0.11691105, ...,  0.15166378,
         6.433085  ,  1.4475136 ],
       [ 1.3107729 ,  3.3536706 ,  8.5924425 , ...,  6.826266  ,
         2.8685987 ,  1.5047977 ],
       [ 2.0164819 ,  3.0697684 ,  7.39879   , ...,  6.933044  ,
         2.57938   ,  2.0720963 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_269/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 269/720 with hyperparameters:
timestamp = 2023-10-23 13:04:55.833139
ndims = 32
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 826720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.6935087808430278e-08...
Train first sample: [ 4.667814    5.728588   -0.61503386  6.5363817   6.270228    6.41928
  9.1463785   7.6107693   3.2592764   4.501069    7.6099105   0.9604095
  5.968021    7.1344857   2.535179    0.9243109   4.2925086   4.152452
  5.719896    3.7976222  10.506067    2.5067234   2.243979    2.6931024
  6.181856    1.4198786   4.5468354   2.3517177   0.97684515  0.95054436
  6.8364124   1.4128516 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-23 13:07:03.421 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 3449.4939, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 127s - loss: nan - MinusLogProbMetric: 3449.4939 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.6935e-08 - 127s/epoch - 650ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 5.645029269476759e-09.
===========
Run 269/720 failed.
Exception type: Exception
Exception message: Training failed for the maximum number of retry.
Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 649, Func.Name : <module>, Message : hyperparams_dict, NFObject, seed_train, training_time = train_function(seeds = [seed_train, seed_test, seed_dist, seed_metrics],', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 322, Func.Name : train_function, Message : raise Exception("Training failed for the maximum number of retry.")']
===========

Directory ../../results/CsplineN_new/run_270/ already exists.
Skipping it.
===========
Run 270/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_271/ already exists.
Skipping it.
===========
Run 271/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_272/ already exists.
Skipping it.
===========
Run 272/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_273/ already exists.
Skipping it.
===========
Run 273/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_274/ already exists.
Skipping it.
===========
Run 274/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_275/ already exists.
Skipping it.
===========
Run 275/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_276/ already exists.
Skipping it.
===========
Run 276/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_277/ already exists.
Skipping it.
===========
Run 277/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_278/ already exists.
Skipping it.
===========
Run 278/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_279/ already exists.
Skipping it.
===========
Run 279/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_280/ already exists.
Skipping it.
===========
Run 280/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_281/ already exists.
Skipping it.
===========
Run 281/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_282/ already exists.
Skipping it.
===========
Run 282/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_283/ already exists.
Skipping it.
===========
Run 283/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_284/ already exists.
Skipping it.
===========
Run 284/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_285/ already exists.
Skipping it.
===========
Run 285/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_286/ already exists.
Skipping it.
===========
Run 286/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_287/ already exists.
Skipping it.
===========
Run 287/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_288/ already exists.
Skipping it.
===========
Run 288/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_289/ already exists.
Skipping it.
===========
Run 289/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_290/ already exists.
Skipping it.
===========
Run 290/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_291/ already exists.
Skipping it.
===========
Run 291/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_292/ already exists.
Skipping it.
===========
Run 292/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_293/ already exists.
Skipping it.
===========
Run 293/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_294/ already exists.
Skipping it.
===========
Run 294/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_295/ already exists.
Skipping it.
===========
Run 295/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_296/ already exists.
Skipping it.
===========
Run 296/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_297/ already exists.
Skipping it.
===========
Run 297/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_298/ already exists.
Skipping it.
===========
Run 298/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_299/ already exists.
Skipping it.
===========
Run 299/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_300/ already exists.
Skipping it.
===========
Run 300/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_301/ already exists.
Skipping it.
===========
Run 301/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_302/ already exists.
Skipping it.
===========
Run 302/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_303/ already exists.
Skipping it.
===========
Run 303/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_304/ already exists.
Skipping it.
===========
Run 304/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_305/ already exists.
Skipping it.
===========
Run 305/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_306/ already exists.
Skipping it.
===========
Run 306/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_307/ already exists.
Skipping it.
===========
Run 307/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_308/ already exists.
Skipping it.
===========
Run 308/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_309/ already exists.
Skipping it.
===========
Run 309/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_310/ already exists.
Skipping it.
===========
Run 310/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_311/ already exists.
Skipping it.
===========
Run 311/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_312/ already exists.
Skipping it.
===========
Run 312/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_313/ already exists.
Skipping it.
===========
Run 313/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_314/ already exists.
Skipping it.
===========
Run 314/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_315/ already exists.
Skipping it.
===========
Run 315/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_316/ already exists.
Skipping it.
===========
Run 316/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_317/ already exists.
Skipping it.
===========
Run 317/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_318/ already exists.
Skipping it.
===========
Run 318/720 already exists. Skipping it.
===========

===========
Generating train data for run 319.
===========
Train data generated in 0.16 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_319/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 933}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_319/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[2.7429588 , 2.5700626 , 7.723717  , ..., 7.7501717 , 2.9186718 ,
        1.775845  ],
       [4.728784  , 6.065677  , 0.9645814 , ..., 1.2628946 , 6.7253027 ,
        1.4381479 ],
       [3.8513129 , 6.1744533 , 0.2601294 , ..., 1.1662469 , 6.572044  ,
        1.3500103 ],
       ...,
       [2.8332918 , 4.2526407 , 9.701975  , ..., 7.0738525 , 2.5952196 ,
        1.8190176 ],
       [3.118363  , 5.4312434 , 0.32481733, ..., 1.9350358 , 6.6219816 ,
        1.3230293 ],
       [1.792576  , 4.9344916 , 9.207544  , ..., 7.378074  , 2.857349  ,
        1.56005   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_319/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_319
self.data_kwargs: {'seed': 933}
self.x_data: [[4.6118093  6.294248   0.6753535  ... 1.3966799  6.5528636  1.404321  ]
 [5.0913153  7.181254   5.302016   ... 4.927679   2.6276157  7.5562596 ]
 [2.3468595  3.6813745  8.449473   ... 7.5625257  2.3049273  1.7164713 ]
 ...
 [1.7774892  4.58926    7.003348   ... 7.297376   3.211844   1.3889377 ]
 [5.1396513  5.5684643  1.4578001  ... 0.69219434 6.9603205  1.3834548 ]
 [3.2499232  3.5331872  8.207085   ... 6.760554   3.3946218  2.0297966 ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_131"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_132 (InputLayer)      [(None, 32)]              0         
                                                                 
 log_prob_layer_11 (LogProbL  (None,)                  1074400   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,074,400
Trainable params: 1,074,400
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_11/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_11'")
self.model: <keras.engine.functional.Functional object at 0x7f5a9c66dff0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f5a645c5f00>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f5a645c5f00>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f5c002e76d0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f594d53c220>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f594d53c790>, <keras.callbacks.ModelCheckpoint object at 0x7f594d53c850>, <keras.callbacks.EarlyStopping object at 0x7f594d53cac0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f594d53caf0>, <keras.callbacks.TerminateOnNaN object at 0x7f594d53c730>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[2.7429588 , 2.5700626 , 7.723717  , ..., 7.7501717 , 2.9186718 ,
        1.775845  ],
       [4.728784  , 6.065677  , 0.9645814 , ..., 1.2628946 , 6.7253027 ,
        1.4381479 ],
       [3.8513129 , 6.1744533 , 0.2601294 , ..., 1.1662469 , 6.572044  ,
        1.3500103 ],
       ...,
       [2.8332918 , 4.2526407 , 9.701975  , ..., 7.0738525 , 2.5952196 ,
        1.8190176 ],
       [3.118363  , 5.4312434 , 0.32481733, ..., 1.9350358 , 6.6219816 ,
        1.3230293 ],
       [1.792576  , 4.9344916 , 9.207544  , ..., 7.378074  , 2.857349  ,
        1.56005   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_319/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 319/720 with hyperparameters:
timestamp = 2023-10-23 13:07:12.401801
ndims = 32
seed_train = 933
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1074400
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 4.6118093   6.294248    0.6753535   5.561676    6.128757    6.145322
  9.785121    7.1052794   3.7474928   5.3494825   6.3791265   0.5788075
  5.683702    6.3188157   1.6447939   1.0492666   3.6812766   3.7191741
  5.81123     3.2186081  10.359394    0.68370855  2.036205    1.0944312
  6.489696    3.6441135   4.6169076   1.7655768   1.5199437   1.3966799
  6.5528636   1.404321  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 34: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-23 13:10:08.349 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 2959.5857, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 176s - loss: nan - MinusLogProbMetric: 2959.5857 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 176s/epoch - 897ms/step
The loss history contains NaN values.
Training failed: trying again with seed 402839 and lr 0.0003333333333333333.
===========
Generating train data for run 319.
===========
Train data generated in 0.24 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_319/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 933}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_319/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[2.7429588 , 2.5700626 , 7.723717  , ..., 7.7501717 , 2.9186718 ,
        1.775845  ],
       [4.728784  , 6.065677  , 0.9645814 , ..., 1.2628946 , 6.7253027 ,
        1.4381479 ],
       [3.8513129 , 6.1744533 , 0.2601294 , ..., 1.1662469 , 6.572044  ,
        1.3500103 ],
       ...,
       [2.8332918 , 4.2526407 , 9.701975  , ..., 7.0738525 , 2.5952196 ,
        1.8190176 ],
       [3.118363  , 5.4312434 , 0.32481733, ..., 1.9350358 , 6.6219816 ,
        1.3230293 ],
       [1.792576  , 4.9344916 , 9.207544  , ..., 7.378074  , 2.857349  ,
        1.56005   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_319/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_319
self.data_kwargs: {'seed': 933}
self.x_data: [[4.6118093  6.294248   0.6753535  ... 1.3966799  6.5528636  1.404321  ]
 [5.0913153  7.181254   5.302016   ... 4.927679   2.6276157  7.5562596 ]
 [2.3468595  3.6813745  8.449473   ... 7.5625257  2.3049273  1.7164713 ]
 ...
 [1.7774892  4.58926    7.003348   ... 7.297376   3.211844   1.3889377 ]
 [5.1396513  5.5684643  1.4578001  ... 0.69219434 6.9603205  1.3834548 ]
 [3.2499232  3.5331872  8.207085   ... 6.760554   3.3946218  2.0297966 ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_142"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_143 (InputLayer)      [(None, 32)]              0         
                                                                 
 log_prob_layer_12 (LogProbL  (None,)                  1074400   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,074,400
Trainable params: 1,074,400
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_12/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_12'")
self.model: <keras.engine.functional.Functional object at 0x7f60275a5ba0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f6016164ac0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f6016164ac0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f58dfd0a200>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f5ff5ca0b80>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f5ff5ca10f0>, <keras.callbacks.ModelCheckpoint object at 0x7f5ff5ca11b0>, <keras.callbacks.EarlyStopping object at 0x7f5ff5ca1420>, <keras.callbacks.ReduceLROnPlateau object at 0x7f5ff5ca1450>, <keras.callbacks.TerminateOnNaN object at 0x7f5ff5ca1090>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[2.7429588 , 2.5700626 , 7.723717  , ..., 7.7501717 , 2.9186718 ,
        1.775845  ],
       [4.728784  , 6.065677  , 0.9645814 , ..., 1.2628946 , 6.7253027 ,
        1.4381479 ],
       [3.8513129 , 6.1744533 , 0.2601294 , ..., 1.1662469 , 6.572044  ,
        1.3500103 ],
       ...,
       [2.8332918 , 4.2526407 , 9.701975  , ..., 7.0738525 , 2.5952196 ,
        1.8190176 ],
       [3.118363  , 5.4312434 , 0.32481733, ..., 1.9350358 , 6.6219816 ,
        1.3230293 ],
       [1.792576  , 4.9344916 , 9.207544  , ..., 7.378074  , 2.857349  ,
        1.56005   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_319/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 319/720 with hyperparameters:
timestamp = 2023-10-23 13:10:20.119282
ndims = 32
seed_train = 933
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1074400
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 4.6118093   6.294248    0.6753535   5.561676    6.128757    6.145322
  9.785121    7.1052794   3.7474928   5.3494825   6.3791265   0.5788075
  5.683702    6.3188157   1.6447939   1.0492666   3.6812766   3.7191741
  5.81123     3.2186081  10.359394    0.68370855  2.036205    1.0944312
  6.489696    3.6441135   4.6169076   1.7655768   1.5199437   1.3966799
  6.5528636   1.404321  ]
Epoch 1/1000
2023-10-23 13:14:07.598 
Epoch 1/1000 
	 loss: 1380.4432, MinusLogProbMetric: 1380.4432, val_loss: 815.6319, val_MinusLogProbMetric: 815.6319

Epoch 1: val_loss improved from inf to 815.63190, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 228s - loss: 1380.4432 - MinusLogProbMetric: 1380.4432 - val_loss: 815.6319 - val_MinusLogProbMetric: 815.6319 - lr: 3.3333e-04 - 228s/epoch - 1s/step
Epoch 2/1000
2023-10-23 13:15:23.198 
Epoch 2/1000 
	 loss: 541.0538, MinusLogProbMetric: 541.0538, val_loss: 399.6104, val_MinusLogProbMetric: 399.6104

Epoch 2: val_loss improved from 815.63190 to 399.61035, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 75s - loss: 541.0538 - MinusLogProbMetric: 541.0538 - val_loss: 399.6104 - val_MinusLogProbMetric: 399.6104 - lr: 3.3333e-04 - 75s/epoch - 383ms/step
Epoch 3/1000
2023-10-23 13:16:37.789 
Epoch 3/1000 
	 loss: 336.1811, MinusLogProbMetric: 336.1811, val_loss: 291.2160, val_MinusLogProbMetric: 291.2160

Epoch 3: val_loss improved from 399.61035 to 291.21597, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 75s - loss: 336.1811 - MinusLogProbMetric: 336.1811 - val_loss: 291.2160 - val_MinusLogProbMetric: 291.2160 - lr: 3.3333e-04 - 75s/epoch - 380ms/step
Epoch 4/1000
2023-10-23 13:17:45.784 
Epoch 4/1000 
	 loss: 324.6952, MinusLogProbMetric: 324.6952, val_loss: 293.7189, val_MinusLogProbMetric: 293.7189

Epoch 4: val_loss did not improve from 291.21597
196/196 - 67s - loss: 324.6952 - MinusLogProbMetric: 324.6952 - val_loss: 293.7189 - val_MinusLogProbMetric: 293.7189 - lr: 3.3333e-04 - 67s/epoch - 341ms/step
Epoch 5/1000
2023-10-23 13:18:57.357 
Epoch 5/1000 
	 loss: 267.1835, MinusLogProbMetric: 267.1835, val_loss: 256.4690, val_MinusLogProbMetric: 256.4690

Epoch 5: val_loss improved from 291.21597 to 256.46902, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 73s - loss: 267.1835 - MinusLogProbMetric: 267.1835 - val_loss: 256.4690 - val_MinusLogProbMetric: 256.4690 - lr: 3.3333e-04 - 73s/epoch - 371ms/step
Epoch 6/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 5: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-23 13:19:04.787 
Epoch 6/1000 
	 loss: nan, MinusLogProbMetric: 257.2935, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 6: val_loss did not improve from 256.46902
196/196 - 6s - loss: nan - MinusLogProbMetric: 257.2935 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 6s/epoch - 32ms/step
The loss history contains NaN values.
Training failed: trying again with seed 402839 and lr 0.0001111111111111111.
===========
Generating train data for run 319.
===========
Train data generated in 0.22 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_319/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 933}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_319/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[2.7429588 , 2.5700626 , 7.723717  , ..., 7.7501717 , 2.9186718 ,
        1.775845  ],
       [4.728784  , 6.065677  , 0.9645814 , ..., 1.2628946 , 6.7253027 ,
        1.4381479 ],
       [3.8513129 , 6.1744533 , 0.2601294 , ..., 1.1662469 , 6.572044  ,
        1.3500103 ],
       ...,
       [2.8332918 , 4.2526407 , 9.701975  , ..., 7.0738525 , 2.5952196 ,
        1.8190176 ],
       [3.118363  , 5.4312434 , 0.32481733, ..., 1.9350358 , 6.6219816 ,
        1.3230293 ],
       [1.792576  , 4.9344916 , 9.207544  , ..., 7.378074  , 2.857349  ,
        1.56005   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_319/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_319
self.data_kwargs: {'seed': 933}
self.x_data: [[4.6118093  6.294248   0.6753535  ... 1.3966799  6.5528636  1.404321  ]
 [5.0913153  7.181254   5.302016   ... 4.927679   2.6276157  7.5562596 ]
 [2.3468595  3.6813745  8.449473   ... 7.5625257  2.3049273  1.7164713 ]
 ...
 [1.7774892  4.58926    7.003348   ... 7.297376   3.211844   1.3889377 ]
 [5.1396513  5.5684643  1.4578001  ... 0.69219434 6.9603205  1.3834548 ]
 [3.2499232  3.5331872  8.207085   ... 6.760554   3.3946218  2.0297966 ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_153"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_154 (InputLayer)      [(None, 32)]              0         
                                                                 
 log_prob_layer_13 (LogProbL  (None,)                  1074400   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,074,400
Trainable params: 1,074,400
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_13/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_13'")
self.model: <keras.engine.functional.Functional object at 0x7f5926e87d60>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f5926ecf190>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f5926ecf190>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f58df90da20>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f5fe457db40>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f5fe457e0b0>, <keras.callbacks.ModelCheckpoint object at 0x7f5fe457e170>, <keras.callbacks.EarlyStopping object at 0x7f5fe457e3e0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f5fe457e410>, <keras.callbacks.TerminateOnNaN object at 0x7f5fe457e050>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[2.7429588 , 2.5700626 , 7.723717  , ..., 7.7501717 , 2.9186718 ,
        1.775845  ],
       [4.728784  , 6.065677  , 0.9645814 , ..., 1.2628946 , 6.7253027 ,
        1.4381479 ],
       [3.8513129 , 6.1744533 , 0.2601294 , ..., 1.1662469 , 6.572044  ,
        1.3500103 ],
       ...,
       [2.8332918 , 4.2526407 , 9.701975  , ..., 7.0738525 , 2.5952196 ,
        1.8190176 ],
       [3.118363  , 5.4312434 , 0.32481733, ..., 1.9350358 , 6.6219816 ,
        1.3230293 ],
       [1.792576  , 4.9344916 , 9.207544  , ..., 7.378074  , 2.857349  ,
        1.56005   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 319/720 with hyperparameters:
timestamp = 2023-10-23 13:19:16.740195
ndims = 32
seed_train = 933
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1074400
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 4.6118093   6.294248    0.6753535   5.561676    6.128757    6.145322
  9.785121    7.1052794   3.7474928   5.3494825   6.3791265   0.5788075
  5.683702    6.3188157   1.6447939   1.0492666   3.6812766   3.7191741
  5.81123     3.2186081  10.359394    0.68370855  2.036205    1.0944312
  6.489696    3.6441135   4.6169076   1.7655768   1.5199437   1.3966799
  6.5528636   1.404321  ]
Epoch 1/1000
2023-10-23 13:23:01.917 
Epoch 1/1000 
	 loss: 356.7576, MinusLogProbMetric: 356.7576, val_loss: 278.3158, val_MinusLogProbMetric: 278.3158

Epoch 1: val_loss improved from inf to 278.31580, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 226s - loss: 356.7576 - MinusLogProbMetric: 356.7576 - val_loss: 278.3158 - val_MinusLogProbMetric: 278.3158 - lr: 1.1111e-04 - 226s/epoch - 1s/step
Epoch 2/1000
2023-10-23 13:24:11.625 
Epoch 2/1000 
	 loss: 218.8208, MinusLogProbMetric: 218.8208, val_loss: 180.9197, val_MinusLogProbMetric: 180.9197

Epoch 2: val_loss improved from 278.31580 to 180.91974, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 69s - loss: 218.8208 - MinusLogProbMetric: 218.8208 - val_loss: 180.9197 - val_MinusLogProbMetric: 180.9197 - lr: 1.1111e-04 - 69s/epoch - 354ms/step
Epoch 3/1000
2023-10-23 13:25:27.087 
Epoch 3/1000 
	 loss: 201.2063, MinusLogProbMetric: 201.2063, val_loss: 173.7672, val_MinusLogProbMetric: 173.7672

Epoch 3: val_loss improved from 180.91974 to 173.76717, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 75s - loss: 201.2063 - MinusLogProbMetric: 201.2063 - val_loss: 173.7672 - val_MinusLogProbMetric: 173.7672 - lr: 1.1111e-04 - 75s/epoch - 384ms/step
Epoch 4/1000
2023-10-23 13:26:39.827 
Epoch 4/1000 
	 loss: 157.6895, MinusLogProbMetric: 157.6895, val_loss: 147.5917, val_MinusLogProbMetric: 147.5917

Epoch 4: val_loss improved from 173.76717 to 147.59169, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 73s - loss: 157.6895 - MinusLogProbMetric: 157.6895 - val_loss: 147.5917 - val_MinusLogProbMetric: 147.5917 - lr: 1.1111e-04 - 73s/epoch - 372ms/step
Epoch 5/1000
2023-10-23 13:27:52.690 
Epoch 5/1000 
	 loss: 130.9596, MinusLogProbMetric: 130.9596, val_loss: 110.9896, val_MinusLogProbMetric: 110.9896

Epoch 5: val_loss improved from 147.59169 to 110.98962, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 73s - loss: 130.9596 - MinusLogProbMetric: 130.9596 - val_loss: 110.9896 - val_MinusLogProbMetric: 110.9896 - lr: 1.1111e-04 - 73s/epoch - 371ms/step
Epoch 6/1000
2023-10-23 13:29:09.404 
Epoch 6/1000 
	 loss: 109.0973, MinusLogProbMetric: 109.0973, val_loss: 99.9647, val_MinusLogProbMetric: 99.9647

Epoch 6: val_loss improved from 110.98962 to 99.96472, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 77s - loss: 109.0973 - MinusLogProbMetric: 109.0973 - val_loss: 99.9647 - val_MinusLogProbMetric: 99.9647 - lr: 1.1111e-04 - 77s/epoch - 392ms/step
Epoch 7/1000
2023-10-23 13:30:22.549 
Epoch 7/1000 
	 loss: 94.1434, MinusLogProbMetric: 94.1434, val_loss: 86.0102, val_MinusLogProbMetric: 86.0102

Epoch 7: val_loss improved from 99.96472 to 86.01018, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 73s - loss: 94.1434 - MinusLogProbMetric: 94.1434 - val_loss: 86.0102 - val_MinusLogProbMetric: 86.0102 - lr: 1.1111e-04 - 73s/epoch - 374ms/step
Epoch 8/1000
2023-10-23 13:31:35.621 
Epoch 8/1000 
	 loss: 92.4985, MinusLogProbMetric: 92.4985, val_loss: 100.5982, val_MinusLogProbMetric: 100.5982

Epoch 8: val_loss did not improve from 86.01018
196/196 - 72s - loss: 92.4985 - MinusLogProbMetric: 92.4985 - val_loss: 100.5982 - val_MinusLogProbMetric: 100.5982 - lr: 1.1111e-04 - 72s/epoch - 366ms/step
Epoch 9/1000
2023-10-23 13:32:44.775 
Epoch 9/1000 
	 loss: 235.5083, MinusLogProbMetric: 235.5083, val_loss: 164.9768, val_MinusLogProbMetric: 164.9768

Epoch 9: val_loss did not improve from 86.01018
196/196 - 69s - loss: 235.5083 - MinusLogProbMetric: 235.5083 - val_loss: 164.9768 - val_MinusLogProbMetric: 164.9768 - lr: 1.1111e-04 - 69s/epoch - 353ms/step
Epoch 10/1000
2023-10-23 13:33:55.738 
Epoch 10/1000 
	 loss: 131.3489, MinusLogProbMetric: 131.3489, val_loss: 112.7342, val_MinusLogProbMetric: 112.7342

Epoch 10: val_loss did not improve from 86.01018
196/196 - 71s - loss: 131.3489 - MinusLogProbMetric: 131.3489 - val_loss: 112.7342 - val_MinusLogProbMetric: 112.7342 - lr: 1.1111e-04 - 71s/epoch - 362ms/step
Epoch 11/1000
2023-10-23 13:35:04.448 
Epoch 11/1000 
	 loss: 108.0751, MinusLogProbMetric: 108.0751, val_loss: 99.3042, val_MinusLogProbMetric: 99.3042

Epoch 11: val_loss did not improve from 86.01018
196/196 - 69s - loss: 108.0751 - MinusLogProbMetric: 108.0751 - val_loss: 99.3042 - val_MinusLogProbMetric: 99.3042 - lr: 1.1111e-04 - 69s/epoch - 351ms/step
Epoch 12/1000
2023-10-23 13:36:20.913 
Epoch 12/1000 
	 loss: 104.0375, MinusLogProbMetric: 104.0375, val_loss: 97.2633, val_MinusLogProbMetric: 97.2633

Epoch 12: val_loss did not improve from 86.01018
196/196 - 76s - loss: 104.0375 - MinusLogProbMetric: 104.0375 - val_loss: 97.2633 - val_MinusLogProbMetric: 97.2633 - lr: 1.1111e-04 - 76s/epoch - 390ms/step
Epoch 13/1000
2023-10-23 13:37:29.917 
Epoch 13/1000 
	 loss: 84.4309, MinusLogProbMetric: 84.4309, val_loss: 78.4184, val_MinusLogProbMetric: 78.4184

Epoch 13: val_loss improved from 86.01018 to 78.41836, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 71s - loss: 84.4309 - MinusLogProbMetric: 84.4309 - val_loss: 78.4184 - val_MinusLogProbMetric: 78.4184 - lr: 1.1111e-04 - 71s/epoch - 360ms/step
Epoch 14/1000
2023-10-23 13:38:38.763 
Epoch 14/1000 
	 loss: 75.5172, MinusLogProbMetric: 75.5172, val_loss: 77.2216, val_MinusLogProbMetric: 77.2216

Epoch 14: val_loss improved from 78.41836 to 77.22164, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 69s - loss: 75.5172 - MinusLogProbMetric: 75.5172 - val_loss: 77.2216 - val_MinusLogProbMetric: 77.2216 - lr: 1.1111e-04 - 69s/epoch - 350ms/step
Epoch 15/1000
2023-10-23 13:39:48.184 
Epoch 15/1000 
	 loss: 70.6380, MinusLogProbMetric: 70.6380, val_loss: 67.1391, val_MinusLogProbMetric: 67.1391

Epoch 15: val_loss improved from 77.22164 to 67.13907, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 70s - loss: 70.6380 - MinusLogProbMetric: 70.6380 - val_loss: 67.1391 - val_MinusLogProbMetric: 67.1391 - lr: 1.1111e-04 - 70s/epoch - 355ms/step
Epoch 16/1000
2023-10-23 13:40:58.108 
Epoch 16/1000 
	 loss: 64.8828, MinusLogProbMetric: 64.8828, val_loss: 62.9197, val_MinusLogProbMetric: 62.9197

Epoch 16: val_loss improved from 67.13907 to 62.91969, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 70s - loss: 64.8828 - MinusLogProbMetric: 64.8828 - val_loss: 62.9197 - val_MinusLogProbMetric: 62.9197 - lr: 1.1111e-04 - 70s/epoch - 355ms/step
Epoch 17/1000
2023-10-23 13:42:14.851 
Epoch 17/1000 
	 loss: 61.1425, MinusLogProbMetric: 61.1425, val_loss: 57.9210, val_MinusLogProbMetric: 57.9210

Epoch 17: val_loss improved from 62.91969 to 57.92101, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 77s - loss: 61.1425 - MinusLogProbMetric: 61.1425 - val_loss: 57.9210 - val_MinusLogProbMetric: 57.9210 - lr: 1.1111e-04 - 77s/epoch - 392ms/step
Epoch 18/1000
2023-10-23 13:43:30.442 
Epoch 18/1000 
	 loss: 135.2091, MinusLogProbMetric: 135.2091, val_loss: 233.8450, val_MinusLogProbMetric: 233.8450

Epoch 18: val_loss did not improve from 57.92101
196/196 - 74s - loss: 135.2091 - MinusLogProbMetric: 135.2091 - val_loss: 233.8450 - val_MinusLogProbMetric: 233.8450 - lr: 1.1111e-04 - 74s/epoch - 380ms/step
Epoch 19/1000
2023-10-23 13:44:41.515 
Epoch 19/1000 
	 loss: 130.0704, MinusLogProbMetric: 130.0704, val_loss: 100.0150, val_MinusLogProbMetric: 100.0150

Epoch 19: val_loss did not improve from 57.92101
196/196 - 71s - loss: 130.0704 - MinusLogProbMetric: 130.0704 - val_loss: 100.0150 - val_MinusLogProbMetric: 100.0150 - lr: 1.1111e-04 - 71s/epoch - 363ms/step
Epoch 20/1000
2023-10-23 13:45:57.995 
Epoch 20/1000 
	 loss: 90.0286, MinusLogProbMetric: 90.0286, val_loss: 80.1548, val_MinusLogProbMetric: 80.1548

Epoch 20: val_loss did not improve from 57.92101
196/196 - 76s - loss: 90.0286 - MinusLogProbMetric: 90.0286 - val_loss: 80.1548 - val_MinusLogProbMetric: 80.1548 - lr: 1.1111e-04 - 76s/epoch - 390ms/step
Epoch 21/1000
2023-10-23 13:47:03.581 
Epoch 21/1000 
	 loss: 76.2034, MinusLogProbMetric: 76.2034, val_loss: 70.6715, val_MinusLogProbMetric: 70.6715

Epoch 21: val_loss did not improve from 57.92101
196/196 - 66s - loss: 76.2034 - MinusLogProbMetric: 76.2034 - val_loss: 70.6715 - val_MinusLogProbMetric: 70.6715 - lr: 1.1111e-04 - 66s/epoch - 335ms/step
Epoch 22/1000
2023-10-23 13:48:17.103 
Epoch 22/1000 
	 loss: 79.9711, MinusLogProbMetric: 79.9711, val_loss: 92.9121, val_MinusLogProbMetric: 92.9121

Epoch 22: val_loss did not improve from 57.92101
196/196 - 74s - loss: 79.9711 - MinusLogProbMetric: 79.9711 - val_loss: 92.9121 - val_MinusLogProbMetric: 92.9121 - lr: 1.1111e-04 - 74s/epoch - 375ms/step
Epoch 23/1000
2023-10-23 13:49:29.265 
Epoch 23/1000 
	 loss: 67.3355, MinusLogProbMetric: 67.3355, val_loss: 59.8706, val_MinusLogProbMetric: 59.8706

Epoch 23: val_loss did not improve from 57.92101
196/196 - 72s - loss: 67.3355 - MinusLogProbMetric: 67.3355 - val_loss: 59.8706 - val_MinusLogProbMetric: 59.8706 - lr: 1.1111e-04 - 72s/epoch - 368ms/step
Epoch 24/1000
2023-10-23 13:50:46.142 
Epoch 24/1000 
	 loss: 56.5293, MinusLogProbMetric: 56.5293, val_loss: 54.1573, val_MinusLogProbMetric: 54.1573

Epoch 24: val_loss improved from 57.92101 to 54.15734, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 78s - loss: 56.5293 - MinusLogProbMetric: 56.5293 - val_loss: 54.1573 - val_MinusLogProbMetric: 54.1573 - lr: 1.1111e-04 - 78s/epoch - 399ms/step
Epoch 25/1000
2023-10-23 13:52:05.042 
Epoch 25/1000 
	 loss: 52.9506, MinusLogProbMetric: 52.9506, val_loss: 53.3699, val_MinusLogProbMetric: 53.3699

Epoch 25: val_loss improved from 54.15734 to 53.36988, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 79s - loss: 52.9506 - MinusLogProbMetric: 52.9506 - val_loss: 53.3699 - val_MinusLogProbMetric: 53.3699 - lr: 1.1111e-04 - 79s/epoch - 401ms/step
Epoch 26/1000
2023-10-23 13:53:15.477 
Epoch 26/1000 
	 loss: 50.7425, MinusLogProbMetric: 50.7425, val_loss: 49.8759, val_MinusLogProbMetric: 49.8759

Epoch 26: val_loss improved from 53.36988 to 49.87591, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 71s - loss: 50.7425 - MinusLogProbMetric: 50.7425 - val_loss: 49.8759 - val_MinusLogProbMetric: 49.8759 - lr: 1.1111e-04 - 71s/epoch - 362ms/step
Epoch 27/1000
2023-10-23 13:54:29.562 
Epoch 27/1000 
	 loss: 61.1104, MinusLogProbMetric: 61.1104, val_loss: 56.2177, val_MinusLogProbMetric: 56.2177

Epoch 27: val_loss did not improve from 49.87591
196/196 - 73s - loss: 61.1104 - MinusLogProbMetric: 61.1104 - val_loss: 56.2177 - val_MinusLogProbMetric: 56.2177 - lr: 1.1111e-04 - 73s/epoch - 371ms/step
Epoch 28/1000
2023-10-23 13:55:42.887 
Epoch 28/1000 
	 loss: 55.8848, MinusLogProbMetric: 55.8848, val_loss: 50.3098, val_MinusLogProbMetric: 50.3098

Epoch 28: val_loss did not improve from 49.87591
196/196 - 73s - loss: 55.8848 - MinusLogProbMetric: 55.8848 - val_loss: 50.3098 - val_MinusLogProbMetric: 50.3098 - lr: 1.1111e-04 - 73s/epoch - 374ms/step
Epoch 29/1000
2023-10-23 13:56:59.683 
Epoch 29/1000 
	 loss: 49.1347, MinusLogProbMetric: 49.1347, val_loss: 46.8632, val_MinusLogProbMetric: 46.8632

Epoch 29: val_loss improved from 49.87591 to 46.86322, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 78s - loss: 49.1347 - MinusLogProbMetric: 49.1347 - val_loss: 46.8632 - val_MinusLogProbMetric: 46.8632 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 30/1000
2023-10-23 13:58:16.435 
Epoch 30/1000 
	 loss: 47.3808, MinusLogProbMetric: 47.3808, val_loss: 45.6974, val_MinusLogProbMetric: 45.6974

Epoch 30: val_loss improved from 46.86322 to 45.69740, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 77s - loss: 47.3808 - MinusLogProbMetric: 47.3808 - val_loss: 45.6974 - val_MinusLogProbMetric: 45.6974 - lr: 1.1111e-04 - 77s/epoch - 392ms/step
Epoch 31/1000
2023-10-23 13:59:30.537 
Epoch 31/1000 
	 loss: 44.4415, MinusLogProbMetric: 44.4415, val_loss: 42.6808, val_MinusLogProbMetric: 42.6808

Epoch 31: val_loss improved from 45.69740 to 42.68075, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 74s - loss: 44.4415 - MinusLogProbMetric: 44.4415 - val_loss: 42.6808 - val_MinusLogProbMetric: 42.6808 - lr: 1.1111e-04 - 74s/epoch - 377ms/step
Epoch 32/1000
2023-10-23 14:00:46.135 
Epoch 32/1000 
	 loss: 41.8992, MinusLogProbMetric: 41.8992, val_loss: 41.0075, val_MinusLogProbMetric: 41.0075

Epoch 32: val_loss improved from 42.68075 to 41.00754, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 76s - loss: 41.8992 - MinusLogProbMetric: 41.8992 - val_loss: 41.0075 - val_MinusLogProbMetric: 41.0075 - lr: 1.1111e-04 - 76s/epoch - 386ms/step
Epoch 33/1000
2023-10-23 14:02:01.481 
Epoch 33/1000 
	 loss: 43.6493, MinusLogProbMetric: 43.6493, val_loss: 42.4672, val_MinusLogProbMetric: 42.4672

Epoch 33: val_loss did not improve from 41.00754
196/196 - 74s - loss: 43.6493 - MinusLogProbMetric: 43.6493 - val_loss: 42.4672 - val_MinusLogProbMetric: 42.4672 - lr: 1.1111e-04 - 74s/epoch - 379ms/step
Epoch 34/1000
2023-10-23 14:03:15.819 
Epoch 34/1000 
	 loss: 74.8131, MinusLogProbMetric: 74.8131, val_loss: 60.5582, val_MinusLogProbMetric: 60.5582

Epoch 34: val_loss did not improve from 41.00754
196/196 - 74s - loss: 74.8131 - MinusLogProbMetric: 74.8131 - val_loss: 60.5582 - val_MinusLogProbMetric: 60.5582 - lr: 1.1111e-04 - 74s/epoch - 379ms/step
Epoch 35/1000
2023-10-23 14:04:29.259 
Epoch 35/1000 
	 loss: 55.2906, MinusLogProbMetric: 55.2906, val_loss: 65.3797, val_MinusLogProbMetric: 65.3797

Epoch 35: val_loss did not improve from 41.00754
196/196 - 73s - loss: 55.2906 - MinusLogProbMetric: 55.2906 - val_loss: 65.3797 - val_MinusLogProbMetric: 65.3797 - lr: 1.1111e-04 - 73s/epoch - 375ms/step
Epoch 36/1000
2023-10-23 14:05:45.238 
Epoch 36/1000 
	 loss: 45.9418, MinusLogProbMetric: 45.9418, val_loss: 41.9508, val_MinusLogProbMetric: 41.9508

Epoch 36: val_loss did not improve from 41.00754
196/196 - 76s - loss: 45.9418 - MinusLogProbMetric: 45.9418 - val_loss: 41.9508 - val_MinusLogProbMetric: 41.9508 - lr: 1.1111e-04 - 76s/epoch - 388ms/step
Epoch 37/1000
2023-10-23 14:07:00.087 
Epoch 37/1000 
	 loss: 40.7489, MinusLogProbMetric: 40.7489, val_loss: 39.7482, val_MinusLogProbMetric: 39.7482

Epoch 37: val_loss improved from 41.00754 to 39.74825, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 76s - loss: 40.7489 - MinusLogProbMetric: 40.7489 - val_loss: 39.7482 - val_MinusLogProbMetric: 39.7482 - lr: 1.1111e-04 - 76s/epoch - 388ms/step
Epoch 38/1000
2023-10-23 14:08:11.066 
Epoch 38/1000 
	 loss: 40.3338, MinusLogProbMetric: 40.3338, val_loss: 38.2161, val_MinusLogProbMetric: 38.2161

Epoch 38: val_loss improved from 39.74825 to 38.21614, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 71s - loss: 40.3338 - MinusLogProbMetric: 40.3338 - val_loss: 38.2161 - val_MinusLogProbMetric: 38.2161 - lr: 1.1111e-04 - 71s/epoch - 361ms/step
Epoch 39/1000
2023-10-23 14:09:24.440 
Epoch 39/1000 
	 loss: 39.2370, MinusLogProbMetric: 39.2370, val_loss: 36.9949, val_MinusLogProbMetric: 36.9949

Epoch 39: val_loss improved from 38.21614 to 36.99487, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 74s - loss: 39.2370 - MinusLogProbMetric: 39.2370 - val_loss: 36.9949 - val_MinusLogProbMetric: 36.9949 - lr: 1.1111e-04 - 74s/epoch - 376ms/step
Epoch 40/1000
2023-10-23 14:10:36.838 
Epoch 40/1000 
	 loss: 36.3906, MinusLogProbMetric: 36.3906, val_loss: 35.8569, val_MinusLogProbMetric: 35.8569

Epoch 40: val_loss improved from 36.99487 to 35.85695, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 72s - loss: 36.3906 - MinusLogProbMetric: 36.3906 - val_loss: 35.8569 - val_MinusLogProbMetric: 35.8569 - lr: 1.1111e-04 - 72s/epoch - 368ms/step
Epoch 41/1000
2023-10-23 14:11:51.284 
Epoch 41/1000 
	 loss: 35.4285, MinusLogProbMetric: 35.4285, val_loss: 35.6561, val_MinusLogProbMetric: 35.6561

Epoch 41: val_loss improved from 35.85695 to 35.65613, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 74s - loss: 35.4285 - MinusLogProbMetric: 35.4285 - val_loss: 35.6561 - val_MinusLogProbMetric: 35.6561 - lr: 1.1111e-04 - 74s/epoch - 380ms/step
Epoch 42/1000
2023-10-23 14:13:07.774 
Epoch 42/1000 
	 loss: 35.2433, MinusLogProbMetric: 35.2433, val_loss: 34.8993, val_MinusLogProbMetric: 34.8993

Epoch 42: val_loss improved from 35.65613 to 34.89931, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 77s - loss: 35.2433 - MinusLogProbMetric: 35.2433 - val_loss: 34.8993 - val_MinusLogProbMetric: 34.8993 - lr: 1.1111e-04 - 77s/epoch - 391ms/step
Epoch 43/1000
2023-10-23 14:14:30.646 
Epoch 43/1000 
	 loss: 34.2892, MinusLogProbMetric: 34.2892, val_loss: 33.2144, val_MinusLogProbMetric: 33.2144

Epoch 43: val_loss improved from 34.89931 to 33.21442, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 83s - loss: 34.2892 - MinusLogProbMetric: 34.2892 - val_loss: 33.2144 - val_MinusLogProbMetric: 33.2144 - lr: 1.1111e-04 - 83s/epoch - 422ms/step
Epoch 44/1000
2023-10-23 14:15:47.016 
Epoch 44/1000 
	 loss: 125.5645, MinusLogProbMetric: 125.5645, val_loss: 103.4756, val_MinusLogProbMetric: 103.4756

Epoch 44: val_loss did not improve from 33.21442
196/196 - 75s - loss: 125.5645 - MinusLogProbMetric: 125.5645 - val_loss: 103.4756 - val_MinusLogProbMetric: 103.4756 - lr: 1.1111e-04 - 75s/epoch - 383ms/step
Epoch 45/1000
2023-10-23 14:16:56.789 
Epoch 45/1000 
	 loss: 88.1162, MinusLogProbMetric: 88.1162, val_loss: 76.7575, val_MinusLogProbMetric: 76.7575

Epoch 45: val_loss did not improve from 33.21442
196/196 - 70s - loss: 88.1162 - MinusLogProbMetric: 88.1162 - val_loss: 76.7575 - val_MinusLogProbMetric: 76.7575 - lr: 1.1111e-04 - 70s/epoch - 356ms/step
Epoch 46/1000
2023-10-23 14:18:05.479 
Epoch 46/1000 
	 loss: 65.1536, MinusLogProbMetric: 65.1536, val_loss: 57.4458, val_MinusLogProbMetric: 57.4458

Epoch 46: val_loss did not improve from 33.21442
196/196 - 69s - loss: 65.1536 - MinusLogProbMetric: 65.1536 - val_loss: 57.4458 - val_MinusLogProbMetric: 57.4458 - lr: 1.1111e-04 - 69s/epoch - 350ms/step
Epoch 47/1000
2023-10-23 14:19:13.700 
Epoch 47/1000 
	 loss: 54.1533, MinusLogProbMetric: 54.1533, val_loss: 50.7508, val_MinusLogProbMetric: 50.7508

Epoch 47: val_loss did not improve from 33.21442
196/196 - 68s - loss: 54.1533 - MinusLogProbMetric: 54.1533 - val_loss: 50.7508 - val_MinusLogProbMetric: 50.7508 - lr: 1.1111e-04 - 68s/epoch - 348ms/step
Epoch 48/1000
2023-10-23 14:20:23.639 
Epoch 48/1000 
	 loss: 48.4142, MinusLogProbMetric: 48.4142, val_loss: 46.3731, val_MinusLogProbMetric: 46.3731

Epoch 48: val_loss did not improve from 33.21442
196/196 - 70s - loss: 48.4142 - MinusLogProbMetric: 48.4142 - val_loss: 46.3731 - val_MinusLogProbMetric: 46.3731 - lr: 1.1111e-04 - 70s/epoch - 357ms/step
Epoch 49/1000
2023-10-23 14:21:37.272 
Epoch 49/1000 
	 loss: 56.7320, MinusLogProbMetric: 56.7320, val_loss: 46.8532, val_MinusLogProbMetric: 46.8532

Epoch 49: val_loss did not improve from 33.21442
196/196 - 74s - loss: 56.7320 - MinusLogProbMetric: 56.7320 - val_loss: 46.8532 - val_MinusLogProbMetric: 46.8532 - lr: 1.1111e-04 - 74s/epoch - 376ms/step
Epoch 50/1000
2023-10-23 14:22:54.414 
Epoch 50/1000 
	 loss: 45.6310, MinusLogProbMetric: 45.6310, val_loss: 43.1055, val_MinusLogProbMetric: 43.1055

Epoch 50: val_loss did not improve from 33.21442
196/196 - 77s - loss: 45.6310 - MinusLogProbMetric: 45.6310 - val_loss: 43.1055 - val_MinusLogProbMetric: 43.1055 - lr: 1.1111e-04 - 77s/epoch - 394ms/step
Epoch 51/1000
2023-10-23 14:24:06.122 
Epoch 51/1000 
	 loss: 41.7475, MinusLogProbMetric: 41.7475, val_loss: 40.2354, val_MinusLogProbMetric: 40.2354

Epoch 51: val_loss did not improve from 33.21442
196/196 - 72s - loss: 41.7475 - MinusLogProbMetric: 41.7475 - val_loss: 40.2354 - val_MinusLogProbMetric: 40.2354 - lr: 1.1111e-04 - 72s/epoch - 366ms/step
Epoch 52/1000
2023-10-23 14:25:14.409 
Epoch 52/1000 
	 loss: 40.1682, MinusLogProbMetric: 40.1682, val_loss: 53.7786, val_MinusLogProbMetric: 53.7786

Epoch 52: val_loss did not improve from 33.21442
196/196 - 68s - loss: 40.1682 - MinusLogProbMetric: 40.1682 - val_loss: 53.7786 - val_MinusLogProbMetric: 53.7786 - lr: 1.1111e-04 - 68s/epoch - 348ms/step
Epoch 53/1000
2023-10-23 14:26:26.274 
Epoch 53/1000 
	 loss: 39.3809, MinusLogProbMetric: 39.3809, val_loss: 37.3085, val_MinusLogProbMetric: 37.3085

Epoch 53: val_loss did not improve from 33.21442
196/196 - 72s - loss: 39.3809 - MinusLogProbMetric: 39.3809 - val_loss: 37.3085 - val_MinusLogProbMetric: 37.3085 - lr: 1.1111e-04 - 72s/epoch - 367ms/step
Epoch 54/1000
2023-10-23 14:27:36.935 
Epoch 54/1000 
	 loss: 41.5457, MinusLogProbMetric: 41.5457, val_loss: 36.7026, val_MinusLogProbMetric: 36.7026

Epoch 54: val_loss did not improve from 33.21442
196/196 - 71s - loss: 41.5457 - MinusLogProbMetric: 41.5457 - val_loss: 36.7026 - val_MinusLogProbMetric: 36.7026 - lr: 1.1111e-04 - 71s/epoch - 360ms/step
Epoch 55/1000
2023-10-23 14:28:51.423 
Epoch 55/1000 
	 loss: 37.5400, MinusLogProbMetric: 37.5400, val_loss: 34.2380, val_MinusLogProbMetric: 34.2380

Epoch 55: val_loss did not improve from 33.21442
196/196 - 74s - loss: 37.5400 - MinusLogProbMetric: 37.5400 - val_loss: 34.2380 - val_MinusLogProbMetric: 34.2380 - lr: 1.1111e-04 - 74s/epoch - 380ms/step
Epoch 56/1000
2023-10-23 14:30:05.808 
Epoch 56/1000 
	 loss: 33.6569, MinusLogProbMetric: 33.6569, val_loss: 33.1545, val_MinusLogProbMetric: 33.1545

Epoch 56: val_loss improved from 33.21442 to 33.15447, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 76s - loss: 33.6569 - MinusLogProbMetric: 33.6569 - val_loss: 33.1545 - val_MinusLogProbMetric: 33.1545 - lr: 1.1111e-04 - 76s/epoch - 386ms/step
Epoch 57/1000
2023-10-23 14:31:19.662 
Epoch 57/1000 
	 loss: 32.6742, MinusLogProbMetric: 32.6742, val_loss: 32.3298, val_MinusLogProbMetric: 32.3298

Epoch 57: val_loss improved from 33.15447 to 32.32977, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 74s - loss: 32.6742 - MinusLogProbMetric: 32.6742 - val_loss: 32.3298 - val_MinusLogProbMetric: 32.3298 - lr: 1.1111e-04 - 74s/epoch - 376ms/step
Epoch 58/1000
2023-10-23 14:32:30.420 
Epoch 58/1000 
	 loss: 31.8020, MinusLogProbMetric: 31.8020, val_loss: 31.4864, val_MinusLogProbMetric: 31.4864

Epoch 58: val_loss improved from 32.32977 to 31.48638, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 71s - loss: 31.8020 - MinusLogProbMetric: 31.8020 - val_loss: 31.4864 - val_MinusLogProbMetric: 31.4864 - lr: 1.1111e-04 - 71s/epoch - 361ms/step
Epoch 59/1000
2023-10-23 14:33:41.775 
Epoch 59/1000 
	 loss: 31.1684, MinusLogProbMetric: 31.1684, val_loss: 31.1953, val_MinusLogProbMetric: 31.1953

Epoch 59: val_loss improved from 31.48638 to 31.19526, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 71s - loss: 31.1684 - MinusLogProbMetric: 31.1684 - val_loss: 31.1953 - val_MinusLogProbMetric: 31.1953 - lr: 1.1111e-04 - 71s/epoch - 364ms/step
Epoch 60/1000
2023-10-23 14:34:55.581 
Epoch 60/1000 
	 loss: 30.5704, MinusLogProbMetric: 30.5704, val_loss: 30.2390, val_MinusLogProbMetric: 30.2390

Epoch 60: val_loss improved from 31.19526 to 30.23902, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 74s - loss: 30.5704 - MinusLogProbMetric: 30.5704 - val_loss: 30.2390 - val_MinusLogProbMetric: 30.2390 - lr: 1.1111e-04 - 74s/epoch - 376ms/step
Epoch 61/1000
2023-10-23 14:36:05.340 
Epoch 61/1000 
	 loss: 30.1344, MinusLogProbMetric: 30.1344, val_loss: 30.7369, val_MinusLogProbMetric: 30.7369

Epoch 61: val_loss did not improve from 30.23902
196/196 - 69s - loss: 30.1344 - MinusLogProbMetric: 30.1344 - val_loss: 30.7369 - val_MinusLogProbMetric: 30.7369 - lr: 1.1111e-04 - 69s/epoch - 350ms/step
Epoch 62/1000
2023-10-23 14:37:14.483 
Epoch 62/1000 
	 loss: 29.6525, MinusLogProbMetric: 29.6525, val_loss: 29.7514, val_MinusLogProbMetric: 29.7514

Epoch 62: val_loss improved from 30.23902 to 29.75140, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 70s - loss: 29.6525 - MinusLogProbMetric: 29.6525 - val_loss: 29.7514 - val_MinusLogProbMetric: 29.7514 - lr: 1.1111e-04 - 70s/epoch - 358ms/step
Epoch 63/1000
2023-10-23 14:38:35.041 
Epoch 63/1000 
	 loss: 29.1568, MinusLogProbMetric: 29.1568, val_loss: 29.2334, val_MinusLogProbMetric: 29.2334

Epoch 63: val_loss improved from 29.75140 to 29.23340, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 81s - loss: 29.1568 - MinusLogProbMetric: 29.1568 - val_loss: 29.2334 - val_MinusLogProbMetric: 29.2334 - lr: 1.1111e-04 - 81s/epoch - 413ms/step
Epoch 64/1000
2023-10-23 14:39:50.464 
Epoch 64/1000 
	 loss: 28.8066, MinusLogProbMetric: 28.8066, val_loss: 28.7833, val_MinusLogProbMetric: 28.7833

Epoch 64: val_loss improved from 29.23340 to 28.78325, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 75s - loss: 28.8066 - MinusLogProbMetric: 28.8066 - val_loss: 28.7833 - val_MinusLogProbMetric: 28.7833 - lr: 1.1111e-04 - 75s/epoch - 385ms/step
Epoch 65/1000
2023-10-23 14:41:02.993 
Epoch 65/1000 
	 loss: 28.4966, MinusLogProbMetric: 28.4966, val_loss: 28.4450, val_MinusLogProbMetric: 28.4450

Epoch 65: val_loss improved from 28.78325 to 28.44503, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 72s - loss: 28.4966 - MinusLogProbMetric: 28.4966 - val_loss: 28.4450 - val_MinusLogProbMetric: 28.4450 - lr: 1.1111e-04 - 72s/epoch - 370ms/step
Epoch 66/1000
2023-10-23 14:42:22.684 
Epoch 66/1000 
	 loss: 28.1381, MinusLogProbMetric: 28.1381, val_loss: 27.8515, val_MinusLogProbMetric: 27.8515

Epoch 66: val_loss improved from 28.44503 to 27.85148, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 80s - loss: 28.1381 - MinusLogProbMetric: 28.1381 - val_loss: 27.8515 - val_MinusLogProbMetric: 27.8515 - lr: 1.1111e-04 - 80s/epoch - 406ms/step
Epoch 67/1000
2023-10-23 14:43:34.751 
Epoch 67/1000 
	 loss: 27.8169, MinusLogProbMetric: 27.8169, val_loss: 27.8884, val_MinusLogProbMetric: 27.8884

Epoch 67: val_loss did not improve from 27.85148
196/196 - 71s - loss: 27.8169 - MinusLogProbMetric: 27.8169 - val_loss: 27.8884 - val_MinusLogProbMetric: 27.8884 - lr: 1.1111e-04 - 71s/epoch - 360ms/step
Epoch 68/1000
2023-10-23 14:44:45.383 
Epoch 68/1000 
	 loss: 27.5051, MinusLogProbMetric: 27.5051, val_loss: 27.7635, val_MinusLogProbMetric: 27.7635

Epoch 68: val_loss improved from 27.85148 to 27.76351, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 72s - loss: 27.5051 - MinusLogProbMetric: 27.5051 - val_loss: 27.7635 - val_MinusLogProbMetric: 27.7635 - lr: 1.1111e-04 - 72s/epoch - 366ms/step
Epoch 69/1000
2023-10-23 14:46:07.433 
Epoch 69/1000 
	 loss: 27.3873, MinusLogProbMetric: 27.3873, val_loss: 27.9778, val_MinusLogProbMetric: 27.9778

Epoch 69: val_loss did not improve from 27.76351
196/196 - 81s - loss: 27.3873 - MinusLogProbMetric: 27.3873 - val_loss: 27.9778 - val_MinusLogProbMetric: 27.9778 - lr: 1.1111e-04 - 81s/epoch - 413ms/step
Epoch 70/1000
2023-10-23 14:47:21.664 
Epoch 70/1000 
	 loss: 27.1029, MinusLogProbMetric: 27.1029, val_loss: 27.5036, val_MinusLogProbMetric: 27.5036

Epoch 70: val_loss improved from 27.76351 to 27.50359, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 75s - loss: 27.1029 - MinusLogProbMetric: 27.1029 - val_loss: 27.5036 - val_MinusLogProbMetric: 27.5036 - lr: 1.1111e-04 - 75s/epoch - 385ms/step
Epoch 71/1000
2023-10-23 14:48:38.070 
Epoch 71/1000 
	 loss: 26.8461, MinusLogProbMetric: 26.8461, val_loss: 26.8173, val_MinusLogProbMetric: 26.8173

Epoch 71: val_loss improved from 27.50359 to 26.81726, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 77s - loss: 26.8461 - MinusLogProbMetric: 26.8461 - val_loss: 26.8173 - val_MinusLogProbMetric: 26.8173 - lr: 1.1111e-04 - 77s/epoch - 391ms/step
Epoch 72/1000
2023-10-23 14:49:49.564 
Epoch 72/1000 
	 loss: 26.5939, MinusLogProbMetric: 26.5939, val_loss: 26.2080, val_MinusLogProbMetric: 26.2080

Epoch 72: val_loss improved from 26.81726 to 26.20798, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 71s - loss: 26.5939 - MinusLogProbMetric: 26.5939 - val_loss: 26.2080 - val_MinusLogProbMetric: 26.2080 - lr: 1.1111e-04 - 71s/epoch - 364ms/step
Epoch 73/1000
2023-10-23 14:51:01.951 
Epoch 73/1000 
	 loss: 26.4156, MinusLogProbMetric: 26.4156, val_loss: 26.8094, val_MinusLogProbMetric: 26.8094

Epoch 73: val_loss did not improve from 26.20798
196/196 - 71s - loss: 26.4156 - MinusLogProbMetric: 26.4156 - val_loss: 26.8094 - val_MinusLogProbMetric: 26.8094 - lr: 1.1111e-04 - 71s/epoch - 363ms/step
Epoch 74/1000
2023-10-23 14:52:15.097 
Epoch 74/1000 
	 loss: 26.1888, MinusLogProbMetric: 26.1888, val_loss: 26.0259, val_MinusLogProbMetric: 26.0259

Epoch 74: val_loss improved from 26.20798 to 26.02587, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 74s - loss: 26.1888 - MinusLogProbMetric: 26.1888 - val_loss: 26.0259 - val_MinusLogProbMetric: 26.0259 - lr: 1.1111e-04 - 74s/epoch - 378ms/step
Epoch 75/1000
2023-10-23 14:53:31.115 
Epoch 75/1000 
	 loss: 26.0198, MinusLogProbMetric: 26.0198, val_loss: 25.8556, val_MinusLogProbMetric: 25.8556

Epoch 75: val_loss improved from 26.02587 to 25.85564, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 76s - loss: 26.0198 - MinusLogProbMetric: 26.0198 - val_loss: 25.8556 - val_MinusLogProbMetric: 25.8556 - lr: 1.1111e-04 - 76s/epoch - 388ms/step
Epoch 76/1000
2023-10-23 14:54:43.121 
Epoch 76/1000 
	 loss: 25.8335, MinusLogProbMetric: 25.8335, val_loss: 25.6626, val_MinusLogProbMetric: 25.6626

Epoch 76: val_loss improved from 25.85564 to 25.66262, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 72s - loss: 25.8335 - MinusLogProbMetric: 25.8335 - val_loss: 25.6626 - val_MinusLogProbMetric: 25.6626 - lr: 1.1111e-04 - 72s/epoch - 367ms/step
Epoch 77/1000
2023-10-23 14:55:51.328 
Epoch 77/1000 
	 loss: 26.6720, MinusLogProbMetric: 26.6720, val_loss: 26.7098, val_MinusLogProbMetric: 26.7098

Epoch 77: val_loss did not improve from 25.66262
196/196 - 67s - loss: 26.6720 - MinusLogProbMetric: 26.6720 - val_loss: 26.7098 - val_MinusLogProbMetric: 26.7098 - lr: 1.1111e-04 - 67s/epoch - 343ms/step
Epoch 78/1000
2023-10-23 14:56:59.591 
Epoch 78/1000 
	 loss: 26.3657, MinusLogProbMetric: 26.3657, val_loss: 27.2626, val_MinusLogProbMetric: 27.2626

Epoch 78: val_loss did not improve from 25.66262
196/196 - 68s - loss: 26.3657 - MinusLogProbMetric: 26.3657 - val_loss: 27.2626 - val_MinusLogProbMetric: 27.2626 - lr: 1.1111e-04 - 68s/epoch - 348ms/step
Epoch 79/1000
2023-10-23 14:58:08.586 
Epoch 79/1000 
	 loss: 25.8499, MinusLogProbMetric: 25.8499, val_loss: 25.7459, val_MinusLogProbMetric: 25.7459

Epoch 79: val_loss did not improve from 25.66262
196/196 - 69s - loss: 25.8499 - MinusLogProbMetric: 25.8499 - val_loss: 25.7459 - val_MinusLogProbMetric: 25.7459 - lr: 1.1111e-04 - 69s/epoch - 352ms/step
Epoch 80/1000
2023-10-23 14:59:17.189 
Epoch 80/1000 
	 loss: 25.5420, MinusLogProbMetric: 25.5420, val_loss: 25.4981, val_MinusLogProbMetric: 25.4981

Epoch 80: val_loss improved from 25.66262 to 25.49807, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 70s - loss: 25.5420 - MinusLogProbMetric: 25.5420 - val_loss: 25.4981 - val_MinusLogProbMetric: 25.4981 - lr: 1.1111e-04 - 70s/epoch - 356ms/step
Epoch 81/1000
2023-10-23 15:00:24.609 
Epoch 81/1000 
	 loss: 25.2801, MinusLogProbMetric: 25.2801, val_loss: 25.0017, val_MinusLogProbMetric: 25.0017

Epoch 81: val_loss improved from 25.49807 to 25.00173, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 67s - loss: 25.2801 - MinusLogProbMetric: 25.2801 - val_loss: 25.0017 - val_MinusLogProbMetric: 25.0017 - lr: 1.1111e-04 - 67s/epoch - 344ms/step
Epoch 82/1000
2023-10-23 15:01:33.051 
Epoch 82/1000 
	 loss: 25.0541, MinusLogProbMetric: 25.0541, val_loss: 24.8968, val_MinusLogProbMetric: 24.8968

Epoch 82: val_loss improved from 25.00173 to 24.89683, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 68s - loss: 25.0541 - MinusLogProbMetric: 25.0541 - val_loss: 24.8968 - val_MinusLogProbMetric: 24.8968 - lr: 1.1111e-04 - 68s/epoch - 349ms/step
Epoch 83/1000
2023-10-23 15:02:44.570 
Epoch 83/1000 
	 loss: 24.8777, MinusLogProbMetric: 24.8777, val_loss: 25.0331, val_MinusLogProbMetric: 25.0331

Epoch 83: val_loss did not improve from 24.89683
196/196 - 70s - loss: 24.8777 - MinusLogProbMetric: 24.8777 - val_loss: 25.0331 - val_MinusLogProbMetric: 25.0331 - lr: 1.1111e-04 - 70s/epoch - 359ms/step
Epoch 84/1000
2023-10-23 15:03:53.809 
Epoch 84/1000 
	 loss: 24.7229, MinusLogProbMetric: 24.7229, val_loss: 24.4893, val_MinusLogProbMetric: 24.4893

Epoch 84: val_loss improved from 24.89683 to 24.48930, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 70s - loss: 24.7229 - MinusLogProbMetric: 24.7229 - val_loss: 24.4893 - val_MinusLogProbMetric: 24.4893 - lr: 1.1111e-04 - 70s/epoch - 359ms/step
Epoch 85/1000
2023-10-23 15:04:58.905 
Epoch 85/1000 
	 loss: 24.6979, MinusLogProbMetric: 24.6979, val_loss: 24.9965, val_MinusLogProbMetric: 24.9965

Epoch 85: val_loss did not improve from 24.48930
196/196 - 64s - loss: 24.6979 - MinusLogProbMetric: 24.6979 - val_loss: 24.9965 - val_MinusLogProbMetric: 24.9965 - lr: 1.1111e-04 - 64s/epoch - 327ms/step
Epoch 86/1000
2023-10-23 15:06:06.919 
Epoch 86/1000 
	 loss: 24.5178, MinusLogProbMetric: 24.5178, val_loss: 24.9855, val_MinusLogProbMetric: 24.9855

Epoch 86: val_loss did not improve from 24.48930
196/196 - 68s - loss: 24.5178 - MinusLogProbMetric: 24.5178 - val_loss: 24.9855 - val_MinusLogProbMetric: 24.9855 - lr: 1.1111e-04 - 68s/epoch - 347ms/step
Epoch 87/1000
2023-10-23 15:07:11.346 
Epoch 87/1000 
	 loss: 24.4553, MinusLogProbMetric: 24.4553, val_loss: 24.2858, val_MinusLogProbMetric: 24.2858

Epoch 87: val_loss improved from 24.48930 to 24.28576, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 66s - loss: 24.4553 - MinusLogProbMetric: 24.4553 - val_loss: 24.2858 - val_MinusLogProbMetric: 24.2858 - lr: 1.1111e-04 - 66s/epoch - 334ms/step
Epoch 88/1000
2023-10-23 15:08:18.491 
Epoch 88/1000 
	 loss: 24.2395, MinusLogProbMetric: 24.2395, val_loss: 24.4723, val_MinusLogProbMetric: 24.4723

Epoch 88: val_loss did not improve from 24.28576
196/196 - 66s - loss: 24.2395 - MinusLogProbMetric: 24.2395 - val_loss: 24.4723 - val_MinusLogProbMetric: 24.4723 - lr: 1.1111e-04 - 66s/epoch - 337ms/step
Epoch 89/1000
2023-10-23 15:09:27.875 
Epoch 89/1000 
	 loss: 24.1753, MinusLogProbMetric: 24.1753, val_loss: 24.0762, val_MinusLogProbMetric: 24.0762

Epoch 89: val_loss improved from 24.28576 to 24.07624, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 70s - loss: 24.1753 - MinusLogProbMetric: 24.1753 - val_loss: 24.0762 - val_MinusLogProbMetric: 24.0762 - lr: 1.1111e-04 - 70s/epoch - 359ms/step
Epoch 90/1000
2023-10-23 15:10:35.008 
Epoch 90/1000 
	 loss: 24.0756, MinusLogProbMetric: 24.0756, val_loss: 24.1002, val_MinusLogProbMetric: 24.1002

Epoch 90: val_loss did not improve from 24.07624
196/196 - 66s - loss: 24.0756 - MinusLogProbMetric: 24.0756 - val_loss: 24.1002 - val_MinusLogProbMetric: 24.1002 - lr: 1.1111e-04 - 66s/epoch - 337ms/step
Epoch 91/1000
2023-10-23 15:11:42.787 
Epoch 91/1000 
	 loss: 24.0097, MinusLogProbMetric: 24.0097, val_loss: 24.7523, val_MinusLogProbMetric: 24.7523

Epoch 91: val_loss did not improve from 24.07624
196/196 - 68s - loss: 24.0097 - MinusLogProbMetric: 24.0097 - val_loss: 24.7523 - val_MinusLogProbMetric: 24.7523 - lr: 1.1111e-04 - 68s/epoch - 346ms/step
Epoch 92/1000
2023-10-23 15:12:48.842 
Epoch 92/1000 
	 loss: 23.8710, MinusLogProbMetric: 23.8710, val_loss: 23.6526, val_MinusLogProbMetric: 23.6526

Epoch 92: val_loss improved from 24.07624 to 23.65261, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 67s - loss: 23.8710 - MinusLogProbMetric: 23.8710 - val_loss: 23.6526 - val_MinusLogProbMetric: 23.6526 - lr: 1.1111e-04 - 67s/epoch - 343ms/step
Epoch 93/1000
2023-10-23 15:13:58.102 
Epoch 93/1000 
	 loss: 23.7870, MinusLogProbMetric: 23.7870, val_loss: 24.0132, val_MinusLogProbMetric: 24.0132

Epoch 93: val_loss did not improve from 23.65261
196/196 - 68s - loss: 23.7870 - MinusLogProbMetric: 23.7870 - val_loss: 24.0132 - val_MinusLogProbMetric: 24.0132 - lr: 1.1111e-04 - 68s/epoch - 348ms/step
Epoch 94/1000
2023-10-23 15:15:06.272 
Epoch 94/1000 
	 loss: 23.7148, MinusLogProbMetric: 23.7148, val_loss: 23.7855, val_MinusLogProbMetric: 23.7855

Epoch 94: val_loss did not improve from 23.65261
196/196 - 68s - loss: 23.7148 - MinusLogProbMetric: 23.7148 - val_loss: 23.7855 - val_MinusLogProbMetric: 23.7855 - lr: 1.1111e-04 - 68s/epoch - 348ms/step
Epoch 95/1000
2023-10-23 15:16:11.358 
Epoch 95/1000 
	 loss: 23.6021, MinusLogProbMetric: 23.6021, val_loss: 23.2512, val_MinusLogProbMetric: 23.2512

Epoch 95: val_loss improved from 23.65261 to 23.25123, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 66s - loss: 23.6021 - MinusLogProbMetric: 23.6021 - val_loss: 23.2512 - val_MinusLogProbMetric: 23.2512 - lr: 1.1111e-04 - 66s/epoch - 337ms/step
Epoch 96/1000
2023-10-23 15:17:17.333 
Epoch 96/1000 
	 loss: 23.4673, MinusLogProbMetric: 23.4673, val_loss: 23.5931, val_MinusLogProbMetric: 23.5931

Epoch 96: val_loss did not improve from 23.25123
196/196 - 65s - loss: 23.4673 - MinusLogProbMetric: 23.4673 - val_loss: 23.5931 - val_MinusLogProbMetric: 23.5931 - lr: 1.1111e-04 - 65s/epoch - 331ms/step
Epoch 97/1000
2023-10-23 15:18:26.726 
Epoch 97/1000 
	 loss: 23.3409, MinusLogProbMetric: 23.3409, val_loss: 23.4345, val_MinusLogProbMetric: 23.4345

Epoch 97: val_loss did not improve from 23.25123
196/196 - 69s - loss: 23.3409 - MinusLogProbMetric: 23.3409 - val_loss: 23.4345 - val_MinusLogProbMetric: 23.4345 - lr: 1.1111e-04 - 69s/epoch - 354ms/step
Epoch 98/1000
2023-10-23 15:19:34.018 
Epoch 98/1000 
	 loss: 23.3935, MinusLogProbMetric: 23.3935, val_loss: 23.0672, val_MinusLogProbMetric: 23.0672

Epoch 98: val_loss improved from 23.25123 to 23.06719, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 68s - loss: 23.3935 - MinusLogProbMetric: 23.3935 - val_loss: 23.0672 - val_MinusLogProbMetric: 23.0672 - lr: 1.1111e-04 - 68s/epoch - 347ms/step
Epoch 99/1000
2023-10-23 15:20:39.402 
Epoch 99/1000 
	 loss: 24.8733, MinusLogProbMetric: 24.8733, val_loss: 24.6185, val_MinusLogProbMetric: 24.6185

Epoch 99: val_loss did not improve from 23.06719
196/196 - 65s - loss: 24.8733 - MinusLogProbMetric: 24.8733 - val_loss: 24.6185 - val_MinusLogProbMetric: 24.6185 - lr: 1.1111e-04 - 65s/epoch - 330ms/step
Epoch 100/1000
2023-10-23 15:21:47.246 
Epoch 100/1000 
	 loss: 23.6012, MinusLogProbMetric: 23.6012, val_loss: 23.3283, val_MinusLogProbMetric: 23.3283

Epoch 100: val_loss did not improve from 23.06719
196/196 - 68s - loss: 23.6012 - MinusLogProbMetric: 23.6012 - val_loss: 23.3283 - val_MinusLogProbMetric: 23.3283 - lr: 1.1111e-04 - 68s/epoch - 346ms/step
Epoch 101/1000
2023-10-23 15:22:56.657 
Epoch 101/1000 
	 loss: 23.1372, MinusLogProbMetric: 23.1372, val_loss: 23.2692, val_MinusLogProbMetric: 23.2692

Epoch 101: val_loss did not improve from 23.06719
196/196 - 69s - loss: 23.1372 - MinusLogProbMetric: 23.1372 - val_loss: 23.2692 - val_MinusLogProbMetric: 23.2692 - lr: 1.1111e-04 - 69s/epoch - 354ms/step
Epoch 102/1000
2023-10-23 15:24:05.059 
Epoch 102/1000 
	 loss: 23.0625, MinusLogProbMetric: 23.0625, val_loss: 23.2545, val_MinusLogProbMetric: 23.2545

Epoch 102: val_loss did not improve from 23.06719
196/196 - 68s - loss: 23.0625 - MinusLogProbMetric: 23.0625 - val_loss: 23.2545 - val_MinusLogProbMetric: 23.2545 - lr: 1.1111e-04 - 68s/epoch - 349ms/step
Epoch 103/1000
2023-10-23 15:25:12.249 
Epoch 103/1000 
	 loss: 22.9961, MinusLogProbMetric: 22.9961, val_loss: 22.9116, val_MinusLogProbMetric: 22.9116

Epoch 103: val_loss improved from 23.06719 to 22.91163, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 68s - loss: 22.9961 - MinusLogProbMetric: 22.9961 - val_loss: 22.9116 - val_MinusLogProbMetric: 22.9116 - lr: 1.1111e-04 - 68s/epoch - 349ms/step
Epoch 104/1000
2023-10-23 15:26:22.429 
Epoch 104/1000 
	 loss: 22.9760, MinusLogProbMetric: 22.9760, val_loss: 22.8889, val_MinusLogProbMetric: 22.8889

Epoch 104: val_loss improved from 22.91163 to 22.88891, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 70s - loss: 22.9760 - MinusLogProbMetric: 22.9760 - val_loss: 22.8889 - val_MinusLogProbMetric: 22.8889 - lr: 1.1111e-04 - 70s/epoch - 358ms/step
Epoch 105/1000
2023-10-23 15:27:29.627 
Epoch 105/1000 
	 loss: 22.8616, MinusLogProbMetric: 22.8616, val_loss: 22.8053, val_MinusLogProbMetric: 22.8053

Epoch 105: val_loss improved from 22.88891 to 22.80531, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 67s - loss: 22.8616 - MinusLogProbMetric: 22.8616 - val_loss: 22.8053 - val_MinusLogProbMetric: 22.8053 - lr: 1.1111e-04 - 67s/epoch - 343ms/step
Epoch 106/1000
2023-10-23 15:28:38.685 
Epoch 106/1000 
	 loss: 22.8452, MinusLogProbMetric: 22.8452, val_loss: 22.5991, val_MinusLogProbMetric: 22.5991

Epoch 106: val_loss improved from 22.80531 to 22.59914, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 69s - loss: 22.8452 - MinusLogProbMetric: 22.8452 - val_loss: 22.5991 - val_MinusLogProbMetric: 22.5991 - lr: 1.1111e-04 - 69s/epoch - 352ms/step
Epoch 107/1000
2023-10-23 15:29:48.339 
Epoch 107/1000 
	 loss: 22.7167, MinusLogProbMetric: 22.7167, val_loss: 22.9074, val_MinusLogProbMetric: 22.9074

Epoch 107: val_loss did not improve from 22.59914
196/196 - 69s - loss: 22.7167 - MinusLogProbMetric: 22.7167 - val_loss: 22.9074 - val_MinusLogProbMetric: 22.9074 - lr: 1.1111e-04 - 69s/epoch - 350ms/step
Epoch 108/1000
2023-10-23 15:30:52.823 
Epoch 108/1000 
	 loss: 22.6554, MinusLogProbMetric: 22.6554, val_loss: 22.6376, val_MinusLogProbMetric: 22.6376

Epoch 108: val_loss did not improve from 22.59914
196/196 - 64s - loss: 22.6554 - MinusLogProbMetric: 22.6554 - val_loss: 22.6376 - val_MinusLogProbMetric: 22.6376 - lr: 1.1111e-04 - 64s/epoch - 329ms/step
Epoch 109/1000
2023-10-23 15:31:58.163 
Epoch 109/1000 
	 loss: 22.6012, MinusLogProbMetric: 22.6012, val_loss: 22.3757, val_MinusLogProbMetric: 22.3757

Epoch 109: val_loss improved from 22.59914 to 22.37572, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 66s - loss: 22.6012 - MinusLogProbMetric: 22.6012 - val_loss: 22.3757 - val_MinusLogProbMetric: 22.3757 - lr: 1.1111e-04 - 66s/epoch - 339ms/step
Epoch 110/1000
2023-10-23 15:33:03.243 
Epoch 110/1000 
	 loss: 22.5347, MinusLogProbMetric: 22.5347, val_loss: 22.5884, val_MinusLogProbMetric: 22.5884

Epoch 110: val_loss did not improve from 22.37572
196/196 - 64s - loss: 22.5347 - MinusLogProbMetric: 22.5347 - val_loss: 22.5884 - val_MinusLogProbMetric: 22.5884 - lr: 1.1111e-04 - 64s/epoch - 327ms/step
Epoch 111/1000
2023-10-23 15:34:05.995 
Epoch 111/1000 
	 loss: 22.4547, MinusLogProbMetric: 22.4547, val_loss: 22.2033, val_MinusLogProbMetric: 22.2033

Epoch 111: val_loss improved from 22.37572 to 22.20329, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 64s - loss: 22.4547 - MinusLogProbMetric: 22.4547 - val_loss: 22.2033 - val_MinusLogProbMetric: 22.2033 - lr: 1.1111e-04 - 64s/epoch - 325ms/step
Epoch 112/1000
2023-10-23 15:35:11.287 
Epoch 112/1000 
	 loss: 22.4182, MinusLogProbMetric: 22.4182, val_loss: 22.3421, val_MinusLogProbMetric: 22.3421

Epoch 112: val_loss did not improve from 22.20329
196/196 - 64s - loss: 22.4182 - MinusLogProbMetric: 22.4182 - val_loss: 22.3421 - val_MinusLogProbMetric: 22.3421 - lr: 1.1111e-04 - 64s/epoch - 328ms/step
Epoch 113/1000
2023-10-23 15:36:14.992 
Epoch 113/1000 
	 loss: 22.3882, MinusLogProbMetric: 22.3882, val_loss: 22.1998, val_MinusLogProbMetric: 22.1998

Epoch 113: val_loss improved from 22.20329 to 22.19977, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 65s - loss: 22.3882 - MinusLogProbMetric: 22.3882 - val_loss: 22.1998 - val_MinusLogProbMetric: 22.1998 - lr: 1.1111e-04 - 65s/epoch - 331ms/step
Epoch 114/1000
2023-10-23 15:37:26.083 
Epoch 114/1000 
	 loss: 22.2831, MinusLogProbMetric: 22.2831, val_loss: 22.0737, val_MinusLogProbMetric: 22.0737

Epoch 114: val_loss improved from 22.19977 to 22.07367, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 71s - loss: 22.2831 - MinusLogProbMetric: 22.2831 - val_loss: 22.0737 - val_MinusLogProbMetric: 22.0737 - lr: 1.1111e-04 - 71s/epoch - 362ms/step
Epoch 115/1000
2023-10-23 15:38:30.332 
Epoch 115/1000 
	 loss: 22.1959, MinusLogProbMetric: 22.1959, val_loss: 22.5602, val_MinusLogProbMetric: 22.5602

Epoch 115: val_loss did not improve from 22.07367
196/196 - 63s - loss: 22.1959 - MinusLogProbMetric: 22.1959 - val_loss: 22.5602 - val_MinusLogProbMetric: 22.5602 - lr: 1.1111e-04 - 63s/epoch - 323ms/step
Epoch 116/1000
2023-10-23 15:39:31.634 
Epoch 116/1000 
	 loss: 22.2194, MinusLogProbMetric: 22.2194, val_loss: 22.1871, val_MinusLogProbMetric: 22.1871

Epoch 116: val_loss did not improve from 22.07367
196/196 - 61s - loss: 22.2194 - MinusLogProbMetric: 22.2194 - val_loss: 22.1871 - val_MinusLogProbMetric: 22.1871 - lr: 1.1111e-04 - 61s/epoch - 313ms/step
Epoch 117/1000
2023-10-23 15:40:34.750 
Epoch 117/1000 
	 loss: 22.2075, MinusLogProbMetric: 22.2075, val_loss: 22.1937, val_MinusLogProbMetric: 22.1937

Epoch 117: val_loss did not improve from 22.07367
196/196 - 63s - loss: 22.2075 - MinusLogProbMetric: 22.2075 - val_loss: 22.1937 - val_MinusLogProbMetric: 22.1937 - lr: 1.1111e-04 - 63s/epoch - 322ms/step
Epoch 118/1000
2023-10-23 15:41:37.683 
Epoch 118/1000 
	 loss: 22.1045, MinusLogProbMetric: 22.1045, val_loss: 21.8788, val_MinusLogProbMetric: 21.8788

Epoch 118: val_loss improved from 22.07367 to 21.87882, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 64s - loss: 22.1045 - MinusLogProbMetric: 22.1045 - val_loss: 21.8788 - val_MinusLogProbMetric: 21.8788 - lr: 1.1111e-04 - 64s/epoch - 327ms/step
Epoch 119/1000
2023-10-23 15:42:41.381 
Epoch 119/1000 
	 loss: 22.0597, MinusLogProbMetric: 22.0597, val_loss: 22.6504, val_MinusLogProbMetric: 22.6504

Epoch 119: val_loss did not improve from 21.87882
196/196 - 63s - loss: 22.0597 - MinusLogProbMetric: 22.0597 - val_loss: 22.6504 - val_MinusLogProbMetric: 22.6504 - lr: 1.1111e-04 - 63s/epoch - 319ms/step
Epoch 120/1000
2023-10-23 15:43:46.766 
Epoch 120/1000 
	 loss: 22.0244, MinusLogProbMetric: 22.0244, val_loss: 21.7397, val_MinusLogProbMetric: 21.7397

Epoch 120: val_loss improved from 21.87882 to 21.73969, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 66s - loss: 22.0244 - MinusLogProbMetric: 22.0244 - val_loss: 21.7397 - val_MinusLogProbMetric: 21.7397 - lr: 1.1111e-04 - 66s/epoch - 339ms/step
Epoch 121/1000
2023-10-23 15:44:54.522 
Epoch 121/1000 
	 loss: 21.9411, MinusLogProbMetric: 21.9411, val_loss: 21.7688, val_MinusLogProbMetric: 21.7688

Epoch 121: val_loss did not improve from 21.73969
196/196 - 67s - loss: 21.9411 - MinusLogProbMetric: 21.9411 - val_loss: 21.7688 - val_MinusLogProbMetric: 21.7688 - lr: 1.1111e-04 - 67s/epoch - 340ms/step
Epoch 122/1000
2023-10-23 15:45:55.248 
Epoch 122/1000 
	 loss: 21.9027, MinusLogProbMetric: 21.9027, val_loss: 21.7896, val_MinusLogProbMetric: 21.7896

Epoch 122: val_loss did not improve from 21.73969
196/196 - 61s - loss: 21.9027 - MinusLogProbMetric: 21.9027 - val_loss: 21.7896 - val_MinusLogProbMetric: 21.7896 - lr: 1.1111e-04 - 61s/epoch - 310ms/step
Epoch 123/1000
2023-10-23 15:47:00.079 
Epoch 123/1000 
	 loss: 21.8755, MinusLogProbMetric: 21.8755, val_loss: 21.8163, val_MinusLogProbMetric: 21.8163

Epoch 123: val_loss did not improve from 21.73969
196/196 - 65s - loss: 21.8755 - MinusLogProbMetric: 21.8755 - val_loss: 21.8163 - val_MinusLogProbMetric: 21.8163 - lr: 1.1111e-04 - 65s/epoch - 331ms/step
Epoch 124/1000
2023-10-23 15:48:05.107 
Epoch 124/1000 
	 loss: 21.8084, MinusLogProbMetric: 21.8084, val_loss: 22.3372, val_MinusLogProbMetric: 22.3372

Epoch 124: val_loss did not improve from 21.73969
196/196 - 65s - loss: 21.8084 - MinusLogProbMetric: 21.8084 - val_loss: 22.3372 - val_MinusLogProbMetric: 22.3372 - lr: 1.1111e-04 - 65s/epoch - 332ms/step
Epoch 125/1000
2023-10-23 15:49:09.342 
Epoch 125/1000 
	 loss: 21.8082, MinusLogProbMetric: 21.8082, val_loss: 21.6659, val_MinusLogProbMetric: 21.6659

Epoch 125: val_loss improved from 21.73969 to 21.66592, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 65s - loss: 21.8082 - MinusLogProbMetric: 21.8082 - val_loss: 21.6659 - val_MinusLogProbMetric: 21.6659 - lr: 1.1111e-04 - 65s/epoch - 333ms/step
Epoch 126/1000
2023-10-23 15:50:14.525 
Epoch 126/1000 
	 loss: 21.6930, MinusLogProbMetric: 21.6930, val_loss: 21.6199, val_MinusLogProbMetric: 21.6199

Epoch 126: val_loss improved from 21.66592 to 21.61991, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 65s - loss: 21.6930 - MinusLogProbMetric: 21.6930 - val_loss: 21.6199 - val_MinusLogProbMetric: 21.6199 - lr: 1.1111e-04 - 65s/epoch - 333ms/step
Epoch 127/1000
2023-10-23 15:51:19.647 
Epoch 127/1000 
	 loss: 21.7946, MinusLogProbMetric: 21.7946, val_loss: 21.5249, val_MinusLogProbMetric: 21.5249

Epoch 127: val_loss improved from 21.61991 to 21.52489, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 65s - loss: 21.7946 - MinusLogProbMetric: 21.7946 - val_loss: 21.5249 - val_MinusLogProbMetric: 21.5249 - lr: 1.1111e-04 - 65s/epoch - 333ms/step
Epoch 128/1000
2023-10-23 15:52:29.165 
Epoch 128/1000 
	 loss: 21.6641, MinusLogProbMetric: 21.6641, val_loss: 21.5589, val_MinusLogProbMetric: 21.5589

Epoch 128: val_loss did not improve from 21.52489
196/196 - 68s - loss: 21.6641 - MinusLogProbMetric: 21.6641 - val_loss: 21.5589 - val_MinusLogProbMetric: 21.5589 - lr: 1.1111e-04 - 68s/epoch - 349ms/step
Epoch 129/1000
2023-10-23 15:53:34.278 
Epoch 129/1000 
	 loss: 21.7198, MinusLogProbMetric: 21.7198, val_loss: 21.6862, val_MinusLogProbMetric: 21.6862

Epoch 129: val_loss did not improve from 21.52489
196/196 - 65s - loss: 21.7198 - MinusLogProbMetric: 21.7198 - val_loss: 21.6862 - val_MinusLogProbMetric: 21.6862 - lr: 1.1111e-04 - 65s/epoch - 332ms/step
Epoch 130/1000
2023-10-23 15:54:42.078 
Epoch 130/1000 
	 loss: 21.6121, MinusLogProbMetric: 21.6121, val_loss: 21.5286, val_MinusLogProbMetric: 21.5286

Epoch 130: val_loss did not improve from 21.52489
196/196 - 68s - loss: 21.6121 - MinusLogProbMetric: 21.6121 - val_loss: 21.5286 - val_MinusLogProbMetric: 21.5286 - lr: 1.1111e-04 - 68s/epoch - 346ms/step
Epoch 131/1000
2023-10-23 15:55:48.624 
Epoch 131/1000 
	 loss: 21.5823, MinusLogProbMetric: 21.5823, val_loss: 21.3145, val_MinusLogProbMetric: 21.3145

Epoch 131: val_loss improved from 21.52489 to 21.31449, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 68s - loss: 21.5823 - MinusLogProbMetric: 21.5823 - val_loss: 21.3145 - val_MinusLogProbMetric: 21.3145 - lr: 1.1111e-04 - 68s/epoch - 345ms/step
Epoch 132/1000
2023-10-23 15:56:54.649 
Epoch 132/1000 
	 loss: 21.5247, MinusLogProbMetric: 21.5247, val_loss: 21.4820, val_MinusLogProbMetric: 21.4820

Epoch 132: val_loss did not improve from 21.31449
196/196 - 65s - loss: 21.5247 - MinusLogProbMetric: 21.5247 - val_loss: 21.4820 - val_MinusLogProbMetric: 21.4820 - lr: 1.1111e-04 - 65s/epoch - 331ms/step
Epoch 133/1000
2023-10-23 15:58:04.439 
Epoch 133/1000 
	 loss: 21.4799, MinusLogProbMetric: 21.4799, val_loss: 21.3674, val_MinusLogProbMetric: 21.3674

Epoch 133: val_loss did not improve from 21.31449
196/196 - 70s - loss: 21.4799 - MinusLogProbMetric: 21.4799 - val_loss: 21.3674 - val_MinusLogProbMetric: 21.3674 - lr: 1.1111e-04 - 70s/epoch - 356ms/step
Epoch 134/1000
2023-10-23 15:59:10.014 
Epoch 134/1000 
	 loss: 21.4017, MinusLogProbMetric: 21.4017, val_loss: 21.3162, val_MinusLogProbMetric: 21.3162

Epoch 134: val_loss did not improve from 21.31449
196/196 - 66s - loss: 21.4017 - MinusLogProbMetric: 21.4017 - val_loss: 21.3162 - val_MinusLogProbMetric: 21.3162 - lr: 1.1111e-04 - 66s/epoch - 335ms/step
Epoch 135/1000
2023-10-23 16:00:20.142 
Epoch 135/1000 
	 loss: 21.4337, MinusLogProbMetric: 21.4337, val_loss: 21.6586, val_MinusLogProbMetric: 21.6586

Epoch 135: val_loss did not improve from 21.31449
196/196 - 70s - loss: 21.4337 - MinusLogProbMetric: 21.4337 - val_loss: 21.6586 - val_MinusLogProbMetric: 21.6586 - lr: 1.1111e-04 - 70s/epoch - 358ms/step
Epoch 136/1000
2023-10-23 16:01:24.658 
Epoch 136/1000 
	 loss: 21.4602, MinusLogProbMetric: 21.4602, val_loss: 21.2764, val_MinusLogProbMetric: 21.2764

Epoch 136: val_loss improved from 21.31449 to 21.27641, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 66s - loss: 21.4602 - MinusLogProbMetric: 21.4602 - val_loss: 21.2764 - val_MinusLogProbMetric: 21.2764 - lr: 1.1111e-04 - 66s/epoch - 335ms/step
Epoch 137/1000
2023-10-23 16:02:35.141 
Epoch 137/1000 
	 loss: 21.3347, MinusLogProbMetric: 21.3347, val_loss: 21.2742, val_MinusLogProbMetric: 21.2742

Epoch 137: val_loss improved from 21.27641 to 21.27417, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 70s - loss: 21.3347 - MinusLogProbMetric: 21.3347 - val_loss: 21.2742 - val_MinusLogProbMetric: 21.2742 - lr: 1.1111e-04 - 70s/epoch - 359ms/step
Epoch 138/1000
2023-10-23 16:03:39.735 
Epoch 138/1000 
	 loss: 21.3005, MinusLogProbMetric: 21.3005, val_loss: 21.3631, val_MinusLogProbMetric: 21.3631

Epoch 138: val_loss did not improve from 21.27417
196/196 - 64s - loss: 21.3005 - MinusLogProbMetric: 21.3005 - val_loss: 21.3631 - val_MinusLogProbMetric: 21.3631 - lr: 1.1111e-04 - 64s/epoch - 324ms/step
Epoch 139/1000
2023-10-23 16:04:50.293 
Epoch 139/1000 
	 loss: 21.2620, MinusLogProbMetric: 21.2620, val_loss: 21.2708, val_MinusLogProbMetric: 21.2708

Epoch 139: val_loss improved from 21.27417 to 21.27085, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 72s - loss: 21.2620 - MinusLogProbMetric: 21.2620 - val_loss: 21.2708 - val_MinusLogProbMetric: 21.2708 - lr: 1.1111e-04 - 72s/epoch - 365ms/step
Epoch 140/1000
2023-10-23 16:05:53.890 
Epoch 140/1000 
	 loss: 21.2652, MinusLogProbMetric: 21.2652, val_loss: 21.2658, val_MinusLogProbMetric: 21.2658

Epoch 140: val_loss improved from 21.27085 to 21.26584, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 64s - loss: 21.2652 - MinusLogProbMetric: 21.2652 - val_loss: 21.2658 - val_MinusLogProbMetric: 21.2658 - lr: 1.1111e-04 - 64s/epoch - 324ms/step
Epoch 141/1000
2023-10-23 16:07:04.637 
Epoch 141/1000 
	 loss: 21.2165, MinusLogProbMetric: 21.2165, val_loss: 21.3400, val_MinusLogProbMetric: 21.3400

Epoch 141: val_loss did not improve from 21.26584
196/196 - 70s - loss: 21.2165 - MinusLogProbMetric: 21.2165 - val_loss: 21.3400 - val_MinusLogProbMetric: 21.3400 - lr: 1.1111e-04 - 70s/epoch - 356ms/step
Epoch 142/1000
2023-10-23 16:08:11.313 
Epoch 142/1000 
	 loss: 21.2073, MinusLogProbMetric: 21.2073, val_loss: 21.6133, val_MinusLogProbMetric: 21.6133

Epoch 142: val_loss did not improve from 21.26584
196/196 - 67s - loss: 21.2073 - MinusLogProbMetric: 21.2073 - val_loss: 21.6133 - val_MinusLogProbMetric: 21.6133 - lr: 1.1111e-04 - 67s/epoch - 340ms/step
Epoch 143/1000
2023-10-23 16:09:14.135 
Epoch 143/1000 
	 loss: 21.1795, MinusLogProbMetric: 21.1795, val_loss: 20.9451, val_MinusLogProbMetric: 20.9451

Epoch 143: val_loss improved from 21.26584 to 20.94507, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 64s - loss: 21.1795 - MinusLogProbMetric: 21.1795 - val_loss: 20.9451 - val_MinusLogProbMetric: 20.9451 - lr: 1.1111e-04 - 64s/epoch - 326ms/step
Epoch 144/1000
2023-10-23 16:10:21.709 
Epoch 144/1000 
	 loss: 21.1161, MinusLogProbMetric: 21.1161, val_loss: 20.9754, val_MinusLogProbMetric: 20.9754

Epoch 144: val_loss did not improve from 20.94507
196/196 - 66s - loss: 21.1161 - MinusLogProbMetric: 21.1161 - val_loss: 20.9754 - val_MinusLogProbMetric: 20.9754 - lr: 1.1111e-04 - 66s/epoch - 339ms/step
Epoch 145/1000
2023-10-23 16:11:29.788 
Epoch 145/1000 
	 loss: 21.1081, MinusLogProbMetric: 21.1081, val_loss: 20.9317, val_MinusLogProbMetric: 20.9317

Epoch 145: val_loss improved from 20.94507 to 20.93172, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 69s - loss: 21.1081 - MinusLogProbMetric: 21.1081 - val_loss: 20.9317 - val_MinusLogProbMetric: 20.9317 - lr: 1.1111e-04 - 69s/epoch - 353ms/step
Epoch 146/1000
2023-10-23 16:12:39.517 
Epoch 146/1000 
	 loss: 21.1997, MinusLogProbMetric: 21.1997, val_loss: 21.1174, val_MinusLogProbMetric: 21.1174

Epoch 146: val_loss did not improve from 20.93172
196/196 - 69s - loss: 21.1997 - MinusLogProbMetric: 21.1997 - val_loss: 21.1174 - val_MinusLogProbMetric: 21.1174 - lr: 1.1111e-04 - 69s/epoch - 350ms/step
Epoch 147/1000
2023-10-23 16:13:46.915 
Epoch 147/1000 
	 loss: 21.0410, MinusLogProbMetric: 21.0410, val_loss: 21.1013, val_MinusLogProbMetric: 21.1013

Epoch 147: val_loss did not improve from 20.93172
196/196 - 67s - loss: 21.0410 - MinusLogProbMetric: 21.0410 - val_loss: 21.1013 - val_MinusLogProbMetric: 21.1013 - lr: 1.1111e-04 - 67s/epoch - 344ms/step
Epoch 148/1000
2023-10-23 16:14:51.855 
Epoch 148/1000 
	 loss: 21.0948, MinusLogProbMetric: 21.0948, val_loss: 21.1972, val_MinusLogProbMetric: 21.1972

Epoch 148: val_loss did not improve from 20.93172
196/196 - 65s - loss: 21.0948 - MinusLogProbMetric: 21.0948 - val_loss: 21.1972 - val_MinusLogProbMetric: 21.1972 - lr: 1.1111e-04 - 65s/epoch - 331ms/step
Epoch 149/1000
2023-10-23 16:16:01.398 
Epoch 149/1000 
	 loss: 20.9458, MinusLogProbMetric: 20.9458, val_loss: 20.8125, val_MinusLogProbMetric: 20.8125

Epoch 149: val_loss improved from 20.93172 to 20.81250, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 70s - loss: 20.9458 - MinusLogProbMetric: 20.9458 - val_loss: 20.8125 - val_MinusLogProbMetric: 20.8125 - lr: 1.1111e-04 - 70s/epoch - 359ms/step
Epoch 150/1000
2023-10-23 16:17:11.234 
Epoch 150/1000 
	 loss: 20.9666, MinusLogProbMetric: 20.9666, val_loss: 20.7167, val_MinusLogProbMetric: 20.7167

Epoch 150: val_loss improved from 20.81250 to 20.71669, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 70s - loss: 20.9666 - MinusLogProbMetric: 20.9666 - val_loss: 20.7167 - val_MinusLogProbMetric: 20.7167 - lr: 1.1111e-04 - 70s/epoch - 357ms/step
Epoch 151/1000
2023-10-23 16:18:18.272 
Epoch 151/1000 
	 loss: 20.9110, MinusLogProbMetric: 20.9110, val_loss: 21.0013, val_MinusLogProbMetric: 21.0013

Epoch 151: val_loss did not improve from 20.71669
196/196 - 66s - loss: 20.9110 - MinusLogProbMetric: 20.9110 - val_loss: 21.0013 - val_MinusLogProbMetric: 21.0013 - lr: 1.1111e-04 - 66s/epoch - 337ms/step
Epoch 152/1000
2023-10-23 16:19:25.183 
Epoch 152/1000 
	 loss: 20.8914, MinusLogProbMetric: 20.8914, val_loss: 21.0780, val_MinusLogProbMetric: 21.0780

Epoch 152: val_loss did not improve from 20.71669
196/196 - 67s - loss: 20.8914 - MinusLogProbMetric: 20.8914 - val_loss: 21.0780 - val_MinusLogProbMetric: 21.0780 - lr: 1.1111e-04 - 67s/epoch - 341ms/step
Epoch 153/1000
2023-10-23 16:20:32.256 
Epoch 153/1000 
	 loss: 20.8501, MinusLogProbMetric: 20.8501, val_loss: 21.0983, val_MinusLogProbMetric: 21.0983

Epoch 153: val_loss did not improve from 20.71669
196/196 - 67s - loss: 20.8501 - MinusLogProbMetric: 20.8501 - val_loss: 21.0983 - val_MinusLogProbMetric: 21.0983 - lr: 1.1111e-04 - 67s/epoch - 342ms/step
Epoch 154/1000
2023-10-23 16:21:36.215 
Epoch 154/1000 
	 loss: 20.8831, MinusLogProbMetric: 20.8831, val_loss: 20.8932, val_MinusLogProbMetric: 20.8932

Epoch 154: val_loss did not improve from 20.71669
196/196 - 64s - loss: 20.8831 - MinusLogProbMetric: 20.8831 - val_loss: 20.8932 - val_MinusLogProbMetric: 20.8932 - lr: 1.1111e-04 - 64s/epoch - 326ms/step
Epoch 155/1000
2023-10-23 16:22:44.385 
Epoch 155/1000 
	 loss: 20.8665, MinusLogProbMetric: 20.8665, val_loss: 20.7323, val_MinusLogProbMetric: 20.7323

Epoch 155: val_loss did not improve from 20.71669
196/196 - 68s - loss: 20.8665 - MinusLogProbMetric: 20.8665 - val_loss: 20.7323 - val_MinusLogProbMetric: 20.7323 - lr: 1.1111e-04 - 68s/epoch - 348ms/step
Epoch 156/1000
2023-10-23 16:23:50.740 
Epoch 156/1000 
	 loss: 20.9096, MinusLogProbMetric: 20.9096, val_loss: 21.1428, val_MinusLogProbMetric: 21.1428

Epoch 156: val_loss did not improve from 20.71669
196/196 - 66s - loss: 20.9096 - MinusLogProbMetric: 20.9096 - val_loss: 21.1428 - val_MinusLogProbMetric: 21.1428 - lr: 1.1111e-04 - 66s/epoch - 339ms/step
Epoch 157/1000
2023-10-23 16:24:57.487 
Epoch 157/1000 
	 loss: 20.7829, MinusLogProbMetric: 20.7829, val_loss: 21.2539, val_MinusLogProbMetric: 21.2539

Epoch 157: val_loss did not improve from 20.71669
196/196 - 67s - loss: 20.7829 - MinusLogProbMetric: 20.7829 - val_loss: 21.2539 - val_MinusLogProbMetric: 21.2539 - lr: 1.1111e-04 - 67s/epoch - 341ms/step
Epoch 158/1000
2023-10-23 16:26:01.363 
Epoch 158/1000 
	 loss: 20.7544, MinusLogProbMetric: 20.7544, val_loss: 21.0458, val_MinusLogProbMetric: 21.0458

Epoch 158: val_loss did not improve from 20.71669
196/196 - 64s - loss: 20.7544 - MinusLogProbMetric: 20.7544 - val_loss: 21.0458 - val_MinusLogProbMetric: 21.0458 - lr: 1.1111e-04 - 64s/epoch - 326ms/step
Epoch 159/1000
2023-10-23 16:27:11.354 
Epoch 159/1000 
	 loss: 20.8008, MinusLogProbMetric: 20.8008, val_loss: 20.6213, val_MinusLogProbMetric: 20.6213

Epoch 159: val_loss improved from 20.71669 to 20.62133, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 71s - loss: 20.8008 - MinusLogProbMetric: 20.8008 - val_loss: 20.6213 - val_MinusLogProbMetric: 20.6213 - lr: 1.1111e-04 - 71s/epoch - 363ms/step
Epoch 160/1000
2023-10-23 16:28:20.089 
Epoch 160/1000 
	 loss: 20.7359, MinusLogProbMetric: 20.7359, val_loss: 21.8092, val_MinusLogProbMetric: 21.8092

Epoch 160: val_loss did not improve from 20.62133
196/196 - 68s - loss: 20.7359 - MinusLogProbMetric: 20.7359 - val_loss: 21.8092 - val_MinusLogProbMetric: 21.8092 - lr: 1.1111e-04 - 68s/epoch - 345ms/step
Epoch 161/1000
2023-10-23 16:29:23.966 
Epoch 161/1000 
	 loss: 20.6815, MinusLogProbMetric: 20.6815, val_loss: 20.5791, val_MinusLogProbMetric: 20.5791

Epoch 161: val_loss improved from 20.62133 to 20.57910, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 65s - loss: 20.6815 - MinusLogProbMetric: 20.6815 - val_loss: 20.5791 - val_MinusLogProbMetric: 20.5791 - lr: 1.1111e-04 - 65s/epoch - 332ms/step
Epoch 162/1000
2023-10-23 16:30:30.832 
Epoch 162/1000 
	 loss: 20.6820, MinusLogProbMetric: 20.6820, val_loss: 20.9055, val_MinusLogProbMetric: 20.9055

Epoch 162: val_loss did not improve from 20.57910
196/196 - 66s - loss: 20.6820 - MinusLogProbMetric: 20.6820 - val_loss: 20.9055 - val_MinusLogProbMetric: 20.9055 - lr: 1.1111e-04 - 66s/epoch - 335ms/step
Epoch 163/1000
2023-10-23 16:31:36.443 
Epoch 163/1000 
	 loss: 20.6705, MinusLogProbMetric: 20.6705, val_loss: 20.6566, val_MinusLogProbMetric: 20.6566

Epoch 163: val_loss did not improve from 20.57910
196/196 - 66s - loss: 20.6705 - MinusLogProbMetric: 20.6705 - val_loss: 20.6566 - val_MinusLogProbMetric: 20.6566 - lr: 1.1111e-04 - 66s/epoch - 335ms/step
Epoch 164/1000
2023-10-23 16:32:46.545 
Epoch 164/1000 
	 loss: 20.6702, MinusLogProbMetric: 20.6702, val_loss: 20.5901, val_MinusLogProbMetric: 20.5901

Epoch 164: val_loss did not improve from 20.57910
196/196 - 70s - loss: 20.6702 - MinusLogProbMetric: 20.6702 - val_loss: 20.5901 - val_MinusLogProbMetric: 20.5901 - lr: 1.1111e-04 - 70s/epoch - 358ms/step
Epoch 165/1000
2023-10-23 16:33:52.905 
Epoch 165/1000 
	 loss: 20.6158, MinusLogProbMetric: 20.6158, val_loss: 20.4871, val_MinusLogProbMetric: 20.4871

Epoch 165: val_loss improved from 20.57910 to 20.48709, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 67s - loss: 20.6158 - MinusLogProbMetric: 20.6158 - val_loss: 20.4871 - val_MinusLogProbMetric: 20.4871 - lr: 1.1111e-04 - 67s/epoch - 344ms/step
Epoch 166/1000
2023-10-23 16:35:01.273 
Epoch 166/1000 
	 loss: 20.5698, MinusLogProbMetric: 20.5698, val_loss: 20.5740, val_MinusLogProbMetric: 20.5740

Epoch 166: val_loss did not improve from 20.48709
196/196 - 67s - loss: 20.5698 - MinusLogProbMetric: 20.5698 - val_loss: 20.5740 - val_MinusLogProbMetric: 20.5740 - lr: 1.1111e-04 - 67s/epoch - 343ms/step
Epoch 167/1000
2023-10-23 16:36:04.931 
Epoch 167/1000 
	 loss: 20.5446, MinusLogProbMetric: 20.5446, val_loss: 20.7210, val_MinusLogProbMetric: 20.7210

Epoch 167: val_loss did not improve from 20.48709
196/196 - 64s - loss: 20.5446 - MinusLogProbMetric: 20.5446 - val_loss: 20.7210 - val_MinusLogProbMetric: 20.7210 - lr: 1.1111e-04 - 64s/epoch - 325ms/step
Epoch 168/1000
2023-10-23 16:37:10.091 
Epoch 168/1000 
	 loss: 20.5246, MinusLogProbMetric: 20.5246, val_loss: 20.3467, val_MinusLogProbMetric: 20.3467

Epoch 168: val_loss improved from 20.48709 to 20.34675, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 66s - loss: 20.5246 - MinusLogProbMetric: 20.5246 - val_loss: 20.3467 - val_MinusLogProbMetric: 20.3467 - lr: 1.1111e-04 - 66s/epoch - 338ms/step
Epoch 169/1000
2023-10-23 16:38:19.280 
Epoch 169/1000 
	 loss: 20.5309, MinusLogProbMetric: 20.5309, val_loss: 20.4487, val_MinusLogProbMetric: 20.4487

Epoch 169: val_loss did not improve from 20.34675
196/196 - 68s - loss: 20.5309 - MinusLogProbMetric: 20.5309 - val_loss: 20.4487 - val_MinusLogProbMetric: 20.4487 - lr: 1.1111e-04 - 68s/epoch - 347ms/step
Epoch 170/1000
2023-10-23 16:39:29.604 
Epoch 170/1000 
	 loss: 20.4761, MinusLogProbMetric: 20.4761, val_loss: 20.7825, val_MinusLogProbMetric: 20.7825

Epoch 170: val_loss did not improve from 20.34675
196/196 - 70s - loss: 20.4761 - MinusLogProbMetric: 20.4761 - val_loss: 20.7825 - val_MinusLogProbMetric: 20.7825 - lr: 1.1111e-04 - 70s/epoch - 359ms/step
Epoch 171/1000
2023-10-23 16:40:34.413 
Epoch 171/1000 
	 loss: 20.4741, MinusLogProbMetric: 20.4741, val_loss: 20.5034, val_MinusLogProbMetric: 20.5034

Epoch 171: val_loss did not improve from 20.34675
196/196 - 65s - loss: 20.4741 - MinusLogProbMetric: 20.4741 - val_loss: 20.5034 - val_MinusLogProbMetric: 20.5034 - lr: 1.1111e-04 - 65s/epoch - 331ms/step
Epoch 172/1000
2023-10-23 16:41:39.585 
Epoch 172/1000 
	 loss: 20.4828, MinusLogProbMetric: 20.4828, val_loss: 20.3279, val_MinusLogProbMetric: 20.3279

Epoch 172: val_loss improved from 20.34675 to 20.32788, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 66s - loss: 20.4828 - MinusLogProbMetric: 20.4828 - val_loss: 20.3279 - val_MinusLogProbMetric: 20.3279 - lr: 1.1111e-04 - 66s/epoch - 338ms/step
Epoch 173/1000
2023-10-23 16:42:49.104 
Epoch 173/1000 
	 loss: 20.3571, MinusLogProbMetric: 20.3571, val_loss: 20.7222, val_MinusLogProbMetric: 20.7222

Epoch 173: val_loss did not improve from 20.32788
196/196 - 68s - loss: 20.3571 - MinusLogProbMetric: 20.3571 - val_loss: 20.7222 - val_MinusLogProbMetric: 20.7222 - lr: 1.1111e-04 - 68s/epoch - 349ms/step
Epoch 174/1000
2023-10-23 16:43:57.052 
Epoch 174/1000 
	 loss: 20.3834, MinusLogProbMetric: 20.3834, val_loss: 20.7146, val_MinusLogProbMetric: 20.7146

Epoch 174: val_loss did not improve from 20.32788
196/196 - 68s - loss: 20.3834 - MinusLogProbMetric: 20.3834 - val_loss: 20.7146 - val_MinusLogProbMetric: 20.7146 - lr: 1.1111e-04 - 68s/epoch - 347ms/step
Epoch 175/1000
2023-10-23 16:45:03.053 
Epoch 175/1000 
	 loss: 20.3920, MinusLogProbMetric: 20.3920, val_loss: 20.8212, val_MinusLogProbMetric: 20.8212

Epoch 175: val_loss did not improve from 20.32788
196/196 - 66s - loss: 20.3920 - MinusLogProbMetric: 20.3920 - val_loss: 20.8212 - val_MinusLogProbMetric: 20.8212 - lr: 1.1111e-04 - 66s/epoch - 337ms/step
Epoch 176/1000
2023-10-23 16:46:12.815 
Epoch 176/1000 
	 loss: 20.3600, MinusLogProbMetric: 20.3600, val_loss: 20.3107, val_MinusLogProbMetric: 20.3107

Epoch 176: val_loss improved from 20.32788 to 20.31074, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 71s - loss: 20.3600 - MinusLogProbMetric: 20.3600 - val_loss: 20.3107 - val_MinusLogProbMetric: 20.3107 - lr: 1.1111e-04 - 71s/epoch - 361ms/step
Epoch 177/1000
2023-10-23 16:47:20.143 
Epoch 177/1000 
	 loss: 20.3546, MinusLogProbMetric: 20.3546, val_loss: 20.1134, val_MinusLogProbMetric: 20.1134

Epoch 177: val_loss improved from 20.31074 to 20.11338, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 67s - loss: 20.3546 - MinusLogProbMetric: 20.3546 - val_loss: 20.1134 - val_MinusLogProbMetric: 20.1134 - lr: 1.1111e-04 - 67s/epoch - 344ms/step
Epoch 178/1000
2023-10-23 16:48:23.052 
Epoch 178/1000 
	 loss: 20.3339, MinusLogProbMetric: 20.3339, val_loss: 20.2740, val_MinusLogProbMetric: 20.2740

Epoch 178: val_loss did not improve from 20.11338
196/196 - 62s - loss: 20.3339 - MinusLogProbMetric: 20.3339 - val_loss: 20.2740 - val_MinusLogProbMetric: 20.2740 - lr: 1.1111e-04 - 62s/epoch - 316ms/step
Epoch 179/1000
2023-10-23 16:49:28.299 
Epoch 179/1000 
	 loss: 20.3502, MinusLogProbMetric: 20.3502, val_loss: 20.3041, val_MinusLogProbMetric: 20.3041

Epoch 179: val_loss did not improve from 20.11338
196/196 - 65s - loss: 20.3502 - MinusLogProbMetric: 20.3502 - val_loss: 20.3041 - val_MinusLogProbMetric: 20.3041 - lr: 1.1111e-04 - 65s/epoch - 333ms/step
Epoch 180/1000
2023-10-23 16:50:31.663 
Epoch 180/1000 
	 loss: 20.3653, MinusLogProbMetric: 20.3653, val_loss: 20.6516, val_MinusLogProbMetric: 20.6516

Epoch 180: val_loss did not improve from 20.11338
196/196 - 63s - loss: 20.3653 - MinusLogProbMetric: 20.3653 - val_loss: 20.6516 - val_MinusLogProbMetric: 20.6516 - lr: 1.1111e-04 - 63s/epoch - 323ms/step
Epoch 181/1000
2023-10-23 16:51:34.461 
Epoch 181/1000 
	 loss: 20.3203, MinusLogProbMetric: 20.3203, val_loss: 20.8040, val_MinusLogProbMetric: 20.8040

Epoch 181: val_loss did not improve from 20.11338
196/196 - 63s - loss: 20.3203 - MinusLogProbMetric: 20.3203 - val_loss: 20.8040 - val_MinusLogProbMetric: 20.8040 - lr: 1.1111e-04 - 63s/epoch - 320ms/step
Epoch 182/1000
2023-10-23 16:52:38.233 
Epoch 182/1000 
	 loss: 20.2792, MinusLogProbMetric: 20.2792, val_loss: 20.3741, val_MinusLogProbMetric: 20.3741

Epoch 182: val_loss did not improve from 20.11338
196/196 - 64s - loss: 20.2792 - MinusLogProbMetric: 20.2792 - val_loss: 20.3741 - val_MinusLogProbMetric: 20.3741 - lr: 1.1111e-04 - 64s/epoch - 325ms/step
Epoch 183/1000
2023-10-23 16:53:43.731 
Epoch 183/1000 
	 loss: 20.1828, MinusLogProbMetric: 20.1828, val_loss: 20.3495, val_MinusLogProbMetric: 20.3495

Epoch 183: val_loss did not improve from 20.11338
196/196 - 65s - loss: 20.1828 - MinusLogProbMetric: 20.1828 - val_loss: 20.3495 - val_MinusLogProbMetric: 20.3495 - lr: 1.1111e-04 - 65s/epoch - 334ms/step
Epoch 184/1000
2023-10-23 16:54:45.575 
Epoch 184/1000 
	 loss: 20.2462, MinusLogProbMetric: 20.2462, val_loss: 20.3113, val_MinusLogProbMetric: 20.3113

Epoch 184: val_loss did not improve from 20.11338
196/196 - 62s - loss: 20.2462 - MinusLogProbMetric: 20.2462 - val_loss: 20.3113 - val_MinusLogProbMetric: 20.3113 - lr: 1.1111e-04 - 62s/epoch - 316ms/step
Epoch 185/1000
2023-10-23 16:55:48.977 
Epoch 185/1000 
	 loss: 20.1864, MinusLogProbMetric: 20.1864, val_loss: 20.0573, val_MinusLogProbMetric: 20.0573

Epoch 185: val_loss improved from 20.11338 to 20.05732, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 64s - loss: 20.1864 - MinusLogProbMetric: 20.1864 - val_loss: 20.0573 - val_MinusLogProbMetric: 20.0573 - lr: 1.1111e-04 - 64s/epoch - 329ms/step
Epoch 186/1000
2023-10-23 16:56:51.613 
Epoch 186/1000 
	 loss: 20.1406, MinusLogProbMetric: 20.1406, val_loss: 20.2195, val_MinusLogProbMetric: 20.2195

Epoch 186: val_loss did not improve from 20.05732
196/196 - 62s - loss: 20.1406 - MinusLogProbMetric: 20.1406 - val_loss: 20.2195 - val_MinusLogProbMetric: 20.2195 - lr: 1.1111e-04 - 62s/epoch - 314ms/step
Epoch 187/1000
2023-10-23 16:57:55.587 
Epoch 187/1000 
	 loss: 20.1415, MinusLogProbMetric: 20.1415, val_loss: 20.1914, val_MinusLogProbMetric: 20.1914

Epoch 187: val_loss did not improve from 20.05732
196/196 - 64s - loss: 20.1415 - MinusLogProbMetric: 20.1415 - val_loss: 20.1914 - val_MinusLogProbMetric: 20.1914 - lr: 1.1111e-04 - 64s/epoch - 326ms/step
Epoch 188/1000
2023-10-23 16:58:58.110 
Epoch 188/1000 
	 loss: 20.0897, MinusLogProbMetric: 20.0897, val_loss: 19.9370, val_MinusLogProbMetric: 19.9370

Epoch 188: val_loss improved from 20.05732 to 19.93703, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 64s - loss: 20.0897 - MinusLogProbMetric: 20.0897 - val_loss: 19.9370 - val_MinusLogProbMetric: 19.9370 - lr: 1.1111e-04 - 64s/epoch - 324ms/step
Epoch 189/1000
2023-10-23 17:00:03.546 
Epoch 189/1000 
	 loss: 20.1558, MinusLogProbMetric: 20.1558, val_loss: 20.0894, val_MinusLogProbMetric: 20.0894

Epoch 189: val_loss did not improve from 19.93703
196/196 - 64s - loss: 20.1558 - MinusLogProbMetric: 20.1558 - val_loss: 20.0894 - val_MinusLogProbMetric: 20.0894 - lr: 1.1111e-04 - 64s/epoch - 329ms/step
Epoch 190/1000
2023-10-23 17:01:06.951 
Epoch 190/1000 
	 loss: 20.0837, MinusLogProbMetric: 20.0837, val_loss: 20.4088, val_MinusLogProbMetric: 20.4088

Epoch 190: val_loss did not improve from 19.93703
196/196 - 63s - loss: 20.0837 - MinusLogProbMetric: 20.0837 - val_loss: 20.4088 - val_MinusLogProbMetric: 20.4088 - lr: 1.1111e-04 - 63s/epoch - 323ms/step
Epoch 191/1000
2023-10-23 17:02:10.682 
Epoch 191/1000 
	 loss: 20.1195, MinusLogProbMetric: 20.1195, val_loss: 21.2548, val_MinusLogProbMetric: 21.2548

Epoch 191: val_loss did not improve from 19.93703
196/196 - 64s - loss: 20.1195 - MinusLogProbMetric: 20.1195 - val_loss: 21.2548 - val_MinusLogProbMetric: 21.2548 - lr: 1.1111e-04 - 64s/epoch - 325ms/step
Epoch 192/1000
2023-10-23 17:03:14.165 
Epoch 192/1000 
	 loss: 20.0725, MinusLogProbMetric: 20.0725, val_loss: 20.4721, val_MinusLogProbMetric: 20.4721

Epoch 192: val_loss did not improve from 19.93703
196/196 - 63s - loss: 20.0725 - MinusLogProbMetric: 20.0725 - val_loss: 20.4721 - val_MinusLogProbMetric: 20.4721 - lr: 1.1111e-04 - 63s/epoch - 324ms/step
Epoch 193/1000
2023-10-23 17:04:18.011 
Epoch 193/1000 
	 loss: 20.0758, MinusLogProbMetric: 20.0758, val_loss: 20.0888, val_MinusLogProbMetric: 20.0888

Epoch 193: val_loss did not improve from 19.93703
196/196 - 64s - loss: 20.0758 - MinusLogProbMetric: 20.0758 - val_loss: 20.0888 - val_MinusLogProbMetric: 20.0888 - lr: 1.1111e-04 - 64s/epoch - 326ms/step
Epoch 194/1000
2023-10-23 17:05:21.892 
Epoch 194/1000 
	 loss: 20.0106, MinusLogProbMetric: 20.0106, val_loss: 20.0165, val_MinusLogProbMetric: 20.0165

Epoch 194: val_loss did not improve from 19.93703
196/196 - 64s - loss: 20.0106 - MinusLogProbMetric: 20.0106 - val_loss: 20.0165 - val_MinusLogProbMetric: 20.0165 - lr: 1.1111e-04 - 64s/epoch - 326ms/step
Epoch 195/1000
2023-10-23 17:06:25.730 
Epoch 195/1000 
	 loss: 20.1043, MinusLogProbMetric: 20.1043, val_loss: 20.0837, val_MinusLogProbMetric: 20.0837

Epoch 195: val_loss did not improve from 19.93703
196/196 - 64s - loss: 20.1043 - MinusLogProbMetric: 20.1043 - val_loss: 20.0837 - val_MinusLogProbMetric: 20.0837 - lr: 1.1111e-04 - 64s/epoch - 326ms/step
Epoch 196/1000
2023-10-23 17:07:29.961 
Epoch 196/1000 
	 loss: 19.9998, MinusLogProbMetric: 19.9998, val_loss: 19.9375, val_MinusLogProbMetric: 19.9375

Epoch 196: val_loss did not improve from 19.93703
196/196 - 64s - loss: 19.9998 - MinusLogProbMetric: 19.9998 - val_loss: 19.9375 - val_MinusLogProbMetric: 19.9375 - lr: 1.1111e-04 - 64s/epoch - 328ms/step
Epoch 197/1000
2023-10-23 17:08:35.686 
Epoch 197/1000 
	 loss: 19.9851, MinusLogProbMetric: 19.9851, val_loss: 19.8844, val_MinusLogProbMetric: 19.8844

Epoch 197: val_loss improved from 19.93703 to 19.88439, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 67s - loss: 19.9851 - MinusLogProbMetric: 19.9851 - val_loss: 19.8844 - val_MinusLogProbMetric: 19.8844 - lr: 1.1111e-04 - 67s/epoch - 340ms/step
Epoch 198/1000
2023-10-23 17:09:39.529 
Epoch 198/1000 
	 loss: 20.0221, MinusLogProbMetric: 20.0221, val_loss: 19.9354, val_MinusLogProbMetric: 19.9354

Epoch 198: val_loss did not improve from 19.88439
196/196 - 63s - loss: 20.0221 - MinusLogProbMetric: 20.0221 - val_loss: 19.9354 - val_MinusLogProbMetric: 19.9354 - lr: 1.1111e-04 - 63s/epoch - 321ms/step
Epoch 199/1000
2023-10-23 17:10:46.576 
Epoch 199/1000 
	 loss: 19.9811, MinusLogProbMetric: 19.9811, val_loss: 20.1111, val_MinusLogProbMetric: 20.1111

Epoch 199: val_loss did not improve from 19.88439
196/196 - 67s - loss: 19.9811 - MinusLogProbMetric: 19.9811 - val_loss: 20.1111 - val_MinusLogProbMetric: 20.1111 - lr: 1.1111e-04 - 67s/epoch - 342ms/step
Epoch 200/1000
2023-10-23 17:11:48.150 
Epoch 200/1000 
	 loss: 19.9571, MinusLogProbMetric: 19.9571, val_loss: 19.8849, val_MinusLogProbMetric: 19.8849

Epoch 200: val_loss did not improve from 19.88439
196/196 - 62s - loss: 19.9571 - MinusLogProbMetric: 19.9571 - val_loss: 19.8849 - val_MinusLogProbMetric: 19.8849 - lr: 1.1111e-04 - 62s/epoch - 314ms/step
Epoch 201/1000
2023-10-23 17:12:51.986 
Epoch 201/1000 
	 loss: 20.0082, MinusLogProbMetric: 20.0082, val_loss: 20.0404, val_MinusLogProbMetric: 20.0404

Epoch 201: val_loss did not improve from 19.88439
196/196 - 64s - loss: 20.0082 - MinusLogProbMetric: 20.0082 - val_loss: 20.0404 - val_MinusLogProbMetric: 20.0404 - lr: 1.1111e-04 - 64s/epoch - 326ms/step
Epoch 202/1000
2023-10-23 17:13:55.082 
Epoch 202/1000 
	 loss: 19.9001, MinusLogProbMetric: 19.9001, val_loss: 19.6673, val_MinusLogProbMetric: 19.6673

Epoch 202: val_loss improved from 19.88439 to 19.66729, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 64s - loss: 19.9001 - MinusLogProbMetric: 19.9001 - val_loss: 19.6673 - val_MinusLogProbMetric: 19.6673 - lr: 1.1111e-04 - 64s/epoch - 327ms/step
Epoch 203/1000
2023-10-23 17:14:56.970 
Epoch 203/1000 
	 loss: 19.9475, MinusLogProbMetric: 19.9475, val_loss: 19.7895, val_MinusLogProbMetric: 19.7895

Epoch 203: val_loss did not improve from 19.66729
196/196 - 61s - loss: 19.9475 - MinusLogProbMetric: 19.9475 - val_loss: 19.7895 - val_MinusLogProbMetric: 19.7895 - lr: 1.1111e-04 - 61s/epoch - 311ms/step
Epoch 204/1000
2023-10-23 17:16:02.841 
Epoch 204/1000 
	 loss: 19.9302, MinusLogProbMetric: 19.9302, val_loss: 19.9824, val_MinusLogProbMetric: 19.9824

Epoch 204: val_loss did not improve from 19.66729
196/196 - 66s - loss: 19.9302 - MinusLogProbMetric: 19.9302 - val_loss: 19.9824 - val_MinusLogProbMetric: 19.9824 - lr: 1.1111e-04 - 66s/epoch - 336ms/step
Epoch 205/1000
2023-10-23 17:17:04.585 
Epoch 205/1000 
	 loss: 19.8418, MinusLogProbMetric: 19.8418, val_loss: 19.8899, val_MinusLogProbMetric: 19.8899

Epoch 205: val_loss did not improve from 19.66729
196/196 - 62s - loss: 19.8418 - MinusLogProbMetric: 19.8418 - val_loss: 19.8899 - val_MinusLogProbMetric: 19.8899 - lr: 1.1111e-04 - 62s/epoch - 315ms/step
Epoch 206/1000
2023-10-23 17:18:07.315 
Epoch 206/1000 
	 loss: 19.8366, MinusLogProbMetric: 19.8366, val_loss: 20.2726, val_MinusLogProbMetric: 20.2726

Epoch 206: val_loss did not improve from 19.66729
196/196 - 63s - loss: 19.8366 - MinusLogProbMetric: 19.8366 - val_loss: 20.2726 - val_MinusLogProbMetric: 20.2726 - lr: 1.1111e-04 - 63s/epoch - 320ms/step
Epoch 207/1000
2023-10-23 17:19:10.454 
Epoch 207/1000 
	 loss: 19.9147, MinusLogProbMetric: 19.9147, val_loss: 19.8859, val_MinusLogProbMetric: 19.8859

Epoch 207: val_loss did not improve from 19.66729
196/196 - 63s - loss: 19.9147 - MinusLogProbMetric: 19.9147 - val_loss: 19.8859 - val_MinusLogProbMetric: 19.8859 - lr: 1.1111e-04 - 63s/epoch - 322ms/step
Epoch 208/1000
2023-10-23 17:20:12.859 
Epoch 208/1000 
	 loss: 19.8283, MinusLogProbMetric: 19.8283, val_loss: 19.8111, val_MinusLogProbMetric: 19.8111

Epoch 208: val_loss did not improve from 19.66729
196/196 - 62s - loss: 19.8283 - MinusLogProbMetric: 19.8283 - val_loss: 19.8111 - val_MinusLogProbMetric: 19.8111 - lr: 1.1111e-04 - 62s/epoch - 318ms/step
Epoch 209/1000
2023-10-23 17:21:17.551 
Epoch 209/1000 
	 loss: 19.9252, MinusLogProbMetric: 19.9252, val_loss: 20.3289, val_MinusLogProbMetric: 20.3289

Epoch 209: val_loss did not improve from 19.66729
196/196 - 65s - loss: 19.9252 - MinusLogProbMetric: 19.9252 - val_loss: 20.3289 - val_MinusLogProbMetric: 20.3289 - lr: 1.1111e-04 - 65s/epoch - 330ms/step
Epoch 210/1000
2023-10-23 17:22:19.278 
Epoch 210/1000 
	 loss: 19.7689, MinusLogProbMetric: 19.7689, val_loss: 20.0360, val_MinusLogProbMetric: 20.0360

Epoch 210: val_loss did not improve from 19.66729
196/196 - 62s - loss: 19.7689 - MinusLogProbMetric: 19.7689 - val_loss: 20.0360 - val_MinusLogProbMetric: 20.0360 - lr: 1.1111e-04 - 62s/epoch - 315ms/step
Epoch 211/1000
2023-10-23 17:23:21.391 
Epoch 211/1000 
	 loss: 19.7737, MinusLogProbMetric: 19.7737, val_loss: 19.6983, val_MinusLogProbMetric: 19.6983

Epoch 211: val_loss did not improve from 19.66729
196/196 - 62s - loss: 19.7737 - MinusLogProbMetric: 19.7737 - val_loss: 19.6983 - val_MinusLogProbMetric: 19.6983 - lr: 1.1111e-04 - 62s/epoch - 317ms/step
Epoch 212/1000
2023-10-23 17:24:26.575 
Epoch 212/1000 
	 loss: 19.7483, MinusLogProbMetric: 19.7483, val_loss: 19.7432, val_MinusLogProbMetric: 19.7432

Epoch 212: val_loss did not improve from 19.66729
196/196 - 65s - loss: 19.7483 - MinusLogProbMetric: 19.7483 - val_loss: 19.7432 - val_MinusLogProbMetric: 19.7432 - lr: 1.1111e-04 - 65s/epoch - 333ms/step
Epoch 213/1000
2023-10-23 17:25:29.634 
Epoch 213/1000 
	 loss: 19.7371, MinusLogProbMetric: 19.7371, val_loss: 19.8890, val_MinusLogProbMetric: 19.8890

Epoch 213: val_loss did not improve from 19.66729
196/196 - 63s - loss: 19.7371 - MinusLogProbMetric: 19.7371 - val_loss: 19.8890 - val_MinusLogProbMetric: 19.8890 - lr: 1.1111e-04 - 63s/epoch - 322ms/step
Epoch 214/1000
2023-10-23 17:26:34.234 
Epoch 214/1000 
	 loss: 19.7374, MinusLogProbMetric: 19.7374, val_loss: 19.7357, val_MinusLogProbMetric: 19.7357

Epoch 214: val_loss did not improve from 19.66729
196/196 - 65s - loss: 19.7374 - MinusLogProbMetric: 19.7374 - val_loss: 19.7357 - val_MinusLogProbMetric: 19.7357 - lr: 1.1111e-04 - 65s/epoch - 330ms/step
Epoch 215/1000
2023-10-23 17:27:38.300 
Epoch 215/1000 
	 loss: 19.7057, MinusLogProbMetric: 19.7057, val_loss: 19.6829, val_MinusLogProbMetric: 19.6829

Epoch 215: val_loss did not improve from 19.66729
196/196 - 64s - loss: 19.7057 - MinusLogProbMetric: 19.7057 - val_loss: 19.6829 - val_MinusLogProbMetric: 19.6829 - lr: 1.1111e-04 - 64s/epoch - 327ms/step
Epoch 216/1000
2023-10-23 17:28:40.643 
Epoch 216/1000 
	 loss: 19.7013, MinusLogProbMetric: 19.7013, val_loss: 19.7161, val_MinusLogProbMetric: 19.7161

Epoch 216: val_loss did not improve from 19.66729
196/196 - 62s - loss: 19.7013 - MinusLogProbMetric: 19.7013 - val_loss: 19.7161 - val_MinusLogProbMetric: 19.7161 - lr: 1.1111e-04 - 62s/epoch - 318ms/step
Epoch 217/1000
2023-10-23 17:29:43.811 
Epoch 217/1000 
	 loss: 19.6551, MinusLogProbMetric: 19.6551, val_loss: 19.6249, val_MinusLogProbMetric: 19.6249

Epoch 217: val_loss improved from 19.66729 to 19.62489, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 64s - loss: 19.6551 - MinusLogProbMetric: 19.6551 - val_loss: 19.6249 - val_MinusLogProbMetric: 19.6249 - lr: 1.1111e-04 - 64s/epoch - 327ms/step
Epoch 218/1000
2023-10-23 17:30:46.716 
Epoch 218/1000 
	 loss: 19.6366, MinusLogProbMetric: 19.6366, val_loss: 19.6683, val_MinusLogProbMetric: 19.6683

Epoch 218: val_loss did not improve from 19.62489
196/196 - 62s - loss: 19.6366 - MinusLogProbMetric: 19.6366 - val_loss: 19.6683 - val_MinusLogProbMetric: 19.6683 - lr: 1.1111e-04 - 62s/epoch - 316ms/step
Epoch 219/1000
2023-10-23 17:31:48.620 
Epoch 219/1000 
	 loss: 19.6547, MinusLogProbMetric: 19.6547, val_loss: 19.7815, val_MinusLogProbMetric: 19.7815

Epoch 219: val_loss did not improve from 19.62489
196/196 - 62s - loss: 19.6547 - MinusLogProbMetric: 19.6547 - val_loss: 19.7815 - val_MinusLogProbMetric: 19.7815 - lr: 1.1111e-04 - 62s/epoch - 316ms/step
Epoch 220/1000
2023-10-23 17:32:52.666 
Epoch 220/1000 
	 loss: 19.6721, MinusLogProbMetric: 19.6721, val_loss: 19.6115, val_MinusLogProbMetric: 19.6115

Epoch 220: val_loss improved from 19.62489 to 19.61152, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 65s - loss: 19.6721 - MinusLogProbMetric: 19.6721 - val_loss: 19.6115 - val_MinusLogProbMetric: 19.6115 - lr: 1.1111e-04 - 65s/epoch - 332ms/step
Epoch 221/1000
2023-10-23 17:33:56.840 
Epoch 221/1000 
	 loss: 19.6289, MinusLogProbMetric: 19.6289, val_loss: 19.8664, val_MinusLogProbMetric: 19.8664

Epoch 221: val_loss did not improve from 19.61152
196/196 - 63s - loss: 19.6289 - MinusLogProbMetric: 19.6289 - val_loss: 19.8664 - val_MinusLogProbMetric: 19.8664 - lr: 1.1111e-04 - 63s/epoch - 322ms/step
Epoch 222/1000
2023-10-23 17:34:59.866 
Epoch 222/1000 
	 loss: 19.5851, MinusLogProbMetric: 19.5851, val_loss: 19.6299, val_MinusLogProbMetric: 19.6299

Epoch 222: val_loss did not improve from 19.61152
196/196 - 63s - loss: 19.5851 - MinusLogProbMetric: 19.5851 - val_loss: 19.6299 - val_MinusLogProbMetric: 19.6299 - lr: 1.1111e-04 - 63s/epoch - 322ms/step
Epoch 223/1000
2023-10-23 17:36:03.980 
Epoch 223/1000 
	 loss: 19.6836, MinusLogProbMetric: 19.6836, val_loss: 19.9164, val_MinusLogProbMetric: 19.9164

Epoch 223: val_loss did not improve from 19.61152
196/196 - 64s - loss: 19.6836 - MinusLogProbMetric: 19.6836 - val_loss: 19.9164 - val_MinusLogProbMetric: 19.9164 - lr: 1.1111e-04 - 64s/epoch - 327ms/step
Epoch 224/1000
2023-10-23 17:37:05.943 
Epoch 224/1000 
	 loss: 19.6052, MinusLogProbMetric: 19.6052, val_loss: 19.9292, val_MinusLogProbMetric: 19.9292

Epoch 224: val_loss did not improve from 19.61152
196/196 - 62s - loss: 19.6052 - MinusLogProbMetric: 19.6052 - val_loss: 19.9292 - val_MinusLogProbMetric: 19.9292 - lr: 1.1111e-04 - 62s/epoch - 316ms/step
Epoch 225/1000
2023-10-23 17:38:11.338 
Epoch 225/1000 
	 loss: 19.5972, MinusLogProbMetric: 19.5972, val_loss: 19.4928, val_MinusLogProbMetric: 19.4928

Epoch 225: val_loss improved from 19.61152 to 19.49282, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 66s - loss: 19.5972 - MinusLogProbMetric: 19.5972 - val_loss: 19.4928 - val_MinusLogProbMetric: 19.4928 - lr: 1.1111e-04 - 66s/epoch - 339ms/step
Epoch 226/1000
2023-10-23 17:39:14.998 
Epoch 226/1000 
	 loss: 19.4846, MinusLogProbMetric: 19.4846, val_loss: 19.6946, val_MinusLogProbMetric: 19.6946

Epoch 226: val_loss did not improve from 19.49282
196/196 - 63s - loss: 19.4846 - MinusLogProbMetric: 19.4846 - val_loss: 19.6946 - val_MinusLogProbMetric: 19.6946 - lr: 1.1111e-04 - 63s/epoch - 319ms/step
Epoch 227/1000
2023-10-23 17:40:17.972 
Epoch 227/1000 
	 loss: 19.5591, MinusLogProbMetric: 19.5591, val_loss: 19.7090, val_MinusLogProbMetric: 19.7090

Epoch 227: val_loss did not improve from 19.49282
196/196 - 63s - loss: 19.5591 - MinusLogProbMetric: 19.5591 - val_loss: 19.7090 - val_MinusLogProbMetric: 19.7090 - lr: 1.1111e-04 - 63s/epoch - 321ms/step
Epoch 228/1000
2023-10-23 17:41:21.174 
Epoch 228/1000 
	 loss: 19.5556, MinusLogProbMetric: 19.5556, val_loss: 19.5211, val_MinusLogProbMetric: 19.5211

Epoch 228: val_loss did not improve from 19.49282
196/196 - 63s - loss: 19.5556 - MinusLogProbMetric: 19.5556 - val_loss: 19.5211 - val_MinusLogProbMetric: 19.5211 - lr: 1.1111e-04 - 63s/epoch - 322ms/step
Epoch 229/1000
2023-10-23 17:42:24.913 
Epoch 229/1000 
	 loss: 19.5445, MinusLogProbMetric: 19.5445, val_loss: 19.6082, val_MinusLogProbMetric: 19.6082

Epoch 229: val_loss did not improve from 19.49282
196/196 - 64s - loss: 19.5445 - MinusLogProbMetric: 19.5445 - val_loss: 19.6082 - val_MinusLogProbMetric: 19.6082 - lr: 1.1111e-04 - 64s/epoch - 325ms/step
Epoch 230/1000
2023-10-23 17:43:27.539 
Epoch 230/1000 
	 loss: 19.5778, MinusLogProbMetric: 19.5778, val_loss: 19.9836, val_MinusLogProbMetric: 19.9836

Epoch 230: val_loss did not improve from 19.49282
196/196 - 63s - loss: 19.5778 - MinusLogProbMetric: 19.5778 - val_loss: 19.9836 - val_MinusLogProbMetric: 19.9836 - lr: 1.1111e-04 - 63s/epoch - 320ms/step
Epoch 231/1000
2023-10-23 17:44:30.146 
Epoch 231/1000 
	 loss: 19.5216, MinusLogProbMetric: 19.5216, val_loss: 20.2845, val_MinusLogProbMetric: 20.2845

Epoch 231: val_loss did not improve from 19.49282
196/196 - 63s - loss: 19.5216 - MinusLogProbMetric: 19.5216 - val_loss: 20.2845 - val_MinusLogProbMetric: 20.2845 - lr: 1.1111e-04 - 63s/epoch - 319ms/step
Epoch 232/1000
2023-10-23 17:45:31.767 
Epoch 232/1000 
	 loss: 19.5035, MinusLogProbMetric: 19.5035, val_loss: 19.6863, val_MinusLogProbMetric: 19.6863

Epoch 232: val_loss did not improve from 19.49282
196/196 - 62s - loss: 19.5035 - MinusLogProbMetric: 19.5035 - val_loss: 19.6863 - val_MinusLogProbMetric: 19.6863 - lr: 1.1111e-04 - 62s/epoch - 314ms/step
Epoch 233/1000
2023-10-23 17:46:35.029 
Epoch 233/1000 
	 loss: 19.4705, MinusLogProbMetric: 19.4705, val_loss: 20.0286, val_MinusLogProbMetric: 20.0286

Epoch 233: val_loss did not improve from 19.49282
196/196 - 63s - loss: 19.4705 - MinusLogProbMetric: 19.4705 - val_loss: 20.0286 - val_MinusLogProbMetric: 20.0286 - lr: 1.1111e-04 - 63s/epoch - 323ms/step
Epoch 234/1000
2023-10-23 17:47:38.990 
Epoch 234/1000 
	 loss: 19.4558, MinusLogProbMetric: 19.4558, val_loss: 19.6340, val_MinusLogProbMetric: 19.6340

Epoch 234: val_loss did not improve from 19.49282
196/196 - 64s - loss: 19.4558 - MinusLogProbMetric: 19.4558 - val_loss: 19.6340 - val_MinusLogProbMetric: 19.6340 - lr: 1.1111e-04 - 64s/epoch - 326ms/step
Epoch 235/1000
2023-10-23 17:48:42.613 
Epoch 235/1000 
	 loss: 19.4996, MinusLogProbMetric: 19.4996, val_loss: 19.4878, val_MinusLogProbMetric: 19.4878

Epoch 235: val_loss improved from 19.49282 to 19.48781, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 65s - loss: 19.4996 - MinusLogProbMetric: 19.4996 - val_loss: 19.4878 - val_MinusLogProbMetric: 19.4878 - lr: 1.1111e-04 - 65s/epoch - 330ms/step
Epoch 236/1000
2023-10-23 17:49:47.844 
Epoch 236/1000 
	 loss: 19.4548, MinusLogProbMetric: 19.4548, val_loss: 19.3617, val_MinusLogProbMetric: 19.3617

Epoch 236: val_loss improved from 19.48781 to 19.36171, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 65s - loss: 19.4548 - MinusLogProbMetric: 19.4548 - val_loss: 19.3617 - val_MinusLogProbMetric: 19.3617 - lr: 1.1111e-04 - 65s/epoch - 333ms/step
Epoch 237/1000
2023-10-23 17:50:52.613 
Epoch 237/1000 
	 loss: 19.4408, MinusLogProbMetric: 19.4408, val_loss: 19.5907, val_MinusLogProbMetric: 19.5907

Epoch 237: val_loss did not improve from 19.36171
196/196 - 64s - loss: 19.4408 - MinusLogProbMetric: 19.4408 - val_loss: 19.5907 - val_MinusLogProbMetric: 19.5907 - lr: 1.1111e-04 - 64s/epoch - 325ms/step
Epoch 238/1000
2023-10-23 17:51:56.302 
Epoch 238/1000 
	 loss: 19.3902, MinusLogProbMetric: 19.3902, val_loss: 19.4344, val_MinusLogProbMetric: 19.4344

Epoch 238: val_loss did not improve from 19.36171
196/196 - 64s - loss: 19.3902 - MinusLogProbMetric: 19.3902 - val_loss: 19.4344 - val_MinusLogProbMetric: 19.4344 - lr: 1.1111e-04 - 64s/epoch - 325ms/step
Epoch 239/1000
2023-10-23 17:53:00.638 
Epoch 239/1000 
	 loss: 19.4502, MinusLogProbMetric: 19.4502, val_loss: 19.3732, val_MinusLogProbMetric: 19.3732

Epoch 239: val_loss did not improve from 19.36171
196/196 - 64s - loss: 19.4502 - MinusLogProbMetric: 19.4502 - val_loss: 19.3732 - val_MinusLogProbMetric: 19.3732 - lr: 1.1111e-04 - 64s/epoch - 328ms/step
Epoch 240/1000
2023-10-23 17:54:04.287 
Epoch 240/1000 
	 loss: 19.4635, MinusLogProbMetric: 19.4635, val_loss: 19.3083, val_MinusLogProbMetric: 19.3083

Epoch 240: val_loss improved from 19.36171 to 19.30834, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 65s - loss: 19.4635 - MinusLogProbMetric: 19.4635 - val_loss: 19.3083 - val_MinusLogProbMetric: 19.3083 - lr: 1.1111e-04 - 65s/epoch - 331ms/step
Epoch 241/1000
2023-10-23 17:55:09.344 
Epoch 241/1000 
	 loss: 19.3893, MinusLogProbMetric: 19.3893, val_loss: 19.4064, val_MinusLogProbMetric: 19.4064

Epoch 241: val_loss did not improve from 19.30834
196/196 - 64s - loss: 19.3893 - MinusLogProbMetric: 19.3893 - val_loss: 19.4064 - val_MinusLogProbMetric: 19.4064 - lr: 1.1111e-04 - 64s/epoch - 326ms/step
Epoch 242/1000
2023-10-23 17:56:15.688 
Epoch 242/1000 
	 loss: 19.4491, MinusLogProbMetric: 19.4491, val_loss: 19.3537, val_MinusLogProbMetric: 19.3537

Epoch 242: val_loss did not improve from 19.30834
196/196 - 66s - loss: 19.4491 - MinusLogProbMetric: 19.4491 - val_loss: 19.3537 - val_MinusLogProbMetric: 19.3537 - lr: 1.1111e-04 - 66s/epoch - 338ms/step
Epoch 243/1000
2023-10-23 17:57:17.796 
Epoch 243/1000 
	 loss: 19.3912, MinusLogProbMetric: 19.3912, val_loss: 19.2963, val_MinusLogProbMetric: 19.2963

Epoch 243: val_loss improved from 19.30834 to 19.29630, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 63s - loss: 19.3912 - MinusLogProbMetric: 19.3912 - val_loss: 19.2963 - val_MinusLogProbMetric: 19.2963 - lr: 1.1111e-04 - 63s/epoch - 322ms/step
Epoch 244/1000
2023-10-23 17:58:22.029 
Epoch 244/1000 
	 loss: 19.3714, MinusLogProbMetric: 19.3714, val_loss: 19.4012, val_MinusLogProbMetric: 19.4012

Epoch 244: val_loss did not improve from 19.29630
196/196 - 63s - loss: 19.3714 - MinusLogProbMetric: 19.3714 - val_loss: 19.4012 - val_MinusLogProbMetric: 19.4012 - lr: 1.1111e-04 - 63s/epoch - 322ms/step
Epoch 245/1000
2023-10-23 17:59:25.780 
Epoch 245/1000 
	 loss: 19.3467, MinusLogProbMetric: 19.3467, val_loss: 19.3414, val_MinusLogProbMetric: 19.3414

Epoch 245: val_loss did not improve from 19.29630
196/196 - 64s - loss: 19.3467 - MinusLogProbMetric: 19.3467 - val_loss: 19.3414 - val_MinusLogProbMetric: 19.3414 - lr: 1.1111e-04 - 64s/epoch - 325ms/step
Epoch 246/1000
2023-10-23 18:00:31.888 
Epoch 246/1000 
	 loss: 19.3417, MinusLogProbMetric: 19.3417, val_loss: 19.4618, val_MinusLogProbMetric: 19.4618

Epoch 246: val_loss did not improve from 19.29630
196/196 - 66s - loss: 19.3417 - MinusLogProbMetric: 19.3417 - val_loss: 19.4618 - val_MinusLogProbMetric: 19.4618 - lr: 1.1111e-04 - 66s/epoch - 337ms/step
Epoch 247/1000
2023-10-23 18:01:34.867 
Epoch 247/1000 
	 loss: 19.3617, MinusLogProbMetric: 19.3617, val_loss: 19.5030, val_MinusLogProbMetric: 19.5030

Epoch 247: val_loss did not improve from 19.29630
196/196 - 63s - loss: 19.3617 - MinusLogProbMetric: 19.3617 - val_loss: 19.5030 - val_MinusLogProbMetric: 19.5030 - lr: 1.1111e-04 - 63s/epoch - 321ms/step
Epoch 248/1000
2023-10-23 18:02:43.458 
Epoch 248/1000 
	 loss: 19.3464, MinusLogProbMetric: 19.3464, val_loss: 19.1490, val_MinusLogProbMetric: 19.1490

Epoch 248: val_loss improved from 19.29630 to 19.14900, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 70s - loss: 19.3464 - MinusLogProbMetric: 19.3464 - val_loss: 19.1490 - val_MinusLogProbMetric: 19.1490 - lr: 1.1111e-04 - 70s/epoch - 355ms/step
Epoch 249/1000
2023-10-23 18:03:48.059 
Epoch 249/1000 
	 loss: 19.3081, MinusLogProbMetric: 19.3081, val_loss: 19.8923, val_MinusLogProbMetric: 19.8923

Epoch 249: val_loss did not improve from 19.14900
196/196 - 64s - loss: 19.3081 - MinusLogProbMetric: 19.3081 - val_loss: 19.8923 - val_MinusLogProbMetric: 19.8923 - lr: 1.1111e-04 - 64s/epoch - 324ms/step
Epoch 250/1000
2023-10-23 18:04:51.167 
Epoch 250/1000 
	 loss: 19.3673, MinusLogProbMetric: 19.3673, val_loss: 19.3645, val_MinusLogProbMetric: 19.3645

Epoch 250: val_loss did not improve from 19.14900
196/196 - 63s - loss: 19.3673 - MinusLogProbMetric: 19.3673 - val_loss: 19.3645 - val_MinusLogProbMetric: 19.3645 - lr: 1.1111e-04 - 63s/epoch - 322ms/step
Epoch 251/1000
2023-10-23 18:05:55.105 
Epoch 251/1000 
	 loss: 19.3587, MinusLogProbMetric: 19.3587, val_loss: 19.3457, val_MinusLogProbMetric: 19.3457

Epoch 251: val_loss did not improve from 19.14900
196/196 - 64s - loss: 19.3587 - MinusLogProbMetric: 19.3587 - val_loss: 19.3457 - val_MinusLogProbMetric: 19.3457 - lr: 1.1111e-04 - 64s/epoch - 326ms/step
Epoch 252/1000
2023-10-23 18:06:58.188 
Epoch 252/1000 
	 loss: 19.2735, MinusLogProbMetric: 19.2735, val_loss: 19.4438, val_MinusLogProbMetric: 19.4438

Epoch 252: val_loss did not improve from 19.14900
196/196 - 63s - loss: 19.2735 - MinusLogProbMetric: 19.2735 - val_loss: 19.4438 - val_MinusLogProbMetric: 19.4438 - lr: 1.1111e-04 - 63s/epoch - 322ms/step
Epoch 253/1000
2023-10-23 18:08:02.271 
Epoch 253/1000 
	 loss: 19.2613, MinusLogProbMetric: 19.2613, val_loss: 19.3401, val_MinusLogProbMetric: 19.3401

Epoch 253: val_loss did not improve from 19.14900
196/196 - 64s - loss: 19.2613 - MinusLogProbMetric: 19.2613 - val_loss: 19.3401 - val_MinusLogProbMetric: 19.3401 - lr: 1.1111e-04 - 64s/epoch - 327ms/step
Epoch 254/1000
2023-10-23 18:09:05.103 
Epoch 254/1000 
	 loss: 19.3078, MinusLogProbMetric: 19.3078, val_loss: 19.2107, val_MinusLogProbMetric: 19.2107

Epoch 254: val_loss did not improve from 19.14900
196/196 - 63s - loss: 19.3078 - MinusLogProbMetric: 19.3078 - val_loss: 19.2107 - val_MinusLogProbMetric: 19.2107 - lr: 1.1111e-04 - 63s/epoch - 321ms/step
Epoch 255/1000
2023-10-23 18:10:09.439 
Epoch 255/1000 
	 loss: 19.2143, MinusLogProbMetric: 19.2143, val_loss: 19.3135, val_MinusLogProbMetric: 19.3135

Epoch 255: val_loss did not improve from 19.14900
196/196 - 64s - loss: 19.2143 - MinusLogProbMetric: 19.2143 - val_loss: 19.3135 - val_MinusLogProbMetric: 19.3135 - lr: 1.1111e-04 - 64s/epoch - 328ms/step
Epoch 256/1000
2023-10-23 18:11:11.038 
Epoch 256/1000 
	 loss: 19.2266, MinusLogProbMetric: 19.2266, val_loss: 19.5073, val_MinusLogProbMetric: 19.5073

Epoch 256: val_loss did not improve from 19.14900
196/196 - 62s - loss: 19.2266 - MinusLogProbMetric: 19.2266 - val_loss: 19.5073 - val_MinusLogProbMetric: 19.5073 - lr: 1.1111e-04 - 62s/epoch - 314ms/step
Epoch 257/1000
2023-10-23 18:12:14.392 
Epoch 257/1000 
	 loss: 19.2447, MinusLogProbMetric: 19.2447, val_loss: 19.5178, val_MinusLogProbMetric: 19.5178

Epoch 257: val_loss did not improve from 19.14900
196/196 - 63s - loss: 19.2447 - MinusLogProbMetric: 19.2447 - val_loss: 19.5178 - val_MinusLogProbMetric: 19.5178 - lr: 1.1111e-04 - 63s/epoch - 323ms/step
Epoch 258/1000
2023-10-23 18:13:20.220 
Epoch 258/1000 
	 loss: 19.2081, MinusLogProbMetric: 19.2081, val_loss: 19.2977, val_MinusLogProbMetric: 19.2977

Epoch 258: val_loss did not improve from 19.14900
196/196 - 66s - loss: 19.2081 - MinusLogProbMetric: 19.2081 - val_loss: 19.2977 - val_MinusLogProbMetric: 19.2977 - lr: 1.1111e-04 - 66s/epoch - 336ms/step
Epoch 259/1000
2023-10-23 18:14:22.758 
Epoch 259/1000 
	 loss: 19.1799, MinusLogProbMetric: 19.1799, val_loss: 19.2157, val_MinusLogProbMetric: 19.2157

Epoch 259: val_loss did not improve from 19.14900
196/196 - 63s - loss: 19.1799 - MinusLogProbMetric: 19.1799 - val_loss: 19.2157 - val_MinusLogProbMetric: 19.2157 - lr: 1.1111e-04 - 63s/epoch - 319ms/step
Epoch 260/1000
2023-10-23 18:15:26.150 
Epoch 260/1000 
	 loss: 19.2351, MinusLogProbMetric: 19.2351, val_loss: 19.4908, val_MinusLogProbMetric: 19.4908

Epoch 260: val_loss did not improve from 19.14900
196/196 - 63s - loss: 19.2351 - MinusLogProbMetric: 19.2351 - val_loss: 19.4908 - val_MinusLogProbMetric: 19.4908 - lr: 1.1111e-04 - 63s/epoch - 323ms/step
Epoch 261/1000
2023-10-23 18:16:29.467 
Epoch 261/1000 
	 loss: 19.1842, MinusLogProbMetric: 19.1842, val_loss: 19.6415, val_MinusLogProbMetric: 19.6415

Epoch 261: val_loss did not improve from 19.14900
196/196 - 63s - loss: 19.1842 - MinusLogProbMetric: 19.1842 - val_loss: 19.6415 - val_MinusLogProbMetric: 19.6415 - lr: 1.1111e-04 - 63s/epoch - 323ms/step
Epoch 262/1000
2023-10-23 18:17:31.874 
Epoch 262/1000 
	 loss: 19.1784, MinusLogProbMetric: 19.1784, val_loss: 19.3656, val_MinusLogProbMetric: 19.3656

Epoch 262: val_loss did not improve from 19.14900
196/196 - 62s - loss: 19.1784 - MinusLogProbMetric: 19.1784 - val_loss: 19.3656 - val_MinusLogProbMetric: 19.3656 - lr: 1.1111e-04 - 62s/epoch - 318ms/step
Epoch 263/1000
2023-10-23 18:18:35.583 
Epoch 263/1000 
	 loss: 19.2178, MinusLogProbMetric: 19.2178, val_loss: 19.2260, val_MinusLogProbMetric: 19.2260

Epoch 263: val_loss did not improve from 19.14900
196/196 - 64s - loss: 19.2178 - MinusLogProbMetric: 19.2178 - val_loss: 19.2260 - val_MinusLogProbMetric: 19.2260 - lr: 1.1111e-04 - 64s/epoch - 325ms/step
Epoch 264/1000
2023-10-23 18:19:38.021 
Epoch 264/1000 
	 loss: 19.1454, MinusLogProbMetric: 19.1454, val_loss: 19.1784, val_MinusLogProbMetric: 19.1784

Epoch 264: val_loss did not improve from 19.14900
196/196 - 62s - loss: 19.1454 - MinusLogProbMetric: 19.1454 - val_loss: 19.1784 - val_MinusLogProbMetric: 19.1784 - lr: 1.1111e-04 - 62s/epoch - 319ms/step
Epoch 265/1000
2023-10-23 18:20:40.530 
Epoch 265/1000 
	 loss: 19.1829, MinusLogProbMetric: 19.1829, val_loss: 19.3463, val_MinusLogProbMetric: 19.3463

Epoch 265: val_loss did not improve from 19.14900
196/196 - 63s - loss: 19.1829 - MinusLogProbMetric: 19.1829 - val_loss: 19.3463 - val_MinusLogProbMetric: 19.3463 - lr: 1.1111e-04 - 63s/epoch - 319ms/step
Epoch 266/1000
2023-10-23 18:21:46.083 
Epoch 266/1000 
	 loss: 19.1638, MinusLogProbMetric: 19.1638, val_loss: 19.3295, val_MinusLogProbMetric: 19.3295

Epoch 266: val_loss did not improve from 19.14900
196/196 - 66s - loss: 19.1638 - MinusLogProbMetric: 19.1638 - val_loss: 19.3295 - val_MinusLogProbMetric: 19.3295 - lr: 1.1111e-04 - 66s/epoch - 334ms/step
Epoch 267/1000
2023-10-23 18:22:49.845 
Epoch 267/1000 
	 loss: 19.1285, MinusLogProbMetric: 19.1285, val_loss: 19.1738, val_MinusLogProbMetric: 19.1738

Epoch 267: val_loss did not improve from 19.14900
196/196 - 64s - loss: 19.1285 - MinusLogProbMetric: 19.1285 - val_loss: 19.1738 - val_MinusLogProbMetric: 19.1738 - lr: 1.1111e-04 - 64s/epoch - 325ms/step
Epoch 268/1000
2023-10-23 18:23:52.316 
Epoch 268/1000 
	 loss: 19.1287, MinusLogProbMetric: 19.1287, val_loss: 18.9582, val_MinusLogProbMetric: 18.9582

Epoch 268: val_loss improved from 19.14900 to 18.95822, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 64s - loss: 19.1287 - MinusLogProbMetric: 19.1287 - val_loss: 18.9582 - val_MinusLogProbMetric: 18.9582 - lr: 1.1111e-04 - 64s/epoch - 325ms/step
Epoch 269/1000
2023-10-23 18:24:59.115 
Epoch 269/1000 
	 loss: 19.1167, MinusLogProbMetric: 19.1167, val_loss: 19.1944, val_MinusLogProbMetric: 19.1944

Epoch 269: val_loss did not improve from 18.95822
196/196 - 66s - loss: 19.1167 - MinusLogProbMetric: 19.1167 - val_loss: 19.1944 - val_MinusLogProbMetric: 19.1944 - lr: 1.1111e-04 - 66s/epoch - 335ms/step
Epoch 270/1000
2023-10-23 18:26:02.873 
Epoch 270/1000 
	 loss: 19.1626, MinusLogProbMetric: 19.1626, val_loss: 19.2764, val_MinusLogProbMetric: 19.2764

Epoch 270: val_loss did not improve from 18.95822
196/196 - 64s - loss: 19.1626 - MinusLogProbMetric: 19.1626 - val_loss: 19.2764 - val_MinusLogProbMetric: 19.2764 - lr: 1.1111e-04 - 64s/epoch - 325ms/step
Epoch 271/1000
2023-10-23 18:27:07.219 
Epoch 271/1000 
	 loss: 19.0793, MinusLogProbMetric: 19.0793, val_loss: 18.9974, val_MinusLogProbMetric: 18.9974

Epoch 271: val_loss did not improve from 18.95822
196/196 - 64s - loss: 19.0793 - MinusLogProbMetric: 19.0793 - val_loss: 18.9974 - val_MinusLogProbMetric: 18.9974 - lr: 1.1111e-04 - 64s/epoch - 328ms/step
Epoch 272/1000
2023-10-23 18:28:11.140 
Epoch 272/1000 
	 loss: 19.0462, MinusLogProbMetric: 19.0462, val_loss: 19.2250, val_MinusLogProbMetric: 19.2250

Epoch 272: val_loss did not improve from 18.95822
196/196 - 64s - loss: 19.0462 - MinusLogProbMetric: 19.0462 - val_loss: 19.2250 - val_MinusLogProbMetric: 19.2250 - lr: 1.1111e-04 - 64s/epoch - 326ms/step
Epoch 273/1000
2023-10-23 18:29:12.954 
Epoch 273/1000 
	 loss: 19.1407, MinusLogProbMetric: 19.1407, val_loss: 19.3264, val_MinusLogProbMetric: 19.3264

Epoch 273: val_loss did not improve from 18.95822
196/196 - 62s - loss: 19.1407 - MinusLogProbMetric: 19.1407 - val_loss: 19.3264 - val_MinusLogProbMetric: 19.3264 - lr: 1.1111e-04 - 62s/epoch - 315ms/step
Epoch 274/1000
2023-10-23 18:30:17.582 
Epoch 274/1000 
	 loss: 19.0794, MinusLogProbMetric: 19.0794, val_loss: 19.1704, val_MinusLogProbMetric: 19.1704

Epoch 274: val_loss did not improve from 18.95822
196/196 - 65s - loss: 19.0794 - MinusLogProbMetric: 19.0794 - val_loss: 19.1704 - val_MinusLogProbMetric: 19.1704 - lr: 1.1111e-04 - 65s/epoch - 330ms/step
Epoch 275/1000
2023-10-23 18:31:20.841 
Epoch 275/1000 
	 loss: 19.1618, MinusLogProbMetric: 19.1618, val_loss: 18.9597, val_MinusLogProbMetric: 18.9597

Epoch 275: val_loss did not improve from 18.95822
196/196 - 63s - loss: 19.1618 - MinusLogProbMetric: 19.1618 - val_loss: 18.9597 - val_MinusLogProbMetric: 18.9597 - lr: 1.1111e-04 - 63s/epoch - 323ms/step
Epoch 276/1000
2023-10-23 18:32:24.828 
Epoch 276/1000 
	 loss: 19.0342, MinusLogProbMetric: 19.0342, val_loss: 19.4841, val_MinusLogProbMetric: 19.4841

Epoch 276: val_loss did not improve from 18.95822
196/196 - 64s - loss: 19.0342 - MinusLogProbMetric: 19.0342 - val_loss: 19.4841 - val_MinusLogProbMetric: 19.4841 - lr: 1.1111e-04 - 64s/epoch - 326ms/step
Epoch 277/1000
2023-10-23 18:33:30.730 
Epoch 277/1000 
	 loss: 19.1254, MinusLogProbMetric: 19.1254, val_loss: 19.3981, val_MinusLogProbMetric: 19.3981

Epoch 277: val_loss did not improve from 18.95822
196/196 - 66s - loss: 19.1254 - MinusLogProbMetric: 19.1254 - val_loss: 19.3981 - val_MinusLogProbMetric: 19.3981 - lr: 1.1111e-04 - 66s/epoch - 336ms/step
Epoch 278/1000
2023-10-23 18:34:33.830 
Epoch 278/1000 
	 loss: 19.1089, MinusLogProbMetric: 19.1089, val_loss: 19.0303, val_MinusLogProbMetric: 19.0303

Epoch 278: val_loss did not improve from 18.95822
196/196 - 63s - loss: 19.1089 - MinusLogProbMetric: 19.1089 - val_loss: 19.0303 - val_MinusLogProbMetric: 19.0303 - lr: 1.1111e-04 - 63s/epoch - 322ms/step
Epoch 279/1000
2023-10-23 18:35:39.244 
Epoch 279/1000 
	 loss: 19.0587, MinusLogProbMetric: 19.0587, val_loss: 19.0837, val_MinusLogProbMetric: 19.0837

Epoch 279: val_loss did not improve from 18.95822
196/196 - 65s - loss: 19.0587 - MinusLogProbMetric: 19.0587 - val_loss: 19.0837 - val_MinusLogProbMetric: 19.0837 - lr: 1.1111e-04 - 65s/epoch - 334ms/step
Epoch 280/1000
2023-10-23 18:36:45.341 
Epoch 280/1000 
	 loss: 19.0633, MinusLogProbMetric: 19.0633, val_loss: 18.9205, val_MinusLogProbMetric: 18.9205

Epoch 280: val_loss improved from 18.95822 to 18.92055, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 67s - loss: 19.0633 - MinusLogProbMetric: 19.0633 - val_loss: 18.9205 - val_MinusLogProbMetric: 18.9205 - lr: 1.1111e-04 - 67s/epoch - 343ms/step
Epoch 281/1000
2023-10-23 18:37:50.231 
Epoch 281/1000 
	 loss: 19.0467, MinusLogProbMetric: 19.0467, val_loss: 19.1804, val_MinusLogProbMetric: 19.1804

Epoch 281: val_loss did not improve from 18.92055
196/196 - 64s - loss: 19.0467 - MinusLogProbMetric: 19.0467 - val_loss: 19.1804 - val_MinusLogProbMetric: 19.1804 - lr: 1.1111e-04 - 64s/epoch - 326ms/step
Epoch 282/1000
2023-10-23 18:38:54.704 
Epoch 282/1000 
	 loss: 19.0247, MinusLogProbMetric: 19.0247, val_loss: 18.9820, val_MinusLogProbMetric: 18.9820

Epoch 282: val_loss did not improve from 18.92055
196/196 - 64s - loss: 19.0247 - MinusLogProbMetric: 19.0247 - val_loss: 18.9820 - val_MinusLogProbMetric: 18.9820 - lr: 1.1111e-04 - 64s/epoch - 329ms/step
Epoch 283/1000
2023-10-23 18:39:57.137 
Epoch 283/1000 
	 loss: 19.0146, MinusLogProbMetric: 19.0146, val_loss: 19.1359, val_MinusLogProbMetric: 19.1359

Epoch 283: val_loss did not improve from 18.92055
196/196 - 62s - loss: 19.0146 - MinusLogProbMetric: 19.0146 - val_loss: 19.1359 - val_MinusLogProbMetric: 19.1359 - lr: 1.1111e-04 - 62s/epoch - 319ms/step
Epoch 284/1000
2023-10-23 18:40:59.258 
Epoch 284/1000 
	 loss: 19.0032, MinusLogProbMetric: 19.0032, val_loss: 18.9725, val_MinusLogProbMetric: 18.9725

Epoch 284: val_loss did not improve from 18.92055
196/196 - 62s - loss: 19.0032 - MinusLogProbMetric: 19.0032 - val_loss: 18.9725 - val_MinusLogProbMetric: 18.9725 - lr: 1.1111e-04 - 62s/epoch - 317ms/step
Epoch 285/1000
2023-10-23 18:42:04.643 
Epoch 285/1000 
	 loss: 19.0083, MinusLogProbMetric: 19.0083, val_loss: 18.7475, val_MinusLogProbMetric: 18.7475

Epoch 285: val_loss improved from 18.92055 to 18.74750, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 66s - loss: 19.0083 - MinusLogProbMetric: 19.0083 - val_loss: 18.7475 - val_MinusLogProbMetric: 18.7475 - lr: 1.1111e-04 - 66s/epoch - 339ms/step
Epoch 286/1000
2023-10-23 18:43:08.907 
Epoch 286/1000 
	 loss: 18.9824, MinusLogProbMetric: 18.9824, val_loss: 18.9381, val_MinusLogProbMetric: 18.9381

Epoch 286: val_loss did not improve from 18.74750
196/196 - 63s - loss: 18.9824 - MinusLogProbMetric: 18.9824 - val_loss: 18.9381 - val_MinusLogProbMetric: 18.9381 - lr: 1.1111e-04 - 63s/epoch - 323ms/step
Epoch 287/1000
2023-10-23 18:44:13.250 
Epoch 287/1000 
	 loss: 18.9453, MinusLogProbMetric: 18.9453, val_loss: 19.1178, val_MinusLogProbMetric: 19.1178

Epoch 287: val_loss did not improve from 18.74750
196/196 - 64s - loss: 18.9453 - MinusLogProbMetric: 18.9453 - val_loss: 19.1178 - val_MinusLogProbMetric: 19.1178 - lr: 1.1111e-04 - 64s/epoch - 328ms/step
Epoch 288/1000
2023-10-23 18:45:17.108 
Epoch 288/1000 
	 loss: 18.9269, MinusLogProbMetric: 18.9269, val_loss: 18.9219, val_MinusLogProbMetric: 18.9219

Epoch 288: val_loss did not improve from 18.74750
196/196 - 64s - loss: 18.9269 - MinusLogProbMetric: 18.9269 - val_loss: 18.9219 - val_MinusLogProbMetric: 18.9219 - lr: 1.1111e-04 - 64s/epoch - 326ms/step
Epoch 289/1000
2023-10-23 18:46:21.600 
Epoch 289/1000 
	 loss: 18.9326, MinusLogProbMetric: 18.9326, val_loss: 19.1026, val_MinusLogProbMetric: 19.1026

Epoch 289: val_loss did not improve from 18.74750
196/196 - 64s - loss: 18.9326 - MinusLogProbMetric: 18.9326 - val_loss: 19.1026 - val_MinusLogProbMetric: 19.1026 - lr: 1.1111e-04 - 64s/epoch - 329ms/step
Epoch 290/1000
2023-10-23 18:47:26.877 
Epoch 290/1000 
	 loss: 18.9387, MinusLogProbMetric: 18.9387, val_loss: 18.9106, val_MinusLogProbMetric: 18.9106

Epoch 290: val_loss did not improve from 18.74750
196/196 - 65s - loss: 18.9387 - MinusLogProbMetric: 18.9387 - val_loss: 18.9106 - val_MinusLogProbMetric: 18.9106 - lr: 1.1111e-04 - 65s/epoch - 333ms/step
Epoch 291/1000
2023-10-23 18:48:33.250 
Epoch 291/1000 
	 loss: 18.9194, MinusLogProbMetric: 18.9194, val_loss: 19.0901, val_MinusLogProbMetric: 19.0901

Epoch 291: val_loss did not improve from 18.74750
196/196 - 66s - loss: 18.9194 - MinusLogProbMetric: 18.9194 - val_loss: 19.0901 - val_MinusLogProbMetric: 19.0901 - lr: 1.1111e-04 - 66s/epoch - 339ms/step
Epoch 292/1000
2023-10-23 18:49:38.764 
Epoch 292/1000 
	 loss: 18.9375, MinusLogProbMetric: 18.9375, val_loss: 18.8669, val_MinusLogProbMetric: 18.8669

Epoch 292: val_loss did not improve from 18.74750
196/196 - 66s - loss: 18.9375 - MinusLogProbMetric: 18.9375 - val_loss: 18.8669 - val_MinusLogProbMetric: 18.8669 - lr: 1.1111e-04 - 66s/epoch - 334ms/step
Epoch 293/1000
2023-10-23 18:50:48.345 
Epoch 293/1000 
	 loss: 18.9885, MinusLogProbMetric: 18.9885, val_loss: 19.2359, val_MinusLogProbMetric: 19.2359

Epoch 293: val_loss did not improve from 18.74750
196/196 - 70s - loss: 18.9885 - MinusLogProbMetric: 18.9885 - val_loss: 19.2359 - val_MinusLogProbMetric: 19.2359 - lr: 1.1111e-04 - 70s/epoch - 355ms/step
Epoch 294/1000
2023-10-23 18:51:57.999 
Epoch 294/1000 
	 loss: 18.9026, MinusLogProbMetric: 18.9026, val_loss: 18.8936, val_MinusLogProbMetric: 18.8936

Epoch 294: val_loss did not improve from 18.74750
196/196 - 70s - loss: 18.9026 - MinusLogProbMetric: 18.9026 - val_loss: 18.8936 - val_MinusLogProbMetric: 18.8936 - lr: 1.1111e-04 - 70s/epoch - 355ms/step
Epoch 295/1000
2023-10-23 18:53:05.054 
Epoch 295/1000 
	 loss: 18.8994, MinusLogProbMetric: 18.8994, val_loss: 18.7797, val_MinusLogProbMetric: 18.7797

Epoch 295: val_loss did not improve from 18.74750
196/196 - 67s - loss: 18.8994 - MinusLogProbMetric: 18.8994 - val_loss: 18.7797 - val_MinusLogProbMetric: 18.7797 - lr: 1.1111e-04 - 67s/epoch - 342ms/step
Epoch 296/1000
2023-10-23 18:54:10.713 
Epoch 296/1000 
	 loss: 18.9363, MinusLogProbMetric: 18.9363, val_loss: 18.9327, val_MinusLogProbMetric: 18.9327

Epoch 296: val_loss did not improve from 18.74750
196/196 - 66s - loss: 18.9363 - MinusLogProbMetric: 18.9363 - val_loss: 18.9327 - val_MinusLogProbMetric: 18.9327 - lr: 1.1111e-04 - 66s/epoch - 335ms/step
Epoch 297/1000
2023-10-23 18:55:16.707 
Epoch 297/1000 
	 loss: 18.8943, MinusLogProbMetric: 18.8943, val_loss: 19.0362, val_MinusLogProbMetric: 19.0362

Epoch 297: val_loss did not improve from 18.74750
196/196 - 66s - loss: 18.8943 - MinusLogProbMetric: 18.8943 - val_loss: 19.0362 - val_MinusLogProbMetric: 19.0362 - lr: 1.1111e-04 - 66s/epoch - 337ms/step
Epoch 298/1000
2023-10-23 18:56:20.965 
Epoch 298/1000 
	 loss: 18.8850, MinusLogProbMetric: 18.8850, val_loss: 18.8746, val_MinusLogProbMetric: 18.8746

Epoch 298: val_loss did not improve from 18.74750
196/196 - 64s - loss: 18.8850 - MinusLogProbMetric: 18.8850 - val_loss: 18.8746 - val_MinusLogProbMetric: 18.8746 - lr: 1.1111e-04 - 64s/epoch - 328ms/step
Epoch 299/1000
2023-10-23 18:57:24.875 
Epoch 299/1000 
	 loss: 18.8725, MinusLogProbMetric: 18.8725, val_loss: 19.3414, val_MinusLogProbMetric: 19.3414

Epoch 299: val_loss did not improve from 18.74750
196/196 - 64s - loss: 18.8725 - MinusLogProbMetric: 18.8725 - val_loss: 19.3414 - val_MinusLogProbMetric: 19.3414 - lr: 1.1111e-04 - 64s/epoch - 326ms/step
Epoch 300/1000
2023-10-23 18:58:29.181 
Epoch 300/1000 
	 loss: 18.8604, MinusLogProbMetric: 18.8604, val_loss: 19.0061, val_MinusLogProbMetric: 19.0061

Epoch 300: val_loss did not improve from 18.74750
196/196 - 64s - loss: 18.8604 - MinusLogProbMetric: 18.8604 - val_loss: 19.0061 - val_MinusLogProbMetric: 19.0061 - lr: 1.1111e-04 - 64s/epoch - 328ms/step
Epoch 301/1000
2023-10-23 18:59:33.338 
Epoch 301/1000 
	 loss: 18.8900, MinusLogProbMetric: 18.8900, val_loss: 18.9014, val_MinusLogProbMetric: 18.9014

Epoch 301: val_loss did not improve from 18.74750
196/196 - 64s - loss: 18.8900 - MinusLogProbMetric: 18.8900 - val_loss: 18.9014 - val_MinusLogProbMetric: 18.9014 - lr: 1.1111e-04 - 64s/epoch - 327ms/step
Epoch 302/1000
2023-10-23 19:00:36.110 
Epoch 302/1000 
	 loss: 18.8911, MinusLogProbMetric: 18.8911, val_loss: 19.0094, val_MinusLogProbMetric: 19.0094

Epoch 302: val_loss did not improve from 18.74750
196/196 - 63s - loss: 18.8911 - MinusLogProbMetric: 18.8911 - val_loss: 19.0094 - val_MinusLogProbMetric: 19.0094 - lr: 1.1111e-04 - 63s/epoch - 320ms/step
Epoch 303/1000
2023-10-23 19:01:38.799 
Epoch 303/1000 
	 loss: 18.8015, MinusLogProbMetric: 18.8015, val_loss: 18.9832, val_MinusLogProbMetric: 18.9832

Epoch 303: val_loss did not improve from 18.74750
196/196 - 63s - loss: 18.8015 - MinusLogProbMetric: 18.8015 - val_loss: 18.9832 - val_MinusLogProbMetric: 18.9832 - lr: 1.1111e-04 - 63s/epoch - 320ms/step
Epoch 304/1000
2023-10-23 19:02:42.714 
Epoch 304/1000 
	 loss: 18.8993, MinusLogProbMetric: 18.8993, val_loss: 19.1325, val_MinusLogProbMetric: 19.1325

Epoch 304: val_loss did not improve from 18.74750
196/196 - 64s - loss: 18.8993 - MinusLogProbMetric: 18.8993 - val_loss: 19.1325 - val_MinusLogProbMetric: 19.1325 - lr: 1.1111e-04 - 64s/epoch - 326ms/step
Epoch 305/1000
2023-10-23 19:03:47.476 
Epoch 305/1000 
	 loss: 18.8642, MinusLogProbMetric: 18.8642, val_loss: 18.9418, val_MinusLogProbMetric: 18.9418

Epoch 305: val_loss did not improve from 18.74750
196/196 - 65s - loss: 18.8642 - MinusLogProbMetric: 18.8642 - val_loss: 18.9418 - val_MinusLogProbMetric: 18.9418 - lr: 1.1111e-04 - 65s/epoch - 330ms/step
Epoch 306/1000
2023-10-23 19:04:52.063 
Epoch 306/1000 
	 loss: 18.8303, MinusLogProbMetric: 18.8303, val_loss: 18.8538, val_MinusLogProbMetric: 18.8538

Epoch 306: val_loss did not improve from 18.74750
196/196 - 65s - loss: 18.8303 - MinusLogProbMetric: 18.8303 - val_loss: 18.8538 - val_MinusLogProbMetric: 18.8538 - lr: 1.1111e-04 - 65s/epoch - 330ms/step
Epoch 307/1000
2023-10-23 19:05:56.245 
Epoch 307/1000 
	 loss: 18.8254, MinusLogProbMetric: 18.8254, val_loss: 18.9484, val_MinusLogProbMetric: 18.9484

Epoch 307: val_loss did not improve from 18.74750
196/196 - 64s - loss: 18.8254 - MinusLogProbMetric: 18.8254 - val_loss: 18.9484 - val_MinusLogProbMetric: 18.9484 - lr: 1.1111e-04 - 64s/epoch - 327ms/step
Epoch 308/1000
2023-10-23 19:06:59.805 
Epoch 308/1000 
	 loss: 18.7709, MinusLogProbMetric: 18.7709, val_loss: 18.9211, val_MinusLogProbMetric: 18.9211

Epoch 308: val_loss did not improve from 18.74750
196/196 - 64s - loss: 18.7709 - MinusLogProbMetric: 18.7709 - val_loss: 18.9211 - val_MinusLogProbMetric: 18.9211 - lr: 1.1111e-04 - 64s/epoch - 324ms/step
Epoch 309/1000
2023-10-23 19:08:03.067 
Epoch 309/1000 
	 loss: 18.8417, MinusLogProbMetric: 18.8417, val_loss: 19.2557, val_MinusLogProbMetric: 19.2557

Epoch 309: val_loss did not improve from 18.74750
196/196 - 63s - loss: 18.8417 - MinusLogProbMetric: 18.8417 - val_loss: 19.2557 - val_MinusLogProbMetric: 19.2557 - lr: 1.1111e-04 - 63s/epoch - 323ms/step
Epoch 310/1000
2023-10-23 19:09:06.448 
Epoch 310/1000 
	 loss: 18.8500, MinusLogProbMetric: 18.8500, val_loss: 18.8200, val_MinusLogProbMetric: 18.8200

Epoch 310: val_loss did not improve from 18.74750
196/196 - 63s - loss: 18.8500 - MinusLogProbMetric: 18.8500 - val_loss: 18.8200 - val_MinusLogProbMetric: 18.8200 - lr: 1.1111e-04 - 63s/epoch - 323ms/step
Epoch 311/1000
2023-10-23 19:10:11.154 
Epoch 311/1000 
	 loss: 18.8340, MinusLogProbMetric: 18.8340, val_loss: 19.2510, val_MinusLogProbMetric: 19.2510

Epoch 311: val_loss did not improve from 18.74750
196/196 - 65s - loss: 18.8340 - MinusLogProbMetric: 18.8340 - val_loss: 19.2510 - val_MinusLogProbMetric: 19.2510 - lr: 1.1111e-04 - 65s/epoch - 330ms/step
Epoch 312/1000
2023-10-23 19:11:15.975 
Epoch 312/1000 
	 loss: 18.8189, MinusLogProbMetric: 18.8189, val_loss: 18.8588, val_MinusLogProbMetric: 18.8588

Epoch 312: val_loss did not improve from 18.74750
196/196 - 65s - loss: 18.8189 - MinusLogProbMetric: 18.8189 - val_loss: 18.8588 - val_MinusLogProbMetric: 18.8588 - lr: 1.1111e-04 - 65s/epoch - 331ms/step
Epoch 313/1000
2023-10-23 19:12:19.768 
Epoch 313/1000 
	 loss: 18.8103, MinusLogProbMetric: 18.8103, val_loss: 19.2309, val_MinusLogProbMetric: 19.2309

Epoch 313: val_loss did not improve from 18.74750
196/196 - 64s - loss: 18.8103 - MinusLogProbMetric: 18.8103 - val_loss: 19.2309 - val_MinusLogProbMetric: 19.2309 - lr: 1.1111e-04 - 64s/epoch - 325ms/step
Epoch 314/1000
2023-10-23 19:13:23.938 
Epoch 314/1000 
	 loss: 18.7535, MinusLogProbMetric: 18.7535, val_loss: 18.6809, val_MinusLogProbMetric: 18.6809

Epoch 314: val_loss improved from 18.74750 to 18.68091, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 65s - loss: 18.7535 - MinusLogProbMetric: 18.7535 - val_loss: 18.6809 - val_MinusLogProbMetric: 18.6809 - lr: 1.1111e-04 - 65s/epoch - 333ms/step
Epoch 315/1000
2023-10-23 19:14:29.805 
Epoch 315/1000 
	 loss: 18.7737, MinusLogProbMetric: 18.7737, val_loss: 18.9941, val_MinusLogProbMetric: 18.9941

Epoch 315: val_loss did not improve from 18.68091
196/196 - 65s - loss: 18.7737 - MinusLogProbMetric: 18.7737 - val_loss: 18.9941 - val_MinusLogProbMetric: 18.9941 - lr: 1.1111e-04 - 65s/epoch - 331ms/step
Epoch 316/1000
2023-10-23 19:15:33.378 
Epoch 316/1000 
	 loss: 18.8028, MinusLogProbMetric: 18.8028, val_loss: 19.0083, val_MinusLogProbMetric: 19.0083

Epoch 316: val_loss did not improve from 18.68091
196/196 - 64s - loss: 18.8028 - MinusLogProbMetric: 18.8028 - val_loss: 19.0083 - val_MinusLogProbMetric: 19.0083 - lr: 1.1111e-04 - 64s/epoch - 324ms/step
Epoch 317/1000
2023-10-23 19:16:36.174 
Epoch 317/1000 
	 loss: 18.7885, MinusLogProbMetric: 18.7885, val_loss: 18.8808, val_MinusLogProbMetric: 18.8808

Epoch 317: val_loss did not improve from 18.68091
196/196 - 63s - loss: 18.7885 - MinusLogProbMetric: 18.7885 - val_loss: 18.8808 - val_MinusLogProbMetric: 18.8808 - lr: 1.1111e-04 - 63s/epoch - 320ms/step
Epoch 318/1000
2023-10-23 19:17:40.892 
Epoch 318/1000 
	 loss: 18.8259, MinusLogProbMetric: 18.8259, val_loss: 18.7206, val_MinusLogProbMetric: 18.7206

Epoch 318: val_loss did not improve from 18.68091
196/196 - 65s - loss: 18.8259 - MinusLogProbMetric: 18.8259 - val_loss: 18.7206 - val_MinusLogProbMetric: 18.7206 - lr: 1.1111e-04 - 65s/epoch - 330ms/step
Epoch 319/1000
2023-10-23 19:18:45.858 
Epoch 319/1000 
	 loss: 18.7224, MinusLogProbMetric: 18.7224, val_loss: 18.6080, val_MinusLogProbMetric: 18.6080

Epoch 319: val_loss improved from 18.68091 to 18.60802, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 66s - loss: 18.7224 - MinusLogProbMetric: 18.7224 - val_loss: 18.6080 - val_MinusLogProbMetric: 18.6080 - lr: 1.1111e-04 - 66s/epoch - 337ms/step
Epoch 320/1000
2023-10-23 19:19:51.167 
Epoch 320/1000 
	 loss: 18.7433, MinusLogProbMetric: 18.7433, val_loss: 18.8060, val_MinusLogProbMetric: 18.8060

Epoch 320: val_loss did not improve from 18.60802
196/196 - 64s - loss: 18.7433 - MinusLogProbMetric: 18.7433 - val_loss: 18.8060 - val_MinusLogProbMetric: 18.8060 - lr: 1.1111e-04 - 64s/epoch - 328ms/step
Epoch 321/1000
2023-10-23 19:20:57.159 
Epoch 321/1000 
	 loss: 18.7274, MinusLogProbMetric: 18.7274, val_loss: 18.6537, val_MinusLogProbMetric: 18.6537

Epoch 321: val_loss did not improve from 18.60802
196/196 - 66s - loss: 18.7274 - MinusLogProbMetric: 18.7274 - val_loss: 18.6537 - val_MinusLogProbMetric: 18.6537 - lr: 1.1111e-04 - 66s/epoch - 337ms/step
Epoch 322/1000
2023-10-23 19:22:00.941 
Epoch 322/1000 
	 loss: 18.7437, MinusLogProbMetric: 18.7437, val_loss: 18.6873, val_MinusLogProbMetric: 18.6873

Epoch 322: val_loss did not improve from 18.60802
196/196 - 64s - loss: 18.7437 - MinusLogProbMetric: 18.7437 - val_loss: 18.6873 - val_MinusLogProbMetric: 18.6873 - lr: 1.1111e-04 - 64s/epoch - 325ms/step
Epoch 323/1000
2023-10-23 19:23:05.695 
Epoch 323/1000 
	 loss: 18.6890, MinusLogProbMetric: 18.6890, val_loss: 19.1724, val_MinusLogProbMetric: 19.1724

Epoch 323: val_loss did not improve from 18.60802
196/196 - 65s - loss: 18.6890 - MinusLogProbMetric: 18.6890 - val_loss: 19.1724 - val_MinusLogProbMetric: 19.1724 - lr: 1.1111e-04 - 65s/epoch - 330ms/step
Epoch 324/1000
2023-10-23 19:24:10.788 
Epoch 324/1000 
	 loss: 18.7335, MinusLogProbMetric: 18.7335, val_loss: 18.6736, val_MinusLogProbMetric: 18.6736

Epoch 324: val_loss did not improve from 18.60802
196/196 - 65s - loss: 18.7335 - MinusLogProbMetric: 18.7335 - val_loss: 18.6736 - val_MinusLogProbMetric: 18.6736 - lr: 1.1111e-04 - 65s/epoch - 332ms/step
Epoch 325/1000
2023-10-23 19:25:15.274 
Epoch 325/1000 
	 loss: 18.7380, MinusLogProbMetric: 18.7380, val_loss: 19.0630, val_MinusLogProbMetric: 19.0630

Epoch 325: val_loss did not improve from 18.60802
196/196 - 64s - loss: 18.7380 - MinusLogProbMetric: 18.7380 - val_loss: 19.0630 - val_MinusLogProbMetric: 19.0630 - lr: 1.1111e-04 - 64s/epoch - 329ms/step
Epoch 326/1000
2023-10-23 19:26:18.685 
Epoch 326/1000 
	 loss: 18.6605, MinusLogProbMetric: 18.6605, val_loss: 18.7847, val_MinusLogProbMetric: 18.7847

Epoch 326: val_loss did not improve from 18.60802
196/196 - 63s - loss: 18.6605 - MinusLogProbMetric: 18.6605 - val_loss: 18.7847 - val_MinusLogProbMetric: 18.7847 - lr: 1.1111e-04 - 63s/epoch - 324ms/step
Epoch 327/1000
2023-10-23 19:27:22.653 
Epoch 327/1000 
	 loss: 18.6819, MinusLogProbMetric: 18.6819, val_loss: 18.8646, val_MinusLogProbMetric: 18.8646

Epoch 327: val_loss did not improve from 18.60802
196/196 - 64s - loss: 18.6819 - MinusLogProbMetric: 18.6819 - val_loss: 18.8646 - val_MinusLogProbMetric: 18.8646 - lr: 1.1111e-04 - 64s/epoch - 326ms/step
Epoch 328/1000
2023-10-23 19:28:27.819 
Epoch 328/1000 
	 loss: 18.6973, MinusLogProbMetric: 18.6973, val_loss: 19.5407, val_MinusLogProbMetric: 19.5407

Epoch 328: val_loss did not improve from 18.60802
196/196 - 65s - loss: 18.6973 - MinusLogProbMetric: 18.6973 - val_loss: 19.5407 - val_MinusLogProbMetric: 19.5407 - lr: 1.1111e-04 - 65s/epoch - 332ms/step
Epoch 329/1000
2023-10-23 19:29:31.550 
Epoch 329/1000 
	 loss: 18.7033, MinusLogProbMetric: 18.7033, val_loss: 18.6094, val_MinusLogProbMetric: 18.6094

Epoch 329: val_loss did not improve from 18.60802
196/196 - 64s - loss: 18.7033 - MinusLogProbMetric: 18.7033 - val_loss: 18.6094 - val_MinusLogProbMetric: 18.6094 - lr: 1.1111e-04 - 64s/epoch - 325ms/step
Epoch 330/1000
2023-10-23 19:30:35.566 
Epoch 330/1000 
	 loss: 18.6582, MinusLogProbMetric: 18.6582, val_loss: 18.8511, val_MinusLogProbMetric: 18.8511

Epoch 330: val_loss did not improve from 18.60802
196/196 - 64s - loss: 18.6582 - MinusLogProbMetric: 18.6582 - val_loss: 18.8511 - val_MinusLogProbMetric: 18.8511 - lr: 1.1111e-04 - 64s/epoch - 327ms/step
Epoch 331/1000
2023-10-23 19:31:39.448 
Epoch 331/1000 
	 loss: 18.6728, MinusLogProbMetric: 18.6728, val_loss: 18.8947, val_MinusLogProbMetric: 18.8947

Epoch 331: val_loss did not improve from 18.60802
196/196 - 64s - loss: 18.6728 - MinusLogProbMetric: 18.6728 - val_loss: 18.8947 - val_MinusLogProbMetric: 18.8947 - lr: 1.1111e-04 - 64s/epoch - 326ms/step
Epoch 332/1000
2023-10-23 19:32:43.298 
Epoch 332/1000 
	 loss: 18.6926, MinusLogProbMetric: 18.6926, val_loss: 18.8415, val_MinusLogProbMetric: 18.8415

Epoch 332: val_loss did not improve from 18.60802
196/196 - 64s - loss: 18.6926 - MinusLogProbMetric: 18.6926 - val_loss: 18.8415 - val_MinusLogProbMetric: 18.8415 - lr: 1.1111e-04 - 64s/epoch - 326ms/step
Epoch 333/1000
2023-10-23 19:33:47.767 
Epoch 333/1000 
	 loss: 18.6553, MinusLogProbMetric: 18.6553, val_loss: 18.7237, val_MinusLogProbMetric: 18.7237

Epoch 333: val_loss did not improve from 18.60802
196/196 - 64s - loss: 18.6553 - MinusLogProbMetric: 18.6553 - val_loss: 18.7237 - val_MinusLogProbMetric: 18.7237 - lr: 1.1111e-04 - 64s/epoch - 329ms/step
Epoch 334/1000
2023-10-23 19:34:51.778 
Epoch 334/1000 
	 loss: 18.6627, MinusLogProbMetric: 18.6627, val_loss: 18.8974, val_MinusLogProbMetric: 18.8974

Epoch 334: val_loss did not improve from 18.60802
196/196 - 64s - loss: 18.6627 - MinusLogProbMetric: 18.6627 - val_loss: 18.8974 - val_MinusLogProbMetric: 18.8974 - lr: 1.1111e-04 - 64s/epoch - 327ms/step
Epoch 335/1000
2023-10-23 19:35:55.574 
Epoch 335/1000 
	 loss: 18.6497, MinusLogProbMetric: 18.6497, val_loss: 18.5975, val_MinusLogProbMetric: 18.5975

Epoch 335: val_loss improved from 18.60802 to 18.59752, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 65s - loss: 18.6497 - MinusLogProbMetric: 18.6497 - val_loss: 18.5975 - val_MinusLogProbMetric: 18.5975 - lr: 1.1111e-04 - 65s/epoch - 331ms/step
Epoch 336/1000
2023-10-23 19:36:59.832 
Epoch 336/1000 
	 loss: 18.6594, MinusLogProbMetric: 18.6594, val_loss: 18.7445, val_MinusLogProbMetric: 18.7445

Epoch 336: val_loss did not improve from 18.59752
196/196 - 63s - loss: 18.6594 - MinusLogProbMetric: 18.6594 - val_loss: 18.7445 - val_MinusLogProbMetric: 18.7445 - lr: 1.1111e-04 - 63s/epoch - 322ms/step
Epoch 337/1000
2023-10-23 19:38:03.730 
Epoch 337/1000 
	 loss: 18.6385, MinusLogProbMetric: 18.6385, val_loss: 18.9207, val_MinusLogProbMetric: 18.9207

Epoch 337: val_loss did not improve from 18.59752
196/196 - 64s - loss: 18.6385 - MinusLogProbMetric: 18.6385 - val_loss: 18.9207 - val_MinusLogProbMetric: 18.9207 - lr: 1.1111e-04 - 64s/epoch - 326ms/step
Epoch 338/1000
2023-10-23 19:39:08.438 
Epoch 338/1000 
	 loss: 18.6842, MinusLogProbMetric: 18.6842, val_loss: 18.6416, val_MinusLogProbMetric: 18.6416

Epoch 338: val_loss did not improve from 18.59752
196/196 - 65s - loss: 18.6842 - MinusLogProbMetric: 18.6842 - val_loss: 18.6416 - val_MinusLogProbMetric: 18.6416 - lr: 1.1111e-04 - 65s/epoch - 330ms/step
Epoch 339/1000
2023-10-23 19:40:12.323 
Epoch 339/1000 
	 loss: 18.6366, MinusLogProbMetric: 18.6366, val_loss: 18.7493, val_MinusLogProbMetric: 18.7493

Epoch 339: val_loss did not improve from 18.59752
196/196 - 64s - loss: 18.6366 - MinusLogProbMetric: 18.6366 - val_loss: 18.7493 - val_MinusLogProbMetric: 18.7493 - lr: 1.1111e-04 - 64s/epoch - 326ms/step
Epoch 340/1000
2023-10-23 19:41:16.524 
Epoch 340/1000 
	 loss: 18.6149, MinusLogProbMetric: 18.6149, val_loss: 18.7339, val_MinusLogProbMetric: 18.7339

Epoch 340: val_loss did not improve from 18.59752
196/196 - 64s - loss: 18.6149 - MinusLogProbMetric: 18.6149 - val_loss: 18.7339 - val_MinusLogProbMetric: 18.7339 - lr: 1.1111e-04 - 64s/epoch - 328ms/step
Epoch 341/1000
2023-10-23 19:42:21.675 
Epoch 341/1000 
	 loss: 18.6375, MinusLogProbMetric: 18.6375, val_loss: 18.8419, val_MinusLogProbMetric: 18.8419

Epoch 341: val_loss did not improve from 18.59752
196/196 - 65s - loss: 18.6375 - MinusLogProbMetric: 18.6375 - val_loss: 18.8419 - val_MinusLogProbMetric: 18.8419 - lr: 1.1111e-04 - 65s/epoch - 332ms/step
Epoch 342/1000
2023-10-23 19:43:28.122 
Epoch 342/1000 
	 loss: 18.6408, MinusLogProbMetric: 18.6408, val_loss: 18.6710, val_MinusLogProbMetric: 18.6710

Epoch 342: val_loss did not improve from 18.59752
196/196 - 66s - loss: 18.6408 - MinusLogProbMetric: 18.6408 - val_loss: 18.6710 - val_MinusLogProbMetric: 18.6710 - lr: 1.1111e-04 - 66s/epoch - 339ms/step
Epoch 343/1000
2023-10-23 19:44:32.332 
Epoch 343/1000 
	 loss: 18.6902, MinusLogProbMetric: 18.6902, val_loss: 18.9711, val_MinusLogProbMetric: 18.9711

Epoch 343: val_loss did not improve from 18.59752
196/196 - 64s - loss: 18.6902 - MinusLogProbMetric: 18.6902 - val_loss: 18.9711 - val_MinusLogProbMetric: 18.9711 - lr: 1.1111e-04 - 64s/epoch - 328ms/step
Epoch 344/1000
2023-10-23 19:45:35.478 
Epoch 344/1000 
	 loss: 18.6232, MinusLogProbMetric: 18.6232, val_loss: 18.5763, val_MinusLogProbMetric: 18.5763

Epoch 344: val_loss improved from 18.59752 to 18.57630, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 64s - loss: 18.6232 - MinusLogProbMetric: 18.6232 - val_loss: 18.5763 - val_MinusLogProbMetric: 18.5763 - lr: 1.1111e-04 - 64s/epoch - 327ms/step
Epoch 345/1000
2023-10-23 19:46:40.926 
Epoch 345/1000 
	 loss: 18.6064, MinusLogProbMetric: 18.6064, val_loss: 18.9423, val_MinusLogProbMetric: 18.9423

Epoch 345: val_loss did not improve from 18.57630
196/196 - 64s - loss: 18.6064 - MinusLogProbMetric: 18.6064 - val_loss: 18.9423 - val_MinusLogProbMetric: 18.9423 - lr: 1.1111e-04 - 64s/epoch - 329ms/step
Epoch 346/1000
2023-10-23 19:47:45.546 
Epoch 346/1000 
	 loss: 18.6176, MinusLogProbMetric: 18.6176, val_loss: 18.8345, val_MinusLogProbMetric: 18.8345

Epoch 346: val_loss did not improve from 18.57630
196/196 - 65s - loss: 18.6176 - MinusLogProbMetric: 18.6176 - val_loss: 18.8345 - val_MinusLogProbMetric: 18.8345 - lr: 1.1111e-04 - 65s/epoch - 330ms/step
Epoch 347/1000
2023-10-23 19:48:49.634 
Epoch 347/1000 
	 loss: 18.5888, MinusLogProbMetric: 18.5888, val_loss: 18.8649, val_MinusLogProbMetric: 18.8649

Epoch 347: val_loss did not improve from 18.57630
196/196 - 64s - loss: 18.5888 - MinusLogProbMetric: 18.5888 - val_loss: 18.8649 - val_MinusLogProbMetric: 18.8649 - lr: 1.1111e-04 - 64s/epoch - 327ms/step
Epoch 348/1000
2023-10-23 19:49:53.222 
Epoch 348/1000 
	 loss: 18.6402, MinusLogProbMetric: 18.6402, val_loss: 18.5002, val_MinusLogProbMetric: 18.5002

Epoch 348: val_loss improved from 18.57630 to 18.50018, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 65s - loss: 18.6402 - MinusLogProbMetric: 18.6402 - val_loss: 18.5002 - val_MinusLogProbMetric: 18.5002 - lr: 1.1111e-04 - 65s/epoch - 330ms/step
Epoch 349/1000
2023-10-23 19:50:59.752 
Epoch 349/1000 
	 loss: 18.5364, MinusLogProbMetric: 18.5364, val_loss: 18.4719, val_MinusLogProbMetric: 18.4719

Epoch 349: val_loss improved from 18.50018 to 18.47193, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 67s - loss: 18.5364 - MinusLogProbMetric: 18.5364 - val_loss: 18.4719 - val_MinusLogProbMetric: 18.4719 - lr: 1.1111e-04 - 67s/epoch - 339ms/step
Epoch 350/1000
2023-10-23 19:52:03.858 
Epoch 350/1000 
	 loss: 18.5839, MinusLogProbMetric: 18.5839, val_loss: 18.6169, val_MinusLogProbMetric: 18.6169

Epoch 350: val_loss did not improve from 18.47193
196/196 - 63s - loss: 18.5839 - MinusLogProbMetric: 18.5839 - val_loss: 18.6169 - val_MinusLogProbMetric: 18.6169 - lr: 1.1111e-04 - 63s/epoch - 322ms/step
Epoch 351/1000
2023-10-23 19:53:07.126 
Epoch 351/1000 
	 loss: 18.6023, MinusLogProbMetric: 18.6023, val_loss: 18.8631, val_MinusLogProbMetric: 18.8631

Epoch 351: val_loss did not improve from 18.47193
196/196 - 63s - loss: 18.6023 - MinusLogProbMetric: 18.6023 - val_loss: 18.8631 - val_MinusLogProbMetric: 18.8631 - lr: 1.1111e-04 - 63s/epoch - 323ms/step
Epoch 352/1000
2023-10-23 19:54:14.090 
Epoch 352/1000 
	 loss: 18.6043, MinusLogProbMetric: 18.6043, val_loss: 18.8499, val_MinusLogProbMetric: 18.8499

Epoch 352: val_loss did not improve from 18.47193
196/196 - 67s - loss: 18.6043 - MinusLogProbMetric: 18.6043 - val_loss: 18.8499 - val_MinusLogProbMetric: 18.8499 - lr: 1.1111e-04 - 67s/epoch - 342ms/step
Epoch 353/1000
2023-10-23 19:55:20.265 
Epoch 353/1000 
	 loss: 18.5540, MinusLogProbMetric: 18.5540, val_loss: 18.6986, val_MinusLogProbMetric: 18.6986

Epoch 353: val_loss did not improve from 18.47193
196/196 - 66s - loss: 18.5540 - MinusLogProbMetric: 18.5540 - val_loss: 18.6986 - val_MinusLogProbMetric: 18.6986 - lr: 1.1111e-04 - 66s/epoch - 338ms/step
Epoch 354/1000
2023-10-23 19:56:24.929 
Epoch 354/1000 
	 loss: 18.5777, MinusLogProbMetric: 18.5777, val_loss: 18.9005, val_MinusLogProbMetric: 18.9005

Epoch 354: val_loss did not improve from 18.47193
196/196 - 65s - loss: 18.5777 - MinusLogProbMetric: 18.5777 - val_loss: 18.9005 - val_MinusLogProbMetric: 18.9005 - lr: 1.1111e-04 - 65s/epoch - 330ms/step
Epoch 355/1000
2023-10-23 19:57:29.474 
Epoch 355/1000 
	 loss: 18.6067, MinusLogProbMetric: 18.6067, val_loss: 18.8612, val_MinusLogProbMetric: 18.8612

Epoch 355: val_loss did not improve from 18.47193
196/196 - 65s - loss: 18.6067 - MinusLogProbMetric: 18.6067 - val_loss: 18.8612 - val_MinusLogProbMetric: 18.8612 - lr: 1.1111e-04 - 65s/epoch - 329ms/step
Epoch 356/1000
2023-10-23 19:58:32.414 
Epoch 356/1000 
	 loss: 18.5404, MinusLogProbMetric: 18.5404, val_loss: 18.8339, val_MinusLogProbMetric: 18.8339

Epoch 356: val_loss did not improve from 18.47193
196/196 - 63s - loss: 18.5404 - MinusLogProbMetric: 18.5404 - val_loss: 18.8339 - val_MinusLogProbMetric: 18.8339 - lr: 1.1111e-04 - 63s/epoch - 321ms/step
Epoch 357/1000
2023-10-23 19:59:35.034 
Epoch 357/1000 
	 loss: 18.5572, MinusLogProbMetric: 18.5572, val_loss: 18.6867, val_MinusLogProbMetric: 18.6867

Epoch 357: val_loss did not improve from 18.47193
196/196 - 63s - loss: 18.5572 - MinusLogProbMetric: 18.5572 - val_loss: 18.6867 - val_MinusLogProbMetric: 18.6867 - lr: 1.1111e-04 - 63s/epoch - 319ms/step
Epoch 358/1000
2023-10-23 20:00:38.920 
Epoch 358/1000 
	 loss: 18.5152, MinusLogProbMetric: 18.5152, val_loss: 18.6541, val_MinusLogProbMetric: 18.6541

Epoch 358: val_loss did not improve from 18.47193
196/196 - 64s - loss: 18.5152 - MinusLogProbMetric: 18.5152 - val_loss: 18.6541 - val_MinusLogProbMetric: 18.6541 - lr: 1.1111e-04 - 64s/epoch - 326ms/step
Epoch 359/1000
2023-10-23 20:01:43.126 
Epoch 359/1000 
	 loss: 18.5087, MinusLogProbMetric: 18.5087, val_loss: 18.8081, val_MinusLogProbMetric: 18.8081

Epoch 359: val_loss did not improve from 18.47193
196/196 - 64s - loss: 18.5087 - MinusLogProbMetric: 18.5087 - val_loss: 18.8081 - val_MinusLogProbMetric: 18.8081 - lr: 1.1111e-04 - 64s/epoch - 328ms/step
Epoch 360/1000
2023-10-23 20:02:47.678 
Epoch 360/1000 
	 loss: 18.5991, MinusLogProbMetric: 18.5991, val_loss: 18.7975, val_MinusLogProbMetric: 18.7975

Epoch 360: val_loss did not improve from 18.47193
196/196 - 65s - loss: 18.5991 - MinusLogProbMetric: 18.5991 - val_loss: 18.7975 - val_MinusLogProbMetric: 18.7975 - lr: 1.1111e-04 - 65s/epoch - 329ms/step
Epoch 361/1000
2023-10-23 20:03:52.139 
Epoch 361/1000 
	 loss: 18.5076, MinusLogProbMetric: 18.5076, val_loss: 18.4308, val_MinusLogProbMetric: 18.4308

Epoch 361: val_loss improved from 18.47193 to 18.43080, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 66s - loss: 18.5076 - MinusLogProbMetric: 18.5076 - val_loss: 18.4308 - val_MinusLogProbMetric: 18.4308 - lr: 1.1111e-04 - 66s/epoch - 335ms/step
Epoch 362/1000
2023-10-23 20:04:57.958 
Epoch 362/1000 
	 loss: 18.4986, MinusLogProbMetric: 18.4986, val_loss: 18.7249, val_MinusLogProbMetric: 18.7249

Epoch 362: val_loss did not improve from 18.43080
196/196 - 65s - loss: 18.4986 - MinusLogProbMetric: 18.4986 - val_loss: 18.7249 - val_MinusLogProbMetric: 18.7249 - lr: 1.1111e-04 - 65s/epoch - 330ms/step
Epoch 363/1000
2023-10-23 20:06:02.743 
Epoch 363/1000 
	 loss: 18.5780, MinusLogProbMetric: 18.5780, val_loss: 18.6258, val_MinusLogProbMetric: 18.6258

Epoch 363: val_loss did not improve from 18.43080
196/196 - 65s - loss: 18.5780 - MinusLogProbMetric: 18.5780 - val_loss: 18.6258 - val_MinusLogProbMetric: 18.6258 - lr: 1.1111e-04 - 65s/epoch - 331ms/step
Epoch 364/1000
2023-10-23 20:07:07.131 
Epoch 364/1000 
	 loss: 18.4727, MinusLogProbMetric: 18.4727, val_loss: 18.6821, val_MinusLogProbMetric: 18.6821

Epoch 364: val_loss did not improve from 18.43080
196/196 - 64s - loss: 18.4727 - MinusLogProbMetric: 18.4727 - val_loss: 18.6821 - val_MinusLogProbMetric: 18.6821 - lr: 1.1111e-04 - 64s/epoch - 328ms/step
Epoch 365/1000
2023-10-23 20:08:12.273 
Epoch 365/1000 
	 loss: 18.5332, MinusLogProbMetric: 18.5332, val_loss: 18.4643, val_MinusLogProbMetric: 18.4643

Epoch 365: val_loss did not improve from 18.43080
196/196 - 65s - loss: 18.5332 - MinusLogProbMetric: 18.5332 - val_loss: 18.4643 - val_MinusLogProbMetric: 18.4643 - lr: 1.1111e-04 - 65s/epoch - 332ms/step
Epoch 366/1000
2023-10-23 20:09:17.301 
Epoch 366/1000 
	 loss: 18.5596, MinusLogProbMetric: 18.5596, val_loss: 18.5647, val_MinusLogProbMetric: 18.5647

Epoch 366: val_loss did not improve from 18.43080
196/196 - 65s - loss: 18.5596 - MinusLogProbMetric: 18.5596 - val_loss: 18.5647 - val_MinusLogProbMetric: 18.5647 - lr: 1.1111e-04 - 65s/epoch - 332ms/step
Epoch 367/1000
2023-10-23 20:10:20.349 
Epoch 367/1000 
	 loss: 18.4626, MinusLogProbMetric: 18.4626, val_loss: 18.6658, val_MinusLogProbMetric: 18.6658

Epoch 367: val_loss did not improve from 18.43080
196/196 - 63s - loss: 18.4626 - MinusLogProbMetric: 18.4626 - val_loss: 18.6658 - val_MinusLogProbMetric: 18.6658 - lr: 1.1111e-04 - 63s/epoch - 322ms/step
Epoch 368/1000
2023-10-23 20:11:23.856 
Epoch 368/1000 
	 loss: 18.4695, MinusLogProbMetric: 18.4695, val_loss: 18.7310, val_MinusLogProbMetric: 18.7310

Epoch 368: val_loss did not improve from 18.43080
196/196 - 64s - loss: 18.4695 - MinusLogProbMetric: 18.4695 - val_loss: 18.7310 - val_MinusLogProbMetric: 18.7310 - lr: 1.1111e-04 - 64s/epoch - 324ms/step
Epoch 369/1000
2023-10-23 20:12:27.701 
Epoch 369/1000 
	 loss: 18.5004, MinusLogProbMetric: 18.5004, val_loss: 18.5796, val_MinusLogProbMetric: 18.5796

Epoch 369: val_loss did not improve from 18.43080
196/196 - 64s - loss: 18.5004 - MinusLogProbMetric: 18.5004 - val_loss: 18.5796 - val_MinusLogProbMetric: 18.5796 - lr: 1.1111e-04 - 64s/epoch - 326ms/step
Epoch 370/1000
2023-10-23 20:13:31.641 
Epoch 370/1000 
	 loss: 18.4484, MinusLogProbMetric: 18.4484, val_loss: 18.4306, val_MinusLogProbMetric: 18.4306

Epoch 370: val_loss improved from 18.43080 to 18.43056, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 65s - loss: 18.4484 - MinusLogProbMetric: 18.4484 - val_loss: 18.4306 - val_MinusLogProbMetric: 18.4306 - lr: 1.1111e-04 - 65s/epoch - 331ms/step
Epoch 371/1000
2023-10-23 20:14:36.667 
Epoch 371/1000 
	 loss: 18.4748, MinusLogProbMetric: 18.4748, val_loss: 18.4597, val_MinusLogProbMetric: 18.4597

Epoch 371: val_loss did not improve from 18.43056
196/196 - 64s - loss: 18.4748 - MinusLogProbMetric: 18.4748 - val_loss: 18.4597 - val_MinusLogProbMetric: 18.4597 - lr: 1.1111e-04 - 64s/epoch - 327ms/step
Epoch 372/1000
2023-10-23 20:15:40.665 
Epoch 372/1000 
	 loss: 18.4802, MinusLogProbMetric: 18.4802, val_loss: 18.4564, val_MinusLogProbMetric: 18.4564

Epoch 372: val_loss did not improve from 18.43056
196/196 - 64s - loss: 18.4802 - MinusLogProbMetric: 18.4802 - val_loss: 18.4564 - val_MinusLogProbMetric: 18.4564 - lr: 1.1111e-04 - 64s/epoch - 327ms/step
Epoch 373/1000
2023-10-23 20:16:45.848 
Epoch 373/1000 
	 loss: 18.4518, MinusLogProbMetric: 18.4518, val_loss: 18.4893, val_MinusLogProbMetric: 18.4893

Epoch 373: val_loss did not improve from 18.43056
196/196 - 65s - loss: 18.4518 - MinusLogProbMetric: 18.4518 - val_loss: 18.4893 - val_MinusLogProbMetric: 18.4893 - lr: 1.1111e-04 - 65s/epoch - 333ms/step
Epoch 374/1000
2023-10-23 20:17:49.593 
Epoch 374/1000 
	 loss: 18.4845, MinusLogProbMetric: 18.4845, val_loss: 18.5186, val_MinusLogProbMetric: 18.5186

Epoch 374: val_loss did not improve from 18.43056
196/196 - 64s - loss: 18.4845 - MinusLogProbMetric: 18.4845 - val_loss: 18.5186 - val_MinusLogProbMetric: 18.5186 - lr: 1.1111e-04 - 64s/epoch - 325ms/step
Epoch 375/1000
2023-10-23 20:18:53.593 
Epoch 375/1000 
	 loss: 18.4831, MinusLogProbMetric: 18.4831, val_loss: 18.4811, val_MinusLogProbMetric: 18.4811

Epoch 375: val_loss did not improve from 18.43056
196/196 - 64s - loss: 18.4831 - MinusLogProbMetric: 18.4831 - val_loss: 18.4811 - val_MinusLogProbMetric: 18.4811 - lr: 1.1111e-04 - 64s/epoch - 327ms/step
Epoch 376/1000
2023-10-23 20:19:57.455 
Epoch 376/1000 
	 loss: 18.4255, MinusLogProbMetric: 18.4255, val_loss: 18.6466, val_MinusLogProbMetric: 18.6466

Epoch 376: val_loss did not improve from 18.43056
196/196 - 64s - loss: 18.4255 - MinusLogProbMetric: 18.4255 - val_loss: 18.6466 - val_MinusLogProbMetric: 18.6466 - lr: 1.1111e-04 - 64s/epoch - 326ms/step
Epoch 377/1000
2023-10-23 20:21:02.379 
Epoch 377/1000 
	 loss: 18.4589, MinusLogProbMetric: 18.4589, val_loss: 18.3784, val_MinusLogProbMetric: 18.3784

Epoch 377: val_loss improved from 18.43056 to 18.37842, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 66s - loss: 18.4589 - MinusLogProbMetric: 18.4589 - val_loss: 18.3784 - val_MinusLogProbMetric: 18.3784 - lr: 1.1111e-04 - 66s/epoch - 337ms/step
Epoch 378/1000
2023-10-23 20:22:07.356 
Epoch 378/1000 
	 loss: 18.4645, MinusLogProbMetric: 18.4645, val_loss: 18.4509, val_MinusLogProbMetric: 18.4509

Epoch 378: val_loss did not improve from 18.37842
196/196 - 64s - loss: 18.4645 - MinusLogProbMetric: 18.4645 - val_loss: 18.4509 - val_MinusLogProbMetric: 18.4509 - lr: 1.1111e-04 - 64s/epoch - 326ms/step
Epoch 379/1000
2023-10-23 20:23:11.106 
Epoch 379/1000 
	 loss: 18.3951, MinusLogProbMetric: 18.3951, val_loss: 18.4777, val_MinusLogProbMetric: 18.4777

Epoch 379: val_loss did not improve from 18.37842
196/196 - 64s - loss: 18.3951 - MinusLogProbMetric: 18.3951 - val_loss: 18.4777 - val_MinusLogProbMetric: 18.4777 - lr: 1.1111e-04 - 64s/epoch - 325ms/step
Epoch 380/1000
2023-10-23 20:24:14.380 
Epoch 380/1000 
	 loss: 18.4246, MinusLogProbMetric: 18.4246, val_loss: 18.4470, val_MinusLogProbMetric: 18.4470

Epoch 380: val_loss did not improve from 18.37842
196/196 - 63s - loss: 18.4246 - MinusLogProbMetric: 18.4246 - val_loss: 18.4470 - val_MinusLogProbMetric: 18.4470 - lr: 1.1111e-04 - 63s/epoch - 323ms/step
Epoch 381/1000
2023-10-23 20:25:17.885 
Epoch 381/1000 
	 loss: 18.4750, MinusLogProbMetric: 18.4750, val_loss: 18.3481, val_MinusLogProbMetric: 18.3481

Epoch 381: val_loss improved from 18.37842 to 18.34807, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 65s - loss: 18.4750 - MinusLogProbMetric: 18.4750 - val_loss: 18.3481 - val_MinusLogProbMetric: 18.3481 - lr: 1.1111e-04 - 65s/epoch - 329ms/step
Epoch 382/1000
2023-10-23 20:26:22.290 
Epoch 382/1000 
	 loss: 18.4626, MinusLogProbMetric: 18.4626, val_loss: 18.4315, val_MinusLogProbMetric: 18.4315

Epoch 382: val_loss did not improve from 18.34807
196/196 - 63s - loss: 18.4626 - MinusLogProbMetric: 18.4626 - val_loss: 18.4315 - val_MinusLogProbMetric: 18.4315 - lr: 1.1111e-04 - 63s/epoch - 323ms/step
Epoch 383/1000
2023-10-23 20:27:26.296 
Epoch 383/1000 
	 loss: 18.4742, MinusLogProbMetric: 18.4742, val_loss: 19.4609, val_MinusLogProbMetric: 19.4609

Epoch 383: val_loss did not improve from 18.34807
196/196 - 64s - loss: 18.4742 - MinusLogProbMetric: 18.4742 - val_loss: 19.4609 - val_MinusLogProbMetric: 19.4609 - lr: 1.1111e-04 - 64s/epoch - 327ms/step
Epoch 384/1000
2023-10-23 20:28:30.856 
Epoch 384/1000 
	 loss: 18.4507, MinusLogProbMetric: 18.4507, val_loss: 18.5175, val_MinusLogProbMetric: 18.5175

Epoch 384: val_loss did not improve from 18.34807
196/196 - 65s - loss: 18.4507 - MinusLogProbMetric: 18.4507 - val_loss: 18.5175 - val_MinusLogProbMetric: 18.5175 - lr: 1.1111e-04 - 65s/epoch - 329ms/step
Epoch 385/1000
2023-10-23 20:29:34.830 
Epoch 385/1000 
	 loss: 18.3794, MinusLogProbMetric: 18.3794, val_loss: 18.5097, val_MinusLogProbMetric: 18.5097

Epoch 385: val_loss did not improve from 18.34807
196/196 - 64s - loss: 18.3794 - MinusLogProbMetric: 18.3794 - val_loss: 18.5097 - val_MinusLogProbMetric: 18.5097 - lr: 1.1111e-04 - 64s/epoch - 326ms/step
Epoch 386/1000
2023-10-23 20:30:38.575 
Epoch 386/1000 
	 loss: 18.4206, MinusLogProbMetric: 18.4206, val_loss: 18.7162, val_MinusLogProbMetric: 18.7162

Epoch 386: val_loss did not improve from 18.34807
196/196 - 64s - loss: 18.4206 - MinusLogProbMetric: 18.4206 - val_loss: 18.7162 - val_MinusLogProbMetric: 18.7162 - lr: 1.1111e-04 - 64s/epoch - 325ms/step
Epoch 387/1000
2023-10-23 20:31:43.330 
Epoch 387/1000 
	 loss: 18.3983, MinusLogProbMetric: 18.3983, val_loss: 18.5979, val_MinusLogProbMetric: 18.5979

Epoch 387: val_loss did not improve from 18.34807
196/196 - 65s - loss: 18.3983 - MinusLogProbMetric: 18.3983 - val_loss: 18.5979 - val_MinusLogProbMetric: 18.5979 - lr: 1.1111e-04 - 65s/epoch - 330ms/step
Epoch 388/1000
2023-10-23 20:32:48.116 
Epoch 388/1000 
	 loss: 18.4104, MinusLogProbMetric: 18.4104, val_loss: 18.8265, val_MinusLogProbMetric: 18.8265

Epoch 388: val_loss did not improve from 18.34807
196/196 - 65s - loss: 18.4104 - MinusLogProbMetric: 18.4104 - val_loss: 18.8265 - val_MinusLogProbMetric: 18.8265 - lr: 1.1111e-04 - 65s/epoch - 331ms/step
Epoch 389/1000
2023-10-23 20:33:53.762 
Epoch 389/1000 
	 loss: 18.4689, MinusLogProbMetric: 18.4689, val_loss: 18.3647, val_MinusLogProbMetric: 18.3647

Epoch 389: val_loss did not improve from 18.34807
196/196 - 66s - loss: 18.4689 - MinusLogProbMetric: 18.4689 - val_loss: 18.3647 - val_MinusLogProbMetric: 18.3647 - lr: 1.1111e-04 - 66s/epoch - 335ms/step
Epoch 390/1000
2023-10-23 20:34:58.603 
Epoch 390/1000 
	 loss: 18.3865, MinusLogProbMetric: 18.3865, val_loss: 19.2836, val_MinusLogProbMetric: 19.2836

Epoch 390: val_loss did not improve from 18.34807
196/196 - 65s - loss: 18.3865 - MinusLogProbMetric: 18.3865 - val_loss: 19.2836 - val_MinusLogProbMetric: 19.2836 - lr: 1.1111e-04 - 65s/epoch - 331ms/step
Epoch 391/1000
2023-10-23 20:36:02.691 
Epoch 391/1000 
	 loss: 18.4623, MinusLogProbMetric: 18.4623, val_loss: 18.3997, val_MinusLogProbMetric: 18.3997

Epoch 391: val_loss did not improve from 18.34807
196/196 - 64s - loss: 18.4623 - MinusLogProbMetric: 18.4623 - val_loss: 18.3997 - val_MinusLogProbMetric: 18.3997 - lr: 1.1111e-04 - 64s/epoch - 327ms/step
Epoch 392/1000
2023-10-23 20:37:07.100 
Epoch 392/1000 
	 loss: 18.4124, MinusLogProbMetric: 18.4124, val_loss: 18.2874, val_MinusLogProbMetric: 18.2874

Epoch 392: val_loss improved from 18.34807 to 18.28741, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 66s - loss: 18.4124 - MinusLogProbMetric: 18.4124 - val_loss: 18.2874 - val_MinusLogProbMetric: 18.2874 - lr: 1.1111e-04 - 66s/epoch - 335ms/step
Epoch 393/1000
2023-10-23 20:38:15.650 
Epoch 393/1000 
	 loss: 18.4140, MinusLogProbMetric: 18.4140, val_loss: 18.4632, val_MinusLogProbMetric: 18.4632

Epoch 393: val_loss did not improve from 18.28741
196/196 - 67s - loss: 18.4140 - MinusLogProbMetric: 18.4140 - val_loss: 18.4632 - val_MinusLogProbMetric: 18.4632 - lr: 1.1111e-04 - 67s/epoch - 343ms/step
Epoch 394/1000
2023-10-23 20:39:25.096 
Epoch 394/1000 
	 loss: 18.4133, MinusLogProbMetric: 18.4133, val_loss: 18.3761, val_MinusLogProbMetric: 18.3761

Epoch 394: val_loss did not improve from 18.28741
196/196 - 69s - loss: 18.4133 - MinusLogProbMetric: 18.4133 - val_loss: 18.3761 - val_MinusLogProbMetric: 18.3761 - lr: 1.1111e-04 - 69s/epoch - 354ms/step
Epoch 395/1000
2023-10-23 20:40:33.168 
Epoch 395/1000 
	 loss: 18.3903, MinusLogProbMetric: 18.3903, val_loss: 18.5206, val_MinusLogProbMetric: 18.5206

Epoch 395: val_loss did not improve from 18.28741
196/196 - 68s - loss: 18.3903 - MinusLogProbMetric: 18.3903 - val_loss: 18.5206 - val_MinusLogProbMetric: 18.5206 - lr: 1.1111e-04 - 68s/epoch - 347ms/step
Epoch 396/1000
2023-10-23 20:41:37.066 
Epoch 396/1000 
	 loss: 18.3862, MinusLogProbMetric: 18.3862, val_loss: 19.2816, val_MinusLogProbMetric: 19.2816

Epoch 396: val_loss did not improve from 18.28741
196/196 - 64s - loss: 18.3862 - MinusLogProbMetric: 18.3862 - val_loss: 19.2816 - val_MinusLogProbMetric: 19.2816 - lr: 1.1111e-04 - 64s/epoch - 326ms/step
Epoch 397/1000
2023-10-23 20:42:40.406 
Epoch 397/1000 
	 loss: 18.3869, MinusLogProbMetric: 18.3869, val_loss: 18.6211, val_MinusLogProbMetric: 18.6211

Epoch 397: val_loss did not improve from 18.28741
196/196 - 63s - loss: 18.3869 - MinusLogProbMetric: 18.3869 - val_loss: 18.6211 - val_MinusLogProbMetric: 18.6211 - lr: 1.1111e-04 - 63s/epoch - 323ms/step
Epoch 398/1000
2023-10-23 20:43:47.243 
Epoch 398/1000 
	 loss: 18.3971, MinusLogProbMetric: 18.3971, val_loss: 18.6747, val_MinusLogProbMetric: 18.6747

Epoch 398: val_loss did not improve from 18.28741
196/196 - 67s - loss: 18.3971 - MinusLogProbMetric: 18.3971 - val_loss: 18.6747 - val_MinusLogProbMetric: 18.6747 - lr: 1.1111e-04 - 67s/epoch - 341ms/step
Epoch 399/1000
2023-10-23 20:44:49.632 
Epoch 399/1000 
	 loss: 18.3401, MinusLogProbMetric: 18.3401, val_loss: 18.5568, val_MinusLogProbMetric: 18.5568

Epoch 399: val_loss did not improve from 18.28741
196/196 - 62s - loss: 18.3401 - MinusLogProbMetric: 18.3401 - val_loss: 18.5568 - val_MinusLogProbMetric: 18.5568 - lr: 1.1111e-04 - 62s/epoch - 318ms/step
Epoch 400/1000
2023-10-23 20:45:57.315 
Epoch 400/1000 
	 loss: 18.3389, MinusLogProbMetric: 18.3389, val_loss: 18.3326, val_MinusLogProbMetric: 18.3326

Epoch 400: val_loss did not improve from 18.28741
196/196 - 68s - loss: 18.3389 - MinusLogProbMetric: 18.3389 - val_loss: 18.3326 - val_MinusLogProbMetric: 18.3326 - lr: 1.1111e-04 - 68s/epoch - 345ms/step
Epoch 401/1000
2023-10-23 20:47:04.607 
Epoch 401/1000 
	 loss: 18.3717, MinusLogProbMetric: 18.3717, val_loss: 18.6275, val_MinusLogProbMetric: 18.6275

Epoch 401: val_loss did not improve from 18.28741
196/196 - 67s - loss: 18.3717 - MinusLogProbMetric: 18.3717 - val_loss: 18.6275 - val_MinusLogProbMetric: 18.6275 - lr: 1.1111e-04 - 67s/epoch - 343ms/step
Epoch 402/1000
2023-10-23 20:48:10.671 
Epoch 402/1000 
	 loss: 18.3769, MinusLogProbMetric: 18.3769, val_loss: 18.2612, val_MinusLogProbMetric: 18.2612

Epoch 402: val_loss improved from 18.28741 to 18.26122, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 67s - loss: 18.3769 - MinusLogProbMetric: 18.3769 - val_loss: 18.2612 - val_MinusLogProbMetric: 18.2612 - lr: 1.1111e-04 - 67s/epoch - 343ms/step
Epoch 403/1000
2023-10-23 20:49:18.058 
Epoch 403/1000 
	 loss: 18.3743, MinusLogProbMetric: 18.3743, val_loss: 18.4427, val_MinusLogProbMetric: 18.4427

Epoch 403: val_loss did not improve from 18.26122
196/196 - 66s - loss: 18.3743 - MinusLogProbMetric: 18.3743 - val_loss: 18.4427 - val_MinusLogProbMetric: 18.4427 - lr: 1.1111e-04 - 66s/epoch - 338ms/step
Epoch 404/1000
2023-10-23 20:50:22.979 
Epoch 404/1000 
	 loss: 18.3119, MinusLogProbMetric: 18.3119, val_loss: 18.5914, val_MinusLogProbMetric: 18.5914

Epoch 404: val_loss did not improve from 18.26122
196/196 - 65s - loss: 18.3119 - MinusLogProbMetric: 18.3119 - val_loss: 18.5914 - val_MinusLogProbMetric: 18.5914 - lr: 1.1111e-04 - 65s/epoch - 331ms/step
Epoch 405/1000
2023-10-23 20:51:30.568 
Epoch 405/1000 
	 loss: 18.3707, MinusLogProbMetric: 18.3707, val_loss: 18.4014, val_MinusLogProbMetric: 18.4014

Epoch 405: val_loss did not improve from 18.26122
196/196 - 68s - loss: 18.3707 - MinusLogProbMetric: 18.3707 - val_loss: 18.4014 - val_MinusLogProbMetric: 18.4014 - lr: 1.1111e-04 - 68s/epoch - 345ms/step
Epoch 406/1000
2023-10-23 20:52:36.838 
Epoch 406/1000 
	 loss: 18.2897, MinusLogProbMetric: 18.2897, val_loss: 18.4589, val_MinusLogProbMetric: 18.4589

Epoch 406: val_loss did not improve from 18.26122
196/196 - 66s - loss: 18.2897 - MinusLogProbMetric: 18.2897 - val_loss: 18.4589 - val_MinusLogProbMetric: 18.4589 - lr: 1.1111e-04 - 66s/epoch - 338ms/step
Epoch 407/1000
2023-10-23 20:53:41.321 
Epoch 407/1000 
	 loss: 18.3111, MinusLogProbMetric: 18.3111, val_loss: 18.6645, val_MinusLogProbMetric: 18.6645

Epoch 407: val_loss did not improve from 18.26122
196/196 - 64s - loss: 18.3111 - MinusLogProbMetric: 18.3111 - val_loss: 18.6645 - val_MinusLogProbMetric: 18.6645 - lr: 1.1111e-04 - 64s/epoch - 329ms/step
Epoch 408/1000
2023-10-23 20:54:48.159 
Epoch 408/1000 
	 loss: 18.3828, MinusLogProbMetric: 18.3828, val_loss: 18.7044, val_MinusLogProbMetric: 18.7044

Epoch 408: val_loss did not improve from 18.26122
196/196 - 67s - loss: 18.3828 - MinusLogProbMetric: 18.3828 - val_loss: 18.7044 - val_MinusLogProbMetric: 18.7044 - lr: 1.1111e-04 - 67s/epoch - 341ms/step
Epoch 409/1000
2023-10-23 20:55:51.391 
Epoch 409/1000 
	 loss: 18.3260, MinusLogProbMetric: 18.3260, val_loss: 18.3114, val_MinusLogProbMetric: 18.3114

Epoch 409: val_loss did not improve from 18.26122
196/196 - 63s - loss: 18.3260 - MinusLogProbMetric: 18.3260 - val_loss: 18.3114 - val_MinusLogProbMetric: 18.3114 - lr: 1.1111e-04 - 63s/epoch - 323ms/step
Epoch 410/1000
2023-10-23 20:56:58.093 
Epoch 410/1000 
	 loss: 18.3311, MinusLogProbMetric: 18.3311, val_loss: 18.3196, val_MinusLogProbMetric: 18.3196

Epoch 410: val_loss did not improve from 18.26122
196/196 - 67s - loss: 18.3311 - MinusLogProbMetric: 18.3311 - val_loss: 18.3196 - val_MinusLogProbMetric: 18.3196 - lr: 1.1111e-04 - 67s/epoch - 340ms/step
Epoch 411/1000
2023-10-23 20:58:03.665 
Epoch 411/1000 
	 loss: 18.2843, MinusLogProbMetric: 18.2843, val_loss: 18.4863, val_MinusLogProbMetric: 18.4863

Epoch 411: val_loss did not improve from 18.26122
196/196 - 66s - loss: 18.2843 - MinusLogProbMetric: 18.2843 - val_loss: 18.4863 - val_MinusLogProbMetric: 18.4863 - lr: 1.1111e-04 - 66s/epoch - 335ms/step
Epoch 412/1000
2023-10-23 20:59:09.776 
Epoch 412/1000 
	 loss: 18.3211, MinusLogProbMetric: 18.3211, val_loss: 18.4658, val_MinusLogProbMetric: 18.4658

Epoch 412: val_loss did not improve from 18.26122
196/196 - 66s - loss: 18.3211 - MinusLogProbMetric: 18.3211 - val_loss: 18.4658 - val_MinusLogProbMetric: 18.4658 - lr: 1.1111e-04 - 66s/epoch - 337ms/step
Epoch 413/1000
2023-10-23 21:00:15.610 
Epoch 413/1000 
	 loss: 18.3244, MinusLogProbMetric: 18.3244, val_loss: 18.6635, val_MinusLogProbMetric: 18.6635

Epoch 413: val_loss did not improve from 18.26122
196/196 - 66s - loss: 18.3244 - MinusLogProbMetric: 18.3244 - val_loss: 18.6635 - val_MinusLogProbMetric: 18.6635 - lr: 1.1111e-04 - 66s/epoch - 336ms/step
Epoch 414/1000
2023-10-23 21:01:18.340 
Epoch 414/1000 
	 loss: 18.3625, MinusLogProbMetric: 18.3625, val_loss: 18.6187, val_MinusLogProbMetric: 18.6187

Epoch 414: val_loss did not improve from 18.26122
196/196 - 63s - loss: 18.3625 - MinusLogProbMetric: 18.3625 - val_loss: 18.6187 - val_MinusLogProbMetric: 18.6187 - lr: 1.1111e-04 - 63s/epoch - 320ms/step
Epoch 415/1000
2023-10-23 21:02:24.672 
Epoch 415/1000 
	 loss: 18.2560, MinusLogProbMetric: 18.2560, val_loss: 18.2845, val_MinusLogProbMetric: 18.2845

Epoch 415: val_loss did not improve from 18.26122
196/196 - 66s - loss: 18.2560 - MinusLogProbMetric: 18.2560 - val_loss: 18.2845 - val_MinusLogProbMetric: 18.2845 - lr: 1.1111e-04 - 66s/epoch - 338ms/step
Epoch 416/1000
2023-10-23 21:03:33.485 
Epoch 416/1000 
	 loss: 18.3327, MinusLogProbMetric: 18.3327, val_loss: 18.2716, val_MinusLogProbMetric: 18.2716

Epoch 416: val_loss did not improve from 18.26122
196/196 - 69s - loss: 18.3327 - MinusLogProbMetric: 18.3327 - val_loss: 18.2716 - val_MinusLogProbMetric: 18.2716 - lr: 1.1111e-04 - 69s/epoch - 351ms/step
Epoch 417/1000
2023-10-23 21:04:39.315 
Epoch 417/1000 
	 loss: 18.3433, MinusLogProbMetric: 18.3433, val_loss: 18.6494, val_MinusLogProbMetric: 18.6494

Epoch 417: val_loss did not improve from 18.26122
196/196 - 66s - loss: 18.3433 - MinusLogProbMetric: 18.3433 - val_loss: 18.6494 - val_MinusLogProbMetric: 18.6494 - lr: 1.1111e-04 - 66s/epoch - 336ms/step
Epoch 418/1000
2023-10-23 21:05:44.760 
Epoch 418/1000 
	 loss: 18.3254, MinusLogProbMetric: 18.3254, val_loss: 18.9652, val_MinusLogProbMetric: 18.9652

Epoch 418: val_loss did not improve from 18.26122
196/196 - 65s - loss: 18.3254 - MinusLogProbMetric: 18.3254 - val_loss: 18.9652 - val_MinusLogProbMetric: 18.9652 - lr: 1.1111e-04 - 65s/epoch - 334ms/step
Epoch 419/1000
2023-10-23 21:06:51.773 
Epoch 419/1000 
	 loss: 18.3302, MinusLogProbMetric: 18.3302, val_loss: 18.3177, val_MinusLogProbMetric: 18.3177

Epoch 419: val_loss did not improve from 18.26122
196/196 - 67s - loss: 18.3302 - MinusLogProbMetric: 18.3302 - val_loss: 18.3177 - val_MinusLogProbMetric: 18.3177 - lr: 1.1111e-04 - 67s/epoch - 342ms/step
Epoch 420/1000
2023-10-23 21:07:57.832 
Epoch 420/1000 
	 loss: 18.3465, MinusLogProbMetric: 18.3465, val_loss: 18.5887, val_MinusLogProbMetric: 18.5887

Epoch 420: val_loss did not improve from 18.26122
196/196 - 66s - loss: 18.3465 - MinusLogProbMetric: 18.3465 - val_loss: 18.5887 - val_MinusLogProbMetric: 18.5887 - lr: 1.1111e-04 - 66s/epoch - 337ms/step
Epoch 421/1000
2023-10-23 21:09:03.558 
Epoch 421/1000 
	 loss: 18.2656, MinusLogProbMetric: 18.2656, val_loss: 18.5019, val_MinusLogProbMetric: 18.5019

Epoch 421: val_loss did not improve from 18.26122
196/196 - 66s - loss: 18.2656 - MinusLogProbMetric: 18.2656 - val_loss: 18.5019 - val_MinusLogProbMetric: 18.5019 - lr: 1.1111e-04 - 66s/epoch - 335ms/step
Epoch 422/1000
2023-10-23 21:10:09.950 
Epoch 422/1000 
	 loss: 18.3082, MinusLogProbMetric: 18.3082, val_loss: 18.5219, val_MinusLogProbMetric: 18.5219

Epoch 422: val_loss did not improve from 18.26122
196/196 - 66s - loss: 18.3082 - MinusLogProbMetric: 18.3082 - val_loss: 18.5219 - val_MinusLogProbMetric: 18.5219 - lr: 1.1111e-04 - 66s/epoch - 339ms/step
Epoch 423/1000
2023-10-23 21:11:15.957 
Epoch 423/1000 
	 loss: 18.2529, MinusLogProbMetric: 18.2529, val_loss: 18.3971, val_MinusLogProbMetric: 18.3971

Epoch 423: val_loss did not improve from 18.26122
196/196 - 66s - loss: 18.2529 - MinusLogProbMetric: 18.2529 - val_loss: 18.3971 - val_MinusLogProbMetric: 18.3971 - lr: 1.1111e-04 - 66s/epoch - 337ms/step
Epoch 424/1000
2023-10-23 21:12:20.635 
Epoch 424/1000 
	 loss: 18.3137, MinusLogProbMetric: 18.3137, val_loss: 18.4864, val_MinusLogProbMetric: 18.4864

Epoch 424: val_loss did not improve from 18.26122
196/196 - 65s - loss: 18.3137 - MinusLogProbMetric: 18.3137 - val_loss: 18.4864 - val_MinusLogProbMetric: 18.4864 - lr: 1.1111e-04 - 65s/epoch - 330ms/step
Epoch 425/1000
2023-10-23 21:13:25.264 
Epoch 425/1000 
	 loss: 18.2621, MinusLogProbMetric: 18.2621, val_loss: 18.2645, val_MinusLogProbMetric: 18.2645

Epoch 425: val_loss did not improve from 18.26122
196/196 - 65s - loss: 18.2621 - MinusLogProbMetric: 18.2621 - val_loss: 18.2645 - val_MinusLogProbMetric: 18.2645 - lr: 1.1111e-04 - 65s/epoch - 330ms/step
Epoch 426/1000
2023-10-23 21:14:28.543 
Epoch 426/1000 
	 loss: 18.2739, MinusLogProbMetric: 18.2739, val_loss: 18.4309, val_MinusLogProbMetric: 18.4309

Epoch 426: val_loss did not improve from 18.26122
196/196 - 63s - loss: 18.2739 - MinusLogProbMetric: 18.2739 - val_loss: 18.4309 - val_MinusLogProbMetric: 18.4309 - lr: 1.1111e-04 - 63s/epoch - 323ms/step
Epoch 427/1000
2023-10-23 21:15:33.173 
Epoch 427/1000 
	 loss: 18.2617, MinusLogProbMetric: 18.2617, val_loss: 18.3572, val_MinusLogProbMetric: 18.3572

Epoch 427: val_loss did not improve from 18.26122
196/196 - 65s - loss: 18.2617 - MinusLogProbMetric: 18.2617 - val_loss: 18.3572 - val_MinusLogProbMetric: 18.3572 - lr: 1.1111e-04 - 65s/epoch - 330ms/step
Epoch 428/1000
2023-10-23 21:16:32.958 
Epoch 428/1000 
	 loss: 18.2622, MinusLogProbMetric: 18.2622, val_loss: 18.4401, val_MinusLogProbMetric: 18.4401

Epoch 428: val_loss did not improve from 18.26122
196/196 - 60s - loss: 18.2622 - MinusLogProbMetric: 18.2622 - val_loss: 18.4401 - val_MinusLogProbMetric: 18.4401 - lr: 1.1111e-04 - 60s/epoch - 305ms/step
Epoch 429/1000
2023-10-23 21:17:32.718 
Epoch 429/1000 
	 loss: 18.1946, MinusLogProbMetric: 18.1946, val_loss: 18.5521, val_MinusLogProbMetric: 18.5521

Epoch 429: val_loss did not improve from 18.26122
196/196 - 60s - loss: 18.1946 - MinusLogProbMetric: 18.1946 - val_loss: 18.5521 - val_MinusLogProbMetric: 18.5521 - lr: 1.1111e-04 - 60s/epoch - 305ms/step
Epoch 430/1000
2023-10-23 21:18:34.818 
Epoch 430/1000 
	 loss: 18.2488, MinusLogProbMetric: 18.2488, val_loss: 18.4014, val_MinusLogProbMetric: 18.4014

Epoch 430: val_loss did not improve from 18.26122
196/196 - 62s - loss: 18.2488 - MinusLogProbMetric: 18.2488 - val_loss: 18.4014 - val_MinusLogProbMetric: 18.4014 - lr: 1.1111e-04 - 62s/epoch - 317ms/step
Epoch 431/1000
2023-10-23 21:19:35.982 
Epoch 431/1000 
	 loss: 18.2160, MinusLogProbMetric: 18.2160, val_loss: 18.5131, val_MinusLogProbMetric: 18.5131

Epoch 431: val_loss did not improve from 18.26122
196/196 - 61s - loss: 18.2160 - MinusLogProbMetric: 18.2160 - val_loss: 18.5131 - val_MinusLogProbMetric: 18.5131 - lr: 1.1111e-04 - 61s/epoch - 312ms/step
Epoch 432/1000
2023-10-23 21:20:38.327 
Epoch 432/1000 
	 loss: 18.2542, MinusLogProbMetric: 18.2542, val_loss: 18.5875, val_MinusLogProbMetric: 18.5875

Epoch 432: val_loss did not improve from 18.26122
196/196 - 62s - loss: 18.2542 - MinusLogProbMetric: 18.2542 - val_loss: 18.5875 - val_MinusLogProbMetric: 18.5875 - lr: 1.1111e-04 - 62s/epoch - 318ms/step
Epoch 433/1000
2023-10-23 21:21:40.834 
Epoch 433/1000 
	 loss: 18.2731, MinusLogProbMetric: 18.2731, val_loss: 18.2313, val_MinusLogProbMetric: 18.2313

Epoch 433: val_loss improved from 18.26122 to 18.23132, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 64s - loss: 18.2731 - MinusLogProbMetric: 18.2731 - val_loss: 18.2313 - val_MinusLogProbMetric: 18.2313 - lr: 1.1111e-04 - 64s/epoch - 324ms/step
Epoch 434/1000
2023-10-23 21:22:43.810 
Epoch 434/1000 
	 loss: 18.2652, MinusLogProbMetric: 18.2652, val_loss: 18.7756, val_MinusLogProbMetric: 18.7756

Epoch 434: val_loss did not improve from 18.23132
196/196 - 62s - loss: 18.2652 - MinusLogProbMetric: 18.2652 - val_loss: 18.7756 - val_MinusLogProbMetric: 18.7756 - lr: 1.1111e-04 - 62s/epoch - 316ms/step
Epoch 435/1000
2023-10-23 21:23:47.060 
Epoch 435/1000 
	 loss: 18.2625, MinusLogProbMetric: 18.2625, val_loss: 18.5243, val_MinusLogProbMetric: 18.5243

Epoch 435: val_loss did not improve from 18.23132
196/196 - 63s - loss: 18.2625 - MinusLogProbMetric: 18.2625 - val_loss: 18.5243 - val_MinusLogProbMetric: 18.5243 - lr: 1.1111e-04 - 63s/epoch - 323ms/step
Epoch 436/1000
2023-10-23 21:24:51.662 
Epoch 436/1000 
	 loss: 18.2324, MinusLogProbMetric: 18.2324, val_loss: 18.5313, val_MinusLogProbMetric: 18.5313

Epoch 436: val_loss did not improve from 18.23132
196/196 - 65s - loss: 18.2324 - MinusLogProbMetric: 18.2324 - val_loss: 18.5313 - val_MinusLogProbMetric: 18.5313 - lr: 1.1111e-04 - 65s/epoch - 330ms/step
Epoch 437/1000
2023-10-23 21:26:02.597 
Epoch 437/1000 
	 loss: 18.3714, MinusLogProbMetric: 18.3714, val_loss: 18.2606, val_MinusLogProbMetric: 18.2606

Epoch 437: val_loss did not improve from 18.23132
196/196 - 71s - loss: 18.3714 - MinusLogProbMetric: 18.3714 - val_loss: 18.2606 - val_MinusLogProbMetric: 18.2606 - lr: 1.1111e-04 - 71s/epoch - 362ms/step
Epoch 438/1000
2023-10-23 21:27:15.063 
Epoch 438/1000 
	 loss: 18.2243, MinusLogProbMetric: 18.2243, val_loss: 18.1005, val_MinusLogProbMetric: 18.1005

Epoch 438: val_loss improved from 18.23132 to 18.10050, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 74s - loss: 18.2243 - MinusLogProbMetric: 18.2243 - val_loss: 18.1005 - val_MinusLogProbMetric: 18.1005 - lr: 1.1111e-04 - 74s/epoch - 375ms/step
Epoch 439/1000
2023-10-23 21:28:26.757 
Epoch 439/1000 
	 loss: 18.1997, MinusLogProbMetric: 18.1997, val_loss: 18.2007, val_MinusLogProbMetric: 18.2007

Epoch 439: val_loss did not improve from 18.10050
196/196 - 71s - loss: 18.1997 - MinusLogProbMetric: 18.1997 - val_loss: 18.2007 - val_MinusLogProbMetric: 18.2007 - lr: 1.1111e-04 - 71s/epoch - 360ms/step
Epoch 440/1000
2023-10-23 21:29:38.359 
Epoch 440/1000 
	 loss: 18.2782, MinusLogProbMetric: 18.2782, val_loss: 18.3161, val_MinusLogProbMetric: 18.3161

Epoch 440: val_loss did not improve from 18.10050
196/196 - 72s - loss: 18.2782 - MinusLogProbMetric: 18.2782 - val_loss: 18.3161 - val_MinusLogProbMetric: 18.3161 - lr: 1.1111e-04 - 72s/epoch - 366ms/step
Epoch 441/1000
2023-10-23 21:30:52.188 
Epoch 441/1000 
	 loss: 18.2362, MinusLogProbMetric: 18.2362, val_loss: 18.2624, val_MinusLogProbMetric: 18.2624

Epoch 441: val_loss did not improve from 18.10050
196/196 - 74s - loss: 18.2362 - MinusLogProbMetric: 18.2362 - val_loss: 18.2624 - val_MinusLogProbMetric: 18.2624 - lr: 1.1111e-04 - 74s/epoch - 376ms/step
Epoch 442/1000
2023-10-23 21:32:06.146 
Epoch 442/1000 
	 loss: 18.2042, MinusLogProbMetric: 18.2042, val_loss: 18.1935, val_MinusLogProbMetric: 18.1935

Epoch 442: val_loss did not improve from 18.10050
196/196 - 74s - loss: 18.2042 - MinusLogProbMetric: 18.2042 - val_loss: 18.1935 - val_MinusLogProbMetric: 18.1935 - lr: 1.1111e-04 - 74s/epoch - 377ms/step
Epoch 443/1000
2023-10-23 21:33:20.160 
Epoch 443/1000 
	 loss: 18.1968, MinusLogProbMetric: 18.1968, val_loss: 18.4423, val_MinusLogProbMetric: 18.4423

Epoch 443: val_loss did not improve from 18.10050
196/196 - 74s - loss: 18.1968 - MinusLogProbMetric: 18.1968 - val_loss: 18.4423 - val_MinusLogProbMetric: 18.4423 - lr: 1.1111e-04 - 74s/epoch - 378ms/step
Epoch 444/1000
2023-10-23 21:34:34.215 
Epoch 444/1000 
	 loss: 18.1941, MinusLogProbMetric: 18.1941, val_loss: 18.6540, val_MinusLogProbMetric: 18.6540

Epoch 444: val_loss did not improve from 18.10050
196/196 - 74s - loss: 18.1941 - MinusLogProbMetric: 18.1941 - val_loss: 18.6540 - val_MinusLogProbMetric: 18.6540 - lr: 1.1111e-04 - 74s/epoch - 378ms/step
Epoch 445/1000
2023-10-23 21:35:47.678 
Epoch 445/1000 
	 loss: 18.2425, MinusLogProbMetric: 18.2425, val_loss: 18.4003, val_MinusLogProbMetric: 18.4003

Epoch 445: val_loss did not improve from 18.10050
196/196 - 73s - loss: 18.2425 - MinusLogProbMetric: 18.2425 - val_loss: 18.4003 - val_MinusLogProbMetric: 18.4003 - lr: 1.1111e-04 - 73s/epoch - 375ms/step
Epoch 446/1000
2023-10-23 21:36:58.053 
Epoch 446/1000 
	 loss: 18.1794, MinusLogProbMetric: 18.1794, val_loss: 18.3452, val_MinusLogProbMetric: 18.3452

Epoch 446: val_loss did not improve from 18.10050
196/196 - 70s - loss: 18.1794 - MinusLogProbMetric: 18.1794 - val_loss: 18.3452 - val_MinusLogProbMetric: 18.3452 - lr: 1.1111e-04 - 70s/epoch - 359ms/step
Epoch 447/1000
2023-10-23 21:38:07.872 
Epoch 447/1000 
	 loss: 18.2093, MinusLogProbMetric: 18.2093, val_loss: 18.4098, val_MinusLogProbMetric: 18.4098

Epoch 447: val_loss did not improve from 18.10050
196/196 - 70s - loss: 18.2093 - MinusLogProbMetric: 18.2093 - val_loss: 18.4098 - val_MinusLogProbMetric: 18.4098 - lr: 1.1111e-04 - 70s/epoch - 356ms/step
Epoch 448/1000
2023-10-23 21:39:24.277 
Epoch 448/1000 
	 loss: 18.1789, MinusLogProbMetric: 18.1789, val_loss: 18.2048, val_MinusLogProbMetric: 18.2048

Epoch 448: val_loss did not improve from 18.10050
196/196 - 76s - loss: 18.1789 - MinusLogProbMetric: 18.1789 - val_loss: 18.2048 - val_MinusLogProbMetric: 18.2048 - lr: 1.1111e-04 - 76s/epoch - 390ms/step
Epoch 449/1000
2023-10-23 21:40:38.300 
Epoch 449/1000 
	 loss: 18.2412, MinusLogProbMetric: 18.2412, val_loss: 18.5189, val_MinusLogProbMetric: 18.5189

Epoch 449: val_loss did not improve from 18.10050
196/196 - 74s - loss: 18.2412 - MinusLogProbMetric: 18.2412 - val_loss: 18.5189 - val_MinusLogProbMetric: 18.5189 - lr: 1.1111e-04 - 74s/epoch - 378ms/step
Epoch 450/1000
2023-10-23 21:41:55.301 
Epoch 450/1000 
	 loss: 18.1858, MinusLogProbMetric: 18.1858, val_loss: 18.6897, val_MinusLogProbMetric: 18.6897

Epoch 450: val_loss did not improve from 18.10050
196/196 - 77s - loss: 18.1858 - MinusLogProbMetric: 18.1858 - val_loss: 18.6897 - val_MinusLogProbMetric: 18.6897 - lr: 1.1111e-04 - 77s/epoch - 393ms/step
Epoch 451/1000
2023-10-23 21:43:11.244 
Epoch 451/1000 
	 loss: 18.2323, MinusLogProbMetric: 18.2323, val_loss: 18.1474, val_MinusLogProbMetric: 18.1474

Epoch 451: val_loss did not improve from 18.10050
196/196 - 76s - loss: 18.2323 - MinusLogProbMetric: 18.2323 - val_loss: 18.1474 - val_MinusLogProbMetric: 18.1474 - lr: 1.1111e-04 - 76s/epoch - 387ms/step
Epoch 452/1000
2023-10-23 21:44:24.786 
Epoch 452/1000 
	 loss: 18.1418, MinusLogProbMetric: 18.1418, val_loss: 18.1874, val_MinusLogProbMetric: 18.1874

Epoch 452: val_loss did not improve from 18.10050
196/196 - 74s - loss: 18.1418 - MinusLogProbMetric: 18.1418 - val_loss: 18.1874 - val_MinusLogProbMetric: 18.1874 - lr: 1.1111e-04 - 74s/epoch - 375ms/step
Epoch 453/1000
2023-10-23 21:45:40.038 
Epoch 453/1000 
	 loss: 18.1698, MinusLogProbMetric: 18.1698, val_loss: 18.3608, val_MinusLogProbMetric: 18.3608

Epoch 453: val_loss did not improve from 18.10050
196/196 - 75s - loss: 18.1698 - MinusLogProbMetric: 18.1698 - val_loss: 18.3608 - val_MinusLogProbMetric: 18.3608 - lr: 1.1111e-04 - 75s/epoch - 384ms/step
Epoch 454/1000
2023-10-23 21:46:57.694 
Epoch 454/1000 
	 loss: 18.1585, MinusLogProbMetric: 18.1585, val_loss: 18.3439, val_MinusLogProbMetric: 18.3439

Epoch 454: val_loss did not improve from 18.10050
196/196 - 78s - loss: 18.1585 - MinusLogProbMetric: 18.1585 - val_loss: 18.3439 - val_MinusLogProbMetric: 18.3439 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 455/1000
2023-10-23 21:48:16.525 
Epoch 455/1000 
	 loss: 18.1853, MinusLogProbMetric: 18.1853, val_loss: 18.1474, val_MinusLogProbMetric: 18.1474

Epoch 455: val_loss did not improve from 18.10050
196/196 - 79s - loss: 18.1853 - MinusLogProbMetric: 18.1853 - val_loss: 18.1474 - val_MinusLogProbMetric: 18.1474 - lr: 1.1111e-04 - 79s/epoch - 402ms/step
Epoch 456/1000
2023-10-23 21:49:28.830 
Epoch 456/1000 
	 loss: 18.1769, MinusLogProbMetric: 18.1769, val_loss: 18.1844, val_MinusLogProbMetric: 18.1844

Epoch 456: val_loss did not improve from 18.10050
196/196 - 72s - loss: 18.1769 - MinusLogProbMetric: 18.1769 - val_loss: 18.1844 - val_MinusLogProbMetric: 18.1844 - lr: 1.1111e-04 - 72s/epoch - 369ms/step
Epoch 457/1000
2023-10-23 21:50:39.261 
Epoch 457/1000 
	 loss: 18.1659, MinusLogProbMetric: 18.1659, val_loss: 18.2584, val_MinusLogProbMetric: 18.2584

Epoch 457: val_loss did not improve from 18.10050
196/196 - 70s - loss: 18.1659 - MinusLogProbMetric: 18.1659 - val_loss: 18.2584 - val_MinusLogProbMetric: 18.2584 - lr: 1.1111e-04 - 70s/epoch - 359ms/step
Epoch 458/1000
2023-10-23 21:51:54.758 
Epoch 458/1000 
	 loss: 18.1565, MinusLogProbMetric: 18.1565, val_loss: 18.4461, val_MinusLogProbMetric: 18.4461

Epoch 458: val_loss did not improve from 18.10050
196/196 - 75s - loss: 18.1565 - MinusLogProbMetric: 18.1565 - val_loss: 18.4461 - val_MinusLogProbMetric: 18.4461 - lr: 1.1111e-04 - 75s/epoch - 385ms/step
Epoch 459/1000
2023-10-23 21:53:10.376 
Epoch 459/1000 
	 loss: 18.1649, MinusLogProbMetric: 18.1649, val_loss: 18.3845, val_MinusLogProbMetric: 18.3845

Epoch 459: val_loss did not improve from 18.10050
196/196 - 76s - loss: 18.1649 - MinusLogProbMetric: 18.1649 - val_loss: 18.3845 - val_MinusLogProbMetric: 18.3845 - lr: 1.1111e-04 - 76s/epoch - 386ms/step
Epoch 460/1000
2023-10-23 21:54:25.808 
Epoch 460/1000 
	 loss: 18.1844, MinusLogProbMetric: 18.1844, val_loss: 18.5166, val_MinusLogProbMetric: 18.5166

Epoch 460: val_loss did not improve from 18.10050
196/196 - 75s - loss: 18.1844 - MinusLogProbMetric: 18.1844 - val_loss: 18.5166 - val_MinusLogProbMetric: 18.5166 - lr: 1.1111e-04 - 75s/epoch - 385ms/step
Epoch 461/1000
2023-10-23 21:55:39.443 
Epoch 461/1000 
	 loss: 18.1849, MinusLogProbMetric: 18.1849, val_loss: 18.3528, val_MinusLogProbMetric: 18.3528

Epoch 461: val_loss did not improve from 18.10050
196/196 - 74s - loss: 18.1849 - MinusLogProbMetric: 18.1849 - val_loss: 18.3528 - val_MinusLogProbMetric: 18.3528 - lr: 1.1111e-04 - 74s/epoch - 376ms/step
Epoch 462/1000
2023-10-23 21:56:49.727 
Epoch 462/1000 
	 loss: 18.1156, MinusLogProbMetric: 18.1156, val_loss: 18.0791, val_MinusLogProbMetric: 18.0791

Epoch 462: val_loss improved from 18.10050 to 18.07909, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 71s - loss: 18.1156 - MinusLogProbMetric: 18.1156 - val_loss: 18.0791 - val_MinusLogProbMetric: 18.0791 - lr: 1.1111e-04 - 71s/epoch - 365ms/step
Epoch 463/1000
2023-10-23 21:58:01.979 
Epoch 463/1000 
	 loss: 18.1579, MinusLogProbMetric: 18.1579, val_loss: 18.2560, val_MinusLogProbMetric: 18.2560

Epoch 463: val_loss did not improve from 18.07909
196/196 - 71s - loss: 18.1579 - MinusLogProbMetric: 18.1579 - val_loss: 18.2560 - val_MinusLogProbMetric: 18.2560 - lr: 1.1111e-04 - 71s/epoch - 363ms/step
Epoch 464/1000
2023-10-23 21:59:18.628 
Epoch 464/1000 
	 loss: 18.1229, MinusLogProbMetric: 18.1229, val_loss: 18.2204, val_MinusLogProbMetric: 18.2204

Epoch 464: val_loss did not improve from 18.07909
196/196 - 77s - loss: 18.1229 - MinusLogProbMetric: 18.1229 - val_loss: 18.2204 - val_MinusLogProbMetric: 18.2204 - lr: 1.1111e-04 - 77s/epoch - 391ms/step
Epoch 465/1000
2023-10-23 22:00:30.841 
Epoch 465/1000 
	 loss: 18.1555, MinusLogProbMetric: 18.1555, val_loss: 18.1152, val_MinusLogProbMetric: 18.1152

Epoch 465: val_loss did not improve from 18.07909
196/196 - 72s - loss: 18.1555 - MinusLogProbMetric: 18.1555 - val_loss: 18.1152 - val_MinusLogProbMetric: 18.1152 - lr: 1.1111e-04 - 72s/epoch - 368ms/step
Epoch 466/1000
2023-10-23 22:01:43.287 
Epoch 466/1000 
	 loss: 18.1673, MinusLogProbMetric: 18.1673, val_loss: 18.2330, val_MinusLogProbMetric: 18.2330

Epoch 466: val_loss did not improve from 18.07909
196/196 - 72s - loss: 18.1673 - MinusLogProbMetric: 18.1673 - val_loss: 18.2330 - val_MinusLogProbMetric: 18.2330 - lr: 1.1111e-04 - 72s/epoch - 370ms/step
Epoch 467/1000
2023-10-23 22:02:59.709 
Epoch 467/1000 
	 loss: 18.1215, MinusLogProbMetric: 18.1215, val_loss: 18.5376, val_MinusLogProbMetric: 18.5376

Epoch 467: val_loss did not improve from 18.07909
196/196 - 76s - loss: 18.1215 - MinusLogProbMetric: 18.1215 - val_loss: 18.5376 - val_MinusLogProbMetric: 18.5376 - lr: 1.1111e-04 - 76s/epoch - 390ms/step
Epoch 468/1000
2023-10-23 22:04:13.035 
Epoch 468/1000 
	 loss: 18.1610, MinusLogProbMetric: 18.1610, val_loss: 18.2199, val_MinusLogProbMetric: 18.2199

Epoch 468: val_loss did not improve from 18.07909
196/196 - 73s - loss: 18.1610 - MinusLogProbMetric: 18.1610 - val_loss: 18.2199 - val_MinusLogProbMetric: 18.2199 - lr: 1.1111e-04 - 73s/epoch - 374ms/step
Epoch 469/1000
2023-10-23 22:05:25.823 
Epoch 469/1000 
	 loss: 18.1667, MinusLogProbMetric: 18.1667, val_loss: 18.1970, val_MinusLogProbMetric: 18.1970

Epoch 469: val_loss did not improve from 18.07909
196/196 - 73s - loss: 18.1667 - MinusLogProbMetric: 18.1667 - val_loss: 18.1970 - val_MinusLogProbMetric: 18.1970 - lr: 1.1111e-04 - 73s/epoch - 371ms/step
Epoch 470/1000
2023-10-23 22:06:42.130 
Epoch 470/1000 
	 loss: 18.1457, MinusLogProbMetric: 18.1457, val_loss: 18.4860, val_MinusLogProbMetric: 18.4860

Epoch 470: val_loss did not improve from 18.07909
196/196 - 76s - loss: 18.1457 - MinusLogProbMetric: 18.1457 - val_loss: 18.4860 - val_MinusLogProbMetric: 18.4860 - lr: 1.1111e-04 - 76s/epoch - 389ms/step
Epoch 471/1000
2023-10-23 22:07:58.523 
Epoch 471/1000 
	 loss: 18.2000, MinusLogProbMetric: 18.2000, val_loss: 18.5570, val_MinusLogProbMetric: 18.5570

Epoch 471: val_loss did not improve from 18.07909
196/196 - 76s - loss: 18.2000 - MinusLogProbMetric: 18.2000 - val_loss: 18.5570 - val_MinusLogProbMetric: 18.5570 - lr: 1.1111e-04 - 76s/epoch - 390ms/step
Epoch 472/1000
2023-10-23 22:09:14.799 
Epoch 472/1000 
	 loss: 18.1433, MinusLogProbMetric: 18.1433, val_loss: 18.1327, val_MinusLogProbMetric: 18.1327

Epoch 472: val_loss did not improve from 18.07909
196/196 - 76s - loss: 18.1433 - MinusLogProbMetric: 18.1433 - val_loss: 18.1327 - val_MinusLogProbMetric: 18.1327 - lr: 1.1111e-04 - 76s/epoch - 389ms/step
Epoch 473/1000
2023-10-23 22:10:33.538 
Epoch 473/1000 
	 loss: 18.1723, MinusLogProbMetric: 18.1723, val_loss: 18.1917, val_MinusLogProbMetric: 18.1917

Epoch 473: val_loss did not improve from 18.07909
196/196 - 79s - loss: 18.1723 - MinusLogProbMetric: 18.1723 - val_loss: 18.1917 - val_MinusLogProbMetric: 18.1917 - lr: 1.1111e-04 - 79s/epoch - 402ms/step
Epoch 474/1000
2023-10-23 22:11:50.215 
Epoch 474/1000 
	 loss: 18.1444, MinusLogProbMetric: 18.1444, val_loss: 18.3483, val_MinusLogProbMetric: 18.3483

Epoch 474: val_loss did not improve from 18.07909
196/196 - 77s - loss: 18.1444 - MinusLogProbMetric: 18.1444 - val_loss: 18.3483 - val_MinusLogProbMetric: 18.3483 - lr: 1.1111e-04 - 77s/epoch - 391ms/step
Epoch 475/1000
2023-10-23 22:13:06.095 
Epoch 475/1000 
	 loss: 18.1363, MinusLogProbMetric: 18.1363, val_loss: 18.2830, val_MinusLogProbMetric: 18.2830

Epoch 475: val_loss did not improve from 18.07909
196/196 - 76s - loss: 18.1363 - MinusLogProbMetric: 18.1363 - val_loss: 18.2830 - val_MinusLogProbMetric: 18.2830 - lr: 1.1111e-04 - 76s/epoch - 387ms/step
Epoch 476/1000
2023-10-23 22:14:21.600 
Epoch 476/1000 
	 loss: 18.1475, MinusLogProbMetric: 18.1475, val_loss: 18.0217, val_MinusLogProbMetric: 18.0217

Epoch 476: val_loss improved from 18.07909 to 18.02167, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 77s - loss: 18.1475 - MinusLogProbMetric: 18.1475 - val_loss: 18.0217 - val_MinusLogProbMetric: 18.0217 - lr: 1.1111e-04 - 77s/epoch - 391ms/step
Epoch 477/1000
2023-10-23 22:15:39.904 
Epoch 477/1000 
	 loss: 18.1122, MinusLogProbMetric: 18.1122, val_loss: 18.2951, val_MinusLogProbMetric: 18.2951

Epoch 477: val_loss did not improve from 18.02167
196/196 - 77s - loss: 18.1122 - MinusLogProbMetric: 18.1122 - val_loss: 18.2951 - val_MinusLogProbMetric: 18.2951 - lr: 1.1111e-04 - 77s/epoch - 394ms/step
Epoch 478/1000
2023-10-23 22:16:57.657 
Epoch 478/1000 
	 loss: 18.1668, MinusLogProbMetric: 18.1668, val_loss: 18.3536, val_MinusLogProbMetric: 18.3536

Epoch 478: val_loss did not improve from 18.02167
196/196 - 78s - loss: 18.1668 - MinusLogProbMetric: 18.1668 - val_loss: 18.3536 - val_MinusLogProbMetric: 18.3536 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 479/1000
2023-10-23 22:18:12.762 
Epoch 479/1000 
	 loss: 18.1791, MinusLogProbMetric: 18.1791, val_loss: 18.1824, val_MinusLogProbMetric: 18.1824

Epoch 479: val_loss did not improve from 18.02167
196/196 - 75s - loss: 18.1791 - MinusLogProbMetric: 18.1791 - val_loss: 18.1824 - val_MinusLogProbMetric: 18.1824 - lr: 1.1111e-04 - 75s/epoch - 383ms/step
Epoch 480/1000
2023-10-23 22:19:28.425 
Epoch 480/1000 
	 loss: 18.1403, MinusLogProbMetric: 18.1403, val_loss: 18.4203, val_MinusLogProbMetric: 18.4203

Epoch 480: val_loss did not improve from 18.02167
196/196 - 76s - loss: 18.1403 - MinusLogProbMetric: 18.1403 - val_loss: 18.4203 - val_MinusLogProbMetric: 18.4203 - lr: 1.1111e-04 - 76s/epoch - 386ms/step
Epoch 481/1000
2023-10-23 22:20:47.235 
Epoch 481/1000 
	 loss: 18.1378, MinusLogProbMetric: 18.1378, val_loss: 18.0771, val_MinusLogProbMetric: 18.0771

Epoch 481: val_loss did not improve from 18.02167
196/196 - 79s - loss: 18.1378 - MinusLogProbMetric: 18.1378 - val_loss: 18.0771 - val_MinusLogProbMetric: 18.0771 - lr: 1.1111e-04 - 79s/epoch - 402ms/step
Epoch 482/1000
2023-10-23 22:22:05.254 
Epoch 482/1000 
	 loss: 18.0664, MinusLogProbMetric: 18.0664, val_loss: 18.2701, val_MinusLogProbMetric: 18.2701

Epoch 482: val_loss did not improve from 18.02167
196/196 - 78s - loss: 18.0664 - MinusLogProbMetric: 18.0664 - val_loss: 18.2701 - val_MinusLogProbMetric: 18.2701 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 483/1000
2023-10-23 22:23:23.420 
Epoch 483/1000 
	 loss: 18.1089, MinusLogProbMetric: 18.1089, val_loss: 18.1645, val_MinusLogProbMetric: 18.1645

Epoch 483: val_loss did not improve from 18.02167
196/196 - 78s - loss: 18.1089 - MinusLogProbMetric: 18.1089 - val_loss: 18.1645 - val_MinusLogProbMetric: 18.1645 - lr: 1.1111e-04 - 78s/epoch - 399ms/step
Epoch 484/1000
2023-10-23 22:24:39.305 
Epoch 484/1000 
	 loss: 18.0564, MinusLogProbMetric: 18.0564, val_loss: 18.0563, val_MinusLogProbMetric: 18.0563

Epoch 484: val_loss did not improve from 18.02167
196/196 - 76s - loss: 18.0564 - MinusLogProbMetric: 18.0564 - val_loss: 18.0563 - val_MinusLogProbMetric: 18.0563 - lr: 1.1111e-04 - 76s/epoch - 387ms/step
Epoch 485/1000
2023-10-23 22:25:54.931 
Epoch 485/1000 
	 loss: 18.0976, MinusLogProbMetric: 18.0976, val_loss: 18.3137, val_MinusLogProbMetric: 18.3137

Epoch 485: val_loss did not improve from 18.02167
196/196 - 76s - loss: 18.0976 - MinusLogProbMetric: 18.0976 - val_loss: 18.3137 - val_MinusLogProbMetric: 18.3137 - lr: 1.1111e-04 - 76s/epoch - 386ms/step
Epoch 486/1000
2023-10-23 22:27:11.277 
Epoch 486/1000 
	 loss: 18.1788, MinusLogProbMetric: 18.1788, val_loss: 18.1091, val_MinusLogProbMetric: 18.1091

Epoch 486: val_loss did not improve from 18.02167
196/196 - 76s - loss: 18.1788 - MinusLogProbMetric: 18.1788 - val_loss: 18.1091 - val_MinusLogProbMetric: 18.1091 - lr: 1.1111e-04 - 76s/epoch - 390ms/step
Epoch 487/1000
2023-10-23 22:28:28.677 
Epoch 487/1000 
	 loss: 18.0831, MinusLogProbMetric: 18.0831, val_loss: 18.1791, val_MinusLogProbMetric: 18.1791

Epoch 487: val_loss did not improve from 18.02167
196/196 - 77s - loss: 18.0831 - MinusLogProbMetric: 18.0831 - val_loss: 18.1791 - val_MinusLogProbMetric: 18.1791 - lr: 1.1111e-04 - 77s/epoch - 395ms/step
Epoch 488/1000
2023-10-23 22:29:46.554 
Epoch 488/1000 
	 loss: 18.1171, MinusLogProbMetric: 18.1171, val_loss: 17.9385, val_MinusLogProbMetric: 17.9385

Epoch 488: val_loss improved from 18.02167 to 17.93846, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 79s - loss: 18.1171 - MinusLogProbMetric: 18.1171 - val_loss: 17.9385 - val_MinusLogProbMetric: 17.9385 - lr: 1.1111e-04 - 79s/epoch - 403ms/step
Epoch 489/1000
2023-10-23 22:31:06.661 
Epoch 489/1000 
	 loss: 18.0671, MinusLogProbMetric: 18.0671, val_loss: 18.1267, val_MinusLogProbMetric: 18.1267

Epoch 489: val_loss did not improve from 17.93846
196/196 - 79s - loss: 18.0671 - MinusLogProbMetric: 18.0671 - val_loss: 18.1267 - val_MinusLogProbMetric: 18.1267 - lr: 1.1111e-04 - 79s/epoch - 403ms/step
Epoch 490/1000
2023-10-23 22:32:22.197 
Epoch 490/1000 
	 loss: 18.1136, MinusLogProbMetric: 18.1136, val_loss: 18.1149, val_MinusLogProbMetric: 18.1149

Epoch 490: val_loss did not improve from 17.93846
196/196 - 76s - loss: 18.1136 - MinusLogProbMetric: 18.1136 - val_loss: 18.1149 - val_MinusLogProbMetric: 18.1149 - lr: 1.1111e-04 - 76s/epoch - 385ms/step
Epoch 491/1000
2023-10-23 22:33:38.924 
Epoch 491/1000 
	 loss: 18.0737, MinusLogProbMetric: 18.0737, val_loss: 18.0223, val_MinusLogProbMetric: 18.0223

Epoch 491: val_loss did not improve from 17.93846
196/196 - 77s - loss: 18.0737 - MinusLogProbMetric: 18.0737 - val_loss: 18.0223 - val_MinusLogProbMetric: 18.0223 - lr: 1.1111e-04 - 77s/epoch - 391ms/step
Epoch 492/1000
2023-10-23 22:34:56.188 
Epoch 492/1000 
	 loss: 18.0895, MinusLogProbMetric: 18.0895, val_loss: 18.0863, val_MinusLogProbMetric: 18.0863

Epoch 492: val_loss did not improve from 17.93846
196/196 - 77s - loss: 18.0895 - MinusLogProbMetric: 18.0895 - val_loss: 18.0863 - val_MinusLogProbMetric: 18.0863 - lr: 1.1111e-04 - 77s/epoch - 394ms/step
Epoch 493/1000
2023-10-23 22:36:11.075 
Epoch 493/1000 
	 loss: 18.0711, MinusLogProbMetric: 18.0711, val_loss: 18.1666, val_MinusLogProbMetric: 18.1666

Epoch 493: val_loss did not improve from 17.93846
196/196 - 75s - loss: 18.0711 - MinusLogProbMetric: 18.0711 - val_loss: 18.1666 - val_MinusLogProbMetric: 18.1666 - lr: 1.1111e-04 - 75s/epoch - 382ms/step
Epoch 494/1000
2023-10-23 22:37:23.312 
Epoch 494/1000 
	 loss: 18.0533, MinusLogProbMetric: 18.0533, val_loss: 18.2928, val_MinusLogProbMetric: 18.2928

Epoch 494: val_loss did not improve from 17.93846
196/196 - 72s - loss: 18.0533 - MinusLogProbMetric: 18.0533 - val_loss: 18.2928 - val_MinusLogProbMetric: 18.2928 - lr: 1.1111e-04 - 72s/epoch - 369ms/step
Epoch 495/1000
2023-10-23 22:38:33.852 
Epoch 495/1000 
	 loss: 18.1130, MinusLogProbMetric: 18.1130, val_loss: 18.6325, val_MinusLogProbMetric: 18.6325

Epoch 495: val_loss did not improve from 17.93846
196/196 - 71s - loss: 18.1130 - MinusLogProbMetric: 18.1130 - val_loss: 18.6325 - val_MinusLogProbMetric: 18.6325 - lr: 1.1111e-04 - 71s/epoch - 360ms/step
Epoch 496/1000
2023-10-23 22:39:43.846 
Epoch 496/1000 
	 loss: 18.0512, MinusLogProbMetric: 18.0512, val_loss: 18.3641, val_MinusLogProbMetric: 18.3641

Epoch 496: val_loss did not improve from 17.93846
196/196 - 70s - loss: 18.0512 - MinusLogProbMetric: 18.0512 - val_loss: 18.3641 - val_MinusLogProbMetric: 18.3641 - lr: 1.1111e-04 - 70s/epoch - 357ms/step
Epoch 497/1000
2023-10-23 22:40:55.797 
Epoch 497/1000 
	 loss: 18.0911, MinusLogProbMetric: 18.0911, val_loss: 18.0377, val_MinusLogProbMetric: 18.0377

Epoch 497: val_loss did not improve from 17.93846
196/196 - 72s - loss: 18.0911 - MinusLogProbMetric: 18.0911 - val_loss: 18.0377 - val_MinusLogProbMetric: 18.0377 - lr: 1.1111e-04 - 72s/epoch - 367ms/step
Epoch 498/1000
2023-10-23 22:42:11.495 
Epoch 498/1000 
	 loss: 18.0597, MinusLogProbMetric: 18.0597, val_loss: 18.2257, val_MinusLogProbMetric: 18.2257

Epoch 498: val_loss did not improve from 17.93846
196/196 - 76s - loss: 18.0597 - MinusLogProbMetric: 18.0597 - val_loss: 18.2257 - val_MinusLogProbMetric: 18.2257 - lr: 1.1111e-04 - 76s/epoch - 386ms/step
Epoch 499/1000
2023-10-23 22:43:22.117 
Epoch 499/1000 
	 loss: 18.0465, MinusLogProbMetric: 18.0465, val_loss: 18.1258, val_MinusLogProbMetric: 18.1258

Epoch 499: val_loss did not improve from 17.93846
196/196 - 71s - loss: 18.0465 - MinusLogProbMetric: 18.0465 - val_loss: 18.1258 - val_MinusLogProbMetric: 18.1258 - lr: 1.1111e-04 - 71s/epoch - 360ms/step
Epoch 500/1000
2023-10-23 22:44:35.289 
Epoch 500/1000 
	 loss: 18.0331, MinusLogProbMetric: 18.0331, val_loss: 18.3471, val_MinusLogProbMetric: 18.3471

Epoch 500: val_loss did not improve from 17.93846
196/196 - 73s - loss: 18.0331 - MinusLogProbMetric: 18.0331 - val_loss: 18.3471 - val_MinusLogProbMetric: 18.3471 - lr: 1.1111e-04 - 73s/epoch - 373ms/step
Epoch 501/1000
2023-10-23 22:45:44.546 
Epoch 501/1000 
	 loss: 18.0653, MinusLogProbMetric: 18.0653, val_loss: 18.0277, val_MinusLogProbMetric: 18.0277

Epoch 501: val_loss did not improve from 17.93846
196/196 - 69s - loss: 18.0653 - MinusLogProbMetric: 18.0653 - val_loss: 18.0277 - val_MinusLogProbMetric: 18.0277 - lr: 1.1111e-04 - 69s/epoch - 353ms/step
Epoch 502/1000
2023-10-23 22:46:56.985 
Epoch 502/1000 
	 loss: 18.0036, MinusLogProbMetric: 18.0036, val_loss: 18.1859, val_MinusLogProbMetric: 18.1859

Epoch 502: val_loss did not improve from 17.93846
196/196 - 72s - loss: 18.0036 - MinusLogProbMetric: 18.0036 - val_loss: 18.1859 - val_MinusLogProbMetric: 18.1859 - lr: 1.1111e-04 - 72s/epoch - 370ms/step
Epoch 503/1000
2023-10-23 22:48:07.646 
Epoch 503/1000 
	 loss: 18.0200, MinusLogProbMetric: 18.0200, val_loss: 18.1579, val_MinusLogProbMetric: 18.1579

Epoch 503: val_loss did not improve from 17.93846
196/196 - 71s - loss: 18.0200 - MinusLogProbMetric: 18.0200 - val_loss: 18.1579 - val_MinusLogProbMetric: 18.1579 - lr: 1.1111e-04 - 71s/epoch - 360ms/step
Epoch 504/1000
2023-10-23 22:49:17.150 
Epoch 504/1000 
	 loss: 17.9965, MinusLogProbMetric: 17.9965, val_loss: 18.1337, val_MinusLogProbMetric: 18.1337

Epoch 504: val_loss did not improve from 17.93846
196/196 - 70s - loss: 17.9965 - MinusLogProbMetric: 17.9965 - val_loss: 18.1337 - val_MinusLogProbMetric: 18.1337 - lr: 1.1111e-04 - 70s/epoch - 355ms/step
Epoch 505/1000
2023-10-23 22:50:30.290 
Epoch 505/1000 
	 loss: 18.0478, MinusLogProbMetric: 18.0478, val_loss: 18.1765, val_MinusLogProbMetric: 18.1765

Epoch 505: val_loss did not improve from 17.93846
196/196 - 73s - loss: 18.0478 - MinusLogProbMetric: 18.0478 - val_loss: 18.1765 - val_MinusLogProbMetric: 18.1765 - lr: 1.1111e-04 - 73s/epoch - 373ms/step
Epoch 506/1000
2023-10-23 22:51:40.071 
Epoch 506/1000 
	 loss: 18.0997, MinusLogProbMetric: 18.0997, val_loss: 18.3767, val_MinusLogProbMetric: 18.3767

Epoch 506: val_loss did not improve from 17.93846
196/196 - 70s - loss: 18.0997 - MinusLogProbMetric: 18.0997 - val_loss: 18.3767 - val_MinusLogProbMetric: 18.3767 - lr: 1.1111e-04 - 70s/epoch - 356ms/step
Epoch 507/1000
2023-10-23 22:52:50.599 
Epoch 507/1000 
	 loss: 18.0707, MinusLogProbMetric: 18.0707, val_loss: 18.2107, val_MinusLogProbMetric: 18.2107

Epoch 507: val_loss did not improve from 17.93846
196/196 - 71s - loss: 18.0707 - MinusLogProbMetric: 18.0707 - val_loss: 18.2107 - val_MinusLogProbMetric: 18.2107 - lr: 1.1111e-04 - 71s/epoch - 360ms/step
Epoch 508/1000
2023-10-23 22:54:08.728 
Epoch 508/1000 
	 loss: 18.0656, MinusLogProbMetric: 18.0656, val_loss: 18.3380, val_MinusLogProbMetric: 18.3380

Epoch 508: val_loss did not improve from 17.93846
196/196 - 78s - loss: 18.0656 - MinusLogProbMetric: 18.0656 - val_loss: 18.3380 - val_MinusLogProbMetric: 18.3380 - lr: 1.1111e-04 - 78s/epoch - 399ms/step
Epoch 509/1000
2023-10-23 22:55:25.992 
Epoch 509/1000 
	 loss: 18.0176, MinusLogProbMetric: 18.0176, val_loss: 18.0571, val_MinusLogProbMetric: 18.0571

Epoch 509: val_loss did not improve from 17.93846
196/196 - 77s - loss: 18.0176 - MinusLogProbMetric: 18.0176 - val_loss: 18.0571 - val_MinusLogProbMetric: 18.0571 - lr: 1.1111e-04 - 77s/epoch - 394ms/step
Epoch 510/1000
2023-10-23 22:56:43.992 
Epoch 510/1000 
	 loss: 18.0263, MinusLogProbMetric: 18.0263, val_loss: 18.1343, val_MinusLogProbMetric: 18.1343

Epoch 510: val_loss did not improve from 17.93846
196/196 - 78s - loss: 18.0263 - MinusLogProbMetric: 18.0263 - val_loss: 18.1343 - val_MinusLogProbMetric: 18.1343 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 511/1000
2023-10-23 22:58:00.094 
Epoch 511/1000 
	 loss: 18.0221, MinusLogProbMetric: 18.0221, val_loss: 18.6447, val_MinusLogProbMetric: 18.6447

Epoch 511: val_loss did not improve from 17.93846
196/196 - 76s - loss: 18.0221 - MinusLogProbMetric: 18.0221 - val_loss: 18.6447 - val_MinusLogProbMetric: 18.6447 - lr: 1.1111e-04 - 76s/epoch - 388ms/step
Epoch 512/1000
2023-10-23 22:59:17.125 
Epoch 512/1000 
	 loss: 18.0274, MinusLogProbMetric: 18.0274, val_loss: 18.0306, val_MinusLogProbMetric: 18.0306

Epoch 512: val_loss did not improve from 17.93846
196/196 - 77s - loss: 18.0274 - MinusLogProbMetric: 18.0274 - val_loss: 18.0306 - val_MinusLogProbMetric: 18.0306 - lr: 1.1111e-04 - 77s/epoch - 393ms/step
Epoch 513/1000
2023-10-23 23:00:32.701 
Epoch 513/1000 
	 loss: 18.0618, MinusLogProbMetric: 18.0618, val_loss: 18.2080, val_MinusLogProbMetric: 18.2080

Epoch 513: val_loss did not improve from 17.93846
196/196 - 76s - loss: 18.0618 - MinusLogProbMetric: 18.0618 - val_loss: 18.2080 - val_MinusLogProbMetric: 18.2080 - lr: 1.1111e-04 - 76s/epoch - 386ms/step
Epoch 514/1000
2023-10-23 23:01:41.456 
Epoch 514/1000 
	 loss: 18.0079, MinusLogProbMetric: 18.0079, val_loss: 18.0207, val_MinusLogProbMetric: 18.0207

Epoch 514: val_loss did not improve from 17.93846
196/196 - 69s - loss: 18.0079 - MinusLogProbMetric: 18.0079 - val_loss: 18.0207 - val_MinusLogProbMetric: 18.0207 - lr: 1.1111e-04 - 69s/epoch - 351ms/step
Epoch 515/1000
2023-10-23 23:02:51.858 
Epoch 515/1000 
	 loss: 17.9850, MinusLogProbMetric: 17.9850, val_loss: 18.2034, val_MinusLogProbMetric: 18.2034

Epoch 515: val_loss did not improve from 17.93846
196/196 - 70s - loss: 17.9850 - MinusLogProbMetric: 17.9850 - val_loss: 18.2034 - val_MinusLogProbMetric: 18.2034 - lr: 1.1111e-04 - 70s/epoch - 359ms/step
Epoch 516/1000
2023-10-23 23:04:01.643 
Epoch 516/1000 
	 loss: 18.0445, MinusLogProbMetric: 18.0445, val_loss: 18.1547, val_MinusLogProbMetric: 18.1547

Epoch 516: val_loss did not improve from 17.93846
196/196 - 70s - loss: 18.0445 - MinusLogProbMetric: 18.0445 - val_loss: 18.1547 - val_MinusLogProbMetric: 18.1547 - lr: 1.1111e-04 - 70s/epoch - 356ms/step
Epoch 517/1000
2023-10-23 23:05:13.178 
Epoch 517/1000 
	 loss: 17.9848, MinusLogProbMetric: 17.9848, val_loss: 17.8748, val_MinusLogProbMetric: 17.8748

Epoch 517: val_loss improved from 17.93846 to 17.87484, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 73s - loss: 17.9848 - MinusLogProbMetric: 17.9848 - val_loss: 17.8748 - val_MinusLogProbMetric: 17.8748 - lr: 1.1111e-04 - 73s/epoch - 371ms/step
Epoch 518/1000
2023-10-23 23:06:28.252 
Epoch 518/1000 
	 loss: 18.0183, MinusLogProbMetric: 18.0183, val_loss: 18.1581, val_MinusLogProbMetric: 18.1581

Epoch 518: val_loss did not improve from 17.87484
196/196 - 74s - loss: 18.0183 - MinusLogProbMetric: 18.0183 - val_loss: 18.1581 - val_MinusLogProbMetric: 18.1581 - lr: 1.1111e-04 - 74s/epoch - 377ms/step
Epoch 519/1000
2023-10-23 23:07:32.238 
Epoch 519/1000 
	 loss: 18.0575, MinusLogProbMetric: 18.0575, val_loss: 18.1820, val_MinusLogProbMetric: 18.1820

Epoch 519: val_loss did not improve from 17.87484
196/196 - 64s - loss: 18.0575 - MinusLogProbMetric: 18.0575 - val_loss: 18.1820 - val_MinusLogProbMetric: 18.1820 - lr: 1.1111e-04 - 64s/epoch - 326ms/step
Epoch 520/1000
2023-10-23 23:08:37.563 
Epoch 520/1000 
	 loss: 18.0392, MinusLogProbMetric: 18.0392, val_loss: 18.2577, val_MinusLogProbMetric: 18.2577

Epoch 520: val_loss did not improve from 17.87484
196/196 - 65s - loss: 18.0392 - MinusLogProbMetric: 18.0392 - val_loss: 18.2577 - val_MinusLogProbMetric: 18.2577 - lr: 1.1111e-04 - 65s/epoch - 333ms/step
Epoch 521/1000
2023-10-23 23:09:44.013 
Epoch 521/1000 
	 loss: 18.0130, MinusLogProbMetric: 18.0130, val_loss: 18.1013, val_MinusLogProbMetric: 18.1013

Epoch 521: val_loss did not improve from 17.87484
196/196 - 66s - loss: 18.0130 - MinusLogProbMetric: 18.0130 - val_loss: 18.1013 - val_MinusLogProbMetric: 18.1013 - lr: 1.1111e-04 - 66s/epoch - 339ms/step
Epoch 522/1000
2023-10-23 23:10:56.348 
Epoch 522/1000 
	 loss: 17.9902, MinusLogProbMetric: 17.9902, val_loss: 18.2277, val_MinusLogProbMetric: 18.2277

Epoch 522: val_loss did not improve from 17.87484
196/196 - 72s - loss: 17.9902 - MinusLogProbMetric: 17.9902 - val_loss: 18.2277 - val_MinusLogProbMetric: 18.2277 - lr: 1.1111e-04 - 72s/epoch - 369ms/step
Epoch 523/1000
2023-10-23 23:12:00.975 
Epoch 523/1000 
	 loss: 18.0437, MinusLogProbMetric: 18.0437, val_loss: 18.3269, val_MinusLogProbMetric: 18.3269

Epoch 523: val_loss did not improve from 17.87484
196/196 - 65s - loss: 18.0437 - MinusLogProbMetric: 18.0437 - val_loss: 18.3269 - val_MinusLogProbMetric: 18.3269 - lr: 1.1111e-04 - 65s/epoch - 330ms/step
Epoch 524/1000
2023-10-23 23:13:07.347 
Epoch 524/1000 
	 loss: 18.0020, MinusLogProbMetric: 18.0020, val_loss: 18.2445, val_MinusLogProbMetric: 18.2445

Epoch 524: val_loss did not improve from 17.87484
196/196 - 66s - loss: 18.0020 - MinusLogProbMetric: 18.0020 - val_loss: 18.2445 - val_MinusLogProbMetric: 18.2445 - lr: 1.1111e-04 - 66s/epoch - 339ms/step
Epoch 525/1000
2023-10-23 23:14:12.446 
Epoch 525/1000 
	 loss: 18.0471, MinusLogProbMetric: 18.0471, val_loss: 18.3723, val_MinusLogProbMetric: 18.3723

Epoch 525: val_loss did not improve from 17.87484
196/196 - 65s - loss: 18.0471 - MinusLogProbMetric: 18.0471 - val_loss: 18.3723 - val_MinusLogProbMetric: 18.3723 - lr: 1.1111e-04 - 65s/epoch - 332ms/step
Epoch 526/1000
2023-10-23 23:15:19.010 
Epoch 526/1000 
	 loss: 17.9905, MinusLogProbMetric: 17.9905, val_loss: 18.0683, val_MinusLogProbMetric: 18.0683

Epoch 526: val_loss did not improve from 17.87484
196/196 - 67s - loss: 17.9905 - MinusLogProbMetric: 17.9905 - val_loss: 18.0683 - val_MinusLogProbMetric: 18.0683 - lr: 1.1111e-04 - 67s/epoch - 340ms/step
Epoch 527/1000
2023-10-23 23:16:24.924 
Epoch 527/1000 
	 loss: 17.9828, MinusLogProbMetric: 17.9828, val_loss: 18.2094, val_MinusLogProbMetric: 18.2094

Epoch 527: val_loss did not improve from 17.87484
196/196 - 66s - loss: 17.9828 - MinusLogProbMetric: 17.9828 - val_loss: 18.2094 - val_MinusLogProbMetric: 18.2094 - lr: 1.1111e-04 - 66s/epoch - 336ms/step
Epoch 528/1000
2023-10-23 23:17:31.460 
Epoch 528/1000 
	 loss: 17.9817, MinusLogProbMetric: 17.9817, val_loss: 18.1018, val_MinusLogProbMetric: 18.1018

Epoch 528: val_loss did not improve from 17.87484
196/196 - 67s - loss: 17.9817 - MinusLogProbMetric: 17.9817 - val_loss: 18.1018 - val_MinusLogProbMetric: 18.1018 - lr: 1.1111e-04 - 67s/epoch - 339ms/step
Epoch 529/1000
2023-10-23 23:18:36.935 
Epoch 529/1000 
	 loss: 17.9808, MinusLogProbMetric: 17.9808, val_loss: 18.3389, val_MinusLogProbMetric: 18.3389

Epoch 529: val_loss did not improve from 17.87484
196/196 - 65s - loss: 17.9808 - MinusLogProbMetric: 17.9808 - val_loss: 18.3389 - val_MinusLogProbMetric: 18.3389 - lr: 1.1111e-04 - 65s/epoch - 334ms/step
Epoch 530/1000
2023-10-23 23:19:46.100 
Epoch 530/1000 
	 loss: 17.9861, MinusLogProbMetric: 17.9861, val_loss: 18.1776, val_MinusLogProbMetric: 18.1776

Epoch 530: val_loss did not improve from 17.87484
196/196 - 69s - loss: 17.9861 - MinusLogProbMetric: 17.9861 - val_loss: 18.1776 - val_MinusLogProbMetric: 18.1776 - lr: 1.1111e-04 - 69s/epoch - 353ms/step
Epoch 531/1000
2023-10-23 23:20:54.322 
Epoch 531/1000 
	 loss: 17.9719, MinusLogProbMetric: 17.9719, val_loss: 17.9259, val_MinusLogProbMetric: 17.9259

Epoch 531: val_loss did not improve from 17.87484
196/196 - 68s - loss: 17.9719 - MinusLogProbMetric: 17.9719 - val_loss: 17.9259 - val_MinusLogProbMetric: 17.9259 - lr: 1.1111e-04 - 68s/epoch - 348ms/step
Epoch 532/1000
2023-10-23 23:22:01.406 
Epoch 532/1000 
	 loss: 17.9853, MinusLogProbMetric: 17.9853, val_loss: 18.0354, val_MinusLogProbMetric: 18.0354

Epoch 532: val_loss did not improve from 17.87484
196/196 - 67s - loss: 17.9853 - MinusLogProbMetric: 17.9853 - val_loss: 18.0354 - val_MinusLogProbMetric: 18.0354 - lr: 1.1111e-04 - 67s/epoch - 342ms/step
Epoch 533/1000
2023-10-23 23:23:03.246 
Epoch 533/1000 
	 loss: 17.9893, MinusLogProbMetric: 17.9893, val_loss: 18.0556, val_MinusLogProbMetric: 18.0556

Epoch 533: val_loss did not improve from 17.87484
196/196 - 62s - loss: 17.9893 - MinusLogProbMetric: 17.9893 - val_loss: 18.0556 - val_MinusLogProbMetric: 18.0556 - lr: 1.1111e-04 - 62s/epoch - 315ms/step
Epoch 534/1000
2023-10-23 23:24:10.109 
Epoch 534/1000 
	 loss: 18.0050, MinusLogProbMetric: 18.0050, val_loss: 18.0741, val_MinusLogProbMetric: 18.0741

Epoch 534: val_loss did not improve from 17.87484
196/196 - 67s - loss: 18.0050 - MinusLogProbMetric: 18.0050 - val_loss: 18.0741 - val_MinusLogProbMetric: 18.0741 - lr: 1.1111e-04 - 67s/epoch - 341ms/step
Epoch 535/1000
2023-10-23 23:25:19.858 
Epoch 535/1000 
	 loss: 17.9550, MinusLogProbMetric: 17.9550, val_loss: 18.1355, val_MinusLogProbMetric: 18.1355

Epoch 535: val_loss did not improve from 17.87484
196/196 - 70s - loss: 17.9550 - MinusLogProbMetric: 17.9550 - val_loss: 18.1355 - val_MinusLogProbMetric: 18.1355 - lr: 1.1111e-04 - 70s/epoch - 356ms/step
Epoch 536/1000
2023-10-23 23:26:26.530 
Epoch 536/1000 
	 loss: 17.9700, MinusLogProbMetric: 17.9700, val_loss: 18.2837, val_MinusLogProbMetric: 18.2837

Epoch 536: val_loss did not improve from 17.87484
196/196 - 67s - loss: 17.9700 - MinusLogProbMetric: 17.9700 - val_loss: 18.2837 - val_MinusLogProbMetric: 18.2837 - lr: 1.1111e-04 - 67s/epoch - 340ms/step
Epoch 537/1000
2023-10-23 23:27:35.599 
Epoch 537/1000 
	 loss: 17.9885, MinusLogProbMetric: 17.9885, val_loss: 18.2426, val_MinusLogProbMetric: 18.2426

Epoch 537: val_loss did not improve from 17.87484
196/196 - 69s - loss: 17.9885 - MinusLogProbMetric: 17.9885 - val_loss: 18.2426 - val_MinusLogProbMetric: 18.2426 - lr: 1.1111e-04 - 69s/epoch - 352ms/step
Epoch 538/1000
2023-10-23 23:28:45.180 
Epoch 538/1000 
	 loss: 17.9579, MinusLogProbMetric: 17.9579, val_loss: 18.0332, val_MinusLogProbMetric: 18.0332

Epoch 538: val_loss did not improve from 17.87484
196/196 - 70s - loss: 17.9579 - MinusLogProbMetric: 17.9579 - val_loss: 18.0332 - val_MinusLogProbMetric: 18.0332 - lr: 1.1111e-04 - 70s/epoch - 355ms/step
Epoch 539/1000
2023-10-23 23:29:47.544 
Epoch 539/1000 
	 loss: 17.9951, MinusLogProbMetric: 17.9951, val_loss: 17.9238, val_MinusLogProbMetric: 17.9238

Epoch 539: val_loss did not improve from 17.87484
196/196 - 62s - loss: 17.9951 - MinusLogProbMetric: 17.9951 - val_loss: 17.9238 - val_MinusLogProbMetric: 17.9238 - lr: 1.1111e-04 - 62s/epoch - 318ms/step
Epoch 540/1000
2023-10-23 23:30:52.258 
Epoch 540/1000 
	 loss: 17.9363, MinusLogProbMetric: 17.9363, val_loss: 18.2683, val_MinusLogProbMetric: 18.2683

Epoch 540: val_loss did not improve from 17.87484
196/196 - 65s - loss: 17.9363 - MinusLogProbMetric: 17.9363 - val_loss: 18.2683 - val_MinusLogProbMetric: 18.2683 - lr: 1.1111e-04 - 65s/epoch - 330ms/step
Epoch 541/1000
2023-10-23 23:32:04.002 
Epoch 541/1000 
	 loss: 17.9555, MinusLogProbMetric: 17.9555, val_loss: 18.1101, val_MinusLogProbMetric: 18.1101

Epoch 541: val_loss did not improve from 17.87484
196/196 - 72s - loss: 17.9555 - MinusLogProbMetric: 17.9555 - val_loss: 18.1101 - val_MinusLogProbMetric: 18.1101 - lr: 1.1111e-04 - 72s/epoch - 366ms/step
Epoch 542/1000
2023-10-23 23:33:11.921 
Epoch 542/1000 
	 loss: 17.9861, MinusLogProbMetric: 17.9861, val_loss: 18.0317, val_MinusLogProbMetric: 18.0317

Epoch 542: val_loss did not improve from 17.87484
196/196 - 68s - loss: 17.9861 - MinusLogProbMetric: 17.9861 - val_loss: 18.0317 - val_MinusLogProbMetric: 18.0317 - lr: 1.1111e-04 - 68s/epoch - 346ms/step
Epoch 543/1000
2023-10-23 23:34:18.124 
Epoch 543/1000 
	 loss: 17.9822, MinusLogProbMetric: 17.9822, val_loss: 18.2130, val_MinusLogProbMetric: 18.2130

Epoch 543: val_loss did not improve from 17.87484
196/196 - 66s - loss: 17.9822 - MinusLogProbMetric: 17.9822 - val_loss: 18.2130 - val_MinusLogProbMetric: 18.2130 - lr: 1.1111e-04 - 66s/epoch - 338ms/step
Epoch 544/1000
2023-10-23 23:35:31.755 
Epoch 544/1000 
	 loss: 17.9398, MinusLogProbMetric: 17.9398, val_loss: 18.0067, val_MinusLogProbMetric: 18.0067

Epoch 544: val_loss did not improve from 17.87484
196/196 - 74s - loss: 17.9398 - MinusLogProbMetric: 17.9398 - val_loss: 18.0067 - val_MinusLogProbMetric: 18.0067 - lr: 1.1111e-04 - 74s/epoch - 376ms/step
Epoch 545/1000
2023-10-23 23:36:43.099 
Epoch 545/1000 
	 loss: 17.9995, MinusLogProbMetric: 17.9995, val_loss: 18.1421, val_MinusLogProbMetric: 18.1421

Epoch 545: val_loss did not improve from 17.87484
196/196 - 71s - loss: 17.9995 - MinusLogProbMetric: 17.9995 - val_loss: 18.1421 - val_MinusLogProbMetric: 18.1421 - lr: 1.1111e-04 - 71s/epoch - 364ms/step
Epoch 546/1000
2023-10-23 23:37:50.427 
Epoch 546/1000 
	 loss: 17.9177, MinusLogProbMetric: 17.9177, val_loss: 18.0747, val_MinusLogProbMetric: 18.0747

Epoch 546: val_loss did not improve from 17.87484
196/196 - 67s - loss: 17.9177 - MinusLogProbMetric: 17.9177 - val_loss: 18.0747 - val_MinusLogProbMetric: 18.0747 - lr: 1.1111e-04 - 67s/epoch - 344ms/step
Epoch 547/1000
2023-10-23 23:39:02.392 
Epoch 547/1000 
	 loss: 17.9839, MinusLogProbMetric: 17.9839, val_loss: 17.9904, val_MinusLogProbMetric: 17.9904

Epoch 547: val_loss did not improve from 17.87484
196/196 - 72s - loss: 17.9839 - MinusLogProbMetric: 17.9839 - val_loss: 17.9904 - val_MinusLogProbMetric: 17.9904 - lr: 1.1111e-04 - 72s/epoch - 367ms/step
Epoch 548/1000
2023-10-23 23:40:12.037 
Epoch 548/1000 
	 loss: 17.9468, MinusLogProbMetric: 17.9468, val_loss: 17.8192, val_MinusLogProbMetric: 17.8192

Epoch 548: val_loss improved from 17.87484 to 17.81916, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 71s - loss: 17.9468 - MinusLogProbMetric: 17.9468 - val_loss: 17.8192 - val_MinusLogProbMetric: 17.8192 - lr: 1.1111e-04 - 71s/epoch - 361ms/step
Epoch 549/1000
2023-10-23 23:41:25.736 
Epoch 549/1000 
	 loss: 17.9357, MinusLogProbMetric: 17.9357, val_loss: 18.2414, val_MinusLogProbMetric: 18.2414

Epoch 549: val_loss did not improve from 17.81916
196/196 - 73s - loss: 17.9357 - MinusLogProbMetric: 17.9357 - val_loss: 18.2414 - val_MinusLogProbMetric: 18.2414 - lr: 1.1111e-04 - 73s/epoch - 370ms/step
Epoch 550/1000
2023-10-23 23:42:36.387 
Epoch 550/1000 
	 loss: 17.9548, MinusLogProbMetric: 17.9548, val_loss: 18.3584, val_MinusLogProbMetric: 18.3584

Epoch 550: val_loss did not improve from 17.81916
196/196 - 71s - loss: 17.9548 - MinusLogProbMetric: 17.9548 - val_loss: 18.3584 - val_MinusLogProbMetric: 18.3584 - lr: 1.1111e-04 - 71s/epoch - 360ms/step
Epoch 551/1000
2023-10-23 23:43:49.251 
Epoch 551/1000 
	 loss: 17.9262, MinusLogProbMetric: 17.9262, val_loss: 17.8767, val_MinusLogProbMetric: 17.8767

Epoch 551: val_loss did not improve from 17.81916
196/196 - 73s - loss: 17.9262 - MinusLogProbMetric: 17.9262 - val_loss: 17.8767 - val_MinusLogProbMetric: 17.8767 - lr: 1.1111e-04 - 73s/epoch - 372ms/step
Epoch 552/1000
2023-10-23 23:44:58.695 
Epoch 552/1000 
	 loss: 18.0008, MinusLogProbMetric: 18.0008, val_loss: 18.0661, val_MinusLogProbMetric: 18.0661

Epoch 552: val_loss did not improve from 17.81916
196/196 - 69s - loss: 18.0008 - MinusLogProbMetric: 18.0008 - val_loss: 18.0661 - val_MinusLogProbMetric: 18.0661 - lr: 1.1111e-04 - 69s/epoch - 354ms/step
Epoch 553/1000
2023-10-23 23:46:06.594 
Epoch 553/1000 
	 loss: 17.9353, MinusLogProbMetric: 17.9353, val_loss: 17.9660, val_MinusLogProbMetric: 17.9660

Epoch 553: val_loss did not improve from 17.81916
196/196 - 68s - loss: 17.9353 - MinusLogProbMetric: 17.9353 - val_loss: 17.9660 - val_MinusLogProbMetric: 17.9660 - lr: 1.1111e-04 - 68s/epoch - 346ms/step
Epoch 554/1000
2023-10-23 23:47:14.760 
Epoch 554/1000 
	 loss: 17.9672, MinusLogProbMetric: 17.9672, val_loss: 18.1385, val_MinusLogProbMetric: 18.1385

Epoch 554: val_loss did not improve from 17.81916
196/196 - 68s - loss: 17.9672 - MinusLogProbMetric: 17.9672 - val_loss: 18.1385 - val_MinusLogProbMetric: 18.1385 - lr: 1.1111e-04 - 68s/epoch - 348ms/step
Epoch 555/1000
2023-10-23 23:48:22.873 
Epoch 555/1000 
	 loss: 17.9848, MinusLogProbMetric: 17.9848, val_loss: 18.0258, val_MinusLogProbMetric: 18.0258

Epoch 555: val_loss did not improve from 17.81916
196/196 - 68s - loss: 17.9848 - MinusLogProbMetric: 17.9848 - val_loss: 18.0258 - val_MinusLogProbMetric: 18.0258 - lr: 1.1111e-04 - 68s/epoch - 347ms/step
Epoch 556/1000
2023-10-23 23:49:29.457 
Epoch 556/1000 
	 loss: 17.9399, MinusLogProbMetric: 17.9399, val_loss: 18.2058, val_MinusLogProbMetric: 18.2058

Epoch 556: val_loss did not improve from 17.81916
196/196 - 67s - loss: 17.9399 - MinusLogProbMetric: 17.9399 - val_loss: 18.2058 - val_MinusLogProbMetric: 18.2058 - lr: 1.1111e-04 - 67s/epoch - 340ms/step
Epoch 557/1000
2023-10-23 23:50:39.047 
Epoch 557/1000 
	 loss: 17.9598, MinusLogProbMetric: 17.9598, val_loss: 17.8722, val_MinusLogProbMetric: 17.8722

Epoch 557: val_loss did not improve from 17.81916
196/196 - 70s - loss: 17.9598 - MinusLogProbMetric: 17.9598 - val_loss: 17.8722 - val_MinusLogProbMetric: 17.8722 - lr: 1.1111e-04 - 70s/epoch - 355ms/step
Epoch 558/1000
2023-10-23 23:51:49.082 
Epoch 558/1000 
	 loss: 17.9644, MinusLogProbMetric: 17.9644, val_loss: 18.1723, val_MinusLogProbMetric: 18.1723

Epoch 558: val_loss did not improve from 17.81916
196/196 - 70s - loss: 17.9644 - MinusLogProbMetric: 17.9644 - val_loss: 18.1723 - val_MinusLogProbMetric: 18.1723 - lr: 1.1111e-04 - 70s/epoch - 357ms/step
Epoch 559/1000
2023-10-23 23:52:58.643 
Epoch 559/1000 
	 loss: 17.9409, MinusLogProbMetric: 17.9409, val_loss: 18.3712, val_MinusLogProbMetric: 18.3712

Epoch 559: val_loss did not improve from 17.81916
196/196 - 70s - loss: 17.9409 - MinusLogProbMetric: 17.9409 - val_loss: 18.3712 - val_MinusLogProbMetric: 18.3712 - lr: 1.1111e-04 - 70s/epoch - 355ms/step
Epoch 560/1000
2023-10-23 23:54:06.433 
Epoch 560/1000 
	 loss: 17.9631, MinusLogProbMetric: 17.9631, val_loss: 18.0489, val_MinusLogProbMetric: 18.0489

Epoch 560: val_loss did not improve from 17.81916
196/196 - 68s - loss: 17.9631 - MinusLogProbMetric: 17.9631 - val_loss: 18.0489 - val_MinusLogProbMetric: 18.0489 - lr: 1.1111e-04 - 68s/epoch - 346ms/step
Epoch 561/1000
2023-10-23 23:55:14.693 
Epoch 561/1000 
	 loss: 17.8884, MinusLogProbMetric: 17.8884, val_loss: 19.0490, val_MinusLogProbMetric: 19.0490

Epoch 561: val_loss did not improve from 17.81916
196/196 - 68s - loss: 17.8884 - MinusLogProbMetric: 17.8884 - val_loss: 19.0490 - val_MinusLogProbMetric: 19.0490 - lr: 1.1111e-04 - 68s/epoch - 348ms/step
Epoch 562/1000
2023-10-23 23:56:24.131 
Epoch 562/1000 
	 loss: 17.9460, MinusLogProbMetric: 17.9460, val_loss: 18.1736, val_MinusLogProbMetric: 18.1736

Epoch 562: val_loss did not improve from 17.81916
196/196 - 69s - loss: 17.9460 - MinusLogProbMetric: 17.9460 - val_loss: 18.1736 - val_MinusLogProbMetric: 18.1736 - lr: 1.1111e-04 - 69s/epoch - 354ms/step
Epoch 563/1000
2023-10-23 23:57:33.544 
Epoch 563/1000 
	 loss: 17.9260, MinusLogProbMetric: 17.9260, val_loss: 17.9610, val_MinusLogProbMetric: 17.9610

Epoch 563: val_loss did not improve from 17.81916
196/196 - 69s - loss: 17.9260 - MinusLogProbMetric: 17.9260 - val_loss: 17.9610 - val_MinusLogProbMetric: 17.9610 - lr: 1.1111e-04 - 69s/epoch - 354ms/step
Epoch 564/1000
2023-10-23 23:58:42.918 
Epoch 564/1000 
	 loss: 17.8887, MinusLogProbMetric: 17.8887, val_loss: 18.1622, val_MinusLogProbMetric: 18.1622

Epoch 564: val_loss did not improve from 17.81916
196/196 - 69s - loss: 17.8887 - MinusLogProbMetric: 17.8887 - val_loss: 18.1622 - val_MinusLogProbMetric: 18.1622 - lr: 1.1111e-04 - 69s/epoch - 354ms/step
Epoch 565/1000
2023-10-23 23:59:51.143 
Epoch 565/1000 
	 loss: 17.9348, MinusLogProbMetric: 17.9348, val_loss: 17.9396, val_MinusLogProbMetric: 17.9396

Epoch 565: val_loss did not improve from 17.81916
196/196 - 68s - loss: 17.9348 - MinusLogProbMetric: 17.9348 - val_loss: 17.9396 - val_MinusLogProbMetric: 17.9396 - lr: 1.1111e-04 - 68s/epoch - 348ms/step
Epoch 566/1000
2023-10-24 00:01:00.107 
Epoch 566/1000 
	 loss: 17.9221, MinusLogProbMetric: 17.9221, val_loss: 17.9967, val_MinusLogProbMetric: 17.9967

Epoch 566: val_loss did not improve from 17.81916
196/196 - 69s - loss: 17.9221 - MinusLogProbMetric: 17.9221 - val_loss: 17.9967 - val_MinusLogProbMetric: 17.9967 - lr: 1.1111e-04 - 69s/epoch - 352ms/step
Epoch 567/1000
2023-10-24 00:02:08.018 
Epoch 567/1000 
	 loss: 17.8858, MinusLogProbMetric: 17.8858, val_loss: 18.0110, val_MinusLogProbMetric: 18.0110

Epoch 567: val_loss did not improve from 17.81916
196/196 - 68s - loss: 17.8858 - MinusLogProbMetric: 17.8858 - val_loss: 18.0110 - val_MinusLogProbMetric: 18.0110 - lr: 1.1111e-04 - 68s/epoch - 346ms/step
Epoch 568/1000
2023-10-24 00:03:15.299 
Epoch 568/1000 
	 loss: 17.9076, MinusLogProbMetric: 17.9076, val_loss: 17.9314, val_MinusLogProbMetric: 17.9314

Epoch 568: val_loss did not improve from 17.81916
196/196 - 67s - loss: 17.9076 - MinusLogProbMetric: 17.9076 - val_loss: 17.9314 - val_MinusLogProbMetric: 17.9314 - lr: 1.1111e-04 - 67s/epoch - 343ms/step
Epoch 569/1000
2023-10-24 00:04:22.904 
Epoch 569/1000 
	 loss: 17.8798, MinusLogProbMetric: 17.8798, val_loss: 17.7505, val_MinusLogProbMetric: 17.7505

Epoch 569: val_loss improved from 17.81916 to 17.75048, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 69s - loss: 17.8798 - MinusLogProbMetric: 17.8798 - val_loss: 17.7505 - val_MinusLogProbMetric: 17.7505 - lr: 1.1111e-04 - 69s/epoch - 350ms/step
Epoch 570/1000
2023-10-24 00:05:33.139 
Epoch 570/1000 
	 loss: 17.9109, MinusLogProbMetric: 17.9109, val_loss: 18.2493, val_MinusLogProbMetric: 18.2493

Epoch 570: val_loss did not improve from 17.75048
196/196 - 69s - loss: 17.9109 - MinusLogProbMetric: 17.9109 - val_loss: 18.2493 - val_MinusLogProbMetric: 18.2493 - lr: 1.1111e-04 - 69s/epoch - 353ms/step
Epoch 571/1000
2023-10-24 00:06:41.664 
Epoch 571/1000 
	 loss: 17.9165, MinusLogProbMetric: 17.9165, val_loss: 18.1244, val_MinusLogProbMetric: 18.1244

Epoch 571: val_loss did not improve from 17.75048
196/196 - 69s - loss: 17.9165 - MinusLogProbMetric: 17.9165 - val_loss: 18.1244 - val_MinusLogProbMetric: 18.1244 - lr: 1.1111e-04 - 69s/epoch - 350ms/step
Epoch 572/1000
2023-10-24 00:07:49.847 
Epoch 572/1000 
	 loss: 17.9167, MinusLogProbMetric: 17.9167, val_loss: 17.9504, val_MinusLogProbMetric: 17.9504

Epoch 572: val_loss did not improve from 17.75048
196/196 - 68s - loss: 17.9167 - MinusLogProbMetric: 17.9167 - val_loss: 17.9504 - val_MinusLogProbMetric: 17.9504 - lr: 1.1111e-04 - 68s/epoch - 348ms/step
Epoch 573/1000
2023-10-24 00:08:57.925 
Epoch 573/1000 
	 loss: 17.9049, MinusLogProbMetric: 17.9049, val_loss: 18.0843, val_MinusLogProbMetric: 18.0843

Epoch 573: val_loss did not improve from 17.75048
196/196 - 68s - loss: 17.9049 - MinusLogProbMetric: 17.9049 - val_loss: 18.0843 - val_MinusLogProbMetric: 18.0843 - lr: 1.1111e-04 - 68s/epoch - 347ms/step
Epoch 574/1000
2023-10-24 00:10:08.669 
Epoch 574/1000 
	 loss: 17.8837, MinusLogProbMetric: 17.8837, val_loss: 18.0317, val_MinusLogProbMetric: 18.0317

Epoch 574: val_loss did not improve from 17.75048
196/196 - 71s - loss: 17.8837 - MinusLogProbMetric: 17.8837 - val_loss: 18.0317 - val_MinusLogProbMetric: 18.0317 - lr: 1.1111e-04 - 71s/epoch - 361ms/step
Epoch 575/1000
2023-10-24 00:11:17.485 
Epoch 575/1000 
	 loss: 17.9312, MinusLogProbMetric: 17.9312, val_loss: 17.9745, val_MinusLogProbMetric: 17.9745

Epoch 575: val_loss did not improve from 17.75048
196/196 - 69s - loss: 17.9312 - MinusLogProbMetric: 17.9312 - val_loss: 17.9745 - val_MinusLogProbMetric: 17.9745 - lr: 1.1111e-04 - 69s/epoch - 351ms/step
Epoch 576/1000
2023-10-24 00:12:26.464 
Epoch 576/1000 
	 loss: 17.9474, MinusLogProbMetric: 17.9474, val_loss: 18.1964, val_MinusLogProbMetric: 18.1964

Epoch 576: val_loss did not improve from 17.75048
196/196 - 69s - loss: 17.9474 - MinusLogProbMetric: 17.9474 - val_loss: 18.1964 - val_MinusLogProbMetric: 18.1964 - lr: 1.1111e-04 - 69s/epoch - 352ms/step
Epoch 577/1000
2023-10-24 00:13:33.934 
Epoch 577/1000 
	 loss: 17.8879, MinusLogProbMetric: 17.8879, val_loss: 18.5483, val_MinusLogProbMetric: 18.5483

Epoch 577: val_loss did not improve from 17.75048
196/196 - 67s - loss: 17.8879 - MinusLogProbMetric: 17.8879 - val_loss: 18.5483 - val_MinusLogProbMetric: 18.5483 - lr: 1.1111e-04 - 67s/epoch - 344ms/step
Epoch 578/1000
2023-10-24 00:14:42.456 
Epoch 578/1000 
	 loss: 17.9291, MinusLogProbMetric: 17.9291, val_loss: 17.8913, val_MinusLogProbMetric: 17.8913

Epoch 578: val_loss did not improve from 17.75048
196/196 - 69s - loss: 17.9291 - MinusLogProbMetric: 17.9291 - val_loss: 17.8913 - val_MinusLogProbMetric: 17.8913 - lr: 1.1111e-04 - 69s/epoch - 350ms/step
Epoch 579/1000
2023-10-24 00:15:51.729 
Epoch 579/1000 
	 loss: 17.8786, MinusLogProbMetric: 17.8786, val_loss: 18.5011, val_MinusLogProbMetric: 18.5011

Epoch 579: val_loss did not improve from 17.75048
196/196 - 69s - loss: 17.8786 - MinusLogProbMetric: 17.8786 - val_loss: 18.5011 - val_MinusLogProbMetric: 18.5011 - lr: 1.1111e-04 - 69s/epoch - 353ms/step
Epoch 580/1000
2023-10-24 00:17:01.457 
Epoch 580/1000 
	 loss: 17.9115, MinusLogProbMetric: 17.9115, val_loss: 17.8945, val_MinusLogProbMetric: 17.8945

Epoch 580: val_loss did not improve from 17.75048
196/196 - 70s - loss: 17.9115 - MinusLogProbMetric: 17.9115 - val_loss: 17.8945 - val_MinusLogProbMetric: 17.8945 - lr: 1.1111e-04 - 70s/epoch - 356ms/step
Epoch 581/1000
2023-10-24 00:18:12.355 
Epoch 581/1000 
	 loss: 17.9150, MinusLogProbMetric: 17.9150, val_loss: 17.7837, val_MinusLogProbMetric: 17.7837

Epoch 581: val_loss did not improve from 17.75048
196/196 - 71s - loss: 17.9150 - MinusLogProbMetric: 17.9150 - val_loss: 17.7837 - val_MinusLogProbMetric: 17.7837 - lr: 1.1111e-04 - 71s/epoch - 362ms/step
Epoch 582/1000
2023-10-24 00:19:20.953 
Epoch 582/1000 
	 loss: 17.8788, MinusLogProbMetric: 17.8788, val_loss: 17.9462, val_MinusLogProbMetric: 17.9462

Epoch 582: val_loss did not improve from 17.75048
196/196 - 69s - loss: 17.8788 - MinusLogProbMetric: 17.8788 - val_loss: 17.9462 - val_MinusLogProbMetric: 17.9462 - lr: 1.1111e-04 - 69s/epoch - 350ms/step
Epoch 583/1000
2023-10-24 00:20:29.436 
Epoch 583/1000 
	 loss: 17.8945, MinusLogProbMetric: 17.8945, val_loss: 18.0730, val_MinusLogProbMetric: 18.0730

Epoch 583: val_loss did not improve from 17.75048
196/196 - 68s - loss: 17.8945 - MinusLogProbMetric: 17.8945 - val_loss: 18.0730 - val_MinusLogProbMetric: 18.0730 - lr: 1.1111e-04 - 68s/epoch - 349ms/step
Epoch 584/1000
2023-10-24 00:21:38.125 
Epoch 584/1000 
	 loss: 17.9832, MinusLogProbMetric: 17.9832, val_loss: 17.9017, val_MinusLogProbMetric: 17.9017

Epoch 584: val_loss did not improve from 17.75048
196/196 - 69s - loss: 17.9832 - MinusLogProbMetric: 17.9832 - val_loss: 17.9017 - val_MinusLogProbMetric: 17.9017 - lr: 1.1111e-04 - 69s/epoch - 350ms/step
Epoch 585/1000
2023-10-24 00:22:47.562 
Epoch 585/1000 
	 loss: 17.8954, MinusLogProbMetric: 17.8954, val_loss: 18.1091, val_MinusLogProbMetric: 18.1091

Epoch 585: val_loss did not improve from 17.75048
196/196 - 69s - loss: 17.8954 - MinusLogProbMetric: 17.8954 - val_loss: 18.1091 - val_MinusLogProbMetric: 18.1091 - lr: 1.1111e-04 - 69s/epoch - 354ms/step
Epoch 586/1000
2023-10-24 00:23:56.774 
Epoch 586/1000 
	 loss: 17.8549, MinusLogProbMetric: 17.8549, val_loss: 17.9653, val_MinusLogProbMetric: 17.9653

Epoch 586: val_loss did not improve from 17.75048
196/196 - 69s - loss: 17.8549 - MinusLogProbMetric: 17.8549 - val_loss: 17.9653 - val_MinusLogProbMetric: 17.9653 - lr: 1.1111e-04 - 69s/epoch - 353ms/step
Epoch 587/1000
2023-10-24 00:25:09.298 
Epoch 587/1000 
	 loss: 17.8226, MinusLogProbMetric: 17.8226, val_loss: 18.2703, val_MinusLogProbMetric: 18.2703

Epoch 587: val_loss did not improve from 17.75048
196/196 - 73s - loss: 17.8226 - MinusLogProbMetric: 17.8226 - val_loss: 18.2703 - val_MinusLogProbMetric: 18.2703 - lr: 1.1111e-04 - 73s/epoch - 370ms/step
Epoch 588/1000
2023-10-24 00:26:20.784 
Epoch 588/1000 
	 loss: 17.8821, MinusLogProbMetric: 17.8821, val_loss: 17.9231, val_MinusLogProbMetric: 17.9231

Epoch 588: val_loss did not improve from 17.75048
196/196 - 71s - loss: 17.8821 - MinusLogProbMetric: 17.8821 - val_loss: 17.9231 - val_MinusLogProbMetric: 17.9231 - lr: 1.1111e-04 - 71s/epoch - 365ms/step
Epoch 589/1000
2023-10-24 00:27:28.335 
Epoch 589/1000 
	 loss: 17.8904, MinusLogProbMetric: 17.8904, val_loss: 18.0243, val_MinusLogProbMetric: 18.0243

Epoch 589: val_loss did not improve from 17.75048
196/196 - 68s - loss: 17.8904 - MinusLogProbMetric: 17.8904 - val_loss: 18.0243 - val_MinusLogProbMetric: 18.0243 - lr: 1.1111e-04 - 68s/epoch - 345ms/step
Epoch 590/1000
2023-10-24 00:28:39.857 
Epoch 590/1000 
	 loss: 17.8442, MinusLogProbMetric: 17.8442, val_loss: 17.9875, val_MinusLogProbMetric: 17.9875

Epoch 590: val_loss did not improve from 17.75048
196/196 - 72s - loss: 17.8442 - MinusLogProbMetric: 17.8442 - val_loss: 17.9875 - val_MinusLogProbMetric: 17.9875 - lr: 1.1111e-04 - 72s/epoch - 365ms/step
Epoch 591/1000
2023-10-24 00:29:49.633 
Epoch 591/1000 
	 loss: 17.8385, MinusLogProbMetric: 17.8385, val_loss: 17.9989, val_MinusLogProbMetric: 17.9989

Epoch 591: val_loss did not improve from 17.75048
196/196 - 70s - loss: 17.8385 - MinusLogProbMetric: 17.8385 - val_loss: 17.9989 - val_MinusLogProbMetric: 17.9989 - lr: 1.1111e-04 - 70s/epoch - 356ms/step
Epoch 592/1000
2023-10-24 00:31:00.592 
Epoch 592/1000 
	 loss: 17.8496, MinusLogProbMetric: 17.8496, val_loss: 17.8934, val_MinusLogProbMetric: 17.8934

Epoch 592: val_loss did not improve from 17.75048
196/196 - 71s - loss: 17.8496 - MinusLogProbMetric: 17.8496 - val_loss: 17.8934 - val_MinusLogProbMetric: 17.8934 - lr: 1.1111e-04 - 71s/epoch - 362ms/step
Epoch 593/1000
2023-10-24 00:32:11.349 
Epoch 593/1000 
	 loss: 17.8382, MinusLogProbMetric: 17.8382, val_loss: 17.8137, val_MinusLogProbMetric: 17.8137

Epoch 593: val_loss did not improve from 17.75048
196/196 - 71s - loss: 17.8382 - MinusLogProbMetric: 17.8382 - val_loss: 17.8137 - val_MinusLogProbMetric: 17.8137 - lr: 1.1111e-04 - 71s/epoch - 361ms/step
Epoch 594/1000
2023-10-24 00:33:21.724 
Epoch 594/1000 
	 loss: 17.8250, MinusLogProbMetric: 17.8250, val_loss: 17.9360, val_MinusLogProbMetric: 17.9360

Epoch 594: val_loss did not improve from 17.75048
196/196 - 70s - loss: 17.8250 - MinusLogProbMetric: 17.8250 - val_loss: 17.9360 - val_MinusLogProbMetric: 17.9360 - lr: 1.1111e-04 - 70s/epoch - 359ms/step
Epoch 595/1000
2023-10-24 00:34:31.698 
Epoch 595/1000 
	 loss: 17.8218, MinusLogProbMetric: 17.8218, val_loss: 17.8918, val_MinusLogProbMetric: 17.8918

Epoch 595: val_loss did not improve from 17.75048
196/196 - 70s - loss: 17.8218 - MinusLogProbMetric: 17.8218 - val_loss: 17.8918 - val_MinusLogProbMetric: 17.8918 - lr: 1.1111e-04 - 70s/epoch - 357ms/step
Epoch 596/1000
2023-10-24 00:35:39.562 
Epoch 596/1000 
	 loss: 17.8348, MinusLogProbMetric: 17.8348, val_loss: 17.8282, val_MinusLogProbMetric: 17.8282

Epoch 596: val_loss did not improve from 17.75048
196/196 - 68s - loss: 17.8348 - MinusLogProbMetric: 17.8348 - val_loss: 17.8282 - val_MinusLogProbMetric: 17.8282 - lr: 1.1111e-04 - 68s/epoch - 346ms/step
Epoch 597/1000
2023-10-24 00:36:46.694 
Epoch 597/1000 
	 loss: 17.8535, MinusLogProbMetric: 17.8535, val_loss: 17.7669, val_MinusLogProbMetric: 17.7669

Epoch 597: val_loss did not improve from 17.75048
196/196 - 67s - loss: 17.8535 - MinusLogProbMetric: 17.8535 - val_loss: 17.7669 - val_MinusLogProbMetric: 17.7669 - lr: 1.1111e-04 - 67s/epoch - 342ms/step
Epoch 598/1000
2023-10-24 00:37:54.311 
Epoch 598/1000 
	 loss: 17.8739, MinusLogProbMetric: 17.8739, val_loss: 18.0477, val_MinusLogProbMetric: 18.0477

Epoch 598: val_loss did not improve from 17.75048
196/196 - 68s - loss: 17.8739 - MinusLogProbMetric: 17.8739 - val_loss: 18.0477 - val_MinusLogProbMetric: 18.0477 - lr: 1.1111e-04 - 68s/epoch - 345ms/step
Epoch 599/1000
2023-10-24 00:39:03.359 
Epoch 599/1000 
	 loss: 17.8704, MinusLogProbMetric: 17.8704, val_loss: 18.0390, val_MinusLogProbMetric: 18.0390

Epoch 599: val_loss did not improve from 17.75048
196/196 - 69s - loss: 17.8704 - MinusLogProbMetric: 17.8704 - val_loss: 18.0390 - val_MinusLogProbMetric: 18.0390 - lr: 1.1111e-04 - 69s/epoch - 352ms/step
Epoch 600/1000
2023-10-24 00:40:11.908 
Epoch 600/1000 
	 loss: 17.8409, MinusLogProbMetric: 17.8409, val_loss: 17.9050, val_MinusLogProbMetric: 17.9050

Epoch 600: val_loss did not improve from 17.75048
196/196 - 69s - loss: 17.8409 - MinusLogProbMetric: 17.8409 - val_loss: 17.9050 - val_MinusLogProbMetric: 17.9050 - lr: 1.1111e-04 - 69s/epoch - 350ms/step
Epoch 601/1000
2023-10-24 00:41:21.931 
Epoch 601/1000 
	 loss: 17.8617, MinusLogProbMetric: 17.8617, val_loss: 18.0602, val_MinusLogProbMetric: 18.0602

Epoch 601: val_loss did not improve from 17.75048
196/196 - 70s - loss: 17.8617 - MinusLogProbMetric: 17.8617 - val_loss: 18.0602 - val_MinusLogProbMetric: 18.0602 - lr: 1.1111e-04 - 70s/epoch - 357ms/step
Epoch 602/1000
2023-10-24 00:42:32.457 
Epoch 602/1000 
	 loss: 17.9024, MinusLogProbMetric: 17.9024, val_loss: 17.9062, val_MinusLogProbMetric: 17.9062

Epoch 602: val_loss did not improve from 17.75048
196/196 - 71s - loss: 17.9024 - MinusLogProbMetric: 17.9024 - val_loss: 17.9062 - val_MinusLogProbMetric: 17.9062 - lr: 1.1111e-04 - 71s/epoch - 360ms/step
Epoch 603/1000
2023-10-24 00:43:42.072 
Epoch 603/1000 
	 loss: 17.8505, MinusLogProbMetric: 17.8505, val_loss: 18.1828, val_MinusLogProbMetric: 18.1828

Epoch 603: val_loss did not improve from 17.75048
196/196 - 70s - loss: 17.8505 - MinusLogProbMetric: 17.8505 - val_loss: 18.1828 - val_MinusLogProbMetric: 18.1828 - lr: 1.1111e-04 - 70s/epoch - 355ms/step
Epoch 604/1000
2023-10-24 00:44:50.591 
Epoch 604/1000 
	 loss: 17.8278, MinusLogProbMetric: 17.8278, val_loss: 17.9732, val_MinusLogProbMetric: 17.9732

Epoch 604: val_loss did not improve from 17.75048
196/196 - 69s - loss: 17.8278 - MinusLogProbMetric: 17.8278 - val_loss: 17.9732 - val_MinusLogProbMetric: 17.9732 - lr: 1.1111e-04 - 69s/epoch - 350ms/step
Epoch 605/1000
2023-10-24 00:46:00.210 
Epoch 605/1000 
	 loss: 17.8708, MinusLogProbMetric: 17.8708, val_loss: 17.9988, val_MinusLogProbMetric: 17.9988

Epoch 605: val_loss did not improve from 17.75048
196/196 - 70s - loss: 17.8708 - MinusLogProbMetric: 17.8708 - val_loss: 17.9988 - val_MinusLogProbMetric: 17.9988 - lr: 1.1111e-04 - 70s/epoch - 355ms/step
Epoch 606/1000
2023-10-24 00:47:09.823 
Epoch 606/1000 
	 loss: 17.8930, MinusLogProbMetric: 17.8930, val_loss: 17.8808, val_MinusLogProbMetric: 17.8808

Epoch 606: val_loss did not improve from 17.75048
196/196 - 70s - loss: 17.8930 - MinusLogProbMetric: 17.8930 - val_loss: 17.8808 - val_MinusLogProbMetric: 17.8808 - lr: 1.1111e-04 - 70s/epoch - 355ms/step
Epoch 607/1000
2023-10-24 00:48:18.975 
Epoch 607/1000 
	 loss: 17.8475, MinusLogProbMetric: 17.8475, val_loss: 17.7979, val_MinusLogProbMetric: 17.7979

Epoch 607: val_loss did not improve from 17.75048
196/196 - 69s - loss: 17.8475 - MinusLogProbMetric: 17.8475 - val_loss: 17.7979 - val_MinusLogProbMetric: 17.7979 - lr: 1.1111e-04 - 69s/epoch - 353ms/step
Epoch 608/1000
2023-10-24 00:49:27.480 
Epoch 608/1000 
	 loss: 17.8619, MinusLogProbMetric: 17.8619, val_loss: 18.0045, val_MinusLogProbMetric: 18.0045

Epoch 608: val_loss did not improve from 17.75048
196/196 - 69s - loss: 17.8619 - MinusLogProbMetric: 17.8619 - val_loss: 18.0045 - val_MinusLogProbMetric: 18.0045 - lr: 1.1111e-04 - 69s/epoch - 350ms/step
Epoch 609/1000
2023-10-24 00:50:36.446 
Epoch 609/1000 
	 loss: 17.8076, MinusLogProbMetric: 17.8076, val_loss: 17.9580, val_MinusLogProbMetric: 17.9580

Epoch 609: val_loss did not improve from 17.75048
196/196 - 69s - loss: 17.8076 - MinusLogProbMetric: 17.8076 - val_loss: 17.9580 - val_MinusLogProbMetric: 17.9580 - lr: 1.1111e-04 - 69s/epoch - 352ms/step
Epoch 610/1000
2023-10-24 00:51:43.102 
Epoch 610/1000 
	 loss: 17.8209, MinusLogProbMetric: 17.8209, val_loss: 18.1518, val_MinusLogProbMetric: 18.1518

Epoch 610: val_loss did not improve from 17.75048
196/196 - 67s - loss: 17.8209 - MinusLogProbMetric: 17.8209 - val_loss: 18.1518 - val_MinusLogProbMetric: 18.1518 - lr: 1.1111e-04 - 67s/epoch - 340ms/step
Epoch 611/1000
2023-10-24 00:52:50.652 
Epoch 611/1000 
	 loss: 17.8466, MinusLogProbMetric: 17.8466, val_loss: 18.1983, val_MinusLogProbMetric: 18.1983

Epoch 611: val_loss did not improve from 17.75048
196/196 - 68s - loss: 17.8466 - MinusLogProbMetric: 17.8466 - val_loss: 18.1983 - val_MinusLogProbMetric: 18.1983 - lr: 1.1111e-04 - 68s/epoch - 345ms/step
Epoch 612/1000
2023-10-24 00:53:58.716 
Epoch 612/1000 
	 loss: 17.8433, MinusLogProbMetric: 17.8433, val_loss: 18.0025, val_MinusLogProbMetric: 18.0025

Epoch 612: val_loss did not improve from 17.75048
196/196 - 68s - loss: 17.8433 - MinusLogProbMetric: 17.8433 - val_loss: 18.0025 - val_MinusLogProbMetric: 18.0025 - lr: 1.1111e-04 - 68s/epoch - 347ms/step
Epoch 613/1000
2023-10-24 00:55:07.260 
Epoch 613/1000 
	 loss: 17.8243, MinusLogProbMetric: 17.8243, val_loss: 17.8243, val_MinusLogProbMetric: 17.8243

Epoch 613: val_loss did not improve from 17.75048
196/196 - 69s - loss: 17.8243 - MinusLogProbMetric: 17.8243 - val_loss: 17.8243 - val_MinusLogProbMetric: 17.8243 - lr: 1.1111e-04 - 69s/epoch - 350ms/step
Epoch 614/1000
2023-10-24 00:56:16.374 
Epoch 614/1000 
	 loss: 17.8711, MinusLogProbMetric: 17.8711, val_loss: 18.5530, val_MinusLogProbMetric: 18.5530

Epoch 614: val_loss did not improve from 17.75048
196/196 - 69s - loss: 17.8711 - MinusLogProbMetric: 17.8711 - val_loss: 18.5530 - val_MinusLogProbMetric: 18.5530 - lr: 1.1111e-04 - 69s/epoch - 353ms/step
Epoch 615/1000
2023-10-24 00:57:25.495 
Epoch 615/1000 
	 loss: 17.8707, MinusLogProbMetric: 17.8707, val_loss: 17.9400, val_MinusLogProbMetric: 17.9400

Epoch 615: val_loss did not improve from 17.75048
196/196 - 69s - loss: 17.8707 - MinusLogProbMetric: 17.8707 - val_loss: 17.9400 - val_MinusLogProbMetric: 17.9400 - lr: 1.1111e-04 - 69s/epoch - 353ms/step
Epoch 616/1000
2023-10-24 00:58:34.940 
Epoch 616/1000 
	 loss: 17.9036, MinusLogProbMetric: 17.9036, val_loss: 17.8595, val_MinusLogProbMetric: 17.8595

Epoch 616: val_loss did not improve from 17.75048
196/196 - 69s - loss: 17.9036 - MinusLogProbMetric: 17.9036 - val_loss: 17.8595 - val_MinusLogProbMetric: 17.8595 - lr: 1.1111e-04 - 69s/epoch - 354ms/step
Epoch 617/1000
2023-10-24 00:59:42.627 
Epoch 617/1000 
	 loss: 17.8405, MinusLogProbMetric: 17.8405, val_loss: 18.2182, val_MinusLogProbMetric: 18.2182

Epoch 617: val_loss did not improve from 17.75048
196/196 - 68s - loss: 17.8405 - MinusLogProbMetric: 17.8405 - val_loss: 18.2182 - val_MinusLogProbMetric: 18.2182 - lr: 1.1111e-04 - 68s/epoch - 345ms/step
Epoch 618/1000
2023-10-24 01:00:53.199 
Epoch 618/1000 
	 loss: 17.8427, MinusLogProbMetric: 17.8427, val_loss: 17.9069, val_MinusLogProbMetric: 17.9069

Epoch 618: val_loss did not improve from 17.75048
196/196 - 71s - loss: 17.8427 - MinusLogProbMetric: 17.8427 - val_loss: 17.9069 - val_MinusLogProbMetric: 17.9069 - lr: 1.1111e-04 - 71s/epoch - 360ms/step
Epoch 619/1000
2023-10-24 01:02:05.130 
Epoch 619/1000 
	 loss: 17.8641, MinusLogProbMetric: 17.8641, val_loss: 18.1279, val_MinusLogProbMetric: 18.1279

Epoch 619: val_loss did not improve from 17.75048
196/196 - 72s - loss: 17.8641 - MinusLogProbMetric: 17.8641 - val_loss: 18.1279 - val_MinusLogProbMetric: 18.1279 - lr: 1.1111e-04 - 72s/epoch - 367ms/step
Epoch 620/1000
2023-10-24 01:03:15.477 
Epoch 620/1000 
	 loss: 17.5999, MinusLogProbMetric: 17.5999, val_loss: 17.7428, val_MinusLogProbMetric: 17.7428

Epoch 620: val_loss improved from 17.75048 to 17.74280, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 71s - loss: 17.5999 - MinusLogProbMetric: 17.5999 - val_loss: 17.7428 - val_MinusLogProbMetric: 17.7428 - lr: 5.5556e-05 - 71s/epoch - 365ms/step
Epoch 621/1000
2023-10-24 01:04:24.000 
Epoch 621/1000 
	 loss: 17.5947, MinusLogProbMetric: 17.5947, val_loss: 17.7452, val_MinusLogProbMetric: 17.7452

Epoch 621: val_loss did not improve from 17.74280
196/196 - 67s - loss: 17.5947 - MinusLogProbMetric: 17.5947 - val_loss: 17.7452 - val_MinusLogProbMetric: 17.7452 - lr: 5.5556e-05 - 67s/epoch - 344ms/step
Epoch 622/1000
2023-10-24 01:05:32.382 
Epoch 622/1000 
	 loss: 17.6043, MinusLogProbMetric: 17.6043, val_loss: 17.8429, val_MinusLogProbMetric: 17.8429

Epoch 622: val_loss did not improve from 17.74280
196/196 - 68s - loss: 17.6043 - MinusLogProbMetric: 17.6043 - val_loss: 17.8429 - val_MinusLogProbMetric: 17.8429 - lr: 5.5556e-05 - 68s/epoch - 349ms/step
Epoch 623/1000
2023-10-24 01:06:42.461 
Epoch 623/1000 
	 loss: 17.5923, MinusLogProbMetric: 17.5923, val_loss: 17.6760, val_MinusLogProbMetric: 17.6760

Epoch 623: val_loss improved from 17.74280 to 17.67603, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 71s - loss: 17.5923 - MinusLogProbMetric: 17.5923 - val_loss: 17.6760 - val_MinusLogProbMetric: 17.6760 - lr: 5.5556e-05 - 71s/epoch - 364ms/step
Epoch 624/1000
2023-10-24 01:07:53.052 
Epoch 624/1000 
	 loss: 17.5951, MinusLogProbMetric: 17.5951, val_loss: 17.6746, val_MinusLogProbMetric: 17.6746

Epoch 624: val_loss improved from 17.67603 to 17.67464, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 70s - loss: 17.5951 - MinusLogProbMetric: 17.5951 - val_loss: 17.6746 - val_MinusLogProbMetric: 17.6746 - lr: 5.5556e-05 - 70s/epoch - 359ms/step
Epoch 625/1000
2023-10-24 01:09:05.035 
Epoch 625/1000 
	 loss: 17.5822, MinusLogProbMetric: 17.5822, val_loss: 17.6405, val_MinusLogProbMetric: 17.6405

Epoch 625: val_loss improved from 17.67464 to 17.64046, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 72s - loss: 17.5822 - MinusLogProbMetric: 17.5822 - val_loss: 17.6405 - val_MinusLogProbMetric: 17.6405 - lr: 5.5556e-05 - 72s/epoch - 368ms/step
Epoch 626/1000
2023-10-24 01:10:15.854 
Epoch 626/1000 
	 loss: 17.5914, MinusLogProbMetric: 17.5914, val_loss: 17.7176, val_MinusLogProbMetric: 17.7176

Epoch 626: val_loss did not improve from 17.64046
196/196 - 70s - loss: 17.5914 - MinusLogProbMetric: 17.5914 - val_loss: 17.7176 - val_MinusLogProbMetric: 17.7176 - lr: 5.5556e-05 - 70s/epoch - 355ms/step
Epoch 627/1000
2023-10-24 01:11:23.167 
Epoch 627/1000 
	 loss: 17.5991, MinusLogProbMetric: 17.5991, val_loss: 17.6769, val_MinusLogProbMetric: 17.6769

Epoch 627: val_loss did not improve from 17.64046
196/196 - 67s - loss: 17.5991 - MinusLogProbMetric: 17.5991 - val_loss: 17.6769 - val_MinusLogProbMetric: 17.6769 - lr: 5.5556e-05 - 67s/epoch - 343ms/step
Epoch 628/1000
2023-10-24 01:12:33.115 
Epoch 628/1000 
	 loss: 17.5958, MinusLogProbMetric: 17.5958, val_loss: 17.6664, val_MinusLogProbMetric: 17.6664

Epoch 628: val_loss did not improve from 17.64046
196/196 - 70s - loss: 17.5958 - MinusLogProbMetric: 17.5958 - val_loss: 17.6664 - val_MinusLogProbMetric: 17.6664 - lr: 5.5556e-05 - 70s/epoch - 357ms/step
Epoch 629/1000
2023-10-24 01:13:42.826 
Epoch 629/1000 
	 loss: 17.5722, MinusLogProbMetric: 17.5722, val_loss: 17.6347, val_MinusLogProbMetric: 17.6347

Epoch 629: val_loss improved from 17.64046 to 17.63470, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 71s - loss: 17.5722 - MinusLogProbMetric: 17.5722 - val_loss: 17.6347 - val_MinusLogProbMetric: 17.6347 - lr: 5.5556e-05 - 71s/epoch - 362ms/step
Epoch 630/1000
2023-10-24 01:14:54.401 
Epoch 630/1000 
	 loss: 17.5769, MinusLogProbMetric: 17.5769, val_loss: 17.7378, val_MinusLogProbMetric: 17.7378

Epoch 630: val_loss did not improve from 17.63470
196/196 - 70s - loss: 17.5769 - MinusLogProbMetric: 17.5769 - val_loss: 17.7378 - val_MinusLogProbMetric: 17.7378 - lr: 5.5556e-05 - 70s/epoch - 359ms/step
Epoch 631/1000
2023-10-24 01:16:02.861 
Epoch 631/1000 
	 loss: 17.5882, MinusLogProbMetric: 17.5882, val_loss: 17.6998, val_MinusLogProbMetric: 17.6998

Epoch 631: val_loss did not improve from 17.63470
196/196 - 68s - loss: 17.5882 - MinusLogProbMetric: 17.5882 - val_loss: 17.6998 - val_MinusLogProbMetric: 17.6998 - lr: 5.5556e-05 - 68s/epoch - 349ms/step
Epoch 632/1000
2023-10-24 01:17:11.172 
Epoch 632/1000 
	 loss: 17.5914, MinusLogProbMetric: 17.5914, val_loss: 17.5985, val_MinusLogProbMetric: 17.5985

Epoch 632: val_loss improved from 17.63470 to 17.59845, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 70s - loss: 17.5914 - MinusLogProbMetric: 17.5914 - val_loss: 17.5985 - val_MinusLogProbMetric: 17.5985 - lr: 5.5556e-05 - 70s/epoch - 355ms/step
Epoch 633/1000
2023-10-24 01:18:20.703 
Epoch 633/1000 
	 loss: 17.5710, MinusLogProbMetric: 17.5710, val_loss: 17.6612, val_MinusLogProbMetric: 17.6612

Epoch 633: val_loss did not improve from 17.59845
196/196 - 68s - loss: 17.5710 - MinusLogProbMetric: 17.5710 - val_loss: 17.6612 - val_MinusLogProbMetric: 17.6612 - lr: 5.5556e-05 - 68s/epoch - 348ms/step
Epoch 634/1000
2023-10-24 01:19:29.866 
Epoch 634/1000 
	 loss: 17.5710, MinusLogProbMetric: 17.5710, val_loss: 17.6067, val_MinusLogProbMetric: 17.6067

Epoch 634: val_loss did not improve from 17.59845
196/196 - 69s - loss: 17.5710 - MinusLogProbMetric: 17.5710 - val_loss: 17.6067 - val_MinusLogProbMetric: 17.6067 - lr: 5.5556e-05 - 69s/epoch - 353ms/step
Epoch 635/1000
2023-10-24 01:20:39.772 
Epoch 635/1000 
	 loss: 17.5795, MinusLogProbMetric: 17.5795, val_loss: 17.7067, val_MinusLogProbMetric: 17.7067

Epoch 635: val_loss did not improve from 17.59845
196/196 - 70s - loss: 17.5795 - MinusLogProbMetric: 17.5795 - val_loss: 17.7067 - val_MinusLogProbMetric: 17.7067 - lr: 5.5556e-05 - 70s/epoch - 357ms/step
Epoch 636/1000
2023-10-24 01:21:49.860 
Epoch 636/1000 
	 loss: 17.5661, MinusLogProbMetric: 17.5661, val_loss: 17.8883, val_MinusLogProbMetric: 17.8883

Epoch 636: val_loss did not improve from 17.59845
196/196 - 70s - loss: 17.5661 - MinusLogProbMetric: 17.5661 - val_loss: 17.8883 - val_MinusLogProbMetric: 17.8883 - lr: 5.5556e-05 - 70s/epoch - 358ms/step
Epoch 637/1000
2023-10-24 01:22:58.894 
Epoch 637/1000 
	 loss: 17.5866, MinusLogProbMetric: 17.5866, val_loss: 17.6450, val_MinusLogProbMetric: 17.6450

Epoch 637: val_loss did not improve from 17.59845
196/196 - 69s - loss: 17.5866 - MinusLogProbMetric: 17.5866 - val_loss: 17.6450 - val_MinusLogProbMetric: 17.6450 - lr: 5.5556e-05 - 69s/epoch - 352ms/step
Epoch 638/1000
2023-10-24 01:24:08.371 
Epoch 638/1000 
	 loss: 17.5776, MinusLogProbMetric: 17.5776, val_loss: 17.8094, val_MinusLogProbMetric: 17.8094

Epoch 638: val_loss did not improve from 17.59845
196/196 - 69s - loss: 17.5776 - MinusLogProbMetric: 17.5776 - val_loss: 17.8094 - val_MinusLogProbMetric: 17.8094 - lr: 5.5556e-05 - 69s/epoch - 354ms/step
Epoch 639/1000
2023-10-24 01:25:16.861 
Epoch 639/1000 
	 loss: 17.5731, MinusLogProbMetric: 17.5731, val_loss: 17.6108, val_MinusLogProbMetric: 17.6108

Epoch 639: val_loss did not improve from 17.59845
196/196 - 68s - loss: 17.5731 - MinusLogProbMetric: 17.5731 - val_loss: 17.6108 - val_MinusLogProbMetric: 17.6108 - lr: 5.5556e-05 - 68s/epoch - 349ms/step
Epoch 640/1000
2023-10-24 01:26:26.022 
Epoch 640/1000 
	 loss: 17.5805, MinusLogProbMetric: 17.5805, val_loss: 17.7630, val_MinusLogProbMetric: 17.7630

Epoch 640: val_loss did not improve from 17.59845
196/196 - 69s - loss: 17.5805 - MinusLogProbMetric: 17.5805 - val_loss: 17.7630 - val_MinusLogProbMetric: 17.7630 - lr: 5.5556e-05 - 69s/epoch - 353ms/step
Epoch 641/1000
2023-10-24 01:27:36.635 
Epoch 641/1000 
	 loss: 17.5797, MinusLogProbMetric: 17.5797, val_loss: 17.6432, val_MinusLogProbMetric: 17.6432

Epoch 641: val_loss did not improve from 17.59845
196/196 - 71s - loss: 17.5797 - MinusLogProbMetric: 17.5797 - val_loss: 17.6432 - val_MinusLogProbMetric: 17.6432 - lr: 5.5556e-05 - 71s/epoch - 360ms/step
Epoch 642/1000
2023-10-24 01:28:47.640 
Epoch 642/1000 
	 loss: 17.5638, MinusLogProbMetric: 17.5638, val_loss: 17.7016, val_MinusLogProbMetric: 17.7016

Epoch 642: val_loss did not improve from 17.59845
196/196 - 71s - loss: 17.5638 - MinusLogProbMetric: 17.5638 - val_loss: 17.7016 - val_MinusLogProbMetric: 17.7016 - lr: 5.5556e-05 - 71s/epoch - 362ms/step
Epoch 643/1000
2023-10-24 01:29:56.515 
Epoch 643/1000 
	 loss: 17.5800, MinusLogProbMetric: 17.5800, val_loss: 17.7166, val_MinusLogProbMetric: 17.7166

Epoch 643: val_loss did not improve from 17.59845
196/196 - 69s - loss: 17.5800 - MinusLogProbMetric: 17.5800 - val_loss: 17.7166 - val_MinusLogProbMetric: 17.7166 - lr: 5.5556e-05 - 69s/epoch - 351ms/step
Epoch 644/1000
2023-10-24 01:31:06.954 
Epoch 644/1000 
	 loss: 17.5758, MinusLogProbMetric: 17.5758, val_loss: 17.5900, val_MinusLogProbMetric: 17.5900

Epoch 644: val_loss improved from 17.59845 to 17.59005, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 72s - loss: 17.5758 - MinusLogProbMetric: 17.5758 - val_loss: 17.5900 - val_MinusLogProbMetric: 17.5900 - lr: 5.5556e-05 - 72s/epoch - 365ms/step
Epoch 645/1000
2023-10-24 01:32:16.993 
Epoch 645/1000 
	 loss: 17.5799, MinusLogProbMetric: 17.5799, val_loss: 17.6523, val_MinusLogProbMetric: 17.6523

Epoch 645: val_loss did not improve from 17.59005
196/196 - 69s - loss: 17.5799 - MinusLogProbMetric: 17.5799 - val_loss: 17.6523 - val_MinusLogProbMetric: 17.6523 - lr: 5.5556e-05 - 69s/epoch - 351ms/step
Epoch 646/1000
2023-10-24 01:33:26.192 
Epoch 646/1000 
	 loss: 17.5574, MinusLogProbMetric: 17.5574, val_loss: 17.6263, val_MinusLogProbMetric: 17.6263

Epoch 646: val_loss did not improve from 17.59005
196/196 - 69s - loss: 17.5574 - MinusLogProbMetric: 17.5574 - val_loss: 17.6263 - val_MinusLogProbMetric: 17.6263 - lr: 5.5556e-05 - 69s/epoch - 353ms/step
Epoch 647/1000
2023-10-24 01:34:35.397 
Epoch 647/1000 
	 loss: 17.5697, MinusLogProbMetric: 17.5697, val_loss: 17.7711, val_MinusLogProbMetric: 17.7711

Epoch 647: val_loss did not improve from 17.59005
196/196 - 69s - loss: 17.5697 - MinusLogProbMetric: 17.5697 - val_loss: 17.7711 - val_MinusLogProbMetric: 17.7711 - lr: 5.5556e-05 - 69s/epoch - 353ms/step
Epoch 648/1000
2023-10-24 01:35:45.493 
Epoch 648/1000 
	 loss: 17.5628, MinusLogProbMetric: 17.5628, val_loss: 17.5727, val_MinusLogProbMetric: 17.5727

Epoch 648: val_loss improved from 17.59005 to 17.57271, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 71s - loss: 17.5628 - MinusLogProbMetric: 17.5628 - val_loss: 17.5727 - val_MinusLogProbMetric: 17.5727 - lr: 5.5556e-05 - 71s/epoch - 365ms/step
Epoch 649/1000
2023-10-24 01:36:57.352 
Epoch 649/1000 
	 loss: 17.5641, MinusLogProbMetric: 17.5641, val_loss: 17.6796, val_MinusLogProbMetric: 17.6796

Epoch 649: val_loss did not improve from 17.57271
196/196 - 70s - loss: 17.5641 - MinusLogProbMetric: 17.5641 - val_loss: 17.6796 - val_MinusLogProbMetric: 17.6796 - lr: 5.5556e-05 - 70s/epoch - 360ms/step
Epoch 650/1000
2023-10-24 01:38:05.778 
Epoch 650/1000 
	 loss: 17.5670, MinusLogProbMetric: 17.5670, val_loss: 17.6886, val_MinusLogProbMetric: 17.6886

Epoch 650: val_loss did not improve from 17.57271
196/196 - 68s - loss: 17.5670 - MinusLogProbMetric: 17.5670 - val_loss: 17.6886 - val_MinusLogProbMetric: 17.6886 - lr: 5.5556e-05 - 68s/epoch - 349ms/step
Epoch 651/1000
2023-10-24 01:39:13.456 
Epoch 651/1000 
	 loss: 17.5475, MinusLogProbMetric: 17.5475, val_loss: 17.6409, val_MinusLogProbMetric: 17.6409

Epoch 651: val_loss did not improve from 17.57271
196/196 - 68s - loss: 17.5475 - MinusLogProbMetric: 17.5475 - val_loss: 17.6409 - val_MinusLogProbMetric: 17.6409 - lr: 5.5556e-05 - 68s/epoch - 345ms/step
Epoch 652/1000
2023-10-24 01:40:22.642 
Epoch 652/1000 
	 loss: 17.5782, MinusLogProbMetric: 17.5782, val_loss: 17.6776, val_MinusLogProbMetric: 17.6776

Epoch 652: val_loss did not improve from 17.57271
196/196 - 69s - loss: 17.5782 - MinusLogProbMetric: 17.5782 - val_loss: 17.6776 - val_MinusLogProbMetric: 17.6776 - lr: 5.5556e-05 - 69s/epoch - 353ms/step
Epoch 653/1000
2023-10-24 01:41:31.485 
Epoch 653/1000 
	 loss: 17.5534, MinusLogProbMetric: 17.5534, val_loss: 17.5841, val_MinusLogProbMetric: 17.5841

Epoch 653: val_loss did not improve from 17.57271
196/196 - 69s - loss: 17.5534 - MinusLogProbMetric: 17.5534 - val_loss: 17.5841 - val_MinusLogProbMetric: 17.5841 - lr: 5.5556e-05 - 69s/epoch - 351ms/step
Epoch 654/1000
2023-10-24 01:42:39.204 
Epoch 654/1000 
	 loss: 17.5390, MinusLogProbMetric: 17.5390, val_loss: 17.6153, val_MinusLogProbMetric: 17.6153

Epoch 654: val_loss did not improve from 17.57271
196/196 - 68s - loss: 17.5390 - MinusLogProbMetric: 17.5390 - val_loss: 17.6153 - val_MinusLogProbMetric: 17.6153 - lr: 5.5556e-05 - 68s/epoch - 345ms/step
Epoch 655/1000
2023-10-24 01:43:48.076 
Epoch 655/1000 
	 loss: 17.5672, MinusLogProbMetric: 17.5672, val_loss: 17.6218, val_MinusLogProbMetric: 17.6218

Epoch 655: val_loss did not improve from 17.57271
196/196 - 69s - loss: 17.5672 - MinusLogProbMetric: 17.5672 - val_loss: 17.6218 - val_MinusLogProbMetric: 17.6218 - lr: 5.5556e-05 - 69s/epoch - 351ms/step
Epoch 656/1000
2023-10-24 01:44:57.496 
Epoch 656/1000 
	 loss: 17.5447, MinusLogProbMetric: 17.5447, val_loss: 17.6097, val_MinusLogProbMetric: 17.6097

Epoch 656: val_loss did not improve from 17.57271
196/196 - 69s - loss: 17.5447 - MinusLogProbMetric: 17.5447 - val_loss: 17.6097 - val_MinusLogProbMetric: 17.6097 - lr: 5.5556e-05 - 69s/epoch - 354ms/step
Epoch 657/1000
2023-10-24 01:46:07.612 
Epoch 657/1000 
	 loss: 17.5575, MinusLogProbMetric: 17.5575, val_loss: 17.6683, val_MinusLogProbMetric: 17.6683

Epoch 657: val_loss did not improve from 17.57271
196/196 - 70s - loss: 17.5575 - MinusLogProbMetric: 17.5575 - val_loss: 17.6683 - val_MinusLogProbMetric: 17.6683 - lr: 5.5556e-05 - 70s/epoch - 358ms/step
Epoch 658/1000
2023-10-24 01:47:17.851 
Epoch 658/1000 
	 loss: 17.5561, MinusLogProbMetric: 17.5561, val_loss: 17.7003, val_MinusLogProbMetric: 17.7003

Epoch 658: val_loss did not improve from 17.57271
196/196 - 70s - loss: 17.5561 - MinusLogProbMetric: 17.5561 - val_loss: 17.7003 - val_MinusLogProbMetric: 17.7003 - lr: 5.5556e-05 - 70s/epoch - 358ms/step
Epoch 659/1000
2023-10-24 01:48:26.938 
Epoch 659/1000 
	 loss: 17.5494, MinusLogProbMetric: 17.5494, val_loss: 17.6589, val_MinusLogProbMetric: 17.6589

Epoch 659: val_loss did not improve from 17.57271
196/196 - 69s - loss: 17.5494 - MinusLogProbMetric: 17.5494 - val_loss: 17.6589 - val_MinusLogProbMetric: 17.6589 - lr: 5.5556e-05 - 69s/epoch - 352ms/step
Epoch 660/1000
2023-10-24 01:49:35.528 
Epoch 660/1000 
	 loss: 17.5343, MinusLogProbMetric: 17.5343, val_loss: 17.6103, val_MinusLogProbMetric: 17.6103

Epoch 660: val_loss did not improve from 17.57271
196/196 - 69s - loss: 17.5343 - MinusLogProbMetric: 17.5343 - val_loss: 17.6103 - val_MinusLogProbMetric: 17.6103 - lr: 5.5556e-05 - 69s/epoch - 350ms/step
Epoch 661/1000
2023-10-24 01:50:45.940 
Epoch 661/1000 
	 loss: 17.5435, MinusLogProbMetric: 17.5435, val_loss: 17.6788, val_MinusLogProbMetric: 17.6788

Epoch 661: val_loss did not improve from 17.57271
196/196 - 70s - loss: 17.5435 - MinusLogProbMetric: 17.5435 - val_loss: 17.6788 - val_MinusLogProbMetric: 17.6788 - lr: 5.5556e-05 - 70s/epoch - 359ms/step
Epoch 662/1000
2023-10-24 01:51:57.202 
Epoch 662/1000 
	 loss: 17.5431, MinusLogProbMetric: 17.5431, val_loss: 17.6746, val_MinusLogProbMetric: 17.6746

Epoch 662: val_loss did not improve from 17.57271
196/196 - 71s - loss: 17.5431 - MinusLogProbMetric: 17.5431 - val_loss: 17.6746 - val_MinusLogProbMetric: 17.6746 - lr: 5.5556e-05 - 71s/epoch - 364ms/step
Epoch 663/1000
2023-10-24 01:53:07.876 
Epoch 663/1000 
	 loss: 17.5487, MinusLogProbMetric: 17.5487, val_loss: 17.5628, val_MinusLogProbMetric: 17.5628

Epoch 663: val_loss improved from 17.57271 to 17.56282, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 72s - loss: 17.5487 - MinusLogProbMetric: 17.5487 - val_loss: 17.5628 - val_MinusLogProbMetric: 17.5628 - lr: 5.5556e-05 - 72s/epoch - 367ms/step
Epoch 664/1000
2023-10-24 01:54:19.484 
Epoch 664/1000 
	 loss: 17.5599, MinusLogProbMetric: 17.5599, val_loss: 17.5741, val_MinusLogProbMetric: 17.5741

Epoch 664: val_loss did not improve from 17.56282
196/196 - 70s - loss: 17.5599 - MinusLogProbMetric: 17.5599 - val_loss: 17.5741 - val_MinusLogProbMetric: 17.5741 - lr: 5.5556e-05 - 70s/epoch - 359ms/step
Epoch 665/1000
2023-10-24 01:55:29.095 
Epoch 665/1000 
	 loss: 17.5563, MinusLogProbMetric: 17.5563, val_loss: 17.6914, val_MinusLogProbMetric: 17.6914

Epoch 665: val_loss did not improve from 17.56282
196/196 - 70s - loss: 17.5563 - MinusLogProbMetric: 17.5563 - val_loss: 17.6914 - val_MinusLogProbMetric: 17.6914 - lr: 5.5556e-05 - 70s/epoch - 355ms/step
Epoch 666/1000
2023-10-24 01:56:38.147 
Epoch 666/1000 
	 loss: 17.5446, MinusLogProbMetric: 17.5446, val_loss: 17.6454, val_MinusLogProbMetric: 17.6454

Epoch 666: val_loss did not improve from 17.56282
196/196 - 69s - loss: 17.5446 - MinusLogProbMetric: 17.5446 - val_loss: 17.6454 - val_MinusLogProbMetric: 17.6454 - lr: 5.5556e-05 - 69s/epoch - 352ms/step
Epoch 667/1000
2023-10-24 01:57:48.426 
Epoch 667/1000 
	 loss: 17.5721, MinusLogProbMetric: 17.5721, val_loss: 17.6791, val_MinusLogProbMetric: 17.6791

Epoch 667: val_loss did not improve from 17.56282
196/196 - 70s - loss: 17.5721 - MinusLogProbMetric: 17.5721 - val_loss: 17.6791 - val_MinusLogProbMetric: 17.6791 - lr: 5.5556e-05 - 70s/epoch - 359ms/step
Epoch 668/1000
2023-10-24 01:58:56.969 
Epoch 668/1000 
	 loss: 17.5808, MinusLogProbMetric: 17.5808, val_loss: 17.7129, val_MinusLogProbMetric: 17.7129

Epoch 668: val_loss did not improve from 17.56282
196/196 - 69s - loss: 17.5808 - MinusLogProbMetric: 17.5808 - val_loss: 17.7129 - val_MinusLogProbMetric: 17.7129 - lr: 5.5556e-05 - 69s/epoch - 350ms/step
Epoch 669/1000
2023-10-24 02:00:06.363 
Epoch 669/1000 
	 loss: 17.5804, MinusLogProbMetric: 17.5804, val_loss: 17.6041, val_MinusLogProbMetric: 17.6041

Epoch 669: val_loss did not improve from 17.56282
196/196 - 69s - loss: 17.5804 - MinusLogProbMetric: 17.5804 - val_loss: 17.6041 - val_MinusLogProbMetric: 17.6041 - lr: 5.5556e-05 - 69s/epoch - 354ms/step
Epoch 670/1000
2023-10-24 02:01:14.292 
Epoch 670/1000 
	 loss: 17.5566, MinusLogProbMetric: 17.5566, val_loss: 17.5663, val_MinusLogProbMetric: 17.5663

Epoch 670: val_loss did not improve from 17.56282
196/196 - 68s - loss: 17.5566 - MinusLogProbMetric: 17.5566 - val_loss: 17.5663 - val_MinusLogProbMetric: 17.5663 - lr: 5.5556e-05 - 68s/epoch - 347ms/step
Epoch 671/1000
2023-10-24 02:02:24.214 
Epoch 671/1000 
	 loss: 17.5312, MinusLogProbMetric: 17.5312, val_loss: 17.6151, val_MinusLogProbMetric: 17.6151

Epoch 671: val_loss did not improve from 17.56282
196/196 - 70s - loss: 17.5312 - MinusLogProbMetric: 17.5312 - val_loss: 17.6151 - val_MinusLogProbMetric: 17.6151 - lr: 5.5556e-05 - 70s/epoch - 357ms/step
Epoch 672/1000
2023-10-24 02:03:34.383 
Epoch 672/1000 
	 loss: 17.5403, MinusLogProbMetric: 17.5403, val_loss: 17.6790, val_MinusLogProbMetric: 17.6790

Epoch 672: val_loss did not improve from 17.56282
196/196 - 70s - loss: 17.5403 - MinusLogProbMetric: 17.5403 - val_loss: 17.6790 - val_MinusLogProbMetric: 17.6790 - lr: 5.5556e-05 - 70s/epoch - 358ms/step
Epoch 673/1000
2023-10-24 02:04:44.987 
Epoch 673/1000 
	 loss: 17.5433, MinusLogProbMetric: 17.5433, val_loss: 17.6649, val_MinusLogProbMetric: 17.6649

Epoch 673: val_loss did not improve from 17.56282
196/196 - 71s - loss: 17.5433 - MinusLogProbMetric: 17.5433 - val_loss: 17.6649 - val_MinusLogProbMetric: 17.6649 - lr: 5.5556e-05 - 71s/epoch - 360ms/step
Epoch 674/1000
2023-10-24 02:05:55.283 
Epoch 674/1000 
	 loss: 17.5410, MinusLogProbMetric: 17.5410, val_loss: 17.6049, val_MinusLogProbMetric: 17.6049

Epoch 674: val_loss did not improve from 17.56282
196/196 - 70s - loss: 17.5410 - MinusLogProbMetric: 17.5410 - val_loss: 17.6049 - val_MinusLogProbMetric: 17.6049 - lr: 5.5556e-05 - 70s/epoch - 359ms/step
Epoch 675/1000
2023-10-24 02:07:06.153 
Epoch 675/1000 
	 loss: 17.8131, MinusLogProbMetric: 17.8131, val_loss: 17.6549, val_MinusLogProbMetric: 17.6549

Epoch 675: val_loss did not improve from 17.56282
196/196 - 71s - loss: 17.8131 - MinusLogProbMetric: 17.8131 - val_loss: 17.6549 - val_MinusLogProbMetric: 17.6549 - lr: 5.5556e-05 - 71s/epoch - 362ms/step
Epoch 676/1000
2023-10-24 02:08:14.328 
Epoch 676/1000 
	 loss: 17.5326, MinusLogProbMetric: 17.5326, val_loss: 17.5665, val_MinusLogProbMetric: 17.5665

Epoch 676: val_loss did not improve from 17.56282
196/196 - 68s - loss: 17.5326 - MinusLogProbMetric: 17.5326 - val_loss: 17.5665 - val_MinusLogProbMetric: 17.5665 - lr: 5.5556e-05 - 68s/epoch - 348ms/step
Epoch 677/1000
2023-10-24 02:09:22.606 
Epoch 677/1000 
	 loss: 17.5572, MinusLogProbMetric: 17.5572, val_loss: 17.6839, val_MinusLogProbMetric: 17.6839

Epoch 677: val_loss did not improve from 17.56282
196/196 - 68s - loss: 17.5572 - MinusLogProbMetric: 17.5572 - val_loss: 17.6839 - val_MinusLogProbMetric: 17.6839 - lr: 5.5556e-05 - 68s/epoch - 348ms/step
Epoch 678/1000
2023-10-24 02:10:31.911 
Epoch 678/1000 
	 loss: 17.5406, MinusLogProbMetric: 17.5406, val_loss: 17.7203, val_MinusLogProbMetric: 17.7203

Epoch 678: val_loss did not improve from 17.56282
196/196 - 69s - loss: 17.5406 - MinusLogProbMetric: 17.5406 - val_loss: 17.7203 - val_MinusLogProbMetric: 17.7203 - lr: 5.5556e-05 - 69s/epoch - 354ms/step
Epoch 679/1000
2023-10-24 02:11:40.773 
Epoch 679/1000 
	 loss: 17.5607, MinusLogProbMetric: 17.5607, val_loss: 17.6294, val_MinusLogProbMetric: 17.6294

Epoch 679: val_loss did not improve from 17.56282
196/196 - 69s - loss: 17.5607 - MinusLogProbMetric: 17.5607 - val_loss: 17.6294 - val_MinusLogProbMetric: 17.6294 - lr: 5.5556e-05 - 69s/epoch - 351ms/step
Epoch 680/1000
2023-10-24 02:12:49.804 
Epoch 680/1000 
	 loss: 17.5537, MinusLogProbMetric: 17.5537, val_loss: 17.7032, val_MinusLogProbMetric: 17.7032

Epoch 680: val_loss did not improve from 17.56282
196/196 - 69s - loss: 17.5537 - MinusLogProbMetric: 17.5537 - val_loss: 17.7032 - val_MinusLogProbMetric: 17.7032 - lr: 5.5556e-05 - 69s/epoch - 352ms/step
Epoch 681/1000
2023-10-24 02:13:57.204 
Epoch 681/1000 
	 loss: 17.5267, MinusLogProbMetric: 17.5267, val_loss: 17.6053, val_MinusLogProbMetric: 17.6053

Epoch 681: val_loss did not improve from 17.56282
196/196 - 67s - loss: 17.5267 - MinusLogProbMetric: 17.5267 - val_loss: 17.6053 - val_MinusLogProbMetric: 17.6053 - lr: 5.5556e-05 - 67s/epoch - 344ms/step
Epoch 682/1000
2023-10-24 02:15:04.610 
Epoch 682/1000 
	 loss: 17.5390, MinusLogProbMetric: 17.5390, val_loss: 17.5598, val_MinusLogProbMetric: 17.5598

Epoch 682: val_loss improved from 17.56282 to 17.55979, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 68s - loss: 17.5390 - MinusLogProbMetric: 17.5390 - val_loss: 17.5598 - val_MinusLogProbMetric: 17.5598 - lr: 5.5556e-05 - 68s/epoch - 349ms/step
Epoch 683/1000
2023-10-24 02:16:15.136 
Epoch 683/1000 
	 loss: 17.5281, MinusLogProbMetric: 17.5281, val_loss: 17.5796, val_MinusLogProbMetric: 17.5796

Epoch 683: val_loss did not improve from 17.55979
196/196 - 69s - loss: 17.5281 - MinusLogProbMetric: 17.5281 - val_loss: 17.5796 - val_MinusLogProbMetric: 17.5796 - lr: 5.5556e-05 - 69s/epoch - 354ms/step
Epoch 684/1000
2023-10-24 02:17:25.752 
Epoch 684/1000 
	 loss: 17.5387, MinusLogProbMetric: 17.5387, val_loss: 17.6413, val_MinusLogProbMetric: 17.6413

Epoch 684: val_loss did not improve from 17.55979
196/196 - 71s - loss: 17.5387 - MinusLogProbMetric: 17.5387 - val_loss: 17.6413 - val_MinusLogProbMetric: 17.6413 - lr: 5.5556e-05 - 71s/epoch - 360ms/step
Epoch 685/1000
2023-10-24 02:18:34.834 
Epoch 685/1000 
	 loss: 17.5618, MinusLogProbMetric: 17.5618, val_loss: 17.6243, val_MinusLogProbMetric: 17.6243

Epoch 685: val_loss did not improve from 17.55979
196/196 - 69s - loss: 17.5618 - MinusLogProbMetric: 17.5618 - val_loss: 17.6243 - val_MinusLogProbMetric: 17.6243 - lr: 5.5556e-05 - 69s/epoch - 352ms/step
Epoch 686/1000
2023-10-24 02:19:43.499 
Epoch 686/1000 
	 loss: 17.5533, MinusLogProbMetric: 17.5533, val_loss: 17.7545, val_MinusLogProbMetric: 17.7545

Epoch 686: val_loss did not improve from 17.55979
196/196 - 69s - loss: 17.5533 - MinusLogProbMetric: 17.5533 - val_loss: 17.7545 - val_MinusLogProbMetric: 17.7545 - lr: 5.5556e-05 - 69s/epoch - 350ms/step
Epoch 687/1000
2023-10-24 02:20:52.651 
Epoch 687/1000 
	 loss: 17.5527, MinusLogProbMetric: 17.5527, val_loss: 17.6654, val_MinusLogProbMetric: 17.6654

Epoch 687: val_loss did not improve from 17.55979
196/196 - 69s - loss: 17.5527 - MinusLogProbMetric: 17.5527 - val_loss: 17.6654 - val_MinusLogProbMetric: 17.6654 - lr: 5.5556e-05 - 69s/epoch - 353ms/step
Epoch 688/1000
2023-10-24 02:22:01.975 
Epoch 688/1000 
	 loss: 17.5532, MinusLogProbMetric: 17.5532, val_loss: 17.6323, val_MinusLogProbMetric: 17.6323

Epoch 688: val_loss did not improve from 17.55979
196/196 - 69s - loss: 17.5532 - MinusLogProbMetric: 17.5532 - val_loss: 17.6323 - val_MinusLogProbMetric: 17.6323 - lr: 5.5556e-05 - 69s/epoch - 354ms/step
Epoch 689/1000
2023-10-24 02:23:09.003 
Epoch 689/1000 
	 loss: 17.5408, MinusLogProbMetric: 17.5408, val_loss: 17.7023, val_MinusLogProbMetric: 17.7023

Epoch 689: val_loss did not improve from 17.55979
196/196 - 67s - loss: 17.5408 - MinusLogProbMetric: 17.5408 - val_loss: 17.7023 - val_MinusLogProbMetric: 17.7023 - lr: 5.5556e-05 - 67s/epoch - 342ms/step
Epoch 690/1000
2023-10-24 02:24:16.907 
Epoch 690/1000 
	 loss: 17.5685, MinusLogProbMetric: 17.5685, val_loss: 17.5590, val_MinusLogProbMetric: 17.5590

Epoch 690: val_loss improved from 17.55979 to 17.55898, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 69s - loss: 17.5685 - MinusLogProbMetric: 17.5685 - val_loss: 17.5590 - val_MinusLogProbMetric: 17.5590 - lr: 5.5556e-05 - 69s/epoch - 352ms/step
Epoch 691/1000
2023-10-24 02:25:29.478 
Epoch 691/1000 
	 loss: 17.5243, MinusLogProbMetric: 17.5243, val_loss: 17.5991, val_MinusLogProbMetric: 17.5991

Epoch 691: val_loss did not improve from 17.55898
196/196 - 71s - loss: 17.5243 - MinusLogProbMetric: 17.5243 - val_loss: 17.5991 - val_MinusLogProbMetric: 17.5991 - lr: 5.5556e-05 - 71s/epoch - 365ms/step
Epoch 692/1000
2023-10-24 02:26:37.966 
Epoch 692/1000 
	 loss: 17.5513, MinusLogProbMetric: 17.5513, val_loss: 17.7164, val_MinusLogProbMetric: 17.7164

Epoch 692: val_loss did not improve from 17.55898
196/196 - 68s - loss: 17.5513 - MinusLogProbMetric: 17.5513 - val_loss: 17.7164 - val_MinusLogProbMetric: 17.7164 - lr: 5.5556e-05 - 68s/epoch - 349ms/step
Epoch 693/1000
2023-10-24 02:27:49.748 
Epoch 693/1000 
	 loss: 17.5423, MinusLogProbMetric: 17.5423, val_loss: 17.5969, val_MinusLogProbMetric: 17.5969

Epoch 693: val_loss did not improve from 17.55898
196/196 - 72s - loss: 17.5423 - MinusLogProbMetric: 17.5423 - val_loss: 17.5969 - val_MinusLogProbMetric: 17.5969 - lr: 5.5556e-05 - 72s/epoch - 366ms/step
Epoch 694/1000
2023-10-24 02:29:05.029 
Epoch 694/1000 
	 loss: 17.5222, MinusLogProbMetric: 17.5222, val_loss: 17.5678, val_MinusLogProbMetric: 17.5678

Epoch 694: val_loss did not improve from 17.55898
196/196 - 75s - loss: 17.5222 - MinusLogProbMetric: 17.5222 - val_loss: 17.5678 - val_MinusLogProbMetric: 17.5678 - lr: 5.5556e-05 - 75s/epoch - 384ms/step
Epoch 695/1000
2023-10-24 02:30:21.482 
Epoch 695/1000 
	 loss: 17.5394, MinusLogProbMetric: 17.5394, val_loss: 17.6542, val_MinusLogProbMetric: 17.6542

Epoch 695: val_loss did not improve from 17.55898
196/196 - 76s - loss: 17.5394 - MinusLogProbMetric: 17.5394 - val_loss: 17.6542 - val_MinusLogProbMetric: 17.6542 - lr: 5.5556e-05 - 76s/epoch - 390ms/step
Epoch 696/1000
2023-10-24 02:31:35.989 
Epoch 696/1000 
	 loss: 17.5295, MinusLogProbMetric: 17.5295, val_loss: 17.6748, val_MinusLogProbMetric: 17.6748

Epoch 696: val_loss did not improve from 17.55898
196/196 - 75s - loss: 17.5295 - MinusLogProbMetric: 17.5295 - val_loss: 17.6748 - val_MinusLogProbMetric: 17.6748 - lr: 5.5556e-05 - 75s/epoch - 380ms/step
Epoch 697/1000
2023-10-24 02:32:45.136 
Epoch 697/1000 
	 loss: 17.5326, MinusLogProbMetric: 17.5326, val_loss: 17.5370, val_MinusLogProbMetric: 17.5370

Epoch 697: val_loss improved from 17.55898 to 17.53703, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 70s - loss: 17.5326 - MinusLogProbMetric: 17.5326 - val_loss: 17.5370 - val_MinusLogProbMetric: 17.5370 - lr: 5.5556e-05 - 70s/epoch - 359ms/step
Epoch 698/1000
2023-10-24 02:33:48.184 
Epoch 698/1000 
	 loss: 17.5094, MinusLogProbMetric: 17.5094, val_loss: 17.6537, val_MinusLogProbMetric: 17.6537

Epoch 698: val_loss did not improve from 17.53703
196/196 - 62s - loss: 17.5094 - MinusLogProbMetric: 17.5094 - val_loss: 17.6537 - val_MinusLogProbMetric: 17.6537 - lr: 5.5556e-05 - 62s/epoch - 316ms/step
Epoch 699/1000
2023-10-24 02:34:59.097 
Epoch 699/1000 
	 loss: 17.5187, MinusLogProbMetric: 17.5187, val_loss: 17.8423, val_MinusLogProbMetric: 17.8423

Epoch 699: val_loss did not improve from 17.53703
196/196 - 71s - loss: 17.5187 - MinusLogProbMetric: 17.5187 - val_loss: 17.8423 - val_MinusLogProbMetric: 17.8423 - lr: 5.5556e-05 - 71s/epoch - 362ms/step
Epoch 700/1000
2023-10-24 02:36:01.232 
Epoch 700/1000 
	 loss: 17.5433, MinusLogProbMetric: 17.5433, val_loss: 17.6073, val_MinusLogProbMetric: 17.6073

Epoch 700: val_loss did not improve from 17.53703
196/196 - 62s - loss: 17.5433 - MinusLogProbMetric: 17.5433 - val_loss: 17.6073 - val_MinusLogProbMetric: 17.6073 - lr: 5.5556e-05 - 62s/epoch - 317ms/step
Epoch 701/1000
2023-10-24 02:37:07.360 
Epoch 701/1000 
	 loss: 17.5401, MinusLogProbMetric: 17.5401, val_loss: 17.6894, val_MinusLogProbMetric: 17.6894

Epoch 701: val_loss did not improve from 17.53703
196/196 - 66s - loss: 17.5401 - MinusLogProbMetric: 17.5401 - val_loss: 17.6894 - val_MinusLogProbMetric: 17.6894 - lr: 5.5556e-05 - 66s/epoch - 337ms/step
Epoch 702/1000
2023-10-24 02:38:09.156 
Epoch 702/1000 
	 loss: 17.5445, MinusLogProbMetric: 17.5445, val_loss: 17.5972, val_MinusLogProbMetric: 17.5972

Epoch 702: val_loss did not improve from 17.53703
196/196 - 62s - loss: 17.5445 - MinusLogProbMetric: 17.5445 - val_loss: 17.5972 - val_MinusLogProbMetric: 17.5972 - lr: 5.5556e-05 - 62s/epoch - 315ms/step
Epoch 703/1000
2023-10-24 02:39:13.666 
Epoch 703/1000 
	 loss: 17.5132, MinusLogProbMetric: 17.5132, val_loss: 17.5839, val_MinusLogProbMetric: 17.5839

Epoch 703: val_loss did not improve from 17.53703
196/196 - 65s - loss: 17.5132 - MinusLogProbMetric: 17.5132 - val_loss: 17.5839 - val_MinusLogProbMetric: 17.5839 - lr: 5.5556e-05 - 65s/epoch - 329ms/step
Epoch 704/1000
2023-10-24 02:40:22.355 
Epoch 704/1000 
	 loss: 17.5347, MinusLogProbMetric: 17.5347, val_loss: 17.6287, val_MinusLogProbMetric: 17.6287

Epoch 704: val_loss did not improve from 17.53703
196/196 - 69s - loss: 17.5347 - MinusLogProbMetric: 17.5347 - val_loss: 17.6287 - val_MinusLogProbMetric: 17.6287 - lr: 5.5556e-05 - 69s/epoch - 350ms/step
Epoch 705/1000
2023-10-24 02:41:21.375 
Epoch 705/1000 
	 loss: 17.5209, MinusLogProbMetric: 17.5209, val_loss: 17.6341, val_MinusLogProbMetric: 17.6341

Epoch 705: val_loss did not improve from 17.53703
196/196 - 59s - loss: 17.5209 - MinusLogProbMetric: 17.5209 - val_loss: 17.6341 - val_MinusLogProbMetric: 17.6341 - lr: 5.5556e-05 - 59s/epoch - 301ms/step
Epoch 706/1000
2023-10-24 02:42:24.727 
Epoch 706/1000 
	 loss: 17.5112, MinusLogProbMetric: 17.5112, val_loss: 17.6024, val_MinusLogProbMetric: 17.6024

Epoch 706: val_loss did not improve from 17.53703
196/196 - 63s - loss: 17.5112 - MinusLogProbMetric: 17.5112 - val_loss: 17.6024 - val_MinusLogProbMetric: 17.6024 - lr: 5.5556e-05 - 63s/epoch - 323ms/step
Epoch 707/1000
2023-10-24 02:43:25.059 
Epoch 707/1000 
	 loss: 17.5154, MinusLogProbMetric: 17.5154, val_loss: 17.6617, val_MinusLogProbMetric: 17.6617

Epoch 707: val_loss did not improve from 17.53703
196/196 - 60s - loss: 17.5154 - MinusLogProbMetric: 17.5154 - val_loss: 17.6617 - val_MinusLogProbMetric: 17.6617 - lr: 5.5556e-05 - 60s/epoch - 308ms/step
Epoch 708/1000
2023-10-24 02:44:24.300 
Epoch 708/1000 
	 loss: 17.5289, MinusLogProbMetric: 17.5289, val_loss: 17.6633, val_MinusLogProbMetric: 17.6633

Epoch 708: val_loss did not improve from 17.53703
196/196 - 59s - loss: 17.5289 - MinusLogProbMetric: 17.5289 - val_loss: 17.6633 - val_MinusLogProbMetric: 17.6633 - lr: 5.5556e-05 - 59s/epoch - 302ms/step
Epoch 709/1000
2023-10-24 02:45:31.042 
Epoch 709/1000 
	 loss: 17.5254, MinusLogProbMetric: 17.5254, val_loss: 17.5862, val_MinusLogProbMetric: 17.5862

Epoch 709: val_loss did not improve from 17.53703
196/196 - 67s - loss: 17.5254 - MinusLogProbMetric: 17.5254 - val_loss: 17.5862 - val_MinusLogProbMetric: 17.5862 - lr: 5.5556e-05 - 67s/epoch - 341ms/step
Epoch 710/1000
2023-10-24 02:46:34.526 
Epoch 710/1000 
	 loss: 17.4995, MinusLogProbMetric: 17.4995, val_loss: 17.5544, val_MinusLogProbMetric: 17.5544

Epoch 710: val_loss did not improve from 17.53703
196/196 - 63s - loss: 17.4995 - MinusLogProbMetric: 17.4995 - val_loss: 17.5544 - val_MinusLogProbMetric: 17.5544 - lr: 5.5556e-05 - 63s/epoch - 324ms/step
Epoch 711/1000
2023-10-24 02:47:34.002 
Epoch 711/1000 
	 loss: 17.5102, MinusLogProbMetric: 17.5102, val_loss: 17.5684, val_MinusLogProbMetric: 17.5684

Epoch 711: val_loss did not improve from 17.53703
196/196 - 59s - loss: 17.5102 - MinusLogProbMetric: 17.5102 - val_loss: 17.5684 - val_MinusLogProbMetric: 17.5684 - lr: 5.5556e-05 - 59s/epoch - 303ms/step
Epoch 712/1000
2023-10-24 02:48:36.085 
Epoch 712/1000 
	 loss: 17.5045, MinusLogProbMetric: 17.5045, val_loss: 17.6689, val_MinusLogProbMetric: 17.6689

Epoch 712: val_loss did not improve from 17.53703
196/196 - 62s - loss: 17.5045 - MinusLogProbMetric: 17.5045 - val_loss: 17.6689 - val_MinusLogProbMetric: 17.6689 - lr: 5.5556e-05 - 62s/epoch - 317ms/step
Epoch 713/1000
2023-10-24 02:49:39.210 
Epoch 713/1000 
	 loss: 17.5126, MinusLogProbMetric: 17.5126, val_loss: 17.5256, val_MinusLogProbMetric: 17.5256

Epoch 713: val_loss improved from 17.53703 to 17.52564, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 64s - loss: 17.5126 - MinusLogProbMetric: 17.5126 - val_loss: 17.5256 - val_MinusLogProbMetric: 17.5256 - lr: 5.5556e-05 - 64s/epoch - 328ms/step
Epoch 714/1000
2023-10-24 02:50:45.044 
Epoch 714/1000 
	 loss: 17.5006, MinusLogProbMetric: 17.5006, val_loss: 17.7292, val_MinusLogProbMetric: 17.7292

Epoch 714: val_loss did not improve from 17.52564
196/196 - 65s - loss: 17.5006 - MinusLogProbMetric: 17.5006 - val_loss: 17.7292 - val_MinusLogProbMetric: 17.7292 - lr: 5.5556e-05 - 65s/epoch - 330ms/step
Epoch 715/1000
2023-10-24 02:51:46.797 
Epoch 715/1000 
	 loss: 17.5172, MinusLogProbMetric: 17.5172, val_loss: 17.6016, val_MinusLogProbMetric: 17.6016

Epoch 715: val_loss did not improve from 17.52564
196/196 - 62s - loss: 17.5172 - MinusLogProbMetric: 17.5172 - val_loss: 17.6016 - val_MinusLogProbMetric: 17.6016 - lr: 5.5556e-05 - 62s/epoch - 315ms/step
Epoch 716/1000
2023-10-24 02:52:47.652 
Epoch 716/1000 
	 loss: 17.8831, MinusLogProbMetric: 17.8831, val_loss: 18.7694, val_MinusLogProbMetric: 18.7694

Epoch 716: val_loss did not improve from 17.52564
196/196 - 61s - loss: 17.8831 - MinusLogProbMetric: 17.8831 - val_loss: 18.7694 - val_MinusLogProbMetric: 18.7694 - lr: 5.5556e-05 - 61s/epoch - 310ms/step
Epoch 717/1000
2023-10-24 02:53:51.958 
Epoch 717/1000 
	 loss: 17.5867, MinusLogProbMetric: 17.5867, val_loss: 17.5819, val_MinusLogProbMetric: 17.5819

Epoch 717: val_loss did not improve from 17.52564
196/196 - 64s - loss: 17.5867 - MinusLogProbMetric: 17.5867 - val_loss: 17.5819 - val_MinusLogProbMetric: 17.5819 - lr: 5.5556e-05 - 64s/epoch - 328ms/step
Epoch 718/1000
2023-10-24 02:54:57.432 
Epoch 718/1000 
	 loss: 17.5075, MinusLogProbMetric: 17.5075, val_loss: 17.6005, val_MinusLogProbMetric: 17.6005

Epoch 718: val_loss did not improve from 17.52564
196/196 - 65s - loss: 17.5075 - MinusLogProbMetric: 17.5075 - val_loss: 17.6005 - val_MinusLogProbMetric: 17.6005 - lr: 5.5556e-05 - 65s/epoch - 334ms/step
Epoch 719/1000
2023-10-24 02:56:03.480 
Epoch 719/1000 
	 loss: 17.5059, MinusLogProbMetric: 17.5059, val_loss: 17.6031, val_MinusLogProbMetric: 17.6031

Epoch 719: val_loss did not improve from 17.52564
196/196 - 66s - loss: 17.5059 - MinusLogProbMetric: 17.5059 - val_loss: 17.6031 - val_MinusLogProbMetric: 17.6031 - lr: 5.5556e-05 - 66s/epoch - 337ms/step
Epoch 720/1000
2023-10-24 02:57:06.678 
Epoch 720/1000 
	 loss: 17.5277, MinusLogProbMetric: 17.5277, val_loss: 17.6425, val_MinusLogProbMetric: 17.6425

Epoch 720: val_loss did not improve from 17.52564
196/196 - 63s - loss: 17.5277 - MinusLogProbMetric: 17.5277 - val_loss: 17.6425 - val_MinusLogProbMetric: 17.6425 - lr: 5.5556e-05 - 63s/epoch - 322ms/step
Epoch 721/1000
2023-10-24 02:58:11.101 
Epoch 721/1000 
	 loss: 17.5278, MinusLogProbMetric: 17.5278, val_loss: 17.6703, val_MinusLogProbMetric: 17.6703

Epoch 721: val_loss did not improve from 17.52564
196/196 - 64s - loss: 17.5278 - MinusLogProbMetric: 17.5278 - val_loss: 17.6703 - val_MinusLogProbMetric: 17.6703 - lr: 5.5556e-05 - 64s/epoch - 329ms/step
Epoch 722/1000
2023-10-24 02:59:10.768 
Epoch 722/1000 
	 loss: 17.5105, MinusLogProbMetric: 17.5105, val_loss: 17.6259, val_MinusLogProbMetric: 17.6259

Epoch 722: val_loss did not improve from 17.52564
196/196 - 60s - loss: 17.5105 - MinusLogProbMetric: 17.5105 - val_loss: 17.6259 - val_MinusLogProbMetric: 17.6259 - lr: 5.5556e-05 - 60s/epoch - 304ms/step
Epoch 723/1000
2023-10-24 03:00:15.943 
Epoch 723/1000 
	 loss: 17.5052, MinusLogProbMetric: 17.5052, val_loss: 17.7393, val_MinusLogProbMetric: 17.7393

Epoch 723: val_loss did not improve from 17.52564
196/196 - 65s - loss: 17.5052 - MinusLogProbMetric: 17.5052 - val_loss: 17.7393 - val_MinusLogProbMetric: 17.7393 - lr: 5.5556e-05 - 65s/epoch - 333ms/step
Epoch 724/1000
2023-10-24 03:01:16.641 
Epoch 724/1000 
	 loss: 17.5289, MinusLogProbMetric: 17.5289, val_loss: 17.6876, val_MinusLogProbMetric: 17.6876

Epoch 724: val_loss did not improve from 17.52564
196/196 - 61s - loss: 17.5289 - MinusLogProbMetric: 17.5289 - val_loss: 17.6876 - val_MinusLogProbMetric: 17.6876 - lr: 5.5556e-05 - 61s/epoch - 310ms/step
Epoch 725/1000
2023-10-24 03:02:19.776 
Epoch 725/1000 
	 loss: 17.5022, MinusLogProbMetric: 17.5022, val_loss: 17.6286, val_MinusLogProbMetric: 17.6286

Epoch 725: val_loss did not improve from 17.52564
196/196 - 63s - loss: 17.5022 - MinusLogProbMetric: 17.5022 - val_loss: 17.6286 - val_MinusLogProbMetric: 17.6286 - lr: 5.5556e-05 - 63s/epoch - 322ms/step
Epoch 726/1000
2023-10-24 03:03:21.409 
Epoch 726/1000 
	 loss: 17.5141, MinusLogProbMetric: 17.5141, val_loss: 17.7713, val_MinusLogProbMetric: 17.7713

Epoch 726: val_loss did not improve from 17.52564
196/196 - 62s - loss: 17.5141 - MinusLogProbMetric: 17.5141 - val_loss: 17.7713 - val_MinusLogProbMetric: 17.7713 - lr: 5.5556e-05 - 62s/epoch - 314ms/step
Epoch 727/1000
2023-10-24 03:04:27.308 
Epoch 727/1000 
	 loss: 17.5286, MinusLogProbMetric: 17.5286, val_loss: 17.6054, val_MinusLogProbMetric: 17.6054

Epoch 727: val_loss did not improve from 17.52564
196/196 - 66s - loss: 17.5286 - MinusLogProbMetric: 17.5286 - val_loss: 17.6054 - val_MinusLogProbMetric: 17.6054 - lr: 5.5556e-05 - 66s/epoch - 336ms/step
Epoch 728/1000
2023-10-24 03:05:30.475 
Epoch 728/1000 
	 loss: 17.5032, MinusLogProbMetric: 17.5032, val_loss: 17.6328, val_MinusLogProbMetric: 17.6328

Epoch 728: val_loss did not improve from 17.52564
196/196 - 63s - loss: 17.5032 - MinusLogProbMetric: 17.5032 - val_loss: 17.6328 - val_MinusLogProbMetric: 17.6328 - lr: 5.5556e-05 - 63s/epoch - 322ms/step
Epoch 729/1000
2023-10-24 03:06:32.655 
Epoch 729/1000 
	 loss: 17.4929, MinusLogProbMetric: 17.4929, val_loss: 17.5756, val_MinusLogProbMetric: 17.5756

Epoch 729: val_loss did not improve from 17.52564
196/196 - 62s - loss: 17.4929 - MinusLogProbMetric: 17.4929 - val_loss: 17.5756 - val_MinusLogProbMetric: 17.5756 - lr: 5.5556e-05 - 62s/epoch - 317ms/step
Epoch 730/1000
2023-10-24 03:07:34.772 
Epoch 730/1000 
	 loss: 17.4918, MinusLogProbMetric: 17.4918, val_loss: 17.6955, val_MinusLogProbMetric: 17.6955

Epoch 730: val_loss did not improve from 17.52564
196/196 - 62s - loss: 17.4918 - MinusLogProbMetric: 17.4918 - val_loss: 17.6955 - val_MinusLogProbMetric: 17.6955 - lr: 5.5556e-05 - 62s/epoch - 317ms/step
Epoch 731/1000
2023-10-24 03:08:35.389 
Epoch 731/1000 
	 loss: 17.4906, MinusLogProbMetric: 17.4906, val_loss: 17.6232, val_MinusLogProbMetric: 17.6232

Epoch 731: val_loss did not improve from 17.52564
196/196 - 61s - loss: 17.4906 - MinusLogProbMetric: 17.4906 - val_loss: 17.6232 - val_MinusLogProbMetric: 17.6232 - lr: 5.5556e-05 - 61s/epoch - 309ms/step
Epoch 732/1000
2023-10-24 03:09:38.062 
Epoch 732/1000 
	 loss: 17.4988, MinusLogProbMetric: 17.4988, val_loss: 17.6305, val_MinusLogProbMetric: 17.6305

Epoch 732: val_loss did not improve from 17.52564
196/196 - 63s - loss: 17.4988 - MinusLogProbMetric: 17.4988 - val_loss: 17.6305 - val_MinusLogProbMetric: 17.6305 - lr: 5.5556e-05 - 63s/epoch - 320ms/step
Epoch 733/1000
2023-10-24 03:10:44.420 
Epoch 733/1000 
	 loss: 17.5001, MinusLogProbMetric: 17.5001, val_loss: 17.5166, val_MinusLogProbMetric: 17.5166

Epoch 733: val_loss improved from 17.52564 to 17.51661, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 68s - loss: 17.5001 - MinusLogProbMetric: 17.5001 - val_loss: 17.5166 - val_MinusLogProbMetric: 17.5166 - lr: 5.5556e-05 - 68s/epoch - 345ms/step
Epoch 734/1000
2023-10-24 03:11:49.473 
Epoch 734/1000 
	 loss: 17.5000, MinusLogProbMetric: 17.5000, val_loss: 17.6022, val_MinusLogProbMetric: 17.6022

Epoch 734: val_loss did not improve from 17.51661
196/196 - 64s - loss: 17.5000 - MinusLogProbMetric: 17.5000 - val_loss: 17.6022 - val_MinusLogProbMetric: 17.6022 - lr: 5.5556e-05 - 64s/epoch - 326ms/step
Epoch 735/1000
2023-10-24 03:12:53.263 
Epoch 735/1000 
	 loss: 17.4938, MinusLogProbMetric: 17.4938, val_loss: 17.5076, val_MinusLogProbMetric: 17.5076

Epoch 735: val_loss improved from 17.51661 to 17.50758, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 65s - loss: 17.4938 - MinusLogProbMetric: 17.4938 - val_loss: 17.5076 - val_MinusLogProbMetric: 17.5076 - lr: 5.5556e-05 - 65s/epoch - 331ms/step
Epoch 736/1000
2023-10-24 03:13:57.390 
Epoch 736/1000 
	 loss: 17.4771, MinusLogProbMetric: 17.4771, val_loss: 17.5704, val_MinusLogProbMetric: 17.5704

Epoch 736: val_loss did not improve from 17.50758
196/196 - 63s - loss: 17.4771 - MinusLogProbMetric: 17.4771 - val_loss: 17.5704 - val_MinusLogProbMetric: 17.5704 - lr: 5.5556e-05 - 63s/epoch - 322ms/step
Epoch 737/1000
2023-10-24 03:15:01.182 
Epoch 737/1000 
	 loss: 17.5116, MinusLogProbMetric: 17.5116, val_loss: 17.5306, val_MinusLogProbMetric: 17.5306

Epoch 737: val_loss did not improve from 17.50758
196/196 - 64s - loss: 17.5116 - MinusLogProbMetric: 17.5116 - val_loss: 17.5306 - val_MinusLogProbMetric: 17.5306 - lr: 5.5556e-05 - 64s/epoch - 325ms/step
Epoch 738/1000
2023-10-24 03:16:06.150 
Epoch 738/1000 
	 loss: 17.4963, MinusLogProbMetric: 17.4963, val_loss: 17.6135, val_MinusLogProbMetric: 17.6135

Epoch 738: val_loss did not improve from 17.50758
196/196 - 65s - loss: 17.4963 - MinusLogProbMetric: 17.4963 - val_loss: 17.6135 - val_MinusLogProbMetric: 17.6135 - lr: 5.5556e-05 - 65s/epoch - 331ms/step
Epoch 739/1000
2023-10-24 03:17:05.781 
Epoch 739/1000 
	 loss: 17.5043, MinusLogProbMetric: 17.5043, val_loss: 17.6214, val_MinusLogProbMetric: 17.6214

Epoch 739: val_loss did not improve from 17.50758
196/196 - 60s - loss: 17.5043 - MinusLogProbMetric: 17.5043 - val_loss: 17.6214 - val_MinusLogProbMetric: 17.6214 - lr: 5.5556e-05 - 60s/epoch - 304ms/step
Epoch 740/1000
2023-10-24 03:18:05.402 
Epoch 740/1000 
	 loss: 17.5188, MinusLogProbMetric: 17.5188, val_loss: 17.6235, val_MinusLogProbMetric: 17.6235

Epoch 740: val_loss did not improve from 17.50758
196/196 - 60s - loss: 17.5188 - MinusLogProbMetric: 17.5188 - val_loss: 17.6235 - val_MinusLogProbMetric: 17.6235 - lr: 5.5556e-05 - 60s/epoch - 304ms/step
Epoch 741/1000
2023-10-24 03:19:13.649 
Epoch 741/1000 
	 loss: 17.5109, MinusLogProbMetric: 17.5109, val_loss: 17.5590, val_MinusLogProbMetric: 17.5590

Epoch 741: val_loss did not improve from 17.50758
196/196 - 68s - loss: 17.5109 - MinusLogProbMetric: 17.5109 - val_loss: 17.5590 - val_MinusLogProbMetric: 17.5590 - lr: 5.5556e-05 - 68s/epoch - 348ms/step
Epoch 742/1000
2023-10-24 03:20:21.582 
Epoch 742/1000 
	 loss: 17.4875, MinusLogProbMetric: 17.4875, val_loss: 17.5376, val_MinusLogProbMetric: 17.5376

Epoch 742: val_loss did not improve from 17.50758
196/196 - 68s - loss: 17.4875 - MinusLogProbMetric: 17.4875 - val_loss: 17.5376 - val_MinusLogProbMetric: 17.5376 - lr: 5.5556e-05 - 68s/epoch - 347ms/step
Epoch 743/1000
2023-10-24 03:21:23.402 
Epoch 743/1000 
	 loss: 17.4844, MinusLogProbMetric: 17.4844, val_loss: 17.5657, val_MinusLogProbMetric: 17.5657

Epoch 743: val_loss did not improve from 17.50758
196/196 - 62s - loss: 17.4844 - MinusLogProbMetric: 17.4844 - val_loss: 17.5657 - val_MinusLogProbMetric: 17.5657 - lr: 5.5556e-05 - 62s/epoch - 315ms/step
Epoch 744/1000
2023-10-24 03:22:29.753 
Epoch 744/1000 
	 loss: 17.4976, MinusLogProbMetric: 17.4976, val_loss: 17.7247, val_MinusLogProbMetric: 17.7247

Epoch 744: val_loss did not improve from 17.50758
196/196 - 66s - loss: 17.4976 - MinusLogProbMetric: 17.4976 - val_loss: 17.7247 - val_MinusLogProbMetric: 17.7247 - lr: 5.5556e-05 - 66s/epoch - 339ms/step
Epoch 745/1000
2023-10-24 03:23:30.486 
Epoch 745/1000 
	 loss: 17.5152, MinusLogProbMetric: 17.5152, val_loss: 17.5568, val_MinusLogProbMetric: 17.5568

Epoch 745: val_loss did not improve from 17.50758
196/196 - 61s - loss: 17.5152 - MinusLogProbMetric: 17.5152 - val_loss: 17.5568 - val_MinusLogProbMetric: 17.5568 - lr: 5.5556e-05 - 61s/epoch - 310ms/step
Epoch 746/1000
2023-10-24 03:24:34.259 
Epoch 746/1000 
	 loss: 17.4932, MinusLogProbMetric: 17.4932, val_loss: 17.5188, val_MinusLogProbMetric: 17.5188

Epoch 746: val_loss did not improve from 17.50758
196/196 - 64s - loss: 17.4932 - MinusLogProbMetric: 17.4932 - val_loss: 17.5188 - val_MinusLogProbMetric: 17.5188 - lr: 5.5556e-05 - 64s/epoch - 325ms/step
Epoch 747/1000
2023-10-24 03:25:34.176 
Epoch 747/1000 
	 loss: 17.4829, MinusLogProbMetric: 17.4829, val_loss: 17.7203, val_MinusLogProbMetric: 17.7203

Epoch 747: val_loss did not improve from 17.50758
196/196 - 60s - loss: 17.4829 - MinusLogProbMetric: 17.4829 - val_loss: 17.7203 - val_MinusLogProbMetric: 17.7203 - lr: 5.5556e-05 - 60s/epoch - 306ms/step
Epoch 748/1000
2023-10-24 03:26:33.593 
Epoch 748/1000 
	 loss: 17.5399, MinusLogProbMetric: 17.5399, val_loss: 17.5128, val_MinusLogProbMetric: 17.5128

Epoch 748: val_loss did not improve from 17.50758
196/196 - 59s - loss: 17.5399 - MinusLogProbMetric: 17.5399 - val_loss: 17.5128 - val_MinusLogProbMetric: 17.5128 - lr: 5.5556e-05 - 59s/epoch - 303ms/step
Epoch 749/1000
2023-10-24 03:27:36.506 
Epoch 749/1000 
	 loss: 17.4905, MinusLogProbMetric: 17.4905, val_loss: 17.5631, val_MinusLogProbMetric: 17.5631

Epoch 749: val_loss did not improve from 17.50758
196/196 - 63s - loss: 17.4905 - MinusLogProbMetric: 17.4905 - val_loss: 17.5631 - val_MinusLogProbMetric: 17.5631 - lr: 5.5556e-05 - 63s/epoch - 321ms/step
Epoch 750/1000
2023-10-24 03:28:37.779 
Epoch 750/1000 
	 loss: 17.5073, MinusLogProbMetric: 17.5073, val_loss: 17.5917, val_MinusLogProbMetric: 17.5917

Epoch 750: val_loss did not improve from 17.50758
196/196 - 61s - loss: 17.5073 - MinusLogProbMetric: 17.5073 - val_loss: 17.5917 - val_MinusLogProbMetric: 17.5917 - lr: 5.5556e-05 - 61s/epoch - 313ms/step
Epoch 751/1000
2023-10-24 03:29:36.727 
Epoch 751/1000 
	 loss: 17.4887, MinusLogProbMetric: 17.4887, val_loss: 17.5065, val_MinusLogProbMetric: 17.5065

Epoch 751: val_loss improved from 17.50758 to 17.50652, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 60s - loss: 17.4887 - MinusLogProbMetric: 17.4887 - val_loss: 17.5065 - val_MinusLogProbMetric: 17.5065 - lr: 5.5556e-05 - 60s/epoch - 307ms/step
Epoch 752/1000
2023-10-24 03:30:37.901 
Epoch 752/1000 
	 loss: 17.5019, MinusLogProbMetric: 17.5019, val_loss: 17.6247, val_MinusLogProbMetric: 17.6247

Epoch 752: val_loss did not improve from 17.50652
196/196 - 60s - loss: 17.5019 - MinusLogProbMetric: 17.5019 - val_loss: 17.6247 - val_MinusLogProbMetric: 17.6247 - lr: 5.5556e-05 - 60s/epoch - 306ms/step
Epoch 753/1000
2023-10-24 03:31:40.675 
Epoch 753/1000 
	 loss: 17.4967, MinusLogProbMetric: 17.4967, val_loss: 17.5291, val_MinusLogProbMetric: 17.5291

Epoch 753: val_loss did not improve from 17.50652
196/196 - 63s - loss: 17.4967 - MinusLogProbMetric: 17.4967 - val_loss: 17.5291 - val_MinusLogProbMetric: 17.5291 - lr: 5.5556e-05 - 63s/epoch - 320ms/step
Epoch 754/1000
2023-10-24 03:32:42.439 
Epoch 754/1000 
	 loss: 17.5454, MinusLogProbMetric: 17.5454, val_loss: 17.6131, val_MinusLogProbMetric: 17.6131

Epoch 754: val_loss did not improve from 17.50652
196/196 - 62s - loss: 17.5454 - MinusLogProbMetric: 17.5454 - val_loss: 17.6131 - val_MinusLogProbMetric: 17.6131 - lr: 5.5556e-05 - 62s/epoch - 315ms/step
Epoch 755/1000
2023-10-24 03:33:44.338 
Epoch 755/1000 
	 loss: 17.4806, MinusLogProbMetric: 17.4806, val_loss: 17.5373, val_MinusLogProbMetric: 17.5373

Epoch 755: val_loss did not improve from 17.50652
196/196 - 62s - loss: 17.4806 - MinusLogProbMetric: 17.4806 - val_loss: 17.5373 - val_MinusLogProbMetric: 17.5373 - lr: 5.5556e-05 - 62s/epoch - 316ms/step
Epoch 756/1000
2023-10-24 03:34:44.318 
Epoch 756/1000 
	 loss: 17.4844, MinusLogProbMetric: 17.4844, val_loss: 17.5361, val_MinusLogProbMetric: 17.5361

Epoch 756: val_loss did not improve from 17.50652
196/196 - 60s - loss: 17.4844 - MinusLogProbMetric: 17.4844 - val_loss: 17.5361 - val_MinusLogProbMetric: 17.5361 - lr: 5.5556e-05 - 60s/epoch - 306ms/step
Epoch 757/1000
2023-10-24 03:35:43.840 
Epoch 757/1000 
	 loss: 17.4988, MinusLogProbMetric: 17.4988, val_loss: 17.5745, val_MinusLogProbMetric: 17.5745

Epoch 757: val_loss did not improve from 17.50652
196/196 - 60s - loss: 17.4988 - MinusLogProbMetric: 17.4988 - val_loss: 17.5745 - val_MinusLogProbMetric: 17.5745 - lr: 5.5556e-05 - 60s/epoch - 304ms/step
Epoch 758/1000
2023-10-24 03:36:49.198 
Epoch 758/1000 
	 loss: 17.4863, MinusLogProbMetric: 17.4863, val_loss: 17.5693, val_MinusLogProbMetric: 17.5693

Epoch 758: val_loss did not improve from 17.50652
196/196 - 65s - loss: 17.4863 - MinusLogProbMetric: 17.4863 - val_loss: 17.5693 - val_MinusLogProbMetric: 17.5693 - lr: 5.5556e-05 - 65s/epoch - 333ms/step
Epoch 759/1000
2023-10-24 03:37:47.745 
Epoch 759/1000 
	 loss: 17.4878, MinusLogProbMetric: 17.4878, val_loss: 17.5247, val_MinusLogProbMetric: 17.5247

Epoch 759: val_loss did not improve from 17.50652
196/196 - 59s - loss: 17.4878 - MinusLogProbMetric: 17.4878 - val_loss: 17.5247 - val_MinusLogProbMetric: 17.5247 - lr: 5.5556e-05 - 59s/epoch - 299ms/step
Epoch 760/1000
2023-10-24 03:38:53.439 
Epoch 760/1000 
	 loss: 17.4765, MinusLogProbMetric: 17.4765, val_loss: 17.6685, val_MinusLogProbMetric: 17.6685

Epoch 760: val_loss did not improve from 17.50652
196/196 - 66s - loss: 17.4765 - MinusLogProbMetric: 17.4765 - val_loss: 17.6685 - val_MinusLogProbMetric: 17.6685 - lr: 5.5556e-05 - 66s/epoch - 335ms/step
Epoch 761/1000
2023-10-24 03:39:57.898 
Epoch 761/1000 
	 loss: 17.5023, MinusLogProbMetric: 17.5023, val_loss: 17.6156, val_MinusLogProbMetric: 17.6156

Epoch 761: val_loss did not improve from 17.50652
196/196 - 64s - loss: 17.5023 - MinusLogProbMetric: 17.5023 - val_loss: 17.6156 - val_MinusLogProbMetric: 17.6156 - lr: 5.5556e-05 - 64s/epoch - 329ms/step
Epoch 762/1000
2023-10-24 03:40:59.868 
Epoch 762/1000 
	 loss: 17.4934, MinusLogProbMetric: 17.4934, val_loss: 17.9020, val_MinusLogProbMetric: 17.9020

Epoch 762: val_loss did not improve from 17.50652
196/196 - 62s - loss: 17.4934 - MinusLogProbMetric: 17.4934 - val_loss: 17.9020 - val_MinusLogProbMetric: 17.9020 - lr: 5.5556e-05 - 62s/epoch - 316ms/step
Epoch 763/1000
2023-10-24 03:42:01.559 
Epoch 763/1000 
	 loss: 17.5133, MinusLogProbMetric: 17.5133, val_loss: 17.5483, val_MinusLogProbMetric: 17.5483

Epoch 763: val_loss did not improve from 17.50652
196/196 - 62s - loss: 17.5133 - MinusLogProbMetric: 17.5133 - val_loss: 17.5483 - val_MinusLogProbMetric: 17.5483 - lr: 5.5556e-05 - 62s/epoch - 315ms/step
Epoch 764/1000
2023-10-24 03:43:00.997 
Epoch 764/1000 
	 loss: 17.4688, MinusLogProbMetric: 17.4688, val_loss: 17.6098, val_MinusLogProbMetric: 17.6098

Epoch 764: val_loss did not improve from 17.50652
196/196 - 59s - loss: 17.4688 - MinusLogProbMetric: 17.4688 - val_loss: 17.6098 - val_MinusLogProbMetric: 17.6098 - lr: 5.5556e-05 - 59s/epoch - 303ms/step
Epoch 765/1000
2023-10-24 03:44:07.744 
Epoch 765/1000 
	 loss: 17.5308, MinusLogProbMetric: 17.5308, val_loss: 17.5727, val_MinusLogProbMetric: 17.5727

Epoch 765: val_loss did not improve from 17.50652
196/196 - 67s - loss: 17.5308 - MinusLogProbMetric: 17.5308 - val_loss: 17.5727 - val_MinusLogProbMetric: 17.5727 - lr: 5.5556e-05 - 67s/epoch - 341ms/step
Epoch 766/1000
2023-10-24 03:45:09.715 
Epoch 766/1000 
	 loss: 17.4872, MinusLogProbMetric: 17.4872, val_loss: 17.5523, val_MinusLogProbMetric: 17.5523

Epoch 766: val_loss did not improve from 17.50652
196/196 - 62s - loss: 17.4872 - MinusLogProbMetric: 17.4872 - val_loss: 17.5523 - val_MinusLogProbMetric: 17.5523 - lr: 5.5556e-05 - 62s/epoch - 316ms/step
Epoch 767/1000
2023-10-24 03:46:09.255 
Epoch 767/1000 
	 loss: 17.4632, MinusLogProbMetric: 17.4632, val_loss: 17.6502, val_MinusLogProbMetric: 17.6502

Epoch 767: val_loss did not improve from 17.50652
196/196 - 60s - loss: 17.4632 - MinusLogProbMetric: 17.4632 - val_loss: 17.6502 - val_MinusLogProbMetric: 17.6502 - lr: 5.5556e-05 - 60s/epoch - 304ms/step
Epoch 768/1000
2023-10-24 03:47:16.903 
Epoch 768/1000 
	 loss: 17.4775, MinusLogProbMetric: 17.4775, val_loss: 17.5248, val_MinusLogProbMetric: 17.5248

Epoch 768: val_loss did not improve from 17.50652
196/196 - 68s - loss: 17.4775 - MinusLogProbMetric: 17.4775 - val_loss: 17.5248 - val_MinusLogProbMetric: 17.5248 - lr: 5.5556e-05 - 68s/epoch - 345ms/step
Epoch 769/1000
2023-10-24 03:48:19.280 
Epoch 769/1000 
	 loss: 17.5042, MinusLogProbMetric: 17.5042, val_loss: 17.5959, val_MinusLogProbMetric: 17.5959

Epoch 769: val_loss did not improve from 17.50652
196/196 - 62s - loss: 17.5042 - MinusLogProbMetric: 17.5042 - val_loss: 17.5959 - val_MinusLogProbMetric: 17.5959 - lr: 5.5556e-05 - 62s/epoch - 318ms/step
Epoch 770/1000
2023-10-24 03:49:27.093 
Epoch 770/1000 
	 loss: 17.4897, MinusLogProbMetric: 17.4897, val_loss: 17.6692, val_MinusLogProbMetric: 17.6692

Epoch 770: val_loss did not improve from 17.50652
196/196 - 68s - loss: 17.4897 - MinusLogProbMetric: 17.4897 - val_loss: 17.6692 - val_MinusLogProbMetric: 17.6692 - lr: 5.5556e-05 - 68s/epoch - 346ms/step
Epoch 771/1000
2023-10-24 03:50:31.489 
Epoch 771/1000 
	 loss: 17.4645, MinusLogProbMetric: 17.4645, val_loss: 17.4997, val_MinusLogProbMetric: 17.4997

Epoch 771: val_loss improved from 17.50652 to 17.49974, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 66s - loss: 17.4645 - MinusLogProbMetric: 17.4645 - val_loss: 17.4997 - val_MinusLogProbMetric: 17.4997 - lr: 5.5556e-05 - 66s/epoch - 335ms/step
Epoch 772/1000
2023-10-24 03:51:36.266 
Epoch 772/1000 
	 loss: 17.4863, MinusLogProbMetric: 17.4863, val_loss: 17.5880, val_MinusLogProbMetric: 17.5880

Epoch 772: val_loss did not improve from 17.49974
196/196 - 63s - loss: 17.4863 - MinusLogProbMetric: 17.4863 - val_loss: 17.5880 - val_MinusLogProbMetric: 17.5880 - lr: 5.5556e-05 - 63s/epoch - 324ms/step
Epoch 773/1000
2023-10-24 03:52:40.090 
Epoch 773/1000 
	 loss: 17.4701, MinusLogProbMetric: 17.4701, val_loss: 17.7307, val_MinusLogProbMetric: 17.7307

Epoch 773: val_loss did not improve from 17.49974
196/196 - 64s - loss: 17.4701 - MinusLogProbMetric: 17.4701 - val_loss: 17.7307 - val_MinusLogProbMetric: 17.7307 - lr: 5.5556e-05 - 64s/epoch - 326ms/step
Epoch 774/1000
2023-10-24 03:53:46.095 
Epoch 774/1000 
	 loss: 17.4687, MinusLogProbMetric: 17.4687, val_loss: 17.5476, val_MinusLogProbMetric: 17.5476

Epoch 774: val_loss did not improve from 17.49974
196/196 - 66s - loss: 17.4687 - MinusLogProbMetric: 17.4687 - val_loss: 17.5476 - val_MinusLogProbMetric: 17.5476 - lr: 5.5556e-05 - 66s/epoch - 337ms/step
Epoch 775/1000
2023-10-24 03:54:49.528 
Epoch 775/1000 
	 loss: 17.4935, MinusLogProbMetric: 17.4935, val_loss: 17.5675, val_MinusLogProbMetric: 17.5675

Epoch 775: val_loss did not improve from 17.49974
196/196 - 63s - loss: 17.4935 - MinusLogProbMetric: 17.4935 - val_loss: 17.5675 - val_MinusLogProbMetric: 17.5675 - lr: 5.5556e-05 - 63s/epoch - 324ms/step
Epoch 776/1000
2023-10-24 03:55:49.286 
Epoch 776/1000 
	 loss: 17.4691, MinusLogProbMetric: 17.4691, val_loss: 17.5181, val_MinusLogProbMetric: 17.5181

Epoch 776: val_loss did not improve from 17.49974
196/196 - 60s - loss: 17.4691 - MinusLogProbMetric: 17.4691 - val_loss: 17.5181 - val_MinusLogProbMetric: 17.5181 - lr: 5.5556e-05 - 60s/epoch - 305ms/step
Epoch 777/1000
2023-10-24 03:56:52.670 
Epoch 777/1000 
	 loss: 17.4940, MinusLogProbMetric: 17.4940, val_loss: 17.6810, val_MinusLogProbMetric: 17.6810

Epoch 777: val_loss did not improve from 17.49974
196/196 - 63s - loss: 17.4940 - MinusLogProbMetric: 17.4940 - val_loss: 17.6810 - val_MinusLogProbMetric: 17.6810 - lr: 5.5556e-05 - 63s/epoch - 323ms/step
Epoch 778/1000
2023-10-24 03:57:58.046 
Epoch 778/1000 
	 loss: 17.4657, MinusLogProbMetric: 17.4657, val_loss: 17.5206, val_MinusLogProbMetric: 17.5206

Epoch 778: val_loss did not improve from 17.49974
196/196 - 65s - loss: 17.4657 - MinusLogProbMetric: 17.4657 - val_loss: 17.5206 - val_MinusLogProbMetric: 17.5206 - lr: 5.5556e-05 - 65s/epoch - 334ms/step
Epoch 779/1000
2023-10-24 03:59:11.313 
Epoch 779/1000 
	 loss: 17.5197, MinusLogProbMetric: 17.5197, val_loss: 17.5568, val_MinusLogProbMetric: 17.5568

Epoch 779: val_loss did not improve from 17.49974
196/196 - 73s - loss: 17.5197 - MinusLogProbMetric: 17.5197 - val_loss: 17.5568 - val_MinusLogProbMetric: 17.5568 - lr: 5.5556e-05 - 73s/epoch - 374ms/step
Epoch 780/1000
2023-10-24 04:00:14.503 
Epoch 780/1000 
	 loss: 17.4681, MinusLogProbMetric: 17.4681, val_loss: 17.5159, val_MinusLogProbMetric: 17.5159

Epoch 780: val_loss did not improve from 17.49974
196/196 - 63s - loss: 17.4681 - MinusLogProbMetric: 17.4681 - val_loss: 17.5159 - val_MinusLogProbMetric: 17.5159 - lr: 5.5556e-05 - 63s/epoch - 322ms/step
Epoch 781/1000
2023-10-24 04:01:15.826 
Epoch 781/1000 
	 loss: 17.4735, MinusLogProbMetric: 17.4735, val_loss: 17.5172, val_MinusLogProbMetric: 17.5172

Epoch 781: val_loss did not improve from 17.49974
196/196 - 61s - loss: 17.4735 - MinusLogProbMetric: 17.4735 - val_loss: 17.5172 - val_MinusLogProbMetric: 17.5172 - lr: 5.5556e-05 - 61s/epoch - 313ms/step
Epoch 782/1000
2023-10-24 04:02:19.465 
Epoch 782/1000 
	 loss: 17.4648, MinusLogProbMetric: 17.4648, val_loss: 17.5195, val_MinusLogProbMetric: 17.5195

Epoch 782: val_loss did not improve from 17.49974
196/196 - 64s - loss: 17.4648 - MinusLogProbMetric: 17.4648 - val_loss: 17.5195 - val_MinusLogProbMetric: 17.5195 - lr: 5.5556e-05 - 64s/epoch - 325ms/step
Epoch 783/1000
2023-10-24 04:03:21.606 
Epoch 783/1000 
	 loss: 17.4643, MinusLogProbMetric: 17.4643, val_loss: 17.5556, val_MinusLogProbMetric: 17.5556

Epoch 783: val_loss did not improve from 17.49974
196/196 - 62s - loss: 17.4643 - MinusLogProbMetric: 17.4643 - val_loss: 17.5556 - val_MinusLogProbMetric: 17.5556 - lr: 5.5556e-05 - 62s/epoch - 317ms/step
Epoch 784/1000
2023-10-24 04:04:22.178 
Epoch 784/1000 
	 loss: 17.4922, MinusLogProbMetric: 17.4922, val_loss: 17.5562, val_MinusLogProbMetric: 17.5562

Epoch 784: val_loss did not improve from 17.49974
196/196 - 61s - loss: 17.4922 - MinusLogProbMetric: 17.4922 - val_loss: 17.5562 - val_MinusLogProbMetric: 17.5562 - lr: 5.5556e-05 - 61s/epoch - 309ms/step
Epoch 785/1000
2023-10-24 04:05:26.238 
Epoch 785/1000 
	 loss: 17.4601, MinusLogProbMetric: 17.4601, val_loss: 17.5913, val_MinusLogProbMetric: 17.5913

Epoch 785: val_loss did not improve from 17.49974
196/196 - 64s - loss: 17.4601 - MinusLogProbMetric: 17.4601 - val_loss: 17.5913 - val_MinusLogProbMetric: 17.5913 - lr: 5.5556e-05 - 64s/epoch - 327ms/step
Epoch 786/1000
2023-10-24 04:06:29.751 
Epoch 786/1000 
	 loss: 17.4514, MinusLogProbMetric: 17.4514, val_loss: 17.6130, val_MinusLogProbMetric: 17.6130

Epoch 786: val_loss did not improve from 17.49974
196/196 - 64s - loss: 17.4514 - MinusLogProbMetric: 17.4514 - val_loss: 17.6130 - val_MinusLogProbMetric: 17.6130 - lr: 5.5556e-05 - 64s/epoch - 324ms/step
Epoch 787/1000
2023-10-24 04:07:33.292 
Epoch 787/1000 
	 loss: 17.4776, MinusLogProbMetric: 17.4776, val_loss: 17.4918, val_MinusLogProbMetric: 17.4918

Epoch 787: val_loss improved from 17.49974 to 17.49183, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 65s - loss: 17.4776 - MinusLogProbMetric: 17.4776 - val_loss: 17.4918 - val_MinusLogProbMetric: 17.4918 - lr: 5.5556e-05 - 65s/epoch - 329ms/step
Epoch 788/1000
2023-10-24 04:08:35.037 
Epoch 788/1000 
	 loss: 17.4916, MinusLogProbMetric: 17.4916, val_loss: 17.5011, val_MinusLogProbMetric: 17.5011

Epoch 788: val_loss did not improve from 17.49183
196/196 - 61s - loss: 17.4916 - MinusLogProbMetric: 17.4916 - val_loss: 17.5011 - val_MinusLogProbMetric: 17.5011 - lr: 5.5556e-05 - 61s/epoch - 310ms/step
Epoch 789/1000
2023-10-24 04:09:36.736 
Epoch 789/1000 
	 loss: 17.4810, MinusLogProbMetric: 17.4810, val_loss: 17.5025, val_MinusLogProbMetric: 17.5025

Epoch 789: val_loss did not improve from 17.49183
196/196 - 62s - loss: 17.4810 - MinusLogProbMetric: 17.4810 - val_loss: 17.5025 - val_MinusLogProbMetric: 17.5025 - lr: 5.5556e-05 - 62s/epoch - 315ms/step
Epoch 790/1000
2023-10-24 04:10:36.762 
Epoch 790/1000 
	 loss: 17.4559, MinusLogProbMetric: 17.4559, val_loss: 17.6808, val_MinusLogProbMetric: 17.6808

Epoch 790: val_loss did not improve from 17.49183
196/196 - 60s - loss: 17.4559 - MinusLogProbMetric: 17.4559 - val_loss: 17.6808 - val_MinusLogProbMetric: 17.6808 - lr: 5.5556e-05 - 60s/epoch - 306ms/step
Epoch 791/1000
2023-10-24 04:11:43.528 
Epoch 791/1000 
	 loss: 17.4497, MinusLogProbMetric: 17.4497, val_loss: 17.7066, val_MinusLogProbMetric: 17.7066

Epoch 791: val_loss did not improve from 17.49183
196/196 - 67s - loss: 17.4497 - MinusLogProbMetric: 17.4497 - val_loss: 17.7066 - val_MinusLogProbMetric: 17.7066 - lr: 5.5556e-05 - 67s/epoch - 341ms/step
Epoch 792/1000
2023-10-24 04:12:46.557 
Epoch 792/1000 
	 loss: 17.4757, MinusLogProbMetric: 17.4757, val_loss: 17.5647, val_MinusLogProbMetric: 17.5647

Epoch 792: val_loss did not improve from 17.49183
196/196 - 63s - loss: 17.4757 - MinusLogProbMetric: 17.4757 - val_loss: 17.5647 - val_MinusLogProbMetric: 17.5647 - lr: 5.5556e-05 - 63s/epoch - 322ms/step
Epoch 793/1000
2023-10-24 04:13:53.954 
Epoch 793/1000 
	 loss: 17.4554, MinusLogProbMetric: 17.4554, val_loss: 17.6808, val_MinusLogProbMetric: 17.6808

Epoch 793: val_loss did not improve from 17.49183
196/196 - 67s - loss: 17.4554 - MinusLogProbMetric: 17.4554 - val_loss: 17.6808 - val_MinusLogProbMetric: 17.6808 - lr: 5.5556e-05 - 67s/epoch - 344ms/step
Epoch 794/1000
2023-10-24 04:14:54.982 
Epoch 794/1000 
	 loss: 17.4699, MinusLogProbMetric: 17.4699, val_loss: 17.4909, val_MinusLogProbMetric: 17.4909

Epoch 794: val_loss improved from 17.49183 to 17.49087, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 62s - loss: 17.4699 - MinusLogProbMetric: 17.4699 - val_loss: 17.4909 - val_MinusLogProbMetric: 17.4909 - lr: 5.5556e-05 - 62s/epoch - 317ms/step
Epoch 795/1000
2023-10-24 04:16:00.833 
Epoch 795/1000 
	 loss: 17.4889, MinusLogProbMetric: 17.4889, val_loss: 17.5219, val_MinusLogProbMetric: 17.5219

Epoch 795: val_loss did not improve from 17.49087
196/196 - 65s - loss: 17.4889 - MinusLogProbMetric: 17.4889 - val_loss: 17.5219 - val_MinusLogProbMetric: 17.5219 - lr: 5.5556e-05 - 65s/epoch - 330ms/step
Epoch 796/1000
2023-10-24 04:17:03.144 
Epoch 796/1000 
	 loss: 17.4785, MinusLogProbMetric: 17.4785, val_loss: 17.5636, val_MinusLogProbMetric: 17.5636

Epoch 796: val_loss did not improve from 17.49087
196/196 - 62s - loss: 17.4785 - MinusLogProbMetric: 17.4785 - val_loss: 17.5636 - val_MinusLogProbMetric: 17.5636 - lr: 5.5556e-05 - 62s/epoch - 318ms/step
Epoch 797/1000
2023-10-24 04:18:04.803 
Epoch 797/1000 
	 loss: 17.4570, MinusLogProbMetric: 17.4570, val_loss: 17.4498, val_MinusLogProbMetric: 17.4498

Epoch 797: val_loss improved from 17.49087 to 17.44977, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 63s - loss: 17.4570 - MinusLogProbMetric: 17.4570 - val_loss: 17.4498 - val_MinusLogProbMetric: 17.4498 - lr: 5.5556e-05 - 63s/epoch - 321ms/step
Epoch 798/1000
2023-10-24 04:19:14.715 
Epoch 798/1000 
	 loss: 17.4491, MinusLogProbMetric: 17.4491, val_loss: 17.5485, val_MinusLogProbMetric: 17.5485

Epoch 798: val_loss did not improve from 17.44977
196/196 - 69s - loss: 17.4491 - MinusLogProbMetric: 17.4491 - val_loss: 17.5485 - val_MinusLogProbMetric: 17.5485 - lr: 5.5556e-05 - 69s/epoch - 350ms/step
Epoch 799/1000
2023-10-24 04:20:18.262 
Epoch 799/1000 
	 loss: 17.4636, MinusLogProbMetric: 17.4636, val_loss: 17.6036, val_MinusLogProbMetric: 17.6036

Epoch 799: val_loss did not improve from 17.44977
196/196 - 64s - loss: 17.4636 - MinusLogProbMetric: 17.4636 - val_loss: 17.6036 - val_MinusLogProbMetric: 17.6036 - lr: 5.5556e-05 - 64s/epoch - 324ms/step
Epoch 800/1000
2023-10-24 04:21:18.588 
Epoch 800/1000 
	 loss: 17.4723, MinusLogProbMetric: 17.4723, val_loss: 17.5659, val_MinusLogProbMetric: 17.5659

Epoch 800: val_loss did not improve from 17.44977
196/196 - 60s - loss: 17.4723 - MinusLogProbMetric: 17.4723 - val_loss: 17.5659 - val_MinusLogProbMetric: 17.5659 - lr: 5.5556e-05 - 60s/epoch - 308ms/step
Epoch 801/1000
2023-10-24 04:22:27.492 
Epoch 801/1000 
	 loss: 17.4560, MinusLogProbMetric: 17.4560, val_loss: 17.5317, val_MinusLogProbMetric: 17.5317

Epoch 801: val_loss did not improve from 17.44977
196/196 - 69s - loss: 17.4560 - MinusLogProbMetric: 17.4560 - val_loss: 17.5317 - val_MinusLogProbMetric: 17.5317 - lr: 5.5556e-05 - 69s/epoch - 352ms/step
Epoch 802/1000
2023-10-24 04:23:26.943 
Epoch 802/1000 
	 loss: 17.4550, MinusLogProbMetric: 17.4550, val_loss: 17.5343, val_MinusLogProbMetric: 17.5343

Epoch 802: val_loss did not improve from 17.44977
196/196 - 59s - loss: 17.4550 - MinusLogProbMetric: 17.4550 - val_loss: 17.5343 - val_MinusLogProbMetric: 17.5343 - lr: 5.5556e-05 - 59s/epoch - 303ms/step
Epoch 803/1000
2023-10-24 04:24:27.642 
Epoch 803/1000 
	 loss: 17.4759, MinusLogProbMetric: 17.4759, val_loss: 17.7659, val_MinusLogProbMetric: 17.7659

Epoch 803: val_loss did not improve from 17.44977
196/196 - 61s - loss: 17.4759 - MinusLogProbMetric: 17.4759 - val_loss: 17.7659 - val_MinusLogProbMetric: 17.7659 - lr: 5.5556e-05 - 61s/epoch - 310ms/step
Epoch 804/1000
2023-10-24 04:25:30.702 
Epoch 804/1000 
	 loss: 17.4708, MinusLogProbMetric: 17.4708, val_loss: 17.4945, val_MinusLogProbMetric: 17.4945

Epoch 804: val_loss did not improve from 17.44977
196/196 - 63s - loss: 17.4708 - MinusLogProbMetric: 17.4708 - val_loss: 17.4945 - val_MinusLogProbMetric: 17.4945 - lr: 5.5556e-05 - 63s/epoch - 322ms/step
Epoch 805/1000
2023-10-24 04:26:41.008 
Epoch 805/1000 
	 loss: 17.4464, MinusLogProbMetric: 17.4464, val_loss: 17.4781, val_MinusLogProbMetric: 17.4781

Epoch 805: val_loss did not improve from 17.44977
196/196 - 70s - loss: 17.4464 - MinusLogProbMetric: 17.4464 - val_loss: 17.4781 - val_MinusLogProbMetric: 17.4781 - lr: 5.5556e-05 - 70s/epoch - 359ms/step
Epoch 806/1000
2023-10-24 04:27:50.093 
Epoch 806/1000 
	 loss: 17.5206, MinusLogProbMetric: 17.5206, val_loss: 17.5453, val_MinusLogProbMetric: 17.5453

Epoch 806: val_loss did not improve from 17.44977
196/196 - 69s - loss: 17.5206 - MinusLogProbMetric: 17.5206 - val_loss: 17.5453 - val_MinusLogProbMetric: 17.5453 - lr: 5.5556e-05 - 69s/epoch - 352ms/step
Epoch 807/1000
2023-10-24 04:28:56.426 
Epoch 807/1000 
	 loss: 17.4598, MinusLogProbMetric: 17.4598, val_loss: 17.4967, val_MinusLogProbMetric: 17.4967

Epoch 807: val_loss did not improve from 17.44977
196/196 - 66s - loss: 17.4598 - MinusLogProbMetric: 17.4598 - val_loss: 17.4967 - val_MinusLogProbMetric: 17.4967 - lr: 5.5556e-05 - 66s/epoch - 338ms/step
Epoch 808/1000
2023-10-24 04:29:58.176 
Epoch 808/1000 
	 loss: 17.4557, MinusLogProbMetric: 17.4557, val_loss: 17.5317, val_MinusLogProbMetric: 17.5317

Epoch 808: val_loss did not improve from 17.44977
196/196 - 62s - loss: 17.4557 - MinusLogProbMetric: 17.4557 - val_loss: 17.5317 - val_MinusLogProbMetric: 17.5317 - lr: 5.5556e-05 - 62s/epoch - 315ms/step
Epoch 809/1000
2023-10-24 04:31:06.744 
Epoch 809/1000 
	 loss: 17.4662, MinusLogProbMetric: 17.4662, val_loss: 17.6020, val_MinusLogProbMetric: 17.6020

Epoch 809: val_loss did not improve from 17.44977
196/196 - 69s - loss: 17.4662 - MinusLogProbMetric: 17.4662 - val_loss: 17.6020 - val_MinusLogProbMetric: 17.6020 - lr: 5.5556e-05 - 69s/epoch - 350ms/step
Epoch 810/1000
2023-10-24 04:32:11.368 
Epoch 810/1000 
	 loss: 17.4649, MinusLogProbMetric: 17.4649, val_loss: 17.6612, val_MinusLogProbMetric: 17.6612

Epoch 810: val_loss did not improve from 17.44977
196/196 - 65s - loss: 17.4649 - MinusLogProbMetric: 17.4649 - val_loss: 17.6612 - val_MinusLogProbMetric: 17.6612 - lr: 5.5556e-05 - 65s/epoch - 330ms/step
Epoch 811/1000
2023-10-24 04:33:12.858 
Epoch 811/1000 
	 loss: 17.4454, MinusLogProbMetric: 17.4454, val_loss: 17.5252, val_MinusLogProbMetric: 17.5252

Epoch 811: val_loss did not improve from 17.44977
196/196 - 61s - loss: 17.4454 - MinusLogProbMetric: 17.4454 - val_loss: 17.5252 - val_MinusLogProbMetric: 17.5252 - lr: 5.5556e-05 - 61s/epoch - 314ms/step
Epoch 812/1000
2023-10-24 04:34:16.593 
Epoch 812/1000 
	 loss: 17.4822, MinusLogProbMetric: 17.4822, val_loss: 17.7271, val_MinusLogProbMetric: 17.7271

Epoch 812: val_loss did not improve from 17.44977
196/196 - 64s - loss: 17.4822 - MinusLogProbMetric: 17.4822 - val_loss: 17.7271 - val_MinusLogProbMetric: 17.7271 - lr: 5.5556e-05 - 64s/epoch - 325ms/step
Epoch 813/1000
2023-10-24 04:35:22.735 
Epoch 813/1000 
	 loss: 17.4621, MinusLogProbMetric: 17.4621, val_loss: 17.5859, val_MinusLogProbMetric: 17.5859

Epoch 813: val_loss did not improve from 17.44977
196/196 - 66s - loss: 17.4621 - MinusLogProbMetric: 17.4621 - val_loss: 17.5859 - val_MinusLogProbMetric: 17.5859 - lr: 5.5556e-05 - 66s/epoch - 337ms/step
Epoch 814/1000
2023-10-24 04:36:26.548 
Epoch 814/1000 
	 loss: 17.4680, MinusLogProbMetric: 17.4680, val_loss: 17.4955, val_MinusLogProbMetric: 17.4955

Epoch 814: val_loss did not improve from 17.44977
196/196 - 64s - loss: 17.4680 - MinusLogProbMetric: 17.4680 - val_loss: 17.4955 - val_MinusLogProbMetric: 17.4955 - lr: 5.5556e-05 - 64s/epoch - 326ms/step
Epoch 815/1000
2023-10-24 04:37:29.973 
Epoch 815/1000 
	 loss: 17.4452, MinusLogProbMetric: 17.4452, val_loss: 17.5104, val_MinusLogProbMetric: 17.5104

Epoch 815: val_loss did not improve from 17.44977
196/196 - 63s - loss: 17.4452 - MinusLogProbMetric: 17.4452 - val_loss: 17.5104 - val_MinusLogProbMetric: 17.5104 - lr: 5.5556e-05 - 63s/epoch - 324ms/step
Epoch 816/1000
2023-10-24 04:38:38.255 
Epoch 816/1000 
	 loss: 17.5161, MinusLogProbMetric: 17.5161, val_loss: 17.4566, val_MinusLogProbMetric: 17.4566

Epoch 816: val_loss did not improve from 17.44977
196/196 - 68s - loss: 17.5161 - MinusLogProbMetric: 17.5161 - val_loss: 17.4566 - val_MinusLogProbMetric: 17.4566 - lr: 5.5556e-05 - 68s/epoch - 348ms/step
Epoch 817/1000
2023-10-24 04:39:39.323 
Epoch 817/1000 
	 loss: 17.4411, MinusLogProbMetric: 17.4411, val_loss: 17.5121, val_MinusLogProbMetric: 17.5121

Epoch 817: val_loss did not improve from 17.44977
196/196 - 61s - loss: 17.4411 - MinusLogProbMetric: 17.4411 - val_loss: 17.5121 - val_MinusLogProbMetric: 17.5121 - lr: 5.5556e-05 - 61s/epoch - 312ms/step
Epoch 818/1000
2023-10-24 04:40:40.556 
Epoch 818/1000 
	 loss: 17.4607, MinusLogProbMetric: 17.4607, val_loss: 17.6152, val_MinusLogProbMetric: 17.6152

Epoch 818: val_loss did not improve from 17.44977
196/196 - 61s - loss: 17.4607 - MinusLogProbMetric: 17.4607 - val_loss: 17.6152 - val_MinusLogProbMetric: 17.6152 - lr: 5.5556e-05 - 61s/epoch - 312ms/step
Epoch 819/1000
2023-10-24 04:41:44.576 
Epoch 819/1000 
	 loss: 17.4509, MinusLogProbMetric: 17.4509, val_loss: 17.5203, val_MinusLogProbMetric: 17.5203

Epoch 819: val_loss did not improve from 17.44977
196/196 - 64s - loss: 17.4509 - MinusLogProbMetric: 17.4509 - val_loss: 17.5203 - val_MinusLogProbMetric: 17.5203 - lr: 5.5556e-05 - 64s/epoch - 327ms/step
Epoch 820/1000
2023-10-24 04:42:54.857 
Epoch 820/1000 
	 loss: 17.4463, MinusLogProbMetric: 17.4463, val_loss: 17.4745, val_MinusLogProbMetric: 17.4745

Epoch 820: val_loss did not improve from 17.44977
196/196 - 70s - loss: 17.4463 - MinusLogProbMetric: 17.4463 - val_loss: 17.4745 - val_MinusLogProbMetric: 17.4745 - lr: 5.5556e-05 - 70s/epoch - 359ms/step
Epoch 821/1000
2023-10-24 04:43:58.681 
Epoch 821/1000 
	 loss: 17.4241, MinusLogProbMetric: 17.4241, val_loss: 17.5100, val_MinusLogProbMetric: 17.5100

Epoch 821: val_loss did not improve from 17.44977
196/196 - 64s - loss: 17.4241 - MinusLogProbMetric: 17.4241 - val_loss: 17.5100 - val_MinusLogProbMetric: 17.5100 - lr: 5.5556e-05 - 64s/epoch - 326ms/step
Epoch 822/1000
2023-10-24 04:45:05.052 
Epoch 822/1000 
	 loss: 17.4228, MinusLogProbMetric: 17.4228, val_loss: 17.6697, val_MinusLogProbMetric: 17.6697

Epoch 822: val_loss did not improve from 17.44977
196/196 - 66s - loss: 17.4228 - MinusLogProbMetric: 17.4228 - val_loss: 17.6697 - val_MinusLogProbMetric: 17.6697 - lr: 5.5556e-05 - 66s/epoch - 339ms/step
Epoch 823/1000
2023-10-24 04:46:12.434 
Epoch 823/1000 
	 loss: 17.4617, MinusLogProbMetric: 17.4617, val_loss: 17.5425, val_MinusLogProbMetric: 17.5425

Epoch 823: val_loss did not improve from 17.44977
196/196 - 67s - loss: 17.4617 - MinusLogProbMetric: 17.4617 - val_loss: 17.5425 - val_MinusLogProbMetric: 17.5425 - lr: 5.5556e-05 - 67s/epoch - 344ms/step
Epoch 824/1000
2023-10-24 04:47:19.072 
Epoch 824/1000 
	 loss: 17.4485, MinusLogProbMetric: 17.4485, val_loss: 17.5695, val_MinusLogProbMetric: 17.5695

Epoch 824: val_loss did not improve from 17.44977
196/196 - 67s - loss: 17.4485 - MinusLogProbMetric: 17.4485 - val_loss: 17.5695 - val_MinusLogProbMetric: 17.5695 - lr: 5.5556e-05 - 67s/epoch - 340ms/step
Epoch 825/1000
2023-10-24 04:48:27.099 
Epoch 825/1000 
	 loss: 17.4493, MinusLogProbMetric: 17.4493, val_loss: 17.5446, val_MinusLogProbMetric: 17.5446

Epoch 825: val_loss did not improve from 17.44977
196/196 - 68s - loss: 17.4493 - MinusLogProbMetric: 17.4493 - val_loss: 17.5446 - val_MinusLogProbMetric: 17.5446 - lr: 5.5556e-05 - 68s/epoch - 347ms/step
Epoch 826/1000
2023-10-24 04:49:32.683 
Epoch 826/1000 
	 loss: 17.4436, MinusLogProbMetric: 17.4436, val_loss: 17.5733, val_MinusLogProbMetric: 17.5733

Epoch 826: val_loss did not improve from 17.44977
196/196 - 66s - loss: 17.4436 - MinusLogProbMetric: 17.4436 - val_loss: 17.5733 - val_MinusLogProbMetric: 17.5733 - lr: 5.5556e-05 - 66s/epoch - 335ms/step
Epoch 827/1000
2023-10-24 04:50:35.803 
Epoch 827/1000 
	 loss: 17.4492, MinusLogProbMetric: 17.4492, val_loss: 17.5532, val_MinusLogProbMetric: 17.5532

Epoch 827: val_loss did not improve from 17.44977
196/196 - 63s - loss: 17.4492 - MinusLogProbMetric: 17.4492 - val_loss: 17.5532 - val_MinusLogProbMetric: 17.5532 - lr: 5.5556e-05 - 63s/epoch - 322ms/step
Epoch 828/1000
2023-10-24 04:51:41.524 
Epoch 828/1000 
	 loss: 17.4650, MinusLogProbMetric: 17.4650, val_loss: 17.4832, val_MinusLogProbMetric: 17.4832

Epoch 828: val_loss did not improve from 17.44977
196/196 - 66s - loss: 17.4650 - MinusLogProbMetric: 17.4650 - val_loss: 17.4832 - val_MinusLogProbMetric: 17.4832 - lr: 5.5556e-05 - 66s/epoch - 335ms/step
Epoch 829/1000
2023-10-24 04:52:46.477 
Epoch 829/1000 
	 loss: 17.4540, MinusLogProbMetric: 17.4540, val_loss: 17.7799, val_MinusLogProbMetric: 17.7799

Epoch 829: val_loss did not improve from 17.44977
196/196 - 65s - loss: 17.4540 - MinusLogProbMetric: 17.4540 - val_loss: 17.7799 - val_MinusLogProbMetric: 17.7799 - lr: 5.5556e-05 - 65s/epoch - 331ms/step
Epoch 830/1000
2023-10-24 04:53:47.863 
Epoch 830/1000 
	 loss: 17.8508, MinusLogProbMetric: 17.8508, val_loss: 17.5280, val_MinusLogProbMetric: 17.5280

Epoch 830: val_loss did not improve from 17.44977
196/196 - 61s - loss: 17.8508 - MinusLogProbMetric: 17.8508 - val_loss: 17.5280 - val_MinusLogProbMetric: 17.5280 - lr: 5.5556e-05 - 61s/epoch - 313ms/step
Epoch 831/1000
2023-10-24 04:54:51.072 
Epoch 831/1000 
	 loss: 17.4578, MinusLogProbMetric: 17.4578, val_loss: 17.4846, val_MinusLogProbMetric: 17.4846

Epoch 831: val_loss did not improve from 17.44977
196/196 - 63s - loss: 17.4578 - MinusLogProbMetric: 17.4578 - val_loss: 17.4846 - val_MinusLogProbMetric: 17.4846 - lr: 5.5556e-05 - 63s/epoch - 322ms/step
Epoch 832/1000
2023-10-24 04:55:54.758 
Epoch 832/1000 
	 loss: 17.4396, MinusLogProbMetric: 17.4396, val_loss: 17.5575, val_MinusLogProbMetric: 17.5575

Epoch 832: val_loss did not improve from 17.44977
196/196 - 64s - loss: 17.4396 - MinusLogProbMetric: 17.4396 - val_loss: 17.5575 - val_MinusLogProbMetric: 17.5575 - lr: 5.5556e-05 - 64s/epoch - 325ms/step
Epoch 833/1000
2023-10-24 04:56:57.385 
Epoch 833/1000 
	 loss: 17.4746, MinusLogProbMetric: 17.4746, val_loss: 17.5330, val_MinusLogProbMetric: 17.5330

Epoch 833: val_loss did not improve from 17.44977
196/196 - 63s - loss: 17.4746 - MinusLogProbMetric: 17.4746 - val_loss: 17.5330 - val_MinusLogProbMetric: 17.5330 - lr: 5.5556e-05 - 63s/epoch - 320ms/step
Epoch 834/1000
2023-10-24 04:57:59.323 
Epoch 834/1000 
	 loss: 17.4681, MinusLogProbMetric: 17.4681, val_loss: 17.5408, val_MinusLogProbMetric: 17.5408

Epoch 834: val_loss did not improve from 17.44977
196/196 - 62s - loss: 17.4681 - MinusLogProbMetric: 17.4681 - val_loss: 17.5408 - val_MinusLogProbMetric: 17.5408 - lr: 5.5556e-05 - 62s/epoch - 316ms/step
Epoch 835/1000
2023-10-24 04:59:00.373 
Epoch 835/1000 
	 loss: 17.4598, MinusLogProbMetric: 17.4598, val_loss: 17.5461, val_MinusLogProbMetric: 17.5461

Epoch 835: val_loss did not improve from 17.44977
196/196 - 61s - loss: 17.4598 - MinusLogProbMetric: 17.4598 - val_loss: 17.5461 - val_MinusLogProbMetric: 17.5461 - lr: 5.5556e-05 - 61s/epoch - 311ms/step
Epoch 836/1000
2023-10-24 05:00:07.472 
Epoch 836/1000 
	 loss: 17.4228, MinusLogProbMetric: 17.4228, val_loss: 17.5238, val_MinusLogProbMetric: 17.5238

Epoch 836: val_loss did not improve from 17.44977
196/196 - 67s - loss: 17.4228 - MinusLogProbMetric: 17.4228 - val_loss: 17.5238 - val_MinusLogProbMetric: 17.5238 - lr: 5.5556e-05 - 67s/epoch - 342ms/step
Epoch 837/1000
2023-10-24 05:01:09.698 
Epoch 837/1000 
	 loss: 17.4546, MinusLogProbMetric: 17.4546, val_loss: 17.5528, val_MinusLogProbMetric: 17.5528

Epoch 837: val_loss did not improve from 17.44977
196/196 - 62s - loss: 17.4546 - MinusLogProbMetric: 17.4546 - val_loss: 17.5528 - val_MinusLogProbMetric: 17.5528 - lr: 5.5556e-05 - 62s/epoch - 317ms/step
Epoch 838/1000
2023-10-24 05:02:14.481 
Epoch 838/1000 
	 loss: 17.4426, MinusLogProbMetric: 17.4426, val_loss: 17.5098, val_MinusLogProbMetric: 17.5098

Epoch 838: val_loss did not improve from 17.44977
196/196 - 65s - loss: 17.4426 - MinusLogProbMetric: 17.4426 - val_loss: 17.5098 - val_MinusLogProbMetric: 17.5098 - lr: 5.5556e-05 - 65s/epoch - 331ms/step
Epoch 839/1000
2023-10-24 05:03:19.007 
Epoch 839/1000 
	 loss: 17.4464, MinusLogProbMetric: 17.4464, val_loss: 17.4665, val_MinusLogProbMetric: 17.4665

Epoch 839: val_loss did not improve from 17.44977
196/196 - 65s - loss: 17.4464 - MinusLogProbMetric: 17.4464 - val_loss: 17.4665 - val_MinusLogProbMetric: 17.4665 - lr: 5.5556e-05 - 65s/epoch - 329ms/step
Epoch 840/1000
2023-10-24 05:04:20.511 
Epoch 840/1000 
	 loss: 17.4486, MinusLogProbMetric: 17.4486, val_loss: 17.5208, val_MinusLogProbMetric: 17.5208

Epoch 840: val_loss did not improve from 17.44977
196/196 - 62s - loss: 17.4486 - MinusLogProbMetric: 17.4486 - val_loss: 17.5208 - val_MinusLogProbMetric: 17.5208 - lr: 5.5556e-05 - 62s/epoch - 314ms/step
Epoch 841/1000
2023-10-24 05:05:30.397 
Epoch 841/1000 
	 loss: 17.4333, MinusLogProbMetric: 17.4333, val_loss: 17.5231, val_MinusLogProbMetric: 17.5231

Epoch 841: val_loss did not improve from 17.44977
196/196 - 70s - loss: 17.4333 - MinusLogProbMetric: 17.4333 - val_loss: 17.5231 - val_MinusLogProbMetric: 17.5231 - lr: 5.5556e-05 - 70s/epoch - 357ms/step
Epoch 842/1000
2023-10-24 05:06:30.550 
Epoch 842/1000 
	 loss: 17.4437, MinusLogProbMetric: 17.4437, val_loss: 17.4722, val_MinusLogProbMetric: 17.4722

Epoch 842: val_loss did not improve from 17.44977
196/196 - 60s - loss: 17.4437 - MinusLogProbMetric: 17.4437 - val_loss: 17.4722 - val_MinusLogProbMetric: 17.4722 - lr: 5.5556e-05 - 60s/epoch - 307ms/step
Epoch 843/1000
2023-10-24 05:07:31.434 
Epoch 843/1000 
	 loss: 17.4136, MinusLogProbMetric: 17.4136, val_loss: 17.5692, val_MinusLogProbMetric: 17.5692

Epoch 843: val_loss did not improve from 17.44977
196/196 - 61s - loss: 17.4136 - MinusLogProbMetric: 17.4136 - val_loss: 17.5692 - val_MinusLogProbMetric: 17.5692 - lr: 5.5556e-05 - 61s/epoch - 311ms/step
Epoch 844/1000
2023-10-24 05:08:37.254 
Epoch 844/1000 
	 loss: 17.4345, MinusLogProbMetric: 17.4345, val_loss: 17.6557, val_MinusLogProbMetric: 17.6557

Epoch 844: val_loss did not improve from 17.44977
196/196 - 66s - loss: 17.4345 - MinusLogProbMetric: 17.4345 - val_loss: 17.6557 - val_MinusLogProbMetric: 17.6557 - lr: 5.5556e-05 - 66s/epoch - 336ms/step
Epoch 845/1000
2023-10-24 05:09:40.299 
Epoch 845/1000 
	 loss: 17.4548, MinusLogProbMetric: 17.4548, val_loss: 17.6064, val_MinusLogProbMetric: 17.6064

Epoch 845: val_loss did not improve from 17.44977
196/196 - 63s - loss: 17.4548 - MinusLogProbMetric: 17.4548 - val_loss: 17.6064 - val_MinusLogProbMetric: 17.6064 - lr: 5.5556e-05 - 63s/epoch - 322ms/step
Epoch 846/1000
2023-10-24 05:10:45.613 
Epoch 846/1000 
	 loss: 17.4436, MinusLogProbMetric: 17.4436, val_loss: 17.5420, val_MinusLogProbMetric: 17.5420

Epoch 846: val_loss did not improve from 17.44977
196/196 - 65s - loss: 17.4436 - MinusLogProbMetric: 17.4436 - val_loss: 17.5420 - val_MinusLogProbMetric: 17.5420 - lr: 5.5556e-05 - 65s/epoch - 333ms/step
Epoch 847/1000
2023-10-24 05:11:57.416 
Epoch 847/1000 
	 loss: 17.4136, MinusLogProbMetric: 17.4136, val_loss: 17.4857, val_MinusLogProbMetric: 17.4857

Epoch 847: val_loss did not improve from 17.44977
196/196 - 72s - loss: 17.4136 - MinusLogProbMetric: 17.4136 - val_loss: 17.4857 - val_MinusLogProbMetric: 17.4857 - lr: 5.5556e-05 - 72s/epoch - 366ms/step
Epoch 848/1000
2023-10-24 05:13:00.747 
Epoch 848/1000 
	 loss: 17.3402, MinusLogProbMetric: 17.3402, val_loss: 17.4016, val_MinusLogProbMetric: 17.4016

Epoch 848: val_loss improved from 17.44977 to 17.40163, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 65s - loss: 17.3402 - MinusLogProbMetric: 17.3402 - val_loss: 17.4016 - val_MinusLogProbMetric: 17.4016 - lr: 2.7778e-05 - 65s/epoch - 330ms/step
Epoch 849/1000
2023-10-24 05:14:05.073 
Epoch 849/1000 
	 loss: 17.3411, MinusLogProbMetric: 17.3411, val_loss: 17.4274, val_MinusLogProbMetric: 17.4274

Epoch 849: val_loss did not improve from 17.40163
196/196 - 63s - loss: 17.3411 - MinusLogProbMetric: 17.3411 - val_loss: 17.4274 - val_MinusLogProbMetric: 17.4274 - lr: 2.7778e-05 - 63s/epoch - 321ms/step
Epoch 850/1000
2023-10-24 05:15:09.868 
Epoch 850/1000 
	 loss: 17.3494, MinusLogProbMetric: 17.3494, val_loss: 17.4534, val_MinusLogProbMetric: 17.4534

Epoch 850: val_loss did not improve from 17.40163
196/196 - 65s - loss: 17.3494 - MinusLogProbMetric: 17.3494 - val_loss: 17.4534 - val_MinusLogProbMetric: 17.4534 - lr: 2.7778e-05 - 65s/epoch - 331ms/step
Epoch 851/1000
2023-10-24 05:16:12.359 
Epoch 851/1000 
	 loss: 17.3425, MinusLogProbMetric: 17.3425, val_loss: 17.4799, val_MinusLogProbMetric: 17.4799

Epoch 851: val_loss did not improve from 17.40163
196/196 - 62s - loss: 17.3425 - MinusLogProbMetric: 17.3425 - val_loss: 17.4799 - val_MinusLogProbMetric: 17.4799 - lr: 2.7778e-05 - 62s/epoch - 319ms/step
Epoch 852/1000
2023-10-24 05:17:13.657 
Epoch 852/1000 
	 loss: 17.3553, MinusLogProbMetric: 17.3553, val_loss: 17.4733, val_MinusLogProbMetric: 17.4733

Epoch 852: val_loss did not improve from 17.40163
196/196 - 61s - loss: 17.3553 - MinusLogProbMetric: 17.3553 - val_loss: 17.4733 - val_MinusLogProbMetric: 17.4733 - lr: 2.7778e-05 - 61s/epoch - 313ms/step
Epoch 853/1000
2023-10-24 05:18:14.467 
Epoch 853/1000 
	 loss: 17.3431, MinusLogProbMetric: 17.3431, val_loss: 17.4167, val_MinusLogProbMetric: 17.4167

Epoch 853: val_loss did not improve from 17.40163
196/196 - 61s - loss: 17.3431 - MinusLogProbMetric: 17.3431 - val_loss: 17.4167 - val_MinusLogProbMetric: 17.4167 - lr: 2.7778e-05 - 61s/epoch - 310ms/step
Epoch 854/1000
2023-10-24 05:19:15.841 
Epoch 854/1000 
	 loss: 17.3568, MinusLogProbMetric: 17.3568, val_loss: 17.4420, val_MinusLogProbMetric: 17.4420

Epoch 854: val_loss did not improve from 17.40163
196/196 - 61s - loss: 17.3568 - MinusLogProbMetric: 17.3568 - val_loss: 17.4420 - val_MinusLogProbMetric: 17.4420 - lr: 2.7778e-05 - 61s/epoch - 313ms/step
Epoch 855/1000
2023-10-24 05:20:16.338 
Epoch 855/1000 
	 loss: 17.3368, MinusLogProbMetric: 17.3368, val_loss: 17.4457, val_MinusLogProbMetric: 17.4457

Epoch 855: val_loss did not improve from 17.40163
196/196 - 60s - loss: 17.3368 - MinusLogProbMetric: 17.3368 - val_loss: 17.4457 - val_MinusLogProbMetric: 17.4457 - lr: 2.7778e-05 - 60s/epoch - 309ms/step
Epoch 856/1000
2023-10-24 05:21:17.351 
Epoch 856/1000 
	 loss: 17.3338, MinusLogProbMetric: 17.3338, val_loss: 17.3987, val_MinusLogProbMetric: 17.3987

Epoch 856: val_loss improved from 17.40163 to 17.39866, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 62s - loss: 17.3338 - MinusLogProbMetric: 17.3338 - val_loss: 17.3987 - val_MinusLogProbMetric: 17.3987 - lr: 2.7778e-05 - 62s/epoch - 317ms/step
Epoch 857/1000
2023-10-24 05:22:19.941 
Epoch 857/1000 
	 loss: 17.3409, MinusLogProbMetric: 17.3409, val_loss: 17.5740, val_MinusLogProbMetric: 17.5740

Epoch 857: val_loss did not improve from 17.39866
196/196 - 61s - loss: 17.3409 - MinusLogProbMetric: 17.3409 - val_loss: 17.5740 - val_MinusLogProbMetric: 17.5740 - lr: 2.7778e-05 - 61s/epoch - 314ms/step
Epoch 858/1000
2023-10-24 05:23:27.119 
Epoch 858/1000 
	 loss: 17.3576, MinusLogProbMetric: 17.3576, val_loss: 17.4587, val_MinusLogProbMetric: 17.4587

Epoch 858: val_loss did not improve from 17.39866
196/196 - 67s - loss: 17.3576 - MinusLogProbMetric: 17.3576 - val_loss: 17.4587 - val_MinusLogProbMetric: 17.4587 - lr: 2.7778e-05 - 67s/epoch - 343ms/step
Epoch 859/1000
2023-10-24 05:24:29.527 
Epoch 859/1000 
	 loss: 17.3341, MinusLogProbMetric: 17.3341, val_loss: 17.4376, val_MinusLogProbMetric: 17.4376

Epoch 859: val_loss did not improve from 17.39866
196/196 - 62s - loss: 17.3341 - MinusLogProbMetric: 17.3341 - val_loss: 17.4376 - val_MinusLogProbMetric: 17.4376 - lr: 2.7778e-05 - 62s/epoch - 318ms/step
Epoch 860/1000
2023-10-24 05:25:31.926 
Epoch 860/1000 
	 loss: 17.3414, MinusLogProbMetric: 17.3414, val_loss: 17.4117, val_MinusLogProbMetric: 17.4117

Epoch 860: val_loss did not improve from 17.39866
196/196 - 62s - loss: 17.3414 - MinusLogProbMetric: 17.3414 - val_loss: 17.4117 - val_MinusLogProbMetric: 17.4117 - lr: 2.7778e-05 - 62s/epoch - 318ms/step
Epoch 861/1000
2023-10-24 05:26:38.625 
Epoch 861/1000 
	 loss: 17.3345, MinusLogProbMetric: 17.3345, val_loss: 17.4095, val_MinusLogProbMetric: 17.4095

Epoch 861: val_loss did not improve from 17.39866
196/196 - 67s - loss: 17.3345 - MinusLogProbMetric: 17.3345 - val_loss: 17.4095 - val_MinusLogProbMetric: 17.4095 - lr: 2.7778e-05 - 67s/epoch - 340ms/step
Epoch 862/1000
2023-10-24 05:27:40.978 
Epoch 862/1000 
	 loss: 17.3346, MinusLogProbMetric: 17.3346, val_loss: 17.4527, val_MinusLogProbMetric: 17.4527

Epoch 862: val_loss did not improve from 17.39866
196/196 - 62s - loss: 17.3346 - MinusLogProbMetric: 17.3346 - val_loss: 17.4527 - val_MinusLogProbMetric: 17.4527 - lr: 2.7778e-05 - 62s/epoch - 318ms/step
Epoch 863/1000
2023-10-24 05:28:39.901 
Epoch 863/1000 
	 loss: 17.3416, MinusLogProbMetric: 17.3416, val_loss: 17.3969, val_MinusLogProbMetric: 17.3969

Epoch 863: val_loss improved from 17.39866 to 17.39692, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 60s - loss: 17.3416 - MinusLogProbMetric: 17.3416 - val_loss: 17.3969 - val_MinusLogProbMetric: 17.3969 - lr: 2.7778e-05 - 60s/epoch - 307ms/step
Epoch 864/1000
2023-10-24 05:29:48.085 
Epoch 864/1000 
	 loss: 17.3413, MinusLogProbMetric: 17.3413, val_loss: 17.3961, val_MinusLogProbMetric: 17.3961

Epoch 864: val_loss improved from 17.39692 to 17.39613, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 68s - loss: 17.3413 - MinusLogProbMetric: 17.3413 - val_loss: 17.3961 - val_MinusLogProbMetric: 17.3961 - lr: 2.7778e-05 - 68s/epoch - 348ms/step
Epoch 865/1000
2023-10-24 05:30:51.479 
Epoch 865/1000 
	 loss: 17.3435, MinusLogProbMetric: 17.3435, val_loss: 17.4446, val_MinusLogProbMetric: 17.4446

Epoch 865: val_loss did not improve from 17.39613
196/196 - 62s - loss: 17.3435 - MinusLogProbMetric: 17.3435 - val_loss: 17.4446 - val_MinusLogProbMetric: 17.4446 - lr: 2.7778e-05 - 62s/epoch - 317ms/step
Epoch 866/1000
2023-10-24 05:31:53.361 
Epoch 866/1000 
	 loss: 17.3430, MinusLogProbMetric: 17.3430, val_loss: 17.4210, val_MinusLogProbMetric: 17.4210

Epoch 866: val_loss did not improve from 17.39613
196/196 - 62s - loss: 17.3430 - MinusLogProbMetric: 17.3430 - val_loss: 17.4210 - val_MinusLogProbMetric: 17.4210 - lr: 2.7778e-05 - 62s/epoch - 316ms/step
Epoch 867/1000
2023-10-24 05:32:57.915 
Epoch 867/1000 
	 loss: 17.3335, MinusLogProbMetric: 17.3335, val_loss: 17.4032, val_MinusLogProbMetric: 17.4032

Epoch 867: val_loss did not improve from 17.39613
196/196 - 65s - loss: 17.3335 - MinusLogProbMetric: 17.3335 - val_loss: 17.4032 - val_MinusLogProbMetric: 17.4032 - lr: 2.7778e-05 - 65s/epoch - 329ms/step
Epoch 868/1000
2023-10-24 05:33:59.256 
Epoch 868/1000 
	 loss: 17.3433, MinusLogProbMetric: 17.3433, val_loss: 17.4154, val_MinusLogProbMetric: 17.4154

Epoch 868: val_loss did not improve from 17.39613
196/196 - 61s - loss: 17.3433 - MinusLogProbMetric: 17.3433 - val_loss: 17.4154 - val_MinusLogProbMetric: 17.4154 - lr: 2.7778e-05 - 61s/epoch - 313ms/step
Epoch 869/1000
2023-10-24 05:35:09.868 
Epoch 869/1000 
	 loss: 17.3357, MinusLogProbMetric: 17.3357, val_loss: 17.3926, val_MinusLogProbMetric: 17.3926

Epoch 869: val_loss improved from 17.39613 to 17.39256, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 72s - loss: 17.3357 - MinusLogProbMetric: 17.3357 - val_loss: 17.3926 - val_MinusLogProbMetric: 17.3926 - lr: 2.7778e-05 - 72s/epoch - 367ms/step
Epoch 870/1000
2023-10-24 05:36:10.697 
Epoch 870/1000 
	 loss: 17.3339, MinusLogProbMetric: 17.3339, val_loss: 17.4076, val_MinusLogProbMetric: 17.4076

Epoch 870: val_loss did not improve from 17.39256
196/196 - 60s - loss: 17.3339 - MinusLogProbMetric: 17.3339 - val_loss: 17.4076 - val_MinusLogProbMetric: 17.4076 - lr: 2.7778e-05 - 60s/epoch - 304ms/step
Epoch 871/1000
2023-10-24 05:37:18.215 
Epoch 871/1000 
	 loss: 17.3442, MinusLogProbMetric: 17.3442, val_loss: 17.4862, val_MinusLogProbMetric: 17.4862

Epoch 871: val_loss did not improve from 17.39256
196/196 - 68s - loss: 17.3442 - MinusLogProbMetric: 17.3442 - val_loss: 17.4862 - val_MinusLogProbMetric: 17.4862 - lr: 2.7778e-05 - 68s/epoch - 344ms/step
Epoch 872/1000
2023-10-24 05:38:18.442 
Epoch 872/1000 
	 loss: 17.3574, MinusLogProbMetric: 17.3574, val_loss: 17.3925, val_MinusLogProbMetric: 17.3925

Epoch 872: val_loss improved from 17.39256 to 17.39252, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 61s - loss: 17.3574 - MinusLogProbMetric: 17.3574 - val_loss: 17.3925 - val_MinusLogProbMetric: 17.3925 - lr: 2.7778e-05 - 61s/epoch - 311ms/step
Epoch 873/1000
2023-10-24 05:39:23.552 
Epoch 873/1000 
	 loss: 17.3390, MinusLogProbMetric: 17.3390, val_loss: 17.4160, val_MinusLogProbMetric: 17.4160

Epoch 873: val_loss did not improve from 17.39252
196/196 - 64s - loss: 17.3390 - MinusLogProbMetric: 17.3390 - val_loss: 17.4160 - val_MinusLogProbMetric: 17.4160 - lr: 2.7778e-05 - 64s/epoch - 329ms/step
Epoch 874/1000
2023-10-24 05:40:32.034 
Epoch 874/1000 
	 loss: 17.3305, MinusLogProbMetric: 17.3305, val_loss: 17.5025, val_MinusLogProbMetric: 17.5025

Epoch 874: val_loss did not improve from 17.39252
196/196 - 68s - loss: 17.3305 - MinusLogProbMetric: 17.3305 - val_loss: 17.5025 - val_MinusLogProbMetric: 17.5025 - lr: 2.7778e-05 - 68s/epoch - 349ms/step
Epoch 875/1000
2023-10-24 05:41:36.553 
Epoch 875/1000 
	 loss: 17.3270, MinusLogProbMetric: 17.3270, val_loss: 17.4315, val_MinusLogProbMetric: 17.4315

Epoch 875: val_loss did not improve from 17.39252
196/196 - 65s - loss: 17.3270 - MinusLogProbMetric: 17.3270 - val_loss: 17.4315 - val_MinusLogProbMetric: 17.4315 - lr: 2.7778e-05 - 65s/epoch - 329ms/step
Epoch 876/1000
2023-10-24 05:42:37.154 
Epoch 876/1000 
	 loss: 17.3290, MinusLogProbMetric: 17.3290, val_loss: 17.4023, val_MinusLogProbMetric: 17.4023

Epoch 876: val_loss did not improve from 17.39252
196/196 - 61s - loss: 17.3290 - MinusLogProbMetric: 17.3290 - val_loss: 17.4023 - val_MinusLogProbMetric: 17.4023 - lr: 2.7778e-05 - 61s/epoch - 309ms/step
Epoch 877/1000
2023-10-24 05:43:38.823 
Epoch 877/1000 
	 loss: 17.3249, MinusLogProbMetric: 17.3249, val_loss: 17.4026, val_MinusLogProbMetric: 17.4026

Epoch 877: val_loss did not improve from 17.39252
196/196 - 62s - loss: 17.3249 - MinusLogProbMetric: 17.3249 - val_loss: 17.4026 - val_MinusLogProbMetric: 17.4026 - lr: 2.7778e-05 - 62s/epoch - 315ms/step
Epoch 878/1000
2023-10-24 05:44:40.703 
Epoch 878/1000 
	 loss: 17.3372, MinusLogProbMetric: 17.3372, val_loss: 17.4385, val_MinusLogProbMetric: 17.4385

Epoch 878: val_loss did not improve from 17.39252
196/196 - 62s - loss: 17.3372 - MinusLogProbMetric: 17.3372 - val_loss: 17.4385 - val_MinusLogProbMetric: 17.4385 - lr: 2.7778e-05 - 62s/epoch - 316ms/step
Epoch 879/1000
2023-10-24 05:45:43.100 
Epoch 879/1000 
	 loss: 17.3259, MinusLogProbMetric: 17.3259, val_loss: 17.4821, val_MinusLogProbMetric: 17.4821

Epoch 879: val_loss did not improve from 17.39252
196/196 - 62s - loss: 17.3259 - MinusLogProbMetric: 17.3259 - val_loss: 17.4821 - val_MinusLogProbMetric: 17.4821 - lr: 2.7778e-05 - 62s/epoch - 318ms/step
Epoch 880/1000
2023-10-24 05:46:44.331 
Epoch 880/1000 
	 loss: 17.3472, MinusLogProbMetric: 17.3472, val_loss: 17.4155, val_MinusLogProbMetric: 17.4155

Epoch 880: val_loss did not improve from 17.39252
196/196 - 61s - loss: 17.3472 - MinusLogProbMetric: 17.3472 - val_loss: 17.4155 - val_MinusLogProbMetric: 17.4155 - lr: 2.7778e-05 - 61s/epoch - 312ms/step
Epoch 881/1000
2023-10-24 05:47:46.677 
Epoch 881/1000 
	 loss: 17.3343, MinusLogProbMetric: 17.3343, val_loss: 17.4116, val_MinusLogProbMetric: 17.4116

Epoch 881: val_loss did not improve from 17.39252
196/196 - 62s - loss: 17.3343 - MinusLogProbMetric: 17.3343 - val_loss: 17.4116 - val_MinusLogProbMetric: 17.4116 - lr: 2.7778e-05 - 62s/epoch - 318ms/step
Epoch 882/1000
2023-10-24 05:48:49.316 
Epoch 882/1000 
	 loss: 17.3283, MinusLogProbMetric: 17.3283, val_loss: 17.4075, val_MinusLogProbMetric: 17.4075

Epoch 882: val_loss did not improve from 17.39252
196/196 - 63s - loss: 17.3283 - MinusLogProbMetric: 17.3283 - val_loss: 17.4075 - val_MinusLogProbMetric: 17.4075 - lr: 2.7778e-05 - 63s/epoch - 320ms/step
Epoch 883/1000
2023-10-24 05:49:49.638 
Epoch 883/1000 
	 loss: 17.3131, MinusLogProbMetric: 17.3131, val_loss: 17.3839, val_MinusLogProbMetric: 17.3839

Epoch 883: val_loss improved from 17.39252 to 17.38395, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 62s - loss: 17.3131 - MinusLogProbMetric: 17.3131 - val_loss: 17.3839 - val_MinusLogProbMetric: 17.3839 - lr: 2.7778e-05 - 62s/epoch - 314ms/step
Epoch 884/1000
2023-10-24 05:50:55.714 
Epoch 884/1000 
	 loss: 17.3345, MinusLogProbMetric: 17.3345, val_loss: 17.4102, val_MinusLogProbMetric: 17.4102

Epoch 884: val_loss did not improve from 17.38395
196/196 - 65s - loss: 17.3345 - MinusLogProbMetric: 17.3345 - val_loss: 17.4102 - val_MinusLogProbMetric: 17.4102 - lr: 2.7778e-05 - 65s/epoch - 331ms/step
Epoch 885/1000
2023-10-24 05:51:57.908 
Epoch 885/1000 
	 loss: 17.3265, MinusLogProbMetric: 17.3265, val_loss: 17.4491, val_MinusLogProbMetric: 17.4491

Epoch 885: val_loss did not improve from 17.38395
196/196 - 62s - loss: 17.3265 - MinusLogProbMetric: 17.3265 - val_loss: 17.4491 - val_MinusLogProbMetric: 17.4491 - lr: 2.7778e-05 - 62s/epoch - 317ms/step
Epoch 886/1000
2023-10-24 05:53:03.357 
Epoch 886/1000 
	 loss: 17.3367, MinusLogProbMetric: 17.3367, val_loss: 17.4373, val_MinusLogProbMetric: 17.4373

Epoch 886: val_loss did not improve from 17.38395
196/196 - 65s - loss: 17.3367 - MinusLogProbMetric: 17.3367 - val_loss: 17.4373 - val_MinusLogProbMetric: 17.4373 - lr: 2.7778e-05 - 65s/epoch - 334ms/step
Epoch 887/1000
2023-10-24 05:54:03.637 
Epoch 887/1000 
	 loss: 17.3351, MinusLogProbMetric: 17.3351, val_loss: 17.4365, val_MinusLogProbMetric: 17.4365

Epoch 887: val_loss did not improve from 17.38395
196/196 - 60s - loss: 17.3351 - MinusLogProbMetric: 17.3351 - val_loss: 17.4365 - val_MinusLogProbMetric: 17.4365 - lr: 2.7778e-05 - 60s/epoch - 308ms/step
Epoch 888/1000
2023-10-24 05:55:06.262 
Epoch 888/1000 
	 loss: 17.3315, MinusLogProbMetric: 17.3315, val_loss: 17.4404, val_MinusLogProbMetric: 17.4404

Epoch 888: val_loss did not improve from 17.38395
196/196 - 63s - loss: 17.3315 - MinusLogProbMetric: 17.3315 - val_loss: 17.4404 - val_MinusLogProbMetric: 17.4404 - lr: 2.7778e-05 - 63s/epoch - 320ms/step
Epoch 889/1000
2023-10-24 05:56:11.931 
Epoch 889/1000 
	 loss: 17.3462, MinusLogProbMetric: 17.3462, val_loss: 17.3952, val_MinusLogProbMetric: 17.3952

Epoch 889: val_loss did not improve from 17.38395
196/196 - 66s - loss: 17.3462 - MinusLogProbMetric: 17.3462 - val_loss: 17.3952 - val_MinusLogProbMetric: 17.3952 - lr: 2.7778e-05 - 66s/epoch - 335ms/step
Epoch 890/1000
2023-10-24 05:57:17.419 
Epoch 890/1000 
	 loss: 17.3329, MinusLogProbMetric: 17.3329, val_loss: 17.4177, val_MinusLogProbMetric: 17.4177

Epoch 890: val_loss did not improve from 17.38395
196/196 - 65s - loss: 17.3329 - MinusLogProbMetric: 17.3329 - val_loss: 17.4177 - val_MinusLogProbMetric: 17.4177 - lr: 2.7778e-05 - 65s/epoch - 334ms/step
Epoch 891/1000
2023-10-24 05:58:23.022 
Epoch 891/1000 
	 loss: 17.3246, MinusLogProbMetric: 17.3246, val_loss: 17.4168, val_MinusLogProbMetric: 17.4168

Epoch 891: val_loss did not improve from 17.38395
196/196 - 66s - loss: 17.3246 - MinusLogProbMetric: 17.3246 - val_loss: 17.4168 - val_MinusLogProbMetric: 17.4168 - lr: 2.7778e-05 - 66s/epoch - 335ms/step
Epoch 892/1000
2023-10-24 05:59:23.389 
Epoch 892/1000 
	 loss: 17.3254, MinusLogProbMetric: 17.3254, val_loss: 17.3927, val_MinusLogProbMetric: 17.3927

Epoch 892: val_loss did not improve from 17.38395
196/196 - 60s - loss: 17.3254 - MinusLogProbMetric: 17.3254 - val_loss: 17.3927 - val_MinusLogProbMetric: 17.3927 - lr: 2.7778e-05 - 60s/epoch - 308ms/step
Epoch 893/1000
2023-10-24 06:00:29.479 
Epoch 893/1000 
	 loss: 17.3294, MinusLogProbMetric: 17.3294, val_loss: 17.3803, val_MinusLogProbMetric: 17.3803

Epoch 893: val_loss improved from 17.38395 to 17.38027, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 67s - loss: 17.3294 - MinusLogProbMetric: 17.3294 - val_loss: 17.3803 - val_MinusLogProbMetric: 17.3803 - lr: 2.7778e-05 - 67s/epoch - 344ms/step
Epoch 894/1000
2023-10-24 06:01:32.457 
Epoch 894/1000 
	 loss: 17.3265, MinusLogProbMetric: 17.3265, val_loss: 17.4088, val_MinusLogProbMetric: 17.4088

Epoch 894: val_loss did not improve from 17.38027
196/196 - 62s - loss: 17.3265 - MinusLogProbMetric: 17.3265 - val_loss: 17.4088 - val_MinusLogProbMetric: 17.4088 - lr: 2.7778e-05 - 62s/epoch - 315ms/step
Epoch 895/1000
2023-10-24 06:02:35.152 
Epoch 895/1000 
	 loss: 17.3260, MinusLogProbMetric: 17.3260, val_loss: 17.4210, val_MinusLogProbMetric: 17.4210

Epoch 895: val_loss did not improve from 17.38027
196/196 - 63s - loss: 17.3260 - MinusLogProbMetric: 17.3260 - val_loss: 17.4210 - val_MinusLogProbMetric: 17.4210 - lr: 2.7778e-05 - 63s/epoch - 320ms/step
Epoch 896/1000
2023-10-24 06:03:46.251 
Epoch 896/1000 
	 loss: 17.3217, MinusLogProbMetric: 17.3217, val_loss: 17.3956, val_MinusLogProbMetric: 17.3956

Epoch 896: val_loss did not improve from 17.38027
196/196 - 71s - loss: 17.3217 - MinusLogProbMetric: 17.3217 - val_loss: 17.3956 - val_MinusLogProbMetric: 17.3956 - lr: 2.7778e-05 - 71s/epoch - 363ms/step
Epoch 897/1000
2023-10-24 06:04:51.181 
Epoch 897/1000 
	 loss: 17.3371, MinusLogProbMetric: 17.3371, val_loss: 17.4046, val_MinusLogProbMetric: 17.4046

Epoch 897: val_loss did not improve from 17.38027
196/196 - 65s - loss: 17.3371 - MinusLogProbMetric: 17.3371 - val_loss: 17.4046 - val_MinusLogProbMetric: 17.4046 - lr: 2.7778e-05 - 65s/epoch - 331ms/step
Epoch 898/1000
2023-10-24 06:05:52.602 
Epoch 898/1000 
	 loss: 17.3275, MinusLogProbMetric: 17.3275, val_loss: 17.3939, val_MinusLogProbMetric: 17.3939

Epoch 898: val_loss did not improve from 17.38027
196/196 - 61s - loss: 17.3275 - MinusLogProbMetric: 17.3275 - val_loss: 17.3939 - val_MinusLogProbMetric: 17.3939 - lr: 2.7778e-05 - 61s/epoch - 313ms/step
Epoch 899/1000
2023-10-24 06:06:56.216 
Epoch 899/1000 
	 loss: 17.3155, MinusLogProbMetric: 17.3155, val_loss: 17.4399, val_MinusLogProbMetric: 17.4399

Epoch 899: val_loss did not improve from 17.38027
196/196 - 64s - loss: 17.3155 - MinusLogProbMetric: 17.3155 - val_loss: 17.4399 - val_MinusLogProbMetric: 17.4399 - lr: 2.7778e-05 - 64s/epoch - 325ms/step
Epoch 900/1000
2023-10-24 06:08:00.923 
Epoch 900/1000 
	 loss: 17.3244, MinusLogProbMetric: 17.3244, val_loss: 17.3890, val_MinusLogProbMetric: 17.3890

Epoch 900: val_loss did not improve from 17.38027
196/196 - 65s - loss: 17.3244 - MinusLogProbMetric: 17.3244 - val_loss: 17.3890 - val_MinusLogProbMetric: 17.3890 - lr: 2.7778e-05 - 65s/epoch - 330ms/step
Epoch 901/1000
2023-10-24 06:09:04.762 
Epoch 901/1000 
	 loss: 17.3215, MinusLogProbMetric: 17.3215, val_loss: 17.4217, val_MinusLogProbMetric: 17.4217

Epoch 901: val_loss did not improve from 17.38027
196/196 - 64s - loss: 17.3215 - MinusLogProbMetric: 17.3215 - val_loss: 17.4217 - val_MinusLogProbMetric: 17.4217 - lr: 2.7778e-05 - 64s/epoch - 326ms/step
Epoch 902/1000
2023-10-24 06:10:06.194 
Epoch 902/1000 
	 loss: 17.3160, MinusLogProbMetric: 17.3160, val_loss: 17.4667, val_MinusLogProbMetric: 17.4667

Epoch 902: val_loss did not improve from 17.38027
196/196 - 61s - loss: 17.3160 - MinusLogProbMetric: 17.3160 - val_loss: 17.4667 - val_MinusLogProbMetric: 17.4667 - lr: 2.7778e-05 - 61s/epoch - 313ms/step
Epoch 903/1000
2023-10-24 06:11:07.403 
Epoch 903/1000 
	 loss: 17.3151, MinusLogProbMetric: 17.3151, val_loss: 17.4019, val_MinusLogProbMetric: 17.4019

Epoch 903: val_loss did not improve from 17.38027
196/196 - 61s - loss: 17.3151 - MinusLogProbMetric: 17.3151 - val_loss: 17.4019 - val_MinusLogProbMetric: 17.4019 - lr: 2.7778e-05 - 61s/epoch - 312ms/step
Epoch 904/1000
2023-10-24 06:12:11.621 
Epoch 904/1000 
	 loss: 17.4268, MinusLogProbMetric: 17.4268, val_loss: 17.5704, val_MinusLogProbMetric: 17.5704

Epoch 904: val_loss did not improve from 17.38027
196/196 - 64s - loss: 17.4268 - MinusLogProbMetric: 17.4268 - val_loss: 17.5704 - val_MinusLogProbMetric: 17.5704 - lr: 2.7778e-05 - 64s/epoch - 328ms/step
Epoch 905/1000
2023-10-24 06:13:18.636 
Epoch 905/1000 
	 loss: 17.3251, MinusLogProbMetric: 17.3251, val_loss: 17.3864, val_MinusLogProbMetric: 17.3864

Epoch 905: val_loss did not improve from 17.38027
196/196 - 67s - loss: 17.3251 - MinusLogProbMetric: 17.3251 - val_loss: 17.3864 - val_MinusLogProbMetric: 17.3864 - lr: 2.7778e-05 - 67s/epoch - 342ms/step
Epoch 906/1000
2023-10-24 06:14:18.386 
Epoch 906/1000 
	 loss: 17.3248, MinusLogProbMetric: 17.3248, val_loss: 17.3680, val_MinusLogProbMetric: 17.3680

Epoch 906: val_loss improved from 17.38027 to 17.36795, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 61s - loss: 17.3248 - MinusLogProbMetric: 17.3248 - val_loss: 17.3680 - val_MinusLogProbMetric: 17.3680 - lr: 2.7778e-05 - 61s/epoch - 312ms/step
Epoch 907/1000
2023-10-24 06:15:25.684 
Epoch 907/1000 
	 loss: 17.3231, MinusLogProbMetric: 17.3231, val_loss: 17.3964, val_MinusLogProbMetric: 17.3964

Epoch 907: val_loss did not improve from 17.36795
196/196 - 66s - loss: 17.3231 - MinusLogProbMetric: 17.3231 - val_loss: 17.3964 - val_MinusLogProbMetric: 17.3964 - lr: 2.7778e-05 - 66s/epoch - 336ms/step
Epoch 908/1000
2023-10-24 06:16:27.818 
Epoch 908/1000 
	 loss: 17.3306, MinusLogProbMetric: 17.3306, val_loss: 17.3709, val_MinusLogProbMetric: 17.3709

Epoch 908: val_loss did not improve from 17.36795
196/196 - 62s - loss: 17.3306 - MinusLogProbMetric: 17.3306 - val_loss: 17.3709 - val_MinusLogProbMetric: 17.3709 - lr: 2.7778e-05 - 62s/epoch - 317ms/step
Epoch 909/1000
2023-10-24 06:17:32.267 
Epoch 909/1000 
	 loss: 17.3208, MinusLogProbMetric: 17.3208, val_loss: 17.4034, val_MinusLogProbMetric: 17.4034

Epoch 909: val_loss did not improve from 17.36795
196/196 - 64s - loss: 17.3208 - MinusLogProbMetric: 17.3208 - val_loss: 17.4034 - val_MinusLogProbMetric: 17.4034 - lr: 2.7778e-05 - 64s/epoch - 329ms/step
Epoch 910/1000
2023-10-24 06:18:45.960 
Epoch 910/1000 
	 loss: 17.3069, MinusLogProbMetric: 17.3069, val_loss: 17.3806, val_MinusLogProbMetric: 17.3806

Epoch 910: val_loss did not improve from 17.36795
196/196 - 74s - loss: 17.3069 - MinusLogProbMetric: 17.3069 - val_loss: 17.3806 - val_MinusLogProbMetric: 17.3806 - lr: 2.7778e-05 - 74s/epoch - 376ms/step
Epoch 911/1000
2023-10-24 06:19:57.150 
Epoch 911/1000 
	 loss: 17.3225, MinusLogProbMetric: 17.3225, val_loss: 17.3686, val_MinusLogProbMetric: 17.3686

Epoch 911: val_loss did not improve from 17.36795
196/196 - 71s - loss: 17.3225 - MinusLogProbMetric: 17.3225 - val_loss: 17.3686 - val_MinusLogProbMetric: 17.3686 - lr: 2.7778e-05 - 71s/epoch - 363ms/step
Epoch 912/1000
2023-10-24 06:21:07.681 
Epoch 912/1000 
	 loss: 17.3216, MinusLogProbMetric: 17.3216, val_loss: 17.4066, val_MinusLogProbMetric: 17.4066

Epoch 912: val_loss did not improve from 17.36795
196/196 - 71s - loss: 17.3216 - MinusLogProbMetric: 17.3216 - val_loss: 17.4066 - val_MinusLogProbMetric: 17.4066 - lr: 2.7778e-05 - 71s/epoch - 360ms/step
Epoch 913/1000
2023-10-24 06:22:20.833 
Epoch 913/1000 
	 loss: 17.3134, MinusLogProbMetric: 17.3134, val_loss: 17.3980, val_MinusLogProbMetric: 17.3980

Epoch 913: val_loss did not improve from 17.36795
196/196 - 73s - loss: 17.3134 - MinusLogProbMetric: 17.3134 - val_loss: 17.3980 - val_MinusLogProbMetric: 17.3980 - lr: 2.7778e-05 - 73s/epoch - 373ms/step
Epoch 914/1000
2023-10-24 06:23:31.409 
Epoch 914/1000 
	 loss: 17.3154, MinusLogProbMetric: 17.3154, val_loss: 17.3853, val_MinusLogProbMetric: 17.3853

Epoch 914: val_loss did not improve from 17.36795
196/196 - 71s - loss: 17.3154 - MinusLogProbMetric: 17.3154 - val_loss: 17.3853 - val_MinusLogProbMetric: 17.3853 - lr: 2.7778e-05 - 71s/epoch - 360ms/step
Epoch 915/1000
2023-10-24 06:24:41.942 
Epoch 915/1000 
	 loss: 17.3369, MinusLogProbMetric: 17.3369, val_loss: 17.3999, val_MinusLogProbMetric: 17.3999

Epoch 915: val_loss did not improve from 17.36795
196/196 - 71s - loss: 17.3369 - MinusLogProbMetric: 17.3369 - val_loss: 17.3999 - val_MinusLogProbMetric: 17.3999 - lr: 2.7778e-05 - 71s/epoch - 360ms/step
Epoch 916/1000
2023-10-24 06:25:52.264 
Epoch 916/1000 
	 loss: 17.3159, MinusLogProbMetric: 17.3159, val_loss: 17.4737, val_MinusLogProbMetric: 17.4737

Epoch 916: val_loss did not improve from 17.36795
196/196 - 70s - loss: 17.3159 - MinusLogProbMetric: 17.3159 - val_loss: 17.4737 - val_MinusLogProbMetric: 17.4737 - lr: 2.7778e-05 - 70s/epoch - 359ms/step
Epoch 917/1000
2023-10-24 06:27:01.286 
Epoch 917/1000 
	 loss: 17.3041, MinusLogProbMetric: 17.3041, val_loss: 17.3889, val_MinusLogProbMetric: 17.3889

Epoch 917: val_loss did not improve from 17.36795
196/196 - 69s - loss: 17.3041 - MinusLogProbMetric: 17.3041 - val_loss: 17.3889 - val_MinusLogProbMetric: 17.3889 - lr: 2.7778e-05 - 69s/epoch - 352ms/step
Epoch 918/1000
2023-10-24 06:28:12.830 
Epoch 918/1000 
	 loss: 17.3216, MinusLogProbMetric: 17.3216, val_loss: 17.4356, val_MinusLogProbMetric: 17.4356

Epoch 918: val_loss did not improve from 17.36795
196/196 - 72s - loss: 17.3216 - MinusLogProbMetric: 17.3216 - val_loss: 17.4356 - val_MinusLogProbMetric: 17.4356 - lr: 2.7778e-05 - 72s/epoch - 365ms/step
Epoch 919/1000
2023-10-24 06:29:24.142 
Epoch 919/1000 
	 loss: 17.3192, MinusLogProbMetric: 17.3192, val_loss: 17.4644, val_MinusLogProbMetric: 17.4644

Epoch 919: val_loss did not improve from 17.36795
196/196 - 71s - loss: 17.3192 - MinusLogProbMetric: 17.3192 - val_loss: 17.4644 - val_MinusLogProbMetric: 17.4644 - lr: 2.7778e-05 - 71s/epoch - 364ms/step
Epoch 920/1000
2023-10-24 06:30:32.718 
Epoch 920/1000 
	 loss: 17.3246, MinusLogProbMetric: 17.3246, val_loss: 17.4667, val_MinusLogProbMetric: 17.4667

Epoch 920: val_loss did not improve from 17.36795
196/196 - 69s - loss: 17.3246 - MinusLogProbMetric: 17.3246 - val_loss: 17.4667 - val_MinusLogProbMetric: 17.4667 - lr: 2.7778e-05 - 69s/epoch - 350ms/step
Epoch 921/1000
2023-10-24 06:31:41.897 
Epoch 921/1000 
	 loss: 17.3245, MinusLogProbMetric: 17.3245, val_loss: 17.4216, val_MinusLogProbMetric: 17.4216

Epoch 921: val_loss did not improve from 17.36795
196/196 - 69s - loss: 17.3245 - MinusLogProbMetric: 17.3245 - val_loss: 17.4216 - val_MinusLogProbMetric: 17.4216 - lr: 2.7778e-05 - 69s/epoch - 353ms/step
Epoch 922/1000
2023-10-24 06:32:53.783 
Epoch 922/1000 
	 loss: 17.3254, MinusLogProbMetric: 17.3254, val_loss: 17.3958, val_MinusLogProbMetric: 17.3958

Epoch 922: val_loss did not improve from 17.36795
196/196 - 72s - loss: 17.3254 - MinusLogProbMetric: 17.3254 - val_loss: 17.3958 - val_MinusLogProbMetric: 17.3958 - lr: 2.7778e-05 - 72s/epoch - 367ms/step
Epoch 923/1000
2023-10-24 06:34:05.339 
Epoch 923/1000 
	 loss: 17.3195, MinusLogProbMetric: 17.3195, val_loss: 17.3813, val_MinusLogProbMetric: 17.3813

Epoch 923: val_loss did not improve from 17.36795
196/196 - 72s - loss: 17.3195 - MinusLogProbMetric: 17.3195 - val_loss: 17.3813 - val_MinusLogProbMetric: 17.3813 - lr: 2.7778e-05 - 72s/epoch - 365ms/step
Epoch 924/1000
2023-10-24 06:35:15.104 
Epoch 924/1000 
	 loss: 17.3191, MinusLogProbMetric: 17.3191, val_loss: 17.3835, val_MinusLogProbMetric: 17.3835

Epoch 924: val_loss did not improve from 17.36795
196/196 - 70s - loss: 17.3191 - MinusLogProbMetric: 17.3191 - val_loss: 17.3835 - val_MinusLogProbMetric: 17.3835 - lr: 2.7778e-05 - 70s/epoch - 356ms/step
Epoch 925/1000
2023-10-24 06:36:26.240 
Epoch 925/1000 
	 loss: 17.3118, MinusLogProbMetric: 17.3118, val_loss: 17.4013, val_MinusLogProbMetric: 17.4013

Epoch 925: val_loss did not improve from 17.36795
196/196 - 71s - loss: 17.3118 - MinusLogProbMetric: 17.3118 - val_loss: 17.4013 - val_MinusLogProbMetric: 17.4013 - lr: 2.7778e-05 - 71s/epoch - 363ms/step
Epoch 926/1000
2023-10-24 06:37:39.087 
Epoch 926/1000 
	 loss: 17.3166, MinusLogProbMetric: 17.3166, val_loss: 17.3825, val_MinusLogProbMetric: 17.3825

Epoch 926: val_loss did not improve from 17.36795
196/196 - 73s - loss: 17.3166 - MinusLogProbMetric: 17.3166 - val_loss: 17.3825 - val_MinusLogProbMetric: 17.3825 - lr: 2.7778e-05 - 73s/epoch - 372ms/step
Epoch 927/1000
2023-10-24 06:38:48.015 
Epoch 927/1000 
	 loss: 17.3219, MinusLogProbMetric: 17.3219, val_loss: 17.4380, val_MinusLogProbMetric: 17.4380

Epoch 927: val_loss did not improve from 17.36795
196/196 - 69s - loss: 17.3219 - MinusLogProbMetric: 17.3219 - val_loss: 17.4380 - val_MinusLogProbMetric: 17.4380 - lr: 2.7778e-05 - 69s/epoch - 352ms/step
Epoch 928/1000
2023-10-24 06:39:57.664 
Epoch 928/1000 
	 loss: 17.3143, MinusLogProbMetric: 17.3143, val_loss: 17.3854, val_MinusLogProbMetric: 17.3854

Epoch 928: val_loss did not improve from 17.36795
196/196 - 70s - loss: 17.3143 - MinusLogProbMetric: 17.3143 - val_loss: 17.3854 - val_MinusLogProbMetric: 17.3854 - lr: 2.7778e-05 - 70s/epoch - 355ms/step
Epoch 929/1000
2023-10-24 06:41:06.902 
Epoch 929/1000 
	 loss: 17.3064, MinusLogProbMetric: 17.3064, val_loss: 17.3840, val_MinusLogProbMetric: 17.3840

Epoch 929: val_loss did not improve from 17.36795
196/196 - 69s - loss: 17.3064 - MinusLogProbMetric: 17.3064 - val_loss: 17.3840 - val_MinusLogProbMetric: 17.3840 - lr: 2.7778e-05 - 69s/epoch - 353ms/step
Epoch 930/1000
2023-10-24 06:42:16.633 
Epoch 930/1000 
	 loss: 17.3203, MinusLogProbMetric: 17.3203, val_loss: 17.3883, val_MinusLogProbMetric: 17.3883

Epoch 930: val_loss did not improve from 17.36795
196/196 - 70s - loss: 17.3203 - MinusLogProbMetric: 17.3203 - val_loss: 17.3883 - val_MinusLogProbMetric: 17.3883 - lr: 2.7778e-05 - 70s/epoch - 356ms/step
Epoch 931/1000
2023-10-24 06:43:26.152 
Epoch 931/1000 
	 loss: 17.3292, MinusLogProbMetric: 17.3292, val_loss: 17.4283, val_MinusLogProbMetric: 17.4283

Epoch 931: val_loss did not improve from 17.36795
196/196 - 70s - loss: 17.3292 - MinusLogProbMetric: 17.3292 - val_loss: 17.4283 - val_MinusLogProbMetric: 17.4283 - lr: 2.7778e-05 - 70s/epoch - 355ms/step
Epoch 932/1000
2023-10-24 06:44:33.643 
Epoch 932/1000 
	 loss: 17.3271, MinusLogProbMetric: 17.3271, val_loss: 17.4510, val_MinusLogProbMetric: 17.4510

Epoch 932: val_loss did not improve from 17.36795
196/196 - 67s - loss: 17.3271 - MinusLogProbMetric: 17.3271 - val_loss: 17.4510 - val_MinusLogProbMetric: 17.4510 - lr: 2.7778e-05 - 67s/epoch - 344ms/step
Epoch 933/1000
2023-10-24 06:45:42.307 
Epoch 933/1000 
	 loss: 17.3091, MinusLogProbMetric: 17.3091, val_loss: 17.4066, val_MinusLogProbMetric: 17.4066

Epoch 933: val_loss did not improve from 17.36795
196/196 - 69s - loss: 17.3091 - MinusLogProbMetric: 17.3091 - val_loss: 17.4066 - val_MinusLogProbMetric: 17.4066 - lr: 2.7778e-05 - 69s/epoch - 350ms/step
Epoch 934/1000
2023-10-24 06:46:52.319 
Epoch 934/1000 
	 loss: 17.3127, MinusLogProbMetric: 17.3127, val_loss: 17.4269, val_MinusLogProbMetric: 17.4269

Epoch 934: val_loss did not improve from 17.36795
196/196 - 70s - loss: 17.3127 - MinusLogProbMetric: 17.3127 - val_loss: 17.4269 - val_MinusLogProbMetric: 17.4269 - lr: 2.7778e-05 - 70s/epoch - 357ms/step
Epoch 935/1000
2023-10-24 06:48:02.161 
Epoch 935/1000 
	 loss: 17.3066, MinusLogProbMetric: 17.3066, val_loss: 17.4057, val_MinusLogProbMetric: 17.4057

Epoch 935: val_loss did not improve from 17.36795
196/196 - 70s - loss: 17.3066 - MinusLogProbMetric: 17.3066 - val_loss: 17.4057 - val_MinusLogProbMetric: 17.4057 - lr: 2.7778e-05 - 70s/epoch - 356ms/step
Epoch 936/1000
2023-10-24 06:49:10.501 
Epoch 936/1000 
	 loss: 17.2943, MinusLogProbMetric: 17.2943, val_loss: 17.3881, val_MinusLogProbMetric: 17.3881

Epoch 936: val_loss did not improve from 17.36795
196/196 - 68s - loss: 17.2943 - MinusLogProbMetric: 17.2943 - val_loss: 17.3881 - val_MinusLogProbMetric: 17.3881 - lr: 2.7778e-05 - 68s/epoch - 349ms/step
Epoch 937/1000
2023-10-24 06:50:19.767 
Epoch 937/1000 
	 loss: 17.3062, MinusLogProbMetric: 17.3062, val_loss: 17.3828, val_MinusLogProbMetric: 17.3828

Epoch 937: val_loss did not improve from 17.36795
196/196 - 69s - loss: 17.3062 - MinusLogProbMetric: 17.3062 - val_loss: 17.3828 - val_MinusLogProbMetric: 17.3828 - lr: 2.7778e-05 - 69s/epoch - 353ms/step
Epoch 938/1000
2023-10-24 06:51:29.393 
Epoch 938/1000 
	 loss: 17.3074, MinusLogProbMetric: 17.3074, val_loss: 17.4261, val_MinusLogProbMetric: 17.4261

Epoch 938: val_loss did not improve from 17.36795
196/196 - 70s - loss: 17.3074 - MinusLogProbMetric: 17.3074 - val_loss: 17.4261 - val_MinusLogProbMetric: 17.4261 - lr: 2.7778e-05 - 70s/epoch - 355ms/step
Epoch 939/1000
2023-10-24 06:52:36.420 
Epoch 939/1000 
	 loss: 17.3198, MinusLogProbMetric: 17.3198, val_loss: 17.4398, val_MinusLogProbMetric: 17.4398

Epoch 939: val_loss did not improve from 17.36795
196/196 - 67s - loss: 17.3198 - MinusLogProbMetric: 17.3198 - val_loss: 17.4398 - val_MinusLogProbMetric: 17.4398 - lr: 2.7778e-05 - 67s/epoch - 342ms/step
Epoch 940/1000
2023-10-24 06:53:44.748 
Epoch 940/1000 
	 loss: 17.3074, MinusLogProbMetric: 17.3074, val_loss: 17.3938, val_MinusLogProbMetric: 17.3938

Epoch 940: val_loss did not improve from 17.36795
196/196 - 68s - loss: 17.3074 - MinusLogProbMetric: 17.3074 - val_loss: 17.3938 - val_MinusLogProbMetric: 17.3938 - lr: 2.7778e-05 - 68s/epoch - 349ms/step
Epoch 941/1000
2023-10-24 06:54:54.118 
Epoch 941/1000 
	 loss: 17.3173, MinusLogProbMetric: 17.3173, val_loss: 17.4169, val_MinusLogProbMetric: 17.4169

Epoch 941: val_loss did not improve from 17.36795
196/196 - 69s - loss: 17.3173 - MinusLogProbMetric: 17.3173 - val_loss: 17.4169 - val_MinusLogProbMetric: 17.4169 - lr: 2.7778e-05 - 69s/epoch - 354ms/step
Epoch 942/1000
2023-10-24 06:56:04.070 
Epoch 942/1000 
	 loss: 17.3200, MinusLogProbMetric: 17.3200, val_loss: 17.3675, val_MinusLogProbMetric: 17.3675

Epoch 942: val_loss improved from 17.36795 to 17.36749, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 71s - loss: 17.3200 - MinusLogProbMetric: 17.3200 - val_loss: 17.3675 - val_MinusLogProbMetric: 17.3675 - lr: 2.7778e-05 - 71s/epoch - 363ms/step
Epoch 943/1000
2023-10-24 06:57:13.301 
Epoch 943/1000 
	 loss: 17.3254, MinusLogProbMetric: 17.3254, val_loss: 17.4040, val_MinusLogProbMetric: 17.4040

Epoch 943: val_loss did not improve from 17.36749
196/196 - 68s - loss: 17.3254 - MinusLogProbMetric: 17.3254 - val_loss: 17.4040 - val_MinusLogProbMetric: 17.4040 - lr: 2.7778e-05 - 68s/epoch - 347ms/step
Epoch 944/1000
2023-10-24 06:58:23.799 
Epoch 944/1000 
	 loss: 17.3174, MinusLogProbMetric: 17.3174, val_loss: 17.4571, val_MinusLogProbMetric: 17.4571

Epoch 944: val_loss did not improve from 17.36749
196/196 - 70s - loss: 17.3174 - MinusLogProbMetric: 17.3174 - val_loss: 17.4571 - val_MinusLogProbMetric: 17.4571 - lr: 2.7778e-05 - 70s/epoch - 360ms/step
Epoch 945/1000
2023-10-24 06:59:32.851 
Epoch 945/1000 
	 loss: 17.3183, MinusLogProbMetric: 17.3183, val_loss: 17.4340, val_MinusLogProbMetric: 17.4340

Epoch 945: val_loss did not improve from 17.36749
196/196 - 69s - loss: 17.3183 - MinusLogProbMetric: 17.3183 - val_loss: 17.4340 - val_MinusLogProbMetric: 17.4340 - lr: 2.7778e-05 - 69s/epoch - 352ms/step
Epoch 946/1000
2023-10-24 07:00:43.403 
Epoch 946/1000 
	 loss: 17.3130, MinusLogProbMetric: 17.3130, val_loss: 17.4039, val_MinusLogProbMetric: 17.4039

Epoch 946: val_loss did not improve from 17.36749
196/196 - 71s - loss: 17.3130 - MinusLogProbMetric: 17.3130 - val_loss: 17.4039 - val_MinusLogProbMetric: 17.4039 - lr: 2.7778e-05 - 71s/epoch - 360ms/step
Epoch 947/1000
2023-10-24 07:01:53.208 
Epoch 947/1000 
	 loss: 17.3021, MinusLogProbMetric: 17.3021, val_loss: 17.4402, val_MinusLogProbMetric: 17.4402

Epoch 947: val_loss did not improve from 17.36749
196/196 - 70s - loss: 17.3021 - MinusLogProbMetric: 17.3021 - val_loss: 17.4402 - val_MinusLogProbMetric: 17.4402 - lr: 2.7778e-05 - 70s/epoch - 356ms/step
Epoch 948/1000
2023-10-24 07:03:01.081 
Epoch 948/1000 
	 loss: 17.3118, MinusLogProbMetric: 17.3118, val_loss: 17.4062, val_MinusLogProbMetric: 17.4062

Epoch 948: val_loss did not improve from 17.36749
196/196 - 68s - loss: 17.3118 - MinusLogProbMetric: 17.3118 - val_loss: 17.4062 - val_MinusLogProbMetric: 17.4062 - lr: 2.7778e-05 - 68s/epoch - 346ms/step
Epoch 949/1000
2023-10-24 07:04:08.623 
Epoch 949/1000 
	 loss: 17.3004, MinusLogProbMetric: 17.3004, val_loss: 17.3665, val_MinusLogProbMetric: 17.3665

Epoch 949: val_loss improved from 17.36749 to 17.36645, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 68s - loss: 17.3004 - MinusLogProbMetric: 17.3004 - val_loss: 17.3665 - val_MinusLogProbMetric: 17.3665 - lr: 2.7778e-05 - 68s/epoch - 349ms/step
Epoch 950/1000
2023-10-24 07:05:17.245 
Epoch 950/1000 
	 loss: 17.3066, MinusLogProbMetric: 17.3066, val_loss: 17.3789, val_MinusLogProbMetric: 17.3789

Epoch 950: val_loss did not improve from 17.36645
196/196 - 68s - loss: 17.3066 - MinusLogProbMetric: 17.3066 - val_loss: 17.3789 - val_MinusLogProbMetric: 17.3789 - lr: 2.7778e-05 - 68s/epoch - 346ms/step
Epoch 951/1000
2023-10-24 07:06:23.085 
Epoch 951/1000 
	 loss: 17.3115, MinusLogProbMetric: 17.3115, val_loss: 17.4027, val_MinusLogProbMetric: 17.4027

Epoch 951: val_loss did not improve from 17.36645
196/196 - 66s - loss: 17.3115 - MinusLogProbMetric: 17.3115 - val_loss: 17.4027 - val_MinusLogProbMetric: 17.4027 - lr: 2.7778e-05 - 66s/epoch - 336ms/step
Epoch 952/1000
2023-10-24 07:07:32.734 
Epoch 952/1000 
	 loss: 17.3077, MinusLogProbMetric: 17.3077, val_loss: 17.3902, val_MinusLogProbMetric: 17.3902

Epoch 952: val_loss did not improve from 17.36645
196/196 - 70s - loss: 17.3077 - MinusLogProbMetric: 17.3077 - val_loss: 17.3902 - val_MinusLogProbMetric: 17.3902 - lr: 2.7778e-05 - 70s/epoch - 355ms/step
Epoch 953/1000
2023-10-24 07:08:41.678 
Epoch 953/1000 
	 loss: 17.3285, MinusLogProbMetric: 17.3285, val_loss: 17.4858, val_MinusLogProbMetric: 17.4858

Epoch 953: val_loss did not improve from 17.36645
196/196 - 69s - loss: 17.3285 - MinusLogProbMetric: 17.3285 - val_loss: 17.4858 - val_MinusLogProbMetric: 17.4858 - lr: 2.7778e-05 - 69s/epoch - 352ms/step
Epoch 954/1000
2023-10-24 07:09:52.386 
Epoch 954/1000 
	 loss: 17.3254, MinusLogProbMetric: 17.3254, val_loss: 17.4520, val_MinusLogProbMetric: 17.4520

Epoch 954: val_loss did not improve from 17.36645
196/196 - 71s - loss: 17.3254 - MinusLogProbMetric: 17.3254 - val_loss: 17.4520 - val_MinusLogProbMetric: 17.4520 - lr: 2.7778e-05 - 71s/epoch - 361ms/step
Epoch 955/1000
2023-10-24 07:11:02.482 
Epoch 955/1000 
	 loss: 17.3176, MinusLogProbMetric: 17.3176, val_loss: 17.3675, val_MinusLogProbMetric: 17.3675

Epoch 955: val_loss did not improve from 17.36645
196/196 - 70s - loss: 17.3176 - MinusLogProbMetric: 17.3176 - val_loss: 17.3675 - val_MinusLogProbMetric: 17.3675 - lr: 2.7778e-05 - 70s/epoch - 358ms/step
Epoch 956/1000
2023-10-24 07:12:11.221 
Epoch 956/1000 
	 loss: 17.3100, MinusLogProbMetric: 17.3100, val_loss: 17.3732, val_MinusLogProbMetric: 17.3732

Epoch 956: val_loss did not improve from 17.36645
196/196 - 69s - loss: 17.3100 - MinusLogProbMetric: 17.3100 - val_loss: 17.3732 - val_MinusLogProbMetric: 17.3732 - lr: 2.7778e-05 - 69s/epoch - 351ms/step
Epoch 957/1000
2023-10-24 07:13:20.661 
Epoch 957/1000 
	 loss: 17.3167, MinusLogProbMetric: 17.3167, val_loss: 17.3667, val_MinusLogProbMetric: 17.3667

Epoch 957: val_loss did not improve from 17.36645
196/196 - 69s - loss: 17.3167 - MinusLogProbMetric: 17.3167 - val_loss: 17.3667 - val_MinusLogProbMetric: 17.3667 - lr: 2.7778e-05 - 69s/epoch - 354ms/step
Epoch 958/1000
2023-10-24 07:14:29.492 
Epoch 958/1000 
	 loss: 17.2957, MinusLogProbMetric: 17.2957, val_loss: 17.4039, val_MinusLogProbMetric: 17.4039

Epoch 958: val_loss did not improve from 17.36645
196/196 - 69s - loss: 17.2957 - MinusLogProbMetric: 17.2957 - val_loss: 17.4039 - val_MinusLogProbMetric: 17.4039 - lr: 2.7778e-05 - 69s/epoch - 351ms/step
Epoch 959/1000
2023-10-24 07:15:39.725 
Epoch 959/1000 
	 loss: 17.3017, MinusLogProbMetric: 17.3017, val_loss: 17.3636, val_MinusLogProbMetric: 17.3636

Epoch 959: val_loss improved from 17.36645 to 17.36362, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 71s - loss: 17.3017 - MinusLogProbMetric: 17.3017 - val_loss: 17.3636 - val_MinusLogProbMetric: 17.3636 - lr: 2.7778e-05 - 71s/epoch - 364ms/step
Epoch 960/1000
2023-10-24 07:16:49.448 
Epoch 960/1000 
	 loss: 17.2997, MinusLogProbMetric: 17.2997, val_loss: 17.3735, val_MinusLogProbMetric: 17.3735

Epoch 960: val_loss did not improve from 17.36362
196/196 - 69s - loss: 17.2997 - MinusLogProbMetric: 17.2997 - val_loss: 17.3735 - val_MinusLogProbMetric: 17.3735 - lr: 2.7778e-05 - 69s/epoch - 350ms/step
Epoch 961/1000
2023-10-24 07:17:58.008 
Epoch 961/1000 
	 loss: 17.3798, MinusLogProbMetric: 17.3798, val_loss: 17.4903, val_MinusLogProbMetric: 17.4903

Epoch 961: val_loss did not improve from 17.36362
196/196 - 69s - loss: 17.3798 - MinusLogProbMetric: 17.3798 - val_loss: 17.4903 - val_MinusLogProbMetric: 17.4903 - lr: 2.7778e-05 - 69s/epoch - 350ms/step
Epoch 962/1000
2023-10-24 07:19:04.763 
Epoch 962/1000 
	 loss: 17.3290, MinusLogProbMetric: 17.3290, val_loss: 17.3977, val_MinusLogProbMetric: 17.3977

Epoch 962: val_loss did not improve from 17.36362
196/196 - 67s - loss: 17.3290 - MinusLogProbMetric: 17.3290 - val_loss: 17.3977 - val_MinusLogProbMetric: 17.3977 - lr: 2.7778e-05 - 67s/epoch - 341ms/step
Epoch 963/1000
2023-10-24 07:20:04.960 
Epoch 963/1000 
	 loss: 17.3069, MinusLogProbMetric: 17.3069, val_loss: 17.3887, val_MinusLogProbMetric: 17.3887

Epoch 963: val_loss did not improve from 17.36362
196/196 - 60s - loss: 17.3069 - MinusLogProbMetric: 17.3069 - val_loss: 17.3887 - val_MinusLogProbMetric: 17.3887 - lr: 2.7778e-05 - 60s/epoch - 307ms/step
Epoch 964/1000
2023-10-24 07:21:10.773 
Epoch 964/1000 
	 loss: 17.2999, MinusLogProbMetric: 17.2999, val_loss: 17.3911, val_MinusLogProbMetric: 17.3911

Epoch 964: val_loss did not improve from 17.36362
196/196 - 66s - loss: 17.2999 - MinusLogProbMetric: 17.2999 - val_loss: 17.3911 - val_MinusLogProbMetric: 17.3911 - lr: 2.7778e-05 - 66s/epoch - 336ms/step
Epoch 965/1000
2023-10-24 07:22:20.586 
Epoch 965/1000 
	 loss: 17.3102, MinusLogProbMetric: 17.3102, val_loss: 17.3895, val_MinusLogProbMetric: 17.3895

Epoch 965: val_loss did not improve from 17.36362
196/196 - 70s - loss: 17.3102 - MinusLogProbMetric: 17.3102 - val_loss: 17.3895 - val_MinusLogProbMetric: 17.3895 - lr: 2.7778e-05 - 70s/epoch - 356ms/step
Epoch 966/1000
2023-10-24 07:23:34.788 
Epoch 966/1000 
	 loss: 17.3003, MinusLogProbMetric: 17.3003, val_loss: 17.4869, val_MinusLogProbMetric: 17.4869

Epoch 966: val_loss did not improve from 17.36362
196/196 - 74s - loss: 17.3003 - MinusLogProbMetric: 17.3003 - val_loss: 17.4869 - val_MinusLogProbMetric: 17.4869 - lr: 2.7778e-05 - 74s/epoch - 379ms/step
Epoch 967/1000
2023-10-24 07:24:48.023 
Epoch 967/1000 
	 loss: 17.3165, MinusLogProbMetric: 17.3165, val_loss: 17.4716, val_MinusLogProbMetric: 17.4716

Epoch 967: val_loss did not improve from 17.36362
196/196 - 73s - loss: 17.3165 - MinusLogProbMetric: 17.3165 - val_loss: 17.4716 - val_MinusLogProbMetric: 17.4716 - lr: 2.7778e-05 - 73s/epoch - 374ms/step
Epoch 968/1000
2023-10-24 07:26:03.016 
Epoch 968/1000 
	 loss: 17.3102, MinusLogProbMetric: 17.3102, val_loss: 17.4504, val_MinusLogProbMetric: 17.4504

Epoch 968: val_loss did not improve from 17.36362
196/196 - 75s - loss: 17.3102 - MinusLogProbMetric: 17.3102 - val_loss: 17.4504 - val_MinusLogProbMetric: 17.4504 - lr: 2.7778e-05 - 75s/epoch - 383ms/step
Epoch 969/1000
2023-10-24 07:27:16.114 
Epoch 969/1000 
	 loss: 17.3081, MinusLogProbMetric: 17.3081, val_loss: 17.3822, val_MinusLogProbMetric: 17.3822

Epoch 969: val_loss did not improve from 17.36362
196/196 - 73s - loss: 17.3081 - MinusLogProbMetric: 17.3081 - val_loss: 17.3822 - val_MinusLogProbMetric: 17.3822 - lr: 2.7778e-05 - 73s/epoch - 373ms/step
Epoch 970/1000
2023-10-24 07:28:28.273 
Epoch 970/1000 
	 loss: 17.3215, MinusLogProbMetric: 17.3215, val_loss: 17.4293, val_MinusLogProbMetric: 17.4293

Epoch 970: val_loss did not improve from 17.36362
196/196 - 72s - loss: 17.3215 - MinusLogProbMetric: 17.3215 - val_loss: 17.4293 - val_MinusLogProbMetric: 17.4293 - lr: 2.7778e-05 - 72s/epoch - 368ms/step
Epoch 971/1000
2023-10-24 07:29:42.010 
Epoch 971/1000 
	 loss: 17.3225, MinusLogProbMetric: 17.3225, val_loss: 17.3861, val_MinusLogProbMetric: 17.3861

Epoch 971: val_loss did not improve from 17.36362
196/196 - 74s - loss: 17.3225 - MinusLogProbMetric: 17.3225 - val_loss: 17.3861 - val_MinusLogProbMetric: 17.3861 - lr: 2.7778e-05 - 74s/epoch - 376ms/step
Epoch 972/1000
2023-10-24 07:30:54.648 
Epoch 972/1000 
	 loss: 17.3025, MinusLogProbMetric: 17.3025, val_loss: 17.3546, val_MinusLogProbMetric: 17.3546

Epoch 972: val_loss improved from 17.36362 to 17.35461, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 74s - loss: 17.3025 - MinusLogProbMetric: 17.3025 - val_loss: 17.3546 - val_MinusLogProbMetric: 17.3546 - lr: 2.7778e-05 - 74s/epoch - 377ms/step
Epoch 973/1000
2023-10-24 07:32:09.312 
Epoch 973/1000 
	 loss: 17.3036, MinusLogProbMetric: 17.3036, val_loss: 17.4193, val_MinusLogProbMetric: 17.4193

Epoch 973: val_loss did not improve from 17.35461
196/196 - 73s - loss: 17.3036 - MinusLogProbMetric: 17.3036 - val_loss: 17.4193 - val_MinusLogProbMetric: 17.4193 - lr: 2.7778e-05 - 73s/epoch - 374ms/step
Epoch 974/1000
2023-10-24 07:33:21.214 
Epoch 974/1000 
	 loss: 17.3056, MinusLogProbMetric: 17.3056, val_loss: 17.3618, val_MinusLogProbMetric: 17.3618

Epoch 974: val_loss did not improve from 17.35461
196/196 - 72s - loss: 17.3056 - MinusLogProbMetric: 17.3056 - val_loss: 17.3618 - val_MinusLogProbMetric: 17.3618 - lr: 2.7778e-05 - 72s/epoch - 367ms/step
Epoch 975/1000
2023-10-24 07:34:32.669 
Epoch 975/1000 
	 loss: 17.3149, MinusLogProbMetric: 17.3149, val_loss: 17.3785, val_MinusLogProbMetric: 17.3785

Epoch 975: val_loss did not improve from 17.35461
196/196 - 71s - loss: 17.3149 - MinusLogProbMetric: 17.3149 - val_loss: 17.3785 - val_MinusLogProbMetric: 17.3785 - lr: 2.7778e-05 - 71s/epoch - 365ms/step
Epoch 976/1000
2023-10-24 07:35:42.501 
Epoch 976/1000 
	 loss: 17.3139, MinusLogProbMetric: 17.3139, val_loss: 17.3799, val_MinusLogProbMetric: 17.3799

Epoch 976: val_loss did not improve from 17.35461
196/196 - 70s - loss: 17.3139 - MinusLogProbMetric: 17.3139 - val_loss: 17.3799 - val_MinusLogProbMetric: 17.3799 - lr: 2.7778e-05 - 70s/epoch - 356ms/step
Epoch 977/1000
2023-10-24 07:36:53.426 
Epoch 977/1000 
	 loss: 17.2997, MinusLogProbMetric: 17.2997, val_loss: 17.3989, val_MinusLogProbMetric: 17.3989

Epoch 977: val_loss did not improve from 17.35461
196/196 - 71s - loss: 17.2997 - MinusLogProbMetric: 17.2997 - val_loss: 17.3989 - val_MinusLogProbMetric: 17.3989 - lr: 2.7778e-05 - 71s/epoch - 362ms/step
Epoch 978/1000
2023-10-24 07:38:03.699 
Epoch 978/1000 
	 loss: 17.3053, MinusLogProbMetric: 17.3053, val_loss: 17.3923, val_MinusLogProbMetric: 17.3923

Epoch 978: val_loss did not improve from 17.35461
196/196 - 70s - loss: 17.3053 - MinusLogProbMetric: 17.3053 - val_loss: 17.3923 - val_MinusLogProbMetric: 17.3923 - lr: 2.7778e-05 - 70s/epoch - 359ms/step
Epoch 979/1000
2023-10-24 07:39:15.768 
Epoch 979/1000 
	 loss: 17.2992, MinusLogProbMetric: 17.2992, val_loss: 17.4336, val_MinusLogProbMetric: 17.4336

Epoch 979: val_loss did not improve from 17.35461
196/196 - 72s - loss: 17.2992 - MinusLogProbMetric: 17.2992 - val_loss: 17.4336 - val_MinusLogProbMetric: 17.4336 - lr: 2.7778e-05 - 72s/epoch - 368ms/step
Epoch 980/1000
2023-10-24 07:40:27.768 
Epoch 980/1000 
	 loss: 17.3085, MinusLogProbMetric: 17.3085, val_loss: 17.3746, val_MinusLogProbMetric: 17.3746

Epoch 980: val_loss did not improve from 17.35461
196/196 - 72s - loss: 17.3085 - MinusLogProbMetric: 17.3085 - val_loss: 17.3746 - val_MinusLogProbMetric: 17.3746 - lr: 2.7778e-05 - 72s/epoch - 367ms/step
Epoch 981/1000
2023-10-24 07:41:38.342 
Epoch 981/1000 
	 loss: 17.3706, MinusLogProbMetric: 17.3706, val_loss: 17.3859, val_MinusLogProbMetric: 17.3859

Epoch 981: val_loss did not improve from 17.35461
196/196 - 71s - loss: 17.3706 - MinusLogProbMetric: 17.3706 - val_loss: 17.3859 - val_MinusLogProbMetric: 17.3859 - lr: 2.7778e-05 - 71s/epoch - 360ms/step
Epoch 982/1000
2023-10-24 07:42:49.552 
Epoch 982/1000 
	 loss: 17.3188, MinusLogProbMetric: 17.3188, val_loss: 17.4639, val_MinusLogProbMetric: 17.4639

Epoch 982: val_loss did not improve from 17.35461
196/196 - 71s - loss: 17.3188 - MinusLogProbMetric: 17.3188 - val_loss: 17.4639 - val_MinusLogProbMetric: 17.4639 - lr: 2.7778e-05 - 71s/epoch - 363ms/step
Epoch 983/1000
2023-10-24 07:44:02.753 
Epoch 983/1000 
	 loss: 17.3233, MinusLogProbMetric: 17.3233, val_loss: 17.4144, val_MinusLogProbMetric: 17.4144

Epoch 983: val_loss did not improve from 17.35461
196/196 - 73s - loss: 17.3233 - MinusLogProbMetric: 17.3233 - val_loss: 17.4144 - val_MinusLogProbMetric: 17.4144 - lr: 2.7778e-05 - 73s/epoch - 373ms/step
Epoch 984/1000
2023-10-24 07:45:14.309 
Epoch 984/1000 
	 loss: 17.3116, MinusLogProbMetric: 17.3116, val_loss: 17.3774, val_MinusLogProbMetric: 17.3774

Epoch 984: val_loss did not improve from 17.35461
196/196 - 72s - loss: 17.3116 - MinusLogProbMetric: 17.3116 - val_loss: 17.3774 - val_MinusLogProbMetric: 17.3774 - lr: 2.7778e-05 - 72s/epoch - 365ms/step
Epoch 985/1000
2023-10-24 07:46:25.440 
Epoch 985/1000 
	 loss: 17.3102, MinusLogProbMetric: 17.3102, val_loss: 17.4469, val_MinusLogProbMetric: 17.4469

Epoch 985: val_loss did not improve from 17.35461
196/196 - 71s - loss: 17.3102 - MinusLogProbMetric: 17.3102 - val_loss: 17.4469 - val_MinusLogProbMetric: 17.4469 - lr: 2.7778e-05 - 71s/epoch - 363ms/step
Epoch 986/1000
2023-10-24 07:47:40.415 
Epoch 986/1000 
	 loss: 17.3067, MinusLogProbMetric: 17.3067, val_loss: 17.4234, val_MinusLogProbMetric: 17.4234

Epoch 986: val_loss did not improve from 17.35461
196/196 - 75s - loss: 17.3067 - MinusLogProbMetric: 17.3067 - val_loss: 17.4234 - val_MinusLogProbMetric: 17.4234 - lr: 2.7778e-05 - 75s/epoch - 382ms/step
Epoch 987/1000
2023-10-24 07:48:52.545 
Epoch 987/1000 
	 loss: 17.3047, MinusLogProbMetric: 17.3047, val_loss: 17.4150, val_MinusLogProbMetric: 17.4150

Epoch 987: val_loss did not improve from 17.35461
196/196 - 72s - loss: 17.3047 - MinusLogProbMetric: 17.3047 - val_loss: 17.4150 - val_MinusLogProbMetric: 17.4150 - lr: 2.7778e-05 - 72s/epoch - 368ms/step
Epoch 988/1000
2023-10-24 07:50:04.103 
Epoch 988/1000 
	 loss: 17.3085, MinusLogProbMetric: 17.3085, val_loss: 17.3626, val_MinusLogProbMetric: 17.3626

Epoch 988: val_loss did not improve from 17.35461
196/196 - 72s - loss: 17.3085 - MinusLogProbMetric: 17.3085 - val_loss: 17.3626 - val_MinusLogProbMetric: 17.3626 - lr: 2.7778e-05 - 72s/epoch - 365ms/step
Epoch 989/1000
2023-10-24 07:51:15.982 
Epoch 989/1000 
	 loss: 17.3012, MinusLogProbMetric: 17.3012, val_loss: 17.3874, val_MinusLogProbMetric: 17.3874

Epoch 989: val_loss did not improve from 17.35461
196/196 - 72s - loss: 17.3012 - MinusLogProbMetric: 17.3012 - val_loss: 17.3874 - val_MinusLogProbMetric: 17.3874 - lr: 2.7778e-05 - 72s/epoch - 367ms/step
Epoch 990/1000
2023-10-24 07:52:29.728 
Epoch 990/1000 
	 loss: 17.2905, MinusLogProbMetric: 17.2905, val_loss: 17.4174, val_MinusLogProbMetric: 17.4174

Epoch 990: val_loss did not improve from 17.35461
196/196 - 74s - loss: 17.2905 - MinusLogProbMetric: 17.2905 - val_loss: 17.4174 - val_MinusLogProbMetric: 17.4174 - lr: 2.7778e-05 - 74s/epoch - 376ms/step
Epoch 991/1000
2023-10-24 07:53:41.768 
Epoch 991/1000 
	 loss: 17.3006, MinusLogProbMetric: 17.3006, val_loss: 17.3407, val_MinusLogProbMetric: 17.3407

Epoch 991: val_loss improved from 17.35461 to 17.34074, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5
196/196 - 73s - loss: 17.3006 - MinusLogProbMetric: 17.3006 - val_loss: 17.3407 - val_MinusLogProbMetric: 17.3407 - lr: 2.7778e-05 - 73s/epoch - 374ms/step
Epoch 992/1000
2023-10-24 07:54:55.823 
Epoch 992/1000 
	 loss: 17.2863, MinusLogProbMetric: 17.2863, val_loss: 17.4017, val_MinusLogProbMetric: 17.4017

Epoch 992: val_loss did not improve from 17.34074
196/196 - 73s - loss: 17.2863 - MinusLogProbMetric: 17.2863 - val_loss: 17.4017 - val_MinusLogProbMetric: 17.4017 - lr: 2.7778e-05 - 73s/epoch - 372ms/step
Epoch 993/1000
2023-10-24 07:56:09.020 
Epoch 993/1000 
	 loss: 17.2971, MinusLogProbMetric: 17.2971, val_loss: 17.4224, val_MinusLogProbMetric: 17.4224

Epoch 993: val_loss did not improve from 17.34074
196/196 - 73s - loss: 17.2971 - MinusLogProbMetric: 17.2971 - val_loss: 17.4224 - val_MinusLogProbMetric: 17.4224 - lr: 2.7778e-05 - 73s/epoch - 373ms/step
Epoch 994/1000
2023-10-24 07:57:20.787 
Epoch 994/1000 
	 loss: 17.2962, MinusLogProbMetric: 17.2962, val_loss: 17.4056, val_MinusLogProbMetric: 17.4056

Epoch 994: val_loss did not improve from 17.34074
196/196 - 72s - loss: 17.2962 - MinusLogProbMetric: 17.2962 - val_loss: 17.4056 - val_MinusLogProbMetric: 17.4056 - lr: 2.7778e-05 - 72s/epoch - 366ms/step
Epoch 995/1000
2023-10-24 07:58:34.820 
Epoch 995/1000 
	 loss: 17.2865, MinusLogProbMetric: 17.2865, val_loss: 17.3433, val_MinusLogProbMetric: 17.3433

Epoch 995: val_loss did not improve from 17.34074
196/196 - 74s - loss: 17.2865 - MinusLogProbMetric: 17.2865 - val_loss: 17.3433 - val_MinusLogProbMetric: 17.3433 - lr: 2.7778e-05 - 74s/epoch - 378ms/step
Epoch 996/1000
2023-10-24 07:59:47.844 
Epoch 996/1000 
	 loss: 17.2914, MinusLogProbMetric: 17.2914, val_loss: 17.3853, val_MinusLogProbMetric: 17.3853

Epoch 996: val_loss did not improve from 17.34074
196/196 - 73s - loss: 17.2914 - MinusLogProbMetric: 17.2914 - val_loss: 17.3853 - val_MinusLogProbMetric: 17.3853 - lr: 2.7778e-05 - 73s/epoch - 373ms/step
Epoch 997/1000
2023-10-24 08:01:01.268 
Epoch 997/1000 
	 loss: 17.2957, MinusLogProbMetric: 17.2957, val_loss: 17.4692, val_MinusLogProbMetric: 17.4692

Epoch 997: val_loss did not improve from 17.34074
196/196 - 73s - loss: 17.2957 - MinusLogProbMetric: 17.2957 - val_loss: 17.4692 - val_MinusLogProbMetric: 17.4692 - lr: 2.7778e-05 - 73s/epoch - 375ms/step
Epoch 998/1000
2023-10-24 08:02:18.012 
Epoch 998/1000 
	 loss: 17.3139, MinusLogProbMetric: 17.3139, val_loss: 17.3807, val_MinusLogProbMetric: 17.3807

Epoch 998: val_loss did not improve from 17.34074
196/196 - 77s - loss: 17.3139 - MinusLogProbMetric: 17.3139 - val_loss: 17.3807 - val_MinusLogProbMetric: 17.3807 - lr: 2.7778e-05 - 77s/epoch - 392ms/step
Epoch 999/1000
2023-10-24 08:03:29.666 
Epoch 999/1000 
	 loss: 17.2975, MinusLogProbMetric: 17.2975, val_loss: 17.3887, val_MinusLogProbMetric: 17.3887

Epoch 999: val_loss did not improve from 17.34074
196/196 - 72s - loss: 17.2975 - MinusLogProbMetric: 17.2975 - val_loss: 17.3887 - val_MinusLogProbMetric: 17.3887 - lr: 2.7778e-05 - 72s/epoch - 366ms/step
Epoch 1000/1000
2023-10-24 08:04:42.823 
Epoch 1000/1000 
	 loss: 17.3023, MinusLogProbMetric: 17.3023, val_loss: 17.3582, val_MinusLogProbMetric: 17.3582

Epoch 1000: val_loss did not improve from 17.34074
196/196 - 73s - loss: 17.3023 - MinusLogProbMetric: 17.3023 - val_loss: 17.3582 - val_MinusLogProbMetric: 17.3582 - lr: 2.7778e-05 - 73s/epoch - 373ms/step
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Training succeeded with seed 933.
Model trained in 67526.24 s.

===========
Computing predictions
===========

Computing metrics...
Checking and setting numerical distributions.
Resetting dist_num.
Resetting dist_num.
Metrics computed in 1.29 s.
                ===========
                print("===========
Failed to plot
===========
")
                Exception type: ValueError
                Exception message: Number of rows must be a positive integer, not 0
                Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 481, Func.Name : prediction_function, Message : Plotters.cornerplotter(X_data_test.numpy(),X_data_nf.numpy(),path_to_results,ndims,norm=True) # type: ignore', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/Plotters.py , Line : 156, Func.Name : cornerplotter, Message : figure=corner.corner(target_samples,color=\'red\',bins=n_bins,labels=[r"%s" % s for s in labels],normalize1d=True)', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/corner.py , Line : 235, Func.Name : corner, Message : fig, axes = pl.subplots(K, K, figsize=(dim, dim))', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/pyplot.py , Line : 1502, Func.Name : subplots, Message : axs = fig.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 905, Func.Name : subplots, Message : gs = self.add_gridspec(nrows, ncols, figure=self, **gridspec_kw)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 1527, Func.Name : add_gridspec, Message : gs = GridSpec(nrows=nrows, ncols=ncols, figure=self, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 379, Func.Name : __init__, Message : super().__init__(nrows, ncols,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 49, Func.Name : __init__, Message : raise ValueError(']
                ===========

results.txt saved
results.json saved
Results log saved
Model predictions computed in 1.75 s.
===========
Run 319/720 done in 68260.85 s.
===========

Directory ../../results/CsplineN_new/run_320/ already exists.
Skipping it.
===========
Run 320/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_321/ already exists.
Skipping it.
===========
Run 321/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_322/ already exists.
Skipping it.
===========
Run 322/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_323/ already exists.
Skipping it.
===========
Run 323/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_324/ already exists.
Skipping it.
===========
Run 324/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_325/ already exists.
Skipping it.
===========
Run 325/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_326/ already exists.
Skipping it.
===========
Run 326/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_327/ already exists.
Skipping it.
===========
Run 327/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_328/ already exists.
Skipping it.
===========
Run 328/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_329/ already exists.
Skipping it.
===========
Run 329/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_330/ already exists.
Skipping it.
===========
Run 330/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_331/ already exists.
Skipping it.
===========
Run 331/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_332/ already exists.
Skipping it.
===========
Run 332/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_333/ already exists.
Skipping it.
===========
Run 333/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_334/ already exists.
Skipping it.
===========
Run 334/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_335/ already exists.
Skipping it.
===========
Run 335/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_336/ already exists.
Skipping it.
===========
Run 336/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_337/ already exists.
Skipping it.
===========
Run 337/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_338/ already exists.
Skipping it.
===========
Run 338/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_339/ already exists.
Skipping it.
===========
Run 339/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_340/ already exists.
Skipping it.
===========
Run 340/720 already exists. Skipping it.
===========

===========
Generating train data for run 341.
===========
Train data generated in 0.22 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_341/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_341/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_341/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_341
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.465355   7.122426   6.507682  ...  1.2216942  7.396178   1.4448237]
 [ 1.8683796  2.8760538  9.147447  ...  4.980697  -1.1366768  2.8016024]
 [ 5.6175413  8.642618   5.6734285 ... -0.9440901  7.0561414  1.3082856]
 ...
 [ 6.145307   2.8273122  6.191003  ...  2.964365   6.077158   2.5509653]
 [ 6.5595512  2.759307   6.0778823 ...  3.4800506  4.3951674  2.1941729]
 [ 5.586527   7.3216014  6.2487698 ...  1.292631   5.797888   1.2879843]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_164"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_165 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_14 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_14/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_14'")
self.model: <keras.engine.functional.Functional object at 0x7f594eeb33a0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f595c733670>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f595c733670>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f602733acb0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f594ee0a3e0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_341/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f594ee0a950>, <keras.callbacks.ModelCheckpoint object at 0x7f594ee0aa10>, <keras.callbacks.EarlyStopping object at 0x7f594ee0ac80>, <keras.callbacks.ReduceLROnPlateau object at 0x7f594ee0acb0>, <keras.callbacks.TerminateOnNaN object at 0x7f594ee0a8f0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_341/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 341/720 with hyperparameters:
timestamp = 2023-10-24 08:04:51.230059
ndims = 64
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 5.465355    7.122426    6.507682    5.369255    4.1136785   6.196197
  4.259451    8.702779    9.329599    4.155665    7.5761204   5.312057
  5.67009     9.304932    0.8909831   1.4673554  -0.03015512  8.003835
  8.704047    8.985184    9.595006    8.103893    4.62255     8.186032
  0.5885266   6.325719    0.92288995  9.5184145   6.6715336   3.3791308
  2.4301429   8.523114    4.619198    5.3705144   0.3660334   5.918427
  6.2292395   7.1572037   9.551745    6.67638     3.2475867   4.406022
  7.1114645   0.65166384  6.9876056   6.6356955   2.3022606   1.2872343
  3.532573    3.8445692   6.0782557   4.5467997   9.491786    0.11065191
  3.0255654   1.2933161   6.592196    2.150623    4.2476006   4.39
  1.805124    1.2216942   7.396178    1.4448237 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-24 08:07:05.042 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7554.6157, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 134s - loss: nan - MinusLogProbMetric: 7554.6157 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 134s/epoch - 682ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 0.0003333333333333333.
===========
Generating train data for run 341.
===========
Train data generated in 0.17 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_341/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_341/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_341/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_341
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.465355   7.122426   6.507682  ...  1.2216942  7.396178   1.4448237]
 [ 1.8683796  2.8760538  9.147447  ...  4.980697  -1.1366768  2.8016024]
 [ 5.6175413  8.642618   5.6734285 ... -0.9440901  7.0561414  1.3082856]
 ...
 [ 6.145307   2.8273122  6.191003  ...  2.964365   6.077158   2.5509653]
 [ 6.5595512  2.759307   6.0778823 ...  3.4800506  4.3951674  2.1941729]
 [ 5.586527   7.3216014  6.2487698 ...  1.292631   5.797888   1.2879843]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_175"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_176 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_15 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_15/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_15'")
self.model: <keras.engine.functional.Functional object at 0x7f5fc2643130>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f5fc2792bf0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f5fc2792bf0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f5fb9ff6a70>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f5fba08e620>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_341/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f5fba08eb90>, <keras.callbacks.ModelCheckpoint object at 0x7f5fba08ec50>, <keras.callbacks.EarlyStopping object at 0x7f5fba08eec0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f5fba08eef0>, <keras.callbacks.TerminateOnNaN object at 0x7f5fba08eb30>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_341/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 341/720 with hyperparameters:
timestamp = 2023-10-24 08:07:12.183780
ndims = 64
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 5.465355    7.122426    6.507682    5.369255    4.1136785   6.196197
  4.259451    8.702779    9.329599    4.155665    7.5761204   5.312057
  5.67009     9.304932    0.8909831   1.4673554  -0.03015512  8.003835
  8.704047    8.985184    9.595006    8.103893    4.62255     8.186032
  0.5885266   6.325719    0.92288995  9.5184145   6.6715336   3.3791308
  2.4301429   8.523114    4.619198    5.3705144   0.3660334   5.918427
  6.2292395   7.1572037   9.551745    6.67638     3.2475867   4.406022
  7.1114645   0.65166384  6.9876056   6.6356955   2.3022606   1.2872343
  3.532573    3.8445692   6.0782557   4.5467997   9.491786    0.11065191
  3.0255654   1.2933161   6.592196    2.150623    4.2476006   4.39
  1.805124    1.2216942   7.396178    1.4448237 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-24 08:09:31.775 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7554.6157, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 139s - loss: nan - MinusLogProbMetric: 7554.6157 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 139s/epoch - 712ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 0.0001111111111111111.
===========
Generating train data for run 341.
===========
Train data generated in 0.20 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_341/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_341/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_341/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_341
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.465355   7.122426   6.507682  ...  1.2216942  7.396178   1.4448237]
 [ 1.8683796  2.8760538  9.147447  ...  4.980697  -1.1366768  2.8016024]
 [ 5.6175413  8.642618   5.6734285 ... -0.9440901  7.0561414  1.3082856]
 ...
 [ 6.145307   2.8273122  6.191003  ...  2.964365   6.077158   2.5509653]
 [ 6.5595512  2.759307   6.0778823 ...  3.4800506  4.3951674  2.1941729]
 [ 5.586527   7.3216014  6.2487698 ...  1.292631   5.797888   1.2879843]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_186"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_187 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_16 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_16/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_16'")
self.model: <keras.engine.functional.Functional object at 0x7f59244db820>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f58dde82350>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f58dde82350>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f59243a1540>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f59243b18d0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_341/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f59243b2ad0>, <keras.callbacks.ModelCheckpoint object at 0x7f59243b25f0>, <keras.callbacks.EarlyStopping object at 0x7f59243b2e60>, <keras.callbacks.ReduceLROnPlateau object at 0x7f59243b3670>, <keras.callbacks.TerminateOnNaN object at 0x7f59243b3040>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_341/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 341/720 with hyperparameters:
timestamp = 2023-10-24 08:09:40.535956
ndims = 64
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 5.465355    7.122426    6.507682    5.369255    4.1136785   6.196197
  4.259451    8.702779    9.329599    4.155665    7.5761204   5.312057
  5.67009     9.304932    0.8909831   1.4673554  -0.03015512  8.003835
  8.704047    8.985184    9.595006    8.103893    4.62255     8.186032
  0.5885266   6.325719    0.92288995  9.5184145   6.6715336   3.3791308
  2.4301429   8.523114    4.619198    5.3705144   0.3660334   5.918427
  6.2292395   7.1572037   9.551745    6.67638     3.2475867   4.406022
  7.1114645   0.65166384  6.9876056   6.6356955   2.3022606   1.2872343
  3.532573    3.8445692   6.0782557   4.5467997   9.491786    0.11065191
  3.0255654   1.2933161   6.592196    2.150623    4.2476006   4.39
  1.805124    1.2216942   7.396178    1.4448237 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-24 08:12:04.387 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7554.6157, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 144s - loss: nan - MinusLogProbMetric: 7554.6157 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.1111e-04 - 144s/epoch - 733ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 3.703703703703703e-05.
===========
Generating train data for run 341.
===========
Train data generated in 0.26 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_341/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_341/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_341/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_341
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.465355   7.122426   6.507682  ...  1.2216942  7.396178   1.4448237]
 [ 1.8683796  2.8760538  9.147447  ...  4.980697  -1.1366768  2.8016024]
 [ 5.6175413  8.642618   5.6734285 ... -0.9440901  7.0561414  1.3082856]
 ...
 [ 6.145307   2.8273122  6.191003  ...  2.964365   6.077158   2.5509653]
 [ 6.5595512  2.759307   6.0778823 ...  3.4800506  4.3951674  2.1941729]
 [ 5.586527   7.3216014  6.2487698 ...  1.292631   5.797888   1.2879843]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_197"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_198 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_17 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_17/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_17'")
self.model: <keras.engine.functional.Functional object at 0x7f58a0d87310>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f57bc30dcf0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f57bc30dcf0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f594f026b60>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f58e5703010>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_341/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f58e5703580>, <keras.callbacks.ModelCheckpoint object at 0x7f58e5703640>, <keras.callbacks.EarlyStopping object at 0x7f58e57038b0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f58e57038e0>, <keras.callbacks.TerminateOnNaN object at 0x7f58e5703520>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_341/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 341/720 with hyperparameters:
timestamp = 2023-10-24 08:12:11.507059
ndims = 64
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 3.703703703703703e-05...
Train first sample: [ 5.465355    7.122426    6.507682    5.369255    4.1136785   6.196197
  4.259451    8.702779    9.329599    4.155665    7.5761204   5.312057
  5.67009     9.304932    0.8909831   1.4673554  -0.03015512  8.003835
  8.704047    8.985184    9.595006    8.103893    4.62255     8.186032
  0.5885266   6.325719    0.92288995  9.5184145   6.6715336   3.3791308
  2.4301429   8.523114    4.619198    5.3705144   0.3660334   5.918427
  6.2292395   7.1572037   9.551745    6.67638     3.2475867   4.406022
  7.1114645   0.65166384  6.9876056   6.6356955   2.3022606   1.2872343
  3.532573    3.8445692   6.0782557   4.5467997   9.491786    0.11065191
  3.0255654   1.2933161   6.592196    2.150623    4.2476006   4.39
  1.805124    1.2216942   7.396178    1.4448237 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-24 08:14:44.602 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7554.6157, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 153s - loss: nan - MinusLogProbMetric: 7554.6157 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.7037e-05 - 153s/epoch - 781ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 1.2345679012345677e-05.
===========
Generating train data for run 341.
===========
Train data generated in 0.27 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_341/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_341/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_341/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_341
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.465355   7.122426   6.507682  ...  1.2216942  7.396178   1.4448237]
 [ 1.8683796  2.8760538  9.147447  ...  4.980697  -1.1366768  2.8016024]
 [ 5.6175413  8.642618   5.6734285 ... -0.9440901  7.0561414  1.3082856]
 ...
 [ 6.145307   2.8273122  6.191003  ...  2.964365   6.077158   2.5509653]
 [ 6.5595512  2.759307   6.0778823 ...  3.4800506  4.3951674  2.1941729]
 [ 5.586527   7.3216014  6.2487698 ...  1.292631   5.797888   1.2879843]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_208"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_209 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_18 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_18/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_18'")
self.model: <keras.engine.functional.Functional object at 0x7f5fdbd8aaa0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f582447e770>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f582447e770>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f58244b3fd0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f5824463dc0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_341/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f5824468370>, <keras.callbacks.ModelCheckpoint object at 0x7f5824468430>, <keras.callbacks.EarlyStopping object at 0x7f58244686a0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f58244686d0>, <keras.callbacks.TerminateOnNaN object at 0x7f5824468310>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_341/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 341/720 with hyperparameters:
timestamp = 2023-10-24 08:14:52.734178
ndims = 64
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.2345679012345677e-05...
Train first sample: [ 5.465355    7.122426    6.507682    5.369255    4.1136785   6.196197
  4.259451    8.702779    9.329599    4.155665    7.5761204   5.312057
  5.67009     9.304932    0.8909831   1.4673554  -0.03015512  8.003835
  8.704047    8.985184    9.595006    8.103893    4.62255     8.186032
  0.5885266   6.325719    0.92288995  9.5184145   6.6715336   3.3791308
  2.4301429   8.523114    4.619198    5.3705144   0.3660334   5.918427
  6.2292395   7.1572037   9.551745    6.67638     3.2475867   4.406022
  7.1114645   0.65166384  6.9876056   6.6356955   2.3022606   1.2872343
  3.532573    3.8445692   6.0782557   4.5467997   9.491786    0.11065191
  3.0255654   1.2933161   6.592196    2.150623    4.2476006   4.39
  1.805124    1.2216942   7.396178    1.4448237 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-24 08:17:04.905 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7554.6157, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 132s - loss: nan - MinusLogProbMetric: 7554.6157 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.2346e-05 - 132s/epoch - 674ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 4.115226337448558e-06.
===========
Generating train data for run 341.
===========
Train data generated in 0.23 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_341/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_341/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_341/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_341
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.465355   7.122426   6.507682  ...  1.2216942  7.396178   1.4448237]
 [ 1.8683796  2.8760538  9.147447  ...  4.980697  -1.1366768  2.8016024]
 [ 5.6175413  8.642618   5.6734285 ... -0.9440901  7.0561414  1.3082856]
 ...
 [ 6.145307   2.8273122  6.191003  ...  2.964365   6.077158   2.5509653]
 [ 6.5595512  2.759307   6.0778823 ...  3.4800506  4.3951674  2.1941729]
 [ 5.586527   7.3216014  6.2487698 ...  1.292631   5.797888   1.2879843]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_219"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_220 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_19 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_19/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_19'")
self.model: <keras.engine.functional.Functional object at 0x7f5fb99aa590>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f5835058880>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f5835058880>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f57ee28fd90>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f5fb92337c0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_341/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f5fb9233d30>, <keras.callbacks.ModelCheckpoint object at 0x7f5fb9233df0>, <keras.callbacks.EarlyStopping object at 0x7f5fb9233fa0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f5fb9233d00>, <keras.callbacks.TerminateOnNaN object at 0x7f5fb9233f40>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_341/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 341/720 with hyperparameters:
timestamp = 2023-10-24 08:17:14.444551
ndims = 64
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.115226337448558e-06...
Train first sample: [ 5.465355    7.122426    6.507682    5.369255    4.1136785   6.196197
  4.259451    8.702779    9.329599    4.155665    7.5761204   5.312057
  5.67009     9.304932    0.8909831   1.4673554  -0.03015512  8.003835
  8.704047    8.985184    9.595006    8.103893    4.62255     8.186032
  0.5885266   6.325719    0.92288995  9.5184145   6.6715336   3.3791308
  2.4301429   8.523114    4.619198    5.3705144   0.3660334   5.918427
  6.2292395   7.1572037   9.551745    6.67638     3.2475867   4.406022
  7.1114645   0.65166384  6.9876056   6.6356955   2.3022606   1.2872343
  3.532573    3.8445692   6.0782557   4.5467997   9.491786    0.11065191
  3.0255654   1.2933161   6.592196    2.150623    4.2476006   4.39
  1.805124    1.2216942   7.396178    1.4448237 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-24 08:19:36.126 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7554.6157, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 142s - loss: nan - MinusLogProbMetric: 7554.6157 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.1152e-06 - 142s/epoch - 722ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 1.3717421124828526e-06.
===========
Generating train data for run 341.
===========
Train data generated in 0.25 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_341/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_341/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_341/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_341
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.465355   7.122426   6.507682  ...  1.2216942  7.396178   1.4448237]
 [ 1.8683796  2.8760538  9.147447  ...  4.980697  -1.1366768  2.8016024]
 [ 5.6175413  8.642618   5.6734285 ... -0.9440901  7.0561414  1.3082856]
 ...
 [ 6.145307   2.8273122  6.191003  ...  2.964365   6.077158   2.5509653]
 [ 6.5595512  2.759307   6.0778823 ...  3.4800506  4.3951674  2.1941729]
 [ 5.586527   7.3216014  6.2487698 ...  1.292631   5.797888   1.2879843]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_230"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_231 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_20 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_20/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_20'")
self.model: <keras.engine.functional.Functional object at 0x7f593eaee1d0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f5890f13010>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f5890f13010>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f58de24fac0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f57e8721ba0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_341/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f57e8722110>, <keras.callbacks.ModelCheckpoint object at 0x7f57e87221d0>, <keras.callbacks.EarlyStopping object at 0x7f57e8722440>, <keras.callbacks.ReduceLROnPlateau object at 0x7f57e8722470>, <keras.callbacks.TerminateOnNaN object at 0x7f57e87220b0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_341/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 341/720 with hyperparameters:
timestamp = 2023-10-24 08:19:44.523322
ndims = 64
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.3717421124828526e-06...
Train first sample: [ 5.465355    7.122426    6.507682    5.369255    4.1136785   6.196197
  4.259451    8.702779    9.329599    4.155665    7.5761204   5.312057
  5.67009     9.304932    0.8909831   1.4673554  -0.03015512  8.003835
  8.704047    8.985184    9.595006    8.103893    4.62255     8.186032
  0.5885266   6.325719    0.92288995  9.5184145   6.6715336   3.3791308
  2.4301429   8.523114    4.619198    5.3705144   0.3660334   5.918427
  6.2292395   7.1572037   9.551745    6.67638     3.2475867   4.406022
  7.1114645   0.65166384  6.9876056   6.6356955   2.3022606   1.2872343
  3.532573    3.8445692   6.0782557   4.5467997   9.491786    0.11065191
  3.0255654   1.2933161   6.592196    2.150623    4.2476006   4.39
  1.805124    1.2216942   7.396178    1.4448237 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-24 08:22:11.556 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7554.6157, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 147s - loss: nan - MinusLogProbMetric: 7554.6157 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.3717e-06 - 147s/epoch - 749ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 4.572473708276175e-07.
===========
Generating train data for run 341.
===========
Train data generated in 0.34 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_341/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_341/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_341/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_341
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.465355   7.122426   6.507682  ...  1.2216942  7.396178   1.4448237]
 [ 1.8683796  2.8760538  9.147447  ...  4.980697  -1.1366768  2.8016024]
 [ 5.6175413  8.642618   5.6734285 ... -0.9440901  7.0561414  1.3082856]
 ...
 [ 6.145307   2.8273122  6.191003  ...  2.964365   6.077158   2.5509653]
 [ 6.5595512  2.759307   6.0778823 ...  3.4800506  4.3951674  2.1941729]
 [ 5.586527   7.3216014  6.2487698 ...  1.292631   5.797888   1.2879843]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_241"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_242 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_21 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_21/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_21'")
self.model: <keras.engine.functional.Functional object at 0x7f5927fe4a60>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f58e481a920>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f58e481a920>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f57c866e980>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f58341ca020>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_341/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f58341ca590>, <keras.callbacks.ModelCheckpoint object at 0x7f58341ca650>, <keras.callbacks.EarlyStopping object at 0x7f58341ca8c0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f58341ca8f0>, <keras.callbacks.TerminateOnNaN object at 0x7f58341ca530>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_341/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 341/720 with hyperparameters:
timestamp = 2023-10-24 08:22:19.932447
ndims = 64
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.572473708276175e-07...
Train first sample: [ 5.465355    7.122426    6.507682    5.369255    4.1136785   6.196197
  4.259451    8.702779    9.329599    4.155665    7.5761204   5.312057
  5.67009     9.304932    0.8909831   1.4673554  -0.03015512  8.003835
  8.704047    8.985184    9.595006    8.103893    4.62255     8.186032
  0.5885266   6.325719    0.92288995  9.5184145   6.6715336   3.3791308
  2.4301429   8.523114    4.619198    5.3705144   0.3660334   5.918427
  6.2292395   7.1572037   9.551745    6.67638     3.2475867   4.406022
  7.1114645   0.65166384  6.9876056   6.6356955   2.3022606   1.2872343
  3.532573    3.8445692   6.0782557   4.5467997   9.491786    0.11065191
  3.0255654   1.2933161   6.592196    2.150623    4.2476006   4.39
  1.805124    1.2216942   7.396178    1.4448237 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-24 08:24:49.467 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7554.6157, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 149s - loss: nan - MinusLogProbMetric: 7554.6157 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.5725e-07 - 149s/epoch - 762ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 1.524157902758725e-07.
===========
Generating train data for run 341.
===========
Train data generated in 0.22 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_341/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_341/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_341/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_341
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.465355   7.122426   6.507682  ...  1.2216942  7.396178   1.4448237]
 [ 1.8683796  2.8760538  9.147447  ...  4.980697  -1.1366768  2.8016024]
 [ 5.6175413  8.642618   5.6734285 ... -0.9440901  7.0561414  1.3082856]
 ...
 [ 6.145307   2.8273122  6.191003  ...  2.964365   6.077158   2.5509653]
 [ 6.5595512  2.759307   6.0778823 ...  3.4800506  4.3951674  2.1941729]
 [ 5.586527   7.3216014  6.2487698 ...  1.292631   5.797888   1.2879843]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_252"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_253 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_22 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_22/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_22'")
self.model: <keras.engine.functional.Functional object at 0x7f584332dcf0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f595c78cf10>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f595c78cf10>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f57f433d480>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f5827fac7f0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_341/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f5827faeec0>, <keras.callbacks.ModelCheckpoint object at 0x7f5827fae2c0>, <keras.callbacks.EarlyStopping object at 0x7f5827fafbb0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f5827faf550>, <keras.callbacks.TerminateOnNaN object at 0x7f5827fae140>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_341/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 341/720 with hyperparameters:
timestamp = 2023-10-24 08:24:56.705298
ndims = 64
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.524157902758725e-07...
Train first sample: [ 5.465355    7.122426    6.507682    5.369255    4.1136785   6.196197
  4.259451    8.702779    9.329599    4.155665    7.5761204   5.312057
  5.67009     9.304932    0.8909831   1.4673554  -0.03015512  8.003835
  8.704047    8.985184    9.595006    8.103893    4.62255     8.186032
  0.5885266   6.325719    0.92288995  9.5184145   6.6715336   3.3791308
  2.4301429   8.523114    4.619198    5.3705144   0.3660334   5.918427
  6.2292395   7.1572037   9.551745    6.67638     3.2475867   4.406022
  7.1114645   0.65166384  6.9876056   6.6356955   2.3022606   1.2872343
  3.532573    3.8445692   6.0782557   4.5467997   9.491786    0.11065191
  3.0255654   1.2933161   6.592196    2.150623    4.2476006   4.39
  1.805124    1.2216942   7.396178    1.4448237 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-24 08:27:11.753 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7554.6157, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 135s - loss: nan - MinusLogProbMetric: 7554.6157 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.5242e-07 - 135s/epoch - 689ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 5.0805263425290834e-08.
===========
Generating train data for run 341.
===========
Train data generated in 0.21 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_341/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_341/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_341/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_341
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.465355   7.122426   6.507682  ...  1.2216942  7.396178   1.4448237]
 [ 1.8683796  2.8760538  9.147447  ...  4.980697  -1.1366768  2.8016024]
 [ 5.6175413  8.642618   5.6734285 ... -0.9440901  7.0561414  1.3082856]
 ...
 [ 6.145307   2.8273122  6.191003  ...  2.964365   6.077158   2.5509653]
 [ 6.5595512  2.759307   6.0778823 ...  3.4800506  4.3951674  2.1941729]
 [ 5.586527   7.3216014  6.2487698 ...  1.292631   5.797888   1.2879843]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_263"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_264 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_23 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_23/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_23'")
self.model: <keras.engine.functional.Functional object at 0x7f5fb0952d40>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f5fb0974730>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f5fb0974730>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f57a819e500>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f5f98083bb0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_341/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f5f980c0610>, <keras.callbacks.ModelCheckpoint object at 0x7f5f980c06d0>, <keras.callbacks.EarlyStopping object at 0x7f5f980c0940>, <keras.callbacks.ReduceLROnPlateau object at 0x7f5f980c0970>, <keras.callbacks.TerminateOnNaN object at 0x7f5f980c05b0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_341/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 341/720 with hyperparameters:
timestamp = 2023-10-24 08:27:20.284746
ndims = 64
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 5.0805263425290834e-08...
Train first sample: [ 5.465355    7.122426    6.507682    5.369255    4.1136785   6.196197
  4.259451    8.702779    9.329599    4.155665    7.5761204   5.312057
  5.67009     9.304932    0.8909831   1.4673554  -0.03015512  8.003835
  8.704047    8.985184    9.595006    8.103893    4.62255     8.186032
  0.5885266   6.325719    0.92288995  9.5184145   6.6715336   3.3791308
  2.4301429   8.523114    4.619198    5.3705144   0.3660334   5.918427
  6.2292395   7.1572037   9.551745    6.67638     3.2475867   4.406022
  7.1114645   0.65166384  6.9876056   6.6356955   2.3022606   1.2872343
  3.532573    3.8445692   6.0782557   4.5467997   9.491786    0.11065191
  3.0255654   1.2933161   6.592196    2.150623    4.2476006   4.39
  1.805124    1.2216942   7.396178    1.4448237 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-24 08:29:57.314 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7554.6157, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 157s - loss: nan - MinusLogProbMetric: 7554.6157 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 5.0805e-08 - 157s/epoch - 801ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 1.6935087808430278e-08.
===========
Generating train data for run 341.
===========
Train data generated in 0.28 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_341/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_341/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_341/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_341
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.465355   7.122426   6.507682  ...  1.2216942  7.396178   1.4448237]
 [ 1.8683796  2.8760538  9.147447  ...  4.980697  -1.1366768  2.8016024]
 [ 5.6175413  8.642618   5.6734285 ... -0.9440901  7.0561414  1.3082856]
 ...
 [ 6.145307   2.8273122  6.191003  ...  2.964365   6.077158   2.5509653]
 [ 6.5595512  2.759307   6.0778823 ...  3.4800506  4.3951674  2.1941729]
 [ 5.586527   7.3216014  6.2487698 ...  1.292631   5.797888   1.2879843]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_274"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_275 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_24 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_24/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_24'")
self.model: <keras.engine.functional.Functional object at 0x7f58df4377c0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f5843076c50>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f5843076c50>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f58430175b0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f5843103970>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_341/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f5843103ee0>, <keras.callbacks.ModelCheckpoint object at 0x7f5843103fa0>, <keras.callbacks.EarlyStopping object at 0x7f5843103eb0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f5843103e80>, <keras.callbacks.TerminateOnNaN object at 0x7f5890ab0250>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_341/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 341/720 with hyperparameters:
timestamp = 2023-10-24 08:30:06.936854
ndims = 64
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.6935087808430278e-08...
Train first sample: [ 5.465355    7.122426    6.507682    5.369255    4.1136785   6.196197
  4.259451    8.702779    9.329599    4.155665    7.5761204   5.312057
  5.67009     9.304932    0.8909831   1.4673554  -0.03015512  8.003835
  8.704047    8.985184    9.595006    8.103893    4.62255     8.186032
  0.5885266   6.325719    0.92288995  9.5184145   6.6715336   3.3791308
  2.4301429   8.523114    4.619198    5.3705144   0.3660334   5.918427
  6.2292395   7.1572037   9.551745    6.67638     3.2475867   4.406022
  7.1114645   0.65166384  6.9876056   6.6356955   2.3022606   1.2872343
  3.532573    3.8445692   6.0782557   4.5467997   9.491786    0.11065191
  3.0255654   1.2933161   6.592196    2.150623    4.2476006   4.39
  1.805124    1.2216942   7.396178    1.4448237 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-24 08:32:38.696 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7554.6157, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 152s - loss: nan - MinusLogProbMetric: 7554.6157 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.6935e-08 - 152s/epoch - 774ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 5.645029269476759e-09.
===========
Run 341/720 failed.
Exception type: Exception
Exception message: Training failed for the maximum number of retry.
Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 649, Func.Name : <module>, Message : hyperparams_dict, NFObject, seed_train, training_time = train_function(seeds = [seed_train, seed_test, seed_dist, seed_metrics],', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 322, Func.Name : train_function, Message : raise Exception("Training failed for the maximum number of retry.")']
===========

===========
Generating train data for run 342.
===========
Train data generated in 0.25 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_342/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_342/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_342/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_342
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.465355   7.122426   6.507682  ...  1.2216942  7.396178   1.4448237]
 [ 1.8683796  2.8760538  9.147447  ...  4.980697  -1.1366768  2.8016024]
 [ 5.6175413  8.642618   5.6734285 ... -0.9440901  7.0561414  1.3082856]
 ...
 [ 6.145307   2.8273122  6.191003  ...  2.964365   6.077158   2.5509653]
 [ 6.5595512  2.759307   6.0778823 ...  3.4800506  4.3951674  2.1941729]
 [ 5.586527   7.3216014  6.2487698 ...  1.292631   5.797888   1.2879843]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_285"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_286 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_25 (LogProbL  (None,)                  3291840   
 ayer)                                                           
                                                                 
=================================================================
Total params: 3,291,840
Trainable params: 3,291,840
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_25/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_25'")
self.model: <keras.engine.functional.Functional object at 0x7f58a12d8a30>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f57c8157310>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f57c8157310>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f58a07191b0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f589108eef0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f589108d780>, <keras.callbacks.ModelCheckpoint object at 0x7f589108e710>, <keras.callbacks.EarlyStopping object at 0x7f589108dea0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f589108caf0>, <keras.callbacks.TerminateOnNaN object at 0x7f589108f340>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_342/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 342/720 with hyperparameters:
timestamp = 2023-10-24 08:32:48.529216
ndims = 64
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 3291840
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 5.465355    7.122426    6.507682    5.369255    4.1136785   6.196197
  4.259451    8.702779    9.329599    4.155665    7.5761204   5.312057
  5.67009     9.304932    0.8909831   1.4673554  -0.03015512  8.003835
  8.704047    8.985184    9.595006    8.103893    4.62255     8.186032
  0.5885266   6.325719    0.92288995  9.5184145   6.6715336   3.3791308
  2.4301429   8.523114    4.619198    5.3705144   0.3660334   5.918427
  6.2292395   7.1572037   9.551745    6.67638     3.2475867   4.406022
  7.1114645   0.65166384  6.9876056   6.6356955   2.3022606   1.2872343
  3.532573    3.8445692   6.0782557   4.5467997   9.491786    0.11065191
  3.0255654   1.2933161   6.592196    2.150623    4.2476006   4.39
  1.805124    1.2216942   7.396178    1.4448237 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 8: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-24 08:35:01.550 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 5645.5659, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 133s - loss: nan - MinusLogProbMetric: 5645.5659 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 133s/epoch - 678ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 0.0003333333333333333.
===========
Generating train data for run 342.
===========
Train data generated in 0.22 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_342/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_342/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_342/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_342
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.465355   7.122426   6.507682  ...  1.2216942  7.396178   1.4448237]
 [ 1.8683796  2.8760538  9.147447  ...  4.980697  -1.1366768  2.8016024]
 [ 5.6175413  8.642618   5.6734285 ... -0.9440901  7.0561414  1.3082856]
 ...
 [ 6.145307   2.8273122  6.191003  ...  2.964365   6.077158   2.5509653]
 [ 6.5595512  2.759307   6.0778823 ...  3.4800506  4.3951674  2.1941729]
 [ 5.586527   7.3216014  6.2487698 ...  1.292631   5.797888   1.2879843]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_296"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_297 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_26 (LogProbL  (None,)                  3291840   
 ayer)                                                           
                                                                 
=================================================================
Total params: 3,291,840
Trainable params: 3,291,840
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_26/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_26'")
self.model: <keras.engine.functional.Functional object at 0x7f5f8f5bfe20>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f589108f700>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f589108f700>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f5827d5d690>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f5f8f3fa140>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f5f8f3fa6b0>, <keras.callbacks.ModelCheckpoint object at 0x7f5f8f3fa770>, <keras.callbacks.EarlyStopping object at 0x7f5f8f3fa9e0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f5f8f3faa10>, <keras.callbacks.TerminateOnNaN object at 0x7f5f8f3fa650>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_342/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 342/720 with hyperparameters:
timestamp = 2023-10-24 08:35:09.254317
ndims = 64
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 3291840
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 5.465355    7.122426    6.507682    5.369255    4.1136785   6.196197
  4.259451    8.702779    9.329599    4.155665    7.5761204   5.312057
  5.67009     9.304932    0.8909831   1.4673554  -0.03015512  8.003835
  8.704047    8.985184    9.595006    8.103893    4.62255     8.186032
  0.5885266   6.325719    0.92288995  9.5184145   6.6715336   3.3791308
  2.4301429   8.523114    4.619198    5.3705144   0.3660334   5.918427
  6.2292395   7.1572037   9.551745    6.67638     3.2475867   4.406022
  7.1114645   0.65166384  6.9876056   6.6356955   2.3022606   1.2872343
  3.532573    3.8445692   6.0782557   4.5467997   9.491786    0.11065191
  3.0255654   1.2933161   6.592196    2.150623    4.2476006   4.39
  1.805124    1.2216942   7.396178    1.4448237 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 102: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-24 08:38:14.192 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 2237.2000, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 185s - loss: nan - MinusLogProbMetric: 2237.2000 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 185s/epoch - 943ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 0.0001111111111111111.
===========
Generating train data for run 342.
===========
Train data generated in 0.21 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_342/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_342/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_342/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_342
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.465355   7.122426   6.507682  ...  1.2216942  7.396178   1.4448237]
 [ 1.8683796  2.8760538  9.147447  ...  4.980697  -1.1366768  2.8016024]
 [ 5.6175413  8.642618   5.6734285 ... -0.9440901  7.0561414  1.3082856]
 ...
 [ 6.145307   2.8273122  6.191003  ...  2.964365   6.077158   2.5509653]
 [ 6.5595512  2.759307   6.0778823 ...  3.4800506  4.3951674  2.1941729]
 [ 5.586527   7.3216014  6.2487698 ...  1.292631   5.797888   1.2879843]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_307"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_308 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_27 (LogProbL  (None,)                  3291840   
 ayer)                                                           
                                                                 
=================================================================
Total params: 3,291,840
Trainable params: 3,291,840
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_27/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_27'")
self.model: <keras.engine.functional.Functional object at 0x7f5aa42c71c0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f58375e91b0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f58375e91b0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f58351952d0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f5842583dc0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f57f4320370>, <keras.callbacks.ModelCheckpoint object at 0x7f57f4320430>, <keras.callbacks.EarlyStopping object at 0x7f57f43206a0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f57f43206d0>, <keras.callbacks.TerminateOnNaN object at 0x7f57f4320310>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_342/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 342/720 with hyperparameters:
timestamp = 2023-10-24 08:38:22.309077
ndims = 64
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 3291840
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 5.465355    7.122426    6.507682    5.369255    4.1136785   6.196197
  4.259451    8.702779    9.329599    4.155665    7.5761204   5.312057
  5.67009     9.304932    0.8909831   1.4673554  -0.03015512  8.003835
  8.704047    8.985184    9.595006    8.103893    4.62255     8.186032
  0.5885266   6.325719    0.92288995  9.5184145   6.6715336   3.3791308
  2.4301429   8.523114    4.619198    5.3705144   0.3660334   5.918427
  6.2292395   7.1572037   9.551745    6.67638     3.2475867   4.406022
  7.1114645   0.65166384  6.9876056   6.6356955   2.3022606   1.2872343
  3.532573    3.8445692   6.0782557   4.5467997   9.491786    0.11065191
  3.0255654   1.2933161   6.592196    2.150623    4.2476006   4.39
  1.805124    1.2216942   7.396178    1.4448237 ]
Epoch 1/1000
2023-10-24 08:41:36.720 
Epoch 1/1000 
	 loss: 2151.7285, MinusLogProbMetric: 2151.7285, val_loss: 958.1857, val_MinusLogProbMetric: 958.1857

Epoch 1: val_loss improved from inf to 958.18567, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 195s - loss: 2151.7285 - MinusLogProbMetric: 2151.7285 - val_loss: 958.1857 - val_MinusLogProbMetric: 958.1857 - lr: 1.1111e-04 - 195s/epoch - 995ms/step
Epoch 2/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 130: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-24 08:42:24.357 
Epoch 2/1000 
	 loss: nan, MinusLogProbMetric: 803.5291, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 2: val_loss did not improve from 958.18567
196/196 - 46s - loss: nan - MinusLogProbMetric: 803.5291 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.1111e-04 - 46s/epoch - 236ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 3.703703703703703e-05.
===========
Generating train data for run 342.
===========
Train data generated in 0.21 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_342/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_342/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_342/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_342
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.465355   7.122426   6.507682  ...  1.2216942  7.396178   1.4448237]
 [ 1.8683796  2.8760538  9.147447  ...  4.980697  -1.1366768  2.8016024]
 [ 5.6175413  8.642618   5.6734285 ... -0.9440901  7.0561414  1.3082856]
 ...
 [ 6.145307   2.8273122  6.191003  ...  2.964365   6.077158   2.5509653]
 [ 6.5595512  2.759307   6.0778823 ...  3.4800506  4.3951674  2.1941729]
 [ 5.586527   7.3216014  6.2487698 ...  1.292631   5.797888   1.2879843]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_318"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_319 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_28 (LogProbL  (None,)                  3291840   
 ayer)                                                           
                                                                 
=================================================================
Total params: 3,291,840
Trainable params: 3,291,840
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_28/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_28'")
self.model: <keras.engine.functional.Functional object at 0x7f5aa41cd480>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f5835f51540>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f5835f51540>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f5827c3b370>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f595de19c90>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f595de1b1c0>, <keras.callbacks.ModelCheckpoint object at 0x7f595de18e50>, <keras.callbacks.EarlyStopping object at 0x7f595de196c0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f595de18df0>, <keras.callbacks.TerminateOnNaN object at 0x7f595de186a0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 342/720 with hyperparameters:
timestamp = 2023-10-24 08:42:41.356817
ndims = 64
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 3291840
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 3.703703703703703e-05...
Train first sample: [ 5.465355    7.122426    6.507682    5.369255    4.1136785   6.196197
  4.259451    8.702779    9.329599    4.155665    7.5761204   5.312057
  5.67009     9.304932    0.8909831   1.4673554  -0.03015512  8.003835
  8.704047    8.985184    9.595006    8.103893    4.62255     8.186032
  0.5885266   6.325719    0.92288995  9.5184145   6.6715336   3.3791308
  2.4301429   8.523114    4.619198    5.3705144   0.3660334   5.918427
  6.2292395   7.1572037   9.551745    6.67638     3.2475867   4.406022
  7.1114645   0.65166384  6.9876056   6.6356955   2.3022606   1.2872343
  3.532573    3.8445692   6.0782557   4.5467997   9.491786    0.11065191
  3.0255654   1.2933161   6.592196    2.150623    4.2476006   4.39
  1.805124    1.2216942   7.396178    1.4448237 ]
Epoch 1/1000
2023-10-24 08:46:00.167 
Epoch 1/1000 
	 loss: 671.3216, MinusLogProbMetric: 671.3216, val_loss: 629.8935, val_MinusLogProbMetric: 629.8935

Epoch 1: val_loss improved from inf to 629.89349, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 199s - loss: 671.3216 - MinusLogProbMetric: 671.3216 - val_loss: 629.8935 - val_MinusLogProbMetric: 629.8935 - lr: 3.7037e-05 - 199s/epoch - 1s/step
Epoch 2/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 135: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-24 08:46:48.402 
Epoch 2/1000 
	 loss: nan, MinusLogProbMetric: 577.0729, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 2: val_loss did not improve from 629.89349
196/196 - 47s - loss: nan - MinusLogProbMetric: 577.0729 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.7037e-05 - 47s/epoch - 241ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 1.2345679012345677e-05.
===========
Generating train data for run 342.
===========
Train data generated in 0.35 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_342/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_342/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_342/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_342
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.465355   7.122426   6.507682  ...  1.2216942  7.396178   1.4448237]
 [ 1.8683796  2.8760538  9.147447  ...  4.980697  -1.1366768  2.8016024]
 [ 5.6175413  8.642618   5.6734285 ... -0.9440901  7.0561414  1.3082856]
 ...
 [ 6.145307   2.8273122  6.191003  ...  2.964365   6.077158   2.5509653]
 [ 6.5595512  2.759307   6.0778823 ...  3.4800506  4.3951674  2.1941729]
 [ 5.586527   7.3216014  6.2487698 ...  1.292631   5.797888   1.2879843]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_329"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_330 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_29 (LogProbL  (None,)                  3291840   
 ayer)                                                           
                                                                 
=================================================================
Total params: 3,291,840
Trainable params: 3,291,840
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_29/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_29'")
self.model: <keras.engine.functional.Functional object at 0x7f58414d09a0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f593dfc7fa0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f593dfc7fa0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f57bc853400>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f5791f89390>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f5791f89900>, <keras.callbacks.ModelCheckpoint object at 0x7f5791f899c0>, <keras.callbacks.EarlyStopping object at 0x7f5791f89c30>, <keras.callbacks.ReduceLROnPlateau object at 0x7f5791f89c60>, <keras.callbacks.TerminateOnNaN object at 0x7f5791f898a0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 342/720 with hyperparameters:
timestamp = 2023-10-24 08:46:57.385344
ndims = 64
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 3291840
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.2345679012345677e-05...
Train first sample: [ 5.465355    7.122426    6.507682    5.369255    4.1136785   6.196197
  4.259451    8.702779    9.329599    4.155665    7.5761204   5.312057
  5.67009     9.304932    0.8909831   1.4673554  -0.03015512  8.003835
  8.704047    8.985184    9.595006    8.103893    4.62255     8.186032
  0.5885266   6.325719    0.92288995  9.5184145   6.6715336   3.3791308
  2.4301429   8.523114    4.619198    5.3705144   0.3660334   5.918427
  6.2292395   7.1572037   9.551745    6.67638     3.2475867   4.406022
  7.1114645   0.65166384  6.9876056   6.6356955   2.3022606   1.2872343
  3.532573    3.8445692   6.0782557   4.5467997   9.491786    0.11065191
  3.0255654   1.2933161   6.592196    2.150623    4.2476006   4.39
  1.805124    1.2216942   7.396178    1.4448237 ]
Epoch 1/1000
2023-10-24 08:50:27.148 
Epoch 1/1000 
	 loss: 480.0719, MinusLogProbMetric: 480.0719, val_loss: 414.7218, val_MinusLogProbMetric: 414.7218

Epoch 1: val_loss improved from inf to 414.72183, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 211s - loss: 480.0719 - MinusLogProbMetric: 480.0719 - val_loss: 414.7218 - val_MinusLogProbMetric: 414.7218 - lr: 1.2346e-05 - 211s/epoch - 1s/step
Epoch 2/1000
2023-10-24 08:51:35.555 
Epoch 2/1000 
	 loss: 415.8312, MinusLogProbMetric: 415.8312, val_loss: 379.7564, val_MinusLogProbMetric: 379.7564

Epoch 2: val_loss improved from 414.72183 to 379.75644, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 68s - loss: 415.8312 - MinusLogProbMetric: 415.8312 - val_loss: 379.7564 - val_MinusLogProbMetric: 379.7564 - lr: 1.2346e-05 - 68s/epoch - 348ms/step
Epoch 3/1000
2023-10-24 08:52:43.704 
Epoch 3/1000 
	 loss: 453.4327, MinusLogProbMetric: 453.4327, val_loss: 437.2048, val_MinusLogProbMetric: 437.2048

Epoch 3: val_loss did not improve from 379.75644
196/196 - 67s - loss: 453.4327 - MinusLogProbMetric: 453.4327 - val_loss: 437.2048 - val_MinusLogProbMetric: 437.2048 - lr: 1.2346e-05 - 67s/epoch - 343ms/step
Epoch 4/1000
2023-10-24 08:53:49.909 
Epoch 4/1000 
	 loss: 399.8939, MinusLogProbMetric: 399.8939, val_loss: 372.6000, val_MinusLogProbMetric: 372.6000

Epoch 4: val_loss improved from 379.75644 to 372.60004, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 67s - loss: 399.8939 - MinusLogProbMetric: 399.8939 - val_loss: 372.6000 - val_MinusLogProbMetric: 372.6000 - lr: 1.2346e-05 - 67s/epoch - 343ms/step
Epoch 5/1000
2023-10-24 08:54:58.297 
Epoch 5/1000 
	 loss: 358.1004, MinusLogProbMetric: 358.1004, val_loss: 342.6048, val_MinusLogProbMetric: 342.6048

Epoch 5: val_loss improved from 372.60004 to 342.60480, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 68s - loss: 358.1004 - MinusLogProbMetric: 358.1004 - val_loss: 342.6048 - val_MinusLogProbMetric: 342.6048 - lr: 1.2346e-05 - 68s/epoch - 349ms/step
Epoch 6/1000
2023-10-24 08:56:06.177 
Epoch 6/1000 
	 loss: 347.0783, MinusLogProbMetric: 347.0783, val_loss: 328.4799, val_MinusLogProbMetric: 328.4799

Epoch 6: val_loss improved from 342.60480 to 328.47989, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 68s - loss: 347.0783 - MinusLogProbMetric: 347.0783 - val_loss: 328.4799 - val_MinusLogProbMetric: 328.4799 - lr: 1.2346e-05 - 68s/epoch - 347ms/step
Epoch 7/1000
2023-10-24 08:57:13.145 
Epoch 7/1000 
	 loss: 321.3326, MinusLogProbMetric: 321.3326, val_loss: 309.3856, val_MinusLogProbMetric: 309.3856

Epoch 7: val_loss improved from 328.47989 to 309.38556, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 67s - loss: 321.3326 - MinusLogProbMetric: 321.3326 - val_loss: 309.3856 - val_MinusLogProbMetric: 309.3856 - lr: 1.2346e-05 - 67s/epoch - 341ms/step
Epoch 8/1000
2023-10-24 08:58:22.309 
Epoch 8/1000 
	 loss: 302.7344, MinusLogProbMetric: 302.7344, val_loss: 297.4059, val_MinusLogProbMetric: 297.4059

Epoch 8: val_loss improved from 309.38556 to 297.40585, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 69s - loss: 302.7344 - MinusLogProbMetric: 302.7344 - val_loss: 297.4059 - val_MinusLogProbMetric: 297.4059 - lr: 1.2346e-05 - 69s/epoch - 353ms/step
Epoch 9/1000
2023-10-24 08:59:29.960 
Epoch 9/1000 
	 loss: 312.9703, MinusLogProbMetric: 312.9703, val_loss: 338.6873, val_MinusLogProbMetric: 338.6873

Epoch 9: val_loss did not improve from 297.40585
196/196 - 67s - loss: 312.9703 - MinusLogProbMetric: 312.9703 - val_loss: 338.6873 - val_MinusLogProbMetric: 338.6873 - lr: 1.2346e-05 - 67s/epoch - 340ms/step
Epoch 10/1000
2023-10-24 09:00:35.789 
Epoch 10/1000 
	 loss: 316.7038, MinusLogProbMetric: 316.7038, val_loss: 294.1862, val_MinusLogProbMetric: 294.1862

Epoch 10: val_loss improved from 297.40585 to 294.18619, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 67s - loss: 316.7038 - MinusLogProbMetric: 316.7038 - val_loss: 294.1862 - val_MinusLogProbMetric: 294.1862 - lr: 1.2346e-05 - 67s/epoch - 340ms/step
Epoch 11/1000
2023-10-24 09:01:44.200 
Epoch 11/1000 
	 loss: 284.7510, MinusLogProbMetric: 284.7510, val_loss: 275.1241, val_MinusLogProbMetric: 275.1241

Epoch 11: val_loss improved from 294.18619 to 275.12415, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 68s - loss: 284.7510 - MinusLogProbMetric: 284.7510 - val_loss: 275.1241 - val_MinusLogProbMetric: 275.1241 - lr: 1.2346e-05 - 68s/epoch - 349ms/step
Epoch 12/1000
2023-10-24 09:02:51.121 
Epoch 12/1000 
	 loss: 269.4380, MinusLogProbMetric: 269.4380, val_loss: 267.2738, val_MinusLogProbMetric: 267.2738

Epoch 12: val_loss improved from 275.12415 to 267.27380, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 67s - loss: 269.4380 - MinusLogProbMetric: 269.4380 - val_loss: 267.2738 - val_MinusLogProbMetric: 267.2738 - lr: 1.2346e-05 - 67s/epoch - 342ms/step
Epoch 13/1000
2023-10-24 09:04:01.058 
Epoch 13/1000 
	 loss: 259.6659, MinusLogProbMetric: 259.6659, val_loss: 253.3124, val_MinusLogProbMetric: 253.3124

Epoch 13: val_loss improved from 267.27380 to 253.31244, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 70s - loss: 259.6659 - MinusLogProbMetric: 259.6659 - val_loss: 253.3124 - val_MinusLogProbMetric: 253.3124 - lr: 1.2346e-05 - 70s/epoch - 357ms/step
Epoch 14/1000
2023-10-24 09:05:11.138 
Epoch 14/1000 
	 loss: 251.5982, MinusLogProbMetric: 251.5982, val_loss: 259.8798, val_MinusLogProbMetric: 259.8798

Epoch 14: val_loss did not improve from 253.31244
196/196 - 69s - loss: 251.5982 - MinusLogProbMetric: 251.5982 - val_loss: 259.8798 - val_MinusLogProbMetric: 259.8798 - lr: 1.2346e-05 - 69s/epoch - 352ms/step
Epoch 15/1000
2023-10-24 09:06:20.493 
Epoch 15/1000 
	 loss: 243.8729, MinusLogProbMetric: 243.8729, val_loss: 238.8369, val_MinusLogProbMetric: 238.8369

Epoch 15: val_loss improved from 253.31244 to 238.83685, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 70s - loss: 243.8729 - MinusLogProbMetric: 243.8729 - val_loss: 238.8369 - val_MinusLogProbMetric: 238.8369 - lr: 1.2346e-05 - 70s/epoch - 359ms/step
Epoch 16/1000
2023-10-24 09:07:32.240 
Epoch 16/1000 
	 loss: 234.5101, MinusLogProbMetric: 234.5101, val_loss: 230.6556, val_MinusLogProbMetric: 230.6556

Epoch 16: val_loss improved from 238.83685 to 230.65561, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 72s - loss: 234.5101 - MinusLogProbMetric: 234.5101 - val_loss: 230.6556 - val_MinusLogProbMetric: 230.6556 - lr: 1.2346e-05 - 72s/epoch - 366ms/step
Epoch 17/1000
2023-10-24 09:08:40.574 
Epoch 17/1000 
	 loss: 229.3099, MinusLogProbMetric: 229.3099, val_loss: 229.1377, val_MinusLogProbMetric: 229.1377

Epoch 17: val_loss improved from 230.65561 to 229.13773, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 68s - loss: 229.3099 - MinusLogProbMetric: 229.3099 - val_loss: 229.1377 - val_MinusLogProbMetric: 229.1377 - lr: 1.2346e-05 - 68s/epoch - 349ms/step
Epoch 18/1000
2023-10-24 09:09:50.676 
Epoch 18/1000 
	 loss: 225.1236, MinusLogProbMetric: 225.1236, val_loss: 220.9382, val_MinusLogProbMetric: 220.9382

Epoch 18: val_loss improved from 229.13773 to 220.93819, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 70s - loss: 225.1236 - MinusLogProbMetric: 225.1236 - val_loss: 220.9382 - val_MinusLogProbMetric: 220.9382 - lr: 1.2346e-05 - 70s/epoch - 358ms/step
Epoch 19/1000
2023-10-24 09:11:00.927 
Epoch 19/1000 
	 loss: 218.4508, MinusLogProbMetric: 218.4508, val_loss: 215.8462, val_MinusLogProbMetric: 215.8462

Epoch 19: val_loss improved from 220.93819 to 215.84616, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 70s - loss: 218.4508 - MinusLogProbMetric: 218.4508 - val_loss: 215.8462 - val_MinusLogProbMetric: 215.8462 - lr: 1.2346e-05 - 70s/epoch - 358ms/step
Epoch 20/1000
2023-10-24 09:12:07.363 
Epoch 20/1000 
	 loss: 217.1759, MinusLogProbMetric: 217.1759, val_loss: 218.1341, val_MinusLogProbMetric: 218.1341

Epoch 20: val_loss did not improve from 215.84616
196/196 - 66s - loss: 217.1759 - MinusLogProbMetric: 217.1759 - val_loss: 218.1341 - val_MinusLogProbMetric: 218.1341 - lr: 1.2346e-05 - 66s/epoch - 334ms/step
Epoch 21/1000
2023-10-24 09:13:10.924 
Epoch 21/1000 
	 loss: 213.1084, MinusLogProbMetric: 213.1084, val_loss: 216.6042, val_MinusLogProbMetric: 216.6042

Epoch 21: val_loss did not improve from 215.84616
196/196 - 64s - loss: 213.1084 - MinusLogProbMetric: 213.1084 - val_loss: 216.6042 - val_MinusLogProbMetric: 216.6042 - lr: 1.2346e-05 - 64s/epoch - 324ms/step
Epoch 22/1000
2023-10-24 09:14:17.321 
Epoch 22/1000 
	 loss: 209.9784, MinusLogProbMetric: 209.9784, val_loss: 207.3687, val_MinusLogProbMetric: 207.3687

Epoch 22: val_loss improved from 215.84616 to 207.36865, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 67s - loss: 209.9784 - MinusLogProbMetric: 209.9784 - val_loss: 207.3687 - val_MinusLogProbMetric: 207.3687 - lr: 1.2346e-05 - 67s/epoch - 343ms/step
Epoch 23/1000
2023-10-24 09:15:24.954 
Epoch 23/1000 
	 loss: 204.0825, MinusLogProbMetric: 204.0825, val_loss: 200.5770, val_MinusLogProbMetric: 200.5770

Epoch 23: val_loss improved from 207.36865 to 200.57701, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 68s - loss: 204.0825 - MinusLogProbMetric: 204.0825 - val_loss: 200.5770 - val_MinusLogProbMetric: 200.5770 - lr: 1.2346e-05 - 68s/epoch - 345ms/step
Epoch 24/1000
2023-10-24 09:16:29.536 
Epoch 24/1000 
	 loss: 199.9664, MinusLogProbMetric: 199.9664, val_loss: 203.8414, val_MinusLogProbMetric: 203.8414

Epoch 24: val_loss did not improve from 200.57701
196/196 - 64s - loss: 199.9664 - MinusLogProbMetric: 199.9664 - val_loss: 203.8414 - val_MinusLogProbMetric: 203.8414 - lr: 1.2346e-05 - 64s/epoch - 324ms/step
Epoch 25/1000
2023-10-24 09:17:36.884 
Epoch 25/1000 
	 loss: 195.7370, MinusLogProbMetric: 195.7370, val_loss: 197.1512, val_MinusLogProbMetric: 197.1512

Epoch 25: val_loss improved from 200.57701 to 197.15117, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 68s - loss: 195.7370 - MinusLogProbMetric: 195.7370 - val_loss: 197.1512 - val_MinusLogProbMetric: 197.1512 - lr: 1.2346e-05 - 68s/epoch - 349ms/step
Epoch 26/1000
2023-10-24 09:18:46.103 
Epoch 26/1000 
	 loss: 195.9148, MinusLogProbMetric: 195.9148, val_loss: 192.1830, val_MinusLogProbMetric: 192.1830

Epoch 26: val_loss improved from 197.15117 to 192.18295, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 69s - loss: 195.9148 - MinusLogProbMetric: 195.9148 - val_loss: 192.1830 - val_MinusLogProbMetric: 192.1830 - lr: 1.2346e-05 - 69s/epoch - 353ms/step
Epoch 27/1000
2023-10-24 09:19:53.686 
Epoch 27/1000 
	 loss: 190.6032, MinusLogProbMetric: 190.6032, val_loss: 189.2824, val_MinusLogProbMetric: 189.2824

Epoch 27: val_loss improved from 192.18295 to 189.28238, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 68s - loss: 190.6032 - MinusLogProbMetric: 190.6032 - val_loss: 189.2824 - val_MinusLogProbMetric: 189.2824 - lr: 1.2346e-05 - 68s/epoch - 345ms/step
Epoch 28/1000
2023-10-24 09:21:02.737 
Epoch 28/1000 
	 loss: 189.0214, MinusLogProbMetric: 189.0214, val_loss: 187.3891, val_MinusLogProbMetric: 187.3891

Epoch 28: val_loss improved from 189.28238 to 187.38910, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 69s - loss: 189.0214 - MinusLogProbMetric: 189.0214 - val_loss: 187.3891 - val_MinusLogProbMetric: 187.3891 - lr: 1.2346e-05 - 69s/epoch - 352ms/step
Epoch 29/1000
2023-10-24 09:22:08.964 
Epoch 29/1000 
	 loss: 184.0311, MinusLogProbMetric: 184.0311, val_loss: 182.7528, val_MinusLogProbMetric: 182.7528

Epoch 29: val_loss improved from 187.38910 to 182.75276, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 66s - loss: 184.0311 - MinusLogProbMetric: 184.0311 - val_loss: 182.7528 - val_MinusLogProbMetric: 182.7528 - lr: 1.2346e-05 - 66s/epoch - 338ms/step
Epoch 30/1000
2023-10-24 09:23:18.753 
Epoch 30/1000 
	 loss: 181.6559, MinusLogProbMetric: 181.6559, val_loss: 180.8410, val_MinusLogProbMetric: 180.8410

Epoch 30: val_loss improved from 182.75276 to 180.84103, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 70s - loss: 181.6559 - MinusLogProbMetric: 181.6559 - val_loss: 180.8410 - val_MinusLogProbMetric: 180.8410 - lr: 1.2346e-05 - 70s/epoch - 356ms/step
Epoch 31/1000
2023-10-24 09:24:26.884 
Epoch 31/1000 
	 loss: 179.7903, MinusLogProbMetric: 179.7903, val_loss: 178.7972, val_MinusLogProbMetric: 178.7972

Epoch 31: val_loss improved from 180.84103 to 178.79716, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 68s - loss: 179.7903 - MinusLogProbMetric: 179.7903 - val_loss: 178.7972 - val_MinusLogProbMetric: 178.7972 - lr: 1.2346e-05 - 68s/epoch - 348ms/step
Epoch 32/1000
2023-10-24 09:25:33.203 
Epoch 32/1000 
	 loss: 177.0926, MinusLogProbMetric: 177.0926, val_loss: 176.2804, val_MinusLogProbMetric: 176.2804

Epoch 32: val_loss improved from 178.79716 to 176.28036, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 66s - loss: 177.0926 - MinusLogProbMetric: 177.0926 - val_loss: 176.2804 - val_MinusLogProbMetric: 176.2804 - lr: 1.2346e-05 - 66s/epoch - 338ms/step
Epoch 33/1000
2023-10-24 09:26:42.464 
Epoch 33/1000 
	 loss: 174.4189, MinusLogProbMetric: 174.4189, val_loss: 173.4785, val_MinusLogProbMetric: 173.4785

Epoch 33: val_loss improved from 176.28036 to 173.47845, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 69s - loss: 174.4189 - MinusLogProbMetric: 174.4189 - val_loss: 173.4785 - val_MinusLogProbMetric: 173.4785 - lr: 1.2346e-05 - 69s/epoch - 353ms/step
Epoch 34/1000
2023-10-24 09:27:51.582 
Epoch 34/1000 
	 loss: 172.4221, MinusLogProbMetric: 172.4221, val_loss: 171.0301, val_MinusLogProbMetric: 171.0301

Epoch 34: val_loss improved from 173.47845 to 171.03009, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 69s - loss: 172.4221 - MinusLogProbMetric: 172.4221 - val_loss: 171.0301 - val_MinusLogProbMetric: 171.0301 - lr: 1.2346e-05 - 69s/epoch - 354ms/step
Epoch 35/1000
2023-10-24 09:29:00.086 
Epoch 35/1000 
	 loss: 170.8078, MinusLogProbMetric: 170.8078, val_loss: 169.1569, val_MinusLogProbMetric: 169.1569

Epoch 35: val_loss improved from 171.03009 to 169.15691, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 68s - loss: 170.8078 - MinusLogProbMetric: 170.8078 - val_loss: 169.1569 - val_MinusLogProbMetric: 169.1569 - lr: 1.2346e-05 - 68s/epoch - 348ms/step
Epoch 36/1000
2023-10-24 09:30:06.273 
Epoch 36/1000 
	 loss: 168.6887, MinusLogProbMetric: 168.6887, val_loss: 170.0711, val_MinusLogProbMetric: 170.0711

Epoch 36: val_loss did not improve from 169.15691
196/196 - 65s - loss: 168.6887 - MinusLogProbMetric: 168.6887 - val_loss: 170.0711 - val_MinusLogProbMetric: 170.0711 - lr: 1.2346e-05 - 65s/epoch - 333ms/step
Epoch 37/1000
2023-10-24 09:31:12.479 
Epoch 37/1000 
	 loss: 168.0149, MinusLogProbMetric: 168.0149, val_loss: 165.3891, val_MinusLogProbMetric: 165.3891

Epoch 37: val_loss improved from 169.15691 to 165.38908, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 67s - loss: 168.0149 - MinusLogProbMetric: 168.0149 - val_loss: 165.3891 - val_MinusLogProbMetric: 165.3891 - lr: 1.2346e-05 - 67s/epoch - 343ms/step
Epoch 38/1000
2023-10-24 09:32:20.112 
Epoch 38/1000 
	 loss: 163.8584, MinusLogProbMetric: 163.8584, val_loss: 164.2451, val_MinusLogProbMetric: 164.2451

Epoch 38: val_loss improved from 165.38908 to 164.24509, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 67s - loss: 163.8584 - MinusLogProbMetric: 163.8584 - val_loss: 164.2451 - val_MinusLogProbMetric: 164.2451 - lr: 1.2346e-05 - 67s/epoch - 343ms/step
Epoch 39/1000
2023-10-24 09:33:27.364 
Epoch 39/1000 
	 loss: 167.6493, MinusLogProbMetric: 167.6493, val_loss: 163.9022, val_MinusLogProbMetric: 163.9022

Epoch 39: val_loss improved from 164.24509 to 163.90222, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 68s - loss: 167.6493 - MinusLogProbMetric: 167.6493 - val_loss: 163.9022 - val_MinusLogProbMetric: 163.9022 - lr: 1.2346e-05 - 68s/epoch - 345ms/step
Epoch 40/1000
2023-10-24 09:34:35.640 
Epoch 40/1000 
	 loss: 161.2790, MinusLogProbMetric: 161.2790, val_loss: 164.0264, val_MinusLogProbMetric: 164.0264

Epoch 40: val_loss did not improve from 163.90222
196/196 - 67s - loss: 161.2790 - MinusLogProbMetric: 161.2790 - val_loss: 164.0264 - val_MinusLogProbMetric: 164.0264 - lr: 1.2346e-05 - 67s/epoch - 343ms/step
Epoch 41/1000
2023-10-24 09:35:44.372 
Epoch 41/1000 
	 loss: 158.7931, MinusLogProbMetric: 158.7931, val_loss: 157.8377, val_MinusLogProbMetric: 157.8377

Epoch 41: val_loss improved from 163.90222 to 157.83774, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 70s - loss: 158.7931 - MinusLogProbMetric: 158.7931 - val_loss: 157.8377 - val_MinusLogProbMetric: 157.8377 - lr: 1.2346e-05 - 70s/epoch - 357ms/step
Epoch 42/1000
2023-10-24 09:36:56.512 
Epoch 42/1000 
	 loss: 159.7282, MinusLogProbMetric: 159.7282, val_loss: 158.4590, val_MinusLogProbMetric: 158.4590

Epoch 42: val_loss did not improve from 157.83774
196/196 - 71s - loss: 159.7282 - MinusLogProbMetric: 159.7282 - val_loss: 158.4590 - val_MinusLogProbMetric: 158.4590 - lr: 1.2346e-05 - 71s/epoch - 362ms/step
Epoch 43/1000
2023-10-24 09:38:05.217 
Epoch 43/1000 
	 loss: 156.5395, MinusLogProbMetric: 156.5395, val_loss: 157.7704, val_MinusLogProbMetric: 157.7704

Epoch 43: val_loss improved from 157.83774 to 157.77039, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 70s - loss: 156.5395 - MinusLogProbMetric: 156.5395 - val_loss: 157.7704 - val_MinusLogProbMetric: 157.7704 - lr: 1.2346e-05 - 70s/epoch - 356ms/step
Epoch 44/1000
2023-10-24 09:39:13.161 
Epoch 44/1000 
	 loss: 159.9658, MinusLogProbMetric: 159.9658, val_loss: 187.3128, val_MinusLogProbMetric: 187.3128

Epoch 44: val_loss did not improve from 157.77039
196/196 - 67s - loss: 159.9658 - MinusLogProbMetric: 159.9658 - val_loss: 187.3128 - val_MinusLogProbMetric: 187.3128 - lr: 1.2346e-05 - 67s/epoch - 341ms/step
Epoch 45/1000
2023-10-24 09:40:23.359 
Epoch 45/1000 
	 loss: 164.1266, MinusLogProbMetric: 164.1266, val_loss: 156.9262, val_MinusLogProbMetric: 156.9262

Epoch 45: val_loss improved from 157.77039 to 156.92619, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 71s - loss: 164.1266 - MinusLogProbMetric: 164.1266 - val_loss: 156.9262 - val_MinusLogProbMetric: 156.9262 - lr: 1.2346e-05 - 71s/epoch - 364ms/step
Epoch 46/1000
2023-10-24 09:41:30.760 
Epoch 46/1000 
	 loss: 157.0291, MinusLogProbMetric: 157.0291, val_loss: 157.9736, val_MinusLogProbMetric: 157.9736

Epoch 46: val_loss did not improve from 156.92619
196/196 - 66s - loss: 157.0291 - MinusLogProbMetric: 157.0291 - val_loss: 157.9736 - val_MinusLogProbMetric: 157.9736 - lr: 1.2346e-05 - 66s/epoch - 338ms/step
Epoch 47/1000
2023-10-24 09:42:37.325 
Epoch 47/1000 
	 loss: 153.1262, MinusLogProbMetric: 153.1262, val_loss: 151.0832, val_MinusLogProbMetric: 151.0832

Epoch 47: val_loss improved from 156.92619 to 151.08316, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 68s - loss: 153.1262 - MinusLogProbMetric: 153.1262 - val_loss: 151.0832 - val_MinusLogProbMetric: 151.0832 - lr: 1.2346e-05 - 68s/epoch - 344ms/step
Epoch 48/1000
2023-10-24 09:43:44.018 
Epoch 48/1000 
	 loss: 150.6772, MinusLogProbMetric: 150.6772, val_loss: 149.3656, val_MinusLogProbMetric: 149.3656

Epoch 48: val_loss improved from 151.08316 to 149.36560, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 67s - loss: 150.6772 - MinusLogProbMetric: 150.6772 - val_loss: 149.3656 - val_MinusLogProbMetric: 149.3656 - lr: 1.2346e-05 - 67s/epoch - 340ms/step
Epoch 49/1000
2023-10-24 09:44:53.641 
Epoch 49/1000 
	 loss: 148.4895, MinusLogProbMetric: 148.4895, val_loss: 147.5538, val_MinusLogProbMetric: 147.5538

Epoch 49: val_loss improved from 149.36560 to 147.55383, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 70s - loss: 148.4895 - MinusLogProbMetric: 148.4895 - val_loss: 147.5538 - val_MinusLogProbMetric: 147.5538 - lr: 1.2346e-05 - 70s/epoch - 355ms/step
Epoch 50/1000
2023-10-24 09:46:02.099 
Epoch 50/1000 
	 loss: 147.2699, MinusLogProbMetric: 147.2699, val_loss: 149.4401, val_MinusLogProbMetric: 149.4401

Epoch 50: val_loss did not improve from 147.55383
196/196 - 68s - loss: 147.2699 - MinusLogProbMetric: 147.2699 - val_loss: 149.4401 - val_MinusLogProbMetric: 149.4401 - lr: 1.2346e-05 - 68s/epoch - 345ms/step
Epoch 51/1000
2023-10-24 09:47:08.532 
Epoch 51/1000 
	 loss: 145.8742, MinusLogProbMetric: 145.8742, val_loss: 144.1201, val_MinusLogProbMetric: 144.1201

Epoch 51: val_loss improved from 147.55383 to 144.12010, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 67s - loss: 145.8742 - MinusLogProbMetric: 145.8742 - val_loss: 144.1201 - val_MinusLogProbMetric: 144.1201 - lr: 1.2346e-05 - 67s/epoch - 344ms/step
Epoch 52/1000
2023-10-24 09:48:19.539 
Epoch 52/1000 
	 loss: 144.2706, MinusLogProbMetric: 144.2706, val_loss: 144.3314, val_MinusLogProbMetric: 144.3314

Epoch 52: val_loss did not improve from 144.12010
196/196 - 70s - loss: 144.2706 - MinusLogProbMetric: 144.2706 - val_loss: 144.3314 - val_MinusLogProbMetric: 144.3314 - lr: 1.2346e-05 - 70s/epoch - 357ms/step
Epoch 53/1000
2023-10-24 09:49:27.976 
Epoch 53/1000 
	 loss: 143.0278, MinusLogProbMetric: 143.0278, val_loss: 142.5734, val_MinusLogProbMetric: 142.5734

Epoch 53: val_loss improved from 144.12010 to 142.57341, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 69s - loss: 143.0278 - MinusLogProbMetric: 143.0278 - val_loss: 142.5734 - val_MinusLogProbMetric: 142.5734 - lr: 1.2346e-05 - 69s/epoch - 354ms/step
Epoch 54/1000
2023-10-24 09:50:35.894 
Epoch 54/1000 
	 loss: 141.4981, MinusLogProbMetric: 141.4981, val_loss: 140.2116, val_MinusLogProbMetric: 140.2116

Epoch 54: val_loss improved from 142.57341 to 140.21156, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 68s - loss: 141.4981 - MinusLogProbMetric: 141.4981 - val_loss: 140.2116 - val_MinusLogProbMetric: 140.2116 - lr: 1.2346e-05 - 68s/epoch - 346ms/step
Epoch 55/1000
2023-10-24 09:51:45.273 
Epoch 55/1000 
	 loss: 140.3103, MinusLogProbMetric: 140.3103, val_loss: 140.0849, val_MinusLogProbMetric: 140.0849

Epoch 55: val_loss improved from 140.21156 to 140.08493, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 69s - loss: 140.3103 - MinusLogProbMetric: 140.3103 - val_loss: 140.0849 - val_MinusLogProbMetric: 140.0849 - lr: 1.2346e-05 - 69s/epoch - 354ms/step
Epoch 56/1000
2023-10-24 09:52:53.411 
Epoch 56/1000 
	 loss: 138.5859, MinusLogProbMetric: 138.5859, val_loss: 138.4971, val_MinusLogProbMetric: 138.4971

Epoch 56: val_loss improved from 140.08493 to 138.49713, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 68s - loss: 138.5859 - MinusLogProbMetric: 138.5859 - val_loss: 138.4971 - val_MinusLogProbMetric: 138.4971 - lr: 1.2346e-05 - 68s/epoch - 348ms/step
Epoch 57/1000
2023-10-24 09:54:02.463 
Epoch 57/1000 
	 loss: 137.3161, MinusLogProbMetric: 137.3161, val_loss: 138.4939, val_MinusLogProbMetric: 138.4939

Epoch 57: val_loss improved from 138.49713 to 138.49390, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 69s - loss: 137.3161 - MinusLogProbMetric: 137.3161 - val_loss: 138.4939 - val_MinusLogProbMetric: 138.4939 - lr: 1.2346e-05 - 69s/epoch - 352ms/step
Epoch 58/1000
2023-10-24 09:55:10.623 
Epoch 58/1000 
	 loss: 145.5210, MinusLogProbMetric: 145.5210, val_loss: 148.6023, val_MinusLogProbMetric: 148.6023

Epoch 58: val_loss did not improve from 138.49390
196/196 - 67s - loss: 145.5210 - MinusLogProbMetric: 145.5210 - val_loss: 148.6023 - val_MinusLogProbMetric: 148.6023 - lr: 1.2346e-05 - 67s/epoch - 343ms/step
Epoch 59/1000
2023-10-24 09:56:19.354 
Epoch 59/1000 
	 loss: 138.6733, MinusLogProbMetric: 138.6733, val_loss: 136.5656, val_MinusLogProbMetric: 136.5656

Epoch 59: val_loss improved from 138.49390 to 136.56558, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 70s - loss: 138.6733 - MinusLogProbMetric: 138.6733 - val_loss: 136.5656 - val_MinusLogProbMetric: 136.5656 - lr: 1.2346e-05 - 70s/epoch - 356ms/step
Epoch 60/1000
2023-10-24 09:57:29.964 
Epoch 60/1000 
	 loss: 141.1860, MinusLogProbMetric: 141.1860, val_loss: 202.2070, val_MinusLogProbMetric: 202.2070

Epoch 60: val_loss did not improve from 136.56558
196/196 - 70s - loss: 141.1860 - MinusLogProbMetric: 141.1860 - val_loss: 202.2070 - val_MinusLogProbMetric: 202.2070 - lr: 1.2346e-05 - 70s/epoch - 355ms/step
Epoch 61/1000
2023-10-24 09:58:36.838 
Epoch 61/1000 
	 loss: 155.6785, MinusLogProbMetric: 155.6785, val_loss: 145.1674, val_MinusLogProbMetric: 145.1674

Epoch 61: val_loss did not improve from 136.56558
196/196 - 67s - loss: 155.6785 - MinusLogProbMetric: 155.6785 - val_loss: 145.1674 - val_MinusLogProbMetric: 145.1674 - lr: 1.2346e-05 - 67s/epoch - 341ms/step
Epoch 62/1000
2023-10-24 09:59:42.333 
Epoch 62/1000 
	 loss: 142.4807, MinusLogProbMetric: 142.4807, val_loss: 140.2445, val_MinusLogProbMetric: 140.2445

Epoch 62: val_loss did not improve from 136.56558
196/196 - 65s - loss: 142.4807 - MinusLogProbMetric: 142.4807 - val_loss: 140.2445 - val_MinusLogProbMetric: 140.2445 - lr: 1.2346e-05 - 65s/epoch - 334ms/step
Epoch 63/1000
2023-10-24 10:00:46.647 
Epoch 63/1000 
	 loss: 138.7494, MinusLogProbMetric: 138.7494, val_loss: 137.4494, val_MinusLogProbMetric: 137.4494

Epoch 63: val_loss did not improve from 136.56558
196/196 - 64s - loss: 138.7494 - MinusLogProbMetric: 138.7494 - val_loss: 137.4494 - val_MinusLogProbMetric: 137.4494 - lr: 1.2346e-05 - 64s/epoch - 328ms/step
Epoch 64/1000
2023-10-24 10:01:53.466 
Epoch 64/1000 
	 loss: 139.1615, MinusLogProbMetric: 139.1615, val_loss: 136.7099, val_MinusLogProbMetric: 136.7099

Epoch 64: val_loss did not improve from 136.56558
196/196 - 67s - loss: 139.1615 - MinusLogProbMetric: 139.1615 - val_loss: 136.7099 - val_MinusLogProbMetric: 136.7099 - lr: 1.2346e-05 - 67s/epoch - 341ms/step
Epoch 65/1000
2023-10-24 10:02:58.255 
Epoch 65/1000 
	 loss: 134.9039, MinusLogProbMetric: 134.9039, val_loss: 134.9659, val_MinusLogProbMetric: 134.9659

Epoch 65: val_loss improved from 136.56558 to 134.96594, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 66s - loss: 134.9039 - MinusLogProbMetric: 134.9039 - val_loss: 134.9659 - val_MinusLogProbMetric: 134.9659 - lr: 1.2346e-05 - 66s/epoch - 336ms/step
Epoch 66/1000
2023-10-24 10:04:05.829 
Epoch 66/1000 
	 loss: 133.7422, MinusLogProbMetric: 133.7422, val_loss: 134.8297, val_MinusLogProbMetric: 134.8297

Epoch 66: val_loss improved from 134.96594 to 134.82973, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 67s - loss: 133.7422 - MinusLogProbMetric: 133.7422 - val_loss: 134.8297 - val_MinusLogProbMetric: 134.8297 - lr: 1.2346e-05 - 67s/epoch - 344ms/step
Epoch 67/1000
2023-10-24 10:05:17.299 
Epoch 67/1000 
	 loss: 132.0977, MinusLogProbMetric: 132.0977, val_loss: 131.1903, val_MinusLogProbMetric: 131.1903

Epoch 67: val_loss improved from 134.82973 to 131.19032, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 72s - loss: 132.0977 - MinusLogProbMetric: 132.0977 - val_loss: 131.1903 - val_MinusLogProbMetric: 131.1903 - lr: 1.2346e-05 - 72s/epoch - 365ms/step
Epoch 68/1000
2023-10-24 10:06:29.911 
Epoch 68/1000 
	 loss: 131.3197, MinusLogProbMetric: 131.3197, val_loss: 129.6241, val_MinusLogProbMetric: 129.6241

Epoch 68: val_loss improved from 131.19032 to 129.62410, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 73s - loss: 131.3197 - MinusLogProbMetric: 131.3197 - val_loss: 129.6241 - val_MinusLogProbMetric: 129.6241 - lr: 1.2346e-05 - 73s/epoch - 372ms/step
Epoch 69/1000
2023-10-24 10:07:39.235 
Epoch 69/1000 
	 loss: 129.4496, MinusLogProbMetric: 129.4496, val_loss: 128.6192, val_MinusLogProbMetric: 128.6192

Epoch 69: val_loss improved from 129.62410 to 128.61917, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 69s - loss: 129.4496 - MinusLogProbMetric: 129.4496 - val_loss: 128.6192 - val_MinusLogProbMetric: 128.6192 - lr: 1.2346e-05 - 69s/epoch - 353ms/step
Epoch 70/1000
2023-10-24 10:08:47.770 
Epoch 70/1000 
	 loss: 127.9553, MinusLogProbMetric: 127.9553, val_loss: 130.1162, val_MinusLogProbMetric: 130.1162

Epoch 70: val_loss did not improve from 128.61917
196/196 - 67s - loss: 127.9553 - MinusLogProbMetric: 127.9553 - val_loss: 130.1162 - val_MinusLogProbMetric: 130.1162 - lr: 1.2346e-05 - 67s/epoch - 344ms/step
Epoch 71/1000
2023-10-24 10:09:53.702 
Epoch 71/1000 
	 loss: 126.8924, MinusLogProbMetric: 126.8924, val_loss: 126.6957, val_MinusLogProbMetric: 126.6957

Epoch 71: val_loss improved from 128.61917 to 126.69569, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 67s - loss: 126.8924 - MinusLogProbMetric: 126.8924 - val_loss: 126.6957 - val_MinusLogProbMetric: 126.6957 - lr: 1.2346e-05 - 67s/epoch - 342ms/step
Epoch 72/1000
2023-10-24 10:11:02.886 
Epoch 72/1000 
	 loss: 126.9016, MinusLogProbMetric: 126.9016, val_loss: 129.3136, val_MinusLogProbMetric: 129.3136

Epoch 72: val_loss did not improve from 126.69569
196/196 - 68s - loss: 126.9016 - MinusLogProbMetric: 126.9016 - val_loss: 129.3136 - val_MinusLogProbMetric: 129.3136 - lr: 1.2346e-05 - 68s/epoch - 347ms/step
Epoch 73/1000
2023-10-24 10:12:11.609 
Epoch 73/1000 
	 loss: 125.1181, MinusLogProbMetric: 125.1181, val_loss: 131.2276, val_MinusLogProbMetric: 131.2276

Epoch 73: val_loss did not improve from 126.69569
196/196 - 69s - loss: 125.1181 - MinusLogProbMetric: 125.1181 - val_loss: 131.2276 - val_MinusLogProbMetric: 131.2276 - lr: 1.2346e-05 - 69s/epoch - 351ms/step
Epoch 74/1000
2023-10-24 10:13:18.179 
Epoch 74/1000 
	 loss: 124.8126, MinusLogProbMetric: 124.8126, val_loss: 123.8500, val_MinusLogProbMetric: 123.8500

Epoch 74: val_loss improved from 126.69569 to 123.85001, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 68s - loss: 124.8126 - MinusLogProbMetric: 124.8126 - val_loss: 123.8500 - val_MinusLogProbMetric: 123.8500 - lr: 1.2346e-05 - 68s/epoch - 345ms/step
Epoch 75/1000
2023-10-24 10:14:25.583 
Epoch 75/1000 
	 loss: 122.6977, MinusLogProbMetric: 122.6977, val_loss: 123.5342, val_MinusLogProbMetric: 123.5342

Epoch 75: val_loss improved from 123.85001 to 123.53417, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 67s - loss: 122.6977 - MinusLogProbMetric: 122.6977 - val_loss: 123.5342 - val_MinusLogProbMetric: 123.5342 - lr: 1.2346e-05 - 67s/epoch - 344ms/step
Epoch 76/1000
2023-10-24 10:15:34.961 
Epoch 76/1000 
	 loss: 126.5697, MinusLogProbMetric: 126.5697, val_loss: 122.9975, val_MinusLogProbMetric: 122.9975

Epoch 76: val_loss improved from 123.53417 to 122.99750, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 69s - loss: 126.5697 - MinusLogProbMetric: 126.5697 - val_loss: 122.9975 - val_MinusLogProbMetric: 122.9975 - lr: 1.2346e-05 - 69s/epoch - 354ms/step
Epoch 77/1000
2023-10-24 10:16:42.872 
Epoch 77/1000 
	 loss: 123.6913, MinusLogProbMetric: 123.6913, val_loss: 121.0013, val_MinusLogProbMetric: 121.0013

Epoch 77: val_loss improved from 122.99750 to 121.00126, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 68s - loss: 123.6913 - MinusLogProbMetric: 123.6913 - val_loss: 121.0013 - val_MinusLogProbMetric: 121.0013 - lr: 1.2346e-05 - 68s/epoch - 346ms/step
Epoch 78/1000
2023-10-24 10:17:51.985 
Epoch 78/1000 
	 loss: 122.9424, MinusLogProbMetric: 122.9424, val_loss: 122.3086, val_MinusLogProbMetric: 122.3086

Epoch 78: val_loss did not improve from 121.00126
196/196 - 68s - loss: 122.9424 - MinusLogProbMetric: 122.9424 - val_loss: 122.3086 - val_MinusLogProbMetric: 122.3086 - lr: 1.2346e-05 - 68s/epoch - 347ms/step
Epoch 79/1000
2023-10-24 10:18:58.257 
Epoch 79/1000 
	 loss: 121.7956, MinusLogProbMetric: 121.7956, val_loss: 120.9584, val_MinusLogProbMetric: 120.9584

Epoch 79: val_loss improved from 121.00126 to 120.95838, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 67s - loss: 121.7956 - MinusLogProbMetric: 121.7956 - val_loss: 120.9584 - val_MinusLogProbMetric: 120.9584 - lr: 1.2346e-05 - 67s/epoch - 342ms/step
Epoch 80/1000
2023-10-24 10:20:07.218 
Epoch 80/1000 
	 loss: 119.5767, MinusLogProbMetric: 119.5767, val_loss: 121.6505, val_MinusLogProbMetric: 121.6505

Epoch 80: val_loss did not improve from 120.95838
196/196 - 68s - loss: 119.5767 - MinusLogProbMetric: 119.5767 - val_loss: 121.6505 - val_MinusLogProbMetric: 121.6505 - lr: 1.2346e-05 - 68s/epoch - 348ms/step
Epoch 81/1000
2023-10-24 10:21:15.111 
Epoch 81/1000 
	 loss: 120.8705, MinusLogProbMetric: 120.8705, val_loss: 119.2618, val_MinusLogProbMetric: 119.2618

Epoch 81: val_loss improved from 120.95838 to 119.26178, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 69s - loss: 120.8705 - MinusLogProbMetric: 120.8705 - val_loss: 119.2618 - val_MinusLogProbMetric: 119.2618 - lr: 1.2346e-05 - 69s/epoch - 351ms/step
Epoch 82/1000
2023-10-24 10:22:24.021 
Epoch 82/1000 
	 loss: 118.7065, MinusLogProbMetric: 118.7065, val_loss: 119.1891, val_MinusLogProbMetric: 119.1891

Epoch 82: val_loss improved from 119.26178 to 119.18912, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 69s - loss: 118.7065 - MinusLogProbMetric: 118.7065 - val_loss: 119.1891 - val_MinusLogProbMetric: 119.1891 - lr: 1.2346e-05 - 69s/epoch - 351ms/step
Epoch 83/1000
2023-10-24 10:23:31.806 
Epoch 83/1000 
	 loss: 117.5114, MinusLogProbMetric: 117.5114, val_loss: 117.4640, val_MinusLogProbMetric: 117.4640

Epoch 83: val_loss improved from 119.18912 to 117.46403, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 68s - loss: 117.5114 - MinusLogProbMetric: 117.5114 - val_loss: 117.4640 - val_MinusLogProbMetric: 117.4640 - lr: 1.2346e-05 - 68s/epoch - 346ms/step
Epoch 84/1000
2023-10-24 10:24:39.762 
Epoch 84/1000 
	 loss: 117.4574, MinusLogProbMetric: 117.4574, val_loss: 117.0833, val_MinusLogProbMetric: 117.0833

Epoch 84: val_loss improved from 117.46403 to 117.08328, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 68s - loss: 117.4574 - MinusLogProbMetric: 117.4574 - val_loss: 117.0833 - val_MinusLogProbMetric: 117.0833 - lr: 1.2346e-05 - 68s/epoch - 346ms/step
Epoch 85/1000
2023-10-24 10:25:49.389 
Epoch 85/1000 
	 loss: 115.5492, MinusLogProbMetric: 115.5492, val_loss: 116.0092, val_MinusLogProbMetric: 116.0092

Epoch 85: val_loss improved from 117.08328 to 116.00921, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 70s - loss: 115.5492 - MinusLogProbMetric: 115.5492 - val_loss: 116.0092 - val_MinusLogProbMetric: 116.0092 - lr: 1.2346e-05 - 70s/epoch - 356ms/step
Epoch 86/1000
2023-10-24 10:26:58.593 
Epoch 86/1000 
	 loss: 115.2490, MinusLogProbMetric: 115.2490, val_loss: 114.0282, val_MinusLogProbMetric: 114.0282

Epoch 86: val_loss improved from 116.00921 to 114.02824, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 69s - loss: 115.2490 - MinusLogProbMetric: 115.2490 - val_loss: 114.0282 - val_MinusLogProbMetric: 114.0282 - lr: 1.2346e-05 - 69s/epoch - 352ms/step
Epoch 87/1000
2023-10-24 10:28:04.911 
Epoch 87/1000 
	 loss: 114.1626, MinusLogProbMetric: 114.1626, val_loss: 114.0776, val_MinusLogProbMetric: 114.0776

Epoch 87: val_loss did not improve from 114.02824
196/196 - 66s - loss: 114.1626 - MinusLogProbMetric: 114.1626 - val_loss: 114.0776 - val_MinusLogProbMetric: 114.0776 - lr: 1.2346e-05 - 66s/epoch - 334ms/step
Epoch 88/1000
2023-10-24 10:29:11.488 
Epoch 88/1000 
	 loss: 113.5337, MinusLogProbMetric: 113.5337, val_loss: 113.0400, val_MinusLogProbMetric: 113.0400

Epoch 88: val_loss improved from 114.02824 to 113.03997, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 68s - loss: 113.5337 - MinusLogProbMetric: 113.5337 - val_loss: 113.0400 - val_MinusLogProbMetric: 113.0400 - lr: 1.2346e-05 - 68s/epoch - 345ms/step
Epoch 89/1000
2023-10-24 10:30:20.101 
Epoch 89/1000 
	 loss: 112.8531, MinusLogProbMetric: 112.8531, val_loss: 114.2121, val_MinusLogProbMetric: 114.2121

Epoch 89: val_loss did not improve from 113.03997
196/196 - 68s - loss: 112.8531 - MinusLogProbMetric: 112.8531 - val_loss: 114.2121 - val_MinusLogProbMetric: 114.2121 - lr: 1.2346e-05 - 68s/epoch - 345ms/step
Epoch 90/1000
2023-10-24 10:31:26.296 
Epoch 90/1000 
	 loss: 112.9432, MinusLogProbMetric: 112.9432, val_loss: 114.2510, val_MinusLogProbMetric: 114.2510

Epoch 90: val_loss did not improve from 113.03997
196/196 - 66s - loss: 112.9432 - MinusLogProbMetric: 112.9432 - val_loss: 114.2510 - val_MinusLogProbMetric: 114.2510 - lr: 1.2346e-05 - 66s/epoch - 338ms/step
Epoch 91/1000
2023-10-24 10:32:33.260 
Epoch 91/1000 
	 loss: 111.3726, MinusLogProbMetric: 111.3726, val_loss: 110.9886, val_MinusLogProbMetric: 110.9886

Epoch 91: val_loss improved from 113.03997 to 110.98865, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 68s - loss: 111.3726 - MinusLogProbMetric: 111.3726 - val_loss: 110.9886 - val_MinusLogProbMetric: 110.9886 - lr: 1.2346e-05 - 68s/epoch - 346ms/step
Epoch 92/1000
2023-10-24 10:33:39.692 
Epoch 92/1000 
	 loss: 111.1497, MinusLogProbMetric: 111.1497, val_loss: 110.4270, val_MinusLogProbMetric: 110.4270

Epoch 92: val_loss improved from 110.98865 to 110.42699, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 66s - loss: 111.1497 - MinusLogProbMetric: 111.1497 - val_loss: 110.4270 - val_MinusLogProbMetric: 110.4270 - lr: 1.2346e-05 - 66s/epoch - 339ms/step
Epoch 93/1000
2023-10-24 10:34:45.883 
Epoch 93/1000 
	 loss: 110.2465, MinusLogProbMetric: 110.2465, val_loss: 111.5387, val_MinusLogProbMetric: 111.5387

Epoch 93: val_loss did not improve from 110.42699
196/196 - 65s - loss: 110.2465 - MinusLogProbMetric: 110.2465 - val_loss: 111.5387 - val_MinusLogProbMetric: 111.5387 - lr: 1.2346e-05 - 65s/epoch - 334ms/step
Epoch 94/1000
2023-10-24 10:35:54.699 
Epoch 94/1000 
	 loss: 109.3371, MinusLogProbMetric: 109.3371, val_loss: 111.8676, val_MinusLogProbMetric: 111.8676

Epoch 94: val_loss did not improve from 110.42699
196/196 - 69s - loss: 109.3371 - MinusLogProbMetric: 109.3371 - val_loss: 111.8676 - val_MinusLogProbMetric: 111.8676 - lr: 1.2346e-05 - 69s/epoch - 351ms/step
Epoch 95/1000
2023-10-24 10:37:03.808 
Epoch 95/1000 
	 loss: 109.0295, MinusLogProbMetric: 109.0295, val_loss: 108.3807, val_MinusLogProbMetric: 108.3807

Epoch 95: val_loss improved from 110.42699 to 108.38068, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 70s - loss: 109.0295 - MinusLogProbMetric: 109.0295 - val_loss: 108.3807 - val_MinusLogProbMetric: 108.3807 - lr: 1.2346e-05 - 70s/epoch - 358ms/step
Epoch 96/1000
2023-10-24 10:38:10.727 
Epoch 96/1000 
	 loss: 108.2968, MinusLogProbMetric: 108.2968, val_loss: 108.6993, val_MinusLogProbMetric: 108.6993

Epoch 96: val_loss did not improve from 108.38068
196/196 - 66s - loss: 108.2968 - MinusLogProbMetric: 108.2968 - val_loss: 108.6993 - val_MinusLogProbMetric: 108.6993 - lr: 1.2346e-05 - 66s/epoch - 336ms/step
Epoch 97/1000
2023-10-24 10:39:16.462 
Epoch 97/1000 
	 loss: 107.9580, MinusLogProbMetric: 107.9580, val_loss: 107.9916, val_MinusLogProbMetric: 107.9916

Epoch 97: val_loss improved from 108.38068 to 107.99161, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 67s - loss: 107.9580 - MinusLogProbMetric: 107.9580 - val_loss: 107.9916 - val_MinusLogProbMetric: 107.9916 - lr: 1.2346e-05 - 67s/epoch - 340ms/step
Epoch 98/1000
2023-10-24 10:40:20.171 
Epoch 98/1000 
	 loss: 106.7917, MinusLogProbMetric: 106.7917, val_loss: 106.5512, val_MinusLogProbMetric: 106.5512

Epoch 98: val_loss improved from 107.99161 to 106.55119, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 64s - loss: 106.7917 - MinusLogProbMetric: 106.7917 - val_loss: 106.5512 - val_MinusLogProbMetric: 106.5512 - lr: 1.2346e-05 - 64s/epoch - 326ms/step
Epoch 99/1000
2023-10-24 10:41:31.327 
Epoch 99/1000 
	 loss: 106.1988, MinusLogProbMetric: 106.1988, val_loss: 109.4825, val_MinusLogProbMetric: 109.4825

Epoch 99: val_loss did not improve from 106.55119
196/196 - 70s - loss: 106.1988 - MinusLogProbMetric: 106.1988 - val_loss: 109.4825 - val_MinusLogProbMetric: 109.4825 - lr: 1.2346e-05 - 70s/epoch - 357ms/step
Epoch 100/1000
2023-10-24 10:42:37.659 
Epoch 100/1000 
	 loss: 106.2075, MinusLogProbMetric: 106.2075, val_loss: 106.8807, val_MinusLogProbMetric: 106.8807

Epoch 100: val_loss did not improve from 106.55119
196/196 - 66s - loss: 106.2075 - MinusLogProbMetric: 106.2075 - val_loss: 106.8807 - val_MinusLogProbMetric: 106.8807 - lr: 1.2346e-05 - 66s/epoch - 338ms/step
Epoch 101/1000
2023-10-24 10:43:43.323 
Epoch 101/1000 
	 loss: 105.0991, MinusLogProbMetric: 105.0991, val_loss: 104.6133, val_MinusLogProbMetric: 104.6133

Epoch 101: val_loss improved from 106.55119 to 104.61334, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 67s - loss: 105.0991 - MinusLogProbMetric: 105.0991 - val_loss: 104.6133 - val_MinusLogProbMetric: 104.6133 - lr: 1.2346e-05 - 67s/epoch - 340ms/step
Epoch 102/1000
2023-10-24 10:44:51.011 
Epoch 102/1000 
	 loss: 104.8537, MinusLogProbMetric: 104.8537, val_loss: 104.0002, val_MinusLogProbMetric: 104.0002

Epoch 102: val_loss improved from 104.61334 to 104.00017, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 68s - loss: 104.8537 - MinusLogProbMetric: 104.8537 - val_loss: 104.0002 - val_MinusLogProbMetric: 104.0002 - lr: 1.2346e-05 - 68s/epoch - 345ms/step
Epoch 103/1000
2023-10-24 10:46:00.443 
Epoch 103/1000 
	 loss: 104.3955, MinusLogProbMetric: 104.3955, val_loss: 105.5632, val_MinusLogProbMetric: 105.5632

Epoch 103: val_loss did not improve from 104.00017
196/196 - 68s - loss: 104.3955 - MinusLogProbMetric: 104.3955 - val_loss: 105.5632 - val_MinusLogProbMetric: 105.5632 - lr: 1.2346e-05 - 68s/epoch - 349ms/step
Epoch 104/1000
2023-10-24 10:47:08.063 
Epoch 104/1000 
	 loss: 103.6626, MinusLogProbMetric: 103.6626, val_loss: 107.3600, val_MinusLogProbMetric: 107.3600

Epoch 104: val_loss did not improve from 104.00017
196/196 - 68s - loss: 103.6626 - MinusLogProbMetric: 103.6626 - val_loss: 107.3600 - val_MinusLogProbMetric: 107.3600 - lr: 1.2346e-05 - 68s/epoch - 345ms/step
Epoch 105/1000
2023-10-24 10:48:12.938 
Epoch 105/1000 
	 loss: 105.6819, MinusLogProbMetric: 105.6819, val_loss: 103.8399, val_MinusLogProbMetric: 103.8399

Epoch 105: val_loss improved from 104.00017 to 103.83986, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 66s - loss: 105.6819 - MinusLogProbMetric: 105.6819 - val_loss: 103.8399 - val_MinusLogProbMetric: 103.8399 - lr: 1.2346e-05 - 66s/epoch - 336ms/step
Epoch 106/1000
2023-10-24 10:49:21.459 
Epoch 106/1000 
	 loss: 102.6848, MinusLogProbMetric: 102.6848, val_loss: 102.0422, val_MinusLogProbMetric: 102.0422

Epoch 106: val_loss improved from 103.83986 to 102.04216, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 69s - loss: 102.6848 - MinusLogProbMetric: 102.6848 - val_loss: 102.0422 - val_MinusLogProbMetric: 102.0422 - lr: 1.2346e-05 - 69s/epoch - 350ms/step
Epoch 107/1000
2023-10-24 10:50:29.835 
Epoch 107/1000 
	 loss: 102.4922, MinusLogProbMetric: 102.4922, val_loss: 103.8825, val_MinusLogProbMetric: 103.8825

Epoch 107: val_loss did not improve from 102.04216
196/196 - 67s - loss: 102.4922 - MinusLogProbMetric: 102.4922 - val_loss: 103.8825 - val_MinusLogProbMetric: 103.8825 - lr: 1.2346e-05 - 67s/epoch - 343ms/step
Epoch 108/1000
2023-10-24 10:51:36.736 
Epoch 108/1000 
	 loss: 102.2692, MinusLogProbMetric: 102.2692, val_loss: 102.9446, val_MinusLogProbMetric: 102.9446

Epoch 108: val_loss did not improve from 102.04216
196/196 - 67s - loss: 102.2692 - MinusLogProbMetric: 102.2692 - val_loss: 102.9446 - val_MinusLogProbMetric: 102.9446 - lr: 1.2346e-05 - 67s/epoch - 341ms/step
Epoch 109/1000
2023-10-24 10:52:44.416 
Epoch 109/1000 
	 loss: 101.3686, MinusLogProbMetric: 101.3686, val_loss: 101.6002, val_MinusLogProbMetric: 101.6002

Epoch 109: val_loss improved from 102.04216 to 101.60023, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 69s - loss: 101.3686 - MinusLogProbMetric: 101.3686 - val_loss: 101.6002 - val_MinusLogProbMetric: 101.6002 - lr: 1.2346e-05 - 69s/epoch - 350ms/step
Epoch 110/1000
2023-10-24 10:53:54.206 
Epoch 110/1000 
	 loss: 101.3024, MinusLogProbMetric: 101.3024, val_loss: 101.1268, val_MinusLogProbMetric: 101.1268

Epoch 110: val_loss improved from 101.60023 to 101.12677, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 70s - loss: 101.3024 - MinusLogProbMetric: 101.3024 - val_loss: 101.1268 - val_MinusLogProbMetric: 101.1268 - lr: 1.2346e-05 - 70s/epoch - 357ms/step
Epoch 111/1000
2023-10-24 10:55:02.593 
Epoch 111/1000 
	 loss: 100.1537, MinusLogProbMetric: 100.1537, val_loss: 100.2556, val_MinusLogProbMetric: 100.2556

Epoch 111: val_loss improved from 101.12677 to 100.25561, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 68s - loss: 100.1537 - MinusLogProbMetric: 100.1537 - val_loss: 100.2556 - val_MinusLogProbMetric: 100.2556 - lr: 1.2346e-05 - 68s/epoch - 347ms/step
Epoch 112/1000
2023-10-24 10:56:12.175 
Epoch 112/1000 
	 loss: 99.8277, MinusLogProbMetric: 99.8277, val_loss: 99.3969, val_MinusLogProbMetric: 99.3969

Epoch 112: val_loss improved from 100.25561 to 99.39693, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 70s - loss: 99.8277 - MinusLogProbMetric: 99.8277 - val_loss: 99.3969 - val_MinusLogProbMetric: 99.3969 - lr: 1.2346e-05 - 70s/epoch - 356ms/step
Epoch 113/1000
2023-10-24 10:57:19.210 
Epoch 113/1000 
	 loss: 99.3796, MinusLogProbMetric: 99.3796, val_loss: 100.0322, val_MinusLogProbMetric: 100.0322

Epoch 113: val_loss did not improve from 99.39693
196/196 - 66s - loss: 99.3796 - MinusLogProbMetric: 99.3796 - val_loss: 100.0322 - val_MinusLogProbMetric: 100.0322 - lr: 1.2346e-05 - 66s/epoch - 337ms/step
Epoch 114/1000
2023-10-24 10:58:24.906 
Epoch 114/1000 
	 loss: 98.9237, MinusLogProbMetric: 98.9237, val_loss: 98.5530, val_MinusLogProbMetric: 98.5530

Epoch 114: val_loss improved from 99.39693 to 98.55299, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 67s - loss: 98.9237 - MinusLogProbMetric: 98.9237 - val_loss: 98.5530 - val_MinusLogProbMetric: 98.5530 - lr: 1.2346e-05 - 67s/epoch - 341ms/step
Epoch 115/1000
2023-10-24 10:59:34.322 
Epoch 115/1000 
	 loss: 98.9714, MinusLogProbMetric: 98.9714, val_loss: 98.7273, val_MinusLogProbMetric: 98.7273

Epoch 115: val_loss did not improve from 98.55299
196/196 - 68s - loss: 98.9714 - MinusLogProbMetric: 98.9714 - val_loss: 98.7273 - val_MinusLogProbMetric: 98.7273 - lr: 1.2346e-05 - 68s/epoch - 349ms/step
Epoch 116/1000
2023-10-24 11:00:41.215 
Epoch 116/1000 
	 loss: 98.1367, MinusLogProbMetric: 98.1367, val_loss: 97.5629, val_MinusLogProbMetric: 97.5629

Epoch 116: val_loss improved from 98.55299 to 97.56287, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 68s - loss: 98.1367 - MinusLogProbMetric: 98.1367 - val_loss: 97.5629 - val_MinusLogProbMetric: 97.5629 - lr: 1.2346e-05 - 68s/epoch - 346ms/step
Epoch 117/1000
2023-10-24 11:01:49.085 
Epoch 117/1000 
	 loss: 98.3239, MinusLogProbMetric: 98.3239, val_loss: 97.5745, val_MinusLogProbMetric: 97.5745

Epoch 117: val_loss did not improve from 97.56287
196/196 - 67s - loss: 98.3239 - MinusLogProbMetric: 98.3239 - val_loss: 97.5745 - val_MinusLogProbMetric: 97.5745 - lr: 1.2346e-05 - 67s/epoch - 341ms/step
Epoch 118/1000
2023-10-24 11:02:57.757 
Epoch 118/1000 
	 loss: 97.7874, MinusLogProbMetric: 97.7874, val_loss: 97.2314, val_MinusLogProbMetric: 97.2314

Epoch 118: val_loss improved from 97.56287 to 97.23138, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 70s - loss: 97.7874 - MinusLogProbMetric: 97.7874 - val_loss: 97.2314 - val_MinusLogProbMetric: 97.2314 - lr: 1.2346e-05 - 70s/epoch - 355ms/step
Epoch 119/1000
2023-10-24 11:04:07.886 
Epoch 119/1000 
	 loss: 97.4375, MinusLogProbMetric: 97.4375, val_loss: 97.0486, val_MinusLogProbMetric: 97.0486

Epoch 119: val_loss improved from 97.23138 to 97.04858, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 70s - loss: 97.4375 - MinusLogProbMetric: 97.4375 - val_loss: 97.0486 - val_MinusLogProbMetric: 97.0486 - lr: 1.2346e-05 - 70s/epoch - 358ms/step
Epoch 120/1000
2023-10-24 11:05:15.953 
Epoch 120/1000 
	 loss: 96.3865, MinusLogProbMetric: 96.3865, val_loss: 96.5799, val_MinusLogProbMetric: 96.5799

Epoch 120: val_loss improved from 97.04858 to 96.57987, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 68s - loss: 96.3865 - MinusLogProbMetric: 96.3865 - val_loss: 96.5799 - val_MinusLogProbMetric: 96.5799 - lr: 1.2346e-05 - 68s/epoch - 348ms/step
Epoch 121/1000
2023-10-24 11:06:24.517 
Epoch 121/1000 
	 loss: 99.6172, MinusLogProbMetric: 99.6172, val_loss: 97.4157, val_MinusLogProbMetric: 97.4157

Epoch 121: val_loss did not improve from 96.57987
196/196 - 67s - loss: 99.6172 - MinusLogProbMetric: 99.6172 - val_loss: 97.4157 - val_MinusLogProbMetric: 97.4157 - lr: 1.2346e-05 - 67s/epoch - 344ms/step
Epoch 122/1000
2023-10-24 11:07:31.116 
Epoch 122/1000 
	 loss: 96.5344, MinusLogProbMetric: 96.5344, val_loss: 95.9119, val_MinusLogProbMetric: 95.9119

Epoch 122: val_loss improved from 96.57987 to 95.91194, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 67s - loss: 96.5344 - MinusLogProbMetric: 96.5344 - val_loss: 95.9119 - val_MinusLogProbMetric: 95.9119 - lr: 1.2346e-05 - 67s/epoch - 344ms/step
Epoch 123/1000
2023-10-24 11:08:39.491 
Epoch 123/1000 
	 loss: 95.9909, MinusLogProbMetric: 95.9909, val_loss: 95.8542, val_MinusLogProbMetric: 95.8542

Epoch 123: val_loss improved from 95.91194 to 95.85416, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 68s - loss: 95.9909 - MinusLogProbMetric: 95.9909 - val_loss: 95.8542 - val_MinusLogProbMetric: 95.8542 - lr: 1.2346e-05 - 68s/epoch - 349ms/step
Epoch 124/1000
2023-10-24 11:09:46.987 
Epoch 124/1000 
	 loss: 95.3128, MinusLogProbMetric: 95.3128, val_loss: 95.9770, val_MinusLogProbMetric: 95.9770

Epoch 124: val_loss did not improve from 95.85416
196/196 - 66s - loss: 95.3128 - MinusLogProbMetric: 95.3128 - val_loss: 95.9770 - val_MinusLogProbMetric: 95.9770 - lr: 1.2346e-05 - 66s/epoch - 339ms/step
Epoch 125/1000
2023-10-24 11:10:49.700 
Epoch 125/1000 
	 loss: 95.1477, MinusLogProbMetric: 95.1477, val_loss: 97.0891, val_MinusLogProbMetric: 97.0891

Epoch 125: val_loss did not improve from 95.85416
196/196 - 63s - loss: 95.1477 - MinusLogProbMetric: 95.1477 - val_loss: 97.0891 - val_MinusLogProbMetric: 97.0891 - lr: 1.2346e-05 - 63s/epoch - 320ms/step
Epoch 126/1000
2023-10-24 11:11:54.157 
Epoch 126/1000 
	 loss: 96.4109, MinusLogProbMetric: 96.4109, val_loss: 96.0848, val_MinusLogProbMetric: 96.0848

Epoch 126: val_loss did not improve from 95.85416
196/196 - 64s - loss: 96.4109 - MinusLogProbMetric: 96.4109 - val_loss: 96.0848 - val_MinusLogProbMetric: 96.0848 - lr: 1.2346e-05 - 64s/epoch - 329ms/step
Epoch 127/1000
2023-10-24 11:13:00.424 
Epoch 127/1000 
	 loss: 94.6306, MinusLogProbMetric: 94.6306, val_loss: 95.0185, val_MinusLogProbMetric: 95.0185

Epoch 127: val_loss improved from 95.85416 to 95.01854, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 67s - loss: 94.6306 - MinusLogProbMetric: 94.6306 - val_loss: 95.0185 - val_MinusLogProbMetric: 95.0185 - lr: 1.2346e-05 - 67s/epoch - 343ms/step
Epoch 128/1000
2023-10-24 11:14:06.724 
Epoch 128/1000 
	 loss: 93.6550, MinusLogProbMetric: 93.6550, val_loss: 93.3786, val_MinusLogProbMetric: 93.3786

Epoch 128: val_loss improved from 95.01854 to 93.37856, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 66s - loss: 93.6550 - MinusLogProbMetric: 93.6550 - val_loss: 93.3786 - val_MinusLogProbMetric: 93.3786 - lr: 1.2346e-05 - 66s/epoch - 337ms/step
Epoch 129/1000
2023-10-24 11:15:14.963 
Epoch 129/1000 
	 loss: 95.1258, MinusLogProbMetric: 95.1258, val_loss: 93.1283, val_MinusLogProbMetric: 93.1283

Epoch 129: val_loss improved from 93.37856 to 93.12827, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 69s - loss: 95.1258 - MinusLogProbMetric: 95.1258 - val_loss: 93.1283 - val_MinusLogProbMetric: 93.1283 - lr: 1.2346e-05 - 69s/epoch - 350ms/step
Epoch 130/1000
2023-10-24 11:16:22.740 
Epoch 130/1000 
	 loss: 92.4986, MinusLogProbMetric: 92.4986, val_loss: 92.6947, val_MinusLogProbMetric: 92.6947

Epoch 130: val_loss improved from 93.12827 to 92.69466, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 68s - loss: 92.4986 - MinusLogProbMetric: 92.4986 - val_loss: 92.6947 - val_MinusLogProbMetric: 92.6947 - lr: 1.2346e-05 - 68s/epoch - 346ms/step
Epoch 131/1000
2023-10-24 11:17:28.505 
Epoch 131/1000 
	 loss: 92.5443, MinusLogProbMetric: 92.5443, val_loss: 92.0764, val_MinusLogProbMetric: 92.0764

Epoch 131: val_loss improved from 92.69466 to 92.07639, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 66s - loss: 92.5443 - MinusLogProbMetric: 92.5443 - val_loss: 92.0764 - val_MinusLogProbMetric: 92.0764 - lr: 1.2346e-05 - 66s/epoch - 335ms/step
Epoch 132/1000
2023-10-24 11:18:33.480 
Epoch 132/1000 
	 loss: 91.8884, MinusLogProbMetric: 91.8884, val_loss: 92.2593, val_MinusLogProbMetric: 92.2593

Epoch 132: val_loss did not improve from 92.07639
196/196 - 64s - loss: 91.8884 - MinusLogProbMetric: 91.8884 - val_loss: 92.2593 - val_MinusLogProbMetric: 92.2593 - lr: 1.2346e-05 - 64s/epoch - 327ms/step
Epoch 133/1000
2023-10-24 11:19:39.392 
Epoch 133/1000 
	 loss: 91.6366, MinusLogProbMetric: 91.6366, val_loss: 90.9764, val_MinusLogProbMetric: 90.9764

Epoch 133: val_loss improved from 92.07639 to 90.97643, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 67s - loss: 91.6366 - MinusLogProbMetric: 91.6366 - val_loss: 90.9764 - val_MinusLogProbMetric: 90.9764 - lr: 1.2346e-05 - 67s/epoch - 342ms/step
Epoch 134/1000
2023-10-24 11:20:48.161 
Epoch 134/1000 
	 loss: 91.9098, MinusLogProbMetric: 91.9098, val_loss: 91.8647, val_MinusLogProbMetric: 91.8647

Epoch 134: val_loss did not improve from 90.97643
196/196 - 68s - loss: 91.9098 - MinusLogProbMetric: 91.9098 - val_loss: 91.8647 - val_MinusLogProbMetric: 91.8647 - lr: 1.2346e-05 - 68s/epoch - 345ms/step
Epoch 135/1000
2023-10-24 11:21:55.039 
Epoch 135/1000 
	 loss: 90.8028, MinusLogProbMetric: 90.8028, val_loss: 92.7875, val_MinusLogProbMetric: 92.7875

Epoch 135: val_loss did not improve from 90.97643
196/196 - 67s - loss: 90.8028 - MinusLogProbMetric: 90.8028 - val_loss: 92.7875 - val_MinusLogProbMetric: 92.7875 - lr: 1.2346e-05 - 67s/epoch - 341ms/step
Epoch 136/1000
2023-10-24 11:23:01.822 
Epoch 136/1000 
	 loss: 90.4684, MinusLogProbMetric: 90.4684, val_loss: 97.6602, val_MinusLogProbMetric: 97.6602

Epoch 136: val_loss did not improve from 90.97643
196/196 - 67s - loss: 90.4684 - MinusLogProbMetric: 90.4684 - val_loss: 97.6602 - val_MinusLogProbMetric: 97.6602 - lr: 1.2346e-05 - 67s/epoch - 341ms/step
Epoch 137/1000
2023-10-24 11:24:08.372 
Epoch 137/1000 
	 loss: 90.0034, MinusLogProbMetric: 90.0034, val_loss: 89.6695, val_MinusLogProbMetric: 89.6695

Epoch 137: val_loss improved from 90.97643 to 89.66955, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 68s - loss: 90.0034 - MinusLogProbMetric: 90.0034 - val_loss: 89.6695 - val_MinusLogProbMetric: 89.6695 - lr: 1.2346e-05 - 68s/epoch - 345ms/step
Epoch 138/1000
2023-10-24 11:25:16.202 
Epoch 138/1000 
	 loss: 89.2204, MinusLogProbMetric: 89.2204, val_loss: 88.9746, val_MinusLogProbMetric: 88.9746

Epoch 138: val_loss improved from 89.66955 to 88.97461, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 68s - loss: 89.2204 - MinusLogProbMetric: 89.2204 - val_loss: 88.9746 - val_MinusLogProbMetric: 88.9746 - lr: 1.2346e-05 - 68s/epoch - 346ms/step
Epoch 139/1000
2023-10-24 11:26:24.462 
Epoch 139/1000 
	 loss: 88.9420, MinusLogProbMetric: 88.9420, val_loss: 89.8493, val_MinusLogProbMetric: 89.8493

Epoch 139: val_loss did not improve from 88.97461
196/196 - 67s - loss: 88.9420 - MinusLogProbMetric: 88.9420 - val_loss: 89.8493 - val_MinusLogProbMetric: 89.8493 - lr: 1.2346e-05 - 67s/epoch - 343ms/step
Epoch 140/1000
2023-10-24 11:27:31.400 
Epoch 140/1000 
	 loss: 88.3315, MinusLogProbMetric: 88.3315, val_loss: 87.9815, val_MinusLogProbMetric: 87.9815

Epoch 140: val_loss improved from 88.97461 to 87.98148, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 68s - loss: 88.3315 - MinusLogProbMetric: 88.3315 - val_loss: 87.9815 - val_MinusLogProbMetric: 87.9815 - lr: 1.2346e-05 - 68s/epoch - 346ms/step
Epoch 141/1000
2023-10-24 11:28:36.975 
Epoch 141/1000 
	 loss: 88.2582, MinusLogProbMetric: 88.2582, val_loss: 88.8099, val_MinusLogProbMetric: 88.8099

Epoch 141: val_loss did not improve from 87.98148
196/196 - 65s - loss: 88.2582 - MinusLogProbMetric: 88.2582 - val_loss: 88.8099 - val_MinusLogProbMetric: 88.8099 - lr: 1.2346e-05 - 65s/epoch - 330ms/step
Epoch 142/1000
2023-10-24 11:29:43.601 
Epoch 142/1000 
	 loss: 87.9873, MinusLogProbMetric: 87.9873, val_loss: 89.0974, val_MinusLogProbMetric: 89.0974

Epoch 142: val_loss did not improve from 87.98148
196/196 - 67s - loss: 87.9873 - MinusLogProbMetric: 87.9873 - val_loss: 89.0974 - val_MinusLogProbMetric: 89.0974 - lr: 1.2346e-05 - 67s/epoch - 340ms/step
Epoch 143/1000
2023-10-24 11:30:51.931 
Epoch 143/1000 
	 loss: 88.3787, MinusLogProbMetric: 88.3787, val_loss: 87.8532, val_MinusLogProbMetric: 87.8532

Epoch 143: val_loss improved from 87.98148 to 87.85325, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 69s - loss: 88.3787 - MinusLogProbMetric: 88.3787 - val_loss: 87.8532 - val_MinusLogProbMetric: 87.8532 - lr: 1.2346e-05 - 69s/epoch - 354ms/step
Epoch 144/1000
2023-10-24 11:31:58.154 
Epoch 144/1000 
	 loss: 87.2165, MinusLogProbMetric: 87.2165, val_loss: 87.1903, val_MinusLogProbMetric: 87.1903

Epoch 144: val_loss improved from 87.85325 to 87.19026, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 66s - loss: 87.2165 - MinusLogProbMetric: 87.2165 - val_loss: 87.1903 - val_MinusLogProbMetric: 87.1903 - lr: 1.2346e-05 - 66s/epoch - 338ms/step
Epoch 145/1000
2023-10-24 11:33:05.111 
Epoch 145/1000 
	 loss: 87.4578, MinusLogProbMetric: 87.4578, val_loss: 86.8992, val_MinusLogProbMetric: 86.8992

Epoch 145: val_loss improved from 87.19026 to 86.89922, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 67s - loss: 87.4578 - MinusLogProbMetric: 87.4578 - val_loss: 86.8992 - val_MinusLogProbMetric: 86.8992 - lr: 1.2346e-05 - 67s/epoch - 341ms/step
Epoch 146/1000
2023-10-24 11:34:10.681 
Epoch 146/1000 
	 loss: 87.0263, MinusLogProbMetric: 87.0263, val_loss: 92.7706, val_MinusLogProbMetric: 92.7706

Epoch 146: val_loss did not improve from 86.89922
196/196 - 65s - loss: 87.0263 - MinusLogProbMetric: 87.0263 - val_loss: 92.7706 - val_MinusLogProbMetric: 92.7706 - lr: 1.2346e-05 - 65s/epoch - 330ms/step
Epoch 147/1000
2023-10-24 11:35:14.256 
Epoch 147/1000 
	 loss: 86.4800, MinusLogProbMetric: 86.4800, val_loss: 86.8088, val_MinusLogProbMetric: 86.8088

Epoch 147: val_loss improved from 86.89922 to 86.80877, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 65s - loss: 86.4800 - MinusLogProbMetric: 86.4800 - val_loss: 86.8088 - val_MinusLogProbMetric: 86.8088 - lr: 1.2346e-05 - 65s/epoch - 330ms/step
Epoch 148/1000
2023-10-24 11:36:20.893 
Epoch 148/1000 
	 loss: 85.7097, MinusLogProbMetric: 85.7097, val_loss: 87.4568, val_MinusLogProbMetric: 87.4568

Epoch 148: val_loss did not improve from 86.80877
196/196 - 66s - loss: 85.7097 - MinusLogProbMetric: 85.7097 - val_loss: 87.4568 - val_MinusLogProbMetric: 87.4568 - lr: 1.2346e-05 - 66s/epoch - 335ms/step
Epoch 149/1000
2023-10-24 11:37:25.002 
Epoch 149/1000 
	 loss: 86.0530, MinusLogProbMetric: 86.0530, val_loss: 89.6350, val_MinusLogProbMetric: 89.6350

Epoch 149: val_loss did not improve from 86.80877
196/196 - 64s - loss: 86.0530 - MinusLogProbMetric: 86.0530 - val_loss: 89.6350 - val_MinusLogProbMetric: 89.6350 - lr: 1.2346e-05 - 64s/epoch - 327ms/step
Epoch 150/1000
2023-10-24 11:38:32.563 
Epoch 150/1000 
	 loss: 85.4862, MinusLogProbMetric: 85.4862, val_loss: 85.5593, val_MinusLogProbMetric: 85.5593

Epoch 150: val_loss improved from 86.80877 to 85.55934, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 69s - loss: 85.4862 - MinusLogProbMetric: 85.4862 - val_loss: 85.5593 - val_MinusLogProbMetric: 85.5593 - lr: 1.2346e-05 - 69s/epoch - 350ms/step
Epoch 151/1000
2023-10-24 11:39:40.416 
Epoch 151/1000 
	 loss: 85.3683, MinusLogProbMetric: 85.3683, val_loss: 85.6713, val_MinusLogProbMetric: 85.6713

Epoch 151: val_loss did not improve from 85.55934
196/196 - 67s - loss: 85.3683 - MinusLogProbMetric: 85.3683 - val_loss: 85.6713 - val_MinusLogProbMetric: 85.6713 - lr: 1.2346e-05 - 67s/epoch - 341ms/step
Epoch 152/1000
2023-10-24 11:40:46.052 
Epoch 152/1000 
	 loss: 84.5946, MinusLogProbMetric: 84.5946, val_loss: 84.7580, val_MinusLogProbMetric: 84.7580

Epoch 152: val_loss improved from 85.55934 to 84.75796, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 67s - loss: 84.5946 - MinusLogProbMetric: 84.5946 - val_loss: 84.7580 - val_MinusLogProbMetric: 84.7580 - lr: 1.2346e-05 - 67s/epoch - 340ms/step
Epoch 153/1000
2023-10-24 11:41:54.380 
Epoch 153/1000 
	 loss: 84.6561, MinusLogProbMetric: 84.6561, val_loss: 85.5488, val_MinusLogProbMetric: 85.5488

Epoch 153: val_loss did not improve from 84.75796
196/196 - 67s - loss: 84.6561 - MinusLogProbMetric: 84.6561 - val_loss: 85.5488 - val_MinusLogProbMetric: 85.5488 - lr: 1.2346e-05 - 67s/epoch - 343ms/step
Epoch 154/1000
2023-10-24 11:42:58.341 
Epoch 154/1000 
	 loss: 84.2576, MinusLogProbMetric: 84.2576, val_loss: 84.4030, val_MinusLogProbMetric: 84.4030

Epoch 154: val_loss improved from 84.75796 to 84.40303, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 65s - loss: 84.2576 - MinusLogProbMetric: 84.2576 - val_loss: 84.4030 - val_MinusLogProbMetric: 84.4030 - lr: 1.2346e-05 - 65s/epoch - 331ms/step
Epoch 155/1000
2023-10-24 11:44:07.048 
Epoch 155/1000 
	 loss: 84.0895, MinusLogProbMetric: 84.0895, val_loss: 84.8177, val_MinusLogProbMetric: 84.8177

Epoch 155: val_loss did not improve from 84.40303
196/196 - 68s - loss: 84.0895 - MinusLogProbMetric: 84.0895 - val_loss: 84.8177 - val_MinusLogProbMetric: 84.8177 - lr: 1.2346e-05 - 68s/epoch - 346ms/step
Epoch 156/1000
2023-10-24 11:45:14.035 
Epoch 156/1000 
	 loss: 83.8433, MinusLogProbMetric: 83.8433, val_loss: 84.5663, val_MinusLogProbMetric: 84.5663

Epoch 156: val_loss did not improve from 84.40303
196/196 - 67s - loss: 83.8433 - MinusLogProbMetric: 83.8433 - val_loss: 84.5663 - val_MinusLogProbMetric: 84.5663 - lr: 1.2346e-05 - 67s/epoch - 342ms/step
Epoch 157/1000
2023-10-24 11:46:20.961 
Epoch 157/1000 
	 loss: 85.2211, MinusLogProbMetric: 85.2211, val_loss: 83.3738, val_MinusLogProbMetric: 83.3738

Epoch 157: val_loss improved from 84.40303 to 83.37381, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 68s - loss: 85.2211 - MinusLogProbMetric: 85.2211 - val_loss: 83.3738 - val_MinusLogProbMetric: 83.3738 - lr: 1.2346e-05 - 68s/epoch - 346ms/step
Epoch 158/1000
2023-10-24 11:47:28.848 
Epoch 158/1000 
	 loss: 82.8133, MinusLogProbMetric: 82.8133, val_loss: 82.6901, val_MinusLogProbMetric: 82.6901

Epoch 158: val_loss improved from 83.37381 to 82.69010, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 68s - loss: 82.8133 - MinusLogProbMetric: 82.8133 - val_loss: 82.6901 - val_MinusLogProbMetric: 82.6901 - lr: 1.2346e-05 - 68s/epoch - 347ms/step
Epoch 159/1000
2023-10-24 11:48:36.627 
Epoch 159/1000 
	 loss: 83.4826, MinusLogProbMetric: 83.4826, val_loss: 82.4561, val_MinusLogProbMetric: 82.4561

Epoch 159: val_loss improved from 82.69010 to 82.45608, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 68s - loss: 83.4826 - MinusLogProbMetric: 83.4826 - val_loss: 82.4561 - val_MinusLogProbMetric: 82.4561 - lr: 1.2346e-05 - 68s/epoch - 346ms/step
Epoch 160/1000
2023-10-24 11:49:43.628 
Epoch 160/1000 
	 loss: 82.3855, MinusLogProbMetric: 82.3855, val_loss: 83.6602, val_MinusLogProbMetric: 83.6602

Epoch 160: val_loss did not improve from 82.45608
196/196 - 66s - loss: 82.3855 - MinusLogProbMetric: 82.3855 - val_loss: 83.6602 - val_MinusLogProbMetric: 83.6602 - lr: 1.2346e-05 - 66s/epoch - 337ms/step
Epoch 161/1000
2023-10-24 11:50:50.699 
Epoch 161/1000 
	 loss: 84.4096, MinusLogProbMetric: 84.4096, val_loss: 83.1003, val_MinusLogProbMetric: 83.1003

Epoch 161: val_loss did not improve from 82.45608
196/196 - 67s - loss: 84.4096 - MinusLogProbMetric: 84.4096 - val_loss: 83.1003 - val_MinusLogProbMetric: 83.1003 - lr: 1.2346e-05 - 67s/epoch - 342ms/step
Epoch 162/1000
2023-10-24 11:51:58.239 
Epoch 162/1000 
	 loss: 82.7737, MinusLogProbMetric: 82.7737, val_loss: 83.3452, val_MinusLogProbMetric: 83.3452

Epoch 162: val_loss did not improve from 82.45608
196/196 - 68s - loss: 82.7737 - MinusLogProbMetric: 82.7737 - val_loss: 83.3452 - val_MinusLogProbMetric: 83.3452 - lr: 1.2346e-05 - 68s/epoch - 345ms/step
Epoch 163/1000
2023-10-24 11:53:06.044 
Epoch 163/1000 
	 loss: 82.6248, MinusLogProbMetric: 82.6248, val_loss: 82.8093, val_MinusLogProbMetric: 82.8093

Epoch 163: val_loss did not improve from 82.45608
196/196 - 68s - loss: 82.6248 - MinusLogProbMetric: 82.6248 - val_loss: 82.8093 - val_MinusLogProbMetric: 82.8093 - lr: 1.2346e-05 - 68s/epoch - 346ms/step
Epoch 164/1000
2023-10-24 11:54:13.894 
Epoch 164/1000 
	 loss: 82.5997, MinusLogProbMetric: 82.5997, val_loss: 82.6649, val_MinusLogProbMetric: 82.6649

Epoch 164: val_loss did not improve from 82.45608
196/196 - 68s - loss: 82.5997 - MinusLogProbMetric: 82.5997 - val_loss: 82.6649 - val_MinusLogProbMetric: 82.6649 - lr: 1.2346e-05 - 68s/epoch - 346ms/step
Epoch 165/1000
2023-10-24 11:55:21.551 
Epoch 165/1000 
	 loss: 93.1542, MinusLogProbMetric: 93.1542, val_loss: 97.4919, val_MinusLogProbMetric: 97.4919

Epoch 165: val_loss did not improve from 82.45608
196/196 - 68s - loss: 93.1542 - MinusLogProbMetric: 93.1542 - val_loss: 97.4919 - val_MinusLogProbMetric: 97.4919 - lr: 1.2346e-05 - 68s/epoch - 345ms/step
Epoch 166/1000
2023-10-24 11:56:28.135 
Epoch 166/1000 
	 loss: 91.8692, MinusLogProbMetric: 91.8692, val_loss: 88.6170, val_MinusLogProbMetric: 88.6170

Epoch 166: val_loss did not improve from 82.45608
196/196 - 67s - loss: 91.8692 - MinusLogProbMetric: 91.8692 - val_loss: 88.6170 - val_MinusLogProbMetric: 88.6170 - lr: 1.2346e-05 - 67s/epoch - 340ms/step
Epoch 167/1000
2023-10-24 11:57:33.513 
Epoch 167/1000 
	 loss: 87.9074, MinusLogProbMetric: 87.9074, val_loss: 86.2504, val_MinusLogProbMetric: 86.2504

Epoch 167: val_loss did not improve from 82.45608
196/196 - 65s - loss: 87.9074 - MinusLogProbMetric: 87.9074 - val_loss: 86.2504 - val_MinusLogProbMetric: 86.2504 - lr: 1.2346e-05 - 65s/epoch - 334ms/step
Epoch 168/1000
2023-10-24 11:58:38.830 
Epoch 168/1000 
	 loss: 85.5183, MinusLogProbMetric: 85.5183, val_loss: 86.5850, val_MinusLogProbMetric: 86.5850

Epoch 168: val_loss did not improve from 82.45608
196/196 - 65s - loss: 85.5183 - MinusLogProbMetric: 85.5183 - val_loss: 86.5850 - val_MinusLogProbMetric: 86.5850 - lr: 1.2346e-05 - 65s/epoch - 333ms/step
Epoch 169/1000
2023-10-24 11:59:44.704 
Epoch 169/1000 
	 loss: 84.5674, MinusLogProbMetric: 84.5674, val_loss: 84.2211, val_MinusLogProbMetric: 84.2211

Epoch 169: val_loss did not improve from 82.45608
196/196 - 66s - loss: 84.5674 - MinusLogProbMetric: 84.5674 - val_loss: 84.2211 - val_MinusLogProbMetric: 84.2211 - lr: 1.2346e-05 - 66s/epoch - 336ms/step
Epoch 170/1000
2023-10-24 12:00:51.010 
Epoch 170/1000 
	 loss: 83.3856, MinusLogProbMetric: 83.3856, val_loss: 84.0639, val_MinusLogProbMetric: 84.0639

Epoch 170: val_loss did not improve from 82.45608
196/196 - 66s - loss: 83.3856 - MinusLogProbMetric: 83.3856 - val_loss: 84.0639 - val_MinusLogProbMetric: 84.0639 - lr: 1.2346e-05 - 66s/epoch - 338ms/step
Epoch 171/1000
2023-10-24 12:01:55.471 
Epoch 171/1000 
	 loss: 83.0055, MinusLogProbMetric: 83.0055, val_loss: 82.8912, val_MinusLogProbMetric: 82.8912

Epoch 171: val_loss did not improve from 82.45608
196/196 - 64s - loss: 83.0055 - MinusLogProbMetric: 83.0055 - val_loss: 82.8912 - val_MinusLogProbMetric: 82.8912 - lr: 1.2346e-05 - 64s/epoch - 329ms/step
Epoch 172/1000
2023-10-24 12:03:03.553 
Epoch 172/1000 
	 loss: 82.6148, MinusLogProbMetric: 82.6148, val_loss: 82.8471, val_MinusLogProbMetric: 82.8471

Epoch 172: val_loss did not improve from 82.45608
196/196 - 68s - loss: 82.6148 - MinusLogProbMetric: 82.6148 - val_loss: 82.8471 - val_MinusLogProbMetric: 82.8471 - lr: 1.2346e-05 - 68s/epoch - 347ms/step
Epoch 173/1000
2023-10-24 12:04:12.006 
Epoch 173/1000 
	 loss: 82.4962, MinusLogProbMetric: 82.4962, val_loss: 81.6903, val_MinusLogProbMetric: 81.6903

Epoch 173: val_loss improved from 82.45608 to 81.69028, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 69s - loss: 82.4962 - MinusLogProbMetric: 82.4962 - val_loss: 81.6903 - val_MinusLogProbMetric: 81.6903 - lr: 1.2346e-05 - 69s/epoch - 353ms/step
Epoch 174/1000
2023-10-24 12:05:20.200 
Epoch 174/1000 
	 loss: 81.3507, MinusLogProbMetric: 81.3507, val_loss: 94.4827, val_MinusLogProbMetric: 94.4827

Epoch 174: val_loss did not improve from 81.69028
196/196 - 67s - loss: 81.3507 - MinusLogProbMetric: 81.3507 - val_loss: 94.4827 - val_MinusLogProbMetric: 94.4827 - lr: 1.2346e-05 - 67s/epoch - 344ms/step
Epoch 175/1000
2023-10-24 12:06:27.120 
Epoch 175/1000 
	 loss: 81.7228, MinusLogProbMetric: 81.7228, val_loss: 81.1463, val_MinusLogProbMetric: 81.1463

Epoch 175: val_loss improved from 81.69028 to 81.14632, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 68s - loss: 81.7228 - MinusLogProbMetric: 81.7228 - val_loss: 81.1463 - val_MinusLogProbMetric: 81.1463 - lr: 1.2346e-05 - 68s/epoch - 346ms/step
Epoch 176/1000
2023-10-24 12:07:36.915 
Epoch 176/1000 
	 loss: 80.1588, MinusLogProbMetric: 80.1588, val_loss: 80.7160, val_MinusLogProbMetric: 80.7160

Epoch 176: val_loss improved from 81.14632 to 80.71598, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 70s - loss: 80.1588 - MinusLogProbMetric: 80.1588 - val_loss: 80.7160 - val_MinusLogProbMetric: 80.7160 - lr: 1.2346e-05 - 70s/epoch - 356ms/step
Epoch 177/1000
2023-10-24 12:08:47.026 
Epoch 177/1000 
	 loss: 80.0649, MinusLogProbMetric: 80.0649, val_loss: 79.8239, val_MinusLogProbMetric: 79.8239

Epoch 177: val_loss improved from 80.71598 to 79.82390, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 70s - loss: 80.0649 - MinusLogProbMetric: 80.0649 - val_loss: 79.8239 - val_MinusLogProbMetric: 79.8239 - lr: 1.2346e-05 - 70s/epoch - 357ms/step
Epoch 178/1000
2023-10-24 12:09:57.139 
Epoch 178/1000 
	 loss: 80.0135, MinusLogProbMetric: 80.0135, val_loss: 79.9683, val_MinusLogProbMetric: 79.9683

Epoch 178: val_loss did not improve from 79.82390
196/196 - 69s - loss: 80.0135 - MinusLogProbMetric: 80.0135 - val_loss: 79.9683 - val_MinusLogProbMetric: 79.9683 - lr: 1.2346e-05 - 69s/epoch - 353ms/step
Epoch 179/1000
2023-10-24 12:11:05.230 
Epoch 179/1000 
	 loss: 79.3026, MinusLogProbMetric: 79.3026, val_loss: 79.3455, val_MinusLogProbMetric: 79.3455

Epoch 179: val_loss improved from 79.82390 to 79.34554, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 69s - loss: 79.3026 - MinusLogProbMetric: 79.3026 - val_loss: 79.3455 - val_MinusLogProbMetric: 79.3455 - lr: 1.2346e-05 - 69s/epoch - 353ms/step
Epoch 180/1000
2023-10-24 12:12:13.106 
Epoch 180/1000 
	 loss: 79.2864, MinusLogProbMetric: 79.2864, val_loss: 79.0198, val_MinusLogProbMetric: 79.0198

Epoch 180: val_loss improved from 79.34554 to 79.01975, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 68s - loss: 79.2864 - MinusLogProbMetric: 79.2864 - val_loss: 79.0198 - val_MinusLogProbMetric: 79.0198 - lr: 1.2346e-05 - 68s/epoch - 346ms/step
Epoch 181/1000
2023-10-24 12:13:22.824 
Epoch 181/1000 
	 loss: 82.2869, MinusLogProbMetric: 82.2869, val_loss: 87.1841, val_MinusLogProbMetric: 87.1841

Epoch 181: val_loss did not improve from 79.01975
196/196 - 69s - loss: 82.2869 - MinusLogProbMetric: 82.2869 - val_loss: 87.1841 - val_MinusLogProbMetric: 87.1841 - lr: 1.2346e-05 - 69s/epoch - 351ms/step
Epoch 182/1000
2023-10-24 12:14:28.415 
Epoch 182/1000 
	 loss: 79.3028, MinusLogProbMetric: 79.3028, val_loss: 78.5058, val_MinusLogProbMetric: 78.5058

Epoch 182: val_loss improved from 79.01975 to 78.50579, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 67s - loss: 79.3028 - MinusLogProbMetric: 79.3028 - val_loss: 78.5058 - val_MinusLogProbMetric: 78.5058 - lr: 1.2346e-05 - 67s/epoch - 339ms/step
Epoch 183/1000
2023-10-24 12:15:38.080 
Epoch 183/1000 
	 loss: 78.0674, MinusLogProbMetric: 78.0674, val_loss: 78.5478, val_MinusLogProbMetric: 78.5478

Epoch 183: val_loss did not improve from 78.50579
196/196 - 69s - loss: 78.0674 - MinusLogProbMetric: 78.0674 - val_loss: 78.5478 - val_MinusLogProbMetric: 78.5478 - lr: 1.2346e-05 - 69s/epoch - 351ms/step
Epoch 184/1000
2023-10-24 12:16:47.066 
Epoch 184/1000 
	 loss: 78.1802, MinusLogProbMetric: 78.1802, val_loss: 77.7054, val_MinusLogProbMetric: 77.7054

Epoch 184: val_loss improved from 78.50579 to 77.70542, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 70s - loss: 78.1802 - MinusLogProbMetric: 78.1802 - val_loss: 77.7054 - val_MinusLogProbMetric: 77.7054 - lr: 1.2346e-05 - 70s/epoch - 357ms/step
Epoch 185/1000
2023-10-24 12:17:55.124 
Epoch 185/1000 
	 loss: 77.9619, MinusLogProbMetric: 77.9619, val_loss: 77.4296, val_MinusLogProbMetric: 77.4296

Epoch 185: val_loss improved from 77.70542 to 77.42957, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 68s - loss: 77.9619 - MinusLogProbMetric: 77.9619 - val_loss: 77.4296 - val_MinusLogProbMetric: 77.4296 - lr: 1.2346e-05 - 68s/epoch - 347ms/step
Epoch 186/1000
2023-10-24 12:19:01.051 
Epoch 186/1000 
	 loss: 77.5221, MinusLogProbMetric: 77.5221, val_loss: 77.3040, val_MinusLogProbMetric: 77.3040

Epoch 186: val_loss improved from 77.42957 to 77.30402, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 66s - loss: 77.5221 - MinusLogProbMetric: 77.5221 - val_loss: 77.3040 - val_MinusLogProbMetric: 77.3040 - lr: 1.2346e-05 - 66s/epoch - 336ms/step
Epoch 187/1000
2023-10-24 12:20:07.711 
Epoch 187/1000 
	 loss: 77.0421, MinusLogProbMetric: 77.0421, val_loss: 76.9897, val_MinusLogProbMetric: 76.9897

Epoch 187: val_loss improved from 77.30402 to 76.98969, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 67s - loss: 77.0421 - MinusLogProbMetric: 77.0421 - val_loss: 76.9897 - val_MinusLogProbMetric: 76.9897 - lr: 1.2346e-05 - 67s/epoch - 340ms/step
Epoch 188/1000
2023-10-24 12:21:15.875 
Epoch 188/1000 
	 loss: 76.9174, MinusLogProbMetric: 76.9174, val_loss: 76.8152, val_MinusLogProbMetric: 76.8152

Epoch 188: val_loss improved from 76.98969 to 76.81519, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 68s - loss: 76.9174 - MinusLogProbMetric: 76.9174 - val_loss: 76.8152 - val_MinusLogProbMetric: 76.8152 - lr: 1.2346e-05 - 68s/epoch - 348ms/step
Epoch 189/1000
2023-10-24 12:22:23.953 
Epoch 189/1000 
	 loss: 79.5231, MinusLogProbMetric: 79.5231, val_loss: 79.1455, val_MinusLogProbMetric: 79.1455

Epoch 189: val_loss did not improve from 76.81519
196/196 - 67s - loss: 79.5231 - MinusLogProbMetric: 79.5231 - val_loss: 79.1455 - val_MinusLogProbMetric: 79.1455 - lr: 1.2346e-05 - 67s/epoch - 343ms/step
Epoch 190/1000
2023-10-24 12:23:33.015 
Epoch 190/1000 
	 loss: 77.2263, MinusLogProbMetric: 77.2263, val_loss: 76.5131, val_MinusLogProbMetric: 76.5131

Epoch 190: val_loss improved from 76.81519 to 76.51306, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 70s - loss: 77.2263 - MinusLogProbMetric: 77.2263 - val_loss: 76.5131 - val_MinusLogProbMetric: 76.5131 - lr: 1.2346e-05 - 70s/epoch - 357ms/step
Epoch 191/1000
2023-10-24 12:24:41.716 
Epoch 191/1000 
	 loss: 75.9727, MinusLogProbMetric: 75.9727, val_loss: 75.9897, val_MinusLogProbMetric: 75.9897

Epoch 191: val_loss improved from 76.51306 to 75.98972, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 69s - loss: 75.9727 - MinusLogProbMetric: 75.9727 - val_loss: 75.9897 - val_MinusLogProbMetric: 75.9897 - lr: 1.2346e-05 - 69s/epoch - 351ms/step
Epoch 192/1000
2023-10-24 12:25:49.362 
Epoch 192/1000 
	 loss: 76.0749, MinusLogProbMetric: 76.0749, val_loss: 79.1856, val_MinusLogProbMetric: 79.1856

Epoch 192: val_loss did not improve from 75.98972
196/196 - 67s - loss: 76.0749 - MinusLogProbMetric: 76.0749 - val_loss: 79.1856 - val_MinusLogProbMetric: 79.1856 - lr: 1.2346e-05 - 67s/epoch - 340ms/step
Epoch 193/1000
2023-10-24 12:26:56.602 
Epoch 193/1000 
	 loss: 76.2381, MinusLogProbMetric: 76.2381, val_loss: 75.9869, val_MinusLogProbMetric: 75.9869

Epoch 193: val_loss improved from 75.98972 to 75.98692, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 68s - loss: 76.2381 - MinusLogProbMetric: 76.2381 - val_loss: 75.9869 - val_MinusLogProbMetric: 75.9869 - lr: 1.2346e-05 - 68s/epoch - 348ms/step
Epoch 194/1000
2023-10-24 12:28:01.231 
Epoch 194/1000 
	 loss: 75.4994, MinusLogProbMetric: 75.4994, val_loss: 76.7002, val_MinusLogProbMetric: 76.7002

Epoch 194: val_loss did not improve from 75.98692
196/196 - 64s - loss: 75.4994 - MinusLogProbMetric: 75.4994 - val_loss: 76.7002 - val_MinusLogProbMetric: 76.7002 - lr: 1.2346e-05 - 64s/epoch - 324ms/step
Epoch 195/1000
2023-10-24 12:29:06.071 
Epoch 195/1000 
	 loss: 75.3812, MinusLogProbMetric: 75.3812, val_loss: 75.4365, val_MinusLogProbMetric: 75.4365

Epoch 195: val_loss improved from 75.98692 to 75.43649, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 66s - loss: 75.3812 - MinusLogProbMetric: 75.3812 - val_loss: 75.4365 - val_MinusLogProbMetric: 75.4365 - lr: 1.2346e-05 - 66s/epoch - 336ms/step
Epoch 196/1000
2023-10-24 12:30:15.700 
Epoch 196/1000 
	 loss: 75.9016, MinusLogProbMetric: 75.9016, val_loss: 75.9928, val_MinusLogProbMetric: 75.9928

Epoch 196: val_loss did not improve from 75.43649
196/196 - 69s - loss: 75.9016 - MinusLogProbMetric: 75.9016 - val_loss: 75.9928 - val_MinusLogProbMetric: 75.9928 - lr: 1.2346e-05 - 69s/epoch - 350ms/step
Epoch 197/1000
2023-10-24 12:31:23.388 
Epoch 197/1000 
	 loss: 74.6400, MinusLogProbMetric: 74.6400, val_loss: 75.0081, val_MinusLogProbMetric: 75.0081

Epoch 197: val_loss improved from 75.43649 to 75.00811, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 69s - loss: 74.6400 - MinusLogProbMetric: 74.6400 - val_loss: 75.0081 - val_MinusLogProbMetric: 75.0081 - lr: 1.2346e-05 - 69s/epoch - 350ms/step
Epoch 198/1000
2023-10-24 12:32:32.049 
Epoch 198/1000 
	 loss: 74.5621, MinusLogProbMetric: 74.5621, val_loss: 74.7248, val_MinusLogProbMetric: 74.7248

Epoch 198: val_loss improved from 75.00811 to 74.72478, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 69s - loss: 74.5621 - MinusLogProbMetric: 74.5621 - val_loss: 74.7248 - val_MinusLogProbMetric: 74.7248 - lr: 1.2346e-05 - 69s/epoch - 350ms/step
Epoch 199/1000
2023-10-24 12:33:40.441 
Epoch 199/1000 
	 loss: 74.4294, MinusLogProbMetric: 74.4294, val_loss: 74.7116, val_MinusLogProbMetric: 74.7116

Epoch 199: val_loss improved from 74.72478 to 74.71162, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 68s - loss: 74.4294 - MinusLogProbMetric: 74.4294 - val_loss: 74.7116 - val_MinusLogProbMetric: 74.7116 - lr: 1.2346e-05 - 68s/epoch - 349ms/step
Epoch 200/1000
2023-10-24 12:34:48.903 
Epoch 200/1000 
	 loss: 74.6959, MinusLogProbMetric: 74.6959, val_loss: 76.5728, val_MinusLogProbMetric: 76.5728

Epoch 200: val_loss did not improve from 74.71162
196/196 - 67s - loss: 74.6959 - MinusLogProbMetric: 74.6959 - val_loss: 76.5728 - val_MinusLogProbMetric: 76.5728 - lr: 1.2346e-05 - 67s/epoch - 344ms/step
Epoch 201/1000
2023-10-24 12:35:53.589 
Epoch 201/1000 
	 loss: 73.9936, MinusLogProbMetric: 73.9936, val_loss: 76.3039, val_MinusLogProbMetric: 76.3039

Epoch 201: val_loss did not improve from 74.71162
196/196 - 65s - loss: 73.9936 - MinusLogProbMetric: 73.9936 - val_loss: 76.3039 - val_MinusLogProbMetric: 76.3039 - lr: 1.2346e-05 - 65s/epoch - 330ms/step
Epoch 202/1000
2023-10-24 12:37:02.603 
Epoch 202/1000 
	 loss: 74.1361, MinusLogProbMetric: 74.1361, val_loss: 74.1305, val_MinusLogProbMetric: 74.1305

Epoch 202: val_loss improved from 74.71162 to 74.13051, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 70s - loss: 74.1361 - MinusLogProbMetric: 74.1361 - val_loss: 74.1305 - val_MinusLogProbMetric: 74.1305 - lr: 1.2346e-05 - 70s/epoch - 358ms/step
Epoch 203/1000
2023-10-24 12:38:14.238 
Epoch 203/1000 
	 loss: 73.6082, MinusLogProbMetric: 73.6082, val_loss: 73.5061, val_MinusLogProbMetric: 73.5061

Epoch 203: val_loss improved from 74.13051 to 73.50613, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 72s - loss: 73.6082 - MinusLogProbMetric: 73.6082 - val_loss: 73.5061 - val_MinusLogProbMetric: 73.5061 - lr: 1.2346e-05 - 72s/epoch - 365ms/step
Epoch 204/1000
2023-10-24 12:39:23.024 
Epoch 204/1000 
	 loss: 73.6031, MinusLogProbMetric: 73.6031, val_loss: 74.4695, val_MinusLogProbMetric: 74.4695

Epoch 204: val_loss did not improve from 73.50613
196/196 - 68s - loss: 73.6031 - MinusLogProbMetric: 73.6031 - val_loss: 74.4695 - val_MinusLogProbMetric: 74.4695 - lr: 1.2346e-05 - 68s/epoch - 346ms/step
Epoch 205/1000
2023-10-24 12:40:31.921 
Epoch 205/1000 
	 loss: 73.2100, MinusLogProbMetric: 73.2100, val_loss: 73.2399, val_MinusLogProbMetric: 73.2399

Epoch 205: val_loss improved from 73.50613 to 73.23994, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 70s - loss: 73.2100 - MinusLogProbMetric: 73.2100 - val_loss: 73.2399 - val_MinusLogProbMetric: 73.2399 - lr: 1.2346e-05 - 70s/epoch - 356ms/step
Epoch 206/1000
2023-10-24 12:41:37.583 
Epoch 206/1000 
	 loss: 77.8027, MinusLogProbMetric: 77.8027, val_loss: 74.0479, val_MinusLogProbMetric: 74.0479

Epoch 206: val_loss did not improve from 73.23994
196/196 - 65s - loss: 77.8027 - MinusLogProbMetric: 77.8027 - val_loss: 74.0479 - val_MinusLogProbMetric: 74.0479 - lr: 1.2346e-05 - 65s/epoch - 330ms/step
Epoch 207/1000
2023-10-24 12:42:42.490 
Epoch 207/1000 
	 loss: 72.8605, MinusLogProbMetric: 72.8605, val_loss: 73.1003, val_MinusLogProbMetric: 73.1003

Epoch 207: val_loss improved from 73.23994 to 73.10027, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 66s - loss: 72.8605 - MinusLogProbMetric: 72.8605 - val_loss: 73.1003 - val_MinusLogProbMetric: 73.1003 - lr: 1.2346e-05 - 66s/epoch - 336ms/step
Epoch 208/1000
2023-10-24 12:43:50.927 
Epoch 208/1000 
	 loss: 72.4648, MinusLogProbMetric: 72.4648, val_loss: 72.6817, val_MinusLogProbMetric: 72.6817

Epoch 208: val_loss improved from 73.10027 to 72.68172, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 69s - loss: 72.4648 - MinusLogProbMetric: 72.4648 - val_loss: 72.6817 - val_MinusLogProbMetric: 72.6817 - lr: 1.2346e-05 - 69s/epoch - 350ms/step
Epoch 209/1000
2023-10-24 12:45:01.717 
Epoch 209/1000 
	 loss: 72.2311, MinusLogProbMetric: 72.2311, val_loss: 73.4180, val_MinusLogProbMetric: 73.4180

Epoch 209: val_loss did not improve from 72.68172
196/196 - 70s - loss: 72.2311 - MinusLogProbMetric: 72.2311 - val_loss: 73.4180 - val_MinusLogProbMetric: 73.4180 - lr: 1.2346e-05 - 70s/epoch - 356ms/step
Epoch 210/1000
2023-10-24 12:46:09.635 
Epoch 210/1000 
	 loss: 72.2113, MinusLogProbMetric: 72.2113, val_loss: 72.7828, val_MinusLogProbMetric: 72.7828

Epoch 210: val_loss did not improve from 72.68172
196/196 - 68s - loss: 72.2113 - MinusLogProbMetric: 72.2113 - val_loss: 72.7828 - val_MinusLogProbMetric: 72.7828 - lr: 1.2346e-05 - 68s/epoch - 346ms/step
Epoch 211/1000
2023-10-24 12:47:14.923 
Epoch 211/1000 
	 loss: 72.0054, MinusLogProbMetric: 72.0054, val_loss: 71.7730, val_MinusLogProbMetric: 71.7730

Epoch 211: val_loss improved from 72.68172 to 71.77302, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 66s - loss: 72.0054 - MinusLogProbMetric: 72.0054 - val_loss: 71.7730 - val_MinusLogProbMetric: 71.7730 - lr: 1.2346e-05 - 66s/epoch - 338ms/step
Epoch 212/1000
2023-10-24 12:48:24.876 
Epoch 212/1000 
	 loss: 71.9472, MinusLogProbMetric: 71.9472, val_loss: 72.5296, val_MinusLogProbMetric: 72.5296

Epoch 212: val_loss did not improve from 71.77302
196/196 - 69s - loss: 71.9472 - MinusLogProbMetric: 71.9472 - val_loss: 72.5296 - val_MinusLogProbMetric: 72.5296 - lr: 1.2346e-05 - 69s/epoch - 352ms/step
Epoch 213/1000
2023-10-24 12:49:31.118 
Epoch 213/1000 
	 loss: 71.5482, MinusLogProbMetric: 71.5482, val_loss: 71.7764, val_MinusLogProbMetric: 71.7764

Epoch 213: val_loss did not improve from 71.77302
196/196 - 66s - loss: 71.5482 - MinusLogProbMetric: 71.5482 - val_loss: 71.7764 - val_MinusLogProbMetric: 71.7764 - lr: 1.2346e-05 - 66s/epoch - 338ms/step
Epoch 214/1000
2023-10-24 12:50:37.469 
Epoch 214/1000 
	 loss: 71.1126, MinusLogProbMetric: 71.1126, val_loss: 71.5196, val_MinusLogProbMetric: 71.5196

Epoch 214: val_loss improved from 71.77302 to 71.51959, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 67s - loss: 71.1126 - MinusLogProbMetric: 71.1126 - val_loss: 71.5196 - val_MinusLogProbMetric: 71.5196 - lr: 1.2346e-05 - 67s/epoch - 343ms/step
Epoch 215/1000
2023-10-24 12:51:46.379 
Epoch 215/1000 
	 loss: 72.4226, MinusLogProbMetric: 72.4226, val_loss: 71.3438, val_MinusLogProbMetric: 71.3438

Epoch 215: val_loss improved from 71.51959 to 71.34381, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 69s - loss: 72.4226 - MinusLogProbMetric: 72.4226 - val_loss: 71.3438 - val_MinusLogProbMetric: 71.3438 - lr: 1.2346e-05 - 69s/epoch - 352ms/step
Epoch 216/1000
2023-10-24 12:52:53.465 
Epoch 216/1000 
	 loss: 70.8046, MinusLogProbMetric: 70.8046, val_loss: 71.0935, val_MinusLogProbMetric: 71.0935

Epoch 216: val_loss improved from 71.34381 to 71.09348, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 67s - loss: 70.8046 - MinusLogProbMetric: 70.8046 - val_loss: 71.0935 - val_MinusLogProbMetric: 71.0935 - lr: 1.2346e-05 - 67s/epoch - 342ms/step
Epoch 217/1000
2023-10-24 12:54:01.957 
Epoch 217/1000 
	 loss: 71.0166, MinusLogProbMetric: 71.0166, val_loss: 70.9846, val_MinusLogProbMetric: 70.9846

Epoch 217: val_loss improved from 71.09348 to 70.98460, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 69s - loss: 71.0166 - MinusLogProbMetric: 71.0166 - val_loss: 70.9846 - val_MinusLogProbMetric: 70.9846 - lr: 1.2346e-05 - 69s/epoch - 350ms/step
Epoch 218/1000
2023-10-24 12:55:09.551 
Epoch 218/1000 
	 loss: 70.6927, MinusLogProbMetric: 70.6927, val_loss: 70.8833, val_MinusLogProbMetric: 70.8833

Epoch 218: val_loss improved from 70.98460 to 70.88333, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 67s - loss: 70.6927 - MinusLogProbMetric: 70.6927 - val_loss: 70.8833 - val_MinusLogProbMetric: 70.8833 - lr: 1.2346e-05 - 67s/epoch - 344ms/step
Epoch 219/1000
2023-10-24 12:56:15.558 
Epoch 219/1000 
	 loss: 70.6496, MinusLogProbMetric: 70.6496, val_loss: 71.6069, val_MinusLogProbMetric: 71.6069

Epoch 219: val_loss did not improve from 70.88333
196/196 - 65s - loss: 70.6496 - MinusLogProbMetric: 70.6496 - val_loss: 71.6069 - val_MinusLogProbMetric: 71.6069 - lr: 1.2346e-05 - 65s/epoch - 332ms/step
Epoch 220/1000
2023-10-24 12:57:22.605 
Epoch 220/1000 
	 loss: 70.3475, MinusLogProbMetric: 70.3475, val_loss: 70.2395, val_MinusLogProbMetric: 70.2395

Epoch 220: val_loss improved from 70.88333 to 70.23946, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 68s - loss: 70.3475 - MinusLogProbMetric: 70.3475 - val_loss: 70.2395 - val_MinusLogProbMetric: 70.2395 - lr: 1.2346e-05 - 68s/epoch - 347ms/step
Epoch 221/1000
2023-10-24 12:58:29.661 
Epoch 221/1000 
	 loss: 69.9297, MinusLogProbMetric: 69.9297, val_loss: 70.1206, val_MinusLogProbMetric: 70.1206

Epoch 221: val_loss improved from 70.23946 to 70.12056, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 67s - loss: 69.9297 - MinusLogProbMetric: 69.9297 - val_loss: 70.1206 - val_MinusLogProbMetric: 70.1206 - lr: 1.2346e-05 - 67s/epoch - 342ms/step
Epoch 222/1000
2023-10-24 12:59:38.519 
Epoch 222/1000 
	 loss: 70.4307, MinusLogProbMetric: 70.4307, val_loss: 70.3712, val_MinusLogProbMetric: 70.3712

Epoch 222: val_loss did not improve from 70.12056
196/196 - 68s - loss: 70.4307 - MinusLogProbMetric: 70.4307 - val_loss: 70.3712 - val_MinusLogProbMetric: 70.3712 - lr: 1.2346e-05 - 68s/epoch - 346ms/step
Epoch 223/1000
2023-10-24 13:00:44.648 
Epoch 223/1000 
	 loss: 69.8533, MinusLogProbMetric: 69.8533, val_loss: 70.0193, val_MinusLogProbMetric: 70.0193

Epoch 223: val_loss improved from 70.12056 to 70.01929, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 67s - loss: 69.8533 - MinusLogProbMetric: 69.8533 - val_loss: 70.0193 - val_MinusLogProbMetric: 70.0193 - lr: 1.2346e-05 - 67s/epoch - 342ms/step
Epoch 224/1000
2023-10-24 13:01:54.058 
Epoch 224/1000 
	 loss: 71.4232, MinusLogProbMetric: 71.4232, val_loss: 71.4579, val_MinusLogProbMetric: 71.4579

Epoch 224: val_loss did not improve from 70.01929
196/196 - 68s - loss: 71.4232 - MinusLogProbMetric: 71.4232 - val_loss: 71.4579 - val_MinusLogProbMetric: 71.4579 - lr: 1.2346e-05 - 68s/epoch - 349ms/step
Epoch 225/1000
2023-10-24 13:03:01.046 
Epoch 225/1000 
	 loss: 69.6721, MinusLogProbMetric: 69.6721, val_loss: 70.0230, val_MinusLogProbMetric: 70.0230

Epoch 225: val_loss did not improve from 70.01929
196/196 - 67s - loss: 69.6721 - MinusLogProbMetric: 69.6721 - val_loss: 70.0230 - val_MinusLogProbMetric: 70.0230 - lr: 1.2346e-05 - 67s/epoch - 342ms/step
Epoch 226/1000
2023-10-24 13:04:08.320 
Epoch 226/1000 
	 loss: 69.1166, MinusLogProbMetric: 69.1166, val_loss: 69.7033, val_MinusLogProbMetric: 69.7033

Epoch 226: val_loss improved from 70.01929 to 69.70333, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 68s - loss: 69.1166 - MinusLogProbMetric: 69.1166 - val_loss: 69.7033 - val_MinusLogProbMetric: 69.7033 - lr: 1.2346e-05 - 68s/epoch - 348ms/step
Epoch 227/1000
2023-10-24 13:05:14.644 
Epoch 227/1000 
	 loss: 68.9382, MinusLogProbMetric: 68.9382, val_loss: 69.1081, val_MinusLogProbMetric: 69.1081

Epoch 227: val_loss improved from 69.70333 to 69.10807, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 66s - loss: 68.9382 - MinusLogProbMetric: 68.9382 - val_loss: 69.1081 - val_MinusLogProbMetric: 69.1081 - lr: 1.2346e-05 - 66s/epoch - 338ms/step
Epoch 228/1000
2023-10-24 13:06:23.449 
Epoch 228/1000 
	 loss: 69.0694, MinusLogProbMetric: 69.0694, val_loss: 69.8941, val_MinusLogProbMetric: 69.8941

Epoch 228: val_loss did not improve from 69.10807
196/196 - 68s - loss: 69.0694 - MinusLogProbMetric: 69.0694 - val_loss: 69.8941 - val_MinusLogProbMetric: 69.8941 - lr: 1.2346e-05 - 68s/epoch - 346ms/step
Epoch 229/1000
2023-10-24 13:07:32.881 
Epoch 229/1000 
	 loss: 70.3236, MinusLogProbMetric: 70.3236, val_loss: 69.6097, val_MinusLogProbMetric: 69.6097

Epoch 229: val_loss did not improve from 69.10807
196/196 - 69s - loss: 70.3236 - MinusLogProbMetric: 70.3236 - val_loss: 69.6097 - val_MinusLogProbMetric: 69.6097 - lr: 1.2346e-05 - 69s/epoch - 354ms/step
Epoch 230/1000
2023-10-24 13:08:39.306 
Epoch 230/1000 
	 loss: 69.1496, MinusLogProbMetric: 69.1496, val_loss: 69.8695, val_MinusLogProbMetric: 69.8695

Epoch 230: val_loss did not improve from 69.10807
196/196 - 66s - loss: 69.1496 - MinusLogProbMetric: 69.1496 - val_loss: 69.8695 - val_MinusLogProbMetric: 69.8695 - lr: 1.2346e-05 - 66s/epoch - 339ms/step
Epoch 231/1000
2023-10-24 13:09:45.498 
Epoch 231/1000 
	 loss: 69.2469, MinusLogProbMetric: 69.2469, val_loss: 69.1090, val_MinusLogProbMetric: 69.1090

Epoch 231: val_loss did not improve from 69.10807
196/196 - 66s - loss: 69.2469 - MinusLogProbMetric: 69.2469 - val_loss: 69.1090 - val_MinusLogProbMetric: 69.1090 - lr: 1.2346e-05 - 66s/epoch - 338ms/step
Epoch 232/1000
2023-10-24 13:10:52.760 
Epoch 232/1000 
	 loss: 68.6979, MinusLogProbMetric: 68.6979, val_loss: 68.9337, val_MinusLogProbMetric: 68.9337

Epoch 232: val_loss improved from 69.10807 to 68.93370, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 68s - loss: 68.6979 - MinusLogProbMetric: 68.6979 - val_loss: 68.9337 - val_MinusLogProbMetric: 68.9337 - lr: 1.2346e-05 - 68s/epoch - 348ms/step
Epoch 233/1000
2023-10-24 13:12:01.987 
Epoch 233/1000 
	 loss: 68.7273, MinusLogProbMetric: 68.7273, val_loss: 68.6171, val_MinusLogProbMetric: 68.6171

Epoch 233: val_loss improved from 68.93370 to 68.61711, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 69s - loss: 68.7273 - MinusLogProbMetric: 68.7273 - val_loss: 68.6171 - val_MinusLogProbMetric: 68.6171 - lr: 1.2346e-05 - 69s/epoch - 353ms/step
Epoch 234/1000
2023-10-24 13:13:08.863 
Epoch 234/1000 
	 loss: 68.6319, MinusLogProbMetric: 68.6319, val_loss: 69.0821, val_MinusLogProbMetric: 69.0821

Epoch 234: val_loss did not improve from 68.61711
196/196 - 66s - loss: 68.6319 - MinusLogProbMetric: 68.6319 - val_loss: 69.0821 - val_MinusLogProbMetric: 69.0821 - lr: 1.2346e-05 - 66s/epoch - 336ms/step
Epoch 235/1000
2023-10-24 13:14:15.707 
Epoch 235/1000 
	 loss: 68.1523, MinusLogProbMetric: 68.1523, val_loss: 68.9894, val_MinusLogProbMetric: 68.9894

Epoch 235: val_loss did not improve from 68.61711
196/196 - 67s - loss: 68.1523 - MinusLogProbMetric: 68.1523 - val_loss: 68.9894 - val_MinusLogProbMetric: 68.9894 - lr: 1.2346e-05 - 67s/epoch - 341ms/step
Epoch 236/1000
2023-10-24 13:15:23.032 
Epoch 236/1000 
	 loss: 72.3746, MinusLogProbMetric: 72.3746, val_loss: 70.4765, val_MinusLogProbMetric: 70.4765

Epoch 236: val_loss did not improve from 68.61711
196/196 - 67s - loss: 72.3746 - MinusLogProbMetric: 72.3746 - val_loss: 70.4765 - val_MinusLogProbMetric: 70.4765 - lr: 1.2346e-05 - 67s/epoch - 343ms/step
Epoch 237/1000
2023-10-24 13:16:28.881 
Epoch 237/1000 
	 loss: 69.5648, MinusLogProbMetric: 69.5648, val_loss: 71.5276, val_MinusLogProbMetric: 71.5276

Epoch 237: val_loss did not improve from 68.61711
196/196 - 66s - loss: 69.5648 - MinusLogProbMetric: 69.5648 - val_loss: 71.5276 - val_MinusLogProbMetric: 71.5276 - lr: 1.2346e-05 - 66s/epoch - 336ms/step
Epoch 238/1000
2023-10-24 13:17:36.005 
Epoch 238/1000 
	 loss: 68.6442, MinusLogProbMetric: 68.6442, val_loss: 68.7987, val_MinusLogProbMetric: 68.7987

Epoch 238: val_loss did not improve from 68.61711
196/196 - 67s - loss: 68.6442 - MinusLogProbMetric: 68.6442 - val_loss: 68.7987 - val_MinusLogProbMetric: 68.7987 - lr: 1.2346e-05 - 67s/epoch - 342ms/step
Epoch 239/1000
2023-10-24 13:18:43.796 
Epoch 239/1000 
	 loss: 68.1353, MinusLogProbMetric: 68.1353, val_loss: 68.2279, val_MinusLogProbMetric: 68.2279

Epoch 239: val_loss improved from 68.61711 to 68.22793, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 69s - loss: 68.1353 - MinusLogProbMetric: 68.1353 - val_loss: 68.2279 - val_MinusLogProbMetric: 68.2279 - lr: 1.2346e-05 - 69s/epoch - 351ms/step
Epoch 240/1000
2023-10-24 13:19:52.681 
Epoch 240/1000 
	 loss: 70.8781, MinusLogProbMetric: 70.8781, val_loss: 71.0294, val_MinusLogProbMetric: 71.0294

Epoch 240: val_loss did not improve from 68.22793
196/196 - 68s - loss: 70.8781 - MinusLogProbMetric: 70.8781 - val_loss: 71.0294 - val_MinusLogProbMetric: 71.0294 - lr: 1.2346e-05 - 68s/epoch - 346ms/step
Epoch 241/1000
2023-10-24 13:21:00.087 
Epoch 241/1000 
	 loss: 68.9000, MinusLogProbMetric: 68.9000, val_loss: 69.0269, val_MinusLogProbMetric: 69.0269

Epoch 241: val_loss did not improve from 68.22793
196/196 - 67s - loss: 68.9000 - MinusLogProbMetric: 68.9000 - val_loss: 69.0269 - val_MinusLogProbMetric: 69.0269 - lr: 1.2346e-05 - 67s/epoch - 344ms/step
Epoch 242/1000
2023-10-24 13:22:06.677 
Epoch 242/1000 
	 loss: 68.1075, MinusLogProbMetric: 68.1075, val_loss: 68.0084, val_MinusLogProbMetric: 68.0084

Epoch 242: val_loss improved from 68.22793 to 68.00838, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 67s - loss: 68.1075 - MinusLogProbMetric: 68.1075 - val_loss: 68.0084 - val_MinusLogProbMetric: 68.0084 - lr: 1.2346e-05 - 67s/epoch - 344ms/step
Epoch 243/1000
2023-10-24 13:23:16.081 
Epoch 243/1000 
	 loss: 67.5573, MinusLogProbMetric: 67.5573, val_loss: 67.9011, val_MinusLogProbMetric: 67.9011

Epoch 243: val_loss improved from 68.00838 to 67.90108, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 70s - loss: 67.5573 - MinusLogProbMetric: 67.5573 - val_loss: 67.9011 - val_MinusLogProbMetric: 67.9011 - lr: 1.2346e-05 - 70s/epoch - 355ms/step
Epoch 244/1000
2023-10-24 13:24:26.373 
Epoch 244/1000 
	 loss: 67.3476, MinusLogProbMetric: 67.3476, val_loss: 67.6193, val_MinusLogProbMetric: 67.6193

Epoch 244: val_loss improved from 67.90108 to 67.61932, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 70s - loss: 67.3476 - MinusLogProbMetric: 67.3476 - val_loss: 67.6193 - val_MinusLogProbMetric: 67.6193 - lr: 1.2346e-05 - 70s/epoch - 359ms/step
Epoch 245/1000
2023-10-24 13:25:33.941 
Epoch 245/1000 
	 loss: 67.7516, MinusLogProbMetric: 67.7516, val_loss: 68.2976, val_MinusLogProbMetric: 68.2976

Epoch 245: val_loss did not improve from 67.61932
196/196 - 67s - loss: 67.7516 - MinusLogProbMetric: 67.7516 - val_loss: 68.2976 - val_MinusLogProbMetric: 68.2976 - lr: 1.2346e-05 - 67s/epoch - 339ms/step
Epoch 246/1000
2023-10-24 13:26:40.119 
Epoch 246/1000 
	 loss: 67.0889, MinusLogProbMetric: 67.0889, val_loss: 67.2651, val_MinusLogProbMetric: 67.2651

Epoch 246: val_loss improved from 67.61932 to 67.26508, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 67s - loss: 67.0889 - MinusLogProbMetric: 67.0889 - val_loss: 67.2651 - val_MinusLogProbMetric: 67.2651 - lr: 1.2346e-05 - 67s/epoch - 343ms/step
Epoch 247/1000
2023-10-24 13:27:48.958 
Epoch 247/1000 
	 loss: 66.7790, MinusLogProbMetric: 66.7790, val_loss: 67.2196, val_MinusLogProbMetric: 67.2196

Epoch 247: val_loss improved from 67.26508 to 67.21964, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 69s - loss: 66.7790 - MinusLogProbMetric: 66.7790 - val_loss: 67.2196 - val_MinusLogProbMetric: 67.2196 - lr: 1.2346e-05 - 69s/epoch - 350ms/step
Epoch 248/1000
2023-10-24 13:28:56.916 
Epoch 248/1000 
	 loss: 67.2499, MinusLogProbMetric: 67.2499, val_loss: 67.3421, val_MinusLogProbMetric: 67.3421

Epoch 248: val_loss did not improve from 67.21964
196/196 - 67s - loss: 67.2499 - MinusLogProbMetric: 67.2499 - val_loss: 67.3421 - val_MinusLogProbMetric: 67.3421 - lr: 1.2346e-05 - 67s/epoch - 342ms/step
Epoch 249/1000
2023-10-24 13:30:04.734 
Epoch 249/1000 
	 loss: 66.3251, MinusLogProbMetric: 66.3251, val_loss: 66.5306, val_MinusLogProbMetric: 66.5306

Epoch 249: val_loss improved from 67.21964 to 66.53065, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 69s - loss: 66.3251 - MinusLogProbMetric: 66.3251 - val_loss: 66.5306 - val_MinusLogProbMetric: 66.5306 - lr: 1.2346e-05 - 69s/epoch - 351ms/step
Epoch 250/1000
2023-10-24 13:31:12.182 
Epoch 250/1000 
	 loss: 66.1584, MinusLogProbMetric: 66.1584, val_loss: 66.7621, val_MinusLogProbMetric: 66.7621

Epoch 250: val_loss did not improve from 66.53065
196/196 - 66s - loss: 66.1584 - MinusLogProbMetric: 66.1584 - val_loss: 66.7621 - val_MinusLogProbMetric: 66.7621 - lr: 1.2346e-05 - 66s/epoch - 339ms/step
Epoch 251/1000
2023-10-24 13:32:19.163 
Epoch 251/1000 
	 loss: 66.2518, MinusLogProbMetric: 66.2518, val_loss: 66.5221, val_MinusLogProbMetric: 66.5221

Epoch 251: val_loss improved from 66.53065 to 66.52209, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 68s - loss: 66.2518 - MinusLogProbMetric: 66.2518 - val_loss: 66.5221 - val_MinusLogProbMetric: 66.5221 - lr: 1.2346e-05 - 68s/epoch - 347ms/step
Epoch 252/1000
2023-10-24 13:33:30.081 
Epoch 252/1000 
	 loss: 65.8980, MinusLogProbMetric: 65.8980, val_loss: 66.6605, val_MinusLogProbMetric: 66.6605

Epoch 252: val_loss did not improve from 66.52209
196/196 - 70s - loss: 65.8980 - MinusLogProbMetric: 65.8980 - val_loss: 66.6605 - val_MinusLogProbMetric: 66.6605 - lr: 1.2346e-05 - 70s/epoch - 356ms/step
Epoch 253/1000
2023-10-24 13:34:39.743 
Epoch 253/1000 
	 loss: 65.9700, MinusLogProbMetric: 65.9700, val_loss: 66.6479, val_MinusLogProbMetric: 66.6479

Epoch 253: val_loss did not improve from 66.52209
196/196 - 70s - loss: 65.9700 - MinusLogProbMetric: 65.9700 - val_loss: 66.6479 - val_MinusLogProbMetric: 66.6479 - lr: 1.2346e-05 - 70s/epoch - 355ms/step
Epoch 254/1000
2023-10-24 13:35:47.642 
Epoch 254/1000 
	 loss: 66.0072, MinusLogProbMetric: 66.0072, val_loss: 66.4959, val_MinusLogProbMetric: 66.4959

Epoch 254: val_loss improved from 66.52209 to 66.49590, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 69s - loss: 66.0072 - MinusLogProbMetric: 66.0072 - val_loss: 66.4959 - val_MinusLogProbMetric: 66.4959 - lr: 1.2346e-05 - 69s/epoch - 351ms/step
Epoch 255/1000
2023-10-24 13:36:56.085 
Epoch 255/1000 
	 loss: 65.4874, MinusLogProbMetric: 65.4874, val_loss: 68.2809, val_MinusLogProbMetric: 68.2809

Epoch 255: val_loss did not improve from 66.49590
196/196 - 68s - loss: 65.4874 - MinusLogProbMetric: 65.4874 - val_loss: 68.2809 - val_MinusLogProbMetric: 68.2809 - lr: 1.2346e-05 - 68s/epoch - 344ms/step
Epoch 256/1000
2023-10-24 13:38:02.799 
Epoch 256/1000 
	 loss: 65.6719, MinusLogProbMetric: 65.6719, val_loss: 65.7173, val_MinusLogProbMetric: 65.7173

Epoch 256: val_loss improved from 66.49590 to 65.71734, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 68s - loss: 65.6719 - MinusLogProbMetric: 65.6719 - val_loss: 65.7173 - val_MinusLogProbMetric: 65.7173 - lr: 1.2346e-05 - 68s/epoch - 346ms/step
Epoch 257/1000
2023-10-24 13:39:10.019 
Epoch 257/1000 
	 loss: 65.2589, MinusLogProbMetric: 65.2589, val_loss: 65.9553, val_MinusLogProbMetric: 65.9553

Epoch 257: val_loss did not improve from 65.71734
196/196 - 66s - loss: 65.2589 - MinusLogProbMetric: 65.2589 - val_loss: 65.9553 - val_MinusLogProbMetric: 65.9553 - lr: 1.2346e-05 - 66s/epoch - 338ms/step
Epoch 258/1000
2023-10-24 13:40:16.165 
Epoch 258/1000 
	 loss: 64.9929, MinusLogProbMetric: 64.9929, val_loss: 65.6495, val_MinusLogProbMetric: 65.6495

Epoch 258: val_loss improved from 65.71734 to 65.64945, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 67s - loss: 64.9929 - MinusLogProbMetric: 64.9929 - val_loss: 65.6495 - val_MinusLogProbMetric: 65.6495 - lr: 1.2346e-05 - 67s/epoch - 342ms/step
Epoch 259/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 134: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-24 13:41:05.514 
Epoch 259/1000 
	 loss: nan, MinusLogProbMetric: 66.0697, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 259: val_loss did not improve from 65.64945
196/196 - 49s - loss: nan - MinusLogProbMetric: 66.0697 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.2346e-05 - 49s/epoch - 247ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 4.115226337448558e-06.
===========
Generating train data for run 342.
===========
Train data generated in 0.35 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_342/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_342/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_342/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_342
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.465355   7.122426   6.507682  ...  1.2216942  7.396178   1.4448237]
 [ 1.8683796  2.8760538  9.147447  ...  4.980697  -1.1366768  2.8016024]
 [ 5.6175413  8.642618   5.6734285 ... -0.9440901  7.0561414  1.3082856]
 ...
 [ 6.145307   2.8273122  6.191003  ...  2.964365   6.077158   2.5509653]
 [ 6.5595512  2.759307   6.0778823 ...  3.4800506  4.3951674  2.1941729]
 [ 5.586527   7.3216014  6.2487698 ...  1.292631   5.797888   1.2879843]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_340"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_341 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_30 (LogProbL  (None,)                  3291840   
 ayer)                                                           
                                                                 
=================================================================
Total params: 3,291,840
Trainable params: 3,291,840
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_30/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_30'")
self.model: <keras.engine.functional.Functional object at 0x7f5b442fdb70>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f58a21510c0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f58a21510c0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f5b5c6cf490>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f595c6d7310>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f595c6d7880>, <keras.callbacks.ModelCheckpoint object at 0x7f595c6d7940>, <keras.callbacks.EarlyStopping object at 0x7f595c6d7bb0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f595c6d7be0>, <keras.callbacks.TerminateOnNaN object at 0x7f595c6d7820>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 342/720 with hyperparameters:
timestamp = 2023-10-24 13:41:14.334405
ndims = 64
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 3291840
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.115226337448558e-06...
Train first sample: [ 5.465355    7.122426    6.507682    5.369255    4.1136785   6.196197
  4.259451    8.702779    9.329599    4.155665    7.5761204   5.312057
  5.67009     9.304932    0.8909831   1.4673554  -0.03015512  8.003835
  8.704047    8.985184    9.595006    8.103893    4.62255     8.186032
  0.5885266   6.325719    0.92288995  9.5184145   6.6715336   3.3791308
  2.4301429   8.523114    4.619198    5.3705144   0.3660334   5.918427
  6.2292395   7.1572037   9.551745    6.67638     3.2475867   4.406022
  7.1114645   0.65166384  6.9876056   6.6356955   2.3022606   1.2872343
  3.532573    3.8445692   6.0782557   4.5467997   9.491786    0.11065191
  3.0255654   1.2933161   6.592196    2.150623    4.2476006   4.39
  1.805124    1.2216942   7.396178    1.4448237 ]
Epoch 1/1000
2023-10-24 13:44:20.071 
Epoch 1/1000 
	 loss: 66.8151, MinusLogProbMetric: 66.8151, val_loss: 65.1512, val_MinusLogProbMetric: 65.1512

Epoch 1: val_loss improved from inf to 65.15125, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 186s - loss: 66.8151 - MinusLogProbMetric: 66.8151 - val_loss: 65.1512 - val_MinusLogProbMetric: 65.1512 - lr: 4.1152e-06 - 186s/epoch - 951ms/step
Epoch 2/1000
2023-10-24 13:45:29.657 
Epoch 2/1000 
	 loss: 66.7513, MinusLogProbMetric: 66.7513, val_loss: 70.3997, val_MinusLogProbMetric: 70.3997

Epoch 2: val_loss did not improve from 65.15125
196/196 - 68s - loss: 66.7513 - MinusLogProbMetric: 66.7513 - val_loss: 70.3997 - val_MinusLogProbMetric: 70.3997 - lr: 4.1152e-06 - 68s/epoch - 349ms/step
Epoch 3/1000
2023-10-24 13:46:38.565 
Epoch 3/1000 
	 loss: 65.8020, MinusLogProbMetric: 65.8020, val_loss: 64.8408, val_MinusLogProbMetric: 64.8408

Epoch 3: val_loss improved from 65.15125 to 64.84077, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 70s - loss: 65.8020 - MinusLogProbMetric: 65.8020 - val_loss: 64.8408 - val_MinusLogProbMetric: 64.8408 - lr: 4.1152e-06 - 70s/epoch - 356ms/step
Epoch 4/1000
2023-10-24 13:47:46.608 
Epoch 4/1000 
	 loss: 64.3385, MinusLogProbMetric: 64.3385, val_loss: 64.2339, val_MinusLogProbMetric: 64.2339

Epoch 4: val_loss improved from 64.84077 to 64.23389, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 68s - loss: 64.3385 - MinusLogProbMetric: 64.3385 - val_loss: 64.2339 - val_MinusLogProbMetric: 64.2339 - lr: 4.1152e-06 - 68s/epoch - 348ms/step
Epoch 5/1000
2023-10-24 13:48:56.462 
Epoch 5/1000 
	 loss: 63.9434, MinusLogProbMetric: 63.9434, val_loss: 66.3086, val_MinusLogProbMetric: 66.3086

Epoch 5: val_loss did not improve from 64.23389
196/196 - 69s - loss: 63.9434 - MinusLogProbMetric: 63.9434 - val_loss: 66.3086 - val_MinusLogProbMetric: 66.3086 - lr: 4.1152e-06 - 69s/epoch - 351ms/step
Epoch 6/1000
2023-10-24 13:50:02.717 
Epoch 6/1000 
	 loss: 63.1035, MinusLogProbMetric: 63.1035, val_loss: 62.9668, val_MinusLogProbMetric: 62.9668

Epoch 6: val_loss improved from 64.23389 to 62.96675, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 67s - loss: 63.1035 - MinusLogProbMetric: 63.1035 - val_loss: 62.9668 - val_MinusLogProbMetric: 62.9668 - lr: 4.1152e-06 - 67s/epoch - 343ms/step
Epoch 7/1000
2023-10-24 13:51:09.802 
Epoch 7/1000 
	 loss: 62.6263, MinusLogProbMetric: 62.6263, val_loss: 63.2005, val_MinusLogProbMetric: 63.2005

Epoch 7: val_loss did not improve from 62.96675
196/196 - 66s - loss: 62.6263 - MinusLogProbMetric: 62.6263 - val_loss: 63.2005 - val_MinusLogProbMetric: 63.2005 - lr: 4.1152e-06 - 66s/epoch - 337ms/step
Epoch 8/1000
2023-10-24 13:52:16.085 
Epoch 8/1000 
	 loss: 62.6105, MinusLogProbMetric: 62.6105, val_loss: 63.3620, val_MinusLogProbMetric: 63.3620

Epoch 8: val_loss did not improve from 62.96675
196/196 - 66s - loss: 62.6105 - MinusLogProbMetric: 62.6105 - val_loss: 63.3620 - val_MinusLogProbMetric: 63.3620 - lr: 4.1152e-06 - 66s/epoch - 338ms/step
Epoch 9/1000
2023-10-24 13:53:20.896 
Epoch 9/1000 
	 loss: 62.4515, MinusLogProbMetric: 62.4515, val_loss: 62.9226, val_MinusLogProbMetric: 62.9226

Epoch 9: val_loss improved from 62.96675 to 62.92257, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 66s - loss: 62.4515 - MinusLogProbMetric: 62.4515 - val_loss: 62.9226 - val_MinusLogProbMetric: 62.9226 - lr: 4.1152e-06 - 66s/epoch - 336ms/step
Epoch 10/1000
2023-10-24 13:54:24.866 
Epoch 10/1000 
	 loss: 62.1768, MinusLogProbMetric: 62.1768, val_loss: 61.5777, val_MinusLogProbMetric: 61.5777

Epoch 10: val_loss improved from 62.92257 to 61.57768, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 64s - loss: 62.1768 - MinusLogProbMetric: 62.1768 - val_loss: 61.5777 - val_MinusLogProbMetric: 61.5777 - lr: 4.1152e-06 - 64s/epoch - 326ms/step
Epoch 11/1000
2023-10-24 13:55:29.577 
Epoch 11/1000 
	 loss: 63.3376, MinusLogProbMetric: 63.3376, val_loss: 63.0946, val_MinusLogProbMetric: 63.0946

Epoch 11: val_loss did not improve from 61.57768
196/196 - 64s - loss: 63.3376 - MinusLogProbMetric: 63.3376 - val_loss: 63.0946 - val_MinusLogProbMetric: 63.0946 - lr: 4.1152e-06 - 64s/epoch - 326ms/step
Epoch 12/1000
2023-10-24 13:56:37.000 
Epoch 12/1000 
	 loss: 62.3480, MinusLogProbMetric: 62.3480, val_loss: 64.5206, val_MinusLogProbMetric: 64.5206

Epoch 12: val_loss did not improve from 61.57768
196/196 - 67s - loss: 62.3480 - MinusLogProbMetric: 62.3480 - val_loss: 64.5206 - val_MinusLogProbMetric: 64.5206 - lr: 4.1152e-06 - 67s/epoch - 344ms/step
Epoch 13/1000
2023-10-24 13:57:40.297 
Epoch 13/1000 
	 loss: 61.8074, MinusLogProbMetric: 61.8074, val_loss: 62.4422, val_MinusLogProbMetric: 62.4422

Epoch 13: val_loss did not improve from 61.57768
196/196 - 63s - loss: 61.8074 - MinusLogProbMetric: 61.8074 - val_loss: 62.4422 - val_MinusLogProbMetric: 62.4422 - lr: 4.1152e-06 - 63s/epoch - 323ms/step
Epoch 14/1000
2023-10-24 13:58:46.245 
Epoch 14/1000 
	 loss: 61.2117, MinusLogProbMetric: 61.2117, val_loss: 61.1106, val_MinusLogProbMetric: 61.1106

Epoch 14: val_loss improved from 61.57768 to 61.11065, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 67s - loss: 61.2117 - MinusLogProbMetric: 61.2117 - val_loss: 61.1106 - val_MinusLogProbMetric: 61.1106 - lr: 4.1152e-06 - 67s/epoch - 341ms/step
Epoch 15/1000
2023-10-24 13:59:52.432 
Epoch 15/1000 
	 loss: 60.7792, MinusLogProbMetric: 60.7792, val_loss: 61.0164, val_MinusLogProbMetric: 61.0164

Epoch 15: val_loss improved from 61.11065 to 61.01640, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 66s - loss: 60.7792 - MinusLogProbMetric: 60.7792 - val_loss: 61.0164 - val_MinusLogProbMetric: 61.0164 - lr: 4.1152e-06 - 66s/epoch - 338ms/step
Epoch 16/1000
2023-10-24 14:00:56.555 
Epoch 16/1000 
	 loss: 60.5482, MinusLogProbMetric: 60.5482, val_loss: 60.5196, val_MinusLogProbMetric: 60.5196

Epoch 16: val_loss improved from 61.01640 to 60.51958, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 64s - loss: 60.5482 - MinusLogProbMetric: 60.5482 - val_loss: 60.5196 - val_MinusLogProbMetric: 60.5196 - lr: 4.1152e-06 - 64s/epoch - 327ms/step
Epoch 17/1000
2023-10-24 14:02:03.338 
Epoch 17/1000 
	 loss: 60.3825, MinusLogProbMetric: 60.3825, val_loss: 60.1176, val_MinusLogProbMetric: 60.1176

Epoch 17: val_loss improved from 60.51958 to 60.11757, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 67s - loss: 60.3825 - MinusLogProbMetric: 60.3825 - val_loss: 60.1176 - val_MinusLogProbMetric: 60.1176 - lr: 4.1152e-06 - 67s/epoch - 341ms/step
Epoch 18/1000
2023-10-24 14:03:12.043 
Epoch 18/1000 
	 loss: 64.3269, MinusLogProbMetric: 64.3269, val_loss: 62.3188, val_MinusLogProbMetric: 62.3188

Epoch 18: val_loss did not improve from 60.11757
196/196 - 68s - loss: 64.3269 - MinusLogProbMetric: 64.3269 - val_loss: 62.3188 - val_MinusLogProbMetric: 62.3188 - lr: 4.1152e-06 - 68s/epoch - 346ms/step
Epoch 19/1000
2023-10-24 14:04:18.743 
Epoch 19/1000 
	 loss: 60.1740, MinusLogProbMetric: 60.1740, val_loss: 60.3935, val_MinusLogProbMetric: 60.3935

Epoch 19: val_loss did not improve from 60.11757
196/196 - 67s - loss: 60.1740 - MinusLogProbMetric: 60.1740 - val_loss: 60.3935 - val_MinusLogProbMetric: 60.3935 - lr: 4.1152e-06 - 67s/epoch - 340ms/step
Epoch 20/1000
2023-10-24 14:05:25.808 
Epoch 20/1000 
	 loss: 59.7621, MinusLogProbMetric: 59.7621, val_loss: 59.9048, val_MinusLogProbMetric: 59.9048

Epoch 20: val_loss improved from 60.11757 to 59.90485, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 68s - loss: 59.7621 - MinusLogProbMetric: 59.7621 - val_loss: 59.9048 - val_MinusLogProbMetric: 59.9048 - lr: 4.1152e-06 - 68s/epoch - 347ms/step
Epoch 21/1000
2023-10-24 14:06:31.851 
Epoch 21/1000 
	 loss: 59.4556, MinusLogProbMetric: 59.4556, val_loss: 59.5052, val_MinusLogProbMetric: 59.5052

Epoch 21: val_loss improved from 59.90485 to 59.50523, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 66s - loss: 59.4556 - MinusLogProbMetric: 59.4556 - val_loss: 59.5052 - val_MinusLogProbMetric: 59.5052 - lr: 4.1152e-06 - 66s/epoch - 338ms/step
Epoch 22/1000
2023-10-24 14:07:40.065 
Epoch 22/1000 
	 loss: 59.2218, MinusLogProbMetric: 59.2218, val_loss: 59.6196, val_MinusLogProbMetric: 59.6196

Epoch 22: val_loss did not improve from 59.50523
196/196 - 67s - loss: 59.2218 - MinusLogProbMetric: 59.2218 - val_loss: 59.6196 - val_MinusLogProbMetric: 59.6196 - lr: 4.1152e-06 - 67s/epoch - 343ms/step
Epoch 23/1000
2023-10-24 14:08:45.308 
Epoch 23/1000 
	 loss: 59.1860, MinusLogProbMetric: 59.1860, val_loss: 59.1737, val_MinusLogProbMetric: 59.1737

Epoch 23: val_loss improved from 59.50523 to 59.17366, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 66s - loss: 59.1860 - MinusLogProbMetric: 59.1860 - val_loss: 59.1737 - val_MinusLogProbMetric: 59.1737 - lr: 4.1152e-06 - 66s/epoch - 337ms/step
Epoch 24/1000
2023-10-24 14:09:50.025 
Epoch 24/1000 
	 loss: 58.8929, MinusLogProbMetric: 58.8929, val_loss: 58.9558, val_MinusLogProbMetric: 58.9558

Epoch 24: val_loss improved from 59.17366 to 58.95585, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 65s - loss: 58.8929 - MinusLogProbMetric: 58.8929 - val_loss: 58.9558 - val_MinusLogProbMetric: 58.9558 - lr: 4.1152e-06 - 65s/epoch - 331ms/step
Epoch 25/1000
2023-10-24 14:10:58.735 
Epoch 25/1000 
	 loss: 58.8524, MinusLogProbMetric: 58.8524, val_loss: 59.0600, val_MinusLogProbMetric: 59.0600

Epoch 25: val_loss did not improve from 58.95585
196/196 - 68s - loss: 58.8524 - MinusLogProbMetric: 58.8524 - val_loss: 59.0600 - val_MinusLogProbMetric: 59.0600 - lr: 4.1152e-06 - 68s/epoch - 345ms/step
Epoch 26/1000
2023-10-24 14:11:52.245 
Epoch 26/1000 
	 loss: 58.7334, MinusLogProbMetric: 58.7334, val_loss: 58.8154, val_MinusLogProbMetric: 58.8154

Epoch 26: val_loss improved from 58.95585 to 58.81540, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 54s - loss: 58.7334 - MinusLogProbMetric: 58.7334 - val_loss: 58.8154 - val_MinusLogProbMetric: 58.8154 - lr: 4.1152e-06 - 54s/epoch - 278ms/step
Epoch 27/1000
2023-10-24 14:12:43.860 
Epoch 27/1000 
	 loss: 58.8975, MinusLogProbMetric: 58.8975, val_loss: 58.4434, val_MinusLogProbMetric: 58.4434

Epoch 27: val_loss improved from 58.81540 to 58.44336, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 52s - loss: 58.8975 - MinusLogProbMetric: 58.8975 - val_loss: 58.4434 - val_MinusLogProbMetric: 58.4434 - lr: 4.1152e-06 - 52s/epoch - 264ms/step
Epoch 28/1000
2023-10-24 14:13:41.050 
Epoch 28/1000 
	 loss: 58.0934, MinusLogProbMetric: 58.0934, val_loss: 59.3979, val_MinusLogProbMetric: 59.3979

Epoch 28: val_loss did not improve from 58.44336
196/196 - 56s - loss: 58.0934 - MinusLogProbMetric: 58.0934 - val_loss: 59.3979 - val_MinusLogProbMetric: 59.3979 - lr: 4.1152e-06 - 56s/epoch - 286ms/step
Epoch 29/1000
2023-10-24 14:14:31.716 
Epoch 29/1000 
	 loss: 58.0103, MinusLogProbMetric: 58.0103, val_loss: 66.0343, val_MinusLogProbMetric: 66.0343

Epoch 29: val_loss did not improve from 58.44336
196/196 - 51s - loss: 58.0103 - MinusLogProbMetric: 58.0103 - val_loss: 66.0343 - val_MinusLogProbMetric: 66.0343 - lr: 4.1152e-06 - 51s/epoch - 258ms/step
Epoch 30/1000
2023-10-24 14:15:22.294 
Epoch 30/1000 
	 loss: 59.3935, MinusLogProbMetric: 59.3935, val_loss: 75.2271, val_MinusLogProbMetric: 75.2271

Epoch 30: val_loss did not improve from 58.44336
196/196 - 51s - loss: 59.3935 - MinusLogProbMetric: 59.3935 - val_loss: 75.2271 - val_MinusLogProbMetric: 75.2271 - lr: 4.1152e-06 - 51s/epoch - 258ms/step
Epoch 31/1000
2023-10-24 14:16:16.712 
Epoch 31/1000 
	 loss: 59.4918, MinusLogProbMetric: 59.4918, val_loss: 58.9367, val_MinusLogProbMetric: 58.9367

Epoch 31: val_loss did not improve from 58.44336
196/196 - 54s - loss: 59.4918 - MinusLogProbMetric: 59.4918 - val_loss: 58.9367 - val_MinusLogProbMetric: 58.9367 - lr: 4.1152e-06 - 54s/epoch - 278ms/step
Epoch 32/1000
2023-10-24 14:17:07.254 
Epoch 32/1000 
	 loss: 57.8103, MinusLogProbMetric: 57.8103, val_loss: 58.0148, val_MinusLogProbMetric: 58.0148

Epoch 32: val_loss improved from 58.44336 to 58.01477, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 51s - loss: 57.8103 - MinusLogProbMetric: 57.8103 - val_loss: 58.0148 - val_MinusLogProbMetric: 58.0148 - lr: 4.1152e-06 - 51s/epoch - 262ms/step
Epoch 33/1000
2023-10-24 14:17:58.982 
Epoch 33/1000 
	 loss: 57.4654, MinusLogProbMetric: 57.4654, val_loss: 58.2846, val_MinusLogProbMetric: 58.2846

Epoch 33: val_loss did not improve from 58.01477
196/196 - 51s - loss: 57.4654 - MinusLogProbMetric: 57.4654 - val_loss: 58.2846 - val_MinusLogProbMetric: 58.2846 - lr: 4.1152e-06 - 51s/epoch - 260ms/step
Epoch 34/1000
2023-10-24 14:19:00.703 
Epoch 34/1000 
	 loss: 59.0272, MinusLogProbMetric: 59.0272, val_loss: 59.1444, val_MinusLogProbMetric: 59.1444

Epoch 34: val_loss did not improve from 58.01477
196/196 - 62s - loss: 59.0272 - MinusLogProbMetric: 59.0272 - val_loss: 59.1444 - val_MinusLogProbMetric: 59.1444 - lr: 4.1152e-06 - 62s/epoch - 315ms/step
Epoch 35/1000
2023-10-24 14:19:51.334 
Epoch 35/1000 
	 loss: 57.2791, MinusLogProbMetric: 57.2791, val_loss: 57.5372, val_MinusLogProbMetric: 57.5372

Epoch 35: val_loss improved from 58.01477 to 57.53716, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 51s - loss: 57.2791 - MinusLogProbMetric: 57.2791 - val_loss: 57.5372 - val_MinusLogProbMetric: 57.5372 - lr: 4.1152e-06 - 51s/epoch - 263ms/step
Epoch 36/1000
2023-10-24 14:20:43.529 
Epoch 36/1000 
	 loss: 57.0036, MinusLogProbMetric: 57.0036, val_loss: 57.2592, val_MinusLogProbMetric: 57.2592

Epoch 36: val_loss improved from 57.53716 to 57.25921, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 52s - loss: 57.0036 - MinusLogProbMetric: 57.0036 - val_loss: 57.2592 - val_MinusLogProbMetric: 57.2592 - lr: 4.1152e-06 - 52s/epoch - 266ms/step
Epoch 37/1000
2023-10-24 14:21:39.635 
Epoch 37/1000 
	 loss: 56.7414, MinusLogProbMetric: 56.7414, val_loss: 57.0006, val_MinusLogProbMetric: 57.0006

Epoch 37: val_loss improved from 57.25921 to 57.00060, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 56s - loss: 56.7414 - MinusLogProbMetric: 56.7414 - val_loss: 57.0006 - val_MinusLogProbMetric: 57.0006 - lr: 4.1152e-06 - 56s/epoch - 286ms/step
Epoch 38/1000
2023-10-24 14:22:30.639 
Epoch 38/1000 
	 loss: 56.6986, MinusLogProbMetric: 56.6986, val_loss: 56.8249, val_MinusLogProbMetric: 56.8249

Epoch 38: val_loss improved from 57.00060 to 56.82492, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 51s - loss: 56.6986 - MinusLogProbMetric: 56.6986 - val_loss: 56.8249 - val_MinusLogProbMetric: 56.8249 - lr: 4.1152e-06 - 51s/epoch - 261ms/step
Epoch 39/1000
2023-10-24 14:23:23.330 
Epoch 39/1000 
	 loss: 56.4442, MinusLogProbMetric: 56.4442, val_loss: 57.0711, val_MinusLogProbMetric: 57.0711

Epoch 39: val_loss did not improve from 56.82492
196/196 - 52s - loss: 56.4442 - MinusLogProbMetric: 56.4442 - val_loss: 57.0711 - val_MinusLogProbMetric: 57.0711 - lr: 4.1152e-06 - 52s/epoch - 265ms/step
Epoch 40/1000
2023-10-24 14:24:19.640 
Epoch 40/1000 
	 loss: 56.3509, MinusLogProbMetric: 56.3509, val_loss: 56.9379, val_MinusLogProbMetric: 56.9379

Epoch 40: val_loss did not improve from 56.82492
196/196 - 56s - loss: 56.3509 - MinusLogProbMetric: 56.3509 - val_loss: 56.9379 - val_MinusLogProbMetric: 56.9379 - lr: 4.1152e-06 - 56s/epoch - 287ms/step
Epoch 41/1000
2023-10-24 14:25:10.115 
Epoch 41/1000 
	 loss: 56.6926, MinusLogProbMetric: 56.6926, val_loss: 58.1329, val_MinusLogProbMetric: 58.1329

Epoch 41: val_loss did not improve from 56.82492
196/196 - 50s - loss: 56.6926 - MinusLogProbMetric: 56.6926 - val_loss: 58.1329 - val_MinusLogProbMetric: 58.1329 - lr: 4.1152e-06 - 50s/epoch - 258ms/step
Epoch 42/1000
2023-10-24 14:26:03.017 
Epoch 42/1000 
	 loss: 56.0719, MinusLogProbMetric: 56.0719, val_loss: 56.6088, val_MinusLogProbMetric: 56.6088

Epoch 42: val_loss improved from 56.82492 to 56.60881, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 54s - loss: 56.0719 - MinusLogProbMetric: 56.0719 - val_loss: 56.6088 - val_MinusLogProbMetric: 56.6088 - lr: 4.1152e-06 - 54s/epoch - 275ms/step
Epoch 43/1000
2023-10-24 14:26:59.530 
Epoch 43/1000 
	 loss: 57.3376, MinusLogProbMetric: 57.3376, val_loss: 56.1337, val_MinusLogProbMetric: 56.1337

Epoch 43: val_loss improved from 56.60881 to 56.13371, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 56s - loss: 57.3376 - MinusLogProbMetric: 57.3376 - val_loss: 56.1337 - val_MinusLogProbMetric: 56.1337 - lr: 4.1152e-06 - 56s/epoch - 287ms/step
Epoch 44/1000
2023-10-24 14:27:50.649 
Epoch 44/1000 
	 loss: 55.8358, MinusLogProbMetric: 55.8358, val_loss: 56.0762, val_MinusLogProbMetric: 56.0762

Epoch 44: val_loss improved from 56.13371 to 56.07618, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 51s - loss: 55.8358 - MinusLogProbMetric: 55.8358 - val_loss: 56.0762 - val_MinusLogProbMetric: 56.0762 - lr: 4.1152e-06 - 51s/epoch - 261ms/step
Epoch 45/1000
2023-10-24 14:28:44.403 
Epoch 45/1000 
	 loss: 55.7091, MinusLogProbMetric: 55.7091, val_loss: 56.1511, val_MinusLogProbMetric: 56.1511

Epoch 45: val_loss did not improve from 56.07618
196/196 - 53s - loss: 55.7091 - MinusLogProbMetric: 55.7091 - val_loss: 56.1511 - val_MinusLogProbMetric: 56.1511 - lr: 4.1152e-06 - 53s/epoch - 271ms/step
Epoch 46/1000
2023-10-24 14:29:38.951 
Epoch 46/1000 
	 loss: 55.7117, MinusLogProbMetric: 55.7117, val_loss: 56.1659, val_MinusLogProbMetric: 56.1659

Epoch 46: val_loss did not improve from 56.07618
196/196 - 55s - loss: 55.7117 - MinusLogProbMetric: 55.7117 - val_loss: 56.1659 - val_MinusLogProbMetric: 56.1659 - lr: 4.1152e-06 - 55s/epoch - 278ms/step
Epoch 47/1000
2023-10-24 14:30:29.228 
Epoch 47/1000 
	 loss: 55.9573, MinusLogProbMetric: 55.9573, val_loss: 55.6251, val_MinusLogProbMetric: 55.6251

Epoch 47: val_loss improved from 56.07618 to 55.62514, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 51s - loss: 55.9573 - MinusLogProbMetric: 55.9573 - val_loss: 55.6251 - val_MinusLogProbMetric: 55.6251 - lr: 4.1152e-06 - 51s/epoch - 260ms/step
Epoch 48/1000
2023-10-24 14:31:24.027 
Epoch 48/1000 
	 loss: 55.3659, MinusLogProbMetric: 55.3659, val_loss: 56.5968, val_MinusLogProbMetric: 56.5968

Epoch 48: val_loss did not improve from 55.62514
196/196 - 54s - loss: 55.3659 - MinusLogProbMetric: 55.3659 - val_loss: 56.5968 - val_MinusLogProbMetric: 56.5968 - lr: 4.1152e-06 - 54s/epoch - 276ms/step
Epoch 49/1000
2023-10-24 14:32:19.698 
Epoch 49/1000 
	 loss: 55.5536, MinusLogProbMetric: 55.5536, val_loss: 55.7421, val_MinusLogProbMetric: 55.7421

Epoch 49: val_loss did not improve from 55.62514
196/196 - 56s - loss: 55.5536 - MinusLogProbMetric: 55.5536 - val_loss: 55.7421 - val_MinusLogProbMetric: 55.7421 - lr: 4.1152e-06 - 56s/epoch - 284ms/step
Epoch 50/1000
2023-10-24 14:33:10.036 
Epoch 50/1000 
	 loss: 55.0489, MinusLogProbMetric: 55.0489, val_loss: 55.3400, val_MinusLogProbMetric: 55.3400

Epoch 50: val_loss improved from 55.62514 to 55.33996, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 51s - loss: 55.0489 - MinusLogProbMetric: 55.0489 - val_loss: 55.3400 - val_MinusLogProbMetric: 55.3400 - lr: 4.1152e-06 - 51s/epoch - 262ms/step
Epoch 51/1000
2023-10-24 14:34:05.084 
Epoch 51/1000 
	 loss: 54.9551, MinusLogProbMetric: 54.9551, val_loss: 55.5988, val_MinusLogProbMetric: 55.5988

Epoch 51: val_loss did not improve from 55.33996
196/196 - 54s - loss: 54.9551 - MinusLogProbMetric: 54.9551 - val_loss: 55.5988 - val_MinusLogProbMetric: 55.5988 - lr: 4.1152e-06 - 54s/epoch - 276ms/step
Epoch 52/1000
2023-10-24 14:34:59.475 
Epoch 52/1000 
	 loss: 55.7749, MinusLogProbMetric: 55.7749, val_loss: 54.8714, val_MinusLogProbMetric: 54.8714

Epoch 52: val_loss improved from 55.33996 to 54.87142, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 55s - loss: 55.7749 - MinusLogProbMetric: 55.7749 - val_loss: 54.8714 - val_MinusLogProbMetric: 54.8714 - lr: 4.1152e-06 - 55s/epoch - 283ms/step
Epoch 53/1000
2023-10-24 14:35:50.081 
Epoch 53/1000 
	 loss: 54.6044, MinusLogProbMetric: 54.6044, val_loss: 54.9470, val_MinusLogProbMetric: 54.9470

Epoch 53: val_loss did not improve from 54.87142
196/196 - 50s - loss: 54.6044 - MinusLogProbMetric: 54.6044 - val_loss: 54.9470 - val_MinusLogProbMetric: 54.9470 - lr: 4.1152e-06 - 50s/epoch - 253ms/step
Epoch 54/1000
2023-10-24 14:36:41.554 
Epoch 54/1000 
	 loss: 54.4596, MinusLogProbMetric: 54.4596, val_loss: 54.7454, val_MinusLogProbMetric: 54.7454

Epoch 54: val_loss improved from 54.87142 to 54.74543, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 52s - loss: 54.4596 - MinusLogProbMetric: 54.4596 - val_loss: 54.7454 - val_MinusLogProbMetric: 54.7454 - lr: 4.1152e-06 - 52s/epoch - 267ms/step
Epoch 55/1000
2023-10-24 14:37:36.853 
Epoch 55/1000 
	 loss: 54.4385, MinusLogProbMetric: 54.4385, val_loss: 55.1410, val_MinusLogProbMetric: 55.1410

Epoch 55: val_loss did not improve from 54.74543
196/196 - 54s - loss: 54.4385 - MinusLogProbMetric: 54.4385 - val_loss: 55.1410 - val_MinusLogProbMetric: 55.1410 - lr: 4.1152e-06 - 54s/epoch - 278ms/step
Epoch 56/1000
2023-10-24 14:38:26.109 
Epoch 56/1000 
	 loss: 54.5477, MinusLogProbMetric: 54.5477, val_loss: 57.8223, val_MinusLogProbMetric: 57.8223

Epoch 56: val_loss did not improve from 54.74543
196/196 - 49s - loss: 54.5477 - MinusLogProbMetric: 54.5477 - val_loss: 57.8223 - val_MinusLogProbMetric: 57.8223 - lr: 4.1152e-06 - 49s/epoch - 251ms/step
Epoch 57/1000
2023-10-24 14:39:17.968 
Epoch 57/1000 
	 loss: 54.2526, MinusLogProbMetric: 54.2526, val_loss: 54.7189, val_MinusLogProbMetric: 54.7189

Epoch 57: val_loss improved from 54.74543 to 54.71888, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 53s - loss: 54.2526 - MinusLogProbMetric: 54.2526 - val_loss: 54.7189 - val_MinusLogProbMetric: 54.7189 - lr: 4.1152e-06 - 53s/epoch - 269ms/step
Epoch 58/1000
2023-10-24 14:40:13.270 
Epoch 58/1000 
	 loss: 54.9860, MinusLogProbMetric: 54.9860, val_loss: 54.2549, val_MinusLogProbMetric: 54.2549

Epoch 58: val_loss improved from 54.71888 to 54.25488, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 55s - loss: 54.9860 - MinusLogProbMetric: 54.9860 - val_loss: 54.2549 - val_MinusLogProbMetric: 54.2549 - lr: 4.1152e-06 - 55s/epoch - 282ms/step
Epoch 59/1000
2023-10-24 14:41:02.370 
Epoch 59/1000 
	 loss: 53.8185, MinusLogProbMetric: 53.8185, val_loss: 54.0392, val_MinusLogProbMetric: 54.0392

Epoch 59: val_loss improved from 54.25488 to 54.03925, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 49s - loss: 53.8185 - MinusLogProbMetric: 53.8185 - val_loss: 54.0392 - val_MinusLogProbMetric: 54.0392 - lr: 4.1152e-06 - 49s/epoch - 251ms/step
Epoch 60/1000
2023-10-24 14:41:52.654 
Epoch 60/1000 
	 loss: 53.8235, MinusLogProbMetric: 53.8235, val_loss: 53.8118, val_MinusLogProbMetric: 53.8118

Epoch 60: val_loss improved from 54.03925 to 53.81175, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 51s - loss: 53.8235 - MinusLogProbMetric: 53.8235 - val_loss: 53.8118 - val_MinusLogProbMetric: 53.8118 - lr: 4.1152e-06 - 51s/epoch - 258ms/step
Epoch 61/1000
2023-10-24 14:42:49.055 
Epoch 61/1000 
	 loss: 55.9636, MinusLogProbMetric: 55.9636, val_loss: 54.1179, val_MinusLogProbMetric: 54.1179

Epoch 61: val_loss did not improve from 53.81175
196/196 - 55s - loss: 55.9636 - MinusLogProbMetric: 55.9636 - val_loss: 54.1179 - val_MinusLogProbMetric: 54.1179 - lr: 4.1152e-06 - 55s/epoch - 282ms/step
Epoch 62/1000
2023-10-24 14:43:37.136 
Epoch 62/1000 
	 loss: 53.5750, MinusLogProbMetric: 53.5750, val_loss: 54.2636, val_MinusLogProbMetric: 54.2636

Epoch 62: val_loss did not improve from 53.81175
196/196 - 48s - loss: 53.5750 - MinusLogProbMetric: 53.5750 - val_loss: 54.2636 - val_MinusLogProbMetric: 54.2636 - lr: 4.1152e-06 - 48s/epoch - 245ms/step
Epoch 63/1000
2023-10-24 14:44:24.781 
Epoch 63/1000 
	 loss: 53.3997, MinusLogProbMetric: 53.3997, val_loss: 53.6693, val_MinusLogProbMetric: 53.6693

Epoch 63: val_loss improved from 53.81175 to 53.66935, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 49s - loss: 53.3997 - MinusLogProbMetric: 53.3997 - val_loss: 53.6693 - val_MinusLogProbMetric: 53.6693 - lr: 4.1152e-06 - 49s/epoch - 248ms/step
Epoch 64/1000
2023-10-24 14:45:19.764 
Epoch 64/1000 
	 loss: 53.6039, MinusLogProbMetric: 53.6039, val_loss: 53.9151, val_MinusLogProbMetric: 53.9151

Epoch 64: val_loss did not improve from 53.66935
196/196 - 54s - loss: 53.6039 - MinusLogProbMetric: 53.6039 - val_loss: 53.9151 - val_MinusLogProbMetric: 53.9151 - lr: 4.1152e-06 - 54s/epoch - 276ms/step
Epoch 65/1000
2023-10-24 14:46:10.902 
Epoch 65/1000 
	 loss: 54.2969, MinusLogProbMetric: 54.2969, val_loss: 53.9400, val_MinusLogProbMetric: 53.9400

Epoch 65: val_loss did not improve from 53.66935
196/196 - 51s - loss: 54.2969 - MinusLogProbMetric: 54.2969 - val_loss: 53.9400 - val_MinusLogProbMetric: 53.9400 - lr: 4.1152e-06 - 51s/epoch - 261ms/step
Epoch 66/1000
2023-10-24 14:46:59.281 
Epoch 66/1000 
	 loss: 53.8900, MinusLogProbMetric: 53.8900, val_loss: 53.8269, val_MinusLogProbMetric: 53.8269

Epoch 66: val_loss did not improve from 53.66935
196/196 - 48s - loss: 53.8900 - MinusLogProbMetric: 53.8900 - val_loss: 53.8269 - val_MinusLogProbMetric: 53.8269 - lr: 4.1152e-06 - 48s/epoch - 247ms/step
Epoch 67/1000
2023-10-24 14:47:51.998 
Epoch 67/1000 
	 loss: 53.0356, MinusLogProbMetric: 53.0356, val_loss: 54.0616, val_MinusLogProbMetric: 54.0616

Epoch 67: val_loss did not improve from 53.66935
196/196 - 53s - loss: 53.0356 - MinusLogProbMetric: 53.0356 - val_loss: 54.0616 - val_MinusLogProbMetric: 54.0616 - lr: 4.1152e-06 - 53s/epoch - 269ms/step
Epoch 68/1000
2023-10-24 14:48:53.988 
Epoch 68/1000 
	 loss: 53.0157, MinusLogProbMetric: 53.0157, val_loss: 53.8642, val_MinusLogProbMetric: 53.8642

Epoch 68: val_loss did not improve from 53.66935
196/196 - 62s - loss: 53.0157 - MinusLogProbMetric: 53.0157 - val_loss: 53.8642 - val_MinusLogProbMetric: 53.8642 - lr: 4.1152e-06 - 62s/epoch - 316ms/step
Epoch 69/1000
2023-10-24 14:49:56.229 
Epoch 69/1000 
	 loss: 56.3226, MinusLogProbMetric: 56.3226, val_loss: 53.4868, val_MinusLogProbMetric: 53.4868

Epoch 69: val_loss improved from 53.66935 to 53.48683, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 63s - loss: 56.3226 - MinusLogProbMetric: 56.3226 - val_loss: 53.4868 - val_MinusLogProbMetric: 53.4868 - lr: 4.1152e-06 - 63s/epoch - 323ms/step
Epoch 70/1000
2023-10-24 14:50:58.491 
Epoch 70/1000 
	 loss: 52.8550, MinusLogProbMetric: 52.8550, val_loss: 53.2608, val_MinusLogProbMetric: 53.2608

Epoch 70: val_loss improved from 53.48683 to 53.26083, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 62s - loss: 52.8550 - MinusLogProbMetric: 52.8550 - val_loss: 53.2608 - val_MinusLogProbMetric: 53.2608 - lr: 4.1152e-06 - 62s/epoch - 317ms/step
Epoch 71/1000
2023-10-24 14:52:00.039 
Epoch 71/1000 
	 loss: 52.6902, MinusLogProbMetric: 52.6902, val_loss: 53.2442, val_MinusLogProbMetric: 53.2442

Epoch 71: val_loss improved from 53.26083 to 53.24417, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 61s - loss: 52.6902 - MinusLogProbMetric: 52.6902 - val_loss: 53.2442 - val_MinusLogProbMetric: 53.2442 - lr: 4.1152e-06 - 61s/epoch - 313ms/step
Epoch 72/1000
2023-10-24 14:53:03.706 
Epoch 72/1000 
	 loss: 52.6238, MinusLogProbMetric: 52.6238, val_loss: 53.3670, val_MinusLogProbMetric: 53.3670

Epoch 72: val_loss did not improve from 53.24417
196/196 - 63s - loss: 52.6238 - MinusLogProbMetric: 52.6238 - val_loss: 53.3670 - val_MinusLogProbMetric: 53.3670 - lr: 4.1152e-06 - 63s/epoch - 320ms/step
Epoch 73/1000
2023-10-24 14:54:00.807 
Epoch 73/1000 
	 loss: 52.5293, MinusLogProbMetric: 52.5293, val_loss: 52.6316, val_MinusLogProbMetric: 52.6316

Epoch 73: val_loss improved from 53.24417 to 52.63157, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 58s - loss: 52.5293 - MinusLogProbMetric: 52.5293 - val_loss: 52.6316 - val_MinusLogProbMetric: 52.6316 - lr: 4.1152e-06 - 58s/epoch - 295ms/step
Epoch 74/1000
2023-10-24 14:54:50.344 
Epoch 74/1000 
	 loss: 52.3000, MinusLogProbMetric: 52.3000, val_loss: 52.7391, val_MinusLogProbMetric: 52.7391

Epoch 74: val_loss did not improve from 52.63157
196/196 - 49s - loss: 52.3000 - MinusLogProbMetric: 52.3000 - val_loss: 52.7391 - val_MinusLogProbMetric: 52.7391 - lr: 4.1152e-06 - 49s/epoch - 249ms/step
Epoch 75/1000
2023-10-24 14:55:41.762 
Epoch 75/1000 
	 loss: 52.1697, MinusLogProbMetric: 52.1697, val_loss: 52.6725, val_MinusLogProbMetric: 52.6725

Epoch 75: val_loss did not improve from 52.63157
196/196 - 51s - loss: 52.1697 - MinusLogProbMetric: 52.1697 - val_loss: 52.6725 - val_MinusLogProbMetric: 52.6725 - lr: 4.1152e-06 - 51s/epoch - 262ms/step
Epoch 76/1000
2023-10-24 14:56:42.276 
Epoch 76/1000 
	 loss: 53.8551, MinusLogProbMetric: 53.8551, val_loss: 56.4413, val_MinusLogProbMetric: 56.4413

Epoch 76: val_loss did not improve from 52.63157
196/196 - 61s - loss: 53.8551 - MinusLogProbMetric: 53.8551 - val_loss: 56.4413 - val_MinusLogProbMetric: 56.4413 - lr: 4.1152e-06 - 61s/epoch - 309ms/step
Epoch 77/1000
2023-10-24 14:57:40.887 
Epoch 77/1000 
	 loss: 52.3096, MinusLogProbMetric: 52.3096, val_loss: 52.5922, val_MinusLogProbMetric: 52.5922

Epoch 77: val_loss improved from 52.63157 to 52.59225, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 60s - loss: 52.3096 - MinusLogProbMetric: 52.3096 - val_loss: 52.5922 - val_MinusLogProbMetric: 52.5922 - lr: 4.1152e-06 - 60s/epoch - 304ms/step
Epoch 78/1000
2023-10-24 14:58:44.757 
Epoch 78/1000 
	 loss: 51.8973, MinusLogProbMetric: 51.8973, val_loss: 52.8332, val_MinusLogProbMetric: 52.8332

Epoch 78: val_loss did not improve from 52.59225
196/196 - 63s - loss: 51.8973 - MinusLogProbMetric: 51.8973 - val_loss: 52.8332 - val_MinusLogProbMetric: 52.8332 - lr: 4.1152e-06 - 63s/epoch - 321ms/step
Epoch 79/1000
2023-10-24 14:59:44.894 
Epoch 79/1000 
	 loss: 51.9095, MinusLogProbMetric: 51.9095, val_loss: 52.6908, val_MinusLogProbMetric: 52.6908

Epoch 79: val_loss did not improve from 52.59225
196/196 - 60s - loss: 51.9095 - MinusLogProbMetric: 51.9095 - val_loss: 52.6908 - val_MinusLogProbMetric: 52.6908 - lr: 4.1152e-06 - 60s/epoch - 307ms/step
Epoch 80/1000
2023-10-24 15:00:46.984 
Epoch 80/1000 
	 loss: 52.1381, MinusLogProbMetric: 52.1381, val_loss: 52.4443, val_MinusLogProbMetric: 52.4443

Epoch 80: val_loss improved from 52.59225 to 52.44428, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 63s - loss: 52.1381 - MinusLogProbMetric: 52.1381 - val_loss: 52.4443 - val_MinusLogProbMetric: 52.4443 - lr: 4.1152e-06 - 63s/epoch - 321ms/step
Epoch 81/1000
2023-10-24 15:01:47.820 
Epoch 81/1000 
	 loss: 51.7873, MinusLogProbMetric: 51.7873, val_loss: 52.3265, val_MinusLogProbMetric: 52.3265

Epoch 81: val_loss improved from 52.44428 to 52.32655, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 61s - loss: 51.7873 - MinusLogProbMetric: 51.7873 - val_loss: 52.3265 - val_MinusLogProbMetric: 52.3265 - lr: 4.1152e-06 - 61s/epoch - 311ms/step
Epoch 82/1000
2023-10-24 15:02:53.044 
Epoch 82/1000 
	 loss: 51.6992, MinusLogProbMetric: 51.6992, val_loss: 52.0156, val_MinusLogProbMetric: 52.0156

Epoch 82: val_loss improved from 52.32655 to 52.01559, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 65s - loss: 51.6992 - MinusLogProbMetric: 51.6992 - val_loss: 52.0156 - val_MinusLogProbMetric: 52.0156 - lr: 4.1152e-06 - 65s/epoch - 333ms/step
Epoch 83/1000
2023-10-24 15:03:56.620 
Epoch 83/1000 
	 loss: 52.5211, MinusLogProbMetric: 52.5211, val_loss: 52.1698, val_MinusLogProbMetric: 52.1698

Epoch 83: val_loss did not improve from 52.01559
196/196 - 63s - loss: 52.5211 - MinusLogProbMetric: 52.5211 - val_loss: 52.1698 - val_MinusLogProbMetric: 52.1698 - lr: 4.1152e-06 - 63s/epoch - 320ms/step
Epoch 84/1000
2023-10-24 15:04:44.764 
Epoch 84/1000 
	 loss: 51.4420, MinusLogProbMetric: 51.4420, val_loss: 51.6398, val_MinusLogProbMetric: 51.6398

Epoch 84: val_loss improved from 52.01559 to 51.63981, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 49s - loss: 51.4420 - MinusLogProbMetric: 51.4420 - val_loss: 51.6398 - val_MinusLogProbMetric: 51.6398 - lr: 4.1152e-06 - 49s/epoch - 250ms/step
Epoch 85/1000
2023-10-24 15:05:34.133 
Epoch 85/1000 
	 loss: 51.4144, MinusLogProbMetric: 51.4144, val_loss: 51.9615, val_MinusLogProbMetric: 51.9615

Epoch 85: val_loss did not improve from 51.63981
196/196 - 49s - loss: 51.4144 - MinusLogProbMetric: 51.4144 - val_loss: 51.9615 - val_MinusLogProbMetric: 51.9615 - lr: 4.1152e-06 - 49s/epoch - 248ms/step
Epoch 86/1000
2023-10-24 15:06:37.313 
Epoch 86/1000 
	 loss: 51.2241, MinusLogProbMetric: 51.2241, val_loss: 51.9360, val_MinusLogProbMetric: 51.9360

Epoch 86: val_loss did not improve from 51.63981
196/196 - 63s - loss: 51.2241 - MinusLogProbMetric: 51.2241 - val_loss: 51.9360 - val_MinusLogProbMetric: 51.9360 - lr: 4.1152e-06 - 63s/epoch - 322ms/step
Epoch 87/1000
2023-10-24 15:07:38.272 
Epoch 87/1000 
	 loss: 51.3644, MinusLogProbMetric: 51.3644, val_loss: 52.3731, val_MinusLogProbMetric: 52.3731

Epoch 87: val_loss did not improve from 51.63981
196/196 - 61s - loss: 51.3644 - MinusLogProbMetric: 51.3644 - val_loss: 52.3731 - val_MinusLogProbMetric: 52.3731 - lr: 4.1152e-06 - 61s/epoch - 311ms/step
Epoch 88/1000
2023-10-24 15:08:44.180 
Epoch 88/1000 
	 loss: 51.4695, MinusLogProbMetric: 51.4695, val_loss: 52.3116, val_MinusLogProbMetric: 52.3116

Epoch 88: val_loss did not improve from 51.63981
196/196 - 66s - loss: 51.4695 - MinusLogProbMetric: 51.4695 - val_loss: 52.3116 - val_MinusLogProbMetric: 52.3116 - lr: 4.1152e-06 - 66s/epoch - 336ms/step
Epoch 89/1000
2023-10-24 15:09:44.382 
Epoch 89/1000 
	 loss: 51.0781, MinusLogProbMetric: 51.0781, val_loss: 127.6670, val_MinusLogProbMetric: 127.6670

Epoch 89: val_loss did not improve from 51.63981
196/196 - 60s - loss: 51.0781 - MinusLogProbMetric: 51.0781 - val_loss: 127.6670 - val_MinusLogProbMetric: 127.6670 - lr: 4.1152e-06 - 60s/epoch - 307ms/step
Epoch 90/1000
2023-10-24 15:10:47.175 
Epoch 90/1000 
	 loss: 70.9602, MinusLogProbMetric: 70.9602, val_loss: 56.9170, val_MinusLogProbMetric: 56.9170

Epoch 90: val_loss did not improve from 51.63981
196/196 - 63s - loss: 70.9602 - MinusLogProbMetric: 70.9602 - val_loss: 56.9170 - val_MinusLogProbMetric: 56.9170 - lr: 4.1152e-06 - 63s/epoch - 320ms/step
Epoch 91/1000
2023-10-24 15:11:49.721 
Epoch 91/1000 
	 loss: 55.2222, MinusLogProbMetric: 55.2222, val_loss: 54.0302, val_MinusLogProbMetric: 54.0302

Epoch 91: val_loss did not improve from 51.63981
196/196 - 63s - loss: 55.2222 - MinusLogProbMetric: 55.2222 - val_loss: 54.0302 - val_MinusLogProbMetric: 54.0302 - lr: 4.1152e-06 - 63s/epoch - 319ms/step
Epoch 92/1000
2023-10-24 15:12:52.456 
Epoch 92/1000 
	 loss: 53.2974, MinusLogProbMetric: 53.2974, val_loss: 52.9361, val_MinusLogProbMetric: 52.9361

Epoch 92: val_loss did not improve from 51.63981
196/196 - 63s - loss: 53.2974 - MinusLogProbMetric: 53.2974 - val_loss: 52.9361 - val_MinusLogProbMetric: 52.9361 - lr: 4.1152e-06 - 63s/epoch - 320ms/step
Epoch 93/1000
2023-10-24 15:13:54.571 
Epoch 93/1000 
	 loss: 52.6899, MinusLogProbMetric: 52.6899, val_loss: 53.0390, val_MinusLogProbMetric: 53.0390

Epoch 93: val_loss did not improve from 51.63981
196/196 - 62s - loss: 52.6899 - MinusLogProbMetric: 52.6899 - val_loss: 53.0390 - val_MinusLogProbMetric: 53.0390 - lr: 4.1152e-06 - 62s/epoch - 317ms/step
Epoch 94/1000
2023-10-24 15:14:56.023 
Epoch 94/1000 
	 loss: 52.4242, MinusLogProbMetric: 52.4242, val_loss: 54.2512, val_MinusLogProbMetric: 54.2512

Epoch 94: val_loss did not improve from 51.63981
196/196 - 61s - loss: 52.4242 - MinusLogProbMetric: 52.4242 - val_loss: 54.2512 - val_MinusLogProbMetric: 54.2512 - lr: 4.1152e-06 - 61s/epoch - 314ms/step
Epoch 95/1000
2023-10-24 15:16:00.134 
Epoch 95/1000 
	 loss: 52.1035, MinusLogProbMetric: 52.1035, val_loss: 52.3787, val_MinusLogProbMetric: 52.3787

Epoch 95: val_loss did not improve from 51.63981
196/196 - 64s - loss: 52.1035 - MinusLogProbMetric: 52.1035 - val_loss: 52.3787 - val_MinusLogProbMetric: 52.3787 - lr: 4.1152e-06 - 64s/epoch - 327ms/step
Epoch 96/1000
2023-10-24 15:17:02.222 
Epoch 96/1000 
	 loss: 54.7706, MinusLogProbMetric: 54.7706, val_loss: 53.0312, val_MinusLogProbMetric: 53.0312

Epoch 96: val_loss did not improve from 51.63981
196/196 - 62s - loss: 54.7706 - MinusLogProbMetric: 54.7706 - val_loss: 53.0312 - val_MinusLogProbMetric: 53.0312 - lr: 4.1152e-06 - 62s/epoch - 317ms/step
Epoch 97/1000
2023-10-24 15:18:02.043 
Epoch 97/1000 
	 loss: 51.5918, MinusLogProbMetric: 51.5918, val_loss: 51.2591, val_MinusLogProbMetric: 51.2591

Epoch 97: val_loss improved from 51.63981 to 51.25908, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 61s - loss: 51.5918 - MinusLogProbMetric: 51.5918 - val_loss: 51.2591 - val_MinusLogProbMetric: 51.2591 - lr: 4.1152e-06 - 61s/epoch - 309ms/step
Epoch 98/1000
2023-10-24 15:19:06.370 
Epoch 98/1000 
	 loss: 50.9290, MinusLogProbMetric: 50.9290, val_loss: 51.3256, val_MinusLogProbMetric: 51.3256

Epoch 98: val_loss did not improve from 51.25908
196/196 - 64s - loss: 50.9290 - MinusLogProbMetric: 50.9290 - val_loss: 51.3256 - val_MinusLogProbMetric: 51.3256 - lr: 4.1152e-06 - 64s/epoch - 324ms/step
Epoch 99/1000
2023-10-24 15:20:09.527 
Epoch 99/1000 
	 loss: 50.7828, MinusLogProbMetric: 50.7828, val_loss: 51.5011, val_MinusLogProbMetric: 51.5011

Epoch 99: val_loss did not improve from 51.25908
196/196 - 63s - loss: 50.7828 - MinusLogProbMetric: 50.7828 - val_loss: 51.5011 - val_MinusLogProbMetric: 51.5011 - lr: 4.1152e-06 - 63s/epoch - 322ms/step
Epoch 100/1000
2023-10-24 15:21:13.698 
Epoch 100/1000 
	 loss: 50.6322, MinusLogProbMetric: 50.6322, val_loss: 51.1081, val_MinusLogProbMetric: 51.1081

Epoch 100: val_loss improved from 51.25908 to 51.10815, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 65s - loss: 50.6322 - MinusLogProbMetric: 50.6322 - val_loss: 51.1081 - val_MinusLogProbMetric: 51.1081 - lr: 4.1152e-06 - 65s/epoch - 333ms/step
Epoch 101/1000
2023-10-24 15:22:18.486 
Epoch 101/1000 
	 loss: 50.4856, MinusLogProbMetric: 50.4856, val_loss: 50.9648, val_MinusLogProbMetric: 50.9648

Epoch 101: val_loss improved from 51.10815 to 50.96477, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 65s - loss: 50.4856 - MinusLogProbMetric: 50.4856 - val_loss: 50.9648 - val_MinusLogProbMetric: 50.9648 - lr: 4.1152e-06 - 65s/epoch - 330ms/step
Epoch 102/1000
2023-10-24 15:23:23.955 
Epoch 102/1000 
	 loss: 51.0527, MinusLogProbMetric: 51.0527, val_loss: 53.5183, val_MinusLogProbMetric: 53.5183

Epoch 102: val_loss did not improve from 50.96477
196/196 - 65s - loss: 51.0527 - MinusLogProbMetric: 51.0527 - val_loss: 53.5183 - val_MinusLogProbMetric: 53.5183 - lr: 4.1152e-06 - 65s/epoch - 330ms/step
Epoch 103/1000
2023-10-24 15:24:27.000 
Epoch 103/1000 
	 loss: 50.3503, MinusLogProbMetric: 50.3503, val_loss: 50.4070, val_MinusLogProbMetric: 50.4070

Epoch 103: val_loss improved from 50.96477 to 50.40699, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 64s - loss: 50.3503 - MinusLogProbMetric: 50.3503 - val_loss: 50.4070 - val_MinusLogProbMetric: 50.4070 - lr: 4.1152e-06 - 64s/epoch - 326ms/step
Epoch 104/1000
2023-10-24 15:25:31.071 
Epoch 104/1000 
	 loss: 50.3495, MinusLogProbMetric: 50.3495, val_loss: 50.3850, val_MinusLogProbMetric: 50.3850

Epoch 104: val_loss improved from 50.40699 to 50.38500, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 64s - loss: 50.3495 - MinusLogProbMetric: 50.3495 - val_loss: 50.3850 - val_MinusLogProbMetric: 50.3850 - lr: 4.1152e-06 - 64s/epoch - 326ms/step
Epoch 105/1000
2023-10-24 15:26:36.117 
Epoch 105/1000 
	 loss: 50.0493, MinusLogProbMetric: 50.0493, val_loss: 50.4116, val_MinusLogProbMetric: 50.4116

Epoch 105: val_loss did not improve from 50.38500
196/196 - 64s - loss: 50.0493 - MinusLogProbMetric: 50.0493 - val_loss: 50.4116 - val_MinusLogProbMetric: 50.4116 - lr: 4.1152e-06 - 64s/epoch - 328ms/step
Epoch 106/1000
2023-10-24 15:27:42.147 
Epoch 106/1000 
	 loss: 49.8189, MinusLogProbMetric: 49.8189, val_loss: 50.4537, val_MinusLogProbMetric: 50.4537

Epoch 106: val_loss did not improve from 50.38500
196/196 - 66s - loss: 49.8189 - MinusLogProbMetric: 49.8189 - val_loss: 50.4537 - val_MinusLogProbMetric: 50.4537 - lr: 4.1152e-06 - 66s/epoch - 337ms/step
Epoch 107/1000
2023-10-24 15:28:45.317 
Epoch 107/1000 
	 loss: 49.8361, MinusLogProbMetric: 49.8361, val_loss: 50.0358, val_MinusLogProbMetric: 50.0358

Epoch 107: val_loss improved from 50.38500 to 50.03577, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 64s - loss: 49.8361 - MinusLogProbMetric: 49.8361 - val_loss: 50.0358 - val_MinusLogProbMetric: 50.0358 - lr: 4.1152e-06 - 64s/epoch - 326ms/step
Epoch 108/1000
2023-10-24 15:29:49.611 
Epoch 108/1000 
	 loss: 49.6656, MinusLogProbMetric: 49.6656, val_loss: 50.1749, val_MinusLogProbMetric: 50.1749

Epoch 108: val_loss did not improve from 50.03577
196/196 - 63s - loss: 49.6656 - MinusLogProbMetric: 49.6656 - val_loss: 50.1749 - val_MinusLogProbMetric: 50.1749 - lr: 4.1152e-06 - 63s/epoch - 324ms/step
Epoch 109/1000
2023-10-24 15:30:53.227 
Epoch 109/1000 
	 loss: 49.8026, MinusLogProbMetric: 49.8026, val_loss: 50.0043, val_MinusLogProbMetric: 50.0043

Epoch 109: val_loss improved from 50.03577 to 50.00426, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 64s - loss: 49.8026 - MinusLogProbMetric: 49.8026 - val_loss: 50.0043 - val_MinusLogProbMetric: 50.0043 - lr: 4.1152e-06 - 64s/epoch - 329ms/step
Epoch 110/1000
2023-10-24 15:31:54.151 
Epoch 110/1000 
	 loss: 49.4263, MinusLogProbMetric: 49.4263, val_loss: 49.7683, val_MinusLogProbMetric: 49.7683

Epoch 110: val_loss improved from 50.00426 to 49.76827, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 61s - loss: 49.4263 - MinusLogProbMetric: 49.4263 - val_loss: 49.7683 - val_MinusLogProbMetric: 49.7683 - lr: 4.1152e-06 - 61s/epoch - 310ms/step
Epoch 111/1000
2023-10-24 15:32:58.249 
Epoch 111/1000 
	 loss: 49.4992, MinusLogProbMetric: 49.4992, val_loss: 49.7457, val_MinusLogProbMetric: 49.7457

Epoch 111: val_loss improved from 49.76827 to 49.74566, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 64s - loss: 49.4992 - MinusLogProbMetric: 49.4992 - val_loss: 49.7457 - val_MinusLogProbMetric: 49.7457 - lr: 4.1152e-06 - 64s/epoch - 328ms/step
Epoch 112/1000
2023-10-24 15:34:03.512 
Epoch 112/1000 
	 loss: 49.3282, MinusLogProbMetric: 49.3282, val_loss: 49.7338, val_MinusLogProbMetric: 49.7338

Epoch 112: val_loss improved from 49.74566 to 49.73378, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 65s - loss: 49.3282 - MinusLogProbMetric: 49.3282 - val_loss: 49.7338 - val_MinusLogProbMetric: 49.7338 - lr: 4.1152e-06 - 65s/epoch - 333ms/step
Epoch 113/1000
2023-10-24 15:35:06.631 
Epoch 113/1000 
	 loss: 49.2710, MinusLogProbMetric: 49.2710, val_loss: 49.7741, val_MinusLogProbMetric: 49.7741

Epoch 113: val_loss did not improve from 49.73378
196/196 - 62s - loss: 49.2710 - MinusLogProbMetric: 49.2710 - val_loss: 49.7741 - val_MinusLogProbMetric: 49.7741 - lr: 4.1152e-06 - 62s/epoch - 318ms/step
Epoch 114/1000
2023-10-24 15:36:09.917 
Epoch 114/1000 
	 loss: 50.0484, MinusLogProbMetric: 50.0484, val_loss: 50.2429, val_MinusLogProbMetric: 50.2429

Epoch 114: val_loss did not improve from 49.73378
196/196 - 63s - loss: 50.0484 - MinusLogProbMetric: 50.0484 - val_loss: 50.2429 - val_MinusLogProbMetric: 50.2429 - lr: 4.1152e-06 - 63s/epoch - 323ms/step
Epoch 115/1000
2023-10-24 15:37:14.251 
Epoch 115/1000 
	 loss: 49.1574, MinusLogProbMetric: 49.1574, val_loss: 49.6882, val_MinusLogProbMetric: 49.6882

Epoch 115: val_loss improved from 49.73378 to 49.68824, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 65s - loss: 49.1574 - MinusLogProbMetric: 49.1574 - val_loss: 49.6882 - val_MinusLogProbMetric: 49.6882 - lr: 4.1152e-06 - 65s/epoch - 333ms/step
Epoch 116/1000
2023-10-24 15:38:20.633 
Epoch 116/1000 
	 loss: 49.0763, MinusLogProbMetric: 49.0763, val_loss: 49.6040, val_MinusLogProbMetric: 49.6040

Epoch 116: val_loss improved from 49.68824 to 49.60405, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 66s - loss: 49.0763 - MinusLogProbMetric: 49.0763 - val_loss: 49.6040 - val_MinusLogProbMetric: 49.6040 - lr: 4.1152e-06 - 66s/epoch - 338ms/step
Epoch 117/1000
2023-10-24 15:39:26.304 
Epoch 117/1000 
	 loss: 48.9753, MinusLogProbMetric: 48.9753, val_loss: 49.2794, val_MinusLogProbMetric: 49.2794

Epoch 117: val_loss improved from 49.60405 to 49.27940, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 66s - loss: 48.9753 - MinusLogProbMetric: 48.9753 - val_loss: 49.2794 - val_MinusLogProbMetric: 49.2794 - lr: 4.1152e-06 - 66s/epoch - 336ms/step
Epoch 118/1000
2023-10-24 15:40:30.888 
Epoch 118/1000 
	 loss: 49.0733, MinusLogProbMetric: 49.0733, val_loss: 49.3259, val_MinusLogProbMetric: 49.3259

Epoch 118: val_loss did not improve from 49.27940
196/196 - 64s - loss: 49.0733 - MinusLogProbMetric: 49.0733 - val_loss: 49.3259 - val_MinusLogProbMetric: 49.3259 - lr: 4.1152e-06 - 64s/epoch - 325ms/step
Epoch 119/1000
2023-10-24 15:41:32.856 
Epoch 119/1000 
	 loss: 48.8125, MinusLogProbMetric: 48.8125, val_loss: 49.1356, val_MinusLogProbMetric: 49.1356

Epoch 119: val_loss improved from 49.27940 to 49.13556, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 63s - loss: 48.8125 - MinusLogProbMetric: 48.8125 - val_loss: 49.1356 - val_MinusLogProbMetric: 49.1356 - lr: 4.1152e-06 - 63s/epoch - 320ms/step
Epoch 120/1000
2023-10-24 15:42:30.885 
Epoch 120/1000 
	 loss: 50.4409, MinusLogProbMetric: 50.4409, val_loss: 49.3238, val_MinusLogProbMetric: 49.3238

Epoch 120: val_loss did not improve from 49.13556
196/196 - 57s - loss: 50.4409 - MinusLogProbMetric: 50.4409 - val_loss: 49.3238 - val_MinusLogProbMetric: 49.3238 - lr: 4.1152e-06 - 57s/epoch - 292ms/step
Epoch 121/1000
2023-10-24 15:43:34.084 
Epoch 121/1000 
	 loss: 48.7001, MinusLogProbMetric: 48.7001, val_loss: 49.0947, val_MinusLogProbMetric: 49.0947

Epoch 121: val_loss improved from 49.13556 to 49.09470, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 64s - loss: 48.7001 - MinusLogProbMetric: 48.7001 - val_loss: 49.0947 - val_MinusLogProbMetric: 49.0947 - lr: 4.1152e-06 - 64s/epoch - 327ms/step
Epoch 122/1000
2023-10-24 15:44:39.260 
Epoch 122/1000 
	 loss: 48.5687, MinusLogProbMetric: 48.5687, val_loss: 49.0283, val_MinusLogProbMetric: 49.0283

Epoch 122: val_loss improved from 49.09470 to 49.02826, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 65s - loss: 48.5687 - MinusLogProbMetric: 48.5687 - val_loss: 49.0283 - val_MinusLogProbMetric: 49.0283 - lr: 4.1152e-06 - 65s/epoch - 333ms/step
Epoch 123/1000
2023-10-24 15:45:44.648 
Epoch 123/1000 
	 loss: 48.5760, MinusLogProbMetric: 48.5760, val_loss: 49.4082, val_MinusLogProbMetric: 49.4082

Epoch 123: val_loss did not improve from 49.02826
196/196 - 65s - loss: 48.5760 - MinusLogProbMetric: 48.5760 - val_loss: 49.4082 - val_MinusLogProbMetric: 49.4082 - lr: 4.1152e-06 - 65s/epoch - 329ms/step
Epoch 124/1000
2023-10-24 15:46:50.460 
Epoch 124/1000 
	 loss: 48.5968, MinusLogProbMetric: 48.5968, val_loss: 48.9921, val_MinusLogProbMetric: 48.9921

Epoch 124: val_loss improved from 49.02826 to 48.99213, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 67s - loss: 48.5968 - MinusLogProbMetric: 48.5968 - val_loss: 48.9921 - val_MinusLogProbMetric: 48.9921 - lr: 4.1152e-06 - 67s/epoch - 340ms/step
Epoch 125/1000
2023-10-24 15:47:53.143 
Epoch 125/1000 
	 loss: 48.4979, MinusLogProbMetric: 48.4979, val_loss: 48.8632, val_MinusLogProbMetric: 48.8632

Epoch 125: val_loss improved from 48.99213 to 48.86316, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 63s - loss: 48.4979 - MinusLogProbMetric: 48.4979 - val_loss: 48.8632 - val_MinusLogProbMetric: 48.8632 - lr: 4.1152e-06 - 63s/epoch - 320ms/step
Epoch 126/1000
2023-10-24 15:48:58.424 
Epoch 126/1000 
	 loss: 48.3489, MinusLogProbMetric: 48.3489, val_loss: 48.7150, val_MinusLogProbMetric: 48.7150

Epoch 126: val_loss improved from 48.86316 to 48.71503, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 66s - loss: 48.3489 - MinusLogProbMetric: 48.3489 - val_loss: 48.7150 - val_MinusLogProbMetric: 48.7150 - lr: 4.1152e-06 - 66s/epoch - 335ms/step
Epoch 127/1000
2023-10-24 15:50:03.402 
Epoch 127/1000 
	 loss: 48.4344, MinusLogProbMetric: 48.4344, val_loss: 48.5960, val_MinusLogProbMetric: 48.5960

Epoch 127: val_loss improved from 48.71503 to 48.59602, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 65s - loss: 48.4344 - MinusLogProbMetric: 48.4344 - val_loss: 48.5960 - val_MinusLogProbMetric: 48.5960 - lr: 4.1152e-06 - 65s/epoch - 329ms/step
Epoch 128/1000
2023-10-24 15:51:09.199 
Epoch 128/1000 
	 loss: 48.2066, MinusLogProbMetric: 48.2066, val_loss: 48.5925, val_MinusLogProbMetric: 48.5925

Epoch 128: val_loss improved from 48.59602 to 48.59251, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 66s - loss: 48.2066 - MinusLogProbMetric: 48.2066 - val_loss: 48.5925 - val_MinusLogProbMetric: 48.5925 - lr: 4.1152e-06 - 66s/epoch - 337ms/step
Epoch 129/1000
2023-10-24 15:52:12.410 
Epoch 129/1000 
	 loss: 49.4203, MinusLogProbMetric: 49.4203, val_loss: 48.9959, val_MinusLogProbMetric: 48.9959

Epoch 129: val_loss did not improve from 48.59251
196/196 - 62s - loss: 49.4203 - MinusLogProbMetric: 49.4203 - val_loss: 48.9959 - val_MinusLogProbMetric: 48.9959 - lr: 4.1152e-06 - 62s/epoch - 318ms/step
Epoch 130/1000
2023-10-24 15:53:17.321 
Epoch 130/1000 
	 loss: 48.1595, MinusLogProbMetric: 48.1595, val_loss: 49.1751, val_MinusLogProbMetric: 49.1751

Epoch 130: val_loss did not improve from 48.59251
196/196 - 65s - loss: 48.1595 - MinusLogProbMetric: 48.1595 - val_loss: 49.1751 - val_MinusLogProbMetric: 49.1751 - lr: 4.1152e-06 - 65s/epoch - 331ms/step
Epoch 131/1000
2023-10-24 15:54:18.355 
Epoch 131/1000 
	 loss: 48.0902, MinusLogProbMetric: 48.0902, val_loss: 48.5798, val_MinusLogProbMetric: 48.5798

Epoch 131: val_loss improved from 48.59251 to 48.57980, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 62s - loss: 48.0902 - MinusLogProbMetric: 48.0902 - val_loss: 48.5798 - val_MinusLogProbMetric: 48.5798 - lr: 4.1152e-06 - 62s/epoch - 317ms/step
Epoch 132/1000
2023-10-24 15:55:22.309 
Epoch 132/1000 
	 loss: 48.3425, MinusLogProbMetric: 48.3425, val_loss: 48.4266, val_MinusLogProbMetric: 48.4266

Epoch 132: val_loss improved from 48.57980 to 48.42661, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 64s - loss: 48.3425 - MinusLogProbMetric: 48.3425 - val_loss: 48.4266 - val_MinusLogProbMetric: 48.4266 - lr: 4.1152e-06 - 64s/epoch - 325ms/step
Epoch 133/1000
2023-10-24 15:56:28.190 
Epoch 133/1000 
	 loss: 47.9711, MinusLogProbMetric: 47.9711, val_loss: 48.3162, val_MinusLogProbMetric: 48.3162

Epoch 133: val_loss improved from 48.42661 to 48.31618, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 66s - loss: 47.9711 - MinusLogProbMetric: 47.9711 - val_loss: 48.3162 - val_MinusLogProbMetric: 48.3162 - lr: 4.1152e-06 - 66s/epoch - 336ms/step
Epoch 134/1000
2023-10-24 15:57:32.276 
Epoch 134/1000 
	 loss: 47.9365, MinusLogProbMetric: 47.9365, val_loss: 48.4115, val_MinusLogProbMetric: 48.4115

Epoch 134: val_loss did not improve from 48.31618
196/196 - 63s - loss: 47.9365 - MinusLogProbMetric: 47.9365 - val_loss: 48.4115 - val_MinusLogProbMetric: 48.4115 - lr: 4.1152e-06 - 63s/epoch - 322ms/step
Epoch 135/1000
2023-10-24 15:58:34.006 
Epoch 135/1000 
	 loss: 47.8682, MinusLogProbMetric: 47.8682, val_loss: 48.3655, val_MinusLogProbMetric: 48.3655

Epoch 135: val_loss did not improve from 48.31618
196/196 - 62s - loss: 47.8682 - MinusLogProbMetric: 47.8682 - val_loss: 48.3655 - val_MinusLogProbMetric: 48.3655 - lr: 4.1152e-06 - 62s/epoch - 315ms/step
Epoch 136/1000
2023-10-24 15:59:38.603 
Epoch 136/1000 
	 loss: 47.7874, MinusLogProbMetric: 47.7874, val_loss: 48.3019, val_MinusLogProbMetric: 48.3019

Epoch 136: val_loss improved from 48.31618 to 48.30193, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 66s - loss: 47.7874 - MinusLogProbMetric: 47.7874 - val_loss: 48.3019 - val_MinusLogProbMetric: 48.3019 - lr: 4.1152e-06 - 66s/epoch - 334ms/step
Epoch 137/1000
2023-10-24 16:00:42.898 
Epoch 137/1000 
	 loss: 47.7302, MinusLogProbMetric: 47.7302, val_loss: 48.2545, val_MinusLogProbMetric: 48.2545

Epoch 137: val_loss improved from 48.30193 to 48.25448, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 64s - loss: 47.7302 - MinusLogProbMetric: 47.7302 - val_loss: 48.2545 - val_MinusLogProbMetric: 48.2545 - lr: 4.1152e-06 - 64s/epoch - 327ms/step
Epoch 138/1000
2023-10-24 16:01:45.846 
Epoch 138/1000 
	 loss: 47.6677, MinusLogProbMetric: 47.6677, val_loss: 48.0275, val_MinusLogProbMetric: 48.0275

Epoch 138: val_loss improved from 48.25448 to 48.02747, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 63s - loss: 47.6677 - MinusLogProbMetric: 47.6677 - val_loss: 48.0275 - val_MinusLogProbMetric: 48.0275 - lr: 4.1152e-06 - 63s/epoch - 321ms/step
Epoch 139/1000
2023-10-24 16:02:49.487 
Epoch 139/1000 
	 loss: 47.6008, MinusLogProbMetric: 47.6008, val_loss: 48.1996, val_MinusLogProbMetric: 48.1996

Epoch 139: val_loss did not improve from 48.02747
196/196 - 63s - loss: 47.6008 - MinusLogProbMetric: 47.6008 - val_loss: 48.1996 - val_MinusLogProbMetric: 48.1996 - lr: 4.1152e-06 - 63s/epoch - 320ms/step
Epoch 140/1000
2023-10-24 16:03:53.030 
Epoch 140/1000 
	 loss: 48.7913, MinusLogProbMetric: 48.7913, val_loss: 48.2396, val_MinusLogProbMetric: 48.2396

Epoch 140: val_loss did not improve from 48.02747
196/196 - 64s - loss: 48.7913 - MinusLogProbMetric: 48.7913 - val_loss: 48.2396 - val_MinusLogProbMetric: 48.2396 - lr: 4.1152e-06 - 64s/epoch - 324ms/step
Epoch 141/1000
2023-10-24 16:04:57.222 
Epoch 141/1000 
	 loss: 47.5123, MinusLogProbMetric: 47.5123, val_loss: 47.7667, val_MinusLogProbMetric: 47.7667

Epoch 141: val_loss improved from 48.02747 to 47.76673, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 65s - loss: 47.5123 - MinusLogProbMetric: 47.5123 - val_loss: 47.7667 - val_MinusLogProbMetric: 47.7667 - lr: 4.1152e-06 - 65s/epoch - 332ms/step
Epoch 142/1000
2023-10-24 16:06:04.214 
Epoch 142/1000 
	 loss: 47.5065, MinusLogProbMetric: 47.5065, val_loss: 47.8915, val_MinusLogProbMetric: 47.8915

Epoch 142: val_loss did not improve from 47.76673
196/196 - 66s - loss: 47.5065 - MinusLogProbMetric: 47.5065 - val_loss: 47.8915 - val_MinusLogProbMetric: 47.8915 - lr: 4.1152e-06 - 66s/epoch - 337ms/step
Epoch 143/1000
2023-10-24 16:07:08.072 
Epoch 143/1000 
	 loss: 47.4247, MinusLogProbMetric: 47.4247, val_loss: 48.2736, val_MinusLogProbMetric: 48.2736

Epoch 143: val_loss did not improve from 47.76673
196/196 - 64s - loss: 47.4247 - MinusLogProbMetric: 47.4247 - val_loss: 48.2736 - val_MinusLogProbMetric: 48.2736 - lr: 4.1152e-06 - 64s/epoch - 326ms/step
Epoch 144/1000
2023-10-24 16:08:12.470 
Epoch 144/1000 
	 loss: 47.4177, MinusLogProbMetric: 47.4177, val_loss: 48.1122, val_MinusLogProbMetric: 48.1122

Epoch 144: val_loss did not improve from 47.76673
196/196 - 64s - loss: 47.4177 - MinusLogProbMetric: 47.4177 - val_loss: 48.1122 - val_MinusLogProbMetric: 48.1122 - lr: 4.1152e-06 - 64s/epoch - 329ms/step
Epoch 145/1000
2023-10-24 16:09:15.290 
Epoch 145/1000 
	 loss: 47.3074, MinusLogProbMetric: 47.3074, val_loss: 47.9178, val_MinusLogProbMetric: 47.9178

Epoch 145: val_loss did not improve from 47.76673
196/196 - 63s - loss: 47.3074 - MinusLogProbMetric: 47.3074 - val_loss: 47.9178 - val_MinusLogProbMetric: 47.9178 - lr: 4.1152e-06 - 63s/epoch - 320ms/step
Epoch 146/1000
2023-10-24 16:10:19.044 
Epoch 146/1000 
	 loss: 47.9217, MinusLogProbMetric: 47.9217, val_loss: 47.6244, val_MinusLogProbMetric: 47.6244

Epoch 146: val_loss improved from 47.76673 to 47.62443, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 65s - loss: 47.9217 - MinusLogProbMetric: 47.9217 - val_loss: 47.6244 - val_MinusLogProbMetric: 47.6244 - lr: 4.1152e-06 - 65s/epoch - 330ms/step
Epoch 147/1000
2023-10-24 16:11:22.310 
Epoch 147/1000 
	 loss: 47.2501, MinusLogProbMetric: 47.2501, val_loss: 47.8776, val_MinusLogProbMetric: 47.8776

Epoch 147: val_loss did not improve from 47.62443
196/196 - 62s - loss: 47.2501 - MinusLogProbMetric: 47.2501 - val_loss: 47.8776 - val_MinusLogProbMetric: 47.8776 - lr: 4.1152e-06 - 62s/epoch - 318ms/step
Epoch 148/1000
2023-10-24 16:12:24.126 
Epoch 148/1000 
	 loss: 47.1245, MinusLogProbMetric: 47.1245, val_loss: 47.6731, val_MinusLogProbMetric: 47.6731

Epoch 148: val_loss did not improve from 47.62443
196/196 - 62s - loss: 47.1245 - MinusLogProbMetric: 47.1245 - val_loss: 47.6731 - val_MinusLogProbMetric: 47.6731 - lr: 4.1152e-06 - 62s/epoch - 315ms/step
Epoch 149/1000
2023-10-24 16:13:26.345 
Epoch 149/1000 
	 loss: 47.1766, MinusLogProbMetric: 47.1766, val_loss: 47.7605, val_MinusLogProbMetric: 47.7605

Epoch 149: val_loss did not improve from 47.62443
196/196 - 62s - loss: 47.1766 - MinusLogProbMetric: 47.1766 - val_loss: 47.7605 - val_MinusLogProbMetric: 47.7605 - lr: 4.1152e-06 - 62s/epoch - 317ms/step
Epoch 150/1000
2023-10-24 16:14:26.627 
Epoch 150/1000 
	 loss: 47.0993, MinusLogProbMetric: 47.0993, val_loss: 47.4933, val_MinusLogProbMetric: 47.4933

Epoch 150: val_loss improved from 47.62443 to 47.49328, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 61s - loss: 47.0993 - MinusLogProbMetric: 47.0993 - val_loss: 47.4933 - val_MinusLogProbMetric: 47.4933 - lr: 4.1152e-06 - 61s/epoch - 312ms/step
Epoch 151/1000
2023-10-24 16:15:31.378 
Epoch 151/1000 
	 loss: 47.0126, MinusLogProbMetric: 47.0126, val_loss: 47.5915, val_MinusLogProbMetric: 47.5915

Epoch 151: val_loss did not improve from 47.49328
196/196 - 64s - loss: 47.0126 - MinusLogProbMetric: 47.0126 - val_loss: 47.5915 - val_MinusLogProbMetric: 47.5915 - lr: 4.1152e-06 - 64s/epoch - 326ms/step
Epoch 152/1000
2023-10-24 16:16:34.977 
Epoch 152/1000 
	 loss: 47.1003, MinusLogProbMetric: 47.1003, val_loss: 47.3597, val_MinusLogProbMetric: 47.3597

Epoch 152: val_loss improved from 47.49328 to 47.35970, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 64s - loss: 47.1003 - MinusLogProbMetric: 47.1003 - val_loss: 47.3597 - val_MinusLogProbMetric: 47.3597 - lr: 4.1152e-06 - 64s/epoch - 328ms/step
Epoch 153/1000
2023-10-24 16:17:36.197 
Epoch 153/1000 
	 loss: 46.8965, MinusLogProbMetric: 46.8965, val_loss: 47.3261, val_MinusLogProbMetric: 47.3261

Epoch 153: val_loss improved from 47.35970 to 47.32610, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 61s - loss: 46.8965 - MinusLogProbMetric: 46.8965 - val_loss: 47.3261 - val_MinusLogProbMetric: 47.3261 - lr: 4.1152e-06 - 61s/epoch - 312ms/step
Epoch 154/1000
2023-10-24 16:18:39.322 
Epoch 154/1000 
	 loss: 47.1341, MinusLogProbMetric: 47.1341, val_loss: 47.2006, val_MinusLogProbMetric: 47.2006

Epoch 154: val_loss improved from 47.32610 to 47.20063, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 63s - loss: 47.1341 - MinusLogProbMetric: 47.1341 - val_loss: 47.2006 - val_MinusLogProbMetric: 47.2006 - lr: 4.1152e-06 - 63s/epoch - 321ms/step
Epoch 155/1000
2023-10-24 16:19:44.594 
Epoch 155/1000 
	 loss: 46.7574, MinusLogProbMetric: 46.7574, val_loss: 47.3985, val_MinusLogProbMetric: 47.3985

Epoch 155: val_loss did not improve from 47.20063
196/196 - 65s - loss: 46.7574 - MinusLogProbMetric: 46.7574 - val_loss: 47.3985 - val_MinusLogProbMetric: 47.3985 - lr: 4.1152e-06 - 65s/epoch - 330ms/step
Epoch 156/1000
2023-10-24 16:20:45.304 
Epoch 156/1000 
	 loss: 46.7340, MinusLogProbMetric: 46.7340, val_loss: 47.1404, val_MinusLogProbMetric: 47.1404

Epoch 156: val_loss improved from 47.20063 to 47.14040, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 61s - loss: 46.7340 - MinusLogProbMetric: 46.7340 - val_loss: 47.1404 - val_MinusLogProbMetric: 47.1404 - lr: 4.1152e-06 - 61s/epoch - 314ms/step
Epoch 157/1000
2023-10-24 16:21:50.739 
Epoch 157/1000 
	 loss: 46.7566, MinusLogProbMetric: 46.7566, val_loss: 47.4997, val_MinusLogProbMetric: 47.4997

Epoch 157: val_loss did not improve from 47.14040
196/196 - 65s - loss: 46.7566 - MinusLogProbMetric: 46.7566 - val_loss: 47.4997 - val_MinusLogProbMetric: 47.4997 - lr: 4.1152e-06 - 65s/epoch - 330ms/step
Epoch 158/1000
2023-10-24 16:22:56.390 
Epoch 158/1000 
	 loss: 46.8979, MinusLogProbMetric: 46.8979, val_loss: 47.3081, val_MinusLogProbMetric: 47.3081

Epoch 158: val_loss did not improve from 47.14040
196/196 - 66s - loss: 46.8979 - MinusLogProbMetric: 46.8979 - val_loss: 47.3081 - val_MinusLogProbMetric: 47.3081 - lr: 4.1152e-06 - 66s/epoch - 335ms/step
Epoch 159/1000
2023-10-24 16:24:03.839 
Epoch 159/1000 
	 loss: 46.6543, MinusLogProbMetric: 46.6543, val_loss: 46.8258, val_MinusLogProbMetric: 46.8258

Epoch 159: val_loss improved from 47.14040 to 46.82584, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 68s - loss: 46.6543 - MinusLogProbMetric: 46.6543 - val_loss: 46.8258 - val_MinusLogProbMetric: 46.8258 - lr: 4.1152e-06 - 68s/epoch - 348ms/step
Epoch 160/1000
2023-10-24 16:25:06.823 
Epoch 160/1000 
	 loss: 46.6389, MinusLogProbMetric: 46.6389, val_loss: 47.0623, val_MinusLogProbMetric: 47.0623

Epoch 160: val_loss did not improve from 46.82584
196/196 - 62s - loss: 46.6389 - MinusLogProbMetric: 46.6389 - val_loss: 47.0623 - val_MinusLogProbMetric: 47.0623 - lr: 4.1152e-06 - 62s/epoch - 317ms/step
Epoch 161/1000
2023-10-24 16:26:12.231 
Epoch 161/1000 
	 loss: 46.4800, MinusLogProbMetric: 46.4800, val_loss: 47.0609, val_MinusLogProbMetric: 47.0609

Epoch 161: val_loss did not improve from 46.82584
196/196 - 65s - loss: 46.4800 - MinusLogProbMetric: 46.4800 - val_loss: 47.0609 - val_MinusLogProbMetric: 47.0609 - lr: 4.1152e-06 - 65s/epoch - 334ms/step
Epoch 162/1000
2023-10-24 16:27:17.403 
Epoch 162/1000 
	 loss: 46.5079, MinusLogProbMetric: 46.5079, val_loss: 46.9822, val_MinusLogProbMetric: 46.9822

Epoch 162: val_loss did not improve from 46.82584
196/196 - 65s - loss: 46.5079 - MinusLogProbMetric: 46.5079 - val_loss: 46.9822 - val_MinusLogProbMetric: 46.9822 - lr: 4.1152e-06 - 65s/epoch - 332ms/step
Epoch 163/1000
2023-10-24 16:28:19.394 
Epoch 163/1000 
	 loss: 46.4204, MinusLogProbMetric: 46.4204, val_loss: 47.2095, val_MinusLogProbMetric: 47.2095

Epoch 163: val_loss did not improve from 46.82584
196/196 - 62s - loss: 46.4204 - MinusLogProbMetric: 46.4204 - val_loss: 47.2095 - val_MinusLogProbMetric: 47.2095 - lr: 4.1152e-06 - 62s/epoch - 316ms/step
Epoch 164/1000
2023-10-24 16:29:24.011 
Epoch 164/1000 
	 loss: 46.4528, MinusLogProbMetric: 46.4528, val_loss: 46.6544, val_MinusLogProbMetric: 46.6544

Epoch 164: val_loss improved from 46.82584 to 46.65438, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 66s - loss: 46.4528 - MinusLogProbMetric: 46.4528 - val_loss: 46.6544 - val_MinusLogProbMetric: 46.6544 - lr: 4.1152e-06 - 66s/epoch - 334ms/step
Epoch 165/1000
2023-10-24 16:30:30.030 
Epoch 165/1000 
	 loss: 46.4841, MinusLogProbMetric: 46.4841, val_loss: 46.7430, val_MinusLogProbMetric: 46.7430

Epoch 165: val_loss did not improve from 46.65438
196/196 - 65s - loss: 46.4841 - MinusLogProbMetric: 46.4841 - val_loss: 46.7430 - val_MinusLogProbMetric: 46.7430 - lr: 4.1152e-06 - 65s/epoch - 332ms/step
Epoch 166/1000
2023-10-24 16:31:33.169 
Epoch 166/1000 
	 loss: 46.5441, MinusLogProbMetric: 46.5441, val_loss: 47.4154, val_MinusLogProbMetric: 47.4154

Epoch 166: val_loss did not improve from 46.65438
196/196 - 63s - loss: 46.5441 - MinusLogProbMetric: 46.5441 - val_loss: 47.4154 - val_MinusLogProbMetric: 47.4154 - lr: 4.1152e-06 - 63s/epoch - 322ms/step
Epoch 167/1000
2023-10-24 16:32:35.889 
Epoch 167/1000 
	 loss: 46.5393, MinusLogProbMetric: 46.5393, val_loss: 46.6018, val_MinusLogProbMetric: 46.6018

Epoch 167: val_loss improved from 46.65438 to 46.60176, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 64s - loss: 46.5393 - MinusLogProbMetric: 46.5393 - val_loss: 46.6018 - val_MinusLogProbMetric: 46.6018 - lr: 4.1152e-06 - 64s/epoch - 324ms/step
Epoch 168/1000
2023-10-24 16:33:39.692 
Epoch 168/1000 
	 loss: 46.2828, MinusLogProbMetric: 46.2828, val_loss: 47.2516, val_MinusLogProbMetric: 47.2516

Epoch 168: val_loss did not improve from 46.60176
196/196 - 63s - loss: 46.2828 - MinusLogProbMetric: 46.2828 - val_loss: 47.2516 - val_MinusLogProbMetric: 47.2516 - lr: 4.1152e-06 - 63s/epoch - 321ms/step
Epoch 169/1000
2023-10-24 16:34:42.280 
Epoch 169/1000 
	 loss: 46.1639, MinusLogProbMetric: 46.1639, val_loss: 46.8056, val_MinusLogProbMetric: 46.8056

Epoch 169: val_loss did not improve from 46.60176
196/196 - 63s - loss: 46.1639 - MinusLogProbMetric: 46.1639 - val_loss: 46.8056 - val_MinusLogProbMetric: 46.8056 - lr: 4.1152e-06 - 63s/epoch - 319ms/step
Epoch 170/1000
2023-10-24 16:35:47.287 
Epoch 170/1000 
	 loss: 46.1571, MinusLogProbMetric: 46.1571, val_loss: 46.5657, val_MinusLogProbMetric: 46.5657

Epoch 170: val_loss improved from 46.60176 to 46.56572, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 66s - loss: 46.1571 - MinusLogProbMetric: 46.1571 - val_loss: 46.5657 - val_MinusLogProbMetric: 46.5657 - lr: 4.1152e-06 - 66s/epoch - 336ms/step
Epoch 171/1000
2023-10-24 16:36:52.787 
Epoch 171/1000 
	 loss: 46.1048, MinusLogProbMetric: 46.1048, val_loss: 46.6195, val_MinusLogProbMetric: 46.6195

Epoch 171: val_loss did not improve from 46.56572
196/196 - 65s - loss: 46.1048 - MinusLogProbMetric: 46.1048 - val_loss: 46.6195 - val_MinusLogProbMetric: 46.6195 - lr: 4.1152e-06 - 65s/epoch - 329ms/step
Epoch 172/1000
2023-10-24 16:37:57.414 
Epoch 172/1000 
	 loss: 46.2187, MinusLogProbMetric: 46.2187, val_loss: 46.3266, val_MinusLogProbMetric: 46.3266

Epoch 172: val_loss improved from 46.56572 to 46.32655, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 65s - loss: 46.2187 - MinusLogProbMetric: 46.2187 - val_loss: 46.3266 - val_MinusLogProbMetric: 46.3266 - lr: 4.1152e-06 - 65s/epoch - 334ms/step
Epoch 173/1000
2023-10-24 16:39:00.048 
Epoch 173/1000 
	 loss: 45.9057, MinusLogProbMetric: 45.9057, val_loss: 46.1946, val_MinusLogProbMetric: 46.1946

Epoch 173: val_loss improved from 46.32655 to 46.19464, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 62s - loss: 45.9057 - MinusLogProbMetric: 45.9057 - val_loss: 46.1946 - val_MinusLogProbMetric: 46.1946 - lr: 4.1152e-06 - 62s/epoch - 318ms/step
Epoch 174/1000
2023-10-24 16:40:00.596 
Epoch 174/1000 
	 loss: 47.2989, MinusLogProbMetric: 47.2989, val_loss: 46.4272, val_MinusLogProbMetric: 46.4272

Epoch 174: val_loss did not improve from 46.19464
196/196 - 60s - loss: 47.2989 - MinusLogProbMetric: 47.2989 - val_loss: 46.4272 - val_MinusLogProbMetric: 46.4272 - lr: 4.1152e-06 - 60s/epoch - 306ms/step
Epoch 175/1000
2023-10-24 16:41:03.054 
Epoch 175/1000 
	 loss: 46.2160, MinusLogProbMetric: 46.2160, val_loss: 46.4028, val_MinusLogProbMetric: 46.4028

Epoch 175: val_loss did not improve from 46.19464
196/196 - 62s - loss: 46.2160 - MinusLogProbMetric: 46.2160 - val_loss: 46.4028 - val_MinusLogProbMetric: 46.4028 - lr: 4.1152e-06 - 62s/epoch - 319ms/step
Epoch 176/1000
2023-10-24 16:42:05.948 
Epoch 176/1000 
	 loss: 45.7548, MinusLogProbMetric: 45.7548, val_loss: 46.1798, val_MinusLogProbMetric: 46.1798

Epoch 176: val_loss improved from 46.19464 to 46.17979, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 64s - loss: 45.7548 - MinusLogProbMetric: 45.7548 - val_loss: 46.1798 - val_MinusLogProbMetric: 46.1798 - lr: 4.1152e-06 - 64s/epoch - 325ms/step
Epoch 177/1000
2023-10-24 16:43:08.684 
Epoch 177/1000 
	 loss: 45.7156, MinusLogProbMetric: 45.7156, val_loss: 46.3313, val_MinusLogProbMetric: 46.3313

Epoch 177: val_loss did not improve from 46.17979
196/196 - 62s - loss: 45.7156 - MinusLogProbMetric: 45.7156 - val_loss: 46.3313 - val_MinusLogProbMetric: 46.3313 - lr: 4.1152e-06 - 62s/epoch - 316ms/step
Epoch 178/1000
2023-10-24 16:44:11.133 
Epoch 178/1000 
	 loss: 45.8407, MinusLogProbMetric: 45.8407, val_loss: 46.1417, val_MinusLogProbMetric: 46.1417

Epoch 178: val_loss improved from 46.17979 to 46.14174, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 63s - loss: 45.8407 - MinusLogProbMetric: 45.8407 - val_loss: 46.1417 - val_MinusLogProbMetric: 46.1417 - lr: 4.1152e-06 - 63s/epoch - 323ms/step
Epoch 179/1000
2023-10-24 16:45:15.890 
Epoch 179/1000 
	 loss: 45.6711, MinusLogProbMetric: 45.6711, val_loss: 46.0831, val_MinusLogProbMetric: 46.0831

Epoch 179: val_loss improved from 46.14174 to 46.08314, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 65s - loss: 45.6711 - MinusLogProbMetric: 45.6711 - val_loss: 46.0831 - val_MinusLogProbMetric: 46.0831 - lr: 4.1152e-06 - 65s/epoch - 331ms/step
Epoch 180/1000
2023-10-24 16:46:21.362 
Epoch 180/1000 
	 loss: 45.5891, MinusLogProbMetric: 45.5891, val_loss: 46.0124, val_MinusLogProbMetric: 46.0124

Epoch 180: val_loss improved from 46.08314 to 46.01243, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 65s - loss: 45.5891 - MinusLogProbMetric: 45.5891 - val_loss: 46.0124 - val_MinusLogProbMetric: 46.0124 - lr: 4.1152e-06 - 65s/epoch - 334ms/step
Epoch 181/1000
2023-10-24 16:47:26.858 
Epoch 181/1000 
	 loss: 46.8175, MinusLogProbMetric: 46.8175, val_loss: 46.0940, val_MinusLogProbMetric: 46.0940

Epoch 181: val_loss did not improve from 46.01243
196/196 - 65s - loss: 46.8175 - MinusLogProbMetric: 46.8175 - val_loss: 46.0940 - val_MinusLogProbMetric: 46.0940 - lr: 4.1152e-06 - 65s/epoch - 329ms/step
Epoch 182/1000
2023-10-24 16:48:31.855 
Epoch 182/1000 
	 loss: 45.5815, MinusLogProbMetric: 45.5815, val_loss: 46.1235, val_MinusLogProbMetric: 46.1235

Epoch 182: val_loss did not improve from 46.01243
196/196 - 65s - loss: 45.5815 - MinusLogProbMetric: 45.5815 - val_loss: 46.1235 - val_MinusLogProbMetric: 46.1235 - lr: 4.1152e-06 - 65s/epoch - 332ms/step
Epoch 183/1000
2023-10-24 16:49:35.279 
Epoch 183/1000 
	 loss: 45.4691, MinusLogProbMetric: 45.4691, val_loss: 45.8673, val_MinusLogProbMetric: 45.8673

Epoch 183: val_loss improved from 46.01243 to 45.86725, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 64s - loss: 45.4691 - MinusLogProbMetric: 45.4691 - val_loss: 45.8673 - val_MinusLogProbMetric: 45.8673 - lr: 4.1152e-06 - 64s/epoch - 327ms/step
Epoch 184/1000
2023-10-24 16:50:41.131 
Epoch 184/1000 
	 loss: 45.4198, MinusLogProbMetric: 45.4198, val_loss: 46.1034, val_MinusLogProbMetric: 46.1034

Epoch 184: val_loss did not improve from 45.86725
196/196 - 65s - loss: 45.4198 - MinusLogProbMetric: 45.4198 - val_loss: 46.1034 - val_MinusLogProbMetric: 46.1034 - lr: 4.1152e-06 - 65s/epoch - 332ms/step
Epoch 185/1000
2023-10-24 16:51:45.320 
Epoch 185/1000 
	 loss: 45.4018, MinusLogProbMetric: 45.4018, val_loss: 45.8552, val_MinusLogProbMetric: 45.8552

Epoch 185: val_loss improved from 45.86725 to 45.85524, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 65s - loss: 45.4018 - MinusLogProbMetric: 45.4018 - val_loss: 45.8552 - val_MinusLogProbMetric: 45.8552 - lr: 4.1152e-06 - 65s/epoch - 332ms/step
Epoch 186/1000
2023-10-24 16:52:50.093 
Epoch 186/1000 
	 loss: 45.4746, MinusLogProbMetric: 45.4746, val_loss: 46.1636, val_MinusLogProbMetric: 46.1636

Epoch 186: val_loss did not improve from 45.85524
196/196 - 64s - loss: 45.4746 - MinusLogProbMetric: 45.4746 - val_loss: 46.1636 - val_MinusLogProbMetric: 46.1636 - lr: 4.1152e-06 - 64s/epoch - 326ms/step
Epoch 187/1000
2023-10-24 16:53:53.905 
Epoch 187/1000 
	 loss: 45.2907, MinusLogProbMetric: 45.2907, val_loss: 45.8148, val_MinusLogProbMetric: 45.8148

Epoch 187: val_loss improved from 45.85524 to 45.81478, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 65s - loss: 45.2907 - MinusLogProbMetric: 45.2907 - val_loss: 45.8148 - val_MinusLogProbMetric: 45.8148 - lr: 4.1152e-06 - 65s/epoch - 329ms/step
Epoch 188/1000
2023-10-24 16:54:57.282 
Epoch 188/1000 
	 loss: 45.2754, MinusLogProbMetric: 45.2754, val_loss: 45.9647, val_MinusLogProbMetric: 45.9647

Epoch 188: val_loss did not improve from 45.81478
196/196 - 63s - loss: 45.2754 - MinusLogProbMetric: 45.2754 - val_loss: 45.9647 - val_MinusLogProbMetric: 45.9647 - lr: 4.1152e-06 - 63s/epoch - 320ms/step
Epoch 189/1000
2023-10-24 16:56:02.024 
Epoch 189/1000 
	 loss: 46.6747, MinusLogProbMetric: 46.6747, val_loss: 45.5267, val_MinusLogProbMetric: 45.5267

Epoch 189: val_loss improved from 45.81478 to 45.52668, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 66s - loss: 46.6747 - MinusLogProbMetric: 46.6747 - val_loss: 45.5267 - val_MinusLogProbMetric: 45.5267 - lr: 4.1152e-06 - 66s/epoch - 335ms/step
Epoch 190/1000
2023-10-24 16:57:07.208 
Epoch 190/1000 
	 loss: 45.1617, MinusLogProbMetric: 45.1617, val_loss: 46.0978, val_MinusLogProbMetric: 46.0978

Epoch 190: val_loss did not improve from 45.52668
196/196 - 64s - loss: 45.1617 - MinusLogProbMetric: 45.1617 - val_loss: 46.0978 - val_MinusLogProbMetric: 46.0978 - lr: 4.1152e-06 - 64s/epoch - 328ms/step
Epoch 191/1000
2023-10-24 16:58:12.478 
Epoch 191/1000 
	 loss: 45.0755, MinusLogProbMetric: 45.0755, val_loss: 45.4931, val_MinusLogProbMetric: 45.4931

Epoch 191: val_loss improved from 45.52668 to 45.49312, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 66s - loss: 45.0755 - MinusLogProbMetric: 45.0755 - val_loss: 45.4931 - val_MinusLogProbMetric: 45.4931 - lr: 4.1152e-06 - 66s/epoch - 337ms/step
Epoch 192/1000
2023-10-24 16:59:18.003 
Epoch 192/1000 
	 loss: 45.0425, MinusLogProbMetric: 45.0425, val_loss: 45.5587, val_MinusLogProbMetric: 45.5587

Epoch 192: val_loss did not improve from 45.49312
196/196 - 65s - loss: 45.0425 - MinusLogProbMetric: 45.0425 - val_loss: 45.5587 - val_MinusLogProbMetric: 45.5587 - lr: 4.1152e-06 - 65s/epoch - 330ms/step
Epoch 193/1000
2023-10-24 17:00:21.715 
Epoch 193/1000 
	 loss: 45.0757, MinusLogProbMetric: 45.0757, val_loss: 45.5216, val_MinusLogProbMetric: 45.5216

Epoch 193: val_loss did not improve from 45.49312
196/196 - 64s - loss: 45.0757 - MinusLogProbMetric: 45.0757 - val_loss: 45.5216 - val_MinusLogProbMetric: 45.5216 - lr: 4.1152e-06 - 64s/epoch - 325ms/step
Epoch 194/1000
2023-10-24 17:01:24.895 
Epoch 194/1000 
	 loss: 46.9082, MinusLogProbMetric: 46.9082, val_loss: 45.8347, val_MinusLogProbMetric: 45.8347

Epoch 194: val_loss did not improve from 45.49312
196/196 - 63s - loss: 46.9082 - MinusLogProbMetric: 46.9082 - val_loss: 45.8347 - val_MinusLogProbMetric: 45.8347 - lr: 4.1152e-06 - 63s/epoch - 322ms/step
Epoch 195/1000
2023-10-24 17:02:30.816 
Epoch 195/1000 
	 loss: 44.9965, MinusLogProbMetric: 44.9965, val_loss: 45.7210, val_MinusLogProbMetric: 45.7210

Epoch 195: val_loss did not improve from 45.49312
196/196 - 66s - loss: 44.9965 - MinusLogProbMetric: 44.9965 - val_loss: 45.7210 - val_MinusLogProbMetric: 45.7210 - lr: 4.1152e-06 - 66s/epoch - 336ms/step
Epoch 196/1000
2023-10-24 17:03:37.077 
Epoch 196/1000 
	 loss: 44.9282, MinusLogProbMetric: 44.9282, val_loss: 45.5520, val_MinusLogProbMetric: 45.5520

Epoch 196: val_loss did not improve from 45.49312
196/196 - 66s - loss: 44.9282 - MinusLogProbMetric: 44.9282 - val_loss: 45.5520 - val_MinusLogProbMetric: 45.5520 - lr: 4.1152e-06 - 66s/epoch - 338ms/step
Epoch 197/1000
2023-10-24 17:04:40.774 
Epoch 197/1000 
	 loss: 44.9019, MinusLogProbMetric: 44.9019, val_loss: 45.4619, val_MinusLogProbMetric: 45.4619

Epoch 197: val_loss improved from 45.49312 to 45.46186, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 65s - loss: 44.9019 - MinusLogProbMetric: 44.9019 - val_loss: 45.4619 - val_MinusLogProbMetric: 45.4619 - lr: 4.1152e-06 - 65s/epoch - 329ms/step
Epoch 198/1000
2023-10-24 17:05:44.809 
Epoch 198/1000 
	 loss: 44.8586, MinusLogProbMetric: 44.8586, val_loss: 45.4057, val_MinusLogProbMetric: 45.4057

Epoch 198: val_loss improved from 45.46186 to 45.40572, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 64s - loss: 44.8586 - MinusLogProbMetric: 44.8586 - val_loss: 45.4057 - val_MinusLogProbMetric: 45.4057 - lr: 4.1152e-06 - 64s/epoch - 326ms/step
Epoch 199/1000
2023-10-24 17:06:49.889 
Epoch 199/1000 
	 loss: 44.9596, MinusLogProbMetric: 44.9596, val_loss: 45.4715, val_MinusLogProbMetric: 45.4715

Epoch 199: val_loss did not improve from 45.40572
196/196 - 64s - loss: 44.9596 - MinusLogProbMetric: 44.9596 - val_loss: 45.4715 - val_MinusLogProbMetric: 45.4715 - lr: 4.1152e-06 - 64s/epoch - 329ms/step
Epoch 200/1000
2023-10-24 17:07:52.353 
Epoch 200/1000 
	 loss: 44.7812, MinusLogProbMetric: 44.7812, val_loss: 45.4661, val_MinusLogProbMetric: 45.4661

Epoch 200: val_loss did not improve from 45.40572
196/196 - 62s - loss: 44.7812 - MinusLogProbMetric: 44.7812 - val_loss: 45.4661 - val_MinusLogProbMetric: 45.4661 - lr: 4.1152e-06 - 62s/epoch - 319ms/step
Epoch 201/1000
2023-10-24 17:08:54.532 
Epoch 201/1000 
	 loss: 45.0349, MinusLogProbMetric: 45.0349, val_loss: 45.2535, val_MinusLogProbMetric: 45.2535

Epoch 201: val_loss improved from 45.40572 to 45.25347, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 63s - loss: 45.0349 - MinusLogProbMetric: 45.0349 - val_loss: 45.2535 - val_MinusLogProbMetric: 45.2535 - lr: 4.1152e-06 - 63s/epoch - 321ms/step
Epoch 202/1000
2023-10-24 17:09:57.904 
Epoch 202/1000 
	 loss: 45.5570, MinusLogProbMetric: 45.5570, val_loss: 45.0807, val_MinusLogProbMetric: 45.0807

Epoch 202: val_loss improved from 45.25347 to 45.08071, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 63s - loss: 45.5570 - MinusLogProbMetric: 45.5570 - val_loss: 45.0807 - val_MinusLogProbMetric: 45.0807 - lr: 4.1152e-06 - 63s/epoch - 324ms/step
Epoch 203/1000
2023-10-24 17:11:03.428 
Epoch 203/1000 
	 loss: 44.6057, MinusLogProbMetric: 44.6057, val_loss: 45.0752, val_MinusLogProbMetric: 45.0752

Epoch 203: val_loss improved from 45.08071 to 45.07515, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 65s - loss: 44.6057 - MinusLogProbMetric: 44.6057 - val_loss: 45.0752 - val_MinusLogProbMetric: 45.0752 - lr: 4.1152e-06 - 65s/epoch - 334ms/step
Epoch 204/1000
2023-10-24 17:12:07.652 
Epoch 204/1000 
	 loss: 44.6013, MinusLogProbMetric: 44.6013, val_loss: 45.2425, val_MinusLogProbMetric: 45.2425

Epoch 204: val_loss did not improve from 45.07515
196/196 - 63s - loss: 44.6013 - MinusLogProbMetric: 44.6013 - val_loss: 45.2425 - val_MinusLogProbMetric: 45.2425 - lr: 4.1152e-06 - 63s/epoch - 323ms/step
Epoch 205/1000
2023-10-24 17:13:09.956 
Epoch 205/1000 
	 loss: 44.5231, MinusLogProbMetric: 44.5231, val_loss: 45.1636, val_MinusLogProbMetric: 45.1636

Epoch 205: val_loss did not improve from 45.07515
196/196 - 62s - loss: 44.5231 - MinusLogProbMetric: 44.5231 - val_loss: 45.1636 - val_MinusLogProbMetric: 45.1636 - lr: 4.1152e-06 - 62s/epoch - 318ms/step
Epoch 206/1000
2023-10-24 17:14:13.532 
Epoch 206/1000 
	 loss: 44.5806, MinusLogProbMetric: 44.5806, val_loss: 44.9294, val_MinusLogProbMetric: 44.9294

Epoch 206: val_loss improved from 45.07515 to 44.92940, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 64s - loss: 44.5806 - MinusLogProbMetric: 44.5806 - val_loss: 44.9294 - val_MinusLogProbMetric: 44.9294 - lr: 4.1152e-06 - 64s/epoch - 328ms/step
Epoch 207/1000
2023-10-24 17:15:15.320 
Epoch 207/1000 
	 loss: 44.5503, MinusLogProbMetric: 44.5503, val_loss: 44.9045, val_MinusLogProbMetric: 44.9045

Epoch 207: val_loss improved from 44.92940 to 44.90446, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 62s - loss: 44.5503 - MinusLogProbMetric: 44.5503 - val_loss: 44.9045 - val_MinusLogProbMetric: 44.9045 - lr: 4.1152e-06 - 62s/epoch - 316ms/step
Epoch 208/1000
2023-10-24 17:16:18.262 
Epoch 208/1000 
	 loss: 44.4362, MinusLogProbMetric: 44.4362, val_loss: 45.2202, val_MinusLogProbMetric: 45.2202

Epoch 208: val_loss did not improve from 44.90446
196/196 - 62s - loss: 44.4362 - MinusLogProbMetric: 44.4362 - val_loss: 45.2202 - val_MinusLogProbMetric: 45.2202 - lr: 4.1152e-06 - 62s/epoch - 316ms/step
Epoch 209/1000
2023-10-24 17:17:22.916 
Epoch 209/1000 
	 loss: 44.4245, MinusLogProbMetric: 44.4245, val_loss: 44.9379, val_MinusLogProbMetric: 44.9379

Epoch 209: val_loss did not improve from 44.90446
196/196 - 65s - loss: 44.4245 - MinusLogProbMetric: 44.4245 - val_loss: 44.9379 - val_MinusLogProbMetric: 44.9379 - lr: 4.1152e-06 - 65s/epoch - 330ms/step
Epoch 210/1000
2023-10-24 17:18:26.186 
Epoch 210/1000 
	 loss: 44.3751, MinusLogProbMetric: 44.3751, val_loss: 44.7960, val_MinusLogProbMetric: 44.7960

Epoch 210: val_loss improved from 44.90446 to 44.79604, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 64s - loss: 44.3751 - MinusLogProbMetric: 44.3751 - val_loss: 44.7960 - val_MinusLogProbMetric: 44.7960 - lr: 4.1152e-06 - 64s/epoch - 327ms/step
Epoch 211/1000
2023-10-24 17:19:31.828 
Epoch 211/1000 
	 loss: 44.3549, MinusLogProbMetric: 44.3549, val_loss: 44.6854, val_MinusLogProbMetric: 44.6854

Epoch 211: val_loss improved from 44.79604 to 44.68535, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 66s - loss: 44.3549 - MinusLogProbMetric: 44.3549 - val_loss: 44.6854 - val_MinusLogProbMetric: 44.6854 - lr: 4.1152e-06 - 66s/epoch - 334ms/step
Epoch 212/1000
2023-10-24 17:20:33.354 
Epoch 212/1000 
	 loss: 44.3119, MinusLogProbMetric: 44.3119, val_loss: 44.7344, val_MinusLogProbMetric: 44.7344

Epoch 212: val_loss did not improve from 44.68535
196/196 - 61s - loss: 44.3119 - MinusLogProbMetric: 44.3119 - val_loss: 44.7344 - val_MinusLogProbMetric: 44.7344 - lr: 4.1152e-06 - 61s/epoch - 310ms/step
Epoch 213/1000
2023-10-24 17:21:36.717 
Epoch 213/1000 
	 loss: 48.7249, MinusLogProbMetric: 48.7249, val_loss: 46.0292, val_MinusLogProbMetric: 46.0292

Epoch 213: val_loss did not improve from 44.68535
196/196 - 63s - loss: 48.7249 - MinusLogProbMetric: 48.7249 - val_loss: 46.0292 - val_MinusLogProbMetric: 46.0292 - lr: 4.1152e-06 - 63s/epoch - 323ms/step
Epoch 214/1000
2023-10-24 17:22:41.494 
Epoch 214/1000 
	 loss: 44.7833, MinusLogProbMetric: 44.7833, val_loss: 44.8914, val_MinusLogProbMetric: 44.8914

Epoch 214: val_loss did not improve from 44.68535
196/196 - 65s - loss: 44.7833 - MinusLogProbMetric: 44.7833 - val_loss: 44.8914 - val_MinusLogProbMetric: 44.8914 - lr: 4.1152e-06 - 65s/epoch - 330ms/step
Epoch 215/1000
2023-10-24 17:23:42.987 
Epoch 215/1000 
	 loss: 44.2443, MinusLogProbMetric: 44.2443, val_loss: 44.7277, val_MinusLogProbMetric: 44.7277

Epoch 215: val_loss did not improve from 44.68535
196/196 - 61s - loss: 44.2443 - MinusLogProbMetric: 44.2443 - val_loss: 44.7277 - val_MinusLogProbMetric: 44.7277 - lr: 4.1152e-06 - 61s/epoch - 314ms/step
Epoch 216/1000
2023-10-24 17:24:44.200 
Epoch 216/1000 
	 loss: 44.2460, MinusLogProbMetric: 44.2460, val_loss: 44.4524, val_MinusLogProbMetric: 44.4524

Epoch 216: val_loss improved from 44.68535 to 44.45238, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 62s - loss: 44.2460 - MinusLogProbMetric: 44.2460 - val_loss: 44.4524 - val_MinusLogProbMetric: 44.4524 - lr: 4.1152e-06 - 62s/epoch - 317ms/step
Epoch 217/1000
2023-10-24 17:25:49.503 
Epoch 217/1000 
	 loss: 44.1126, MinusLogProbMetric: 44.1126, val_loss: 44.5567, val_MinusLogProbMetric: 44.5567

Epoch 217: val_loss did not improve from 44.45238
196/196 - 64s - loss: 44.1126 - MinusLogProbMetric: 44.1126 - val_loss: 44.5567 - val_MinusLogProbMetric: 44.5567 - lr: 4.1152e-06 - 64s/epoch - 329ms/step
Epoch 218/1000
2023-10-24 17:26:54.194 
Epoch 218/1000 
	 loss: 44.0671, MinusLogProbMetric: 44.0671, val_loss: 44.5302, val_MinusLogProbMetric: 44.5302

Epoch 218: val_loss did not improve from 44.45238
196/196 - 65s - loss: 44.0671 - MinusLogProbMetric: 44.0671 - val_loss: 44.5302 - val_MinusLogProbMetric: 44.5302 - lr: 4.1152e-06 - 65s/epoch - 330ms/step
Epoch 219/1000
2023-10-24 17:27:56.548 
Epoch 219/1000 
	 loss: 45.0039, MinusLogProbMetric: 45.0039, val_loss: 119.7728, val_MinusLogProbMetric: 119.7728

Epoch 219: val_loss did not improve from 44.45238
196/196 - 62s - loss: 45.0039 - MinusLogProbMetric: 45.0039 - val_loss: 119.7728 - val_MinusLogProbMetric: 119.7728 - lr: 4.1152e-06 - 62s/epoch - 318ms/step
Epoch 220/1000
2023-10-24 17:28:59.658 
Epoch 220/1000 
	 loss: 56.2731, MinusLogProbMetric: 56.2731, val_loss: 49.1741, val_MinusLogProbMetric: 49.1741

Epoch 220: val_loss did not improve from 44.45238
196/196 - 63s - loss: 56.2731 - MinusLogProbMetric: 56.2731 - val_loss: 49.1741 - val_MinusLogProbMetric: 49.1741 - lr: 4.1152e-06 - 63s/epoch - 322ms/step
Epoch 221/1000
2023-10-24 17:30:03.759 
Epoch 221/1000 
	 loss: 46.5322, MinusLogProbMetric: 46.5322, val_loss: 46.1142, val_MinusLogProbMetric: 46.1142

Epoch 221: val_loss did not improve from 44.45238
196/196 - 64s - loss: 46.5322 - MinusLogProbMetric: 46.5322 - val_loss: 46.1142 - val_MinusLogProbMetric: 46.1142 - lr: 4.1152e-06 - 64s/epoch - 327ms/step
Epoch 222/1000
2023-10-24 17:31:10.379 
Epoch 222/1000 
	 loss: 44.8590, MinusLogProbMetric: 44.8590, val_loss: 45.1075, val_MinusLogProbMetric: 45.1075

Epoch 222: val_loss did not improve from 44.45238
196/196 - 67s - loss: 44.8590 - MinusLogProbMetric: 44.8590 - val_loss: 45.1075 - val_MinusLogProbMetric: 45.1075 - lr: 4.1152e-06 - 67s/epoch - 340ms/step
Epoch 223/1000
2023-10-24 17:32:14.744 
Epoch 223/1000 
	 loss: 44.5420, MinusLogProbMetric: 44.5420, val_loss: 44.9134, val_MinusLogProbMetric: 44.9134

Epoch 223: val_loss did not improve from 44.45238
196/196 - 64s - loss: 44.5420 - MinusLogProbMetric: 44.5420 - val_loss: 44.9134 - val_MinusLogProbMetric: 44.9134 - lr: 4.1152e-06 - 64s/epoch - 328ms/step
Epoch 224/1000
2023-10-24 17:33:18.325 
Epoch 224/1000 
	 loss: 46.3578, MinusLogProbMetric: 46.3578, val_loss: 45.4678, val_MinusLogProbMetric: 45.4678

Epoch 224: val_loss did not improve from 44.45238
196/196 - 64s - loss: 46.3578 - MinusLogProbMetric: 46.3578 - val_loss: 45.4678 - val_MinusLogProbMetric: 45.4678 - lr: 4.1152e-06 - 64s/epoch - 324ms/step
Epoch 225/1000
2023-10-24 17:34:21.722 
Epoch 225/1000 
	 loss: 44.6592, MinusLogProbMetric: 44.6592, val_loss: 44.8769, val_MinusLogProbMetric: 44.8769

Epoch 225: val_loss did not improve from 44.45238
196/196 - 63s - loss: 44.6592 - MinusLogProbMetric: 44.6592 - val_loss: 44.8769 - val_MinusLogProbMetric: 44.8769 - lr: 4.1152e-06 - 63s/epoch - 323ms/step
Epoch 226/1000
2023-10-24 17:35:24.981 
Epoch 226/1000 
	 loss: 44.2542, MinusLogProbMetric: 44.2542, val_loss: 44.4139, val_MinusLogProbMetric: 44.4139

Epoch 226: val_loss improved from 44.45238 to 44.41394, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 64s - loss: 44.2542 - MinusLogProbMetric: 44.2542 - val_loss: 44.4139 - val_MinusLogProbMetric: 44.4139 - lr: 4.1152e-06 - 64s/epoch - 327ms/step
Epoch 227/1000
2023-10-24 17:36:30.032 
Epoch 227/1000 
	 loss: 44.3630, MinusLogProbMetric: 44.3630, val_loss: 44.3632, val_MinusLogProbMetric: 44.3632

Epoch 227: val_loss improved from 44.41394 to 44.36318, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 65s - loss: 44.3630 - MinusLogProbMetric: 44.3630 - val_loss: 44.3632 - val_MinusLogProbMetric: 44.3632 - lr: 4.1152e-06 - 65s/epoch - 332ms/step
Epoch 228/1000
2023-10-24 17:37:34.885 
Epoch 228/1000 
	 loss: 43.9017, MinusLogProbMetric: 43.9017, val_loss: 44.3143, val_MinusLogProbMetric: 44.3143

Epoch 228: val_loss improved from 44.36318 to 44.31427, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 65s - loss: 43.9017 - MinusLogProbMetric: 43.9017 - val_loss: 44.3143 - val_MinusLogProbMetric: 44.3143 - lr: 4.1152e-06 - 65s/epoch - 330ms/step
Epoch 229/1000
2023-10-24 17:38:40.044 
Epoch 229/1000 
	 loss: 43.8324, MinusLogProbMetric: 43.8324, val_loss: 44.2716, val_MinusLogProbMetric: 44.2716

Epoch 229: val_loss improved from 44.31427 to 44.27161, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 65s - loss: 43.8324 - MinusLogProbMetric: 43.8324 - val_loss: 44.2716 - val_MinusLogProbMetric: 44.2716 - lr: 4.1152e-06 - 65s/epoch - 333ms/step
Epoch 230/1000
2023-10-24 17:39:44.054 
Epoch 230/1000 
	 loss: 44.0941, MinusLogProbMetric: 44.0941, val_loss: 44.3996, val_MinusLogProbMetric: 44.3996

Epoch 230: val_loss did not improve from 44.27161
196/196 - 63s - loss: 44.0941 - MinusLogProbMetric: 44.0941 - val_loss: 44.3996 - val_MinusLogProbMetric: 44.3996 - lr: 4.1152e-06 - 63s/epoch - 322ms/step
Epoch 231/1000
2023-10-24 17:40:49.082 
Epoch 231/1000 
	 loss: 43.7540, MinusLogProbMetric: 43.7540, val_loss: 44.9096, val_MinusLogProbMetric: 44.9096

Epoch 231: val_loss did not improve from 44.27161
196/196 - 65s - loss: 43.7540 - MinusLogProbMetric: 43.7540 - val_loss: 44.9096 - val_MinusLogProbMetric: 44.9096 - lr: 4.1152e-06 - 65s/epoch - 332ms/step
Epoch 232/1000
2023-10-24 17:41:51.980 
Epoch 232/1000 
	 loss: 44.0518, MinusLogProbMetric: 44.0518, val_loss: 44.1078, val_MinusLogProbMetric: 44.1078

Epoch 232: val_loss improved from 44.27161 to 44.10782, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 64s - loss: 44.0518 - MinusLogProbMetric: 44.0518 - val_loss: 44.1078 - val_MinusLogProbMetric: 44.1078 - lr: 4.1152e-06 - 64s/epoch - 325ms/step
Epoch 233/1000
2023-10-24 17:42:55.691 
Epoch 233/1000 
	 loss: 43.6444, MinusLogProbMetric: 43.6444, val_loss: 44.0903, val_MinusLogProbMetric: 44.0903

Epoch 233: val_loss improved from 44.10782 to 44.09031, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 64s - loss: 43.6444 - MinusLogProbMetric: 43.6444 - val_loss: 44.0903 - val_MinusLogProbMetric: 44.0903 - lr: 4.1152e-06 - 64s/epoch - 324ms/step
Epoch 234/1000
2023-10-24 17:44:01.315 
Epoch 234/1000 
	 loss: 43.6298, MinusLogProbMetric: 43.6298, val_loss: 44.2603, val_MinusLogProbMetric: 44.2603

Epoch 234: val_loss did not improve from 44.09031
196/196 - 65s - loss: 43.6298 - MinusLogProbMetric: 43.6298 - val_loss: 44.2603 - val_MinusLogProbMetric: 44.2603 - lr: 4.1152e-06 - 65s/epoch - 331ms/step
Epoch 235/1000
2023-10-24 17:45:06.797 
Epoch 235/1000 
	 loss: 43.8585, MinusLogProbMetric: 43.8585, val_loss: 44.0327, val_MinusLogProbMetric: 44.0327

Epoch 235: val_loss improved from 44.09031 to 44.03270, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 66s - loss: 43.8585 - MinusLogProbMetric: 43.8585 - val_loss: 44.0327 - val_MinusLogProbMetric: 44.0327 - lr: 4.1152e-06 - 66s/epoch - 339ms/step
Epoch 236/1000
2023-10-24 17:46:11.469 
Epoch 236/1000 
	 loss: 44.6556, MinusLogProbMetric: 44.6556, val_loss: 43.9117, val_MinusLogProbMetric: 43.9117

Epoch 236: val_loss improved from 44.03270 to 43.91173, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 65s - loss: 44.6556 - MinusLogProbMetric: 44.6556 - val_loss: 43.9117 - val_MinusLogProbMetric: 43.9117 - lr: 4.1152e-06 - 65s/epoch - 329ms/step
Epoch 237/1000
2023-10-24 17:47:14.776 
Epoch 237/1000 
	 loss: 43.4962, MinusLogProbMetric: 43.4962, val_loss: 43.8341, val_MinusLogProbMetric: 43.8341

Epoch 237: val_loss improved from 43.91173 to 43.83413, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 63s - loss: 43.4962 - MinusLogProbMetric: 43.4962 - val_loss: 43.8341 - val_MinusLogProbMetric: 43.8341 - lr: 4.1152e-06 - 63s/epoch - 323ms/step
Epoch 238/1000
2023-10-24 17:48:17.933 
Epoch 238/1000 
	 loss: 43.4546, MinusLogProbMetric: 43.4546, val_loss: 43.8012, val_MinusLogProbMetric: 43.8012

Epoch 238: val_loss improved from 43.83413 to 43.80122, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 63s - loss: 43.4546 - MinusLogProbMetric: 43.4546 - val_loss: 43.8012 - val_MinusLogProbMetric: 43.8012 - lr: 4.1152e-06 - 63s/epoch - 322ms/step
Epoch 239/1000
2023-10-24 17:49:19.904 
Epoch 239/1000 
	 loss: 43.4766, MinusLogProbMetric: 43.4766, val_loss: 43.8335, val_MinusLogProbMetric: 43.8335

Epoch 239: val_loss did not improve from 43.80122
196/196 - 61s - loss: 43.4766 - MinusLogProbMetric: 43.4766 - val_loss: 43.8335 - val_MinusLogProbMetric: 43.8335 - lr: 4.1152e-06 - 61s/epoch - 312ms/step
Epoch 240/1000
2023-10-24 17:50:25.255 
Epoch 240/1000 
	 loss: 43.7750, MinusLogProbMetric: 43.7750, val_loss: 43.8863, val_MinusLogProbMetric: 43.8863

Epoch 240: val_loss did not improve from 43.80122
196/196 - 65s - loss: 43.7750 - MinusLogProbMetric: 43.7750 - val_loss: 43.8863 - val_MinusLogProbMetric: 43.8863 - lr: 4.1152e-06 - 65s/epoch - 333ms/step
Epoch 241/1000
2023-10-24 17:51:29.335 
Epoch 241/1000 
	 loss: 43.3038, MinusLogProbMetric: 43.3038, val_loss: 44.0119, val_MinusLogProbMetric: 44.0119

Epoch 241: val_loss did not improve from 43.80122
196/196 - 64s - loss: 43.3038 - MinusLogProbMetric: 43.3038 - val_loss: 44.0119 - val_MinusLogProbMetric: 44.0119 - lr: 4.1152e-06 - 64s/epoch - 327ms/step
Epoch 242/1000
2023-10-24 17:52:32.103 
Epoch 242/1000 
	 loss: 43.3032, MinusLogProbMetric: 43.3032, val_loss: 43.5587, val_MinusLogProbMetric: 43.5587

Epoch 242: val_loss improved from 43.80122 to 43.55867, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 64s - loss: 43.3032 - MinusLogProbMetric: 43.3032 - val_loss: 43.5587 - val_MinusLogProbMetric: 43.5587 - lr: 4.1152e-06 - 64s/epoch - 325ms/step
Epoch 243/1000
2023-10-24 17:53:36.303 
Epoch 243/1000 
	 loss: 43.4655, MinusLogProbMetric: 43.4655, val_loss: 43.8306, val_MinusLogProbMetric: 43.8306

Epoch 243: val_loss did not improve from 43.55867
196/196 - 63s - loss: 43.4655 - MinusLogProbMetric: 43.4655 - val_loss: 43.8306 - val_MinusLogProbMetric: 43.8306 - lr: 4.1152e-06 - 63s/epoch - 323ms/step
Epoch 244/1000
2023-10-24 17:54:37.784 
Epoch 244/1000 
	 loss: 43.3166, MinusLogProbMetric: 43.3166, val_loss: 43.7112, val_MinusLogProbMetric: 43.7112

Epoch 244: val_loss did not improve from 43.55867
196/196 - 61s - loss: 43.3166 - MinusLogProbMetric: 43.3166 - val_loss: 43.7112 - val_MinusLogProbMetric: 43.7112 - lr: 4.1152e-06 - 61s/epoch - 314ms/step
Epoch 245/1000
2023-10-24 17:55:39.186 
Epoch 245/1000 
	 loss: 43.1977, MinusLogProbMetric: 43.1977, val_loss: 43.5118, val_MinusLogProbMetric: 43.5118

Epoch 245: val_loss improved from 43.55867 to 43.51178, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 62s - loss: 43.1977 - MinusLogProbMetric: 43.1977 - val_loss: 43.5118 - val_MinusLogProbMetric: 43.5118 - lr: 4.1152e-06 - 62s/epoch - 317ms/step
Epoch 246/1000
2023-10-24 17:56:42.996 
Epoch 246/1000 
	 loss: 43.2647, MinusLogProbMetric: 43.2647, val_loss: 43.7404, val_MinusLogProbMetric: 43.7404

Epoch 246: val_loss did not improve from 43.51178
196/196 - 63s - loss: 43.2647 - MinusLogProbMetric: 43.2647 - val_loss: 43.7404 - val_MinusLogProbMetric: 43.7404 - lr: 4.1152e-06 - 63s/epoch - 322ms/step
Epoch 247/1000
2023-10-24 17:57:45.093 
Epoch 247/1000 
	 loss: 43.1487, MinusLogProbMetric: 43.1487, val_loss: 43.5589, val_MinusLogProbMetric: 43.5589

Epoch 247: val_loss did not improve from 43.51178
196/196 - 62s - loss: 43.1487 - MinusLogProbMetric: 43.1487 - val_loss: 43.5589 - val_MinusLogProbMetric: 43.5589 - lr: 4.1152e-06 - 62s/epoch - 317ms/step
Epoch 248/1000
2023-10-24 17:58:49.745 
Epoch 248/1000 
	 loss: 43.1278, MinusLogProbMetric: 43.1278, val_loss: 43.5480, val_MinusLogProbMetric: 43.5480

Epoch 248: val_loss did not improve from 43.51178
196/196 - 65s - loss: 43.1278 - MinusLogProbMetric: 43.1278 - val_loss: 43.5480 - val_MinusLogProbMetric: 43.5480 - lr: 4.1152e-06 - 65s/epoch - 330ms/step
Epoch 249/1000
2023-10-24 17:59:53.123 
Epoch 249/1000 
	 loss: 43.3053, MinusLogProbMetric: 43.3053, val_loss: 43.3846, val_MinusLogProbMetric: 43.3846

Epoch 249: val_loss improved from 43.51178 to 43.38463, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 64s - loss: 43.3053 - MinusLogProbMetric: 43.3053 - val_loss: 43.3846 - val_MinusLogProbMetric: 43.3846 - lr: 4.1152e-06 - 64s/epoch - 327ms/step
Epoch 250/1000
2023-10-24 18:00:56.823 
Epoch 250/1000 
	 loss: 43.0732, MinusLogProbMetric: 43.0732, val_loss: 43.5373, val_MinusLogProbMetric: 43.5373

Epoch 250: val_loss did not improve from 43.38463
196/196 - 63s - loss: 43.0732 - MinusLogProbMetric: 43.0732 - val_loss: 43.5373 - val_MinusLogProbMetric: 43.5373 - lr: 4.1152e-06 - 63s/epoch - 321ms/step
Epoch 251/1000
2023-10-24 18:01:59.664 
Epoch 251/1000 
	 loss: 42.9914, MinusLogProbMetric: 42.9914, val_loss: 43.4333, val_MinusLogProbMetric: 43.4333

Epoch 251: val_loss did not improve from 43.38463
196/196 - 63s - loss: 42.9914 - MinusLogProbMetric: 42.9914 - val_loss: 43.4333 - val_MinusLogProbMetric: 43.4333 - lr: 4.1152e-06 - 63s/epoch - 321ms/step
Epoch 252/1000
2023-10-24 18:03:01.932 
Epoch 252/1000 
	 loss: 43.4898, MinusLogProbMetric: 43.4898, val_loss: 43.4610, val_MinusLogProbMetric: 43.4610

Epoch 252: val_loss did not improve from 43.38463
196/196 - 62s - loss: 43.4898 - MinusLogProbMetric: 43.4898 - val_loss: 43.4610 - val_MinusLogProbMetric: 43.4610 - lr: 4.1152e-06 - 62s/epoch - 318ms/step
Epoch 253/1000
2023-10-24 18:04:04.273 
Epoch 253/1000 
	 loss: 42.9310, MinusLogProbMetric: 42.9310, val_loss: 43.4169, val_MinusLogProbMetric: 43.4169

Epoch 253: val_loss did not improve from 43.38463
196/196 - 62s - loss: 42.9310 - MinusLogProbMetric: 42.9310 - val_loss: 43.4169 - val_MinusLogProbMetric: 43.4169 - lr: 4.1152e-06 - 62s/epoch - 318ms/step
Epoch 254/1000
2023-10-24 18:05:07.061 
Epoch 254/1000 
	 loss: 42.9031, MinusLogProbMetric: 42.9031, val_loss: 43.2301, val_MinusLogProbMetric: 43.2301

Epoch 254: val_loss improved from 43.38463 to 43.23006, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 64s - loss: 42.9031 - MinusLogProbMetric: 42.9031 - val_loss: 43.2301 - val_MinusLogProbMetric: 43.2301 - lr: 4.1152e-06 - 64s/epoch - 324ms/step
Epoch 255/1000
2023-10-24 18:06:09.879 
Epoch 255/1000 
	 loss: 42.9111, MinusLogProbMetric: 42.9111, val_loss: 43.3947, val_MinusLogProbMetric: 43.3947

Epoch 255: val_loss did not improve from 43.23006
196/196 - 62s - loss: 42.9111 - MinusLogProbMetric: 42.9111 - val_loss: 43.3947 - val_MinusLogProbMetric: 43.3947 - lr: 4.1152e-06 - 62s/epoch - 317ms/step
Epoch 256/1000
2023-10-24 18:07:15.802 
Epoch 256/1000 
	 loss: 42.9551, MinusLogProbMetric: 42.9551, val_loss: 43.2049, val_MinusLogProbMetric: 43.2049

Epoch 256: val_loss improved from 43.23006 to 43.20490, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 67s - loss: 42.9551 - MinusLogProbMetric: 42.9551 - val_loss: 43.2049 - val_MinusLogProbMetric: 43.2049 - lr: 4.1152e-06 - 67s/epoch - 341ms/step
Epoch 257/1000
2023-10-24 18:08:20.949 
Epoch 257/1000 
	 loss: 42.8737, MinusLogProbMetric: 42.8737, val_loss: 43.5958, val_MinusLogProbMetric: 43.5958

Epoch 257: val_loss did not improve from 43.20490
196/196 - 64s - loss: 42.8737 - MinusLogProbMetric: 42.8737 - val_loss: 43.5958 - val_MinusLogProbMetric: 43.5958 - lr: 4.1152e-06 - 64s/epoch - 328ms/step
Epoch 258/1000
2023-10-24 18:09:24.473 
Epoch 258/1000 
	 loss: 42.8435, MinusLogProbMetric: 42.8435, val_loss: 43.0186, val_MinusLogProbMetric: 43.0186

Epoch 258: val_loss improved from 43.20490 to 43.01858, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 64s - loss: 42.8435 - MinusLogProbMetric: 42.8435 - val_loss: 43.0186 - val_MinusLogProbMetric: 43.0186 - lr: 4.1152e-06 - 64s/epoch - 328ms/step
Epoch 259/1000
2023-10-24 18:10:26.746 
Epoch 259/1000 
	 loss: 42.8048, MinusLogProbMetric: 42.8048, val_loss: 43.3066, val_MinusLogProbMetric: 43.3066

Epoch 259: val_loss did not improve from 43.01858
196/196 - 61s - loss: 42.8048 - MinusLogProbMetric: 42.8048 - val_loss: 43.3066 - val_MinusLogProbMetric: 43.3066 - lr: 4.1152e-06 - 61s/epoch - 314ms/step
Epoch 260/1000
2023-10-24 18:11:30.754 
Epoch 260/1000 
	 loss: 42.7279, MinusLogProbMetric: 42.7279, val_loss: 43.4205, val_MinusLogProbMetric: 43.4205

Epoch 260: val_loss did not improve from 43.01858
196/196 - 64s - loss: 42.7279 - MinusLogProbMetric: 42.7279 - val_loss: 43.4205 - val_MinusLogProbMetric: 43.4205 - lr: 4.1152e-06 - 64s/epoch - 327ms/step
Epoch 261/1000
2023-10-24 18:12:31.987 
Epoch 261/1000 
	 loss: 42.6575, MinusLogProbMetric: 42.6575, val_loss: 43.2591, val_MinusLogProbMetric: 43.2591

Epoch 261: val_loss did not improve from 43.01858
196/196 - 61s - loss: 42.6575 - MinusLogProbMetric: 42.6575 - val_loss: 43.2591 - val_MinusLogProbMetric: 43.2591 - lr: 4.1152e-06 - 61s/epoch - 312ms/step
Epoch 262/1000
2023-10-24 18:13:34.800 
Epoch 262/1000 
	 loss: 43.0523, MinusLogProbMetric: 43.0523, val_loss: 43.8762, val_MinusLogProbMetric: 43.8762

Epoch 262: val_loss did not improve from 43.01858
196/196 - 63s - loss: 43.0523 - MinusLogProbMetric: 43.0523 - val_loss: 43.8762 - val_MinusLogProbMetric: 43.8762 - lr: 4.1152e-06 - 63s/epoch - 320ms/step
Epoch 263/1000
2023-10-24 18:14:39.053 
Epoch 263/1000 
	 loss: 42.6313, MinusLogProbMetric: 42.6313, val_loss: 43.1541, val_MinusLogProbMetric: 43.1541

Epoch 263: val_loss did not improve from 43.01858
196/196 - 64s - loss: 42.6313 - MinusLogProbMetric: 42.6313 - val_loss: 43.1541 - val_MinusLogProbMetric: 43.1541 - lr: 4.1152e-06 - 64s/epoch - 328ms/step
Epoch 264/1000
2023-10-24 18:15:41.813 
Epoch 264/1000 
	 loss: 42.6586, MinusLogProbMetric: 42.6586, val_loss: 43.0640, val_MinusLogProbMetric: 43.0640

Epoch 264: val_loss did not improve from 43.01858
196/196 - 63s - loss: 42.6586 - MinusLogProbMetric: 42.6586 - val_loss: 43.0640 - val_MinusLogProbMetric: 43.0640 - lr: 4.1152e-06 - 63s/epoch - 320ms/step
Epoch 265/1000
2023-10-24 18:16:44.796 
Epoch 265/1000 
	 loss: 42.5895, MinusLogProbMetric: 42.5895, val_loss: 43.0610, val_MinusLogProbMetric: 43.0610

Epoch 265: val_loss did not improve from 43.01858
196/196 - 63s - loss: 42.5895 - MinusLogProbMetric: 42.5895 - val_loss: 43.0610 - val_MinusLogProbMetric: 43.0610 - lr: 4.1152e-06 - 63s/epoch - 321ms/step
Epoch 266/1000
2023-10-24 18:17:48.138 
Epoch 266/1000 
	 loss: 42.5919, MinusLogProbMetric: 42.5919, val_loss: 42.9932, val_MinusLogProbMetric: 42.9932

Epoch 266: val_loss improved from 43.01858 to 42.99318, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 64s - loss: 42.5919 - MinusLogProbMetric: 42.5919 - val_loss: 42.9932 - val_MinusLogProbMetric: 42.9932 - lr: 4.1152e-06 - 64s/epoch - 328ms/step
Epoch 267/1000
2023-10-24 18:18:53.403 
Epoch 267/1000 
	 loss: 42.8199, MinusLogProbMetric: 42.8199, val_loss: 42.9386, val_MinusLogProbMetric: 42.9386

Epoch 267: val_loss improved from 42.99318 to 42.93860, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 65s - loss: 42.8199 - MinusLogProbMetric: 42.8199 - val_loss: 42.9386 - val_MinusLogProbMetric: 42.9386 - lr: 4.1152e-06 - 65s/epoch - 333ms/step
Epoch 268/1000
2023-10-24 18:19:58.354 
Epoch 268/1000 
	 loss: 42.5139, MinusLogProbMetric: 42.5139, val_loss: 42.8618, val_MinusLogProbMetric: 42.8618

Epoch 268: val_loss improved from 42.93860 to 42.86181, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 65s - loss: 42.5139 - MinusLogProbMetric: 42.5139 - val_loss: 42.8618 - val_MinusLogProbMetric: 42.8618 - lr: 4.1152e-06 - 65s/epoch - 331ms/step
Epoch 269/1000
2023-10-24 18:21:04.644 
Epoch 269/1000 
	 loss: 42.5362, MinusLogProbMetric: 42.5362, val_loss: 43.1918, val_MinusLogProbMetric: 43.1918

Epoch 269: val_loss did not improve from 42.86181
196/196 - 65s - loss: 42.5362 - MinusLogProbMetric: 42.5362 - val_loss: 43.1918 - val_MinusLogProbMetric: 43.1918 - lr: 4.1152e-06 - 65s/epoch - 334ms/step
Epoch 270/1000
2023-10-24 18:22:08.376 
Epoch 270/1000 
	 loss: 42.4050, MinusLogProbMetric: 42.4050, val_loss: 42.8928, val_MinusLogProbMetric: 42.8928

Epoch 270: val_loss did not improve from 42.86181
196/196 - 64s - loss: 42.4050 - MinusLogProbMetric: 42.4050 - val_loss: 42.8928 - val_MinusLogProbMetric: 42.8928 - lr: 4.1152e-06 - 64s/epoch - 325ms/step
Epoch 271/1000
2023-10-24 18:23:12.566 
Epoch 271/1000 
	 loss: 42.8907, MinusLogProbMetric: 42.8907, val_loss: 43.3875, val_MinusLogProbMetric: 43.3875

Epoch 271: val_loss did not improve from 42.86181
196/196 - 64s - loss: 42.8907 - MinusLogProbMetric: 42.8907 - val_loss: 43.3875 - val_MinusLogProbMetric: 43.3875 - lr: 4.1152e-06 - 64s/epoch - 327ms/step
Epoch 272/1000
2023-10-24 18:24:13.240 
Epoch 272/1000 
	 loss: 42.3815, MinusLogProbMetric: 42.3815, val_loss: 42.8582, val_MinusLogProbMetric: 42.8582

Epoch 272: val_loss improved from 42.86181 to 42.85820, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 62s - loss: 42.3815 - MinusLogProbMetric: 42.3815 - val_loss: 42.8582 - val_MinusLogProbMetric: 42.8582 - lr: 4.1152e-06 - 62s/epoch - 314ms/step
Epoch 273/1000
2023-10-24 18:25:19.122 
Epoch 273/1000 
	 loss: 42.3463, MinusLogProbMetric: 42.3463, val_loss: 43.0461, val_MinusLogProbMetric: 43.0461

Epoch 273: val_loss did not improve from 42.85820
196/196 - 65s - loss: 42.3463 - MinusLogProbMetric: 42.3463 - val_loss: 43.0461 - val_MinusLogProbMetric: 43.0461 - lr: 4.1152e-06 - 65s/epoch - 331ms/step
Epoch 274/1000
2023-10-24 18:26:23.578 
Epoch 274/1000 
	 loss: 42.4613, MinusLogProbMetric: 42.4613, val_loss: 43.0666, val_MinusLogProbMetric: 43.0666

Epoch 274: val_loss did not improve from 42.85820
196/196 - 64s - loss: 42.4613 - MinusLogProbMetric: 42.4613 - val_loss: 43.0666 - val_MinusLogProbMetric: 43.0666 - lr: 4.1152e-06 - 64s/epoch - 329ms/step
Epoch 275/1000
2023-10-24 18:27:27.427 
Epoch 275/1000 
	 loss: 42.2798, MinusLogProbMetric: 42.2798, val_loss: 43.0455, val_MinusLogProbMetric: 43.0455

Epoch 275: val_loss did not improve from 42.85820
196/196 - 64s - loss: 42.2798 - MinusLogProbMetric: 42.2798 - val_loss: 43.0455 - val_MinusLogProbMetric: 43.0455 - lr: 4.1152e-06 - 64s/epoch - 326ms/step
Epoch 276/1000
2023-10-24 18:28:30.283 
Epoch 276/1000 
	 loss: 42.3508, MinusLogProbMetric: 42.3508, val_loss: 42.6631, val_MinusLogProbMetric: 42.6631

Epoch 276: val_loss improved from 42.85820 to 42.66305, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 64s - loss: 42.3508 - MinusLogProbMetric: 42.3508 - val_loss: 42.6631 - val_MinusLogProbMetric: 42.6631 - lr: 4.1152e-06 - 64s/epoch - 325ms/step
Epoch 277/1000
2023-10-24 18:29:33.333 
Epoch 277/1000 
	 loss: 42.2308, MinusLogProbMetric: 42.2308, val_loss: 42.6058, val_MinusLogProbMetric: 42.6058

Epoch 277: val_loss improved from 42.66305 to 42.60578, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 63s - loss: 42.2308 - MinusLogProbMetric: 42.2308 - val_loss: 42.6058 - val_MinusLogProbMetric: 42.6058 - lr: 4.1152e-06 - 63s/epoch - 322ms/step
Epoch 278/1000
2023-10-24 18:30:37.637 
Epoch 278/1000 
	 loss: 42.7828, MinusLogProbMetric: 42.7828, val_loss: 42.5193, val_MinusLogProbMetric: 42.5193

Epoch 278: val_loss improved from 42.60578 to 42.51927, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 64s - loss: 42.7828 - MinusLogProbMetric: 42.7828 - val_loss: 42.5193 - val_MinusLogProbMetric: 42.5193 - lr: 4.1152e-06 - 64s/epoch - 328ms/step
Epoch 279/1000
2023-10-24 18:31:43.306 
Epoch 279/1000 
	 loss: 42.1628, MinusLogProbMetric: 42.1628, val_loss: 42.5503, val_MinusLogProbMetric: 42.5503

Epoch 279: val_loss did not improve from 42.51927
196/196 - 65s - loss: 42.1628 - MinusLogProbMetric: 42.1628 - val_loss: 42.5503 - val_MinusLogProbMetric: 42.5503 - lr: 4.1152e-06 - 65s/epoch - 330ms/step
Epoch 280/1000
2023-10-24 18:32:47.640 
Epoch 280/1000 
	 loss: 42.2650, MinusLogProbMetric: 42.2650, val_loss: 42.9226, val_MinusLogProbMetric: 42.9226

Epoch 280: val_loss did not improve from 42.51927
196/196 - 64s - loss: 42.2650 - MinusLogProbMetric: 42.2650 - val_loss: 42.9226 - val_MinusLogProbMetric: 42.9226 - lr: 4.1152e-06 - 64s/epoch - 328ms/step
Epoch 281/1000
2023-10-24 18:33:51.733 
Epoch 281/1000 
	 loss: 42.1292, MinusLogProbMetric: 42.1292, val_loss: 42.7881, val_MinusLogProbMetric: 42.7881

Epoch 281: val_loss did not improve from 42.51927
196/196 - 64s - loss: 42.1292 - MinusLogProbMetric: 42.1292 - val_loss: 42.7881 - val_MinusLogProbMetric: 42.7881 - lr: 4.1152e-06 - 64s/epoch - 327ms/step
Epoch 282/1000
2023-10-24 18:34:54.519 
Epoch 282/1000 
	 loss: 42.1341, MinusLogProbMetric: 42.1341, val_loss: 48.2675, val_MinusLogProbMetric: 48.2675

Epoch 282: val_loss did not improve from 42.51927
196/196 - 63s - loss: 42.1341 - MinusLogProbMetric: 42.1341 - val_loss: 48.2675 - val_MinusLogProbMetric: 48.2675 - lr: 4.1152e-06 - 63s/epoch - 320ms/step
Epoch 283/1000
2023-10-24 18:35:58.971 
Epoch 283/1000 
	 loss: 42.5753, MinusLogProbMetric: 42.5753, val_loss: 42.6102, val_MinusLogProbMetric: 42.6102

Epoch 283: val_loss did not improve from 42.51927
196/196 - 64s - loss: 42.5753 - MinusLogProbMetric: 42.5753 - val_loss: 42.6102 - val_MinusLogProbMetric: 42.6102 - lr: 4.1152e-06 - 64s/epoch - 329ms/step
Epoch 284/1000
2023-10-24 18:37:02.011 
Epoch 284/1000 
	 loss: 42.0393, MinusLogProbMetric: 42.0393, val_loss: 42.7280, val_MinusLogProbMetric: 42.7280

Epoch 284: val_loss did not improve from 42.51927
196/196 - 63s - loss: 42.0393 - MinusLogProbMetric: 42.0393 - val_loss: 42.7280 - val_MinusLogProbMetric: 42.7280 - lr: 4.1152e-06 - 63s/epoch - 322ms/step
Epoch 285/1000
2023-10-24 18:38:04.331 
Epoch 285/1000 
	 loss: 46.3717, MinusLogProbMetric: 46.3717, val_loss: 43.8351, val_MinusLogProbMetric: 43.8351

Epoch 285: val_loss did not improve from 42.51927
196/196 - 62s - loss: 46.3717 - MinusLogProbMetric: 46.3717 - val_loss: 43.8351 - val_MinusLogProbMetric: 43.8351 - lr: 4.1152e-06 - 62s/epoch - 318ms/step
Epoch 286/1000
2023-10-24 18:39:10.202 
Epoch 286/1000 
	 loss: 42.2427, MinusLogProbMetric: 42.2427, val_loss: 42.4564, val_MinusLogProbMetric: 42.4564

Epoch 286: val_loss improved from 42.51927 to 42.45636, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 67s - loss: 42.2427 - MinusLogProbMetric: 42.2427 - val_loss: 42.4564 - val_MinusLogProbMetric: 42.4564 - lr: 4.1152e-06 - 67s/epoch - 341ms/step
Epoch 287/1000
2023-10-24 18:40:14.957 
Epoch 287/1000 
	 loss: 42.0062, MinusLogProbMetric: 42.0062, val_loss: 42.4190, val_MinusLogProbMetric: 42.4190

Epoch 287: val_loss improved from 42.45636 to 42.41895, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 65s - loss: 42.0062 - MinusLogProbMetric: 42.0062 - val_loss: 42.4190 - val_MinusLogProbMetric: 42.4190 - lr: 4.1152e-06 - 65s/epoch - 330ms/step
Epoch 288/1000
2023-10-24 18:41:20.248 
Epoch 288/1000 
	 loss: 41.9494, MinusLogProbMetric: 41.9494, val_loss: 42.3795, val_MinusLogProbMetric: 42.3795

Epoch 288: val_loss improved from 42.41895 to 42.37947, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 65s - loss: 41.9494 - MinusLogProbMetric: 41.9494 - val_loss: 42.3795 - val_MinusLogProbMetric: 42.3795 - lr: 4.1152e-06 - 65s/epoch - 333ms/step
Epoch 289/1000
2023-10-24 18:42:24.999 
Epoch 289/1000 
	 loss: 41.9145, MinusLogProbMetric: 41.9145, val_loss: 51.5790, val_MinusLogProbMetric: 51.5790

Epoch 289: val_loss did not improve from 42.37947
196/196 - 64s - loss: 41.9145 - MinusLogProbMetric: 41.9145 - val_loss: 51.5790 - val_MinusLogProbMetric: 51.5790 - lr: 4.1152e-06 - 64s/epoch - 326ms/step
Epoch 290/1000
2023-10-24 18:43:30.006 
Epoch 290/1000 
	 loss: 42.8265, MinusLogProbMetric: 42.8265, val_loss: 42.2528, val_MinusLogProbMetric: 42.2528

Epoch 290: val_loss improved from 42.37947 to 42.25276, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 66s - loss: 42.8265 - MinusLogProbMetric: 42.8265 - val_loss: 42.2528 - val_MinusLogProbMetric: 42.2528 - lr: 4.1152e-06 - 66s/epoch - 336ms/step
Epoch 291/1000
2023-10-24 18:44:34.603 
Epoch 291/1000 
	 loss: 41.8671, MinusLogProbMetric: 41.8671, val_loss: 42.3039, val_MinusLogProbMetric: 42.3039

Epoch 291: val_loss did not improve from 42.25276
196/196 - 64s - loss: 41.8671 - MinusLogProbMetric: 41.8671 - val_loss: 42.3039 - val_MinusLogProbMetric: 42.3039 - lr: 4.1152e-06 - 64s/epoch - 325ms/step
Epoch 292/1000
2023-10-24 18:45:39.854 
Epoch 292/1000 
	 loss: 41.8409, MinusLogProbMetric: 41.8409, val_loss: 42.1868, val_MinusLogProbMetric: 42.1868

Epoch 292: val_loss improved from 42.25276 to 42.18675, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 66s - loss: 41.8409 - MinusLogProbMetric: 41.8409 - val_loss: 42.1868 - val_MinusLogProbMetric: 42.1868 - lr: 4.1152e-06 - 66s/epoch - 337ms/step
Epoch 293/1000
2023-10-24 18:46:45.586 
Epoch 293/1000 
	 loss: 41.8069, MinusLogProbMetric: 41.8069, val_loss: 42.4449, val_MinusLogProbMetric: 42.4449

Epoch 293: val_loss did not improve from 42.18675
196/196 - 65s - loss: 41.8069 - MinusLogProbMetric: 41.8069 - val_loss: 42.4449 - val_MinusLogProbMetric: 42.4449 - lr: 4.1152e-06 - 65s/epoch - 331ms/step
Epoch 294/1000
2023-10-24 18:47:50.431 
Epoch 294/1000 
	 loss: 41.8057, MinusLogProbMetric: 41.8057, val_loss: 42.0374, val_MinusLogProbMetric: 42.0374

Epoch 294: val_loss improved from 42.18675 to 42.03741, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 66s - loss: 41.8057 - MinusLogProbMetric: 41.8057 - val_loss: 42.0374 - val_MinusLogProbMetric: 42.0374 - lr: 4.1152e-06 - 66s/epoch - 335ms/step
Epoch 295/1000
2023-10-24 18:48:45.238 
Epoch 295/1000 
	 loss: 41.7502, MinusLogProbMetric: 41.7502, val_loss: 42.1533, val_MinusLogProbMetric: 42.1533

Epoch 295: val_loss did not improve from 42.03741
196/196 - 54s - loss: 41.7502 - MinusLogProbMetric: 41.7502 - val_loss: 42.1533 - val_MinusLogProbMetric: 42.1533 - lr: 4.1152e-06 - 54s/epoch - 275ms/step
Epoch 296/1000
2023-10-24 18:49:35.972 
Epoch 296/1000 
	 loss: 41.7730, MinusLogProbMetric: 41.7730, val_loss: 42.3355, val_MinusLogProbMetric: 42.3355

Epoch 296: val_loss did not improve from 42.03741
196/196 - 51s - loss: 41.7730 - MinusLogProbMetric: 41.7730 - val_loss: 42.3355 - val_MinusLogProbMetric: 42.3355 - lr: 4.1152e-06 - 51s/epoch - 259ms/step
Epoch 297/1000
2023-10-24 18:50:32.997 
Epoch 297/1000 
	 loss: 41.7303, MinusLogProbMetric: 41.7303, val_loss: 42.1779, val_MinusLogProbMetric: 42.1779

Epoch 297: val_loss did not improve from 42.03741
196/196 - 57s - loss: 41.7303 - MinusLogProbMetric: 41.7303 - val_loss: 42.1779 - val_MinusLogProbMetric: 42.1779 - lr: 4.1152e-06 - 57s/epoch - 291ms/step
Epoch 298/1000
2023-10-24 18:51:37.336 
Epoch 298/1000 
	 loss: 41.6871, MinusLogProbMetric: 41.6871, val_loss: 42.1143, val_MinusLogProbMetric: 42.1143

Epoch 298: val_loss did not improve from 42.03741
196/196 - 64s - loss: 41.6871 - MinusLogProbMetric: 41.6871 - val_loss: 42.1143 - val_MinusLogProbMetric: 42.1143 - lr: 4.1152e-06 - 64s/epoch - 328ms/step
Epoch 299/1000
2023-10-24 18:52:37.802 
Epoch 299/1000 
	 loss: 42.1238, MinusLogProbMetric: 42.1238, val_loss: 42.0076, val_MinusLogProbMetric: 42.0076

Epoch 299: val_loss improved from 42.03741 to 42.00761, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 61s - loss: 42.1238 - MinusLogProbMetric: 42.1238 - val_loss: 42.0076 - val_MinusLogProbMetric: 42.0076 - lr: 4.1152e-06 - 61s/epoch - 313ms/step
Epoch 300/1000
2023-10-24 18:53:44.020 
Epoch 300/1000 
	 loss: 41.6361, MinusLogProbMetric: 41.6361, val_loss: 42.1585, val_MinusLogProbMetric: 42.1585

Epoch 300: val_loss did not improve from 42.00761
196/196 - 65s - loss: 41.6361 - MinusLogProbMetric: 41.6361 - val_loss: 42.1585 - val_MinusLogProbMetric: 42.1585 - lr: 4.1152e-06 - 65s/epoch - 334ms/step
Epoch 301/1000
2023-10-24 18:54:47.416 
Epoch 301/1000 
	 loss: 43.8125, MinusLogProbMetric: 43.8125, val_loss: 42.2714, val_MinusLogProbMetric: 42.2714

Epoch 301: val_loss did not improve from 42.00761
196/196 - 63s - loss: 43.8125 - MinusLogProbMetric: 43.8125 - val_loss: 42.2714 - val_MinusLogProbMetric: 42.2714 - lr: 4.1152e-06 - 63s/epoch - 323ms/step
Epoch 302/1000
2023-10-24 18:55:49.477 
Epoch 302/1000 
	 loss: 41.8362, MinusLogProbMetric: 41.8362, val_loss: 42.1902, val_MinusLogProbMetric: 42.1902

Epoch 302: val_loss did not improve from 42.00761
196/196 - 62s - loss: 41.8362 - MinusLogProbMetric: 41.8362 - val_loss: 42.1902 - val_MinusLogProbMetric: 42.1902 - lr: 4.1152e-06 - 62s/epoch - 317ms/step
Epoch 303/1000
2023-10-24 18:56:50.680 
Epoch 303/1000 
	 loss: 42.3212, MinusLogProbMetric: 42.3212, val_loss: 42.0681, val_MinusLogProbMetric: 42.0681

Epoch 303: val_loss did not improve from 42.00761
196/196 - 61s - loss: 42.3212 - MinusLogProbMetric: 42.3212 - val_loss: 42.0681 - val_MinusLogProbMetric: 42.0681 - lr: 4.1152e-06 - 61s/epoch - 312ms/step
Epoch 304/1000
2023-10-24 18:57:54.101 
Epoch 304/1000 
	 loss: 41.6528, MinusLogProbMetric: 41.6528, val_loss: 42.1784, val_MinusLogProbMetric: 42.1784

Epoch 304: val_loss did not improve from 42.00761
196/196 - 63s - loss: 41.6528 - MinusLogProbMetric: 41.6528 - val_loss: 42.1784 - val_MinusLogProbMetric: 42.1784 - lr: 4.1152e-06 - 63s/epoch - 324ms/step
Epoch 305/1000
2023-10-24 18:58:58.700 
Epoch 305/1000 
	 loss: 41.6218, MinusLogProbMetric: 41.6218, val_loss: 42.0442, val_MinusLogProbMetric: 42.0442

Epoch 305: val_loss did not improve from 42.00761
196/196 - 65s - loss: 41.6218 - MinusLogProbMetric: 41.6218 - val_loss: 42.0442 - val_MinusLogProbMetric: 42.0442 - lr: 4.1152e-06 - 65s/epoch - 330ms/step
Epoch 306/1000
2023-10-24 19:00:02.928 
Epoch 306/1000 
	 loss: 42.9988, MinusLogProbMetric: 42.9988, val_loss: 43.0216, val_MinusLogProbMetric: 43.0216

Epoch 306: val_loss did not improve from 42.00761
196/196 - 64s - loss: 42.9988 - MinusLogProbMetric: 42.9988 - val_loss: 43.0216 - val_MinusLogProbMetric: 43.0216 - lr: 4.1152e-06 - 64s/epoch - 328ms/step
Epoch 307/1000
2023-10-24 19:01:06.161 
Epoch 307/1000 
	 loss: 41.6531, MinusLogProbMetric: 41.6531, val_loss: 42.0637, val_MinusLogProbMetric: 42.0637

Epoch 307: val_loss did not improve from 42.00761
196/196 - 63s - loss: 41.6531 - MinusLogProbMetric: 41.6531 - val_loss: 42.0637 - val_MinusLogProbMetric: 42.0637 - lr: 4.1152e-06 - 63s/epoch - 323ms/step
Epoch 308/1000
2023-10-24 19:02:08.753 
Epoch 308/1000 
	 loss: 41.4616, MinusLogProbMetric: 41.4616, val_loss: 42.0523, val_MinusLogProbMetric: 42.0523

Epoch 308: val_loss did not improve from 42.00761
196/196 - 63s - loss: 41.4616 - MinusLogProbMetric: 41.4616 - val_loss: 42.0523 - val_MinusLogProbMetric: 42.0523 - lr: 4.1152e-06 - 63s/epoch - 319ms/step
Epoch 309/1000
2023-10-24 19:03:13.119 
Epoch 309/1000 
	 loss: 41.4219, MinusLogProbMetric: 41.4219, val_loss: 41.8126, val_MinusLogProbMetric: 41.8126

Epoch 309: val_loss improved from 42.00761 to 41.81259, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 65s - loss: 41.4219 - MinusLogProbMetric: 41.4219 - val_loss: 41.8126 - val_MinusLogProbMetric: 41.8126 - lr: 4.1152e-06 - 65s/epoch - 333ms/step
Epoch 310/1000
2023-10-24 19:04:17.180 
Epoch 310/1000 
	 loss: 41.4279, MinusLogProbMetric: 41.4279, val_loss: 42.0163, val_MinusLogProbMetric: 42.0163

Epoch 310: val_loss did not improve from 41.81259
196/196 - 63s - loss: 41.4279 - MinusLogProbMetric: 41.4279 - val_loss: 42.0163 - val_MinusLogProbMetric: 42.0163 - lr: 4.1152e-06 - 63s/epoch - 323ms/step
Epoch 311/1000
2023-10-24 19:05:21.394 
Epoch 311/1000 
	 loss: 41.3570, MinusLogProbMetric: 41.3570, val_loss: 41.8108, val_MinusLogProbMetric: 41.8108

Epoch 311: val_loss improved from 41.81259 to 41.81079, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 65s - loss: 41.3570 - MinusLogProbMetric: 41.3570 - val_loss: 41.8108 - val_MinusLogProbMetric: 41.8108 - lr: 4.1152e-06 - 65s/epoch - 332ms/step
Epoch 312/1000
2023-10-24 19:06:25.474 
Epoch 312/1000 
	 loss: 41.4033, MinusLogProbMetric: 41.4033, val_loss: 41.9225, val_MinusLogProbMetric: 41.9225

Epoch 312: val_loss did not improve from 41.81079
196/196 - 63s - loss: 41.4033 - MinusLogProbMetric: 41.4033 - val_loss: 41.9225 - val_MinusLogProbMetric: 41.9225 - lr: 4.1152e-06 - 63s/epoch - 322ms/step
Epoch 313/1000
2023-10-24 19:07:31.865 
Epoch 313/1000 
	 loss: 41.5663, MinusLogProbMetric: 41.5663, val_loss: 41.7159, val_MinusLogProbMetric: 41.7159

Epoch 313: val_loss improved from 41.81079 to 41.71589, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 67s - loss: 41.5663 - MinusLogProbMetric: 41.5663 - val_loss: 41.7159 - val_MinusLogProbMetric: 41.7159 - lr: 4.1152e-06 - 67s/epoch - 344ms/step
Epoch 314/1000
2023-10-24 19:08:35.641 
Epoch 314/1000 
	 loss: 41.3403, MinusLogProbMetric: 41.3403, val_loss: 41.7996, val_MinusLogProbMetric: 41.7996

Epoch 314: val_loss did not improve from 41.71589
196/196 - 63s - loss: 41.3403 - MinusLogProbMetric: 41.3403 - val_loss: 41.7996 - val_MinusLogProbMetric: 41.7996 - lr: 4.1152e-06 - 63s/epoch - 320ms/step
Epoch 315/1000
2023-10-24 19:09:39.564 
Epoch 315/1000 
	 loss: 41.2889, MinusLogProbMetric: 41.2889, val_loss: 41.8944, val_MinusLogProbMetric: 41.8944

Epoch 315: val_loss did not improve from 41.71589
196/196 - 64s - loss: 41.2889 - MinusLogProbMetric: 41.2889 - val_loss: 41.8944 - val_MinusLogProbMetric: 41.8944 - lr: 4.1152e-06 - 64s/epoch - 326ms/step
Epoch 316/1000
2023-10-24 19:10:41.959 
Epoch 316/1000 
	 loss: 41.2854, MinusLogProbMetric: 41.2854, val_loss: 41.6583, val_MinusLogProbMetric: 41.6583

Epoch 316: val_loss improved from 41.71589 to 41.65834, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 63s - loss: 41.2854 - MinusLogProbMetric: 41.2854 - val_loss: 41.6583 - val_MinusLogProbMetric: 41.6583 - lr: 4.1152e-06 - 63s/epoch - 322ms/step
Epoch 317/1000
2023-10-24 19:11:45.539 
Epoch 317/1000 
	 loss: 41.2527, MinusLogProbMetric: 41.2527, val_loss: 41.6504, val_MinusLogProbMetric: 41.6504

Epoch 317: val_loss improved from 41.65834 to 41.65044, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 64s - loss: 41.2527 - MinusLogProbMetric: 41.2527 - val_loss: 41.6504 - val_MinusLogProbMetric: 41.6504 - lr: 4.1152e-06 - 64s/epoch - 325ms/step
Epoch 318/1000
2023-10-24 19:12:49.753 
Epoch 318/1000 
	 loss: 41.5606, MinusLogProbMetric: 41.5606, val_loss: 42.0083, val_MinusLogProbMetric: 42.0083

Epoch 318: val_loss did not improve from 41.65044
196/196 - 63s - loss: 41.5606 - MinusLogProbMetric: 41.5606 - val_loss: 42.0083 - val_MinusLogProbMetric: 42.0083 - lr: 4.1152e-06 - 63s/epoch - 323ms/step
Epoch 319/1000
2023-10-24 19:13:53.950 
Epoch 319/1000 
	 loss: 41.1891, MinusLogProbMetric: 41.1891, val_loss: 41.9054, val_MinusLogProbMetric: 41.9054

Epoch 319: val_loss did not improve from 41.65044
196/196 - 64s - loss: 41.1891 - MinusLogProbMetric: 41.1891 - val_loss: 41.9054 - val_MinusLogProbMetric: 41.9054 - lr: 4.1152e-06 - 64s/epoch - 328ms/step
Epoch 320/1000
2023-10-24 19:15:00.210 
Epoch 320/1000 
	 loss: 42.2193, MinusLogProbMetric: 42.2193, val_loss: 41.6355, val_MinusLogProbMetric: 41.6355

Epoch 320: val_loss improved from 41.65044 to 41.63552, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 67s - loss: 42.2193 - MinusLogProbMetric: 42.2193 - val_loss: 41.6355 - val_MinusLogProbMetric: 41.6355 - lr: 4.1152e-06 - 67s/epoch - 342ms/step
Epoch 321/1000
2023-10-24 19:16:03.994 
Epoch 321/1000 
	 loss: 41.1549, MinusLogProbMetric: 41.1549, val_loss: 41.7627, val_MinusLogProbMetric: 41.7627

Epoch 321: val_loss did not improve from 41.63552
196/196 - 63s - loss: 41.1549 - MinusLogProbMetric: 41.1549 - val_loss: 41.7627 - val_MinusLogProbMetric: 41.7627 - lr: 4.1152e-06 - 63s/epoch - 321ms/step
Epoch 322/1000
2023-10-24 19:17:08.923 
Epoch 322/1000 
	 loss: 41.2134, MinusLogProbMetric: 41.2134, val_loss: 41.6057, val_MinusLogProbMetric: 41.6057

Epoch 322: val_loss improved from 41.63552 to 41.60565, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 66s - loss: 41.2134 - MinusLogProbMetric: 41.2134 - val_loss: 41.6057 - val_MinusLogProbMetric: 41.6057 - lr: 4.1152e-06 - 66s/epoch - 335ms/step
Epoch 323/1000
2023-10-24 19:18:12.445 
Epoch 323/1000 
	 loss: 41.0642, MinusLogProbMetric: 41.0642, val_loss: 41.7015, val_MinusLogProbMetric: 41.7015

Epoch 323: val_loss did not improve from 41.60565
196/196 - 63s - loss: 41.0642 - MinusLogProbMetric: 41.0642 - val_loss: 41.7015 - val_MinusLogProbMetric: 41.7015 - lr: 4.1152e-06 - 63s/epoch - 320ms/step
Epoch 324/1000
2023-10-24 19:19:16.782 
Epoch 324/1000 
	 loss: 41.1744, MinusLogProbMetric: 41.1744, val_loss: 41.4589, val_MinusLogProbMetric: 41.4589

Epoch 324: val_loss improved from 41.60565 to 41.45893, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 65s - loss: 41.1744 - MinusLogProbMetric: 41.1744 - val_loss: 41.4589 - val_MinusLogProbMetric: 41.4589 - lr: 4.1152e-06 - 65s/epoch - 332ms/step
Epoch 325/1000
2023-10-24 19:20:21.038 
Epoch 325/1000 
	 loss: 41.0173, MinusLogProbMetric: 41.0173, val_loss: 41.5109, val_MinusLogProbMetric: 41.5109

Epoch 325: val_loss did not improve from 41.45893
196/196 - 63s - loss: 41.0173 - MinusLogProbMetric: 41.0173 - val_loss: 41.5109 - val_MinusLogProbMetric: 41.5109 - lr: 4.1152e-06 - 63s/epoch - 324ms/step
Epoch 326/1000
2023-10-24 19:21:25.833 
Epoch 326/1000 
	 loss: 41.0361, MinusLogProbMetric: 41.0361, val_loss: 41.6256, val_MinusLogProbMetric: 41.6256

Epoch 326: val_loss did not improve from 41.45893
196/196 - 65s - loss: 41.0361 - MinusLogProbMetric: 41.0361 - val_loss: 41.6256 - val_MinusLogProbMetric: 41.6256 - lr: 4.1152e-06 - 65s/epoch - 331ms/step
Epoch 327/1000
2023-10-24 19:22:28.583 
Epoch 327/1000 
	 loss: 41.6478, MinusLogProbMetric: 41.6478, val_loss: 41.5144, val_MinusLogProbMetric: 41.5144

Epoch 327: val_loss did not improve from 41.45893
196/196 - 63s - loss: 41.6478 - MinusLogProbMetric: 41.6478 - val_loss: 41.5144 - val_MinusLogProbMetric: 41.5144 - lr: 4.1152e-06 - 63s/epoch - 320ms/step
Epoch 328/1000
2023-10-24 19:23:33.498 
Epoch 328/1000 
	 loss: 41.0316, MinusLogProbMetric: 41.0316, val_loss: 41.4762, val_MinusLogProbMetric: 41.4762

Epoch 328: val_loss did not improve from 41.45893
196/196 - 65s - loss: 41.0316 - MinusLogProbMetric: 41.0316 - val_loss: 41.4762 - val_MinusLogProbMetric: 41.4762 - lr: 4.1152e-06 - 65s/epoch - 331ms/step
Epoch 329/1000
2023-10-24 19:24:37.350 
Epoch 329/1000 
	 loss: 40.9217, MinusLogProbMetric: 40.9217, val_loss: 41.4080, val_MinusLogProbMetric: 41.4080

Epoch 329: val_loss improved from 41.45893 to 41.40800, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 65s - loss: 40.9217 - MinusLogProbMetric: 40.9217 - val_loss: 41.4080 - val_MinusLogProbMetric: 41.4080 - lr: 4.1152e-06 - 65s/epoch - 330ms/step
Epoch 330/1000
2023-10-24 19:25:41.379 
Epoch 330/1000 
	 loss: 40.9794, MinusLogProbMetric: 40.9794, val_loss: 41.3514, val_MinusLogProbMetric: 41.3514

Epoch 330: val_loss improved from 41.40800 to 41.35139, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 64s - loss: 40.9794 - MinusLogProbMetric: 40.9794 - val_loss: 41.3514 - val_MinusLogProbMetric: 41.3514 - lr: 4.1152e-06 - 64s/epoch - 326ms/step
Epoch 331/1000
2023-10-24 19:26:45.959 
Epoch 331/1000 
	 loss: 41.3134, MinusLogProbMetric: 41.3134, val_loss: 41.4842, val_MinusLogProbMetric: 41.4842

Epoch 331: val_loss did not improve from 41.35139
196/196 - 64s - loss: 41.3134 - MinusLogProbMetric: 41.3134 - val_loss: 41.4842 - val_MinusLogProbMetric: 41.4842 - lr: 4.1152e-06 - 64s/epoch - 326ms/step
Epoch 332/1000
2023-10-24 19:27:49.990 
Epoch 332/1000 
	 loss: 41.0225, MinusLogProbMetric: 41.0225, val_loss: 41.2030, val_MinusLogProbMetric: 41.2030

Epoch 332: val_loss improved from 41.35139 to 41.20300, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 65s - loss: 41.0225 - MinusLogProbMetric: 41.0225 - val_loss: 41.2030 - val_MinusLogProbMetric: 41.2030 - lr: 4.1152e-06 - 65s/epoch - 332ms/step
Epoch 333/1000
2023-10-24 19:28:53.907 
Epoch 333/1000 
	 loss: 41.0536, MinusLogProbMetric: 41.0536, val_loss: 41.8363, val_MinusLogProbMetric: 41.8363

Epoch 333: val_loss did not improve from 41.20300
196/196 - 63s - loss: 41.0536 - MinusLogProbMetric: 41.0536 - val_loss: 41.8363 - val_MinusLogProbMetric: 41.8363 - lr: 4.1152e-06 - 63s/epoch - 321ms/step
Epoch 334/1000
2023-10-24 19:29:57.510 
Epoch 334/1000 
	 loss: 40.9219, MinusLogProbMetric: 40.9219, val_loss: 41.7505, val_MinusLogProbMetric: 41.7505

Epoch 334: val_loss did not improve from 41.20300
196/196 - 64s - loss: 40.9219 - MinusLogProbMetric: 40.9219 - val_loss: 41.7505 - val_MinusLogProbMetric: 41.7505 - lr: 4.1152e-06 - 64s/epoch - 324ms/step
Epoch 335/1000
2023-10-24 19:31:01.423 
Epoch 335/1000 
	 loss: 41.1676, MinusLogProbMetric: 41.1676, val_loss: 41.2378, val_MinusLogProbMetric: 41.2378

Epoch 335: val_loss did not improve from 41.20300
196/196 - 64s - loss: 41.1676 - MinusLogProbMetric: 41.1676 - val_loss: 41.2378 - val_MinusLogProbMetric: 41.2378 - lr: 4.1152e-06 - 64s/epoch - 326ms/step
Epoch 336/1000
2023-10-24 19:32:06.530 
Epoch 336/1000 
	 loss: 41.0576, MinusLogProbMetric: 41.0576, val_loss: 41.4995, val_MinusLogProbMetric: 41.4995

Epoch 336: val_loss did not improve from 41.20300
196/196 - 65s - loss: 41.0576 - MinusLogProbMetric: 41.0576 - val_loss: 41.4995 - val_MinusLogProbMetric: 41.4995 - lr: 4.1152e-06 - 65s/epoch - 332ms/step
Epoch 337/1000
2023-10-24 19:33:10.610 
Epoch 337/1000 
	 loss: 40.7644, MinusLogProbMetric: 40.7644, val_loss: 41.2948, val_MinusLogProbMetric: 41.2948

Epoch 337: val_loss did not improve from 41.20300
196/196 - 64s - loss: 40.7644 - MinusLogProbMetric: 40.7644 - val_loss: 41.2948 - val_MinusLogProbMetric: 41.2948 - lr: 4.1152e-06 - 64s/epoch - 327ms/step
Epoch 338/1000
2023-10-24 19:34:13.802 
Epoch 338/1000 
	 loss: 40.7154, MinusLogProbMetric: 40.7154, val_loss: 41.0481, val_MinusLogProbMetric: 41.0481

Epoch 338: val_loss improved from 41.20300 to 41.04815, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 64s - loss: 40.7154 - MinusLogProbMetric: 40.7154 - val_loss: 41.0481 - val_MinusLogProbMetric: 41.0481 - lr: 4.1152e-06 - 64s/epoch - 326ms/step
Epoch 339/1000
2023-10-24 19:35:19.772 
Epoch 339/1000 
	 loss: 40.7098, MinusLogProbMetric: 40.7098, val_loss: 41.1044, val_MinusLogProbMetric: 41.1044

Epoch 339: val_loss did not improve from 41.04815
196/196 - 65s - loss: 40.7098 - MinusLogProbMetric: 40.7098 - val_loss: 41.1044 - val_MinusLogProbMetric: 41.1044 - lr: 4.1152e-06 - 65s/epoch - 333ms/step
Epoch 340/1000
2023-10-24 19:36:23.771 
Epoch 340/1000 
	 loss: 41.0734, MinusLogProbMetric: 41.0734, val_loss: 41.1445, val_MinusLogProbMetric: 41.1445

Epoch 340: val_loss did not improve from 41.04815
196/196 - 64s - loss: 41.0734 - MinusLogProbMetric: 41.0734 - val_loss: 41.1445 - val_MinusLogProbMetric: 41.1445 - lr: 4.1152e-06 - 64s/epoch - 327ms/step
Epoch 341/1000
2023-10-24 19:37:26.158 
Epoch 341/1000 
	 loss: 40.6649, MinusLogProbMetric: 40.6649, val_loss: 41.3190, val_MinusLogProbMetric: 41.3190

Epoch 341: val_loss did not improve from 41.04815
196/196 - 62s - loss: 40.6649 - MinusLogProbMetric: 40.6649 - val_loss: 41.3190 - val_MinusLogProbMetric: 41.3190 - lr: 4.1152e-06 - 62s/epoch - 318ms/step
Epoch 342/1000
2023-10-24 19:38:29.982 
Epoch 342/1000 
	 loss: 40.6811, MinusLogProbMetric: 40.6811, val_loss: 41.2220, val_MinusLogProbMetric: 41.2220

Epoch 342: val_loss did not improve from 41.04815
196/196 - 64s - loss: 40.6811 - MinusLogProbMetric: 40.6811 - val_loss: 41.2220 - val_MinusLogProbMetric: 41.2220 - lr: 4.1152e-06 - 64s/epoch - 326ms/step
Epoch 343/1000
2023-10-24 19:39:32.939 
Epoch 343/1000 
	 loss: 40.7858, MinusLogProbMetric: 40.7858, val_loss: 41.1672, val_MinusLogProbMetric: 41.1672

Epoch 343: val_loss did not improve from 41.04815
196/196 - 63s - loss: 40.7858 - MinusLogProbMetric: 40.7858 - val_loss: 41.1672 - val_MinusLogProbMetric: 41.1672 - lr: 4.1152e-06 - 63s/epoch - 321ms/step
Epoch 344/1000
2023-10-24 19:40:35.568 
Epoch 344/1000 
	 loss: 40.9131, MinusLogProbMetric: 40.9131, val_loss: 41.0753, val_MinusLogProbMetric: 41.0753

Epoch 344: val_loss did not improve from 41.04815
196/196 - 63s - loss: 40.9131 - MinusLogProbMetric: 40.9131 - val_loss: 41.0753 - val_MinusLogProbMetric: 41.0753 - lr: 4.1152e-06 - 63s/epoch - 320ms/step
Epoch 345/1000
2023-10-24 19:41:37.156 
Epoch 345/1000 
	 loss: 40.5710, MinusLogProbMetric: 40.5710, val_loss: 41.0551, val_MinusLogProbMetric: 41.0551

Epoch 345: val_loss did not improve from 41.04815
196/196 - 62s - loss: 40.5710 - MinusLogProbMetric: 40.5710 - val_loss: 41.0551 - val_MinusLogProbMetric: 41.0551 - lr: 4.1152e-06 - 62s/epoch - 314ms/step
Epoch 346/1000
2023-10-24 19:42:40.206 
Epoch 346/1000 
	 loss: 40.8510, MinusLogProbMetric: 40.8510, val_loss: 40.9706, val_MinusLogProbMetric: 40.9706

Epoch 346: val_loss improved from 41.04815 to 40.97058, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 64s - loss: 40.8510 - MinusLogProbMetric: 40.8510 - val_loss: 40.9706 - val_MinusLogProbMetric: 40.9706 - lr: 4.1152e-06 - 64s/epoch - 326ms/step
Epoch 347/1000
2023-10-24 19:43:43.047 
Epoch 347/1000 
	 loss: 40.5578, MinusLogProbMetric: 40.5578, val_loss: 41.3887, val_MinusLogProbMetric: 41.3887

Epoch 347: val_loss did not improve from 40.97058
196/196 - 62s - loss: 40.5578 - MinusLogProbMetric: 40.5578 - val_loss: 41.3887 - val_MinusLogProbMetric: 41.3887 - lr: 4.1152e-06 - 62s/epoch - 316ms/step
Epoch 348/1000
2023-10-24 19:44:46.017 
Epoch 348/1000 
	 loss: 40.5535, MinusLogProbMetric: 40.5535, val_loss: 41.1015, val_MinusLogProbMetric: 41.1015

Epoch 348: val_loss did not improve from 40.97058
196/196 - 63s - loss: 40.5535 - MinusLogProbMetric: 40.5535 - val_loss: 41.1015 - val_MinusLogProbMetric: 41.1015 - lr: 4.1152e-06 - 63s/epoch - 321ms/step
Epoch 349/1000
2023-10-24 19:45:50.394 
Epoch 349/1000 
	 loss: 40.5087, MinusLogProbMetric: 40.5087, val_loss: 41.0207, val_MinusLogProbMetric: 41.0207

Epoch 349: val_loss did not improve from 40.97058
196/196 - 64s - loss: 40.5087 - MinusLogProbMetric: 40.5087 - val_loss: 41.0207 - val_MinusLogProbMetric: 41.0207 - lr: 4.1152e-06 - 64s/epoch - 328ms/step
Epoch 350/1000
2023-10-24 19:46:53.127 
Epoch 350/1000 
	 loss: 40.4877, MinusLogProbMetric: 40.4877, val_loss: 41.2236, val_MinusLogProbMetric: 41.2236

Epoch 350: val_loss did not improve from 40.97058
196/196 - 63s - loss: 40.4877 - MinusLogProbMetric: 40.4877 - val_loss: 41.2236 - val_MinusLogProbMetric: 41.2236 - lr: 4.1152e-06 - 63s/epoch - 320ms/step
Epoch 351/1000
2023-10-24 19:47:56.313 
Epoch 351/1000 
	 loss: 41.5537, MinusLogProbMetric: 41.5537, val_loss: 40.8142, val_MinusLogProbMetric: 40.8142

Epoch 351: val_loss improved from 40.97058 to 40.81424, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 64s - loss: 41.5537 - MinusLogProbMetric: 41.5537 - val_loss: 40.8142 - val_MinusLogProbMetric: 40.8142 - lr: 4.1152e-06 - 64s/epoch - 328ms/step
Epoch 352/1000
2023-10-24 19:49:00.208 
Epoch 352/1000 
	 loss: 40.4297, MinusLogProbMetric: 40.4297, val_loss: 41.0608, val_MinusLogProbMetric: 41.0608

Epoch 352: val_loss did not improve from 40.81424
196/196 - 63s - loss: 40.4297 - MinusLogProbMetric: 40.4297 - val_loss: 41.0608 - val_MinusLogProbMetric: 41.0608 - lr: 4.1152e-06 - 63s/epoch - 321ms/step
Epoch 353/1000
2023-10-24 19:50:06.145 
Epoch 353/1000 
	 loss: 40.4060, MinusLogProbMetric: 40.4060, val_loss: 41.0699, val_MinusLogProbMetric: 41.0699

Epoch 353: val_loss did not improve from 40.81424
196/196 - 66s - loss: 40.4060 - MinusLogProbMetric: 40.4060 - val_loss: 41.0699 - val_MinusLogProbMetric: 41.0699 - lr: 4.1152e-06 - 66s/epoch - 336ms/step
Epoch 354/1000
2023-10-24 19:51:10.383 
Epoch 354/1000 
	 loss: 40.4409, MinusLogProbMetric: 40.4409, val_loss: 41.2052, val_MinusLogProbMetric: 41.2052

Epoch 354: val_loss did not improve from 40.81424
196/196 - 64s - loss: 40.4409 - MinusLogProbMetric: 40.4409 - val_loss: 41.2052 - val_MinusLogProbMetric: 41.2052 - lr: 4.1152e-06 - 64s/epoch - 328ms/step
Epoch 355/1000
2023-10-24 19:52:10.729 
Epoch 355/1000 
	 loss: 40.4377, MinusLogProbMetric: 40.4377, val_loss: 40.8339, val_MinusLogProbMetric: 40.8339

Epoch 355: val_loss did not improve from 40.81424
196/196 - 60s - loss: 40.4377 - MinusLogProbMetric: 40.4377 - val_loss: 40.8339 - val_MinusLogProbMetric: 40.8339 - lr: 4.1152e-06 - 60s/epoch - 308ms/step
Epoch 356/1000
2023-10-24 19:53:15.310 
Epoch 356/1000 
	 loss: 40.4669, MinusLogProbMetric: 40.4669, val_loss: 41.5283, val_MinusLogProbMetric: 41.5283

Epoch 356: val_loss did not improve from 40.81424
196/196 - 65s - loss: 40.4669 - MinusLogProbMetric: 40.4669 - val_loss: 41.5283 - val_MinusLogProbMetric: 41.5283 - lr: 4.1152e-06 - 65s/epoch - 329ms/step
Epoch 357/1000
2023-10-24 19:54:19.525 
Epoch 357/1000 
	 loss: 40.3339, MinusLogProbMetric: 40.3339, val_loss: 40.6462, val_MinusLogProbMetric: 40.6462

Epoch 357: val_loss improved from 40.81424 to 40.64616, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 65s - loss: 40.3339 - MinusLogProbMetric: 40.3339 - val_loss: 40.6462 - val_MinusLogProbMetric: 40.6462 - lr: 4.1152e-06 - 65s/epoch - 332ms/step
Epoch 358/1000
2023-10-24 19:55:24.962 
Epoch 358/1000 
	 loss: 40.8696, MinusLogProbMetric: 40.8696, val_loss: 40.8603, val_MinusLogProbMetric: 40.8603

Epoch 358: val_loss did not improve from 40.64616
196/196 - 65s - loss: 40.8696 - MinusLogProbMetric: 40.8696 - val_loss: 40.8603 - val_MinusLogProbMetric: 40.8603 - lr: 4.1152e-06 - 65s/epoch - 330ms/step
Epoch 359/1000
2023-10-24 19:56:26.263 
Epoch 359/1000 
	 loss: 40.2983, MinusLogProbMetric: 40.2983, val_loss: 40.8326, val_MinusLogProbMetric: 40.8326

Epoch 359: val_loss did not improve from 40.64616
196/196 - 61s - loss: 40.2983 - MinusLogProbMetric: 40.2983 - val_loss: 40.8326 - val_MinusLogProbMetric: 40.8326 - lr: 4.1152e-06 - 61s/epoch - 313ms/step
Epoch 360/1000
2023-10-24 19:57:30.214 
Epoch 360/1000 
	 loss: 40.2702, MinusLogProbMetric: 40.2702, val_loss: 40.6664, val_MinusLogProbMetric: 40.6664

Epoch 360: val_loss did not improve from 40.64616
196/196 - 64s - loss: 40.2702 - MinusLogProbMetric: 40.2702 - val_loss: 40.6664 - val_MinusLogProbMetric: 40.6664 - lr: 4.1152e-06 - 64s/epoch - 326ms/step
Epoch 361/1000
2023-10-24 19:58:33.983 
Epoch 361/1000 
	 loss: 40.2630, MinusLogProbMetric: 40.2630, val_loss: 40.7399, val_MinusLogProbMetric: 40.7399

Epoch 361: val_loss did not improve from 40.64616
196/196 - 64s - loss: 40.2630 - MinusLogProbMetric: 40.2630 - val_loss: 40.7399 - val_MinusLogProbMetric: 40.7399 - lr: 4.1152e-06 - 64s/epoch - 325ms/step
Epoch 362/1000
2023-10-24 19:59:36.995 
Epoch 362/1000 
	 loss: 40.1930, MinusLogProbMetric: 40.1930, val_loss: 40.9115, val_MinusLogProbMetric: 40.9115

Epoch 362: val_loss did not improve from 40.64616
196/196 - 63s - loss: 40.1930 - MinusLogProbMetric: 40.1930 - val_loss: 40.9115 - val_MinusLogProbMetric: 40.9115 - lr: 4.1152e-06 - 63s/epoch - 321ms/step
Epoch 363/1000
2023-10-24 20:00:41.465 
Epoch 363/1000 
	 loss: 41.4321, MinusLogProbMetric: 41.4321, val_loss: 40.8811, val_MinusLogProbMetric: 40.8811

Epoch 363: val_loss did not improve from 40.64616
196/196 - 64s - loss: 41.4321 - MinusLogProbMetric: 41.4321 - val_loss: 40.8811 - val_MinusLogProbMetric: 40.8811 - lr: 4.1152e-06 - 64s/epoch - 329ms/step
Epoch 364/1000
2023-10-24 20:01:44.212 
Epoch 364/1000 
	 loss: 40.1998, MinusLogProbMetric: 40.1998, val_loss: 40.6174, val_MinusLogProbMetric: 40.6174

Epoch 364: val_loss improved from 40.64616 to 40.61737, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 64s - loss: 40.1998 - MinusLogProbMetric: 40.1998 - val_loss: 40.6174 - val_MinusLogProbMetric: 40.6174 - lr: 4.1152e-06 - 64s/epoch - 325ms/step
Epoch 365/1000
2023-10-24 20:02:45.758 
Epoch 365/1000 
	 loss: 40.1915, MinusLogProbMetric: 40.1915, val_loss: 41.0298, val_MinusLogProbMetric: 41.0298

Epoch 365: val_loss did not improve from 40.61737
196/196 - 61s - loss: 40.1915 - MinusLogProbMetric: 40.1915 - val_loss: 41.0298 - val_MinusLogProbMetric: 41.0298 - lr: 4.1152e-06 - 61s/epoch - 309ms/step
Epoch 366/1000
2023-10-24 20:03:47.317 
Epoch 366/1000 
	 loss: 40.2049, MinusLogProbMetric: 40.2049, val_loss: 41.3902, val_MinusLogProbMetric: 41.3902

Epoch 366: val_loss did not improve from 40.61737
196/196 - 62s - loss: 40.2049 - MinusLogProbMetric: 40.2049 - val_loss: 41.3902 - val_MinusLogProbMetric: 41.3902 - lr: 4.1152e-06 - 62s/epoch - 314ms/step
Epoch 367/1000
2023-10-24 20:04:50.124 
Epoch 367/1000 
	 loss: 40.1806, MinusLogProbMetric: 40.1806, val_loss: 40.6202, val_MinusLogProbMetric: 40.6202

Epoch 367: val_loss did not improve from 40.61737
196/196 - 63s - loss: 40.1806 - MinusLogProbMetric: 40.1806 - val_loss: 40.6202 - val_MinusLogProbMetric: 40.6202 - lr: 4.1152e-06 - 63s/epoch - 320ms/step
Epoch 368/1000
2023-10-24 20:05:56.202 
Epoch 368/1000 
	 loss: 40.1279, MinusLogProbMetric: 40.1279, val_loss: 40.4875, val_MinusLogProbMetric: 40.4875

Epoch 368: val_loss improved from 40.61737 to 40.48749, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 67s - loss: 40.1279 - MinusLogProbMetric: 40.1279 - val_loss: 40.4875 - val_MinusLogProbMetric: 40.4875 - lr: 4.1152e-06 - 67s/epoch - 342ms/step
Epoch 369/1000
2023-10-24 20:07:02.169 
Epoch 369/1000 
	 loss: 40.0748, MinusLogProbMetric: 40.0748, val_loss: 40.5255, val_MinusLogProbMetric: 40.5255

Epoch 369: val_loss did not improve from 40.48749
196/196 - 65s - loss: 40.0748 - MinusLogProbMetric: 40.0748 - val_loss: 40.5255 - val_MinusLogProbMetric: 40.5255 - lr: 4.1152e-06 - 65s/epoch - 331ms/step
Epoch 370/1000
2023-10-24 20:08:07.195 
Epoch 370/1000 
	 loss: 40.0562, MinusLogProbMetric: 40.0562, val_loss: 40.9605, val_MinusLogProbMetric: 40.9605

Epoch 370: val_loss did not improve from 40.48749
196/196 - 65s - loss: 40.0562 - MinusLogProbMetric: 40.0562 - val_loss: 40.9605 - val_MinusLogProbMetric: 40.9605 - lr: 4.1152e-06 - 65s/epoch - 332ms/step
Epoch 371/1000
2023-10-24 20:09:09.867 
Epoch 371/1000 
	 loss: 40.0318, MinusLogProbMetric: 40.0318, val_loss: 40.5295, val_MinusLogProbMetric: 40.5295

Epoch 371: val_loss did not improve from 40.48749
196/196 - 63s - loss: 40.0318 - MinusLogProbMetric: 40.0318 - val_loss: 40.5295 - val_MinusLogProbMetric: 40.5295 - lr: 4.1152e-06 - 63s/epoch - 320ms/step
Epoch 372/1000
2023-10-24 20:10:14.894 
Epoch 372/1000 
	 loss: 40.0238, MinusLogProbMetric: 40.0238, val_loss: 40.4442, val_MinusLogProbMetric: 40.4442

Epoch 372: val_loss improved from 40.48749 to 40.44415, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 66s - loss: 40.0238 - MinusLogProbMetric: 40.0238 - val_loss: 40.4442 - val_MinusLogProbMetric: 40.4442 - lr: 4.1152e-06 - 66s/epoch - 336ms/step
Epoch 373/1000
2023-10-24 20:11:20.960 
Epoch 373/1000 
	 loss: 40.1910, MinusLogProbMetric: 40.1910, val_loss: 40.4519, val_MinusLogProbMetric: 40.4519

Epoch 373: val_loss did not improve from 40.44415
196/196 - 65s - loss: 40.1910 - MinusLogProbMetric: 40.1910 - val_loss: 40.4519 - val_MinusLogProbMetric: 40.4519 - lr: 4.1152e-06 - 65s/epoch - 333ms/step
Epoch 374/1000
2023-10-24 20:12:25.369 
Epoch 374/1000 
	 loss: 39.9503, MinusLogProbMetric: 39.9503, val_loss: 40.4113, val_MinusLogProbMetric: 40.4113

Epoch 374: val_loss improved from 40.44415 to 40.41134, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 65s - loss: 39.9503 - MinusLogProbMetric: 39.9503 - val_loss: 40.4113 - val_MinusLogProbMetric: 40.4113 - lr: 4.1152e-06 - 65s/epoch - 333ms/step
Epoch 375/1000
2023-10-24 20:13:31.219 
Epoch 375/1000 
	 loss: 40.1552, MinusLogProbMetric: 40.1552, val_loss: 40.6871, val_MinusLogProbMetric: 40.6871

Epoch 375: val_loss did not improve from 40.41134
196/196 - 65s - loss: 40.1552 - MinusLogProbMetric: 40.1552 - val_loss: 40.6871 - val_MinusLogProbMetric: 40.6871 - lr: 4.1152e-06 - 65s/epoch - 332ms/step
Epoch 376/1000
2023-10-24 20:14:32.227 
Epoch 376/1000 
	 loss: 40.2219, MinusLogProbMetric: 40.2219, val_loss: 41.0565, val_MinusLogProbMetric: 41.0565

Epoch 376: val_loss did not improve from 40.41134
196/196 - 61s - loss: 40.2219 - MinusLogProbMetric: 40.2219 - val_loss: 41.0565 - val_MinusLogProbMetric: 41.0565 - lr: 4.1152e-06 - 61s/epoch - 311ms/step
Epoch 377/1000
2023-10-24 20:15:36.714 
Epoch 377/1000 
	 loss: 39.9599, MinusLogProbMetric: 39.9599, val_loss: 40.3345, val_MinusLogProbMetric: 40.3345

Epoch 377: val_loss improved from 40.41134 to 40.33445, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 65s - loss: 39.9599 - MinusLogProbMetric: 39.9599 - val_loss: 40.3345 - val_MinusLogProbMetric: 40.3345 - lr: 4.1152e-06 - 65s/epoch - 333ms/step
Epoch 378/1000
2023-10-24 20:16:41.412 
Epoch 378/1000 
	 loss: 39.9153, MinusLogProbMetric: 39.9153, val_loss: 40.3906, val_MinusLogProbMetric: 40.3906

Epoch 378: val_loss did not improve from 40.33445
196/196 - 64s - loss: 39.9153 - MinusLogProbMetric: 39.9153 - val_loss: 40.3906 - val_MinusLogProbMetric: 40.3906 - lr: 4.1152e-06 - 64s/epoch - 326ms/step
Epoch 379/1000
2023-10-24 20:17:43.595 
Epoch 379/1000 
	 loss: 40.0781, MinusLogProbMetric: 40.0781, val_loss: 40.9947, val_MinusLogProbMetric: 40.9947

Epoch 379: val_loss did not improve from 40.33445
196/196 - 62s - loss: 40.0781 - MinusLogProbMetric: 40.0781 - val_loss: 40.9947 - val_MinusLogProbMetric: 40.9947 - lr: 4.1152e-06 - 62s/epoch - 317ms/step
Epoch 380/1000
2023-10-24 20:18:47.450 
Epoch 380/1000 
	 loss: 39.8905, MinusLogProbMetric: 39.8905, val_loss: 40.6250, val_MinusLogProbMetric: 40.6250

Epoch 380: val_loss did not improve from 40.33445
196/196 - 64s - loss: 39.8905 - MinusLogProbMetric: 39.8905 - val_loss: 40.6250 - val_MinusLogProbMetric: 40.6250 - lr: 4.1152e-06 - 64s/epoch - 326ms/step
Epoch 381/1000
2023-10-24 20:19:51.380 
Epoch 381/1000 
	 loss: 39.9206, MinusLogProbMetric: 39.9206, val_loss: 40.2203, val_MinusLogProbMetric: 40.2203

Epoch 381: val_loss improved from 40.33445 to 40.22035, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 65s - loss: 39.9206 - MinusLogProbMetric: 39.9206 - val_loss: 40.2203 - val_MinusLogProbMetric: 40.2203 - lr: 4.1152e-06 - 65s/epoch - 331ms/step
Epoch 382/1000
2023-10-24 20:20:54.910 
Epoch 382/1000 
	 loss: 39.7827, MinusLogProbMetric: 39.7827, val_loss: 41.0777, val_MinusLogProbMetric: 41.0777

Epoch 382: val_loss did not improve from 40.22035
196/196 - 63s - loss: 39.7827 - MinusLogProbMetric: 39.7827 - val_loss: 41.0777 - val_MinusLogProbMetric: 41.0777 - lr: 4.1152e-06 - 63s/epoch - 319ms/step
Epoch 383/1000
2023-10-24 20:21:59.743 
Epoch 383/1000 
	 loss: 39.8000, MinusLogProbMetric: 39.8000, val_loss: 40.3360, val_MinusLogProbMetric: 40.3360

Epoch 383: val_loss did not improve from 40.22035
196/196 - 65s - loss: 39.8000 - MinusLogProbMetric: 39.8000 - val_loss: 40.3360 - val_MinusLogProbMetric: 40.3360 - lr: 4.1152e-06 - 65s/epoch - 331ms/step
Epoch 384/1000
2023-10-24 20:23:03.918 
Epoch 384/1000 
	 loss: 39.8252, MinusLogProbMetric: 39.8252, val_loss: 40.4509, val_MinusLogProbMetric: 40.4509

Epoch 384: val_loss did not improve from 40.22035
196/196 - 64s - loss: 39.8252 - MinusLogProbMetric: 39.8252 - val_loss: 40.4509 - val_MinusLogProbMetric: 40.4509 - lr: 4.1152e-06 - 64s/epoch - 327ms/step
Epoch 385/1000
2023-10-24 20:24:07.231 
Epoch 385/1000 
	 loss: 39.7370, MinusLogProbMetric: 39.7370, val_loss: 40.5606, val_MinusLogProbMetric: 40.5606

Epoch 385: val_loss did not improve from 40.22035
196/196 - 63s - loss: 39.7370 - MinusLogProbMetric: 39.7370 - val_loss: 40.5606 - val_MinusLogProbMetric: 40.5606 - lr: 4.1152e-06 - 63s/epoch - 323ms/step
Epoch 386/1000
2023-10-24 20:25:10.189 
Epoch 386/1000 
	 loss: 39.8364, MinusLogProbMetric: 39.8364, val_loss: 40.1057, val_MinusLogProbMetric: 40.1057

Epoch 386: val_loss improved from 40.22035 to 40.10573, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 64s - loss: 39.8364 - MinusLogProbMetric: 39.8364 - val_loss: 40.1057 - val_MinusLogProbMetric: 40.1057 - lr: 4.1152e-06 - 64s/epoch - 325ms/step
Epoch 387/1000
2023-10-24 20:26:15.882 
Epoch 387/1000 
	 loss: 39.7102, MinusLogProbMetric: 39.7102, val_loss: 40.8881, val_MinusLogProbMetric: 40.8881

Epoch 387: val_loss did not improve from 40.10573
196/196 - 65s - loss: 39.7102 - MinusLogProbMetric: 39.7102 - val_loss: 40.8881 - val_MinusLogProbMetric: 40.8881 - lr: 4.1152e-06 - 65s/epoch - 331ms/step
Epoch 388/1000
2023-10-24 20:27:18.363 
Epoch 388/1000 
	 loss: 40.3707, MinusLogProbMetric: 40.3707, val_loss: 40.6666, val_MinusLogProbMetric: 40.6666

Epoch 388: val_loss did not improve from 40.10573
196/196 - 62s - loss: 40.3707 - MinusLogProbMetric: 40.3707 - val_loss: 40.6666 - val_MinusLogProbMetric: 40.6666 - lr: 4.1152e-06 - 62s/epoch - 319ms/step
Epoch 389/1000
2023-10-24 20:28:23.289 
Epoch 389/1000 
	 loss: 39.6951, MinusLogProbMetric: 39.6951, val_loss: 40.6643, val_MinusLogProbMetric: 40.6643

Epoch 389: val_loss did not improve from 40.10573
196/196 - 65s - loss: 39.6951 - MinusLogProbMetric: 39.6951 - val_loss: 40.6643 - val_MinusLogProbMetric: 40.6643 - lr: 4.1152e-06 - 65s/epoch - 331ms/step
Epoch 390/1000
2023-10-24 20:29:27.494 
Epoch 390/1000 
	 loss: 39.6918, MinusLogProbMetric: 39.6918, val_loss: 40.1029, val_MinusLogProbMetric: 40.1029

Epoch 390: val_loss improved from 40.10573 to 40.10286, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 65s - loss: 39.6918 - MinusLogProbMetric: 39.6918 - val_loss: 40.1029 - val_MinusLogProbMetric: 40.1029 - lr: 4.1152e-06 - 65s/epoch - 332ms/step
Epoch 391/1000
2023-10-24 20:30:33.761 
Epoch 391/1000 
	 loss: 39.6190, MinusLogProbMetric: 39.6190, val_loss: 40.0493, val_MinusLogProbMetric: 40.0493

Epoch 391: val_loss improved from 40.10286 to 40.04929, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 66s - loss: 39.6190 - MinusLogProbMetric: 39.6190 - val_loss: 40.0493 - val_MinusLogProbMetric: 40.0493 - lr: 4.1152e-06 - 66s/epoch - 338ms/step
Epoch 392/1000
2023-10-24 20:31:35.468 
Epoch 392/1000 
	 loss: 39.7246, MinusLogProbMetric: 39.7246, val_loss: 40.2100, val_MinusLogProbMetric: 40.2100

Epoch 392: val_loss did not improve from 40.04929
196/196 - 61s - loss: 39.7246 - MinusLogProbMetric: 39.7246 - val_loss: 40.2100 - val_MinusLogProbMetric: 40.2100 - lr: 4.1152e-06 - 61s/epoch - 311ms/step
Epoch 393/1000
2023-10-24 20:32:35.521 
Epoch 393/1000 
	 loss: 39.5852, MinusLogProbMetric: 39.5852, val_loss: 40.6150, val_MinusLogProbMetric: 40.6150

Epoch 393: val_loss did not improve from 40.04929
196/196 - 60s - loss: 39.5852 - MinusLogProbMetric: 39.5852 - val_loss: 40.6150 - val_MinusLogProbMetric: 40.6150 - lr: 4.1152e-06 - 60s/epoch - 306ms/step
Epoch 394/1000
2023-10-24 20:33:38.302 
Epoch 394/1000 
	 loss: 39.6242, MinusLogProbMetric: 39.6242, val_loss: 40.1216, val_MinusLogProbMetric: 40.1216

Epoch 394: val_loss did not improve from 40.04929
196/196 - 63s - loss: 39.6242 - MinusLogProbMetric: 39.6242 - val_loss: 40.1216 - val_MinusLogProbMetric: 40.1216 - lr: 4.1152e-06 - 63s/epoch - 320ms/step
Epoch 395/1000
2023-10-24 20:34:41.163 
Epoch 395/1000 
	 loss: 39.5692, MinusLogProbMetric: 39.5692, val_loss: 40.1729, val_MinusLogProbMetric: 40.1729

Epoch 395: val_loss did not improve from 40.04929
196/196 - 63s - loss: 39.5692 - MinusLogProbMetric: 39.5692 - val_loss: 40.1729 - val_MinusLogProbMetric: 40.1729 - lr: 4.1152e-06 - 63s/epoch - 321ms/step
Epoch 396/1000
2023-10-24 20:35:44.676 
Epoch 396/1000 
	 loss: 39.5394, MinusLogProbMetric: 39.5394, val_loss: 40.4654, val_MinusLogProbMetric: 40.4654

Epoch 396: val_loss did not improve from 40.04929
196/196 - 64s - loss: 39.5394 - MinusLogProbMetric: 39.5394 - val_loss: 40.4654 - val_MinusLogProbMetric: 40.4654 - lr: 4.1152e-06 - 64s/epoch - 324ms/step
Epoch 397/1000
2023-10-24 20:36:49.762 
Epoch 397/1000 
	 loss: 40.1480, MinusLogProbMetric: 40.1480, val_loss: 40.0448, val_MinusLogProbMetric: 40.0448

Epoch 397: val_loss improved from 40.04929 to 40.04479, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 66s - loss: 40.1480 - MinusLogProbMetric: 40.1480 - val_loss: 40.0448 - val_MinusLogProbMetric: 40.0448 - lr: 4.1152e-06 - 66s/epoch - 336ms/step
Epoch 398/1000
2023-10-24 20:37:54.301 
Epoch 398/1000 
	 loss: 39.5090, MinusLogProbMetric: 39.5090, val_loss: 40.1121, val_MinusLogProbMetric: 40.1121

Epoch 398: val_loss did not improve from 40.04479
196/196 - 64s - loss: 39.5090 - MinusLogProbMetric: 39.5090 - val_loss: 40.1121 - val_MinusLogProbMetric: 40.1121 - lr: 4.1152e-06 - 64s/epoch - 325ms/step
Epoch 399/1000
2023-10-24 20:38:58.745 
Epoch 399/1000 
	 loss: 39.4752, MinusLogProbMetric: 39.4752, val_loss: 39.9508, val_MinusLogProbMetric: 39.9508

Epoch 399: val_loss improved from 40.04479 to 39.95078, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 65s - loss: 39.4752 - MinusLogProbMetric: 39.4752 - val_loss: 39.9508 - val_MinusLogProbMetric: 39.9508 - lr: 4.1152e-06 - 65s/epoch - 333ms/step
Epoch 400/1000
2023-10-24 20:40:03.119 
Epoch 400/1000 
	 loss: 39.5519, MinusLogProbMetric: 39.5519, val_loss: 40.2204, val_MinusLogProbMetric: 40.2204

Epoch 400: val_loss did not improve from 39.95078
196/196 - 64s - loss: 39.5519 - MinusLogProbMetric: 39.5519 - val_loss: 40.2204 - val_MinusLogProbMetric: 40.2204 - lr: 4.1152e-06 - 64s/epoch - 324ms/step
Epoch 401/1000
2023-10-24 20:41:07.435 
Epoch 401/1000 
	 loss: 39.4333, MinusLogProbMetric: 39.4333, val_loss: 39.8833, val_MinusLogProbMetric: 39.8833

Epoch 401: val_loss improved from 39.95078 to 39.88331, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 65s - loss: 39.4333 - MinusLogProbMetric: 39.4333 - val_loss: 39.8833 - val_MinusLogProbMetric: 39.8833 - lr: 4.1152e-06 - 65s/epoch - 332ms/step
Epoch 402/1000
2023-10-24 20:42:14.861 
Epoch 402/1000 
	 loss: 39.4089, MinusLogProbMetric: 39.4089, val_loss: 41.0156, val_MinusLogProbMetric: 41.0156

Epoch 402: val_loss did not improve from 39.88331
196/196 - 67s - loss: 39.4089 - MinusLogProbMetric: 39.4089 - val_loss: 41.0156 - val_MinusLogProbMetric: 41.0156 - lr: 4.1152e-06 - 67s/epoch - 340ms/step
Epoch 403/1000
2023-10-24 20:43:16.784 
Epoch 403/1000 
	 loss: 59.4560, MinusLogProbMetric: 59.4560, val_loss: 46.0954, val_MinusLogProbMetric: 46.0954

Epoch 403: val_loss did not improve from 39.88331
196/196 - 62s - loss: 59.4560 - MinusLogProbMetric: 59.4560 - val_loss: 46.0954 - val_MinusLogProbMetric: 46.0954 - lr: 4.1152e-06 - 62s/epoch - 316ms/step
Epoch 404/1000
2023-10-24 20:44:21.256 
Epoch 404/1000 
	 loss: 43.4630, MinusLogProbMetric: 43.4630, val_loss: 42.3618, val_MinusLogProbMetric: 42.3618

Epoch 404: val_loss did not improve from 39.88331
196/196 - 64s - loss: 43.4630 - MinusLogProbMetric: 43.4630 - val_loss: 42.3618 - val_MinusLogProbMetric: 42.3618 - lr: 4.1152e-06 - 64s/epoch - 329ms/step
Epoch 405/1000
2023-10-24 20:45:26.079 
Epoch 405/1000 
	 loss: 41.5106, MinusLogProbMetric: 41.5106, val_loss: 41.6686, val_MinusLogProbMetric: 41.6686

Epoch 405: val_loss did not improve from 39.88331
196/196 - 65s - loss: 41.5106 - MinusLogProbMetric: 41.5106 - val_loss: 41.6686 - val_MinusLogProbMetric: 41.6686 - lr: 4.1152e-06 - 65s/epoch - 331ms/step
Epoch 406/1000
2023-10-24 20:46:30.750 
Epoch 406/1000 
	 loss: 40.8485, MinusLogProbMetric: 40.8485, val_loss: 41.8140, val_MinusLogProbMetric: 41.8140

Epoch 406: val_loss did not improve from 39.88331
196/196 - 65s - loss: 40.8485 - MinusLogProbMetric: 40.8485 - val_loss: 41.8140 - val_MinusLogProbMetric: 41.8140 - lr: 4.1152e-06 - 65s/epoch - 330ms/step
Epoch 407/1000
2023-10-24 20:47:34.466 
Epoch 407/1000 
	 loss: 40.2316, MinusLogProbMetric: 40.2316, val_loss: 40.6148, val_MinusLogProbMetric: 40.6148

Epoch 407: val_loss did not improve from 39.88331
196/196 - 64s - loss: 40.2316 - MinusLogProbMetric: 40.2316 - val_loss: 40.6148 - val_MinusLogProbMetric: 40.6148 - lr: 4.1152e-06 - 64s/epoch - 325ms/step
Epoch 408/1000
2023-10-24 20:48:33.797 
Epoch 408/1000 
	 loss: 39.8185, MinusLogProbMetric: 39.8185, val_loss: 40.2772, val_MinusLogProbMetric: 40.2772

Epoch 408: val_loss did not improve from 39.88331
196/196 - 59s - loss: 39.8185 - MinusLogProbMetric: 39.8185 - val_loss: 40.2772 - val_MinusLogProbMetric: 40.2772 - lr: 4.1152e-06 - 59s/epoch - 303ms/step
Epoch 409/1000
2023-10-24 20:49:32.099 
Epoch 409/1000 
	 loss: 39.9067, MinusLogProbMetric: 39.9067, val_loss: 39.9657, val_MinusLogProbMetric: 39.9657

Epoch 409: val_loss did not improve from 39.88331
196/196 - 58s - loss: 39.9067 - MinusLogProbMetric: 39.9067 - val_loss: 39.9657 - val_MinusLogProbMetric: 39.9657 - lr: 4.1152e-06 - 58s/epoch - 297ms/step
Epoch 410/1000
2023-10-24 20:50:38.764 
Epoch 410/1000 
	 loss: 39.4888, MinusLogProbMetric: 39.4888, val_loss: 40.0458, val_MinusLogProbMetric: 40.0458

Epoch 410: val_loss did not improve from 39.88331
196/196 - 67s - loss: 39.4888 - MinusLogProbMetric: 39.4888 - val_loss: 40.0458 - val_MinusLogProbMetric: 40.0458 - lr: 4.1152e-06 - 67s/epoch - 340ms/step
Epoch 411/1000
2023-10-24 20:51:45.106 
Epoch 411/1000 
	 loss: 39.4424, MinusLogProbMetric: 39.4424, val_loss: 39.9207, val_MinusLogProbMetric: 39.9207

Epoch 411: val_loss did not improve from 39.88331
196/196 - 66s - loss: 39.4424 - MinusLogProbMetric: 39.4424 - val_loss: 39.9207 - val_MinusLogProbMetric: 39.9207 - lr: 4.1152e-06 - 66s/epoch - 338ms/step
Epoch 412/1000
2023-10-24 20:52:52.412 
Epoch 412/1000 
	 loss: 39.3517, MinusLogProbMetric: 39.3517, val_loss: 39.9998, val_MinusLogProbMetric: 39.9998

Epoch 412: val_loss did not improve from 39.88331
196/196 - 67s - loss: 39.3517 - MinusLogProbMetric: 39.3517 - val_loss: 39.9998 - val_MinusLogProbMetric: 39.9998 - lr: 4.1152e-06 - 67s/epoch - 343ms/step
Epoch 413/1000
2023-10-24 20:54:01.158 
Epoch 413/1000 
	 loss: 39.6251, MinusLogProbMetric: 39.6251, val_loss: 39.7210, val_MinusLogProbMetric: 39.7210

Epoch 413: val_loss improved from 39.88331 to 39.72098, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 70s - loss: 39.6251 - MinusLogProbMetric: 39.6251 - val_loss: 39.7210 - val_MinusLogProbMetric: 39.7210 - lr: 4.1152e-06 - 70s/epoch - 355ms/step
Epoch 414/1000
2023-10-24 20:55:08.864 
Epoch 414/1000 
	 loss: 39.4873, MinusLogProbMetric: 39.4873, val_loss: 39.7751, val_MinusLogProbMetric: 39.7751

Epoch 414: val_loss did not improve from 39.72098
196/196 - 67s - loss: 39.4873 - MinusLogProbMetric: 39.4873 - val_loss: 39.7751 - val_MinusLogProbMetric: 39.7751 - lr: 4.1152e-06 - 67s/epoch - 341ms/step
Epoch 415/1000
2023-10-24 20:56:13.400 
Epoch 415/1000 
	 loss: 39.2443, MinusLogProbMetric: 39.2443, val_loss: 39.6121, val_MinusLogProbMetric: 39.6121

Epoch 415: val_loss improved from 39.72098 to 39.61211, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 65s - loss: 39.2443 - MinusLogProbMetric: 39.2443 - val_loss: 39.6121 - val_MinusLogProbMetric: 39.6121 - lr: 4.1152e-06 - 65s/epoch - 334ms/step
Epoch 416/1000
2023-10-24 20:57:20.703 
Epoch 416/1000 
	 loss: 39.2454, MinusLogProbMetric: 39.2454, val_loss: 39.6964, val_MinusLogProbMetric: 39.6964

Epoch 416: val_loss did not improve from 39.61211
196/196 - 66s - loss: 39.2454 - MinusLogProbMetric: 39.2454 - val_loss: 39.6964 - val_MinusLogProbMetric: 39.6964 - lr: 4.1152e-06 - 66s/epoch - 339ms/step
Epoch 417/1000
2023-10-24 20:58:27.748 
Epoch 417/1000 
	 loss: 39.1776, MinusLogProbMetric: 39.1776, val_loss: 39.7261, val_MinusLogProbMetric: 39.7261

Epoch 417: val_loss did not improve from 39.61211
196/196 - 67s - loss: 39.1776 - MinusLogProbMetric: 39.1776 - val_loss: 39.7261 - val_MinusLogProbMetric: 39.7261 - lr: 4.1152e-06 - 67s/epoch - 342ms/step
Epoch 418/1000
2023-10-24 20:59:37.113 
Epoch 418/1000 
	 loss: 39.2625, MinusLogProbMetric: 39.2625, val_loss: 39.6427, val_MinusLogProbMetric: 39.6427

Epoch 418: val_loss did not improve from 39.61211
196/196 - 69s - loss: 39.2625 - MinusLogProbMetric: 39.2625 - val_loss: 39.6427 - val_MinusLogProbMetric: 39.6427 - lr: 4.1152e-06 - 69s/epoch - 354ms/step
Epoch 419/1000
2023-10-24 21:00:45.827 
Epoch 419/1000 
	 loss: 39.1989, MinusLogProbMetric: 39.1989, val_loss: 40.2924, val_MinusLogProbMetric: 40.2924

Epoch 419: val_loss did not improve from 39.61211
196/196 - 69s - loss: 39.1989 - MinusLogProbMetric: 39.1989 - val_loss: 40.2924 - val_MinusLogProbMetric: 40.2924 - lr: 4.1152e-06 - 69s/epoch - 351ms/step
Epoch 420/1000
2023-10-24 21:01:53.473 
Epoch 420/1000 
	 loss: 39.1743, MinusLogProbMetric: 39.1743, val_loss: 39.8255, val_MinusLogProbMetric: 39.8255

Epoch 420: val_loss did not improve from 39.61211
196/196 - 68s - loss: 39.1743 - MinusLogProbMetric: 39.1743 - val_loss: 39.8255 - val_MinusLogProbMetric: 39.8255 - lr: 4.1152e-06 - 68s/epoch - 345ms/step
Epoch 421/1000
2023-10-24 21:03:00.398 
Epoch 421/1000 
	 loss: 39.4317, MinusLogProbMetric: 39.4317, val_loss: 39.5286, val_MinusLogProbMetric: 39.5286

Epoch 421: val_loss improved from 39.61211 to 39.52863, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 68s - loss: 39.4317 - MinusLogProbMetric: 39.4317 - val_loss: 39.5286 - val_MinusLogProbMetric: 39.5286 - lr: 4.1152e-06 - 68s/epoch - 346ms/step
Epoch 422/1000
2023-10-24 21:04:08.793 
Epoch 422/1000 
	 loss: 39.1230, MinusLogProbMetric: 39.1230, val_loss: 39.4624, val_MinusLogProbMetric: 39.4624

Epoch 422: val_loss improved from 39.52863 to 39.46240, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 68s - loss: 39.1230 - MinusLogProbMetric: 39.1230 - val_loss: 39.4624 - val_MinusLogProbMetric: 39.4624 - lr: 4.1152e-06 - 68s/epoch - 349ms/step
Epoch 423/1000
2023-10-24 21:05:16.665 
Epoch 423/1000 
	 loss: 39.0882, MinusLogProbMetric: 39.0882, val_loss: 39.5833, val_MinusLogProbMetric: 39.5833

Epoch 423: val_loss did not improve from 39.46240
196/196 - 67s - loss: 39.0882 - MinusLogProbMetric: 39.0882 - val_loss: 39.5833 - val_MinusLogProbMetric: 39.5833 - lr: 4.1152e-06 - 67s/epoch - 342ms/step
Epoch 424/1000
2023-10-24 21:06:22.571 
Epoch 424/1000 
	 loss: 39.0572, MinusLogProbMetric: 39.0572, val_loss: 39.4993, val_MinusLogProbMetric: 39.4993

Epoch 424: val_loss did not improve from 39.46240
196/196 - 66s - loss: 39.0572 - MinusLogProbMetric: 39.0572 - val_loss: 39.4993 - val_MinusLogProbMetric: 39.4993 - lr: 4.1152e-06 - 66s/epoch - 336ms/step
Epoch 425/1000
2023-10-24 21:07:29.076 
Epoch 425/1000 
	 loss: 39.0702, MinusLogProbMetric: 39.0702, val_loss: 39.6561, val_MinusLogProbMetric: 39.6561

Epoch 425: val_loss did not improve from 39.46240
196/196 - 66s - loss: 39.0702 - MinusLogProbMetric: 39.0702 - val_loss: 39.6561 - val_MinusLogProbMetric: 39.6561 - lr: 4.1152e-06 - 66s/epoch - 339ms/step
Epoch 426/1000
2023-10-24 21:08:35.103 
Epoch 426/1000 
	 loss: 39.6664, MinusLogProbMetric: 39.6664, val_loss: 39.5405, val_MinusLogProbMetric: 39.5405

Epoch 426: val_loss did not improve from 39.46240
196/196 - 66s - loss: 39.6664 - MinusLogProbMetric: 39.6664 - val_loss: 39.5405 - val_MinusLogProbMetric: 39.5405 - lr: 4.1152e-06 - 66s/epoch - 337ms/step
Epoch 427/1000
2023-10-24 21:09:43.901 
Epoch 427/1000 
	 loss: 39.1164, MinusLogProbMetric: 39.1164, val_loss: 39.4163, val_MinusLogProbMetric: 39.4163

Epoch 427: val_loss improved from 39.46240 to 39.41631, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 70s - loss: 39.1164 - MinusLogProbMetric: 39.1164 - val_loss: 39.4163 - val_MinusLogProbMetric: 39.4163 - lr: 4.1152e-06 - 70s/epoch - 356ms/step
Epoch 428/1000
2023-10-24 21:10:50.914 
Epoch 428/1000 
	 loss: 38.9971, MinusLogProbMetric: 38.9971, val_loss: 39.3734, val_MinusLogProbMetric: 39.3734

Epoch 428: val_loss improved from 39.41631 to 39.37343, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 67s - loss: 38.9971 - MinusLogProbMetric: 38.9971 - val_loss: 39.3734 - val_MinusLogProbMetric: 39.3734 - lr: 4.1152e-06 - 67s/epoch - 341ms/step
Epoch 429/1000
2023-10-24 21:11:59.886 
Epoch 429/1000 
	 loss: 38.9683, MinusLogProbMetric: 38.9683, val_loss: 39.5294, val_MinusLogProbMetric: 39.5294

Epoch 429: val_loss did not improve from 39.37343
196/196 - 68s - loss: 38.9683 - MinusLogProbMetric: 38.9683 - val_loss: 39.5294 - val_MinusLogProbMetric: 39.5294 - lr: 4.1152e-06 - 68s/epoch - 348ms/step
Epoch 430/1000
2023-10-24 21:13:06.724 
Epoch 430/1000 
	 loss: 39.1417, MinusLogProbMetric: 39.1417, val_loss: 39.2838, val_MinusLogProbMetric: 39.2838

Epoch 430: val_loss improved from 39.37343 to 39.28384, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 68s - loss: 39.1417 - MinusLogProbMetric: 39.1417 - val_loss: 39.2838 - val_MinusLogProbMetric: 39.2838 - lr: 4.1152e-06 - 68s/epoch - 345ms/step
Epoch 431/1000
2023-10-24 21:14:14.254 
Epoch 431/1000 
	 loss: 38.9368, MinusLogProbMetric: 38.9368, val_loss: 39.4175, val_MinusLogProbMetric: 39.4175

Epoch 431: val_loss did not improve from 39.28384
196/196 - 67s - loss: 38.9368 - MinusLogProbMetric: 38.9368 - val_loss: 39.4175 - val_MinusLogProbMetric: 39.4175 - lr: 4.1152e-06 - 67s/epoch - 341ms/step
Epoch 432/1000
2023-10-24 21:15:22.842 
Epoch 432/1000 
	 loss: 38.8868, MinusLogProbMetric: 38.8868, val_loss: 39.3399, val_MinusLogProbMetric: 39.3399

Epoch 432: val_loss did not improve from 39.28384
196/196 - 69s - loss: 38.8868 - MinusLogProbMetric: 38.8868 - val_loss: 39.3399 - val_MinusLogProbMetric: 39.3399 - lr: 4.1152e-06 - 69s/epoch - 350ms/step
Epoch 433/1000
2023-10-24 21:16:30.674 
Epoch 433/1000 
	 loss: 38.9872, MinusLogProbMetric: 38.9872, val_loss: 39.9803, val_MinusLogProbMetric: 39.9803

Epoch 433: val_loss did not improve from 39.28384
196/196 - 68s - loss: 38.9872 - MinusLogProbMetric: 38.9872 - val_loss: 39.9803 - val_MinusLogProbMetric: 39.9803 - lr: 4.1152e-06 - 68s/epoch - 346ms/step
Epoch 434/1000
2023-10-24 21:17:38.027 
Epoch 434/1000 
	 loss: 38.8772, MinusLogProbMetric: 38.8772, val_loss: 39.2896, val_MinusLogProbMetric: 39.2896

Epoch 434: val_loss did not improve from 39.28384
196/196 - 67s - loss: 38.8772 - MinusLogProbMetric: 38.8772 - val_loss: 39.2896 - val_MinusLogProbMetric: 39.2896 - lr: 4.1152e-06 - 67s/epoch - 344ms/step
Epoch 435/1000
2023-10-24 21:18:45.106 
Epoch 435/1000 
	 loss: 38.9615, MinusLogProbMetric: 38.9615, val_loss: 39.4580, val_MinusLogProbMetric: 39.4580

Epoch 435: val_loss did not improve from 39.28384
196/196 - 67s - loss: 38.9615 - MinusLogProbMetric: 38.9615 - val_loss: 39.4580 - val_MinusLogProbMetric: 39.4580 - lr: 4.1152e-06 - 67s/epoch - 342ms/step
Epoch 436/1000
2023-10-24 21:19:52.509 
Epoch 436/1000 
	 loss: 38.8617, MinusLogProbMetric: 38.8617, val_loss: 39.4782, val_MinusLogProbMetric: 39.4782

Epoch 436: val_loss did not improve from 39.28384
196/196 - 67s - loss: 38.8617 - MinusLogProbMetric: 38.8617 - val_loss: 39.4782 - val_MinusLogProbMetric: 39.4782 - lr: 4.1152e-06 - 67s/epoch - 344ms/step
Epoch 437/1000
2023-10-24 21:21:00.376 
Epoch 437/1000 
	 loss: 38.8377, MinusLogProbMetric: 38.8377, val_loss: 39.3196, val_MinusLogProbMetric: 39.3196

Epoch 437: val_loss did not improve from 39.28384
196/196 - 68s - loss: 38.8377 - MinusLogProbMetric: 38.8377 - val_loss: 39.3196 - val_MinusLogProbMetric: 39.3196 - lr: 4.1152e-06 - 68s/epoch - 346ms/step
Epoch 438/1000
2023-10-24 21:22:08.381 
Epoch 438/1000 
	 loss: 38.8717, MinusLogProbMetric: 38.8717, val_loss: 39.3935, val_MinusLogProbMetric: 39.3935

Epoch 438: val_loss did not improve from 39.28384
196/196 - 68s - loss: 38.8717 - MinusLogProbMetric: 38.8717 - val_loss: 39.3935 - val_MinusLogProbMetric: 39.3935 - lr: 4.1152e-06 - 68s/epoch - 347ms/step
Epoch 439/1000
2023-10-24 21:23:15.263 
Epoch 439/1000 
	 loss: 38.9007, MinusLogProbMetric: 38.9007, val_loss: 39.3770, val_MinusLogProbMetric: 39.3770

Epoch 439: val_loss did not improve from 39.28384
196/196 - 67s - loss: 38.9007 - MinusLogProbMetric: 38.9007 - val_loss: 39.3770 - val_MinusLogProbMetric: 39.3770 - lr: 4.1152e-06 - 67s/epoch - 341ms/step
Epoch 440/1000
2023-10-24 21:24:21.469 
Epoch 440/1000 
	 loss: 38.8041, MinusLogProbMetric: 38.8041, val_loss: 39.1210, val_MinusLogProbMetric: 39.1210

Epoch 440: val_loss improved from 39.28384 to 39.12103, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 67s - loss: 38.8041 - MinusLogProbMetric: 38.8041 - val_loss: 39.1210 - val_MinusLogProbMetric: 39.1210 - lr: 4.1152e-06 - 67s/epoch - 343ms/step
Epoch 441/1000
2023-10-24 21:25:29.804 
Epoch 441/1000 
	 loss: 40.1930, MinusLogProbMetric: 40.1930, val_loss: 39.1709, val_MinusLogProbMetric: 39.1709

Epoch 441: val_loss did not improve from 39.12103
196/196 - 67s - loss: 40.1930 - MinusLogProbMetric: 40.1930 - val_loss: 39.1709 - val_MinusLogProbMetric: 39.1709 - lr: 4.1152e-06 - 67s/epoch - 344ms/step
Epoch 442/1000
2023-10-24 21:26:36.976 
Epoch 442/1000 
	 loss: 38.7443, MinusLogProbMetric: 38.7443, val_loss: 39.5401, val_MinusLogProbMetric: 39.5401

Epoch 442: val_loss did not improve from 39.12103
196/196 - 67s - loss: 38.7443 - MinusLogProbMetric: 38.7443 - val_loss: 39.5401 - val_MinusLogProbMetric: 39.5401 - lr: 4.1152e-06 - 67s/epoch - 343ms/step
Epoch 443/1000
2023-10-24 21:27:42.194 
Epoch 443/1000 
	 loss: 38.7504, MinusLogProbMetric: 38.7504, val_loss: 39.3885, val_MinusLogProbMetric: 39.3885

Epoch 443: val_loss did not improve from 39.12103
196/196 - 65s - loss: 38.7504 - MinusLogProbMetric: 38.7504 - val_loss: 39.3885 - val_MinusLogProbMetric: 39.3885 - lr: 4.1152e-06 - 65s/epoch - 333ms/step
Epoch 444/1000
2023-10-24 21:28:47.742 
Epoch 444/1000 
	 loss: 38.7538, MinusLogProbMetric: 38.7538, val_loss: 39.0820, val_MinusLogProbMetric: 39.0820

Epoch 444: val_loss improved from 39.12103 to 39.08199, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 66s - loss: 38.7538 - MinusLogProbMetric: 38.7538 - val_loss: 39.0820 - val_MinusLogProbMetric: 39.0820 - lr: 4.1152e-06 - 66s/epoch - 339ms/step
Epoch 445/1000
2023-10-24 21:29:57.667 
Epoch 445/1000 
	 loss: 38.7546, MinusLogProbMetric: 38.7546, val_loss: 39.2507, val_MinusLogProbMetric: 39.2507

Epoch 445: val_loss did not improve from 39.08199
196/196 - 69s - loss: 38.7546 - MinusLogProbMetric: 38.7546 - val_loss: 39.2507 - val_MinusLogProbMetric: 39.2507 - lr: 4.1152e-06 - 69s/epoch - 352ms/step
Epoch 446/1000
2023-10-24 21:31:05.780 
Epoch 446/1000 
	 loss: 38.6761, MinusLogProbMetric: 38.6761, val_loss: 39.3828, val_MinusLogProbMetric: 39.3828

Epoch 446: val_loss did not improve from 39.08199
196/196 - 68s - loss: 38.6761 - MinusLogProbMetric: 38.6761 - val_loss: 39.3828 - val_MinusLogProbMetric: 39.3828 - lr: 4.1152e-06 - 68s/epoch - 347ms/step
Epoch 447/1000
2023-10-24 21:32:12.459 
Epoch 447/1000 
	 loss: 38.7484, MinusLogProbMetric: 38.7484, val_loss: 39.2619, val_MinusLogProbMetric: 39.2619

Epoch 447: val_loss did not improve from 39.08199
196/196 - 67s - loss: 38.7484 - MinusLogProbMetric: 38.7484 - val_loss: 39.2619 - val_MinusLogProbMetric: 39.2619 - lr: 4.1152e-06 - 67s/epoch - 340ms/step
Epoch 448/1000
2023-10-24 21:33:17.161 
Epoch 448/1000 
	 loss: 38.6296, MinusLogProbMetric: 38.6296, val_loss: 39.3501, val_MinusLogProbMetric: 39.3501

Epoch 448: val_loss did not improve from 39.08199
196/196 - 65s - loss: 38.6296 - MinusLogProbMetric: 38.6296 - val_loss: 39.3501 - val_MinusLogProbMetric: 39.3501 - lr: 4.1152e-06 - 65s/epoch - 330ms/step
Epoch 449/1000
2023-10-24 21:34:24.023 
Epoch 449/1000 
	 loss: 38.7350, MinusLogProbMetric: 38.7350, val_loss: 39.2480, val_MinusLogProbMetric: 39.2480

Epoch 449: val_loss did not improve from 39.08199
196/196 - 67s - loss: 38.7350 - MinusLogProbMetric: 38.7350 - val_loss: 39.2480 - val_MinusLogProbMetric: 39.2480 - lr: 4.1152e-06 - 67s/epoch - 341ms/step
Epoch 450/1000
2023-10-24 21:35:30.701 
Epoch 450/1000 
	 loss: 38.6785, MinusLogProbMetric: 38.6785, val_loss: 38.9964, val_MinusLogProbMetric: 38.9964

Epoch 450: val_loss improved from 39.08199 to 38.99635, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 68s - loss: 38.6785 - MinusLogProbMetric: 38.6785 - val_loss: 38.9964 - val_MinusLogProbMetric: 38.9964 - lr: 4.1152e-06 - 68s/epoch - 345ms/step
Epoch 451/1000
2023-10-24 21:36:40.317 
Epoch 451/1000 
	 loss: 38.6354, MinusLogProbMetric: 38.6354, val_loss: 39.1710, val_MinusLogProbMetric: 39.1710

Epoch 451: val_loss did not improve from 38.99635
196/196 - 69s - loss: 38.6354 - MinusLogProbMetric: 38.6354 - val_loss: 39.1710 - val_MinusLogProbMetric: 39.1710 - lr: 4.1152e-06 - 69s/epoch - 351ms/step
Epoch 452/1000
2023-10-24 21:37:48.253 
Epoch 452/1000 
	 loss: 38.6513, MinusLogProbMetric: 38.6513, val_loss: 38.9917, val_MinusLogProbMetric: 38.9917

Epoch 452: val_loss improved from 38.99635 to 38.99166, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 69s - loss: 38.6513 - MinusLogProbMetric: 38.6513 - val_loss: 38.9917 - val_MinusLogProbMetric: 38.9917 - lr: 4.1152e-06 - 69s/epoch - 352ms/step
Epoch 453/1000
2023-10-24 21:38:55.938 
Epoch 453/1000 
	 loss: 38.5900, MinusLogProbMetric: 38.5900, val_loss: 39.0667, val_MinusLogProbMetric: 39.0667

Epoch 453: val_loss did not improve from 38.99166
196/196 - 67s - loss: 38.5900 - MinusLogProbMetric: 38.5900 - val_loss: 39.0667 - val_MinusLogProbMetric: 39.0667 - lr: 4.1152e-06 - 67s/epoch - 340ms/step
Epoch 454/1000
2023-10-24 21:40:04.817 
Epoch 454/1000 
	 loss: 38.7752, MinusLogProbMetric: 38.7752, val_loss: 39.0176, val_MinusLogProbMetric: 39.0176

Epoch 454: val_loss did not improve from 38.99166
196/196 - 69s - loss: 38.7752 - MinusLogProbMetric: 38.7752 - val_loss: 39.0176 - val_MinusLogProbMetric: 39.0176 - lr: 4.1152e-06 - 69s/epoch - 351ms/step
Epoch 455/1000
2023-10-24 21:41:13.296 
Epoch 455/1000 
	 loss: 38.5058, MinusLogProbMetric: 38.5058, val_loss: 39.0208, val_MinusLogProbMetric: 39.0208

Epoch 455: val_loss did not improve from 38.99166
196/196 - 68s - loss: 38.5058 - MinusLogProbMetric: 38.5058 - val_loss: 39.0208 - val_MinusLogProbMetric: 39.0208 - lr: 4.1152e-06 - 68s/epoch - 349ms/step
Epoch 456/1000
2023-10-24 21:42:18.364 
Epoch 456/1000 
	 loss: 38.5229, MinusLogProbMetric: 38.5229, val_loss: 39.2036, val_MinusLogProbMetric: 39.2036

Epoch 456: val_loss did not improve from 38.99166
196/196 - 65s - loss: 38.5229 - MinusLogProbMetric: 38.5229 - val_loss: 39.2036 - val_MinusLogProbMetric: 39.2036 - lr: 4.1152e-06 - 65s/epoch - 332ms/step
Epoch 457/1000
2023-10-24 21:43:24.928 
Epoch 457/1000 
	 loss: 38.6169, MinusLogProbMetric: 38.6169, val_loss: 38.8488, val_MinusLogProbMetric: 38.8488

Epoch 457: val_loss improved from 38.99166 to 38.84881, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 68s - loss: 38.6169 - MinusLogProbMetric: 38.6169 - val_loss: 38.8488 - val_MinusLogProbMetric: 38.8488 - lr: 4.1152e-06 - 68s/epoch - 345ms/step
Epoch 458/1000
2023-10-24 21:44:34.089 
Epoch 458/1000 
	 loss: 38.5328, MinusLogProbMetric: 38.5328, val_loss: 39.1607, val_MinusLogProbMetric: 39.1607

Epoch 458: val_loss did not improve from 38.84881
196/196 - 68s - loss: 38.5328 - MinusLogProbMetric: 38.5328 - val_loss: 39.1607 - val_MinusLogProbMetric: 39.1607 - lr: 4.1152e-06 - 68s/epoch - 347ms/step
Epoch 459/1000
2023-10-24 21:45:40.835 
Epoch 459/1000 
	 loss: 38.4696, MinusLogProbMetric: 38.4696, val_loss: 38.9348, val_MinusLogProbMetric: 38.9348

Epoch 459: val_loss did not improve from 38.84881
196/196 - 67s - loss: 38.4696 - MinusLogProbMetric: 38.4696 - val_loss: 38.9348 - val_MinusLogProbMetric: 38.9348 - lr: 4.1152e-06 - 67s/epoch - 341ms/step
Epoch 460/1000
2023-10-24 21:46:48.868 
Epoch 460/1000 
	 loss: 38.4505, MinusLogProbMetric: 38.4505, val_loss: 40.8705, val_MinusLogProbMetric: 40.8705

Epoch 460: val_loss did not improve from 38.84881
196/196 - 68s - loss: 38.4505 - MinusLogProbMetric: 38.4505 - val_loss: 40.8705 - val_MinusLogProbMetric: 40.8705 - lr: 4.1152e-06 - 68s/epoch - 347ms/step
Epoch 461/1000
2023-10-24 21:47:56.247 
Epoch 461/1000 
	 loss: 38.6236, MinusLogProbMetric: 38.6236, val_loss: 39.1707, val_MinusLogProbMetric: 39.1707

Epoch 461: val_loss did not improve from 38.84881
196/196 - 67s - loss: 38.6236 - MinusLogProbMetric: 38.6236 - val_loss: 39.1707 - val_MinusLogProbMetric: 39.1707 - lr: 4.1152e-06 - 67s/epoch - 344ms/step
Epoch 462/1000
2023-10-24 21:49:05.227 
Epoch 462/1000 
	 loss: 38.4619, MinusLogProbMetric: 38.4619, val_loss: 39.0424, val_MinusLogProbMetric: 39.0424

Epoch 462: val_loss did not improve from 38.84881
196/196 - 69s - loss: 38.4619 - MinusLogProbMetric: 38.4619 - val_loss: 39.0424 - val_MinusLogProbMetric: 39.0424 - lr: 4.1152e-06 - 69s/epoch - 352ms/step
Epoch 463/1000
2023-10-24 21:50:10.978 
Epoch 463/1000 
	 loss: 38.4240, MinusLogProbMetric: 38.4240, val_loss: 38.9754, val_MinusLogProbMetric: 38.9754

Epoch 463: val_loss did not improve from 38.84881
196/196 - 66s - loss: 38.4240 - MinusLogProbMetric: 38.4240 - val_loss: 38.9754 - val_MinusLogProbMetric: 38.9754 - lr: 4.1152e-06 - 66s/epoch - 335ms/step
Epoch 464/1000
2023-10-24 21:51:19.180 
Epoch 464/1000 
	 loss: 39.1962, MinusLogProbMetric: 39.1962, val_loss: 38.8759, val_MinusLogProbMetric: 38.8759

Epoch 464: val_loss did not improve from 38.84881
196/196 - 68s - loss: 39.1962 - MinusLogProbMetric: 39.1962 - val_loss: 38.8759 - val_MinusLogProbMetric: 38.8759 - lr: 4.1152e-06 - 68s/epoch - 348ms/step
Epoch 465/1000
2023-10-24 21:52:27.047 
Epoch 465/1000 
	 loss: 38.3900, MinusLogProbMetric: 38.3900, val_loss: 38.8850, val_MinusLogProbMetric: 38.8850

Epoch 465: val_loss did not improve from 38.84881
196/196 - 68s - loss: 38.3900 - MinusLogProbMetric: 38.3900 - val_loss: 38.8850 - val_MinusLogProbMetric: 38.8850 - lr: 4.1152e-06 - 68s/epoch - 346ms/step
Epoch 466/1000
2023-10-24 21:53:35.692 
Epoch 466/1000 
	 loss: 38.3829, MinusLogProbMetric: 38.3829, val_loss: 39.0003, val_MinusLogProbMetric: 39.0003

Epoch 466: val_loss did not improve from 38.84881
196/196 - 69s - loss: 38.3829 - MinusLogProbMetric: 38.3829 - val_loss: 39.0003 - val_MinusLogProbMetric: 39.0003 - lr: 4.1152e-06 - 69s/epoch - 350ms/step
Epoch 467/1000
2023-10-24 21:54:42.396 
Epoch 467/1000 
	 loss: 38.3649, MinusLogProbMetric: 38.3649, val_loss: 38.9144, val_MinusLogProbMetric: 38.9144

Epoch 467: val_loss did not improve from 38.84881
196/196 - 67s - loss: 38.3649 - MinusLogProbMetric: 38.3649 - val_loss: 38.9144 - val_MinusLogProbMetric: 38.9144 - lr: 4.1152e-06 - 67s/epoch - 340ms/step
Epoch 468/1000
2023-10-24 21:55:49.228 
Epoch 468/1000 
	 loss: 38.9273, MinusLogProbMetric: 38.9273, val_loss: 38.8360, val_MinusLogProbMetric: 38.8360

Epoch 468: val_loss improved from 38.84881 to 38.83600, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 68s - loss: 38.9273 - MinusLogProbMetric: 38.9273 - val_loss: 38.8360 - val_MinusLogProbMetric: 38.8360 - lr: 4.1152e-06 - 68s/epoch - 346ms/step
Epoch 469/1000
2023-10-24 21:56:59.279 
Epoch 469/1000 
	 loss: 38.3215, MinusLogProbMetric: 38.3215, val_loss: 38.6453, val_MinusLogProbMetric: 38.6453

Epoch 469: val_loss improved from 38.83600 to 38.64531, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 70s - loss: 38.3215 - MinusLogProbMetric: 38.3215 - val_loss: 38.6453 - val_MinusLogProbMetric: 38.6453 - lr: 4.1152e-06 - 70s/epoch - 358ms/step
Epoch 470/1000
2023-10-24 21:58:07.438 
Epoch 470/1000 
	 loss: 38.3119, MinusLogProbMetric: 38.3119, val_loss: 39.0129, val_MinusLogProbMetric: 39.0129

Epoch 470: val_loss did not improve from 38.64531
196/196 - 67s - loss: 38.3119 - MinusLogProbMetric: 38.3119 - val_loss: 39.0129 - val_MinusLogProbMetric: 39.0129 - lr: 4.1152e-06 - 67s/epoch - 343ms/step
Epoch 471/1000
2023-10-24 21:59:15.394 
Epoch 471/1000 
	 loss: 38.3365, MinusLogProbMetric: 38.3365, val_loss: 38.7117, val_MinusLogProbMetric: 38.7117

Epoch 471: val_loss did not improve from 38.64531
196/196 - 68s - loss: 38.3365 - MinusLogProbMetric: 38.3365 - val_loss: 38.7117 - val_MinusLogProbMetric: 38.7117 - lr: 4.1152e-06 - 68s/epoch - 347ms/step
Epoch 472/1000
2023-10-24 22:00:23.504 
Epoch 472/1000 
	 loss: 38.3130, MinusLogProbMetric: 38.3130, val_loss: 38.8239, val_MinusLogProbMetric: 38.8239

Epoch 472: val_loss did not improve from 38.64531
196/196 - 68s - loss: 38.3130 - MinusLogProbMetric: 38.3130 - val_loss: 38.8239 - val_MinusLogProbMetric: 38.8239 - lr: 4.1152e-06 - 68s/epoch - 347ms/step
Epoch 473/1000
2023-10-24 22:01:31.070 
Epoch 473/1000 
	 loss: 38.2806, MinusLogProbMetric: 38.2806, val_loss: 38.6325, val_MinusLogProbMetric: 38.6325

Epoch 473: val_loss improved from 38.64531 to 38.63255, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 68s - loss: 38.2806 - MinusLogProbMetric: 38.2806 - val_loss: 38.6325 - val_MinusLogProbMetric: 38.6325 - lr: 4.1152e-06 - 68s/epoch - 349ms/step
Epoch 474/1000
2023-10-24 22:02:39.546 
Epoch 474/1000 
	 loss: 38.7713, MinusLogProbMetric: 38.7713, val_loss: 38.7118, val_MinusLogProbMetric: 38.7118

Epoch 474: val_loss did not improve from 38.63255
196/196 - 68s - loss: 38.7713 - MinusLogProbMetric: 38.7713 - val_loss: 38.7118 - val_MinusLogProbMetric: 38.7118 - lr: 4.1152e-06 - 68s/epoch - 345ms/step
Epoch 475/1000
2023-10-24 22:03:45.364 
Epoch 475/1000 
	 loss: 38.3279, MinusLogProbMetric: 38.3279, val_loss: 38.8601, val_MinusLogProbMetric: 38.8601

Epoch 475: val_loss did not improve from 38.63255
196/196 - 66s - loss: 38.3279 - MinusLogProbMetric: 38.3279 - val_loss: 38.8601 - val_MinusLogProbMetric: 38.8601 - lr: 4.1152e-06 - 66s/epoch - 336ms/step
Epoch 476/1000
2023-10-24 22:04:51.616 
Epoch 476/1000 
	 loss: 38.1938, MinusLogProbMetric: 38.1938, val_loss: 38.7104, val_MinusLogProbMetric: 38.7104

Epoch 476: val_loss did not improve from 38.63255
196/196 - 66s - loss: 38.1938 - MinusLogProbMetric: 38.1938 - val_loss: 38.7104 - val_MinusLogProbMetric: 38.7104 - lr: 4.1152e-06 - 66s/epoch - 338ms/step
Epoch 477/1000
2023-10-24 22:05:57.941 
Epoch 477/1000 
	 loss: 38.1928, MinusLogProbMetric: 38.1928, val_loss: 38.6675, val_MinusLogProbMetric: 38.6675

Epoch 477: val_loss did not improve from 38.63255
196/196 - 66s - loss: 38.1928 - MinusLogProbMetric: 38.1928 - val_loss: 38.6675 - val_MinusLogProbMetric: 38.6675 - lr: 4.1152e-06 - 66s/epoch - 338ms/step
Epoch 478/1000
2023-10-24 22:07:06.874 
Epoch 478/1000 
	 loss: 38.2618, MinusLogProbMetric: 38.2618, val_loss: 38.8425, val_MinusLogProbMetric: 38.8425

Epoch 478: val_loss did not improve from 38.63255
196/196 - 69s - loss: 38.2618 - MinusLogProbMetric: 38.2618 - val_loss: 38.8425 - val_MinusLogProbMetric: 38.8425 - lr: 4.1152e-06 - 69s/epoch - 352ms/step
Epoch 479/1000
2023-10-24 22:08:15.323 
Epoch 479/1000 
	 loss: 38.1776, MinusLogProbMetric: 38.1776, val_loss: 38.7769, val_MinusLogProbMetric: 38.7769

Epoch 479: val_loss did not improve from 38.63255
196/196 - 68s - loss: 38.1776 - MinusLogProbMetric: 38.1776 - val_loss: 38.7769 - val_MinusLogProbMetric: 38.7769 - lr: 4.1152e-06 - 68s/epoch - 349ms/step
Epoch 480/1000
2023-10-24 22:09:20.680 
Epoch 480/1000 
	 loss: 39.2045, MinusLogProbMetric: 39.2045, val_loss: 38.6892, val_MinusLogProbMetric: 38.6892

Epoch 480: val_loss did not improve from 38.63255
196/196 - 65s - loss: 39.2045 - MinusLogProbMetric: 39.2045 - val_loss: 38.6892 - val_MinusLogProbMetric: 38.6892 - lr: 4.1152e-06 - 65s/epoch - 333ms/step
Epoch 481/1000
2023-10-24 22:10:28.851 
Epoch 481/1000 
	 loss: 38.1626, MinusLogProbMetric: 38.1626, val_loss: 38.5699, val_MinusLogProbMetric: 38.5699

Epoch 481: val_loss improved from 38.63255 to 38.56987, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 69s - loss: 38.1626 - MinusLogProbMetric: 38.1626 - val_loss: 38.5699 - val_MinusLogProbMetric: 38.5699 - lr: 4.1152e-06 - 69s/epoch - 352ms/step
Epoch 482/1000
2023-10-24 22:11:37.959 
Epoch 482/1000 
	 loss: 38.1619, MinusLogProbMetric: 38.1619, val_loss: 38.7926, val_MinusLogProbMetric: 38.7926

Epoch 482: val_loss did not improve from 38.56987
196/196 - 68s - loss: 38.1619 - MinusLogProbMetric: 38.1619 - val_loss: 38.7926 - val_MinusLogProbMetric: 38.7926 - lr: 4.1152e-06 - 68s/epoch - 348ms/step
Epoch 483/1000
2023-10-24 22:12:47.532 
Epoch 483/1000 
	 loss: 38.1175, MinusLogProbMetric: 38.1175, val_loss: 38.8907, val_MinusLogProbMetric: 38.8907

Epoch 483: val_loss did not improve from 38.56987
196/196 - 70s - loss: 38.1175 - MinusLogProbMetric: 38.1175 - val_loss: 38.8907 - val_MinusLogProbMetric: 38.8907 - lr: 4.1152e-06 - 70s/epoch - 355ms/step
Epoch 484/1000
2023-10-24 22:13:57.083 
Epoch 484/1000 
	 loss: 38.1124, MinusLogProbMetric: 38.1124, val_loss: 38.5980, val_MinusLogProbMetric: 38.5980

Epoch 484: val_loss did not improve from 38.56987
196/196 - 70s - loss: 38.1124 - MinusLogProbMetric: 38.1124 - val_loss: 38.5980 - val_MinusLogProbMetric: 38.5980 - lr: 4.1152e-06 - 70s/epoch - 355ms/step
Epoch 485/1000
2023-10-24 22:15:05.947 
Epoch 485/1000 
	 loss: 38.0819, MinusLogProbMetric: 38.0819, val_loss: 38.8240, val_MinusLogProbMetric: 38.8240

Epoch 485: val_loss did not improve from 38.56987
196/196 - 69s - loss: 38.0819 - MinusLogProbMetric: 38.0819 - val_loss: 38.8240 - val_MinusLogProbMetric: 38.8240 - lr: 4.1152e-06 - 69s/epoch - 351ms/step
Epoch 486/1000
2023-10-24 22:16:13.877 
Epoch 486/1000 
	 loss: 38.1307, MinusLogProbMetric: 38.1307, val_loss: 38.8381, val_MinusLogProbMetric: 38.8381

Epoch 486: val_loss did not improve from 38.56987
196/196 - 68s - loss: 38.1307 - MinusLogProbMetric: 38.1307 - val_loss: 38.8381 - val_MinusLogProbMetric: 38.8381 - lr: 4.1152e-06 - 68s/epoch - 347ms/step
Epoch 487/1000
2023-10-24 22:17:22.794 
Epoch 487/1000 
	 loss: 38.0900, MinusLogProbMetric: 38.0900, val_loss: 38.6186, val_MinusLogProbMetric: 38.6186

Epoch 487: val_loss did not improve from 38.56987
196/196 - 69s - loss: 38.0900 - MinusLogProbMetric: 38.0900 - val_loss: 38.6186 - val_MinusLogProbMetric: 38.6186 - lr: 4.1152e-06 - 69s/epoch - 352ms/step
Epoch 488/1000
2023-10-24 22:18:31.295 
Epoch 488/1000 
	 loss: 38.0657, MinusLogProbMetric: 38.0657, val_loss: 38.4853, val_MinusLogProbMetric: 38.4853

Epoch 488: val_loss improved from 38.56987 to 38.48532, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 69s - loss: 38.0657 - MinusLogProbMetric: 38.0657 - val_loss: 38.4853 - val_MinusLogProbMetric: 38.4853 - lr: 4.1152e-06 - 69s/epoch - 354ms/step
Epoch 489/1000
2023-10-24 22:19:40.773 
Epoch 489/1000 
	 loss: 38.0727, MinusLogProbMetric: 38.0727, val_loss: 38.5127, val_MinusLogProbMetric: 38.5127

Epoch 489: val_loss did not improve from 38.48532
196/196 - 69s - loss: 38.0727 - MinusLogProbMetric: 38.0727 - val_loss: 38.5127 - val_MinusLogProbMetric: 38.5127 - lr: 4.1152e-06 - 69s/epoch - 350ms/step
Epoch 490/1000
2023-10-24 22:20:49.782 
Epoch 490/1000 
	 loss: 38.0419, MinusLogProbMetric: 38.0419, val_loss: 38.6986, val_MinusLogProbMetric: 38.6986

Epoch 490: val_loss did not improve from 38.48532
196/196 - 69s - loss: 38.0419 - MinusLogProbMetric: 38.0419 - val_loss: 38.6986 - val_MinusLogProbMetric: 38.6986 - lr: 4.1152e-06 - 69s/epoch - 352ms/step
Epoch 491/1000
2023-10-24 22:21:58.312 
Epoch 491/1000 
	 loss: 38.0450, MinusLogProbMetric: 38.0450, val_loss: 38.5648, val_MinusLogProbMetric: 38.5648

Epoch 491: val_loss did not improve from 38.48532
196/196 - 69s - loss: 38.0450 - MinusLogProbMetric: 38.0450 - val_loss: 38.5648 - val_MinusLogProbMetric: 38.5648 - lr: 4.1152e-06 - 69s/epoch - 350ms/step
Epoch 492/1000
2023-10-24 22:23:04.808 
Epoch 492/1000 
	 loss: 38.0205, MinusLogProbMetric: 38.0205, val_loss: 38.6340, val_MinusLogProbMetric: 38.6340

Epoch 492: val_loss did not improve from 38.48532
196/196 - 66s - loss: 38.0205 - MinusLogProbMetric: 38.0205 - val_loss: 38.6340 - val_MinusLogProbMetric: 38.6340 - lr: 4.1152e-06 - 66s/epoch - 339ms/step
Epoch 493/1000
2023-10-24 22:24:10.625 
Epoch 493/1000 
	 loss: 38.4673, MinusLogProbMetric: 38.4673, val_loss: 38.7972, val_MinusLogProbMetric: 38.7972

Epoch 493: val_loss did not improve from 38.48532
196/196 - 66s - loss: 38.4673 - MinusLogProbMetric: 38.4673 - val_loss: 38.7972 - val_MinusLogProbMetric: 38.7972 - lr: 4.1152e-06 - 66s/epoch - 336ms/step
Epoch 494/1000
2023-10-24 22:25:16.414 
Epoch 494/1000 
	 loss: 38.0220, MinusLogProbMetric: 38.0220, val_loss: 38.4183, val_MinusLogProbMetric: 38.4183

Epoch 494: val_loss improved from 38.48532 to 38.41827, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 67s - loss: 38.0220 - MinusLogProbMetric: 38.0220 - val_loss: 38.4183 - val_MinusLogProbMetric: 38.4183 - lr: 4.1152e-06 - 67s/epoch - 340ms/step
Epoch 495/1000
2023-10-24 22:26:22.320 
Epoch 495/1000 
	 loss: 38.1738, MinusLogProbMetric: 38.1738, val_loss: 38.5729, val_MinusLogProbMetric: 38.5729

Epoch 495: val_loss did not improve from 38.41827
196/196 - 65s - loss: 38.1738 - MinusLogProbMetric: 38.1738 - val_loss: 38.5729 - val_MinusLogProbMetric: 38.5729 - lr: 4.1152e-06 - 65s/epoch - 331ms/step
Epoch 496/1000
2023-10-24 22:27:27.008 
Epoch 496/1000 
	 loss: 37.9472, MinusLogProbMetric: 37.9472, val_loss: 38.3329, val_MinusLogProbMetric: 38.3329

Epoch 496: val_loss improved from 38.41827 to 38.33286, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 66s - loss: 37.9472 - MinusLogProbMetric: 37.9472 - val_loss: 38.3329 - val_MinusLogProbMetric: 38.3329 - lr: 4.1152e-06 - 66s/epoch - 335ms/step
Epoch 497/1000
2023-10-24 22:28:36.789 
Epoch 497/1000 
	 loss: 38.3693, MinusLogProbMetric: 38.3693, val_loss: 38.3162, val_MinusLogProbMetric: 38.3162

Epoch 497: val_loss improved from 38.33286 to 38.31622, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 70s - loss: 38.3693 - MinusLogProbMetric: 38.3693 - val_loss: 38.3162 - val_MinusLogProbMetric: 38.3162 - lr: 4.1152e-06 - 70s/epoch - 355ms/step
Epoch 498/1000
2023-10-24 22:29:45.374 
Epoch 498/1000 
	 loss: 37.9689, MinusLogProbMetric: 37.9689, val_loss: 38.4050, val_MinusLogProbMetric: 38.4050

Epoch 498: val_loss did not improve from 38.31622
196/196 - 68s - loss: 37.9689 - MinusLogProbMetric: 37.9689 - val_loss: 38.4050 - val_MinusLogProbMetric: 38.4050 - lr: 4.1152e-06 - 68s/epoch - 346ms/step
Epoch 499/1000
2023-10-24 22:30:52.810 
Epoch 499/1000 
	 loss: 37.9274, MinusLogProbMetric: 37.9274, val_loss: 38.7927, val_MinusLogProbMetric: 38.7927

Epoch 499: val_loss did not improve from 38.31622
196/196 - 67s - loss: 37.9274 - MinusLogProbMetric: 37.9274 - val_loss: 38.7927 - val_MinusLogProbMetric: 38.7927 - lr: 4.1152e-06 - 67s/epoch - 344ms/step
Epoch 500/1000
2023-10-24 22:32:00.063 
Epoch 500/1000 
	 loss: 37.9065, MinusLogProbMetric: 37.9065, val_loss: 38.3130, val_MinusLogProbMetric: 38.3130

Epoch 500: val_loss improved from 38.31622 to 38.31301, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 68s - loss: 37.9065 - MinusLogProbMetric: 37.9065 - val_loss: 38.3130 - val_MinusLogProbMetric: 38.3130 - lr: 4.1152e-06 - 68s/epoch - 348ms/step
Epoch 501/1000
2023-10-24 22:33:04.433 
Epoch 501/1000 
	 loss: 37.8858, MinusLogProbMetric: 37.8858, val_loss: 38.3621, val_MinusLogProbMetric: 38.3621

Epoch 501: val_loss did not improve from 38.31301
196/196 - 63s - loss: 37.8858 - MinusLogProbMetric: 37.8858 - val_loss: 38.3621 - val_MinusLogProbMetric: 38.3621 - lr: 4.1152e-06 - 63s/epoch - 324ms/step
Epoch 502/1000
2023-10-24 22:34:10.069 
Epoch 502/1000 
	 loss: 37.8949, MinusLogProbMetric: 37.8949, val_loss: 38.3150, val_MinusLogProbMetric: 38.3150

Epoch 502: val_loss did not improve from 38.31301
196/196 - 66s - loss: 37.8949 - MinusLogProbMetric: 37.8949 - val_loss: 38.3150 - val_MinusLogProbMetric: 38.3150 - lr: 4.1152e-06 - 66s/epoch - 335ms/step
Epoch 503/1000
2023-10-24 22:35:18.803 
Epoch 503/1000 
	 loss: 37.8153, MinusLogProbMetric: 37.8153, val_loss: 38.3311, val_MinusLogProbMetric: 38.3311

Epoch 503: val_loss did not improve from 38.31301
196/196 - 69s - loss: 37.8153 - MinusLogProbMetric: 37.8153 - val_loss: 38.3311 - val_MinusLogProbMetric: 38.3311 - lr: 4.1152e-06 - 69s/epoch - 351ms/step
Epoch 504/1000
2023-10-24 22:36:25.416 
Epoch 504/1000 
	 loss: 37.9224, MinusLogProbMetric: 37.9224, val_loss: 38.3506, val_MinusLogProbMetric: 38.3506

Epoch 504: val_loss did not improve from 38.31301
196/196 - 67s - loss: 37.9224 - MinusLogProbMetric: 37.9224 - val_loss: 38.3506 - val_MinusLogProbMetric: 38.3506 - lr: 4.1152e-06 - 67s/epoch - 340ms/step
Epoch 505/1000
2023-10-24 22:37:33.065 
Epoch 505/1000 
	 loss: 37.9093, MinusLogProbMetric: 37.9093, val_loss: 38.2861, val_MinusLogProbMetric: 38.2861

Epoch 505: val_loss improved from 38.31301 to 38.28613, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 69s - loss: 37.9093 - MinusLogProbMetric: 37.9093 - val_loss: 38.2861 - val_MinusLogProbMetric: 38.2861 - lr: 4.1152e-06 - 69s/epoch - 350ms/step
Epoch 506/1000
2023-10-24 22:38:39.858 
Epoch 506/1000 
	 loss: 37.9325, MinusLogProbMetric: 37.9325, val_loss: 38.2951, val_MinusLogProbMetric: 38.2951

Epoch 506: val_loss did not improve from 38.28613
196/196 - 66s - loss: 37.9325 - MinusLogProbMetric: 37.9325 - val_loss: 38.2951 - val_MinusLogProbMetric: 38.2951 - lr: 4.1152e-06 - 66s/epoch - 336ms/step
Epoch 507/1000
2023-10-24 22:39:44.218 
Epoch 507/1000 
	 loss: 37.8208, MinusLogProbMetric: 37.8208, val_loss: 38.3445, val_MinusLogProbMetric: 38.3445

Epoch 507: val_loss did not improve from 38.28613
196/196 - 64s - loss: 37.8208 - MinusLogProbMetric: 37.8208 - val_loss: 38.3445 - val_MinusLogProbMetric: 38.3445 - lr: 4.1152e-06 - 64s/epoch - 328ms/step
Epoch 508/1000
2023-10-24 22:40:49.287 
Epoch 508/1000 
	 loss: 37.8183, MinusLogProbMetric: 37.8183, val_loss: 38.5236, val_MinusLogProbMetric: 38.5236

Epoch 508: val_loss did not improve from 38.28613
196/196 - 65s - loss: 37.8183 - MinusLogProbMetric: 37.8183 - val_loss: 38.5236 - val_MinusLogProbMetric: 38.5236 - lr: 4.1152e-06 - 65s/epoch - 332ms/step
Epoch 509/1000
2023-10-24 22:41:58.021 
Epoch 509/1000 
	 loss: 37.8000, MinusLogProbMetric: 37.8000, val_loss: 38.2059, val_MinusLogProbMetric: 38.2059

Epoch 509: val_loss improved from 38.28613 to 38.20590, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 70s - loss: 37.8000 - MinusLogProbMetric: 37.8000 - val_loss: 38.2059 - val_MinusLogProbMetric: 38.2059 - lr: 4.1152e-06 - 70s/epoch - 356ms/step
Epoch 510/1000
2023-10-24 22:43:05.555 
Epoch 510/1000 
	 loss: 37.7823, MinusLogProbMetric: 37.7823, val_loss: 38.2191, val_MinusLogProbMetric: 38.2191

Epoch 510: val_loss did not improve from 38.20590
196/196 - 67s - loss: 37.7823 - MinusLogProbMetric: 37.7823 - val_loss: 38.2191 - val_MinusLogProbMetric: 38.2191 - lr: 4.1152e-06 - 67s/epoch - 340ms/step
Epoch 511/1000
2023-10-24 22:44:13.288 
Epoch 511/1000 
	 loss: 37.7362, MinusLogProbMetric: 37.7362, val_loss: 38.3635, val_MinusLogProbMetric: 38.3635

Epoch 511: val_loss did not improve from 38.20590
196/196 - 68s - loss: 37.7362 - MinusLogProbMetric: 37.7362 - val_loss: 38.3635 - val_MinusLogProbMetric: 38.3635 - lr: 4.1152e-06 - 68s/epoch - 346ms/step
Epoch 512/1000
2023-10-24 22:45:21.134 
Epoch 512/1000 
	 loss: 37.8479, MinusLogProbMetric: 37.8479, val_loss: 38.2212, val_MinusLogProbMetric: 38.2212

Epoch 512: val_loss did not improve from 38.20590
196/196 - 68s - loss: 37.8479 - MinusLogProbMetric: 37.8479 - val_loss: 38.2212 - val_MinusLogProbMetric: 38.2212 - lr: 4.1152e-06 - 68s/epoch - 346ms/step
Epoch 513/1000
2023-10-24 22:46:28.905 
Epoch 513/1000 
	 loss: 37.6916, MinusLogProbMetric: 37.6916, val_loss: 38.1001, val_MinusLogProbMetric: 38.1001

Epoch 513: val_loss improved from 38.20590 to 38.10010, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 69s - loss: 37.6916 - MinusLogProbMetric: 37.6916 - val_loss: 38.1001 - val_MinusLogProbMetric: 38.1001 - lr: 4.1152e-06 - 69s/epoch - 351ms/step
Epoch 514/1000
2023-10-24 22:47:37.698 
Epoch 514/1000 
	 loss: 37.7086, MinusLogProbMetric: 37.7086, val_loss: 38.3138, val_MinusLogProbMetric: 38.3138

Epoch 514: val_loss did not improve from 38.10010
196/196 - 68s - loss: 37.7086 - MinusLogProbMetric: 37.7086 - val_loss: 38.3138 - val_MinusLogProbMetric: 38.3138 - lr: 4.1152e-06 - 68s/epoch - 345ms/step
Epoch 515/1000
2023-10-24 22:48:45.603 
Epoch 515/1000 
	 loss: 37.7150, MinusLogProbMetric: 37.7150, val_loss: 38.1493, val_MinusLogProbMetric: 38.1493

Epoch 515: val_loss did not improve from 38.10010
196/196 - 68s - loss: 37.7150 - MinusLogProbMetric: 37.7150 - val_loss: 38.1493 - val_MinusLogProbMetric: 38.1493 - lr: 4.1152e-06 - 68s/epoch - 346ms/step
Epoch 516/1000
2023-10-24 22:49:52.519 
Epoch 516/1000 
	 loss: 37.6866, MinusLogProbMetric: 37.6866, val_loss: 38.2562, val_MinusLogProbMetric: 38.2562

Epoch 516: val_loss did not improve from 38.10010
196/196 - 67s - loss: 37.6866 - MinusLogProbMetric: 37.6866 - val_loss: 38.2562 - val_MinusLogProbMetric: 38.2562 - lr: 4.1152e-06 - 67s/epoch - 341ms/step
Epoch 517/1000
2023-10-24 22:51:01.769 
Epoch 517/1000 
	 loss: 37.6548, MinusLogProbMetric: 37.6548, val_loss: 38.1018, val_MinusLogProbMetric: 38.1018

Epoch 517: val_loss did not improve from 38.10010
196/196 - 69s - loss: 37.6548 - MinusLogProbMetric: 37.6548 - val_loss: 38.1018 - val_MinusLogProbMetric: 38.1018 - lr: 4.1152e-06 - 69s/epoch - 353ms/step
Epoch 518/1000
2023-10-24 22:52:09.671 
Epoch 518/1000 
	 loss: 37.6321, MinusLogProbMetric: 37.6321, val_loss: 38.1768, val_MinusLogProbMetric: 38.1768

Epoch 518: val_loss did not improve from 38.10010
196/196 - 68s - loss: 37.6321 - MinusLogProbMetric: 37.6321 - val_loss: 38.1768 - val_MinusLogProbMetric: 38.1768 - lr: 4.1152e-06 - 68s/epoch - 346ms/step
Epoch 519/1000
2023-10-24 22:53:14.338 
Epoch 519/1000 
	 loss: 37.6752, MinusLogProbMetric: 37.6752, val_loss: 38.0265, val_MinusLogProbMetric: 38.0265

Epoch 519: val_loss improved from 38.10010 to 38.02654, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 66s - loss: 37.6752 - MinusLogProbMetric: 37.6752 - val_loss: 38.0265 - val_MinusLogProbMetric: 38.0265 - lr: 4.1152e-06 - 66s/epoch - 335ms/step
Epoch 520/1000
2023-10-24 22:54:22.987 
Epoch 520/1000 
	 loss: 37.6182, MinusLogProbMetric: 37.6182, val_loss: 38.1241, val_MinusLogProbMetric: 38.1241

Epoch 520: val_loss did not improve from 38.02654
196/196 - 68s - loss: 37.6182 - MinusLogProbMetric: 37.6182 - val_loss: 38.1241 - val_MinusLogProbMetric: 38.1241 - lr: 4.1152e-06 - 68s/epoch - 345ms/step
Epoch 521/1000
2023-10-24 22:55:31.247 
Epoch 521/1000 
	 loss: 37.7953, MinusLogProbMetric: 37.7953, val_loss: 38.0015, val_MinusLogProbMetric: 38.0015

Epoch 521: val_loss improved from 38.02654 to 38.00146, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 69s - loss: 37.7953 - MinusLogProbMetric: 37.7953 - val_loss: 38.0015 - val_MinusLogProbMetric: 38.0015 - lr: 4.1152e-06 - 69s/epoch - 353ms/step
Epoch 522/1000
2023-10-24 22:56:40.650 
Epoch 522/1000 
	 loss: 37.5869, MinusLogProbMetric: 37.5869, val_loss: 38.0793, val_MinusLogProbMetric: 38.0793

Epoch 522: val_loss did not improve from 38.00146
196/196 - 68s - loss: 37.5869 - MinusLogProbMetric: 37.5869 - val_loss: 38.0793 - val_MinusLogProbMetric: 38.0793 - lr: 4.1152e-06 - 68s/epoch - 349ms/step
Epoch 523/1000
2023-10-24 22:57:47.074 
Epoch 523/1000 
	 loss: 37.5766, MinusLogProbMetric: 37.5766, val_loss: 38.0045, val_MinusLogProbMetric: 38.0045

Epoch 523: val_loss did not improve from 38.00146
196/196 - 66s - loss: 37.5766 - MinusLogProbMetric: 37.5766 - val_loss: 38.0045 - val_MinusLogProbMetric: 38.0045 - lr: 4.1152e-06 - 66s/epoch - 339ms/step
Epoch 524/1000
2023-10-24 22:58:53.799 
Epoch 524/1000 
	 loss: 37.6053, MinusLogProbMetric: 37.6053, val_loss: 37.9999, val_MinusLogProbMetric: 37.9999

Epoch 524: val_loss improved from 38.00146 to 37.99991, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 68s - loss: 37.6053 - MinusLogProbMetric: 37.6053 - val_loss: 37.9999 - val_MinusLogProbMetric: 37.9999 - lr: 4.1152e-06 - 68s/epoch - 345ms/step
Epoch 525/1000
2023-10-24 23:00:01.738 
Epoch 525/1000 
	 loss: 37.5889, MinusLogProbMetric: 37.5889, val_loss: 38.0266, val_MinusLogProbMetric: 38.0266

Epoch 525: val_loss did not improve from 37.99991
196/196 - 67s - loss: 37.5889 - MinusLogProbMetric: 37.5889 - val_loss: 38.0266 - val_MinusLogProbMetric: 38.0266 - lr: 4.1152e-06 - 67s/epoch - 342ms/step
Epoch 526/1000
2023-10-24 23:01:09.295 
Epoch 526/1000 
	 loss: 37.8259, MinusLogProbMetric: 37.8259, val_loss: 39.1310, val_MinusLogProbMetric: 39.1310

Epoch 526: val_loss did not improve from 37.99991
196/196 - 68s - loss: 37.8259 - MinusLogProbMetric: 37.8259 - val_loss: 39.1310 - val_MinusLogProbMetric: 39.1310 - lr: 4.1152e-06 - 68s/epoch - 345ms/step
Epoch 527/1000
2023-10-24 23:02:15.383 
Epoch 527/1000 
	 loss: 37.6381, MinusLogProbMetric: 37.6381, val_loss: 37.9868, val_MinusLogProbMetric: 37.9868

Epoch 527: val_loss improved from 37.99991 to 37.98677, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 67s - loss: 37.6381 - MinusLogProbMetric: 37.6381 - val_loss: 37.9868 - val_MinusLogProbMetric: 37.9868 - lr: 4.1152e-06 - 67s/epoch - 341ms/step
Epoch 528/1000
2023-10-24 23:03:24.093 
Epoch 528/1000 
	 loss: 37.5365, MinusLogProbMetric: 37.5365, val_loss: 37.9069, val_MinusLogProbMetric: 37.9069

Epoch 528: val_loss improved from 37.98677 to 37.90694, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 69s - loss: 37.5365 - MinusLogProbMetric: 37.5365 - val_loss: 37.9069 - val_MinusLogProbMetric: 37.9069 - lr: 4.1152e-06 - 69s/epoch - 351ms/step
Epoch 529/1000
2023-10-24 23:04:32.084 
Epoch 529/1000 
	 loss: 37.4917, MinusLogProbMetric: 37.4917, val_loss: 38.0744, val_MinusLogProbMetric: 38.0744

Epoch 529: val_loss did not improve from 37.90694
196/196 - 67s - loss: 37.4917 - MinusLogProbMetric: 37.4917 - val_loss: 38.0744 - val_MinusLogProbMetric: 38.0744 - lr: 4.1152e-06 - 67s/epoch - 342ms/step
Epoch 530/1000
2023-10-24 23:05:39.469 
Epoch 530/1000 
	 loss: 37.6456, MinusLogProbMetric: 37.6456, val_loss: 37.9499, val_MinusLogProbMetric: 37.9499

Epoch 530: val_loss did not improve from 37.90694
196/196 - 67s - loss: 37.6456 - MinusLogProbMetric: 37.6456 - val_loss: 37.9499 - val_MinusLogProbMetric: 37.9499 - lr: 4.1152e-06 - 67s/epoch - 344ms/step
Epoch 531/1000
2023-10-24 23:06:48.197 
Epoch 531/1000 
	 loss: 37.4790, MinusLogProbMetric: 37.4790, val_loss: 38.0577, val_MinusLogProbMetric: 38.0577

Epoch 531: val_loss did not improve from 37.90694
196/196 - 69s - loss: 37.4790 - MinusLogProbMetric: 37.4790 - val_loss: 38.0577 - val_MinusLogProbMetric: 38.0577 - lr: 4.1152e-06 - 69s/epoch - 351ms/step
Epoch 532/1000
2023-10-24 23:07:55.930 
Epoch 532/1000 
	 loss: 37.4681, MinusLogProbMetric: 37.4681, val_loss: 38.0360, val_MinusLogProbMetric: 38.0360

Epoch 532: val_loss did not improve from 37.90694
196/196 - 68s - loss: 37.4681 - MinusLogProbMetric: 37.4681 - val_loss: 38.0360 - val_MinusLogProbMetric: 38.0360 - lr: 4.1152e-06 - 68s/epoch - 346ms/step
Epoch 533/1000
2023-10-24 23:09:03.236 
Epoch 533/1000 
	 loss: 38.0134, MinusLogProbMetric: 38.0134, val_loss: 37.9911, val_MinusLogProbMetric: 37.9911

Epoch 533: val_loss did not improve from 37.90694
196/196 - 67s - loss: 38.0134 - MinusLogProbMetric: 38.0134 - val_loss: 37.9911 - val_MinusLogProbMetric: 37.9911 - lr: 4.1152e-06 - 67s/epoch - 343ms/step
Epoch 534/1000
2023-10-24 23:10:09.142 
Epoch 534/1000 
	 loss: 38.2378, MinusLogProbMetric: 38.2378, val_loss: 38.2210, val_MinusLogProbMetric: 38.2210

Epoch 534: val_loss did not improve from 37.90694
196/196 - 66s - loss: 38.2378 - MinusLogProbMetric: 38.2378 - val_loss: 38.2210 - val_MinusLogProbMetric: 38.2210 - lr: 4.1152e-06 - 66s/epoch - 336ms/step
Epoch 535/1000
2023-10-24 23:11:15.561 
Epoch 535/1000 
	 loss: 37.6618, MinusLogProbMetric: 37.6618, val_loss: 38.0470, val_MinusLogProbMetric: 38.0470

Epoch 535: val_loss did not improve from 37.90694
196/196 - 66s - loss: 37.6618 - MinusLogProbMetric: 37.6618 - val_loss: 38.0470 - val_MinusLogProbMetric: 38.0470 - lr: 4.1152e-06 - 66s/epoch - 339ms/step
Epoch 536/1000
2023-10-24 23:12:21.701 
Epoch 536/1000 
	 loss: 37.5420, MinusLogProbMetric: 37.5420, val_loss: 37.9558, val_MinusLogProbMetric: 37.9558

Epoch 536: val_loss did not improve from 37.90694
196/196 - 66s - loss: 37.5420 - MinusLogProbMetric: 37.5420 - val_loss: 37.9558 - val_MinusLogProbMetric: 37.9558 - lr: 4.1152e-06 - 66s/epoch - 337ms/step
Epoch 537/1000
2023-10-24 23:13:29.913 
Epoch 537/1000 
	 loss: 37.5050, MinusLogProbMetric: 37.5050, val_loss: 37.9118, val_MinusLogProbMetric: 37.9118

Epoch 537: val_loss did not improve from 37.90694
196/196 - 68s - loss: 37.5050 - MinusLogProbMetric: 37.5050 - val_loss: 37.9118 - val_MinusLogProbMetric: 37.9118 - lr: 4.1152e-06 - 68s/epoch - 348ms/step
Epoch 538/1000
2023-10-24 23:14:36.679 
Epoch 538/1000 
	 loss: 37.5751, MinusLogProbMetric: 37.5751, val_loss: 38.1812, val_MinusLogProbMetric: 38.1812

Epoch 538: val_loss did not improve from 37.90694
196/196 - 67s - loss: 37.5751 - MinusLogProbMetric: 37.5751 - val_loss: 38.1812 - val_MinusLogProbMetric: 38.1812 - lr: 4.1152e-06 - 67s/epoch - 341ms/step
Epoch 539/1000
2023-10-24 23:15:44.227 
Epoch 539/1000 
	 loss: 37.4817, MinusLogProbMetric: 37.4817, val_loss: 38.1941, val_MinusLogProbMetric: 38.1941

Epoch 539: val_loss did not improve from 37.90694
196/196 - 68s - loss: 37.4817 - MinusLogProbMetric: 37.4817 - val_loss: 38.1941 - val_MinusLogProbMetric: 38.1941 - lr: 4.1152e-06 - 68s/epoch - 345ms/step
Epoch 540/1000
2023-10-24 23:16:50.237 
Epoch 540/1000 
	 loss: 38.0941, MinusLogProbMetric: 38.0941, val_loss: 38.0131, val_MinusLogProbMetric: 38.0131

Epoch 540: val_loss did not improve from 37.90694
196/196 - 66s - loss: 38.0941 - MinusLogProbMetric: 38.0941 - val_loss: 38.0131 - val_MinusLogProbMetric: 38.0131 - lr: 4.1152e-06 - 66s/epoch - 337ms/step
Epoch 541/1000
2023-10-24 23:17:57.848 
Epoch 541/1000 
	 loss: 37.4007, MinusLogProbMetric: 37.4007, val_loss: 37.7686, val_MinusLogProbMetric: 37.7686

Epoch 541: val_loss improved from 37.90694 to 37.76860, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 69s - loss: 37.4007 - MinusLogProbMetric: 37.4007 - val_loss: 37.7686 - val_MinusLogProbMetric: 37.7686 - lr: 4.1152e-06 - 69s/epoch - 350ms/step
Epoch 542/1000
2023-10-24 23:19:05.103 
Epoch 542/1000 
	 loss: 37.3968, MinusLogProbMetric: 37.3968, val_loss: 38.1120, val_MinusLogProbMetric: 38.1120

Epoch 542: val_loss did not improve from 37.76860
196/196 - 66s - loss: 37.3968 - MinusLogProbMetric: 37.3968 - val_loss: 38.1120 - val_MinusLogProbMetric: 38.1120 - lr: 4.1152e-06 - 66s/epoch - 338ms/step
Epoch 543/1000
2023-10-24 23:20:13.354 
Epoch 543/1000 
	 loss: 37.3997, MinusLogProbMetric: 37.3997, val_loss: 38.1398, val_MinusLogProbMetric: 38.1398

Epoch 543: val_loss did not improve from 37.76860
196/196 - 68s - loss: 37.3997 - MinusLogProbMetric: 37.3997 - val_loss: 38.1398 - val_MinusLogProbMetric: 38.1398 - lr: 4.1152e-06 - 68s/epoch - 348ms/step
Epoch 544/1000
2023-10-24 23:21:19.685 
Epoch 544/1000 
	 loss: 37.3263, MinusLogProbMetric: 37.3263, val_loss: 37.8477, val_MinusLogProbMetric: 37.8477

Epoch 544: val_loss did not improve from 37.76860
196/196 - 66s - loss: 37.3263 - MinusLogProbMetric: 37.3263 - val_loss: 37.8477 - val_MinusLogProbMetric: 37.8477 - lr: 4.1152e-06 - 66s/epoch - 338ms/step
Epoch 545/1000
2023-10-24 23:22:26.941 
Epoch 545/1000 
	 loss: 37.3138, MinusLogProbMetric: 37.3138, val_loss: 37.9148, val_MinusLogProbMetric: 37.9148

Epoch 545: val_loss did not improve from 37.76860
196/196 - 67s - loss: 37.3138 - MinusLogProbMetric: 37.3138 - val_loss: 37.9148 - val_MinusLogProbMetric: 37.9148 - lr: 4.1152e-06 - 67s/epoch - 343ms/step
Epoch 546/1000
2023-10-24 23:23:34.912 
Epoch 546/1000 
	 loss: 37.3196, MinusLogProbMetric: 37.3196, val_loss: 37.8165, val_MinusLogProbMetric: 37.8165

Epoch 546: val_loss did not improve from 37.76860
196/196 - 68s - loss: 37.3196 - MinusLogProbMetric: 37.3196 - val_loss: 37.8165 - val_MinusLogProbMetric: 37.8165 - lr: 4.1152e-06 - 68s/epoch - 347ms/step
Epoch 547/1000
2023-10-24 23:24:42.693 
Epoch 547/1000 
	 loss: 37.6043, MinusLogProbMetric: 37.6043, val_loss: 37.7174, val_MinusLogProbMetric: 37.7174

Epoch 547: val_loss improved from 37.76860 to 37.71740, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 69s - loss: 37.6043 - MinusLogProbMetric: 37.6043 - val_loss: 37.7174 - val_MinusLogProbMetric: 37.7174 - lr: 4.1152e-06 - 69s/epoch - 350ms/step
Epoch 548/1000
2023-10-24 23:25:52.029 
Epoch 548/1000 
	 loss: 37.3192, MinusLogProbMetric: 37.3192, val_loss: 37.9353, val_MinusLogProbMetric: 37.9353

Epoch 548: val_loss did not improve from 37.71740
196/196 - 68s - loss: 37.3192 - MinusLogProbMetric: 37.3192 - val_loss: 37.9353 - val_MinusLogProbMetric: 37.9353 - lr: 4.1152e-06 - 68s/epoch - 349ms/step
Epoch 549/1000
2023-10-24 23:26:59.963 
Epoch 549/1000 
	 loss: 37.3341, MinusLogProbMetric: 37.3341, val_loss: 37.8097, val_MinusLogProbMetric: 37.8097

Epoch 549: val_loss did not improve from 37.71740
196/196 - 68s - loss: 37.3341 - MinusLogProbMetric: 37.3341 - val_loss: 37.8097 - val_MinusLogProbMetric: 37.8097 - lr: 4.1152e-06 - 68s/epoch - 347ms/step
Epoch 550/1000
2023-10-24 23:28:06.126 
Epoch 550/1000 
	 loss: 38.3301, MinusLogProbMetric: 38.3301, val_loss: 47.0502, val_MinusLogProbMetric: 47.0502

Epoch 550: val_loss did not improve from 37.71740
196/196 - 66s - loss: 38.3301 - MinusLogProbMetric: 38.3301 - val_loss: 47.0502 - val_MinusLogProbMetric: 47.0502 - lr: 4.1152e-06 - 66s/epoch - 338ms/step
Epoch 551/1000
2023-10-24 23:29:14.959 
Epoch 551/1000 
	 loss: 37.9392, MinusLogProbMetric: 37.9392, val_loss: 37.6610, val_MinusLogProbMetric: 37.6610

Epoch 551: val_loss improved from 37.71740 to 37.66102, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 70s - loss: 37.9392 - MinusLogProbMetric: 37.9392 - val_loss: 37.6610 - val_MinusLogProbMetric: 37.6610 - lr: 4.1152e-06 - 70s/epoch - 357ms/step
Epoch 552/1000
2023-10-24 23:30:23.346 
Epoch 552/1000 
	 loss: 37.2843, MinusLogProbMetric: 37.2843, val_loss: 37.7333, val_MinusLogProbMetric: 37.7333

Epoch 552: val_loss did not improve from 37.66102
196/196 - 67s - loss: 37.2843 - MinusLogProbMetric: 37.2843 - val_loss: 37.7333 - val_MinusLogProbMetric: 37.7333 - lr: 4.1152e-06 - 67s/epoch - 343ms/step
Epoch 553/1000
2023-10-24 23:31:31.118 
Epoch 553/1000 
	 loss: 37.2586, MinusLogProbMetric: 37.2586, val_loss: 37.9960, val_MinusLogProbMetric: 37.9960

Epoch 553: val_loss did not improve from 37.66102
196/196 - 68s - loss: 37.2586 - MinusLogProbMetric: 37.2586 - val_loss: 37.9960 - val_MinusLogProbMetric: 37.9960 - lr: 4.1152e-06 - 68s/epoch - 346ms/step
Epoch 554/1000
2023-10-24 23:32:38.504 
Epoch 554/1000 
	 loss: 37.2485, MinusLogProbMetric: 37.2485, val_loss: 38.0500, val_MinusLogProbMetric: 38.0500

Epoch 554: val_loss did not improve from 37.66102
196/196 - 67s - loss: 37.2485 - MinusLogProbMetric: 37.2485 - val_loss: 38.0500 - val_MinusLogProbMetric: 38.0500 - lr: 4.1152e-06 - 67s/epoch - 344ms/step
Epoch 555/1000
2023-10-24 23:33:47.166 
Epoch 555/1000 
	 loss: 37.2308, MinusLogProbMetric: 37.2308, val_loss: 37.6286, val_MinusLogProbMetric: 37.6286

Epoch 555: val_loss improved from 37.66102 to 37.62860, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 69s - loss: 37.2308 - MinusLogProbMetric: 37.2308 - val_loss: 37.6286 - val_MinusLogProbMetric: 37.6286 - lr: 4.1152e-06 - 69s/epoch - 354ms/step
Epoch 556/1000
2023-10-24 23:34:51.954 
Epoch 556/1000 
	 loss: 37.2119, MinusLogProbMetric: 37.2119, val_loss: 37.7389, val_MinusLogProbMetric: 37.7389

Epoch 556: val_loss did not improve from 37.62860
196/196 - 64s - loss: 37.2119 - MinusLogProbMetric: 37.2119 - val_loss: 37.7389 - val_MinusLogProbMetric: 37.7389 - lr: 4.1152e-06 - 64s/epoch - 327ms/step
Epoch 557/1000
2023-10-24 23:35:55.900 
Epoch 557/1000 
	 loss: 37.2566, MinusLogProbMetric: 37.2566, val_loss: 37.6134, val_MinusLogProbMetric: 37.6134

Epoch 557: val_loss improved from 37.62860 to 37.61343, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 65s - loss: 37.2566 - MinusLogProbMetric: 37.2566 - val_loss: 37.6134 - val_MinusLogProbMetric: 37.6134 - lr: 4.1152e-06 - 65s/epoch - 330ms/step
Epoch 558/1000
2023-10-24 23:37:04.494 
Epoch 558/1000 
	 loss: 37.2656, MinusLogProbMetric: 37.2656, val_loss: 37.7070, val_MinusLogProbMetric: 37.7070

Epoch 558: val_loss did not improve from 37.61343
196/196 - 68s - loss: 37.2656 - MinusLogProbMetric: 37.2656 - val_loss: 37.7070 - val_MinusLogProbMetric: 37.7070 - lr: 4.1152e-06 - 68s/epoch - 346ms/step
Epoch 559/1000
2023-10-24 23:38:11.983 
Epoch 559/1000 
	 loss: 37.1691, MinusLogProbMetric: 37.1691, val_loss: 37.7002, val_MinusLogProbMetric: 37.7002

Epoch 559: val_loss did not improve from 37.61343
196/196 - 67s - loss: 37.1691 - MinusLogProbMetric: 37.1691 - val_loss: 37.7002 - val_MinusLogProbMetric: 37.7002 - lr: 4.1152e-06 - 67s/epoch - 344ms/step
Epoch 560/1000
2023-10-24 23:39:20.567 
Epoch 560/1000 
	 loss: 37.2363, MinusLogProbMetric: 37.2363, val_loss: 37.6875, val_MinusLogProbMetric: 37.6875

Epoch 560: val_loss did not improve from 37.61343
196/196 - 69s - loss: 37.2363 - MinusLogProbMetric: 37.2363 - val_loss: 37.6875 - val_MinusLogProbMetric: 37.6875 - lr: 4.1152e-06 - 69s/epoch - 350ms/step
Epoch 561/1000
2023-10-24 23:40:28.575 
Epoch 561/1000 
	 loss: 37.1447, MinusLogProbMetric: 37.1447, val_loss: 37.7919, val_MinusLogProbMetric: 37.7919

Epoch 561: val_loss did not improve from 37.61343
196/196 - 68s - loss: 37.1447 - MinusLogProbMetric: 37.1447 - val_loss: 37.7919 - val_MinusLogProbMetric: 37.7919 - lr: 4.1152e-06 - 68s/epoch - 347ms/step
Epoch 562/1000
2023-10-24 23:41:35.902 
Epoch 562/1000 
	 loss: 37.1036, MinusLogProbMetric: 37.1036, val_loss: 37.5851, val_MinusLogProbMetric: 37.5851

Epoch 562: val_loss improved from 37.61343 to 37.58514, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 68s - loss: 37.1036 - MinusLogProbMetric: 37.1036 - val_loss: 37.5851 - val_MinusLogProbMetric: 37.5851 - lr: 4.1152e-06 - 68s/epoch - 348ms/step
Epoch 563/1000
2023-10-24 23:42:42.329 
Epoch 563/1000 
	 loss: 37.2048, MinusLogProbMetric: 37.2048, val_loss: 37.5282, val_MinusLogProbMetric: 37.5282

Epoch 563: val_loss improved from 37.58514 to 37.52824, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 66s - loss: 37.2048 - MinusLogProbMetric: 37.2048 - val_loss: 37.5282 - val_MinusLogProbMetric: 37.5282 - lr: 4.1152e-06 - 66s/epoch - 339ms/step
Epoch 564/1000
2023-10-24 23:43:50.784 
Epoch 564/1000 
	 loss: 37.1294, MinusLogProbMetric: 37.1294, val_loss: 37.5210, val_MinusLogProbMetric: 37.5210

Epoch 564: val_loss improved from 37.52824 to 37.52103, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 68s - loss: 37.1294 - MinusLogProbMetric: 37.1294 - val_loss: 37.5210 - val_MinusLogProbMetric: 37.5210 - lr: 4.1152e-06 - 68s/epoch - 349ms/step
Epoch 565/1000
2023-10-24 23:44:59.404 
Epoch 565/1000 
	 loss: 37.1004, MinusLogProbMetric: 37.1004, val_loss: 37.6428, val_MinusLogProbMetric: 37.6428

Epoch 565: val_loss did not improve from 37.52103
196/196 - 68s - loss: 37.1004 - MinusLogProbMetric: 37.1004 - val_loss: 37.6428 - val_MinusLogProbMetric: 37.6428 - lr: 4.1152e-06 - 68s/epoch - 346ms/step
Epoch 566/1000
2023-10-24 23:46:06.531 
Epoch 566/1000 
	 loss: 38.5812, MinusLogProbMetric: 38.5812, val_loss: 37.5162, val_MinusLogProbMetric: 37.5162

Epoch 566: val_loss improved from 37.52103 to 37.51624, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 68s - loss: 38.5812 - MinusLogProbMetric: 38.5812 - val_loss: 37.5162 - val_MinusLogProbMetric: 37.5162 - lr: 4.1152e-06 - 68s/epoch - 347ms/step
Epoch 567/1000
2023-10-24 23:47:14.945 
Epoch 567/1000 
	 loss: 37.0850, MinusLogProbMetric: 37.0850, val_loss: 37.4811, val_MinusLogProbMetric: 37.4811

Epoch 567: val_loss improved from 37.51624 to 37.48111, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 68s - loss: 37.0850 - MinusLogProbMetric: 37.0850 - val_loss: 37.4811 - val_MinusLogProbMetric: 37.4811 - lr: 4.1152e-06 - 68s/epoch - 349ms/step
Epoch 568/1000
2023-10-24 23:48:23.420 
Epoch 568/1000 
	 loss: 37.1797, MinusLogProbMetric: 37.1797, val_loss: 37.7409, val_MinusLogProbMetric: 37.7409

Epoch 568: val_loss did not improve from 37.48111
196/196 - 68s - loss: 37.1797 - MinusLogProbMetric: 37.1797 - val_loss: 37.7409 - val_MinusLogProbMetric: 37.7409 - lr: 4.1152e-06 - 68s/epoch - 345ms/step
Epoch 569/1000
2023-10-24 23:49:28.977 
Epoch 569/1000 
	 loss: 37.0821, MinusLogProbMetric: 37.0821, val_loss: 37.6833, val_MinusLogProbMetric: 37.6833

Epoch 569: val_loss did not improve from 37.48111
196/196 - 66s - loss: 37.0821 - MinusLogProbMetric: 37.0821 - val_loss: 37.6833 - val_MinusLogProbMetric: 37.6833 - lr: 4.1152e-06 - 66s/epoch - 334ms/step
Epoch 570/1000
2023-10-24 23:50:37.138 
Epoch 570/1000 
	 loss: 37.0366, MinusLogProbMetric: 37.0366, val_loss: 37.5407, val_MinusLogProbMetric: 37.5407

Epoch 570: val_loss did not improve from 37.48111
196/196 - 68s - loss: 37.0366 - MinusLogProbMetric: 37.0366 - val_loss: 37.5407 - val_MinusLogProbMetric: 37.5407 - lr: 4.1152e-06 - 68s/epoch - 348ms/step
Epoch 571/1000
2023-10-24 23:51:46.140 
Epoch 571/1000 
	 loss: 37.0436, MinusLogProbMetric: 37.0436, val_loss: 37.5814, val_MinusLogProbMetric: 37.5814

Epoch 571: val_loss did not improve from 37.48111
196/196 - 69s - loss: 37.0436 - MinusLogProbMetric: 37.0436 - val_loss: 37.5814 - val_MinusLogProbMetric: 37.5814 - lr: 4.1152e-06 - 69s/epoch - 352ms/step
Epoch 572/1000
2023-10-24 23:52:52.730 
Epoch 572/1000 
	 loss: 36.9841, MinusLogProbMetric: 36.9841, val_loss: 37.4554, val_MinusLogProbMetric: 37.4554

Epoch 572: val_loss improved from 37.48111 to 37.45537, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 68s - loss: 36.9841 - MinusLogProbMetric: 36.9841 - val_loss: 37.4554 - val_MinusLogProbMetric: 37.4554 - lr: 4.1152e-06 - 68s/epoch - 345ms/step
Epoch 573/1000
2023-10-24 23:54:00.127 
Epoch 573/1000 
	 loss: 36.9898, MinusLogProbMetric: 36.9898, val_loss: 37.4070, val_MinusLogProbMetric: 37.4070

Epoch 573: val_loss improved from 37.45537 to 37.40696, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 67s - loss: 36.9898 - MinusLogProbMetric: 36.9898 - val_loss: 37.4070 - val_MinusLogProbMetric: 37.4070 - lr: 4.1152e-06 - 67s/epoch - 343ms/step
Epoch 574/1000
2023-10-24 23:55:07.138 
Epoch 574/1000 
	 loss: 37.0363, MinusLogProbMetric: 37.0363, val_loss: 37.3866, val_MinusLogProbMetric: 37.3866

Epoch 574: val_loss improved from 37.40696 to 37.38659, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 67s - loss: 37.0363 - MinusLogProbMetric: 37.0363 - val_loss: 37.3866 - val_MinusLogProbMetric: 37.3866 - lr: 4.1152e-06 - 67s/epoch - 342ms/step
Epoch 575/1000
2023-10-24 23:56:17.138 
Epoch 575/1000 
	 loss: 36.9435, MinusLogProbMetric: 36.9435, val_loss: 37.5080, val_MinusLogProbMetric: 37.5080

Epoch 575: val_loss did not improve from 37.38659
196/196 - 69s - loss: 36.9435 - MinusLogProbMetric: 36.9435 - val_loss: 37.5080 - val_MinusLogProbMetric: 37.5080 - lr: 4.1152e-06 - 69s/epoch - 353ms/step
Epoch 576/1000
2023-10-24 23:57:24.004 
Epoch 576/1000 
	 loss: 37.0438, MinusLogProbMetric: 37.0438, val_loss: 37.6368, val_MinusLogProbMetric: 37.6368

Epoch 576: val_loss did not improve from 37.38659
196/196 - 67s - loss: 37.0438 - MinusLogProbMetric: 37.0438 - val_loss: 37.6368 - val_MinusLogProbMetric: 37.6368 - lr: 4.1152e-06 - 67s/epoch - 341ms/step
Epoch 577/1000
2023-10-24 23:58:31.507 
Epoch 577/1000 
	 loss: 36.9677, MinusLogProbMetric: 36.9677, val_loss: 37.4380, val_MinusLogProbMetric: 37.4380

Epoch 577: val_loss did not improve from 37.38659
196/196 - 67s - loss: 36.9677 - MinusLogProbMetric: 36.9677 - val_loss: 37.4380 - val_MinusLogProbMetric: 37.4380 - lr: 4.1152e-06 - 67s/epoch - 344ms/step
Epoch 578/1000
2023-10-24 23:59:39.801 
Epoch 578/1000 
	 loss: 36.9163, MinusLogProbMetric: 36.9163, val_loss: 37.4593, val_MinusLogProbMetric: 37.4593

Epoch 578: val_loss did not improve from 37.38659
196/196 - 68s - loss: 36.9163 - MinusLogProbMetric: 36.9163 - val_loss: 37.4593 - val_MinusLogProbMetric: 37.4593 - lr: 4.1152e-06 - 68s/epoch - 348ms/step
Epoch 579/1000
2023-10-25 00:00:46.574 
Epoch 579/1000 
	 loss: 36.9399, MinusLogProbMetric: 36.9399, val_loss: 37.5264, val_MinusLogProbMetric: 37.5264

Epoch 579: val_loss did not improve from 37.38659
196/196 - 67s - loss: 36.9399 - MinusLogProbMetric: 36.9399 - val_loss: 37.5264 - val_MinusLogProbMetric: 37.5264 - lr: 4.1152e-06 - 67s/epoch - 341ms/step
Epoch 580/1000
2023-10-25 00:01:54.326 
Epoch 580/1000 
	 loss: 36.9231, MinusLogProbMetric: 36.9231, val_loss: 37.3507, val_MinusLogProbMetric: 37.3507

Epoch 580: val_loss improved from 37.38659 to 37.35073, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 69s - loss: 36.9231 - MinusLogProbMetric: 36.9231 - val_loss: 37.3507 - val_MinusLogProbMetric: 37.3507 - lr: 4.1152e-06 - 69s/epoch - 351ms/step
Epoch 581/1000
2023-10-25 00:03:01.763 
Epoch 581/1000 
	 loss: 37.0230, MinusLogProbMetric: 37.0230, val_loss: 37.3831, val_MinusLogProbMetric: 37.3831

Epoch 581: val_loss did not improve from 37.35073
196/196 - 66s - loss: 37.0230 - MinusLogProbMetric: 37.0230 - val_loss: 37.3831 - val_MinusLogProbMetric: 37.3831 - lr: 4.1152e-06 - 66s/epoch - 339ms/step
Epoch 582/1000
2023-10-25 00:04:08.331 
Epoch 582/1000 
	 loss: 36.8764, MinusLogProbMetric: 36.8764, val_loss: 37.4262, val_MinusLogProbMetric: 37.4262

Epoch 582: val_loss did not improve from 37.35073
196/196 - 67s - loss: 36.8764 - MinusLogProbMetric: 36.8764 - val_loss: 37.4262 - val_MinusLogProbMetric: 37.4262 - lr: 4.1152e-06 - 67s/epoch - 340ms/step
Epoch 583/1000
2023-10-25 00:05:16.963 
Epoch 583/1000 
	 loss: 37.2826, MinusLogProbMetric: 37.2826, val_loss: 37.4247, val_MinusLogProbMetric: 37.4247

Epoch 583: val_loss did not improve from 37.35073
196/196 - 69s - loss: 37.2826 - MinusLogProbMetric: 37.2826 - val_loss: 37.4247 - val_MinusLogProbMetric: 37.4247 - lr: 4.1152e-06 - 69s/epoch - 350ms/step
Epoch 584/1000
2023-10-25 00:06:23.499 
Epoch 584/1000 
	 loss: 36.8587, MinusLogProbMetric: 36.8587, val_loss: 37.3619, val_MinusLogProbMetric: 37.3619

Epoch 584: val_loss did not improve from 37.35073
196/196 - 67s - loss: 36.8587 - MinusLogProbMetric: 36.8587 - val_loss: 37.3619 - val_MinusLogProbMetric: 37.3619 - lr: 4.1152e-06 - 67s/epoch - 339ms/step
Epoch 585/1000
2023-10-25 00:07:31.435 
Epoch 585/1000 
	 loss: 36.9661, MinusLogProbMetric: 36.9661, val_loss: 37.3248, val_MinusLogProbMetric: 37.3248

Epoch 585: val_loss improved from 37.35073 to 37.32478, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 69s - loss: 36.9661 - MinusLogProbMetric: 36.9661 - val_loss: 37.3248 - val_MinusLogProbMetric: 37.3248 - lr: 4.1152e-06 - 69s/epoch - 352ms/step
Epoch 586/1000
2023-10-25 00:08:38.243 
Epoch 586/1000 
	 loss: 36.8520, MinusLogProbMetric: 36.8520, val_loss: 37.3512, val_MinusLogProbMetric: 37.3512

Epoch 586: val_loss did not improve from 37.32478
196/196 - 66s - loss: 36.8520 - MinusLogProbMetric: 36.8520 - val_loss: 37.3512 - val_MinusLogProbMetric: 37.3512 - lr: 4.1152e-06 - 66s/epoch - 335ms/step
Epoch 587/1000
2023-10-25 00:09:46.127 
Epoch 587/1000 
	 loss: 36.8302, MinusLogProbMetric: 36.8302, val_loss: 37.4082, val_MinusLogProbMetric: 37.4082

Epoch 587: val_loss did not improve from 37.32478
196/196 - 68s - loss: 36.8302 - MinusLogProbMetric: 36.8302 - val_loss: 37.4082 - val_MinusLogProbMetric: 37.4082 - lr: 4.1152e-06 - 68s/epoch - 346ms/step
Epoch 588/1000
2023-10-25 00:10:55.729 
Epoch 588/1000 
	 loss: 36.8620, MinusLogProbMetric: 36.8620, val_loss: 37.3091, val_MinusLogProbMetric: 37.3091

Epoch 588: val_loss improved from 37.32478 to 37.30905, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 71s - loss: 36.8620 - MinusLogProbMetric: 36.8620 - val_loss: 37.3091 - val_MinusLogProbMetric: 37.3091 - lr: 4.1152e-06 - 71s/epoch - 360ms/step
Epoch 589/1000
2023-10-25 00:12:04.408 
Epoch 589/1000 
	 loss: 36.8255, MinusLogProbMetric: 36.8255, val_loss: 37.5266, val_MinusLogProbMetric: 37.5266

Epoch 589: val_loss did not improve from 37.30905
196/196 - 68s - loss: 36.8255 - MinusLogProbMetric: 36.8255 - val_loss: 37.5266 - val_MinusLogProbMetric: 37.5266 - lr: 4.1152e-06 - 68s/epoch - 345ms/step
Epoch 590/1000
2023-10-25 00:13:13.490 
Epoch 590/1000 
	 loss: 36.7899, MinusLogProbMetric: 36.7899, val_loss: 37.3216, val_MinusLogProbMetric: 37.3216

Epoch 590: val_loss did not improve from 37.30905
196/196 - 69s - loss: 36.7899 - MinusLogProbMetric: 36.7899 - val_loss: 37.3216 - val_MinusLogProbMetric: 37.3216 - lr: 4.1152e-06 - 69s/epoch - 352ms/step
Epoch 591/1000
2023-10-25 00:14:19.229 
Epoch 591/1000 
	 loss: 36.9573, MinusLogProbMetric: 36.9573, val_loss: 37.4065, val_MinusLogProbMetric: 37.4065

Epoch 591: val_loss did not improve from 37.30905
196/196 - 66s - loss: 36.9573 - MinusLogProbMetric: 36.9573 - val_loss: 37.4065 - val_MinusLogProbMetric: 37.4065 - lr: 4.1152e-06 - 66s/epoch - 335ms/step
Epoch 592/1000
2023-10-25 00:15:24.686 
Epoch 592/1000 
	 loss: 36.7937, MinusLogProbMetric: 36.7937, val_loss: 37.3047, val_MinusLogProbMetric: 37.3047

Epoch 592: val_loss improved from 37.30905 to 37.30472, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 66s - loss: 36.7937 - MinusLogProbMetric: 36.7937 - val_loss: 37.3047 - val_MinusLogProbMetric: 37.3047 - lr: 4.1152e-06 - 66s/epoch - 338ms/step
Epoch 593/1000
2023-10-25 00:16:35.161 
Epoch 593/1000 
	 loss: 36.8223, MinusLogProbMetric: 36.8223, val_loss: 37.3283, val_MinusLogProbMetric: 37.3283

Epoch 593: val_loss did not improve from 37.30472
196/196 - 70s - loss: 36.8223 - MinusLogProbMetric: 36.8223 - val_loss: 37.3283 - val_MinusLogProbMetric: 37.3283 - lr: 4.1152e-06 - 70s/epoch - 355ms/step
Epoch 594/1000
2023-10-25 00:17:41.104 
Epoch 594/1000 
	 loss: 36.8003, MinusLogProbMetric: 36.8003, val_loss: 37.2387, val_MinusLogProbMetric: 37.2387

Epoch 594: val_loss improved from 37.30472 to 37.23872, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 67s - loss: 36.8003 - MinusLogProbMetric: 36.8003 - val_loss: 37.2387 - val_MinusLogProbMetric: 37.2387 - lr: 4.1152e-06 - 67s/epoch - 341ms/step
Epoch 595/1000
2023-10-25 00:18:49.080 
Epoch 595/1000 
	 loss: 36.7566, MinusLogProbMetric: 36.7566, val_loss: 37.2383, val_MinusLogProbMetric: 37.2383

Epoch 595: val_loss improved from 37.23872 to 37.23834, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 68s - loss: 36.7566 - MinusLogProbMetric: 36.7566 - val_loss: 37.2383 - val_MinusLogProbMetric: 37.2383 - lr: 4.1152e-06 - 68s/epoch - 347ms/step
Epoch 596/1000
2023-10-25 00:19:58.588 
Epoch 596/1000 
	 loss: 36.7598, MinusLogProbMetric: 36.7598, val_loss: 37.4605, val_MinusLogProbMetric: 37.4605

Epoch 596: val_loss did not improve from 37.23834
196/196 - 69s - loss: 36.7598 - MinusLogProbMetric: 36.7598 - val_loss: 37.4605 - val_MinusLogProbMetric: 37.4605 - lr: 4.1152e-06 - 69s/epoch - 350ms/step
Epoch 597/1000
2023-10-25 00:21:03.988 
Epoch 597/1000 
	 loss: 36.7298, MinusLogProbMetric: 36.7298, val_loss: 37.3809, val_MinusLogProbMetric: 37.3809

Epoch 597: val_loss did not improve from 37.23834
196/196 - 65s - loss: 36.7298 - MinusLogProbMetric: 36.7298 - val_loss: 37.3809 - val_MinusLogProbMetric: 37.3809 - lr: 4.1152e-06 - 65s/epoch - 334ms/step
Epoch 598/1000
2023-10-25 00:22:10.767 
Epoch 598/1000 
	 loss: 36.7766, MinusLogProbMetric: 36.7766, val_loss: 37.2341, val_MinusLogProbMetric: 37.2341

Epoch 598: val_loss improved from 37.23834 to 37.23410, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 68s - loss: 36.7766 - MinusLogProbMetric: 36.7766 - val_loss: 37.2341 - val_MinusLogProbMetric: 37.2341 - lr: 4.1152e-06 - 68s/epoch - 345ms/step
Epoch 599/1000
2023-10-25 00:23:17.344 
Epoch 599/1000 
	 loss: 36.7040, MinusLogProbMetric: 36.7040, val_loss: 37.1535, val_MinusLogProbMetric: 37.1535

Epoch 599: val_loss improved from 37.23410 to 37.15355, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 66s - loss: 36.7040 - MinusLogProbMetric: 36.7040 - val_loss: 37.1535 - val_MinusLogProbMetric: 37.1535 - lr: 4.1152e-06 - 66s/epoch - 339ms/step
Epoch 600/1000
2023-10-25 00:24:24.047 
Epoch 600/1000 
	 loss: 36.8733, MinusLogProbMetric: 36.8733, val_loss: 37.4205, val_MinusLogProbMetric: 37.4205

Epoch 600: val_loss did not improve from 37.15355
196/196 - 66s - loss: 36.8733 - MinusLogProbMetric: 36.8733 - val_loss: 37.4205 - val_MinusLogProbMetric: 37.4205 - lr: 4.1152e-06 - 66s/epoch - 336ms/step
Epoch 601/1000
2023-10-25 00:25:32.921 
Epoch 601/1000 
	 loss: 36.7194, MinusLogProbMetric: 36.7194, val_loss: 37.2272, val_MinusLogProbMetric: 37.2272

Epoch 601: val_loss did not improve from 37.15355
196/196 - 69s - loss: 36.7194 - MinusLogProbMetric: 36.7194 - val_loss: 37.2272 - val_MinusLogProbMetric: 37.2272 - lr: 4.1152e-06 - 69s/epoch - 351ms/step
Epoch 602/1000
2023-10-25 00:26:39.757 
Epoch 602/1000 
	 loss: 36.6746, MinusLogProbMetric: 36.6746, val_loss: 37.3447, val_MinusLogProbMetric: 37.3447

Epoch 602: val_loss did not improve from 37.15355
196/196 - 67s - loss: 36.6746 - MinusLogProbMetric: 36.6746 - val_loss: 37.3447 - val_MinusLogProbMetric: 37.3447 - lr: 4.1152e-06 - 67s/epoch - 341ms/step
Epoch 603/1000
2023-10-25 00:27:47.070 
Epoch 603/1000 
	 loss: 36.6767, MinusLogProbMetric: 36.6767, val_loss: 37.3368, val_MinusLogProbMetric: 37.3368

Epoch 603: val_loss did not improve from 37.15355
196/196 - 67s - loss: 36.6767 - MinusLogProbMetric: 36.6767 - val_loss: 37.3368 - val_MinusLogProbMetric: 37.3368 - lr: 4.1152e-06 - 67s/epoch - 343ms/step
Epoch 604/1000
2023-10-25 00:28:53.831 
Epoch 604/1000 
	 loss: 36.7289, MinusLogProbMetric: 36.7289, val_loss: 36.9890, val_MinusLogProbMetric: 36.9890

Epoch 604: val_loss improved from 37.15355 to 36.98896, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 68s - loss: 36.7289 - MinusLogProbMetric: 36.7289 - val_loss: 36.9890 - val_MinusLogProbMetric: 36.9890 - lr: 4.1152e-06 - 68s/epoch - 346ms/step
Epoch 605/1000
2023-10-25 00:30:02.729 
Epoch 605/1000 
	 loss: 36.7148, MinusLogProbMetric: 36.7148, val_loss: 37.1146, val_MinusLogProbMetric: 37.1146

Epoch 605: val_loss did not improve from 36.98896
196/196 - 68s - loss: 36.7148 - MinusLogProbMetric: 36.7148 - val_loss: 37.1146 - val_MinusLogProbMetric: 37.1146 - lr: 4.1152e-06 - 68s/epoch - 346ms/step
Epoch 606/1000
2023-10-25 00:31:11.117 
Epoch 606/1000 
	 loss: 36.6630, MinusLogProbMetric: 36.6630, val_loss: 37.2926, val_MinusLogProbMetric: 37.2926

Epoch 606: val_loss did not improve from 36.98896
196/196 - 68s - loss: 36.6630 - MinusLogProbMetric: 36.6630 - val_loss: 37.2926 - val_MinusLogProbMetric: 37.2926 - lr: 4.1152e-06 - 68s/epoch - 349ms/step
Epoch 607/1000
2023-10-25 00:32:16.674 
Epoch 607/1000 
	 loss: 36.6012, MinusLogProbMetric: 36.6012, val_loss: 37.3646, val_MinusLogProbMetric: 37.3646

Epoch 607: val_loss did not improve from 36.98896
196/196 - 66s - loss: 36.6012 - MinusLogProbMetric: 36.6012 - val_loss: 37.3646 - val_MinusLogProbMetric: 37.3646 - lr: 4.1152e-06 - 66s/epoch - 334ms/step
Epoch 608/1000
2023-10-25 00:33:19.802 
Epoch 608/1000 
	 loss: 36.7178, MinusLogProbMetric: 36.7178, val_loss: 37.0983, val_MinusLogProbMetric: 37.0983

Epoch 608: val_loss did not improve from 36.98896
196/196 - 63s - loss: 36.7178 - MinusLogProbMetric: 36.7178 - val_loss: 37.0983 - val_MinusLogProbMetric: 37.0983 - lr: 4.1152e-06 - 63s/epoch - 322ms/step
Epoch 609/1000
2023-10-25 00:34:26.655 
Epoch 609/1000 
	 loss: 36.8352, MinusLogProbMetric: 36.8352, val_loss: 37.3501, val_MinusLogProbMetric: 37.3501

Epoch 609: val_loss did not improve from 36.98896
196/196 - 67s - loss: 36.8352 - MinusLogProbMetric: 36.8352 - val_loss: 37.3501 - val_MinusLogProbMetric: 37.3501 - lr: 4.1152e-06 - 67s/epoch - 341ms/step
Epoch 610/1000
2023-10-25 00:35:34.420 
Epoch 610/1000 
	 loss: 36.6081, MinusLogProbMetric: 36.6081, val_loss: 37.0857, val_MinusLogProbMetric: 37.0857

Epoch 610: val_loss did not improve from 36.98896
196/196 - 68s - loss: 36.6081 - MinusLogProbMetric: 36.6081 - val_loss: 37.0857 - val_MinusLogProbMetric: 37.0857 - lr: 4.1152e-06 - 68s/epoch - 346ms/step
Epoch 611/1000
2023-10-25 00:36:41.818 
Epoch 611/1000 
	 loss: 36.6296, MinusLogProbMetric: 36.6296, val_loss: 37.0008, val_MinusLogProbMetric: 37.0008

Epoch 611: val_loss did not improve from 36.98896
196/196 - 67s - loss: 36.6296 - MinusLogProbMetric: 36.6296 - val_loss: 37.0008 - val_MinusLogProbMetric: 37.0008 - lr: 4.1152e-06 - 67s/epoch - 344ms/step
Epoch 612/1000
2023-10-25 00:37:50.290 
Epoch 612/1000 
	 loss: 36.5988, MinusLogProbMetric: 36.5988, val_loss: 36.9654, val_MinusLogProbMetric: 36.9654

Epoch 612: val_loss improved from 36.98896 to 36.96541, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 70s - loss: 36.5988 - MinusLogProbMetric: 36.5988 - val_loss: 36.9654 - val_MinusLogProbMetric: 36.9654 - lr: 4.1152e-06 - 70s/epoch - 355ms/step
Epoch 613/1000
2023-10-25 00:38:59.266 
Epoch 613/1000 
	 loss: 38.8908, MinusLogProbMetric: 38.8908, val_loss: 37.3976, val_MinusLogProbMetric: 37.3976

Epoch 613: val_loss did not improve from 36.96541
196/196 - 68s - loss: 38.8908 - MinusLogProbMetric: 38.8908 - val_loss: 37.3976 - val_MinusLogProbMetric: 37.3976 - lr: 4.1152e-06 - 68s/epoch - 346ms/step
Epoch 614/1000
2023-10-25 00:40:07.025 
Epoch 614/1000 
	 loss: 36.6421, MinusLogProbMetric: 36.6421, val_loss: 37.1644, val_MinusLogProbMetric: 37.1644

Epoch 614: val_loss did not improve from 36.96541
196/196 - 68s - loss: 36.6421 - MinusLogProbMetric: 36.6421 - val_loss: 37.1644 - val_MinusLogProbMetric: 37.1644 - lr: 4.1152e-06 - 68s/epoch - 346ms/step
Epoch 615/1000
2023-10-25 00:41:13.717 
Epoch 615/1000 
	 loss: 36.5577, MinusLogProbMetric: 36.5577, val_loss: 37.0215, val_MinusLogProbMetric: 37.0215

Epoch 615: val_loss did not improve from 36.96541
196/196 - 67s - loss: 36.5577 - MinusLogProbMetric: 36.5577 - val_loss: 37.0215 - val_MinusLogProbMetric: 37.0215 - lr: 4.1152e-06 - 67s/epoch - 340ms/step
Epoch 616/1000
2023-10-25 00:42:19.759 
Epoch 616/1000 
	 loss: 36.5993, MinusLogProbMetric: 36.5993, val_loss: 37.0162, val_MinusLogProbMetric: 37.0162

Epoch 616: val_loss did not improve from 36.96541
196/196 - 66s - loss: 36.5993 - MinusLogProbMetric: 36.5993 - val_loss: 37.0162 - val_MinusLogProbMetric: 37.0162 - lr: 4.1152e-06 - 66s/epoch - 337ms/step
Epoch 617/1000
2023-10-25 00:43:25.858 
Epoch 617/1000 
	 loss: 36.5347, MinusLogProbMetric: 36.5347, val_loss: 37.1935, val_MinusLogProbMetric: 37.1935

Epoch 617: val_loss did not improve from 36.96541
196/196 - 66s - loss: 36.5347 - MinusLogProbMetric: 36.5347 - val_loss: 37.1935 - val_MinusLogProbMetric: 37.1935 - lr: 4.1152e-06 - 66s/epoch - 337ms/step
Epoch 618/1000
2023-10-25 00:44:29.607 
Epoch 618/1000 
	 loss: 36.5965, MinusLogProbMetric: 36.5965, val_loss: 37.0932, val_MinusLogProbMetric: 37.0932

Epoch 618: val_loss did not improve from 36.96541
196/196 - 64s - loss: 36.5965 - MinusLogProbMetric: 36.5965 - val_loss: 37.0932 - val_MinusLogProbMetric: 37.0932 - lr: 4.1152e-06 - 64s/epoch - 325ms/step
Epoch 619/1000
2023-10-25 00:45:36.597 
Epoch 619/1000 
	 loss: 36.5331, MinusLogProbMetric: 36.5331, val_loss: 37.0696, val_MinusLogProbMetric: 37.0696

Epoch 619: val_loss did not improve from 36.96541
196/196 - 67s - loss: 36.5331 - MinusLogProbMetric: 36.5331 - val_loss: 37.0696 - val_MinusLogProbMetric: 37.0696 - lr: 4.1152e-06 - 67s/epoch - 342ms/step
Epoch 620/1000
2023-10-25 00:46:43.242 
Epoch 620/1000 
	 loss: 36.5666, MinusLogProbMetric: 36.5666, val_loss: 37.0410, val_MinusLogProbMetric: 37.0410

Epoch 620: val_loss did not improve from 36.96541
196/196 - 67s - loss: 36.5666 - MinusLogProbMetric: 36.5666 - val_loss: 37.0410 - val_MinusLogProbMetric: 37.0410 - lr: 4.1152e-06 - 67s/epoch - 340ms/step
Epoch 621/1000
2023-10-25 00:47:50.204 
Epoch 621/1000 
	 loss: 36.4918, MinusLogProbMetric: 36.4918, val_loss: 36.9616, val_MinusLogProbMetric: 36.9616

Epoch 621: val_loss improved from 36.96541 to 36.96162, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 68s - loss: 36.4918 - MinusLogProbMetric: 36.4918 - val_loss: 36.9616 - val_MinusLogProbMetric: 36.9616 - lr: 4.1152e-06 - 68s/epoch - 347ms/step
Epoch 622/1000
2023-10-25 00:48:58.864 
Epoch 622/1000 
	 loss: 36.4734, MinusLogProbMetric: 36.4734, val_loss: 37.1274, val_MinusLogProbMetric: 37.1274

Epoch 622: val_loss did not improve from 36.96162
196/196 - 68s - loss: 36.4734 - MinusLogProbMetric: 36.4734 - val_loss: 37.1274 - val_MinusLogProbMetric: 37.1274 - lr: 4.1152e-06 - 68s/epoch - 345ms/step
Epoch 623/1000
2023-10-25 00:50:05.794 
Epoch 623/1000 
	 loss: 36.5164, MinusLogProbMetric: 36.5164, val_loss: 37.1649, val_MinusLogProbMetric: 37.1649

Epoch 623: val_loss did not improve from 36.96162
196/196 - 67s - loss: 36.5164 - MinusLogProbMetric: 36.5164 - val_loss: 37.1649 - val_MinusLogProbMetric: 37.1649 - lr: 4.1152e-06 - 67s/epoch - 341ms/step
Epoch 624/1000
2023-10-25 00:51:14.473 
Epoch 624/1000 
	 loss: 36.4432, MinusLogProbMetric: 36.4432, val_loss: 37.3190, val_MinusLogProbMetric: 37.3190

Epoch 624: val_loss did not improve from 36.96162
196/196 - 69s - loss: 36.4432 - MinusLogProbMetric: 36.4432 - val_loss: 37.3190 - val_MinusLogProbMetric: 37.3190 - lr: 4.1152e-06 - 69s/epoch - 350ms/step
Epoch 625/1000
2023-10-25 00:52:21.422 
Epoch 625/1000 
	 loss: 36.4951, MinusLogProbMetric: 36.4951, val_loss: 37.3616, val_MinusLogProbMetric: 37.3616

Epoch 625: val_loss did not improve from 36.96162
196/196 - 67s - loss: 36.4951 - MinusLogProbMetric: 36.4951 - val_loss: 37.3616 - val_MinusLogProbMetric: 37.3616 - lr: 4.1152e-06 - 67s/epoch - 342ms/step
Epoch 626/1000
2023-10-25 00:53:29.377 
Epoch 626/1000 
	 loss: 36.4508, MinusLogProbMetric: 36.4508, val_loss: 37.0695, val_MinusLogProbMetric: 37.0695

Epoch 626: val_loss did not improve from 36.96162
196/196 - 68s - loss: 36.4508 - MinusLogProbMetric: 36.4508 - val_loss: 37.0695 - val_MinusLogProbMetric: 37.0695 - lr: 4.1152e-06 - 68s/epoch - 347ms/step
Epoch 627/1000
2023-10-25 00:54:36.024 
Epoch 627/1000 
	 loss: 36.4002, MinusLogProbMetric: 36.4002, val_loss: 36.9224, val_MinusLogProbMetric: 36.9224

Epoch 627: val_loss improved from 36.96162 to 36.92242, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 68s - loss: 36.4002 - MinusLogProbMetric: 36.4002 - val_loss: 36.9224 - val_MinusLogProbMetric: 36.9224 - lr: 4.1152e-06 - 68s/epoch - 345ms/step
Epoch 628/1000
2023-10-25 00:55:43.991 
Epoch 628/1000 
	 loss: 36.8803, MinusLogProbMetric: 36.8803, val_loss: 36.8816, val_MinusLogProbMetric: 36.8816

Epoch 628: val_loss improved from 36.92242 to 36.88160, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 68s - loss: 36.8803 - MinusLogProbMetric: 36.8803 - val_loss: 36.8816 - val_MinusLogProbMetric: 36.8816 - lr: 4.1152e-06 - 68s/epoch - 347ms/step
Epoch 629/1000
2023-10-25 00:56:51.661 
Epoch 629/1000 
	 loss: 36.4415, MinusLogProbMetric: 36.4415, val_loss: 37.2418, val_MinusLogProbMetric: 37.2418

Epoch 629: val_loss did not improve from 36.88160
196/196 - 67s - loss: 36.4415 - MinusLogProbMetric: 36.4415 - val_loss: 37.2418 - val_MinusLogProbMetric: 37.2418 - lr: 4.1152e-06 - 67s/epoch - 340ms/step
Epoch 630/1000
2023-10-25 00:57:58.448 
Epoch 630/1000 
	 loss: 36.4023, MinusLogProbMetric: 36.4023, val_loss: 36.8456, val_MinusLogProbMetric: 36.8456

Epoch 630: val_loss improved from 36.88160 to 36.84559, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 68s - loss: 36.4023 - MinusLogProbMetric: 36.4023 - val_loss: 36.8456 - val_MinusLogProbMetric: 36.8456 - lr: 4.1152e-06 - 68s/epoch - 345ms/step
Epoch 631/1000
2023-10-25 00:59:07.064 
Epoch 631/1000 
	 loss: 36.3840, MinusLogProbMetric: 36.3840, val_loss: 36.7554, val_MinusLogProbMetric: 36.7554

Epoch 631: val_loss improved from 36.84559 to 36.75542, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 69s - loss: 36.3840 - MinusLogProbMetric: 36.3840 - val_loss: 36.7554 - val_MinusLogProbMetric: 36.7554 - lr: 4.1152e-06 - 69s/epoch - 350ms/step
Epoch 632/1000
2023-10-25 01:00:13.773 
Epoch 632/1000 
	 loss: 36.4246, MinusLogProbMetric: 36.4246, val_loss: 37.0021, val_MinusLogProbMetric: 37.0021

Epoch 632: val_loss did not improve from 36.75542
196/196 - 66s - loss: 36.4246 - MinusLogProbMetric: 36.4246 - val_loss: 37.0021 - val_MinusLogProbMetric: 37.0021 - lr: 4.1152e-06 - 66s/epoch - 335ms/step
Epoch 633/1000
2023-10-25 01:01:20.254 
Epoch 633/1000 
	 loss: 36.3591, MinusLogProbMetric: 36.3591, val_loss: 37.3864, val_MinusLogProbMetric: 37.3864

Epoch 633: val_loss did not improve from 36.75542
196/196 - 66s - loss: 36.3591 - MinusLogProbMetric: 36.3591 - val_loss: 37.3864 - val_MinusLogProbMetric: 37.3864 - lr: 4.1152e-06 - 66s/epoch - 339ms/step
Epoch 634/1000
2023-10-25 01:02:28.196 
Epoch 634/1000 
	 loss: 36.4445, MinusLogProbMetric: 36.4445, val_loss: 36.9169, val_MinusLogProbMetric: 36.9169

Epoch 634: val_loss did not improve from 36.75542
196/196 - 68s - loss: 36.4445 - MinusLogProbMetric: 36.4445 - val_loss: 36.9169 - val_MinusLogProbMetric: 36.9169 - lr: 4.1152e-06 - 68s/epoch - 347ms/step
Epoch 635/1000
2023-10-25 01:03:37.125 
Epoch 635/1000 
	 loss: 36.3560, MinusLogProbMetric: 36.3560, val_loss: 37.0600, val_MinusLogProbMetric: 37.0600

Epoch 635: val_loss did not improve from 36.75542
196/196 - 69s - loss: 36.3560 - MinusLogProbMetric: 36.3560 - val_loss: 37.0600 - val_MinusLogProbMetric: 37.0600 - lr: 4.1152e-06 - 69s/epoch - 352ms/step
Epoch 636/1000
2023-10-25 01:04:45.217 
Epoch 636/1000 
	 loss: 36.4243, MinusLogProbMetric: 36.4243, val_loss: 36.8184, val_MinusLogProbMetric: 36.8184

Epoch 636: val_loss did not improve from 36.75542
196/196 - 68s - loss: 36.4243 - MinusLogProbMetric: 36.4243 - val_loss: 36.8184 - val_MinusLogProbMetric: 36.8184 - lr: 4.1152e-06 - 68s/epoch - 347ms/step
Epoch 637/1000
2023-10-25 01:05:52.462 
Epoch 637/1000 
	 loss: 36.3368, MinusLogProbMetric: 36.3368, val_loss: 37.0644, val_MinusLogProbMetric: 37.0644

Epoch 637: val_loss did not improve from 36.75542
196/196 - 67s - loss: 36.3368 - MinusLogProbMetric: 36.3368 - val_loss: 37.0644 - val_MinusLogProbMetric: 37.0644 - lr: 4.1152e-06 - 67s/epoch - 343ms/step
Epoch 638/1000
2023-10-25 01:06:59.758 
Epoch 638/1000 
	 loss: 36.3478, MinusLogProbMetric: 36.3478, val_loss: 37.0461, val_MinusLogProbMetric: 37.0461

Epoch 638: val_loss did not improve from 36.75542
196/196 - 67s - loss: 36.3478 - MinusLogProbMetric: 36.3478 - val_loss: 37.0461 - val_MinusLogProbMetric: 37.0461 - lr: 4.1152e-06 - 67s/epoch - 343ms/step
Epoch 639/1000
2023-10-25 01:08:08.998 
Epoch 639/1000 
	 loss: 36.6152, MinusLogProbMetric: 36.6152, val_loss: 36.8002, val_MinusLogProbMetric: 36.8002

Epoch 639: val_loss did not improve from 36.75542
196/196 - 69s - loss: 36.6152 - MinusLogProbMetric: 36.6152 - val_loss: 36.8002 - val_MinusLogProbMetric: 36.8002 - lr: 4.1152e-06 - 69s/epoch - 353ms/step
Epoch 640/1000
2023-10-25 01:09:17.450 
Epoch 640/1000 
	 loss: 36.2850, MinusLogProbMetric: 36.2850, val_loss: 36.8335, val_MinusLogProbMetric: 36.8335

Epoch 640: val_loss did not improve from 36.75542
196/196 - 68s - loss: 36.2850 - MinusLogProbMetric: 36.2850 - val_loss: 36.8335 - val_MinusLogProbMetric: 36.8335 - lr: 4.1152e-06 - 68s/epoch - 349ms/step
Epoch 641/1000
2023-10-25 01:10:26.154 
Epoch 641/1000 
	 loss: 36.3368, MinusLogProbMetric: 36.3368, val_loss: 36.9562, val_MinusLogProbMetric: 36.9562

Epoch 641: val_loss did not improve from 36.75542
196/196 - 69s - loss: 36.3368 - MinusLogProbMetric: 36.3368 - val_loss: 36.9562 - val_MinusLogProbMetric: 36.9562 - lr: 4.1152e-06 - 69s/epoch - 351ms/step
Epoch 642/1000
2023-10-25 01:11:35.251 
Epoch 642/1000 
	 loss: 36.2934, MinusLogProbMetric: 36.2934, val_loss: 37.0074, val_MinusLogProbMetric: 37.0074

Epoch 642: val_loss did not improve from 36.75542
196/196 - 69s - loss: 36.2934 - MinusLogProbMetric: 36.2934 - val_loss: 37.0074 - val_MinusLogProbMetric: 37.0074 - lr: 4.1152e-06 - 69s/epoch - 353ms/step
Epoch 643/1000
2023-10-25 01:12:42.818 
Epoch 643/1000 
	 loss: 36.3047, MinusLogProbMetric: 36.3047, val_loss: 36.7979, val_MinusLogProbMetric: 36.7979

Epoch 643: val_loss did not improve from 36.75542
196/196 - 68s - loss: 36.3047 - MinusLogProbMetric: 36.3047 - val_loss: 36.7979 - val_MinusLogProbMetric: 36.7979 - lr: 4.1152e-06 - 68s/epoch - 345ms/step
Epoch 644/1000
2023-10-25 01:13:52.301 
Epoch 644/1000 
	 loss: 36.3155, MinusLogProbMetric: 36.3155, val_loss: 36.7722, val_MinusLogProbMetric: 36.7722

Epoch 644: val_loss did not improve from 36.75542
196/196 - 69s - loss: 36.3155 - MinusLogProbMetric: 36.3155 - val_loss: 36.7722 - val_MinusLogProbMetric: 36.7722 - lr: 4.1152e-06 - 69s/epoch - 354ms/step
Epoch 645/1000
2023-10-25 01:14:45.231 
Epoch 645/1000 
	 loss: 36.2647, MinusLogProbMetric: 36.2647, val_loss: 36.8461, val_MinusLogProbMetric: 36.8461

Epoch 645: val_loss did not improve from 36.75542
196/196 - 53s - loss: 36.2647 - MinusLogProbMetric: 36.2647 - val_loss: 36.8461 - val_MinusLogProbMetric: 36.8461 - lr: 4.1152e-06 - 53s/epoch - 270ms/step
Epoch 646/1000
2023-10-25 01:15:47.949 
Epoch 646/1000 
	 loss: 36.2708, MinusLogProbMetric: 36.2708, val_loss: 36.6959, val_MinusLogProbMetric: 36.6959

Epoch 646: val_loss improved from 36.75542 to 36.69587, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 64s - loss: 36.2708 - MinusLogProbMetric: 36.2708 - val_loss: 36.6959 - val_MinusLogProbMetric: 36.6959 - lr: 4.1152e-06 - 64s/epoch - 326ms/step
Epoch 647/1000
2023-10-25 01:16:49.737 
Epoch 647/1000 
	 loss: 36.2300, MinusLogProbMetric: 36.2300, val_loss: 36.7446, val_MinusLogProbMetric: 36.7446

Epoch 647: val_loss did not improve from 36.69587
196/196 - 61s - loss: 36.2300 - MinusLogProbMetric: 36.2300 - val_loss: 36.7446 - val_MinusLogProbMetric: 36.7446 - lr: 4.1152e-06 - 61s/epoch - 309ms/step
Epoch 648/1000
2023-10-25 01:17:46.276 
Epoch 648/1000 
	 loss: 36.3179, MinusLogProbMetric: 36.3179, val_loss: 36.6892, val_MinusLogProbMetric: 36.6892

Epoch 648: val_loss improved from 36.69587 to 36.68919, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 58s - loss: 36.3179 - MinusLogProbMetric: 36.3179 - val_loss: 36.6892 - val_MinusLogProbMetric: 36.6892 - lr: 4.1152e-06 - 58s/epoch - 294ms/step
Epoch 649/1000
2023-10-25 01:18:54.300 
Epoch 649/1000 
	 loss: 36.2218, MinusLogProbMetric: 36.2218, val_loss: 36.7948, val_MinusLogProbMetric: 36.7948

Epoch 649: val_loss did not improve from 36.68919
196/196 - 67s - loss: 36.2218 - MinusLogProbMetric: 36.2218 - val_loss: 36.7948 - val_MinusLogProbMetric: 36.7948 - lr: 4.1152e-06 - 67s/epoch - 342ms/step
Epoch 650/1000
2023-10-25 01:20:01.255 
Epoch 650/1000 
	 loss: 36.2157, MinusLogProbMetric: 36.2157, val_loss: 36.8754, val_MinusLogProbMetric: 36.8754

Epoch 650: val_loss did not improve from 36.68919
196/196 - 67s - loss: 36.2157 - MinusLogProbMetric: 36.2157 - val_loss: 36.8754 - val_MinusLogProbMetric: 36.8754 - lr: 4.1152e-06 - 67s/epoch - 342ms/step
Epoch 651/1000
2023-10-25 01:21:07.510 
Epoch 651/1000 
	 loss: 36.2221, MinusLogProbMetric: 36.2221, val_loss: 36.8486, val_MinusLogProbMetric: 36.8486

Epoch 651: val_loss did not improve from 36.68919
196/196 - 66s - loss: 36.2221 - MinusLogProbMetric: 36.2221 - val_loss: 36.8486 - val_MinusLogProbMetric: 36.8486 - lr: 4.1152e-06 - 66s/epoch - 338ms/step
Epoch 652/1000
2023-10-25 01:22:15.797 
Epoch 652/1000 
	 loss: 36.2756, MinusLogProbMetric: 36.2756, val_loss: 36.7520, val_MinusLogProbMetric: 36.7520

Epoch 652: val_loss did not improve from 36.68919
196/196 - 68s - loss: 36.2756 - MinusLogProbMetric: 36.2756 - val_loss: 36.7520 - val_MinusLogProbMetric: 36.7520 - lr: 4.1152e-06 - 68s/epoch - 348ms/step
Epoch 653/1000
2023-10-25 01:23:23.465 
Epoch 653/1000 
	 loss: 36.2125, MinusLogProbMetric: 36.2125, val_loss: 36.9872, val_MinusLogProbMetric: 36.9872

Epoch 653: val_loss did not improve from 36.68919
196/196 - 68s - loss: 36.2125 - MinusLogProbMetric: 36.2125 - val_loss: 36.9872 - val_MinusLogProbMetric: 36.9872 - lr: 4.1152e-06 - 68s/epoch - 345ms/step
Epoch 654/1000
2023-10-25 01:24:28.274 
Epoch 654/1000 
	 loss: 36.2789, MinusLogProbMetric: 36.2789, val_loss: 36.8881, val_MinusLogProbMetric: 36.8881

Epoch 654: val_loss did not improve from 36.68919
196/196 - 65s - loss: 36.2789 - MinusLogProbMetric: 36.2789 - val_loss: 36.8881 - val_MinusLogProbMetric: 36.8881 - lr: 4.1152e-06 - 65s/epoch - 331ms/step
Epoch 655/1000
2023-10-25 01:25:34.032 
Epoch 655/1000 
	 loss: 36.1562, MinusLogProbMetric: 36.1562, val_loss: 37.3733, val_MinusLogProbMetric: 37.3733

Epoch 655: val_loss did not improve from 36.68919
196/196 - 66s - loss: 36.1562 - MinusLogProbMetric: 36.1562 - val_loss: 37.3733 - val_MinusLogProbMetric: 37.3733 - lr: 4.1152e-06 - 66s/epoch - 335ms/step
Epoch 656/1000
2023-10-25 01:26:41.989 
Epoch 656/1000 
	 loss: 36.2736, MinusLogProbMetric: 36.2736, val_loss: 36.8191, val_MinusLogProbMetric: 36.8191

Epoch 656: val_loss did not improve from 36.68919
196/196 - 68s - loss: 36.2736 - MinusLogProbMetric: 36.2736 - val_loss: 36.8191 - val_MinusLogProbMetric: 36.8191 - lr: 4.1152e-06 - 68s/epoch - 347ms/step
Epoch 657/1000
2023-10-25 01:27:47.268 
Epoch 657/1000 
	 loss: 36.2908, MinusLogProbMetric: 36.2908, val_loss: 36.7731, val_MinusLogProbMetric: 36.7731

Epoch 657: val_loss did not improve from 36.68919
196/196 - 65s - loss: 36.2908 - MinusLogProbMetric: 36.2908 - val_loss: 36.7731 - val_MinusLogProbMetric: 36.7731 - lr: 4.1152e-06 - 65s/epoch - 333ms/step
Epoch 658/1000
2023-10-25 01:28:55.906 
Epoch 658/1000 
	 loss: 36.1528, MinusLogProbMetric: 36.1528, val_loss: 36.8318, val_MinusLogProbMetric: 36.8318

Epoch 658: val_loss did not improve from 36.68919
196/196 - 69s - loss: 36.1528 - MinusLogProbMetric: 36.1528 - val_loss: 36.8318 - val_MinusLogProbMetric: 36.8318 - lr: 4.1152e-06 - 69s/epoch - 350ms/step
Epoch 659/1000
2023-10-25 01:30:00.839 
Epoch 659/1000 
	 loss: 36.1667, MinusLogProbMetric: 36.1667, val_loss: 36.7010, val_MinusLogProbMetric: 36.7010

Epoch 659: val_loss did not improve from 36.68919
196/196 - 65s - loss: 36.1667 - MinusLogProbMetric: 36.1667 - val_loss: 36.7010 - val_MinusLogProbMetric: 36.7010 - lr: 4.1152e-06 - 65s/epoch - 331ms/step
Epoch 660/1000
2023-10-25 01:31:10.107 
Epoch 660/1000 
	 loss: 36.2798, MinusLogProbMetric: 36.2798, val_loss: 36.5221, val_MinusLogProbMetric: 36.5221

Epoch 660: val_loss improved from 36.68919 to 36.52205, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 70s - loss: 36.2798 - MinusLogProbMetric: 36.2798 - val_loss: 36.5221 - val_MinusLogProbMetric: 36.5221 - lr: 4.1152e-06 - 70s/epoch - 359ms/step
Epoch 661/1000
2023-10-25 01:32:19.057 
Epoch 661/1000 
	 loss: 36.1143, MinusLogProbMetric: 36.1143, val_loss: 36.5132, val_MinusLogProbMetric: 36.5132

Epoch 661: val_loss improved from 36.52205 to 36.51324, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 69s - loss: 36.1143 - MinusLogProbMetric: 36.1143 - val_loss: 36.5132 - val_MinusLogProbMetric: 36.5132 - lr: 4.1152e-06 - 69s/epoch - 352ms/step
Epoch 662/1000
2023-10-25 01:33:26.225 
Epoch 662/1000 
	 loss: 36.2290, MinusLogProbMetric: 36.2290, val_loss: 36.6114, val_MinusLogProbMetric: 36.6114

Epoch 662: val_loss did not improve from 36.51324
196/196 - 66s - loss: 36.2290 - MinusLogProbMetric: 36.2290 - val_loss: 36.6114 - val_MinusLogProbMetric: 36.6114 - lr: 4.1152e-06 - 66s/epoch - 337ms/step
Epoch 663/1000
2023-10-25 01:34:31.464 
Epoch 663/1000 
	 loss: 36.3554, MinusLogProbMetric: 36.3554, val_loss: 36.8056, val_MinusLogProbMetric: 36.8056

Epoch 663: val_loss did not improve from 36.51324
196/196 - 65s - loss: 36.3554 - MinusLogProbMetric: 36.3554 - val_loss: 36.8056 - val_MinusLogProbMetric: 36.8056 - lr: 4.1152e-06 - 65s/epoch - 333ms/step
Epoch 664/1000
2023-10-25 01:35:37.466 
Epoch 664/1000 
	 loss: 36.0732, MinusLogProbMetric: 36.0732, val_loss: 36.9338, val_MinusLogProbMetric: 36.9338

Epoch 664: val_loss did not improve from 36.51324
196/196 - 66s - loss: 36.0732 - MinusLogProbMetric: 36.0732 - val_loss: 36.9338 - val_MinusLogProbMetric: 36.9338 - lr: 4.1152e-06 - 66s/epoch - 337ms/step
Epoch 665/1000
2023-10-25 01:36:46.317 
Epoch 665/1000 
	 loss: 36.0870, MinusLogProbMetric: 36.0870, val_loss: 36.8169, val_MinusLogProbMetric: 36.8169

Epoch 665: val_loss did not improve from 36.51324
196/196 - 69s - loss: 36.0870 - MinusLogProbMetric: 36.0870 - val_loss: 36.8169 - val_MinusLogProbMetric: 36.8169 - lr: 4.1152e-06 - 69s/epoch - 351ms/step
Epoch 666/1000
2023-10-25 01:37:54.934 
Epoch 666/1000 
	 loss: 36.0420, MinusLogProbMetric: 36.0420, val_loss: 36.6803, val_MinusLogProbMetric: 36.6803

Epoch 666: val_loss did not improve from 36.51324
196/196 - 69s - loss: 36.0420 - MinusLogProbMetric: 36.0420 - val_loss: 36.6803 - val_MinusLogProbMetric: 36.6803 - lr: 4.1152e-06 - 69s/epoch - 350ms/step
Epoch 667/1000
2023-10-25 01:39:00.534 
Epoch 667/1000 
	 loss: 36.2880, MinusLogProbMetric: 36.2880, val_loss: 36.5363, val_MinusLogProbMetric: 36.5363

Epoch 667: val_loss did not improve from 36.51324
196/196 - 66s - loss: 36.2880 - MinusLogProbMetric: 36.2880 - val_loss: 36.5363 - val_MinusLogProbMetric: 36.5363 - lr: 4.1152e-06 - 66s/epoch - 335ms/step
Epoch 668/1000
2023-10-25 01:40:07.877 
Epoch 668/1000 
	 loss: 36.0999, MinusLogProbMetric: 36.0999, val_loss: 36.7128, val_MinusLogProbMetric: 36.7128

Epoch 668: val_loss did not improve from 36.51324
196/196 - 67s - loss: 36.0999 - MinusLogProbMetric: 36.0999 - val_loss: 36.7128 - val_MinusLogProbMetric: 36.7128 - lr: 4.1152e-06 - 67s/epoch - 344ms/step
Epoch 669/1000
2023-10-25 01:41:15.875 
Epoch 669/1000 
	 loss: 36.0372, MinusLogProbMetric: 36.0372, val_loss: 36.5034, val_MinusLogProbMetric: 36.5034

Epoch 669: val_loss improved from 36.51324 to 36.50340, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 69s - loss: 36.0372 - MinusLogProbMetric: 36.0372 - val_loss: 36.5034 - val_MinusLogProbMetric: 36.5034 - lr: 4.1152e-06 - 69s/epoch - 351ms/step
Epoch 670/1000
2023-10-25 01:42:22.549 
Epoch 670/1000 
	 loss: 36.3496, MinusLogProbMetric: 36.3496, val_loss: 36.5488, val_MinusLogProbMetric: 36.5488

Epoch 670: val_loss did not improve from 36.50340
196/196 - 66s - loss: 36.3496 - MinusLogProbMetric: 36.3496 - val_loss: 36.5488 - val_MinusLogProbMetric: 36.5488 - lr: 4.1152e-06 - 66s/epoch - 336ms/step
Epoch 671/1000
2023-10-25 01:43:31.649 
Epoch 671/1000 
	 loss: 36.0386, MinusLogProbMetric: 36.0386, val_loss: 36.7522, val_MinusLogProbMetric: 36.7522

Epoch 671: val_loss did not improve from 36.50340
196/196 - 69s - loss: 36.0386 - MinusLogProbMetric: 36.0386 - val_loss: 36.7522 - val_MinusLogProbMetric: 36.7522 - lr: 4.1152e-06 - 69s/epoch - 353ms/step
Epoch 672/1000
2023-10-25 01:44:38.603 
Epoch 672/1000 
	 loss: 36.0243, MinusLogProbMetric: 36.0243, val_loss: 36.6234, val_MinusLogProbMetric: 36.6234

Epoch 672: val_loss did not improve from 36.50340
196/196 - 67s - loss: 36.0243 - MinusLogProbMetric: 36.0243 - val_loss: 36.6234 - val_MinusLogProbMetric: 36.6234 - lr: 4.1152e-06 - 67s/epoch - 342ms/step
Epoch 673/1000
2023-10-25 01:45:44.896 
Epoch 673/1000 
	 loss: 35.9806, MinusLogProbMetric: 35.9806, val_loss: 36.5671, val_MinusLogProbMetric: 36.5671

Epoch 673: val_loss did not improve from 36.50340
196/196 - 66s - loss: 35.9806 - MinusLogProbMetric: 35.9806 - val_loss: 36.5671 - val_MinusLogProbMetric: 36.5671 - lr: 4.1152e-06 - 66s/epoch - 338ms/step
Epoch 674/1000
2023-10-25 01:46:52.117 
Epoch 674/1000 
	 loss: 36.7570, MinusLogProbMetric: 36.7570, val_loss: 36.5242, val_MinusLogProbMetric: 36.5242

Epoch 674: val_loss did not improve from 36.50340
196/196 - 67s - loss: 36.7570 - MinusLogProbMetric: 36.7570 - val_loss: 36.5242 - val_MinusLogProbMetric: 36.5242 - lr: 4.1152e-06 - 67s/epoch - 343ms/step
Epoch 675/1000
2023-10-25 01:48:00.460 
Epoch 675/1000 
	 loss: 35.9827, MinusLogProbMetric: 35.9827, val_loss: 36.5695, val_MinusLogProbMetric: 36.5695

Epoch 675: val_loss did not improve from 36.50340
196/196 - 68s - loss: 35.9827 - MinusLogProbMetric: 35.9827 - val_loss: 36.5695 - val_MinusLogProbMetric: 36.5695 - lr: 4.1152e-06 - 68s/epoch - 349ms/step
Epoch 676/1000
2023-10-25 01:49:06.240 
Epoch 676/1000 
	 loss: 35.9822, MinusLogProbMetric: 35.9822, val_loss: 36.5730, val_MinusLogProbMetric: 36.5730

Epoch 676: val_loss did not improve from 36.50340
196/196 - 66s - loss: 35.9822 - MinusLogProbMetric: 35.9822 - val_loss: 36.5730 - val_MinusLogProbMetric: 36.5730 - lr: 4.1152e-06 - 66s/epoch - 336ms/step
Epoch 677/1000
2023-10-25 01:50:14.195 
Epoch 677/1000 
	 loss: 35.9589, MinusLogProbMetric: 35.9589, val_loss: 36.6670, val_MinusLogProbMetric: 36.6670

Epoch 677: val_loss did not improve from 36.50340
196/196 - 68s - loss: 35.9589 - MinusLogProbMetric: 35.9589 - val_loss: 36.6670 - val_MinusLogProbMetric: 36.6670 - lr: 4.1152e-06 - 68s/epoch - 347ms/step
Epoch 678/1000
2023-10-25 01:51:20.460 
Epoch 678/1000 
	 loss: 35.9423, MinusLogProbMetric: 35.9423, val_loss: 36.5992, val_MinusLogProbMetric: 36.5992

Epoch 678: val_loss did not improve from 36.50340
196/196 - 66s - loss: 35.9423 - MinusLogProbMetric: 35.9423 - val_loss: 36.5992 - val_MinusLogProbMetric: 36.5992 - lr: 4.1152e-06 - 66s/epoch - 338ms/step
Epoch 679/1000
2023-10-25 01:52:28.937 
Epoch 679/1000 
	 loss: 35.9188, MinusLogProbMetric: 35.9188, val_loss: 36.5173, val_MinusLogProbMetric: 36.5173

Epoch 679: val_loss did not improve from 36.50340
196/196 - 68s - loss: 35.9188 - MinusLogProbMetric: 35.9188 - val_loss: 36.5173 - val_MinusLogProbMetric: 36.5173 - lr: 4.1152e-06 - 68s/epoch - 349ms/step
Epoch 680/1000
2023-10-25 01:53:38.383 
Epoch 680/1000 
	 loss: 36.3654, MinusLogProbMetric: 36.3654, val_loss: 36.4003, val_MinusLogProbMetric: 36.4003

Epoch 680: val_loss improved from 36.50340 to 36.40032, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 70s - loss: 36.3654 - MinusLogProbMetric: 36.3654 - val_loss: 36.4003 - val_MinusLogProbMetric: 36.4003 - lr: 4.1152e-06 - 70s/epoch - 359ms/step
Epoch 681/1000
2023-10-25 01:54:44.083 
Epoch 681/1000 
	 loss: 35.8955, MinusLogProbMetric: 35.8955, val_loss: 36.6736, val_MinusLogProbMetric: 36.6736

Epoch 681: val_loss did not improve from 36.40032
196/196 - 65s - loss: 35.8955 - MinusLogProbMetric: 35.8955 - val_loss: 36.6736 - val_MinusLogProbMetric: 36.6736 - lr: 4.1152e-06 - 65s/epoch - 330ms/step
Epoch 682/1000
2023-10-25 01:55:51.616 
Epoch 682/1000 
	 loss: 35.9717, MinusLogProbMetric: 35.9717, val_loss: 36.6157, val_MinusLogProbMetric: 36.6157

Epoch 682: val_loss did not improve from 36.40032
196/196 - 68s - loss: 35.9717 - MinusLogProbMetric: 35.9717 - val_loss: 36.6157 - val_MinusLogProbMetric: 36.6157 - lr: 4.1152e-06 - 68s/epoch - 345ms/step
Epoch 683/1000
2023-10-25 01:56:58.079 
Epoch 683/1000 
	 loss: 35.9376, MinusLogProbMetric: 35.9376, val_loss: 36.4011, val_MinusLogProbMetric: 36.4011

Epoch 683: val_loss did not improve from 36.40032
196/196 - 66s - loss: 35.9376 - MinusLogProbMetric: 35.9376 - val_loss: 36.4011 - val_MinusLogProbMetric: 36.4011 - lr: 4.1152e-06 - 66s/epoch - 339ms/step
Epoch 684/1000
2023-10-25 01:58:05.805 
Epoch 684/1000 
	 loss: 35.8928, MinusLogProbMetric: 35.8928, val_loss: 36.4523, val_MinusLogProbMetric: 36.4523

Epoch 684: val_loss did not improve from 36.40032
196/196 - 68s - loss: 35.8928 - MinusLogProbMetric: 35.8928 - val_loss: 36.4523 - val_MinusLogProbMetric: 36.4523 - lr: 4.1152e-06 - 68s/epoch - 346ms/step
Epoch 685/1000
2023-10-25 01:59:12.765 
Epoch 685/1000 
	 loss: 36.0238, MinusLogProbMetric: 36.0238, val_loss: 36.4011, val_MinusLogProbMetric: 36.4011

Epoch 685: val_loss did not improve from 36.40032
196/196 - 67s - loss: 36.0238 - MinusLogProbMetric: 36.0238 - val_loss: 36.4011 - val_MinusLogProbMetric: 36.4011 - lr: 4.1152e-06 - 67s/epoch - 342ms/step
Epoch 686/1000
2023-10-25 02:00:18.204 
Epoch 686/1000 
	 loss: 35.8670, MinusLogProbMetric: 35.8670, val_loss: 36.8232, val_MinusLogProbMetric: 36.8232

Epoch 686: val_loss did not improve from 36.40032
196/196 - 65s - loss: 35.8670 - MinusLogProbMetric: 35.8670 - val_loss: 36.8232 - val_MinusLogProbMetric: 36.8232 - lr: 4.1152e-06 - 65s/epoch - 334ms/step
Epoch 687/1000
2023-10-25 02:01:23.393 
Epoch 687/1000 
	 loss: 35.8800, MinusLogProbMetric: 35.8800, val_loss: 36.4253, val_MinusLogProbMetric: 36.4253

Epoch 687: val_loss did not improve from 36.40032
196/196 - 65s - loss: 35.8800 - MinusLogProbMetric: 35.8800 - val_loss: 36.4253 - val_MinusLogProbMetric: 36.4253 - lr: 4.1152e-06 - 65s/epoch - 333ms/step
Epoch 688/1000
2023-10-25 02:02:29.737 
Epoch 688/1000 
	 loss: 35.8826, MinusLogProbMetric: 35.8826, val_loss: 36.3489, val_MinusLogProbMetric: 36.3489

Epoch 688: val_loss improved from 36.40032 to 36.34887, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 67s - loss: 35.8826 - MinusLogProbMetric: 35.8826 - val_loss: 36.3489 - val_MinusLogProbMetric: 36.3489 - lr: 4.1152e-06 - 67s/epoch - 344ms/step
Epoch 689/1000
2023-10-25 02:03:38.738 
Epoch 689/1000 
	 loss: 35.8811, MinusLogProbMetric: 35.8811, val_loss: 36.4831, val_MinusLogProbMetric: 36.4831

Epoch 689: val_loss did not improve from 36.34887
196/196 - 68s - loss: 35.8811 - MinusLogProbMetric: 35.8811 - val_loss: 36.4831 - val_MinusLogProbMetric: 36.4831 - lr: 4.1152e-06 - 68s/epoch - 347ms/step
Epoch 690/1000
2023-10-25 02:04:43.493 
Epoch 690/1000 
	 loss: 35.8518, MinusLogProbMetric: 35.8518, val_loss: 36.3633, val_MinusLogProbMetric: 36.3633

Epoch 690: val_loss did not improve from 36.34887
196/196 - 65s - loss: 35.8518 - MinusLogProbMetric: 35.8518 - val_loss: 36.3633 - val_MinusLogProbMetric: 36.3633 - lr: 4.1152e-06 - 65s/epoch - 330ms/step
Epoch 691/1000
2023-10-25 02:05:49.462 
Epoch 691/1000 
	 loss: 35.9361, MinusLogProbMetric: 35.9361, val_loss: 36.5480, val_MinusLogProbMetric: 36.5480

Epoch 691: val_loss did not improve from 36.34887
196/196 - 66s - loss: 35.9361 - MinusLogProbMetric: 35.9361 - val_loss: 36.5480 - val_MinusLogProbMetric: 36.5480 - lr: 4.1152e-06 - 66s/epoch - 337ms/step
Epoch 692/1000
2023-10-25 02:06:55.269 
Epoch 692/1000 
	 loss: 35.8245, MinusLogProbMetric: 35.8245, val_loss: 36.3549, val_MinusLogProbMetric: 36.3549

Epoch 692: val_loss did not improve from 36.34887
196/196 - 66s - loss: 35.8245 - MinusLogProbMetric: 35.8245 - val_loss: 36.3549 - val_MinusLogProbMetric: 36.3549 - lr: 4.1152e-06 - 66s/epoch - 336ms/step
Epoch 693/1000
2023-10-25 02:08:03.269 
Epoch 693/1000 
	 loss: 35.8552, MinusLogProbMetric: 35.8552, val_loss: 36.5046, val_MinusLogProbMetric: 36.5046

Epoch 693: val_loss did not improve from 36.34887
196/196 - 68s - loss: 35.8552 - MinusLogProbMetric: 35.8552 - val_loss: 36.5046 - val_MinusLogProbMetric: 36.5046 - lr: 4.1152e-06 - 68s/epoch - 347ms/step
Epoch 694/1000
2023-10-25 02:09:09.506 
Epoch 694/1000 
	 loss: 36.5279, MinusLogProbMetric: 36.5279, val_loss: 36.2451, val_MinusLogProbMetric: 36.2451

Epoch 694: val_loss improved from 36.34887 to 36.24506, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 67s - loss: 36.5279 - MinusLogProbMetric: 36.5279 - val_loss: 36.2451 - val_MinusLogProbMetric: 36.2451 - lr: 4.1152e-06 - 67s/epoch - 343ms/step
Epoch 695/1000
2023-10-25 02:10:15.602 
Epoch 695/1000 
	 loss: 35.8249, MinusLogProbMetric: 35.8249, val_loss: 36.3096, val_MinusLogProbMetric: 36.3096

Epoch 695: val_loss did not improve from 36.24506
196/196 - 65s - loss: 35.8249 - MinusLogProbMetric: 35.8249 - val_loss: 36.3096 - val_MinusLogProbMetric: 36.3096 - lr: 4.1152e-06 - 65s/epoch - 332ms/step
Epoch 696/1000
2023-10-25 02:11:22.563 
Epoch 696/1000 
	 loss: 35.8070, MinusLogProbMetric: 35.8070, val_loss: 36.4275, val_MinusLogProbMetric: 36.4275

Epoch 696: val_loss did not improve from 36.24506
196/196 - 67s - loss: 35.8070 - MinusLogProbMetric: 35.8070 - val_loss: 36.4275 - val_MinusLogProbMetric: 36.4275 - lr: 4.1152e-06 - 67s/epoch - 342ms/step
Epoch 697/1000
2023-10-25 02:12:30.530 
Epoch 697/1000 
	 loss: 35.7846, MinusLogProbMetric: 35.7846, val_loss: 36.1882, val_MinusLogProbMetric: 36.1882

Epoch 697: val_loss improved from 36.24506 to 36.18821, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 69s - loss: 35.7846 - MinusLogProbMetric: 35.7846 - val_loss: 36.1882 - val_MinusLogProbMetric: 36.1882 - lr: 4.1152e-06 - 69s/epoch - 351ms/step
Epoch 698/1000
2023-10-25 02:13:37.353 
Epoch 698/1000 
	 loss: 35.8077, MinusLogProbMetric: 35.8077, val_loss: 36.3291, val_MinusLogProbMetric: 36.3291

Epoch 698: val_loss did not improve from 36.18821
196/196 - 66s - loss: 35.8077 - MinusLogProbMetric: 35.8077 - val_loss: 36.3291 - val_MinusLogProbMetric: 36.3291 - lr: 4.1152e-06 - 66s/epoch - 336ms/step
Epoch 699/1000
2023-10-25 02:14:45.191 
Epoch 699/1000 
	 loss: 35.7553, MinusLogProbMetric: 35.7553, val_loss: 36.3036, val_MinusLogProbMetric: 36.3036

Epoch 699: val_loss did not improve from 36.18821
196/196 - 68s - loss: 35.7553 - MinusLogProbMetric: 35.7553 - val_loss: 36.3036 - val_MinusLogProbMetric: 36.3036 - lr: 4.1152e-06 - 68s/epoch - 346ms/step
Epoch 700/1000
2023-10-25 02:15:51.494 
Epoch 700/1000 
	 loss: 35.8919, MinusLogProbMetric: 35.8919, val_loss: 36.2669, val_MinusLogProbMetric: 36.2669

Epoch 700: val_loss did not improve from 36.18821
196/196 - 66s - loss: 35.8919 - MinusLogProbMetric: 35.8919 - val_loss: 36.2669 - val_MinusLogProbMetric: 36.2669 - lr: 4.1152e-06 - 66s/epoch - 338ms/step
Epoch 701/1000
2023-10-25 02:16:58.304 
Epoch 701/1000 
	 loss: 35.7738, MinusLogProbMetric: 35.7738, val_loss: 36.5476, val_MinusLogProbMetric: 36.5476

Epoch 701: val_loss did not improve from 36.18821
196/196 - 67s - loss: 35.7738 - MinusLogProbMetric: 35.7738 - val_loss: 36.5476 - val_MinusLogProbMetric: 36.5476 - lr: 4.1152e-06 - 67s/epoch - 341ms/step
Epoch 702/1000
2023-10-25 02:18:06.117 
Epoch 702/1000 
	 loss: 35.7665, MinusLogProbMetric: 35.7665, val_loss: 36.1466, val_MinusLogProbMetric: 36.1466

Epoch 702: val_loss improved from 36.18821 to 36.14656, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 69s - loss: 35.7665 - MinusLogProbMetric: 35.7665 - val_loss: 36.1466 - val_MinusLogProbMetric: 36.1466 - lr: 4.1152e-06 - 69s/epoch - 352ms/step
Epoch 703/1000
2023-10-25 02:19:15.049 
Epoch 703/1000 
	 loss: 35.7505, MinusLogProbMetric: 35.7505, val_loss: 36.6944, val_MinusLogProbMetric: 36.6944

Epoch 703: val_loss did not improve from 36.14656
196/196 - 68s - loss: 35.7505 - MinusLogProbMetric: 35.7505 - val_loss: 36.6944 - val_MinusLogProbMetric: 36.6944 - lr: 4.1152e-06 - 68s/epoch - 346ms/step
Epoch 704/1000
2023-10-25 02:20:21.181 
Epoch 704/1000 
	 loss: 35.7889, MinusLogProbMetric: 35.7889, val_loss: 36.2491, val_MinusLogProbMetric: 36.2491

Epoch 704: val_loss did not improve from 36.14656
196/196 - 66s - loss: 35.7889 - MinusLogProbMetric: 35.7889 - val_loss: 36.2491 - val_MinusLogProbMetric: 36.2491 - lr: 4.1152e-06 - 66s/epoch - 337ms/step
Epoch 705/1000
2023-10-25 02:21:28.164 
Epoch 705/1000 
	 loss: 35.7329, MinusLogProbMetric: 35.7329, val_loss: 36.2000, val_MinusLogProbMetric: 36.2000

Epoch 705: val_loss did not improve from 36.14656
196/196 - 67s - loss: 35.7329 - MinusLogProbMetric: 35.7329 - val_loss: 36.2000 - val_MinusLogProbMetric: 36.2000 - lr: 4.1152e-06 - 67s/epoch - 342ms/step
Epoch 706/1000
2023-10-25 02:22:36.188 
Epoch 706/1000 
	 loss: 35.7346, MinusLogProbMetric: 35.7346, val_loss: 36.1711, val_MinusLogProbMetric: 36.1711

Epoch 706: val_loss did not improve from 36.14656
196/196 - 68s - loss: 35.7346 - MinusLogProbMetric: 35.7346 - val_loss: 36.1711 - val_MinusLogProbMetric: 36.1711 - lr: 4.1152e-06 - 68s/epoch - 347ms/step
Epoch 707/1000
2023-10-25 02:23:41.950 
Epoch 707/1000 
	 loss: 35.6892, MinusLogProbMetric: 35.6892, val_loss: 36.0944, val_MinusLogProbMetric: 36.0944

Epoch 707: val_loss improved from 36.14656 to 36.09439, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 67s - loss: 35.6892 - MinusLogProbMetric: 35.6892 - val_loss: 36.0944 - val_MinusLogProbMetric: 36.0944 - lr: 4.1152e-06 - 67s/epoch - 341ms/step
Epoch 708/1000
2023-10-25 02:24:48.275 
Epoch 708/1000 
	 loss: 35.6971, MinusLogProbMetric: 35.6971, val_loss: 36.2178, val_MinusLogProbMetric: 36.2178

Epoch 708: val_loss did not improve from 36.09439
196/196 - 65s - loss: 35.6971 - MinusLogProbMetric: 35.6971 - val_loss: 36.2178 - val_MinusLogProbMetric: 36.2178 - lr: 4.1152e-06 - 65s/epoch - 333ms/step
Epoch 709/1000
2023-10-25 02:25:53.947 
Epoch 709/1000 
	 loss: 35.9973, MinusLogProbMetric: 35.9973, val_loss: 36.2345, val_MinusLogProbMetric: 36.2345

Epoch 709: val_loss did not improve from 36.09439
196/196 - 66s - loss: 35.9973 - MinusLogProbMetric: 35.9973 - val_loss: 36.2345 - val_MinusLogProbMetric: 36.2345 - lr: 4.1152e-06 - 66s/epoch - 335ms/step
Epoch 710/1000
2023-10-25 02:27:00.287 
Epoch 710/1000 
	 loss: 35.6705, MinusLogProbMetric: 35.6705, val_loss: 36.2062, val_MinusLogProbMetric: 36.2062

Epoch 710: val_loss did not improve from 36.09439
196/196 - 66s - loss: 35.6705 - MinusLogProbMetric: 35.6705 - val_loss: 36.2062 - val_MinusLogProbMetric: 36.2062 - lr: 4.1152e-06 - 66s/epoch - 338ms/step
Epoch 711/1000
2023-10-25 02:28:05.384 
Epoch 711/1000 
	 loss: 35.6402, MinusLogProbMetric: 35.6402, val_loss: 36.1167, val_MinusLogProbMetric: 36.1167

Epoch 711: val_loss did not improve from 36.09439
196/196 - 65s - loss: 35.6402 - MinusLogProbMetric: 35.6402 - val_loss: 36.1167 - val_MinusLogProbMetric: 36.1167 - lr: 4.1152e-06 - 65s/epoch - 332ms/step
Epoch 712/1000
2023-10-25 02:29:12.672 
Epoch 712/1000 
	 loss: 35.6597, MinusLogProbMetric: 35.6597, val_loss: 36.1543, val_MinusLogProbMetric: 36.1543

Epoch 712: val_loss did not improve from 36.09439
196/196 - 67s - loss: 35.6597 - MinusLogProbMetric: 35.6597 - val_loss: 36.1543 - val_MinusLogProbMetric: 36.1543 - lr: 4.1152e-06 - 67s/epoch - 343ms/step
Epoch 713/1000
2023-10-25 02:30:20.005 
Epoch 713/1000 
	 loss: 35.7441, MinusLogProbMetric: 35.7441, val_loss: 36.1925, val_MinusLogProbMetric: 36.1925

Epoch 713: val_loss did not improve from 36.09439
196/196 - 67s - loss: 35.7441 - MinusLogProbMetric: 35.7441 - val_loss: 36.1925 - val_MinusLogProbMetric: 36.1925 - lr: 4.1152e-06 - 67s/epoch - 344ms/step
Epoch 714/1000
2023-10-25 02:31:26.294 
Epoch 714/1000 
	 loss: 35.6449, MinusLogProbMetric: 35.6449, val_loss: 36.0844, val_MinusLogProbMetric: 36.0844

Epoch 714: val_loss improved from 36.09439 to 36.08445, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 67s - loss: 35.6449 - MinusLogProbMetric: 35.6449 - val_loss: 36.0844 - val_MinusLogProbMetric: 36.0844 - lr: 4.1152e-06 - 67s/epoch - 344ms/step
Epoch 715/1000
2023-10-25 02:32:33.993 
Epoch 715/1000 
	 loss: 35.6821, MinusLogProbMetric: 35.6821, val_loss: 36.4777, val_MinusLogProbMetric: 36.4777

Epoch 715: val_loss did not improve from 36.08445
196/196 - 67s - loss: 35.6821 - MinusLogProbMetric: 35.6821 - val_loss: 36.4777 - val_MinusLogProbMetric: 36.4777 - lr: 4.1152e-06 - 67s/epoch - 340ms/step
Epoch 716/1000
2023-10-25 02:33:40.487 
Epoch 716/1000 
	 loss: 35.6279, MinusLogProbMetric: 35.6279, val_loss: 36.1332, val_MinusLogProbMetric: 36.1332

Epoch 716: val_loss did not improve from 36.08445
196/196 - 66s - loss: 35.6279 - MinusLogProbMetric: 35.6279 - val_loss: 36.1332 - val_MinusLogProbMetric: 36.1332 - lr: 4.1152e-06 - 66s/epoch - 339ms/step
Epoch 717/1000
2023-10-25 02:34:47.954 
Epoch 717/1000 
	 loss: 35.5813, MinusLogProbMetric: 35.5813, val_loss: 36.3170, val_MinusLogProbMetric: 36.3170

Epoch 717: val_loss did not improve from 36.08445
196/196 - 67s - loss: 35.5813 - MinusLogProbMetric: 35.5813 - val_loss: 36.3170 - val_MinusLogProbMetric: 36.3170 - lr: 4.1152e-06 - 67s/epoch - 344ms/step
Epoch 718/1000
2023-10-25 02:35:56.052 
Epoch 718/1000 
	 loss: 35.6874, MinusLogProbMetric: 35.6874, val_loss: 36.8090, val_MinusLogProbMetric: 36.8090

Epoch 718: val_loss did not improve from 36.08445
196/196 - 68s - loss: 35.6874 - MinusLogProbMetric: 35.6874 - val_loss: 36.8090 - val_MinusLogProbMetric: 36.8090 - lr: 4.1152e-06 - 68s/epoch - 347ms/step
Epoch 719/1000
2023-10-25 02:37:04.114 
Epoch 719/1000 
	 loss: 35.6446, MinusLogProbMetric: 35.6446, val_loss: 36.3873, val_MinusLogProbMetric: 36.3873

Epoch 719: val_loss did not improve from 36.08445
196/196 - 68s - loss: 35.6446 - MinusLogProbMetric: 35.6446 - val_loss: 36.3873 - val_MinusLogProbMetric: 36.3873 - lr: 4.1152e-06 - 68s/epoch - 347ms/step
Epoch 720/1000
2023-10-25 02:38:11.141 
Epoch 720/1000 
	 loss: 35.6202, MinusLogProbMetric: 35.6202, val_loss: 36.1215, val_MinusLogProbMetric: 36.1215

Epoch 720: val_loss did not improve from 36.08445
196/196 - 67s - loss: 35.6202 - MinusLogProbMetric: 35.6202 - val_loss: 36.1215 - val_MinusLogProbMetric: 36.1215 - lr: 4.1152e-06 - 67s/epoch - 342ms/step
Epoch 721/1000
2023-10-25 02:39:18.698 
Epoch 721/1000 
	 loss: 35.5914, MinusLogProbMetric: 35.5914, val_loss: 36.0676, val_MinusLogProbMetric: 36.0676

Epoch 721: val_loss improved from 36.08445 to 36.06763, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 68s - loss: 35.5914 - MinusLogProbMetric: 35.5914 - val_loss: 36.0676 - val_MinusLogProbMetric: 36.0676 - lr: 4.1152e-06 - 68s/epoch - 349ms/step
Epoch 722/1000
2023-10-25 02:40:24.717 
Epoch 722/1000 
	 loss: 35.8243, MinusLogProbMetric: 35.8243, val_loss: 35.9451, val_MinusLogProbMetric: 35.9451

Epoch 722: val_loss improved from 36.06763 to 35.94512, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 66s - loss: 35.8243 - MinusLogProbMetric: 35.8243 - val_loss: 35.9451 - val_MinusLogProbMetric: 35.9451 - lr: 4.1152e-06 - 66s/epoch - 337ms/step
Epoch 723/1000
2023-10-25 02:41:33.467 
Epoch 723/1000 
	 loss: 35.5788, MinusLogProbMetric: 35.5788, val_loss: 36.1802, val_MinusLogProbMetric: 36.1802

Epoch 723: val_loss did not improve from 35.94512
196/196 - 68s - loss: 35.5788 - MinusLogProbMetric: 35.5788 - val_loss: 36.1802 - val_MinusLogProbMetric: 36.1802 - lr: 4.1152e-06 - 68s/epoch - 346ms/step
Epoch 724/1000
2023-10-25 02:42:41.614 
Epoch 724/1000 
	 loss: 35.5371, MinusLogProbMetric: 35.5371, val_loss: 36.5300, val_MinusLogProbMetric: 36.5300

Epoch 724: val_loss did not improve from 35.94512
196/196 - 68s - loss: 35.5371 - MinusLogProbMetric: 35.5371 - val_loss: 36.5300 - val_MinusLogProbMetric: 36.5300 - lr: 4.1152e-06 - 68s/epoch - 348ms/step
Epoch 725/1000
2023-10-25 02:43:49.720 
Epoch 725/1000 
	 loss: 35.5792, MinusLogProbMetric: 35.5792, val_loss: 36.1130, val_MinusLogProbMetric: 36.1130

Epoch 725: val_loss did not improve from 35.94512
196/196 - 68s - loss: 35.5792 - MinusLogProbMetric: 35.5792 - val_loss: 36.1130 - val_MinusLogProbMetric: 36.1130 - lr: 4.1152e-06 - 68s/epoch - 347ms/step
Epoch 726/1000
2023-10-25 02:44:56.378 
Epoch 726/1000 
	 loss: 35.5332, MinusLogProbMetric: 35.5332, val_loss: 36.0806, val_MinusLogProbMetric: 36.0806

Epoch 726: val_loss did not improve from 35.94512
196/196 - 67s - loss: 35.5332 - MinusLogProbMetric: 35.5332 - val_loss: 36.0806 - val_MinusLogProbMetric: 36.0806 - lr: 4.1152e-06 - 67s/epoch - 340ms/step
Epoch 727/1000
2023-10-25 02:46:04.290 
Epoch 727/1000 
	 loss: 35.5338, MinusLogProbMetric: 35.5338, val_loss: 36.3422, val_MinusLogProbMetric: 36.3422

Epoch 727: val_loss did not improve from 35.94512
196/196 - 68s - loss: 35.5338 - MinusLogProbMetric: 35.5338 - val_loss: 36.3422 - val_MinusLogProbMetric: 36.3422 - lr: 4.1152e-06 - 68s/epoch - 346ms/step
Epoch 728/1000
2023-10-25 02:47:12.145 
Epoch 728/1000 
	 loss: 35.5838, MinusLogProbMetric: 35.5838, val_loss: 36.0971, val_MinusLogProbMetric: 36.0971

Epoch 728: val_loss did not improve from 35.94512
196/196 - 68s - loss: 35.5838 - MinusLogProbMetric: 35.5838 - val_loss: 36.0971 - val_MinusLogProbMetric: 36.0971 - lr: 4.1152e-06 - 68s/epoch - 346ms/step
Epoch 729/1000
2023-10-25 02:48:20.546 
Epoch 729/1000 
	 loss: 35.4798, MinusLogProbMetric: 35.4798, val_loss: 36.0602, val_MinusLogProbMetric: 36.0602

Epoch 729: val_loss did not improve from 35.94512
196/196 - 68s - loss: 35.4798 - MinusLogProbMetric: 35.4798 - val_loss: 36.0602 - val_MinusLogProbMetric: 36.0602 - lr: 4.1152e-06 - 68s/epoch - 349ms/step
Epoch 730/1000
2023-10-25 02:49:28.446 
Epoch 730/1000 
	 loss: 35.5132, MinusLogProbMetric: 35.5132, val_loss: 36.0800, val_MinusLogProbMetric: 36.0800

Epoch 730: val_loss did not improve from 35.94512
196/196 - 68s - loss: 35.5132 - MinusLogProbMetric: 35.5132 - val_loss: 36.0800 - val_MinusLogProbMetric: 36.0800 - lr: 4.1152e-06 - 68s/epoch - 346ms/step
Epoch 731/1000
2023-10-25 02:50:38.074 
Epoch 731/1000 
	 loss: 35.5022, MinusLogProbMetric: 35.5022, val_loss: 36.1021, val_MinusLogProbMetric: 36.1021

Epoch 731: val_loss did not improve from 35.94512
196/196 - 70s - loss: 35.5022 - MinusLogProbMetric: 35.5022 - val_loss: 36.1021 - val_MinusLogProbMetric: 36.1021 - lr: 4.1152e-06 - 70s/epoch - 355ms/step
Epoch 732/1000
2023-10-25 02:51:42.998 
Epoch 732/1000 
	 loss: 35.5695, MinusLogProbMetric: 35.5695, val_loss: 36.1129, val_MinusLogProbMetric: 36.1129

Epoch 732: val_loss did not improve from 35.94512
196/196 - 65s - loss: 35.5695 - MinusLogProbMetric: 35.5695 - val_loss: 36.1129 - val_MinusLogProbMetric: 36.1129 - lr: 4.1152e-06 - 65s/epoch - 331ms/step
Epoch 733/1000
2023-10-25 02:52:49.980 
Epoch 733/1000 
	 loss: 35.4937, MinusLogProbMetric: 35.4937, val_loss: 36.0290, val_MinusLogProbMetric: 36.0290

Epoch 733: val_loss did not improve from 35.94512
196/196 - 67s - loss: 35.4937 - MinusLogProbMetric: 35.4937 - val_loss: 36.0290 - val_MinusLogProbMetric: 36.0290 - lr: 4.1152e-06 - 67s/epoch - 342ms/step
Epoch 734/1000
2023-10-25 02:53:57.147 
Epoch 734/1000 
	 loss: 35.4844, MinusLogProbMetric: 35.4844, val_loss: 35.9381, val_MinusLogProbMetric: 35.9381

Epoch 734: val_loss improved from 35.94512 to 35.93815, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 68s - loss: 35.4844 - MinusLogProbMetric: 35.4844 - val_loss: 35.9381 - val_MinusLogProbMetric: 35.9381 - lr: 4.1152e-06 - 68s/epoch - 349ms/step
Epoch 735/1000
2023-10-25 02:55:03.673 
Epoch 735/1000 
	 loss: 35.4660, MinusLogProbMetric: 35.4660, val_loss: 36.2743, val_MinusLogProbMetric: 36.2743

Epoch 735: val_loss did not improve from 35.93815
196/196 - 65s - loss: 35.4660 - MinusLogProbMetric: 35.4660 - val_loss: 36.2743 - val_MinusLogProbMetric: 36.2743 - lr: 4.1152e-06 - 65s/epoch - 333ms/step
Epoch 736/1000
2023-10-25 02:56:08.656 
Epoch 736/1000 
	 loss: 35.4379, MinusLogProbMetric: 35.4379, val_loss: 35.9713, val_MinusLogProbMetric: 35.9713

Epoch 736: val_loss did not improve from 35.93815
196/196 - 65s - loss: 35.4379 - MinusLogProbMetric: 35.4379 - val_loss: 35.9713 - val_MinusLogProbMetric: 35.9713 - lr: 4.1152e-06 - 65s/epoch - 332ms/step
Epoch 737/1000
2023-10-25 02:57:14.196 
Epoch 737/1000 
	 loss: 35.5342, MinusLogProbMetric: 35.5342, val_loss: 36.0632, val_MinusLogProbMetric: 36.0632

Epoch 737: val_loss did not improve from 35.93815
196/196 - 66s - loss: 35.5342 - MinusLogProbMetric: 35.5342 - val_loss: 36.0632 - val_MinusLogProbMetric: 36.0632 - lr: 4.1152e-06 - 66s/epoch - 334ms/step
Epoch 738/1000
2023-10-25 02:58:21.502 
Epoch 738/1000 
	 loss: 35.4305, MinusLogProbMetric: 35.4305, val_loss: 36.0479, val_MinusLogProbMetric: 36.0479

Epoch 738: val_loss did not improve from 35.93815
196/196 - 67s - loss: 35.4305 - MinusLogProbMetric: 35.4305 - val_loss: 36.0479 - val_MinusLogProbMetric: 36.0479 - lr: 4.1152e-06 - 67s/epoch - 343ms/step
Epoch 739/1000
2023-10-25 02:59:28.911 
Epoch 739/1000 
	 loss: 35.4082, MinusLogProbMetric: 35.4082, val_loss: 36.1015, val_MinusLogProbMetric: 36.1015

Epoch 739: val_loss did not improve from 35.93815
196/196 - 67s - loss: 35.4082 - MinusLogProbMetric: 35.4082 - val_loss: 36.1015 - val_MinusLogProbMetric: 36.1015 - lr: 4.1152e-06 - 67s/epoch - 344ms/step
Epoch 740/1000
2023-10-25 03:00:36.987 
Epoch 740/1000 
	 loss: 35.4051, MinusLogProbMetric: 35.4051, val_loss: 35.9770, val_MinusLogProbMetric: 35.9770

Epoch 740: val_loss did not improve from 35.93815
196/196 - 68s - loss: 35.4051 - MinusLogProbMetric: 35.4051 - val_loss: 35.9770 - val_MinusLogProbMetric: 35.9770 - lr: 4.1152e-06 - 68s/epoch - 347ms/step
Epoch 741/1000
2023-10-25 03:01:43.954 
Epoch 741/1000 
	 loss: 35.5040, MinusLogProbMetric: 35.5040, val_loss: 36.1070, val_MinusLogProbMetric: 36.1070

Epoch 741: val_loss did not improve from 35.93815
196/196 - 67s - loss: 35.5040 - MinusLogProbMetric: 35.5040 - val_loss: 36.1070 - val_MinusLogProbMetric: 36.1070 - lr: 4.1152e-06 - 67s/epoch - 342ms/step
Epoch 742/1000
2023-10-25 03:02:50.804 
Epoch 742/1000 
	 loss: 35.4146, MinusLogProbMetric: 35.4146, val_loss: 35.9245, val_MinusLogProbMetric: 35.9245

Epoch 742: val_loss improved from 35.93815 to 35.92451, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 68s - loss: 35.4146 - MinusLogProbMetric: 35.4146 - val_loss: 35.9245 - val_MinusLogProbMetric: 35.9245 - lr: 4.1152e-06 - 68s/epoch - 347ms/step
Epoch 743/1000
2023-10-25 03:04:00.756 
Epoch 743/1000 
	 loss: 35.4655, MinusLogProbMetric: 35.4655, val_loss: 37.0524, val_MinusLogProbMetric: 37.0524

Epoch 743: val_loss did not improve from 35.92451
196/196 - 69s - loss: 35.4655 - MinusLogProbMetric: 35.4655 - val_loss: 37.0524 - val_MinusLogProbMetric: 37.0524 - lr: 4.1152e-06 - 69s/epoch - 351ms/step
Epoch 744/1000
2023-10-25 03:05:07.574 
Epoch 744/1000 
	 loss: 35.4312, MinusLogProbMetric: 35.4312, val_loss: 35.8755, val_MinusLogProbMetric: 35.8755

Epoch 744: val_loss improved from 35.92451 to 35.87551, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 68s - loss: 35.4312 - MinusLogProbMetric: 35.4312 - val_loss: 35.8755 - val_MinusLogProbMetric: 35.8755 - lr: 4.1152e-06 - 68s/epoch - 346ms/step
Epoch 745/1000
2023-10-25 03:06:15.450 
Epoch 745/1000 
	 loss: 35.3707, MinusLogProbMetric: 35.3707, val_loss: 35.7998, val_MinusLogProbMetric: 35.7998

Epoch 745: val_loss improved from 35.87551 to 35.79979, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 68s - loss: 35.3707 - MinusLogProbMetric: 35.3707 - val_loss: 35.7998 - val_MinusLogProbMetric: 35.7998 - lr: 4.1152e-06 - 68s/epoch - 346ms/step
Epoch 746/1000
2023-10-25 03:07:22.663 
Epoch 746/1000 
	 loss: 35.3904, MinusLogProbMetric: 35.3904, val_loss: 36.1334, val_MinusLogProbMetric: 36.1334

Epoch 746: val_loss did not improve from 35.79979
196/196 - 66s - loss: 35.3904 - MinusLogProbMetric: 35.3904 - val_loss: 36.1334 - val_MinusLogProbMetric: 36.1334 - lr: 4.1152e-06 - 66s/epoch - 338ms/step
Epoch 747/1000
2023-10-25 03:08:29.105 
Epoch 747/1000 
	 loss: 35.3620, MinusLogProbMetric: 35.3620, val_loss: 35.8798, val_MinusLogProbMetric: 35.8798

Epoch 747: val_loss did not improve from 35.79979
196/196 - 66s - loss: 35.3620 - MinusLogProbMetric: 35.3620 - val_loss: 35.8798 - val_MinusLogProbMetric: 35.8798 - lr: 4.1152e-06 - 66s/epoch - 339ms/step
Epoch 748/1000
2023-10-25 03:09:33.708 
Epoch 748/1000 
	 loss: 35.4017, MinusLogProbMetric: 35.4017, val_loss: 36.1294, val_MinusLogProbMetric: 36.1294

Epoch 748: val_loss did not improve from 35.79979
196/196 - 65s - loss: 35.4017 - MinusLogProbMetric: 35.4017 - val_loss: 36.1294 - val_MinusLogProbMetric: 36.1294 - lr: 4.1152e-06 - 65s/epoch - 330ms/step
Epoch 749/1000
2023-10-25 03:10:26.510 
Epoch 749/1000 
	 loss: 35.3764, MinusLogProbMetric: 35.3764, val_loss: 35.8928, val_MinusLogProbMetric: 35.8928

Epoch 749: val_loss did not improve from 35.79979
196/196 - 53s - loss: 35.3764 - MinusLogProbMetric: 35.3764 - val_loss: 35.8928 - val_MinusLogProbMetric: 35.8928 - lr: 4.1152e-06 - 53s/epoch - 269ms/step
Epoch 750/1000
2023-10-25 03:11:30.736 
Epoch 750/1000 
	 loss: 35.3626, MinusLogProbMetric: 35.3626, val_loss: 35.7867, val_MinusLogProbMetric: 35.7867

Epoch 750: val_loss improved from 35.79979 to 35.78672, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 65s - loss: 35.3626 - MinusLogProbMetric: 35.3626 - val_loss: 35.7867 - val_MinusLogProbMetric: 35.7867 - lr: 4.1152e-06 - 65s/epoch - 332ms/step
Epoch 751/1000
2023-10-25 03:12:37.082 
Epoch 751/1000 
	 loss: 36.2144, MinusLogProbMetric: 36.2144, val_loss: 35.8363, val_MinusLogProbMetric: 35.8363

Epoch 751: val_loss did not improve from 35.78672
196/196 - 65s - loss: 36.2144 - MinusLogProbMetric: 36.2144 - val_loss: 35.8363 - val_MinusLogProbMetric: 35.8363 - lr: 4.1152e-06 - 65s/epoch - 334ms/step
Epoch 752/1000
2023-10-25 03:13:44.203 
Epoch 752/1000 
	 loss: 35.3423, MinusLogProbMetric: 35.3423, val_loss: 35.7791, val_MinusLogProbMetric: 35.7791

Epoch 752: val_loss improved from 35.78672 to 35.77906, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 68s - loss: 35.3423 - MinusLogProbMetric: 35.3423 - val_loss: 35.7791 - val_MinusLogProbMetric: 35.7791 - lr: 4.1152e-06 - 68s/epoch - 348ms/step
Epoch 753/1000
2023-10-25 03:14:52.622 
Epoch 753/1000 
	 loss: 35.2977, MinusLogProbMetric: 35.2977, val_loss: 35.8552, val_MinusLogProbMetric: 35.8552

Epoch 753: val_loss did not improve from 35.77906
196/196 - 67s - loss: 35.2977 - MinusLogProbMetric: 35.2977 - val_loss: 35.8552 - val_MinusLogProbMetric: 35.8552 - lr: 4.1152e-06 - 67s/epoch - 343ms/step
Epoch 754/1000
2023-10-25 03:15:58.907 
Epoch 754/1000 
	 loss: 35.3078, MinusLogProbMetric: 35.3078, val_loss: 35.9763, val_MinusLogProbMetric: 35.9763

Epoch 754: val_loss did not improve from 35.77906
196/196 - 66s - loss: 35.3078 - MinusLogProbMetric: 35.3078 - val_loss: 35.9763 - val_MinusLogProbMetric: 35.9763 - lr: 4.1152e-06 - 66s/epoch - 338ms/step
Epoch 755/1000
2023-10-25 03:17:04.673 
Epoch 755/1000 
	 loss: 35.3154, MinusLogProbMetric: 35.3154, val_loss: 35.7285, val_MinusLogProbMetric: 35.7285

Epoch 755: val_loss improved from 35.77906 to 35.72851, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 67s - loss: 35.3154 - MinusLogProbMetric: 35.3154 - val_loss: 35.7285 - val_MinusLogProbMetric: 35.7285 - lr: 4.1152e-06 - 67s/epoch - 340ms/step
Epoch 756/1000
2023-10-25 03:18:11.786 
Epoch 756/1000 
	 loss: 35.5711, MinusLogProbMetric: 35.5711, val_loss: 35.9060, val_MinusLogProbMetric: 35.9060

Epoch 756: val_loss did not improve from 35.72851
196/196 - 66s - loss: 35.5711 - MinusLogProbMetric: 35.5711 - val_loss: 35.9060 - val_MinusLogProbMetric: 35.9060 - lr: 4.1152e-06 - 66s/epoch - 338ms/step
Epoch 757/1000
2023-10-25 03:19:18.610 
Epoch 757/1000 
	 loss: 35.2926, MinusLogProbMetric: 35.2926, val_loss: 35.7543, val_MinusLogProbMetric: 35.7543

Epoch 757: val_loss did not improve from 35.72851
196/196 - 67s - loss: 35.2926 - MinusLogProbMetric: 35.2926 - val_loss: 35.7543 - val_MinusLogProbMetric: 35.7543 - lr: 4.1152e-06 - 67s/epoch - 341ms/step
Epoch 758/1000
2023-10-25 03:20:25.927 
Epoch 758/1000 
	 loss: 35.2802, MinusLogProbMetric: 35.2802, val_loss: 35.7854, val_MinusLogProbMetric: 35.7854

Epoch 758: val_loss did not improve from 35.72851
196/196 - 67s - loss: 35.2802 - MinusLogProbMetric: 35.2802 - val_loss: 35.7854 - val_MinusLogProbMetric: 35.7854 - lr: 4.1152e-06 - 67s/epoch - 343ms/step
Epoch 759/1000
2023-10-25 03:21:34.260 
Epoch 759/1000 
	 loss: 35.2927, MinusLogProbMetric: 35.2927, val_loss: 35.6444, val_MinusLogProbMetric: 35.6444

Epoch 759: val_loss improved from 35.72851 to 35.64438, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 69s - loss: 35.2927 - MinusLogProbMetric: 35.2927 - val_loss: 35.6444 - val_MinusLogProbMetric: 35.6444 - lr: 4.1152e-06 - 69s/epoch - 354ms/step
Epoch 760/1000
2023-10-25 03:22:41.165 
Epoch 760/1000 
	 loss: 35.3182, MinusLogProbMetric: 35.3182, val_loss: 35.7328, val_MinusLogProbMetric: 35.7328

Epoch 760: val_loss did not improve from 35.64438
196/196 - 66s - loss: 35.3182 - MinusLogProbMetric: 35.3182 - val_loss: 35.7328 - val_MinusLogProbMetric: 35.7328 - lr: 4.1152e-06 - 66s/epoch - 336ms/step
Epoch 761/1000
2023-10-25 03:23:48.058 
Epoch 761/1000 
	 loss: 35.2476, MinusLogProbMetric: 35.2476, val_loss: 35.7318, val_MinusLogProbMetric: 35.7318

Epoch 761: val_loss did not improve from 35.64438
196/196 - 67s - loss: 35.2476 - MinusLogProbMetric: 35.2476 - val_loss: 35.7318 - val_MinusLogProbMetric: 35.7318 - lr: 4.1152e-06 - 67s/epoch - 341ms/step
Epoch 762/1000
2023-10-25 03:24:53.466 
Epoch 762/1000 
	 loss: 35.2625, MinusLogProbMetric: 35.2625, val_loss: 35.7642, val_MinusLogProbMetric: 35.7642

Epoch 762: val_loss did not improve from 35.64438
196/196 - 65s - loss: 35.2625 - MinusLogProbMetric: 35.2625 - val_loss: 35.7642 - val_MinusLogProbMetric: 35.7642 - lr: 4.1152e-06 - 65s/epoch - 334ms/step
Epoch 763/1000
2023-10-25 03:26:00.279 
Epoch 763/1000 
	 loss: 35.2193, MinusLogProbMetric: 35.2193, val_loss: 35.7769, val_MinusLogProbMetric: 35.7769

Epoch 763: val_loss did not improve from 35.64438
196/196 - 67s - loss: 35.2193 - MinusLogProbMetric: 35.2193 - val_loss: 35.7769 - val_MinusLogProbMetric: 35.7769 - lr: 4.1152e-06 - 67s/epoch - 341ms/step
Epoch 764/1000
2023-10-25 03:27:05.980 
Epoch 764/1000 
	 loss: 35.3208, MinusLogProbMetric: 35.3208, val_loss: 35.8907, val_MinusLogProbMetric: 35.8907

Epoch 764: val_loss did not improve from 35.64438
196/196 - 66s - loss: 35.3208 - MinusLogProbMetric: 35.3208 - val_loss: 35.8907 - val_MinusLogProbMetric: 35.8907 - lr: 4.1152e-06 - 66s/epoch - 335ms/step
Epoch 765/1000
2023-10-25 03:28:13.826 
Epoch 765/1000 
	 loss: 35.2346, MinusLogProbMetric: 35.2346, val_loss: 35.6037, val_MinusLogProbMetric: 35.6037

Epoch 765: val_loss improved from 35.64438 to 35.60372, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 69s - loss: 35.2346 - MinusLogProbMetric: 35.2346 - val_loss: 35.6037 - val_MinusLogProbMetric: 35.6037 - lr: 4.1152e-06 - 69s/epoch - 352ms/step
Epoch 766/1000
2023-10-25 03:29:22.325 
Epoch 766/1000 
	 loss: 35.2284, MinusLogProbMetric: 35.2284, val_loss: 35.5680, val_MinusLogProbMetric: 35.5680

Epoch 766: val_loss improved from 35.60372 to 35.56802, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 68s - loss: 35.2284 - MinusLogProbMetric: 35.2284 - val_loss: 35.5680 - val_MinusLogProbMetric: 35.5680 - lr: 4.1152e-06 - 68s/epoch - 349ms/step
Epoch 767/1000
2023-10-25 03:30:29.728 
Epoch 767/1000 
	 loss: 35.1915, MinusLogProbMetric: 35.1915, val_loss: 35.6893, val_MinusLogProbMetric: 35.6893

Epoch 767: val_loss did not improve from 35.56802
196/196 - 66s - loss: 35.1915 - MinusLogProbMetric: 35.1915 - val_loss: 35.6893 - val_MinusLogProbMetric: 35.6893 - lr: 4.1152e-06 - 66s/epoch - 339ms/step
Epoch 768/1000
2023-10-25 03:31:36.746 
Epoch 768/1000 
	 loss: 35.1563, MinusLogProbMetric: 35.1563, val_loss: 35.6879, val_MinusLogProbMetric: 35.6879

Epoch 768: val_loss did not improve from 35.56802
196/196 - 67s - loss: 35.1563 - MinusLogProbMetric: 35.1563 - val_loss: 35.6879 - val_MinusLogProbMetric: 35.6879 - lr: 4.1152e-06 - 67s/epoch - 342ms/step
Epoch 769/1000
2023-10-25 03:32:44.546 
Epoch 769/1000 
	 loss: 35.3158, MinusLogProbMetric: 35.3158, val_loss: 35.7367, val_MinusLogProbMetric: 35.7367

Epoch 769: val_loss did not improve from 35.56802
196/196 - 68s - loss: 35.3158 - MinusLogProbMetric: 35.3158 - val_loss: 35.7367 - val_MinusLogProbMetric: 35.7367 - lr: 4.1152e-06 - 68s/epoch - 346ms/step
Epoch 770/1000
2023-10-25 03:33:50.772 
Epoch 770/1000 
	 loss: 35.1897, MinusLogProbMetric: 35.1897, val_loss: 35.8186, val_MinusLogProbMetric: 35.8186

Epoch 770: val_loss did not improve from 35.56802
196/196 - 66s - loss: 35.1897 - MinusLogProbMetric: 35.1897 - val_loss: 35.8186 - val_MinusLogProbMetric: 35.8186 - lr: 4.1152e-06 - 66s/epoch - 338ms/step
Epoch 771/1000
2023-10-25 03:34:56.681 
Epoch 771/1000 
	 loss: 35.1933, MinusLogProbMetric: 35.1933, val_loss: 35.6320, val_MinusLogProbMetric: 35.6320

Epoch 771: val_loss did not improve from 35.56802
196/196 - 66s - loss: 35.1933 - MinusLogProbMetric: 35.1933 - val_loss: 35.6320 - val_MinusLogProbMetric: 35.6320 - lr: 4.1152e-06 - 66s/epoch - 336ms/step
Epoch 772/1000
2023-10-25 03:36:03.589 
Epoch 772/1000 
	 loss: 36.0907, MinusLogProbMetric: 36.0907, val_loss: 35.7690, val_MinusLogProbMetric: 35.7690

Epoch 772: val_loss did not improve from 35.56802
196/196 - 67s - loss: 36.0907 - MinusLogProbMetric: 36.0907 - val_loss: 35.7690 - val_MinusLogProbMetric: 35.7690 - lr: 4.1152e-06 - 67s/epoch - 341ms/step
Epoch 773/1000
2023-10-25 03:37:09.143 
Epoch 773/1000 
	 loss: 35.1678, MinusLogProbMetric: 35.1678, val_loss: 35.8306, val_MinusLogProbMetric: 35.8306

Epoch 773: val_loss did not improve from 35.56802
196/196 - 66s - loss: 35.1678 - MinusLogProbMetric: 35.1678 - val_loss: 35.8306 - val_MinusLogProbMetric: 35.8306 - lr: 4.1152e-06 - 66s/epoch - 334ms/step
Epoch 774/1000
2023-10-25 03:38:15.893 
Epoch 774/1000 
	 loss: 35.1621, MinusLogProbMetric: 35.1621, val_loss: 35.6741, val_MinusLogProbMetric: 35.6741

Epoch 774: val_loss did not improve from 35.56802
196/196 - 67s - loss: 35.1621 - MinusLogProbMetric: 35.1621 - val_loss: 35.6741 - val_MinusLogProbMetric: 35.6741 - lr: 4.1152e-06 - 67s/epoch - 341ms/step
Epoch 775/1000
2023-10-25 03:39:24.443 
Epoch 775/1000 
	 loss: 35.1561, MinusLogProbMetric: 35.1561, val_loss: 35.6718, val_MinusLogProbMetric: 35.6718

Epoch 775: val_loss did not improve from 35.56802
196/196 - 69s - loss: 35.1561 - MinusLogProbMetric: 35.1561 - val_loss: 35.6718 - val_MinusLogProbMetric: 35.6718 - lr: 4.1152e-06 - 69s/epoch - 350ms/step
Epoch 776/1000
2023-10-25 03:40:33.950 
Epoch 776/1000 
	 loss: 35.1280, MinusLogProbMetric: 35.1280, val_loss: 35.5703, val_MinusLogProbMetric: 35.5703

Epoch 776: val_loss did not improve from 35.56802
196/196 - 70s - loss: 35.1280 - MinusLogProbMetric: 35.1280 - val_loss: 35.5703 - val_MinusLogProbMetric: 35.5703 - lr: 4.1152e-06 - 70s/epoch - 355ms/step
Epoch 777/1000
2023-10-25 03:41:40.201 
Epoch 777/1000 
	 loss: 35.0956, MinusLogProbMetric: 35.0956, val_loss: 35.7655, val_MinusLogProbMetric: 35.7655

Epoch 777: val_loss did not improve from 35.56802
196/196 - 66s - loss: 35.0956 - MinusLogProbMetric: 35.0956 - val_loss: 35.7655 - val_MinusLogProbMetric: 35.7655 - lr: 4.1152e-06 - 66s/epoch - 338ms/step
Epoch 778/1000
2023-10-25 03:42:45.315 
Epoch 778/1000 
	 loss: 35.2218, MinusLogProbMetric: 35.2218, val_loss: 35.8823, val_MinusLogProbMetric: 35.8823

Epoch 778: val_loss did not improve from 35.56802
196/196 - 65s - loss: 35.2218 - MinusLogProbMetric: 35.2218 - val_loss: 35.8823 - val_MinusLogProbMetric: 35.8823 - lr: 4.1152e-06 - 65s/epoch - 332ms/step
Epoch 779/1000
2023-10-25 03:43:50.760 
Epoch 779/1000 
	 loss: 35.4968, MinusLogProbMetric: 35.4968, val_loss: 35.7845, val_MinusLogProbMetric: 35.7845

Epoch 779: val_loss did not improve from 35.56802
196/196 - 65s - loss: 35.4968 - MinusLogProbMetric: 35.4968 - val_loss: 35.7845 - val_MinusLogProbMetric: 35.7845 - lr: 4.1152e-06 - 65s/epoch - 334ms/step
Epoch 780/1000
2023-10-25 03:44:57.586 
Epoch 780/1000 
	 loss: 35.2653, MinusLogProbMetric: 35.2653, val_loss: 35.6928, val_MinusLogProbMetric: 35.6928

Epoch 780: val_loss did not improve from 35.56802
196/196 - 67s - loss: 35.2653 - MinusLogProbMetric: 35.2653 - val_loss: 35.6928 - val_MinusLogProbMetric: 35.6928 - lr: 4.1152e-06 - 67s/epoch - 341ms/step
Epoch 781/1000
2023-10-25 03:46:03.777 
Epoch 781/1000 
	 loss: 35.1041, MinusLogProbMetric: 35.1041, val_loss: 35.8744, val_MinusLogProbMetric: 35.8744

Epoch 781: val_loss did not improve from 35.56802
196/196 - 66s - loss: 35.1041 - MinusLogProbMetric: 35.1041 - val_loss: 35.8744 - val_MinusLogProbMetric: 35.8744 - lr: 4.1152e-06 - 66s/epoch - 338ms/step
Epoch 782/1000
2023-10-25 03:47:11.187 
Epoch 782/1000 
	 loss: 35.1255, MinusLogProbMetric: 35.1255, val_loss: 35.6669, val_MinusLogProbMetric: 35.6669

Epoch 782: val_loss did not improve from 35.56802
196/196 - 67s - loss: 35.1255 - MinusLogProbMetric: 35.1255 - val_loss: 35.6669 - val_MinusLogProbMetric: 35.6669 - lr: 4.1152e-06 - 67s/epoch - 344ms/step
Epoch 783/1000
2023-10-25 03:48:14.778 
Epoch 783/1000 
	 loss: 35.1297, MinusLogProbMetric: 35.1297, val_loss: 35.6296, val_MinusLogProbMetric: 35.6296

Epoch 783: val_loss did not improve from 35.56802
196/196 - 64s - loss: 35.1297 - MinusLogProbMetric: 35.1297 - val_loss: 35.6296 - val_MinusLogProbMetric: 35.6296 - lr: 4.1152e-06 - 64s/epoch - 324ms/step
Epoch 784/1000
2023-10-25 03:49:18.669 
Epoch 784/1000 
	 loss: 35.0839, MinusLogProbMetric: 35.0839, val_loss: 35.7326, val_MinusLogProbMetric: 35.7326

Epoch 784: val_loss did not improve from 35.56802
196/196 - 64s - loss: 35.0839 - MinusLogProbMetric: 35.0839 - val_loss: 35.7326 - val_MinusLogProbMetric: 35.7326 - lr: 4.1152e-06 - 64s/epoch - 326ms/step
Epoch 785/1000
2023-10-25 03:50:23.742 
Epoch 785/1000 
	 loss: 35.0638, MinusLogProbMetric: 35.0638, val_loss: 35.5448, val_MinusLogProbMetric: 35.5448

Epoch 785: val_loss improved from 35.56802 to 35.54481, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 66s - loss: 35.0638 - MinusLogProbMetric: 35.0638 - val_loss: 35.5448 - val_MinusLogProbMetric: 35.5448 - lr: 4.1152e-06 - 66s/epoch - 337ms/step
Epoch 786/1000
2023-10-25 03:51:28.867 
Epoch 786/1000 
	 loss: 35.1072, MinusLogProbMetric: 35.1072, val_loss: 35.8212, val_MinusLogProbMetric: 35.8212

Epoch 786: val_loss did not improve from 35.54481
196/196 - 64s - loss: 35.1072 - MinusLogProbMetric: 35.1072 - val_loss: 35.8212 - val_MinusLogProbMetric: 35.8212 - lr: 4.1152e-06 - 64s/epoch - 327ms/step
Epoch 787/1000
2023-10-25 03:52:33.629 
Epoch 787/1000 
	 loss: 35.1032, MinusLogProbMetric: 35.1032, val_loss: 35.5952, val_MinusLogProbMetric: 35.5952

Epoch 787: val_loss did not improve from 35.54481
196/196 - 65s - loss: 35.1032 - MinusLogProbMetric: 35.1032 - val_loss: 35.5952 - val_MinusLogProbMetric: 35.5952 - lr: 4.1152e-06 - 65s/epoch - 330ms/step
Epoch 788/1000
2023-10-25 03:53:42.501 
Epoch 788/1000 
	 loss: 35.0331, MinusLogProbMetric: 35.0331, val_loss: 35.5140, val_MinusLogProbMetric: 35.5140

Epoch 788: val_loss improved from 35.54481 to 35.51400, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 70s - loss: 35.0331 - MinusLogProbMetric: 35.0331 - val_loss: 35.5140 - val_MinusLogProbMetric: 35.5140 - lr: 4.1152e-06 - 70s/epoch - 357ms/step
Epoch 789/1000
2023-10-25 03:54:49.619 
Epoch 789/1000 
	 loss: 35.0113, MinusLogProbMetric: 35.0113, val_loss: 35.6319, val_MinusLogProbMetric: 35.6319

Epoch 789: val_loss did not improve from 35.51400
196/196 - 66s - loss: 35.0113 - MinusLogProbMetric: 35.0113 - val_loss: 35.6319 - val_MinusLogProbMetric: 35.6319 - lr: 4.1152e-06 - 66s/epoch - 337ms/step
Epoch 790/1000
2023-10-25 03:55:55.977 
Epoch 790/1000 
	 loss: 35.0455, MinusLogProbMetric: 35.0455, val_loss: 35.7244, val_MinusLogProbMetric: 35.7244

Epoch 790: val_loss did not improve from 35.51400
196/196 - 66s - loss: 35.0455 - MinusLogProbMetric: 35.0455 - val_loss: 35.7244 - val_MinusLogProbMetric: 35.7244 - lr: 4.1152e-06 - 66s/epoch - 339ms/step
Epoch 791/1000
2023-10-25 03:57:03.611 
Epoch 791/1000 
	 loss: 35.0761, MinusLogProbMetric: 35.0761, val_loss: 35.5731, val_MinusLogProbMetric: 35.5731

Epoch 791: val_loss did not improve from 35.51400
196/196 - 68s - loss: 35.0761 - MinusLogProbMetric: 35.0761 - val_loss: 35.5731 - val_MinusLogProbMetric: 35.5731 - lr: 4.1152e-06 - 68s/epoch - 345ms/step
Epoch 792/1000
2023-10-25 03:58:10.554 
Epoch 792/1000 
	 loss: 35.0064, MinusLogProbMetric: 35.0064, val_loss: 35.5123, val_MinusLogProbMetric: 35.5123

Epoch 792: val_loss improved from 35.51400 to 35.51232, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 68s - loss: 35.0064 - MinusLogProbMetric: 35.0064 - val_loss: 35.5123 - val_MinusLogProbMetric: 35.5123 - lr: 4.1152e-06 - 68s/epoch - 347ms/step
Epoch 793/1000
2023-10-25 03:59:18.119 
Epoch 793/1000 
	 loss: 35.6707, MinusLogProbMetric: 35.6707, val_loss: 35.5338, val_MinusLogProbMetric: 35.5338

Epoch 793: val_loss did not improve from 35.51232
196/196 - 66s - loss: 35.6707 - MinusLogProbMetric: 35.6707 - val_loss: 35.5338 - val_MinusLogProbMetric: 35.5338 - lr: 4.1152e-06 - 66s/epoch - 339ms/step
Epoch 794/1000
2023-10-25 04:00:24.018 
Epoch 794/1000 
	 loss: 35.0367, MinusLogProbMetric: 35.0367, val_loss: 35.4671, val_MinusLogProbMetric: 35.4671

Epoch 794: val_loss improved from 35.51232 to 35.46711, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 67s - loss: 35.0367 - MinusLogProbMetric: 35.0367 - val_loss: 35.4671 - val_MinusLogProbMetric: 35.4671 - lr: 4.1152e-06 - 67s/epoch - 341ms/step
Epoch 795/1000
2023-10-25 04:01:30.066 
Epoch 795/1000 
	 loss: 35.0140, MinusLogProbMetric: 35.0140, val_loss: 35.6335, val_MinusLogProbMetric: 35.6335

Epoch 795: val_loss did not improve from 35.46711
196/196 - 65s - loss: 35.0140 - MinusLogProbMetric: 35.0140 - val_loss: 35.6335 - val_MinusLogProbMetric: 35.6335 - lr: 4.1152e-06 - 65s/epoch - 332ms/step
Epoch 796/1000
2023-10-25 04:02:36.960 
Epoch 796/1000 
	 loss: 35.0017, MinusLogProbMetric: 35.0017, val_loss: 35.6331, val_MinusLogProbMetric: 35.6331

Epoch 796: val_loss did not improve from 35.46711
196/196 - 67s - loss: 35.0017 - MinusLogProbMetric: 35.0017 - val_loss: 35.6331 - val_MinusLogProbMetric: 35.6331 - lr: 4.1152e-06 - 67s/epoch - 341ms/step
Epoch 797/1000
2023-10-25 04:03:43.395 
Epoch 797/1000 
	 loss: 35.0129, MinusLogProbMetric: 35.0129, val_loss: 35.7628, val_MinusLogProbMetric: 35.7628

Epoch 797: val_loss did not improve from 35.46711
196/196 - 66s - loss: 35.0129 - MinusLogProbMetric: 35.0129 - val_loss: 35.7628 - val_MinusLogProbMetric: 35.7628 - lr: 4.1152e-06 - 66s/epoch - 339ms/step
Epoch 798/1000
2023-10-25 04:04:51.867 
Epoch 798/1000 
	 loss: 35.0238, MinusLogProbMetric: 35.0238, val_loss: 35.5175, val_MinusLogProbMetric: 35.5175

Epoch 798: val_loss did not improve from 35.46711
196/196 - 68s - loss: 35.0238 - MinusLogProbMetric: 35.0238 - val_loss: 35.5175 - val_MinusLogProbMetric: 35.5175 - lr: 4.1152e-06 - 68s/epoch - 349ms/step
Epoch 799/1000
2023-10-25 04:05:59.988 
Epoch 799/1000 
	 loss: 34.9906, MinusLogProbMetric: 34.9906, val_loss: 35.4860, val_MinusLogProbMetric: 35.4860

Epoch 799: val_loss did not improve from 35.46711
196/196 - 68s - loss: 34.9906 - MinusLogProbMetric: 34.9906 - val_loss: 35.4860 - val_MinusLogProbMetric: 35.4860 - lr: 4.1152e-06 - 68s/epoch - 348ms/step
Epoch 800/1000
2023-10-25 04:07:06.157 
Epoch 800/1000 
	 loss: 34.9953, MinusLogProbMetric: 34.9953, val_loss: 35.5228, val_MinusLogProbMetric: 35.5228

Epoch 800: val_loss did not improve from 35.46711
196/196 - 66s - loss: 34.9953 - MinusLogProbMetric: 34.9953 - val_loss: 35.5228 - val_MinusLogProbMetric: 35.5228 - lr: 4.1152e-06 - 66s/epoch - 338ms/step
Epoch 801/1000
2023-10-25 04:08:10.924 
Epoch 801/1000 
	 loss: 34.9400, MinusLogProbMetric: 34.9400, val_loss: 35.4602, val_MinusLogProbMetric: 35.4602

Epoch 801: val_loss improved from 35.46711 to 35.46016, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 66s - loss: 34.9400 - MinusLogProbMetric: 34.9400 - val_loss: 35.4602 - val_MinusLogProbMetric: 35.4602 - lr: 4.1152e-06 - 66s/epoch - 336ms/step
Epoch 802/1000
2023-10-25 04:09:14.999 
Epoch 802/1000 
	 loss: 34.9529, MinusLogProbMetric: 34.9529, val_loss: 35.5439, val_MinusLogProbMetric: 35.5439

Epoch 802: val_loss did not improve from 35.46016
196/196 - 63s - loss: 34.9529 - MinusLogProbMetric: 34.9529 - val_loss: 35.5439 - val_MinusLogProbMetric: 35.5439 - lr: 4.1152e-06 - 63s/epoch - 322ms/step
Epoch 803/1000
2023-10-25 04:10:22.210 
Epoch 803/1000 
	 loss: 34.9709, MinusLogProbMetric: 34.9709, val_loss: 35.6876, val_MinusLogProbMetric: 35.6876

Epoch 803: val_loss did not improve from 35.46016
196/196 - 67s - loss: 34.9709 - MinusLogProbMetric: 34.9709 - val_loss: 35.6876 - val_MinusLogProbMetric: 35.6876 - lr: 4.1152e-06 - 67s/epoch - 343ms/step
Epoch 804/1000
2023-10-25 04:11:30.166 
Epoch 804/1000 
	 loss: 34.9690, MinusLogProbMetric: 34.9690, val_loss: 35.5216, val_MinusLogProbMetric: 35.5216

Epoch 804: val_loss did not improve from 35.46016
196/196 - 68s - loss: 34.9690 - MinusLogProbMetric: 34.9690 - val_loss: 35.5216 - val_MinusLogProbMetric: 35.5216 - lr: 4.1152e-06 - 68s/epoch - 347ms/step
Epoch 805/1000
2023-10-25 04:12:35.679 
Epoch 805/1000 
	 loss: 34.9412, MinusLogProbMetric: 34.9412, val_loss: 36.2909, val_MinusLogProbMetric: 36.2909

Epoch 805: val_loss did not improve from 35.46016
196/196 - 66s - loss: 34.9412 - MinusLogProbMetric: 34.9412 - val_loss: 36.2909 - val_MinusLogProbMetric: 36.2909 - lr: 4.1152e-06 - 66s/epoch - 334ms/step
Epoch 806/1000
2023-10-25 04:13:43.436 
Epoch 806/1000 
	 loss: 34.9330, MinusLogProbMetric: 34.9330, val_loss: 35.3244, val_MinusLogProbMetric: 35.3244

Epoch 806: val_loss improved from 35.46016 to 35.32441, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 69s - loss: 34.9330 - MinusLogProbMetric: 34.9330 - val_loss: 35.3244 - val_MinusLogProbMetric: 35.3244 - lr: 4.1152e-06 - 69s/epoch - 352ms/step
Epoch 807/1000
2023-10-25 04:14:50.772 
Epoch 807/1000 
	 loss: 34.8903, MinusLogProbMetric: 34.8903, val_loss: 35.5269, val_MinusLogProbMetric: 35.5269

Epoch 807: val_loss did not improve from 35.32441
196/196 - 66s - loss: 34.8903 - MinusLogProbMetric: 34.8903 - val_loss: 35.5269 - val_MinusLogProbMetric: 35.5269 - lr: 4.1152e-06 - 66s/epoch - 338ms/step
Epoch 808/1000
2023-10-25 04:15:56.666 
Epoch 808/1000 
	 loss: 34.8750, MinusLogProbMetric: 34.8750, val_loss: 35.4380, val_MinusLogProbMetric: 35.4380

Epoch 808: val_loss did not improve from 35.32441
196/196 - 66s - loss: 34.8750 - MinusLogProbMetric: 34.8750 - val_loss: 35.4380 - val_MinusLogProbMetric: 35.4380 - lr: 4.1152e-06 - 66s/epoch - 336ms/step
Epoch 809/1000
2023-10-25 04:17:05.435 
Epoch 809/1000 
	 loss: 34.9203, MinusLogProbMetric: 34.9203, val_loss: 35.3416, val_MinusLogProbMetric: 35.3416

Epoch 809: val_loss did not improve from 35.32441
196/196 - 69s - loss: 34.9203 - MinusLogProbMetric: 34.9203 - val_loss: 35.3416 - val_MinusLogProbMetric: 35.3416 - lr: 4.1152e-06 - 69s/epoch - 351ms/step
Epoch 810/1000
2023-10-25 04:18:13.095 
Epoch 810/1000 
	 loss: 34.8843, MinusLogProbMetric: 34.8843, val_loss: 35.4846, val_MinusLogProbMetric: 35.4846

Epoch 810: val_loss did not improve from 35.32441
196/196 - 68s - loss: 34.8843 - MinusLogProbMetric: 34.8843 - val_loss: 35.4846 - val_MinusLogProbMetric: 35.4846 - lr: 4.1152e-06 - 68s/epoch - 345ms/step
Epoch 811/1000
2023-10-25 04:19:20.124 
Epoch 811/1000 
	 loss: 34.9257, MinusLogProbMetric: 34.9257, val_loss: 35.5003, val_MinusLogProbMetric: 35.5003

Epoch 811: val_loss did not improve from 35.32441
196/196 - 67s - loss: 34.9257 - MinusLogProbMetric: 34.9257 - val_loss: 35.5003 - val_MinusLogProbMetric: 35.5003 - lr: 4.1152e-06 - 67s/epoch - 342ms/step
Epoch 812/1000
2023-10-25 04:20:27.051 
Epoch 812/1000 
	 loss: 34.8944, MinusLogProbMetric: 34.8944, val_loss: 35.4953, val_MinusLogProbMetric: 35.4953

Epoch 812: val_loss did not improve from 35.32441
196/196 - 67s - loss: 34.8944 - MinusLogProbMetric: 34.8944 - val_loss: 35.4953 - val_MinusLogProbMetric: 35.4953 - lr: 4.1152e-06 - 67s/epoch - 341ms/step
Epoch 813/1000
2023-10-25 04:21:33.375 
Epoch 813/1000 
	 loss: 34.8866, MinusLogProbMetric: 34.8866, val_loss: 35.2815, val_MinusLogProbMetric: 35.2815

Epoch 813: val_loss improved from 35.32441 to 35.28154, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 67s - loss: 34.8866 - MinusLogProbMetric: 34.8866 - val_loss: 35.2815 - val_MinusLogProbMetric: 35.2815 - lr: 4.1152e-06 - 67s/epoch - 344ms/step
Epoch 814/1000
2023-10-25 04:22:42.120 
Epoch 814/1000 
	 loss: 34.8288, MinusLogProbMetric: 34.8288, val_loss: 35.4436, val_MinusLogProbMetric: 35.4436

Epoch 814: val_loss did not improve from 35.28154
196/196 - 68s - loss: 34.8288 - MinusLogProbMetric: 34.8288 - val_loss: 35.4436 - val_MinusLogProbMetric: 35.4436 - lr: 4.1152e-06 - 68s/epoch - 345ms/step
Epoch 815/1000
2023-10-25 04:23:47.484 
Epoch 815/1000 
	 loss: 35.3610, MinusLogProbMetric: 35.3610, val_loss: 35.5158, val_MinusLogProbMetric: 35.5158

Epoch 815: val_loss did not improve from 35.28154
196/196 - 65s - loss: 35.3610 - MinusLogProbMetric: 35.3610 - val_loss: 35.5158 - val_MinusLogProbMetric: 35.5158 - lr: 4.1152e-06 - 65s/epoch - 333ms/step
Epoch 816/1000
2023-10-25 04:24:53.955 
Epoch 816/1000 
	 loss: 34.8586, MinusLogProbMetric: 34.8586, val_loss: 35.2637, val_MinusLogProbMetric: 35.2637

Epoch 816: val_loss improved from 35.28154 to 35.26369, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 67s - loss: 34.8586 - MinusLogProbMetric: 34.8586 - val_loss: 35.2637 - val_MinusLogProbMetric: 35.2637 - lr: 4.1152e-06 - 67s/epoch - 344ms/step
Epoch 817/1000
2023-10-25 04:25:59.365 
Epoch 817/1000 
	 loss: 34.8242, MinusLogProbMetric: 34.8242, val_loss: 35.2973, val_MinusLogProbMetric: 35.2973

Epoch 817: val_loss did not improve from 35.26369
196/196 - 65s - loss: 34.8242 - MinusLogProbMetric: 34.8242 - val_loss: 35.2973 - val_MinusLogProbMetric: 35.2973 - lr: 4.1152e-06 - 65s/epoch - 329ms/step
Epoch 818/1000
2023-10-25 04:27:03.986 
Epoch 818/1000 
	 loss: 34.8044, MinusLogProbMetric: 34.8044, val_loss: 35.2525, val_MinusLogProbMetric: 35.2525

Epoch 818: val_loss improved from 35.26369 to 35.25249, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 66s - loss: 34.8044 - MinusLogProbMetric: 34.8044 - val_loss: 35.2525 - val_MinusLogProbMetric: 35.2525 - lr: 4.1152e-06 - 66s/epoch - 336ms/step
Epoch 819/1000
2023-10-25 04:28:12.948 
Epoch 819/1000 
	 loss: 34.7947, MinusLogProbMetric: 34.7947, val_loss: 35.3956, val_MinusLogProbMetric: 35.3956

Epoch 819: val_loss did not improve from 35.25249
196/196 - 68s - loss: 34.7947 - MinusLogProbMetric: 34.7947 - val_loss: 35.3956 - val_MinusLogProbMetric: 35.3956 - lr: 4.1152e-06 - 68s/epoch - 345ms/step
Epoch 820/1000
2023-10-25 04:29:17.379 
Epoch 820/1000 
	 loss: 34.8146, MinusLogProbMetric: 34.8146, val_loss: 35.1797, val_MinusLogProbMetric: 35.1797

Epoch 820: val_loss improved from 35.25249 to 35.17968, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 66s - loss: 34.8146 - MinusLogProbMetric: 34.8146 - val_loss: 35.1797 - val_MinusLogProbMetric: 35.1797 - lr: 4.1152e-06 - 66s/epoch - 335ms/step
Epoch 821/1000
2023-10-25 04:30:21.544 
Epoch 821/1000 
	 loss: 34.7828, MinusLogProbMetric: 34.7828, val_loss: 35.5549, val_MinusLogProbMetric: 35.5549

Epoch 821: val_loss did not improve from 35.17968
196/196 - 63s - loss: 34.7828 - MinusLogProbMetric: 34.7828 - val_loss: 35.5549 - val_MinusLogProbMetric: 35.5549 - lr: 4.1152e-06 - 63s/epoch - 321ms/step
Epoch 822/1000
2023-10-25 04:31:26.607 
Epoch 822/1000 
	 loss: 34.8743, MinusLogProbMetric: 34.8743, val_loss: 35.5141, val_MinusLogProbMetric: 35.5141

Epoch 822: val_loss did not improve from 35.17968
196/196 - 65s - loss: 34.8743 - MinusLogProbMetric: 34.8743 - val_loss: 35.5141 - val_MinusLogProbMetric: 35.5141 - lr: 4.1152e-06 - 65s/epoch - 332ms/step
Epoch 823/1000
2023-10-25 04:32:30.745 
Epoch 823/1000 
	 loss: 34.7956, MinusLogProbMetric: 34.7956, val_loss: 35.5303, val_MinusLogProbMetric: 35.5303

Epoch 823: val_loss did not improve from 35.17968
196/196 - 64s - loss: 34.7956 - MinusLogProbMetric: 34.7956 - val_loss: 35.5303 - val_MinusLogProbMetric: 35.5303 - lr: 4.1152e-06 - 64s/epoch - 327ms/step
Epoch 824/1000
2023-10-25 04:33:36.265 
Epoch 824/1000 
	 loss: 34.8650, MinusLogProbMetric: 34.8650, val_loss: 35.3209, val_MinusLogProbMetric: 35.3209

Epoch 824: val_loss did not improve from 35.17968
196/196 - 66s - loss: 34.8650 - MinusLogProbMetric: 34.8650 - val_loss: 35.3209 - val_MinusLogProbMetric: 35.3209 - lr: 4.1152e-06 - 66s/epoch - 334ms/step
Epoch 825/1000
2023-10-25 04:34:40.987 
Epoch 825/1000 
	 loss: 34.8303, MinusLogProbMetric: 34.8303, val_loss: 35.3116, val_MinusLogProbMetric: 35.3116

Epoch 825: val_loss did not improve from 35.17968
196/196 - 65s - loss: 34.8303 - MinusLogProbMetric: 34.8303 - val_loss: 35.3116 - val_MinusLogProbMetric: 35.3116 - lr: 4.1152e-06 - 65s/epoch - 330ms/step
Epoch 826/1000
2023-10-25 04:35:48.909 
Epoch 826/1000 
	 loss: 34.7728, MinusLogProbMetric: 34.7728, val_loss: 35.2084, val_MinusLogProbMetric: 35.2084

Epoch 826: val_loss did not improve from 35.17968
196/196 - 68s - loss: 34.7728 - MinusLogProbMetric: 34.7728 - val_loss: 35.2084 - val_MinusLogProbMetric: 35.2084 - lr: 4.1152e-06 - 68s/epoch - 347ms/step
Epoch 827/1000
2023-10-25 04:36:55.337 
Epoch 827/1000 
	 loss: 34.7774, MinusLogProbMetric: 34.7774, val_loss: 35.3422, val_MinusLogProbMetric: 35.3422

Epoch 827: val_loss did not improve from 35.17968
196/196 - 66s - loss: 34.7774 - MinusLogProbMetric: 34.7774 - val_loss: 35.3422 - val_MinusLogProbMetric: 35.3422 - lr: 4.1152e-06 - 66s/epoch - 339ms/step
Epoch 828/1000
2023-10-25 04:38:02.181 
Epoch 828/1000 
	 loss: 34.8172, MinusLogProbMetric: 34.8172, val_loss: 35.1925, val_MinusLogProbMetric: 35.1925

Epoch 828: val_loss did not improve from 35.17968
196/196 - 67s - loss: 34.8172 - MinusLogProbMetric: 34.8172 - val_loss: 35.1925 - val_MinusLogProbMetric: 35.1925 - lr: 4.1152e-06 - 67s/epoch - 341ms/step
Epoch 829/1000
2023-10-25 04:39:08.573 
Epoch 829/1000 
	 loss: 34.7572, MinusLogProbMetric: 34.7572, val_loss: 35.3208, val_MinusLogProbMetric: 35.3208

Epoch 829: val_loss did not improve from 35.17968
196/196 - 66s - loss: 34.7572 - MinusLogProbMetric: 34.7572 - val_loss: 35.3208 - val_MinusLogProbMetric: 35.3208 - lr: 4.1152e-06 - 66s/epoch - 339ms/step
Epoch 830/1000
2023-10-25 04:40:17.472 
Epoch 830/1000 
	 loss: 35.0833, MinusLogProbMetric: 35.0833, val_loss: 35.4208, val_MinusLogProbMetric: 35.4208

Epoch 830: val_loss did not improve from 35.17968
196/196 - 69s - loss: 35.0833 - MinusLogProbMetric: 35.0833 - val_loss: 35.4208 - val_MinusLogProbMetric: 35.4208 - lr: 4.1152e-06 - 69s/epoch - 352ms/step
Epoch 831/1000
2023-10-25 04:41:23.430 
Epoch 831/1000 
	 loss: 34.7293, MinusLogProbMetric: 34.7293, val_loss: 35.4850, val_MinusLogProbMetric: 35.4850

Epoch 831: val_loss did not improve from 35.17968
196/196 - 66s - loss: 34.7293 - MinusLogProbMetric: 34.7293 - val_loss: 35.4850 - val_MinusLogProbMetric: 35.4850 - lr: 4.1152e-06 - 66s/epoch - 336ms/step
Epoch 832/1000
2023-10-25 04:42:32.384 
Epoch 832/1000 
	 loss: 34.7493, MinusLogProbMetric: 34.7493, val_loss: 35.3149, val_MinusLogProbMetric: 35.3149

Epoch 832: val_loss did not improve from 35.17968
196/196 - 69s - loss: 34.7493 - MinusLogProbMetric: 34.7493 - val_loss: 35.3149 - val_MinusLogProbMetric: 35.3149 - lr: 4.1152e-06 - 69s/epoch - 352ms/step
Epoch 833/1000
2023-10-25 04:43:39.026 
Epoch 833/1000 
	 loss: 34.7525, MinusLogProbMetric: 34.7525, val_loss: 35.3139, val_MinusLogProbMetric: 35.3139

Epoch 833: val_loss did not improve from 35.17968
196/196 - 67s - loss: 34.7525 - MinusLogProbMetric: 34.7525 - val_loss: 35.3139 - val_MinusLogProbMetric: 35.3139 - lr: 4.1152e-06 - 67s/epoch - 340ms/step
Epoch 834/1000
2023-10-25 04:44:46.716 
Epoch 834/1000 
	 loss: 34.8736, MinusLogProbMetric: 34.8736, val_loss: 35.5097, val_MinusLogProbMetric: 35.5097

Epoch 834: val_loss did not improve from 35.17968
196/196 - 68s - loss: 34.8736 - MinusLogProbMetric: 34.8736 - val_loss: 35.5097 - val_MinusLogProbMetric: 35.5097 - lr: 4.1152e-06 - 68s/epoch - 345ms/step
Epoch 835/1000
2023-10-25 04:45:53.018 
Epoch 835/1000 
	 loss: 34.7443, MinusLogProbMetric: 34.7443, val_loss: 35.3625, val_MinusLogProbMetric: 35.3625

Epoch 835: val_loss did not improve from 35.17968
196/196 - 66s - loss: 34.7443 - MinusLogProbMetric: 34.7443 - val_loss: 35.3625 - val_MinusLogProbMetric: 35.3625 - lr: 4.1152e-06 - 66s/epoch - 338ms/step
Epoch 836/1000
2023-10-25 04:46:58.096 
Epoch 836/1000 
	 loss: 34.8774, MinusLogProbMetric: 34.8774, val_loss: 35.5413, val_MinusLogProbMetric: 35.5413

Epoch 836: val_loss did not improve from 35.17968
196/196 - 65s - loss: 34.8774 - MinusLogProbMetric: 34.8774 - val_loss: 35.5413 - val_MinusLogProbMetric: 35.5413 - lr: 4.1152e-06 - 65s/epoch - 332ms/step
Epoch 837/1000
2023-10-25 04:48:06.531 
Epoch 837/1000 
	 loss: 34.7369, MinusLogProbMetric: 34.7369, val_loss: 35.2310, val_MinusLogProbMetric: 35.2310

Epoch 837: val_loss did not improve from 35.17968
196/196 - 68s - loss: 34.7369 - MinusLogProbMetric: 34.7369 - val_loss: 35.2310 - val_MinusLogProbMetric: 35.2310 - lr: 4.1152e-06 - 68s/epoch - 349ms/step
Epoch 838/1000
2023-10-25 04:49:14.329 
Epoch 838/1000 
	 loss: 35.0301, MinusLogProbMetric: 35.0301, val_loss: 35.2824, val_MinusLogProbMetric: 35.2824

Epoch 838: val_loss did not improve from 35.17968
196/196 - 68s - loss: 35.0301 - MinusLogProbMetric: 35.0301 - val_loss: 35.2824 - val_MinusLogProbMetric: 35.2824 - lr: 4.1152e-06 - 68s/epoch - 346ms/step
Epoch 839/1000
2023-10-25 04:50:21.746 
Epoch 839/1000 
	 loss: 34.6853, MinusLogProbMetric: 34.6853, val_loss: 35.1874, val_MinusLogProbMetric: 35.1874

Epoch 839: val_loss did not improve from 35.17968
196/196 - 67s - loss: 34.6853 - MinusLogProbMetric: 34.6853 - val_loss: 35.1874 - val_MinusLogProbMetric: 35.1874 - lr: 4.1152e-06 - 67s/epoch - 344ms/step
Epoch 840/1000
2023-10-25 04:51:27.001 
Epoch 840/1000 
	 loss: 34.6828, MinusLogProbMetric: 34.6828, val_loss: 35.2696, val_MinusLogProbMetric: 35.2696

Epoch 840: val_loss did not improve from 35.17968
196/196 - 65s - loss: 34.6828 - MinusLogProbMetric: 34.6828 - val_loss: 35.2696 - val_MinusLogProbMetric: 35.2696 - lr: 4.1152e-06 - 65s/epoch - 333ms/step
Epoch 841/1000
2023-10-25 04:52:34.502 
Epoch 841/1000 
	 loss: 34.6639, MinusLogProbMetric: 34.6639, val_loss: 35.3462, val_MinusLogProbMetric: 35.3462

Epoch 841: val_loss did not improve from 35.17968
196/196 - 67s - loss: 34.6639 - MinusLogProbMetric: 34.6639 - val_loss: 35.3462 - val_MinusLogProbMetric: 35.3462 - lr: 4.1152e-06 - 67s/epoch - 344ms/step
Epoch 842/1000
2023-10-25 04:53:39.990 
Epoch 842/1000 
	 loss: 34.7096, MinusLogProbMetric: 34.7096, val_loss: 35.2795, val_MinusLogProbMetric: 35.2795

Epoch 842: val_loss did not improve from 35.17968
196/196 - 65s - loss: 34.7096 - MinusLogProbMetric: 34.7096 - val_loss: 35.2795 - val_MinusLogProbMetric: 35.2795 - lr: 4.1152e-06 - 65s/epoch - 334ms/step
Epoch 843/1000
2023-10-25 04:54:47.335 
Epoch 843/1000 
	 loss: 34.6872, MinusLogProbMetric: 34.6872, val_loss: 35.0756, val_MinusLogProbMetric: 35.0756

Epoch 843: val_loss improved from 35.17968 to 35.07556, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 69s - loss: 34.6872 - MinusLogProbMetric: 34.6872 - val_loss: 35.0756 - val_MinusLogProbMetric: 35.0756 - lr: 4.1152e-06 - 69s/epoch - 350ms/step
Epoch 844/1000
2023-10-25 04:55:57.404 
Epoch 844/1000 
	 loss: 34.6540, MinusLogProbMetric: 34.6540, val_loss: 35.0666, val_MinusLogProbMetric: 35.0666

Epoch 844: val_loss improved from 35.07556 to 35.06662, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 70s - loss: 34.6540 - MinusLogProbMetric: 34.6540 - val_loss: 35.0666 - val_MinusLogProbMetric: 35.0666 - lr: 4.1152e-06 - 70s/epoch - 357ms/step
Epoch 845/1000
2023-10-25 04:57:02.969 
Epoch 845/1000 
	 loss: 34.6441, MinusLogProbMetric: 34.6441, val_loss: 35.1383, val_MinusLogProbMetric: 35.1383

Epoch 845: val_loss did not improve from 35.06662
196/196 - 64s - loss: 34.6441 - MinusLogProbMetric: 34.6441 - val_loss: 35.1383 - val_MinusLogProbMetric: 35.1383 - lr: 4.1152e-06 - 64s/epoch - 329ms/step
Epoch 846/1000
2023-10-25 04:58:08.784 
Epoch 846/1000 
	 loss: 34.8540, MinusLogProbMetric: 34.8540, val_loss: 35.2293, val_MinusLogProbMetric: 35.2293

Epoch 846: val_loss did not improve from 35.06662
196/196 - 66s - loss: 34.8540 - MinusLogProbMetric: 34.8540 - val_loss: 35.2293 - val_MinusLogProbMetric: 35.2293 - lr: 4.1152e-06 - 66s/epoch - 336ms/step
Epoch 847/1000
2023-10-25 04:59:15.066 
Epoch 847/1000 
	 loss: 34.6278, MinusLogProbMetric: 34.6278, val_loss: 35.0711, val_MinusLogProbMetric: 35.0711

Epoch 847: val_loss did not improve from 35.06662
196/196 - 66s - loss: 34.6278 - MinusLogProbMetric: 34.6278 - val_loss: 35.0711 - val_MinusLogProbMetric: 35.0711 - lr: 4.1152e-06 - 66s/epoch - 338ms/step
Epoch 848/1000
2023-10-25 05:00:20.656 
Epoch 848/1000 
	 loss: 34.6200, MinusLogProbMetric: 34.6200, val_loss: 35.1169, val_MinusLogProbMetric: 35.1169

Epoch 848: val_loss did not improve from 35.06662
196/196 - 66s - loss: 34.6200 - MinusLogProbMetric: 34.6200 - val_loss: 35.1169 - val_MinusLogProbMetric: 35.1169 - lr: 4.1152e-06 - 66s/epoch - 335ms/step
Epoch 849/1000
2023-10-25 05:01:25.449 
Epoch 849/1000 
	 loss: 34.5902, MinusLogProbMetric: 34.5902, val_loss: 35.2899, val_MinusLogProbMetric: 35.2899

Epoch 849: val_loss did not improve from 35.06662
196/196 - 65s - loss: 34.5902 - MinusLogProbMetric: 34.5902 - val_loss: 35.2899 - val_MinusLogProbMetric: 35.2899 - lr: 4.1152e-06 - 65s/epoch - 331ms/step
Epoch 850/1000
2023-10-25 05:02:31.977 
Epoch 850/1000 
	 loss: 34.6638, MinusLogProbMetric: 34.6638, val_loss: 35.1634, val_MinusLogProbMetric: 35.1634

Epoch 850: val_loss did not improve from 35.06662
196/196 - 67s - loss: 34.6638 - MinusLogProbMetric: 34.6638 - val_loss: 35.1634 - val_MinusLogProbMetric: 35.1634 - lr: 4.1152e-06 - 67s/epoch - 339ms/step
Epoch 851/1000
2023-10-25 05:03:40.654 
Epoch 851/1000 
	 loss: 34.5866, MinusLogProbMetric: 34.5866, val_loss: 35.2389, val_MinusLogProbMetric: 35.2389

Epoch 851: val_loss did not improve from 35.06662
196/196 - 69s - loss: 34.5866 - MinusLogProbMetric: 34.5866 - val_loss: 35.2389 - val_MinusLogProbMetric: 35.2389 - lr: 4.1152e-06 - 69s/epoch - 350ms/step
Epoch 852/1000
2023-10-25 05:04:46.156 
Epoch 852/1000 
	 loss: 35.5566, MinusLogProbMetric: 35.5566, val_loss: 35.3928, val_MinusLogProbMetric: 35.3928

Epoch 852: val_loss did not improve from 35.06662
196/196 - 65s - loss: 35.5566 - MinusLogProbMetric: 35.5566 - val_loss: 35.3928 - val_MinusLogProbMetric: 35.3928 - lr: 4.1152e-06 - 65s/epoch - 334ms/step
Epoch 853/1000
2023-10-25 05:05:51.946 
Epoch 853/1000 
	 loss: 34.6325, MinusLogProbMetric: 34.6325, val_loss: 35.0847, val_MinusLogProbMetric: 35.0847

Epoch 853: val_loss did not improve from 35.06662
196/196 - 66s - loss: 34.6325 - MinusLogProbMetric: 34.6325 - val_loss: 35.0847 - val_MinusLogProbMetric: 35.0847 - lr: 4.1152e-06 - 66s/epoch - 336ms/step
Epoch 854/1000
2023-10-25 05:06:58.019 
Epoch 854/1000 
	 loss: 34.5549, MinusLogProbMetric: 34.5549, val_loss: 35.1408, val_MinusLogProbMetric: 35.1408

Epoch 854: val_loss did not improve from 35.06662
196/196 - 66s - loss: 34.5549 - MinusLogProbMetric: 34.5549 - val_loss: 35.1408 - val_MinusLogProbMetric: 35.1408 - lr: 4.1152e-06 - 66s/epoch - 337ms/step
Epoch 855/1000
2023-10-25 05:08:03.217 
Epoch 855/1000 
	 loss: 34.5398, MinusLogProbMetric: 34.5398, val_loss: 35.2361, val_MinusLogProbMetric: 35.2361

Epoch 855: val_loss did not improve from 35.06662
196/196 - 65s - loss: 34.5398 - MinusLogProbMetric: 34.5398 - val_loss: 35.2361 - val_MinusLogProbMetric: 35.2361 - lr: 4.1152e-06 - 65s/epoch - 333ms/step
Epoch 856/1000
2023-10-25 05:09:11.529 
Epoch 856/1000 
	 loss: 34.5609, MinusLogProbMetric: 34.5609, val_loss: 35.1100, val_MinusLogProbMetric: 35.1100

Epoch 856: val_loss did not improve from 35.06662
196/196 - 68s - loss: 34.5609 - MinusLogProbMetric: 34.5609 - val_loss: 35.1100 - val_MinusLogProbMetric: 35.1100 - lr: 4.1152e-06 - 68s/epoch - 349ms/step
Epoch 857/1000
2023-10-25 05:10:17.719 
Epoch 857/1000 
	 loss: 34.5220, MinusLogProbMetric: 34.5220, val_loss: 35.0750, val_MinusLogProbMetric: 35.0750

Epoch 857: val_loss did not improve from 35.06662
196/196 - 66s - loss: 34.5220 - MinusLogProbMetric: 34.5220 - val_loss: 35.0750 - val_MinusLogProbMetric: 35.0750 - lr: 4.1152e-06 - 66s/epoch - 338ms/step
Epoch 858/1000
2023-10-25 05:11:25.277 
Epoch 858/1000 
	 loss: 34.8042, MinusLogProbMetric: 34.8042, val_loss: 35.1256, val_MinusLogProbMetric: 35.1256

Epoch 858: val_loss did not improve from 35.06662
196/196 - 68s - loss: 34.8042 - MinusLogProbMetric: 34.8042 - val_loss: 35.1256 - val_MinusLogProbMetric: 35.1256 - lr: 4.1152e-06 - 68s/epoch - 345ms/step
Epoch 859/1000
2023-10-25 05:12:30.455 
Epoch 859/1000 
	 loss: 34.6455, MinusLogProbMetric: 34.6455, val_loss: 35.0943, val_MinusLogProbMetric: 35.0943

Epoch 859: val_loss did not improve from 35.06662
196/196 - 65s - loss: 34.6455 - MinusLogProbMetric: 34.6455 - val_loss: 35.0943 - val_MinusLogProbMetric: 35.0943 - lr: 4.1152e-06 - 65s/epoch - 333ms/step
Epoch 860/1000
2023-10-25 05:13:38.917 
Epoch 860/1000 
	 loss: 34.5318, MinusLogProbMetric: 34.5318, val_loss: 35.1147, val_MinusLogProbMetric: 35.1147

Epoch 860: val_loss did not improve from 35.06662
196/196 - 68s - loss: 34.5318 - MinusLogProbMetric: 34.5318 - val_loss: 35.1147 - val_MinusLogProbMetric: 35.1147 - lr: 4.1152e-06 - 68s/epoch - 349ms/step
Epoch 861/1000
2023-10-25 05:14:45.495 
Epoch 861/1000 
	 loss: 34.5246, MinusLogProbMetric: 34.5246, val_loss: 34.9080, val_MinusLogProbMetric: 34.9080

Epoch 861: val_loss improved from 35.06662 to 34.90800, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 68s - loss: 34.5246 - MinusLogProbMetric: 34.5246 - val_loss: 34.9080 - val_MinusLogProbMetric: 34.9080 - lr: 4.1152e-06 - 68s/epoch - 345ms/step
Epoch 862/1000
2023-10-25 05:15:52.771 
Epoch 862/1000 
	 loss: 34.5629, MinusLogProbMetric: 34.5629, val_loss: 36.2193, val_MinusLogProbMetric: 36.2193

Epoch 862: val_loss did not improve from 34.90800
196/196 - 66s - loss: 34.5629 - MinusLogProbMetric: 34.5629 - val_loss: 36.2193 - val_MinusLogProbMetric: 36.2193 - lr: 4.1152e-06 - 66s/epoch - 338ms/step
Epoch 863/1000
2023-10-25 05:16:58.030 
Epoch 863/1000 
	 loss: 34.5823, MinusLogProbMetric: 34.5823, val_loss: 34.9443, val_MinusLogProbMetric: 34.9443

Epoch 863: val_loss did not improve from 34.90800
196/196 - 65s - loss: 34.5823 - MinusLogProbMetric: 34.5823 - val_loss: 34.9443 - val_MinusLogProbMetric: 34.9443 - lr: 4.1152e-06 - 65s/epoch - 333ms/step
Epoch 864/1000
2023-10-25 05:18:05.626 
Epoch 864/1000 
	 loss: 34.5389, MinusLogProbMetric: 34.5389, val_loss: 35.0404, val_MinusLogProbMetric: 35.0404

Epoch 864: val_loss did not improve from 34.90800
196/196 - 68s - loss: 34.5389 - MinusLogProbMetric: 34.5389 - val_loss: 35.0404 - val_MinusLogProbMetric: 35.0404 - lr: 4.1152e-06 - 68s/epoch - 345ms/step
Epoch 865/1000
2023-10-25 05:19:10.086 
Epoch 865/1000 
	 loss: 34.5630, MinusLogProbMetric: 34.5630, val_loss: 35.0222, val_MinusLogProbMetric: 35.0222

Epoch 865: val_loss did not improve from 34.90800
196/196 - 64s - loss: 34.5630 - MinusLogProbMetric: 34.5630 - val_loss: 35.0222 - val_MinusLogProbMetric: 35.0222 - lr: 4.1152e-06 - 64s/epoch - 329ms/step
Epoch 866/1000
2023-10-25 05:20:18.674 
Epoch 866/1000 
	 loss: 34.5157, MinusLogProbMetric: 34.5157, val_loss: 34.9561, val_MinusLogProbMetric: 34.9561

Epoch 866: val_loss did not improve from 34.90800
196/196 - 69s - loss: 34.5157 - MinusLogProbMetric: 34.5157 - val_loss: 34.9561 - val_MinusLogProbMetric: 34.9561 - lr: 4.1152e-06 - 69s/epoch - 350ms/step
Epoch 867/1000
2023-10-25 05:21:27.223 
Epoch 867/1000 
	 loss: 34.5274, MinusLogProbMetric: 34.5274, val_loss: 35.0350, val_MinusLogProbMetric: 35.0350

Epoch 867: val_loss did not improve from 34.90800
196/196 - 69s - loss: 34.5274 - MinusLogProbMetric: 34.5274 - val_loss: 35.0350 - val_MinusLogProbMetric: 35.0350 - lr: 4.1152e-06 - 69s/epoch - 350ms/step
Epoch 868/1000
2023-10-25 05:22:35.345 
Epoch 868/1000 
	 loss: 34.4779, MinusLogProbMetric: 34.4779, val_loss: 35.0637, val_MinusLogProbMetric: 35.0637

Epoch 868: val_loss did not improve from 34.90800
196/196 - 68s - loss: 34.4779 - MinusLogProbMetric: 34.4779 - val_loss: 35.0637 - val_MinusLogProbMetric: 35.0637 - lr: 4.1152e-06 - 68s/epoch - 348ms/step
Epoch 869/1000
2023-10-25 05:23:43.164 
Epoch 869/1000 
	 loss: 34.5358, MinusLogProbMetric: 34.5358, val_loss: 34.9440, val_MinusLogProbMetric: 34.9440

Epoch 869: val_loss did not improve from 34.90800
196/196 - 68s - loss: 34.5358 - MinusLogProbMetric: 34.5358 - val_loss: 34.9440 - val_MinusLogProbMetric: 34.9440 - lr: 4.1152e-06 - 68s/epoch - 346ms/step
Epoch 870/1000
2023-10-25 05:24:50.651 
Epoch 870/1000 
	 loss: 34.5499, MinusLogProbMetric: 34.5499, val_loss: 35.0357, val_MinusLogProbMetric: 35.0357

Epoch 870: val_loss did not improve from 34.90800
196/196 - 67s - loss: 34.5499 - MinusLogProbMetric: 34.5499 - val_loss: 35.0357 - val_MinusLogProbMetric: 35.0357 - lr: 4.1152e-06 - 67s/epoch - 344ms/step
Epoch 871/1000
2023-10-25 05:25:56.987 
Epoch 871/1000 
	 loss: 34.5331, MinusLogProbMetric: 34.5331, val_loss: 34.8822, val_MinusLogProbMetric: 34.8822

Epoch 871: val_loss improved from 34.90800 to 34.88217, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 67s - loss: 34.5331 - MinusLogProbMetric: 34.5331 - val_loss: 34.8822 - val_MinusLogProbMetric: 34.8822 - lr: 4.1152e-06 - 67s/epoch - 344ms/step
Epoch 872/1000
2023-10-25 05:27:03.016 
Epoch 872/1000 
	 loss: 34.4266, MinusLogProbMetric: 34.4266, val_loss: 35.0393, val_MinusLogProbMetric: 35.0393

Epoch 872: val_loss did not improve from 34.88217
196/196 - 65s - loss: 34.4266 - MinusLogProbMetric: 34.4266 - val_loss: 35.0393 - val_MinusLogProbMetric: 35.0393 - lr: 4.1152e-06 - 65s/epoch - 331ms/step
Epoch 873/1000
2023-10-25 05:28:09.778 
Epoch 873/1000 
	 loss: 34.4505, MinusLogProbMetric: 34.4505, val_loss: 35.1525, val_MinusLogProbMetric: 35.1525

Epoch 873: val_loss did not improve from 34.88217
196/196 - 67s - loss: 34.4505 - MinusLogProbMetric: 34.4505 - val_loss: 35.1525 - val_MinusLogProbMetric: 35.1525 - lr: 4.1152e-06 - 67s/epoch - 341ms/step
Epoch 874/1000
2023-10-25 05:29:16.512 
Epoch 874/1000 
	 loss: 34.4584, MinusLogProbMetric: 34.4584, val_loss: 35.1669, val_MinusLogProbMetric: 35.1669

Epoch 874: val_loss did not improve from 34.88217
196/196 - 67s - loss: 34.4584 - MinusLogProbMetric: 34.4584 - val_loss: 35.1669 - val_MinusLogProbMetric: 35.1669 - lr: 4.1152e-06 - 67s/epoch - 340ms/step
Epoch 875/1000
2023-10-25 05:30:22.895 
Epoch 875/1000 
	 loss: 34.4257, MinusLogProbMetric: 34.4257, val_loss: 35.1387, val_MinusLogProbMetric: 35.1387

Epoch 875: val_loss did not improve from 34.88217
196/196 - 66s - loss: 34.4257 - MinusLogProbMetric: 34.4257 - val_loss: 35.1387 - val_MinusLogProbMetric: 35.1387 - lr: 4.1152e-06 - 66s/epoch - 339ms/step
Epoch 876/1000
2023-10-25 05:31:29.664 
Epoch 876/1000 
	 loss: 34.4680, MinusLogProbMetric: 34.4680, val_loss: 34.9126, val_MinusLogProbMetric: 34.9126

Epoch 876: val_loss did not improve from 34.88217
196/196 - 67s - loss: 34.4680 - MinusLogProbMetric: 34.4680 - val_loss: 34.9126 - val_MinusLogProbMetric: 34.9126 - lr: 4.1152e-06 - 67s/epoch - 341ms/step
Epoch 877/1000
2023-10-25 05:32:37.146 
Epoch 877/1000 
	 loss: 34.4623, MinusLogProbMetric: 34.4623, val_loss: 35.0367, val_MinusLogProbMetric: 35.0367

Epoch 877: val_loss did not improve from 34.88217
196/196 - 67s - loss: 34.4623 - MinusLogProbMetric: 34.4623 - val_loss: 35.0367 - val_MinusLogProbMetric: 35.0367 - lr: 4.1152e-06 - 67s/epoch - 344ms/step
Epoch 878/1000
2023-10-25 05:33:45.283 
Epoch 878/1000 
	 loss: 34.4406, MinusLogProbMetric: 34.4406, val_loss: 35.1265, val_MinusLogProbMetric: 35.1265

Epoch 878: val_loss did not improve from 34.88217
196/196 - 68s - loss: 34.4406 - MinusLogProbMetric: 34.4406 - val_loss: 35.1265 - val_MinusLogProbMetric: 35.1265 - lr: 4.1152e-06 - 68s/epoch - 348ms/step
Epoch 879/1000
2023-10-25 05:34:51.012 
Epoch 879/1000 
	 loss: 34.4413, MinusLogProbMetric: 34.4413, val_loss: 34.8735, val_MinusLogProbMetric: 34.8735

Epoch 879: val_loss improved from 34.88217 to 34.87354, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 67s - loss: 34.4413 - MinusLogProbMetric: 34.4413 - val_loss: 34.8735 - val_MinusLogProbMetric: 34.8735 - lr: 4.1152e-06 - 67s/epoch - 340ms/step
Epoch 880/1000
2023-10-25 05:35:58.680 
Epoch 880/1000 
	 loss: 34.4062, MinusLogProbMetric: 34.4062, val_loss: 35.0598, val_MinusLogProbMetric: 35.0598

Epoch 880: val_loss did not improve from 34.87354
196/196 - 67s - loss: 34.4062 - MinusLogProbMetric: 34.4062 - val_loss: 35.0598 - val_MinusLogProbMetric: 35.0598 - lr: 4.1152e-06 - 67s/epoch - 340ms/step
Epoch 881/1000
2023-10-25 05:37:06.349 
Epoch 881/1000 
	 loss: 35.3090, MinusLogProbMetric: 35.3090, val_loss: 35.1067, val_MinusLogProbMetric: 35.1067

Epoch 881: val_loss did not improve from 34.87354
196/196 - 68s - loss: 35.3090 - MinusLogProbMetric: 35.3090 - val_loss: 35.1067 - val_MinusLogProbMetric: 35.1067 - lr: 4.1152e-06 - 68s/epoch - 345ms/step
Epoch 882/1000
2023-10-25 05:38:14.647 
Epoch 882/1000 
	 loss: 34.4190, MinusLogProbMetric: 34.4190, val_loss: 34.8608, val_MinusLogProbMetric: 34.8608

Epoch 882: val_loss improved from 34.87354 to 34.86079, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 69s - loss: 34.4190 - MinusLogProbMetric: 34.4190 - val_loss: 34.8608 - val_MinusLogProbMetric: 34.8608 - lr: 4.1152e-06 - 69s/epoch - 354ms/step
Epoch 883/1000
2023-10-25 05:39:21.496 
Epoch 883/1000 
	 loss: 34.4181, MinusLogProbMetric: 34.4181, val_loss: 35.0125, val_MinusLogProbMetric: 35.0125

Epoch 883: val_loss did not improve from 34.86079
196/196 - 66s - loss: 34.4181 - MinusLogProbMetric: 34.4181 - val_loss: 35.0125 - val_MinusLogProbMetric: 35.0125 - lr: 4.1152e-06 - 66s/epoch - 336ms/step
Epoch 884/1000
2023-10-25 05:40:27.967 
Epoch 884/1000 
	 loss: 34.4185, MinusLogProbMetric: 34.4185, val_loss: 35.1134, val_MinusLogProbMetric: 35.1134

Epoch 884: val_loss did not improve from 34.86079
196/196 - 66s - loss: 34.4185 - MinusLogProbMetric: 34.4185 - val_loss: 35.1134 - val_MinusLogProbMetric: 35.1134 - lr: 4.1152e-06 - 66s/epoch - 339ms/step
Epoch 885/1000
2023-10-25 05:41:35.347 
Epoch 885/1000 
	 loss: 34.5049, MinusLogProbMetric: 34.5049, val_loss: 35.0630, val_MinusLogProbMetric: 35.0630

Epoch 885: val_loss did not improve from 34.86079
196/196 - 67s - loss: 34.5049 - MinusLogProbMetric: 34.5049 - val_loss: 35.0630 - val_MinusLogProbMetric: 35.0630 - lr: 4.1152e-06 - 67s/epoch - 344ms/step
Epoch 886/1000
2023-10-25 05:42:41.077 
Epoch 886/1000 
	 loss: 34.4333, MinusLogProbMetric: 34.4333, val_loss: 34.9742, val_MinusLogProbMetric: 34.9742

Epoch 886: val_loss did not improve from 34.86079
196/196 - 66s - loss: 34.4333 - MinusLogProbMetric: 34.4333 - val_loss: 34.9742 - val_MinusLogProbMetric: 34.9742 - lr: 4.1152e-06 - 66s/epoch - 335ms/step
Epoch 887/1000
2023-10-25 05:43:46.014 
Epoch 887/1000 
	 loss: 35.5051, MinusLogProbMetric: 35.5051, val_loss: 34.9332, val_MinusLogProbMetric: 34.9332

Epoch 887: val_loss did not improve from 34.86079
196/196 - 65s - loss: 35.5051 - MinusLogProbMetric: 35.5051 - val_loss: 34.9332 - val_MinusLogProbMetric: 34.9332 - lr: 4.1152e-06 - 65s/epoch - 331ms/step
Epoch 888/1000
2023-10-25 05:44:52.060 
Epoch 888/1000 
	 loss: 34.3746, MinusLogProbMetric: 34.3746, val_loss: 34.8857, val_MinusLogProbMetric: 34.8857

Epoch 888: val_loss did not improve from 34.86079
196/196 - 66s - loss: 34.3746 - MinusLogProbMetric: 34.3746 - val_loss: 34.8857 - val_MinusLogProbMetric: 34.8857 - lr: 4.1152e-06 - 66s/epoch - 337ms/step
Epoch 889/1000
2023-10-25 05:45:59.328 
Epoch 889/1000 
	 loss: 34.3971, MinusLogProbMetric: 34.3971, val_loss: 34.8372, val_MinusLogProbMetric: 34.8372

Epoch 889: val_loss improved from 34.86079 to 34.83717, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 68s - loss: 34.3971 - MinusLogProbMetric: 34.3971 - val_loss: 34.8372 - val_MinusLogProbMetric: 34.8372 - lr: 4.1152e-06 - 68s/epoch - 349ms/step
Epoch 890/1000
2023-10-25 05:47:05.896 
Epoch 890/1000 
	 loss: 34.3650, MinusLogProbMetric: 34.3650, val_loss: 34.9800, val_MinusLogProbMetric: 34.9800

Epoch 890: val_loss did not improve from 34.83717
196/196 - 66s - loss: 34.3650 - MinusLogProbMetric: 34.3650 - val_loss: 34.9800 - val_MinusLogProbMetric: 34.9800 - lr: 4.1152e-06 - 66s/epoch - 334ms/step
Epoch 891/1000
2023-10-25 05:48:11.900 
Epoch 891/1000 
	 loss: 34.3584, MinusLogProbMetric: 34.3584, val_loss: 34.9110, val_MinusLogProbMetric: 34.9110

Epoch 891: val_loss did not improve from 34.83717
196/196 - 66s - loss: 34.3584 - MinusLogProbMetric: 34.3584 - val_loss: 34.9110 - val_MinusLogProbMetric: 34.9110 - lr: 4.1152e-06 - 66s/epoch - 337ms/step
Epoch 892/1000
2023-10-25 05:49:19.459 
Epoch 892/1000 
	 loss: 34.3752, MinusLogProbMetric: 34.3752, val_loss: 35.0586, val_MinusLogProbMetric: 35.0586

Epoch 892: val_loss did not improve from 34.83717
196/196 - 68s - loss: 34.3752 - MinusLogProbMetric: 34.3752 - val_loss: 35.0586 - val_MinusLogProbMetric: 35.0586 - lr: 4.1152e-06 - 68s/epoch - 345ms/step
Epoch 893/1000
2023-10-25 05:50:25.415 
Epoch 893/1000 
	 loss: 34.3329, MinusLogProbMetric: 34.3329, val_loss: 34.8425, val_MinusLogProbMetric: 34.8425

Epoch 893: val_loss did not improve from 34.83717
196/196 - 66s - loss: 34.3329 - MinusLogProbMetric: 34.3329 - val_loss: 34.8425 - val_MinusLogProbMetric: 34.8425 - lr: 4.1152e-06 - 66s/epoch - 336ms/step
Epoch 894/1000
2023-10-25 05:51:30.217 
Epoch 894/1000 
	 loss: 34.3043, MinusLogProbMetric: 34.3043, val_loss: 34.8527, val_MinusLogProbMetric: 34.8527

Epoch 894: val_loss did not improve from 34.83717
196/196 - 65s - loss: 34.3043 - MinusLogProbMetric: 34.3043 - val_loss: 34.8527 - val_MinusLogProbMetric: 34.8527 - lr: 4.1152e-06 - 65s/epoch - 331ms/step
Epoch 895/1000
2023-10-25 05:52:36.331 
Epoch 895/1000 
	 loss: 34.2928, MinusLogProbMetric: 34.2928, val_loss: 35.0876, val_MinusLogProbMetric: 35.0876

Epoch 895: val_loss did not improve from 34.83717
196/196 - 66s - loss: 34.2928 - MinusLogProbMetric: 34.2928 - val_loss: 35.0876 - val_MinusLogProbMetric: 35.0876 - lr: 4.1152e-06 - 66s/epoch - 337ms/step
Epoch 896/1000
2023-10-25 05:53:41.597 
Epoch 896/1000 
	 loss: 34.3351, MinusLogProbMetric: 34.3351, val_loss: 34.9812, val_MinusLogProbMetric: 34.9812

Epoch 896: val_loss did not improve from 34.83717
196/196 - 65s - loss: 34.3351 - MinusLogProbMetric: 34.3351 - val_loss: 34.9812 - val_MinusLogProbMetric: 34.9812 - lr: 4.1152e-06 - 65s/epoch - 333ms/step
Epoch 897/1000
2023-10-25 05:54:47.216 
Epoch 897/1000 
	 loss: 34.3009, MinusLogProbMetric: 34.3009, val_loss: 34.7686, val_MinusLogProbMetric: 34.7686

Epoch 897: val_loss improved from 34.83717 to 34.76863, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 67s - loss: 34.3009 - MinusLogProbMetric: 34.3009 - val_loss: 34.7686 - val_MinusLogProbMetric: 34.7686 - lr: 4.1152e-06 - 67s/epoch - 340ms/step
Epoch 898/1000
2023-10-25 05:55:54.282 
Epoch 898/1000 
	 loss: 34.2656, MinusLogProbMetric: 34.2656, val_loss: 34.8301, val_MinusLogProbMetric: 34.8301

Epoch 898: val_loss did not improve from 34.76863
196/196 - 66s - loss: 34.2656 - MinusLogProbMetric: 34.2656 - val_loss: 34.8301 - val_MinusLogProbMetric: 34.8301 - lr: 4.1152e-06 - 66s/epoch - 337ms/step
Epoch 899/1000
2023-10-25 05:56:57.764 
Epoch 899/1000 
	 loss: 34.2624, MinusLogProbMetric: 34.2624, val_loss: 35.0016, val_MinusLogProbMetric: 35.0016

Epoch 899: val_loss did not improve from 34.76863
196/196 - 63s - loss: 34.2624 - MinusLogProbMetric: 34.2624 - val_loss: 35.0016 - val_MinusLogProbMetric: 35.0016 - lr: 4.1152e-06 - 63s/epoch - 324ms/step
Epoch 900/1000
2023-10-25 05:58:02.521 
Epoch 900/1000 
	 loss: 34.3429, MinusLogProbMetric: 34.3429, val_loss: 35.2092, val_MinusLogProbMetric: 35.2092

Epoch 900: val_loss did not improve from 34.76863
196/196 - 65s - loss: 34.3429 - MinusLogProbMetric: 34.3429 - val_loss: 35.2092 - val_MinusLogProbMetric: 35.2092 - lr: 4.1152e-06 - 65s/epoch - 330ms/step
Epoch 901/1000
2023-10-25 05:59:10.674 
Epoch 901/1000 
	 loss: 34.2817, MinusLogProbMetric: 34.2817, val_loss: 34.7519, val_MinusLogProbMetric: 34.7519

Epoch 901: val_loss improved from 34.76863 to 34.75193, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 69s - loss: 34.2817 - MinusLogProbMetric: 34.2817 - val_loss: 34.7519 - val_MinusLogProbMetric: 34.7519 - lr: 4.1152e-06 - 69s/epoch - 353ms/step
Epoch 902/1000
2023-10-25 06:00:18.913 
Epoch 902/1000 
	 loss: 34.2973, MinusLogProbMetric: 34.2973, val_loss: 35.0849, val_MinusLogProbMetric: 35.0849

Epoch 902: val_loss did not improve from 34.75193
196/196 - 67s - loss: 34.2973 - MinusLogProbMetric: 34.2973 - val_loss: 35.0849 - val_MinusLogProbMetric: 35.0849 - lr: 4.1152e-06 - 67s/epoch - 343ms/step
Epoch 903/1000
2023-10-25 06:01:25.173 
Epoch 903/1000 
	 loss: 34.2499, MinusLogProbMetric: 34.2499, val_loss: 34.9030, val_MinusLogProbMetric: 34.9030

Epoch 903: val_loss did not improve from 34.75193
196/196 - 66s - loss: 34.2499 - MinusLogProbMetric: 34.2499 - val_loss: 34.9030 - val_MinusLogProbMetric: 34.9030 - lr: 4.1152e-06 - 66s/epoch - 338ms/step
Epoch 904/1000
2023-10-25 06:02:31.995 
Epoch 904/1000 
	 loss: 34.4958, MinusLogProbMetric: 34.4958, val_loss: 35.0112, val_MinusLogProbMetric: 35.0112

Epoch 904: val_loss did not improve from 34.75193
196/196 - 67s - loss: 34.4958 - MinusLogProbMetric: 34.4958 - val_loss: 35.0112 - val_MinusLogProbMetric: 35.0112 - lr: 4.1152e-06 - 67s/epoch - 341ms/step
Epoch 905/1000
2023-10-25 06:03:36.877 
Epoch 905/1000 
	 loss: 34.2579, MinusLogProbMetric: 34.2579, val_loss: 34.7740, val_MinusLogProbMetric: 34.7740

Epoch 905: val_loss did not improve from 34.75193
196/196 - 65s - loss: 34.2579 - MinusLogProbMetric: 34.2579 - val_loss: 34.7740 - val_MinusLogProbMetric: 34.7740 - lr: 4.1152e-06 - 65s/epoch - 331ms/step
Epoch 906/1000
2023-10-25 06:04:43.383 
Epoch 906/1000 
	 loss: 34.2235, MinusLogProbMetric: 34.2235, val_loss: 34.6850, val_MinusLogProbMetric: 34.6850

Epoch 906: val_loss improved from 34.75193 to 34.68496, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 68s - loss: 34.2235 - MinusLogProbMetric: 34.2235 - val_loss: 34.6850 - val_MinusLogProbMetric: 34.6850 - lr: 4.1152e-06 - 68s/epoch - 345ms/step
Epoch 907/1000
2023-10-25 06:05:51.478 
Epoch 907/1000 
	 loss: 34.2386, MinusLogProbMetric: 34.2386, val_loss: 34.9462, val_MinusLogProbMetric: 34.9462

Epoch 907: val_loss did not improve from 34.68496
196/196 - 67s - loss: 34.2386 - MinusLogProbMetric: 34.2386 - val_loss: 34.9462 - val_MinusLogProbMetric: 34.9462 - lr: 4.1152e-06 - 67s/epoch - 341ms/step
Epoch 908/1000
2023-10-25 06:06:57.335 
Epoch 908/1000 
	 loss: 34.2314, MinusLogProbMetric: 34.2314, val_loss: 34.7828, val_MinusLogProbMetric: 34.7828

Epoch 908: val_loss did not improve from 34.68496
196/196 - 66s - loss: 34.2314 - MinusLogProbMetric: 34.2314 - val_loss: 34.7828 - val_MinusLogProbMetric: 34.7828 - lr: 4.1152e-06 - 66s/epoch - 336ms/step
Epoch 909/1000
2023-10-25 06:08:04.358 
Epoch 909/1000 
	 loss: 34.2389, MinusLogProbMetric: 34.2389, val_loss: 34.8741, val_MinusLogProbMetric: 34.8741

Epoch 909: val_loss did not improve from 34.68496
196/196 - 67s - loss: 34.2389 - MinusLogProbMetric: 34.2389 - val_loss: 34.8741 - val_MinusLogProbMetric: 34.8741 - lr: 4.1152e-06 - 67s/epoch - 342ms/step
Epoch 910/1000
2023-10-25 06:09:08.486 
Epoch 910/1000 
	 loss: 34.2657, MinusLogProbMetric: 34.2657, val_loss: 34.8941, val_MinusLogProbMetric: 34.8941

Epoch 910: val_loss did not improve from 34.68496
196/196 - 64s - loss: 34.2657 - MinusLogProbMetric: 34.2657 - val_loss: 34.8941 - val_MinusLogProbMetric: 34.8941 - lr: 4.1152e-06 - 64s/epoch - 327ms/step
Epoch 911/1000
2023-10-25 06:10:16.734 
Epoch 911/1000 
	 loss: 34.2379, MinusLogProbMetric: 34.2379, val_loss: 34.8738, val_MinusLogProbMetric: 34.8738

Epoch 911: val_loss did not improve from 34.68496
196/196 - 68s - loss: 34.2379 - MinusLogProbMetric: 34.2379 - val_loss: 34.8738 - val_MinusLogProbMetric: 34.8738 - lr: 4.1152e-06 - 68s/epoch - 348ms/step
Epoch 912/1000
2023-10-25 06:11:23.534 
Epoch 912/1000 
	 loss: 34.1878, MinusLogProbMetric: 34.1878, val_loss: 35.0704, val_MinusLogProbMetric: 35.0704

Epoch 912: val_loss did not improve from 34.68496
196/196 - 67s - loss: 34.1878 - MinusLogProbMetric: 34.1878 - val_loss: 35.0704 - val_MinusLogProbMetric: 35.0704 - lr: 4.1152e-06 - 67s/epoch - 341ms/step
Epoch 913/1000
2023-10-25 06:12:26.926 
Epoch 913/1000 
	 loss: 34.1987, MinusLogProbMetric: 34.1987, val_loss: 34.8143, val_MinusLogProbMetric: 34.8143

Epoch 913: val_loss did not improve from 34.68496
196/196 - 63s - loss: 34.1987 - MinusLogProbMetric: 34.1987 - val_loss: 34.8143 - val_MinusLogProbMetric: 34.8143 - lr: 4.1152e-06 - 63s/epoch - 323ms/step
Epoch 914/1000
2023-10-25 06:13:33.661 
Epoch 914/1000 
	 loss: 34.3088, MinusLogProbMetric: 34.3088, val_loss: 35.1807, val_MinusLogProbMetric: 35.1807

Epoch 914: val_loss did not improve from 34.68496
196/196 - 67s - loss: 34.3088 - MinusLogProbMetric: 34.3088 - val_loss: 35.1807 - val_MinusLogProbMetric: 35.1807 - lr: 4.1152e-06 - 67s/epoch - 340ms/step
Epoch 915/1000
2023-10-25 06:14:41.646 
Epoch 915/1000 
	 loss: 34.1795, MinusLogProbMetric: 34.1795, val_loss: 34.6087, val_MinusLogProbMetric: 34.6087

Epoch 915: val_loss improved from 34.68496 to 34.60875, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 69s - loss: 34.1795 - MinusLogProbMetric: 34.1795 - val_loss: 34.6087 - val_MinusLogProbMetric: 34.6087 - lr: 4.1152e-06 - 69s/epoch - 352ms/step
Epoch 916/1000
2023-10-25 06:15:50.058 
Epoch 916/1000 
	 loss: 34.1516, MinusLogProbMetric: 34.1516, val_loss: 34.8659, val_MinusLogProbMetric: 34.8659

Epoch 916: val_loss did not improve from 34.60875
196/196 - 67s - loss: 34.1516 - MinusLogProbMetric: 34.1516 - val_loss: 34.8659 - val_MinusLogProbMetric: 34.8659 - lr: 4.1152e-06 - 67s/epoch - 344ms/step
Epoch 917/1000
2023-10-25 06:16:58.133 
Epoch 917/1000 
	 loss: 34.1983, MinusLogProbMetric: 34.1983, val_loss: 35.0009, val_MinusLogProbMetric: 35.0009

Epoch 917: val_loss did not improve from 34.60875
196/196 - 68s - loss: 34.1983 - MinusLogProbMetric: 34.1983 - val_loss: 35.0009 - val_MinusLogProbMetric: 35.0009 - lr: 4.1152e-06 - 68s/epoch - 347ms/step
Epoch 918/1000
2023-10-25 06:18:05.987 
Epoch 918/1000 
	 loss: 34.1546, MinusLogProbMetric: 34.1546, val_loss: 34.6002, val_MinusLogProbMetric: 34.6002

Epoch 918: val_loss improved from 34.60875 to 34.60024, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 69s - loss: 34.1546 - MinusLogProbMetric: 34.1546 - val_loss: 34.6002 - val_MinusLogProbMetric: 34.6002 - lr: 4.1152e-06 - 69s/epoch - 351ms/step
Epoch 919/1000
2023-10-25 06:19:13.814 
Epoch 919/1000 
	 loss: 34.2754, MinusLogProbMetric: 34.2754, val_loss: 34.7092, val_MinusLogProbMetric: 34.7092

Epoch 919: val_loss did not improve from 34.60024
196/196 - 67s - loss: 34.2754 - MinusLogProbMetric: 34.2754 - val_loss: 34.7092 - val_MinusLogProbMetric: 34.7092 - lr: 4.1152e-06 - 67s/epoch - 341ms/step
Epoch 920/1000
2023-10-25 06:20:22.083 
Epoch 920/1000 
	 loss: 34.1684, MinusLogProbMetric: 34.1684, val_loss: 34.7227, val_MinusLogProbMetric: 34.7227

Epoch 920: val_loss did not improve from 34.60024
196/196 - 68s - loss: 34.1684 - MinusLogProbMetric: 34.1684 - val_loss: 34.7227 - val_MinusLogProbMetric: 34.7227 - lr: 4.1152e-06 - 68s/epoch - 348ms/step
Epoch 921/1000
2023-10-25 06:21:27.964 
Epoch 921/1000 
	 loss: 34.1247, MinusLogProbMetric: 34.1247, val_loss: 34.8126, val_MinusLogProbMetric: 34.8126

Epoch 921: val_loss did not improve from 34.60024
196/196 - 66s - loss: 34.1247 - MinusLogProbMetric: 34.1247 - val_loss: 34.8126 - val_MinusLogProbMetric: 34.8126 - lr: 4.1152e-06 - 66s/epoch - 336ms/step
Epoch 922/1000
2023-10-25 06:22:35.222 
Epoch 922/1000 
	 loss: 34.1360, MinusLogProbMetric: 34.1360, val_loss: 34.8616, val_MinusLogProbMetric: 34.8616

Epoch 922: val_loss did not improve from 34.60024
196/196 - 67s - loss: 34.1360 - MinusLogProbMetric: 34.1360 - val_loss: 34.8616 - val_MinusLogProbMetric: 34.8616 - lr: 4.1152e-06 - 67s/epoch - 343ms/step
Epoch 923/1000
2023-10-25 06:23:41.558 
Epoch 923/1000 
	 loss: 34.1673, MinusLogProbMetric: 34.1673, val_loss: 34.5455, val_MinusLogProbMetric: 34.5455

Epoch 923: val_loss improved from 34.60024 to 34.54552, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 67s - loss: 34.1673 - MinusLogProbMetric: 34.1673 - val_loss: 34.5455 - val_MinusLogProbMetric: 34.5455 - lr: 4.1152e-06 - 67s/epoch - 342ms/step
Epoch 924/1000
2023-10-25 06:24:48.294 
Epoch 924/1000 
	 loss: 34.0886, MinusLogProbMetric: 34.0886, val_loss: 34.6507, val_MinusLogProbMetric: 34.6507

Epoch 924: val_loss did not improve from 34.54552
196/196 - 66s - loss: 34.0886 - MinusLogProbMetric: 34.0886 - val_loss: 34.6507 - val_MinusLogProbMetric: 34.6507 - lr: 4.1152e-06 - 66s/epoch - 337ms/step
Epoch 925/1000
2023-10-25 06:25:56.709 
Epoch 925/1000 
	 loss: 34.1195, MinusLogProbMetric: 34.1195, val_loss: 34.7867, val_MinusLogProbMetric: 34.7867

Epoch 925: val_loss did not improve from 34.54552
196/196 - 68s - loss: 34.1195 - MinusLogProbMetric: 34.1195 - val_loss: 34.7867 - val_MinusLogProbMetric: 34.7867 - lr: 4.1152e-06 - 68s/epoch - 349ms/step
Epoch 926/1000
2023-10-25 06:27:05.163 
Epoch 926/1000 
	 loss: 34.1323, MinusLogProbMetric: 34.1323, val_loss: 34.7523, val_MinusLogProbMetric: 34.7523

Epoch 926: val_loss did not improve from 34.54552
196/196 - 68s - loss: 34.1323 - MinusLogProbMetric: 34.1323 - val_loss: 34.7523 - val_MinusLogProbMetric: 34.7523 - lr: 4.1152e-06 - 68s/epoch - 349ms/step
Epoch 927/1000
2023-10-25 06:28:09.178 
Epoch 927/1000 
	 loss: 34.1607, MinusLogProbMetric: 34.1607, val_loss: 34.7808, val_MinusLogProbMetric: 34.7808

Epoch 927: val_loss did not improve from 34.54552
196/196 - 64s - loss: 34.1607 - MinusLogProbMetric: 34.1607 - val_loss: 34.7808 - val_MinusLogProbMetric: 34.7808 - lr: 4.1152e-06 - 64s/epoch - 327ms/step
Epoch 928/1000
2023-10-25 06:29:15.757 
Epoch 928/1000 
	 loss: 34.1412, MinusLogProbMetric: 34.1412, val_loss: 34.6613, val_MinusLogProbMetric: 34.6613

Epoch 928: val_loss did not improve from 34.54552
196/196 - 67s - loss: 34.1412 - MinusLogProbMetric: 34.1412 - val_loss: 34.6613 - val_MinusLogProbMetric: 34.6613 - lr: 4.1152e-06 - 67s/epoch - 340ms/step
Epoch 929/1000
2023-10-25 06:30:22.112 
Epoch 929/1000 
	 loss: 36.6260, MinusLogProbMetric: 36.6260, val_loss: 34.7725, val_MinusLogProbMetric: 34.7725

Epoch 929: val_loss did not improve from 34.54552
196/196 - 66s - loss: 36.6260 - MinusLogProbMetric: 36.6260 - val_loss: 34.7725 - val_MinusLogProbMetric: 34.7725 - lr: 4.1152e-06 - 66s/epoch - 339ms/step
Epoch 930/1000
2023-10-25 06:31:30.698 
Epoch 930/1000 
	 loss: 34.1707, MinusLogProbMetric: 34.1707, val_loss: 34.6648, val_MinusLogProbMetric: 34.6648

Epoch 930: val_loss did not improve from 34.54552
196/196 - 69s - loss: 34.1707 - MinusLogProbMetric: 34.1707 - val_loss: 34.6648 - val_MinusLogProbMetric: 34.6648 - lr: 4.1152e-06 - 69s/epoch - 350ms/step
Epoch 931/1000
2023-10-25 06:32:37.015 
Epoch 931/1000 
	 loss: 34.1249, MinusLogProbMetric: 34.1249, val_loss: 34.8116, val_MinusLogProbMetric: 34.8116

Epoch 931: val_loss did not improve from 34.54552
196/196 - 66s - loss: 34.1249 - MinusLogProbMetric: 34.1249 - val_loss: 34.8116 - val_MinusLogProbMetric: 34.8116 - lr: 4.1152e-06 - 66s/epoch - 338ms/step
Epoch 932/1000
2023-10-25 06:33:42.801 
Epoch 932/1000 
	 loss: 34.0727, MinusLogProbMetric: 34.0727, val_loss: 34.6319, val_MinusLogProbMetric: 34.6319

Epoch 932: val_loss did not improve from 34.54552
196/196 - 66s - loss: 34.0727 - MinusLogProbMetric: 34.0727 - val_loss: 34.6319 - val_MinusLogProbMetric: 34.6319 - lr: 4.1152e-06 - 66s/epoch - 336ms/step
Epoch 933/1000
2023-10-25 06:34:48.514 
Epoch 933/1000 
	 loss: 34.1240, MinusLogProbMetric: 34.1240, val_loss: 34.7494, val_MinusLogProbMetric: 34.7494

Epoch 933: val_loss did not improve from 34.54552
196/196 - 66s - loss: 34.1240 - MinusLogProbMetric: 34.1240 - val_loss: 34.7494 - val_MinusLogProbMetric: 34.7494 - lr: 4.1152e-06 - 66s/epoch - 335ms/step
Epoch 934/1000
2023-10-25 06:35:55.883 
Epoch 934/1000 
	 loss: 34.4875, MinusLogProbMetric: 34.4875, val_loss: 34.7087, val_MinusLogProbMetric: 34.7087

Epoch 934: val_loss did not improve from 34.54552
196/196 - 67s - loss: 34.4875 - MinusLogProbMetric: 34.4875 - val_loss: 34.7087 - val_MinusLogProbMetric: 34.7087 - lr: 4.1152e-06 - 67s/epoch - 344ms/step
Epoch 935/1000
2023-10-25 06:37:02.737 
Epoch 935/1000 
	 loss: 34.1140, MinusLogProbMetric: 34.1140, val_loss: 34.5547, val_MinusLogProbMetric: 34.5547

Epoch 935: val_loss did not improve from 34.54552
196/196 - 67s - loss: 34.1140 - MinusLogProbMetric: 34.1140 - val_loss: 34.5547 - val_MinusLogProbMetric: 34.5547 - lr: 4.1152e-06 - 67s/epoch - 341ms/step
Epoch 936/1000
2023-10-25 06:38:10.510 
Epoch 936/1000 
	 loss: 34.0633, MinusLogProbMetric: 34.0633, val_loss: 34.6105, val_MinusLogProbMetric: 34.6105

Epoch 936: val_loss did not improve from 34.54552
196/196 - 68s - loss: 34.0633 - MinusLogProbMetric: 34.0633 - val_loss: 34.6105 - val_MinusLogProbMetric: 34.6105 - lr: 4.1152e-06 - 68s/epoch - 346ms/step
Epoch 937/1000
2023-10-25 06:39:16.114 
Epoch 937/1000 
	 loss: 34.0476, MinusLogProbMetric: 34.0476, val_loss: 34.5606, val_MinusLogProbMetric: 34.5606

Epoch 937: val_loss did not improve from 34.54552
196/196 - 66s - loss: 34.0476 - MinusLogProbMetric: 34.0476 - val_loss: 34.5606 - val_MinusLogProbMetric: 34.5606 - lr: 4.1152e-06 - 66s/epoch - 335ms/step
Epoch 938/1000
2023-10-25 06:40:24.463 
Epoch 938/1000 
	 loss: 34.0557, MinusLogProbMetric: 34.0557, val_loss: 34.4977, val_MinusLogProbMetric: 34.4977

Epoch 938: val_loss improved from 34.54552 to 34.49768, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 69s - loss: 34.0557 - MinusLogProbMetric: 34.0557 - val_loss: 34.4977 - val_MinusLogProbMetric: 34.4977 - lr: 4.1152e-06 - 69s/epoch - 354ms/step
Epoch 939/1000
2023-10-25 06:41:32.118 
Epoch 939/1000 
	 loss: 34.1697, MinusLogProbMetric: 34.1697, val_loss: 34.6141, val_MinusLogProbMetric: 34.6141

Epoch 939: val_loss did not improve from 34.49768
196/196 - 67s - loss: 34.1697 - MinusLogProbMetric: 34.1697 - val_loss: 34.6141 - val_MinusLogProbMetric: 34.6141 - lr: 4.1152e-06 - 67s/epoch - 340ms/step
Epoch 940/1000
2023-10-25 06:42:38.763 
Epoch 940/1000 
	 loss: 34.0784, MinusLogProbMetric: 34.0784, val_loss: 34.7470, val_MinusLogProbMetric: 34.7470

Epoch 940: val_loss did not improve from 34.49768
196/196 - 67s - loss: 34.0784 - MinusLogProbMetric: 34.0784 - val_loss: 34.7470 - val_MinusLogProbMetric: 34.7470 - lr: 4.1152e-06 - 67s/epoch - 340ms/step
Epoch 941/1000
2023-10-25 06:43:48.203 
Epoch 941/1000 
	 loss: 34.0314, MinusLogProbMetric: 34.0314, val_loss: 34.8458, val_MinusLogProbMetric: 34.8458

Epoch 941: val_loss did not improve from 34.49768
196/196 - 69s - loss: 34.0314 - MinusLogProbMetric: 34.0314 - val_loss: 34.8458 - val_MinusLogProbMetric: 34.8458 - lr: 4.1152e-06 - 69s/epoch - 354ms/step
Epoch 942/1000
2023-10-25 06:44:55.176 
Epoch 942/1000 
	 loss: 34.0495, MinusLogProbMetric: 34.0495, val_loss: 34.5885, val_MinusLogProbMetric: 34.5885

Epoch 942: val_loss did not improve from 34.49768
196/196 - 67s - loss: 34.0495 - MinusLogProbMetric: 34.0495 - val_loss: 34.5885 - val_MinusLogProbMetric: 34.5885 - lr: 4.1152e-06 - 67s/epoch - 342ms/step
Epoch 943/1000
2023-10-25 06:46:01.969 
Epoch 943/1000 
	 loss: 34.0063, MinusLogProbMetric: 34.0063, val_loss: 34.4489, val_MinusLogProbMetric: 34.4489

Epoch 943: val_loss improved from 34.49768 to 34.44889, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 68s - loss: 34.0063 - MinusLogProbMetric: 34.0063 - val_loss: 34.4489 - val_MinusLogProbMetric: 34.4489 - lr: 4.1152e-06 - 68s/epoch - 346ms/step
Epoch 944/1000
2023-10-25 06:47:08.906 
Epoch 944/1000 
	 loss: 34.0124, MinusLogProbMetric: 34.0124, val_loss: 34.6393, val_MinusLogProbMetric: 34.6393

Epoch 944: val_loss did not improve from 34.44889
196/196 - 66s - loss: 34.0124 - MinusLogProbMetric: 34.0124 - val_loss: 34.6393 - val_MinusLogProbMetric: 34.6393 - lr: 4.1152e-06 - 66s/epoch - 336ms/step
Epoch 945/1000
2023-10-25 06:48:15.781 
Epoch 945/1000 
	 loss: 34.2078, MinusLogProbMetric: 34.2078, val_loss: 34.7439, val_MinusLogProbMetric: 34.7439

Epoch 945: val_loss did not improve from 34.44889
196/196 - 67s - loss: 34.2078 - MinusLogProbMetric: 34.2078 - val_loss: 34.7439 - val_MinusLogProbMetric: 34.7439 - lr: 4.1152e-06 - 67s/epoch - 341ms/step
Epoch 946/1000
2023-10-25 06:49:22.255 
Epoch 946/1000 
	 loss: 34.0371, MinusLogProbMetric: 34.0371, val_loss: 34.6999, val_MinusLogProbMetric: 34.6999

Epoch 946: val_loss did not improve from 34.44889
196/196 - 66s - loss: 34.0371 - MinusLogProbMetric: 34.0371 - val_loss: 34.6999 - val_MinusLogProbMetric: 34.6999 - lr: 4.1152e-06 - 66s/epoch - 339ms/step
Epoch 947/1000
2023-10-25 06:50:27.270 
Epoch 947/1000 
	 loss: 34.0144, MinusLogProbMetric: 34.0144, val_loss: 34.5226, val_MinusLogProbMetric: 34.5226

Epoch 947: val_loss did not improve from 34.44889
196/196 - 65s - loss: 34.0144 - MinusLogProbMetric: 34.0144 - val_loss: 34.5226 - val_MinusLogProbMetric: 34.5226 - lr: 4.1152e-06 - 65s/epoch - 332ms/step
Epoch 948/1000
2023-10-25 06:51:35.293 
Epoch 948/1000 
	 loss: 33.9885, MinusLogProbMetric: 33.9885, val_loss: 34.5981, val_MinusLogProbMetric: 34.5981

Epoch 948: val_loss did not improve from 34.44889
196/196 - 68s - loss: 33.9885 - MinusLogProbMetric: 33.9885 - val_loss: 34.5981 - val_MinusLogProbMetric: 34.5981 - lr: 4.1152e-06 - 68s/epoch - 347ms/step
Epoch 949/1000
2023-10-25 06:52:44.410 
Epoch 949/1000 
	 loss: 33.9835, MinusLogProbMetric: 33.9835, val_loss: 34.3303, val_MinusLogProbMetric: 34.3303

Epoch 949: val_loss improved from 34.44889 to 34.33028, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 70s - loss: 33.9835 - MinusLogProbMetric: 33.9835 - val_loss: 34.3303 - val_MinusLogProbMetric: 34.3303 - lr: 4.1152e-06 - 70s/epoch - 358ms/step
Epoch 950/1000
2023-10-25 06:53:51.641 
Epoch 950/1000 
	 loss: 33.9751, MinusLogProbMetric: 33.9751, val_loss: 34.7106, val_MinusLogProbMetric: 34.7106

Epoch 950: val_loss did not improve from 34.33028
196/196 - 66s - loss: 33.9751 - MinusLogProbMetric: 33.9751 - val_loss: 34.7106 - val_MinusLogProbMetric: 34.7106 - lr: 4.1152e-06 - 66s/epoch - 338ms/step
Epoch 951/1000
2023-10-25 06:54:57.859 
Epoch 951/1000 
	 loss: 34.0570, MinusLogProbMetric: 34.0570, val_loss: 34.4561, val_MinusLogProbMetric: 34.4561

Epoch 951: val_loss did not improve from 34.33028
196/196 - 66s - loss: 34.0570 - MinusLogProbMetric: 34.0570 - val_loss: 34.4561 - val_MinusLogProbMetric: 34.4561 - lr: 4.1152e-06 - 66s/epoch - 338ms/step
Epoch 952/1000
2023-10-25 06:56:03.209 
Epoch 952/1000 
	 loss: 33.9948, MinusLogProbMetric: 33.9948, val_loss: 34.7698, val_MinusLogProbMetric: 34.7698

Epoch 952: val_loss did not improve from 34.33028
196/196 - 65s - loss: 33.9948 - MinusLogProbMetric: 33.9948 - val_loss: 34.7698 - val_MinusLogProbMetric: 34.7698 - lr: 4.1152e-06 - 65s/epoch - 333ms/step
Epoch 953/1000
2023-10-25 06:57:10.331 
Epoch 953/1000 
	 loss: 34.0105, MinusLogProbMetric: 34.0105, val_loss: 34.5179, val_MinusLogProbMetric: 34.5179

Epoch 953: val_loss did not improve from 34.33028
196/196 - 67s - loss: 34.0105 - MinusLogProbMetric: 34.0105 - val_loss: 34.5179 - val_MinusLogProbMetric: 34.5179 - lr: 4.1152e-06 - 67s/epoch - 342ms/step
Epoch 954/1000
2023-10-25 06:58:16.284 
Epoch 954/1000 
	 loss: 33.9386, MinusLogProbMetric: 33.9386, val_loss: 34.5146, val_MinusLogProbMetric: 34.5146

Epoch 954: val_loss did not improve from 34.33028
196/196 - 66s - loss: 33.9386 - MinusLogProbMetric: 33.9386 - val_loss: 34.5146 - val_MinusLogProbMetric: 34.5146 - lr: 4.1152e-06 - 66s/epoch - 336ms/step
Epoch 955/1000
2023-10-25 06:59:23.037 
Epoch 955/1000 
	 loss: 34.0624, MinusLogProbMetric: 34.0624, val_loss: 34.4507, val_MinusLogProbMetric: 34.4507

Epoch 955: val_loss did not improve from 34.33028
196/196 - 67s - loss: 34.0624 - MinusLogProbMetric: 34.0624 - val_loss: 34.4507 - val_MinusLogProbMetric: 34.4507 - lr: 4.1152e-06 - 67s/epoch - 341ms/step
Epoch 956/1000
2023-10-25 07:00:29.097 
Epoch 956/1000 
	 loss: 34.3944, MinusLogProbMetric: 34.3944, val_loss: 37.8570, val_MinusLogProbMetric: 37.8570

Epoch 956: val_loss did not improve from 34.33028
196/196 - 66s - loss: 34.3944 - MinusLogProbMetric: 34.3944 - val_loss: 37.8570 - val_MinusLogProbMetric: 37.8570 - lr: 4.1152e-06 - 66s/epoch - 337ms/step
Epoch 957/1000
2023-10-25 07:01:36.537 
Epoch 957/1000 
	 loss: 34.2212, MinusLogProbMetric: 34.2212, val_loss: 34.5192, val_MinusLogProbMetric: 34.5192

Epoch 957: val_loss did not improve from 34.33028
196/196 - 67s - loss: 34.2212 - MinusLogProbMetric: 34.2212 - val_loss: 34.5192 - val_MinusLogProbMetric: 34.5192 - lr: 4.1152e-06 - 67s/epoch - 344ms/step
Epoch 958/1000
2023-10-25 07:02:43.335 
Epoch 958/1000 
	 loss: 33.9189, MinusLogProbMetric: 33.9189, val_loss: 34.4960, val_MinusLogProbMetric: 34.4960

Epoch 958: val_loss did not improve from 34.33028
196/196 - 67s - loss: 33.9189 - MinusLogProbMetric: 33.9189 - val_loss: 34.4960 - val_MinusLogProbMetric: 34.4960 - lr: 4.1152e-06 - 67s/epoch - 341ms/step
Epoch 959/1000
2023-10-25 07:03:48.800 
Epoch 959/1000 
	 loss: 33.9321, MinusLogProbMetric: 33.9321, val_loss: 34.4953, val_MinusLogProbMetric: 34.4953

Epoch 959: val_loss did not improve from 34.33028
196/196 - 65s - loss: 33.9321 - MinusLogProbMetric: 33.9321 - val_loss: 34.4953 - val_MinusLogProbMetric: 34.4953 - lr: 4.1152e-06 - 65s/epoch - 334ms/step
Epoch 960/1000
2023-10-25 07:04:57.170 
Epoch 960/1000 
	 loss: 33.9161, MinusLogProbMetric: 33.9161, val_loss: 34.3778, val_MinusLogProbMetric: 34.3778

Epoch 960: val_loss did not improve from 34.33028
196/196 - 68s - loss: 33.9161 - MinusLogProbMetric: 33.9161 - val_loss: 34.3778 - val_MinusLogProbMetric: 34.3778 - lr: 4.1152e-06 - 68s/epoch - 349ms/step
Epoch 961/1000
2023-10-25 07:06:03.306 
Epoch 961/1000 
	 loss: 33.8713, MinusLogProbMetric: 33.8713, val_loss: 34.4926, val_MinusLogProbMetric: 34.4926

Epoch 961: val_loss did not improve from 34.33028
196/196 - 66s - loss: 33.8713 - MinusLogProbMetric: 33.8713 - val_loss: 34.4926 - val_MinusLogProbMetric: 34.4926 - lr: 4.1152e-06 - 66s/epoch - 337ms/step
Epoch 962/1000
2023-10-25 07:07:08.931 
Epoch 962/1000 
	 loss: 33.8918, MinusLogProbMetric: 33.8918, val_loss: 34.5264, val_MinusLogProbMetric: 34.5264

Epoch 962: val_loss did not improve from 34.33028
196/196 - 66s - loss: 33.8918 - MinusLogProbMetric: 33.8918 - val_loss: 34.5264 - val_MinusLogProbMetric: 34.5264 - lr: 4.1152e-06 - 66s/epoch - 335ms/step
Epoch 963/1000
2023-10-25 07:08:15.958 
Epoch 963/1000 
	 loss: 33.8742, MinusLogProbMetric: 33.8742, val_loss: 34.3271, val_MinusLogProbMetric: 34.3271

Epoch 963: val_loss improved from 34.33028 to 34.32708, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 68s - loss: 33.8742 - MinusLogProbMetric: 33.8742 - val_loss: 34.3271 - val_MinusLogProbMetric: 34.3271 - lr: 4.1152e-06 - 68s/epoch - 347ms/step
Epoch 964/1000
2023-10-25 07:09:21.238 
Epoch 964/1000 
	 loss: 33.9464, MinusLogProbMetric: 33.9464, val_loss: 34.5247, val_MinusLogProbMetric: 34.5247

Epoch 964: val_loss did not improve from 34.32708
196/196 - 64s - loss: 33.9464 - MinusLogProbMetric: 33.9464 - val_loss: 34.5247 - val_MinusLogProbMetric: 34.5247 - lr: 4.1152e-06 - 64s/epoch - 328ms/step
Epoch 965/1000
2023-10-25 07:10:28.782 
Epoch 965/1000 
	 loss: 34.1716, MinusLogProbMetric: 34.1716, val_loss: 34.9092, val_MinusLogProbMetric: 34.9092

Epoch 965: val_loss did not improve from 34.32708
196/196 - 68s - loss: 34.1716 - MinusLogProbMetric: 34.1716 - val_loss: 34.9092 - val_MinusLogProbMetric: 34.9092 - lr: 4.1152e-06 - 68s/epoch - 345ms/step
Epoch 966/1000
2023-10-25 07:11:36.178 
Epoch 966/1000 
	 loss: 33.9090, MinusLogProbMetric: 33.9090, val_loss: 34.5864, val_MinusLogProbMetric: 34.5864

Epoch 966: val_loss did not improve from 34.32708
196/196 - 67s - loss: 33.9090 - MinusLogProbMetric: 33.9090 - val_loss: 34.5864 - val_MinusLogProbMetric: 34.5864 - lr: 4.1152e-06 - 67s/epoch - 344ms/step
Epoch 967/1000
2023-10-25 07:12:43.652 
Epoch 967/1000 
	 loss: 33.8701, MinusLogProbMetric: 33.8701, val_loss: 34.3218, val_MinusLogProbMetric: 34.3218

Epoch 967: val_loss improved from 34.32708 to 34.32179, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 69s - loss: 33.8701 - MinusLogProbMetric: 33.8701 - val_loss: 34.3218 - val_MinusLogProbMetric: 34.3218 - lr: 4.1152e-06 - 69s/epoch - 350ms/step
Epoch 968/1000
2023-10-25 07:13:49.440 
Epoch 968/1000 
	 loss: 33.8938, MinusLogProbMetric: 33.8938, val_loss: 34.4325, val_MinusLogProbMetric: 34.4325

Epoch 968: val_loss did not improve from 34.32179
196/196 - 65s - loss: 33.8938 - MinusLogProbMetric: 33.8938 - val_loss: 34.4325 - val_MinusLogProbMetric: 34.4325 - lr: 4.1152e-06 - 65s/epoch - 330ms/step
Epoch 969/1000
2023-10-25 07:14:55.976 
Epoch 969/1000 
	 loss: 33.8478, MinusLogProbMetric: 33.8478, val_loss: 34.4248, val_MinusLogProbMetric: 34.4248

Epoch 969: val_loss did not improve from 34.32179
196/196 - 67s - loss: 33.8478 - MinusLogProbMetric: 33.8478 - val_loss: 34.4248 - val_MinusLogProbMetric: 34.4248 - lr: 4.1152e-06 - 67s/epoch - 339ms/step
Epoch 970/1000
2023-10-25 07:16:02.716 
Epoch 970/1000 
	 loss: 33.8490, MinusLogProbMetric: 33.8490, val_loss: 34.4946, val_MinusLogProbMetric: 34.4946

Epoch 970: val_loss did not improve from 34.32179
196/196 - 67s - loss: 33.8490 - MinusLogProbMetric: 33.8490 - val_loss: 34.4946 - val_MinusLogProbMetric: 34.4946 - lr: 4.1152e-06 - 67s/epoch - 340ms/step
Epoch 971/1000
2023-10-25 07:17:08.298 
Epoch 971/1000 
	 loss: 33.8318, MinusLogProbMetric: 33.8318, val_loss: 34.4856, val_MinusLogProbMetric: 34.4856

Epoch 971: val_loss did not improve from 34.32179
196/196 - 66s - loss: 33.8318 - MinusLogProbMetric: 33.8318 - val_loss: 34.4856 - val_MinusLogProbMetric: 34.4856 - lr: 4.1152e-06 - 66s/epoch - 335ms/step
Epoch 972/1000
2023-10-25 07:18:14.474 
Epoch 972/1000 
	 loss: 33.9497, MinusLogProbMetric: 33.9497, val_loss: 34.3799, val_MinusLogProbMetric: 34.3799

Epoch 972: val_loss did not improve from 34.32179
196/196 - 66s - loss: 33.9497 - MinusLogProbMetric: 33.9497 - val_loss: 34.3799 - val_MinusLogProbMetric: 34.3799 - lr: 4.1152e-06 - 66s/epoch - 338ms/step
Epoch 973/1000
2023-10-25 07:19:22.745 
Epoch 973/1000 
	 loss: 33.8500, MinusLogProbMetric: 33.8500, val_loss: 34.3964, val_MinusLogProbMetric: 34.3964

Epoch 973: val_loss did not improve from 34.32179
196/196 - 68s - loss: 33.8500 - MinusLogProbMetric: 33.8500 - val_loss: 34.3964 - val_MinusLogProbMetric: 34.3964 - lr: 4.1152e-06 - 68s/epoch - 348ms/step
Epoch 974/1000
2023-10-25 07:20:28.011 
Epoch 974/1000 
	 loss: 33.8359, MinusLogProbMetric: 33.8359, val_loss: 34.2836, val_MinusLogProbMetric: 34.2836

Epoch 974: val_loss improved from 34.32179 to 34.28357, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 66s - loss: 33.8359 - MinusLogProbMetric: 33.8359 - val_loss: 34.2836 - val_MinusLogProbMetric: 34.2836 - lr: 4.1152e-06 - 66s/epoch - 337ms/step
Epoch 975/1000
2023-10-25 07:21:35.521 
Epoch 975/1000 
	 loss: 33.8346, MinusLogProbMetric: 33.8346, val_loss: 34.3174, val_MinusLogProbMetric: 34.3174

Epoch 975: val_loss did not improve from 34.28357
196/196 - 67s - loss: 33.8346 - MinusLogProbMetric: 33.8346 - val_loss: 34.3174 - val_MinusLogProbMetric: 34.3174 - lr: 4.1152e-06 - 67s/epoch - 340ms/step
Epoch 976/1000
2023-10-25 07:22:44.055 
Epoch 976/1000 
	 loss: 33.8510, MinusLogProbMetric: 33.8510, val_loss: 37.8496, val_MinusLogProbMetric: 37.8496

Epoch 976: val_loss did not improve from 34.28357
196/196 - 69s - loss: 33.8510 - MinusLogProbMetric: 33.8510 - val_loss: 37.8496 - val_MinusLogProbMetric: 37.8496 - lr: 4.1152e-06 - 69s/epoch - 350ms/step
Epoch 977/1000
2023-10-25 07:23:50.586 
Epoch 977/1000 
	 loss: 34.2202, MinusLogProbMetric: 34.2202, val_loss: 34.3551, val_MinusLogProbMetric: 34.3551

Epoch 977: val_loss did not improve from 34.28357
196/196 - 67s - loss: 34.2202 - MinusLogProbMetric: 34.2202 - val_loss: 34.3551 - val_MinusLogProbMetric: 34.3551 - lr: 4.1152e-06 - 67s/epoch - 339ms/step
Epoch 978/1000
2023-10-25 07:24:57.075 
Epoch 978/1000 
	 loss: 33.7994, MinusLogProbMetric: 33.7994, val_loss: 34.3913, val_MinusLogProbMetric: 34.3913

Epoch 978: val_loss did not improve from 34.28357
196/196 - 66s - loss: 33.7994 - MinusLogProbMetric: 33.7994 - val_loss: 34.3913 - val_MinusLogProbMetric: 34.3913 - lr: 4.1152e-06 - 66s/epoch - 339ms/step
Epoch 979/1000
2023-10-25 07:26:04.116 
Epoch 979/1000 
	 loss: 33.8168, MinusLogProbMetric: 33.8168, val_loss: 34.6301, val_MinusLogProbMetric: 34.6301

Epoch 979: val_loss did not improve from 34.28357
196/196 - 67s - loss: 33.8168 - MinusLogProbMetric: 33.8168 - val_loss: 34.6301 - val_MinusLogProbMetric: 34.6301 - lr: 4.1152e-06 - 67s/epoch - 342ms/step
Epoch 980/1000
2023-10-25 07:27:12.283 
Epoch 980/1000 
	 loss: 33.8112, MinusLogProbMetric: 33.8112, val_loss: 34.2790, val_MinusLogProbMetric: 34.2790

Epoch 980: val_loss improved from 34.28357 to 34.27903, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 69s - loss: 33.8112 - MinusLogProbMetric: 33.8112 - val_loss: 34.2790 - val_MinusLogProbMetric: 34.2790 - lr: 4.1152e-06 - 69s/epoch - 352ms/step
Epoch 981/1000
2023-10-25 07:28:20.306 
Epoch 981/1000 
	 loss: 33.8531, MinusLogProbMetric: 33.8531, val_loss: 34.2304, val_MinusLogProbMetric: 34.2304

Epoch 981: val_loss improved from 34.27903 to 34.23045, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 68s - loss: 33.8531 - MinusLogProbMetric: 33.8531 - val_loss: 34.2304 - val_MinusLogProbMetric: 34.2304 - lr: 4.1152e-06 - 68s/epoch - 347ms/step
Epoch 982/1000
2023-10-25 07:29:27.612 
Epoch 982/1000 
	 loss: 33.8001, MinusLogProbMetric: 33.8001, val_loss: 34.3723, val_MinusLogProbMetric: 34.3723

Epoch 982: val_loss did not improve from 34.23045
196/196 - 66s - loss: 33.8001 - MinusLogProbMetric: 33.8001 - val_loss: 34.3723 - val_MinusLogProbMetric: 34.3723 - lr: 4.1152e-06 - 66s/epoch - 339ms/step
Epoch 983/1000
2023-10-25 07:30:31.706 
Epoch 983/1000 
	 loss: 33.7481, MinusLogProbMetric: 33.7481, val_loss: 34.4083, val_MinusLogProbMetric: 34.4083

Epoch 983: val_loss did not improve from 34.23045
196/196 - 64s - loss: 33.7481 - MinusLogProbMetric: 33.7481 - val_loss: 34.4083 - val_MinusLogProbMetric: 34.4083 - lr: 4.1152e-06 - 64s/epoch - 327ms/step
Epoch 984/1000
2023-10-25 07:31:38.265 
Epoch 984/1000 
	 loss: 34.0633, MinusLogProbMetric: 34.0633, val_loss: 34.4046, val_MinusLogProbMetric: 34.4046

Epoch 984: val_loss did not improve from 34.23045
196/196 - 67s - loss: 34.0633 - MinusLogProbMetric: 34.0633 - val_loss: 34.4046 - val_MinusLogProbMetric: 34.4046 - lr: 4.1152e-06 - 67s/epoch - 340ms/step
Epoch 985/1000
2023-10-25 07:32:45.361 
Epoch 985/1000 
	 loss: 34.1219, MinusLogProbMetric: 34.1219, val_loss: 34.3740, val_MinusLogProbMetric: 34.3740

Epoch 985: val_loss did not improve from 34.23045
196/196 - 67s - loss: 34.1219 - MinusLogProbMetric: 34.1219 - val_loss: 34.3740 - val_MinusLogProbMetric: 34.3740 - lr: 4.1152e-06 - 67s/epoch - 342ms/step
Epoch 986/1000
2023-10-25 07:33:53.552 
Epoch 986/1000 
	 loss: 33.7534, MinusLogProbMetric: 33.7534, val_loss: 34.2493, val_MinusLogProbMetric: 34.2493

Epoch 986: val_loss did not improve from 34.23045
196/196 - 68s - loss: 33.7534 - MinusLogProbMetric: 33.7534 - val_loss: 34.2493 - val_MinusLogProbMetric: 34.2493 - lr: 4.1152e-06 - 68s/epoch - 348ms/step
Epoch 987/1000
2023-10-25 07:35:00.473 
Epoch 987/1000 
	 loss: 33.7564, MinusLogProbMetric: 33.7564, val_loss: 34.3089, val_MinusLogProbMetric: 34.3089

Epoch 987: val_loss did not improve from 34.23045
196/196 - 67s - loss: 33.7564 - MinusLogProbMetric: 33.7564 - val_loss: 34.3089 - val_MinusLogProbMetric: 34.3089 - lr: 4.1152e-06 - 67s/epoch - 341ms/step
Epoch 988/1000
2023-10-25 07:36:07.131 
Epoch 988/1000 
	 loss: 33.8377, MinusLogProbMetric: 33.8377, val_loss: 39.7965, val_MinusLogProbMetric: 39.7965

Epoch 988: val_loss did not improve from 34.23045
196/196 - 67s - loss: 33.8377 - MinusLogProbMetric: 33.8377 - val_loss: 39.7965 - val_MinusLogProbMetric: 39.7965 - lr: 4.1152e-06 - 67s/epoch - 340ms/step
Epoch 989/1000
2023-10-25 07:37:13.172 
Epoch 989/1000 
	 loss: 34.1049, MinusLogProbMetric: 34.1049, val_loss: 34.1991, val_MinusLogProbMetric: 34.1991

Epoch 989: val_loss improved from 34.23045 to 34.19909, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 67s - loss: 34.1049 - MinusLogProbMetric: 34.1049 - val_loss: 34.1991 - val_MinusLogProbMetric: 34.1991 - lr: 4.1152e-06 - 67s/epoch - 342ms/step
Epoch 990/1000
2023-10-25 07:38:23.187 
Epoch 990/1000 
	 loss: 33.8735, MinusLogProbMetric: 33.8735, val_loss: 34.2347, val_MinusLogProbMetric: 34.2347

Epoch 990: val_loss did not improve from 34.19909
196/196 - 69s - loss: 33.8735 - MinusLogProbMetric: 33.8735 - val_loss: 34.2347 - val_MinusLogProbMetric: 34.2347 - lr: 4.1152e-06 - 69s/epoch - 352ms/step
Epoch 991/1000
2023-10-25 07:39:31.980 
Epoch 991/1000 
	 loss: 33.7757, MinusLogProbMetric: 33.7757, val_loss: 34.5157, val_MinusLogProbMetric: 34.5157

Epoch 991: val_loss did not improve from 34.19909
196/196 - 69s - loss: 33.7757 - MinusLogProbMetric: 33.7757 - val_loss: 34.5157 - val_MinusLogProbMetric: 34.5157 - lr: 4.1152e-06 - 69s/epoch - 351ms/step
Epoch 992/1000
2023-10-25 07:40:41.295 
Epoch 992/1000 
	 loss: 33.8015, MinusLogProbMetric: 33.8015, val_loss: 34.3811, val_MinusLogProbMetric: 34.3811

Epoch 992: val_loss did not improve from 34.19909
196/196 - 69s - loss: 33.8015 - MinusLogProbMetric: 33.8015 - val_loss: 34.3811 - val_MinusLogProbMetric: 34.3811 - lr: 4.1152e-06 - 69s/epoch - 354ms/step
Epoch 993/1000
2023-10-25 07:41:48.448 
Epoch 993/1000 
	 loss: 33.7516, MinusLogProbMetric: 33.7516, val_loss: 34.3081, val_MinusLogProbMetric: 34.3081

Epoch 993: val_loss did not improve from 34.19909
196/196 - 67s - loss: 33.7516 - MinusLogProbMetric: 33.7516 - val_loss: 34.3081 - val_MinusLogProbMetric: 34.3081 - lr: 4.1152e-06 - 67s/epoch - 343ms/step
Epoch 994/1000
2023-10-25 07:42:55.477 
Epoch 994/1000 
	 loss: 33.7135, MinusLogProbMetric: 33.7135, val_loss: 34.3511, val_MinusLogProbMetric: 34.3511

Epoch 994: val_loss did not improve from 34.19909
196/196 - 67s - loss: 33.7135 - MinusLogProbMetric: 33.7135 - val_loss: 34.3511 - val_MinusLogProbMetric: 34.3511 - lr: 4.1152e-06 - 67s/epoch - 342ms/step
Epoch 995/1000
2023-10-25 07:44:01.683 
Epoch 995/1000 
	 loss: 33.7474, MinusLogProbMetric: 33.7474, val_loss: 34.2364, val_MinusLogProbMetric: 34.2364

Epoch 995: val_loss did not improve from 34.19909
196/196 - 66s - loss: 33.7474 - MinusLogProbMetric: 33.7474 - val_loss: 34.2364 - val_MinusLogProbMetric: 34.2364 - lr: 4.1152e-06 - 66s/epoch - 338ms/step
Epoch 996/1000
2023-10-25 07:45:07.587 
Epoch 996/1000 
	 loss: 33.7012, MinusLogProbMetric: 33.7012, val_loss: 34.3153, val_MinusLogProbMetric: 34.3153

Epoch 996: val_loss did not improve from 34.19909
196/196 - 66s - loss: 33.7012 - MinusLogProbMetric: 33.7012 - val_loss: 34.3153 - val_MinusLogProbMetric: 34.3153 - lr: 4.1152e-06 - 66s/epoch - 336ms/step
Epoch 997/1000
2023-10-25 07:46:15.016 
Epoch 997/1000 
	 loss: 33.7925, MinusLogProbMetric: 33.7925, val_loss: 34.8602, val_MinusLogProbMetric: 34.8602

Epoch 997: val_loss did not improve from 34.19909
196/196 - 67s - loss: 33.7925 - MinusLogProbMetric: 33.7925 - val_loss: 34.8602 - val_MinusLogProbMetric: 34.8602 - lr: 4.1152e-06 - 67s/epoch - 344ms/step
Epoch 998/1000
2023-10-25 07:47:22.901 
Epoch 998/1000 
	 loss: 33.7478, MinusLogProbMetric: 33.7478, val_loss: 34.2060, val_MinusLogProbMetric: 34.2060

Epoch 998: val_loss did not improve from 34.19909
196/196 - 68s - loss: 33.7478 - MinusLogProbMetric: 33.7478 - val_loss: 34.2060 - val_MinusLogProbMetric: 34.2060 - lr: 4.1152e-06 - 68s/epoch - 346ms/step
Epoch 999/1000
2023-10-25 07:48:31.051 
Epoch 999/1000 
	 loss: 33.6890, MinusLogProbMetric: 33.6890, val_loss: 34.4505, val_MinusLogProbMetric: 34.4505

Epoch 999: val_loss did not improve from 34.19909
196/196 - 68s - loss: 33.6890 - MinusLogProbMetric: 33.6890 - val_loss: 34.4505 - val_MinusLogProbMetric: 34.4505 - lr: 4.1152e-06 - 68s/epoch - 348ms/step
Epoch 1000/1000
2023-10-25 07:49:39.244 
Epoch 1000/1000 
	 loss: 33.7828, MinusLogProbMetric: 33.7828, val_loss: 34.1338, val_MinusLogProbMetric: 34.1338

Epoch 1000: val_loss improved from 34.19909 to 34.13383, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_342/weights/best_weights.h5
196/196 - 69s - loss: 33.7828 - MinusLogProbMetric: 33.7828 - val_loss: 34.1338 - val_MinusLogProbMetric: 34.1338 - lr: 4.1152e-06 - 69s/epoch - 354ms/step
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Training succeeded with seed 377.
Model trained in 65306.21 s.

===========
Computing predictions
===========

Computing metrics...
Checking and setting numerical distributions.
Resetting dist_num.
Resetting dist_num.
Metrics computed in 1.28 s.
                ===========
                print("===========
Failed to plot
===========
")
                Exception type: ValueError
                Exception message: Number of rows must be a positive integer, not 0
                Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 481, Func.Name : prediction_function, Message : Plotters.cornerplotter(X_data_test.numpy(),X_data_nf.numpy(),path_to_results,ndims,norm=True) # type: ignore', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/Plotters.py , Line : 156, Func.Name : cornerplotter, Message : figure=corner.corner(target_samples,color=\'red\',bins=n_bins,labels=[r"%s" % s for s in labels],normalize1d=True)', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/corner.py , Line : 235, Func.Name : corner, Message : fig, axes = pl.subplots(K, K, figsize=(dim, dim))', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/pyplot.py , Line : 1502, Func.Name : subplots, Message : axs = fig.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 905, Func.Name : subplots, Message : gs = self.add_gridspec(nrows, ncols, figure=self, **gridspec_kw)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 1527, Func.Name : add_gridspec, Message : gs = GridSpec(nrows=nrows, ncols=ncols, figure=self, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 379, Func.Name : __init__, Message : super().__init__(nrows, ncols,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 49, Func.Name : __init__, Message : raise ValueError(']
                ===========

results.txt saved
results.json saved
Results log saved
Model predictions computed in 1.62 s.
===========
Run 342/720 done in 83822.94 s.
===========

Directory ../../results/CsplineN_new/run_343/ already exists.
Skipping it.
===========
Run 343/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_344/ already exists.
Skipping it.
===========
Run 344/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_345/ already exists.
Skipping it.
===========
Run 345/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_346/ already exists.
Skipping it.
===========
Run 346/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_347/ already exists.
Skipping it.
===========
Run 347/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_348/ already exists.
Skipping it.
===========
Run 348/720 already exists. Skipping it.
===========

===========
Generating train data for run 349.
===========
Train data generated in 0.24 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_349/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_349/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_349/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_349
self.data_kwargs: {'seed': 440}
self.x_data: [[ 2.0547452   3.776408    6.9601746  ...  5.8637457  -0.4179731
   2.747595  ]
 [ 6.405085    2.999549    6.1033454  ...  2.8107233   3.0569289
   2.003619  ]
 [ 2.0047038   4.064143    6.2048492  ...  5.6568336   0.71395487
   2.6926432 ]
 ...
 [ 5.645455    7.0971727   6.202566   ...  0.61970913  6.238492
   1.3539525 ]
 [ 6.475603    3.1151166   6.2248073  ...  2.8990176   3.6724448
   2.289971  ]
 [ 2.2290072   4.101192    7.1413527  ...  6.717821   -2.0046902
   2.3999293 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_351"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_352 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_31 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_31/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_31'")
self.model: <keras.engine.functional.Functional object at 0x7f57ee3301c0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f57a9c6bee0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f57a9c6bee0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f5834969e40>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f57a9aabb50>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_349/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f57a9aabf70>, <keras.callbacks.ModelCheckpoint object at 0x7f57a9a141c0>, <keras.callbacks.EarlyStopping object at 0x7f57a9a14430>, <keras.callbacks.ReduceLROnPlateau object at 0x7f57a9a14460>, <keras.callbacks.TerminateOnNaN object at 0x7f57a9a140a0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_349/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 349/720 with hyperparameters:
timestamp = 2023-10-25 07:49:50.474324
ndims = 64
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 2.0547452   3.776408    6.9601746   1.2163222   8.644804    1.0450234
  9.756204    5.9333344   9.3759165   6.2056174   7.6663713   0.8787037
  2.7781603   1.7675867   3.9287794   0.8286338   3.6572614   4.846633
  0.68292063  6.317236    6.48625     3.843101    5.88724     2.0270782
  5.1297107   7.7579265   2.6054757   6.6726594   1.0543873   7.123391
  3.5526195   2.046467    5.620422   -0.869851    8.648443    0.02500496
  7.4141846   2.6777635   7.6790566   9.76465     2.481489    6.2672224
  5.7068043   4.504637    1.1161722   9.691578    4.0927534   8.503739
  6.9964066   3.1729455   8.086372    4.0078015   8.096493    5.813139
  8.181514    6.97417     7.246071    5.0315676  10.666024    6.2678714
  3.6855178   5.8637457  -0.4179731   2.747595  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 07:52:07.349 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7287.4268, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 137s - loss: nan - MinusLogProbMetric: 7287.4268 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 137s/epoch - 697ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 0.0003333333333333333.
===========
Generating train data for run 349.
===========
Train data generated in 0.19 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_349/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_349/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_349/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_349
self.data_kwargs: {'seed': 440}
self.x_data: [[ 2.0547452   3.776408    6.9601746  ...  5.8637457  -0.4179731
   2.747595  ]
 [ 6.405085    2.999549    6.1033454  ...  2.8107233   3.0569289
   2.003619  ]
 [ 2.0047038   4.064143    6.2048492  ...  5.6568336   0.71395487
   2.6926432 ]
 ...
 [ 5.645455    7.0971727   6.202566   ...  0.61970913  6.238492
   1.3539525 ]
 [ 6.475603    3.1151166   6.2248073  ...  2.8990176   3.6724448
   2.289971  ]
 [ 2.2290072   4.101192    7.1413527  ...  6.717821   -2.0046902
   2.3999293 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_362"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_363 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_32 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_32/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_32'")
self.model: <keras.engine.functional.Functional object at 0x7f5b886213f0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f58de863eb0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f58de863eb0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f58dfae02e0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f5afe18aa10>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_349/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f5afe18af80>, <keras.callbacks.ModelCheckpoint object at 0x7f5afe18b040>, <keras.callbacks.EarlyStopping object at 0x7f5afe18b2b0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f5afe18b2e0>, <keras.callbacks.TerminateOnNaN object at 0x7f5afe18af20>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_349/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 349/720 with hyperparameters:
timestamp = 2023-10-25 07:52:15.489066
ndims = 64
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 2.0547452   3.776408    6.9601746   1.2163222   8.644804    1.0450234
  9.756204    5.9333344   9.3759165   6.2056174   7.6663713   0.8787037
  2.7781603   1.7675867   3.9287794   0.8286338   3.6572614   4.846633
  0.68292063  6.317236    6.48625     3.843101    5.88724     2.0270782
  5.1297107   7.7579265   2.6054757   6.6726594   1.0543873   7.123391
  3.5526195   2.046467    5.620422   -0.869851    8.648443    0.02500496
  7.4141846   2.6777635   7.6790566   9.76465     2.481489    6.2672224
  5.7068043   4.504637    1.1161722   9.691578    4.0927534   8.503739
  6.9964066   3.1729455   8.086372    4.0078015   8.096493    5.813139
  8.181514    6.97417     7.246071    5.0315676  10.666024    6.2678714
  3.6855178   5.8637457  -0.4179731   2.747595  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 07:54:15.909 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7287.4268, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 120s - loss: nan - MinusLogProbMetric: 7287.4268 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 120s/epoch - 614ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 0.0001111111111111111.
===========
Generating train data for run 349.
===========
Train data generated in 0.19 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_349/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_349/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_349/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_349
self.data_kwargs: {'seed': 440}
self.x_data: [[ 2.0547452   3.776408    6.9601746  ...  5.8637457  -0.4179731
   2.747595  ]
 [ 6.405085    2.999549    6.1033454  ...  2.8107233   3.0569289
   2.003619  ]
 [ 2.0047038   4.064143    6.2048492  ...  5.6568336   0.71395487
   2.6926432 ]
 ...
 [ 5.645455    7.0971727   6.202566   ...  0.61970913  6.238492
   1.3539525 ]
 [ 6.475603    3.1151166   6.2248073  ...  2.8990176   3.6724448
   2.289971  ]
 [ 2.2290072   4.101192    7.1413527  ...  6.717821   -2.0046902
   2.3999293 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_373"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_374 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_33 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_33/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_33'")
self.model: <keras.engine.functional.Functional object at 0x7f57ee232b30>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f58919b1750>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f58919b1750>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f5fed36c310>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f5fed46cb50>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_349/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f5fed46c430>, <keras.callbacks.ModelCheckpoint object at 0x7f5fed46d3c0>, <keras.callbacks.EarlyStopping object at 0x7f5fed46ccd0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f5fed46d2d0>, <keras.callbacks.TerminateOnNaN object at 0x7f5fed46cbb0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_349/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 349/720 with hyperparameters:
timestamp = 2023-10-25 07:54:29.014119
ndims = 64
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 2.0547452   3.776408    6.9601746   1.2163222   8.644804    1.0450234
  9.756204    5.9333344   9.3759165   6.2056174   7.6663713   0.8787037
  2.7781603   1.7675867   3.9287794   0.8286338   3.6572614   4.846633
  0.68292063  6.317236    6.48625     3.843101    5.88724     2.0270782
  5.1297107   7.7579265   2.6054757   6.6726594   1.0543873   7.123391
  3.5526195   2.046467    5.620422   -0.869851    8.648443    0.02500496
  7.4141846   2.6777635   7.6790566   9.76465     2.481489    6.2672224
  5.7068043   4.504637    1.1161722   9.691578    4.0927534   8.503739
  6.9964066   3.1729455   8.086372    4.0078015   8.096493    5.813139
  8.181514    6.97417     7.246071    5.0315676  10.666024    6.2678714
  3.6855178   5.8637457  -0.4179731   2.747595  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 07:56:32.146 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7287.4268, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 123s - loss: nan - MinusLogProbMetric: 7287.4268 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.1111e-04 - 123s/epoch - 628ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 3.703703703703703e-05.
===========
Generating train data for run 349.
===========
Train data generated in 0.24 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_349/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_349/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_349/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_349
self.data_kwargs: {'seed': 440}
self.x_data: [[ 2.0547452   3.776408    6.9601746  ...  5.8637457  -0.4179731
   2.747595  ]
 [ 6.405085    2.999549    6.1033454  ...  2.8107233   3.0569289
   2.003619  ]
 [ 2.0047038   4.064143    6.2048492  ...  5.6568336   0.71395487
   2.6926432 ]
 ...
 [ 5.645455    7.0971727   6.202566   ...  0.61970913  6.238492
   1.3539525 ]
 [ 6.475603    3.1151166   6.2248073  ...  2.8990176   3.6724448
   2.289971  ]
 [ 2.2290072   4.101192    7.1413527  ...  6.717821   -2.0046902
   2.3999293 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_384"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_385 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_34 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_34/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_34'")
self.model: <keras.engine.functional.Functional object at 0x7f57a8b4e320>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f5f8e83bcd0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f5f8e83bcd0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f589a6056c0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f5899d77df0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_349/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f5899dc83a0>, <keras.callbacks.ModelCheckpoint object at 0x7f5899dc8460>, <keras.callbacks.EarlyStopping object at 0x7f5899dc86d0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f5899dc8700>, <keras.callbacks.TerminateOnNaN object at 0x7f5899dc8340>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_349/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 349/720 with hyperparameters:
timestamp = 2023-10-25 07:56:41.396651
ndims = 64
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 3.703703703703703e-05...
Train first sample: [ 2.0547452   3.776408    6.9601746   1.2163222   8.644804    1.0450234
  9.756204    5.9333344   9.3759165   6.2056174   7.6663713   0.8787037
  2.7781603   1.7675867   3.9287794   0.8286338   3.6572614   4.846633
  0.68292063  6.317236    6.48625     3.843101    5.88724     2.0270782
  5.1297107   7.7579265   2.6054757   6.6726594   1.0543873   7.123391
  3.5526195   2.046467    5.620422   -0.869851    8.648443    0.02500496
  7.4141846   2.6777635   7.6790566   9.76465     2.481489    6.2672224
  5.7068043   4.504637    1.1161722   9.691578    4.0927534   8.503739
  6.9964066   3.1729455   8.086372    4.0078015   8.096493    5.813139
  8.181514    6.97417     7.246071    5.0315676  10.666024    6.2678714
  3.6855178   5.8637457  -0.4179731   2.747595  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 07:59:15.956 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7287.4268, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 154s - loss: nan - MinusLogProbMetric: 7287.4268 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.7037e-05 - 154s/epoch - 787ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 1.2345679012345677e-05.
===========
Generating train data for run 349.
===========
Train data generated in 0.27 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_349/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_349/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_349/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_349
self.data_kwargs: {'seed': 440}
self.x_data: [[ 2.0547452   3.776408    6.9601746  ...  5.8637457  -0.4179731
   2.747595  ]
 [ 6.405085    2.999549    6.1033454  ...  2.8107233   3.0569289
   2.003619  ]
 [ 2.0047038   4.064143    6.2048492  ...  5.6568336   0.71395487
   2.6926432 ]
 ...
 [ 5.645455    7.0971727   6.202566   ...  0.61970913  6.238492
   1.3539525 ]
 [ 6.475603    3.1151166   6.2248073  ...  2.8990176   3.6724448
   2.289971  ]
 [ 2.2290072   4.101192    7.1413527  ...  6.717821   -2.0046902
   2.3999293 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_395"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_396 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_35 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_35/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_35'")
self.model: <keras.engine.functional.Functional object at 0x7f58a20f6bf0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f5c0040eef0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f5c0040eef0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f5b5c299930>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f599c1d3c40>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_349/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f599c1d36a0>, <keras.callbacks.ModelCheckpoint object at 0x7f599c1d3370>, <keras.callbacks.EarlyStopping object at 0x7f599c1d2ef0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f599c1d36d0>, <keras.callbacks.TerminateOnNaN object at 0x7f599c1d3250>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_349/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 349/720 with hyperparameters:
timestamp = 2023-10-25 07:59:24.776585
ndims = 64
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.2345679012345677e-05...
Train first sample: [ 2.0547452   3.776408    6.9601746   1.2163222   8.644804    1.0450234
  9.756204    5.9333344   9.3759165   6.2056174   7.6663713   0.8787037
  2.7781603   1.7675867   3.9287794   0.8286338   3.6572614   4.846633
  0.68292063  6.317236    6.48625     3.843101    5.88724     2.0270782
  5.1297107   7.7579265   2.6054757   6.6726594   1.0543873   7.123391
  3.5526195   2.046467    5.620422   -0.869851    8.648443    0.02500496
  7.4141846   2.6777635   7.6790566   9.76465     2.481489    6.2672224
  5.7068043   4.504637    1.1161722   9.691578    4.0927534   8.503739
  6.9964066   3.1729455   8.086372    4.0078015   8.096493    5.813139
  8.181514    6.97417     7.246071    5.0315676  10.666024    6.2678714
  3.6855178   5.8637457  -0.4179731   2.747595  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 08:01:21.674 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7287.4268, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 117s - loss: nan - MinusLogProbMetric: 7287.4268 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.2346e-05 - 117s/epoch - 596ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 4.115226337448558e-06.
===========
Generating train data for run 349.
===========
Train data generated in 0.24 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_349/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_349/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_349/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_349
self.data_kwargs: {'seed': 440}
self.x_data: [[ 2.0547452   3.776408    6.9601746  ...  5.8637457  -0.4179731
   2.747595  ]
 [ 6.405085    2.999549    6.1033454  ...  2.8107233   3.0569289
   2.003619  ]
 [ 2.0047038   4.064143    6.2048492  ...  5.6568336   0.71395487
   2.6926432 ]
 ...
 [ 5.645455    7.0971727   6.202566   ...  0.61970913  6.238492
   1.3539525 ]
 [ 6.475603    3.1151166   6.2248073  ...  2.8990176   3.6724448
   2.289971  ]
 [ 2.2290072   4.101192    7.1413527  ...  6.717821   -2.0046902
   2.3999293 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_406"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_407 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_36 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_36/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_36'")
self.model: <keras.engine.functional.Functional object at 0x7f589bdd7f10>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f58e4297a90>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f58e4297a90>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f593e23ae00>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f589bd47700>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_349/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f589bd47c70>, <keras.callbacks.ModelCheckpoint object at 0x7f589bd47d30>, <keras.callbacks.EarlyStopping object at 0x7f589bd47fa0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f589bd47fd0>, <keras.callbacks.TerminateOnNaN object at 0x7f589bd47c10>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_349/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 349/720 with hyperparameters:
timestamp = 2023-10-25 08:01:29.781899
ndims = 64
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.115226337448558e-06...
Train first sample: [ 2.0547452   3.776408    6.9601746   1.2163222   8.644804    1.0450234
  9.756204    5.9333344   9.3759165   6.2056174   7.6663713   0.8787037
  2.7781603   1.7675867   3.9287794   0.8286338   3.6572614   4.846633
  0.68292063  6.317236    6.48625     3.843101    5.88724     2.0270782
  5.1297107   7.7579265   2.6054757   6.6726594   1.0543873   7.123391
  3.5526195   2.046467    5.620422   -0.869851    8.648443    0.02500496
  7.4141846   2.6777635   7.6790566   9.76465     2.481489    6.2672224
  5.7068043   4.504637    1.1161722   9.691578    4.0927534   8.503739
  6.9964066   3.1729455   8.086372    4.0078015   8.096493    5.813139
  8.181514    6.97417     7.246071    5.0315676  10.666024    6.2678714
  3.6855178   5.8637457  -0.4179731   2.747595  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 08:03:44.779 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7287.4268, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 135s - loss: nan - MinusLogProbMetric: 7287.4268 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.1152e-06 - 135s/epoch - 687ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 1.3717421124828526e-06.
===========
Generating train data for run 349.
===========
Train data generated in 0.29 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_349/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_349/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_349/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_349
self.data_kwargs: {'seed': 440}
self.x_data: [[ 2.0547452   3.776408    6.9601746  ...  5.8637457  -0.4179731
   2.747595  ]
 [ 6.405085    2.999549    6.1033454  ...  2.8107233   3.0569289
   2.003619  ]
 [ 2.0047038   4.064143    6.2048492  ...  5.6568336   0.71395487
   2.6926432 ]
 ...
 [ 5.645455    7.0971727   6.202566   ...  0.61970913  6.238492
   1.3539525 ]
 [ 6.475603    3.1151166   6.2248073  ...  2.8990176   3.6724448
   2.289971  ]
 [ 2.2290072   4.101192    7.1413527  ...  6.717821   -2.0046902
   2.3999293 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_417"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_418 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_37 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_37/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_37'")
self.model: <keras.engine.functional.Functional object at 0x7f57a85afa00>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f57a943ae30>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f57a943ae30>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f5b4461b3a0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f58e4bd6cb0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_349/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f58e4bd7220>, <keras.callbacks.ModelCheckpoint object at 0x7f58e4bd72e0>, <keras.callbacks.EarlyStopping object at 0x7f58e4bd7550>, <keras.callbacks.ReduceLROnPlateau object at 0x7f58e4bd7580>, <keras.callbacks.TerminateOnNaN object at 0x7f58e4bd71c0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_349/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 349/720 with hyperparameters:
timestamp = 2023-10-25 08:03:54.112331
ndims = 64
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.3717421124828526e-06...
Train first sample: [ 2.0547452   3.776408    6.9601746   1.2163222   8.644804    1.0450234
  9.756204    5.9333344   9.3759165   6.2056174   7.6663713   0.8787037
  2.7781603   1.7675867   3.9287794   0.8286338   3.6572614   4.846633
  0.68292063  6.317236    6.48625     3.843101    5.88724     2.0270782
  5.1297107   7.7579265   2.6054757   6.6726594   1.0543873   7.123391
  3.5526195   2.046467    5.620422   -0.869851    8.648443    0.02500496
  7.4141846   2.6777635   7.6790566   9.76465     2.481489    6.2672224
  5.7068043   4.504637    1.1161722   9.691578    4.0927534   8.503739
  6.9964066   3.1729455   8.086372    4.0078015   8.096493    5.813139
  8.181514    6.97417     7.246071    5.0315676  10.666024    6.2678714
  3.6855178   5.8637457  -0.4179731   2.747595  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 08:05:51.305 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7287.4268, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 117s - loss: nan - MinusLogProbMetric: 7287.4268 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.3717e-06 - 117s/epoch - 597ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 4.572473708276175e-07.
===========
Generating train data for run 349.
===========
Train data generated in 0.21 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_349/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_349/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_349/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_349
self.data_kwargs: {'seed': 440}
self.x_data: [[ 2.0547452   3.776408    6.9601746  ...  5.8637457  -0.4179731
   2.747595  ]
 [ 6.405085    2.999549    6.1033454  ...  2.8107233   3.0569289
   2.003619  ]
 [ 2.0047038   4.064143    6.2048492  ...  5.6568336   0.71395487
   2.6926432 ]
 ...
 [ 5.645455    7.0971727   6.202566   ...  0.61970913  6.238492
   1.3539525 ]
 [ 6.475603    3.1151166   6.2248073  ...  2.8990176   3.6724448
   2.289971  ]
 [ 2.2290072   4.101192    7.1413527  ...  6.717821   -2.0046902
   2.3999293 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_428"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_429 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_38 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_38/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_38'")
self.model: <keras.engine.functional.Functional object at 0x7f56841540d0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f594e5702e0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f594e5702e0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f5778958dc0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f571c38aa40>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_349/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f571c38afb0>, <keras.callbacks.ModelCheckpoint object at 0x7f571c38b070>, <keras.callbacks.EarlyStopping object at 0x7f571c38b2e0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f571c38b310>, <keras.callbacks.TerminateOnNaN object at 0x7f571c38af50>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_349/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 349/720 with hyperparameters:
timestamp = 2023-10-25 08:05:58.968889
ndims = 64
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.572473708276175e-07...
Train first sample: [ 2.0547452   3.776408    6.9601746   1.2163222   8.644804    1.0450234
  9.756204    5.9333344   9.3759165   6.2056174   7.6663713   0.8787037
  2.7781603   1.7675867   3.9287794   0.8286338   3.6572614   4.846633
  0.68292063  6.317236    6.48625     3.843101    5.88724     2.0270782
  5.1297107   7.7579265   2.6054757   6.6726594   1.0543873   7.123391
  3.5526195   2.046467    5.620422   -0.869851    8.648443    0.02500496
  7.4141846   2.6777635   7.6790566   9.76465     2.481489    6.2672224
  5.7068043   4.504637    1.1161722   9.691578    4.0927534   8.503739
  6.9964066   3.1729455   8.086372    4.0078015   8.096493    5.813139
  8.181514    6.97417     7.246071    5.0315676  10.666024    6.2678714
  3.6855178   5.8637457  -0.4179731   2.747595  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 08:08:32.619 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7287.4268, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 154s - loss: nan - MinusLogProbMetric: 7287.4268 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.5725e-07 - 154s/epoch - 783ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 1.524157902758725e-07.
===========
Generating train data for run 349.
===========
Train data generated in 0.28 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_349/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_349/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_349/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_349
self.data_kwargs: {'seed': 440}
self.x_data: [[ 2.0547452   3.776408    6.9601746  ...  5.8637457  -0.4179731
   2.747595  ]
 [ 6.405085    2.999549    6.1033454  ...  2.8107233   3.0569289
   2.003619  ]
 [ 2.0047038   4.064143    6.2048492  ...  5.6568336   0.71395487
   2.6926432 ]
 ...
 [ 5.645455    7.0971727   6.202566   ...  0.61970913  6.238492
   1.3539525 ]
 [ 6.475603    3.1151166   6.2248073  ...  2.8990176   3.6724448
   2.289971  ]
 [ 2.2290072   4.101192    7.1413527  ...  6.717821   -2.0046902
   2.3999293 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_439"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_440 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_39 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_39/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_39'")
self.model: <keras.engine.functional.Functional object at 0x7f58a00e3730>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f5827aeb250>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f5827aeb250>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f593cbd48b0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f58df38cb80>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_349/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f58df38d0f0>, <keras.callbacks.ModelCheckpoint object at 0x7f58df38d1b0>, <keras.callbacks.EarlyStopping object at 0x7f58df38d420>, <keras.callbacks.ReduceLROnPlateau object at 0x7f58df38d450>, <keras.callbacks.TerminateOnNaN object at 0x7f58df38d090>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_349/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 349/720 with hyperparameters:
timestamp = 2023-10-25 08:08:42.250335
ndims = 64
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.524157902758725e-07...
Train first sample: [ 2.0547452   3.776408    6.9601746   1.2163222   8.644804    1.0450234
  9.756204    5.9333344   9.3759165   6.2056174   7.6663713   0.8787037
  2.7781603   1.7675867   3.9287794   0.8286338   3.6572614   4.846633
  0.68292063  6.317236    6.48625     3.843101    5.88724     2.0270782
  5.1297107   7.7579265   2.6054757   6.6726594   1.0543873   7.123391
  3.5526195   2.046467    5.620422   -0.869851    8.648443    0.02500496
  7.4141846   2.6777635   7.6790566   9.76465     2.481489    6.2672224
  5.7068043   4.504637    1.1161722   9.691578    4.0927534   8.503739
  6.9964066   3.1729455   8.086372    4.0078015   8.096493    5.813139
  8.181514    6.97417     7.246071    5.0315676  10.666024    6.2678714
  3.6855178   5.8637457  -0.4179731   2.747595  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 08:10:48.344 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7287.4268, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 126s - loss: nan - MinusLogProbMetric: 7287.4268 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.5242e-07 - 126s/epoch - 642ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 5.0805263425290834e-08.
===========
Generating train data for run 349.
===========
Train data generated in 0.22 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_349/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_349/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_349/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_349
self.data_kwargs: {'seed': 440}
self.x_data: [[ 2.0547452   3.776408    6.9601746  ...  5.8637457  -0.4179731
   2.747595  ]
 [ 6.405085    2.999549    6.1033454  ...  2.8107233   3.0569289
   2.003619  ]
 [ 2.0047038   4.064143    6.2048492  ...  5.6568336   0.71395487
   2.6926432 ]
 ...
 [ 5.645455    7.0971727   6.202566   ...  0.61970913  6.238492
   1.3539525 ]
 [ 6.475603    3.1151166   6.2248073  ...  2.8990176   3.6724448
   2.289971  ]
 [ 2.2290072   4.101192    7.1413527  ...  6.717821   -2.0046902
   2.3999293 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_450"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_451 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_40 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_40/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_40'")
self.model: <keras.engine.functional.Functional object at 0x7f5971f49d50>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f5bb826b760>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f5bb826b760>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f5ff5b03130>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f6026f4bbe0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_349/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f6026f4aa10>, <keras.callbacks.ModelCheckpoint object at 0x7f5a1856bfd0>, <keras.callbacks.EarlyStopping object at 0x7f6026f4ad70>, <keras.callbacks.ReduceLROnPlateau object at 0x7f6026f490c0>, <keras.callbacks.TerminateOnNaN object at 0x7f58e444cc70>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_349/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 349/720 with hyperparameters:
timestamp = 2023-10-25 08:11:03.908317
ndims = 64
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 5.0805263425290834e-08...
Train first sample: [ 2.0547452   3.776408    6.9601746   1.2163222   8.644804    1.0450234
  9.756204    5.9333344   9.3759165   6.2056174   7.6663713   0.8787037
  2.7781603   1.7675867   3.9287794   0.8286338   3.6572614   4.846633
  0.68292063  6.317236    6.48625     3.843101    5.88724     2.0270782
  5.1297107   7.7579265   2.6054757   6.6726594   1.0543873   7.123391
  3.5526195   2.046467    5.620422   -0.869851    8.648443    0.02500496
  7.4141846   2.6777635   7.6790566   9.76465     2.481489    6.2672224
  5.7068043   4.504637    1.1161722   9.691578    4.0927534   8.503739
  6.9964066   3.1729455   8.086372    4.0078015   8.096493    5.813139
  8.181514    6.97417     7.246071    5.0315676  10.666024    6.2678714
  3.6855178   5.8637457  -0.4179731   2.747595  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 08:13:10.813 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7287.4268, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 127s - loss: nan - MinusLogProbMetric: 7287.4268 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 5.0805e-08 - 127s/epoch - 646ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 1.6935087808430278e-08.
===========
Generating train data for run 349.
===========
Train data generated in 0.22 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_349/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_349/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_349/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_349
self.data_kwargs: {'seed': 440}
self.x_data: [[ 2.0547452   3.776408    6.9601746  ...  5.8637457  -0.4179731
   2.747595  ]
 [ 6.405085    2.999549    6.1033454  ...  2.8107233   3.0569289
   2.003619  ]
 [ 2.0047038   4.064143    6.2048492  ...  5.6568336   0.71395487
   2.6926432 ]
 ...
 [ 5.645455    7.0971727   6.202566   ...  0.61970913  6.238492
   1.3539525 ]
 [ 6.475603    3.1151166   6.2248073  ...  2.8990176   3.6724448
   2.289971  ]
 [ 2.2290072   4.101192    7.1413527  ...  6.717821   -2.0046902
   2.3999293 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_461"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_462 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_41 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_41/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_41'")
self.model: <keras.engine.functional.Functional object at 0x7f57bd39ba90>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f57bd54ea10>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f57bd54ea10>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f57bd33ab30>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f57bd3d26b0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_349/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f57bd3d2c20>, <keras.callbacks.ModelCheckpoint object at 0x7f57bd3d2ce0>, <keras.callbacks.EarlyStopping object at 0x7f57bd3d2f50>, <keras.callbacks.ReduceLROnPlateau object at 0x7f57bd3d2f80>, <keras.callbacks.TerminateOnNaN object at 0x7f57bd3d2bc0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_349/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 349/720 with hyperparameters:
timestamp = 2023-10-25 08:13:18.427316
ndims = 64
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.6935087808430278e-08...
Train first sample: [ 2.0547452   3.776408    6.9601746   1.2163222   8.644804    1.0450234
  9.756204    5.9333344   9.3759165   6.2056174   7.6663713   0.8787037
  2.7781603   1.7675867   3.9287794   0.8286338   3.6572614   4.846633
  0.68292063  6.317236    6.48625     3.843101    5.88724     2.0270782
  5.1297107   7.7579265   2.6054757   6.6726594   1.0543873   7.123391
  3.5526195   2.046467    5.620422   -0.869851    8.648443    0.02500496
  7.4141846   2.6777635   7.6790566   9.76465     2.481489    6.2672224
  5.7068043   4.504637    1.1161722   9.691578    4.0927534   8.503739
  6.9964066   3.1729455   8.086372    4.0078015   8.096493    5.813139
  8.181514    6.97417     7.246071    5.0315676  10.666024    6.2678714
  3.6855178   5.8637457  -0.4179731   2.747595  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 08:15:38.728 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7287.4268, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 140s - loss: nan - MinusLogProbMetric: 7287.4268 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.6935e-08 - 140s/epoch - 715ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 5.645029269476759e-09.
===========
Run 349/720 failed.
Exception type: Exception
Exception message: Training failed for the maximum number of retry.
Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 649, Func.Name : <module>, Message : hyperparams_dict, NFObject, seed_train, training_time = train_function(seeds = [seed_train, seed_test, seed_dist, seed_metrics],', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 322, Func.Name : train_function, Message : raise Exception("Training failed for the maximum number of retry.")']
===========

===========
Generating train data for run 350.
===========
Train data generated in 0.37 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_350/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_350/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_350/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_350
self.data_kwargs: {'seed': 440}
self.x_data: [[ 2.0547452   3.776408    6.9601746  ...  5.8637457  -0.4179731
   2.747595  ]
 [ 6.405085    2.999549    6.1033454  ...  2.8107233   3.0569289
   2.003619  ]
 [ 2.0047038   4.064143    6.2048492  ...  5.6568336   0.71395487
   2.6926432 ]
 ...
 [ 5.645455    7.0971727   6.202566   ...  0.61970913  6.238492
   1.3539525 ]
 [ 6.475603    3.1151166   6.2248073  ...  2.8990176   3.6724448
   2.289971  ]
 [ 2.2290072   4.101192    7.1413527  ...  6.717821   -2.0046902
   2.3999293 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_472"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_473 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_42 (LogProbL  (None,)                  3291840   
 ayer)                                                           
                                                                 
=================================================================
Total params: 3,291,840
Trainable params: 3,291,840
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_42/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_42'")
self.model: <keras.engine.functional.Functional object at 0x7f5788acde40>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f57ed8af550>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f57ed8af550>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f595c6000a0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f58a15782b0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f57a0350c10>, <keras.callbacks.ModelCheckpoint object at 0x7f57a0350bb0>, <keras.callbacks.EarlyStopping object at 0x7f57a0351210>, <keras.callbacks.ReduceLROnPlateau object at 0x7f57a0351330>, <keras.callbacks.TerminateOnNaN object at 0x7f57a0350340>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_350/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 350/720 with hyperparameters:
timestamp = 2023-10-25 08:15:48.555481
ndims = 64
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 3291840
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 2.0547452   3.776408    6.9601746   1.2163222   8.644804    1.0450234
  9.756204    5.9333344   9.3759165   6.2056174   7.6663713   0.8787037
  2.7781603   1.7675867   3.9287794   0.8286338   3.6572614   4.846633
  0.68292063  6.317236    6.48625     3.843101    5.88724     2.0270782
  5.1297107   7.7579265   2.6054757   6.6726594   1.0543873   7.123391
  3.5526195   2.046467    5.620422   -0.869851    8.648443    0.02500496
  7.4141846   2.6777635   7.6790566   9.76465     2.481489    6.2672224
  5.7068043   4.504637    1.1161722   9.691578    4.0927534   8.503739
  6.9964066   3.1729455   8.086372    4.0078015   8.096493    5.813139
  8.181514    6.97417     7.246071    5.0315676  10.666024    6.2678714
  3.6855178   5.8637457  -0.4179731   2.747595  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 11: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 08:17:54.623 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6707.8735, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 126s - loss: nan - MinusLogProbMetric: 6707.8735 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 126s/epoch - 643ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 0.0003333333333333333.
===========
Generating train data for run 350.
===========
Train data generated in 0.25 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_350/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_350/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_350/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_350
self.data_kwargs: {'seed': 440}
self.x_data: [[ 2.0547452   3.776408    6.9601746  ...  5.8637457  -0.4179731
   2.747595  ]
 [ 6.405085    2.999549    6.1033454  ...  2.8107233   3.0569289
   2.003619  ]
 [ 2.0047038   4.064143    6.2048492  ...  5.6568336   0.71395487
   2.6926432 ]
 ...
 [ 5.645455    7.0971727   6.202566   ...  0.61970913  6.238492
   1.3539525 ]
 [ 6.475603    3.1151166   6.2248073  ...  2.8990176   3.6724448
   2.289971  ]
 [ 2.2290072   4.101192    7.1413527  ...  6.717821   -2.0046902
   2.3999293 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_483"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_484 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_43 (LogProbL  (None,)                  3291840   
 ayer)                                                           
                                                                 
=================================================================
Total params: 3,291,840
Trainable params: 3,291,840
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_43/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_43'")
self.model: <keras.engine.functional.Functional object at 0x7f57884994e0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f57a0351930>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f57a0351930>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f5970c06e60>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f5788400d00>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f5788401270>, <keras.callbacks.ModelCheckpoint object at 0x7f5788401330>, <keras.callbacks.EarlyStopping object at 0x7f57884015a0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f57884015d0>, <keras.callbacks.TerminateOnNaN object at 0x7f5788401210>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_350/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 350/720 with hyperparameters:
timestamp = 2023-10-25 08:18:03.713648
ndims = 64
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 3291840
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 2.0547452   3.776408    6.9601746   1.2163222   8.644804    1.0450234
  9.756204    5.9333344   9.3759165   6.2056174   7.6663713   0.8787037
  2.7781603   1.7675867   3.9287794   0.8286338   3.6572614   4.846633
  0.68292063  6.317236    6.48625     3.843101    5.88724     2.0270782
  5.1297107   7.7579265   2.6054757   6.6726594   1.0543873   7.123391
  3.5526195   2.046467    5.620422   -0.869851    8.648443    0.02500496
  7.4141846   2.6777635   7.6790566   9.76465     2.481489    6.2672224
  5.7068043   4.504637    1.1161722   9.691578    4.0927534   8.503739
  6.9964066   3.1729455   8.086372    4.0078015   8.096493    5.813139
  8.181514    6.97417     7.246071    5.0315676  10.666024    6.2678714
  3.6855178   5.8637457  -0.4179731   2.747595  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 2: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 08:20:33.032 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6804.6235, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 149s - loss: nan - MinusLogProbMetric: 6804.6235 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 149s/epoch - 760ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 0.0001111111111111111.
===========
Generating train data for run 350.
===========
Train data generated in 0.23 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_350/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_350/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_350/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_350
self.data_kwargs: {'seed': 440}
self.x_data: [[ 2.0547452   3.776408    6.9601746  ...  5.8637457  -0.4179731
   2.747595  ]
 [ 6.405085    2.999549    6.1033454  ...  2.8107233   3.0569289
   2.003619  ]
 [ 2.0047038   4.064143    6.2048492  ...  5.6568336   0.71395487
   2.6926432 ]
 ...
 [ 5.645455    7.0971727   6.202566   ...  0.61970913  6.238492
   1.3539525 ]
 [ 6.475603    3.1151166   6.2248073  ...  2.8990176   3.6724448
   2.289971  ]
 [ 2.2290072   4.101192    7.1413527  ...  6.717821   -2.0046902
   2.3999293 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_494"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_495 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_44 (LogProbL  (None,)                  3291840   
 ayer)                                                           
                                                                 
=================================================================
Total params: 3,291,840
Trainable params: 3,291,840
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_44/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_44'")
self.model: <keras.engine.functional.Functional object at 0x7f57b054be80>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f5779e28a30>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f5779e28a30>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f5554445750>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f58341ec4f0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f58341ec430>, <keras.callbacks.ModelCheckpoint object at 0x7f58341ef850>, <keras.callbacks.EarlyStopping object at 0x7f58341ec6d0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f58341ed6f0>, <keras.callbacks.TerminateOnNaN object at 0x7f58341ecca0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_350/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 350/720 with hyperparameters:
timestamp = 2023-10-25 08:20:41.637450
ndims = 64
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 3291840
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 2.0547452   3.776408    6.9601746   1.2163222   8.644804    1.0450234
  9.756204    5.9333344   9.3759165   6.2056174   7.6663713   0.8787037
  2.7781603   1.7675867   3.9287794   0.8286338   3.6572614   4.846633
  0.68292063  6.317236    6.48625     3.843101    5.88724     2.0270782
  5.1297107   7.7579265   2.6054757   6.6726594   1.0543873   7.123391
  3.5526195   2.046467    5.620422   -0.869851    8.648443    0.02500496
  7.4141846   2.6777635   7.6790566   9.76465     2.481489    6.2672224
  5.7068043   4.504637    1.1161722   9.691578    4.0927534   8.503739
  6.9964066   3.1729455   8.086372    4.0078015   8.096493    5.813139
  8.181514    6.97417     7.246071    5.0315676  10.666024    6.2678714
  3.6855178   5.8637457  -0.4179731   2.747595  ]
Epoch 1/1000
2023-10-25 08:23:42.746 
Epoch 1/1000 
	 loss: 3446.7590, MinusLogProbMetric: 3446.7590, val_loss: 1289.5371, val_MinusLogProbMetric: 1289.5371

Epoch 1: val_loss improved from inf to 1289.53711, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 182s - loss: 3446.7590 - MinusLogProbMetric: 3446.7590 - val_loss: 1289.5371 - val_MinusLogProbMetric: 1289.5371 - lr: 1.1111e-04 - 182s/epoch - 928ms/step
Epoch 2/1000
2023-10-25 08:24:49.556 
Epoch 2/1000 
	 loss: 1117.7174, MinusLogProbMetric: 1117.7174, val_loss: 827.3810, val_MinusLogProbMetric: 827.3810

Epoch 2: val_loss improved from 1289.53711 to 827.38104, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 67s - loss: 1117.7174 - MinusLogProbMetric: 1117.7174 - val_loss: 827.3810 - val_MinusLogProbMetric: 827.3810 - lr: 1.1111e-04 - 67s/epoch - 340ms/step
Epoch 3/1000
2023-10-25 08:25:56.147 
Epoch 3/1000 
	 loss: 689.1760, MinusLogProbMetric: 689.1760, val_loss: 601.9383, val_MinusLogProbMetric: 601.9383

Epoch 3: val_loss improved from 827.38104 to 601.93829, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 67s - loss: 689.1760 - MinusLogProbMetric: 689.1760 - val_loss: 601.9383 - val_MinusLogProbMetric: 601.9383 - lr: 1.1111e-04 - 67s/epoch - 340ms/step
Epoch 4/1000
2023-10-25 08:27:02.071 
Epoch 4/1000 
	 loss: 563.1014, MinusLogProbMetric: 563.1014, val_loss: 509.7760, val_MinusLogProbMetric: 509.7760

Epoch 4: val_loss improved from 601.93829 to 509.77597, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 66s - loss: 563.1014 - MinusLogProbMetric: 563.1014 - val_loss: 509.7760 - val_MinusLogProbMetric: 509.7760 - lr: 1.1111e-04 - 66s/epoch - 336ms/step
Epoch 5/1000
2023-10-25 08:28:10.549 
Epoch 5/1000 
	 loss: 522.4209, MinusLogProbMetric: 522.4209, val_loss: 536.6489, val_MinusLogProbMetric: 536.6489

Epoch 5: val_loss did not improve from 509.77597
196/196 - 68s - loss: 522.4209 - MinusLogProbMetric: 522.4209 - val_loss: 536.6489 - val_MinusLogProbMetric: 536.6489 - lr: 1.1111e-04 - 68s/epoch - 345ms/step
Epoch 6/1000
2023-10-25 08:29:18.397 
Epoch 6/1000 
	 loss: 461.4681, MinusLogProbMetric: 461.4681, val_loss: 428.2999, val_MinusLogProbMetric: 428.2999

Epoch 6: val_loss improved from 509.77597 to 428.29990, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 69s - loss: 461.4681 - MinusLogProbMetric: 461.4681 - val_loss: 428.2999 - val_MinusLogProbMetric: 428.2999 - lr: 1.1111e-04 - 69s/epoch - 351ms/step
Epoch 7/1000
2023-10-25 08:30:25.637 
Epoch 7/1000 
	 loss: 423.0390, MinusLogProbMetric: 423.0390, val_loss: 501.4986, val_MinusLogProbMetric: 501.4986

Epoch 7: val_loss did not improve from 428.29990
196/196 - 66s - loss: 423.0390 - MinusLogProbMetric: 423.0390 - val_loss: 501.4986 - val_MinusLogProbMetric: 501.4986 - lr: 1.1111e-04 - 66s/epoch - 338ms/step
Epoch 8/1000
2023-10-25 08:31:32.882 
Epoch 8/1000 
	 loss: 459.2026, MinusLogProbMetric: 459.2026, val_loss: 404.6685, val_MinusLogProbMetric: 404.6685

Epoch 8: val_loss improved from 428.29990 to 404.66855, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 68s - loss: 459.2026 - MinusLogProbMetric: 459.2026 - val_loss: 404.6685 - val_MinusLogProbMetric: 404.6685 - lr: 1.1111e-04 - 68s/epoch - 349ms/step
Epoch 9/1000
2023-10-25 08:32:42.705 
Epoch 9/1000 
	 loss: 390.7598, MinusLogProbMetric: 390.7598, val_loss: 379.3639, val_MinusLogProbMetric: 379.3639

Epoch 9: val_loss improved from 404.66855 to 379.36386, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 70s - loss: 390.7598 - MinusLogProbMetric: 390.7598 - val_loss: 379.3639 - val_MinusLogProbMetric: 379.3639 - lr: 1.1111e-04 - 70s/epoch - 356ms/step
Epoch 10/1000
2023-10-25 08:33:51.211 
Epoch 10/1000 
	 loss: 417.4834, MinusLogProbMetric: 417.4834, val_loss: 439.9560, val_MinusLogProbMetric: 439.9560

Epoch 10: val_loss did not improve from 379.36386
196/196 - 67s - loss: 417.4834 - MinusLogProbMetric: 417.4834 - val_loss: 439.9560 - val_MinusLogProbMetric: 439.9560 - lr: 1.1111e-04 - 67s/epoch - 344ms/step
Epoch 11/1000
2023-10-25 08:34:57.410 
Epoch 11/1000 
	 loss: 468.8387, MinusLogProbMetric: 468.8387, val_loss: 402.2593, val_MinusLogProbMetric: 402.2593

Epoch 11: val_loss did not improve from 379.36386
196/196 - 66s - loss: 468.8387 - MinusLogProbMetric: 468.8387 - val_loss: 402.2593 - val_MinusLogProbMetric: 402.2593 - lr: 1.1111e-04 - 66s/epoch - 338ms/step
Epoch 12/1000
2023-10-25 08:36:05.744 
Epoch 12/1000 
	 loss: 384.7594, MinusLogProbMetric: 384.7594, val_loss: 362.9399, val_MinusLogProbMetric: 362.9399

Epoch 12: val_loss improved from 379.36386 to 362.93994, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 69s - loss: 384.7594 - MinusLogProbMetric: 384.7594 - val_loss: 362.9399 - val_MinusLogProbMetric: 362.9399 - lr: 1.1111e-04 - 69s/epoch - 354ms/step
Epoch 13/1000
2023-10-25 08:37:13.230 
Epoch 13/1000 
	 loss: 361.0312, MinusLogProbMetric: 361.0312, val_loss: 344.1092, val_MinusLogProbMetric: 344.1092

Epoch 13: val_loss improved from 362.93994 to 344.10916, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 67s - loss: 361.0312 - MinusLogProbMetric: 361.0312 - val_loss: 344.1092 - val_MinusLogProbMetric: 344.1092 - lr: 1.1111e-04 - 67s/epoch - 343ms/step
Epoch 14/1000
2023-10-25 08:38:04.297 
Epoch 14/1000 
	 loss: 371.8971, MinusLogProbMetric: 371.8971, val_loss: 349.1250, val_MinusLogProbMetric: 349.1250

Epoch 14: val_loss did not improve from 344.10916
196/196 - 50s - loss: 371.8971 - MinusLogProbMetric: 371.8971 - val_loss: 349.1250 - val_MinusLogProbMetric: 349.1250 - lr: 1.1111e-04 - 50s/epoch - 256ms/step
Epoch 15/1000
2023-10-25 08:38:54.420 
Epoch 15/1000 
	 loss: 338.1219, MinusLogProbMetric: 338.1219, val_loss: 330.0678, val_MinusLogProbMetric: 330.0678

Epoch 15: val_loss improved from 344.10916 to 330.06778, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 51s - loss: 338.1219 - MinusLogProbMetric: 338.1219 - val_loss: 330.0678 - val_MinusLogProbMetric: 330.0678 - lr: 1.1111e-04 - 51s/epoch - 260ms/step
Epoch 16/1000
2023-10-25 08:39:54.488 
Epoch 16/1000 
	 loss: 322.5178, MinusLogProbMetric: 322.5178, val_loss: 309.3491, val_MinusLogProbMetric: 309.3491

Epoch 16: val_loss improved from 330.06778 to 309.34906, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 60s - loss: 322.5178 - MinusLogProbMetric: 322.5178 - val_loss: 309.3491 - val_MinusLogProbMetric: 309.3491 - lr: 1.1111e-04 - 60s/epoch - 306ms/step
Epoch 17/1000
2023-10-25 08:40:43.979 
Epoch 17/1000 
	 loss: 302.1106, MinusLogProbMetric: 302.1106, val_loss: 288.4408, val_MinusLogProbMetric: 288.4408

Epoch 17: val_loss improved from 309.34906 to 288.44080, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 50s - loss: 302.1106 - MinusLogProbMetric: 302.1106 - val_loss: 288.4408 - val_MinusLogProbMetric: 288.4408 - lr: 1.1111e-04 - 50s/epoch - 253ms/step
Epoch 18/1000
2023-10-25 08:41:33.079 
Epoch 18/1000 
	 loss: 280.6382, MinusLogProbMetric: 280.6382, val_loss: 273.3370, val_MinusLogProbMetric: 273.3370

Epoch 18: val_loss improved from 288.44080 to 273.33701, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 49s - loss: 280.6382 - MinusLogProbMetric: 280.6382 - val_loss: 273.3370 - val_MinusLogProbMetric: 273.3370 - lr: 1.1111e-04 - 49s/epoch - 250ms/step
Epoch 19/1000
2023-10-25 08:42:29.180 
Epoch 19/1000 
	 loss: 269.7571, MinusLogProbMetric: 269.7571, val_loss: 263.7812, val_MinusLogProbMetric: 263.7812

Epoch 19: val_loss improved from 273.33701 to 263.78119, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 56s - loss: 269.7571 - MinusLogProbMetric: 269.7571 - val_loss: 263.7812 - val_MinusLogProbMetric: 263.7812 - lr: 1.1111e-04 - 56s/epoch - 287ms/step
Epoch 20/1000
2023-10-25 08:43:18.810 
Epoch 20/1000 
	 loss: 260.1412, MinusLogProbMetric: 260.1412, val_loss: 258.2689, val_MinusLogProbMetric: 258.2689

Epoch 20: val_loss improved from 263.78119 to 258.26892, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 49s - loss: 260.1412 - MinusLogProbMetric: 260.1412 - val_loss: 258.2689 - val_MinusLogProbMetric: 258.2689 - lr: 1.1111e-04 - 49s/epoch - 252ms/step
Epoch 21/1000
2023-10-25 08:44:07.863 
Epoch 21/1000 
	 loss: 254.2196, MinusLogProbMetric: 254.2196, val_loss: 250.7244, val_MinusLogProbMetric: 250.7244

Epoch 21: val_loss improved from 258.26892 to 250.72443, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 49s - loss: 254.2196 - MinusLogProbMetric: 254.2196 - val_loss: 250.7244 - val_MinusLogProbMetric: 250.7244 - lr: 1.1111e-04 - 49s/epoch - 250ms/step
Epoch 22/1000
2023-10-25 08:44:59.460 
Epoch 22/1000 
	 loss: 247.9887, MinusLogProbMetric: 247.9887, val_loss: 252.6103, val_MinusLogProbMetric: 252.6103

Epoch 22: val_loss did not improve from 250.72443
196/196 - 51s - loss: 247.9887 - MinusLogProbMetric: 247.9887 - val_loss: 252.6103 - val_MinusLogProbMetric: 252.6103 - lr: 1.1111e-04 - 51s/epoch - 259ms/step
Epoch 23/1000
2023-10-25 08:45:51.687 
Epoch 23/1000 
	 loss: 241.3501, MinusLogProbMetric: 241.3501, val_loss: 237.3316, val_MinusLogProbMetric: 237.3316

Epoch 23: val_loss improved from 250.72443 to 237.33162, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 53s - loss: 241.3501 - MinusLogProbMetric: 241.3501 - val_loss: 237.3316 - val_MinusLogProbMetric: 237.3316 - lr: 1.1111e-04 - 53s/epoch - 270ms/step
Epoch 24/1000
2023-10-25 08:46:40.200 
Epoch 24/1000 
	 loss: 235.2940, MinusLogProbMetric: 235.2940, val_loss: 232.0626, val_MinusLogProbMetric: 232.0626

Epoch 24: val_loss improved from 237.33162 to 232.06264, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 48s - loss: 235.2940 - MinusLogProbMetric: 235.2940 - val_loss: 232.0626 - val_MinusLogProbMetric: 232.0626 - lr: 1.1111e-04 - 48s/epoch - 247ms/step
Epoch 25/1000
2023-10-25 08:47:30.457 
Epoch 25/1000 
	 loss: 229.8674, MinusLogProbMetric: 229.8674, val_loss: 225.4444, val_MinusLogProbMetric: 225.4444

Epoch 25: val_loss improved from 232.06264 to 225.44443, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 50s - loss: 229.8674 - MinusLogProbMetric: 229.8674 - val_loss: 225.4444 - val_MinusLogProbMetric: 225.4444 - lr: 1.1111e-04 - 50s/epoch - 257ms/step
Epoch 26/1000
2023-10-25 08:48:25.260 
Epoch 26/1000 
	 loss: 224.5240, MinusLogProbMetric: 224.5240, val_loss: 221.5951, val_MinusLogProbMetric: 221.5951

Epoch 26: val_loss improved from 225.44443 to 221.59508, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 55s - loss: 224.5240 - MinusLogProbMetric: 224.5240 - val_loss: 221.5951 - val_MinusLogProbMetric: 221.5951 - lr: 1.1111e-04 - 55s/epoch - 280ms/step
Epoch 27/1000
2023-10-25 08:49:14.586 
Epoch 27/1000 
	 loss: 218.8494, MinusLogProbMetric: 218.8494, val_loss: 222.0770, val_MinusLogProbMetric: 222.0770

Epoch 27: val_loss did not improve from 221.59508
196/196 - 49s - loss: 218.8494 - MinusLogProbMetric: 218.8494 - val_loss: 222.0770 - val_MinusLogProbMetric: 222.0770 - lr: 1.1111e-04 - 49s/epoch - 248ms/step
Epoch 28/1000
2023-10-25 08:50:03.272 
Epoch 28/1000 
	 loss: 217.1894, MinusLogProbMetric: 217.1894, val_loss: 214.3094, val_MinusLogProbMetric: 214.3094

Epoch 28: val_loss improved from 221.59508 to 214.30943, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 50s - loss: 217.1894 - MinusLogProbMetric: 217.1894 - val_loss: 214.3094 - val_MinusLogProbMetric: 214.3094 - lr: 1.1111e-04 - 50s/epoch - 253ms/step
Epoch 29/1000
2023-10-25 08:51:02.313 
Epoch 29/1000 
	 loss: 211.6412, MinusLogProbMetric: 211.6412, val_loss: 208.6454, val_MinusLogProbMetric: 208.6454

Epoch 29: val_loss improved from 214.30943 to 208.64542, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 59s - loss: 211.6412 - MinusLogProbMetric: 211.6412 - val_loss: 208.6454 - val_MinusLogProbMetric: 208.6454 - lr: 1.1111e-04 - 59s/epoch - 301ms/step
Epoch 30/1000
2023-10-25 08:51:52.324 
Epoch 30/1000 
	 loss: 206.5219, MinusLogProbMetric: 206.5219, val_loss: 204.8706, val_MinusLogProbMetric: 204.8706

Epoch 30: val_loss improved from 208.64542 to 204.87062, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 50s - loss: 206.5219 - MinusLogProbMetric: 206.5219 - val_loss: 204.8706 - val_MinusLogProbMetric: 204.8706 - lr: 1.1111e-04 - 50s/epoch - 254ms/step
Epoch 31/1000
2023-10-25 08:52:41.069 
Epoch 31/1000 
	 loss: 202.6296, MinusLogProbMetric: 202.6296, val_loss: 200.6270, val_MinusLogProbMetric: 200.6270

Epoch 31: val_loss improved from 204.87062 to 200.62701, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 49s - loss: 202.6296 - MinusLogProbMetric: 202.6296 - val_loss: 200.6270 - val_MinusLogProbMetric: 200.6270 - lr: 1.1111e-04 - 49s/epoch - 248ms/step
Epoch 32/1000
2023-10-25 08:53:31.721 
Epoch 32/1000 
	 loss: 203.4358, MinusLogProbMetric: 203.4358, val_loss: 204.3329, val_MinusLogProbMetric: 204.3329

Epoch 32: val_loss did not improve from 200.62701
196/196 - 50s - loss: 203.4358 - MinusLogProbMetric: 203.4358 - val_loss: 204.3329 - val_MinusLogProbMetric: 204.3329 - lr: 1.1111e-04 - 50s/epoch - 255ms/step
Epoch 33/1000
2023-10-25 08:54:23.201 
Epoch 33/1000 
	 loss: 198.3092, MinusLogProbMetric: 198.3092, val_loss: 195.5492, val_MinusLogProbMetric: 195.5492

Epoch 33: val_loss improved from 200.62701 to 195.54916, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 52s - loss: 198.3092 - MinusLogProbMetric: 198.3092 - val_loss: 195.5492 - val_MinusLogProbMetric: 195.5492 - lr: 1.1111e-04 - 52s/epoch - 265ms/step
Epoch 34/1000
2023-10-25 08:55:11.720 
Epoch 34/1000 
	 loss: 198.0090, MinusLogProbMetric: 198.0090, val_loss: 192.9959, val_MinusLogProbMetric: 192.9959

Epoch 34: val_loss improved from 195.54916 to 192.99593, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 49s - loss: 198.0090 - MinusLogProbMetric: 198.0090 - val_loss: 192.9959 - val_MinusLogProbMetric: 192.9959 - lr: 1.1111e-04 - 49s/epoch - 249ms/step
Epoch 35/1000
2023-10-25 08:56:02.937 
Epoch 35/1000 
	 loss: 191.0596, MinusLogProbMetric: 191.0596, val_loss: 189.9341, val_MinusLogProbMetric: 189.9341

Epoch 35: val_loss improved from 192.99593 to 189.93411, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 51s - loss: 191.0596 - MinusLogProbMetric: 191.0596 - val_loss: 189.9341 - val_MinusLogProbMetric: 189.9341 - lr: 1.1111e-04 - 51s/epoch - 262ms/step
Epoch 36/1000
2023-10-25 08:56:56.754 
Epoch 36/1000 
	 loss: 190.0847, MinusLogProbMetric: 190.0847, val_loss: 186.7212, val_MinusLogProbMetric: 186.7212

Epoch 36: val_loss improved from 189.93411 to 186.72121, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 54s - loss: 190.0847 - MinusLogProbMetric: 190.0847 - val_loss: 186.7212 - val_MinusLogProbMetric: 186.7212 - lr: 1.1111e-04 - 54s/epoch - 274ms/step
Epoch 37/1000
2023-10-25 08:57:45.888 
Epoch 37/1000 
	 loss: 186.4641, MinusLogProbMetric: 186.4641, val_loss: 185.3271, val_MinusLogProbMetric: 185.3271

Epoch 37: val_loss improved from 186.72121 to 185.32710, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 49s - loss: 186.4641 - MinusLogProbMetric: 186.4641 - val_loss: 185.3271 - val_MinusLogProbMetric: 185.3271 - lr: 1.1111e-04 - 49s/epoch - 251ms/step
Epoch 38/1000
2023-10-25 08:58:35.259 
Epoch 38/1000 
	 loss: 181.6580, MinusLogProbMetric: 181.6580, val_loss: 179.5150, val_MinusLogProbMetric: 179.5150

Epoch 38: val_loss improved from 185.32710 to 179.51500, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 49s - loss: 181.6580 - MinusLogProbMetric: 181.6580 - val_loss: 179.5150 - val_MinusLogProbMetric: 179.5150 - lr: 1.1111e-04 - 49s/epoch - 252ms/step
Epoch 39/1000
2023-10-25 08:59:31.519 
Epoch 39/1000 
	 loss: 178.5515, MinusLogProbMetric: 178.5515, val_loss: 176.7272, val_MinusLogProbMetric: 176.7272

Epoch 39: val_loss improved from 179.51500 to 176.72725, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 56s - loss: 178.5515 - MinusLogProbMetric: 178.5515 - val_loss: 176.7272 - val_MinusLogProbMetric: 176.7272 - lr: 1.1111e-04 - 56s/epoch - 288ms/step
Epoch 40/1000
2023-10-25 09:00:23.091 
Epoch 40/1000 
	 loss: 175.0861, MinusLogProbMetric: 175.0861, val_loss: 173.4421, val_MinusLogProbMetric: 173.4421

Epoch 40: val_loss improved from 176.72725 to 173.44208, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 51s - loss: 175.0861 - MinusLogProbMetric: 175.0861 - val_loss: 173.4421 - val_MinusLogProbMetric: 173.4421 - lr: 1.1111e-04 - 51s/epoch - 262ms/step
Epoch 41/1000
2023-10-25 09:01:12.153 
Epoch 41/1000 
	 loss: 172.1820, MinusLogProbMetric: 172.1820, val_loss: 171.0287, val_MinusLogProbMetric: 171.0287

Epoch 41: val_loss improved from 173.44208 to 171.02875, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 49s - loss: 172.1820 - MinusLogProbMetric: 172.1820 - val_loss: 171.0287 - val_MinusLogProbMetric: 171.0287 - lr: 1.1111e-04 - 49s/epoch - 250ms/step
Epoch 42/1000
2023-10-25 09:02:04.177 
Epoch 42/1000 
	 loss: 170.0935, MinusLogProbMetric: 170.0935, val_loss: 169.9218, val_MinusLogProbMetric: 169.9218

Epoch 42: val_loss improved from 171.02875 to 169.92177, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 52s - loss: 170.0935 - MinusLogProbMetric: 170.0935 - val_loss: 169.9218 - val_MinusLogProbMetric: 169.9218 - lr: 1.1111e-04 - 52s/epoch - 265ms/step
Epoch 43/1000
2023-10-25 09:02:59.706 
Epoch 43/1000 
	 loss: 167.6414, MinusLogProbMetric: 167.6414, val_loss: 166.6865, val_MinusLogProbMetric: 166.6865

Epoch 43: val_loss improved from 169.92177 to 166.68648, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 55s - loss: 167.6414 - MinusLogProbMetric: 167.6414 - val_loss: 166.6865 - val_MinusLogProbMetric: 166.6865 - lr: 1.1111e-04 - 55s/epoch - 283ms/step
Epoch 44/1000
2023-10-25 09:03:48.647 
Epoch 44/1000 
	 loss: 166.8697, MinusLogProbMetric: 166.8697, val_loss: 163.0646, val_MinusLogProbMetric: 163.0646

Epoch 44: val_loss improved from 166.68648 to 163.06464, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 49s - loss: 166.8697 - MinusLogProbMetric: 166.8697 - val_loss: 163.0646 - val_MinusLogProbMetric: 163.0646 - lr: 1.1111e-04 - 49s/epoch - 249ms/step
Epoch 45/1000
2023-10-25 09:04:38.877 
Epoch 45/1000 
	 loss: 161.8459, MinusLogProbMetric: 161.8459, val_loss: 161.3407, val_MinusLogProbMetric: 161.3407

Epoch 45: val_loss improved from 163.06464 to 161.34068, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 50s - loss: 161.8459 - MinusLogProbMetric: 161.8459 - val_loss: 161.3407 - val_MinusLogProbMetric: 161.3407 - lr: 1.1111e-04 - 50s/epoch - 257ms/step
Epoch 46/1000
2023-10-25 09:05:32.916 
Epoch 46/1000 
	 loss: 160.9845, MinusLogProbMetric: 160.9845, val_loss: 159.1334, val_MinusLogProbMetric: 159.1334

Epoch 46: val_loss improved from 161.34068 to 159.13342, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 54s - loss: 160.9845 - MinusLogProbMetric: 160.9845 - val_loss: 159.1334 - val_MinusLogProbMetric: 159.1334 - lr: 1.1111e-04 - 54s/epoch - 275ms/step
Epoch 47/1000
2023-10-25 09:06:22.641 
Epoch 47/1000 
	 loss: 157.3658, MinusLogProbMetric: 157.3658, val_loss: 156.2931, val_MinusLogProbMetric: 156.2931

Epoch 47: val_loss improved from 159.13342 to 156.29306, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 50s - loss: 157.3658 - MinusLogProbMetric: 157.3658 - val_loss: 156.2931 - val_MinusLogProbMetric: 156.2931 - lr: 1.1111e-04 - 50s/epoch - 254ms/step
Epoch 48/1000
2023-10-25 09:07:11.328 
Epoch 48/1000 
	 loss: 155.0323, MinusLogProbMetric: 155.0323, val_loss: 154.5217, val_MinusLogProbMetric: 154.5217

Epoch 48: val_loss improved from 156.29306 to 154.52174, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 49s - loss: 155.0323 - MinusLogProbMetric: 155.0323 - val_loss: 154.5217 - val_MinusLogProbMetric: 154.5217 - lr: 1.1111e-04 - 49s/epoch - 248ms/step
Epoch 49/1000
2023-10-25 09:08:06.191 
Epoch 49/1000 
	 loss: 188.0228, MinusLogProbMetric: 188.0228, val_loss: 207.0427, val_MinusLogProbMetric: 207.0427

Epoch 49: val_loss did not improve from 154.52174
196/196 - 54s - loss: 188.0228 - MinusLogProbMetric: 188.0228 - val_loss: 207.0427 - val_MinusLogProbMetric: 207.0427 - lr: 1.1111e-04 - 54s/epoch - 276ms/step
Epoch 50/1000
2023-10-25 09:08:58.442 
Epoch 50/1000 
	 loss: 186.4076, MinusLogProbMetric: 186.4076, val_loss: 178.8287, val_MinusLogProbMetric: 178.8287

Epoch 50: val_loss did not improve from 154.52174
196/196 - 52s - loss: 186.4076 - MinusLogProbMetric: 186.4076 - val_loss: 178.8287 - val_MinusLogProbMetric: 178.8287 - lr: 1.1111e-04 - 52s/epoch - 267ms/step
Epoch 51/1000
2023-10-25 09:09:46.654 
Epoch 51/1000 
	 loss: 174.5204, MinusLogProbMetric: 174.5204, val_loss: 170.9618, val_MinusLogProbMetric: 170.9618

Epoch 51: val_loss did not improve from 154.52174
196/196 - 48s - loss: 174.5204 - MinusLogProbMetric: 174.5204 - val_loss: 170.9618 - val_MinusLogProbMetric: 170.9618 - lr: 1.1111e-04 - 48s/epoch - 246ms/step
Epoch 52/1000
2023-10-25 09:10:35.363 
Epoch 52/1000 
	 loss: 168.4258, MinusLogProbMetric: 168.4258, val_loss: 166.4079, val_MinusLogProbMetric: 166.4079

Epoch 52: val_loss did not improve from 154.52174
196/196 - 49s - loss: 168.4258 - MinusLogProbMetric: 168.4258 - val_loss: 166.4079 - val_MinusLogProbMetric: 166.4079 - lr: 1.1111e-04 - 49s/epoch - 248ms/step
Epoch 53/1000
2023-10-25 09:11:30.880 
Epoch 53/1000 
	 loss: 163.3999, MinusLogProbMetric: 163.3999, val_loss: 160.8247, val_MinusLogProbMetric: 160.8247

Epoch 53: val_loss did not improve from 154.52174
196/196 - 56s - loss: 163.3999 - MinusLogProbMetric: 163.3999 - val_loss: 160.8247 - val_MinusLogProbMetric: 160.8247 - lr: 1.1111e-04 - 56s/epoch - 283ms/step
Epoch 54/1000
2023-10-25 09:12:20.262 
Epoch 54/1000 
	 loss: 220.9939, MinusLogProbMetric: 220.9939, val_loss: 304.1169, val_MinusLogProbMetric: 304.1169

Epoch 54: val_loss did not improve from 154.52174
196/196 - 49s - loss: 220.9939 - MinusLogProbMetric: 220.9939 - val_loss: 304.1169 - val_MinusLogProbMetric: 304.1169 - lr: 1.1111e-04 - 49s/epoch - 252ms/step
Epoch 55/1000
2023-10-25 09:13:08.461 
Epoch 55/1000 
	 loss: 252.7111, MinusLogProbMetric: 252.7111, val_loss: 224.9457, val_MinusLogProbMetric: 224.9457

Epoch 55: val_loss did not improve from 154.52174
196/196 - 48s - loss: 252.7111 - MinusLogProbMetric: 252.7111 - val_loss: 224.9457 - val_MinusLogProbMetric: 224.9457 - lr: 1.1111e-04 - 48s/epoch - 246ms/step
Epoch 56/1000
2023-10-25 09:14:02.459 
Epoch 56/1000 
	 loss: 210.9267, MinusLogProbMetric: 210.9267, val_loss: 201.1376, val_MinusLogProbMetric: 201.1376

Epoch 56: val_loss did not improve from 154.52174
196/196 - 54s - loss: 210.9267 - MinusLogProbMetric: 210.9267 - val_loss: 201.1376 - val_MinusLogProbMetric: 201.1376 - lr: 1.1111e-04 - 54s/epoch - 275ms/step
Epoch 57/1000
2023-10-25 09:14:59.248 
Epoch 57/1000 
	 loss: 194.4699, MinusLogProbMetric: 194.4699, val_loss: 188.3631, val_MinusLogProbMetric: 188.3631

Epoch 57: val_loss did not improve from 154.52174
196/196 - 57s - loss: 194.4699 - MinusLogProbMetric: 194.4699 - val_loss: 188.3631 - val_MinusLogProbMetric: 188.3631 - lr: 1.1111e-04 - 57s/epoch - 290ms/step
Epoch 58/1000
2023-10-25 09:15:47.707 
Epoch 58/1000 
	 loss: 184.3708, MinusLogProbMetric: 184.3708, val_loss: 180.3397, val_MinusLogProbMetric: 180.3397

Epoch 58: val_loss did not improve from 154.52174
196/196 - 48s - loss: 184.3708 - MinusLogProbMetric: 184.3708 - val_loss: 180.3397 - val_MinusLogProbMetric: 180.3397 - lr: 1.1111e-04 - 48s/epoch - 247ms/step
Epoch 59/1000
2023-10-25 09:16:38.782 
Epoch 59/1000 
	 loss: 176.9972, MinusLogProbMetric: 176.9972, val_loss: 174.8551, val_MinusLogProbMetric: 174.8551

Epoch 59: val_loss did not improve from 154.52174
196/196 - 51s - loss: 176.9972 - MinusLogProbMetric: 176.9972 - val_loss: 174.8551 - val_MinusLogProbMetric: 174.8551 - lr: 1.1111e-04 - 51s/epoch - 261ms/step
Epoch 60/1000
2023-10-25 09:17:33.678 
Epoch 60/1000 
	 loss: 181.1044, MinusLogProbMetric: 181.1044, val_loss: 171.6190, val_MinusLogProbMetric: 171.6190

Epoch 60: val_loss did not improve from 154.52174
196/196 - 55s - loss: 181.1044 - MinusLogProbMetric: 181.1044 - val_loss: 171.6190 - val_MinusLogProbMetric: 171.6190 - lr: 1.1111e-04 - 55s/epoch - 280ms/step
Epoch 61/1000
2023-10-25 09:18:32.257 
Epoch 61/1000 
	 loss: 167.8801, MinusLogProbMetric: 167.8801, val_loss: 164.4875, val_MinusLogProbMetric: 164.4875

Epoch 61: val_loss did not improve from 154.52174
196/196 - 59s - loss: 167.8801 - MinusLogProbMetric: 167.8801 - val_loss: 164.4875 - val_MinusLogProbMetric: 164.4875 - lr: 1.1111e-04 - 59s/epoch - 299ms/step
Epoch 62/1000
2023-10-25 09:19:21.406 
Epoch 62/1000 
	 loss: 162.6192, MinusLogProbMetric: 162.6192, val_loss: 162.0281, val_MinusLogProbMetric: 162.0281

Epoch 62: val_loss did not improve from 154.52174
196/196 - 49s - loss: 162.6192 - MinusLogProbMetric: 162.6192 - val_loss: 162.0281 - val_MinusLogProbMetric: 162.0281 - lr: 1.1111e-04 - 49s/epoch - 251ms/step
Epoch 63/1000
2023-10-25 09:20:09.897 
Epoch 63/1000 
	 loss: 186.8195, MinusLogProbMetric: 186.8195, val_loss: 171.3391, val_MinusLogProbMetric: 171.3391

Epoch 63: val_loss did not improve from 154.52174
196/196 - 48s - loss: 186.8195 - MinusLogProbMetric: 186.8195 - val_loss: 171.3391 - val_MinusLogProbMetric: 171.3391 - lr: 1.1111e-04 - 48s/epoch - 247ms/step
Epoch 64/1000
2023-10-25 09:21:00.162 
Epoch 64/1000 
	 loss: 168.5783, MinusLogProbMetric: 168.5783, val_loss: 163.9758, val_MinusLogProbMetric: 163.9758

Epoch 64: val_loss did not improve from 154.52174
196/196 - 50s - loss: 168.5783 - MinusLogProbMetric: 168.5783 - val_loss: 163.9758 - val_MinusLogProbMetric: 163.9758 - lr: 1.1111e-04 - 50s/epoch - 256ms/step
Epoch 65/1000
2023-10-25 09:21:59.861 
Epoch 65/1000 
	 loss: 159.4231, MinusLogProbMetric: 159.4231, val_loss: 157.7533, val_MinusLogProbMetric: 157.7533

Epoch 65: val_loss did not improve from 154.52174
196/196 - 60s - loss: 159.4231 - MinusLogProbMetric: 159.4231 - val_loss: 157.7533 - val_MinusLogProbMetric: 157.7533 - lr: 1.1111e-04 - 60s/epoch - 305ms/step
Epoch 66/1000
2023-10-25 09:22:56.516 
Epoch 66/1000 
	 loss: 155.4931, MinusLogProbMetric: 155.4931, val_loss: 154.3250, val_MinusLogProbMetric: 154.3250

Epoch 66: val_loss improved from 154.52174 to 154.32495, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 58s - loss: 155.4931 - MinusLogProbMetric: 155.4931 - val_loss: 154.3250 - val_MinusLogProbMetric: 154.3250 - lr: 1.1111e-04 - 58s/epoch - 294ms/step
Epoch 67/1000
2023-10-25 09:23:57.045 
Epoch 67/1000 
	 loss: 151.6903, MinusLogProbMetric: 151.6903, val_loss: 149.3996, val_MinusLogProbMetric: 149.3996

Epoch 67: val_loss improved from 154.32495 to 149.39955, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 61s - loss: 151.6903 - MinusLogProbMetric: 151.6903 - val_loss: 149.3996 - val_MinusLogProbMetric: 149.3996 - lr: 1.1111e-04 - 61s/epoch - 309ms/step
Epoch 68/1000
2023-10-25 09:24:57.507 
Epoch 68/1000 
	 loss: 148.4722, MinusLogProbMetric: 148.4722, val_loss: 146.9096, val_MinusLogProbMetric: 146.9096

Epoch 68: val_loss improved from 149.39955 to 146.90964, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 60s - loss: 148.4722 - MinusLogProbMetric: 148.4722 - val_loss: 146.9096 - val_MinusLogProbMetric: 146.9096 - lr: 1.1111e-04 - 60s/epoch - 308ms/step
Epoch 69/1000
2023-10-25 09:25:59.196 
Epoch 69/1000 
	 loss: 145.1839, MinusLogProbMetric: 145.1839, val_loss: 145.6997, val_MinusLogProbMetric: 145.6997

Epoch 69: val_loss improved from 146.90964 to 145.69971, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 62s - loss: 145.1839 - MinusLogProbMetric: 145.1839 - val_loss: 145.6997 - val_MinusLogProbMetric: 145.6997 - lr: 1.1111e-04 - 62s/epoch - 315ms/step
Epoch 70/1000
2023-10-25 09:26:58.483 
Epoch 70/1000 
	 loss: 143.9383, MinusLogProbMetric: 143.9383, val_loss: 143.6127, val_MinusLogProbMetric: 143.6127

Epoch 70: val_loss improved from 145.69971 to 143.61273, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 59s - loss: 143.9383 - MinusLogProbMetric: 143.9383 - val_loss: 143.6127 - val_MinusLogProbMetric: 143.6127 - lr: 1.1111e-04 - 59s/epoch - 302ms/step
Epoch 71/1000
2023-10-25 09:27:58.264 
Epoch 71/1000 
	 loss: 141.0287, MinusLogProbMetric: 141.0287, val_loss: 139.8836, val_MinusLogProbMetric: 139.8836

Epoch 71: val_loss improved from 143.61273 to 139.88364, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 60s - loss: 141.0287 - MinusLogProbMetric: 141.0287 - val_loss: 139.8836 - val_MinusLogProbMetric: 139.8836 - lr: 1.1111e-04 - 60s/epoch - 306ms/step
Epoch 72/1000
2023-10-25 09:28:58.093 
Epoch 72/1000 
	 loss: 138.7578, MinusLogProbMetric: 138.7578, val_loss: 138.0622, val_MinusLogProbMetric: 138.0622

Epoch 72: val_loss improved from 139.88364 to 138.06219, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 60s - loss: 138.7578 - MinusLogProbMetric: 138.7578 - val_loss: 138.0622 - val_MinusLogProbMetric: 138.0622 - lr: 1.1111e-04 - 60s/epoch - 305ms/step
Epoch 73/1000
2023-10-25 09:29:56.083 
Epoch 73/1000 
	 loss: 136.6482, MinusLogProbMetric: 136.6482, val_loss: 136.1051, val_MinusLogProbMetric: 136.1051

Epoch 73: val_loss improved from 138.06219 to 136.10507, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 58s - loss: 136.6482 - MinusLogProbMetric: 136.6482 - val_loss: 136.1051 - val_MinusLogProbMetric: 136.1051 - lr: 1.1111e-04 - 58s/epoch - 296ms/step
Epoch 74/1000
2023-10-25 09:30:56.171 
Epoch 74/1000 
	 loss: 133.8661, MinusLogProbMetric: 133.8661, val_loss: 133.4769, val_MinusLogProbMetric: 133.4769

Epoch 74: val_loss improved from 136.10507 to 133.47685, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 60s - loss: 133.8661 - MinusLogProbMetric: 133.8661 - val_loss: 133.4769 - val_MinusLogProbMetric: 133.4769 - lr: 1.1111e-04 - 60s/epoch - 306ms/step
Epoch 75/1000
2023-10-25 09:31:57.220 
Epoch 75/1000 
	 loss: 132.0147, MinusLogProbMetric: 132.0147, val_loss: 131.7865, val_MinusLogProbMetric: 131.7865

Epoch 75: val_loss improved from 133.47685 to 131.78651, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 61s - loss: 132.0147 - MinusLogProbMetric: 132.0147 - val_loss: 131.7865 - val_MinusLogProbMetric: 131.7865 - lr: 1.1111e-04 - 61s/epoch - 311ms/step
Epoch 76/1000
2023-10-25 09:32:55.787 
Epoch 76/1000 
	 loss: 130.2693, MinusLogProbMetric: 130.2693, val_loss: 129.9094, val_MinusLogProbMetric: 129.9094

Epoch 76: val_loss improved from 131.78651 to 129.90939, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 59s - loss: 130.2693 - MinusLogProbMetric: 130.2693 - val_loss: 129.9094 - val_MinusLogProbMetric: 129.9094 - lr: 1.1111e-04 - 59s/epoch - 299ms/step
Epoch 77/1000
2023-10-25 09:33:53.966 
Epoch 77/1000 
	 loss: 128.7926, MinusLogProbMetric: 128.7926, val_loss: 128.1090, val_MinusLogProbMetric: 128.1090

Epoch 77: val_loss improved from 129.90939 to 128.10902, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 58s - loss: 128.7926 - MinusLogProbMetric: 128.7926 - val_loss: 128.1090 - val_MinusLogProbMetric: 128.1090 - lr: 1.1111e-04 - 58s/epoch - 297ms/step
Epoch 78/1000
2023-10-25 09:34:52.179 
Epoch 78/1000 
	 loss: 127.6042, MinusLogProbMetric: 127.6042, val_loss: 126.8722, val_MinusLogProbMetric: 126.8722

Epoch 78: val_loss improved from 128.10902 to 126.87221, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 58s - loss: 127.6042 - MinusLogProbMetric: 127.6042 - val_loss: 126.8722 - val_MinusLogProbMetric: 126.8722 - lr: 1.1111e-04 - 58s/epoch - 297ms/step
Epoch 79/1000
2023-10-25 09:35:50.480 
Epoch 79/1000 
	 loss: 126.1304, MinusLogProbMetric: 126.1304, val_loss: 125.7373, val_MinusLogProbMetric: 125.7373

Epoch 79: val_loss improved from 126.87221 to 125.73727, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 58s - loss: 126.1304 - MinusLogProbMetric: 126.1304 - val_loss: 125.7373 - val_MinusLogProbMetric: 125.7373 - lr: 1.1111e-04 - 58s/epoch - 296ms/step
Epoch 80/1000
2023-10-25 09:36:47.638 
Epoch 80/1000 
	 loss: 124.8726, MinusLogProbMetric: 124.8726, val_loss: 124.2000, val_MinusLogProbMetric: 124.2000

Epoch 80: val_loss improved from 125.73727 to 124.19995, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 57s - loss: 124.8726 - MinusLogProbMetric: 124.8726 - val_loss: 124.2000 - val_MinusLogProbMetric: 124.2000 - lr: 1.1111e-04 - 57s/epoch - 291ms/step
Epoch 81/1000
2023-10-25 09:37:48.278 
Epoch 81/1000 
	 loss: 123.7603, MinusLogProbMetric: 123.7603, val_loss: 123.4994, val_MinusLogProbMetric: 123.4994

Epoch 81: val_loss improved from 124.19995 to 123.49940, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 61s - loss: 123.7603 - MinusLogProbMetric: 123.7603 - val_loss: 123.4994 - val_MinusLogProbMetric: 123.4994 - lr: 1.1111e-04 - 61s/epoch - 311ms/step
Epoch 82/1000
2023-10-25 09:38:47.505 
Epoch 82/1000 
	 loss: 123.3057, MinusLogProbMetric: 123.3057, val_loss: 123.7152, val_MinusLogProbMetric: 123.7152

Epoch 82: val_loss did not improve from 123.49940
196/196 - 58s - loss: 123.3057 - MinusLogProbMetric: 123.3057 - val_loss: 123.7152 - val_MinusLogProbMetric: 123.7152 - lr: 1.1111e-04 - 58s/epoch - 298ms/step
Epoch 83/1000
2023-10-25 09:39:45.947 
Epoch 83/1000 
	 loss: 122.1917, MinusLogProbMetric: 122.1917, val_loss: 121.8295, val_MinusLogProbMetric: 121.8295

Epoch 83: val_loss improved from 123.49940 to 121.82950, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 59s - loss: 122.1917 - MinusLogProbMetric: 122.1917 - val_loss: 121.8295 - val_MinusLogProbMetric: 121.8295 - lr: 1.1111e-04 - 59s/epoch - 302ms/step
Epoch 84/1000
2023-10-25 09:40:45.956 
Epoch 84/1000 
	 loss: 122.8711, MinusLogProbMetric: 122.8711, val_loss: 120.9287, val_MinusLogProbMetric: 120.9287

Epoch 84: val_loss improved from 121.82950 to 120.92866, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 60s - loss: 122.8711 - MinusLogProbMetric: 122.8711 - val_loss: 120.9287 - val_MinusLogProbMetric: 120.9287 - lr: 1.1111e-04 - 60s/epoch - 307ms/step
Epoch 85/1000
2023-10-25 09:41:46.552 
Epoch 85/1000 
	 loss: 120.2319, MinusLogProbMetric: 120.2319, val_loss: 119.8738, val_MinusLogProbMetric: 119.8738

Epoch 85: val_loss improved from 120.92866 to 119.87379, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 60s - loss: 120.2319 - MinusLogProbMetric: 120.2319 - val_loss: 119.8738 - val_MinusLogProbMetric: 119.8738 - lr: 1.1111e-04 - 60s/epoch - 308ms/step
Epoch 86/1000
2023-10-25 09:42:45.960 
Epoch 86/1000 
	 loss: 119.4227, MinusLogProbMetric: 119.4227, val_loss: 119.1087, val_MinusLogProbMetric: 119.1087

Epoch 86: val_loss improved from 119.87379 to 119.10868, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 60s - loss: 119.4227 - MinusLogProbMetric: 119.4227 - val_loss: 119.1087 - val_MinusLogProbMetric: 119.1087 - lr: 1.1111e-04 - 60s/epoch - 305ms/step
Epoch 87/1000
2023-10-25 09:43:45.875 
Epoch 87/1000 
	 loss: 120.5787, MinusLogProbMetric: 120.5787, val_loss: 118.4512, val_MinusLogProbMetric: 118.4512

Epoch 87: val_loss improved from 119.10868 to 118.45121, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 60s - loss: 120.5787 - MinusLogProbMetric: 120.5787 - val_loss: 118.4512 - val_MinusLogProbMetric: 118.4512 - lr: 1.1111e-04 - 60s/epoch - 306ms/step
Epoch 88/1000
2023-10-25 09:44:45.283 
Epoch 88/1000 
	 loss: 117.7658, MinusLogProbMetric: 117.7658, val_loss: 116.8135, val_MinusLogProbMetric: 116.8135

Epoch 88: val_loss improved from 118.45121 to 116.81351, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 59s - loss: 117.7658 - MinusLogProbMetric: 117.7658 - val_loss: 116.8135 - val_MinusLogProbMetric: 116.8135 - lr: 1.1111e-04 - 59s/epoch - 302ms/step
Epoch 89/1000
2023-10-25 09:45:44.579 
Epoch 89/1000 
	 loss: 116.1388, MinusLogProbMetric: 116.1388, val_loss: 116.8284, val_MinusLogProbMetric: 116.8284

Epoch 89: val_loss did not improve from 116.81351
196/196 - 58s - loss: 116.1388 - MinusLogProbMetric: 116.1388 - val_loss: 116.8284 - val_MinusLogProbMetric: 116.8284 - lr: 1.1111e-04 - 58s/epoch - 298ms/step
Epoch 90/1000
2023-10-25 09:46:40.395 
Epoch 90/1000 
	 loss: 114.9998, MinusLogProbMetric: 114.9998, val_loss: 115.0403, val_MinusLogProbMetric: 115.0403

Epoch 90: val_loss improved from 116.81351 to 115.04027, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 57s - loss: 114.9998 - MinusLogProbMetric: 114.9998 - val_loss: 115.0403 - val_MinusLogProbMetric: 115.0403 - lr: 1.1111e-04 - 57s/epoch - 289ms/step
Epoch 91/1000
2023-10-25 09:47:38.099 
Epoch 91/1000 
	 loss: 113.9670, MinusLogProbMetric: 113.9670, val_loss: 114.3170, val_MinusLogProbMetric: 114.3170

Epoch 91: val_loss improved from 115.04027 to 114.31700, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 58s - loss: 113.9670 - MinusLogProbMetric: 113.9670 - val_loss: 114.3170 - val_MinusLogProbMetric: 114.3170 - lr: 1.1111e-04 - 58s/epoch - 295ms/step
Epoch 92/1000
2023-10-25 09:48:36.888 
Epoch 92/1000 
	 loss: 112.8553, MinusLogProbMetric: 112.8553, val_loss: 112.6888, val_MinusLogProbMetric: 112.6888

Epoch 92: val_loss improved from 114.31700 to 112.68884, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 59s - loss: 112.8553 - MinusLogProbMetric: 112.8553 - val_loss: 112.6888 - val_MinusLogProbMetric: 112.6888 - lr: 1.1111e-04 - 59s/epoch - 300ms/step
Epoch 93/1000
2023-10-25 09:49:35.109 
Epoch 93/1000 
	 loss: 111.9087, MinusLogProbMetric: 111.9087, val_loss: 112.0688, val_MinusLogProbMetric: 112.0688

Epoch 93: val_loss improved from 112.68884 to 112.06879, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 58s - loss: 111.9087 - MinusLogProbMetric: 111.9087 - val_loss: 112.0688 - val_MinusLogProbMetric: 112.0688 - lr: 1.1111e-04 - 58s/epoch - 297ms/step
Epoch 94/1000
2023-10-25 09:50:34.027 
Epoch 94/1000 
	 loss: 115.7610, MinusLogProbMetric: 115.7610, val_loss: 112.8942, val_MinusLogProbMetric: 112.8942

Epoch 94: val_loss did not improve from 112.06879
196/196 - 58s - loss: 115.7610 - MinusLogProbMetric: 115.7610 - val_loss: 112.8942 - val_MinusLogProbMetric: 112.8942 - lr: 1.1111e-04 - 58s/epoch - 297ms/step
Epoch 95/1000
2023-10-25 09:51:31.863 
Epoch 95/1000 
	 loss: 119.4527, MinusLogProbMetric: 119.4527, val_loss: 125.2770, val_MinusLogProbMetric: 125.2770

Epoch 95: val_loss did not improve from 112.06879
196/196 - 58s - loss: 119.4527 - MinusLogProbMetric: 119.4527 - val_loss: 125.2770 - val_MinusLogProbMetric: 125.2770 - lr: 1.1111e-04 - 58s/epoch - 295ms/step
Epoch 96/1000
2023-10-25 09:52:29.002 
Epoch 96/1000 
	 loss: 119.1612, MinusLogProbMetric: 119.1612, val_loss: 117.4930, val_MinusLogProbMetric: 117.4930

Epoch 96: val_loss did not improve from 112.06879
196/196 - 57s - loss: 119.1612 - MinusLogProbMetric: 119.1612 - val_loss: 117.4930 - val_MinusLogProbMetric: 117.4930 - lr: 1.1111e-04 - 57s/epoch - 292ms/step
Epoch 97/1000
2023-10-25 09:53:26.567 
Epoch 97/1000 
	 loss: 114.0645, MinusLogProbMetric: 114.0645, val_loss: 112.5670, val_MinusLogProbMetric: 112.5670

Epoch 97: val_loss did not improve from 112.06879
196/196 - 58s - loss: 114.0645 - MinusLogProbMetric: 114.0645 - val_loss: 112.5670 - val_MinusLogProbMetric: 112.5670 - lr: 1.1111e-04 - 58s/epoch - 294ms/step
Epoch 98/1000
2023-10-25 09:54:22.445 
Epoch 98/1000 
	 loss: 112.1709, MinusLogProbMetric: 112.1709, val_loss: 111.5380, val_MinusLogProbMetric: 111.5380

Epoch 98: val_loss improved from 112.06879 to 111.53803, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 57s - loss: 112.1709 - MinusLogProbMetric: 112.1709 - val_loss: 111.5380 - val_MinusLogProbMetric: 111.5380 - lr: 1.1111e-04 - 57s/epoch - 290ms/step
Epoch 99/1000
2023-10-25 09:55:20.653 
Epoch 99/1000 
	 loss: 111.0134, MinusLogProbMetric: 111.0134, val_loss: 110.6546, val_MinusLogProbMetric: 110.6546

Epoch 99: val_loss improved from 111.53803 to 110.65459, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 58s - loss: 111.0134 - MinusLogProbMetric: 111.0134 - val_loss: 110.6546 - val_MinusLogProbMetric: 110.6546 - lr: 1.1111e-04 - 58s/epoch - 297ms/step
Epoch 100/1000
2023-10-25 09:56:19.964 
Epoch 100/1000 
	 loss: 109.6008, MinusLogProbMetric: 109.6008, val_loss: 109.8215, val_MinusLogProbMetric: 109.8215

Epoch 100: val_loss improved from 110.65459 to 109.82146, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 59s - loss: 109.6008 - MinusLogProbMetric: 109.6008 - val_loss: 109.8215 - val_MinusLogProbMetric: 109.8215 - lr: 1.1111e-04 - 59s/epoch - 303ms/step
Epoch 101/1000
2023-10-25 09:57:19.618 
Epoch 101/1000 
	 loss: 109.8827, MinusLogProbMetric: 109.8827, val_loss: 108.2926, val_MinusLogProbMetric: 108.2926

Epoch 101: val_loss improved from 109.82146 to 108.29261, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 59s - loss: 109.8827 - MinusLogProbMetric: 109.8827 - val_loss: 108.2926 - val_MinusLogProbMetric: 108.2926 - lr: 1.1111e-04 - 59s/epoch - 304ms/step
Epoch 102/1000
2023-10-25 09:58:16.343 
Epoch 102/1000 
	 loss: 108.0723, MinusLogProbMetric: 108.0723, val_loss: 108.1265, val_MinusLogProbMetric: 108.1265

Epoch 102: val_loss improved from 108.29261 to 108.12650, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 57s - loss: 108.0723 - MinusLogProbMetric: 108.0723 - val_loss: 108.1265 - val_MinusLogProbMetric: 108.1265 - lr: 1.1111e-04 - 57s/epoch - 290ms/step
Epoch 103/1000
2023-10-25 09:59:13.932 
Epoch 103/1000 
	 loss: 107.3346, MinusLogProbMetric: 107.3346, val_loss: 107.1463, val_MinusLogProbMetric: 107.1463

Epoch 103: val_loss improved from 108.12650 to 107.14632, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 58s - loss: 107.3346 - MinusLogProbMetric: 107.3346 - val_loss: 107.1463 - val_MinusLogProbMetric: 107.1463 - lr: 1.1111e-04 - 58s/epoch - 294ms/step
Epoch 104/1000
2023-10-25 10:00:12.554 
Epoch 104/1000 
	 loss: 106.7315, MinusLogProbMetric: 106.7315, val_loss: 106.3854, val_MinusLogProbMetric: 106.3854

Epoch 104: val_loss improved from 107.14632 to 106.38544, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 59s - loss: 106.7315 - MinusLogProbMetric: 106.7315 - val_loss: 106.3854 - val_MinusLogProbMetric: 106.3854 - lr: 1.1111e-04 - 59s/epoch - 299ms/step
Epoch 105/1000
2023-10-25 10:01:13.506 
Epoch 105/1000 
	 loss: 105.7740, MinusLogProbMetric: 105.7740, val_loss: 105.5045, val_MinusLogProbMetric: 105.5045

Epoch 105: val_loss improved from 106.38544 to 105.50448, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 61s - loss: 105.7740 - MinusLogProbMetric: 105.7740 - val_loss: 105.5045 - val_MinusLogProbMetric: 105.5045 - lr: 1.1111e-04 - 61s/epoch - 309ms/step
Epoch 106/1000
2023-10-25 10:02:15.428 
Epoch 106/1000 
	 loss: 105.0332, MinusLogProbMetric: 105.0332, val_loss: 105.3074, val_MinusLogProbMetric: 105.3074

Epoch 106: val_loss improved from 105.50448 to 105.30739, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 62s - loss: 105.0332 - MinusLogProbMetric: 105.0332 - val_loss: 105.3074 - val_MinusLogProbMetric: 105.3074 - lr: 1.1111e-04 - 62s/epoch - 318ms/step
Epoch 107/1000
2023-10-25 10:03:15.779 
Epoch 107/1000 
	 loss: 104.8432, MinusLogProbMetric: 104.8432, val_loss: 103.9196, val_MinusLogProbMetric: 103.9196

Epoch 107: val_loss improved from 105.30739 to 103.91964, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 66s - loss: 104.8432 - MinusLogProbMetric: 104.8432 - val_loss: 103.9196 - val_MinusLogProbMetric: 103.9196 - lr: 1.1111e-04 - 66s/epoch - 337ms/step
Epoch 108/1000
2023-10-25 10:04:20.046 
Epoch 108/1000 
	 loss: 103.8163, MinusLogProbMetric: 103.8163, val_loss: 104.2386, val_MinusLogProbMetric: 104.2386

Epoch 108: val_loss did not improve from 103.91964
196/196 - 58s - loss: 103.8163 - MinusLogProbMetric: 103.8163 - val_loss: 104.2386 - val_MinusLogProbMetric: 104.2386 - lr: 1.1111e-04 - 58s/epoch - 294ms/step
Epoch 109/1000
2023-10-25 10:05:17.855 
Epoch 109/1000 
	 loss: 103.5146, MinusLogProbMetric: 103.5146, val_loss: 102.8396, val_MinusLogProbMetric: 102.8396

Epoch 109: val_loss improved from 103.91964 to 102.83956, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 59s - loss: 103.5146 - MinusLogProbMetric: 103.5146 - val_loss: 102.8396 - val_MinusLogProbMetric: 102.8396 - lr: 1.1111e-04 - 59s/epoch - 300ms/step
Epoch 110/1000
2023-10-25 10:06:19.476 
Epoch 110/1000 
	 loss: 103.6879, MinusLogProbMetric: 103.6879, val_loss: 103.2981, val_MinusLogProbMetric: 103.2981

Epoch 110: val_loss did not improve from 102.83956
196/196 - 61s - loss: 103.6879 - MinusLogProbMetric: 103.6879 - val_loss: 103.2981 - val_MinusLogProbMetric: 103.2981 - lr: 1.1111e-04 - 61s/epoch - 310ms/step
Epoch 111/1000
2023-10-25 10:07:19.459 
Epoch 111/1000 
	 loss: 102.1499, MinusLogProbMetric: 102.1499, val_loss: 101.5647, val_MinusLogProbMetric: 101.5647

Epoch 111: val_loss improved from 102.83956 to 101.56474, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 61s - loss: 102.1499 - MinusLogProbMetric: 102.1499 - val_loss: 101.5647 - val_MinusLogProbMetric: 101.5647 - lr: 1.1111e-04 - 61s/epoch - 310ms/step
Epoch 112/1000
2023-10-25 10:08:18.520 
Epoch 112/1000 
	 loss: 102.0582, MinusLogProbMetric: 102.0582, val_loss: 102.1286, val_MinusLogProbMetric: 102.1286

Epoch 112: val_loss did not improve from 101.56474
196/196 - 58s - loss: 102.0582 - MinusLogProbMetric: 102.0582 - val_loss: 102.1286 - val_MinusLogProbMetric: 102.1286 - lr: 1.1111e-04 - 58s/epoch - 297ms/step
Epoch 113/1000
2023-10-25 10:09:12.561 
Epoch 113/1000 
	 loss: 102.4059, MinusLogProbMetric: 102.4059, val_loss: 101.3796, val_MinusLogProbMetric: 101.3796

Epoch 113: val_loss improved from 101.56474 to 101.37962, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 55s - loss: 102.4059 - MinusLogProbMetric: 102.4059 - val_loss: 101.3796 - val_MinusLogProbMetric: 101.3796 - lr: 1.1111e-04 - 55s/epoch - 280ms/step
Epoch 114/1000
2023-10-25 10:10:02.450 
Epoch 114/1000 
	 loss: 100.5459, MinusLogProbMetric: 100.5459, val_loss: 100.7859, val_MinusLogProbMetric: 100.7859

Epoch 114: val_loss improved from 101.37962 to 100.78590, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 50s - loss: 100.5459 - MinusLogProbMetric: 100.5459 - val_loss: 100.7859 - val_MinusLogProbMetric: 100.7859 - lr: 1.1111e-04 - 50s/epoch - 254ms/step
Epoch 115/1000
2023-10-25 10:10:53.880 
Epoch 115/1000 
	 loss: 99.8887, MinusLogProbMetric: 99.8887, val_loss: 100.7332, val_MinusLogProbMetric: 100.7332

Epoch 115: val_loss improved from 100.78590 to 100.73323, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 51s - loss: 99.8887 - MinusLogProbMetric: 99.8887 - val_loss: 100.7332 - val_MinusLogProbMetric: 100.7332 - lr: 1.1111e-04 - 51s/epoch - 263ms/step
Epoch 116/1000
2023-10-25 10:11:50.454 
Epoch 116/1000 
	 loss: 99.7256, MinusLogProbMetric: 99.7256, val_loss: 99.6105, val_MinusLogProbMetric: 99.6105

Epoch 116: val_loss improved from 100.73323 to 99.61047, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 57s - loss: 99.7256 - MinusLogProbMetric: 99.7256 - val_loss: 99.6105 - val_MinusLogProbMetric: 99.6105 - lr: 1.1111e-04 - 57s/epoch - 289ms/step
Epoch 117/1000
2023-10-25 10:12:53.266 
Epoch 117/1000 
	 loss: 99.1935, MinusLogProbMetric: 99.1935, val_loss: 98.6534, val_MinusLogProbMetric: 98.6534

Epoch 117: val_loss improved from 99.61047 to 98.65341, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 63s - loss: 99.1935 - MinusLogProbMetric: 99.1935 - val_loss: 98.6534 - val_MinusLogProbMetric: 98.6534 - lr: 1.1111e-04 - 63s/epoch - 320ms/step
Epoch 118/1000
2023-10-25 10:13:54.261 
Epoch 118/1000 
	 loss: 98.9391, MinusLogProbMetric: 98.9391, val_loss: 98.6194, val_MinusLogProbMetric: 98.6194

Epoch 118: val_loss improved from 98.65341 to 98.61937, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 61s - loss: 98.9391 - MinusLogProbMetric: 98.9391 - val_loss: 98.6194 - val_MinusLogProbMetric: 98.6194 - lr: 1.1111e-04 - 61s/epoch - 311ms/step
Epoch 119/1000
2023-10-25 10:14:55.010 
Epoch 119/1000 
	 loss: 98.0511, MinusLogProbMetric: 98.0511, val_loss: 98.4790, val_MinusLogProbMetric: 98.4790

Epoch 119: val_loss improved from 98.61937 to 98.47901, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 61s - loss: 98.0511 - MinusLogProbMetric: 98.0511 - val_loss: 98.4790 - val_MinusLogProbMetric: 98.4790 - lr: 1.1111e-04 - 61s/epoch - 310ms/step
Epoch 120/1000
2023-10-25 10:15:53.735 
Epoch 120/1000 
	 loss: 97.6451, MinusLogProbMetric: 97.6451, val_loss: 97.6075, val_MinusLogProbMetric: 97.6075

Epoch 120: val_loss improved from 98.47901 to 97.60751, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 59s - loss: 97.6451 - MinusLogProbMetric: 97.6451 - val_loss: 97.6075 - val_MinusLogProbMetric: 97.6075 - lr: 1.1111e-04 - 59s/epoch - 299ms/step
Epoch 121/1000
2023-10-25 10:16:54.343 
Epoch 121/1000 
	 loss: 97.8220, MinusLogProbMetric: 97.8220, val_loss: 97.9719, val_MinusLogProbMetric: 97.9719

Epoch 121: val_loss did not improve from 97.60751
196/196 - 60s - loss: 97.8220 - MinusLogProbMetric: 97.8220 - val_loss: 97.9719 - val_MinusLogProbMetric: 97.9719 - lr: 1.1111e-04 - 60s/epoch - 305ms/step
Epoch 122/1000
2023-10-25 10:17:53.101 
Epoch 122/1000 
	 loss: 96.6267, MinusLogProbMetric: 96.6267, val_loss: 96.7896, val_MinusLogProbMetric: 96.7896

Epoch 122: val_loss improved from 97.60751 to 96.78963, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 60s - loss: 96.6267 - MinusLogProbMetric: 96.6267 - val_loss: 96.7896 - val_MinusLogProbMetric: 96.7896 - lr: 1.1111e-04 - 60s/epoch - 304ms/step
Epoch 123/1000
2023-10-25 10:18:53.103 
Epoch 123/1000 
	 loss: 96.1811, MinusLogProbMetric: 96.1811, val_loss: 96.6420, val_MinusLogProbMetric: 96.6420

Epoch 123: val_loss improved from 96.78963 to 96.64197, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 60s - loss: 96.1811 - MinusLogProbMetric: 96.1811 - val_loss: 96.6420 - val_MinusLogProbMetric: 96.6420 - lr: 1.1111e-04 - 60s/epoch - 306ms/step
Epoch 124/1000
2023-10-25 10:19:51.644 
Epoch 124/1000 
	 loss: 95.7489, MinusLogProbMetric: 95.7489, val_loss: 96.4610, val_MinusLogProbMetric: 96.4610

Epoch 124: val_loss improved from 96.64197 to 96.46101, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 59s - loss: 95.7489 - MinusLogProbMetric: 95.7489 - val_loss: 96.4610 - val_MinusLogProbMetric: 96.4610 - lr: 1.1111e-04 - 59s/epoch - 299ms/step
Epoch 125/1000
2023-10-25 10:20:49.598 
Epoch 125/1000 
	 loss: 95.8138, MinusLogProbMetric: 95.8138, val_loss: 95.2845, val_MinusLogProbMetric: 95.2845

Epoch 125: val_loss improved from 96.46101 to 95.28453, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 58s - loss: 95.8138 - MinusLogProbMetric: 95.8138 - val_loss: 95.2845 - val_MinusLogProbMetric: 95.2845 - lr: 1.1111e-04 - 58s/epoch - 296ms/step
Epoch 126/1000
2023-10-25 10:21:51.033 
Epoch 126/1000 
	 loss: 107.6696, MinusLogProbMetric: 107.6696, val_loss: 106.8449, val_MinusLogProbMetric: 106.8449

Epoch 126: val_loss did not improve from 95.28453
196/196 - 60s - loss: 107.6696 - MinusLogProbMetric: 107.6696 - val_loss: 106.8449 - val_MinusLogProbMetric: 106.8449 - lr: 1.1111e-04 - 60s/epoch - 309ms/step
Epoch 127/1000
2023-10-25 10:22:54.179 
Epoch 127/1000 
	 loss: 98.2843, MinusLogProbMetric: 98.2843, val_loss: 96.2114, val_MinusLogProbMetric: 96.2114

Epoch 127: val_loss did not improve from 95.28453
196/196 - 63s - loss: 98.2843 - MinusLogProbMetric: 98.2843 - val_loss: 96.2114 - val_MinusLogProbMetric: 96.2114 - lr: 1.1111e-04 - 63s/epoch - 322ms/step
Epoch 128/1000
2023-10-25 10:23:52.845 
Epoch 128/1000 
	 loss: 95.3427, MinusLogProbMetric: 95.3427, val_loss: 95.4259, val_MinusLogProbMetric: 95.4259

Epoch 128: val_loss did not improve from 95.28453
196/196 - 59s - loss: 95.3427 - MinusLogProbMetric: 95.3427 - val_loss: 95.4259 - val_MinusLogProbMetric: 95.4259 - lr: 1.1111e-04 - 59s/epoch - 299ms/step
Epoch 129/1000
2023-10-25 10:24:52.407 
Epoch 129/1000 
	 loss: 94.5653, MinusLogProbMetric: 94.5653, val_loss: 94.3625, val_MinusLogProbMetric: 94.3625

Epoch 129: val_loss improved from 95.28453 to 94.36246, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 61s - loss: 94.5653 - MinusLogProbMetric: 94.5653 - val_loss: 94.3625 - val_MinusLogProbMetric: 94.3625 - lr: 1.1111e-04 - 61s/epoch - 309ms/step
Epoch 130/1000
2023-10-25 10:25:53.553 
Epoch 130/1000 
	 loss: 94.5674, MinusLogProbMetric: 94.5674, val_loss: 95.2407, val_MinusLogProbMetric: 95.2407

Epoch 130: val_loss did not improve from 94.36246
196/196 - 60s - loss: 94.5674 - MinusLogProbMetric: 94.5674 - val_loss: 95.2407 - val_MinusLogProbMetric: 95.2407 - lr: 1.1111e-04 - 60s/epoch - 307ms/step
Epoch 131/1000
2023-10-25 10:26:58.061 
Epoch 131/1000 
	 loss: 94.0086, MinusLogProbMetric: 94.0086, val_loss: 96.3184, val_MinusLogProbMetric: 96.3184

Epoch 131: val_loss did not improve from 94.36246
196/196 - 65s - loss: 94.0086 - MinusLogProbMetric: 94.0086 - val_loss: 96.3184 - val_MinusLogProbMetric: 96.3184 - lr: 1.1111e-04 - 65s/epoch - 329ms/step
Epoch 132/1000
2023-10-25 10:27:57.690 
Epoch 132/1000 
	 loss: 93.8350, MinusLogProbMetric: 93.8350, val_loss: 93.6473, val_MinusLogProbMetric: 93.6473

Epoch 132: val_loss improved from 94.36246 to 93.64728, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 61s - loss: 93.8350 - MinusLogProbMetric: 93.8350 - val_loss: 93.6473 - val_MinusLogProbMetric: 93.6473 - lr: 1.1111e-04 - 61s/epoch - 309ms/step
Epoch 133/1000
2023-10-25 10:28:59.234 
Epoch 133/1000 
	 loss: 92.8159, MinusLogProbMetric: 92.8159, val_loss: 93.0037, val_MinusLogProbMetric: 93.0037

Epoch 133: val_loss improved from 93.64728 to 93.00369, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 62s - loss: 92.8159 - MinusLogProbMetric: 92.8159 - val_loss: 93.0037 - val_MinusLogProbMetric: 93.0037 - lr: 1.1111e-04 - 62s/epoch - 314ms/step
Epoch 134/1000
2023-10-25 10:29:57.877 
Epoch 134/1000 
	 loss: 92.9382, MinusLogProbMetric: 92.9382, val_loss: 92.7731, val_MinusLogProbMetric: 92.7731

Epoch 134: val_loss improved from 93.00369 to 92.77310, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 59s - loss: 92.9382 - MinusLogProbMetric: 92.9382 - val_loss: 92.7731 - val_MinusLogProbMetric: 92.7731 - lr: 1.1111e-04 - 59s/epoch - 299ms/step
Epoch 135/1000
2023-10-25 10:30:55.507 
Epoch 135/1000 
	 loss: 91.7800, MinusLogProbMetric: 91.7800, val_loss: 92.4684, val_MinusLogProbMetric: 92.4684

Epoch 135: val_loss improved from 92.77310 to 92.46841, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 58s - loss: 91.7800 - MinusLogProbMetric: 91.7800 - val_loss: 92.4684 - val_MinusLogProbMetric: 92.4684 - lr: 1.1111e-04 - 58s/epoch - 294ms/step
Epoch 136/1000
2023-10-25 10:31:55.776 
Epoch 136/1000 
	 loss: 91.4961, MinusLogProbMetric: 91.4961, val_loss: 91.8821, val_MinusLogProbMetric: 91.8821

Epoch 136: val_loss improved from 92.46841 to 91.88206, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 60s - loss: 91.4961 - MinusLogProbMetric: 91.4961 - val_loss: 91.8821 - val_MinusLogProbMetric: 91.8821 - lr: 1.1111e-04 - 60s/epoch - 307ms/step
Epoch 137/1000
2023-10-25 10:32:55.187 
Epoch 137/1000 
	 loss: 91.1254, MinusLogProbMetric: 91.1254, val_loss: 91.0769, val_MinusLogProbMetric: 91.0769

Epoch 137: val_loss improved from 91.88206 to 91.07688, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 59s - loss: 91.1254 - MinusLogProbMetric: 91.1254 - val_loss: 91.0769 - val_MinusLogProbMetric: 91.0769 - lr: 1.1111e-04 - 59s/epoch - 303ms/step
Epoch 138/1000
2023-10-25 10:33:53.686 
Epoch 138/1000 
	 loss: 91.7210, MinusLogProbMetric: 91.7210, val_loss: 91.0983, val_MinusLogProbMetric: 91.0983

Epoch 138: val_loss did not improve from 91.07688
196/196 - 58s - loss: 91.7210 - MinusLogProbMetric: 91.7210 - val_loss: 91.0983 - val_MinusLogProbMetric: 91.0983 - lr: 1.1111e-04 - 58s/epoch - 294ms/step
Epoch 139/1000
2023-10-25 10:34:50.932 
Epoch 139/1000 
	 loss: 96.8732, MinusLogProbMetric: 96.8732, val_loss: 93.3001, val_MinusLogProbMetric: 93.3001

Epoch 139: val_loss did not improve from 91.07688
196/196 - 57s - loss: 96.8732 - MinusLogProbMetric: 96.8732 - val_loss: 93.3001 - val_MinusLogProbMetric: 93.3001 - lr: 1.1111e-04 - 57s/epoch - 292ms/step
Epoch 140/1000
2023-10-25 10:35:53.961 
Epoch 140/1000 
	 loss: 91.5516, MinusLogProbMetric: 91.5516, val_loss: 91.0237, val_MinusLogProbMetric: 91.0237

Epoch 140: val_loss improved from 91.07688 to 91.02367, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 64s - loss: 91.5516 - MinusLogProbMetric: 91.5516 - val_loss: 91.0237 - val_MinusLogProbMetric: 91.0237 - lr: 1.1111e-04 - 64s/epoch - 326ms/step
Epoch 141/1000
2023-10-25 10:36:54.547 
Epoch 141/1000 
	 loss: 90.4233, MinusLogProbMetric: 90.4233, val_loss: 90.2673, val_MinusLogProbMetric: 90.2673

Epoch 141: val_loss improved from 91.02367 to 90.26727, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 61s - loss: 90.4233 - MinusLogProbMetric: 90.4233 - val_loss: 90.2673 - val_MinusLogProbMetric: 90.2673 - lr: 1.1111e-04 - 61s/epoch - 309ms/step
Epoch 142/1000
2023-10-25 10:37:57.819 
Epoch 142/1000 
	 loss: 89.4887, MinusLogProbMetric: 89.4887, val_loss: 89.4899, val_MinusLogProbMetric: 89.4899

Epoch 142: val_loss improved from 90.26727 to 89.48994, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 63s - loss: 89.4887 - MinusLogProbMetric: 89.4887 - val_loss: 89.4899 - val_MinusLogProbMetric: 89.4899 - lr: 1.1111e-04 - 63s/epoch - 322ms/step
Epoch 143/1000
2023-10-25 10:38:58.305 
Epoch 143/1000 
	 loss: 89.5535, MinusLogProbMetric: 89.5535, val_loss: 89.5389, val_MinusLogProbMetric: 89.5389

Epoch 143: val_loss did not improve from 89.48994
196/196 - 60s - loss: 89.5535 - MinusLogProbMetric: 89.5535 - val_loss: 89.5389 - val_MinusLogProbMetric: 89.5389 - lr: 1.1111e-04 - 60s/epoch - 305ms/step
Epoch 144/1000
2023-10-25 10:39:56.897 
Epoch 144/1000 
	 loss: 88.8791, MinusLogProbMetric: 88.8791, val_loss: 94.0056, val_MinusLogProbMetric: 94.0056

Epoch 144: val_loss did not improve from 89.48994
196/196 - 59s - loss: 88.8791 - MinusLogProbMetric: 88.8791 - val_loss: 94.0056 - val_MinusLogProbMetric: 94.0056 - lr: 1.1111e-04 - 59s/epoch - 299ms/step
Epoch 145/1000
2023-10-25 10:40:57.795 
Epoch 145/1000 
	 loss: 88.6750, MinusLogProbMetric: 88.6750, val_loss: 89.5898, val_MinusLogProbMetric: 89.5898

Epoch 145: val_loss did not improve from 89.48994
196/196 - 61s - loss: 88.6750 - MinusLogProbMetric: 88.6750 - val_loss: 89.5898 - val_MinusLogProbMetric: 89.5898 - lr: 1.1111e-04 - 61s/epoch - 311ms/step
Epoch 146/1000
2023-10-25 10:41:59.188 
Epoch 146/1000 
	 loss: 94.1564, MinusLogProbMetric: 94.1564, val_loss: 96.2723, val_MinusLogProbMetric: 96.2723

Epoch 146: val_loss did not improve from 89.48994
196/196 - 61s - loss: 94.1564 - MinusLogProbMetric: 94.1564 - val_loss: 96.2723 - val_MinusLogProbMetric: 96.2723 - lr: 1.1111e-04 - 61s/epoch - 313ms/step
Epoch 147/1000
2023-10-25 10:42:59.789 
Epoch 147/1000 
	 loss: 89.7814, MinusLogProbMetric: 89.7814, val_loss: 88.4775, val_MinusLogProbMetric: 88.4775

Epoch 147: val_loss improved from 89.48994 to 88.47751, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 61s - loss: 89.7814 - MinusLogProbMetric: 89.7814 - val_loss: 88.4775 - val_MinusLogProbMetric: 88.4775 - lr: 1.1111e-04 - 61s/epoch - 314ms/step
Epoch 148/1000
2023-10-25 10:44:00.310 
Epoch 148/1000 
	 loss: 88.3095, MinusLogProbMetric: 88.3095, val_loss: 88.0687, val_MinusLogProbMetric: 88.0687

Epoch 148: val_loss improved from 88.47751 to 88.06875, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 61s - loss: 88.3095 - MinusLogProbMetric: 88.3095 - val_loss: 88.0687 - val_MinusLogProbMetric: 88.0687 - lr: 1.1111e-04 - 61s/epoch - 309ms/step
Epoch 149/1000
2023-10-25 10:45:00.695 
Epoch 149/1000 
	 loss: 87.6923, MinusLogProbMetric: 87.6923, val_loss: 89.2430, val_MinusLogProbMetric: 89.2430

Epoch 149: val_loss did not improve from 88.06875
196/196 - 60s - loss: 87.6923 - MinusLogProbMetric: 87.6923 - val_loss: 89.2430 - val_MinusLogProbMetric: 89.2430 - lr: 1.1111e-04 - 60s/epoch - 304ms/step
Epoch 150/1000
2023-10-25 10:46:01.447 
Epoch 150/1000 
	 loss: 87.4363, MinusLogProbMetric: 87.4363, val_loss: 87.6449, val_MinusLogProbMetric: 87.6449

Epoch 150: val_loss improved from 88.06875 to 87.64492, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 62s - loss: 87.4363 - MinusLogProbMetric: 87.4363 - val_loss: 87.6449 - val_MinusLogProbMetric: 87.6449 - lr: 1.1111e-04 - 62s/epoch - 315ms/step
Epoch 151/1000
2023-10-25 10:47:03.953 
Epoch 151/1000 
	 loss: 86.9450, MinusLogProbMetric: 86.9450, val_loss: 86.4334, val_MinusLogProbMetric: 86.4334

Epoch 151: val_loss improved from 87.64492 to 86.43338, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 62s - loss: 86.9450 - MinusLogProbMetric: 86.9450 - val_loss: 86.4334 - val_MinusLogProbMetric: 86.4334 - lr: 1.1111e-04 - 62s/epoch - 318ms/step
Epoch 152/1000
2023-10-25 10:48:02.243 
Epoch 152/1000 
	 loss: 86.3322, MinusLogProbMetric: 86.3322, val_loss: 86.8887, val_MinusLogProbMetric: 86.8887

Epoch 152: val_loss did not improve from 86.43338
196/196 - 58s - loss: 86.3322 - MinusLogProbMetric: 86.3322 - val_loss: 86.8887 - val_MinusLogProbMetric: 86.8887 - lr: 1.1111e-04 - 58s/epoch - 294ms/step
Epoch 153/1000
2023-10-25 10:48:59.256 
Epoch 153/1000 
	 loss: 86.1939, MinusLogProbMetric: 86.1939, val_loss: 86.6285, val_MinusLogProbMetric: 86.6285

Epoch 153: val_loss did not improve from 86.43338
196/196 - 57s - loss: 86.1939 - MinusLogProbMetric: 86.1939 - val_loss: 86.6285 - val_MinusLogProbMetric: 86.6285 - lr: 1.1111e-04 - 57s/epoch - 291ms/step
Epoch 154/1000
2023-10-25 10:49:55.763 
Epoch 154/1000 
	 loss: 86.1335, MinusLogProbMetric: 86.1335, val_loss: 86.1007, val_MinusLogProbMetric: 86.1007

Epoch 154: val_loss improved from 86.43338 to 86.10068, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 57s - loss: 86.1335 - MinusLogProbMetric: 86.1335 - val_loss: 86.1007 - val_MinusLogProbMetric: 86.1007 - lr: 1.1111e-04 - 57s/epoch - 293ms/step
Epoch 155/1000
2023-10-25 10:50:55.368 
Epoch 155/1000 
	 loss: 85.5972, MinusLogProbMetric: 85.5972, val_loss: 85.6934, val_MinusLogProbMetric: 85.6934

Epoch 155: val_loss improved from 86.10068 to 85.69340, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 60s - loss: 85.5972 - MinusLogProbMetric: 85.5972 - val_loss: 85.6934 - val_MinusLogProbMetric: 85.6934 - lr: 1.1111e-04 - 60s/epoch - 304ms/step
Epoch 156/1000
2023-10-25 10:51:53.659 
Epoch 156/1000 
	 loss: 85.6308, MinusLogProbMetric: 85.6308, val_loss: 86.5215, val_MinusLogProbMetric: 86.5215

Epoch 156: val_loss did not improve from 85.69340
196/196 - 57s - loss: 85.6308 - MinusLogProbMetric: 85.6308 - val_loss: 86.5215 - val_MinusLogProbMetric: 86.5215 - lr: 1.1111e-04 - 57s/epoch - 292ms/step
Epoch 157/1000
2023-10-25 10:52:51.707 
Epoch 157/1000 
	 loss: 85.5721, MinusLogProbMetric: 85.5721, val_loss: 86.5540, val_MinusLogProbMetric: 86.5540

Epoch 157: val_loss did not improve from 85.69340
196/196 - 58s - loss: 85.5721 - MinusLogProbMetric: 85.5721 - val_loss: 86.5540 - val_MinusLogProbMetric: 86.5540 - lr: 1.1111e-04 - 58s/epoch - 296ms/step
Epoch 158/1000
2023-10-25 10:53:49.246 
Epoch 158/1000 
	 loss: 87.6045, MinusLogProbMetric: 87.6045, val_loss: 86.4359, val_MinusLogProbMetric: 86.4359

Epoch 158: val_loss did not improve from 85.69340
196/196 - 58s - loss: 87.6045 - MinusLogProbMetric: 87.6045 - val_loss: 86.4359 - val_MinusLogProbMetric: 86.4359 - lr: 1.1111e-04 - 58s/epoch - 294ms/step
Epoch 159/1000
2023-10-25 10:54:48.117 
Epoch 159/1000 
	 loss: 85.7815, MinusLogProbMetric: 85.7815, val_loss: 85.3784, val_MinusLogProbMetric: 85.3784

Epoch 159: val_loss improved from 85.69340 to 85.37843, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 60s - loss: 85.7815 - MinusLogProbMetric: 85.7815 - val_loss: 85.3784 - val_MinusLogProbMetric: 85.3784 - lr: 1.1111e-04 - 60s/epoch - 305ms/step
Epoch 160/1000
2023-10-25 10:55:46.765 
Epoch 160/1000 
	 loss: 84.5527, MinusLogProbMetric: 84.5527, val_loss: 84.4913, val_MinusLogProbMetric: 84.4913

Epoch 160: val_loss improved from 85.37843 to 84.49133, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 59s - loss: 84.5527 - MinusLogProbMetric: 84.5527 - val_loss: 84.4913 - val_MinusLogProbMetric: 84.4913 - lr: 1.1111e-04 - 59s/epoch - 299ms/step
Epoch 161/1000
2023-10-25 10:56:43.886 
Epoch 161/1000 
	 loss: 97.9926, MinusLogProbMetric: 97.9926, val_loss: 90.1695, val_MinusLogProbMetric: 90.1695

Epoch 161: val_loss did not improve from 84.49133
196/196 - 56s - loss: 97.9926 - MinusLogProbMetric: 97.9926 - val_loss: 90.1695 - val_MinusLogProbMetric: 90.1695 - lr: 1.1111e-04 - 56s/epoch - 287ms/step
Epoch 162/1000
2023-10-25 10:57:42.959 
Epoch 162/1000 
	 loss: 86.9836, MinusLogProbMetric: 86.9836, val_loss: 86.2111, val_MinusLogProbMetric: 86.2111

Epoch 162: val_loss did not improve from 84.49133
196/196 - 59s - loss: 86.9836 - MinusLogProbMetric: 86.9836 - val_loss: 86.2111 - val_MinusLogProbMetric: 86.2111 - lr: 1.1111e-04 - 59s/epoch - 301ms/step
Epoch 163/1000
2023-10-25 10:58:44.183 
Epoch 163/1000 
	 loss: 85.7224, MinusLogProbMetric: 85.7224, val_loss: 84.5837, val_MinusLogProbMetric: 84.5837

Epoch 163: val_loss did not improve from 84.49133
196/196 - 61s - loss: 85.7224 - MinusLogProbMetric: 85.7224 - val_loss: 84.5837 - val_MinusLogProbMetric: 84.5837 - lr: 1.1111e-04 - 61s/epoch - 312ms/step
Epoch 164/1000
2023-10-25 10:59:42.228 
Epoch 164/1000 
	 loss: 85.3548, MinusLogProbMetric: 85.3548, val_loss: 84.3031, val_MinusLogProbMetric: 84.3031

Epoch 164: val_loss improved from 84.49133 to 84.30307, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 59s - loss: 85.3548 - MinusLogProbMetric: 85.3548 - val_loss: 84.3031 - val_MinusLogProbMetric: 84.3031 - lr: 1.1111e-04 - 59s/epoch - 301ms/step
Epoch 165/1000
2023-10-25 11:00:43.962 
Epoch 165/1000 
	 loss: 83.9565, MinusLogProbMetric: 83.9565, val_loss: 84.3163, val_MinusLogProbMetric: 84.3163

Epoch 165: val_loss did not improve from 84.30307
196/196 - 61s - loss: 83.9565 - MinusLogProbMetric: 83.9565 - val_loss: 84.3163 - val_MinusLogProbMetric: 84.3163 - lr: 1.1111e-04 - 61s/epoch - 310ms/step
Epoch 166/1000
2023-10-25 11:01:46.536 
Epoch 166/1000 
	 loss: 82.8647, MinusLogProbMetric: 82.8647, val_loss: 82.9564, val_MinusLogProbMetric: 82.9564

Epoch 166: val_loss improved from 84.30307 to 82.95641, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 63s - loss: 82.8647 - MinusLogProbMetric: 82.8647 - val_loss: 82.9564 - val_MinusLogProbMetric: 82.9564 - lr: 1.1111e-04 - 63s/epoch - 323ms/step
Epoch 167/1000
2023-10-25 11:02:51.051 
Epoch 167/1000 
	 loss: 83.0021, MinusLogProbMetric: 83.0021, val_loss: 84.2872, val_MinusLogProbMetric: 84.2872

Epoch 167: val_loss did not improve from 82.95641
196/196 - 64s - loss: 83.0021 - MinusLogProbMetric: 83.0021 - val_loss: 84.2872 - val_MinusLogProbMetric: 84.2872 - lr: 1.1111e-04 - 64s/epoch - 325ms/step
Epoch 168/1000
2023-10-25 11:03:54.031 
Epoch 168/1000 
	 loss: 82.2775, MinusLogProbMetric: 82.2775, val_loss: 82.2030, val_MinusLogProbMetric: 82.2030

Epoch 168: val_loss improved from 82.95641 to 82.20296, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 64s - loss: 82.2775 - MinusLogProbMetric: 82.2775 - val_loss: 82.2030 - val_MinusLogProbMetric: 82.2030 - lr: 1.1111e-04 - 64s/epoch - 326ms/step
Epoch 169/1000
2023-10-25 11:04:53.745 
Epoch 169/1000 
	 loss: 82.6734, MinusLogProbMetric: 82.6734, val_loss: 84.7368, val_MinusLogProbMetric: 84.7368

Epoch 169: val_loss did not improve from 82.20296
196/196 - 59s - loss: 82.6734 - MinusLogProbMetric: 82.6734 - val_loss: 84.7368 - val_MinusLogProbMetric: 84.7368 - lr: 1.1111e-04 - 59s/epoch - 300ms/step
Epoch 170/1000
2023-10-25 11:05:54.717 
Epoch 170/1000 
	 loss: 82.0544, MinusLogProbMetric: 82.0544, val_loss: 82.3158, val_MinusLogProbMetric: 82.3158

Epoch 170: val_loss did not improve from 82.20296
196/196 - 61s - loss: 82.0544 - MinusLogProbMetric: 82.0544 - val_loss: 82.3158 - val_MinusLogProbMetric: 82.3158 - lr: 1.1111e-04 - 61s/epoch - 311ms/step
Epoch 171/1000
2023-10-25 11:06:52.719 
Epoch 171/1000 
	 loss: 81.7006, MinusLogProbMetric: 81.7006, val_loss: 81.4914, val_MinusLogProbMetric: 81.4914

Epoch 171: val_loss improved from 82.20296 to 81.49139, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 59s - loss: 81.7006 - MinusLogProbMetric: 81.7006 - val_loss: 81.4914 - val_MinusLogProbMetric: 81.4914 - lr: 1.1111e-04 - 59s/epoch - 301ms/step
Epoch 172/1000
2023-10-25 11:07:57.230 
Epoch 172/1000 
	 loss: 81.6404, MinusLogProbMetric: 81.6404, val_loss: 80.9962, val_MinusLogProbMetric: 80.9962

Epoch 172: val_loss improved from 81.49139 to 80.99615, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 64s - loss: 81.6404 - MinusLogProbMetric: 81.6404 - val_loss: 80.9962 - val_MinusLogProbMetric: 80.9962 - lr: 1.1111e-04 - 64s/epoch - 329ms/step
Epoch 173/1000
2023-10-25 11:08:59.367 
Epoch 173/1000 
	 loss: 81.0948, MinusLogProbMetric: 81.0948, val_loss: 80.6521, val_MinusLogProbMetric: 80.6521

Epoch 173: val_loss improved from 80.99615 to 80.65215, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 62s - loss: 81.0948 - MinusLogProbMetric: 81.0948 - val_loss: 80.6521 - val_MinusLogProbMetric: 80.6521 - lr: 1.1111e-04 - 62s/epoch - 317ms/step
Epoch 174/1000
2023-10-25 11:10:00.376 
Epoch 174/1000 
	 loss: 81.5142, MinusLogProbMetric: 81.5142, val_loss: 81.9260, val_MinusLogProbMetric: 81.9260

Epoch 174: val_loss did not improve from 80.65215
196/196 - 60s - loss: 81.5142 - MinusLogProbMetric: 81.5142 - val_loss: 81.9260 - val_MinusLogProbMetric: 81.9260 - lr: 1.1111e-04 - 60s/epoch - 307ms/step
Epoch 175/1000
2023-10-25 11:10:57.391 
Epoch 175/1000 
	 loss: 80.1024, MinusLogProbMetric: 80.1024, val_loss: 80.2050, val_MinusLogProbMetric: 80.2050

Epoch 175: val_loss improved from 80.65215 to 80.20499, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 58s - loss: 80.1024 - MinusLogProbMetric: 80.1024 - val_loss: 80.2050 - val_MinusLogProbMetric: 80.2050 - lr: 1.1111e-04 - 58s/epoch - 297ms/step
Epoch 176/1000
2023-10-25 11:11:56.144 
Epoch 176/1000 
	 loss: 79.8566, MinusLogProbMetric: 79.8566, val_loss: 81.1824, val_MinusLogProbMetric: 81.1824

Epoch 176: val_loss did not improve from 80.20499
196/196 - 58s - loss: 79.8566 - MinusLogProbMetric: 79.8566 - val_loss: 81.1824 - val_MinusLogProbMetric: 81.1824 - lr: 1.1111e-04 - 58s/epoch - 293ms/step
Epoch 177/1000
2023-10-25 11:12:53.104 
Epoch 177/1000 
	 loss: 79.5652, MinusLogProbMetric: 79.5652, val_loss: 79.6066, val_MinusLogProbMetric: 79.6066

Epoch 177: val_loss improved from 80.20499 to 79.60655, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 58s - loss: 79.5652 - MinusLogProbMetric: 79.5652 - val_loss: 79.6066 - val_MinusLogProbMetric: 79.6066 - lr: 1.1111e-04 - 58s/epoch - 295ms/step
Epoch 178/1000
2023-10-25 11:13:52.140 
Epoch 178/1000 
	 loss: 79.7978, MinusLogProbMetric: 79.7978, val_loss: 80.9774, val_MinusLogProbMetric: 80.9774

Epoch 178: val_loss did not improve from 79.60655
196/196 - 58s - loss: 79.7978 - MinusLogProbMetric: 79.7978 - val_loss: 80.9774 - val_MinusLogProbMetric: 80.9774 - lr: 1.1111e-04 - 58s/epoch - 297ms/step
Epoch 179/1000
2023-10-25 11:14:49.404 
Epoch 179/1000 
	 loss: 79.0785, MinusLogProbMetric: 79.0785, val_loss: 79.4863, val_MinusLogProbMetric: 79.4863

Epoch 179: val_loss improved from 79.60655 to 79.48631, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 58s - loss: 79.0785 - MinusLogProbMetric: 79.0785 - val_loss: 79.4863 - val_MinusLogProbMetric: 79.4863 - lr: 1.1111e-04 - 58s/epoch - 296ms/step
Epoch 180/1000
2023-10-25 11:15:50.238 
Epoch 180/1000 
	 loss: 82.1113, MinusLogProbMetric: 82.1113, val_loss: 82.4962, val_MinusLogProbMetric: 82.4962

Epoch 180: val_loss did not improve from 79.48631
196/196 - 60s - loss: 82.1113 - MinusLogProbMetric: 82.1113 - val_loss: 82.4962 - val_MinusLogProbMetric: 82.4962 - lr: 1.1111e-04 - 60s/epoch - 306ms/step
Epoch 181/1000
2023-10-25 11:16:49.067 
Epoch 181/1000 
	 loss: 79.0083, MinusLogProbMetric: 79.0083, val_loss: 79.8723, val_MinusLogProbMetric: 79.8723

Epoch 181: val_loss did not improve from 79.48631
196/196 - 59s - loss: 79.0083 - MinusLogProbMetric: 79.0083 - val_loss: 79.8723 - val_MinusLogProbMetric: 79.8723 - lr: 1.1111e-04 - 59s/epoch - 300ms/step
Epoch 182/1000
2023-10-25 11:17:48.743 
Epoch 182/1000 
	 loss: 78.6186, MinusLogProbMetric: 78.6186, val_loss: 78.7856, val_MinusLogProbMetric: 78.7856

Epoch 182: val_loss improved from 79.48631 to 78.78559, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 61s - loss: 78.6186 - MinusLogProbMetric: 78.6186 - val_loss: 78.7856 - val_MinusLogProbMetric: 78.7856 - lr: 1.1111e-04 - 61s/epoch - 309ms/step
Epoch 183/1000
2023-10-25 11:18:52.484 
Epoch 183/1000 
	 loss: 78.1878, MinusLogProbMetric: 78.1878, val_loss: 80.1139, val_MinusLogProbMetric: 80.1139

Epoch 183: val_loss did not improve from 78.78559
196/196 - 63s - loss: 78.1878 - MinusLogProbMetric: 78.1878 - val_loss: 80.1139 - val_MinusLogProbMetric: 80.1139 - lr: 1.1111e-04 - 63s/epoch - 321ms/step
Epoch 184/1000
2023-10-25 11:19:52.796 
Epoch 184/1000 
	 loss: 79.2669, MinusLogProbMetric: 79.2669, val_loss: 79.4119, val_MinusLogProbMetric: 79.4119

Epoch 184: val_loss did not improve from 78.78559
196/196 - 60s - loss: 79.2669 - MinusLogProbMetric: 79.2669 - val_loss: 79.4119 - val_MinusLogProbMetric: 79.4119 - lr: 1.1111e-04 - 60s/epoch - 308ms/step
Epoch 185/1000
2023-10-25 11:20:54.784 
Epoch 185/1000 
	 loss: 78.6199, MinusLogProbMetric: 78.6199, val_loss: 79.5007, val_MinusLogProbMetric: 79.5007

Epoch 185: val_loss did not improve from 78.78559
196/196 - 62s - loss: 78.6199 - MinusLogProbMetric: 78.6199 - val_loss: 79.5007 - val_MinusLogProbMetric: 79.5007 - lr: 1.1111e-04 - 62s/epoch - 316ms/step
Epoch 186/1000
2023-10-25 11:21:57.127 
Epoch 186/1000 
	 loss: 78.0199, MinusLogProbMetric: 78.0199, val_loss: 77.8442, val_MinusLogProbMetric: 77.8442

Epoch 186: val_loss improved from 78.78559 to 77.84418, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 63s - loss: 78.0199 - MinusLogProbMetric: 78.0199 - val_loss: 77.8442 - val_MinusLogProbMetric: 77.8442 - lr: 1.1111e-04 - 63s/epoch - 323ms/step
Epoch 187/1000
2023-10-25 11:23:01.672 
Epoch 187/1000 
	 loss: 77.5525, MinusLogProbMetric: 77.5525, val_loss: 77.3776, val_MinusLogProbMetric: 77.3776

Epoch 187: val_loss improved from 77.84418 to 77.37762, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 65s - loss: 77.5525 - MinusLogProbMetric: 77.5525 - val_loss: 77.3776 - val_MinusLogProbMetric: 77.3776 - lr: 1.1111e-04 - 65s/epoch - 329ms/step
Epoch 188/1000
2023-10-25 11:24:02.452 
Epoch 188/1000 
	 loss: 77.5282, MinusLogProbMetric: 77.5282, val_loss: 77.0289, val_MinusLogProbMetric: 77.0289

Epoch 188: val_loss improved from 77.37762 to 77.02886, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 61s - loss: 77.5282 - MinusLogProbMetric: 77.5282 - val_loss: 77.0289 - val_MinusLogProbMetric: 77.0289 - lr: 1.1111e-04 - 61s/epoch - 310ms/step
Epoch 189/1000
2023-10-25 11:25:00.461 
Epoch 189/1000 
	 loss: 77.4009, MinusLogProbMetric: 77.4009, val_loss: 77.3109, val_MinusLogProbMetric: 77.3109

Epoch 189: val_loss did not improve from 77.02886
196/196 - 57s - loss: 77.4009 - MinusLogProbMetric: 77.4009 - val_loss: 77.3109 - val_MinusLogProbMetric: 77.3109 - lr: 1.1111e-04 - 57s/epoch - 292ms/step
Epoch 190/1000
2023-10-25 11:26:02.750 
Epoch 190/1000 
	 loss: 81.2652, MinusLogProbMetric: 81.2652, val_loss: 98.5846, val_MinusLogProbMetric: 98.5846

Epoch 190: val_loss did not improve from 77.02886
196/196 - 62s - loss: 81.2652 - MinusLogProbMetric: 81.2652 - val_loss: 98.5846 - val_MinusLogProbMetric: 98.5846 - lr: 1.1111e-04 - 62s/epoch - 318ms/step
Epoch 191/1000
2023-10-25 11:27:04.864 
Epoch 191/1000 
	 loss: 80.4207, MinusLogProbMetric: 80.4207, val_loss: 81.6196, val_MinusLogProbMetric: 81.6196

Epoch 191: val_loss did not improve from 77.02886
196/196 - 62s - loss: 80.4207 - MinusLogProbMetric: 80.4207 - val_loss: 81.6196 - val_MinusLogProbMetric: 81.6196 - lr: 1.1111e-04 - 62s/epoch - 317ms/step
Epoch 192/1000
2023-10-25 11:28:07.499 
Epoch 192/1000 
	 loss: 78.3185, MinusLogProbMetric: 78.3185, val_loss: 77.1578, val_MinusLogProbMetric: 77.1578

Epoch 192: val_loss did not improve from 77.02886
196/196 - 63s - loss: 78.3185 - MinusLogProbMetric: 78.3185 - val_loss: 77.1578 - val_MinusLogProbMetric: 77.1578 - lr: 1.1111e-04 - 63s/epoch - 320ms/step
Epoch 193/1000
2023-10-25 11:29:04.315 
Epoch 193/1000 
	 loss: 78.1383, MinusLogProbMetric: 78.1383, val_loss: 78.4867, val_MinusLogProbMetric: 78.4867

Epoch 193: val_loss did not improve from 77.02886
196/196 - 57s - loss: 78.1383 - MinusLogProbMetric: 78.1383 - val_loss: 78.4867 - val_MinusLogProbMetric: 78.4867 - lr: 1.1111e-04 - 57s/epoch - 290ms/step
Epoch 194/1000
2023-10-25 11:30:03.298 
Epoch 194/1000 
	 loss: 77.3155, MinusLogProbMetric: 77.3155, val_loss: 76.7920, val_MinusLogProbMetric: 76.7920

Epoch 194: val_loss improved from 77.02886 to 76.79198, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 60s - loss: 77.3155 - MinusLogProbMetric: 77.3155 - val_loss: 76.7920 - val_MinusLogProbMetric: 76.7920 - lr: 1.1111e-04 - 60s/epoch - 306ms/step
Epoch 195/1000
2023-10-25 11:31:02.521 
Epoch 195/1000 
	 loss: 76.2755, MinusLogProbMetric: 76.2755, val_loss: 78.4748, val_MinusLogProbMetric: 78.4748

Epoch 195: val_loss did not improve from 76.79198
196/196 - 58s - loss: 76.2755 - MinusLogProbMetric: 76.2755 - val_loss: 78.4748 - val_MinusLogProbMetric: 78.4748 - lr: 1.1111e-04 - 58s/epoch - 297ms/step
Epoch 196/1000
2023-10-25 11:31:59.981 
Epoch 196/1000 
	 loss: 77.0893, MinusLogProbMetric: 77.0893, val_loss: 84.7642, val_MinusLogProbMetric: 84.7642

Epoch 196: val_loss did not improve from 76.79198
196/196 - 57s - loss: 77.0893 - MinusLogProbMetric: 77.0893 - val_loss: 84.7642 - val_MinusLogProbMetric: 84.7642 - lr: 1.1111e-04 - 57s/epoch - 293ms/step
Epoch 197/1000
2023-10-25 11:32:55.886 
Epoch 197/1000 
	 loss: 77.5919, MinusLogProbMetric: 77.5919, val_loss: 77.6768, val_MinusLogProbMetric: 77.6768

Epoch 197: val_loss did not improve from 76.79198
196/196 - 56s - loss: 77.5919 - MinusLogProbMetric: 77.5919 - val_loss: 77.6768 - val_MinusLogProbMetric: 77.6768 - lr: 1.1111e-04 - 56s/epoch - 285ms/step
Epoch 198/1000
2023-10-25 11:33:54.406 
Epoch 198/1000 
	 loss: 77.4892, MinusLogProbMetric: 77.4892, val_loss: 75.9233, val_MinusLogProbMetric: 75.9233

Epoch 198: val_loss improved from 76.79198 to 75.92332, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 59s - loss: 77.4892 - MinusLogProbMetric: 77.4892 - val_loss: 75.9233 - val_MinusLogProbMetric: 75.9233 - lr: 1.1111e-04 - 59s/epoch - 303ms/step
Epoch 199/1000
2023-10-25 11:34:51.718 
Epoch 199/1000 
	 loss: 84.2726, MinusLogProbMetric: 84.2726, val_loss: 135.1388, val_MinusLogProbMetric: 135.1388

Epoch 199: val_loss did not improve from 75.92332
196/196 - 56s - loss: 84.2726 - MinusLogProbMetric: 84.2726 - val_loss: 135.1388 - val_MinusLogProbMetric: 135.1388 - lr: 1.1111e-04 - 56s/epoch - 288ms/step
Epoch 200/1000
2023-10-25 11:35:54.125 
Epoch 200/1000 
	 loss: 91.4044, MinusLogProbMetric: 91.4044, val_loss: 81.7800, val_MinusLogProbMetric: 81.7800

Epoch 200: val_loss did not improve from 75.92332
196/196 - 62s - loss: 91.4044 - MinusLogProbMetric: 91.4044 - val_loss: 81.7800 - val_MinusLogProbMetric: 81.7800 - lr: 1.1111e-04 - 62s/epoch - 318ms/step
Epoch 201/1000
2023-10-25 11:36:57.709 
Epoch 201/1000 
	 loss: 79.7752, MinusLogProbMetric: 79.7752, val_loss: 78.5589, val_MinusLogProbMetric: 78.5589

Epoch 201: val_loss did not improve from 75.92332
196/196 - 64s - loss: 79.7752 - MinusLogProbMetric: 79.7752 - val_loss: 78.5589 - val_MinusLogProbMetric: 78.5589 - lr: 1.1111e-04 - 64s/epoch - 324ms/step
Epoch 202/1000
2023-10-25 11:37:59.306 
Epoch 202/1000 
	 loss: 77.4488, MinusLogProbMetric: 77.4488, val_loss: 76.3714, val_MinusLogProbMetric: 76.3714

Epoch 202: val_loss did not improve from 75.92332
196/196 - 62s - loss: 77.4488 - MinusLogProbMetric: 77.4488 - val_loss: 76.3714 - val_MinusLogProbMetric: 76.3714 - lr: 1.1111e-04 - 62s/epoch - 314ms/step
Epoch 203/1000
2023-10-25 11:38:57.592 
Epoch 203/1000 
	 loss: 75.9151, MinusLogProbMetric: 75.9151, val_loss: 76.0757, val_MinusLogProbMetric: 76.0757

Epoch 203: val_loss did not improve from 75.92332
196/196 - 58s - loss: 75.9151 - MinusLogProbMetric: 75.9151 - val_loss: 76.0757 - val_MinusLogProbMetric: 76.0757 - lr: 1.1111e-04 - 58s/epoch - 297ms/step
Epoch 204/1000
2023-10-25 11:39:57.835 
Epoch 204/1000 
	 loss: 75.2954, MinusLogProbMetric: 75.2954, val_loss: 75.2224, val_MinusLogProbMetric: 75.2224

Epoch 204: val_loss improved from 75.92332 to 75.22243, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 61s - loss: 75.2954 - MinusLogProbMetric: 75.2954 - val_loss: 75.2224 - val_MinusLogProbMetric: 75.2224 - lr: 1.1111e-04 - 61s/epoch - 312ms/step
Epoch 205/1000
2023-10-25 11:40:57.729 
Epoch 205/1000 
	 loss: 74.8746, MinusLogProbMetric: 74.8746, val_loss: 75.6697, val_MinusLogProbMetric: 75.6697

Epoch 205: val_loss did not improve from 75.22243
196/196 - 59s - loss: 74.8746 - MinusLogProbMetric: 74.8746 - val_loss: 75.6697 - val_MinusLogProbMetric: 75.6697 - lr: 1.1111e-04 - 59s/epoch - 301ms/step
Epoch 206/1000
2023-10-25 11:41:58.260 
Epoch 206/1000 
	 loss: 74.1789, MinusLogProbMetric: 74.1789, val_loss: 74.6082, val_MinusLogProbMetric: 74.6082

Epoch 206: val_loss improved from 75.22243 to 74.60818, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 61s - loss: 74.1789 - MinusLogProbMetric: 74.1789 - val_loss: 74.6082 - val_MinusLogProbMetric: 74.6082 - lr: 1.1111e-04 - 61s/epoch - 313ms/step
Epoch 207/1000
2023-10-25 11:42:59.477 
Epoch 207/1000 
	 loss: 75.9205, MinusLogProbMetric: 75.9205, val_loss: 75.4160, val_MinusLogProbMetric: 75.4160

Epoch 207: val_loss did not improve from 74.60818
196/196 - 60s - loss: 75.9205 - MinusLogProbMetric: 75.9205 - val_loss: 75.4160 - val_MinusLogProbMetric: 75.4160 - lr: 1.1111e-04 - 60s/epoch - 308ms/step
Epoch 208/1000
2023-10-25 11:43:57.943 
Epoch 208/1000 
	 loss: 73.9893, MinusLogProbMetric: 73.9893, val_loss: 74.9362, val_MinusLogProbMetric: 74.9362

Epoch 208: val_loss did not improve from 74.60818
196/196 - 58s - loss: 73.9893 - MinusLogProbMetric: 73.9893 - val_loss: 74.9362 - val_MinusLogProbMetric: 74.9362 - lr: 1.1111e-04 - 58s/epoch - 298ms/step
Epoch 209/1000
2023-10-25 11:44:53.729 
Epoch 209/1000 
	 loss: 73.5153, MinusLogProbMetric: 73.5153, val_loss: 74.4146, val_MinusLogProbMetric: 74.4146

Epoch 209: val_loss improved from 74.60818 to 74.41460, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 57s - loss: 73.5153 - MinusLogProbMetric: 73.5153 - val_loss: 74.4146 - val_MinusLogProbMetric: 74.4146 - lr: 1.1111e-04 - 57s/epoch - 289ms/step
Epoch 210/1000
2023-10-25 11:45:52.513 
Epoch 210/1000 
	 loss: 76.9854, MinusLogProbMetric: 76.9854, val_loss: 86.1841, val_MinusLogProbMetric: 86.1841

Epoch 210: val_loss did not improve from 74.41460
196/196 - 58s - loss: 76.9854 - MinusLogProbMetric: 76.9854 - val_loss: 86.1841 - val_MinusLogProbMetric: 86.1841 - lr: 1.1111e-04 - 58s/epoch - 295ms/step
Epoch 211/1000
2023-10-25 11:46:56.454 
Epoch 211/1000 
	 loss: 74.8964, MinusLogProbMetric: 74.8964, val_loss: 73.4963, val_MinusLogProbMetric: 73.4963

Epoch 211: val_loss improved from 74.41460 to 73.49632, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 65s - loss: 74.8964 - MinusLogProbMetric: 74.8964 - val_loss: 73.4963 - val_MinusLogProbMetric: 73.4963 - lr: 1.1111e-04 - 65s/epoch - 331ms/step
Epoch 212/1000
2023-10-25 11:47:59.556 
Epoch 212/1000 
	 loss: 72.7036, MinusLogProbMetric: 72.7036, val_loss: 73.7365, val_MinusLogProbMetric: 73.7365

Epoch 212: val_loss did not improve from 73.49632
196/196 - 62s - loss: 72.7036 - MinusLogProbMetric: 72.7036 - val_loss: 73.7365 - val_MinusLogProbMetric: 73.7365 - lr: 1.1111e-04 - 62s/epoch - 317ms/step
Epoch 213/1000
2023-10-25 11:48:59.844 
Epoch 213/1000 
	 loss: 72.6275, MinusLogProbMetric: 72.6275, val_loss: 73.5599, val_MinusLogProbMetric: 73.5599

Epoch 213: val_loss did not improve from 73.49632
196/196 - 60s - loss: 72.6275 - MinusLogProbMetric: 72.6275 - val_loss: 73.5599 - val_MinusLogProbMetric: 73.5599 - lr: 1.1111e-04 - 60s/epoch - 308ms/step
Epoch 214/1000
2023-10-25 11:50:02.741 
Epoch 214/1000 
	 loss: 72.2599, MinusLogProbMetric: 72.2599, val_loss: 71.6178, val_MinusLogProbMetric: 71.6178

Epoch 214: val_loss improved from 73.49632 to 71.61780, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 64s - loss: 72.2599 - MinusLogProbMetric: 72.2599 - val_loss: 71.6178 - val_MinusLogProbMetric: 71.6178 - lr: 1.1111e-04 - 64s/epoch - 325ms/step
Epoch 215/1000
2023-10-25 11:51:07.958 
Epoch 215/1000 
	 loss: 75.9503, MinusLogProbMetric: 75.9503, val_loss: 72.9162, val_MinusLogProbMetric: 72.9162

Epoch 215: val_loss did not improve from 71.61780
196/196 - 64s - loss: 75.9503 - MinusLogProbMetric: 75.9503 - val_loss: 72.9162 - val_MinusLogProbMetric: 72.9162 - lr: 1.1111e-04 - 64s/epoch - 328ms/step
Epoch 216/1000
2023-10-25 11:52:10.513 
Epoch 216/1000 
	 loss: 71.9995, MinusLogProbMetric: 71.9995, val_loss: 72.5066, val_MinusLogProbMetric: 72.5066

Epoch 216: val_loss did not improve from 71.61780
196/196 - 63s - loss: 71.9995 - MinusLogProbMetric: 71.9995 - val_loss: 72.5066 - val_MinusLogProbMetric: 72.5066 - lr: 1.1111e-04 - 63s/epoch - 319ms/step
Epoch 217/1000
2023-10-25 11:53:09.934 
Epoch 217/1000 
	 loss: 71.5553, MinusLogProbMetric: 71.5553, val_loss: 73.0354, val_MinusLogProbMetric: 73.0354

Epoch 217: val_loss did not improve from 71.61780
196/196 - 59s - loss: 71.5553 - MinusLogProbMetric: 71.5553 - val_loss: 73.0354 - val_MinusLogProbMetric: 73.0354 - lr: 1.1111e-04 - 59s/epoch - 303ms/step
Epoch 218/1000
2023-10-25 11:54:07.700 
Epoch 218/1000 
	 loss: 71.3121, MinusLogProbMetric: 71.3121, val_loss: 71.8458, val_MinusLogProbMetric: 71.8458

Epoch 218: val_loss did not improve from 71.61780
196/196 - 58s - loss: 71.3121 - MinusLogProbMetric: 71.3121 - val_loss: 71.8458 - val_MinusLogProbMetric: 71.8458 - lr: 1.1111e-04 - 58s/epoch - 295ms/step
Epoch 219/1000
2023-10-25 11:55:07.878 
Epoch 219/1000 
	 loss: 71.1191, MinusLogProbMetric: 71.1191, val_loss: 72.0634, val_MinusLogProbMetric: 72.0634

Epoch 219: val_loss did not improve from 71.61780
196/196 - 60s - loss: 71.1191 - MinusLogProbMetric: 71.1191 - val_loss: 72.0634 - val_MinusLogProbMetric: 72.0634 - lr: 1.1111e-04 - 60s/epoch - 307ms/step
Epoch 220/1000
2023-10-25 11:56:06.460 
Epoch 220/1000 
	 loss: 75.2628, MinusLogProbMetric: 75.2628, val_loss: 71.2084, val_MinusLogProbMetric: 71.2084

Epoch 220: val_loss improved from 71.61780 to 71.20840, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 59s - loss: 75.2628 - MinusLogProbMetric: 75.2628 - val_loss: 71.2084 - val_MinusLogProbMetric: 71.2084 - lr: 1.1111e-04 - 59s/epoch - 303ms/step
Epoch 221/1000
2023-10-25 11:57:10.653 
Epoch 221/1000 
	 loss: 70.4094, MinusLogProbMetric: 70.4094, val_loss: 71.1611, val_MinusLogProbMetric: 71.1611

Epoch 221: val_loss improved from 71.20840 to 71.16106, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 64s - loss: 70.4094 - MinusLogProbMetric: 70.4094 - val_loss: 71.1611 - val_MinusLogProbMetric: 71.1611 - lr: 1.1111e-04 - 64s/epoch - 327ms/step
Epoch 222/1000
2023-10-25 11:58:11.773 
Epoch 222/1000 
	 loss: 70.1259, MinusLogProbMetric: 70.1259, val_loss: 70.3049, val_MinusLogProbMetric: 70.3049

Epoch 222: val_loss improved from 71.16106 to 70.30489, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 61s - loss: 70.1259 - MinusLogProbMetric: 70.1259 - val_loss: 70.3049 - val_MinusLogProbMetric: 70.3049 - lr: 1.1111e-04 - 61s/epoch - 312ms/step
Epoch 223/1000
2023-10-25 11:59:15.320 
Epoch 223/1000 
	 loss: 70.5139, MinusLogProbMetric: 70.5139, val_loss: 70.2084, val_MinusLogProbMetric: 70.2084

Epoch 223: val_loss improved from 70.30489 to 70.20844, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 64s - loss: 70.5139 - MinusLogProbMetric: 70.5139 - val_loss: 70.2084 - val_MinusLogProbMetric: 70.2084 - lr: 1.1111e-04 - 64s/epoch - 325ms/step
Epoch 224/1000
2023-10-25 12:00:16.948 
Epoch 224/1000 
	 loss: 69.6177, MinusLogProbMetric: 69.6177, val_loss: 72.3255, val_MinusLogProbMetric: 72.3255

Epoch 224: val_loss did not improve from 70.20844
196/196 - 61s - loss: 69.6177 - MinusLogProbMetric: 69.6177 - val_loss: 72.3255 - val_MinusLogProbMetric: 72.3255 - lr: 1.1111e-04 - 61s/epoch - 310ms/step
Epoch 225/1000
2023-10-25 12:01:18.826 
Epoch 225/1000 
	 loss: 71.4483, MinusLogProbMetric: 71.4483, val_loss: 69.2874, val_MinusLogProbMetric: 69.2874

Epoch 225: val_loss improved from 70.20844 to 69.28741, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 63s - loss: 71.4483 - MinusLogProbMetric: 71.4483 - val_loss: 69.2874 - val_MinusLogProbMetric: 69.2874 - lr: 1.1111e-04 - 63s/epoch - 320ms/step
Epoch 226/1000
2023-10-25 12:02:18.483 
Epoch 226/1000 
	 loss: 69.3166, MinusLogProbMetric: 69.3166, val_loss: 71.0831, val_MinusLogProbMetric: 71.0831

Epoch 226: val_loss did not improve from 69.28741
196/196 - 59s - loss: 69.3166 - MinusLogProbMetric: 69.3166 - val_loss: 71.0831 - val_MinusLogProbMetric: 71.0831 - lr: 1.1111e-04 - 59s/epoch - 300ms/step
Epoch 227/1000
2023-10-25 12:03:17.914 
Epoch 227/1000 
	 loss: 69.1913, MinusLogProbMetric: 69.1913, val_loss: 69.8782, val_MinusLogProbMetric: 69.8782

Epoch 227: val_loss did not improve from 69.28741
196/196 - 59s - loss: 69.1913 - MinusLogProbMetric: 69.1913 - val_loss: 69.8782 - val_MinusLogProbMetric: 69.8782 - lr: 1.1111e-04 - 59s/epoch - 303ms/step
Epoch 228/1000
2023-10-25 12:04:16.461 
Epoch 228/1000 
	 loss: 69.5679, MinusLogProbMetric: 69.5679, val_loss: 71.4717, val_MinusLogProbMetric: 71.4717

Epoch 228: val_loss did not improve from 69.28741
196/196 - 59s - loss: 69.5679 - MinusLogProbMetric: 69.5679 - val_loss: 71.4717 - val_MinusLogProbMetric: 71.4717 - lr: 1.1111e-04 - 59s/epoch - 299ms/step
Epoch 229/1000
2023-10-25 12:05:17.895 
Epoch 229/1000 
	 loss: 69.9210, MinusLogProbMetric: 69.9210, val_loss: 69.3579, val_MinusLogProbMetric: 69.3579

Epoch 229: val_loss did not improve from 69.28741
196/196 - 61s - loss: 69.9210 - MinusLogProbMetric: 69.9210 - val_loss: 69.3579 - val_MinusLogProbMetric: 69.3579 - lr: 1.1111e-04 - 61s/epoch - 313ms/step
Epoch 230/1000
2023-10-25 12:06:17.404 
Epoch 230/1000 
	 loss: 68.8810, MinusLogProbMetric: 68.8810, val_loss: 69.4177, val_MinusLogProbMetric: 69.4177

Epoch 230: val_loss did not improve from 69.28741
196/196 - 60s - loss: 68.8810 - MinusLogProbMetric: 68.8810 - val_loss: 69.4177 - val_MinusLogProbMetric: 69.4177 - lr: 1.1111e-04 - 60s/epoch - 304ms/step
Epoch 231/1000
2023-10-25 12:07:15.774 
Epoch 231/1000 
	 loss: 68.7529, MinusLogProbMetric: 68.7529, val_loss: 69.2604, val_MinusLogProbMetric: 69.2604

Epoch 231: val_loss improved from 69.28741 to 69.26041, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 59s - loss: 68.7529 - MinusLogProbMetric: 68.7529 - val_loss: 69.2604 - val_MinusLogProbMetric: 69.2604 - lr: 1.1111e-04 - 59s/epoch - 302ms/step
Epoch 232/1000
2023-10-25 12:08:16.664 
Epoch 232/1000 
	 loss: 68.5697, MinusLogProbMetric: 68.5697, val_loss: 68.9382, val_MinusLogProbMetric: 68.9382

Epoch 232: val_loss improved from 69.26041 to 68.93819, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 61s - loss: 68.5697 - MinusLogProbMetric: 68.5697 - val_loss: 68.9382 - val_MinusLogProbMetric: 68.9382 - lr: 1.1111e-04 - 61s/epoch - 310ms/step
Epoch 233/1000
2023-10-25 12:09:15.054 
Epoch 233/1000 
	 loss: 71.0224, MinusLogProbMetric: 71.0224, val_loss: 68.1485, val_MinusLogProbMetric: 68.1485

Epoch 233: val_loss improved from 68.93819 to 68.14847, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 58s - loss: 71.0224 - MinusLogProbMetric: 71.0224 - val_loss: 68.1485 - val_MinusLogProbMetric: 68.1485 - lr: 1.1111e-04 - 58s/epoch - 298ms/step
Epoch 234/1000
2023-10-25 12:10:16.305 
Epoch 234/1000 
	 loss: 72.6540, MinusLogProbMetric: 72.6540, val_loss: 69.9754, val_MinusLogProbMetric: 69.9754

Epoch 234: val_loss did not improve from 68.14847
196/196 - 60s - loss: 72.6540 - MinusLogProbMetric: 72.6540 - val_loss: 69.9754 - val_MinusLogProbMetric: 69.9754 - lr: 1.1111e-04 - 60s/epoch - 308ms/step
Epoch 235/1000
2023-10-25 12:11:16.134 
Epoch 235/1000 
	 loss: 68.1987, MinusLogProbMetric: 68.1987, val_loss: 68.1853, val_MinusLogProbMetric: 68.1853

Epoch 235: val_loss did not improve from 68.14847
196/196 - 60s - loss: 68.1987 - MinusLogProbMetric: 68.1987 - val_loss: 68.1853 - val_MinusLogProbMetric: 68.1853 - lr: 1.1111e-04 - 60s/epoch - 305ms/step
Epoch 236/1000
2023-10-25 12:12:16.170 
Epoch 236/1000 
	 loss: 68.0527, MinusLogProbMetric: 68.0527, val_loss: 68.3412, val_MinusLogProbMetric: 68.3412

Epoch 236: val_loss did not improve from 68.14847
196/196 - 60s - loss: 68.0527 - MinusLogProbMetric: 68.0527 - val_loss: 68.3412 - val_MinusLogProbMetric: 68.3412 - lr: 1.1111e-04 - 60s/epoch - 306ms/step
Epoch 237/1000
2023-10-25 12:13:18.303 
Epoch 237/1000 
	 loss: 68.9061, MinusLogProbMetric: 68.9061, val_loss: 70.3105, val_MinusLogProbMetric: 70.3105

Epoch 237: val_loss did not improve from 68.14847
196/196 - 62s - loss: 68.9061 - MinusLogProbMetric: 68.9061 - val_loss: 70.3105 - val_MinusLogProbMetric: 70.3105 - lr: 1.1111e-04 - 62s/epoch - 317ms/step
Epoch 238/1000
2023-10-25 12:14:19.231 
Epoch 238/1000 
	 loss: 68.6176, MinusLogProbMetric: 68.6176, val_loss: 68.7230, val_MinusLogProbMetric: 68.7230

Epoch 238: val_loss did not improve from 68.14847
196/196 - 61s - loss: 68.6176 - MinusLogProbMetric: 68.6176 - val_loss: 68.7230 - val_MinusLogProbMetric: 68.7230 - lr: 1.1111e-04 - 61s/epoch - 311ms/step
Epoch 239/1000
2023-10-25 12:15:21.004 
Epoch 239/1000 
	 loss: 68.4573, MinusLogProbMetric: 68.4573, val_loss: 68.7037, val_MinusLogProbMetric: 68.7037

Epoch 239: val_loss did not improve from 68.14847
196/196 - 62s - loss: 68.4573 - MinusLogProbMetric: 68.4573 - val_loss: 68.7037 - val_MinusLogProbMetric: 68.7037 - lr: 1.1111e-04 - 62s/epoch - 315ms/step
Epoch 240/1000
2023-10-25 12:16:23.962 
Epoch 240/1000 
	 loss: 68.0974, MinusLogProbMetric: 68.0974, val_loss: 69.5933, val_MinusLogProbMetric: 69.5933

Epoch 240: val_loss did not improve from 68.14847
196/196 - 63s - loss: 68.0974 - MinusLogProbMetric: 68.0974 - val_loss: 69.5933 - val_MinusLogProbMetric: 69.5933 - lr: 1.1111e-04 - 63s/epoch - 321ms/step
Epoch 241/1000
2023-10-25 12:17:25.799 
Epoch 241/1000 
	 loss: 74.6557, MinusLogProbMetric: 74.6557, val_loss: 70.6744, val_MinusLogProbMetric: 70.6744

Epoch 241: val_loss did not improve from 68.14847
196/196 - 62s - loss: 74.6557 - MinusLogProbMetric: 74.6557 - val_loss: 70.6744 - val_MinusLogProbMetric: 70.6744 - lr: 1.1111e-04 - 62s/epoch - 315ms/step
Epoch 242/1000
2023-10-25 12:18:26.947 
Epoch 242/1000 
	 loss: 73.2090, MinusLogProbMetric: 73.2090, val_loss: 70.1952, val_MinusLogProbMetric: 70.1952

Epoch 242: val_loss did not improve from 68.14847
196/196 - 61s - loss: 73.2090 - MinusLogProbMetric: 73.2090 - val_loss: 70.1952 - val_MinusLogProbMetric: 70.1952 - lr: 1.1111e-04 - 61s/epoch - 312ms/step
Epoch 243/1000
2023-10-25 12:19:24.645 
Epoch 243/1000 
	 loss: 69.3255, MinusLogProbMetric: 69.3255, val_loss: 69.1213, val_MinusLogProbMetric: 69.1213

Epoch 243: val_loss did not improve from 68.14847
196/196 - 58s - loss: 69.3255 - MinusLogProbMetric: 69.3255 - val_loss: 69.1213 - val_MinusLogProbMetric: 69.1213 - lr: 1.1111e-04 - 58s/epoch - 294ms/step
Epoch 244/1000
2023-10-25 12:20:22.840 
Epoch 244/1000 
	 loss: 67.5345, MinusLogProbMetric: 67.5345, val_loss: 67.7561, val_MinusLogProbMetric: 67.7561

Epoch 244: val_loss improved from 68.14847 to 67.75612, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 59s - loss: 67.5345 - MinusLogProbMetric: 67.5345 - val_loss: 67.7561 - val_MinusLogProbMetric: 67.7561 - lr: 1.1111e-04 - 59s/epoch - 302ms/step
Epoch 245/1000
2023-10-25 12:21:22.038 
Epoch 245/1000 
	 loss: 70.2491, MinusLogProbMetric: 70.2491, val_loss: 69.3570, val_MinusLogProbMetric: 69.3570

Epoch 245: val_loss did not improve from 67.75612
196/196 - 58s - loss: 70.2491 - MinusLogProbMetric: 70.2491 - val_loss: 69.3570 - val_MinusLogProbMetric: 69.3570 - lr: 1.1111e-04 - 58s/epoch - 297ms/step
Epoch 246/1000
2023-10-25 12:22:19.395 
Epoch 246/1000 
	 loss: 66.9846, MinusLogProbMetric: 66.9846, val_loss: 68.0880, val_MinusLogProbMetric: 68.0880

Epoch 246: val_loss did not improve from 67.75612
196/196 - 57s - loss: 66.9846 - MinusLogProbMetric: 66.9846 - val_loss: 68.0880 - val_MinusLogProbMetric: 68.0880 - lr: 1.1111e-04 - 57s/epoch - 293ms/step
Epoch 247/1000
2023-10-25 12:23:18.581 
Epoch 247/1000 
	 loss: 66.9775, MinusLogProbMetric: 66.9775, val_loss: 66.0855, val_MinusLogProbMetric: 66.0855

Epoch 247: val_loss improved from 67.75612 to 66.08549, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 60s - loss: 66.9775 - MinusLogProbMetric: 66.9775 - val_loss: 66.0855 - val_MinusLogProbMetric: 66.0855 - lr: 1.1111e-04 - 60s/epoch - 307ms/step
Epoch 248/1000
2023-10-25 12:24:17.150 
Epoch 248/1000 
	 loss: 68.9340, MinusLogProbMetric: 68.9340, val_loss: 68.7923, val_MinusLogProbMetric: 68.7923

Epoch 248: val_loss did not improve from 66.08549
196/196 - 58s - loss: 68.9340 - MinusLogProbMetric: 68.9340 - val_loss: 68.7923 - val_MinusLogProbMetric: 68.7923 - lr: 1.1111e-04 - 58s/epoch - 294ms/step
Epoch 249/1000
2023-10-25 12:25:17.107 
Epoch 249/1000 
	 loss: 67.1870, MinusLogProbMetric: 67.1870, val_loss: 66.7689, val_MinusLogProbMetric: 66.7689

Epoch 249: val_loss did not improve from 66.08549
196/196 - 60s - loss: 67.1870 - MinusLogProbMetric: 67.1870 - val_loss: 66.7689 - val_MinusLogProbMetric: 66.7689 - lr: 1.1111e-04 - 60s/epoch - 306ms/step
Epoch 250/1000
2023-10-25 12:26:18.867 
Epoch 250/1000 
	 loss: 66.7954, MinusLogProbMetric: 66.7954, val_loss: 69.2768, val_MinusLogProbMetric: 69.2768

Epoch 250: val_loss did not improve from 66.08549
196/196 - 62s - loss: 66.7954 - MinusLogProbMetric: 66.7954 - val_loss: 69.2768 - val_MinusLogProbMetric: 69.2768 - lr: 1.1111e-04 - 62s/epoch - 315ms/step
Epoch 251/1000
2023-10-25 12:27:17.003 
Epoch 251/1000 
	 loss: 68.1810, MinusLogProbMetric: 68.1810, val_loss: 66.5553, val_MinusLogProbMetric: 66.5553

Epoch 251: val_loss did not improve from 66.08549
196/196 - 58s - loss: 68.1810 - MinusLogProbMetric: 68.1810 - val_loss: 66.5553 - val_MinusLogProbMetric: 66.5553 - lr: 1.1111e-04 - 58s/epoch - 297ms/step
Epoch 252/1000
2023-10-25 12:28:14.216 
Epoch 252/1000 
	 loss: 67.2664, MinusLogProbMetric: 67.2664, val_loss: 64.8440, val_MinusLogProbMetric: 64.8440

Epoch 252: val_loss improved from 66.08549 to 64.84399, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 58s - loss: 67.2664 - MinusLogProbMetric: 67.2664 - val_loss: 64.8440 - val_MinusLogProbMetric: 64.8440 - lr: 1.1111e-04 - 58s/epoch - 297ms/step
Epoch 253/1000
2023-10-25 12:29:17.190 
Epoch 253/1000 
	 loss: 64.8561, MinusLogProbMetric: 64.8561, val_loss: 65.0221, val_MinusLogProbMetric: 65.0221

Epoch 253: val_loss did not improve from 64.84399
196/196 - 62s - loss: 64.8561 - MinusLogProbMetric: 64.8561 - val_loss: 65.0221 - val_MinusLogProbMetric: 65.0221 - lr: 1.1111e-04 - 62s/epoch - 316ms/step
Epoch 254/1000
2023-10-25 12:30:20.369 
Epoch 254/1000 
	 loss: 71.6170, MinusLogProbMetric: 71.6170, val_loss: 69.4385, val_MinusLogProbMetric: 69.4385

Epoch 254: val_loss did not improve from 64.84399
196/196 - 63s - loss: 71.6170 - MinusLogProbMetric: 71.6170 - val_loss: 69.4385 - val_MinusLogProbMetric: 69.4385 - lr: 1.1111e-04 - 63s/epoch - 322ms/step
Epoch 255/1000
2023-10-25 12:31:23.064 
Epoch 255/1000 
	 loss: 66.2649, MinusLogProbMetric: 66.2649, val_loss: 66.5411, val_MinusLogProbMetric: 66.5411

Epoch 255: val_loss did not improve from 64.84399
196/196 - 63s - loss: 66.2649 - MinusLogProbMetric: 66.2649 - val_loss: 66.5411 - val_MinusLogProbMetric: 66.5411 - lr: 1.1111e-04 - 63s/epoch - 320ms/step
Epoch 256/1000
2023-10-25 12:32:22.779 
Epoch 256/1000 
	 loss: 66.2212, MinusLogProbMetric: 66.2212, val_loss: 65.9173, val_MinusLogProbMetric: 65.9173

Epoch 256: val_loss did not improve from 64.84399
196/196 - 60s - loss: 66.2212 - MinusLogProbMetric: 66.2212 - val_loss: 65.9173 - val_MinusLogProbMetric: 65.9173 - lr: 1.1111e-04 - 60s/epoch - 305ms/step
Epoch 257/1000
2023-10-25 12:33:22.170 
Epoch 257/1000 
	 loss: 64.7422, MinusLogProbMetric: 64.7422, val_loss: 65.7667, val_MinusLogProbMetric: 65.7667

Epoch 257: val_loss did not improve from 64.84399
196/196 - 59s - loss: 64.7422 - MinusLogProbMetric: 64.7422 - val_loss: 65.7667 - val_MinusLogProbMetric: 65.7667 - lr: 1.1111e-04 - 59s/epoch - 303ms/step
Epoch 258/1000
2023-10-25 12:34:18.735 
Epoch 258/1000 
	 loss: 65.5623, MinusLogProbMetric: 65.5623, val_loss: 64.7978, val_MinusLogProbMetric: 64.7978

Epoch 258: val_loss improved from 64.84399 to 64.79778, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 57s - loss: 65.5623 - MinusLogProbMetric: 65.5623 - val_loss: 64.7978 - val_MinusLogProbMetric: 64.7978 - lr: 1.1111e-04 - 57s/epoch - 293ms/step
Epoch 259/1000
2023-10-25 12:35:17.605 
Epoch 259/1000 
	 loss: 70.5320, MinusLogProbMetric: 70.5320, val_loss: 67.0934, val_MinusLogProbMetric: 67.0934

Epoch 259: val_loss did not improve from 64.79778
196/196 - 58s - loss: 70.5320 - MinusLogProbMetric: 70.5320 - val_loss: 67.0934 - val_MinusLogProbMetric: 67.0934 - lr: 1.1111e-04 - 58s/epoch - 296ms/step
Epoch 260/1000
2023-10-25 12:36:20.206 
Epoch 260/1000 
	 loss: 65.3919, MinusLogProbMetric: 65.3919, val_loss: 66.3995, val_MinusLogProbMetric: 66.3995

Epoch 260: val_loss did not improve from 64.79778
196/196 - 63s - loss: 65.3919 - MinusLogProbMetric: 65.3919 - val_loss: 66.3995 - val_MinusLogProbMetric: 66.3995 - lr: 1.1111e-04 - 63s/epoch - 319ms/step
Epoch 261/1000
2023-10-25 12:37:11.831 
Epoch 261/1000 
	 loss: 65.6041, MinusLogProbMetric: 65.6041, val_loss: 64.4499, val_MinusLogProbMetric: 64.4499

Epoch 261: val_loss improved from 64.79778 to 64.44988, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 52s - loss: 65.6041 - MinusLogProbMetric: 65.6041 - val_loss: 64.4499 - val_MinusLogProbMetric: 64.4499 - lr: 1.1111e-04 - 52s/epoch - 267ms/step
Epoch 262/1000
2023-10-25 12:38:02.544 
Epoch 262/1000 
	 loss: 64.1132, MinusLogProbMetric: 64.1132, val_loss: 64.1282, val_MinusLogProbMetric: 64.1282

Epoch 262: val_loss improved from 64.44988 to 64.12820, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 51s - loss: 64.1132 - MinusLogProbMetric: 64.1132 - val_loss: 64.1282 - val_MinusLogProbMetric: 64.1282 - lr: 1.1111e-04 - 51s/epoch - 259ms/step
Epoch 263/1000
2023-10-25 12:38:55.415 
Epoch 263/1000 
	 loss: 64.0300, MinusLogProbMetric: 64.0300, val_loss: 63.5919, val_MinusLogProbMetric: 63.5919

Epoch 263: val_loss improved from 64.12820 to 63.59192, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 53s - loss: 64.0300 - MinusLogProbMetric: 64.0300 - val_loss: 63.5919 - val_MinusLogProbMetric: 63.5919 - lr: 1.1111e-04 - 53s/epoch - 269ms/step
Epoch 264/1000
2023-10-25 12:39:49.084 
Epoch 264/1000 
	 loss: 63.4684, MinusLogProbMetric: 63.4684, val_loss: 64.0521, val_MinusLogProbMetric: 64.0521

Epoch 264: val_loss did not improve from 63.59192
196/196 - 53s - loss: 63.4684 - MinusLogProbMetric: 63.4684 - val_loss: 64.0521 - val_MinusLogProbMetric: 64.0521 - lr: 1.1111e-04 - 53s/epoch - 270ms/step
Epoch 265/1000
2023-10-25 12:40:44.643 
Epoch 265/1000 
	 loss: 67.0877, MinusLogProbMetric: 67.0877, val_loss: 63.7192, val_MinusLogProbMetric: 63.7192

Epoch 265: val_loss did not improve from 63.59192
196/196 - 56s - loss: 67.0877 - MinusLogProbMetric: 67.0877 - val_loss: 63.7192 - val_MinusLogProbMetric: 63.7192 - lr: 1.1111e-04 - 56s/epoch - 283ms/step
Epoch 266/1000
2023-10-25 12:41:38.853 
Epoch 266/1000 
	 loss: 63.4274, MinusLogProbMetric: 63.4274, val_loss: 63.8547, val_MinusLogProbMetric: 63.8547

Epoch 266: val_loss did not improve from 63.59192
196/196 - 54s - loss: 63.4274 - MinusLogProbMetric: 63.4274 - val_loss: 63.8547 - val_MinusLogProbMetric: 63.8547 - lr: 1.1111e-04 - 54s/epoch - 277ms/step
Epoch 267/1000
2023-10-25 12:42:35.110 
Epoch 267/1000 
	 loss: 62.8984, MinusLogProbMetric: 62.8984, val_loss: 63.4123, val_MinusLogProbMetric: 63.4123

Epoch 267: val_loss improved from 63.59192 to 63.41235, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 57s - loss: 62.8984 - MinusLogProbMetric: 62.8984 - val_loss: 63.4123 - val_MinusLogProbMetric: 63.4123 - lr: 1.1111e-04 - 57s/epoch - 292ms/step
Epoch 268/1000
2023-10-25 12:43:31.741 
Epoch 268/1000 
	 loss: 65.0230, MinusLogProbMetric: 65.0230, val_loss: 63.3707, val_MinusLogProbMetric: 63.3707

Epoch 268: val_loss improved from 63.41235 to 63.37074, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 57s - loss: 65.0230 - MinusLogProbMetric: 65.0230 - val_loss: 63.3707 - val_MinusLogProbMetric: 63.3707 - lr: 1.1111e-04 - 57s/epoch - 289ms/step
Epoch 269/1000
2023-10-25 12:44:28.018 
Epoch 269/1000 
	 loss: 67.2284, MinusLogProbMetric: 67.2284, val_loss: 65.3540, val_MinusLogProbMetric: 65.3540

Epoch 269: val_loss did not improve from 63.37074
196/196 - 55s - loss: 67.2284 - MinusLogProbMetric: 67.2284 - val_loss: 65.3540 - val_MinusLogProbMetric: 65.3540 - lr: 1.1111e-04 - 55s/epoch - 282ms/step
Epoch 270/1000
2023-10-25 12:45:25.231 
Epoch 270/1000 
	 loss: 67.7333, MinusLogProbMetric: 67.7333, val_loss: 65.1590, val_MinusLogProbMetric: 65.1590

Epoch 270: val_loss did not improve from 63.37074
196/196 - 57s - loss: 67.7333 - MinusLogProbMetric: 67.7333 - val_loss: 65.1590 - val_MinusLogProbMetric: 65.1590 - lr: 1.1111e-04 - 57s/epoch - 292ms/step
Epoch 271/1000
2023-10-25 12:46:20.737 
Epoch 271/1000 
	 loss: 62.7743, MinusLogProbMetric: 62.7743, val_loss: 62.5290, val_MinusLogProbMetric: 62.5290

Epoch 271: val_loss improved from 63.37074 to 62.52905, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 56s - loss: 62.7743 - MinusLogProbMetric: 62.7743 - val_loss: 62.5290 - val_MinusLogProbMetric: 62.5290 - lr: 1.1111e-04 - 56s/epoch - 288ms/step
Epoch 272/1000
2023-10-25 12:47:14.633 
Epoch 272/1000 
	 loss: 62.0839, MinusLogProbMetric: 62.0839, val_loss: 62.1550, val_MinusLogProbMetric: 62.1550

Epoch 272: val_loss improved from 62.52905 to 62.15501, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_350/weights/best_weights.h5
196/196 - 54s - loss: 62.0839 - MinusLogProbMetric: 62.0839 - val_loss: 62.1550 - val_MinusLogProbMetric: 62.1550 - lr: 1.1111e-04 - 54s/epoch - 275ms/step
Epoch 273/1000
2023-10-25 12:48:08.006 
Epoch 273/1000 
	 loss: 62.1175, MinusLogProbMetric: 62.1175, val_loss: 63.8067, val_MinusLogProbMetric: 63.8067

Epoch 273: val_loss did not improve from 62.15501
196/196 - 53s - loss: 62.1175 - MinusLogProbMetric: 62.1175 - val_loss: 63.8067 - val_MinusLogProbMetric: 63.8067 - lr: 1.1111e-04 - 53s/epoch - 268ms/step
Epoch 274/1000
2023-10-25 12:49:07.124 
Epoch 274/1000 
	 loss: 62.1801, MinusLogProbMetric: 62.1801, val_loss: 62.4256, val_MinusLogProbMetric: 62.4256

Epoch 274: val_loss did not improve from 62.15501
196/196 - 59s - loss: 62.1801 - MinusLogProbMetric: 62.1801 - val_loss: 62.4256 - val_MinusLogProbMetric: 62.4256 - lr: 1.1111e-04 - 59s/epoch - 302ms/step
Epoch 275/1000
2023-10-25 12:50:09.903 
Epoch 275/1000 
	 loss: 61.8219, MinusLogProbMetric: 61.8219, val_loss: 62.8506, val_MinusLogProbMetric: 62.8506

Epoch 275: val_loss did not improve from 62.15501
196/196 - 63s - loss: 61.8219 - MinusLogProbMetric: 61.8219 - val_loss: 62.8506 - val_MinusLogProbMetric: 62.8506 - lr: 1.1111e-04 - 63s/epoch - 320ms/step
Epoch 276/1000
2023-10-25 12:51:10.583 
Epoch 276/1000 
	 loss: 62.3065, MinusLogProbMetric: 62.3065, val_loss: 63.8215, val_MinusLogProbMetric: 63.8215

Epoch 276: val_loss did not improve from 62.15501
196/196 - 61s - loss: 62.3065 - MinusLogProbMetric: 62.3065 - val_loss: 63.8215 - val_MinusLogProbMetric: 63.8215 - lr: 1.1111e-04 - 61s/epoch - 310ms/step
Epoch 277/1000
2023-10-25 12:52:13.658 
Epoch 277/1000 
	 loss: 92.6713, MinusLogProbMetric: 92.6713, val_loss: 76.9107, val_MinusLogProbMetric: 76.9107

Epoch 277: val_loss did not improve from 62.15501
196/196 - 63s - loss: 92.6713 - MinusLogProbMetric: 92.6713 - val_loss: 76.9107 - val_MinusLogProbMetric: 76.9107 - lr: 1.1111e-04 - 63s/epoch - 322ms/step
Epoch 278/1000
2023-10-25 12:53:15.344 
Epoch 278/1000 
	 loss: 70.0261, MinusLogProbMetric: 70.0261, val_loss: 67.5383, val_MinusLogProbMetric: 67.5383

Epoch 278: val_loss did not improve from 62.15501
196/196 - 62s - loss: 70.0261 - MinusLogProbMetric: 70.0261 - val_loss: 67.5383 - val_MinusLogProbMetric: 67.5383 - lr: 1.1111e-04 - 62s/epoch - 315ms/step
Epoch 279/1000
2023-10-25 12:54:14.704 
Epoch 279/1000 
	 loss: 66.6521, MinusLogProbMetric: 66.6521, val_loss: 66.6817, val_MinusLogProbMetric: 66.6817

Epoch 279: val_loss did not improve from 62.15501
196/196 - 59s - loss: 66.6521 - MinusLogProbMetric: 66.6521 - val_loss: 66.6817 - val_MinusLogProbMetric: 66.6817 - lr: 1.1111e-04 - 59s/epoch - 303ms/step
Epoch 280/1000
2023-10-25 12:55:14.721 
Epoch 280/1000 
	 loss: 68.3726, MinusLogProbMetric: 68.3726, val_loss: 119.7127, val_MinusLogProbMetric: 119.7127

Epoch 280: val_loss did not improve from 62.15501
196/196 - 60s - loss: 68.3726 - MinusLogProbMetric: 68.3726 - val_loss: 119.7127 - val_MinusLogProbMetric: 119.7127 - lr: 1.1111e-04 - 60s/epoch - 306ms/step
Epoch 281/1000
2023-10-25 12:56:15.597 
Epoch 281/1000 
	 loss: 80.5031, MinusLogProbMetric: 80.5031, val_loss: 68.0907, val_MinusLogProbMetric: 68.0907

Epoch 281: val_loss did not improve from 62.15501
196/196 - 61s - loss: 80.5031 - MinusLogProbMetric: 80.5031 - val_loss: 68.0907 - val_MinusLogProbMetric: 68.0907 - lr: 1.1111e-04 - 61s/epoch - 311ms/step
Epoch 282/1000
2023-10-25 12:57:13.456 
Epoch 282/1000 
	 loss: 150.2447, MinusLogProbMetric: 150.2447, val_loss: 115.8312, val_MinusLogProbMetric: 115.8312

Epoch 282: val_loss did not improve from 62.15501
196/196 - 58s - loss: 150.2447 - MinusLogProbMetric: 150.2447 - val_loss: 115.8312 - val_MinusLogProbMetric: 115.8312 - lr: 1.1111e-04 - 58s/epoch - 295ms/step
Epoch 283/1000
2023-10-25 12:58:15.040 
Epoch 283/1000 
	 loss: 103.6377, MinusLogProbMetric: 103.6377, val_loss: 95.9980, val_MinusLogProbMetric: 95.9980

Epoch 283: val_loss did not improve from 62.15501
196/196 - 62s - loss: 103.6377 - MinusLogProbMetric: 103.6377 - val_loss: 95.9980 - val_MinusLogProbMetric: 95.9980 - lr: 1.1111e-04 - 62s/epoch - 314ms/step
Epoch 284/1000
2023-10-25 12:59:16.134 
Epoch 284/1000 
	 loss: 92.4747, MinusLogProbMetric: 92.4747, val_loss: 89.8742, val_MinusLogProbMetric: 89.8742

Epoch 284: val_loss did not improve from 62.15501
196/196 - 61s - loss: 92.4747 - MinusLogProbMetric: 92.4747 - val_loss: 89.8742 - val_MinusLogProbMetric: 89.8742 - lr: 1.1111e-04 - 61s/epoch - 312ms/step
Epoch 285/1000
2023-10-25 13:00:19.353 
Epoch 285/1000 
	 loss: 86.7502, MinusLogProbMetric: 86.7502, val_loss: 85.6939, val_MinusLogProbMetric: 85.6939

Epoch 285: val_loss did not improve from 62.15501
196/196 - 63s - loss: 86.7502 - MinusLogProbMetric: 86.7502 - val_loss: 85.6939 - val_MinusLogProbMetric: 85.6939 - lr: 1.1111e-04 - 63s/epoch - 323ms/step
Epoch 286/1000
2023-10-25 13:01:17.839 
Epoch 286/1000 
	 loss: 83.1156, MinusLogProbMetric: 83.1156, val_loss: 83.6674, val_MinusLogProbMetric: 83.6674

Epoch 286: val_loss did not improve from 62.15501
196/196 - 58s - loss: 83.1156 - MinusLogProbMetric: 83.1156 - val_loss: 83.6674 - val_MinusLogProbMetric: 83.6674 - lr: 1.1111e-04 - 58s/epoch - 298ms/step
Epoch 287/1000
2023-10-25 13:02:20.538 
Epoch 287/1000 
	 loss: 80.7125, MinusLogProbMetric: 80.7125, val_loss: 79.5481, val_MinusLogProbMetric: 79.5481

Epoch 287: val_loss did not improve from 62.15501
196/196 - 63s - loss: 80.7125 - MinusLogProbMetric: 80.7125 - val_loss: 79.5481 - val_MinusLogProbMetric: 79.5481 - lr: 1.1111e-04 - 63s/epoch - 320ms/step
Epoch 288/1000
2023-10-25 13:03:23.892 
Epoch 288/1000 
	 loss: 78.9920, MinusLogProbMetric: 78.9920, val_loss: 78.0295, val_MinusLogProbMetric: 78.0295

Epoch 288: val_loss did not improve from 62.15501
196/196 - 63s - loss: 78.9920 - MinusLogProbMetric: 78.9920 - val_loss: 78.0295 - val_MinusLogProbMetric: 78.0295 - lr: 1.1111e-04 - 63s/epoch - 323ms/step
Epoch 289/1000
2023-10-25 13:04:24.782 
Epoch 289/1000 
	 loss: 76.0435, MinusLogProbMetric: 76.0435, val_loss: 75.7435, val_MinusLogProbMetric: 75.7435

Epoch 289: val_loss did not improve from 62.15501
196/196 - 61s - loss: 76.0435 - MinusLogProbMetric: 76.0435 - val_loss: 75.7435 - val_MinusLogProbMetric: 75.7435 - lr: 1.1111e-04 - 61s/epoch - 311ms/step
Epoch 290/1000
2023-10-25 13:05:26.389 
Epoch 290/1000 
	 loss: 74.4873, MinusLogProbMetric: 74.4873, val_loss: 73.9548, val_MinusLogProbMetric: 73.9548

Epoch 290: val_loss did not improve from 62.15501
196/196 - 62s - loss: 74.4873 - MinusLogProbMetric: 74.4873 - val_loss: 73.9548 - val_MinusLogProbMetric: 73.9548 - lr: 1.1111e-04 - 62s/epoch - 314ms/step
Epoch 291/1000
2023-10-25 13:06:27.543 
Epoch 291/1000 
	 loss: 73.2384, MinusLogProbMetric: 73.2384, val_loss: 72.8789, val_MinusLogProbMetric: 72.8789

Epoch 291: val_loss did not improve from 62.15501
196/196 - 61s - loss: 73.2384 - MinusLogProbMetric: 73.2384 - val_loss: 72.8789 - val_MinusLogProbMetric: 72.8789 - lr: 1.1111e-04 - 61s/epoch - 312ms/step
Epoch 292/1000
2023-10-25 13:07:31.597 
Epoch 292/1000 
	 loss: 72.1605, MinusLogProbMetric: 72.1605, val_loss: 71.9708, val_MinusLogProbMetric: 71.9708

Epoch 292: val_loss did not improve from 62.15501
196/196 - 64s - loss: 72.1605 - MinusLogProbMetric: 72.1605 - val_loss: 71.9708 - val_MinusLogProbMetric: 71.9708 - lr: 1.1111e-04 - 64s/epoch - 327ms/step
Epoch 293/1000
2023-10-25 13:08:31.003 
Epoch 293/1000 
	 loss: 70.9292, MinusLogProbMetric: 70.9292, val_loss: 71.4087, val_MinusLogProbMetric: 71.4087

Epoch 293: val_loss did not improve from 62.15501
196/196 - 59s - loss: 70.9292 - MinusLogProbMetric: 70.9292 - val_loss: 71.4087 - val_MinusLogProbMetric: 71.4087 - lr: 1.1111e-04 - 59s/epoch - 303ms/step
Epoch 294/1000
2023-10-25 13:09:32.700 
Epoch 294/1000 
	 loss: 70.0985, MinusLogProbMetric: 70.0985, val_loss: 69.9224, val_MinusLogProbMetric: 69.9224

Epoch 294: val_loss did not improve from 62.15501
196/196 - 62s - loss: 70.0985 - MinusLogProbMetric: 70.0985 - val_loss: 69.9224 - val_MinusLogProbMetric: 69.9224 - lr: 1.1111e-04 - 62s/epoch - 315ms/step
Epoch 295/1000
2023-10-25 13:10:32.790 
Epoch 295/1000 
	 loss: 69.3429, MinusLogProbMetric: 69.3429, val_loss: 69.2392, val_MinusLogProbMetric: 69.2392

Epoch 295: val_loss did not improve from 62.15501
196/196 - 60s - loss: 69.3429 - MinusLogProbMetric: 69.3429 - val_loss: 69.2392 - val_MinusLogProbMetric: 69.2392 - lr: 1.1111e-04 - 60s/epoch - 307ms/step
Epoch 296/1000
2023-10-25 13:11:33.401 
Epoch 296/1000 
	 loss: 68.7907, MinusLogProbMetric: 68.7907, val_loss: 68.9919, val_MinusLogProbMetric: 68.9919

Epoch 296: val_loss did not improve from 62.15501
196/196 - 61s - loss: 68.7907 - MinusLogProbMetric: 68.7907 - val_loss: 68.9919 - val_MinusLogProbMetric: 68.9919 - lr: 1.1111e-04 - 61s/epoch - 309ms/step
Epoch 297/1000
2023-10-25 13:12:35.490 
Epoch 297/1000 
	 loss: 68.6759, MinusLogProbMetric: 68.6759, val_loss: 68.1704, val_MinusLogProbMetric: 68.1704

Epoch 297: val_loss did not improve from 62.15501
196/196 - 62s - loss: 68.6759 - MinusLogProbMetric: 68.6759 - val_loss: 68.1704 - val_MinusLogProbMetric: 68.1704 - lr: 1.1111e-04 - 62s/epoch - 317ms/step
Epoch 298/1000
2023-10-25 13:13:35.085 
Epoch 298/1000 
	 loss: 68.8757, MinusLogProbMetric: 68.8757, val_loss: 68.0753, val_MinusLogProbMetric: 68.0753

Epoch 298: val_loss did not improve from 62.15501
196/196 - 60s - loss: 68.8757 - MinusLogProbMetric: 68.8757 - val_loss: 68.0753 - val_MinusLogProbMetric: 68.0753 - lr: 1.1111e-04 - 60s/epoch - 304ms/step
Epoch 299/1000
2023-10-25 13:14:35.108 
Epoch 299/1000 
	 loss: 67.3220, MinusLogProbMetric: 67.3220, val_loss: 68.3062, val_MinusLogProbMetric: 68.3062

Epoch 299: val_loss did not improve from 62.15501
196/196 - 60s - loss: 67.3220 - MinusLogProbMetric: 67.3220 - val_loss: 68.3062 - val_MinusLogProbMetric: 68.3062 - lr: 1.1111e-04 - 60s/epoch - 306ms/step
Epoch 300/1000
2023-10-25 13:15:36.665 
Epoch 300/1000 
	 loss: 67.0712, MinusLogProbMetric: 67.0712, val_loss: 66.5790, val_MinusLogProbMetric: 66.5790

Epoch 300: val_loss did not improve from 62.15501
196/196 - 62s - loss: 67.0712 - MinusLogProbMetric: 67.0712 - val_loss: 66.5790 - val_MinusLogProbMetric: 66.5790 - lr: 1.1111e-04 - 62s/epoch - 314ms/step
Epoch 301/1000
2023-10-25 13:16:35.770 
Epoch 301/1000 
	 loss: 250.8818, MinusLogProbMetric: 250.8818, val_loss: 312.3084, val_MinusLogProbMetric: 312.3084

Epoch 301: val_loss did not improve from 62.15501
196/196 - 59s - loss: 250.8818 - MinusLogProbMetric: 250.8818 - val_loss: 312.3084 - val_MinusLogProbMetric: 312.3084 - lr: 1.1111e-04 - 59s/epoch - 302ms/step
Epoch 302/1000
2023-10-25 13:17:35.866 
Epoch 302/1000 
	 loss: 251.5089, MinusLogProbMetric: 251.5089, val_loss: 218.2022, val_MinusLogProbMetric: 218.2022

Epoch 302: val_loss did not improve from 62.15501
196/196 - 60s - loss: 251.5089 - MinusLogProbMetric: 251.5089 - val_loss: 218.2022 - val_MinusLogProbMetric: 218.2022 - lr: 1.1111e-04 - 60s/epoch - 307ms/step
Epoch 303/1000
2023-10-25 13:18:36.260 
Epoch 303/1000 
	 loss: 201.0506, MinusLogProbMetric: 201.0506, val_loss: 221.7704, val_MinusLogProbMetric: 221.7704

Epoch 303: val_loss did not improve from 62.15501
196/196 - 60s - loss: 201.0506 - MinusLogProbMetric: 201.0506 - val_loss: 221.7704 - val_MinusLogProbMetric: 221.7704 - lr: 1.1111e-04 - 60s/epoch - 308ms/step
Epoch 304/1000
2023-10-25 13:19:35.048 
Epoch 304/1000 
	 loss: 188.5553, MinusLogProbMetric: 188.5553, val_loss: 176.2765, val_MinusLogProbMetric: 176.2765

Epoch 304: val_loss did not improve from 62.15501
196/196 - 59s - loss: 188.5553 - MinusLogProbMetric: 188.5553 - val_loss: 176.2765 - val_MinusLogProbMetric: 176.2765 - lr: 1.1111e-04 - 59s/epoch - 300ms/step
Epoch 305/1000
2023-10-25 13:20:34.573 
Epoch 305/1000 
	 loss: 169.5917, MinusLogProbMetric: 169.5917, val_loss: 163.0970, val_MinusLogProbMetric: 163.0970

Epoch 305: val_loss did not improve from 62.15501
196/196 - 60s - loss: 169.5917 - MinusLogProbMetric: 169.5917 - val_loss: 163.0970 - val_MinusLogProbMetric: 163.0970 - lr: 1.1111e-04 - 60s/epoch - 304ms/step
Epoch 306/1000
2023-10-25 13:21:36.953 
Epoch 306/1000 
	 loss: 160.2664, MinusLogProbMetric: 160.2664, val_loss: 157.3487, val_MinusLogProbMetric: 157.3487

Epoch 306: val_loss did not improve from 62.15501
196/196 - 62s - loss: 160.2664 - MinusLogProbMetric: 160.2664 - val_loss: 157.3487 - val_MinusLogProbMetric: 157.3487 - lr: 1.1111e-04 - 62s/epoch - 318ms/step
Epoch 307/1000
2023-10-25 13:22:39.048 
Epoch 307/1000 
	 loss: 154.6278, MinusLogProbMetric: 154.6278, val_loss: 151.6994, val_MinusLogProbMetric: 151.6994

Epoch 307: val_loss did not improve from 62.15501
196/196 - 62s - loss: 154.6278 - MinusLogProbMetric: 154.6278 - val_loss: 151.6994 - val_MinusLogProbMetric: 151.6994 - lr: 1.1111e-04 - 62s/epoch - 317ms/step
Epoch 308/1000
2023-10-25 13:23:38.308 
Epoch 308/1000 
	 loss: 152.2192, MinusLogProbMetric: 152.2192, val_loss: 156.6964, val_MinusLogProbMetric: 156.6964

Epoch 308: val_loss did not improve from 62.15501
196/196 - 59s - loss: 152.2192 - MinusLogProbMetric: 152.2192 - val_loss: 156.6964 - val_MinusLogProbMetric: 156.6964 - lr: 1.1111e-04 - 59s/epoch - 302ms/step
Epoch 309/1000
2023-10-25 13:24:41.067 
Epoch 309/1000 
	 loss: 148.0705, MinusLogProbMetric: 148.0705, val_loss: 145.0462, val_MinusLogProbMetric: 145.0462

Epoch 309: val_loss did not improve from 62.15501
196/196 - 63s - loss: 148.0705 - MinusLogProbMetric: 148.0705 - val_loss: 145.0462 - val_MinusLogProbMetric: 145.0462 - lr: 1.1111e-04 - 63s/epoch - 320ms/step
Epoch 310/1000
2023-10-25 13:25:42.169 
Epoch 310/1000 
	 loss: 143.0975, MinusLogProbMetric: 143.0975, val_loss: 152.0056, val_MinusLogProbMetric: 152.0056

Epoch 310: val_loss did not improve from 62.15501
196/196 - 61s - loss: 143.0975 - MinusLogProbMetric: 143.0975 - val_loss: 152.0056 - val_MinusLogProbMetric: 152.0056 - lr: 1.1111e-04 - 61s/epoch - 312ms/step
Epoch 311/1000
2023-10-25 13:26:41.939 
Epoch 311/1000 
	 loss: 141.1856, MinusLogProbMetric: 141.1856, val_loss: 138.9457, val_MinusLogProbMetric: 138.9457

Epoch 311: val_loss did not improve from 62.15501
196/196 - 60s - loss: 141.1856 - MinusLogProbMetric: 141.1856 - val_loss: 138.9457 - val_MinusLogProbMetric: 138.9457 - lr: 1.1111e-04 - 60s/epoch - 305ms/step
Epoch 312/1000
2023-10-25 13:27:40.746 
Epoch 312/1000 
	 loss: 136.6644, MinusLogProbMetric: 136.6644, val_loss: 135.7237, val_MinusLogProbMetric: 135.7237

Epoch 312: val_loss did not improve from 62.15501
196/196 - 59s - loss: 136.6644 - MinusLogProbMetric: 136.6644 - val_loss: 135.7237 - val_MinusLogProbMetric: 135.7237 - lr: 1.1111e-04 - 59s/epoch - 300ms/step
Epoch 313/1000
2023-10-25 13:28:41.294 
Epoch 313/1000 
	 loss: 136.3453, MinusLogProbMetric: 136.3453, val_loss: 134.2462, val_MinusLogProbMetric: 134.2462

Epoch 313: val_loss did not improve from 62.15501
196/196 - 61s - loss: 136.3453 - MinusLogProbMetric: 136.3453 - val_loss: 134.2462 - val_MinusLogProbMetric: 134.2462 - lr: 1.1111e-04 - 61s/epoch - 309ms/step
Epoch 314/1000
2023-10-25 13:29:39.420 
Epoch 314/1000 
	 loss: 132.6683, MinusLogProbMetric: 132.6683, val_loss: 131.4972, val_MinusLogProbMetric: 131.4972

Epoch 314: val_loss did not improve from 62.15501
196/196 - 58s - loss: 132.6683 - MinusLogProbMetric: 132.6683 - val_loss: 131.4972 - val_MinusLogProbMetric: 131.4972 - lr: 1.1111e-04 - 58s/epoch - 297ms/step
Epoch 315/1000
2023-10-25 13:30:40.049 
Epoch 315/1000 
	 loss: 130.5515, MinusLogProbMetric: 130.5515, val_loss: 131.0209, val_MinusLogProbMetric: 131.0209

Epoch 315: val_loss did not improve from 62.15501
196/196 - 61s - loss: 130.5515 - MinusLogProbMetric: 130.5515 - val_loss: 131.0209 - val_MinusLogProbMetric: 131.0209 - lr: 1.1111e-04 - 61s/epoch - 309ms/step
Epoch 316/1000
2023-10-25 13:31:40.160 
Epoch 316/1000 
	 loss: 132.6349, MinusLogProbMetric: 132.6349, val_loss: 134.9353, val_MinusLogProbMetric: 134.9353

Epoch 316: val_loss did not improve from 62.15501
196/196 - 60s - loss: 132.6349 - MinusLogProbMetric: 132.6349 - val_loss: 134.9353 - val_MinusLogProbMetric: 134.9353 - lr: 1.1111e-04 - 60s/epoch - 307ms/step
Epoch 317/1000
2023-10-25 13:32:39.573 
Epoch 317/1000 
	 loss: 132.0009, MinusLogProbMetric: 132.0009, val_loss: 130.0212, val_MinusLogProbMetric: 130.0212

Epoch 317: val_loss did not improve from 62.15501
196/196 - 59s - loss: 132.0009 - MinusLogProbMetric: 132.0009 - val_loss: 130.0212 - val_MinusLogProbMetric: 130.0212 - lr: 1.1111e-04 - 59s/epoch - 303ms/step
Epoch 318/1000
2023-10-25 13:33:37.725 
Epoch 318/1000 
	 loss: 128.9301, MinusLogProbMetric: 128.9301, val_loss: 127.8887, val_MinusLogProbMetric: 127.8887

Epoch 318: val_loss did not improve from 62.15501
196/196 - 58s - loss: 128.9301 - MinusLogProbMetric: 128.9301 - val_loss: 127.8887 - val_MinusLogProbMetric: 127.8887 - lr: 1.1111e-04 - 58s/epoch - 297ms/step
Epoch 319/1000
2023-10-25 13:34:38.616 
Epoch 319/1000 
	 loss: 126.8685, MinusLogProbMetric: 126.8685, val_loss: 126.8326, val_MinusLogProbMetric: 126.8326

Epoch 319: val_loss did not improve from 62.15501
196/196 - 61s - loss: 126.8685 - MinusLogProbMetric: 126.8685 - val_loss: 126.8326 - val_MinusLogProbMetric: 126.8326 - lr: 1.1111e-04 - 61s/epoch - 311ms/step
Epoch 320/1000
2023-10-25 13:35:38.886 
Epoch 320/1000 
	 loss: 125.2778, MinusLogProbMetric: 125.2778, val_loss: 124.9773, val_MinusLogProbMetric: 124.9773

Epoch 320: val_loss did not improve from 62.15501
196/196 - 60s - loss: 125.2778 - MinusLogProbMetric: 125.2778 - val_loss: 124.9773 - val_MinusLogProbMetric: 124.9773 - lr: 1.1111e-04 - 60s/epoch - 307ms/step
Epoch 321/1000
2023-10-25 13:36:37.287 
Epoch 321/1000 
	 loss: 123.6917, MinusLogProbMetric: 123.6917, val_loss: 123.0238, val_MinusLogProbMetric: 123.0238

Epoch 321: val_loss did not improve from 62.15501
196/196 - 58s - loss: 123.6917 - MinusLogProbMetric: 123.6917 - val_loss: 123.0238 - val_MinusLogProbMetric: 123.0238 - lr: 1.1111e-04 - 58s/epoch - 298ms/step
Epoch 322/1000
2023-10-25 13:37:37.430 
Epoch 322/1000 
	 loss: 122.2576, MinusLogProbMetric: 122.2576, val_loss: 121.5746, val_MinusLogProbMetric: 121.5746

Epoch 322: val_loss did not improve from 62.15501
196/196 - 60s - loss: 122.2576 - MinusLogProbMetric: 122.2576 - val_loss: 121.5746 - val_MinusLogProbMetric: 121.5746 - lr: 1.1111e-04 - 60s/epoch - 307ms/step
Epoch 323/1000
2023-10-25 13:38:37.224 
Epoch 323/1000 
	 loss: 120.4431, MinusLogProbMetric: 120.4431, val_loss: 120.1017, val_MinusLogProbMetric: 120.1017

Epoch 323: val_loss did not improve from 62.15501
196/196 - 60s - loss: 120.4431 - MinusLogProbMetric: 120.4431 - val_loss: 120.1017 - val_MinusLogProbMetric: 120.1017 - lr: 5.5556e-05 - 60s/epoch - 305ms/step
Epoch 324/1000
2023-10-25 13:39:38.391 
Epoch 324/1000 
	 loss: 119.8121, MinusLogProbMetric: 119.8121, val_loss: 127.3496, val_MinusLogProbMetric: 127.3496

Epoch 324: val_loss did not improve from 62.15501
196/196 - 61s - loss: 119.8121 - MinusLogProbMetric: 119.8121 - val_loss: 127.3496 - val_MinusLogProbMetric: 127.3496 - lr: 5.5556e-05 - 61s/epoch - 312ms/step
Epoch 325/1000
2023-10-25 13:40:38.811 
Epoch 325/1000 
	 loss: 129.2612, MinusLogProbMetric: 129.2612, val_loss: 128.9518, val_MinusLogProbMetric: 128.9518

Epoch 325: val_loss did not improve from 62.15501
196/196 - 60s - loss: 129.2612 - MinusLogProbMetric: 129.2612 - val_loss: 128.9518 - val_MinusLogProbMetric: 128.9518 - lr: 5.5556e-05 - 60s/epoch - 308ms/step
Epoch 326/1000
2023-10-25 13:41:36.244 
Epoch 326/1000 
	 loss: 131.8363, MinusLogProbMetric: 131.8363, val_loss: 126.7751, val_MinusLogProbMetric: 126.7751

Epoch 326: val_loss did not improve from 62.15501
196/196 - 57s - loss: 131.8363 - MinusLogProbMetric: 131.8363 - val_loss: 126.7751 - val_MinusLogProbMetric: 126.7751 - lr: 5.5556e-05 - 57s/epoch - 293ms/step
Epoch 327/1000
2023-10-25 13:42:36.096 
Epoch 327/1000 
	 loss: 123.7775, MinusLogProbMetric: 123.7775, val_loss: 121.7614, val_MinusLogProbMetric: 121.7614

Epoch 327: val_loss did not improve from 62.15501
196/196 - 60s - loss: 123.7775 - MinusLogProbMetric: 123.7775 - val_loss: 121.7614 - val_MinusLogProbMetric: 121.7614 - lr: 5.5556e-05 - 60s/epoch - 305ms/step
Epoch 328/1000
2023-10-25 13:43:37.174 
Epoch 328/1000 
	 loss: 120.0586, MinusLogProbMetric: 120.0586, val_loss: 118.7614, val_MinusLogProbMetric: 118.7614

Epoch 328: val_loss did not improve from 62.15501
196/196 - 61s - loss: 120.0586 - MinusLogProbMetric: 120.0586 - val_loss: 118.7614 - val_MinusLogProbMetric: 118.7614 - lr: 5.5556e-05 - 61s/epoch - 312ms/step
Epoch 329/1000
2023-10-25 13:44:38.629 
Epoch 329/1000 
	 loss: 118.1416, MinusLogProbMetric: 118.1416, val_loss: 117.5721, val_MinusLogProbMetric: 117.5721

Epoch 329: val_loss did not improve from 62.15501
196/196 - 61s - loss: 118.1416 - MinusLogProbMetric: 118.1416 - val_loss: 117.5721 - val_MinusLogProbMetric: 117.5721 - lr: 5.5556e-05 - 61s/epoch - 314ms/step
Epoch 330/1000
2023-10-25 13:45:38.793 
Epoch 330/1000 
	 loss: 117.2208, MinusLogProbMetric: 117.2208, val_loss: 116.7317, val_MinusLogProbMetric: 116.7317

Epoch 330: val_loss did not improve from 62.15501
196/196 - 60s - loss: 117.2208 - MinusLogProbMetric: 117.2208 - val_loss: 116.7317 - val_MinusLogProbMetric: 116.7317 - lr: 5.5556e-05 - 60s/epoch - 307ms/step
Epoch 331/1000
2023-10-25 13:46:38.063 
Epoch 331/1000 
	 loss: 116.5290, MinusLogProbMetric: 116.5290, val_loss: 116.3173, val_MinusLogProbMetric: 116.3173

Epoch 331: val_loss did not improve from 62.15501
196/196 - 59s - loss: 116.5290 - MinusLogProbMetric: 116.5290 - val_loss: 116.3173 - val_MinusLogProbMetric: 116.3173 - lr: 5.5556e-05 - 59s/epoch - 302ms/step
Epoch 332/1000
2023-10-25 13:47:39.596 
Epoch 332/1000 
	 loss: 115.9401, MinusLogProbMetric: 115.9401, val_loss: 115.6943, val_MinusLogProbMetric: 115.6943

Epoch 332: val_loss did not improve from 62.15501
196/196 - 62s - loss: 115.9401 - MinusLogProbMetric: 115.9401 - val_loss: 115.6943 - val_MinusLogProbMetric: 115.6943 - lr: 5.5556e-05 - 62s/epoch - 314ms/step
Epoch 333/1000
2023-10-25 13:48:40.682 
Epoch 333/1000 
	 loss: 115.3173, MinusLogProbMetric: 115.3173, val_loss: 115.2418, val_MinusLogProbMetric: 115.2418

Epoch 333: val_loss did not improve from 62.15501
196/196 - 61s - loss: 115.3173 - MinusLogProbMetric: 115.3173 - val_loss: 115.2418 - val_MinusLogProbMetric: 115.2418 - lr: 5.5556e-05 - 61s/epoch - 312ms/step
Epoch 334/1000
2023-10-25 13:49:40.731 
Epoch 334/1000 
	 loss: 114.7315, MinusLogProbMetric: 114.7315, val_loss: 114.7371, val_MinusLogProbMetric: 114.7371

Epoch 334: val_loss did not improve from 62.15501
196/196 - 60s - loss: 114.7315 - MinusLogProbMetric: 114.7315 - val_loss: 114.7371 - val_MinusLogProbMetric: 114.7371 - lr: 5.5556e-05 - 60s/epoch - 306ms/step
Epoch 335/1000
2023-10-25 13:50:40.355 
Epoch 335/1000 
	 loss: 114.1793, MinusLogProbMetric: 114.1793, val_loss: 114.0697, val_MinusLogProbMetric: 114.0697

Epoch 335: val_loss did not improve from 62.15501
196/196 - 60s - loss: 114.1793 - MinusLogProbMetric: 114.1793 - val_loss: 114.0697 - val_MinusLogProbMetric: 114.0697 - lr: 5.5556e-05 - 60s/epoch - 304ms/step
Epoch 336/1000
2023-10-25 13:51:39.566 
Epoch 336/1000 
	 loss: 114.0478, MinusLogProbMetric: 114.0478, val_loss: 158.0094, val_MinusLogProbMetric: 158.0094

Epoch 336: val_loss did not improve from 62.15501
196/196 - 59s - loss: 114.0478 - MinusLogProbMetric: 114.0478 - val_loss: 158.0094 - val_MinusLogProbMetric: 158.0094 - lr: 5.5556e-05 - 59s/epoch - 302ms/step
Epoch 337/1000
2023-10-25 13:52:38.717 
Epoch 337/1000 
	 loss: 126.4635, MinusLogProbMetric: 126.4635, val_loss: 120.0026, val_MinusLogProbMetric: 120.0026

Epoch 337: val_loss did not improve from 62.15501
196/196 - 59s - loss: 126.4635 - MinusLogProbMetric: 126.4635 - val_loss: 120.0026 - val_MinusLogProbMetric: 120.0026 - lr: 5.5556e-05 - 59s/epoch - 302ms/step
Epoch 338/1000
2023-10-25 13:53:37.451 
Epoch 338/1000 
	 loss: 117.4844, MinusLogProbMetric: 117.4844, val_loss: 113.1840, val_MinusLogProbMetric: 113.1840

Epoch 338: val_loss did not improve from 62.15501
196/196 - 59s - loss: 117.4844 - MinusLogProbMetric: 117.4844 - val_loss: 113.1840 - val_MinusLogProbMetric: 113.1840 - lr: 5.5556e-05 - 59s/epoch - 300ms/step
Epoch 339/1000
2023-10-25 13:54:38.391 
Epoch 339/1000 
	 loss: 113.2376, MinusLogProbMetric: 113.2376, val_loss: 116.9783, val_MinusLogProbMetric: 116.9783

Epoch 339: val_loss did not improve from 62.15501
196/196 - 61s - loss: 113.2376 - MinusLogProbMetric: 113.2376 - val_loss: 116.9783 - val_MinusLogProbMetric: 116.9783 - lr: 5.5556e-05 - 61s/epoch - 311ms/step
Epoch 340/1000
2023-10-25 13:55:40.721 
Epoch 340/1000 
	 loss: 121.6332, MinusLogProbMetric: 121.6332, val_loss: 113.2127, val_MinusLogProbMetric: 113.2127

Epoch 340: val_loss did not improve from 62.15501
196/196 - 62s - loss: 121.6332 - MinusLogProbMetric: 121.6332 - val_loss: 113.2127 - val_MinusLogProbMetric: 113.2127 - lr: 5.5556e-05 - 62s/epoch - 318ms/step
Epoch 341/1000
2023-10-25 13:56:39.609 
Epoch 341/1000 
	 loss: 147.6387, MinusLogProbMetric: 147.6387, val_loss: 131.0313, val_MinusLogProbMetric: 131.0313

Epoch 341: val_loss did not improve from 62.15501
196/196 - 59s - loss: 147.6387 - MinusLogProbMetric: 147.6387 - val_loss: 131.0313 - val_MinusLogProbMetric: 131.0313 - lr: 5.5556e-05 - 59s/epoch - 300ms/step
Epoch 342/1000
2023-10-25 13:57:37.642 
Epoch 342/1000 
	 loss: 128.1003, MinusLogProbMetric: 128.1003, val_loss: 126.1966, val_MinusLogProbMetric: 126.1966

Epoch 342: val_loss did not improve from 62.15501
196/196 - 58s - loss: 128.1003 - MinusLogProbMetric: 128.1003 - val_loss: 126.1966 - val_MinusLogProbMetric: 126.1966 - lr: 5.5556e-05 - 58s/epoch - 296ms/step
Epoch 343/1000
2023-10-25 13:58:35.844 
Epoch 343/1000 
	 loss: 127.7261, MinusLogProbMetric: 127.7261, val_loss: 215.1138, val_MinusLogProbMetric: 215.1138

Epoch 343: val_loss did not improve from 62.15501
196/196 - 58s - loss: 127.7261 - MinusLogProbMetric: 127.7261 - val_loss: 215.1138 - val_MinusLogProbMetric: 215.1138 - lr: 5.5556e-05 - 58s/epoch - 297ms/step
Epoch 344/1000
2023-10-25 13:59:34.965 
Epoch 344/1000 
	 loss: 139.0560, MinusLogProbMetric: 139.0560, val_loss: 128.9160, val_MinusLogProbMetric: 128.9160

Epoch 344: val_loss did not improve from 62.15501
196/196 - 59s - loss: 139.0560 - MinusLogProbMetric: 139.0560 - val_loss: 128.9160 - val_MinusLogProbMetric: 128.9160 - lr: 5.5556e-05 - 59s/epoch - 302ms/step
Epoch 345/1000
2023-10-25 14:00:35.919 
Epoch 345/1000 
	 loss: 127.2421, MinusLogProbMetric: 127.2421, val_loss: 126.1882, val_MinusLogProbMetric: 126.1882

Epoch 345: val_loss did not improve from 62.15501
196/196 - 61s - loss: 127.2421 - MinusLogProbMetric: 127.2421 - val_loss: 126.1882 - val_MinusLogProbMetric: 126.1882 - lr: 5.5556e-05 - 61s/epoch - 311ms/step
Epoch 346/1000
2023-10-25 14:01:35.246 
Epoch 346/1000 
	 loss: 125.1473, MinusLogProbMetric: 125.1473, val_loss: 124.4999, val_MinusLogProbMetric: 124.4999

Epoch 346: val_loss did not improve from 62.15501
196/196 - 59s - loss: 125.1473 - MinusLogProbMetric: 125.1473 - val_loss: 124.4999 - val_MinusLogProbMetric: 124.4999 - lr: 5.5556e-05 - 59s/epoch - 303ms/step
Epoch 347/1000
2023-10-25 14:02:33.511 
Epoch 347/1000 
	 loss: 123.8270, MinusLogProbMetric: 123.8270, val_loss: 123.5368, val_MinusLogProbMetric: 123.5368

Epoch 347: val_loss did not improve from 62.15501
196/196 - 58s - loss: 123.8270 - MinusLogProbMetric: 123.8270 - val_loss: 123.5368 - val_MinusLogProbMetric: 123.5368 - lr: 5.5556e-05 - 58s/epoch - 297ms/step
Epoch 348/1000
2023-10-25 14:03:32.425 
Epoch 348/1000 
	 loss: 122.7653, MinusLogProbMetric: 122.7653, val_loss: 122.2915, val_MinusLogProbMetric: 122.2915

Epoch 348: val_loss did not improve from 62.15501
196/196 - 59s - loss: 122.7653 - MinusLogProbMetric: 122.7653 - val_loss: 122.2915 - val_MinusLogProbMetric: 122.2915 - lr: 5.5556e-05 - 59s/epoch - 301ms/step
Epoch 349/1000
2023-10-25 14:04:32.956 
Epoch 349/1000 
	 loss: 121.7338, MinusLogProbMetric: 121.7338, val_loss: 121.2651, val_MinusLogProbMetric: 121.2651

Epoch 349: val_loss did not improve from 62.15501
196/196 - 61s - loss: 121.7338 - MinusLogProbMetric: 121.7338 - val_loss: 121.2651 - val_MinusLogProbMetric: 121.2651 - lr: 5.5556e-05 - 61s/epoch - 309ms/step
Epoch 350/1000
2023-10-25 14:05:30.130 
Epoch 350/1000 
	 loss: 120.5370, MinusLogProbMetric: 120.5370, val_loss: 119.8568, val_MinusLogProbMetric: 119.8568

Epoch 350: val_loss did not improve from 62.15501
196/196 - 57s - loss: 120.5370 - MinusLogProbMetric: 120.5370 - val_loss: 119.8568 - val_MinusLogProbMetric: 119.8568 - lr: 5.5556e-05 - 57s/epoch - 292ms/step
Epoch 351/1000
2023-10-25 14:06:28.476 
Epoch 351/1000 
	 loss: 119.2037, MinusLogProbMetric: 119.2037, val_loss: 118.7762, val_MinusLogProbMetric: 118.7762

Epoch 351: val_loss did not improve from 62.15501
196/196 - 58s - loss: 119.2037 - MinusLogProbMetric: 119.2037 - val_loss: 118.7762 - val_MinusLogProbMetric: 118.7762 - lr: 5.5556e-05 - 58s/epoch - 298ms/step
Epoch 352/1000
2023-10-25 14:07:28.340 
Epoch 352/1000 
	 loss: 118.0899, MinusLogProbMetric: 118.0899, val_loss: 117.9381, val_MinusLogProbMetric: 117.9381

Epoch 352: val_loss did not improve from 62.15501
196/196 - 60s - loss: 118.0899 - MinusLogProbMetric: 118.0899 - val_loss: 117.9381 - val_MinusLogProbMetric: 117.9381 - lr: 5.5556e-05 - 60s/epoch - 305ms/step
Epoch 353/1000
2023-10-25 14:08:30.700 
Epoch 353/1000 
	 loss: 117.3739, MinusLogProbMetric: 117.3739, val_loss: 117.2253, val_MinusLogProbMetric: 117.2253

Epoch 353: val_loss did not improve from 62.15501
196/196 - 62s - loss: 117.3739 - MinusLogProbMetric: 117.3739 - val_loss: 117.2253 - val_MinusLogProbMetric: 117.2253 - lr: 5.5556e-05 - 62s/epoch - 318ms/step
Epoch 354/1000
2023-10-25 14:09:30.652 
Epoch 354/1000 
	 loss: 116.5979, MinusLogProbMetric: 116.5979, val_loss: 116.4770, val_MinusLogProbMetric: 116.4770

Epoch 354: val_loss did not improve from 62.15501
196/196 - 60s - loss: 116.5979 - MinusLogProbMetric: 116.5979 - val_loss: 116.4770 - val_MinusLogProbMetric: 116.4770 - lr: 5.5556e-05 - 60s/epoch - 306ms/step
Epoch 355/1000
2023-10-25 14:10:29.054 
Epoch 355/1000 
	 loss: 115.6825, MinusLogProbMetric: 115.6825, val_loss: 115.2859, val_MinusLogProbMetric: 115.2859

Epoch 355: val_loss did not improve from 62.15501
196/196 - 58s - loss: 115.6825 - MinusLogProbMetric: 115.6825 - val_loss: 115.2859 - val_MinusLogProbMetric: 115.2859 - lr: 5.5556e-05 - 58s/epoch - 298ms/step
Epoch 356/1000
2023-10-25 14:11:26.608 
Epoch 356/1000 
	 loss: 114.6940, MinusLogProbMetric: 114.6940, val_loss: 113.9134, val_MinusLogProbMetric: 113.9134

Epoch 356: val_loss did not improve from 62.15501
196/196 - 58s - loss: 114.6940 - MinusLogProbMetric: 114.6940 - val_loss: 113.9134 - val_MinusLogProbMetric: 113.9134 - lr: 5.5556e-05 - 58s/epoch - 294ms/step
Epoch 357/1000
2023-10-25 14:12:23.964 
Epoch 357/1000 
	 loss: 156.5338, MinusLogProbMetric: 156.5338, val_loss: 131.7688, val_MinusLogProbMetric: 131.7688

Epoch 357: val_loss did not improve from 62.15501
196/196 - 57s - loss: 156.5338 - MinusLogProbMetric: 156.5338 - val_loss: 131.7688 - val_MinusLogProbMetric: 131.7688 - lr: 5.5556e-05 - 57s/epoch - 293ms/step
Epoch 358/1000
2023-10-25 14:13:24.852 
Epoch 358/1000 
	 loss: 107.6742, MinusLogProbMetric: 107.6742, val_loss: 98.0964, val_MinusLogProbMetric: 98.0964

Epoch 358: val_loss did not improve from 62.15501
196/196 - 61s - loss: 107.6742 - MinusLogProbMetric: 107.6742 - val_loss: 98.0964 - val_MinusLogProbMetric: 98.0964 - lr: 5.5556e-05 - 61s/epoch - 311ms/step
Epoch 359/1000
2023-10-25 14:14:24.162 
Epoch 359/1000 
	 loss: 95.3469, MinusLogProbMetric: 95.3469, val_loss: 92.6239, val_MinusLogProbMetric: 92.6239

Epoch 359: val_loss did not improve from 62.15501
196/196 - 59s - loss: 95.3469 - MinusLogProbMetric: 95.3469 - val_loss: 92.6239 - val_MinusLogProbMetric: 92.6239 - lr: 5.5556e-05 - 59s/epoch - 303ms/step
Epoch 360/1000
2023-10-25 14:15:23.421 
Epoch 360/1000 
	 loss: 90.3142, MinusLogProbMetric: 90.3142, val_loss: 88.7439, val_MinusLogProbMetric: 88.7439

Epoch 360: val_loss did not improve from 62.15501
196/196 - 59s - loss: 90.3142 - MinusLogProbMetric: 90.3142 - val_loss: 88.7439 - val_MinusLogProbMetric: 88.7439 - lr: 5.5556e-05 - 59s/epoch - 302ms/step
Epoch 361/1000
2023-10-25 14:16:22.917 
Epoch 361/1000 
	 loss: 88.1199, MinusLogProbMetric: 88.1199, val_loss: 87.2025, val_MinusLogProbMetric: 87.2025

Epoch 361: val_loss did not improve from 62.15501
196/196 - 59s - loss: 88.1199 - MinusLogProbMetric: 88.1199 - val_loss: 87.2025 - val_MinusLogProbMetric: 87.2025 - lr: 5.5556e-05 - 59s/epoch - 304ms/step
Epoch 362/1000
2023-10-25 14:17:21.567 
Epoch 362/1000 
	 loss: 86.0894, MinusLogProbMetric: 86.0894, val_loss: 85.3821, val_MinusLogProbMetric: 85.3821

Epoch 362: val_loss did not improve from 62.15501
196/196 - 59s - loss: 86.0894 - MinusLogProbMetric: 86.0894 - val_loss: 85.3821 - val_MinusLogProbMetric: 85.3821 - lr: 5.5556e-05 - 59s/epoch - 299ms/step
Epoch 363/1000
2023-10-25 14:18:22.015 
Epoch 363/1000 
	 loss: 84.5861, MinusLogProbMetric: 84.5861, val_loss: 83.9570, val_MinusLogProbMetric: 83.9570

Epoch 363: val_loss did not improve from 62.15501
196/196 - 60s - loss: 84.5861 - MinusLogProbMetric: 84.5861 - val_loss: 83.9570 - val_MinusLogProbMetric: 83.9570 - lr: 5.5556e-05 - 60s/epoch - 308ms/step
Epoch 364/1000
2023-10-25 14:19:21.495 
Epoch 364/1000 
	 loss: 83.4103, MinusLogProbMetric: 83.4103, val_loss: 83.1496, val_MinusLogProbMetric: 83.1496

Epoch 364: val_loss did not improve from 62.15501
196/196 - 59s - loss: 83.4103 - MinusLogProbMetric: 83.4103 - val_loss: 83.1496 - val_MinusLogProbMetric: 83.1496 - lr: 5.5556e-05 - 59s/epoch - 303ms/step
Epoch 365/1000
2023-10-25 14:20:23.617 
Epoch 365/1000 
	 loss: 82.3027, MinusLogProbMetric: 82.3027, val_loss: 81.6361, val_MinusLogProbMetric: 81.6361

Epoch 365: val_loss did not improve from 62.15501
196/196 - 62s - loss: 82.3027 - MinusLogProbMetric: 82.3027 - val_loss: 81.6361 - val_MinusLogProbMetric: 81.6361 - lr: 5.5556e-05 - 62s/epoch - 317ms/step
Epoch 366/1000
2023-10-25 14:21:24.086 
Epoch 366/1000 
	 loss: 81.2772, MinusLogProbMetric: 81.2772, val_loss: 80.6986, val_MinusLogProbMetric: 80.6986

Epoch 366: val_loss did not improve from 62.15501
196/196 - 60s - loss: 81.2772 - MinusLogProbMetric: 81.2772 - val_loss: 80.6986 - val_MinusLogProbMetric: 80.6986 - lr: 5.5556e-05 - 60s/epoch - 309ms/step
Epoch 367/1000
2023-10-25 14:22:21.587 
Epoch 367/1000 
	 loss: 80.3472, MinusLogProbMetric: 80.3472, val_loss: 80.1491, val_MinusLogProbMetric: 80.1491

Epoch 367: val_loss did not improve from 62.15501
196/196 - 57s - loss: 80.3472 - MinusLogProbMetric: 80.3472 - val_loss: 80.1491 - val_MinusLogProbMetric: 80.1491 - lr: 5.5556e-05 - 57s/epoch - 293ms/step
Epoch 368/1000
2023-10-25 14:23:19.010 
Epoch 368/1000 
	 loss: 79.5600, MinusLogProbMetric: 79.5600, val_loss: 79.0952, val_MinusLogProbMetric: 79.0952

Epoch 368: val_loss did not improve from 62.15501
196/196 - 57s - loss: 79.5600 - MinusLogProbMetric: 79.5600 - val_loss: 79.0952 - val_MinusLogProbMetric: 79.0952 - lr: 5.5556e-05 - 57s/epoch - 293ms/step
Epoch 369/1000
2023-10-25 14:24:20.162 
Epoch 369/1000 
	 loss: 78.8381, MinusLogProbMetric: 78.8381, val_loss: 79.0675, val_MinusLogProbMetric: 79.0675

Epoch 369: val_loss did not improve from 62.15501
196/196 - 61s - loss: 78.8381 - MinusLogProbMetric: 78.8381 - val_loss: 79.0675 - val_MinusLogProbMetric: 79.0675 - lr: 5.5556e-05 - 61s/epoch - 312ms/step
Epoch 370/1000
2023-10-25 14:25:19.775 
Epoch 370/1000 
	 loss: 78.2212, MinusLogProbMetric: 78.2212, val_loss: 77.9643, val_MinusLogProbMetric: 77.9643

Epoch 370: val_loss did not improve from 62.15501
196/196 - 60s - loss: 78.2212 - MinusLogProbMetric: 78.2212 - val_loss: 77.9643 - val_MinusLogProbMetric: 77.9643 - lr: 5.5556e-05 - 60s/epoch - 304ms/step
Epoch 371/1000
2023-10-25 14:26:19.786 
Epoch 371/1000 
	 loss: 77.4625, MinusLogProbMetric: 77.4625, val_loss: 77.3253, val_MinusLogProbMetric: 77.3253

Epoch 371: val_loss did not improve from 62.15501
196/196 - 60s - loss: 77.4625 - MinusLogProbMetric: 77.4625 - val_loss: 77.3253 - val_MinusLogProbMetric: 77.3253 - lr: 5.5556e-05 - 60s/epoch - 306ms/step
Epoch 372/1000
2023-10-25 14:27:19.562 
Epoch 372/1000 
	 loss: 76.9499, MinusLogProbMetric: 76.9499, val_loss: 76.8304, val_MinusLogProbMetric: 76.8304

Epoch 372: val_loss did not improve from 62.15501
Restoring model weights from the end of the best epoch: 272.
196/196 - 60s - loss: 76.9499 - MinusLogProbMetric: 76.9499 - val_loss: 76.8304 - val_MinusLogProbMetric: 76.8304 - lr: 5.5556e-05 - 60s/epoch - 308ms/step
Epoch 372: early stopping
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Training succeeded with seed 440.
Model trained in 21998.55 s.

===========
Computing predictions
===========

Computing metrics...
Checking and setting numerical distributions.
Resetting dist_num.
Resetting dist_num.
Metrics computed in 0.87 s.
                ===========
                print("===========
Failed to plot
===========
")
                Exception type: ValueError
                Exception message: Number of rows must be a positive integer, not 0
                Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 481, Func.Name : prediction_function, Message : Plotters.cornerplotter(X_data_test.numpy(),X_data_nf.numpy(),path_to_results,ndims,norm=True) # type: ignore', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/Plotters.py , Line : 156, Func.Name : cornerplotter, Message : figure=corner.corner(target_samples,color=\'red\',bins=n_bins,labels=[r"%s" % s for s in labels],normalize1d=True)', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/corner.py , Line : 235, Func.Name : corner, Message : fig, axes = pl.subplots(K, K, figsize=(dim, dim))', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/pyplot.py , Line : 1502, Func.Name : subplots, Message : axs = fig.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 905, Func.Name : subplots, Message : gs = self.add_gridspec(nrows, ncols, figure=self, **gridspec_kw)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 1527, Func.Name : add_gridspec, Message : gs = GridSpec(nrows=nrows, ncols=ncols, figure=self, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 379, Func.Name : __init__, Message : super().__init__(nrows, ncols,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 49, Func.Name : __init__, Message : raise ValueError(']
                ===========

results.txt saved
results.json saved
Results log saved
Model predictions computed in 1.11 s.
===========
Run 350/720 done in 22302.15 s.
===========

Directory ../../results/CsplineN_new/run_351/ already exists.
Skipping it.
===========
Run 351/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_352/ already exists.
Skipping it.
===========
Run 352/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_353/ already exists.
Skipping it.
===========
Run 353/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_354/ already exists.
Skipping it.
===========
Run 354/720 already exists. Skipping it.
===========

===========
Generating train data for run 355.
===========
Train data generated in 0.19 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_355/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_355/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_355/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_355
self.data_kwargs: {'seed': 520}
self.x_data: [[2.1731846  3.8937397  6.9821544  ... 5.669328   0.68163395 3.5624375 ]
 [6.7535415  2.6749182  6.1859965  ... 3.3760262  4.638645   2.336814  ]
 [6.1686287  2.7568936  6.2012696  ... 2.5904071  3.339636   2.9827425 ]
 ...
 [6.6194353  2.9398146  6.1845684  ... 3.0365293  3.5444417  1.3393478 ]
 [5.613061   6.348512   5.164087   ... 1.829057   7.7712007  1.4071141 ]
 [6.893006   3.0565336  6.057088   ... 2.613742   4.4370584  2.0332472 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_500"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_501 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_45 (LogProbL  (None,)                  908640    
 ayer)                                                           
                                                                 
=================================================================
Total params: 908,640
Trainable params: 908,640
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_45/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_45'")
self.model: <keras.engine.functional.Functional object at 0x7f594f6c3d00>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f57f70eaf80>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f57f70eaf80>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f5827079f00>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f5835d8ae90>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_355/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f5835d8b400>, <keras.callbacks.ModelCheckpoint object at 0x7f5835d8b4c0>, <keras.callbacks.EarlyStopping object at 0x7f5835d8b730>, <keras.callbacks.ReduceLROnPlateau object at 0x7f5835d8b760>, <keras.callbacks.TerminateOnNaN object at 0x7f5835d8b3a0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_355/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 355/720 with hyperparameters:
timestamp = 2023-10-25 14:27:26.361332
ndims = 64
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 908640
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 2.1731846   3.8937397   6.9821544   1.4459838   8.670284    0.996012
  9.747296    5.3488703  10.829596    5.798153    7.4385934  -0.39963475
  2.887594    1.6816981   1.9483483   2.4950166   3.377345    6.4428034
  0.7434967  10.024268    5.9064217   3.9731474   5.977063    2.0677786
  5.421294    7.7411385   2.3877835   6.667169    1.0165741   6.879494
  2.333092    1.6771841   5.2052946  -0.01726444  8.224516   -0.06788193
  7.1896877   2.6926644   7.7422037   9.618431    2.508284    5.3540187
  8.24927     4.96435     2.6443048   8.933116    4.0272393   8.583376
  6.9591885   2.9305875   8.131322    3.9143033   8.096406    5.8134246
  9.931866    7.011071    7.2540216   5.1431737  10.5702715   6.340821
  4.2660613   5.669328    0.68163395  3.5624375 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 14:28:31.112 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6409.8218, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 65s - loss: nan - MinusLogProbMetric: 6409.8218 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 65s/epoch - 329ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 0.0003333333333333333.
===========
Generating train data for run 355.
===========
Train data generated in 0.13 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_355/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_355/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_355/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_355
self.data_kwargs: {'seed': 520}
self.x_data: [[2.1731846  3.8937397  6.9821544  ... 5.669328   0.68163395 3.5624375 ]
 [6.7535415  2.6749182  6.1859965  ... 3.3760262  4.638645   2.336814  ]
 [6.1686287  2.7568936  6.2012696  ... 2.5904071  3.339636   2.9827425 ]
 ...
 [6.6194353  2.9398146  6.1845684  ... 3.0365293  3.5444417  1.3393478 ]
 [5.613061   6.348512   5.164087   ... 1.829057   7.7712007  1.4071141 ]
 [6.893006   3.0565336  6.057088   ... 2.613742   4.4370584  2.0332472 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_506"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_507 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_46 (LogProbL  (None,)                  908640    
 ayer)                                                           
                                                                 
=================================================================
Total params: 908,640
Trainable params: 908,640
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_46/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_46'")
self.model: <keras.engine.functional.Functional object at 0x7f57895c00d0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f5834577d90>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f5834577d90>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f594f328340>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f594f388130>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_355/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f594f3899c0>, <keras.callbacks.ModelCheckpoint object at 0x7f594f3883d0>, <keras.callbacks.EarlyStopping object at 0x7f594f388b80>, <keras.callbacks.ReduceLROnPlateau object at 0x7f594f3881f0>, <keras.callbacks.TerminateOnNaN object at 0x7f594f3891e0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_355/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 355/720 with hyperparameters:
timestamp = 2023-10-25 14:28:44.325483
ndims = 64
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 908640
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 2.1731846   3.8937397   6.9821544   1.4459838   8.670284    0.996012
  9.747296    5.3488703  10.829596    5.798153    7.4385934  -0.39963475
  2.887594    1.6816981   1.9483483   2.4950166   3.377345    6.4428034
  0.7434967  10.024268    5.9064217   3.9731474   5.977063    2.0677786
  5.421294    7.7411385   2.3877835   6.667169    1.0165741   6.879494
  2.333092    1.6771841   5.2052946  -0.01726444  8.224516   -0.06788193
  7.1896877   2.6926644   7.7422037   9.618431    2.508284    5.3540187
  8.24927     4.96435     2.6443048   8.933116    4.0272393   8.583376
  6.9591885   2.9305875   8.131322    3.9143033   8.096406    5.8134246
  9.931866    7.011071    7.2540216   5.1431737  10.5702715   6.340821
  4.2660613   5.669328    0.68163395  3.5624375 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 14:29:56.688 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6409.8218, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 72s - loss: nan - MinusLogProbMetric: 6409.8218 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 72s/epoch - 369ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 0.0001111111111111111.
===========
Generating train data for run 355.
===========
Train data generated in 0.26 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_355/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_355/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_355/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_355
self.data_kwargs: {'seed': 520}
self.x_data: [[2.1731846  3.8937397  6.9821544  ... 5.669328   0.68163395 3.5624375 ]
 [6.7535415  2.6749182  6.1859965  ... 3.3760262  4.638645   2.336814  ]
 [6.1686287  2.7568936  6.2012696  ... 2.5904071  3.339636   2.9827425 ]
 ...
 [6.6194353  2.9398146  6.1845684  ... 3.0365293  3.5444417  1.3393478 ]
 [5.613061   6.348512   5.164087   ... 1.829057   7.7712007  1.4071141 ]
 [6.893006   3.0565336  6.057088   ... 2.613742   4.4370584  2.0332472 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_512"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_513 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_47 (LogProbL  (None,)                  908640    
 ayer)                                                           
                                                                 
=================================================================
Total params: 908,640
Trainable params: 908,640
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_47/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_47'")
self.model: <keras.engine.functional.Functional object at 0x7f58ddec3dc0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f5fdb576320>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f5fdb576320>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f57a18438e0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f58dde5e560>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_355/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f58dde5ead0>, <keras.callbacks.ModelCheckpoint object at 0x7f58dde5eb90>, <keras.callbacks.EarlyStopping object at 0x7f58dde5ee00>, <keras.callbacks.ReduceLROnPlateau object at 0x7f58dde5ee30>, <keras.callbacks.TerminateOnNaN object at 0x7f58dde5ea70>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_355/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 355/720 with hyperparameters:
timestamp = 2023-10-25 14:30:02.525319
ndims = 64
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 908640
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 2.1731846   3.8937397   6.9821544   1.4459838   8.670284    0.996012
  9.747296    5.3488703  10.829596    5.798153    7.4385934  -0.39963475
  2.887594    1.6816981   1.9483483   2.4950166   3.377345    6.4428034
  0.7434967  10.024268    5.9064217   3.9731474   5.977063    2.0677786
  5.421294    7.7411385   2.3877835   6.667169    1.0165741   6.879494
  2.333092    1.6771841   5.2052946  -0.01726444  8.224516   -0.06788193
  7.1896877   2.6926644   7.7422037   9.618431    2.508284    5.3540187
  8.24927     4.96435     2.6443048   8.933116    4.0272393   8.583376
  6.9591885   2.9305875   8.131322    3.9143033   8.096406    5.8134246
  9.931866    7.011071    7.2540216   5.1431737  10.5702715   6.340821
  4.2660613   5.669328    0.68163395  3.5624375 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 14:31:13.036 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6409.8218, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 70s - loss: nan - MinusLogProbMetric: 6409.8218 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.1111e-04 - 70s/epoch - 360ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 3.703703703703703e-05.
===========
Generating train data for run 355.
===========
Train data generated in 0.25 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_355/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_355/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_355/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_355
self.data_kwargs: {'seed': 520}
self.x_data: [[2.1731846  3.8937397  6.9821544  ... 5.669328   0.68163395 3.5624375 ]
 [6.7535415  2.6749182  6.1859965  ... 3.3760262  4.638645   2.336814  ]
 [6.1686287  2.7568936  6.2012696  ... 2.5904071  3.339636   2.9827425 ]
 ...
 [6.6194353  2.9398146  6.1845684  ... 3.0365293  3.5444417  1.3393478 ]
 [5.613061   6.348512   5.164087   ... 1.829057   7.7712007  1.4071141 ]
 [6.893006   3.0565336  6.057088   ... 2.613742   4.4370584  2.0332472 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_518"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_519 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_48 (LogProbL  (None,)                  908640    
 ayer)                                                           
                                                                 
=================================================================
Total params: 908,640
Trainable params: 908,640
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_48/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_48'")
self.model: <keras.engine.functional.Functional object at 0x7f58991abf10>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f577845ded0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f577845ded0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f589b79eda0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f57a1a102e0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_355/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f57a1a10850>, <keras.callbacks.ModelCheckpoint object at 0x7f57a1a10910>, <keras.callbacks.EarlyStopping object at 0x7f57a1a10b80>, <keras.callbacks.ReduceLROnPlateau object at 0x7f57a1a10bb0>, <keras.callbacks.TerminateOnNaN object at 0x7f57a1a107f0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_355/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 355/720 with hyperparameters:
timestamp = 2023-10-25 14:31:18.583301
ndims = 64
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 908640
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 3.703703703703703e-05...
Train first sample: [ 2.1731846   3.8937397   6.9821544   1.4459838   8.670284    0.996012
  9.747296    5.3488703  10.829596    5.798153    7.4385934  -0.39963475
  2.887594    1.6816981   1.9483483   2.4950166   3.377345    6.4428034
  0.7434967  10.024268    5.9064217   3.9731474   5.977063    2.0677786
  5.421294    7.7411385   2.3877835   6.667169    1.0165741   6.879494
  2.333092    1.6771841   5.2052946  -0.01726444  8.224516   -0.06788193
  7.1896877   2.6926644   7.7422037   9.618431    2.508284    5.3540187
  8.24927     4.96435     2.6443048   8.933116    4.0272393   8.583376
  6.9591885   2.9305875   8.131322    3.9143033   8.096406    5.8134246
  9.931866    7.011071    7.2540216   5.1431737  10.5702715   6.340821
  4.2660613   5.669328    0.68163395  3.5624375 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 14:32:26.505 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6409.8218, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 68s - loss: nan - MinusLogProbMetric: 6409.8218 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.7037e-05 - 68s/epoch - 346ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 1.2345679012345677e-05.
===========
Generating train data for run 355.
===========
Train data generated in 0.27 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_355/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_355/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_355/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_355
self.data_kwargs: {'seed': 520}
self.x_data: [[2.1731846  3.8937397  6.9821544  ... 5.669328   0.68163395 3.5624375 ]
 [6.7535415  2.6749182  6.1859965  ... 3.3760262  4.638645   2.336814  ]
 [6.1686287  2.7568936  6.2012696  ... 2.5904071  3.339636   2.9827425 ]
 ...
 [6.6194353  2.9398146  6.1845684  ... 3.0365293  3.5444417  1.3393478 ]
 [5.613061   6.348512   5.164087   ... 1.829057   7.7712007  1.4071141 ]
 [6.893006   3.0565336  6.057088   ... 2.613742   4.4370584  2.0332472 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_524"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_525 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_49 (LogProbL  (None,)                  908640    
 ayer)                                                           
                                                                 
=================================================================
Total params: 908,640
Trainable params: 908,640
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_49/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_49'")
self.model: <keras.engine.functional.Functional object at 0x7f576d1f5b10>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f576cfbf340>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f576cfbf340>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f552f866dd0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f5761c22470>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_355/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f5761c229e0>, <keras.callbacks.ModelCheckpoint object at 0x7f5761c22aa0>, <keras.callbacks.EarlyStopping object at 0x7f5761c22d10>, <keras.callbacks.ReduceLROnPlateau object at 0x7f5761c22d40>, <keras.callbacks.TerminateOnNaN object at 0x7f5761c22980>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_355/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 355/720 with hyperparameters:
timestamp = 2023-10-25 14:32:32.491285
ndims = 64
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 908640
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.2345679012345677e-05...
Train first sample: [ 2.1731846   3.8937397   6.9821544   1.4459838   8.670284    0.996012
  9.747296    5.3488703  10.829596    5.798153    7.4385934  -0.39963475
  2.887594    1.6816981   1.9483483   2.4950166   3.377345    6.4428034
  0.7434967  10.024268    5.9064217   3.9731474   5.977063    2.0677786
  5.421294    7.7411385   2.3877835   6.667169    1.0165741   6.879494
  2.333092    1.6771841   5.2052946  -0.01726444  8.224516   -0.06788193
  7.1896877   2.6926644   7.7422037   9.618431    2.508284    5.3540187
  8.24927     4.96435     2.6443048   8.933116    4.0272393   8.583376
  6.9591885   2.9305875   8.131322    3.9143033   8.096406    5.8134246
  9.931866    7.011071    7.2540216   5.1431737  10.5702715   6.340821
  4.2660613   5.669328    0.68163395  3.5624375 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 14:33:56.841 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6409.8218, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 84s - loss: nan - MinusLogProbMetric: 6409.8218 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.2346e-05 - 84s/epoch - 430ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 4.115226337448558e-06.
===========
Generating train data for run 355.
===========
Train data generated in 0.20 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_355/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_355/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_355/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_355
self.data_kwargs: {'seed': 520}
self.x_data: [[2.1731846  3.8937397  6.9821544  ... 5.669328   0.68163395 3.5624375 ]
 [6.7535415  2.6749182  6.1859965  ... 3.3760262  4.638645   2.336814  ]
 [6.1686287  2.7568936  6.2012696  ... 2.5904071  3.339636   2.9827425 ]
 ...
 [6.6194353  2.9398146  6.1845684  ... 3.0365293  3.5444417  1.3393478 ]
 [5.613061   6.348512   5.164087   ... 1.829057   7.7712007  1.4071141 ]
 [6.893006   3.0565336  6.057088   ... 2.613742   4.4370584  2.0332472 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_530"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_531 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_50 (LogProbL  (None,)                  908640    
 ayer)                                                           
                                                                 
=================================================================
Total params: 908,640
Trainable params: 908,640
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_50/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_50'")
self.model: <keras.engine.functional.Functional object at 0x7f6026f57f40>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f60265a0b20>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f60265a0b20>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f58ddb81330>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f59703d2560>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_355/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f59703d1f00>, <keras.callbacks.ModelCheckpoint object at 0x7f59703d07c0>, <keras.callbacks.EarlyStopping object at 0x7f59703d18a0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f59703d0070>, <keras.callbacks.TerminateOnNaN object at 0x7f59703d0a00>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_355/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 355/720 with hyperparameters:
timestamp = 2023-10-25 14:34:02.573828
ndims = 64
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 908640
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.115226337448558e-06...
Train first sample: [ 2.1731846   3.8937397   6.9821544   1.4459838   8.670284    0.996012
  9.747296    5.3488703  10.829596    5.798153    7.4385934  -0.39963475
  2.887594    1.6816981   1.9483483   2.4950166   3.377345    6.4428034
  0.7434967  10.024268    5.9064217   3.9731474   5.977063    2.0677786
  5.421294    7.7411385   2.3877835   6.667169    1.0165741   6.879494
  2.333092    1.6771841   5.2052946  -0.01726444  8.224516   -0.06788193
  7.1896877   2.6926644   7.7422037   9.618431    2.508284    5.3540187
  8.24927     4.96435     2.6443048   8.933116    4.0272393   8.583376
  6.9591885   2.9305875   8.131322    3.9143033   8.096406    5.8134246
  9.931866    7.011071    7.2540216   5.1431737  10.5702715   6.340821
  4.2660613   5.669328    0.68163395  3.5624375 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 14:35:12.126 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6409.8218, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 70s - loss: nan - MinusLogProbMetric: 6409.8218 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.1152e-06 - 70s/epoch - 355ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 1.3717421124828526e-06.
===========
Generating train data for run 355.
===========
Train data generated in 0.17 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_355/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_355/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_355/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_355
self.data_kwargs: {'seed': 520}
self.x_data: [[2.1731846  3.8937397  6.9821544  ... 5.669328   0.68163395 3.5624375 ]
 [6.7535415  2.6749182  6.1859965  ... 3.3760262  4.638645   2.336814  ]
 [6.1686287  2.7568936  6.2012696  ... 2.5904071  3.339636   2.9827425 ]
 ...
 [6.6194353  2.9398146  6.1845684  ... 3.0365293  3.5444417  1.3393478 ]
 [5.613061   6.348512   5.164087   ... 1.829057   7.7712007  1.4071141 ]
 [6.893006   3.0565336  6.057088   ... 2.613742   4.4370584  2.0332472 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_536"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_537 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_51 (LogProbL  (None,)                  908640    
 ayer)                                                           
                                                                 
=================================================================
Total params: 908,640
Trainable params: 908,640
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_51/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_51'")
self.model: <keras.engine.functional.Functional object at 0x7f5af0503250>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f57a8110160>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f57a8110160>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f57a04eada0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f57bc912350>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_355/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f57bc9128c0>, <keras.callbacks.ModelCheckpoint object at 0x7f57bc912980>, <keras.callbacks.EarlyStopping object at 0x7f57bc912bf0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f57bc912c20>, <keras.callbacks.TerminateOnNaN object at 0x7f57bc912860>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_355/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 355/720 with hyperparameters:
timestamp = 2023-10-25 14:35:17.614322
ndims = 64
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 908640
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.3717421124828526e-06...
Train first sample: [ 2.1731846   3.8937397   6.9821544   1.4459838   8.670284    0.996012
  9.747296    5.3488703  10.829596    5.798153    7.4385934  -0.39963475
  2.887594    1.6816981   1.9483483   2.4950166   3.377345    6.4428034
  0.7434967  10.024268    5.9064217   3.9731474   5.977063    2.0677786
  5.421294    7.7411385   2.3877835   6.667169    1.0165741   6.879494
  2.333092    1.6771841   5.2052946  -0.01726444  8.224516   -0.06788193
  7.1896877   2.6926644   7.7422037   9.618431    2.508284    5.3540187
  8.24927     4.96435     2.6443048   8.933116    4.0272393   8.583376
  6.9591885   2.9305875   8.131322    3.9143033   8.096406    5.8134246
  9.931866    7.011071    7.2540216   5.1431737  10.5702715   6.340821
  4.2660613   5.669328    0.68163395  3.5624375 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 14:36:26.801 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6409.8218, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 69s - loss: nan - MinusLogProbMetric: 6409.8218 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.3717e-06 - 69s/epoch - 353ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 4.572473708276175e-07.
===========
Generating train data for run 355.
===========
Train data generated in 0.23 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_355/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_355/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_355/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_355
self.data_kwargs: {'seed': 520}
self.x_data: [[2.1731846  3.8937397  6.9821544  ... 5.669328   0.68163395 3.5624375 ]
 [6.7535415  2.6749182  6.1859965  ... 3.3760262  4.638645   2.336814  ]
 [6.1686287  2.7568936  6.2012696  ... 2.5904071  3.339636   2.9827425 ]
 ...
 [6.6194353  2.9398146  6.1845684  ... 3.0365293  3.5444417  1.3393478 ]
 [5.613061   6.348512   5.164087   ... 1.829057   7.7712007  1.4071141 ]
 [6.893006   3.0565336  6.057088   ... 2.613742   4.4370584  2.0332472 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_542"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_543 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_52 (LogProbL  (None,)                  908640    
 ayer)                                                           
                                                                 
=================================================================
Total params: 908,640
Trainable params: 908,640
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_52/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_52'")
self.model: <keras.engine.functional.Functional object at 0x7f554d356c20>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f554ca77f70>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f554ca77f70>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f554d1e0580>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f5545874340>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_355/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f55458748b0>, <keras.callbacks.ModelCheckpoint object at 0x7f5545874970>, <keras.callbacks.EarlyStopping object at 0x7f5545874be0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f5545874c10>, <keras.callbacks.TerminateOnNaN object at 0x7f5545874850>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_355/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 355/720 with hyperparameters:
timestamp = 2023-10-25 14:36:32.034831
ndims = 64
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 908640
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.572473708276175e-07...
Train first sample: [ 2.1731846   3.8937397   6.9821544   1.4459838   8.670284    0.996012
  9.747296    5.3488703  10.829596    5.798153    7.4385934  -0.39963475
  2.887594    1.6816981   1.9483483   2.4950166   3.377345    6.4428034
  0.7434967  10.024268    5.9064217   3.9731474   5.977063    2.0677786
  5.421294    7.7411385   2.3877835   6.667169    1.0165741   6.879494
  2.333092    1.6771841   5.2052946  -0.01726444  8.224516   -0.06788193
  7.1896877   2.6926644   7.7422037   9.618431    2.508284    5.3540187
  8.24927     4.96435     2.6443048   8.933116    4.0272393   8.583376
  6.9591885   2.9305875   8.131322    3.9143033   8.096406    5.8134246
  9.931866    7.011071    7.2540216   5.1431737  10.5702715   6.340821
  4.2660613   5.669328    0.68163395  3.5624375 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 14:37:52.389 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6409.8218, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 80s - loss: nan - MinusLogProbMetric: 6409.8218 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.5725e-07 - 80s/epoch - 410ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 1.524157902758725e-07.
===========
Generating train data for run 355.
===========
Train data generated in 0.19 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_355/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_355/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_355/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_355
self.data_kwargs: {'seed': 520}
self.x_data: [[2.1731846  3.8937397  6.9821544  ... 5.669328   0.68163395 3.5624375 ]
 [6.7535415  2.6749182  6.1859965  ... 3.3760262  4.638645   2.336814  ]
 [6.1686287  2.7568936  6.2012696  ... 2.5904071  3.339636   2.9827425 ]
 ...
 [6.6194353  2.9398146  6.1845684  ... 3.0365293  3.5444417  1.3393478 ]
 [5.613061   6.348512   5.164087   ... 1.829057   7.7712007  1.4071141 ]
 [6.893006   3.0565336  6.057088   ... 2.613742   4.4370584  2.0332472 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_548"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_549 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_53 (LogProbL  (None,)                  908640    
 ayer)                                                           
                                                                 
=================================================================
Total params: 908,640
Trainable params: 908,640
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_53/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_53'")
self.model: <keras.engine.functional.Functional object at 0x7f576d4cd0c0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f583775afe0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f583775afe0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f5791af05b0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f5761cca8f0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_355/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f5761cca7a0>, <keras.callbacks.ModelCheckpoint object at 0x7f5761cc9c30>, <keras.callbacks.EarlyStopping object at 0x7f5761cc9ab0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f5761ccab00>, <keras.callbacks.TerminateOnNaN object at 0x7f5761cca320>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_355/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 355/720 with hyperparameters:
timestamp = 2023-10-25 14:37:56.643164
ndims = 64
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 908640
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.524157902758725e-07...
Train first sample: [ 2.1731846   3.8937397   6.9821544   1.4459838   8.670284    0.996012
  9.747296    5.3488703  10.829596    5.798153    7.4385934  -0.39963475
  2.887594    1.6816981   1.9483483   2.4950166   3.377345    6.4428034
  0.7434967  10.024268    5.9064217   3.9731474   5.977063    2.0677786
  5.421294    7.7411385   2.3877835   6.667169    1.0165741   6.879494
  2.333092    1.6771841   5.2052946  -0.01726444  8.224516   -0.06788193
  7.1896877   2.6926644   7.7422037   9.618431    2.508284    5.3540187
  8.24927     4.96435     2.6443048   8.933116    4.0272393   8.583376
  6.9591885   2.9305875   8.131322    3.9143033   8.096406    5.8134246
  9.931866    7.011071    7.2540216   5.1431737  10.5702715   6.340821
  4.2660613   5.669328    0.68163395  3.5624375 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 14:39:04.751 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6409.8218, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 68s - loss: nan - MinusLogProbMetric: 6409.8218 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.5242e-07 - 68s/epoch - 347ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 5.0805263425290834e-08.
===========
Generating train data for run 355.
===========
Train data generated in 0.22 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_355/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_355/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_355/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_355
self.data_kwargs: {'seed': 520}
self.x_data: [[2.1731846  3.8937397  6.9821544  ... 5.669328   0.68163395 3.5624375 ]
 [6.7535415  2.6749182  6.1859965  ... 3.3760262  4.638645   2.336814  ]
 [6.1686287  2.7568936  6.2012696  ... 2.5904071  3.339636   2.9827425 ]
 ...
 [6.6194353  2.9398146  6.1845684  ... 3.0365293  3.5444417  1.3393478 ]
 [5.613061   6.348512   5.164087   ... 1.829057   7.7712007  1.4071141 ]
 [6.893006   3.0565336  6.057088   ... 2.613742   4.4370584  2.0332472 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_554"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_555 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_54 (LogProbL  (None,)                  908640    
 ayer)                                                           
                                                                 
=================================================================
Total params: 908,640
Trainable params: 908,640
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_54/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_54'")
self.model: <keras.engine.functional.Functional object at 0x7f59268a5d20>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f58997ef850>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f58997ef850>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f576d3c2a10>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f5926834dc0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_355/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f5926835330>, <keras.callbacks.ModelCheckpoint object at 0x7f59268353f0>, <keras.callbacks.EarlyStopping object at 0x7f5926835660>, <keras.callbacks.ReduceLROnPlateau object at 0x7f5926835690>, <keras.callbacks.TerminateOnNaN object at 0x7f59268352d0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_355/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 355/720 with hyperparameters:
timestamp = 2023-10-25 14:39:10.072601
ndims = 64
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 908640
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 5.0805263425290834e-08...
Train first sample: [ 2.1731846   3.8937397   6.9821544   1.4459838   8.670284    0.996012
  9.747296    5.3488703  10.829596    5.798153    7.4385934  -0.39963475
  2.887594    1.6816981   1.9483483   2.4950166   3.377345    6.4428034
  0.7434967  10.024268    5.9064217   3.9731474   5.977063    2.0677786
  5.421294    7.7411385   2.3877835   6.667169    1.0165741   6.879494
  2.333092    1.6771841   5.2052946  -0.01726444  8.224516   -0.06788193
  7.1896877   2.6926644   7.7422037   9.618431    2.508284    5.3540187
  8.24927     4.96435     2.6443048   8.933116    4.0272393   8.583376
  6.9591885   2.9305875   8.131322    3.9143033   8.096406    5.8134246
  9.931866    7.011071    7.2540216   5.1431737  10.5702715   6.340821
  4.2660613   5.669328    0.68163395  3.5624375 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 14:40:15.554 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6409.8218, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 65s - loss: nan - MinusLogProbMetric: 6409.8218 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 5.0805e-08 - 65s/epoch - 334ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 1.6935087808430278e-08.
===========
Generating train data for run 355.
===========
Train data generated in 0.23 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_355/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_355/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_355/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_355
self.data_kwargs: {'seed': 520}
self.x_data: [[2.1731846  3.8937397  6.9821544  ... 5.669328   0.68163395 3.5624375 ]
 [6.7535415  2.6749182  6.1859965  ... 3.3760262  4.638645   2.336814  ]
 [6.1686287  2.7568936  6.2012696  ... 2.5904071  3.339636   2.9827425 ]
 ...
 [6.6194353  2.9398146  6.1845684  ... 3.0365293  3.5444417  1.3393478 ]
 [5.613061   6.348512   5.164087   ... 1.829057   7.7712007  1.4071141 ]
 [6.893006   3.0565336  6.057088   ... 2.613742   4.4370584  2.0332472 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_560"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_561 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_55 (LogProbL  (None,)                  908640    
 ayer)                                                           
                                                                 
=================================================================
Total params: 908,640
Trainable params: 908,640
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_55/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_55'")
self.model: <keras.engine.functional.Functional object at 0x7f553d93f850>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f552c317ee0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f552c317ee0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f554433f970>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f5761bce2c0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_355/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f5761bce830>, <keras.callbacks.ModelCheckpoint object at 0x7f5761bce8f0>, <keras.callbacks.EarlyStopping object at 0x7f5761bceb60>, <keras.callbacks.ReduceLROnPlateau object at 0x7f5761bceb90>, <keras.callbacks.TerminateOnNaN object at 0x7f5761bce7d0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_355/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 355/720 with hyperparameters:
timestamp = 2023-10-25 14:40:21.096861
ndims = 64
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 908640
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.6935087808430278e-08...
Train first sample: [ 2.1731846   3.8937397   6.9821544   1.4459838   8.670284    0.996012
  9.747296    5.3488703  10.829596    5.798153    7.4385934  -0.39963475
  2.887594    1.6816981   1.9483483   2.4950166   3.377345    6.4428034
  0.7434967  10.024268    5.9064217   3.9731474   5.977063    2.0677786
  5.421294    7.7411385   2.3877835   6.667169    1.0165741   6.879494
  2.333092    1.6771841   5.2052946  -0.01726444  8.224516   -0.06788193
  7.1896877   2.6926644   7.7422037   9.618431    2.508284    5.3540187
  8.24927     4.96435     2.6443048   8.933116    4.0272393   8.583376
  6.9591885   2.9305875   8.131322    3.9143033   8.096406    5.8134246
  9.931866    7.011071    7.2540216   5.1431737  10.5702715   6.340821
  4.2660613   5.669328    0.68163395  3.5624375 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 14:41:44.562 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6409.8218, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 83s - loss: nan - MinusLogProbMetric: 6409.8218 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.6935e-08 - 83s/epoch - 425ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 5.645029269476759e-09.
===========
Run 355/720 failed.
Exception type: Exception
Exception message: Training failed for the maximum number of retry.
Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 649, Func.Name : <module>, Message : hyperparams_dict, NFObject, seed_train, training_time = train_function(seeds = [seed_train, seed_test, seed_dist, seed_metrics],', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 322, Func.Name : train_function, Message : raise Exception("Training failed for the maximum number of retry.")']
===========

===========
Generating train data for run 356.
===========
Train data generated in 0.29 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_356/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_356/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_356/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_356
self.data_kwargs: {'seed': 520}
self.x_data: [[2.1731846  3.8937397  6.9821544  ... 5.669328   0.68163395 3.5624375 ]
 [6.7535415  2.6749182  6.1859965  ... 3.3760262  4.638645   2.336814  ]
 [6.1686287  2.7568936  6.2012696  ... 2.5904071  3.339636   2.9827425 ]
 ...
 [6.6194353  2.9398146  6.1845684  ... 3.0365293  3.5444417  1.3393478 ]
 [5.613061   6.348512   5.164087   ... 1.829057   7.7712007  1.4071141 ]
 [6.893006   3.0565336  6.057088   ... 2.613742   4.4370584  2.0332472 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_566"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_567 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_56 (LogProbL  (None,)                  2139360   
 ayer)                                                           
                                                                 
=================================================================
Total params: 2,139,360
Trainable params: 2,139,360
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_56/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_56'")
self.model: <keras.engine.functional.Functional object at 0x7f5b5c6e25f0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f5a005a0dc0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f5a005a0dc0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f5fdb7642e0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f589b949b10>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_356/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f593c8cb2e0>, <keras.callbacks.ModelCheckpoint object at 0x7f593c8cbc10>, <keras.callbacks.EarlyStopping object at 0x7f593c8cab00>, <keras.callbacks.ReduceLROnPlateau object at 0x7f593c8cac50>, <keras.callbacks.TerminateOnNaN object at 0x7f593c8ca9b0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_356/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 356/720 with hyperparameters:
timestamp = 2023-10-25 14:41:50.921908
ndims = 64
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 2139360
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 2.1731846   3.8937397   6.9821544   1.4459838   8.670284    0.996012
  9.747296    5.3488703  10.829596    5.798153    7.4385934  -0.39963475
  2.887594    1.6816981   1.9483483   2.4950166   3.377345    6.4428034
  0.7434967  10.024268    5.9064217   3.9731474   5.977063    2.0677786
  5.421294    7.7411385   2.3877835   6.667169    1.0165741   6.879494
  2.333092    1.6771841   5.2052946  -0.01726444  8.224516   -0.06788193
  7.1896877   2.6926644   7.7422037   9.618431    2.508284    5.3540187
  8.24927     4.96435     2.6443048   8.933116    4.0272393   8.583376
  6.9591885   2.9305875   8.131322    3.9143033   8.096406    5.8134246
  9.931866    7.011071    7.2540216   5.1431737  10.5702715   6.340821
  4.2660613   5.669328    0.68163395  3.5624375 ]
Epoch 1/1000
2023-10-25 14:43:38.151 
Epoch 1/1000 
	 loss: 656.5735, MinusLogProbMetric: 656.5735, val_loss: 144.3281, val_MinusLogProbMetric: 144.3281

Epoch 1: val_loss improved from inf to 144.32806, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_356/weights/best_weights.h5
196/196 - 107s - loss: 656.5735 - MinusLogProbMetric: 656.5735 - val_loss: 144.3281 - val_MinusLogProbMetric: 144.3281 - lr: 0.0010 - 107s/epoch - 548ms/step
Epoch 2/1000
2023-10-25 14:44:16.903 
Epoch 2/1000 
	 loss: 121.7800, MinusLogProbMetric: 121.7800, val_loss: 96.0312, val_MinusLogProbMetric: 96.0312

Epoch 2: val_loss improved from 144.32806 to 96.03117, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_356/weights/best_weights.h5
196/196 - 39s - loss: 121.7800 - MinusLogProbMetric: 121.7800 - val_loss: 96.0312 - val_MinusLogProbMetric: 96.0312 - lr: 0.0010 - 39s/epoch - 198ms/step
Epoch 3/1000
2023-10-25 14:44:57.637 
Epoch 3/1000 
	 loss: 76.1174, MinusLogProbMetric: 76.1174, val_loss: 65.4565, val_MinusLogProbMetric: 65.4565

Epoch 3: val_loss improved from 96.03117 to 65.45654, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_356/weights/best_weights.h5
196/196 - 41s - loss: 76.1174 - MinusLogProbMetric: 76.1174 - val_loss: 65.4565 - val_MinusLogProbMetric: 65.4565 - lr: 0.0010 - 41s/epoch - 207ms/step
Epoch 4/1000
2023-10-25 14:45:36.946 
Epoch 4/1000 
	 loss: 59.6631, MinusLogProbMetric: 59.6631, val_loss: 54.3370, val_MinusLogProbMetric: 54.3370

Epoch 4: val_loss improved from 65.45654 to 54.33698, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_356/weights/best_weights.h5
196/196 - 39s - loss: 59.6631 - MinusLogProbMetric: 59.6631 - val_loss: 54.3370 - val_MinusLogProbMetric: 54.3370 - lr: 0.0010 - 39s/epoch - 201ms/step
Epoch 5/1000
2023-10-25 14:46:17.802 
Epoch 5/1000 
	 loss: 52.0257, MinusLogProbMetric: 52.0257, val_loss: 50.4510, val_MinusLogProbMetric: 50.4510

Epoch 5: val_loss improved from 54.33698 to 50.45105, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_356/weights/best_weights.h5
196/196 - 41s - loss: 52.0257 - MinusLogProbMetric: 52.0257 - val_loss: 50.4510 - val_MinusLogProbMetric: 50.4510 - lr: 0.0010 - 41s/epoch - 209ms/step
Epoch 6/1000
2023-10-25 14:46:59.935 
Epoch 6/1000 
	 loss: 48.4637, MinusLogProbMetric: 48.4637, val_loss: 46.6310, val_MinusLogProbMetric: 46.6310

Epoch 6: val_loss improved from 50.45105 to 46.63103, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_356/weights/best_weights.h5
196/196 - 42s - loss: 48.4637 - MinusLogProbMetric: 48.4637 - val_loss: 46.6310 - val_MinusLogProbMetric: 46.6310 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 7/1000
2023-10-25 14:47:38.922 
Epoch 7/1000 
	 loss: 45.6456, MinusLogProbMetric: 45.6456, val_loss: 45.1780, val_MinusLogProbMetric: 45.1780

Epoch 7: val_loss improved from 46.63103 to 45.17800, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_356/weights/best_weights.h5
196/196 - 39s - loss: 45.6456 - MinusLogProbMetric: 45.6456 - val_loss: 45.1780 - val_MinusLogProbMetric: 45.1780 - lr: 0.0010 - 39s/epoch - 198ms/step
Epoch 8/1000
2023-10-25 14:48:19.007 
Epoch 8/1000 
	 loss: 46.8884, MinusLogProbMetric: 46.8884, val_loss: 42.4061, val_MinusLogProbMetric: 42.4061

Epoch 8: val_loss improved from 45.17800 to 42.40611, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_356/weights/best_weights.h5
196/196 - 40s - loss: 46.8884 - MinusLogProbMetric: 46.8884 - val_loss: 42.4061 - val_MinusLogProbMetric: 42.4061 - lr: 0.0010 - 40s/epoch - 205ms/step
Epoch 9/1000
2023-10-25 14:48:58.674 
Epoch 9/1000 
	 loss: 42.9589, MinusLogProbMetric: 42.9589, val_loss: 41.5466, val_MinusLogProbMetric: 41.5466

Epoch 9: val_loss improved from 42.40611 to 41.54664, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_356/weights/best_weights.h5
196/196 - 40s - loss: 42.9589 - MinusLogProbMetric: 42.9589 - val_loss: 41.5466 - val_MinusLogProbMetric: 41.5466 - lr: 0.0010 - 40s/epoch - 202ms/step
Epoch 10/1000
2023-10-25 14:49:37.336 
Epoch 10/1000 
	 loss: 41.4188, MinusLogProbMetric: 41.4188, val_loss: 44.6297, val_MinusLogProbMetric: 44.6297

Epoch 10: val_loss did not improve from 41.54664
196/196 - 38s - loss: 41.4188 - MinusLogProbMetric: 41.4188 - val_loss: 44.6297 - val_MinusLogProbMetric: 44.6297 - lr: 0.0010 - 38s/epoch - 194ms/step
Epoch 11/1000
2023-10-25 14:50:10.439 
Epoch 11/1000 
	 loss: 41.0730, MinusLogProbMetric: 41.0730, val_loss: 41.5611, val_MinusLogProbMetric: 41.5611

Epoch 11: val_loss did not improve from 41.54664
196/196 - 33s - loss: 41.0730 - MinusLogProbMetric: 41.0730 - val_loss: 41.5611 - val_MinusLogProbMetric: 41.5611 - lr: 0.0010 - 33s/epoch - 169ms/step
Epoch 12/1000
2023-10-25 14:50:42.405 
Epoch 12/1000 
	 loss: 39.9203, MinusLogProbMetric: 39.9203, val_loss: 38.8989, val_MinusLogProbMetric: 38.8989

Epoch 12: val_loss improved from 41.54664 to 38.89892, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_356/weights/best_weights.h5
196/196 - 32s - loss: 39.9203 - MinusLogProbMetric: 39.9203 - val_loss: 38.8989 - val_MinusLogProbMetric: 38.8989 - lr: 0.0010 - 32s/epoch - 166ms/step
Epoch 13/1000
2023-10-25 14:51:15.025 
Epoch 13/1000 
	 loss: 39.9617, MinusLogProbMetric: 39.9617, val_loss: 39.1207, val_MinusLogProbMetric: 39.1207

Epoch 13: val_loss did not improve from 38.89892
196/196 - 32s - loss: 39.9617 - MinusLogProbMetric: 39.9617 - val_loss: 39.1207 - val_MinusLogProbMetric: 39.1207 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 14/1000
2023-10-25 14:51:50.135 
Epoch 14/1000 
	 loss: 38.4676, MinusLogProbMetric: 38.4676, val_loss: 39.9829, val_MinusLogProbMetric: 39.9829

Epoch 14: val_loss did not improve from 38.89892
196/196 - 35s - loss: 38.4676 - MinusLogProbMetric: 38.4676 - val_loss: 39.9829 - val_MinusLogProbMetric: 39.9829 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 15/1000
2023-10-25 14:52:28.817 
Epoch 15/1000 
	 loss: 38.1137, MinusLogProbMetric: 38.1137, val_loss: 37.5272, val_MinusLogProbMetric: 37.5272

Epoch 15: val_loss improved from 38.89892 to 37.52724, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_356/weights/best_weights.h5
196/196 - 39s - loss: 38.1137 - MinusLogProbMetric: 38.1137 - val_loss: 37.5272 - val_MinusLogProbMetric: 37.5272 - lr: 0.0010 - 39s/epoch - 201ms/step
Epoch 16/1000
2023-10-25 14:53:11.762 
Epoch 16/1000 
	 loss: 37.8556, MinusLogProbMetric: 37.8556, val_loss: 41.0887, val_MinusLogProbMetric: 41.0887

Epoch 16: val_loss did not improve from 37.52724
196/196 - 42s - loss: 37.8556 - MinusLogProbMetric: 37.8556 - val_loss: 41.0887 - val_MinusLogProbMetric: 41.0887 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 17/1000
2023-10-25 14:53:50.121 
Epoch 17/1000 
	 loss: 37.1987, MinusLogProbMetric: 37.1987, val_loss: 35.9584, val_MinusLogProbMetric: 35.9584

Epoch 17: val_loss improved from 37.52724 to 35.95835, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_356/weights/best_weights.h5
196/196 - 39s - loss: 37.1987 - MinusLogProbMetric: 37.1987 - val_loss: 35.9584 - val_MinusLogProbMetric: 35.9584 - lr: 0.0010 - 39s/epoch - 199ms/step
Epoch 18/1000
2023-10-25 14:54:29.653 
Epoch 18/1000 
	 loss: 36.5252, MinusLogProbMetric: 36.5252, val_loss: 37.6581, val_MinusLogProbMetric: 37.6581

Epoch 18: val_loss did not improve from 35.95835
196/196 - 39s - loss: 36.5252 - MinusLogProbMetric: 36.5252 - val_loss: 37.6581 - val_MinusLogProbMetric: 37.6581 - lr: 0.0010 - 39s/epoch - 198ms/step
Epoch 19/1000
2023-10-25 14:55:07.695 
Epoch 19/1000 
	 loss: 36.8086, MinusLogProbMetric: 36.8086, val_loss: 37.2563, val_MinusLogProbMetric: 37.2563

Epoch 19: val_loss did not improve from 35.95835
196/196 - 38s - loss: 36.8086 - MinusLogProbMetric: 36.8086 - val_loss: 37.2563 - val_MinusLogProbMetric: 37.2563 - lr: 0.0010 - 38s/epoch - 194ms/step
Epoch 20/1000
2023-10-25 14:55:46.379 
Epoch 20/1000 
	 loss: 36.4213, MinusLogProbMetric: 36.4213, val_loss: 36.0921, val_MinusLogProbMetric: 36.0921

Epoch 20: val_loss did not improve from 35.95835
196/196 - 39s - loss: 36.4213 - MinusLogProbMetric: 36.4213 - val_loss: 36.0921 - val_MinusLogProbMetric: 36.0921 - lr: 0.0010 - 39s/epoch - 197ms/step
Epoch 21/1000
2023-10-25 14:56:24.161 
Epoch 21/1000 
	 loss: 36.2175, MinusLogProbMetric: 36.2175, val_loss: 36.2546, val_MinusLogProbMetric: 36.2546

Epoch 21: val_loss did not improve from 35.95835
196/196 - 38s - loss: 36.2175 - MinusLogProbMetric: 36.2175 - val_loss: 36.2546 - val_MinusLogProbMetric: 36.2546 - lr: 0.0010 - 38s/epoch - 193ms/step
Epoch 22/1000
2023-10-25 14:57:04.388 
Epoch 22/1000 
	 loss: 35.6840, MinusLogProbMetric: 35.6840, val_loss: 38.1081, val_MinusLogProbMetric: 38.1081

Epoch 22: val_loss did not improve from 35.95835
196/196 - 40s - loss: 35.6840 - MinusLogProbMetric: 35.6840 - val_loss: 38.1081 - val_MinusLogProbMetric: 38.1081 - lr: 0.0010 - 40s/epoch - 205ms/step
Epoch 23/1000
2023-10-25 14:57:42.698 
Epoch 23/1000 
	 loss: 35.4646, MinusLogProbMetric: 35.4646, val_loss: 36.8357, val_MinusLogProbMetric: 36.8357

Epoch 23: val_loss did not improve from 35.95835
196/196 - 38s - loss: 35.4646 - MinusLogProbMetric: 35.4646 - val_loss: 36.8357 - val_MinusLogProbMetric: 36.8357 - lr: 0.0010 - 38s/epoch - 195ms/step
Epoch 24/1000
2023-10-25 14:58:22.619 
Epoch 24/1000 
	 loss: 35.7435, MinusLogProbMetric: 35.7435, val_loss: 36.2921, val_MinusLogProbMetric: 36.2921

Epoch 24: val_loss did not improve from 35.95835
196/196 - 40s - loss: 35.7435 - MinusLogProbMetric: 35.7435 - val_loss: 36.2921 - val_MinusLogProbMetric: 36.2921 - lr: 0.0010 - 40s/epoch - 204ms/step
Epoch 25/1000
2023-10-25 14:59:02.104 
Epoch 25/1000 
	 loss: 35.1486, MinusLogProbMetric: 35.1486, val_loss: 34.4045, val_MinusLogProbMetric: 34.4045

Epoch 25: val_loss improved from 35.95835 to 34.40447, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_356/weights/best_weights.h5
196/196 - 40s - loss: 35.1486 - MinusLogProbMetric: 35.1486 - val_loss: 34.4045 - val_MinusLogProbMetric: 34.4045 - lr: 0.0010 - 40s/epoch - 205ms/step
Epoch 26/1000
2023-10-25 14:59:40.838 
Epoch 26/1000 
	 loss: 34.9656, MinusLogProbMetric: 34.9656, val_loss: 35.5392, val_MinusLogProbMetric: 35.5392

Epoch 26: val_loss did not improve from 34.40447
196/196 - 38s - loss: 34.9656 - MinusLogProbMetric: 34.9656 - val_loss: 35.5392 - val_MinusLogProbMetric: 35.5392 - lr: 0.0010 - 38s/epoch - 194ms/step
Epoch 27/1000
2023-10-25 15:00:18.523 
Epoch 27/1000 
	 loss: 34.4644, MinusLogProbMetric: 34.4644, val_loss: 36.8126, val_MinusLogProbMetric: 36.8126

Epoch 27: val_loss did not improve from 34.40447
196/196 - 38s - loss: 34.4644 - MinusLogProbMetric: 34.4644 - val_loss: 36.8126 - val_MinusLogProbMetric: 36.8126 - lr: 0.0010 - 38s/epoch - 192ms/step
Epoch 28/1000
2023-10-25 15:00:59.723 
Epoch 28/1000 
	 loss: 34.7648, MinusLogProbMetric: 34.7648, val_loss: 33.7368, val_MinusLogProbMetric: 33.7368

Epoch 28: val_loss improved from 34.40447 to 33.73677, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_356/weights/best_weights.h5
196/196 - 42s - loss: 34.7648 - MinusLogProbMetric: 34.7648 - val_loss: 33.7368 - val_MinusLogProbMetric: 33.7368 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 29/1000
2023-10-25 15:01:39.689 
Epoch 29/1000 
	 loss: 34.6843, MinusLogProbMetric: 34.6843, val_loss: 36.4088, val_MinusLogProbMetric: 36.4088

Epoch 29: val_loss did not improve from 33.73677
196/196 - 39s - loss: 34.6843 - MinusLogProbMetric: 34.6843 - val_loss: 36.4088 - val_MinusLogProbMetric: 36.4088 - lr: 0.0010 - 39s/epoch - 200ms/step
Epoch 30/1000
2023-10-25 15:02:21.175 
Epoch 30/1000 
	 loss: 34.0526, MinusLogProbMetric: 34.0526, val_loss: 33.4348, val_MinusLogProbMetric: 33.4348

Epoch 30: val_loss improved from 33.73677 to 33.43481, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_356/weights/best_weights.h5
196/196 - 42s - loss: 34.0526 - MinusLogProbMetric: 34.0526 - val_loss: 33.4348 - val_MinusLogProbMetric: 33.4348 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 31/1000
2023-10-25 15:02:59.003 
Epoch 31/1000 
	 loss: 35.2476, MinusLogProbMetric: 35.2476, val_loss: 33.8795, val_MinusLogProbMetric: 33.8795

Epoch 31: val_loss did not improve from 33.43481
196/196 - 37s - loss: 35.2476 - MinusLogProbMetric: 35.2476 - val_loss: 33.8795 - val_MinusLogProbMetric: 33.8795 - lr: 0.0010 - 37s/epoch - 190ms/step
Epoch 32/1000
2023-10-25 15:03:37.892 
Epoch 32/1000 
	 loss: 33.6717, MinusLogProbMetric: 33.6717, val_loss: 32.5350, val_MinusLogProbMetric: 32.5350

Epoch 32: val_loss improved from 33.43481 to 32.53496, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_356/weights/best_weights.h5
196/196 - 40s - loss: 33.6717 - MinusLogProbMetric: 33.6717 - val_loss: 32.5350 - val_MinusLogProbMetric: 32.5350 - lr: 0.0010 - 40s/epoch - 202ms/step
Epoch 33/1000
2023-10-25 15:04:16.732 
Epoch 33/1000 
	 loss: 33.8787, MinusLogProbMetric: 33.8787, val_loss: 33.4227, val_MinusLogProbMetric: 33.4227

Epoch 33: val_loss did not improve from 32.53496
196/196 - 38s - loss: 33.8787 - MinusLogProbMetric: 33.8787 - val_loss: 33.4227 - val_MinusLogProbMetric: 33.4227 - lr: 0.0010 - 38s/epoch - 194ms/step
Epoch 34/1000
2023-10-25 15:04:58.467 
Epoch 34/1000 
	 loss: 33.6035, MinusLogProbMetric: 33.6035, val_loss: 34.5962, val_MinusLogProbMetric: 34.5962

Epoch 34: val_loss did not improve from 32.53496
196/196 - 42s - loss: 33.6035 - MinusLogProbMetric: 33.6035 - val_loss: 34.5962 - val_MinusLogProbMetric: 34.5962 - lr: 0.0010 - 42s/epoch - 213ms/step
Epoch 35/1000
2023-10-25 15:05:38.927 
Epoch 35/1000 
	 loss: 33.4521, MinusLogProbMetric: 33.4521, val_loss: 34.4696, val_MinusLogProbMetric: 34.4696

Epoch 35: val_loss did not improve from 32.53496
196/196 - 40s - loss: 33.4521 - MinusLogProbMetric: 33.4521 - val_loss: 34.4696 - val_MinusLogProbMetric: 34.4696 - lr: 0.0010 - 40s/epoch - 206ms/step
Epoch 36/1000
2023-10-25 15:06:17.905 
Epoch 36/1000 
	 loss: 33.2126, MinusLogProbMetric: 33.2126, val_loss: 33.0286, val_MinusLogProbMetric: 33.0286

Epoch 36: val_loss did not improve from 32.53496
196/196 - 39s - loss: 33.2126 - MinusLogProbMetric: 33.2126 - val_loss: 33.0286 - val_MinusLogProbMetric: 33.0286 - lr: 0.0010 - 39s/epoch - 199ms/step
Epoch 37/1000
2023-10-25 15:06:55.930 
Epoch 37/1000 
	 loss: 33.4729, MinusLogProbMetric: 33.4729, val_loss: 33.6843, val_MinusLogProbMetric: 33.6843

Epoch 37: val_loss did not improve from 32.53496
196/196 - 38s - loss: 33.4729 - MinusLogProbMetric: 33.4729 - val_loss: 33.6843 - val_MinusLogProbMetric: 33.6843 - lr: 0.0010 - 38s/epoch - 194ms/step
Epoch 38/1000
2023-10-25 15:07:36.054 
Epoch 38/1000 
	 loss: 33.0274, MinusLogProbMetric: 33.0274, val_loss: 33.6704, val_MinusLogProbMetric: 33.6704

Epoch 38: val_loss did not improve from 32.53496
196/196 - 40s - loss: 33.0274 - MinusLogProbMetric: 33.0274 - val_loss: 33.6704 - val_MinusLogProbMetric: 33.6704 - lr: 0.0010 - 40s/epoch - 205ms/step
Epoch 39/1000
2023-10-25 15:08:14.685 
Epoch 39/1000 
	 loss: 33.2147, MinusLogProbMetric: 33.2147, val_loss: 32.2332, val_MinusLogProbMetric: 32.2332

Epoch 39: val_loss improved from 32.53496 to 32.23315, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_356/weights/best_weights.h5
196/196 - 39s - loss: 33.2147 - MinusLogProbMetric: 33.2147 - val_loss: 32.2332 - val_MinusLogProbMetric: 32.2332 - lr: 0.0010 - 39s/epoch - 200ms/step
Epoch 40/1000
2023-10-25 15:08:55.451 
Epoch 40/1000 
	 loss: 33.0521, MinusLogProbMetric: 33.0521, val_loss: 33.3604, val_MinusLogProbMetric: 33.3604

Epoch 40: val_loss did not improve from 32.23315
196/196 - 40s - loss: 33.0521 - MinusLogProbMetric: 33.0521 - val_loss: 33.3604 - val_MinusLogProbMetric: 33.3604 - lr: 0.0010 - 40s/epoch - 205ms/step
Epoch 41/1000
2023-10-25 15:09:34.005 
Epoch 41/1000 
	 loss: 33.2415, MinusLogProbMetric: 33.2415, val_loss: 38.9899, val_MinusLogProbMetric: 38.9899

Epoch 41: val_loss did not improve from 32.23315
196/196 - 39s - loss: 33.2415 - MinusLogProbMetric: 33.2415 - val_loss: 38.9899 - val_MinusLogProbMetric: 38.9899 - lr: 0.0010 - 39s/epoch - 197ms/step
Epoch 42/1000
2023-10-25 15:10:14.154 
Epoch 42/1000 
	 loss: 33.0304, MinusLogProbMetric: 33.0304, val_loss: 33.2338, val_MinusLogProbMetric: 33.2338

Epoch 42: val_loss did not improve from 32.23315
196/196 - 40s - loss: 33.0304 - MinusLogProbMetric: 33.0304 - val_loss: 33.2338 - val_MinusLogProbMetric: 33.2338 - lr: 0.0010 - 40s/epoch - 205ms/step
Epoch 43/1000
2023-10-25 15:10:52.735 
Epoch 43/1000 
	 loss: 32.7396, MinusLogProbMetric: 32.7396, val_loss: 34.4052, val_MinusLogProbMetric: 34.4052

Epoch 43: val_loss did not improve from 32.23315
196/196 - 39s - loss: 32.7396 - MinusLogProbMetric: 32.7396 - val_loss: 34.4052 - val_MinusLogProbMetric: 34.4052 - lr: 0.0010 - 39s/epoch - 197ms/step
Epoch 44/1000
2023-10-25 15:11:30.264 
Epoch 44/1000 
	 loss: 32.4986, MinusLogProbMetric: 32.4986, val_loss: 33.6888, val_MinusLogProbMetric: 33.6888

Epoch 44: val_loss did not improve from 32.23315
196/196 - 38s - loss: 32.4986 - MinusLogProbMetric: 32.4986 - val_loss: 33.6888 - val_MinusLogProbMetric: 33.6888 - lr: 0.0010 - 38s/epoch - 191ms/step
Epoch 45/1000
2023-10-25 15:12:08.058 
Epoch 45/1000 
	 loss: 32.3669, MinusLogProbMetric: 32.3669, val_loss: 33.1384, val_MinusLogProbMetric: 33.1384

Epoch 45: val_loss did not improve from 32.23315
196/196 - 38s - loss: 32.3669 - MinusLogProbMetric: 32.3669 - val_loss: 33.1384 - val_MinusLogProbMetric: 33.1384 - lr: 0.0010 - 38s/epoch - 193ms/step
Epoch 46/1000
2023-10-25 15:12:48.140 
Epoch 46/1000 
	 loss: 32.8166, MinusLogProbMetric: 32.8166, val_loss: 32.4560, val_MinusLogProbMetric: 32.4560

Epoch 46: val_loss did not improve from 32.23315
196/196 - 40s - loss: 32.8166 - MinusLogProbMetric: 32.8166 - val_loss: 32.4560 - val_MinusLogProbMetric: 32.4560 - lr: 0.0010 - 40s/epoch - 204ms/step
Epoch 47/1000
2023-10-25 15:13:25.732 
Epoch 47/1000 
	 loss: 32.9134, MinusLogProbMetric: 32.9134, val_loss: 33.7538, val_MinusLogProbMetric: 33.7538

Epoch 47: val_loss did not improve from 32.23315
196/196 - 38s - loss: 32.9134 - MinusLogProbMetric: 32.9134 - val_loss: 33.7538 - val_MinusLogProbMetric: 33.7538 - lr: 0.0010 - 38s/epoch - 192ms/step
Epoch 48/1000
2023-10-25 15:14:04.759 
Epoch 48/1000 
	 loss: 32.7287, MinusLogProbMetric: 32.7287, val_loss: 32.2823, val_MinusLogProbMetric: 32.2823

Epoch 48: val_loss did not improve from 32.23315
196/196 - 39s - loss: 32.7287 - MinusLogProbMetric: 32.7287 - val_loss: 32.2823 - val_MinusLogProbMetric: 32.2823 - lr: 0.0010 - 39s/epoch - 199ms/step
Epoch 49/1000
2023-10-25 15:14:43.729 
Epoch 49/1000 
	 loss: 32.2664, MinusLogProbMetric: 32.2664, val_loss: 34.2198, val_MinusLogProbMetric: 34.2198

Epoch 49: val_loss did not improve from 32.23315
196/196 - 39s - loss: 32.2664 - MinusLogProbMetric: 32.2664 - val_loss: 34.2198 - val_MinusLogProbMetric: 34.2198 - lr: 0.0010 - 39s/epoch - 199ms/step
Epoch 50/1000
2023-10-25 15:15:23.522 
Epoch 50/1000 
	 loss: 32.0158, MinusLogProbMetric: 32.0158, val_loss: 32.2215, val_MinusLogProbMetric: 32.2215

Epoch 50: val_loss improved from 32.23315 to 32.22153, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_356/weights/best_weights.h5
196/196 - 40s - loss: 32.0158 - MinusLogProbMetric: 32.0158 - val_loss: 32.2215 - val_MinusLogProbMetric: 32.2215 - lr: 0.0010 - 40s/epoch - 206ms/step
Epoch 51/1000
2023-10-25 15:16:04.580 
Epoch 51/1000 
	 loss: 31.8793, MinusLogProbMetric: 31.8793, val_loss: 32.4955, val_MinusLogProbMetric: 32.4955

Epoch 51: val_loss did not improve from 32.22153
196/196 - 40s - loss: 31.8793 - MinusLogProbMetric: 31.8793 - val_loss: 32.4955 - val_MinusLogProbMetric: 32.4955 - lr: 0.0010 - 40s/epoch - 206ms/step
Epoch 52/1000
2023-10-25 15:16:43.796 
Epoch 52/1000 
	 loss: 32.0186, MinusLogProbMetric: 32.0186, val_loss: 32.3778, val_MinusLogProbMetric: 32.3778

Epoch 52: val_loss did not improve from 32.22153
196/196 - 39s - loss: 32.0186 - MinusLogProbMetric: 32.0186 - val_loss: 32.3778 - val_MinusLogProbMetric: 32.3778 - lr: 0.0010 - 39s/epoch - 200ms/step
Epoch 53/1000
2023-10-25 15:17:22.647 
Epoch 53/1000 
	 loss: 31.9359, MinusLogProbMetric: 31.9359, val_loss: 31.5545, val_MinusLogProbMetric: 31.5545

Epoch 53: val_loss improved from 32.22153 to 31.55450, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_356/weights/best_weights.h5
196/196 - 40s - loss: 31.9359 - MinusLogProbMetric: 31.9359 - val_loss: 31.5545 - val_MinusLogProbMetric: 31.5545 - lr: 0.0010 - 40s/epoch - 202ms/step
Epoch 54/1000
2023-10-25 15:18:02.161 
Epoch 54/1000 
	 loss: 32.1674, MinusLogProbMetric: 32.1674, val_loss: 32.6894, val_MinusLogProbMetric: 32.6894

Epoch 54: val_loss did not improve from 31.55450
196/196 - 39s - loss: 32.1674 - MinusLogProbMetric: 32.1674 - val_loss: 32.6894 - val_MinusLogProbMetric: 32.6894 - lr: 0.0010 - 39s/epoch - 198ms/step
Epoch 55/1000
2023-10-25 15:18:40.950 
Epoch 55/1000 
	 loss: 31.6691, MinusLogProbMetric: 31.6691, val_loss: 33.1993, val_MinusLogProbMetric: 33.1993

Epoch 55: val_loss did not improve from 31.55450
196/196 - 39s - loss: 31.6691 - MinusLogProbMetric: 31.6691 - val_loss: 33.1993 - val_MinusLogProbMetric: 33.1993 - lr: 0.0010 - 39s/epoch - 198ms/step
Epoch 56/1000
2023-10-25 15:19:19.271 
Epoch 56/1000 
	 loss: 31.8718, MinusLogProbMetric: 31.8718, val_loss: 33.7243, val_MinusLogProbMetric: 33.7243

Epoch 56: val_loss did not improve from 31.55450
196/196 - 38s - loss: 31.8718 - MinusLogProbMetric: 31.8718 - val_loss: 33.7243 - val_MinusLogProbMetric: 33.7243 - lr: 0.0010 - 38s/epoch - 195ms/step
Epoch 57/1000
2023-10-25 15:19:57.551 
Epoch 57/1000 
	 loss: 31.7886, MinusLogProbMetric: 31.7886, val_loss: 31.7273, val_MinusLogProbMetric: 31.7273

Epoch 57: val_loss did not improve from 31.55450
196/196 - 38s - loss: 31.7886 - MinusLogProbMetric: 31.7886 - val_loss: 31.7273 - val_MinusLogProbMetric: 31.7273 - lr: 0.0010 - 38s/epoch - 195ms/step
Epoch 58/1000
2023-10-25 15:20:34.632 
Epoch 58/1000 
	 loss: 31.7199, MinusLogProbMetric: 31.7199, val_loss: 34.7376, val_MinusLogProbMetric: 34.7376

Epoch 58: val_loss did not improve from 31.55450
196/196 - 37s - loss: 31.7199 - MinusLogProbMetric: 31.7199 - val_loss: 34.7376 - val_MinusLogProbMetric: 34.7376 - lr: 0.0010 - 37s/epoch - 189ms/step
Epoch 59/1000
2023-10-25 15:21:13.436 
Epoch 59/1000 
	 loss: 31.5046, MinusLogProbMetric: 31.5046, val_loss: 32.5735, val_MinusLogProbMetric: 32.5735

Epoch 59: val_loss did not improve from 31.55450
196/196 - 39s - loss: 31.5046 - MinusLogProbMetric: 31.5046 - val_loss: 32.5735 - val_MinusLogProbMetric: 32.5735 - lr: 0.0010 - 39s/epoch - 198ms/step
Epoch 60/1000
2023-10-25 15:21:51.087 
Epoch 60/1000 
	 loss: 31.5680, MinusLogProbMetric: 31.5680, val_loss: 31.1076, val_MinusLogProbMetric: 31.1076

Epoch 60: val_loss improved from 31.55450 to 31.10759, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_356/weights/best_weights.h5
196/196 - 38s - loss: 31.5680 - MinusLogProbMetric: 31.5680 - val_loss: 31.1076 - val_MinusLogProbMetric: 31.1076 - lr: 0.0010 - 38s/epoch - 196ms/step
Epoch 61/1000
2023-10-25 15:22:32.108 
Epoch 61/1000 
	 loss: 31.5288, MinusLogProbMetric: 31.5288, val_loss: 32.7382, val_MinusLogProbMetric: 32.7382

Epoch 61: val_loss did not improve from 31.10759
196/196 - 40s - loss: 31.5288 - MinusLogProbMetric: 31.5288 - val_loss: 32.7382 - val_MinusLogProbMetric: 32.7382 - lr: 0.0010 - 40s/epoch - 206ms/step
Epoch 62/1000
2023-10-25 15:23:11.749 
Epoch 62/1000 
	 loss: 31.3994, MinusLogProbMetric: 31.3994, val_loss: 32.1756, val_MinusLogProbMetric: 32.1756

Epoch 62: val_loss did not improve from 31.10759
196/196 - 40s - loss: 31.3994 - MinusLogProbMetric: 31.3994 - val_loss: 32.1756 - val_MinusLogProbMetric: 32.1756 - lr: 0.0010 - 40s/epoch - 202ms/step
Epoch 63/1000
2023-10-25 15:23:51.075 
Epoch 63/1000 
	 loss: 31.3945, MinusLogProbMetric: 31.3945, val_loss: 31.3756, val_MinusLogProbMetric: 31.3756

Epoch 63: val_loss did not improve from 31.10759
196/196 - 39s - loss: 31.3945 - MinusLogProbMetric: 31.3945 - val_loss: 31.3756 - val_MinusLogProbMetric: 31.3756 - lr: 0.0010 - 39s/epoch - 201ms/step
Epoch 64/1000
2023-10-25 15:24:28.807 
Epoch 64/1000 
	 loss: 31.5601, MinusLogProbMetric: 31.5601, val_loss: 31.9237, val_MinusLogProbMetric: 31.9237

Epoch 64: val_loss did not improve from 31.10759
196/196 - 38s - loss: 31.5601 - MinusLogProbMetric: 31.5601 - val_loss: 31.9237 - val_MinusLogProbMetric: 31.9237 - lr: 0.0010 - 38s/epoch - 192ms/step
Epoch 65/1000
2023-10-25 15:25:07.166 
Epoch 65/1000 
	 loss: 31.1918, MinusLogProbMetric: 31.1918, val_loss: 31.3924, val_MinusLogProbMetric: 31.3924

Epoch 65: val_loss did not improve from 31.10759
196/196 - 38s - loss: 31.1918 - MinusLogProbMetric: 31.1918 - val_loss: 31.3924 - val_MinusLogProbMetric: 31.3924 - lr: 0.0010 - 38s/epoch - 196ms/step
Epoch 66/1000
2023-10-25 15:25:45.725 
Epoch 66/1000 
	 loss: 31.1587, MinusLogProbMetric: 31.1587, val_loss: 32.3416, val_MinusLogProbMetric: 32.3416

Epoch 66: val_loss did not improve from 31.10759
196/196 - 39s - loss: 31.1587 - MinusLogProbMetric: 31.1587 - val_loss: 32.3416 - val_MinusLogProbMetric: 32.3416 - lr: 0.0010 - 39s/epoch - 197ms/step
Epoch 67/1000
2023-10-25 15:26:25.489 
Epoch 67/1000 
	 loss: 30.9989, MinusLogProbMetric: 30.9989, val_loss: 32.8118, val_MinusLogProbMetric: 32.8118

Epoch 67: val_loss did not improve from 31.10759
196/196 - 40s - loss: 30.9989 - MinusLogProbMetric: 30.9989 - val_loss: 32.8118 - val_MinusLogProbMetric: 32.8118 - lr: 0.0010 - 40s/epoch - 203ms/step
Epoch 68/1000
2023-10-25 15:27:06.150 
Epoch 68/1000 
	 loss: 31.2889, MinusLogProbMetric: 31.2889, val_loss: 31.0106, val_MinusLogProbMetric: 31.0106

Epoch 68: val_loss improved from 31.10759 to 31.01064, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_356/weights/best_weights.h5
196/196 - 41s - loss: 31.2889 - MinusLogProbMetric: 31.2889 - val_loss: 31.0106 - val_MinusLogProbMetric: 31.0106 - lr: 0.0010 - 41s/epoch - 211ms/step
Epoch 69/1000
2023-10-25 15:27:47.459 
Epoch 69/1000 
	 loss: 31.0588, MinusLogProbMetric: 31.0588, val_loss: 33.6959, val_MinusLogProbMetric: 33.6959

Epoch 69: val_loss did not improve from 31.01064
196/196 - 41s - loss: 31.0588 - MinusLogProbMetric: 31.0588 - val_loss: 33.6959 - val_MinusLogProbMetric: 33.6959 - lr: 0.0010 - 41s/epoch - 207ms/step
Epoch 70/1000
2023-10-25 15:28:28.373 
Epoch 70/1000 
	 loss: 31.0961, MinusLogProbMetric: 31.0961, val_loss: 30.9946, val_MinusLogProbMetric: 30.9946

Epoch 70: val_loss improved from 31.01064 to 30.99455, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_356/weights/best_weights.h5
196/196 - 42s - loss: 31.0961 - MinusLogProbMetric: 31.0961 - val_loss: 30.9946 - val_MinusLogProbMetric: 30.9946 - lr: 0.0010 - 42s/epoch - 212ms/step
Epoch 71/1000
2023-10-25 15:29:09.123 
Epoch 71/1000 
	 loss: 30.9579, MinusLogProbMetric: 30.9579, val_loss: 31.8226, val_MinusLogProbMetric: 31.8226

Epoch 71: val_loss did not improve from 30.99455
196/196 - 40s - loss: 30.9579 - MinusLogProbMetric: 30.9579 - val_loss: 31.8226 - val_MinusLogProbMetric: 31.8226 - lr: 0.0010 - 40s/epoch - 204ms/step
Epoch 72/1000
2023-10-25 15:29:49.678 
Epoch 72/1000 
	 loss: 31.0681, MinusLogProbMetric: 31.0681, val_loss: 31.7943, val_MinusLogProbMetric: 31.7943

Epoch 72: val_loss did not improve from 30.99455
196/196 - 41s - loss: 31.0681 - MinusLogProbMetric: 31.0681 - val_loss: 31.7943 - val_MinusLogProbMetric: 31.7943 - lr: 0.0010 - 41s/epoch - 207ms/step
Epoch 73/1000
2023-10-25 15:30:30.992 
Epoch 73/1000 
	 loss: 30.9878, MinusLogProbMetric: 30.9878, val_loss: 31.3118, val_MinusLogProbMetric: 31.3118

Epoch 73: val_loss did not improve from 30.99455
196/196 - 41s - loss: 30.9878 - MinusLogProbMetric: 30.9878 - val_loss: 31.3118 - val_MinusLogProbMetric: 31.3118 - lr: 0.0010 - 41s/epoch - 211ms/step
Epoch 74/1000
2023-10-25 15:31:11.081 
Epoch 74/1000 
	 loss: 30.8715, MinusLogProbMetric: 30.8715, val_loss: 30.9848, val_MinusLogProbMetric: 30.9848

Epoch 74: val_loss improved from 30.99455 to 30.98476, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_356/weights/best_weights.h5
196/196 - 41s - loss: 30.8715 - MinusLogProbMetric: 30.8715 - val_loss: 30.9848 - val_MinusLogProbMetric: 30.9848 - lr: 0.0010 - 41s/epoch - 207ms/step
Epoch 75/1000
2023-10-25 15:31:52.415 
Epoch 75/1000 
	 loss: 30.9234, MinusLogProbMetric: 30.9234, val_loss: 31.8492, val_MinusLogProbMetric: 31.8492

Epoch 75: val_loss did not improve from 30.98476
196/196 - 41s - loss: 30.9234 - MinusLogProbMetric: 30.9234 - val_loss: 31.8492 - val_MinusLogProbMetric: 31.8492 - lr: 0.0010 - 41s/epoch - 208ms/step
Epoch 76/1000
2023-10-25 15:32:33.665 
Epoch 76/1000 
	 loss: 31.1233, MinusLogProbMetric: 31.1233, val_loss: 32.9897, val_MinusLogProbMetric: 32.9897

Epoch 76: val_loss did not improve from 30.98476
196/196 - 41s - loss: 31.1233 - MinusLogProbMetric: 31.1233 - val_loss: 32.9897 - val_MinusLogProbMetric: 32.9897 - lr: 0.0010 - 41s/epoch - 210ms/step
Epoch 77/1000
2023-10-25 15:33:15.085 
Epoch 77/1000 
	 loss: 31.0153, MinusLogProbMetric: 31.0153, val_loss: 31.4100, val_MinusLogProbMetric: 31.4100

Epoch 77: val_loss did not improve from 30.98476
196/196 - 41s - loss: 31.0153 - MinusLogProbMetric: 31.0153 - val_loss: 31.4100 - val_MinusLogProbMetric: 31.4100 - lr: 0.0010 - 41s/epoch - 211ms/step
Epoch 78/1000
2023-10-25 15:33:55.256 
Epoch 78/1000 
	 loss: 30.7198, MinusLogProbMetric: 30.7198, val_loss: 31.0579, val_MinusLogProbMetric: 31.0579

Epoch 78: val_loss did not improve from 30.98476
196/196 - 40s - loss: 30.7198 - MinusLogProbMetric: 30.7198 - val_loss: 31.0579 - val_MinusLogProbMetric: 31.0579 - lr: 0.0010 - 40s/epoch - 205ms/step
Epoch 79/1000
2023-10-25 15:34:35.195 
Epoch 79/1000 
	 loss: 30.8394, MinusLogProbMetric: 30.8394, val_loss: 31.2162, val_MinusLogProbMetric: 31.2162

Epoch 79: val_loss did not improve from 30.98476
196/196 - 40s - loss: 30.8394 - MinusLogProbMetric: 30.8394 - val_loss: 31.2162 - val_MinusLogProbMetric: 31.2162 - lr: 0.0010 - 40s/epoch - 204ms/step
Epoch 80/1000
2023-10-25 15:35:14.444 
Epoch 80/1000 
	 loss: 30.7108, MinusLogProbMetric: 30.7108, val_loss: 31.8490, val_MinusLogProbMetric: 31.8490

Epoch 80: val_loss did not improve from 30.98476
196/196 - 39s - loss: 30.7108 - MinusLogProbMetric: 30.7108 - val_loss: 31.8490 - val_MinusLogProbMetric: 31.8490 - lr: 0.0010 - 39s/epoch - 200ms/step
Epoch 81/1000
2023-10-25 15:35:53.683 
Epoch 81/1000 
	 loss: 30.7818, MinusLogProbMetric: 30.7818, val_loss: 32.2658, val_MinusLogProbMetric: 32.2658

Epoch 81: val_loss did not improve from 30.98476
196/196 - 39s - loss: 30.7818 - MinusLogProbMetric: 30.7818 - val_loss: 32.2658 - val_MinusLogProbMetric: 32.2658 - lr: 0.0010 - 39s/epoch - 200ms/step
Epoch 82/1000
2023-10-25 15:36:32.129 
Epoch 82/1000 
	 loss: 30.6540, MinusLogProbMetric: 30.6540, val_loss: 32.0706, val_MinusLogProbMetric: 32.0706

Epoch 82: val_loss did not improve from 30.98476
196/196 - 38s - loss: 30.6540 - MinusLogProbMetric: 30.6540 - val_loss: 32.0706 - val_MinusLogProbMetric: 32.0706 - lr: 0.0010 - 38s/epoch - 196ms/step
Epoch 83/1000
2023-10-25 15:37:11.372 
Epoch 83/1000 
	 loss: 30.5294, MinusLogProbMetric: 30.5294, val_loss: 30.6420, val_MinusLogProbMetric: 30.6420

Epoch 83: val_loss improved from 30.98476 to 30.64199, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_356/weights/best_weights.h5
196/196 - 40s - loss: 30.5294 - MinusLogProbMetric: 30.5294 - val_loss: 30.6420 - val_MinusLogProbMetric: 30.6420 - lr: 0.0010 - 40s/epoch - 203ms/step
Epoch 84/1000
2023-10-25 15:37:52.486 
Epoch 84/1000 
	 loss: 30.6379, MinusLogProbMetric: 30.6379, val_loss: 30.3884, val_MinusLogProbMetric: 30.3884

Epoch 84: val_loss improved from 30.64199 to 30.38843, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_356/weights/best_weights.h5
196/196 - 41s - loss: 30.6379 - MinusLogProbMetric: 30.6379 - val_loss: 30.3884 - val_MinusLogProbMetric: 30.3884 - lr: 0.0010 - 41s/epoch - 210ms/step
Epoch 85/1000
2023-10-25 15:38:33.637 
Epoch 85/1000 
	 loss: 30.7004, MinusLogProbMetric: 30.7004, val_loss: 30.4761, val_MinusLogProbMetric: 30.4761

Epoch 85: val_loss did not improve from 30.38843
196/196 - 41s - loss: 30.7004 - MinusLogProbMetric: 30.7004 - val_loss: 30.4761 - val_MinusLogProbMetric: 30.4761 - lr: 0.0010 - 41s/epoch - 207ms/step
Epoch 86/1000
2023-10-25 15:39:14.615 
Epoch 86/1000 
	 loss: 30.5641, MinusLogProbMetric: 30.5641, val_loss: 30.7819, val_MinusLogProbMetric: 30.7819

Epoch 86: val_loss did not improve from 30.38843
196/196 - 41s - loss: 30.5641 - MinusLogProbMetric: 30.5641 - val_loss: 30.7819 - val_MinusLogProbMetric: 30.7819 - lr: 0.0010 - 41s/epoch - 209ms/step
Epoch 87/1000
2023-10-25 15:39:55.363 
Epoch 87/1000 
	 loss: 30.4647, MinusLogProbMetric: 30.4647, val_loss: 32.7656, val_MinusLogProbMetric: 32.7656

Epoch 87: val_loss did not improve from 30.38843
196/196 - 41s - loss: 30.4647 - MinusLogProbMetric: 30.4647 - val_loss: 32.7656 - val_MinusLogProbMetric: 32.7656 - lr: 0.0010 - 41s/epoch - 208ms/step
Epoch 88/1000
2023-10-25 15:40:34.725 
Epoch 88/1000 
	 loss: 30.6439, MinusLogProbMetric: 30.6439, val_loss: 30.7913, val_MinusLogProbMetric: 30.7913

Epoch 88: val_loss did not improve from 30.38843
196/196 - 39s - loss: 30.6439 - MinusLogProbMetric: 30.6439 - val_loss: 30.7913 - val_MinusLogProbMetric: 30.7913 - lr: 0.0010 - 39s/epoch - 201ms/step
Epoch 89/1000
2023-10-25 15:41:15.251 
Epoch 89/1000 
	 loss: 30.4063, MinusLogProbMetric: 30.4063, val_loss: 31.5404, val_MinusLogProbMetric: 31.5404

Epoch 89: val_loss did not improve from 30.38843
196/196 - 41s - loss: 30.4063 - MinusLogProbMetric: 30.4063 - val_loss: 31.5404 - val_MinusLogProbMetric: 31.5404 - lr: 0.0010 - 41s/epoch - 207ms/step
Epoch 90/1000
2023-10-25 15:41:57.579 
Epoch 90/1000 
	 loss: 30.7992, MinusLogProbMetric: 30.7992, val_loss: 30.9441, val_MinusLogProbMetric: 30.9441

Epoch 90: val_loss did not improve from 30.38843
196/196 - 42s - loss: 30.7992 - MinusLogProbMetric: 30.7992 - val_loss: 30.9441 - val_MinusLogProbMetric: 30.9441 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 91/1000
2023-10-25 15:42:38.391 
Epoch 91/1000 
	 loss: 30.3821, MinusLogProbMetric: 30.3821, val_loss: 31.5202, val_MinusLogProbMetric: 31.5202

Epoch 91: val_loss did not improve from 30.38843
196/196 - 41s - loss: 30.3821 - MinusLogProbMetric: 30.3821 - val_loss: 31.5202 - val_MinusLogProbMetric: 31.5202 - lr: 0.0010 - 41s/epoch - 208ms/step
Epoch 92/1000
2023-10-25 15:43:16.869 
Epoch 92/1000 
	 loss: 30.4784, MinusLogProbMetric: 30.4784, val_loss: 30.4151, val_MinusLogProbMetric: 30.4151

Epoch 92: val_loss did not improve from 30.38843
196/196 - 38s - loss: 30.4784 - MinusLogProbMetric: 30.4784 - val_loss: 30.4151 - val_MinusLogProbMetric: 30.4151 - lr: 0.0010 - 38s/epoch - 196ms/step
Epoch 93/1000
2023-10-25 15:43:57.957 
Epoch 93/1000 
	 loss: 30.5483, MinusLogProbMetric: 30.5483, val_loss: 30.7537, val_MinusLogProbMetric: 30.7537

Epoch 93: val_loss did not improve from 30.38843
196/196 - 41s - loss: 30.5483 - MinusLogProbMetric: 30.5483 - val_loss: 30.7537 - val_MinusLogProbMetric: 30.7537 - lr: 0.0010 - 41s/epoch - 210ms/step
Epoch 94/1000
2023-10-25 15:44:38.386 
Epoch 94/1000 
	 loss: 30.2666, MinusLogProbMetric: 30.2666, val_loss: 30.5146, val_MinusLogProbMetric: 30.5146

Epoch 94: val_loss did not improve from 30.38843
196/196 - 40s - loss: 30.2666 - MinusLogProbMetric: 30.2666 - val_loss: 30.5146 - val_MinusLogProbMetric: 30.5146 - lr: 0.0010 - 40s/epoch - 206ms/step
Epoch 95/1000
2023-10-25 15:45:19.723 
Epoch 95/1000 
	 loss: 30.3449, MinusLogProbMetric: 30.3449, val_loss: 30.4084, val_MinusLogProbMetric: 30.4084

Epoch 95: val_loss did not improve from 30.38843
196/196 - 41s - loss: 30.3449 - MinusLogProbMetric: 30.3449 - val_loss: 30.4084 - val_MinusLogProbMetric: 30.4084 - lr: 0.0010 - 41s/epoch - 211ms/step
Epoch 96/1000
2023-10-25 15:45:59.954 
Epoch 96/1000 
	 loss: 30.3472, MinusLogProbMetric: 30.3472, val_loss: 31.5542, val_MinusLogProbMetric: 31.5542

Epoch 96: val_loss did not improve from 30.38843
196/196 - 40s - loss: 30.3472 - MinusLogProbMetric: 30.3472 - val_loss: 31.5542 - val_MinusLogProbMetric: 31.5542 - lr: 0.0010 - 40s/epoch - 205ms/step
Epoch 97/1000
2023-10-25 15:46:38.117 
Epoch 97/1000 
	 loss: 30.3641, MinusLogProbMetric: 30.3641, val_loss: 30.2071, val_MinusLogProbMetric: 30.2071

Epoch 97: val_loss improved from 30.38843 to 30.20712, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_356/weights/best_weights.h5
196/196 - 39s - loss: 30.3641 - MinusLogProbMetric: 30.3641 - val_loss: 30.2071 - val_MinusLogProbMetric: 30.2071 - lr: 0.0010 - 39s/epoch - 198ms/step
Epoch 98/1000
2023-10-25 15:47:16.371 
Epoch 98/1000 
	 loss: 30.2954, MinusLogProbMetric: 30.2954, val_loss: 30.6803, val_MinusLogProbMetric: 30.6803

Epoch 98: val_loss did not improve from 30.20712
196/196 - 38s - loss: 30.2954 - MinusLogProbMetric: 30.2954 - val_loss: 30.6803 - val_MinusLogProbMetric: 30.6803 - lr: 0.0010 - 38s/epoch - 192ms/step
Epoch 99/1000
2023-10-25 15:47:54.808 
Epoch 99/1000 
	 loss: 30.1952, MinusLogProbMetric: 30.1952, val_loss: 31.2868, val_MinusLogProbMetric: 31.2868

Epoch 99: val_loss did not improve from 30.20712
196/196 - 38s - loss: 30.1952 - MinusLogProbMetric: 30.1952 - val_loss: 31.2868 - val_MinusLogProbMetric: 31.2868 - lr: 0.0010 - 38s/epoch - 196ms/step
Epoch 100/1000
2023-10-25 15:48:34.544 
Epoch 100/1000 
	 loss: 30.3474, MinusLogProbMetric: 30.3474, val_loss: 30.2408, val_MinusLogProbMetric: 30.2408

Epoch 100: val_loss did not improve from 30.20712
196/196 - 40s - loss: 30.3474 - MinusLogProbMetric: 30.3474 - val_loss: 30.2408 - val_MinusLogProbMetric: 30.2408 - lr: 0.0010 - 40s/epoch - 203ms/step
Epoch 101/1000
2023-10-25 15:49:16.663 
Epoch 101/1000 
	 loss: 30.0926, MinusLogProbMetric: 30.0926, val_loss: 30.2113, val_MinusLogProbMetric: 30.2113

Epoch 101: val_loss did not improve from 30.20712
196/196 - 42s - loss: 30.0926 - MinusLogProbMetric: 30.0926 - val_loss: 30.2113 - val_MinusLogProbMetric: 30.2113 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 102/1000
2023-10-25 15:49:56.557 
Epoch 102/1000 
	 loss: 30.1594, MinusLogProbMetric: 30.1594, val_loss: 32.0929, val_MinusLogProbMetric: 32.0929

Epoch 102: val_loss did not improve from 30.20712
196/196 - 40s - loss: 30.1594 - MinusLogProbMetric: 30.1594 - val_loss: 32.0929 - val_MinusLogProbMetric: 32.0929 - lr: 0.0010 - 40s/epoch - 204ms/step
Epoch 103/1000
2023-10-25 15:50:35.190 
Epoch 103/1000 
	 loss: 30.2427, MinusLogProbMetric: 30.2427, val_loss: 30.6157, val_MinusLogProbMetric: 30.6157

Epoch 103: val_loss did not improve from 30.20712
196/196 - 39s - loss: 30.2427 - MinusLogProbMetric: 30.2427 - val_loss: 30.6157 - val_MinusLogProbMetric: 30.6157 - lr: 0.0010 - 39s/epoch - 197ms/step
Epoch 104/1000
2023-10-25 15:51:17.493 
Epoch 104/1000 
	 loss: 30.2176, MinusLogProbMetric: 30.2176, val_loss: 30.8427, val_MinusLogProbMetric: 30.8427

Epoch 104: val_loss did not improve from 30.20712
196/196 - 42s - loss: 30.2176 - MinusLogProbMetric: 30.2176 - val_loss: 30.8427 - val_MinusLogProbMetric: 30.8427 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 105/1000
2023-10-25 15:51:59.991 
Epoch 105/1000 
	 loss: 30.1518, MinusLogProbMetric: 30.1518, val_loss: 30.8787, val_MinusLogProbMetric: 30.8787

Epoch 105: val_loss did not improve from 30.20712
196/196 - 42s - loss: 30.1518 - MinusLogProbMetric: 30.1518 - val_loss: 30.8787 - val_MinusLogProbMetric: 30.8787 - lr: 0.0010 - 42s/epoch - 217ms/step
Epoch 106/1000
2023-10-25 15:52:40.189 
Epoch 106/1000 
	 loss: 30.0766, MinusLogProbMetric: 30.0766, val_loss: 30.2628, val_MinusLogProbMetric: 30.2628

Epoch 106: val_loss did not improve from 30.20712
196/196 - 40s - loss: 30.0766 - MinusLogProbMetric: 30.0766 - val_loss: 30.2628 - val_MinusLogProbMetric: 30.2628 - lr: 0.0010 - 40s/epoch - 205ms/step
Epoch 107/1000
2023-10-25 15:53:19.933 
Epoch 107/1000 
	 loss: 30.0155, MinusLogProbMetric: 30.0155, val_loss: 31.3600, val_MinusLogProbMetric: 31.3600

Epoch 107: val_loss did not improve from 30.20712
196/196 - 40s - loss: 30.0155 - MinusLogProbMetric: 30.0155 - val_loss: 31.3600 - val_MinusLogProbMetric: 31.3600 - lr: 0.0010 - 40s/epoch - 203ms/step
Epoch 108/1000
2023-10-25 15:53:58.005 
Epoch 108/1000 
	 loss: 30.1899, MinusLogProbMetric: 30.1899, val_loss: 30.1696, val_MinusLogProbMetric: 30.1696

Epoch 108: val_loss improved from 30.20712 to 30.16958, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_356/weights/best_weights.h5
196/196 - 39s - loss: 30.1899 - MinusLogProbMetric: 30.1899 - val_loss: 30.1696 - val_MinusLogProbMetric: 30.1696 - lr: 0.0010 - 39s/epoch - 198ms/step
Epoch 109/1000
2023-10-25 15:54:37.543 
Epoch 109/1000 
	 loss: 29.9881, MinusLogProbMetric: 29.9881, val_loss: 30.0369, val_MinusLogProbMetric: 30.0369

Epoch 109: val_loss improved from 30.16958 to 30.03694, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_356/weights/best_weights.h5
196/196 - 40s - loss: 29.9881 - MinusLogProbMetric: 29.9881 - val_loss: 30.0369 - val_MinusLogProbMetric: 30.0369 - lr: 0.0010 - 40s/epoch - 202ms/step
Epoch 110/1000
2023-10-25 15:55:16.741 
Epoch 110/1000 
	 loss: 30.1128, MinusLogProbMetric: 30.1128, val_loss: 32.6264, val_MinusLogProbMetric: 32.6264

Epoch 110: val_loss did not improve from 30.03694
196/196 - 39s - loss: 30.1128 - MinusLogProbMetric: 30.1128 - val_loss: 32.6264 - val_MinusLogProbMetric: 32.6264 - lr: 0.0010 - 39s/epoch - 197ms/step
Epoch 111/1000
2023-10-25 15:55:55.940 
Epoch 111/1000 
	 loss: 30.0099, MinusLogProbMetric: 30.0099, val_loss: 30.2199, val_MinusLogProbMetric: 30.2199

Epoch 111: val_loss did not improve from 30.03694
196/196 - 39s - loss: 30.0099 - MinusLogProbMetric: 30.0099 - val_loss: 30.2199 - val_MinusLogProbMetric: 30.2199 - lr: 0.0010 - 39s/epoch - 200ms/step
Epoch 112/1000
2023-10-25 15:56:35.483 
Epoch 112/1000 
	 loss: 29.9228, MinusLogProbMetric: 29.9228, val_loss: 30.6915, val_MinusLogProbMetric: 30.6915

Epoch 112: val_loss did not improve from 30.03694
196/196 - 40s - loss: 29.9228 - MinusLogProbMetric: 29.9228 - val_loss: 30.6915 - val_MinusLogProbMetric: 30.6915 - lr: 0.0010 - 40s/epoch - 202ms/step
Epoch 113/1000
2023-10-25 15:57:13.635 
Epoch 113/1000 
	 loss: 30.0229, MinusLogProbMetric: 30.0229, val_loss: 30.7818, val_MinusLogProbMetric: 30.7818

Epoch 113: val_loss did not improve from 30.03694
196/196 - 38s - loss: 30.0229 - MinusLogProbMetric: 30.0229 - val_loss: 30.7818 - val_MinusLogProbMetric: 30.7818 - lr: 0.0010 - 38s/epoch - 195ms/step
Epoch 114/1000
2023-10-25 15:57:52.592 
Epoch 114/1000 
	 loss: 29.9262, MinusLogProbMetric: 29.9262, val_loss: 30.5284, val_MinusLogProbMetric: 30.5284

Epoch 114: val_loss did not improve from 30.03694
196/196 - 39s - loss: 29.9262 - MinusLogProbMetric: 29.9262 - val_loss: 30.5284 - val_MinusLogProbMetric: 30.5284 - lr: 0.0010 - 39s/epoch - 199ms/step
Epoch 115/1000
2023-10-25 15:58:34.029 
Epoch 115/1000 
	 loss: 29.9664, MinusLogProbMetric: 29.9664, val_loss: 30.1510, val_MinusLogProbMetric: 30.1510

Epoch 115: val_loss did not improve from 30.03694
196/196 - 41s - loss: 29.9664 - MinusLogProbMetric: 29.9664 - val_loss: 30.1510 - val_MinusLogProbMetric: 30.1510 - lr: 0.0010 - 41s/epoch - 211ms/step
Epoch 116/1000
2023-10-25 15:59:16.101 
Epoch 116/1000 
	 loss: 29.8100, MinusLogProbMetric: 29.8100, val_loss: 29.7039, val_MinusLogProbMetric: 29.7039

Epoch 116: val_loss improved from 30.03694 to 29.70387, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_356/weights/best_weights.h5
196/196 - 43s - loss: 29.8100 - MinusLogProbMetric: 29.8100 - val_loss: 29.7039 - val_MinusLogProbMetric: 29.7039 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 117/1000
2023-10-25 15:59:55.408 
Epoch 117/1000 
	 loss: 29.8746, MinusLogProbMetric: 29.8746, val_loss: 29.9922, val_MinusLogProbMetric: 29.9922

Epoch 117: val_loss did not improve from 29.70387
196/196 - 39s - loss: 29.8746 - MinusLogProbMetric: 29.8746 - val_loss: 29.9922 - val_MinusLogProbMetric: 29.9922 - lr: 0.0010 - 39s/epoch - 197ms/step
Epoch 118/1000
2023-10-25 16:00:36.438 
Epoch 118/1000 
	 loss: 29.7676, MinusLogProbMetric: 29.7676, val_loss: 30.5538, val_MinusLogProbMetric: 30.5538

Epoch 118: val_loss did not improve from 29.70387
196/196 - 41s - loss: 29.7676 - MinusLogProbMetric: 29.7676 - val_loss: 30.5538 - val_MinusLogProbMetric: 30.5538 - lr: 0.0010 - 41s/epoch - 209ms/step
Epoch 119/1000
2023-10-25 16:01:16.877 
Epoch 119/1000 
	 loss: 29.8461, MinusLogProbMetric: 29.8461, val_loss: 30.2851, val_MinusLogProbMetric: 30.2851

Epoch 119: val_loss did not improve from 29.70387
196/196 - 40s - loss: 29.8461 - MinusLogProbMetric: 29.8461 - val_loss: 30.2851 - val_MinusLogProbMetric: 30.2851 - lr: 0.0010 - 40s/epoch - 206ms/step
Epoch 120/1000
2023-10-25 16:01:55.828 
Epoch 120/1000 
	 loss: 29.7718, MinusLogProbMetric: 29.7718, val_loss: 29.5890, val_MinusLogProbMetric: 29.5890

Epoch 120: val_loss improved from 29.70387 to 29.58900, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_356/weights/best_weights.h5
196/196 - 40s - loss: 29.7718 - MinusLogProbMetric: 29.7718 - val_loss: 29.5890 - val_MinusLogProbMetric: 29.5890 - lr: 0.0010 - 40s/epoch - 202ms/step
Epoch 121/1000
2023-10-25 16:02:36.916 
Epoch 121/1000 
	 loss: 29.7693, MinusLogProbMetric: 29.7693, val_loss: 30.2890, val_MinusLogProbMetric: 30.2890

Epoch 121: val_loss did not improve from 29.58900
196/196 - 40s - loss: 29.7693 - MinusLogProbMetric: 29.7693 - val_loss: 30.2890 - val_MinusLogProbMetric: 30.2890 - lr: 0.0010 - 40s/epoch - 206ms/step
Epoch 122/1000
2023-10-25 16:03:15.299 
Epoch 122/1000 
	 loss: 29.9659, MinusLogProbMetric: 29.9659, val_loss: 29.9717, val_MinusLogProbMetric: 29.9717

Epoch 122: val_loss did not improve from 29.58900
196/196 - 38s - loss: 29.9659 - MinusLogProbMetric: 29.9659 - val_loss: 29.9717 - val_MinusLogProbMetric: 29.9717 - lr: 0.0010 - 38s/epoch - 196ms/step
Epoch 123/1000
2023-10-25 16:03:55.500 
Epoch 123/1000 
	 loss: 29.7332, MinusLogProbMetric: 29.7332, val_loss: 30.3761, val_MinusLogProbMetric: 30.3761

Epoch 123: val_loss did not improve from 29.58900
196/196 - 40s - loss: 29.7332 - MinusLogProbMetric: 29.7332 - val_loss: 30.3761 - val_MinusLogProbMetric: 30.3761 - lr: 0.0010 - 40s/epoch - 205ms/step
Epoch 124/1000
2023-10-25 16:04:37.711 
Epoch 124/1000 
	 loss: 29.7984, MinusLogProbMetric: 29.7984, val_loss: 30.3975, val_MinusLogProbMetric: 30.3975

Epoch 124: val_loss did not improve from 29.58900
196/196 - 42s - loss: 29.7984 - MinusLogProbMetric: 29.7984 - val_loss: 30.3975 - val_MinusLogProbMetric: 30.3975 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 125/1000
2023-10-25 16:05:20.125 
Epoch 125/1000 
	 loss: 29.8401, MinusLogProbMetric: 29.8401, val_loss: 31.1147, val_MinusLogProbMetric: 31.1147

Epoch 125: val_loss did not improve from 29.58900
196/196 - 42s - loss: 29.8401 - MinusLogProbMetric: 29.8401 - val_loss: 31.1147 - val_MinusLogProbMetric: 31.1147 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 126/1000
2023-10-25 16:05:54.649 
Epoch 126/1000 
	 loss: 29.9143, MinusLogProbMetric: 29.9143, val_loss: 29.9523, val_MinusLogProbMetric: 29.9523

Epoch 126: val_loss did not improve from 29.58900
196/196 - 35s - loss: 29.9143 - MinusLogProbMetric: 29.9143 - val_loss: 29.9523 - val_MinusLogProbMetric: 29.9523 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 127/1000
2023-10-25 16:06:27.251 
Epoch 127/1000 
	 loss: 29.9030, MinusLogProbMetric: 29.9030, val_loss: 30.3366, val_MinusLogProbMetric: 30.3366

Epoch 127: val_loss did not improve from 29.58900
196/196 - 33s - loss: 29.9030 - MinusLogProbMetric: 29.9030 - val_loss: 30.3366 - val_MinusLogProbMetric: 30.3366 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 128/1000
2023-10-25 16:07:00.800 
Epoch 128/1000 
	 loss: 29.6617, MinusLogProbMetric: 29.6617, val_loss: 31.3307, val_MinusLogProbMetric: 31.3307

Epoch 128: val_loss did not improve from 29.58900
196/196 - 34s - loss: 29.6617 - MinusLogProbMetric: 29.6617 - val_loss: 31.3307 - val_MinusLogProbMetric: 31.3307 - lr: 0.0010 - 34s/epoch - 171ms/step
Epoch 129/1000
2023-10-25 16:07:35.802 
Epoch 129/1000 
	 loss: 29.7998, MinusLogProbMetric: 29.7998, val_loss: 29.9225, val_MinusLogProbMetric: 29.9225

Epoch 129: val_loss did not improve from 29.58900
196/196 - 35s - loss: 29.7998 - MinusLogProbMetric: 29.7998 - val_loss: 29.9225 - val_MinusLogProbMetric: 29.9225 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 130/1000
2023-10-25 16:08:09.998 
Epoch 130/1000 
	 loss: 29.7294, MinusLogProbMetric: 29.7294, val_loss: 29.9292, val_MinusLogProbMetric: 29.9292

Epoch 130: val_loss did not improve from 29.58900
196/196 - 34s - loss: 29.7294 - MinusLogProbMetric: 29.7294 - val_loss: 29.9292 - val_MinusLogProbMetric: 29.9292 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 131/1000
2023-10-25 16:08:42.642 
Epoch 131/1000 
	 loss: 29.7395, MinusLogProbMetric: 29.7395, val_loss: 30.4071, val_MinusLogProbMetric: 30.4071

Epoch 131: val_loss did not improve from 29.58900
196/196 - 33s - loss: 29.7395 - MinusLogProbMetric: 29.7395 - val_loss: 30.4071 - val_MinusLogProbMetric: 30.4071 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 132/1000
2023-10-25 16:09:15.589 
Epoch 132/1000 
	 loss: 29.6734, MinusLogProbMetric: 29.6734, val_loss: 29.7818, val_MinusLogProbMetric: 29.7818

Epoch 132: val_loss did not improve from 29.58900
196/196 - 33s - loss: 29.6734 - MinusLogProbMetric: 29.6734 - val_loss: 29.7818 - val_MinusLogProbMetric: 29.7818 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 133/1000
2023-10-25 16:09:49.065 
Epoch 133/1000 
	 loss: 29.6926, MinusLogProbMetric: 29.6926, val_loss: 30.0150, val_MinusLogProbMetric: 30.0150

Epoch 133: val_loss did not improve from 29.58900
196/196 - 33s - loss: 29.6926 - MinusLogProbMetric: 29.6926 - val_loss: 30.0150 - val_MinusLogProbMetric: 30.0150 - lr: 0.0010 - 33s/epoch - 171ms/step
Epoch 134/1000
2023-10-25 16:10:25.070 
Epoch 134/1000 
	 loss: 29.5866, MinusLogProbMetric: 29.5866, val_loss: 29.8903, val_MinusLogProbMetric: 29.8903

Epoch 134: val_loss did not improve from 29.58900
196/196 - 36s - loss: 29.5866 - MinusLogProbMetric: 29.5866 - val_loss: 29.8903 - val_MinusLogProbMetric: 29.8903 - lr: 0.0010 - 36s/epoch - 184ms/step
Epoch 135/1000
2023-10-25 16:10:58.283 
Epoch 135/1000 
	 loss: 29.6881, MinusLogProbMetric: 29.6881, val_loss: 30.2356, val_MinusLogProbMetric: 30.2356

Epoch 135: val_loss did not improve from 29.58900
196/196 - 33s - loss: 29.6881 - MinusLogProbMetric: 29.6881 - val_loss: 30.2356 - val_MinusLogProbMetric: 30.2356 - lr: 0.0010 - 33s/epoch - 169ms/step
Epoch 136/1000
2023-10-25 16:11:31.658 
Epoch 136/1000 
	 loss: 29.5946, MinusLogProbMetric: 29.5946, val_loss: 30.6238, val_MinusLogProbMetric: 30.6238

Epoch 136: val_loss did not improve from 29.58900
196/196 - 33s - loss: 29.5946 - MinusLogProbMetric: 29.5946 - val_loss: 30.6238 - val_MinusLogProbMetric: 30.6238 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 137/1000
2023-10-25 16:12:07.920 
Epoch 137/1000 
	 loss: 29.7119, MinusLogProbMetric: 29.7119, val_loss: 30.1054, val_MinusLogProbMetric: 30.1054

Epoch 137: val_loss did not improve from 29.58900
196/196 - 36s - loss: 29.7119 - MinusLogProbMetric: 29.7119 - val_loss: 30.1054 - val_MinusLogProbMetric: 30.1054 - lr: 0.0010 - 36s/epoch - 185ms/step
Epoch 138/1000
2023-10-25 16:12:43.139 
Epoch 138/1000 
	 loss: 29.5143, MinusLogProbMetric: 29.5143, val_loss: 29.8564, val_MinusLogProbMetric: 29.8564

Epoch 138: val_loss did not improve from 29.58900
196/196 - 35s - loss: 29.5143 - MinusLogProbMetric: 29.5143 - val_loss: 29.8564 - val_MinusLogProbMetric: 29.8564 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 139/1000
2023-10-25 16:13:17.466 
Epoch 139/1000 
	 loss: 29.5671, MinusLogProbMetric: 29.5671, val_loss: 31.0202, val_MinusLogProbMetric: 31.0202

Epoch 139: val_loss did not improve from 29.58900
196/196 - 34s - loss: 29.5671 - MinusLogProbMetric: 29.5671 - val_loss: 31.0202 - val_MinusLogProbMetric: 31.0202 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 140/1000
2023-10-25 16:13:51.935 
Epoch 140/1000 
	 loss: 29.5449, MinusLogProbMetric: 29.5449, val_loss: 31.0097, val_MinusLogProbMetric: 31.0097

Epoch 140: val_loss did not improve from 29.58900
196/196 - 34s - loss: 29.5449 - MinusLogProbMetric: 29.5449 - val_loss: 31.0097 - val_MinusLogProbMetric: 31.0097 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 141/1000
2023-10-25 16:14:31.234 
Epoch 141/1000 
	 loss: 29.6036, MinusLogProbMetric: 29.6036, val_loss: 30.2473, val_MinusLogProbMetric: 30.2473

Epoch 141: val_loss did not improve from 29.58900
196/196 - 39s - loss: 29.6036 - MinusLogProbMetric: 29.6036 - val_loss: 30.2473 - val_MinusLogProbMetric: 30.2473 - lr: 0.0010 - 39s/epoch - 200ms/step
Epoch 142/1000
2023-10-25 16:15:05.864 
Epoch 142/1000 
	 loss: 29.5964, MinusLogProbMetric: 29.5964, val_loss: 29.9929, val_MinusLogProbMetric: 29.9929

Epoch 142: val_loss did not improve from 29.58900
196/196 - 35s - loss: 29.5964 - MinusLogProbMetric: 29.5964 - val_loss: 29.9929 - val_MinusLogProbMetric: 29.9929 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 143/1000
2023-10-25 16:15:39.933 
Epoch 143/1000 
	 loss: 29.4227, MinusLogProbMetric: 29.4227, val_loss: 30.0879, val_MinusLogProbMetric: 30.0879

Epoch 143: val_loss did not improve from 29.58900
196/196 - 34s - loss: 29.4227 - MinusLogProbMetric: 29.4227 - val_loss: 30.0879 - val_MinusLogProbMetric: 30.0879 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 144/1000
2023-10-25 16:16:15.260 
Epoch 144/1000 
	 loss: 29.7002, MinusLogProbMetric: 29.7002, val_loss: 29.3547, val_MinusLogProbMetric: 29.3547

Epoch 144: val_loss improved from 29.58900 to 29.35468, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_356/weights/best_weights.h5
196/196 - 36s - loss: 29.7002 - MinusLogProbMetric: 29.7002 - val_loss: 29.3547 - val_MinusLogProbMetric: 29.3547 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 145/1000
2023-10-25 16:16:51.243 
Epoch 145/1000 
	 loss: 29.4353, MinusLogProbMetric: 29.4353, val_loss: 30.8998, val_MinusLogProbMetric: 30.8998

Epoch 145: val_loss did not improve from 29.35468
196/196 - 35s - loss: 29.4353 - MinusLogProbMetric: 29.4353 - val_loss: 30.8998 - val_MinusLogProbMetric: 30.8998 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 146/1000
2023-10-25 16:17:26.049 
Epoch 146/1000 
	 loss: 29.4496, MinusLogProbMetric: 29.4496, val_loss: 30.1330, val_MinusLogProbMetric: 30.1330

Epoch 146: val_loss did not improve from 29.35468
196/196 - 35s - loss: 29.4496 - MinusLogProbMetric: 29.4496 - val_loss: 30.1330 - val_MinusLogProbMetric: 30.1330 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 147/1000
2023-10-25 16:18:00.868 
Epoch 147/1000 
	 loss: 29.5245, MinusLogProbMetric: 29.5245, val_loss: 30.4172, val_MinusLogProbMetric: 30.4172

Epoch 147: val_loss did not improve from 29.35468
196/196 - 35s - loss: 29.5245 - MinusLogProbMetric: 29.5245 - val_loss: 30.4172 - val_MinusLogProbMetric: 30.4172 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 148/1000
2023-10-25 16:18:40.083 
Epoch 148/1000 
	 loss: 29.4484, MinusLogProbMetric: 29.4484, val_loss: 29.6973, val_MinusLogProbMetric: 29.6973

Epoch 148: val_loss did not improve from 29.35468
196/196 - 39s - loss: 29.4484 - MinusLogProbMetric: 29.4484 - val_loss: 29.6973 - val_MinusLogProbMetric: 29.6973 - lr: 0.0010 - 39s/epoch - 200ms/step
Epoch 149/1000
2023-10-25 16:19:16.667 
Epoch 149/1000 
	 loss: 29.5246, MinusLogProbMetric: 29.5246, val_loss: 29.5811, val_MinusLogProbMetric: 29.5811

Epoch 149: val_loss did not improve from 29.35468
196/196 - 37s - loss: 29.5246 - MinusLogProbMetric: 29.5246 - val_loss: 29.5811 - val_MinusLogProbMetric: 29.5811 - lr: 0.0010 - 37s/epoch - 187ms/step
Epoch 150/1000
2023-10-25 16:19:50.331 
Epoch 150/1000 
	 loss: 29.4621, MinusLogProbMetric: 29.4621, val_loss: 29.6362, val_MinusLogProbMetric: 29.6362

Epoch 150: val_loss did not improve from 29.35468
196/196 - 34s - loss: 29.4621 - MinusLogProbMetric: 29.4621 - val_loss: 29.6362 - val_MinusLogProbMetric: 29.6362 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 151/1000
2023-10-25 16:20:24.145 
Epoch 151/1000 
	 loss: 29.5260, MinusLogProbMetric: 29.5260, val_loss: 30.3871, val_MinusLogProbMetric: 30.3871

Epoch 151: val_loss did not improve from 29.35468
196/196 - 34s - loss: 29.5260 - MinusLogProbMetric: 29.5260 - val_loss: 30.3871 - val_MinusLogProbMetric: 30.3871 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 152/1000
2023-10-25 16:21:01.110 
Epoch 152/1000 
	 loss: 29.3469, MinusLogProbMetric: 29.3469, val_loss: 29.8365, val_MinusLogProbMetric: 29.8365

Epoch 152: val_loss did not improve from 29.35468
196/196 - 37s - loss: 29.3469 - MinusLogProbMetric: 29.3469 - val_loss: 29.8365 - val_MinusLogProbMetric: 29.8365 - lr: 0.0010 - 37s/epoch - 189ms/step
Epoch 153/1000
2023-10-25 16:21:40.121 
Epoch 153/1000 
	 loss: 29.3624, MinusLogProbMetric: 29.3624, val_loss: 29.5685, val_MinusLogProbMetric: 29.5685

Epoch 153: val_loss did not improve from 29.35468
196/196 - 39s - loss: 29.3624 - MinusLogProbMetric: 29.3624 - val_loss: 29.5685 - val_MinusLogProbMetric: 29.5685 - lr: 0.0010 - 39s/epoch - 199ms/step
Epoch 154/1000
2023-10-25 16:22:14.504 
Epoch 154/1000 
	 loss: 29.4731, MinusLogProbMetric: 29.4731, val_loss: 29.2495, val_MinusLogProbMetric: 29.2495

Epoch 154: val_loss improved from 29.35468 to 29.24945, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_356/weights/best_weights.h5
196/196 - 35s - loss: 29.4731 - MinusLogProbMetric: 29.4731 - val_loss: 29.2495 - val_MinusLogProbMetric: 29.2495 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 155/1000
2023-10-25 16:22:50.283 
Epoch 155/1000 
	 loss: 29.3557, MinusLogProbMetric: 29.3557, val_loss: 30.0325, val_MinusLogProbMetric: 30.0325

Epoch 155: val_loss did not improve from 29.24945
196/196 - 35s - loss: 29.3557 - MinusLogProbMetric: 29.3557 - val_loss: 30.0325 - val_MinusLogProbMetric: 30.0325 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 156/1000
2023-10-25 16:23:26.130 
Epoch 156/1000 
	 loss: 29.3604, MinusLogProbMetric: 29.3604, val_loss: 30.6451, val_MinusLogProbMetric: 30.6451

Epoch 156: val_loss did not improve from 29.24945
196/196 - 36s - loss: 29.3604 - MinusLogProbMetric: 29.3604 - val_loss: 30.6451 - val_MinusLogProbMetric: 30.6451 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 157/1000
2023-10-25 16:24:00.558 
Epoch 157/1000 
	 loss: 29.4009, MinusLogProbMetric: 29.4009, val_loss: 29.5035, val_MinusLogProbMetric: 29.5035

Epoch 157: val_loss did not improve from 29.24945
196/196 - 34s - loss: 29.4009 - MinusLogProbMetric: 29.4009 - val_loss: 29.5035 - val_MinusLogProbMetric: 29.5035 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 158/1000
2023-10-25 16:24:35.201 
Epoch 158/1000 
	 loss: 29.5050, MinusLogProbMetric: 29.5050, val_loss: 30.1734, val_MinusLogProbMetric: 30.1734

Epoch 158: val_loss did not improve from 29.24945
196/196 - 35s - loss: 29.5050 - MinusLogProbMetric: 29.5050 - val_loss: 30.1734 - val_MinusLogProbMetric: 30.1734 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 159/1000
2023-10-25 16:25:11.927 
Epoch 159/1000 
	 loss: 29.2461, MinusLogProbMetric: 29.2461, val_loss: 29.9695, val_MinusLogProbMetric: 29.9695

Epoch 159: val_loss did not improve from 29.24945
196/196 - 37s - loss: 29.2461 - MinusLogProbMetric: 29.2461 - val_loss: 29.9695 - val_MinusLogProbMetric: 29.9695 - lr: 0.0010 - 37s/epoch - 187ms/step
Epoch 160/1000
2023-10-25 16:25:53.333 
Epoch 160/1000 
	 loss: 29.2759, MinusLogProbMetric: 29.2759, val_loss: 29.4745, val_MinusLogProbMetric: 29.4745

Epoch 160: val_loss did not improve from 29.24945
196/196 - 41s - loss: 29.2759 - MinusLogProbMetric: 29.2759 - val_loss: 29.4745 - val_MinusLogProbMetric: 29.4745 - lr: 0.0010 - 41s/epoch - 211ms/step
Epoch 161/1000
2023-10-25 16:26:30.830 
Epoch 161/1000 
	 loss: 29.3938, MinusLogProbMetric: 29.3938, val_loss: 29.6668, val_MinusLogProbMetric: 29.6668

Epoch 161: val_loss did not improve from 29.24945
196/196 - 37s - loss: 29.3938 - MinusLogProbMetric: 29.3938 - val_loss: 29.6668 - val_MinusLogProbMetric: 29.6668 - lr: 0.0010 - 37s/epoch - 191ms/step
Epoch 162/1000
2023-10-25 16:27:05.085 
Epoch 162/1000 
	 loss: 29.2532, MinusLogProbMetric: 29.2532, val_loss: 29.7322, val_MinusLogProbMetric: 29.7322

Epoch 162: val_loss did not improve from 29.24945
196/196 - 34s - loss: 29.2532 - MinusLogProbMetric: 29.2532 - val_loss: 29.7322 - val_MinusLogProbMetric: 29.7322 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 163/1000
2023-10-25 16:27:40.923 
Epoch 163/1000 
	 loss: 29.3091, MinusLogProbMetric: 29.3091, val_loss: 29.8491, val_MinusLogProbMetric: 29.8491

Epoch 163: val_loss did not improve from 29.24945
196/196 - 36s - loss: 29.3091 - MinusLogProbMetric: 29.3091 - val_loss: 29.8491 - val_MinusLogProbMetric: 29.8491 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 164/1000
2023-10-25 16:28:20.858 
Epoch 164/1000 
	 loss: 29.2956, MinusLogProbMetric: 29.2956, val_loss: 29.7687, val_MinusLogProbMetric: 29.7687

Epoch 164: val_loss did not improve from 29.24945
196/196 - 40s - loss: 29.2956 - MinusLogProbMetric: 29.2956 - val_loss: 29.7687 - val_MinusLogProbMetric: 29.7687 - lr: 0.0010 - 40s/epoch - 204ms/step
Epoch 165/1000
2023-10-25 16:28:55.360 
Epoch 165/1000 
	 loss: 29.1485, MinusLogProbMetric: 29.1485, val_loss: 30.3010, val_MinusLogProbMetric: 30.3010

Epoch 165: val_loss did not improve from 29.24945
196/196 - 34s - loss: 29.1485 - MinusLogProbMetric: 29.1485 - val_loss: 30.3010 - val_MinusLogProbMetric: 30.3010 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 166/1000
2023-10-25 16:29:30.160 
Epoch 166/1000 
	 loss: 29.3393, MinusLogProbMetric: 29.3393, val_loss: 29.9840, val_MinusLogProbMetric: 29.9840

Epoch 166: val_loss did not improve from 29.24945
196/196 - 35s - loss: 29.3393 - MinusLogProbMetric: 29.3393 - val_loss: 29.9840 - val_MinusLogProbMetric: 29.9840 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 167/1000
2023-10-25 16:30:06.160 
Epoch 167/1000 
	 loss: 29.2400, MinusLogProbMetric: 29.2400, val_loss: 29.9240, val_MinusLogProbMetric: 29.9240

Epoch 167: val_loss did not improve from 29.24945
196/196 - 36s - loss: 29.2400 - MinusLogProbMetric: 29.2400 - val_loss: 29.9240 - val_MinusLogProbMetric: 29.9240 - lr: 0.0010 - 36s/epoch - 184ms/step
Epoch 168/1000
2023-10-25 16:30:42.361 
Epoch 168/1000 
	 loss: 29.2067, MinusLogProbMetric: 29.2067, val_loss: 29.6279, val_MinusLogProbMetric: 29.6279

Epoch 168: val_loss did not improve from 29.24945
196/196 - 36s - loss: 29.2067 - MinusLogProbMetric: 29.2067 - val_loss: 29.6279 - val_MinusLogProbMetric: 29.6279 - lr: 0.0010 - 36s/epoch - 185ms/step
Epoch 169/1000
2023-10-25 16:31:16.806 
Epoch 169/1000 
	 loss: 29.2488, MinusLogProbMetric: 29.2488, val_loss: 30.0190, val_MinusLogProbMetric: 30.0190

Epoch 169: val_loss did not improve from 29.24945
196/196 - 34s - loss: 29.2488 - MinusLogProbMetric: 29.2488 - val_loss: 30.0190 - val_MinusLogProbMetric: 30.0190 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 170/1000
2023-10-25 16:31:58.122 
Epoch 170/1000 
	 loss: 29.1578, MinusLogProbMetric: 29.1578, val_loss: 29.7654, val_MinusLogProbMetric: 29.7654

Epoch 170: val_loss did not improve from 29.24945
196/196 - 41s - loss: 29.1578 - MinusLogProbMetric: 29.1578 - val_loss: 29.7654 - val_MinusLogProbMetric: 29.7654 - lr: 0.0010 - 41s/epoch - 211ms/step
Epoch 171/1000
2023-10-25 16:32:41.078 
Epoch 171/1000 
	 loss: 29.1434, MinusLogProbMetric: 29.1434, val_loss: 30.3536, val_MinusLogProbMetric: 30.3536

Epoch 171: val_loss did not improve from 29.24945
196/196 - 43s - loss: 29.1434 - MinusLogProbMetric: 29.1434 - val_loss: 30.3536 - val_MinusLogProbMetric: 30.3536 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 172/1000
2023-10-25 16:33:21.621 
Epoch 172/1000 
	 loss: 29.2147, MinusLogProbMetric: 29.2147, val_loss: 29.7601, val_MinusLogProbMetric: 29.7601

Epoch 172: val_loss did not improve from 29.24945
196/196 - 41s - loss: 29.2147 - MinusLogProbMetric: 29.2147 - val_loss: 29.7601 - val_MinusLogProbMetric: 29.7601 - lr: 0.0010 - 41s/epoch - 207ms/step
Epoch 173/1000
2023-10-25 16:33:59.421 
Epoch 173/1000 
	 loss: 29.2489, MinusLogProbMetric: 29.2489, val_loss: 30.5949, val_MinusLogProbMetric: 30.5949

Epoch 173: val_loss did not improve from 29.24945
196/196 - 38s - loss: 29.2489 - MinusLogProbMetric: 29.2489 - val_loss: 30.5949 - val_MinusLogProbMetric: 30.5949 - lr: 0.0010 - 38s/epoch - 193ms/step
Epoch 174/1000
2023-10-25 16:34:36.962 
Epoch 174/1000 
	 loss: 29.1012, MinusLogProbMetric: 29.1012, val_loss: 29.7114, val_MinusLogProbMetric: 29.7114

Epoch 174: val_loss did not improve from 29.24945
196/196 - 38s - loss: 29.1012 - MinusLogProbMetric: 29.1012 - val_loss: 29.7114 - val_MinusLogProbMetric: 29.7114 - lr: 0.0010 - 38s/epoch - 192ms/step
Epoch 175/1000
2023-10-25 16:35:14.639 
Epoch 175/1000 
	 loss: 29.2611, MinusLogProbMetric: 29.2611, val_loss: 30.1972, val_MinusLogProbMetric: 30.1972

Epoch 175: val_loss did not improve from 29.24945
196/196 - 38s - loss: 29.2611 - MinusLogProbMetric: 29.2611 - val_loss: 30.1972 - val_MinusLogProbMetric: 30.1972 - lr: 0.0010 - 38s/epoch - 192ms/step
Epoch 176/1000
2023-10-25 16:35:52.273 
Epoch 176/1000 
	 loss: 29.1986, MinusLogProbMetric: 29.1986, val_loss: 29.6439, val_MinusLogProbMetric: 29.6439

Epoch 176: val_loss did not improve from 29.24945
196/196 - 38s - loss: 29.1986 - MinusLogProbMetric: 29.1986 - val_loss: 29.6439 - val_MinusLogProbMetric: 29.6439 - lr: 0.0010 - 38s/epoch - 192ms/step
Epoch 177/1000
2023-10-25 16:36:30.060 
Epoch 177/1000 
	 loss: 29.2060, MinusLogProbMetric: 29.2060, val_loss: 30.7551, val_MinusLogProbMetric: 30.7551

Epoch 177: val_loss did not improve from 29.24945
196/196 - 38s - loss: 29.2060 - MinusLogProbMetric: 29.2060 - val_loss: 30.7551 - val_MinusLogProbMetric: 30.7551 - lr: 0.0010 - 38s/epoch - 193ms/step
Epoch 178/1000
2023-10-25 16:37:09.140 
Epoch 178/1000 
	 loss: 29.1130, MinusLogProbMetric: 29.1130, val_loss: 29.4496, val_MinusLogProbMetric: 29.4496

Epoch 178: val_loss did not improve from 29.24945
196/196 - 39s - loss: 29.1130 - MinusLogProbMetric: 29.1130 - val_loss: 29.4496 - val_MinusLogProbMetric: 29.4496 - lr: 0.0010 - 39s/epoch - 199ms/step
Epoch 179/1000
2023-10-25 16:37:47.284 
Epoch 179/1000 
	 loss: 29.2194, MinusLogProbMetric: 29.2194, val_loss: 29.3127, val_MinusLogProbMetric: 29.3127

Epoch 179: val_loss did not improve from 29.24945
196/196 - 38s - loss: 29.2194 - MinusLogProbMetric: 29.2194 - val_loss: 29.3127 - val_MinusLogProbMetric: 29.3127 - lr: 0.0010 - 38s/epoch - 195ms/step
Epoch 180/1000
2023-10-25 16:38:25.165 
Epoch 180/1000 
	 loss: 29.0557, MinusLogProbMetric: 29.0557, val_loss: 29.4601, val_MinusLogProbMetric: 29.4601

Epoch 180: val_loss did not improve from 29.24945
196/196 - 38s - loss: 29.0557 - MinusLogProbMetric: 29.0557 - val_loss: 29.4601 - val_MinusLogProbMetric: 29.4601 - lr: 0.0010 - 38s/epoch - 193ms/step
Epoch 181/1000
2023-10-25 16:39:03.176 
Epoch 181/1000 
	 loss: 29.0202, MinusLogProbMetric: 29.0202, val_loss: 29.9588, val_MinusLogProbMetric: 29.9588

Epoch 181: val_loss did not improve from 29.24945
196/196 - 38s - loss: 29.0202 - MinusLogProbMetric: 29.0202 - val_loss: 29.9588 - val_MinusLogProbMetric: 29.9588 - lr: 0.0010 - 38s/epoch - 194ms/step
Epoch 182/1000
2023-10-25 16:39:40.859 
Epoch 182/1000 
	 loss: 29.2079, MinusLogProbMetric: 29.2079, val_loss: 29.6500, val_MinusLogProbMetric: 29.6500

Epoch 182: val_loss did not improve from 29.24945
196/196 - 38s - loss: 29.2079 - MinusLogProbMetric: 29.2079 - val_loss: 29.6500 - val_MinusLogProbMetric: 29.6500 - lr: 0.0010 - 38s/epoch - 192ms/step
Epoch 183/1000
2023-10-25 16:40:20.530 
Epoch 183/1000 
	 loss: 29.1803, MinusLogProbMetric: 29.1803, val_loss: 29.8931, val_MinusLogProbMetric: 29.8931

Epoch 183: val_loss did not improve from 29.24945
196/196 - 40s - loss: 29.1803 - MinusLogProbMetric: 29.1803 - val_loss: 29.8931 - val_MinusLogProbMetric: 29.8931 - lr: 0.0010 - 40s/epoch - 202ms/step
Epoch 184/1000
2023-10-25 16:41:02.439 
Epoch 184/1000 
	 loss: 29.1589, MinusLogProbMetric: 29.1589, val_loss: 29.6618, val_MinusLogProbMetric: 29.6618

Epoch 184: val_loss did not improve from 29.24945
196/196 - 42s - loss: 29.1589 - MinusLogProbMetric: 29.1589 - val_loss: 29.6618 - val_MinusLogProbMetric: 29.6618 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 185/1000
2023-10-25 16:41:42.236 
Epoch 185/1000 
	 loss: 29.0185, MinusLogProbMetric: 29.0185, val_loss: 29.7212, val_MinusLogProbMetric: 29.7212

Epoch 185: val_loss did not improve from 29.24945
196/196 - 40s - loss: 29.0185 - MinusLogProbMetric: 29.0185 - val_loss: 29.7212 - val_MinusLogProbMetric: 29.7212 - lr: 0.0010 - 40s/epoch - 203ms/step
Epoch 186/1000
2023-10-25 16:42:20.003 
Epoch 186/1000 
	 loss: 29.1417, MinusLogProbMetric: 29.1417, val_loss: 29.9244, val_MinusLogProbMetric: 29.9244

Epoch 186: val_loss did not improve from 29.24945
196/196 - 38s - loss: 29.1417 - MinusLogProbMetric: 29.1417 - val_loss: 29.9244 - val_MinusLogProbMetric: 29.9244 - lr: 0.0010 - 38s/epoch - 193ms/step
Epoch 187/1000
2023-10-25 16:42:58.270 
Epoch 187/1000 
	 loss: 29.0254, MinusLogProbMetric: 29.0254, val_loss: 29.3393, val_MinusLogProbMetric: 29.3393

Epoch 187: val_loss did not improve from 29.24945
196/196 - 38s - loss: 29.0254 - MinusLogProbMetric: 29.0254 - val_loss: 29.3393 - val_MinusLogProbMetric: 29.3393 - lr: 0.0010 - 38s/epoch - 195ms/step
Epoch 188/1000
2023-10-25 16:43:39.908 
Epoch 188/1000 
	 loss: 29.1260, MinusLogProbMetric: 29.1260, val_loss: 29.9591, val_MinusLogProbMetric: 29.9591

Epoch 188: val_loss did not improve from 29.24945
196/196 - 42s - loss: 29.1260 - MinusLogProbMetric: 29.1260 - val_loss: 29.9591 - val_MinusLogProbMetric: 29.9591 - lr: 0.0010 - 42s/epoch - 212ms/step
Epoch 189/1000
2023-10-25 16:44:23.087 
Epoch 189/1000 
	 loss: 29.0160, MinusLogProbMetric: 29.0160, val_loss: 29.4190, val_MinusLogProbMetric: 29.4190

Epoch 189: val_loss did not improve from 29.24945
196/196 - 43s - loss: 29.0160 - MinusLogProbMetric: 29.0160 - val_loss: 29.4190 - val_MinusLogProbMetric: 29.4190 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 190/1000
2023-10-25 16:45:06.877 
Epoch 190/1000 
	 loss: 29.0530, MinusLogProbMetric: 29.0530, val_loss: 30.0096, val_MinusLogProbMetric: 30.0096

Epoch 190: val_loss did not improve from 29.24945
196/196 - 44s - loss: 29.0530 - MinusLogProbMetric: 29.0530 - val_loss: 30.0096 - val_MinusLogProbMetric: 30.0096 - lr: 0.0010 - 44s/epoch - 223ms/step
Epoch 191/1000
2023-10-25 16:45:49.953 
Epoch 191/1000 
	 loss: 29.0658, MinusLogProbMetric: 29.0658, val_loss: 30.2775, val_MinusLogProbMetric: 30.2775

Epoch 191: val_loss did not improve from 29.24945
196/196 - 43s - loss: 29.0658 - MinusLogProbMetric: 29.0658 - val_loss: 30.2775 - val_MinusLogProbMetric: 30.2775 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 192/1000
2023-10-25 16:46:34.181 
Epoch 192/1000 
	 loss: 29.1417, MinusLogProbMetric: 29.1417, val_loss: 29.3448, val_MinusLogProbMetric: 29.3448

Epoch 192: val_loss did not improve from 29.24945
196/196 - 44s - loss: 29.1417 - MinusLogProbMetric: 29.1417 - val_loss: 29.3448 - val_MinusLogProbMetric: 29.3448 - lr: 0.0010 - 44s/epoch - 226ms/step
Epoch 193/1000
2023-10-25 16:47:18.393 
Epoch 193/1000 
	 loss: 29.1492, MinusLogProbMetric: 29.1492, val_loss: 29.6002, val_MinusLogProbMetric: 29.6002

Epoch 193: val_loss did not improve from 29.24945
196/196 - 44s - loss: 29.1492 - MinusLogProbMetric: 29.1492 - val_loss: 29.6002 - val_MinusLogProbMetric: 29.6002 - lr: 0.0010 - 44s/epoch - 226ms/step
Epoch 194/1000
2023-10-25 16:48:02.931 
Epoch 194/1000 
	 loss: 28.9381, MinusLogProbMetric: 28.9381, val_loss: 30.2413, val_MinusLogProbMetric: 30.2413

Epoch 194: val_loss did not improve from 29.24945
196/196 - 45s - loss: 28.9381 - MinusLogProbMetric: 28.9381 - val_loss: 30.2413 - val_MinusLogProbMetric: 30.2413 - lr: 0.0010 - 45s/epoch - 227ms/step
Epoch 195/1000
2023-10-25 16:48:46.944 
Epoch 195/1000 
	 loss: 28.9732, MinusLogProbMetric: 28.9732, val_loss: 29.6873, val_MinusLogProbMetric: 29.6873

Epoch 195: val_loss did not improve from 29.24945
196/196 - 44s - loss: 28.9732 - MinusLogProbMetric: 28.9732 - val_loss: 29.6873 - val_MinusLogProbMetric: 29.6873 - lr: 0.0010 - 44s/epoch - 225ms/step
Epoch 196/1000
2023-10-25 16:49:31.643 
Epoch 196/1000 
	 loss: 28.9657, MinusLogProbMetric: 28.9657, val_loss: 29.9442, val_MinusLogProbMetric: 29.9442

Epoch 196: val_loss did not improve from 29.24945
196/196 - 45s - loss: 28.9657 - MinusLogProbMetric: 28.9657 - val_loss: 29.9442 - val_MinusLogProbMetric: 29.9442 - lr: 0.0010 - 45s/epoch - 228ms/step
Epoch 197/1000
2023-10-25 16:50:15.832 
Epoch 197/1000 
	 loss: 29.0105, MinusLogProbMetric: 29.0105, val_loss: 29.4008, val_MinusLogProbMetric: 29.4008

Epoch 197: val_loss did not improve from 29.24945
196/196 - 44s - loss: 29.0105 - MinusLogProbMetric: 29.0105 - val_loss: 29.4008 - val_MinusLogProbMetric: 29.4008 - lr: 0.0010 - 44s/epoch - 225ms/step
Epoch 198/1000
2023-10-25 16:50:57.590 
Epoch 198/1000 
	 loss: 29.0009, MinusLogProbMetric: 29.0009, val_loss: 29.7199, val_MinusLogProbMetric: 29.7199

Epoch 198: val_loss did not improve from 29.24945
196/196 - 42s - loss: 29.0009 - MinusLogProbMetric: 29.0009 - val_loss: 29.7199 - val_MinusLogProbMetric: 29.7199 - lr: 0.0010 - 42s/epoch - 213ms/step
Epoch 199/1000
2023-10-25 16:51:38.971 
Epoch 199/1000 
	 loss: 28.9866, MinusLogProbMetric: 28.9866, val_loss: 29.2630, val_MinusLogProbMetric: 29.2630

Epoch 199: val_loss did not improve from 29.24945
196/196 - 41s - loss: 28.9866 - MinusLogProbMetric: 28.9866 - val_loss: 29.2630 - val_MinusLogProbMetric: 29.2630 - lr: 0.0010 - 41s/epoch - 211ms/step
Epoch 200/1000
2023-10-25 16:52:18.959 
Epoch 200/1000 
	 loss: 29.0485, MinusLogProbMetric: 29.0485, val_loss: 29.3460, val_MinusLogProbMetric: 29.3460

Epoch 200: val_loss did not improve from 29.24945
196/196 - 40s - loss: 29.0485 - MinusLogProbMetric: 29.0485 - val_loss: 29.3460 - val_MinusLogProbMetric: 29.3460 - lr: 0.0010 - 40s/epoch - 204ms/step
Epoch 201/1000
2023-10-25 16:53:01.797 
Epoch 201/1000 
	 loss: 28.8118, MinusLogProbMetric: 28.8118, val_loss: 29.5008, val_MinusLogProbMetric: 29.5008

Epoch 201: val_loss did not improve from 29.24945
196/196 - 43s - loss: 28.8118 - MinusLogProbMetric: 28.8118 - val_loss: 29.5008 - val_MinusLogProbMetric: 29.5008 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 202/1000
2023-10-25 16:53:46.239 
Epoch 202/1000 
	 loss: 28.9030, MinusLogProbMetric: 28.9030, val_loss: 30.8350, val_MinusLogProbMetric: 30.8350

Epoch 202: val_loss did not improve from 29.24945
196/196 - 44s - loss: 28.9030 - MinusLogProbMetric: 28.9030 - val_loss: 30.8350 - val_MinusLogProbMetric: 30.8350 - lr: 0.0010 - 44s/epoch - 227ms/step
Epoch 203/1000
2023-10-25 16:54:30.632 
Epoch 203/1000 
	 loss: 29.0349, MinusLogProbMetric: 29.0349, val_loss: 29.4562, val_MinusLogProbMetric: 29.4562

Epoch 203: val_loss did not improve from 29.24945
196/196 - 44s - loss: 29.0349 - MinusLogProbMetric: 29.0349 - val_loss: 29.4562 - val_MinusLogProbMetric: 29.4562 - lr: 0.0010 - 44s/epoch - 226ms/step
Epoch 204/1000
2023-10-25 16:55:15.012 
Epoch 204/1000 
	 loss: 28.8945, MinusLogProbMetric: 28.8945, val_loss: 29.6039, val_MinusLogProbMetric: 29.6039

Epoch 204: val_loss did not improve from 29.24945
196/196 - 44s - loss: 28.8945 - MinusLogProbMetric: 28.8945 - val_loss: 29.6039 - val_MinusLogProbMetric: 29.6039 - lr: 0.0010 - 44s/epoch - 226ms/step
Epoch 205/1000
2023-10-25 16:55:58.420 
Epoch 205/1000 
	 loss: 28.0614, MinusLogProbMetric: 28.0614, val_loss: 28.9152, val_MinusLogProbMetric: 28.9152

Epoch 205: val_loss improved from 29.24945 to 28.91515, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_356/weights/best_weights.h5
196/196 - 44s - loss: 28.0614 - MinusLogProbMetric: 28.0614 - val_loss: 28.9152 - val_MinusLogProbMetric: 28.9152 - lr: 5.0000e-04 - 44s/epoch - 225ms/step
Epoch 206/1000
2023-10-25 16:56:40.046 
Epoch 206/1000 
	 loss: 28.0644, MinusLogProbMetric: 28.0644, val_loss: 28.6594, val_MinusLogProbMetric: 28.6594

Epoch 206: val_loss improved from 28.91515 to 28.65940, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_356/weights/best_weights.h5
196/196 - 42s - loss: 28.0644 - MinusLogProbMetric: 28.0644 - val_loss: 28.6594 - val_MinusLogProbMetric: 28.6594 - lr: 5.0000e-04 - 42s/epoch - 213ms/step
Epoch 207/1000
2023-10-25 16:57:24.280 
Epoch 207/1000 
	 loss: 28.0408, MinusLogProbMetric: 28.0408, val_loss: 28.6766, val_MinusLogProbMetric: 28.6766

Epoch 207: val_loss did not improve from 28.65940
196/196 - 43s - loss: 28.0408 - MinusLogProbMetric: 28.0408 - val_loss: 28.6766 - val_MinusLogProbMetric: 28.6766 - lr: 5.0000e-04 - 43s/epoch - 222ms/step
Epoch 208/1000
2023-10-25 16:58:06.522 
Epoch 208/1000 
	 loss: 28.0603, MinusLogProbMetric: 28.0603, val_loss: 28.7327, val_MinusLogProbMetric: 28.7327

Epoch 208: val_loss did not improve from 28.65940
196/196 - 42s - loss: 28.0603 - MinusLogProbMetric: 28.0603 - val_loss: 28.7327 - val_MinusLogProbMetric: 28.7327 - lr: 5.0000e-04 - 42s/epoch - 215ms/step
Epoch 209/1000
2023-10-25 16:58:50.632 
Epoch 209/1000 
	 loss: 28.0514, MinusLogProbMetric: 28.0514, val_loss: 28.5980, val_MinusLogProbMetric: 28.5980

Epoch 209: val_loss improved from 28.65940 to 28.59798, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_356/weights/best_weights.h5
196/196 - 45s - loss: 28.0514 - MinusLogProbMetric: 28.0514 - val_loss: 28.5980 - val_MinusLogProbMetric: 28.5980 - lr: 5.0000e-04 - 45s/epoch - 229ms/step
Epoch 210/1000
2023-10-25 16:59:35.879 
Epoch 210/1000 
	 loss: 28.0489, MinusLogProbMetric: 28.0489, val_loss: 28.8474, val_MinusLogProbMetric: 28.8474

Epoch 210: val_loss did not improve from 28.59798
196/196 - 44s - loss: 28.0489 - MinusLogProbMetric: 28.0489 - val_loss: 28.8474 - val_MinusLogProbMetric: 28.8474 - lr: 5.0000e-04 - 44s/epoch - 227ms/step
Epoch 211/1000
2023-10-25 17:00:20.251 
Epoch 211/1000 
	 loss: 28.0360, MinusLogProbMetric: 28.0360, val_loss: 28.5813, val_MinusLogProbMetric: 28.5813

Epoch 211: val_loss improved from 28.59798 to 28.58127, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_356/weights/best_weights.h5
196/196 - 45s - loss: 28.0360 - MinusLogProbMetric: 28.0360 - val_loss: 28.5813 - val_MinusLogProbMetric: 28.5813 - lr: 5.0000e-04 - 45s/epoch - 230ms/step
Epoch 212/1000
2023-10-25 17:01:05.562 
Epoch 212/1000 
	 loss: 28.0521, MinusLogProbMetric: 28.0521, val_loss: 28.7329, val_MinusLogProbMetric: 28.7329

Epoch 212: val_loss did not improve from 28.58127
196/196 - 45s - loss: 28.0521 - MinusLogProbMetric: 28.0521 - val_loss: 28.7329 - val_MinusLogProbMetric: 28.7329 - lr: 5.0000e-04 - 45s/epoch - 227ms/step
Epoch 213/1000
2023-10-25 17:01:50.381 
Epoch 213/1000 
	 loss: 28.0119, MinusLogProbMetric: 28.0119, val_loss: 28.6468, val_MinusLogProbMetric: 28.6468

Epoch 213: val_loss did not improve from 28.58127
196/196 - 45s - loss: 28.0119 - MinusLogProbMetric: 28.0119 - val_loss: 28.6468 - val_MinusLogProbMetric: 28.6468 - lr: 5.0000e-04 - 45s/epoch - 229ms/step
Epoch 214/1000
2023-10-25 17:02:35.163 
Epoch 214/1000 
	 loss: 28.1032, MinusLogProbMetric: 28.1032, val_loss: 29.2706, val_MinusLogProbMetric: 29.2706

Epoch 214: val_loss did not improve from 28.58127
196/196 - 45s - loss: 28.1032 - MinusLogProbMetric: 28.1032 - val_loss: 29.2706 - val_MinusLogProbMetric: 29.2706 - lr: 5.0000e-04 - 45s/epoch - 228ms/step
Epoch 215/1000
2023-10-25 17:03:19.956 
Epoch 215/1000 
	 loss: 28.0133, MinusLogProbMetric: 28.0133, val_loss: 29.0126, val_MinusLogProbMetric: 29.0126

Epoch 215: val_loss did not improve from 28.58127
196/196 - 45s - loss: 28.0133 - MinusLogProbMetric: 28.0133 - val_loss: 29.0126 - val_MinusLogProbMetric: 29.0126 - lr: 5.0000e-04 - 45s/epoch - 229ms/step
Epoch 216/1000
2023-10-25 17:04:02.821 
Epoch 216/1000 
	 loss: 28.0381, MinusLogProbMetric: 28.0381, val_loss: 28.7281, val_MinusLogProbMetric: 28.7281

Epoch 216: val_loss did not improve from 28.58127
196/196 - 43s - loss: 28.0381 - MinusLogProbMetric: 28.0381 - val_loss: 28.7281 - val_MinusLogProbMetric: 28.7281 - lr: 5.0000e-04 - 43s/epoch - 219ms/step
Epoch 217/1000
2023-10-25 17:04:45.432 
Epoch 217/1000 
	 loss: 28.0420, MinusLogProbMetric: 28.0420, val_loss: 28.5750, val_MinusLogProbMetric: 28.5750

Epoch 217: val_loss improved from 28.58127 to 28.57503, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_356/weights/best_weights.h5
196/196 - 43s - loss: 28.0420 - MinusLogProbMetric: 28.0420 - val_loss: 28.5750 - val_MinusLogProbMetric: 28.5750 - lr: 5.0000e-04 - 43s/epoch - 221ms/step
Epoch 218/1000
2023-10-25 17:05:30.736 
Epoch 218/1000 
	 loss: 28.0185, MinusLogProbMetric: 28.0185, val_loss: 29.4434, val_MinusLogProbMetric: 29.4434

Epoch 218: val_loss did not improve from 28.57503
196/196 - 45s - loss: 28.0185 - MinusLogProbMetric: 28.0185 - val_loss: 29.4434 - val_MinusLogProbMetric: 29.4434 - lr: 5.0000e-04 - 45s/epoch - 227ms/step
Epoch 219/1000
2023-10-25 17:06:15.316 
Epoch 219/1000 
	 loss: 28.0449, MinusLogProbMetric: 28.0449, val_loss: 28.4993, val_MinusLogProbMetric: 28.4993

Epoch 219: val_loss improved from 28.57503 to 28.49935, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_356/weights/best_weights.h5
196/196 - 45s - loss: 28.0449 - MinusLogProbMetric: 28.0449 - val_loss: 28.4993 - val_MinusLogProbMetric: 28.4993 - lr: 5.0000e-04 - 45s/epoch - 232ms/step
Epoch 220/1000
2023-10-25 17:07:00.553 
Epoch 220/1000 
	 loss: 28.0539, MinusLogProbMetric: 28.0539, val_loss: 28.7750, val_MinusLogProbMetric: 28.7750

Epoch 220: val_loss did not improve from 28.49935
196/196 - 44s - loss: 28.0539 - MinusLogProbMetric: 28.0539 - val_loss: 28.7750 - val_MinusLogProbMetric: 28.7750 - lr: 5.0000e-04 - 44s/epoch - 227ms/step
Epoch 221/1000
2023-10-25 17:07:45.198 
Epoch 221/1000 
	 loss: 27.9635, MinusLogProbMetric: 27.9635, val_loss: 29.2103, val_MinusLogProbMetric: 29.2103

Epoch 221: val_loss did not improve from 28.49935
196/196 - 45s - loss: 27.9635 - MinusLogProbMetric: 27.9635 - val_loss: 29.2103 - val_MinusLogProbMetric: 29.2103 - lr: 5.0000e-04 - 45s/epoch - 228ms/step
Epoch 222/1000
2023-10-25 17:08:29.905 
Epoch 222/1000 
	 loss: 28.0409, MinusLogProbMetric: 28.0409, val_loss: 28.8202, val_MinusLogProbMetric: 28.8202

Epoch 222: val_loss did not improve from 28.49935
196/196 - 45s - loss: 28.0409 - MinusLogProbMetric: 28.0409 - val_loss: 28.8202 - val_MinusLogProbMetric: 28.8202 - lr: 5.0000e-04 - 45s/epoch - 228ms/step
Epoch 223/1000
2023-10-25 17:09:12.965 
Epoch 223/1000 
	 loss: 28.1108, MinusLogProbMetric: 28.1108, val_loss: 28.7297, val_MinusLogProbMetric: 28.7297

Epoch 223: val_loss did not improve from 28.49935
196/196 - 43s - loss: 28.1108 - MinusLogProbMetric: 28.1108 - val_loss: 28.7297 - val_MinusLogProbMetric: 28.7297 - lr: 5.0000e-04 - 43s/epoch - 220ms/step
Epoch 224/1000
2023-10-25 17:09:56.526 
Epoch 224/1000 
	 loss: 27.9331, MinusLogProbMetric: 27.9331, val_loss: 28.7948, val_MinusLogProbMetric: 28.7948

Epoch 224: val_loss did not improve from 28.49935
196/196 - 44s - loss: 27.9331 - MinusLogProbMetric: 27.9331 - val_loss: 28.7948 - val_MinusLogProbMetric: 28.7948 - lr: 5.0000e-04 - 44s/epoch - 222ms/step
Epoch 225/1000
2023-10-25 17:10:38.612 
Epoch 225/1000 
	 loss: 28.0778, MinusLogProbMetric: 28.0778, val_loss: 28.6801, val_MinusLogProbMetric: 28.6801

Epoch 225: val_loss did not improve from 28.49935
196/196 - 42s - loss: 28.0778 - MinusLogProbMetric: 28.0778 - val_loss: 28.6801 - val_MinusLogProbMetric: 28.6801 - lr: 5.0000e-04 - 42s/epoch - 215ms/step
Epoch 226/1000
2023-10-25 17:11:19.477 
Epoch 226/1000 
	 loss: 28.0376, MinusLogProbMetric: 28.0376, val_loss: 28.6139, val_MinusLogProbMetric: 28.6139

Epoch 226: val_loss did not improve from 28.49935
196/196 - 41s - loss: 28.0376 - MinusLogProbMetric: 28.0376 - val_loss: 28.6139 - val_MinusLogProbMetric: 28.6139 - lr: 5.0000e-04 - 41s/epoch - 208ms/step
Epoch 227/1000
2023-10-25 17:12:04.116 
Epoch 227/1000 
	 loss: 28.0396, MinusLogProbMetric: 28.0396, val_loss: 28.8978, val_MinusLogProbMetric: 28.8978

Epoch 227: val_loss did not improve from 28.49935
196/196 - 45s - loss: 28.0396 - MinusLogProbMetric: 28.0396 - val_loss: 28.8978 - val_MinusLogProbMetric: 28.8978 - lr: 5.0000e-04 - 45s/epoch - 228ms/step
Epoch 228/1000
2023-10-25 17:12:48.614 
Epoch 228/1000 
	 loss: 28.0016, MinusLogProbMetric: 28.0016, val_loss: 28.6987, val_MinusLogProbMetric: 28.6987

Epoch 228: val_loss did not improve from 28.49935
196/196 - 44s - loss: 28.0016 - MinusLogProbMetric: 28.0016 - val_loss: 28.6987 - val_MinusLogProbMetric: 28.6987 - lr: 5.0000e-04 - 44s/epoch - 227ms/step
Epoch 229/1000
2023-10-25 17:13:33.365 
Epoch 229/1000 
	 loss: 28.0621, MinusLogProbMetric: 28.0621, val_loss: 28.7652, val_MinusLogProbMetric: 28.7652

Epoch 229: val_loss did not improve from 28.49935
196/196 - 45s - loss: 28.0621 - MinusLogProbMetric: 28.0621 - val_loss: 28.7652 - val_MinusLogProbMetric: 28.7652 - lr: 5.0000e-04 - 45s/epoch - 228ms/step
Epoch 230/1000
2023-10-25 17:14:18.180 
Epoch 230/1000 
	 loss: 27.9717, MinusLogProbMetric: 27.9717, val_loss: 29.1085, val_MinusLogProbMetric: 29.1085

Epoch 230: val_loss did not improve from 28.49935
196/196 - 45s - loss: 27.9717 - MinusLogProbMetric: 27.9717 - val_loss: 29.1085 - val_MinusLogProbMetric: 29.1085 - lr: 5.0000e-04 - 45s/epoch - 229ms/step
Epoch 231/1000
2023-10-25 17:15:02.586 
Epoch 231/1000 
	 loss: 28.0627, MinusLogProbMetric: 28.0627, val_loss: 29.0493, val_MinusLogProbMetric: 29.0493

Epoch 231: val_loss did not improve from 28.49935
196/196 - 44s - loss: 28.0627 - MinusLogProbMetric: 28.0627 - val_loss: 29.0493 - val_MinusLogProbMetric: 29.0493 - lr: 5.0000e-04 - 44s/epoch - 227ms/step
Epoch 232/1000
2023-10-25 17:15:46.969 
Epoch 232/1000 
	 loss: 27.9722, MinusLogProbMetric: 27.9722, val_loss: 28.6046, val_MinusLogProbMetric: 28.6046

Epoch 232: val_loss did not improve from 28.49935
196/196 - 44s - loss: 27.9722 - MinusLogProbMetric: 27.9722 - val_loss: 28.6046 - val_MinusLogProbMetric: 28.6046 - lr: 5.0000e-04 - 44s/epoch - 226ms/step
Epoch 233/1000
2023-10-25 17:16:31.619 
Epoch 233/1000 
	 loss: 27.9827, MinusLogProbMetric: 27.9827, val_loss: 28.6429, val_MinusLogProbMetric: 28.6429

Epoch 233: val_loss did not improve from 28.49935
196/196 - 45s - loss: 27.9827 - MinusLogProbMetric: 27.9827 - val_loss: 28.6429 - val_MinusLogProbMetric: 28.6429 - lr: 5.0000e-04 - 45s/epoch - 228ms/step
Epoch 234/1000
2023-10-25 17:17:16.278 
Epoch 234/1000 
	 loss: 28.0120, MinusLogProbMetric: 28.0120, val_loss: 28.7325, val_MinusLogProbMetric: 28.7325

Epoch 234: val_loss did not improve from 28.49935
196/196 - 45s - loss: 28.0120 - MinusLogProbMetric: 28.0120 - val_loss: 28.7325 - val_MinusLogProbMetric: 28.7325 - lr: 5.0000e-04 - 45s/epoch - 228ms/step
Epoch 235/1000
2023-10-25 17:18:00.829 
Epoch 235/1000 
	 loss: 27.9611, MinusLogProbMetric: 27.9611, val_loss: 28.7231, val_MinusLogProbMetric: 28.7231

Epoch 235: val_loss did not improve from 28.49935
196/196 - 45s - loss: 27.9611 - MinusLogProbMetric: 27.9611 - val_loss: 28.7231 - val_MinusLogProbMetric: 28.7231 - lr: 5.0000e-04 - 45s/epoch - 227ms/step
Epoch 236/1000
2023-10-25 17:18:45.149 
Epoch 236/1000 
	 loss: 27.9965, MinusLogProbMetric: 27.9965, val_loss: 28.9048, val_MinusLogProbMetric: 28.9048

Epoch 236: val_loss did not improve from 28.49935
196/196 - 44s - loss: 27.9965 - MinusLogProbMetric: 27.9965 - val_loss: 28.9048 - val_MinusLogProbMetric: 28.9048 - lr: 5.0000e-04 - 44s/epoch - 226ms/step
Epoch 237/1000
2023-10-25 17:19:29.779 
Epoch 237/1000 
	 loss: 27.9522, MinusLogProbMetric: 27.9522, val_loss: 28.9284, val_MinusLogProbMetric: 28.9284

Epoch 237: val_loss did not improve from 28.49935
196/196 - 45s - loss: 27.9522 - MinusLogProbMetric: 27.9522 - val_loss: 28.9284 - val_MinusLogProbMetric: 28.9284 - lr: 5.0000e-04 - 45s/epoch - 228ms/step
Epoch 238/1000
2023-10-25 17:20:14.195 
Epoch 238/1000 
	 loss: 27.9846, MinusLogProbMetric: 27.9846, val_loss: 28.6930, val_MinusLogProbMetric: 28.6930

Epoch 238: val_loss did not improve from 28.49935
196/196 - 44s - loss: 27.9846 - MinusLogProbMetric: 27.9846 - val_loss: 28.6930 - val_MinusLogProbMetric: 28.6930 - lr: 5.0000e-04 - 44s/epoch - 227ms/step
Epoch 239/1000
2023-10-25 17:20:58.789 
Epoch 239/1000 
	 loss: 27.9724, MinusLogProbMetric: 27.9724, val_loss: 28.7746, val_MinusLogProbMetric: 28.7746

Epoch 239: val_loss did not improve from 28.49935
196/196 - 45s - loss: 27.9724 - MinusLogProbMetric: 27.9724 - val_loss: 28.7746 - val_MinusLogProbMetric: 28.7746 - lr: 5.0000e-04 - 45s/epoch - 228ms/step
Epoch 240/1000
2023-10-25 17:21:42.962 
Epoch 240/1000 
	 loss: 28.0396, MinusLogProbMetric: 28.0396, val_loss: 28.9314, val_MinusLogProbMetric: 28.9314

Epoch 240: val_loss did not improve from 28.49935
196/196 - 44s - loss: 28.0396 - MinusLogProbMetric: 28.0396 - val_loss: 28.9314 - val_MinusLogProbMetric: 28.9314 - lr: 5.0000e-04 - 44s/epoch - 225ms/step
Epoch 241/1000
2023-10-25 17:22:27.422 
Epoch 241/1000 
	 loss: 27.9719, MinusLogProbMetric: 27.9719, val_loss: 28.6208, val_MinusLogProbMetric: 28.6208

Epoch 241: val_loss did not improve from 28.49935
196/196 - 44s - loss: 27.9719 - MinusLogProbMetric: 27.9719 - val_loss: 28.6208 - val_MinusLogProbMetric: 28.6208 - lr: 5.0000e-04 - 44s/epoch - 227ms/step
Epoch 242/1000
2023-10-25 17:23:12.345 
Epoch 242/1000 
	 loss: 28.0156, MinusLogProbMetric: 28.0156, val_loss: 29.2501, val_MinusLogProbMetric: 29.2501

Epoch 242: val_loss did not improve from 28.49935
196/196 - 45s - loss: 28.0156 - MinusLogProbMetric: 28.0156 - val_loss: 29.2501 - val_MinusLogProbMetric: 29.2501 - lr: 5.0000e-04 - 45s/epoch - 229ms/step
Epoch 243/1000
2023-10-25 17:23:57.103 
Epoch 243/1000 
	 loss: 27.9359, MinusLogProbMetric: 27.9359, val_loss: 30.2324, val_MinusLogProbMetric: 30.2324

Epoch 243: val_loss did not improve from 28.49935
196/196 - 45s - loss: 27.9359 - MinusLogProbMetric: 27.9359 - val_loss: 30.2324 - val_MinusLogProbMetric: 30.2324 - lr: 5.0000e-04 - 45s/epoch - 228ms/step
Epoch 244/1000
2023-10-25 17:24:41.533 
Epoch 244/1000 
	 loss: 28.0054, MinusLogProbMetric: 28.0054, val_loss: 28.7160, val_MinusLogProbMetric: 28.7160

Epoch 244: val_loss did not improve from 28.49935
196/196 - 44s - loss: 28.0054 - MinusLogProbMetric: 28.0054 - val_loss: 28.7160 - val_MinusLogProbMetric: 28.7160 - lr: 5.0000e-04 - 44s/epoch - 227ms/step
Epoch 245/1000
2023-10-25 17:25:25.898 
Epoch 245/1000 
	 loss: 27.9637, MinusLogProbMetric: 27.9637, val_loss: 28.7390, val_MinusLogProbMetric: 28.7390

Epoch 245: val_loss did not improve from 28.49935
196/196 - 44s - loss: 27.9637 - MinusLogProbMetric: 27.9637 - val_loss: 28.7390 - val_MinusLogProbMetric: 28.7390 - lr: 5.0000e-04 - 44s/epoch - 226ms/step
Epoch 246/1000
2023-10-25 17:26:10.810 
Epoch 246/1000 
	 loss: 28.0543, MinusLogProbMetric: 28.0543, val_loss: 29.6507, val_MinusLogProbMetric: 29.6507

Epoch 246: val_loss did not improve from 28.49935
196/196 - 45s - loss: 28.0543 - MinusLogProbMetric: 28.0543 - val_loss: 29.6507 - val_MinusLogProbMetric: 29.6507 - lr: 5.0000e-04 - 45s/epoch - 229ms/step
Epoch 247/1000
2023-10-25 17:26:55.672 
Epoch 247/1000 
	 loss: 27.9815, MinusLogProbMetric: 27.9815, val_loss: 28.8277, val_MinusLogProbMetric: 28.8277

Epoch 247: val_loss did not improve from 28.49935
196/196 - 45s - loss: 27.9815 - MinusLogProbMetric: 27.9815 - val_loss: 28.8277 - val_MinusLogProbMetric: 28.8277 - lr: 5.0000e-04 - 45s/epoch - 229ms/step
Epoch 248/1000
2023-10-25 17:27:40.306 
Epoch 248/1000 
	 loss: 28.0114, MinusLogProbMetric: 28.0114, val_loss: 28.8512, val_MinusLogProbMetric: 28.8512

Epoch 248: val_loss did not improve from 28.49935
196/196 - 45s - loss: 28.0114 - MinusLogProbMetric: 28.0114 - val_loss: 28.8512 - val_MinusLogProbMetric: 28.8512 - lr: 5.0000e-04 - 45s/epoch - 228ms/step
Epoch 249/1000
2023-10-25 17:28:25.171 
Epoch 249/1000 
	 loss: 27.9462, MinusLogProbMetric: 27.9462, val_loss: 28.8557, val_MinusLogProbMetric: 28.8557

Epoch 249: val_loss did not improve from 28.49935
196/196 - 45s - loss: 27.9462 - MinusLogProbMetric: 27.9462 - val_loss: 28.8557 - val_MinusLogProbMetric: 28.8557 - lr: 5.0000e-04 - 45s/epoch - 229ms/step
Epoch 250/1000
2023-10-25 17:29:09.966 
Epoch 250/1000 
	 loss: 28.0035, MinusLogProbMetric: 28.0035, val_loss: 28.5552, val_MinusLogProbMetric: 28.5552

Epoch 250: val_loss did not improve from 28.49935
196/196 - 45s - loss: 28.0035 - MinusLogProbMetric: 28.0035 - val_loss: 28.5552 - val_MinusLogProbMetric: 28.5552 - lr: 5.0000e-04 - 45s/epoch - 229ms/step
Epoch 251/1000
2023-10-25 17:29:54.340 
Epoch 251/1000 
	 loss: 27.9617, MinusLogProbMetric: 27.9617, val_loss: 28.6152, val_MinusLogProbMetric: 28.6152

Epoch 251: val_loss did not improve from 28.49935
196/196 - 44s - loss: 27.9617 - MinusLogProbMetric: 27.9617 - val_loss: 28.6152 - val_MinusLogProbMetric: 28.6152 - lr: 5.0000e-04 - 44s/epoch - 226ms/step
Epoch 252/1000
2023-10-25 17:30:38.832 
Epoch 252/1000 
	 loss: 28.0213, MinusLogProbMetric: 28.0213, val_loss: 28.8284, val_MinusLogProbMetric: 28.8284

Epoch 252: val_loss did not improve from 28.49935
196/196 - 44s - loss: 28.0213 - MinusLogProbMetric: 28.0213 - val_loss: 28.8284 - val_MinusLogProbMetric: 28.8284 - lr: 5.0000e-04 - 44s/epoch - 227ms/step
Epoch 253/1000
2023-10-25 17:31:23.556 
Epoch 253/1000 
	 loss: 27.9194, MinusLogProbMetric: 27.9194, val_loss: 28.7977, val_MinusLogProbMetric: 28.7977

Epoch 253: val_loss did not improve from 28.49935
196/196 - 45s - loss: 27.9194 - MinusLogProbMetric: 27.9194 - val_loss: 28.7977 - val_MinusLogProbMetric: 28.7977 - lr: 5.0000e-04 - 45s/epoch - 228ms/step
Epoch 254/1000
2023-10-25 17:32:08.051 
Epoch 254/1000 
	 loss: 27.9778, MinusLogProbMetric: 27.9778, val_loss: 28.5195, val_MinusLogProbMetric: 28.5195

Epoch 254: val_loss did not improve from 28.49935
196/196 - 44s - loss: 27.9778 - MinusLogProbMetric: 27.9778 - val_loss: 28.5195 - val_MinusLogProbMetric: 28.5195 - lr: 5.0000e-04 - 44s/epoch - 227ms/step
Epoch 255/1000
2023-10-25 17:32:52.676 
Epoch 255/1000 
	 loss: 27.9330, MinusLogProbMetric: 27.9330, val_loss: 28.6059, val_MinusLogProbMetric: 28.6059

Epoch 255: val_loss did not improve from 28.49935
196/196 - 45s - loss: 27.9330 - MinusLogProbMetric: 27.9330 - val_loss: 28.6059 - val_MinusLogProbMetric: 28.6059 - lr: 5.0000e-04 - 45s/epoch - 228ms/step
Epoch 256/1000
2023-10-25 17:33:37.232 
Epoch 256/1000 
	 loss: 27.9773, MinusLogProbMetric: 27.9773, val_loss: 28.5779, val_MinusLogProbMetric: 28.5779

Epoch 256: val_loss did not improve from 28.49935
196/196 - 45s - loss: 27.9773 - MinusLogProbMetric: 27.9773 - val_loss: 28.5779 - val_MinusLogProbMetric: 28.5779 - lr: 5.0000e-04 - 45s/epoch - 227ms/step
Epoch 257/1000
2023-10-25 17:34:21.763 
Epoch 257/1000 
	 loss: 27.9447, MinusLogProbMetric: 27.9447, val_loss: 28.6581, val_MinusLogProbMetric: 28.6581

Epoch 257: val_loss did not improve from 28.49935
196/196 - 45s - loss: 27.9447 - MinusLogProbMetric: 27.9447 - val_loss: 28.6581 - val_MinusLogProbMetric: 28.6581 - lr: 5.0000e-04 - 45s/epoch - 227ms/step
Epoch 258/1000
2023-10-25 17:35:05.860 
Epoch 258/1000 
	 loss: 27.9702, MinusLogProbMetric: 27.9702, val_loss: 28.7032, val_MinusLogProbMetric: 28.7032

Epoch 258: val_loss did not improve from 28.49935
196/196 - 44s - loss: 27.9702 - MinusLogProbMetric: 27.9702 - val_loss: 28.7032 - val_MinusLogProbMetric: 28.7032 - lr: 5.0000e-04 - 44s/epoch - 225ms/step
Epoch 259/1000
2023-10-25 17:35:50.280 
Epoch 259/1000 
	 loss: 27.9490, MinusLogProbMetric: 27.9490, val_loss: 28.8490, val_MinusLogProbMetric: 28.8490

Epoch 259: val_loss did not improve from 28.49935
196/196 - 44s - loss: 27.9490 - MinusLogProbMetric: 27.9490 - val_loss: 28.8490 - val_MinusLogProbMetric: 28.8490 - lr: 5.0000e-04 - 44s/epoch - 227ms/step
Epoch 260/1000
2023-10-25 17:36:35.242 
Epoch 260/1000 
	 loss: 27.9422, MinusLogProbMetric: 27.9422, val_loss: 29.3649, val_MinusLogProbMetric: 29.3649

Epoch 260: val_loss did not improve from 28.49935
196/196 - 45s - loss: 27.9422 - MinusLogProbMetric: 27.9422 - val_loss: 29.3649 - val_MinusLogProbMetric: 29.3649 - lr: 5.0000e-04 - 45s/epoch - 229ms/step
Epoch 261/1000
2023-10-25 17:37:19.514 
Epoch 261/1000 
	 loss: 27.9687, MinusLogProbMetric: 27.9687, val_loss: 28.7080, val_MinusLogProbMetric: 28.7080

Epoch 261: val_loss did not improve from 28.49935
196/196 - 44s - loss: 27.9687 - MinusLogProbMetric: 27.9687 - val_loss: 28.7080 - val_MinusLogProbMetric: 28.7080 - lr: 5.0000e-04 - 44s/epoch - 226ms/step
Epoch 262/1000
2023-10-25 17:38:03.768 
Epoch 262/1000 
	 loss: 28.0030, MinusLogProbMetric: 28.0030, val_loss: 28.8833, val_MinusLogProbMetric: 28.8833

Epoch 262: val_loss did not improve from 28.49935
196/196 - 44s - loss: 28.0030 - MinusLogProbMetric: 28.0030 - val_loss: 28.8833 - val_MinusLogProbMetric: 28.8833 - lr: 5.0000e-04 - 44s/epoch - 226ms/step
Epoch 263/1000
2023-10-25 17:38:48.445 
Epoch 263/1000 
	 loss: 27.8952, MinusLogProbMetric: 27.8952, val_loss: 28.8608, val_MinusLogProbMetric: 28.8608

Epoch 263: val_loss did not improve from 28.49935
196/196 - 45s - loss: 27.8952 - MinusLogProbMetric: 27.8952 - val_loss: 28.8608 - val_MinusLogProbMetric: 28.8608 - lr: 5.0000e-04 - 45s/epoch - 228ms/step
Epoch 264/1000
2023-10-25 17:39:32.964 
Epoch 264/1000 
	 loss: 27.9896, MinusLogProbMetric: 27.9896, val_loss: 28.9773, val_MinusLogProbMetric: 28.9773

Epoch 264: val_loss did not improve from 28.49935
196/196 - 45s - loss: 27.9896 - MinusLogProbMetric: 27.9896 - val_loss: 28.9773 - val_MinusLogProbMetric: 28.9773 - lr: 5.0000e-04 - 45s/epoch - 227ms/step
Epoch 265/1000
2023-10-25 17:40:17.481 
Epoch 265/1000 
	 loss: 27.9116, MinusLogProbMetric: 27.9116, val_loss: 28.7114, val_MinusLogProbMetric: 28.7114

Epoch 265: val_loss did not improve from 28.49935
196/196 - 45s - loss: 27.9116 - MinusLogProbMetric: 27.9116 - val_loss: 28.7114 - val_MinusLogProbMetric: 28.7114 - lr: 5.0000e-04 - 45s/epoch - 227ms/step
Epoch 266/1000
2023-10-25 17:41:01.989 
Epoch 266/1000 
	 loss: 27.9704, MinusLogProbMetric: 27.9704, val_loss: 28.9365, val_MinusLogProbMetric: 28.9365

Epoch 266: val_loss did not improve from 28.49935
196/196 - 45s - loss: 27.9704 - MinusLogProbMetric: 27.9704 - val_loss: 28.9365 - val_MinusLogProbMetric: 28.9365 - lr: 5.0000e-04 - 45s/epoch - 227ms/step
Epoch 267/1000
2023-10-25 17:41:46.225 
Epoch 267/1000 
	 loss: 27.9673, MinusLogProbMetric: 27.9673, val_loss: 28.7001, val_MinusLogProbMetric: 28.7001

Epoch 267: val_loss did not improve from 28.49935
196/196 - 44s - loss: 27.9673 - MinusLogProbMetric: 27.9673 - val_loss: 28.7001 - val_MinusLogProbMetric: 28.7001 - lr: 5.0000e-04 - 44s/epoch - 226ms/step
Epoch 268/1000
2023-10-25 17:42:30.843 
Epoch 268/1000 
	 loss: 27.9091, MinusLogProbMetric: 27.9091, val_loss: 28.5298, val_MinusLogProbMetric: 28.5298

Epoch 268: val_loss did not improve from 28.49935
196/196 - 45s - loss: 27.9091 - MinusLogProbMetric: 27.9091 - val_loss: 28.5298 - val_MinusLogProbMetric: 28.5298 - lr: 5.0000e-04 - 45s/epoch - 228ms/step
Epoch 269/1000
2023-10-25 17:43:15.569 
Epoch 269/1000 
	 loss: 27.8963, MinusLogProbMetric: 27.8963, val_loss: 28.7155, val_MinusLogProbMetric: 28.7155

Epoch 269: val_loss did not improve from 28.49935
196/196 - 45s - loss: 27.8963 - MinusLogProbMetric: 27.8963 - val_loss: 28.7155 - val_MinusLogProbMetric: 28.7155 - lr: 5.0000e-04 - 45s/epoch - 228ms/step
Epoch 270/1000
2023-10-25 17:44:00.208 
Epoch 270/1000 
	 loss: 27.5897, MinusLogProbMetric: 27.5897, val_loss: 28.4835, val_MinusLogProbMetric: 28.4835

Epoch 270: val_loss improved from 28.49935 to 28.48350, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_356/weights/best_weights.h5
196/196 - 45s - loss: 27.5897 - MinusLogProbMetric: 27.5897 - val_loss: 28.4835 - val_MinusLogProbMetric: 28.4835 - lr: 2.5000e-04 - 45s/epoch - 232ms/step
Epoch 271/1000
2023-10-25 17:44:45.618 
Epoch 271/1000 
	 loss: 27.5785, MinusLogProbMetric: 27.5785, val_loss: 28.3953, val_MinusLogProbMetric: 28.3953

Epoch 271: val_loss improved from 28.48350 to 28.39527, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_356/weights/best_weights.h5
196/196 - 45s - loss: 27.5785 - MinusLogProbMetric: 27.5785 - val_loss: 28.3953 - val_MinusLogProbMetric: 28.3953 - lr: 2.5000e-04 - 45s/epoch - 231ms/step
Epoch 272/1000
2023-10-25 17:45:30.811 
Epoch 272/1000 
	 loss: 27.5899, MinusLogProbMetric: 27.5899, val_loss: 28.4215, val_MinusLogProbMetric: 28.4215

Epoch 272: val_loss did not improve from 28.39527
196/196 - 44s - loss: 27.5899 - MinusLogProbMetric: 27.5899 - val_loss: 28.4215 - val_MinusLogProbMetric: 28.4215 - lr: 2.5000e-04 - 44s/epoch - 227ms/step
Epoch 273/1000
2023-10-25 17:46:15.225 
Epoch 273/1000 
	 loss: 27.5855, MinusLogProbMetric: 27.5855, val_loss: 28.4155, val_MinusLogProbMetric: 28.4155

Epoch 273: val_loss did not improve from 28.39527
196/196 - 44s - loss: 27.5855 - MinusLogProbMetric: 27.5855 - val_loss: 28.4155 - val_MinusLogProbMetric: 28.4155 - lr: 2.5000e-04 - 44s/epoch - 227ms/step
Epoch 274/1000
2023-10-25 17:46:59.590 
Epoch 274/1000 
	 loss: 27.5711, MinusLogProbMetric: 27.5711, val_loss: 28.3694, val_MinusLogProbMetric: 28.3694

Epoch 274: val_loss improved from 28.39527 to 28.36939, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_356/weights/best_weights.h5
196/196 - 45s - loss: 27.5711 - MinusLogProbMetric: 27.5711 - val_loss: 28.3694 - val_MinusLogProbMetric: 28.3694 - lr: 2.5000e-04 - 45s/epoch - 230ms/step
Epoch 275/1000
2023-10-25 17:47:44.903 
Epoch 275/1000 
	 loss: 27.5857, MinusLogProbMetric: 27.5857, val_loss: 28.3804, val_MinusLogProbMetric: 28.3804

Epoch 275: val_loss did not improve from 28.36939
196/196 - 45s - loss: 27.5857 - MinusLogProbMetric: 27.5857 - val_loss: 28.3804 - val_MinusLogProbMetric: 28.3804 - lr: 2.5000e-04 - 45s/epoch - 228ms/step
Epoch 276/1000
2023-10-25 17:48:29.483 
Epoch 276/1000 
	 loss: 27.5894, MinusLogProbMetric: 27.5894, val_loss: 28.3790, val_MinusLogProbMetric: 28.3790

Epoch 276: val_loss did not improve from 28.36939
196/196 - 45s - loss: 27.5894 - MinusLogProbMetric: 27.5894 - val_loss: 28.3790 - val_MinusLogProbMetric: 28.3790 - lr: 2.5000e-04 - 45s/epoch - 227ms/step
Epoch 277/1000
2023-10-25 17:49:14.227 
Epoch 277/1000 
	 loss: 27.5651, MinusLogProbMetric: 27.5651, val_loss: 28.3864, val_MinusLogProbMetric: 28.3864

Epoch 277: val_loss did not improve from 28.36939
196/196 - 45s - loss: 27.5651 - MinusLogProbMetric: 27.5651 - val_loss: 28.3864 - val_MinusLogProbMetric: 28.3864 - lr: 2.5000e-04 - 45s/epoch - 228ms/step
Epoch 278/1000
2023-10-25 17:49:58.442 
Epoch 278/1000 
	 loss: 27.5832, MinusLogProbMetric: 27.5832, val_loss: 28.4121, val_MinusLogProbMetric: 28.4121

Epoch 278: val_loss did not improve from 28.36939
196/196 - 44s - loss: 27.5832 - MinusLogProbMetric: 27.5832 - val_loss: 28.4121 - val_MinusLogProbMetric: 28.4121 - lr: 2.5000e-04 - 44s/epoch - 226ms/step
Epoch 279/1000
2023-10-25 17:50:43.241 
Epoch 279/1000 
	 loss: 27.5841, MinusLogProbMetric: 27.5841, val_loss: 28.4136, val_MinusLogProbMetric: 28.4136

Epoch 279: val_loss did not improve from 28.36939
196/196 - 45s - loss: 27.5841 - MinusLogProbMetric: 27.5841 - val_loss: 28.4136 - val_MinusLogProbMetric: 28.4136 - lr: 2.5000e-04 - 45s/epoch - 229ms/step
Epoch 280/1000
2023-10-25 17:51:27.862 
Epoch 280/1000 
	 loss: 27.5553, MinusLogProbMetric: 27.5553, val_loss: 28.4661, val_MinusLogProbMetric: 28.4661

Epoch 280: val_loss did not improve from 28.36939
196/196 - 45s - loss: 27.5553 - MinusLogProbMetric: 27.5553 - val_loss: 28.4661 - val_MinusLogProbMetric: 28.4661 - lr: 2.5000e-04 - 45s/epoch - 228ms/step
Epoch 281/1000
2023-10-25 17:52:12.344 
Epoch 281/1000 
	 loss: 27.5571, MinusLogProbMetric: 27.5571, val_loss: 28.3843, val_MinusLogProbMetric: 28.3843

Epoch 281: val_loss did not improve from 28.36939
196/196 - 44s - loss: 27.5571 - MinusLogProbMetric: 27.5571 - val_loss: 28.3843 - val_MinusLogProbMetric: 28.3843 - lr: 2.5000e-04 - 44s/epoch - 227ms/step
Epoch 282/1000
2023-10-25 17:52:57.171 
Epoch 282/1000 
	 loss: 27.5765, MinusLogProbMetric: 27.5765, val_loss: 28.3213, val_MinusLogProbMetric: 28.3213

Epoch 282: val_loss improved from 28.36939 to 28.32131, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_356/weights/best_weights.h5
196/196 - 46s - loss: 27.5765 - MinusLogProbMetric: 27.5765 - val_loss: 28.3213 - val_MinusLogProbMetric: 28.3213 - lr: 2.5000e-04 - 46s/epoch - 233ms/step
Epoch 283/1000
2023-10-25 17:53:42.465 
Epoch 283/1000 
	 loss: 27.5694, MinusLogProbMetric: 27.5694, val_loss: 28.4538, val_MinusLogProbMetric: 28.4538

Epoch 283: val_loss did not improve from 28.32131
196/196 - 44s - loss: 27.5694 - MinusLogProbMetric: 27.5694 - val_loss: 28.4538 - val_MinusLogProbMetric: 28.4538 - lr: 2.5000e-04 - 44s/epoch - 227ms/step
Epoch 284/1000
2023-10-25 17:54:26.946 
Epoch 284/1000 
	 loss: 27.5645, MinusLogProbMetric: 27.5645, val_loss: 28.4525, val_MinusLogProbMetric: 28.4525

Epoch 284: val_loss did not improve from 28.32131
196/196 - 44s - loss: 27.5645 - MinusLogProbMetric: 27.5645 - val_loss: 28.4525 - val_MinusLogProbMetric: 28.4525 - lr: 2.5000e-04 - 44s/epoch - 227ms/step
Epoch 285/1000
2023-10-25 17:55:11.390 
Epoch 285/1000 
	 loss: 27.5677, MinusLogProbMetric: 27.5677, val_loss: 28.5837, val_MinusLogProbMetric: 28.5837

Epoch 285: val_loss did not improve from 28.32131
196/196 - 44s - loss: 27.5677 - MinusLogProbMetric: 27.5677 - val_loss: 28.5837 - val_MinusLogProbMetric: 28.5837 - lr: 2.5000e-04 - 44s/epoch - 227ms/step
Epoch 286/1000
2023-10-25 17:55:49.624 
Epoch 286/1000 
	 loss: 27.6016, MinusLogProbMetric: 27.6016, val_loss: 28.3933, val_MinusLogProbMetric: 28.3933

Epoch 286: val_loss did not improve from 28.32131
196/196 - 38s - loss: 27.6016 - MinusLogProbMetric: 27.6016 - val_loss: 28.3933 - val_MinusLogProbMetric: 28.3933 - lr: 2.5000e-04 - 38s/epoch - 195ms/step
Epoch 287/1000
2023-10-25 17:56:28.447 
Epoch 287/1000 
	 loss: 27.5670, MinusLogProbMetric: 27.5670, val_loss: 28.4082, val_MinusLogProbMetric: 28.4082

Epoch 287: val_loss did not improve from 28.32131
196/196 - 39s - loss: 27.5670 - MinusLogProbMetric: 27.5670 - val_loss: 28.4082 - val_MinusLogProbMetric: 28.4082 - lr: 2.5000e-04 - 39s/epoch - 198ms/step
Epoch 288/1000
2023-10-25 17:57:10.657 
Epoch 288/1000 
	 loss: 27.5776, MinusLogProbMetric: 27.5776, val_loss: 28.4299, val_MinusLogProbMetric: 28.4299

Epoch 288: val_loss did not improve from 28.32131
196/196 - 42s - loss: 27.5776 - MinusLogProbMetric: 27.5776 - val_loss: 28.4299 - val_MinusLogProbMetric: 28.4299 - lr: 2.5000e-04 - 42s/epoch - 215ms/step
Epoch 289/1000
2023-10-25 17:57:54.075 
Epoch 289/1000 
	 loss: 27.5718, MinusLogProbMetric: 27.5718, val_loss: 28.3694, val_MinusLogProbMetric: 28.3694

Epoch 289: val_loss did not improve from 28.32131
196/196 - 43s - loss: 27.5718 - MinusLogProbMetric: 27.5718 - val_loss: 28.3694 - val_MinusLogProbMetric: 28.3694 - lr: 2.5000e-04 - 43s/epoch - 222ms/step
Epoch 290/1000
2023-10-25 17:58:34.596 
Epoch 290/1000 
	 loss: 27.5790, MinusLogProbMetric: 27.5790, val_loss: 28.4585, val_MinusLogProbMetric: 28.4585

Epoch 290: val_loss did not improve from 28.32131
196/196 - 41s - loss: 27.5790 - MinusLogProbMetric: 27.5790 - val_loss: 28.4585 - val_MinusLogProbMetric: 28.4585 - lr: 2.5000e-04 - 41s/epoch - 207ms/step
Epoch 291/1000
2023-10-25 17:59:16.873 
Epoch 291/1000 
	 loss: 27.5678, MinusLogProbMetric: 27.5678, val_loss: 28.3818, val_MinusLogProbMetric: 28.3818

Epoch 291: val_loss did not improve from 28.32131
196/196 - 42s - loss: 27.5678 - MinusLogProbMetric: 27.5678 - val_loss: 28.3818 - val_MinusLogProbMetric: 28.3818 - lr: 2.5000e-04 - 42s/epoch - 216ms/step
Epoch 292/1000
2023-10-25 17:59:58.177 
Epoch 292/1000 
	 loss: 27.5738, MinusLogProbMetric: 27.5738, val_loss: 28.5176, val_MinusLogProbMetric: 28.5176

Epoch 292: val_loss did not improve from 28.32131
196/196 - 41s - loss: 27.5738 - MinusLogProbMetric: 27.5738 - val_loss: 28.5176 - val_MinusLogProbMetric: 28.5176 - lr: 2.5000e-04 - 41s/epoch - 211ms/step
Epoch 293/1000
2023-10-25 18:00:41.294 
Epoch 293/1000 
	 loss: 27.5593, MinusLogProbMetric: 27.5593, val_loss: 28.4200, val_MinusLogProbMetric: 28.4200

Epoch 293: val_loss did not improve from 28.32131
196/196 - 43s - loss: 27.5593 - MinusLogProbMetric: 27.5593 - val_loss: 28.4200 - val_MinusLogProbMetric: 28.4200 - lr: 2.5000e-04 - 43s/epoch - 220ms/step
Epoch 294/1000
2023-10-25 18:01:23.331 
Epoch 294/1000 
	 loss: 27.5719, MinusLogProbMetric: 27.5719, val_loss: 28.4598, val_MinusLogProbMetric: 28.4598

Epoch 294: val_loss did not improve from 28.32131
196/196 - 42s - loss: 27.5719 - MinusLogProbMetric: 27.5719 - val_loss: 28.4598 - val_MinusLogProbMetric: 28.4598 - lr: 2.5000e-04 - 42s/epoch - 214ms/step
Epoch 295/1000
2023-10-25 18:02:01.206 
Epoch 295/1000 
	 loss: 27.5803, MinusLogProbMetric: 27.5803, val_loss: 28.5602, val_MinusLogProbMetric: 28.5602

Epoch 295: val_loss did not improve from 28.32131
196/196 - 38s - loss: 27.5803 - MinusLogProbMetric: 27.5803 - val_loss: 28.5602 - val_MinusLogProbMetric: 28.5602 - lr: 2.5000e-04 - 38s/epoch - 193ms/step
Epoch 296/1000
2023-10-25 18:02:40.793 
Epoch 296/1000 
	 loss: 27.5715, MinusLogProbMetric: 27.5715, val_loss: 28.3812, val_MinusLogProbMetric: 28.3812

Epoch 296: val_loss did not improve from 28.32131
196/196 - 40s - loss: 27.5715 - MinusLogProbMetric: 27.5715 - val_loss: 28.3812 - val_MinusLogProbMetric: 28.3812 - lr: 2.5000e-04 - 40s/epoch - 202ms/step
Epoch 297/1000
2023-10-25 18:03:24.659 
Epoch 297/1000 
	 loss: 27.5527, MinusLogProbMetric: 27.5527, val_loss: 28.6453, val_MinusLogProbMetric: 28.6453

Epoch 297: val_loss did not improve from 28.32131
196/196 - 44s - loss: 27.5527 - MinusLogProbMetric: 27.5527 - val_loss: 28.6453 - val_MinusLogProbMetric: 28.6453 - lr: 2.5000e-04 - 44s/epoch - 224ms/step
Epoch 298/1000
2023-10-25 18:04:04.466 
Epoch 298/1000 
	 loss: 27.5739, MinusLogProbMetric: 27.5739, val_loss: 28.5695, val_MinusLogProbMetric: 28.5695

Epoch 298: val_loss did not improve from 28.32131
196/196 - 40s - loss: 27.5739 - MinusLogProbMetric: 27.5739 - val_loss: 28.5695 - val_MinusLogProbMetric: 28.5695 - lr: 2.5000e-04 - 40s/epoch - 203ms/step
Epoch 299/1000
2023-10-25 18:04:43.768 
Epoch 299/1000 
	 loss: 27.5424, MinusLogProbMetric: 27.5424, val_loss: 28.4717, val_MinusLogProbMetric: 28.4717

Epoch 299: val_loss did not improve from 28.32131
196/196 - 39s - loss: 27.5424 - MinusLogProbMetric: 27.5424 - val_loss: 28.4717 - val_MinusLogProbMetric: 28.4717 - lr: 2.5000e-04 - 39s/epoch - 201ms/step
Epoch 300/1000
2023-10-25 18:05:25.880 
Epoch 300/1000 
	 loss: 27.5432, MinusLogProbMetric: 27.5432, val_loss: 28.4622, val_MinusLogProbMetric: 28.4622

Epoch 300: val_loss did not improve from 28.32131
196/196 - 42s - loss: 27.5432 - MinusLogProbMetric: 27.5432 - val_loss: 28.4622 - val_MinusLogProbMetric: 28.4622 - lr: 2.5000e-04 - 42s/epoch - 215ms/step
Epoch 301/1000
2023-10-25 18:06:09.411 
Epoch 301/1000 
	 loss: 27.5613, MinusLogProbMetric: 27.5613, val_loss: 28.6112, val_MinusLogProbMetric: 28.6112

Epoch 301: val_loss did not improve from 28.32131
196/196 - 44s - loss: 27.5613 - MinusLogProbMetric: 27.5613 - val_loss: 28.6112 - val_MinusLogProbMetric: 28.6112 - lr: 2.5000e-04 - 44s/epoch - 222ms/step
Epoch 302/1000
2023-10-25 18:06:52.046 
Epoch 302/1000 
	 loss: 27.5642, MinusLogProbMetric: 27.5642, val_loss: 28.5208, val_MinusLogProbMetric: 28.5208

Epoch 302: val_loss did not improve from 28.32131
196/196 - 43s - loss: 27.5642 - MinusLogProbMetric: 27.5642 - val_loss: 28.5208 - val_MinusLogProbMetric: 28.5208 - lr: 2.5000e-04 - 43s/epoch - 218ms/step
Epoch 303/1000
2023-10-25 18:07:35.507 
Epoch 303/1000 
	 loss: 27.5937, MinusLogProbMetric: 27.5937, val_loss: 28.3685, val_MinusLogProbMetric: 28.3685

Epoch 303: val_loss did not improve from 28.32131
196/196 - 43s - loss: 27.5937 - MinusLogProbMetric: 27.5937 - val_loss: 28.3685 - val_MinusLogProbMetric: 28.3685 - lr: 2.5000e-04 - 43s/epoch - 222ms/step
Epoch 304/1000
2023-10-25 18:08:18.020 
Epoch 304/1000 
	 loss: 27.5460, MinusLogProbMetric: 27.5460, val_loss: 28.3490, val_MinusLogProbMetric: 28.3490

Epoch 304: val_loss did not improve from 28.32131
196/196 - 43s - loss: 27.5460 - MinusLogProbMetric: 27.5460 - val_loss: 28.3490 - val_MinusLogProbMetric: 28.3490 - lr: 2.5000e-04 - 43s/epoch - 217ms/step
Epoch 305/1000
2023-10-25 18:08:59.032 
Epoch 305/1000 
	 loss: 27.5619, MinusLogProbMetric: 27.5619, val_loss: 28.4634, val_MinusLogProbMetric: 28.4634

Epoch 305: val_loss did not improve from 28.32131
196/196 - 41s - loss: 27.5619 - MinusLogProbMetric: 27.5619 - val_loss: 28.4634 - val_MinusLogProbMetric: 28.4634 - lr: 2.5000e-04 - 41s/epoch - 209ms/step
Epoch 306/1000
2023-10-25 18:09:42.268 
Epoch 306/1000 
	 loss: 27.5463, MinusLogProbMetric: 27.5463, val_loss: 28.4020, val_MinusLogProbMetric: 28.4020

Epoch 306: val_loss did not improve from 28.32131
196/196 - 43s - loss: 27.5463 - MinusLogProbMetric: 27.5463 - val_loss: 28.4020 - val_MinusLogProbMetric: 28.4020 - lr: 2.5000e-04 - 43s/epoch - 221ms/step
Epoch 307/1000
2023-10-25 18:10:25.070 
Epoch 307/1000 
	 loss: 27.5517, MinusLogProbMetric: 27.5517, val_loss: 28.4394, val_MinusLogProbMetric: 28.4394

Epoch 307: val_loss did not improve from 28.32131
196/196 - 43s - loss: 27.5517 - MinusLogProbMetric: 27.5517 - val_loss: 28.4394 - val_MinusLogProbMetric: 28.4394 - lr: 2.5000e-04 - 43s/epoch - 218ms/step
Epoch 308/1000
2023-10-25 18:11:04.039 
Epoch 308/1000 
	 loss: 27.5689, MinusLogProbMetric: 27.5689, val_loss: 28.3651, val_MinusLogProbMetric: 28.3651

Epoch 308: val_loss did not improve from 28.32131
196/196 - 39s - loss: 27.5689 - MinusLogProbMetric: 27.5689 - val_loss: 28.3651 - val_MinusLogProbMetric: 28.3651 - lr: 2.5000e-04 - 39s/epoch - 199ms/step
Epoch 309/1000
2023-10-25 18:11:45.594 
Epoch 309/1000 
	 loss: 27.5603, MinusLogProbMetric: 27.5603, val_loss: 28.3758, val_MinusLogProbMetric: 28.3758

Epoch 309: val_loss did not improve from 28.32131
196/196 - 42s - loss: 27.5603 - MinusLogProbMetric: 27.5603 - val_loss: 28.3758 - val_MinusLogProbMetric: 28.3758 - lr: 2.5000e-04 - 42s/epoch - 212ms/step
Epoch 310/1000
2023-10-25 18:12:28.441 
Epoch 310/1000 
	 loss: 27.5272, MinusLogProbMetric: 27.5272, val_loss: 28.3821, val_MinusLogProbMetric: 28.3821

Epoch 310: val_loss did not improve from 28.32131
196/196 - 43s - loss: 27.5272 - MinusLogProbMetric: 27.5272 - val_loss: 28.3821 - val_MinusLogProbMetric: 28.3821 - lr: 2.5000e-04 - 43s/epoch - 219ms/step
Epoch 311/1000
2023-10-25 18:13:06.600 
Epoch 311/1000 
	 loss: 27.5643, MinusLogProbMetric: 27.5643, val_loss: 28.3758, val_MinusLogProbMetric: 28.3758

Epoch 311: val_loss did not improve from 28.32131
196/196 - 38s - loss: 27.5643 - MinusLogProbMetric: 27.5643 - val_loss: 28.3758 - val_MinusLogProbMetric: 28.3758 - lr: 2.5000e-04 - 38s/epoch - 195ms/step
Epoch 312/1000
2023-10-25 18:13:46.895 
Epoch 312/1000 
	 loss: 27.5169, MinusLogProbMetric: 27.5169, val_loss: 28.3952, val_MinusLogProbMetric: 28.3952

Epoch 312: val_loss did not improve from 28.32131
196/196 - 40s - loss: 27.5169 - MinusLogProbMetric: 27.5169 - val_loss: 28.3952 - val_MinusLogProbMetric: 28.3952 - lr: 2.5000e-04 - 40s/epoch - 206ms/step
Epoch 313/1000
2023-10-25 18:14:28.872 
Epoch 313/1000 
	 loss: 27.5497, MinusLogProbMetric: 27.5497, val_loss: 28.4173, val_MinusLogProbMetric: 28.4173

Epoch 313: val_loss did not improve from 28.32131
196/196 - 42s - loss: 27.5497 - MinusLogProbMetric: 27.5497 - val_loss: 28.4173 - val_MinusLogProbMetric: 28.4173 - lr: 2.5000e-04 - 42s/epoch - 214ms/step
Epoch 314/1000
2023-10-25 18:15:12.479 
Epoch 314/1000 
	 loss: 27.5329, MinusLogProbMetric: 27.5329, val_loss: 28.4225, val_MinusLogProbMetric: 28.4225

Epoch 314: val_loss did not improve from 28.32131
196/196 - 44s - loss: 27.5329 - MinusLogProbMetric: 27.5329 - val_loss: 28.4225 - val_MinusLogProbMetric: 28.4225 - lr: 2.5000e-04 - 44s/epoch - 222ms/step
Epoch 315/1000
2023-10-25 18:15:54.361 
Epoch 315/1000 
	 loss: 27.5469, MinusLogProbMetric: 27.5469, val_loss: 28.3801, val_MinusLogProbMetric: 28.3801

Epoch 315: val_loss did not improve from 28.32131
196/196 - 42s - loss: 27.5469 - MinusLogProbMetric: 27.5469 - val_loss: 28.3801 - val_MinusLogProbMetric: 28.3801 - lr: 2.5000e-04 - 42s/epoch - 214ms/step
Epoch 316/1000
2023-10-25 18:16:35.277 
Epoch 316/1000 
	 loss: 27.5392, MinusLogProbMetric: 27.5392, val_loss: 28.4286, val_MinusLogProbMetric: 28.4286

Epoch 316: val_loss did not improve from 28.32131
196/196 - 41s - loss: 27.5392 - MinusLogProbMetric: 27.5392 - val_loss: 28.4286 - val_MinusLogProbMetric: 28.4286 - lr: 2.5000e-04 - 41s/epoch - 209ms/step
Epoch 317/1000
2023-10-25 18:17:15.523 
Epoch 317/1000 
	 loss: 27.5380, MinusLogProbMetric: 27.5380, val_loss: 28.4752, val_MinusLogProbMetric: 28.4752

Epoch 317: val_loss did not improve from 28.32131
196/196 - 40s - loss: 27.5380 - MinusLogProbMetric: 27.5380 - val_loss: 28.4752 - val_MinusLogProbMetric: 28.4752 - lr: 2.5000e-04 - 40s/epoch - 205ms/step
Epoch 318/1000
2023-10-25 18:17:59.472 
Epoch 318/1000 
	 loss: 27.5485, MinusLogProbMetric: 27.5485, val_loss: 28.4115, val_MinusLogProbMetric: 28.4115

Epoch 318: val_loss did not improve from 28.32131
196/196 - 44s - loss: 27.5485 - MinusLogProbMetric: 27.5485 - val_loss: 28.4115 - val_MinusLogProbMetric: 28.4115 - lr: 2.5000e-04 - 44s/epoch - 224ms/step
Epoch 319/1000
2023-10-25 18:18:40.682 
Epoch 319/1000 
	 loss: 27.5389, MinusLogProbMetric: 27.5389, val_loss: 28.7614, val_MinusLogProbMetric: 28.7614

Epoch 319: val_loss did not improve from 28.32131
196/196 - 41s - loss: 27.5389 - MinusLogProbMetric: 27.5389 - val_loss: 28.7614 - val_MinusLogProbMetric: 28.7614 - lr: 2.5000e-04 - 41s/epoch - 210ms/step
Epoch 320/1000
2023-10-25 18:19:22.871 
Epoch 320/1000 
	 loss: 27.5382, MinusLogProbMetric: 27.5382, val_loss: 28.3761, val_MinusLogProbMetric: 28.3761

Epoch 320: val_loss did not improve from 28.32131
196/196 - 42s - loss: 27.5382 - MinusLogProbMetric: 27.5382 - val_loss: 28.3761 - val_MinusLogProbMetric: 28.3761 - lr: 2.5000e-04 - 42s/epoch - 215ms/step
Epoch 321/1000
2023-10-25 18:20:04.478 
Epoch 321/1000 
	 loss: 27.5516, MinusLogProbMetric: 27.5516, val_loss: 28.4233, val_MinusLogProbMetric: 28.4233

Epoch 321: val_loss did not improve from 28.32131
196/196 - 42s - loss: 27.5516 - MinusLogProbMetric: 27.5516 - val_loss: 28.4233 - val_MinusLogProbMetric: 28.4233 - lr: 2.5000e-04 - 42s/epoch - 212ms/step
Epoch 322/1000
2023-10-25 18:20:48.245 
Epoch 322/1000 
	 loss: 27.5629, MinusLogProbMetric: 27.5629, val_loss: 28.4722, val_MinusLogProbMetric: 28.4722

Epoch 322: val_loss did not improve from 28.32131
196/196 - 44s - loss: 27.5629 - MinusLogProbMetric: 27.5629 - val_loss: 28.4722 - val_MinusLogProbMetric: 28.4722 - lr: 2.5000e-04 - 44s/epoch - 223ms/step
Epoch 323/1000
2023-10-25 18:21:30.316 
Epoch 323/1000 
	 loss: 27.5479, MinusLogProbMetric: 27.5479, val_loss: 28.4237, val_MinusLogProbMetric: 28.4237

Epoch 323: val_loss did not improve from 28.32131
196/196 - 42s - loss: 27.5479 - MinusLogProbMetric: 27.5479 - val_loss: 28.4237 - val_MinusLogProbMetric: 28.4237 - lr: 2.5000e-04 - 42s/epoch - 215ms/step
Epoch 324/1000
2023-10-25 18:22:13.143 
Epoch 324/1000 
	 loss: 27.5318, MinusLogProbMetric: 27.5318, val_loss: 28.3591, val_MinusLogProbMetric: 28.3591

Epoch 324: val_loss did not improve from 28.32131
196/196 - 43s - loss: 27.5318 - MinusLogProbMetric: 27.5318 - val_loss: 28.3591 - val_MinusLogProbMetric: 28.3591 - lr: 2.5000e-04 - 43s/epoch - 218ms/step
Epoch 325/1000
2023-10-25 18:22:51.102 
Epoch 325/1000 
	 loss: 27.5577, MinusLogProbMetric: 27.5577, val_loss: 28.3746, val_MinusLogProbMetric: 28.3746

Epoch 325: val_loss did not improve from 28.32131
196/196 - 38s - loss: 27.5577 - MinusLogProbMetric: 27.5577 - val_loss: 28.3746 - val_MinusLogProbMetric: 28.3746 - lr: 2.5000e-04 - 38s/epoch - 194ms/step
Epoch 326/1000
2023-10-25 18:23:33.223 
Epoch 326/1000 
	 loss: 27.5277, MinusLogProbMetric: 27.5277, val_loss: 28.5915, val_MinusLogProbMetric: 28.5915

Epoch 326: val_loss did not improve from 28.32131
196/196 - 42s - loss: 27.5277 - MinusLogProbMetric: 27.5277 - val_loss: 28.5915 - val_MinusLogProbMetric: 28.5915 - lr: 2.5000e-04 - 42s/epoch - 215ms/step
Epoch 327/1000
2023-10-25 18:24:17.337 
Epoch 327/1000 
	 loss: 27.5409, MinusLogProbMetric: 27.5409, val_loss: 28.3706, val_MinusLogProbMetric: 28.3706

Epoch 327: val_loss did not improve from 28.32131
196/196 - 44s - loss: 27.5409 - MinusLogProbMetric: 27.5409 - val_loss: 28.3706 - val_MinusLogProbMetric: 28.3706 - lr: 2.5000e-04 - 44s/epoch - 225ms/step
Epoch 328/1000
2023-10-25 18:24:59.178 
Epoch 328/1000 
	 loss: 27.5406, MinusLogProbMetric: 27.5406, val_loss: 28.4338, val_MinusLogProbMetric: 28.4338

Epoch 328: val_loss did not improve from 28.32131
196/196 - 42s - loss: 27.5406 - MinusLogProbMetric: 27.5406 - val_loss: 28.4338 - val_MinusLogProbMetric: 28.4338 - lr: 2.5000e-04 - 42s/epoch - 213ms/step
Epoch 329/1000
2023-10-25 18:25:38.822 
Epoch 329/1000 
	 loss: 27.5391, MinusLogProbMetric: 27.5391, val_loss: 28.5300, val_MinusLogProbMetric: 28.5300

Epoch 329: val_loss did not improve from 28.32131
196/196 - 40s - loss: 27.5391 - MinusLogProbMetric: 27.5391 - val_loss: 28.5300 - val_MinusLogProbMetric: 28.5300 - lr: 2.5000e-04 - 40s/epoch - 202ms/step
Epoch 330/1000
2023-10-25 18:26:20.777 
Epoch 330/1000 
	 loss: 27.5640, MinusLogProbMetric: 27.5640, val_loss: 28.4386, val_MinusLogProbMetric: 28.4386

Epoch 330: val_loss did not improve from 28.32131
196/196 - 42s - loss: 27.5640 - MinusLogProbMetric: 27.5640 - val_loss: 28.4386 - val_MinusLogProbMetric: 28.4386 - lr: 2.5000e-04 - 42s/epoch - 214ms/step
Epoch 331/1000
2023-10-25 18:27:04.748 
Epoch 331/1000 
	 loss: 27.5301, MinusLogProbMetric: 27.5301, val_loss: 28.3706, val_MinusLogProbMetric: 28.3706

Epoch 331: val_loss did not improve from 28.32131
196/196 - 44s - loss: 27.5301 - MinusLogProbMetric: 27.5301 - val_loss: 28.3706 - val_MinusLogProbMetric: 28.3706 - lr: 2.5000e-04 - 44s/epoch - 224ms/step
Epoch 332/1000
2023-10-25 18:27:44.018 
Epoch 332/1000 
	 loss: 27.5336, MinusLogProbMetric: 27.5336, val_loss: 28.4699, val_MinusLogProbMetric: 28.4699

Epoch 332: val_loss did not improve from 28.32131
196/196 - 39s - loss: 27.5336 - MinusLogProbMetric: 27.5336 - val_loss: 28.4699 - val_MinusLogProbMetric: 28.4699 - lr: 2.5000e-04 - 39s/epoch - 200ms/step
Epoch 333/1000
2023-10-25 18:28:23.546 
Epoch 333/1000 
	 loss: 27.4219, MinusLogProbMetric: 27.4219, val_loss: 28.3088, val_MinusLogProbMetric: 28.3088

Epoch 333: val_loss improved from 28.32131 to 28.30883, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_356/weights/best_weights.h5
196/196 - 40s - loss: 27.4219 - MinusLogProbMetric: 27.4219 - val_loss: 28.3088 - val_MinusLogProbMetric: 28.3088 - lr: 1.2500e-04 - 40s/epoch - 206ms/step
Epoch 334/1000
2023-10-25 18:29:07.151 
Epoch 334/1000 
	 loss: 27.4065, MinusLogProbMetric: 27.4065, val_loss: 28.3572, val_MinusLogProbMetric: 28.3572

Epoch 334: val_loss did not improve from 28.30883
196/196 - 43s - loss: 27.4065 - MinusLogProbMetric: 27.4065 - val_loss: 28.3572 - val_MinusLogProbMetric: 28.3572 - lr: 1.2500e-04 - 43s/epoch - 218ms/step
Epoch 335/1000
2023-10-25 18:29:50.802 
Epoch 335/1000 
	 loss: 27.4098, MinusLogProbMetric: 27.4098, val_loss: 28.3224, val_MinusLogProbMetric: 28.3224

Epoch 335: val_loss did not improve from 28.30883
196/196 - 44s - loss: 27.4098 - MinusLogProbMetric: 27.4098 - val_loss: 28.3224 - val_MinusLogProbMetric: 28.3224 - lr: 1.2500e-04 - 44s/epoch - 223ms/step
Epoch 336/1000
2023-10-25 18:30:30.294 
Epoch 336/1000 
	 loss: 27.4082, MinusLogProbMetric: 27.4082, val_loss: 28.3133, val_MinusLogProbMetric: 28.3133

Epoch 336: val_loss did not improve from 28.30883
196/196 - 39s - loss: 27.4082 - MinusLogProbMetric: 27.4082 - val_loss: 28.3133 - val_MinusLogProbMetric: 28.3133 - lr: 1.2500e-04 - 39s/epoch - 201ms/step
Epoch 337/1000
2023-10-25 18:31:10.616 
Epoch 337/1000 
	 loss: 27.4121, MinusLogProbMetric: 27.4121, val_loss: 28.2807, val_MinusLogProbMetric: 28.2807

Epoch 337: val_loss improved from 28.30883 to 28.28068, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_356/weights/best_weights.h5
196/196 - 41s - loss: 27.4121 - MinusLogProbMetric: 27.4121 - val_loss: 28.2807 - val_MinusLogProbMetric: 28.2807 - lr: 1.2500e-04 - 41s/epoch - 209ms/step
Epoch 338/1000
2023-10-25 18:31:51.846 
Epoch 338/1000 
	 loss: 27.4046, MinusLogProbMetric: 27.4046, val_loss: 28.3105, val_MinusLogProbMetric: 28.3105

Epoch 338: val_loss did not improve from 28.28068
196/196 - 41s - loss: 27.4046 - MinusLogProbMetric: 27.4046 - val_loss: 28.3105 - val_MinusLogProbMetric: 28.3105 - lr: 1.2500e-04 - 41s/epoch - 207ms/step
Epoch 339/1000
2023-10-25 18:32:35.799 
Epoch 339/1000 
	 loss: 27.4071, MinusLogProbMetric: 27.4071, val_loss: 28.3702, val_MinusLogProbMetric: 28.3702

Epoch 339: val_loss did not improve from 28.28068
196/196 - 44s - loss: 27.4071 - MinusLogProbMetric: 27.4071 - val_loss: 28.3702 - val_MinusLogProbMetric: 28.3702 - lr: 1.2500e-04 - 44s/epoch - 224ms/step
Epoch 340/1000
2023-10-25 18:33:19.081 
Epoch 340/1000 
	 loss: 27.4035, MinusLogProbMetric: 27.4035, val_loss: 28.3885, val_MinusLogProbMetric: 28.3885

Epoch 340: val_loss did not improve from 28.28068
196/196 - 43s - loss: 27.4035 - MinusLogProbMetric: 27.4035 - val_loss: 28.3885 - val_MinusLogProbMetric: 28.3885 - lr: 1.2500e-04 - 43s/epoch - 221ms/step
Epoch 341/1000
2023-10-25 18:34:01.532 
Epoch 341/1000 
	 loss: 27.4058, MinusLogProbMetric: 27.4058, val_loss: 28.2857, val_MinusLogProbMetric: 28.2857

Epoch 341: val_loss did not improve from 28.28068
196/196 - 42s - loss: 27.4058 - MinusLogProbMetric: 27.4058 - val_loss: 28.2857 - val_MinusLogProbMetric: 28.2857 - lr: 1.2500e-04 - 42s/epoch - 217ms/step
Epoch 342/1000
2023-10-25 18:34:41.990 
Epoch 342/1000 
	 loss: 27.4095, MinusLogProbMetric: 27.4095, val_loss: 28.3220, val_MinusLogProbMetric: 28.3220

Epoch 342: val_loss did not improve from 28.28068
196/196 - 40s - loss: 27.4095 - MinusLogProbMetric: 27.4095 - val_loss: 28.3220 - val_MinusLogProbMetric: 28.3220 - lr: 1.2500e-04 - 40s/epoch - 206ms/step
Epoch 343/1000
2023-10-25 18:35:26.402 
Epoch 343/1000 
	 loss: 27.3992, MinusLogProbMetric: 27.3992, val_loss: 28.2938, val_MinusLogProbMetric: 28.2938

Epoch 343: val_loss did not improve from 28.28068
196/196 - 44s - loss: 27.3992 - MinusLogProbMetric: 27.3992 - val_loss: 28.2938 - val_MinusLogProbMetric: 28.2938 - lr: 1.2500e-04 - 44s/epoch - 227ms/step
Epoch 344/1000
2023-10-25 18:36:10.216 
Epoch 344/1000 
	 loss: 27.3976, MinusLogProbMetric: 27.3976, val_loss: 28.3408, val_MinusLogProbMetric: 28.3408

Epoch 344: val_loss did not improve from 28.28068
196/196 - 44s - loss: 27.3976 - MinusLogProbMetric: 27.3976 - val_loss: 28.3408 - val_MinusLogProbMetric: 28.3408 - lr: 1.2500e-04 - 44s/epoch - 224ms/step
Epoch 345/1000
2023-10-25 18:36:51.216 
Epoch 345/1000 
	 loss: 27.4008, MinusLogProbMetric: 27.4008, val_loss: 28.2830, val_MinusLogProbMetric: 28.2830

Epoch 345: val_loss did not improve from 28.28068
196/196 - 41s - loss: 27.4008 - MinusLogProbMetric: 27.4008 - val_loss: 28.2830 - val_MinusLogProbMetric: 28.2830 - lr: 1.2500e-04 - 41s/epoch - 209ms/step
Epoch 346/1000
2023-10-25 18:37:30.380 
Epoch 346/1000 
	 loss: 27.4040, MinusLogProbMetric: 27.4040, val_loss: 28.3055, val_MinusLogProbMetric: 28.3055

Epoch 346: val_loss did not improve from 28.28068
196/196 - 39s - loss: 27.4040 - MinusLogProbMetric: 27.4040 - val_loss: 28.3055 - val_MinusLogProbMetric: 28.3055 - lr: 1.2500e-04 - 39s/epoch - 200ms/step
Epoch 347/1000
2023-10-25 18:38:13.400 
Epoch 347/1000 
	 loss: 27.4054, MinusLogProbMetric: 27.4054, val_loss: 28.3268, val_MinusLogProbMetric: 28.3268

Epoch 347: val_loss did not improve from 28.28068
196/196 - 43s - loss: 27.4054 - MinusLogProbMetric: 27.4054 - val_loss: 28.3268 - val_MinusLogProbMetric: 28.3268 - lr: 1.2500e-04 - 43s/epoch - 219ms/step
Epoch 348/1000
2023-10-25 18:38:57.109 
Epoch 348/1000 
	 loss: 27.4009, MinusLogProbMetric: 27.4009, val_loss: 28.3304, val_MinusLogProbMetric: 28.3304

Epoch 348: val_loss did not improve from 28.28068
196/196 - 44s - loss: 27.4009 - MinusLogProbMetric: 27.4009 - val_loss: 28.3304 - val_MinusLogProbMetric: 28.3304 - lr: 1.2500e-04 - 44s/epoch - 223ms/step
Epoch 349/1000
2023-10-25 18:39:39.027 
Epoch 349/1000 
	 loss: 27.3954, MinusLogProbMetric: 27.3954, val_loss: 28.3510, val_MinusLogProbMetric: 28.3510

Epoch 349: val_loss did not improve from 28.28068
196/196 - 42s - loss: 27.3954 - MinusLogProbMetric: 27.3954 - val_loss: 28.3510 - val_MinusLogProbMetric: 28.3510 - lr: 1.2500e-04 - 42s/epoch - 214ms/step
Epoch 350/1000
2023-10-25 18:40:20.955 
Epoch 350/1000 
	 loss: 27.4111, MinusLogProbMetric: 27.4111, val_loss: 28.3163, val_MinusLogProbMetric: 28.3163

Epoch 350: val_loss did not improve from 28.28068
196/196 - 42s - loss: 27.4111 - MinusLogProbMetric: 27.4111 - val_loss: 28.3163 - val_MinusLogProbMetric: 28.3163 - lr: 1.2500e-04 - 42s/epoch - 214ms/step
Epoch 351/1000
2023-10-25 18:41:04.086 
Epoch 351/1000 
	 loss: 27.4053, MinusLogProbMetric: 27.4053, val_loss: 28.3173, val_MinusLogProbMetric: 28.3173

Epoch 351: val_loss did not improve from 28.28068
196/196 - 43s - loss: 27.4053 - MinusLogProbMetric: 27.4053 - val_loss: 28.3173 - val_MinusLogProbMetric: 28.3173 - lr: 1.2500e-04 - 43s/epoch - 220ms/step
Epoch 352/1000
2023-10-25 18:41:48.683 
Epoch 352/1000 
	 loss: 27.4070, MinusLogProbMetric: 27.4070, val_loss: 28.2934, val_MinusLogProbMetric: 28.2934

Epoch 352: val_loss did not improve from 28.28068
196/196 - 45s - loss: 27.4070 - MinusLogProbMetric: 27.4070 - val_loss: 28.2934 - val_MinusLogProbMetric: 28.2934 - lr: 1.2500e-04 - 45s/epoch - 228ms/step
Epoch 353/1000
2023-10-25 18:42:28.549 
Epoch 353/1000 
	 loss: 27.3997, MinusLogProbMetric: 27.3997, val_loss: 28.2949, val_MinusLogProbMetric: 28.2949

Epoch 353: val_loss did not improve from 28.28068
196/196 - 40s - loss: 27.3997 - MinusLogProbMetric: 27.3997 - val_loss: 28.2949 - val_MinusLogProbMetric: 28.2949 - lr: 1.2500e-04 - 40s/epoch - 203ms/step
Epoch 354/1000
2023-10-25 18:43:10.454 
Epoch 354/1000 
	 loss: 27.3999, MinusLogProbMetric: 27.3999, val_loss: 28.3569, val_MinusLogProbMetric: 28.3569

Epoch 354: val_loss did not improve from 28.28068
196/196 - 42s - loss: 27.3999 - MinusLogProbMetric: 27.3999 - val_loss: 28.3569 - val_MinusLogProbMetric: 28.3569 - lr: 1.2500e-04 - 42s/epoch - 214ms/step
Epoch 355/1000
2023-10-25 18:43:50.703 
Epoch 355/1000 
	 loss: 27.4024, MinusLogProbMetric: 27.4024, val_loss: 28.3202, val_MinusLogProbMetric: 28.3202

Epoch 355: val_loss did not improve from 28.28068
196/196 - 40s - loss: 27.4024 - MinusLogProbMetric: 27.4024 - val_loss: 28.3202 - val_MinusLogProbMetric: 28.3202 - lr: 1.2500e-04 - 40s/epoch - 205ms/step
Epoch 356/1000
2023-10-25 18:44:34.035 
Epoch 356/1000 
	 loss: 27.4012, MinusLogProbMetric: 27.4012, val_loss: 28.3383, val_MinusLogProbMetric: 28.3383

Epoch 356: val_loss did not improve from 28.28068
196/196 - 43s - loss: 27.4012 - MinusLogProbMetric: 27.4012 - val_loss: 28.3383 - val_MinusLogProbMetric: 28.3383 - lr: 1.2500e-04 - 43s/epoch - 221ms/step
Epoch 357/1000
2023-10-25 18:45:18.027 
Epoch 357/1000 
	 loss: 27.3959, MinusLogProbMetric: 27.3959, val_loss: 28.2839, val_MinusLogProbMetric: 28.2839

Epoch 357: val_loss did not improve from 28.28068
196/196 - 44s - loss: 27.3959 - MinusLogProbMetric: 27.3959 - val_loss: 28.2839 - val_MinusLogProbMetric: 28.2839 - lr: 1.2500e-04 - 44s/epoch - 224ms/step
Epoch 358/1000
2023-10-25 18:45:59.259 
Epoch 358/1000 
	 loss: 27.3949, MinusLogProbMetric: 27.3949, val_loss: 28.3074, val_MinusLogProbMetric: 28.3074

Epoch 358: val_loss did not improve from 28.28068
196/196 - 41s - loss: 27.3949 - MinusLogProbMetric: 27.3949 - val_loss: 28.3074 - val_MinusLogProbMetric: 28.3074 - lr: 1.2500e-04 - 41s/epoch - 210ms/step
Epoch 359/1000
2023-10-25 18:46:39.893 
Epoch 359/1000 
	 loss: 27.4005, MinusLogProbMetric: 27.4005, val_loss: 28.3489, val_MinusLogProbMetric: 28.3489

Epoch 359: val_loss did not improve from 28.28068
196/196 - 41s - loss: 27.4005 - MinusLogProbMetric: 27.4005 - val_loss: 28.3489 - val_MinusLogProbMetric: 28.3489 - lr: 1.2500e-04 - 41s/epoch - 207ms/step
Epoch 360/1000
2023-10-25 18:47:17.196 
Epoch 360/1000 
	 loss: 27.3897, MinusLogProbMetric: 27.3897, val_loss: 28.3077, val_MinusLogProbMetric: 28.3077

Epoch 360: val_loss did not improve from 28.28068
196/196 - 37s - loss: 27.3897 - MinusLogProbMetric: 27.3897 - val_loss: 28.3077 - val_MinusLogProbMetric: 28.3077 - lr: 1.2500e-04 - 37s/epoch - 190ms/step
Epoch 361/1000
2023-10-25 18:47:56.099 
Epoch 361/1000 
	 loss: 27.3955, MinusLogProbMetric: 27.3955, val_loss: 28.3012, val_MinusLogProbMetric: 28.3012

Epoch 361: val_loss did not improve from 28.28068
196/196 - 39s - loss: 27.3955 - MinusLogProbMetric: 27.3955 - val_loss: 28.3012 - val_MinusLogProbMetric: 28.3012 - lr: 1.2500e-04 - 39s/epoch - 198ms/step
Epoch 362/1000
2023-10-25 18:48:37.666 
Epoch 362/1000 
	 loss: 27.3955, MinusLogProbMetric: 27.3955, val_loss: 28.2946, val_MinusLogProbMetric: 28.2946

Epoch 362: val_loss did not improve from 28.28068
196/196 - 42s - loss: 27.3955 - MinusLogProbMetric: 27.3955 - val_loss: 28.2946 - val_MinusLogProbMetric: 28.2946 - lr: 1.2500e-04 - 42s/epoch - 212ms/step
Epoch 363/1000
2023-10-25 18:49:21.448 
Epoch 363/1000 
	 loss: 27.4008, MinusLogProbMetric: 27.4008, val_loss: 28.3196, val_MinusLogProbMetric: 28.3196

Epoch 363: val_loss did not improve from 28.28068
196/196 - 44s - loss: 27.4008 - MinusLogProbMetric: 27.4008 - val_loss: 28.3196 - val_MinusLogProbMetric: 28.3196 - lr: 1.2500e-04 - 44s/epoch - 223ms/step
Epoch 364/1000
2023-10-25 18:49:59.144 
Epoch 364/1000 
	 loss: 27.3965, MinusLogProbMetric: 27.3965, val_loss: 28.3018, val_MinusLogProbMetric: 28.3018

Epoch 364: val_loss did not improve from 28.28068
196/196 - 38s - loss: 27.3965 - MinusLogProbMetric: 27.3965 - val_loss: 28.3018 - val_MinusLogProbMetric: 28.3018 - lr: 1.2500e-04 - 38s/epoch - 192ms/step
Epoch 365/1000
2023-10-25 18:50:38.045 
Epoch 365/1000 
	 loss: 27.3949, MinusLogProbMetric: 27.3949, val_loss: 28.3162, val_MinusLogProbMetric: 28.3162

Epoch 365: val_loss did not improve from 28.28068
196/196 - 39s - loss: 27.3949 - MinusLogProbMetric: 27.3949 - val_loss: 28.3162 - val_MinusLogProbMetric: 28.3162 - lr: 1.2500e-04 - 39s/epoch - 198ms/step
Epoch 366/1000
2023-10-25 18:51:15.882 
Epoch 366/1000 
	 loss: 27.3959, MinusLogProbMetric: 27.3959, val_loss: 28.2992, val_MinusLogProbMetric: 28.2992

Epoch 366: val_loss did not improve from 28.28068
196/196 - 38s - loss: 27.3959 - MinusLogProbMetric: 27.3959 - val_loss: 28.2992 - val_MinusLogProbMetric: 28.2992 - lr: 1.2500e-04 - 38s/epoch - 193ms/step
Epoch 367/1000
2023-10-25 18:51:55.343 
Epoch 367/1000 
	 loss: 27.3983, MinusLogProbMetric: 27.3983, val_loss: 28.3238, val_MinusLogProbMetric: 28.3238

Epoch 367: val_loss did not improve from 28.28068
196/196 - 39s - loss: 27.3983 - MinusLogProbMetric: 27.3983 - val_loss: 28.3238 - val_MinusLogProbMetric: 28.3238 - lr: 1.2500e-04 - 39s/epoch - 201ms/step
Epoch 368/1000
2023-10-25 18:52:37.973 
Epoch 368/1000 
	 loss: 27.4072, MinusLogProbMetric: 27.4072, val_loss: 28.3023, val_MinusLogProbMetric: 28.3023

Epoch 368: val_loss did not improve from 28.28068
196/196 - 43s - loss: 27.4072 - MinusLogProbMetric: 27.4072 - val_loss: 28.3023 - val_MinusLogProbMetric: 28.3023 - lr: 1.2500e-04 - 43s/epoch - 217ms/step
Epoch 369/1000
2023-10-25 18:53:21.452 
Epoch 369/1000 
	 loss: 27.3994, MinusLogProbMetric: 27.3994, val_loss: 28.2868, val_MinusLogProbMetric: 28.2868

Epoch 369: val_loss did not improve from 28.28068
196/196 - 43s - loss: 27.3994 - MinusLogProbMetric: 27.3994 - val_loss: 28.2868 - val_MinusLogProbMetric: 28.2868 - lr: 1.2500e-04 - 43s/epoch - 222ms/step
Epoch 370/1000
2023-10-25 18:53:56.748 
Epoch 370/1000 
	 loss: 27.3893, MinusLogProbMetric: 27.3893, val_loss: 28.3925, val_MinusLogProbMetric: 28.3925

Epoch 370: val_loss did not improve from 28.28068
196/196 - 35s - loss: 27.3893 - MinusLogProbMetric: 27.3893 - val_loss: 28.3925 - val_MinusLogProbMetric: 28.3925 - lr: 1.2500e-04 - 35s/epoch - 180ms/step
Epoch 371/1000
2023-10-25 18:54:31.965 
Epoch 371/1000 
	 loss: 27.3920, MinusLogProbMetric: 27.3920, val_loss: 28.3598, val_MinusLogProbMetric: 28.3598

Epoch 371: val_loss did not improve from 28.28068
196/196 - 35s - loss: 27.3920 - MinusLogProbMetric: 27.3920 - val_loss: 28.3598 - val_MinusLogProbMetric: 28.3598 - lr: 1.2500e-04 - 35s/epoch - 180ms/step
Epoch 372/1000
2023-10-25 18:55:10.888 
Epoch 372/1000 
	 loss: 27.3915, MinusLogProbMetric: 27.3915, val_loss: 28.3158, val_MinusLogProbMetric: 28.3158

Epoch 372: val_loss did not improve from 28.28068
196/196 - 39s - loss: 27.3915 - MinusLogProbMetric: 27.3915 - val_loss: 28.3158 - val_MinusLogProbMetric: 28.3158 - lr: 1.2500e-04 - 39s/epoch - 199ms/step
Epoch 373/1000
2023-10-25 18:55:49.016 
Epoch 373/1000 
	 loss: 27.4020, MinusLogProbMetric: 27.4020, val_loss: 28.3248, val_MinusLogProbMetric: 28.3248

Epoch 373: val_loss did not improve from 28.28068
196/196 - 38s - loss: 27.4020 - MinusLogProbMetric: 27.4020 - val_loss: 28.3248 - val_MinusLogProbMetric: 28.3248 - lr: 1.2500e-04 - 38s/epoch - 195ms/step
Epoch 374/1000
2023-10-25 18:56:29.218 
Epoch 374/1000 
	 loss: 27.3958, MinusLogProbMetric: 27.3958, val_loss: 28.3344, val_MinusLogProbMetric: 28.3344

Epoch 374: val_loss did not improve from 28.28068
196/196 - 40s - loss: 27.3958 - MinusLogProbMetric: 27.3958 - val_loss: 28.3344 - val_MinusLogProbMetric: 28.3344 - lr: 1.2500e-04 - 40s/epoch - 205ms/step
Epoch 375/1000
2023-10-25 18:57:09.796 
Epoch 375/1000 
	 loss: 27.3960, MinusLogProbMetric: 27.3960, val_loss: 28.3092, val_MinusLogProbMetric: 28.3092

Epoch 375: val_loss did not improve from 28.28068
196/196 - 41s - loss: 27.3960 - MinusLogProbMetric: 27.3960 - val_loss: 28.3092 - val_MinusLogProbMetric: 28.3092 - lr: 1.2500e-04 - 41s/epoch - 207ms/step
Epoch 376/1000
2023-10-25 18:57:50.210 
Epoch 376/1000 
	 loss: 27.3932, MinusLogProbMetric: 27.3932, val_loss: 28.3063, val_MinusLogProbMetric: 28.3063

Epoch 376: val_loss did not improve from 28.28068
196/196 - 40s - loss: 27.3932 - MinusLogProbMetric: 27.3932 - val_loss: 28.3063 - val_MinusLogProbMetric: 28.3063 - lr: 1.2500e-04 - 40s/epoch - 206ms/step
Epoch 377/1000
2023-10-25 18:58:28.193 
Epoch 377/1000 
	 loss: 27.3878, MinusLogProbMetric: 27.3878, val_loss: 28.3621, val_MinusLogProbMetric: 28.3621

Epoch 377: val_loss did not improve from 28.28068
196/196 - 38s - loss: 27.3878 - MinusLogProbMetric: 27.3878 - val_loss: 28.3621 - val_MinusLogProbMetric: 28.3621 - lr: 1.2500e-04 - 38s/epoch - 194ms/step
Epoch 378/1000
2023-10-25 18:59:05.823 
Epoch 378/1000 
	 loss: 27.3893, MinusLogProbMetric: 27.3893, val_loss: 28.2921, val_MinusLogProbMetric: 28.2921

Epoch 378: val_loss did not improve from 28.28068
196/196 - 38s - loss: 27.3893 - MinusLogProbMetric: 27.3893 - val_loss: 28.2921 - val_MinusLogProbMetric: 28.2921 - lr: 1.2500e-04 - 38s/epoch - 192ms/step
Epoch 379/1000
2023-10-25 18:59:47.345 
Epoch 379/1000 
	 loss: 27.3855, MinusLogProbMetric: 27.3855, val_loss: 28.2858, val_MinusLogProbMetric: 28.2858

Epoch 379: val_loss did not improve from 28.28068
196/196 - 42s - loss: 27.3855 - MinusLogProbMetric: 27.3855 - val_loss: 28.2858 - val_MinusLogProbMetric: 28.2858 - lr: 1.2500e-04 - 42s/epoch - 212ms/step
Epoch 380/1000
2023-10-25 19:00:29.586 
Epoch 380/1000 
	 loss: 27.3828, MinusLogProbMetric: 27.3828, val_loss: 28.2947, val_MinusLogProbMetric: 28.2947

Epoch 380: val_loss did not improve from 28.28068
196/196 - 42s - loss: 27.3828 - MinusLogProbMetric: 27.3828 - val_loss: 28.2947 - val_MinusLogProbMetric: 28.2947 - lr: 1.2500e-04 - 42s/epoch - 215ms/step
Epoch 381/1000
2023-10-25 19:01:07.898 
Epoch 381/1000 
	 loss: 27.3850, MinusLogProbMetric: 27.3850, val_loss: 28.3264, val_MinusLogProbMetric: 28.3264

Epoch 381: val_loss did not improve from 28.28068
196/196 - 38s - loss: 27.3850 - MinusLogProbMetric: 27.3850 - val_loss: 28.3264 - val_MinusLogProbMetric: 28.3264 - lr: 1.2500e-04 - 38s/epoch - 195ms/step
Epoch 382/1000
2023-10-25 19:01:45.798 
Epoch 382/1000 
	 loss: 27.3900, MinusLogProbMetric: 27.3900, val_loss: 28.3868, val_MinusLogProbMetric: 28.3868

Epoch 382: val_loss did not improve from 28.28068
196/196 - 38s - loss: 27.3900 - MinusLogProbMetric: 27.3900 - val_loss: 28.3868 - val_MinusLogProbMetric: 28.3868 - lr: 1.2500e-04 - 38s/epoch - 193ms/step
Epoch 383/1000
2023-10-25 19:02:24.191 
Epoch 383/1000 
	 loss: 27.3906, MinusLogProbMetric: 27.3906, val_loss: 28.2989, val_MinusLogProbMetric: 28.2989

Epoch 383: val_loss did not improve from 28.28068
196/196 - 38s - loss: 27.3906 - MinusLogProbMetric: 27.3906 - val_loss: 28.2989 - val_MinusLogProbMetric: 28.2989 - lr: 1.2500e-04 - 38s/epoch - 196ms/step
Epoch 384/1000
2023-10-25 19:03:06.853 
Epoch 384/1000 
	 loss: 27.3913, MinusLogProbMetric: 27.3913, val_loss: 28.3205, val_MinusLogProbMetric: 28.3205

Epoch 384: val_loss did not improve from 28.28068
196/196 - 43s - loss: 27.3913 - MinusLogProbMetric: 27.3913 - val_loss: 28.3205 - val_MinusLogProbMetric: 28.3205 - lr: 1.2500e-04 - 43s/epoch - 218ms/step
Epoch 385/1000
2023-10-25 19:03:48.596 
Epoch 385/1000 
	 loss: 27.3817, MinusLogProbMetric: 27.3817, val_loss: 28.3898, val_MinusLogProbMetric: 28.3898

Epoch 385: val_loss did not improve from 28.28068
196/196 - 42s - loss: 27.3817 - MinusLogProbMetric: 27.3817 - val_loss: 28.3898 - val_MinusLogProbMetric: 28.3898 - lr: 1.2500e-04 - 42s/epoch - 213ms/step
Epoch 386/1000
2023-10-25 19:04:26.470 
Epoch 386/1000 
	 loss: 27.4022, MinusLogProbMetric: 27.4022, val_loss: 28.3617, val_MinusLogProbMetric: 28.3617

Epoch 386: val_loss did not improve from 28.28068
196/196 - 38s - loss: 27.4022 - MinusLogProbMetric: 27.4022 - val_loss: 28.3617 - val_MinusLogProbMetric: 28.3617 - lr: 1.2500e-04 - 38s/epoch - 193ms/step
Epoch 387/1000
2023-10-25 19:05:02.542 
Epoch 387/1000 
	 loss: 27.3839, MinusLogProbMetric: 27.3839, val_loss: 28.3496, val_MinusLogProbMetric: 28.3496

Epoch 387: val_loss did not improve from 28.28068
196/196 - 36s - loss: 27.3839 - MinusLogProbMetric: 27.3839 - val_loss: 28.3496 - val_MinusLogProbMetric: 28.3496 - lr: 1.2500e-04 - 36s/epoch - 184ms/step
Epoch 388/1000
2023-10-25 19:05:39.346 
Epoch 388/1000 
	 loss: 27.3362, MinusLogProbMetric: 27.3362, val_loss: 28.2864, val_MinusLogProbMetric: 28.2864

Epoch 388: val_loss did not improve from 28.28068
196/196 - 37s - loss: 27.3362 - MinusLogProbMetric: 27.3362 - val_loss: 28.2864 - val_MinusLogProbMetric: 28.2864 - lr: 6.2500e-05 - 37s/epoch - 188ms/step
Epoch 389/1000
2023-10-25 19:06:21.673 
Epoch 389/1000 
	 loss: 27.3360, MinusLogProbMetric: 27.3360, val_loss: 28.2843, val_MinusLogProbMetric: 28.2843

Epoch 389: val_loss did not improve from 28.28068
196/196 - 42s - loss: 27.3360 - MinusLogProbMetric: 27.3360 - val_loss: 28.2843 - val_MinusLogProbMetric: 28.2843 - lr: 6.2500e-05 - 42s/epoch - 216ms/step
Epoch 390/1000
2023-10-25 19:07:02.436 
Epoch 390/1000 
	 loss: 27.3304, MinusLogProbMetric: 27.3304, val_loss: 28.2708, val_MinusLogProbMetric: 28.2708

Epoch 390: val_loss improved from 28.28068 to 28.27075, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_356/weights/best_weights.h5
196/196 - 41s - loss: 27.3304 - MinusLogProbMetric: 27.3304 - val_loss: 28.2708 - val_MinusLogProbMetric: 28.2708 - lr: 6.2500e-05 - 41s/epoch - 212ms/step
Epoch 391/1000
2023-10-25 19:07:41.517 
Epoch 391/1000 
	 loss: 27.3332, MinusLogProbMetric: 27.3332, val_loss: 28.2697, val_MinusLogProbMetric: 28.2697

Epoch 391: val_loss improved from 28.27075 to 28.26969, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_356/weights/best_weights.h5
196/196 - 39s - loss: 27.3332 - MinusLogProbMetric: 27.3332 - val_loss: 28.2697 - val_MinusLogProbMetric: 28.2697 - lr: 6.2500e-05 - 39s/epoch - 200ms/step
Epoch 392/1000
2023-10-25 19:08:25.864 
Epoch 392/1000 
	 loss: 27.3333, MinusLogProbMetric: 27.3333, val_loss: 28.2733, val_MinusLogProbMetric: 28.2733

Epoch 392: val_loss did not improve from 28.26969
196/196 - 43s - loss: 27.3333 - MinusLogProbMetric: 27.3333 - val_loss: 28.2733 - val_MinusLogProbMetric: 28.2733 - lr: 6.2500e-05 - 43s/epoch - 222ms/step
Epoch 393/1000
2023-10-25 19:09:06.157 
Epoch 393/1000 
	 loss: 27.3315, MinusLogProbMetric: 27.3315, val_loss: 28.2752, val_MinusLogProbMetric: 28.2752

Epoch 393: val_loss did not improve from 28.26969
196/196 - 40s - loss: 27.3315 - MinusLogProbMetric: 27.3315 - val_loss: 28.2752 - val_MinusLogProbMetric: 28.2752 - lr: 6.2500e-05 - 40s/epoch - 206ms/step
Epoch 394/1000
2023-10-25 19:09:43.628 
Epoch 394/1000 
	 loss: 27.3356, MinusLogProbMetric: 27.3356, val_loss: 28.2674, val_MinusLogProbMetric: 28.2674

Epoch 394: val_loss improved from 28.26969 to 28.26735, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_356/weights/best_weights.h5
196/196 - 38s - loss: 27.3356 - MinusLogProbMetric: 27.3356 - val_loss: 28.2674 - val_MinusLogProbMetric: 28.2674 - lr: 6.2500e-05 - 38s/epoch - 195ms/step
Epoch 395/1000
2023-10-25 19:10:25.575 
Epoch 395/1000 
	 loss: 27.3332, MinusLogProbMetric: 27.3332, val_loss: 28.2839, val_MinusLogProbMetric: 28.2839

Epoch 395: val_loss did not improve from 28.26735
196/196 - 41s - loss: 27.3332 - MinusLogProbMetric: 27.3332 - val_loss: 28.2839 - val_MinusLogProbMetric: 28.2839 - lr: 6.2500e-05 - 41s/epoch - 210ms/step
Epoch 396/1000
2023-10-25 19:11:04.414 
Epoch 396/1000 
	 loss: 27.3284, MinusLogProbMetric: 27.3284, val_loss: 28.2657, val_MinusLogProbMetric: 28.2657

Epoch 396: val_loss improved from 28.26735 to 28.26568, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_356/weights/best_weights.h5
196/196 - 39s - loss: 27.3284 - MinusLogProbMetric: 27.3284 - val_loss: 28.2657 - val_MinusLogProbMetric: 28.2657 - lr: 6.2500e-05 - 39s/epoch - 201ms/step
Epoch 397/1000
2023-10-25 19:11:44.607 
Epoch 397/1000 
	 loss: 27.3385, MinusLogProbMetric: 27.3385, val_loss: 28.2636, val_MinusLogProbMetric: 28.2636

Epoch 397: val_loss improved from 28.26568 to 28.26355, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_356/weights/best_weights.h5
196/196 - 40s - loss: 27.3385 - MinusLogProbMetric: 27.3385 - val_loss: 28.2636 - val_MinusLogProbMetric: 28.2636 - lr: 6.2500e-05 - 40s/epoch - 205ms/step
Epoch 398/1000
2023-10-25 19:12:21.040 
Epoch 398/1000 
	 loss: 27.3317, MinusLogProbMetric: 27.3317, val_loss: 28.2658, val_MinusLogProbMetric: 28.2658

Epoch 398: val_loss did not improve from 28.26355
196/196 - 36s - loss: 27.3317 - MinusLogProbMetric: 27.3317 - val_loss: 28.2658 - val_MinusLogProbMetric: 28.2658 - lr: 6.2500e-05 - 36s/epoch - 183ms/step
Epoch 399/1000
2023-10-25 19:12:57.437 
Epoch 399/1000 
	 loss: 27.3304, MinusLogProbMetric: 27.3304, val_loss: 28.2671, val_MinusLogProbMetric: 28.2671

Epoch 399: val_loss did not improve from 28.26355
196/196 - 36s - loss: 27.3304 - MinusLogProbMetric: 27.3304 - val_loss: 28.2671 - val_MinusLogProbMetric: 28.2671 - lr: 6.2500e-05 - 36s/epoch - 186ms/step
Epoch 400/1000
2023-10-25 19:13:39.323 
Epoch 400/1000 
	 loss: 27.3306, MinusLogProbMetric: 27.3306, val_loss: 28.2756, val_MinusLogProbMetric: 28.2756

Epoch 400: val_loss did not improve from 28.26355
196/196 - 42s - loss: 27.3306 - MinusLogProbMetric: 27.3306 - val_loss: 28.2756 - val_MinusLogProbMetric: 28.2756 - lr: 6.2500e-05 - 42s/epoch - 214ms/step
Epoch 401/1000
2023-10-25 19:14:19.093 
Epoch 401/1000 
	 loss: 27.3318, MinusLogProbMetric: 27.3318, val_loss: 28.2758, val_MinusLogProbMetric: 28.2758

Epoch 401: val_loss did not improve from 28.26355
196/196 - 40s - loss: 27.3318 - MinusLogProbMetric: 27.3318 - val_loss: 28.2758 - val_MinusLogProbMetric: 28.2758 - lr: 6.2500e-05 - 40s/epoch - 203ms/step
Epoch 402/1000
2023-10-25 19:15:02.458 
Epoch 402/1000 
	 loss: 27.3321, MinusLogProbMetric: 27.3321, val_loss: 28.2869, val_MinusLogProbMetric: 28.2869

Epoch 402: val_loss did not improve from 28.26355
196/196 - 43s - loss: 27.3321 - MinusLogProbMetric: 27.3321 - val_loss: 28.2869 - val_MinusLogProbMetric: 28.2869 - lr: 6.2500e-05 - 43s/epoch - 221ms/step
Epoch 403/1000
2023-10-25 19:15:44.444 
Epoch 403/1000 
	 loss: 27.3315, MinusLogProbMetric: 27.3315, val_loss: 28.2793, val_MinusLogProbMetric: 28.2793

Epoch 403: val_loss did not improve from 28.26355
196/196 - 42s - loss: 27.3315 - MinusLogProbMetric: 27.3315 - val_loss: 28.2793 - val_MinusLogProbMetric: 28.2793 - lr: 6.2500e-05 - 42s/epoch - 214ms/step
Epoch 404/1000
2023-10-25 19:16:24.593 
Epoch 404/1000 
	 loss: 27.3330, MinusLogProbMetric: 27.3330, val_loss: 28.2783, val_MinusLogProbMetric: 28.2783

Epoch 404: val_loss did not improve from 28.26355
196/196 - 40s - loss: 27.3330 - MinusLogProbMetric: 27.3330 - val_loss: 28.2783 - val_MinusLogProbMetric: 28.2783 - lr: 6.2500e-05 - 40s/epoch - 205ms/step
Epoch 405/1000
2023-10-25 19:17:08.742 
Epoch 405/1000 
	 loss: 27.3309, MinusLogProbMetric: 27.3309, val_loss: 28.2750, val_MinusLogProbMetric: 28.2750

Epoch 405: val_loss did not improve from 28.26355
196/196 - 44s - loss: 27.3309 - MinusLogProbMetric: 27.3309 - val_loss: 28.2750 - val_MinusLogProbMetric: 28.2750 - lr: 6.2500e-05 - 44s/epoch - 225ms/step
Epoch 406/1000
2023-10-25 19:17:53.241 
Epoch 406/1000 
	 loss: 27.3304, MinusLogProbMetric: 27.3304, val_loss: 28.2733, val_MinusLogProbMetric: 28.2733

Epoch 406: val_loss did not improve from 28.26355
196/196 - 44s - loss: 27.3304 - MinusLogProbMetric: 27.3304 - val_loss: 28.2733 - val_MinusLogProbMetric: 28.2733 - lr: 6.2500e-05 - 44s/epoch - 227ms/step
Epoch 407/1000
2023-10-25 19:18:37.539 
Epoch 407/1000 
	 loss: 27.3314, MinusLogProbMetric: 27.3314, val_loss: 28.2858, val_MinusLogProbMetric: 28.2858

Epoch 407: val_loss did not improve from 28.26355
196/196 - 44s - loss: 27.3314 - MinusLogProbMetric: 27.3314 - val_loss: 28.2858 - val_MinusLogProbMetric: 28.2858 - lr: 6.2500e-05 - 44s/epoch - 226ms/step
Epoch 408/1000
2023-10-25 19:19:21.036 
Epoch 408/1000 
	 loss: 27.3311, MinusLogProbMetric: 27.3311, val_loss: 28.3065, val_MinusLogProbMetric: 28.3065

Epoch 408: val_loss did not improve from 28.26355
196/196 - 43s - loss: 27.3311 - MinusLogProbMetric: 27.3311 - val_loss: 28.3065 - val_MinusLogProbMetric: 28.3065 - lr: 6.2500e-05 - 43s/epoch - 222ms/step
Epoch 409/1000
2023-10-25 19:20:05.060 
Epoch 409/1000 
	 loss: 27.3282, MinusLogProbMetric: 27.3282, val_loss: 28.2910, val_MinusLogProbMetric: 28.2910

Epoch 409: val_loss did not improve from 28.26355
196/196 - 44s - loss: 27.3282 - MinusLogProbMetric: 27.3282 - val_loss: 28.2910 - val_MinusLogProbMetric: 28.2910 - lr: 6.2500e-05 - 44s/epoch - 225ms/step
Epoch 410/1000
2023-10-25 19:20:49.103 
Epoch 410/1000 
	 loss: 27.3270, MinusLogProbMetric: 27.3270, val_loss: 28.2580, val_MinusLogProbMetric: 28.2580

Epoch 410: val_loss improved from 28.26355 to 28.25803, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_356/weights/best_weights.h5
196/196 - 45s - loss: 27.3270 - MinusLogProbMetric: 27.3270 - val_loss: 28.2580 - val_MinusLogProbMetric: 28.2580 - lr: 6.2500e-05 - 45s/epoch - 228ms/step
Epoch 411/1000
2023-10-25 19:21:33.900 
Epoch 411/1000 
	 loss: 27.3248, MinusLogProbMetric: 27.3248, val_loss: 28.2716, val_MinusLogProbMetric: 28.2716

Epoch 411: val_loss did not improve from 28.25803
196/196 - 44s - loss: 27.3248 - MinusLogProbMetric: 27.3248 - val_loss: 28.2716 - val_MinusLogProbMetric: 28.2716 - lr: 6.2500e-05 - 44s/epoch - 225ms/step
Epoch 412/1000
2023-10-25 19:22:17.947 
Epoch 412/1000 
	 loss: 27.3320, MinusLogProbMetric: 27.3320, val_loss: 28.2787, val_MinusLogProbMetric: 28.2787

Epoch 412: val_loss did not improve from 28.25803
196/196 - 44s - loss: 27.3320 - MinusLogProbMetric: 27.3320 - val_loss: 28.2787 - val_MinusLogProbMetric: 28.2787 - lr: 6.2500e-05 - 44s/epoch - 225ms/step
Epoch 413/1000
2023-10-25 19:23:02.061 
Epoch 413/1000 
	 loss: 27.3269, MinusLogProbMetric: 27.3269, val_loss: 28.2865, val_MinusLogProbMetric: 28.2865

Epoch 413: val_loss did not improve from 28.25803
196/196 - 44s - loss: 27.3269 - MinusLogProbMetric: 27.3269 - val_loss: 28.2865 - val_MinusLogProbMetric: 28.2865 - lr: 6.2500e-05 - 44s/epoch - 225ms/step
Epoch 414/1000
2023-10-25 19:23:46.157 
Epoch 414/1000 
	 loss: 27.3290, MinusLogProbMetric: 27.3290, val_loss: 28.2642, val_MinusLogProbMetric: 28.2642

Epoch 414: val_loss did not improve from 28.25803
196/196 - 44s - loss: 27.3290 - MinusLogProbMetric: 27.3290 - val_loss: 28.2642 - val_MinusLogProbMetric: 28.2642 - lr: 6.2500e-05 - 44s/epoch - 225ms/step
Epoch 415/1000
2023-10-25 19:24:30.500 
Epoch 415/1000 
	 loss: 27.3270, MinusLogProbMetric: 27.3270, val_loss: 28.2724, val_MinusLogProbMetric: 28.2724

Epoch 415: val_loss did not improve from 28.25803
196/196 - 44s - loss: 27.3270 - MinusLogProbMetric: 27.3270 - val_loss: 28.2724 - val_MinusLogProbMetric: 28.2724 - lr: 6.2500e-05 - 44s/epoch - 226ms/step
Epoch 416/1000
2023-10-25 19:25:14.721 
Epoch 416/1000 
	 loss: 27.3304, MinusLogProbMetric: 27.3304, val_loss: 28.2818, val_MinusLogProbMetric: 28.2818

Epoch 416: val_loss did not improve from 28.25803
196/196 - 44s - loss: 27.3304 - MinusLogProbMetric: 27.3304 - val_loss: 28.2818 - val_MinusLogProbMetric: 28.2818 - lr: 6.2500e-05 - 44s/epoch - 226ms/step
Epoch 417/1000
2023-10-25 19:25:59.105 
Epoch 417/1000 
	 loss: 27.3281, MinusLogProbMetric: 27.3281, val_loss: 28.3108, val_MinusLogProbMetric: 28.3108

Epoch 417: val_loss did not improve from 28.25803
196/196 - 44s - loss: 27.3281 - MinusLogProbMetric: 27.3281 - val_loss: 28.3108 - val_MinusLogProbMetric: 28.3108 - lr: 6.2500e-05 - 44s/epoch - 226ms/step
Epoch 418/1000
2023-10-25 19:26:43.254 
Epoch 418/1000 
	 loss: 27.3276, MinusLogProbMetric: 27.3276, val_loss: 28.2744, val_MinusLogProbMetric: 28.2744

Epoch 418: val_loss did not improve from 28.25803
196/196 - 44s - loss: 27.3276 - MinusLogProbMetric: 27.3276 - val_loss: 28.2744 - val_MinusLogProbMetric: 28.2744 - lr: 6.2500e-05 - 44s/epoch - 225ms/step
Epoch 419/1000
2023-10-25 19:27:28.059 
Epoch 419/1000 
	 loss: 27.3271, MinusLogProbMetric: 27.3271, val_loss: 28.2817, val_MinusLogProbMetric: 28.2817

Epoch 419: val_loss did not improve from 28.25803
196/196 - 45s - loss: 27.3271 - MinusLogProbMetric: 27.3271 - val_loss: 28.2817 - val_MinusLogProbMetric: 28.2817 - lr: 6.2500e-05 - 45s/epoch - 229ms/step
Epoch 420/1000
2023-10-25 19:28:12.299 
Epoch 420/1000 
	 loss: 27.3278, MinusLogProbMetric: 27.3278, val_loss: 28.2817, val_MinusLogProbMetric: 28.2817

Epoch 420: val_loss did not improve from 28.25803
196/196 - 44s - loss: 27.3278 - MinusLogProbMetric: 27.3278 - val_loss: 28.2817 - val_MinusLogProbMetric: 28.2817 - lr: 6.2500e-05 - 44s/epoch - 226ms/step
Epoch 421/1000
2023-10-25 19:28:56.843 
Epoch 421/1000 
	 loss: 27.3288, MinusLogProbMetric: 27.3288, val_loss: 28.2777, val_MinusLogProbMetric: 28.2777

Epoch 421: val_loss did not improve from 28.25803
196/196 - 45s - loss: 27.3288 - MinusLogProbMetric: 27.3288 - val_loss: 28.2777 - val_MinusLogProbMetric: 28.2777 - lr: 6.2500e-05 - 45s/epoch - 227ms/step
Epoch 422/1000
2023-10-25 19:29:41.661 
Epoch 422/1000 
	 loss: 27.3243, MinusLogProbMetric: 27.3243, val_loss: 28.2772, val_MinusLogProbMetric: 28.2772

Epoch 422: val_loss did not improve from 28.25803
196/196 - 45s - loss: 27.3243 - MinusLogProbMetric: 27.3243 - val_loss: 28.2772 - val_MinusLogProbMetric: 28.2772 - lr: 6.2500e-05 - 45s/epoch - 229ms/step
Epoch 423/1000
2023-10-25 19:30:26.337 
Epoch 423/1000 
	 loss: 27.3248, MinusLogProbMetric: 27.3248, val_loss: 28.2803, val_MinusLogProbMetric: 28.2803

Epoch 423: val_loss did not improve from 28.25803
196/196 - 45s - loss: 27.3248 - MinusLogProbMetric: 27.3248 - val_loss: 28.2803 - val_MinusLogProbMetric: 28.2803 - lr: 6.2500e-05 - 45s/epoch - 228ms/step
Epoch 424/1000
2023-10-25 19:31:11.121 
Epoch 424/1000 
	 loss: 27.3272, MinusLogProbMetric: 27.3272, val_loss: 28.2927, val_MinusLogProbMetric: 28.2927

Epoch 424: val_loss did not improve from 28.25803
196/196 - 45s - loss: 27.3272 - MinusLogProbMetric: 27.3272 - val_loss: 28.2927 - val_MinusLogProbMetric: 28.2927 - lr: 6.2500e-05 - 45s/epoch - 228ms/step
Epoch 425/1000
2023-10-25 19:31:55.790 
Epoch 425/1000 
	 loss: 27.3276, MinusLogProbMetric: 27.3276, val_loss: 28.2967, val_MinusLogProbMetric: 28.2967

Epoch 425: val_loss did not improve from 28.25803
196/196 - 45s - loss: 27.3276 - MinusLogProbMetric: 27.3276 - val_loss: 28.2967 - val_MinusLogProbMetric: 28.2967 - lr: 6.2500e-05 - 45s/epoch - 228ms/step
Epoch 426/1000
2023-10-25 19:32:40.697 
Epoch 426/1000 
	 loss: 27.3255, MinusLogProbMetric: 27.3255, val_loss: 28.2790, val_MinusLogProbMetric: 28.2790

Epoch 426: val_loss did not improve from 28.25803
196/196 - 45s - loss: 27.3255 - MinusLogProbMetric: 27.3255 - val_loss: 28.2790 - val_MinusLogProbMetric: 28.2790 - lr: 6.2500e-05 - 45s/epoch - 229ms/step
Epoch 427/1000
2023-10-25 19:33:24.976 
Epoch 427/1000 
	 loss: 27.3258, MinusLogProbMetric: 27.3258, val_loss: 28.2751, val_MinusLogProbMetric: 28.2751

Epoch 427: val_loss did not improve from 28.25803
196/196 - 44s - loss: 27.3258 - MinusLogProbMetric: 27.3258 - val_loss: 28.2751 - val_MinusLogProbMetric: 28.2751 - lr: 6.2500e-05 - 44s/epoch - 226ms/step
Epoch 428/1000
2023-10-25 19:34:09.158 
Epoch 428/1000 
	 loss: 27.3240, MinusLogProbMetric: 27.3240, val_loss: 28.3181, val_MinusLogProbMetric: 28.3181

Epoch 428: val_loss did not improve from 28.25803
196/196 - 44s - loss: 27.3240 - MinusLogProbMetric: 27.3240 - val_loss: 28.3181 - val_MinusLogProbMetric: 28.3181 - lr: 6.2500e-05 - 44s/epoch - 225ms/step
Epoch 429/1000
2023-10-25 19:34:53.917 
Epoch 429/1000 
	 loss: 27.3243, MinusLogProbMetric: 27.3243, val_loss: 28.2554, val_MinusLogProbMetric: 28.2554

Epoch 429: val_loss improved from 28.25803 to 28.25544, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_356/weights/best_weights.h5
196/196 - 45s - loss: 27.3243 - MinusLogProbMetric: 27.3243 - val_loss: 28.2554 - val_MinusLogProbMetric: 28.2554 - lr: 6.2500e-05 - 45s/epoch - 232ms/step
Epoch 430/1000
2023-10-25 19:35:39.100 
Epoch 430/1000 
	 loss: 27.3228, MinusLogProbMetric: 27.3228, val_loss: 28.2933, val_MinusLogProbMetric: 28.2933

Epoch 430: val_loss did not improve from 28.25544
196/196 - 44s - loss: 27.3228 - MinusLogProbMetric: 27.3228 - val_loss: 28.2933 - val_MinusLogProbMetric: 28.2933 - lr: 6.2500e-05 - 44s/epoch - 227ms/step
Epoch 431/1000
2023-10-25 19:36:23.668 
Epoch 431/1000 
	 loss: 27.3236, MinusLogProbMetric: 27.3236, val_loss: 28.2730, val_MinusLogProbMetric: 28.2730

Epoch 431: val_loss did not improve from 28.25544
196/196 - 45s - loss: 27.3236 - MinusLogProbMetric: 27.3236 - val_loss: 28.2730 - val_MinusLogProbMetric: 28.2730 - lr: 6.2500e-05 - 45s/epoch - 227ms/step
Epoch 432/1000
2023-10-25 19:37:07.828 
Epoch 432/1000 
	 loss: 27.3262, MinusLogProbMetric: 27.3262, val_loss: 28.2865, val_MinusLogProbMetric: 28.2865

Epoch 432: val_loss did not improve from 28.25544
196/196 - 44s - loss: 27.3262 - MinusLogProbMetric: 27.3262 - val_loss: 28.2865 - val_MinusLogProbMetric: 28.2865 - lr: 6.2500e-05 - 44s/epoch - 225ms/step
Epoch 433/1000
2023-10-25 19:37:52.438 
Epoch 433/1000 
	 loss: 27.3269, MinusLogProbMetric: 27.3269, val_loss: 28.2884, val_MinusLogProbMetric: 28.2884

Epoch 433: val_loss did not improve from 28.25544
196/196 - 45s - loss: 27.3269 - MinusLogProbMetric: 27.3269 - val_loss: 28.2884 - val_MinusLogProbMetric: 28.2884 - lr: 6.2500e-05 - 45s/epoch - 228ms/step
Epoch 434/1000
2023-10-25 19:38:37.341 
Epoch 434/1000 
	 loss: 27.3254, MinusLogProbMetric: 27.3254, val_loss: 28.2855, val_MinusLogProbMetric: 28.2855

Epoch 434: val_loss did not improve from 28.25544
196/196 - 45s - loss: 27.3254 - MinusLogProbMetric: 27.3254 - val_loss: 28.2855 - val_MinusLogProbMetric: 28.2855 - lr: 6.2500e-05 - 45s/epoch - 229ms/step
Epoch 435/1000
2023-10-25 19:39:21.697 
Epoch 435/1000 
	 loss: 27.3242, MinusLogProbMetric: 27.3242, val_loss: 28.2783, val_MinusLogProbMetric: 28.2783

Epoch 435: val_loss did not improve from 28.25544
196/196 - 44s - loss: 27.3242 - MinusLogProbMetric: 27.3242 - val_loss: 28.2783 - val_MinusLogProbMetric: 28.2783 - lr: 6.2500e-05 - 44s/epoch - 226ms/step
Epoch 436/1000
2023-10-25 19:40:05.877 
Epoch 436/1000 
	 loss: 27.3213, MinusLogProbMetric: 27.3213, val_loss: 28.2703, val_MinusLogProbMetric: 28.2703

Epoch 436: val_loss did not improve from 28.25544
196/196 - 44s - loss: 27.3213 - MinusLogProbMetric: 27.3213 - val_loss: 28.2703 - val_MinusLogProbMetric: 28.2703 - lr: 6.2500e-05 - 44s/epoch - 225ms/step
Epoch 437/1000
2023-10-25 19:40:47.676 
Epoch 437/1000 
	 loss: 27.3215, MinusLogProbMetric: 27.3215, val_loss: 28.2831, val_MinusLogProbMetric: 28.2831

Epoch 437: val_loss did not improve from 28.25544
196/196 - 42s - loss: 27.3215 - MinusLogProbMetric: 27.3215 - val_loss: 28.2831 - val_MinusLogProbMetric: 28.2831 - lr: 6.2500e-05 - 42s/epoch - 213ms/step
Epoch 438/1000
2023-10-25 19:41:27.455 
Epoch 438/1000 
	 loss: 27.3231, MinusLogProbMetric: 27.3231, val_loss: 28.2599, val_MinusLogProbMetric: 28.2599

Epoch 438: val_loss did not improve from 28.25544
196/196 - 40s - loss: 27.3231 - MinusLogProbMetric: 27.3231 - val_loss: 28.2599 - val_MinusLogProbMetric: 28.2599 - lr: 6.2500e-05 - 40s/epoch - 203ms/step
Epoch 439/1000
2023-10-25 19:42:10.254 
Epoch 439/1000 
	 loss: 27.3243, MinusLogProbMetric: 27.3243, val_loss: 28.3032, val_MinusLogProbMetric: 28.3032

Epoch 439: val_loss did not improve from 28.25544
196/196 - 43s - loss: 27.3243 - MinusLogProbMetric: 27.3243 - val_loss: 28.3032 - val_MinusLogProbMetric: 28.3032 - lr: 6.2500e-05 - 43s/epoch - 218ms/step
Epoch 440/1000
2023-10-25 19:42:51.678 
Epoch 440/1000 
	 loss: 27.3235, MinusLogProbMetric: 27.3235, val_loss: 28.2666, val_MinusLogProbMetric: 28.2666

Epoch 440: val_loss did not improve from 28.25544
196/196 - 41s - loss: 27.3235 - MinusLogProbMetric: 27.3235 - val_loss: 28.2666 - val_MinusLogProbMetric: 28.2666 - lr: 6.2500e-05 - 41s/epoch - 211ms/step
Epoch 441/1000
2023-10-25 19:43:35.321 
Epoch 441/1000 
	 loss: 27.3243, MinusLogProbMetric: 27.3243, val_loss: 28.2832, val_MinusLogProbMetric: 28.2832

Epoch 441: val_loss did not improve from 28.25544
196/196 - 44s - loss: 27.3243 - MinusLogProbMetric: 27.3243 - val_loss: 28.2832 - val_MinusLogProbMetric: 28.2832 - lr: 6.2500e-05 - 44s/epoch - 223ms/step
Epoch 442/1000
2023-10-25 19:44:20.327 
Epoch 442/1000 
	 loss: 27.3200, MinusLogProbMetric: 27.3200, val_loss: 28.2932, val_MinusLogProbMetric: 28.2932

Epoch 442: val_loss did not improve from 28.25544
196/196 - 45s - loss: 27.3200 - MinusLogProbMetric: 27.3200 - val_loss: 28.2932 - val_MinusLogProbMetric: 28.2932 - lr: 6.2500e-05 - 45s/epoch - 230ms/step
Epoch 443/1000
2023-10-25 19:45:04.181 
Epoch 443/1000 
	 loss: 27.3232, MinusLogProbMetric: 27.3232, val_loss: 28.2980, val_MinusLogProbMetric: 28.2980

Epoch 443: val_loss did not improve from 28.25544
196/196 - 44s - loss: 27.3232 - MinusLogProbMetric: 27.3232 - val_loss: 28.2980 - val_MinusLogProbMetric: 28.2980 - lr: 6.2500e-05 - 44s/epoch - 224ms/step
Epoch 444/1000
2023-10-25 19:45:46.430 
Epoch 444/1000 
	 loss: 27.3227, MinusLogProbMetric: 27.3227, val_loss: 28.2777, val_MinusLogProbMetric: 28.2777

Epoch 444: val_loss did not improve from 28.25544
196/196 - 42s - loss: 27.3227 - MinusLogProbMetric: 27.3227 - val_loss: 28.2777 - val_MinusLogProbMetric: 28.2777 - lr: 6.2500e-05 - 42s/epoch - 216ms/step
Epoch 445/1000
2023-10-25 19:46:30.708 
Epoch 445/1000 
	 loss: 27.3188, MinusLogProbMetric: 27.3188, val_loss: 28.2895, val_MinusLogProbMetric: 28.2895

Epoch 445: val_loss did not improve from 28.25544
196/196 - 44s - loss: 27.3188 - MinusLogProbMetric: 27.3188 - val_loss: 28.2895 - val_MinusLogProbMetric: 28.2895 - lr: 6.2500e-05 - 44s/epoch - 226ms/step
Epoch 446/1000
2023-10-25 19:47:15.343 
Epoch 446/1000 
	 loss: 27.3189, MinusLogProbMetric: 27.3189, val_loss: 28.2910, val_MinusLogProbMetric: 28.2910

Epoch 446: val_loss did not improve from 28.25544
196/196 - 45s - loss: 27.3189 - MinusLogProbMetric: 27.3189 - val_loss: 28.2910 - val_MinusLogProbMetric: 28.2910 - lr: 6.2500e-05 - 45s/epoch - 228ms/step
Epoch 447/1000
2023-10-25 19:48:00.020 
Epoch 447/1000 
	 loss: 27.3213, MinusLogProbMetric: 27.3213, val_loss: 28.3161, val_MinusLogProbMetric: 28.3161

Epoch 447: val_loss did not improve from 28.25544
196/196 - 45s - loss: 27.3213 - MinusLogProbMetric: 27.3213 - val_loss: 28.3161 - val_MinusLogProbMetric: 28.3161 - lr: 6.2500e-05 - 45s/epoch - 228ms/step
Epoch 448/1000
2023-10-25 19:48:44.128 
Epoch 448/1000 
	 loss: 27.3236, MinusLogProbMetric: 27.3236, val_loss: 28.2753, val_MinusLogProbMetric: 28.2753

Epoch 448: val_loss did not improve from 28.25544
196/196 - 44s - loss: 27.3236 - MinusLogProbMetric: 27.3236 - val_loss: 28.2753 - val_MinusLogProbMetric: 28.2753 - lr: 6.2500e-05 - 44s/epoch - 225ms/step
Epoch 449/1000
2023-10-25 19:49:28.453 
Epoch 449/1000 
	 loss: 27.3209, MinusLogProbMetric: 27.3209, val_loss: 28.3159, val_MinusLogProbMetric: 28.3159

Epoch 449: val_loss did not improve from 28.25544
196/196 - 44s - loss: 27.3209 - MinusLogProbMetric: 27.3209 - val_loss: 28.3159 - val_MinusLogProbMetric: 28.3159 - lr: 6.2500e-05 - 44s/epoch - 226ms/step
Epoch 450/1000
2023-10-25 19:50:12.823 
Epoch 450/1000 
	 loss: 27.3223, MinusLogProbMetric: 27.3223, val_loss: 28.2762, val_MinusLogProbMetric: 28.2762

Epoch 450: val_loss did not improve from 28.25544
196/196 - 44s - loss: 27.3223 - MinusLogProbMetric: 27.3223 - val_loss: 28.2762 - val_MinusLogProbMetric: 28.2762 - lr: 6.2500e-05 - 44s/epoch - 226ms/step
Epoch 451/1000
2023-10-25 19:50:57.788 
Epoch 451/1000 
	 loss: 27.3240, MinusLogProbMetric: 27.3240, val_loss: 28.2726, val_MinusLogProbMetric: 28.2726

Epoch 451: val_loss did not improve from 28.25544
196/196 - 45s - loss: 27.3240 - MinusLogProbMetric: 27.3240 - val_loss: 28.2726 - val_MinusLogProbMetric: 28.2726 - lr: 6.2500e-05 - 45s/epoch - 229ms/step
Epoch 452/1000
2023-10-25 19:51:42.164 
Epoch 452/1000 
	 loss: 27.3212, MinusLogProbMetric: 27.3212, val_loss: 28.2793, val_MinusLogProbMetric: 28.2793

Epoch 452: val_loss did not improve from 28.25544
196/196 - 44s - loss: 27.3212 - MinusLogProbMetric: 27.3212 - val_loss: 28.2793 - val_MinusLogProbMetric: 28.2793 - lr: 6.2500e-05 - 44s/epoch - 226ms/step
Epoch 453/1000
2023-10-25 19:52:26.767 
Epoch 453/1000 
	 loss: 27.3229, MinusLogProbMetric: 27.3229, val_loss: 28.2808, val_MinusLogProbMetric: 28.2808

Epoch 453: val_loss did not improve from 28.25544
196/196 - 45s - loss: 27.3229 - MinusLogProbMetric: 27.3229 - val_loss: 28.2808 - val_MinusLogProbMetric: 28.2808 - lr: 6.2500e-05 - 45s/epoch - 228ms/step
Epoch 454/1000
2023-10-25 19:53:11.852 
Epoch 454/1000 
	 loss: 27.3185, MinusLogProbMetric: 27.3185, val_loss: 28.2761, val_MinusLogProbMetric: 28.2761

Epoch 454: val_loss did not improve from 28.25544
196/196 - 45s - loss: 27.3185 - MinusLogProbMetric: 27.3185 - val_loss: 28.2761 - val_MinusLogProbMetric: 28.2761 - lr: 6.2500e-05 - 45s/epoch - 230ms/step
Epoch 455/1000
2023-10-25 19:53:56.307 
Epoch 455/1000 
	 loss: 27.3217, MinusLogProbMetric: 27.3217, val_loss: 28.2768, val_MinusLogProbMetric: 28.2768

Epoch 455: val_loss did not improve from 28.25544
196/196 - 44s - loss: 27.3217 - MinusLogProbMetric: 27.3217 - val_loss: 28.2768 - val_MinusLogProbMetric: 28.2768 - lr: 6.2500e-05 - 44s/epoch - 227ms/step
Epoch 456/1000
2023-10-25 19:54:40.969 
Epoch 456/1000 
	 loss: 27.3174, MinusLogProbMetric: 27.3174, val_loss: 28.3169, val_MinusLogProbMetric: 28.3169

Epoch 456: val_loss did not improve from 28.25544
196/196 - 45s - loss: 27.3174 - MinusLogProbMetric: 27.3174 - val_loss: 28.3169 - val_MinusLogProbMetric: 28.3169 - lr: 6.2500e-05 - 45s/epoch - 228ms/step
Epoch 457/1000
2023-10-25 19:55:25.739 
Epoch 457/1000 
	 loss: 27.3207, MinusLogProbMetric: 27.3207, val_loss: 28.2813, val_MinusLogProbMetric: 28.2813

Epoch 457: val_loss did not improve from 28.25544
196/196 - 45s - loss: 27.3207 - MinusLogProbMetric: 27.3207 - val_loss: 28.2813 - val_MinusLogProbMetric: 28.2813 - lr: 6.2500e-05 - 45s/epoch - 228ms/step
Epoch 458/1000
2023-10-25 19:56:10.198 
Epoch 458/1000 
	 loss: 27.3197, MinusLogProbMetric: 27.3197, val_loss: 28.2784, val_MinusLogProbMetric: 28.2784

Epoch 458: val_loss did not improve from 28.25544
196/196 - 44s - loss: 27.3197 - MinusLogProbMetric: 27.3197 - val_loss: 28.2784 - val_MinusLogProbMetric: 28.2784 - lr: 6.2500e-05 - 44s/epoch - 227ms/step
Epoch 459/1000
2023-10-25 19:56:54.508 
Epoch 459/1000 
	 loss: 27.3149, MinusLogProbMetric: 27.3149, val_loss: 28.2895, val_MinusLogProbMetric: 28.2895

Epoch 459: val_loss did not improve from 28.25544
196/196 - 44s - loss: 27.3149 - MinusLogProbMetric: 27.3149 - val_loss: 28.2895 - val_MinusLogProbMetric: 28.2895 - lr: 6.2500e-05 - 44s/epoch - 226ms/step
Epoch 460/1000
2023-10-25 19:57:39.461 
Epoch 460/1000 
	 loss: 27.3187, MinusLogProbMetric: 27.3187, val_loss: 28.2838, val_MinusLogProbMetric: 28.2838

Epoch 460: val_loss did not improve from 28.25544
196/196 - 45s - loss: 27.3187 - MinusLogProbMetric: 27.3187 - val_loss: 28.2838 - val_MinusLogProbMetric: 28.2838 - lr: 6.2500e-05 - 45s/epoch - 229ms/step
Epoch 461/1000
2023-10-25 19:58:24.077 
Epoch 461/1000 
	 loss: 27.3196, MinusLogProbMetric: 27.3196, val_loss: 28.2807, val_MinusLogProbMetric: 28.2807

Epoch 461: val_loss did not improve from 28.25544
196/196 - 45s - loss: 27.3196 - MinusLogProbMetric: 27.3196 - val_loss: 28.2807 - val_MinusLogProbMetric: 28.2807 - lr: 6.2500e-05 - 45s/epoch - 228ms/step
Epoch 462/1000
2023-10-25 19:59:09.032 
Epoch 462/1000 
	 loss: 27.3185, MinusLogProbMetric: 27.3185, val_loss: 28.2822, val_MinusLogProbMetric: 28.2822

Epoch 462: val_loss did not improve from 28.25544
196/196 - 45s - loss: 27.3185 - MinusLogProbMetric: 27.3185 - val_loss: 28.2822 - val_MinusLogProbMetric: 28.2822 - lr: 6.2500e-05 - 45s/epoch - 229ms/step
Epoch 463/1000
2023-10-25 19:59:53.667 
Epoch 463/1000 
	 loss: 27.3152, MinusLogProbMetric: 27.3152, val_loss: 28.2871, val_MinusLogProbMetric: 28.2871

Epoch 463: val_loss did not improve from 28.25544
196/196 - 45s - loss: 27.3152 - MinusLogProbMetric: 27.3152 - val_loss: 28.2871 - val_MinusLogProbMetric: 28.2871 - lr: 6.2500e-05 - 45s/epoch - 228ms/step
Epoch 464/1000
2023-10-25 20:00:38.060 
Epoch 464/1000 
	 loss: 27.3187, MinusLogProbMetric: 27.3187, val_loss: 28.2790, val_MinusLogProbMetric: 28.2790

Epoch 464: val_loss did not improve from 28.25544
196/196 - 44s - loss: 27.3187 - MinusLogProbMetric: 27.3187 - val_loss: 28.2790 - val_MinusLogProbMetric: 28.2790 - lr: 6.2500e-05 - 44s/epoch - 226ms/step
Epoch 465/1000
2023-10-25 20:01:22.750 
Epoch 465/1000 
	 loss: 27.3173, MinusLogProbMetric: 27.3173, val_loss: 28.3035, val_MinusLogProbMetric: 28.3035

Epoch 465: val_loss did not improve from 28.25544
196/196 - 45s - loss: 27.3173 - MinusLogProbMetric: 27.3173 - val_loss: 28.3035 - val_MinusLogProbMetric: 28.3035 - lr: 6.2500e-05 - 45s/epoch - 228ms/step
Epoch 466/1000
2023-10-25 20:02:08.078 
Epoch 466/1000 
	 loss: 27.3196, MinusLogProbMetric: 27.3196, val_loss: 28.2973, val_MinusLogProbMetric: 28.2973

Epoch 466: val_loss did not improve from 28.25544
196/196 - 45s - loss: 27.3196 - MinusLogProbMetric: 27.3196 - val_loss: 28.2973 - val_MinusLogProbMetric: 28.2973 - lr: 6.2500e-05 - 45s/epoch - 231ms/step
Epoch 467/1000
2023-10-25 20:02:52.647 
Epoch 467/1000 
	 loss: 27.3167, MinusLogProbMetric: 27.3167, val_loss: 28.2911, val_MinusLogProbMetric: 28.2911

Epoch 467: val_loss did not improve from 28.25544
196/196 - 45s - loss: 27.3167 - MinusLogProbMetric: 27.3167 - val_loss: 28.2911 - val_MinusLogProbMetric: 28.2911 - lr: 6.2500e-05 - 45s/epoch - 227ms/step
Epoch 468/1000
2023-10-25 20:03:37.722 
Epoch 468/1000 
	 loss: 27.3185, MinusLogProbMetric: 27.3185, val_loss: 28.2828, val_MinusLogProbMetric: 28.2828

Epoch 468: val_loss did not improve from 28.25544
196/196 - 45s - loss: 27.3185 - MinusLogProbMetric: 27.3185 - val_loss: 28.2828 - val_MinusLogProbMetric: 28.2828 - lr: 6.2500e-05 - 45s/epoch - 230ms/step
Epoch 469/1000
2023-10-25 20:04:22.345 
Epoch 469/1000 
	 loss: 27.3177, MinusLogProbMetric: 27.3177, val_loss: 28.2788, val_MinusLogProbMetric: 28.2788

Epoch 469: val_loss did not improve from 28.25544
196/196 - 45s - loss: 27.3177 - MinusLogProbMetric: 27.3177 - val_loss: 28.2788 - val_MinusLogProbMetric: 28.2788 - lr: 6.2500e-05 - 45s/epoch - 228ms/step
Epoch 470/1000
2023-10-25 20:05:06.697 
Epoch 470/1000 
	 loss: 27.3122, MinusLogProbMetric: 27.3122, val_loss: 28.2961, val_MinusLogProbMetric: 28.2961

Epoch 470: val_loss did not improve from 28.25544
196/196 - 44s - loss: 27.3122 - MinusLogProbMetric: 27.3122 - val_loss: 28.2961 - val_MinusLogProbMetric: 28.2961 - lr: 6.2500e-05 - 44s/epoch - 226ms/step
Epoch 471/1000
2023-10-25 20:05:51.226 
Epoch 471/1000 
	 loss: 27.3170, MinusLogProbMetric: 27.3170, val_loss: 28.3001, val_MinusLogProbMetric: 28.3001

Epoch 471: val_loss did not improve from 28.25544
196/196 - 45s - loss: 27.3170 - MinusLogProbMetric: 27.3170 - val_loss: 28.3001 - val_MinusLogProbMetric: 28.3001 - lr: 6.2500e-05 - 45s/epoch - 227ms/step
Epoch 472/1000
2023-10-25 20:06:35.876 
Epoch 472/1000 
	 loss: 27.3202, MinusLogProbMetric: 27.3202, val_loss: 28.2870, val_MinusLogProbMetric: 28.2870

Epoch 472: val_loss did not improve from 28.25544
196/196 - 45s - loss: 27.3202 - MinusLogProbMetric: 27.3202 - val_loss: 28.2870 - val_MinusLogProbMetric: 28.2870 - lr: 6.2500e-05 - 45s/epoch - 228ms/step
Epoch 473/1000
2023-10-25 20:07:20.101 
Epoch 473/1000 
	 loss: 27.3217, MinusLogProbMetric: 27.3217, val_loss: 28.2752, val_MinusLogProbMetric: 28.2752

Epoch 473: val_loss did not improve from 28.25544
196/196 - 44s - loss: 27.3217 - MinusLogProbMetric: 27.3217 - val_loss: 28.2752 - val_MinusLogProbMetric: 28.2752 - lr: 6.2500e-05 - 44s/epoch - 226ms/step
Epoch 474/1000
2023-10-25 20:08:04.853 
Epoch 474/1000 
	 loss: 27.3139, MinusLogProbMetric: 27.3139, val_loss: 28.2877, val_MinusLogProbMetric: 28.2877

Epoch 474: val_loss did not improve from 28.25544
196/196 - 45s - loss: 27.3139 - MinusLogProbMetric: 27.3139 - val_loss: 28.2877 - val_MinusLogProbMetric: 28.2877 - lr: 6.2500e-05 - 45s/epoch - 228ms/step
Epoch 475/1000
2023-10-25 20:08:49.529 
Epoch 475/1000 
	 loss: 27.3106, MinusLogProbMetric: 27.3106, val_loss: 28.2774, val_MinusLogProbMetric: 28.2774

Epoch 475: val_loss did not improve from 28.25544
196/196 - 45s - loss: 27.3106 - MinusLogProbMetric: 27.3106 - val_loss: 28.2774 - val_MinusLogProbMetric: 28.2774 - lr: 6.2500e-05 - 45s/epoch - 228ms/step
Epoch 476/1000
2023-10-25 20:09:33.722 
Epoch 476/1000 
	 loss: 27.3177, MinusLogProbMetric: 27.3177, val_loss: 28.2789, val_MinusLogProbMetric: 28.2789

Epoch 476: val_loss did not improve from 28.25544
196/196 - 44s - loss: 27.3177 - MinusLogProbMetric: 27.3177 - val_loss: 28.2789 - val_MinusLogProbMetric: 28.2789 - lr: 6.2500e-05 - 44s/epoch - 225ms/step
Epoch 477/1000
2023-10-25 20:10:17.998 
Epoch 477/1000 
	 loss: 27.3276, MinusLogProbMetric: 27.3276, val_loss: 28.2788, val_MinusLogProbMetric: 28.2788

Epoch 477: val_loss did not improve from 28.25544
196/196 - 44s - loss: 27.3276 - MinusLogProbMetric: 27.3276 - val_loss: 28.2788 - val_MinusLogProbMetric: 28.2788 - lr: 6.2500e-05 - 44s/epoch - 226ms/step
Epoch 478/1000
2023-10-25 20:11:02.804 
Epoch 478/1000 
	 loss: 27.3139, MinusLogProbMetric: 27.3139, val_loss: 28.2776, val_MinusLogProbMetric: 28.2776

Epoch 478: val_loss did not improve from 28.25544
196/196 - 45s - loss: 27.3139 - MinusLogProbMetric: 27.3139 - val_loss: 28.2776 - val_MinusLogProbMetric: 28.2776 - lr: 6.2500e-05 - 45s/epoch - 229ms/step
Epoch 479/1000
2023-10-25 20:11:47.293 
Epoch 479/1000 
	 loss: 27.3141, MinusLogProbMetric: 27.3141, val_loss: 28.2803, val_MinusLogProbMetric: 28.2803

Epoch 479: val_loss did not improve from 28.25544
196/196 - 44s - loss: 27.3141 - MinusLogProbMetric: 27.3141 - val_loss: 28.2803 - val_MinusLogProbMetric: 28.2803 - lr: 6.2500e-05 - 44s/epoch - 227ms/step
Epoch 480/1000
2023-10-25 20:12:31.380 
Epoch 480/1000 
	 loss: 27.2935, MinusLogProbMetric: 27.2935, val_loss: 28.2543, val_MinusLogProbMetric: 28.2543

Epoch 480: val_loss improved from 28.25544 to 28.25425, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_356/weights/best_weights.h5
196/196 - 45s - loss: 27.2935 - MinusLogProbMetric: 27.2935 - val_loss: 28.2543 - val_MinusLogProbMetric: 28.2543 - lr: 3.1250e-05 - 45s/epoch - 230ms/step
Epoch 481/1000
2023-10-25 20:13:16.575 
Epoch 481/1000 
	 loss: 27.2896, MinusLogProbMetric: 27.2896, val_loss: 28.2561, val_MinusLogProbMetric: 28.2561

Epoch 481: val_loss did not improve from 28.25425
196/196 - 44s - loss: 27.2896 - MinusLogProbMetric: 27.2896 - val_loss: 28.2561 - val_MinusLogProbMetric: 28.2561 - lr: 3.1250e-05 - 44s/epoch - 226ms/step
Epoch 482/1000
2023-10-25 20:13:56.856 
Epoch 482/1000 
	 loss: 27.2923, MinusLogProbMetric: 27.2923, val_loss: 28.2655, val_MinusLogProbMetric: 28.2655

Epoch 482: val_loss did not improve from 28.25425
196/196 - 40s - loss: 27.2923 - MinusLogProbMetric: 27.2923 - val_loss: 28.2655 - val_MinusLogProbMetric: 28.2655 - lr: 3.1250e-05 - 40s/epoch - 206ms/step
Epoch 483/1000
2023-10-25 20:14:37.846 
Epoch 483/1000 
	 loss: 27.2897, MinusLogProbMetric: 27.2897, val_loss: 28.2650, val_MinusLogProbMetric: 28.2650

Epoch 483: val_loss did not improve from 28.25425
196/196 - 41s - loss: 27.2897 - MinusLogProbMetric: 27.2897 - val_loss: 28.2650 - val_MinusLogProbMetric: 28.2650 - lr: 3.1250e-05 - 41s/epoch - 209ms/step
Epoch 484/1000
2023-10-25 20:15:20.948 
Epoch 484/1000 
	 loss: 27.2898, MinusLogProbMetric: 27.2898, val_loss: 28.2609, val_MinusLogProbMetric: 28.2609

Epoch 484: val_loss did not improve from 28.25425
196/196 - 43s - loss: 27.2898 - MinusLogProbMetric: 27.2898 - val_loss: 28.2609 - val_MinusLogProbMetric: 28.2609 - lr: 3.1250e-05 - 43s/epoch - 220ms/step
Epoch 485/1000
2023-10-25 20:16:04.375 
Epoch 485/1000 
	 loss: 27.2887, MinusLogProbMetric: 27.2887, val_loss: 28.2598, val_MinusLogProbMetric: 28.2598

Epoch 485: val_loss did not improve from 28.25425
196/196 - 43s - loss: 27.2887 - MinusLogProbMetric: 27.2887 - val_loss: 28.2598 - val_MinusLogProbMetric: 28.2598 - lr: 3.1250e-05 - 43s/epoch - 222ms/step
Epoch 486/1000
2023-10-25 20:16:48.353 
Epoch 486/1000 
	 loss: 27.2908, MinusLogProbMetric: 27.2908, val_loss: 28.2651, val_MinusLogProbMetric: 28.2651

Epoch 486: val_loss did not improve from 28.25425
196/196 - 44s - loss: 27.2908 - MinusLogProbMetric: 27.2908 - val_loss: 28.2651 - val_MinusLogProbMetric: 28.2651 - lr: 3.1250e-05 - 44s/epoch - 224ms/step
Epoch 487/1000
2023-10-25 20:17:32.714 
Epoch 487/1000 
	 loss: 27.2927, MinusLogProbMetric: 27.2927, val_loss: 28.2648, val_MinusLogProbMetric: 28.2648

Epoch 487: val_loss did not improve from 28.25425
196/196 - 44s - loss: 27.2927 - MinusLogProbMetric: 27.2927 - val_loss: 28.2648 - val_MinusLogProbMetric: 28.2648 - lr: 3.1250e-05 - 44s/epoch - 226ms/step
Epoch 488/1000
2023-10-25 20:18:14.710 
Epoch 488/1000 
	 loss: 27.2885, MinusLogProbMetric: 27.2885, val_loss: 28.2585, val_MinusLogProbMetric: 28.2585

Epoch 488: val_loss did not improve from 28.25425
196/196 - 42s - loss: 27.2885 - MinusLogProbMetric: 27.2885 - val_loss: 28.2585 - val_MinusLogProbMetric: 28.2585 - lr: 3.1250e-05 - 42s/epoch - 214ms/step
Epoch 489/1000
2023-10-25 20:18:59.065 
Epoch 489/1000 
	 loss: 27.2907, MinusLogProbMetric: 27.2907, val_loss: 28.2643, val_MinusLogProbMetric: 28.2643

Epoch 489: val_loss did not improve from 28.25425
196/196 - 44s - loss: 27.2907 - MinusLogProbMetric: 27.2907 - val_loss: 28.2643 - val_MinusLogProbMetric: 28.2643 - lr: 3.1250e-05 - 44s/epoch - 226ms/step
Epoch 490/1000
2023-10-25 20:19:43.182 
Epoch 490/1000 
	 loss: 27.2900, MinusLogProbMetric: 27.2900, val_loss: 28.2667, val_MinusLogProbMetric: 28.2667

Epoch 490: val_loss did not improve from 28.25425
196/196 - 44s - loss: 27.2900 - MinusLogProbMetric: 27.2900 - val_loss: 28.2667 - val_MinusLogProbMetric: 28.2667 - lr: 3.1250e-05 - 44s/epoch - 225ms/step
Epoch 491/1000
2023-10-25 20:20:27.179 
Epoch 491/1000 
	 loss: 27.2891, MinusLogProbMetric: 27.2891, val_loss: 28.2530, val_MinusLogProbMetric: 28.2530

Epoch 491: val_loss improved from 28.25425 to 28.25298, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_356/weights/best_weights.h5
196/196 - 45s - loss: 27.2891 - MinusLogProbMetric: 27.2891 - val_loss: 28.2530 - val_MinusLogProbMetric: 28.2530 - lr: 3.1250e-05 - 45s/epoch - 229ms/step
Epoch 492/1000
2023-10-25 20:21:11.880 
Epoch 492/1000 
	 loss: 27.2903, MinusLogProbMetric: 27.2903, val_loss: 28.2726, val_MinusLogProbMetric: 28.2726

Epoch 492: val_loss did not improve from 28.25298
196/196 - 44s - loss: 27.2903 - MinusLogProbMetric: 27.2903 - val_loss: 28.2726 - val_MinusLogProbMetric: 28.2726 - lr: 3.1250e-05 - 44s/epoch - 224ms/step
Epoch 493/1000
2023-10-25 20:21:55.289 
Epoch 493/1000 
	 loss: 27.2893, MinusLogProbMetric: 27.2893, val_loss: 28.2624, val_MinusLogProbMetric: 28.2624

Epoch 493: val_loss did not improve from 28.25298
196/196 - 43s - loss: 27.2893 - MinusLogProbMetric: 27.2893 - val_loss: 28.2624 - val_MinusLogProbMetric: 28.2624 - lr: 3.1250e-05 - 43s/epoch - 221ms/step
Epoch 494/1000
2023-10-25 20:22:38.775 
Epoch 494/1000 
	 loss: 27.2913, MinusLogProbMetric: 27.2913, val_loss: 28.2817, val_MinusLogProbMetric: 28.2817

Epoch 494: val_loss did not improve from 28.25298
196/196 - 43s - loss: 27.2913 - MinusLogProbMetric: 27.2913 - val_loss: 28.2817 - val_MinusLogProbMetric: 28.2817 - lr: 3.1250e-05 - 43s/epoch - 222ms/step
Epoch 495/1000
2023-10-25 20:23:22.841 
Epoch 495/1000 
	 loss: 27.2927, MinusLogProbMetric: 27.2927, val_loss: 28.2748, val_MinusLogProbMetric: 28.2748

Epoch 495: val_loss did not improve from 28.25298
196/196 - 44s - loss: 27.2927 - MinusLogProbMetric: 27.2927 - val_loss: 28.2748 - val_MinusLogProbMetric: 28.2748 - lr: 3.1250e-05 - 44s/epoch - 225ms/step
Epoch 496/1000
2023-10-25 20:24:06.931 
Epoch 496/1000 
	 loss: 27.2892, MinusLogProbMetric: 27.2892, val_loss: 28.2656, val_MinusLogProbMetric: 28.2656

Epoch 496: val_loss did not improve from 28.25298
196/196 - 44s - loss: 27.2892 - MinusLogProbMetric: 27.2892 - val_loss: 28.2656 - val_MinusLogProbMetric: 28.2656 - lr: 3.1250e-05 - 44s/epoch - 225ms/step
Epoch 497/1000
2023-10-25 20:24:51.016 
Epoch 497/1000 
	 loss: 27.2895, MinusLogProbMetric: 27.2895, val_loss: 28.2634, val_MinusLogProbMetric: 28.2634

Epoch 497: val_loss did not improve from 28.25298
196/196 - 44s - loss: 27.2895 - MinusLogProbMetric: 27.2895 - val_loss: 28.2634 - val_MinusLogProbMetric: 28.2634 - lr: 3.1250e-05 - 44s/epoch - 225ms/step
Epoch 498/1000
2023-10-25 20:25:34.928 
Epoch 498/1000 
	 loss: 27.2905, MinusLogProbMetric: 27.2905, val_loss: 28.2891, val_MinusLogProbMetric: 28.2891

Epoch 498: val_loss did not improve from 28.25298
196/196 - 44s - loss: 27.2905 - MinusLogProbMetric: 27.2905 - val_loss: 28.2891 - val_MinusLogProbMetric: 28.2891 - lr: 3.1250e-05 - 44s/epoch - 224ms/step
Epoch 499/1000
2023-10-25 20:26:19.015 
Epoch 499/1000 
	 loss: 27.2894, MinusLogProbMetric: 27.2894, val_loss: 28.2672, val_MinusLogProbMetric: 28.2672

Epoch 499: val_loss did not improve from 28.25298
196/196 - 44s - loss: 27.2894 - MinusLogProbMetric: 27.2894 - val_loss: 28.2672 - val_MinusLogProbMetric: 28.2672 - lr: 3.1250e-05 - 44s/epoch - 225ms/step
Epoch 500/1000
2023-10-25 20:27:03.234 
Epoch 500/1000 
	 loss: 27.2886, MinusLogProbMetric: 27.2886, val_loss: 28.2603, val_MinusLogProbMetric: 28.2603

Epoch 500: val_loss did not improve from 28.25298
196/196 - 44s - loss: 27.2886 - MinusLogProbMetric: 27.2886 - val_loss: 28.2603 - val_MinusLogProbMetric: 28.2603 - lr: 3.1250e-05 - 44s/epoch - 226ms/step
Epoch 501/1000
2023-10-25 20:27:47.123 
Epoch 501/1000 
	 loss: 27.2901, MinusLogProbMetric: 27.2901, val_loss: 28.2792, val_MinusLogProbMetric: 28.2792

Epoch 501: val_loss did not improve from 28.25298
196/196 - 44s - loss: 27.2901 - MinusLogProbMetric: 27.2901 - val_loss: 28.2792 - val_MinusLogProbMetric: 28.2792 - lr: 3.1250e-05 - 44s/epoch - 224ms/step
Epoch 502/1000
2023-10-25 20:28:31.366 
Epoch 502/1000 
	 loss: 27.2896, MinusLogProbMetric: 27.2896, val_loss: 28.2830, val_MinusLogProbMetric: 28.2830

Epoch 502: val_loss did not improve from 28.25298
196/196 - 44s - loss: 27.2896 - MinusLogProbMetric: 27.2896 - val_loss: 28.2830 - val_MinusLogProbMetric: 28.2830 - lr: 3.1250e-05 - 44s/epoch - 226ms/step
Epoch 503/1000
2023-10-25 20:29:15.782 
Epoch 503/1000 
	 loss: 27.2908, MinusLogProbMetric: 27.2908, val_loss: 28.2688, val_MinusLogProbMetric: 28.2688

Epoch 503: val_loss did not improve from 28.25298
196/196 - 44s - loss: 27.2908 - MinusLogProbMetric: 27.2908 - val_loss: 28.2688 - val_MinusLogProbMetric: 28.2688 - lr: 3.1250e-05 - 44s/epoch - 227ms/step
Epoch 504/1000
2023-10-25 20:30:00.100 
Epoch 504/1000 
	 loss: 27.2900, MinusLogProbMetric: 27.2900, val_loss: 28.2684, val_MinusLogProbMetric: 28.2684

Epoch 504: val_loss did not improve from 28.25298
196/196 - 44s - loss: 27.2900 - MinusLogProbMetric: 27.2900 - val_loss: 28.2684 - val_MinusLogProbMetric: 28.2684 - lr: 3.1250e-05 - 44s/epoch - 226ms/step
Epoch 505/1000
2023-10-25 20:30:45.054 
Epoch 505/1000 
	 loss: 27.2888, MinusLogProbMetric: 27.2888, val_loss: 28.2631, val_MinusLogProbMetric: 28.2631

Epoch 505: val_loss did not improve from 28.25298
196/196 - 45s - loss: 27.2888 - MinusLogProbMetric: 27.2888 - val_loss: 28.2631 - val_MinusLogProbMetric: 28.2631 - lr: 3.1250e-05 - 45s/epoch - 229ms/step
Epoch 506/1000
2023-10-25 20:31:29.460 
Epoch 506/1000 
	 loss: 27.2865, MinusLogProbMetric: 27.2865, val_loss: 28.2585, val_MinusLogProbMetric: 28.2585

Epoch 506: val_loss did not improve from 28.25298
196/196 - 44s - loss: 27.2865 - MinusLogProbMetric: 27.2865 - val_loss: 28.2585 - val_MinusLogProbMetric: 28.2585 - lr: 3.1250e-05 - 44s/epoch - 227ms/step
Epoch 507/1000
2023-10-25 20:32:14.135 
Epoch 507/1000 
	 loss: 27.2891, MinusLogProbMetric: 27.2891, val_loss: 28.2686, val_MinusLogProbMetric: 28.2686

Epoch 507: val_loss did not improve from 28.25298
196/196 - 45s - loss: 27.2891 - MinusLogProbMetric: 27.2891 - val_loss: 28.2686 - val_MinusLogProbMetric: 28.2686 - lr: 3.1250e-05 - 45s/epoch - 228ms/step
Epoch 508/1000
2023-10-25 20:32:58.850 
Epoch 508/1000 
	 loss: 27.2873, MinusLogProbMetric: 27.2873, val_loss: 28.2710, val_MinusLogProbMetric: 28.2710

Epoch 508: val_loss did not improve from 28.25298
196/196 - 45s - loss: 27.2873 - MinusLogProbMetric: 27.2873 - val_loss: 28.2710 - val_MinusLogProbMetric: 28.2710 - lr: 3.1250e-05 - 45s/epoch - 228ms/step
Epoch 509/1000
2023-10-25 20:33:43.246 
Epoch 509/1000 
	 loss: 27.2857, MinusLogProbMetric: 27.2857, val_loss: 28.2722, val_MinusLogProbMetric: 28.2722

Epoch 509: val_loss did not improve from 28.25298
196/196 - 44s - loss: 27.2857 - MinusLogProbMetric: 27.2857 - val_loss: 28.2722 - val_MinusLogProbMetric: 28.2722 - lr: 3.1250e-05 - 44s/epoch - 226ms/step
Epoch 510/1000
2023-10-25 20:34:27.323 
Epoch 510/1000 
	 loss: 27.2878, MinusLogProbMetric: 27.2878, val_loss: 28.2626, val_MinusLogProbMetric: 28.2626

Epoch 510: val_loss did not improve from 28.25298
196/196 - 44s - loss: 27.2878 - MinusLogProbMetric: 27.2878 - val_loss: 28.2626 - val_MinusLogProbMetric: 28.2626 - lr: 3.1250e-05 - 44s/epoch - 225ms/step
Epoch 511/1000
2023-10-25 20:35:11.440 
Epoch 511/1000 
	 loss: 27.2886, MinusLogProbMetric: 27.2886, val_loss: 28.2590, val_MinusLogProbMetric: 28.2590

Epoch 511: val_loss did not improve from 28.25298
196/196 - 44s - loss: 27.2886 - MinusLogProbMetric: 27.2886 - val_loss: 28.2590 - val_MinusLogProbMetric: 28.2590 - lr: 3.1250e-05 - 44s/epoch - 225ms/step
Epoch 512/1000
2023-10-25 20:35:56.447 
Epoch 512/1000 
	 loss: 27.2848, MinusLogProbMetric: 27.2848, val_loss: 28.2706, val_MinusLogProbMetric: 28.2706

Epoch 512: val_loss did not improve from 28.25298
196/196 - 45s - loss: 27.2848 - MinusLogProbMetric: 27.2848 - val_loss: 28.2706 - val_MinusLogProbMetric: 28.2706 - lr: 3.1250e-05 - 45s/epoch - 230ms/step
Epoch 513/1000
2023-10-25 20:36:41.088 
Epoch 513/1000 
	 loss: 27.2871, MinusLogProbMetric: 27.2871, val_loss: 28.2634, val_MinusLogProbMetric: 28.2634

Epoch 513: val_loss did not improve from 28.25298
196/196 - 45s - loss: 27.2871 - MinusLogProbMetric: 27.2871 - val_loss: 28.2634 - val_MinusLogProbMetric: 28.2634 - lr: 3.1250e-05 - 45s/epoch - 228ms/step
Epoch 514/1000
2023-10-25 20:37:25.443 
Epoch 514/1000 
	 loss: 27.2866, MinusLogProbMetric: 27.2866, val_loss: 28.2563, val_MinusLogProbMetric: 28.2563

Epoch 514: val_loss did not improve from 28.25298
196/196 - 44s - loss: 27.2866 - MinusLogProbMetric: 27.2866 - val_loss: 28.2563 - val_MinusLogProbMetric: 28.2563 - lr: 3.1250e-05 - 44s/epoch - 226ms/step
Epoch 515/1000
2023-10-25 20:38:09.928 
Epoch 515/1000 
	 loss: 27.2874, MinusLogProbMetric: 27.2874, val_loss: 28.2804, val_MinusLogProbMetric: 28.2804

Epoch 515: val_loss did not improve from 28.25298
196/196 - 44s - loss: 27.2874 - MinusLogProbMetric: 27.2874 - val_loss: 28.2804 - val_MinusLogProbMetric: 28.2804 - lr: 3.1250e-05 - 44s/epoch - 227ms/step
Epoch 516/1000
2023-10-25 20:38:54.510 
Epoch 516/1000 
	 loss: 27.2858, MinusLogProbMetric: 27.2858, val_loss: 28.2603, val_MinusLogProbMetric: 28.2603

Epoch 516: val_loss did not improve from 28.25298
196/196 - 45s - loss: 27.2858 - MinusLogProbMetric: 27.2858 - val_loss: 28.2603 - val_MinusLogProbMetric: 28.2603 - lr: 3.1250e-05 - 45s/epoch - 227ms/step
Epoch 517/1000
2023-10-25 20:39:39.356 
Epoch 517/1000 
	 loss: 27.2862, MinusLogProbMetric: 27.2862, val_loss: 28.2661, val_MinusLogProbMetric: 28.2661

Epoch 517: val_loss did not improve from 28.25298
196/196 - 45s - loss: 27.2862 - MinusLogProbMetric: 27.2862 - val_loss: 28.2661 - val_MinusLogProbMetric: 28.2661 - lr: 3.1250e-05 - 45s/epoch - 229ms/step
Epoch 518/1000
2023-10-25 20:40:23.433 
Epoch 518/1000 
	 loss: 27.2846, MinusLogProbMetric: 27.2846, val_loss: 28.2798, val_MinusLogProbMetric: 28.2798

Epoch 518: val_loss did not improve from 28.25298
196/196 - 44s - loss: 27.2846 - MinusLogProbMetric: 27.2846 - val_loss: 28.2798 - val_MinusLogProbMetric: 28.2798 - lr: 3.1250e-05 - 44s/epoch - 225ms/step
Epoch 519/1000
2023-10-25 20:41:08.110 
Epoch 519/1000 
	 loss: 27.2854, MinusLogProbMetric: 27.2854, val_loss: 28.2678, val_MinusLogProbMetric: 28.2678

Epoch 519: val_loss did not improve from 28.25298
196/196 - 45s - loss: 27.2854 - MinusLogProbMetric: 27.2854 - val_loss: 28.2678 - val_MinusLogProbMetric: 28.2678 - lr: 3.1250e-05 - 45s/epoch - 228ms/step
Epoch 520/1000
2023-10-25 20:41:51.156 
Epoch 520/1000 
	 loss: 27.2862, MinusLogProbMetric: 27.2862, val_loss: 28.2661, val_MinusLogProbMetric: 28.2661

Epoch 520: val_loss did not improve from 28.25298
196/196 - 43s - loss: 27.2862 - MinusLogProbMetric: 27.2862 - val_loss: 28.2661 - val_MinusLogProbMetric: 28.2661 - lr: 3.1250e-05 - 43s/epoch - 220ms/step
Epoch 521/1000
2023-10-25 20:42:31.461 
Epoch 521/1000 
	 loss: 27.2839, MinusLogProbMetric: 27.2839, val_loss: 28.2645, val_MinusLogProbMetric: 28.2645

Epoch 521: val_loss did not improve from 28.25298
196/196 - 40s - loss: 27.2839 - MinusLogProbMetric: 27.2839 - val_loss: 28.2645 - val_MinusLogProbMetric: 28.2645 - lr: 3.1250e-05 - 40s/epoch - 206ms/step
Epoch 522/1000
2023-10-25 20:43:13.152 
Epoch 522/1000 
	 loss: 27.2856, MinusLogProbMetric: 27.2856, val_loss: 28.2673, val_MinusLogProbMetric: 28.2673

Epoch 522: val_loss did not improve from 28.25298
196/196 - 42s - loss: 27.2856 - MinusLogProbMetric: 27.2856 - val_loss: 28.2673 - val_MinusLogProbMetric: 28.2673 - lr: 3.1250e-05 - 42s/epoch - 213ms/step
Epoch 523/1000
2023-10-25 20:43:56.912 
Epoch 523/1000 
	 loss: 27.2862, MinusLogProbMetric: 27.2862, val_loss: 28.2554, val_MinusLogProbMetric: 28.2554

Epoch 523: val_loss did not improve from 28.25298
196/196 - 44s - loss: 27.2862 - MinusLogProbMetric: 27.2862 - val_loss: 28.2554 - val_MinusLogProbMetric: 28.2554 - lr: 3.1250e-05 - 44s/epoch - 223ms/step
Epoch 524/1000
2023-10-25 20:44:39.074 
Epoch 524/1000 
	 loss: 27.2862, MinusLogProbMetric: 27.2862, val_loss: 28.2709, val_MinusLogProbMetric: 28.2709

Epoch 524: val_loss did not improve from 28.25298
196/196 - 42s - loss: 27.2862 - MinusLogProbMetric: 27.2862 - val_loss: 28.2709 - val_MinusLogProbMetric: 28.2709 - lr: 3.1250e-05 - 42s/epoch - 215ms/step
Epoch 525/1000
2023-10-25 20:45:22.943 
Epoch 525/1000 
	 loss: 27.2858, MinusLogProbMetric: 27.2858, val_loss: 28.2611, val_MinusLogProbMetric: 28.2611

Epoch 525: val_loss did not improve from 28.25298
196/196 - 44s - loss: 27.2858 - MinusLogProbMetric: 27.2858 - val_loss: 28.2611 - val_MinusLogProbMetric: 28.2611 - lr: 3.1250e-05 - 44s/epoch - 224ms/step
Epoch 526/1000
2023-10-25 20:46:07.030 
Epoch 526/1000 
	 loss: 27.2862, MinusLogProbMetric: 27.2862, val_loss: 28.2650, val_MinusLogProbMetric: 28.2650

Epoch 526: val_loss did not improve from 28.25298
196/196 - 44s - loss: 27.2862 - MinusLogProbMetric: 27.2862 - val_loss: 28.2650 - val_MinusLogProbMetric: 28.2650 - lr: 3.1250e-05 - 44s/epoch - 225ms/step
Epoch 527/1000
2023-10-25 20:46:48.477 
Epoch 527/1000 
	 loss: 27.2844, MinusLogProbMetric: 27.2844, val_loss: 28.2672, val_MinusLogProbMetric: 28.2672

Epoch 527: val_loss did not improve from 28.25298
196/196 - 41s - loss: 27.2844 - MinusLogProbMetric: 27.2844 - val_loss: 28.2672 - val_MinusLogProbMetric: 28.2672 - lr: 3.1250e-05 - 41s/epoch - 211ms/step
Epoch 528/1000
2023-10-25 20:47:32.708 
Epoch 528/1000 
	 loss: 27.2843, MinusLogProbMetric: 27.2843, val_loss: 28.2775, val_MinusLogProbMetric: 28.2775

Epoch 528: val_loss did not improve from 28.25298
196/196 - 44s - loss: 27.2843 - MinusLogProbMetric: 27.2843 - val_loss: 28.2775 - val_MinusLogProbMetric: 28.2775 - lr: 3.1250e-05 - 44s/epoch - 226ms/step
Epoch 529/1000
2023-10-25 20:48:17.363 
Epoch 529/1000 
	 loss: 27.2832, MinusLogProbMetric: 27.2832, val_loss: 28.2714, val_MinusLogProbMetric: 28.2714

Epoch 529: val_loss did not improve from 28.25298
196/196 - 45s - loss: 27.2832 - MinusLogProbMetric: 27.2832 - val_loss: 28.2714 - val_MinusLogProbMetric: 28.2714 - lr: 3.1250e-05 - 45s/epoch - 228ms/step
Epoch 530/1000
2023-10-25 20:49:01.635 
Epoch 530/1000 
	 loss: 27.2842, MinusLogProbMetric: 27.2842, val_loss: 28.2609, val_MinusLogProbMetric: 28.2609

Epoch 530: val_loss did not improve from 28.25298
196/196 - 44s - loss: 27.2842 - MinusLogProbMetric: 27.2842 - val_loss: 28.2609 - val_MinusLogProbMetric: 28.2609 - lr: 3.1250e-05 - 44s/epoch - 226ms/step
Epoch 531/1000
2023-10-25 20:49:46.455 
Epoch 531/1000 
	 loss: 27.2854, MinusLogProbMetric: 27.2854, val_loss: 28.2646, val_MinusLogProbMetric: 28.2646

Epoch 531: val_loss did not improve from 28.25298
196/196 - 45s - loss: 27.2854 - MinusLogProbMetric: 27.2854 - val_loss: 28.2646 - val_MinusLogProbMetric: 28.2646 - lr: 3.1250e-05 - 45s/epoch - 229ms/step
Epoch 532/1000
2023-10-25 20:50:31.468 
Epoch 532/1000 
	 loss: 27.2863, MinusLogProbMetric: 27.2863, val_loss: 28.2838, val_MinusLogProbMetric: 28.2838

Epoch 532: val_loss did not improve from 28.25298
196/196 - 45s - loss: 27.2863 - MinusLogProbMetric: 27.2863 - val_loss: 28.2838 - val_MinusLogProbMetric: 28.2838 - lr: 3.1250e-05 - 45s/epoch - 230ms/step
Epoch 533/1000
2023-10-25 20:51:16.063 
Epoch 533/1000 
	 loss: 27.2862, MinusLogProbMetric: 27.2862, val_loss: 28.2717, val_MinusLogProbMetric: 28.2717

Epoch 533: val_loss did not improve from 28.25298
196/196 - 45s - loss: 27.2862 - MinusLogProbMetric: 27.2862 - val_loss: 28.2717 - val_MinusLogProbMetric: 28.2717 - lr: 3.1250e-05 - 45s/epoch - 228ms/step
Epoch 534/1000
2023-10-25 20:52:00.659 
Epoch 534/1000 
	 loss: 27.2839, MinusLogProbMetric: 27.2839, val_loss: 28.2595, val_MinusLogProbMetric: 28.2595

Epoch 534: val_loss did not improve from 28.25298
196/196 - 45s - loss: 27.2839 - MinusLogProbMetric: 27.2839 - val_loss: 28.2595 - val_MinusLogProbMetric: 28.2595 - lr: 3.1250e-05 - 45s/epoch - 228ms/step
Epoch 535/1000
2023-10-25 20:52:45.042 
Epoch 535/1000 
	 loss: 27.2872, MinusLogProbMetric: 27.2872, val_loss: 28.2754, val_MinusLogProbMetric: 28.2754

Epoch 535: val_loss did not improve from 28.25298
196/196 - 44s - loss: 27.2872 - MinusLogProbMetric: 27.2872 - val_loss: 28.2754 - val_MinusLogProbMetric: 28.2754 - lr: 3.1250e-05 - 44s/epoch - 226ms/step
Epoch 536/1000
2023-10-25 20:53:30.041 
Epoch 536/1000 
	 loss: 27.2837, MinusLogProbMetric: 27.2837, val_loss: 28.2685, val_MinusLogProbMetric: 28.2685

Epoch 536: val_loss did not improve from 28.25298
196/196 - 45s - loss: 27.2837 - MinusLogProbMetric: 27.2837 - val_loss: 28.2685 - val_MinusLogProbMetric: 28.2685 - lr: 3.1250e-05 - 45s/epoch - 230ms/step
Epoch 537/1000
2023-10-25 20:54:14.914 
Epoch 537/1000 
	 loss: 27.2865, MinusLogProbMetric: 27.2865, val_loss: 28.2820, val_MinusLogProbMetric: 28.2820

Epoch 537: val_loss did not improve from 28.25298
196/196 - 45s - loss: 27.2865 - MinusLogProbMetric: 27.2865 - val_loss: 28.2820 - val_MinusLogProbMetric: 28.2820 - lr: 3.1250e-05 - 45s/epoch - 229ms/step
Epoch 538/1000
2023-10-25 20:54:59.628 
Epoch 538/1000 
	 loss: 27.2849, MinusLogProbMetric: 27.2849, val_loss: 28.2809, val_MinusLogProbMetric: 28.2809

Epoch 538: val_loss did not improve from 28.25298
196/196 - 45s - loss: 27.2849 - MinusLogProbMetric: 27.2849 - val_loss: 28.2809 - val_MinusLogProbMetric: 28.2809 - lr: 3.1250e-05 - 45s/epoch - 228ms/step
Epoch 539/1000
2023-10-25 20:55:44.221 
Epoch 539/1000 
	 loss: 27.2840, MinusLogProbMetric: 27.2840, val_loss: 28.2601, val_MinusLogProbMetric: 28.2601

Epoch 539: val_loss did not improve from 28.25298
196/196 - 45s - loss: 27.2840 - MinusLogProbMetric: 27.2840 - val_loss: 28.2601 - val_MinusLogProbMetric: 28.2601 - lr: 3.1250e-05 - 45s/epoch - 227ms/step
Epoch 540/1000
2023-10-25 20:56:28.520 
Epoch 540/1000 
	 loss: 27.2822, MinusLogProbMetric: 27.2822, val_loss: 28.2647, val_MinusLogProbMetric: 28.2647

Epoch 540: val_loss did not improve from 28.25298
196/196 - 44s - loss: 27.2822 - MinusLogProbMetric: 27.2822 - val_loss: 28.2647 - val_MinusLogProbMetric: 28.2647 - lr: 3.1250e-05 - 44s/epoch - 226ms/step
Epoch 541/1000
2023-10-25 20:57:12.795 
Epoch 541/1000 
	 loss: 27.2857, MinusLogProbMetric: 27.2857, val_loss: 28.2746, val_MinusLogProbMetric: 28.2746

Epoch 541: val_loss did not improve from 28.25298
196/196 - 44s - loss: 27.2857 - MinusLogProbMetric: 27.2857 - val_loss: 28.2746 - val_MinusLogProbMetric: 28.2746 - lr: 3.1250e-05 - 44s/epoch - 226ms/step
Epoch 542/1000
2023-10-25 20:57:57.303 
Epoch 542/1000 
	 loss: 27.2724, MinusLogProbMetric: 27.2724, val_loss: 28.2556, val_MinusLogProbMetric: 28.2556

Epoch 542: val_loss did not improve from 28.25298
196/196 - 45s - loss: 27.2724 - MinusLogProbMetric: 27.2724 - val_loss: 28.2556 - val_MinusLogProbMetric: 28.2556 - lr: 1.5625e-05 - 45s/epoch - 227ms/step
Epoch 543/1000
2023-10-25 20:58:41.891 
Epoch 543/1000 
	 loss: 27.2718, MinusLogProbMetric: 27.2718, val_loss: 28.2617, val_MinusLogProbMetric: 28.2617

Epoch 543: val_loss did not improve from 28.25298
196/196 - 45s - loss: 27.2718 - MinusLogProbMetric: 27.2718 - val_loss: 28.2617 - val_MinusLogProbMetric: 28.2617 - lr: 1.5625e-05 - 45s/epoch - 227ms/step
Epoch 544/1000
2023-10-25 20:59:26.468 
Epoch 544/1000 
	 loss: 27.2722, MinusLogProbMetric: 27.2722, val_loss: 28.2596, val_MinusLogProbMetric: 28.2596

Epoch 544: val_loss did not improve from 28.25298
196/196 - 45s - loss: 27.2722 - MinusLogProbMetric: 27.2722 - val_loss: 28.2596 - val_MinusLogProbMetric: 28.2596 - lr: 1.5625e-05 - 45s/epoch - 227ms/step
Epoch 545/1000
2023-10-25 21:00:10.801 
Epoch 545/1000 
	 loss: 27.2704, MinusLogProbMetric: 27.2704, val_loss: 28.2544, val_MinusLogProbMetric: 28.2544

Epoch 545: val_loss did not improve from 28.25298
196/196 - 44s - loss: 27.2704 - MinusLogProbMetric: 27.2704 - val_loss: 28.2544 - val_MinusLogProbMetric: 28.2544 - lr: 1.5625e-05 - 44s/epoch - 226ms/step
Epoch 546/1000
2023-10-25 21:00:55.484 
Epoch 546/1000 
	 loss: 27.2710, MinusLogProbMetric: 27.2710, val_loss: 28.2612, val_MinusLogProbMetric: 28.2612

Epoch 546: val_loss did not improve from 28.25298
196/196 - 45s - loss: 27.2710 - MinusLogProbMetric: 27.2710 - val_loss: 28.2612 - val_MinusLogProbMetric: 28.2612 - lr: 1.5625e-05 - 45s/epoch - 228ms/step
Epoch 547/1000
2023-10-25 21:01:39.838 
Epoch 547/1000 
	 loss: 27.2718, MinusLogProbMetric: 27.2718, val_loss: 28.2613, val_MinusLogProbMetric: 28.2613

Epoch 547: val_loss did not improve from 28.25298
196/196 - 44s - loss: 27.2718 - MinusLogProbMetric: 27.2718 - val_loss: 28.2613 - val_MinusLogProbMetric: 28.2613 - lr: 1.5625e-05 - 44s/epoch - 226ms/step
Epoch 548/1000
2023-10-25 21:02:24.477 
Epoch 548/1000 
	 loss: 27.2729, MinusLogProbMetric: 27.2729, val_loss: 28.2579, val_MinusLogProbMetric: 28.2579

Epoch 548: val_loss did not improve from 28.25298
196/196 - 45s - loss: 27.2729 - MinusLogProbMetric: 27.2729 - val_loss: 28.2579 - val_MinusLogProbMetric: 28.2579 - lr: 1.5625e-05 - 45s/epoch - 228ms/step
Epoch 549/1000
2023-10-25 21:03:06.129 
Epoch 549/1000 
	 loss: 27.2720, MinusLogProbMetric: 27.2720, val_loss: 28.2662, val_MinusLogProbMetric: 28.2662

Epoch 549: val_loss did not improve from 28.25298
196/196 - 42s - loss: 27.2720 - MinusLogProbMetric: 27.2720 - val_loss: 28.2662 - val_MinusLogProbMetric: 28.2662 - lr: 1.5625e-05 - 42s/epoch - 212ms/step
Epoch 550/1000
2023-10-25 21:03:48.745 
Epoch 550/1000 
	 loss: 27.2716, MinusLogProbMetric: 27.2716, val_loss: 28.2583, val_MinusLogProbMetric: 28.2583

Epoch 550: val_loss did not improve from 28.25298
196/196 - 43s - loss: 27.2716 - MinusLogProbMetric: 27.2716 - val_loss: 28.2583 - val_MinusLogProbMetric: 28.2583 - lr: 1.5625e-05 - 43s/epoch - 217ms/step
Epoch 551/1000
2023-10-25 21:04:32.496 
Epoch 551/1000 
	 loss: 27.2708, MinusLogProbMetric: 27.2708, val_loss: 28.2605, val_MinusLogProbMetric: 28.2605

Epoch 551: val_loss did not improve from 28.25298
196/196 - 44s - loss: 27.2708 - MinusLogProbMetric: 27.2708 - val_loss: 28.2605 - val_MinusLogProbMetric: 28.2605 - lr: 1.5625e-05 - 44s/epoch - 223ms/step
Epoch 552/1000
2023-10-25 21:05:15.700 
Epoch 552/1000 
	 loss: 27.2717, MinusLogProbMetric: 27.2717, val_loss: 28.2630, val_MinusLogProbMetric: 28.2630

Epoch 552: val_loss did not improve from 28.25298
196/196 - 43s - loss: 27.2717 - MinusLogProbMetric: 27.2717 - val_loss: 28.2630 - val_MinusLogProbMetric: 28.2630 - lr: 1.5625e-05 - 43s/epoch - 220ms/step
Epoch 553/1000
2023-10-25 21:05:58.910 
Epoch 553/1000 
	 loss: 27.2705, MinusLogProbMetric: 27.2705, val_loss: 28.2611, val_MinusLogProbMetric: 28.2611

Epoch 553: val_loss did not improve from 28.25298
196/196 - 43s - loss: 27.2705 - MinusLogProbMetric: 27.2705 - val_loss: 28.2611 - val_MinusLogProbMetric: 28.2611 - lr: 1.5625e-05 - 43s/epoch - 220ms/step
Epoch 554/1000
2023-10-25 21:06:43.613 
Epoch 554/1000 
	 loss: 27.2719, MinusLogProbMetric: 27.2719, val_loss: 28.2613, val_MinusLogProbMetric: 28.2613

Epoch 554: val_loss did not improve from 28.25298
196/196 - 45s - loss: 27.2719 - MinusLogProbMetric: 27.2719 - val_loss: 28.2613 - val_MinusLogProbMetric: 28.2613 - lr: 1.5625e-05 - 45s/epoch - 228ms/step
Epoch 555/1000
2023-10-25 21:07:28.458 
Epoch 555/1000 
	 loss: 27.2713, MinusLogProbMetric: 27.2713, val_loss: 28.2548, val_MinusLogProbMetric: 28.2548

Epoch 555: val_loss did not improve from 28.25298
196/196 - 45s - loss: 27.2713 - MinusLogProbMetric: 27.2713 - val_loss: 28.2548 - val_MinusLogProbMetric: 28.2548 - lr: 1.5625e-05 - 45s/epoch - 229ms/step
Epoch 556/1000
2023-10-25 21:08:11.793 
Epoch 556/1000 
	 loss: 27.2716, MinusLogProbMetric: 27.2716, val_loss: 28.2601, val_MinusLogProbMetric: 28.2601

Epoch 556: val_loss did not improve from 28.25298
196/196 - 43s - loss: 27.2716 - MinusLogProbMetric: 27.2716 - val_loss: 28.2601 - val_MinusLogProbMetric: 28.2601 - lr: 1.5625e-05 - 43s/epoch - 221ms/step
Epoch 557/1000
2023-10-25 21:08:56.849 
Epoch 557/1000 
	 loss: 27.2709, MinusLogProbMetric: 27.2709, val_loss: 28.2616, val_MinusLogProbMetric: 28.2616

Epoch 557: val_loss did not improve from 28.25298
196/196 - 45s - loss: 27.2709 - MinusLogProbMetric: 27.2709 - val_loss: 28.2616 - val_MinusLogProbMetric: 28.2616 - lr: 1.5625e-05 - 45s/epoch - 230ms/step
Epoch 558/1000
2023-10-25 21:09:40.861 
Epoch 558/1000 
	 loss: 27.2718, MinusLogProbMetric: 27.2718, val_loss: 28.2631, val_MinusLogProbMetric: 28.2631

Epoch 558: val_loss did not improve from 28.25298
196/196 - 44s - loss: 27.2718 - MinusLogProbMetric: 27.2718 - val_loss: 28.2631 - val_MinusLogProbMetric: 28.2631 - lr: 1.5625e-05 - 44s/epoch - 225ms/step
Epoch 559/1000
2023-10-25 21:10:24.637 
Epoch 559/1000 
	 loss: 27.2728, MinusLogProbMetric: 27.2728, val_loss: 28.2561, val_MinusLogProbMetric: 28.2561

Epoch 559: val_loss did not improve from 28.25298
196/196 - 44s - loss: 27.2728 - MinusLogProbMetric: 27.2728 - val_loss: 28.2561 - val_MinusLogProbMetric: 28.2561 - lr: 1.5625e-05 - 44s/epoch - 223ms/step
Epoch 560/1000
2023-10-25 21:11:09.015 
Epoch 560/1000 
	 loss: 27.2715, MinusLogProbMetric: 27.2715, val_loss: 28.2658, val_MinusLogProbMetric: 28.2658

Epoch 560: val_loss did not improve from 28.25298
196/196 - 44s - loss: 27.2715 - MinusLogProbMetric: 27.2715 - val_loss: 28.2658 - val_MinusLogProbMetric: 28.2658 - lr: 1.5625e-05 - 44s/epoch - 226ms/step
Epoch 561/1000
2023-10-25 21:11:53.297 
Epoch 561/1000 
	 loss: 27.2711, MinusLogProbMetric: 27.2711, val_loss: 28.2575, val_MinusLogProbMetric: 28.2575

Epoch 561: val_loss did not improve from 28.25298
196/196 - 44s - loss: 27.2711 - MinusLogProbMetric: 27.2711 - val_loss: 28.2575 - val_MinusLogProbMetric: 28.2575 - lr: 1.5625e-05 - 44s/epoch - 226ms/step
Epoch 562/1000
2023-10-25 21:12:37.582 
Epoch 562/1000 
	 loss: 27.2707, MinusLogProbMetric: 27.2707, val_loss: 28.2657, val_MinusLogProbMetric: 28.2657

Epoch 562: val_loss did not improve from 28.25298
196/196 - 44s - loss: 27.2707 - MinusLogProbMetric: 27.2707 - val_loss: 28.2657 - val_MinusLogProbMetric: 28.2657 - lr: 1.5625e-05 - 44s/epoch - 226ms/step
Epoch 563/1000
2023-10-25 21:13:22.258 
Epoch 563/1000 
	 loss: 27.2712, MinusLogProbMetric: 27.2712, val_loss: 28.2581, val_MinusLogProbMetric: 28.2581

Epoch 563: val_loss did not improve from 28.25298
196/196 - 45s - loss: 27.2712 - MinusLogProbMetric: 27.2712 - val_loss: 28.2581 - val_MinusLogProbMetric: 28.2581 - lr: 1.5625e-05 - 45s/epoch - 228ms/step
Epoch 564/1000
2023-10-25 21:14:07.196 
Epoch 564/1000 
	 loss: 27.2706, MinusLogProbMetric: 27.2706, val_loss: 28.2716, val_MinusLogProbMetric: 28.2716

Epoch 564: val_loss did not improve from 28.25298
196/196 - 45s - loss: 27.2706 - MinusLogProbMetric: 27.2706 - val_loss: 28.2716 - val_MinusLogProbMetric: 28.2716 - lr: 1.5625e-05 - 45s/epoch - 229ms/step
Epoch 565/1000
2023-10-25 21:14:51.840 
Epoch 565/1000 
	 loss: 27.2714, MinusLogProbMetric: 27.2714, val_loss: 28.2512, val_MinusLogProbMetric: 28.2512

Epoch 565: val_loss improved from 28.25298 to 28.25121, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_356/weights/best_weights.h5
196/196 - 46s - loss: 27.2714 - MinusLogProbMetric: 27.2714 - val_loss: 28.2512 - val_MinusLogProbMetric: 28.2512 - lr: 1.5625e-05 - 46s/epoch - 232ms/step
Epoch 566/1000
2023-10-25 21:15:37.150 
Epoch 566/1000 
	 loss: 27.2710, MinusLogProbMetric: 27.2710, val_loss: 28.2601, val_MinusLogProbMetric: 28.2601

Epoch 566: val_loss did not improve from 28.25121
196/196 - 44s - loss: 27.2710 - MinusLogProbMetric: 27.2710 - val_loss: 28.2601 - val_MinusLogProbMetric: 28.2601 - lr: 1.5625e-05 - 44s/epoch - 227ms/step
Epoch 567/1000
2023-10-25 21:16:21.091 
Epoch 567/1000 
	 loss: 27.2703, MinusLogProbMetric: 27.2703, val_loss: 28.2565, val_MinusLogProbMetric: 28.2565

Epoch 567: val_loss did not improve from 28.25121
196/196 - 44s - loss: 27.2703 - MinusLogProbMetric: 27.2703 - val_loss: 28.2565 - val_MinusLogProbMetric: 28.2565 - lr: 1.5625e-05 - 44s/epoch - 224ms/step
Epoch 568/1000
2023-10-25 21:17:05.364 
Epoch 568/1000 
	 loss: 27.2708, MinusLogProbMetric: 27.2708, val_loss: 28.2625, val_MinusLogProbMetric: 28.2625

Epoch 568: val_loss did not improve from 28.25121
196/196 - 44s - loss: 27.2708 - MinusLogProbMetric: 27.2708 - val_loss: 28.2625 - val_MinusLogProbMetric: 28.2625 - lr: 1.5625e-05 - 44s/epoch - 226ms/step
Epoch 569/1000
2023-10-25 21:17:49.417 
Epoch 569/1000 
	 loss: 27.2713, MinusLogProbMetric: 27.2713, val_loss: 28.2569, val_MinusLogProbMetric: 28.2569

Epoch 569: val_loss did not improve from 28.25121
196/196 - 44s - loss: 27.2713 - MinusLogProbMetric: 27.2713 - val_loss: 28.2569 - val_MinusLogProbMetric: 28.2569 - lr: 1.5625e-05 - 44s/epoch - 225ms/step
Epoch 570/1000
2023-10-25 21:18:33.976 
Epoch 570/1000 
	 loss: 27.2710, MinusLogProbMetric: 27.2710, val_loss: 28.2638, val_MinusLogProbMetric: 28.2638

Epoch 570: val_loss did not improve from 28.25121
196/196 - 45s - loss: 27.2710 - MinusLogProbMetric: 27.2710 - val_loss: 28.2638 - val_MinusLogProbMetric: 28.2638 - lr: 1.5625e-05 - 45s/epoch - 227ms/step
Epoch 571/1000
2023-10-25 21:19:18.599 
Epoch 571/1000 
	 loss: 27.2710, MinusLogProbMetric: 27.2710, val_loss: 28.2584, val_MinusLogProbMetric: 28.2584

Epoch 571: val_loss did not improve from 28.25121
196/196 - 45s - loss: 27.2710 - MinusLogProbMetric: 27.2710 - val_loss: 28.2584 - val_MinusLogProbMetric: 28.2584 - lr: 1.5625e-05 - 45s/epoch - 228ms/step
Epoch 572/1000
2023-10-25 21:20:02.862 
Epoch 572/1000 
	 loss: 27.2721, MinusLogProbMetric: 27.2721, val_loss: 28.2635, val_MinusLogProbMetric: 28.2635

Epoch 572: val_loss did not improve from 28.25121
196/196 - 44s - loss: 27.2721 - MinusLogProbMetric: 27.2721 - val_loss: 28.2635 - val_MinusLogProbMetric: 28.2635 - lr: 1.5625e-05 - 44s/epoch - 226ms/step
Epoch 573/1000
2023-10-25 21:20:47.143 
Epoch 573/1000 
	 loss: 27.2706, MinusLogProbMetric: 27.2706, val_loss: 28.2603, val_MinusLogProbMetric: 28.2603

Epoch 573: val_loss did not improve from 28.25121
196/196 - 44s - loss: 27.2706 - MinusLogProbMetric: 27.2706 - val_loss: 28.2603 - val_MinusLogProbMetric: 28.2603 - lr: 1.5625e-05 - 44s/epoch - 226ms/step
Epoch 574/1000
2023-10-25 21:21:31.376 
Epoch 574/1000 
	 loss: 27.2700, MinusLogProbMetric: 27.2700, val_loss: 28.2636, val_MinusLogProbMetric: 28.2636

Epoch 574: val_loss did not improve from 28.25121
196/196 - 44s - loss: 27.2700 - MinusLogProbMetric: 27.2700 - val_loss: 28.2636 - val_MinusLogProbMetric: 28.2636 - lr: 1.5625e-05 - 44s/epoch - 226ms/step
Epoch 575/1000
2023-10-25 21:22:15.883 
Epoch 575/1000 
	 loss: 27.2709, MinusLogProbMetric: 27.2709, val_loss: 28.2638, val_MinusLogProbMetric: 28.2638

Epoch 575: val_loss did not improve from 28.25121
196/196 - 45s - loss: 27.2709 - MinusLogProbMetric: 27.2709 - val_loss: 28.2638 - val_MinusLogProbMetric: 28.2638 - lr: 1.5625e-05 - 45s/epoch - 227ms/step
Epoch 576/1000
2023-10-25 21:23:00.392 
Epoch 576/1000 
	 loss: 27.2710, MinusLogProbMetric: 27.2710, val_loss: 28.2589, val_MinusLogProbMetric: 28.2589

Epoch 576: val_loss did not improve from 28.25121
196/196 - 45s - loss: 27.2710 - MinusLogProbMetric: 27.2710 - val_loss: 28.2589 - val_MinusLogProbMetric: 28.2589 - lr: 1.5625e-05 - 45s/epoch - 227ms/step
Epoch 577/1000
2023-10-25 21:23:44.335 
Epoch 577/1000 
	 loss: 27.2702, MinusLogProbMetric: 27.2702, val_loss: 28.2590, val_MinusLogProbMetric: 28.2590

Epoch 577: val_loss did not improve from 28.25121
196/196 - 44s - loss: 27.2702 - MinusLogProbMetric: 27.2702 - val_loss: 28.2590 - val_MinusLogProbMetric: 28.2590 - lr: 1.5625e-05 - 44s/epoch - 224ms/step
Epoch 578/1000
2023-10-25 21:24:28.387 
Epoch 578/1000 
	 loss: 27.2706, MinusLogProbMetric: 27.2706, val_loss: 28.2613, val_MinusLogProbMetric: 28.2613

Epoch 578: val_loss did not improve from 28.25121
196/196 - 44s - loss: 27.2706 - MinusLogProbMetric: 27.2706 - val_loss: 28.2613 - val_MinusLogProbMetric: 28.2613 - lr: 1.5625e-05 - 44s/epoch - 225ms/step
Epoch 579/1000
2023-10-25 21:25:12.602 
Epoch 579/1000 
	 loss: 27.2689, MinusLogProbMetric: 27.2689, val_loss: 28.2591, val_MinusLogProbMetric: 28.2591

Epoch 579: val_loss did not improve from 28.25121
196/196 - 44s - loss: 27.2689 - MinusLogProbMetric: 27.2689 - val_loss: 28.2591 - val_MinusLogProbMetric: 28.2591 - lr: 1.5625e-05 - 44s/epoch - 226ms/step
Epoch 580/1000
2023-10-25 21:25:56.323 
Epoch 580/1000 
	 loss: 27.2701, MinusLogProbMetric: 27.2701, val_loss: 28.2581, val_MinusLogProbMetric: 28.2581

Epoch 580: val_loss did not improve from 28.25121
196/196 - 44s - loss: 27.2701 - MinusLogProbMetric: 27.2701 - val_loss: 28.2581 - val_MinusLogProbMetric: 28.2581 - lr: 1.5625e-05 - 44s/epoch - 223ms/step
Epoch 581/1000
2023-10-25 21:26:40.777 
Epoch 581/1000 
	 loss: 27.2693, MinusLogProbMetric: 27.2693, val_loss: 28.2633, val_MinusLogProbMetric: 28.2633

Epoch 581: val_loss did not improve from 28.25121
196/196 - 44s - loss: 27.2693 - MinusLogProbMetric: 27.2693 - val_loss: 28.2633 - val_MinusLogProbMetric: 28.2633 - lr: 1.5625e-05 - 44s/epoch - 227ms/step
Epoch 582/1000
2023-10-25 21:27:24.910 
Epoch 582/1000 
	 loss: 27.2696, MinusLogProbMetric: 27.2696, val_loss: 28.2618, val_MinusLogProbMetric: 28.2618

Epoch 582: val_loss did not improve from 28.25121
196/196 - 44s - loss: 27.2696 - MinusLogProbMetric: 27.2696 - val_loss: 28.2618 - val_MinusLogProbMetric: 28.2618 - lr: 1.5625e-05 - 44s/epoch - 225ms/step
Epoch 583/1000
2023-10-25 21:28:09.404 
Epoch 583/1000 
	 loss: 27.2696, MinusLogProbMetric: 27.2696, val_loss: 28.2590, val_MinusLogProbMetric: 28.2590

Epoch 583: val_loss did not improve from 28.25121
196/196 - 44s - loss: 27.2696 - MinusLogProbMetric: 27.2696 - val_loss: 28.2590 - val_MinusLogProbMetric: 28.2590 - lr: 1.5625e-05 - 44s/epoch - 227ms/step
Epoch 584/1000
2023-10-25 21:28:53.672 
Epoch 584/1000 
	 loss: 27.2700, MinusLogProbMetric: 27.2700, val_loss: 28.2590, val_MinusLogProbMetric: 28.2590

Epoch 584: val_loss did not improve from 28.25121
196/196 - 44s - loss: 27.2700 - MinusLogProbMetric: 27.2700 - val_loss: 28.2590 - val_MinusLogProbMetric: 28.2590 - lr: 1.5625e-05 - 44s/epoch - 226ms/step
Epoch 585/1000
2023-10-25 21:29:37.802 
Epoch 585/1000 
	 loss: 27.2696, MinusLogProbMetric: 27.2696, val_loss: 28.2644, val_MinusLogProbMetric: 28.2644

Epoch 585: val_loss did not improve from 28.25121
196/196 - 44s - loss: 27.2696 - MinusLogProbMetric: 27.2696 - val_loss: 28.2644 - val_MinusLogProbMetric: 28.2644 - lr: 1.5625e-05 - 44s/epoch - 225ms/step
Epoch 586/1000
2023-10-25 21:30:21.953 
Epoch 586/1000 
	 loss: 27.2694, MinusLogProbMetric: 27.2694, val_loss: 28.2581, val_MinusLogProbMetric: 28.2581

Epoch 586: val_loss did not improve from 28.25121
196/196 - 44s - loss: 27.2694 - MinusLogProbMetric: 27.2694 - val_loss: 28.2581 - val_MinusLogProbMetric: 28.2581 - lr: 1.5625e-05 - 44s/epoch - 225ms/step
Epoch 587/1000
2023-10-25 21:31:06.013 
Epoch 587/1000 
	 loss: 27.2690, MinusLogProbMetric: 27.2690, val_loss: 28.2607, val_MinusLogProbMetric: 28.2607

Epoch 587: val_loss did not improve from 28.25121
196/196 - 44s - loss: 27.2690 - MinusLogProbMetric: 27.2690 - val_loss: 28.2607 - val_MinusLogProbMetric: 28.2607 - lr: 1.5625e-05 - 44s/epoch - 225ms/step
Epoch 588/1000
2023-10-25 21:31:50.654 
Epoch 588/1000 
	 loss: 27.2695, MinusLogProbMetric: 27.2695, val_loss: 28.2636, val_MinusLogProbMetric: 28.2636

Epoch 588: val_loss did not improve from 28.25121
196/196 - 45s - loss: 27.2695 - MinusLogProbMetric: 27.2695 - val_loss: 28.2636 - val_MinusLogProbMetric: 28.2636 - lr: 1.5625e-05 - 45s/epoch - 228ms/step
Epoch 589/1000
2023-10-25 21:32:34.891 
Epoch 589/1000 
	 loss: 27.2708, MinusLogProbMetric: 27.2708, val_loss: 28.2662, val_MinusLogProbMetric: 28.2662

Epoch 589: val_loss did not improve from 28.25121
196/196 - 44s - loss: 27.2708 - MinusLogProbMetric: 27.2708 - val_loss: 28.2662 - val_MinusLogProbMetric: 28.2662 - lr: 1.5625e-05 - 44s/epoch - 226ms/step
Epoch 590/1000
2023-10-25 21:33:18.726 
Epoch 590/1000 
	 loss: 27.2693, MinusLogProbMetric: 27.2693, val_loss: 28.2585, val_MinusLogProbMetric: 28.2585

Epoch 590: val_loss did not improve from 28.25121
196/196 - 44s - loss: 27.2693 - MinusLogProbMetric: 27.2693 - val_loss: 28.2585 - val_MinusLogProbMetric: 28.2585 - lr: 1.5625e-05 - 44s/epoch - 224ms/step
Epoch 591/1000
2023-10-25 21:34:02.635 
Epoch 591/1000 
	 loss: 27.2710, MinusLogProbMetric: 27.2710, val_loss: 28.2533, val_MinusLogProbMetric: 28.2533

Epoch 591: val_loss did not improve from 28.25121
196/196 - 44s - loss: 27.2710 - MinusLogProbMetric: 27.2710 - val_loss: 28.2533 - val_MinusLogProbMetric: 28.2533 - lr: 1.5625e-05 - 44s/epoch - 224ms/step
Epoch 592/1000
2023-10-25 21:34:47.417 
Epoch 592/1000 
	 loss: 27.2687, MinusLogProbMetric: 27.2687, val_loss: 28.2616, val_MinusLogProbMetric: 28.2616

Epoch 592: val_loss did not improve from 28.25121
196/196 - 45s - loss: 27.2687 - MinusLogProbMetric: 27.2687 - val_loss: 28.2616 - val_MinusLogProbMetric: 28.2616 - lr: 1.5625e-05 - 45s/epoch - 228ms/step
Epoch 593/1000
2023-10-25 21:35:31.540 
Epoch 593/1000 
	 loss: 27.2692, MinusLogProbMetric: 27.2692, val_loss: 28.2570, val_MinusLogProbMetric: 28.2570

Epoch 593: val_loss did not improve from 28.25121
196/196 - 44s - loss: 27.2692 - MinusLogProbMetric: 27.2692 - val_loss: 28.2570 - val_MinusLogProbMetric: 28.2570 - lr: 1.5625e-05 - 44s/epoch - 225ms/step
Epoch 594/1000
2023-10-25 21:36:15.335 
Epoch 594/1000 
	 loss: 27.2692, MinusLogProbMetric: 27.2692, val_loss: 28.2668, val_MinusLogProbMetric: 28.2668

Epoch 594: val_loss did not improve from 28.25121
196/196 - 44s - loss: 27.2692 - MinusLogProbMetric: 27.2692 - val_loss: 28.2668 - val_MinusLogProbMetric: 28.2668 - lr: 1.5625e-05 - 44s/epoch - 223ms/step
Epoch 595/1000
2023-10-25 21:36:59.611 
Epoch 595/1000 
	 loss: 27.2689, MinusLogProbMetric: 27.2689, val_loss: 28.2645, val_MinusLogProbMetric: 28.2645

Epoch 595: val_loss did not improve from 28.25121
196/196 - 44s - loss: 27.2689 - MinusLogProbMetric: 27.2689 - val_loss: 28.2645 - val_MinusLogProbMetric: 28.2645 - lr: 1.5625e-05 - 44s/epoch - 226ms/step
Epoch 596/1000
2023-10-25 21:37:44.100 
Epoch 596/1000 
	 loss: 27.2688, MinusLogProbMetric: 27.2688, val_loss: 28.2633, val_MinusLogProbMetric: 28.2633

Epoch 596: val_loss did not improve from 28.25121
196/196 - 44s - loss: 27.2688 - MinusLogProbMetric: 27.2688 - val_loss: 28.2633 - val_MinusLogProbMetric: 28.2633 - lr: 1.5625e-05 - 44s/epoch - 227ms/step
Epoch 597/1000
2023-10-25 21:38:27.784 
Epoch 597/1000 
	 loss: 27.2686, MinusLogProbMetric: 27.2686, val_loss: 28.2608, val_MinusLogProbMetric: 28.2608

Epoch 597: val_loss did not improve from 28.25121
196/196 - 44s - loss: 27.2686 - MinusLogProbMetric: 27.2686 - val_loss: 28.2608 - val_MinusLogProbMetric: 28.2608 - lr: 1.5625e-05 - 44s/epoch - 223ms/step
Epoch 598/1000
2023-10-25 21:39:12.032 
Epoch 598/1000 
	 loss: 27.2692, MinusLogProbMetric: 27.2692, val_loss: 28.2601, val_MinusLogProbMetric: 28.2601

Epoch 598: val_loss did not improve from 28.25121
196/196 - 44s - loss: 27.2692 - MinusLogProbMetric: 27.2692 - val_loss: 28.2601 - val_MinusLogProbMetric: 28.2601 - lr: 1.5625e-05 - 44s/epoch - 226ms/step
Epoch 599/1000
2023-10-25 21:39:56.588 
Epoch 599/1000 
	 loss: 27.2701, MinusLogProbMetric: 27.2701, val_loss: 28.2555, val_MinusLogProbMetric: 28.2555

Epoch 599: val_loss did not improve from 28.25121
196/196 - 45s - loss: 27.2701 - MinusLogProbMetric: 27.2701 - val_loss: 28.2555 - val_MinusLogProbMetric: 28.2555 - lr: 1.5625e-05 - 45s/epoch - 227ms/step
Epoch 600/1000
2023-10-25 21:40:40.955 
Epoch 600/1000 
	 loss: 27.2701, MinusLogProbMetric: 27.2701, val_loss: 28.2557, val_MinusLogProbMetric: 28.2557

Epoch 600: val_loss did not improve from 28.25121
196/196 - 44s - loss: 27.2701 - MinusLogProbMetric: 27.2701 - val_loss: 28.2557 - val_MinusLogProbMetric: 28.2557 - lr: 1.5625e-05 - 44s/epoch - 226ms/step
Epoch 601/1000
2023-10-25 21:41:25.325 
Epoch 601/1000 
	 loss: 27.2679, MinusLogProbMetric: 27.2679, val_loss: 28.2642, val_MinusLogProbMetric: 28.2642

Epoch 601: val_loss did not improve from 28.25121
196/196 - 44s - loss: 27.2679 - MinusLogProbMetric: 27.2679 - val_loss: 28.2642 - val_MinusLogProbMetric: 28.2642 - lr: 1.5625e-05 - 44s/epoch - 226ms/step
Epoch 602/1000
2023-10-25 21:42:09.979 
Epoch 602/1000 
	 loss: 27.2680, MinusLogProbMetric: 27.2680, val_loss: 28.2631, val_MinusLogProbMetric: 28.2631

Epoch 602: val_loss did not improve from 28.25121
196/196 - 45s - loss: 27.2680 - MinusLogProbMetric: 27.2680 - val_loss: 28.2631 - val_MinusLogProbMetric: 28.2631 - lr: 1.5625e-05 - 45s/epoch - 228ms/step
Epoch 603/1000
2023-10-25 21:42:54.272 
Epoch 603/1000 
	 loss: 27.2695, MinusLogProbMetric: 27.2695, val_loss: 28.2642, val_MinusLogProbMetric: 28.2642

Epoch 603: val_loss did not improve from 28.25121
196/196 - 44s - loss: 27.2695 - MinusLogProbMetric: 27.2695 - val_loss: 28.2642 - val_MinusLogProbMetric: 28.2642 - lr: 1.5625e-05 - 44s/epoch - 226ms/step
Epoch 604/1000
2023-10-25 21:43:39.141 
Epoch 604/1000 
	 loss: 27.2685, MinusLogProbMetric: 27.2685, val_loss: 28.2567, val_MinusLogProbMetric: 28.2567

Epoch 604: val_loss did not improve from 28.25121
196/196 - 45s - loss: 27.2685 - MinusLogProbMetric: 27.2685 - val_loss: 28.2567 - val_MinusLogProbMetric: 28.2567 - lr: 1.5625e-05 - 45s/epoch - 229ms/step
Epoch 605/1000
2023-10-25 21:44:23.721 
Epoch 605/1000 
	 loss: 27.2694, MinusLogProbMetric: 27.2694, val_loss: 28.2569, val_MinusLogProbMetric: 28.2569

Epoch 605: val_loss did not improve from 28.25121
196/196 - 45s - loss: 27.2694 - MinusLogProbMetric: 27.2694 - val_loss: 28.2569 - val_MinusLogProbMetric: 28.2569 - lr: 1.5625e-05 - 45s/epoch - 227ms/step
Epoch 606/1000
2023-10-25 21:45:08.700 
Epoch 606/1000 
	 loss: 27.2689, MinusLogProbMetric: 27.2689, val_loss: 28.2578, val_MinusLogProbMetric: 28.2578

Epoch 606: val_loss did not improve from 28.25121
196/196 - 45s - loss: 27.2689 - MinusLogProbMetric: 27.2689 - val_loss: 28.2578 - val_MinusLogProbMetric: 28.2578 - lr: 1.5625e-05 - 45s/epoch - 229ms/step
Epoch 607/1000
2023-10-25 21:45:53.433 
Epoch 607/1000 
	 loss: 27.2683, MinusLogProbMetric: 27.2683, val_loss: 28.2635, val_MinusLogProbMetric: 28.2635

Epoch 607: val_loss did not improve from 28.25121
196/196 - 45s - loss: 27.2683 - MinusLogProbMetric: 27.2683 - val_loss: 28.2635 - val_MinusLogProbMetric: 28.2635 - lr: 1.5625e-05 - 45s/epoch - 228ms/step
Epoch 608/1000
2023-10-25 21:46:37.946 
Epoch 608/1000 
	 loss: 27.2687, MinusLogProbMetric: 27.2687, val_loss: 28.2537, val_MinusLogProbMetric: 28.2537

Epoch 608: val_loss did not improve from 28.25121
196/196 - 45s - loss: 27.2687 - MinusLogProbMetric: 27.2687 - val_loss: 28.2537 - val_MinusLogProbMetric: 28.2537 - lr: 1.5625e-05 - 45s/epoch - 227ms/step
Epoch 609/1000
2023-10-25 21:47:22.556 
Epoch 609/1000 
	 loss: 27.2676, MinusLogProbMetric: 27.2676, val_loss: 28.2591, val_MinusLogProbMetric: 28.2591

Epoch 609: val_loss did not improve from 28.25121
196/196 - 45s - loss: 27.2676 - MinusLogProbMetric: 27.2676 - val_loss: 28.2591 - val_MinusLogProbMetric: 28.2591 - lr: 1.5625e-05 - 45s/epoch - 228ms/step
Epoch 610/1000
2023-10-25 21:48:07.307 
Epoch 610/1000 
	 loss: 27.2692, MinusLogProbMetric: 27.2692, val_loss: 28.2634, val_MinusLogProbMetric: 28.2634

Epoch 610: val_loss did not improve from 28.25121
196/196 - 45s - loss: 27.2692 - MinusLogProbMetric: 27.2692 - val_loss: 28.2634 - val_MinusLogProbMetric: 28.2634 - lr: 1.5625e-05 - 45s/epoch - 228ms/step
Epoch 611/1000
2023-10-25 21:48:52.321 
Epoch 611/1000 
	 loss: 27.2680, MinusLogProbMetric: 27.2680, val_loss: 28.2617, val_MinusLogProbMetric: 28.2617

Epoch 611: val_loss did not improve from 28.25121
196/196 - 45s - loss: 27.2680 - MinusLogProbMetric: 27.2680 - val_loss: 28.2617 - val_MinusLogProbMetric: 28.2617 - lr: 1.5625e-05 - 45s/epoch - 230ms/step
Epoch 612/1000
2023-10-25 21:49:37.085 
Epoch 612/1000 
	 loss: 27.2673, MinusLogProbMetric: 27.2673, val_loss: 28.2584, val_MinusLogProbMetric: 28.2584

Epoch 612: val_loss did not improve from 28.25121
196/196 - 45s - loss: 27.2673 - MinusLogProbMetric: 27.2673 - val_loss: 28.2584 - val_MinusLogProbMetric: 28.2584 - lr: 1.5625e-05 - 45s/epoch - 228ms/step
Epoch 613/1000
2023-10-25 21:50:21.862 
Epoch 613/1000 
	 loss: 27.2688, MinusLogProbMetric: 27.2688, val_loss: 28.2578, val_MinusLogProbMetric: 28.2578

Epoch 613: val_loss did not improve from 28.25121
196/196 - 45s - loss: 27.2688 - MinusLogProbMetric: 27.2688 - val_loss: 28.2578 - val_MinusLogProbMetric: 28.2578 - lr: 1.5625e-05 - 45s/epoch - 228ms/step
Epoch 614/1000
2023-10-25 21:51:06.968 
Epoch 614/1000 
	 loss: 27.2684, MinusLogProbMetric: 27.2684, val_loss: 28.2615, val_MinusLogProbMetric: 28.2615

Epoch 614: val_loss did not improve from 28.25121
196/196 - 45s - loss: 27.2684 - MinusLogProbMetric: 27.2684 - val_loss: 28.2615 - val_MinusLogProbMetric: 28.2615 - lr: 1.5625e-05 - 45s/epoch - 230ms/step
Epoch 615/1000
2023-10-25 21:51:51.691 
Epoch 615/1000 
	 loss: 27.2687, MinusLogProbMetric: 27.2687, val_loss: 28.2565, val_MinusLogProbMetric: 28.2565

Epoch 615: val_loss did not improve from 28.25121
196/196 - 45s - loss: 27.2687 - MinusLogProbMetric: 27.2687 - val_loss: 28.2565 - val_MinusLogProbMetric: 28.2565 - lr: 1.5625e-05 - 45s/epoch - 228ms/step
Epoch 616/1000
2023-10-25 21:52:36.026 
Epoch 616/1000 
	 loss: 27.2630, MinusLogProbMetric: 27.2630, val_loss: 28.2561, val_MinusLogProbMetric: 28.2561

Epoch 616: val_loss did not improve from 28.25121
196/196 - 44s - loss: 27.2630 - MinusLogProbMetric: 27.2630 - val_loss: 28.2561 - val_MinusLogProbMetric: 28.2561 - lr: 7.8125e-06 - 44s/epoch - 226ms/step
Epoch 617/1000
2023-10-25 21:53:20.570 
Epoch 617/1000 
	 loss: 27.2626, MinusLogProbMetric: 27.2626, val_loss: 28.2571, val_MinusLogProbMetric: 28.2571

Epoch 617: val_loss did not improve from 28.25121
196/196 - 45s - loss: 27.2626 - MinusLogProbMetric: 27.2626 - val_loss: 28.2571 - val_MinusLogProbMetric: 28.2571 - lr: 7.8125e-06 - 45s/epoch - 227ms/step
Epoch 618/1000
2023-10-25 21:54:04.715 
Epoch 618/1000 
	 loss: 27.2624, MinusLogProbMetric: 27.2624, val_loss: 28.2556, val_MinusLogProbMetric: 28.2556

Epoch 618: val_loss did not improve from 28.25121
196/196 - 44s - loss: 27.2624 - MinusLogProbMetric: 27.2624 - val_loss: 28.2556 - val_MinusLogProbMetric: 28.2556 - lr: 7.8125e-06 - 44s/epoch - 225ms/step
Epoch 619/1000
2023-10-25 21:54:49.412 
Epoch 619/1000 
	 loss: 27.2627, MinusLogProbMetric: 27.2627, val_loss: 28.2583, val_MinusLogProbMetric: 28.2583

Epoch 619: val_loss did not improve from 28.25121
196/196 - 45s - loss: 27.2627 - MinusLogProbMetric: 27.2627 - val_loss: 28.2583 - val_MinusLogProbMetric: 28.2583 - lr: 7.8125e-06 - 45s/epoch - 228ms/step
Epoch 620/1000
2023-10-25 21:55:33.740 
Epoch 620/1000 
	 loss: 27.2624, MinusLogProbMetric: 27.2624, val_loss: 28.2592, val_MinusLogProbMetric: 28.2592

Epoch 620: val_loss did not improve from 28.25121
196/196 - 44s - loss: 27.2624 - MinusLogProbMetric: 27.2624 - val_loss: 28.2592 - val_MinusLogProbMetric: 28.2592 - lr: 7.8125e-06 - 44s/epoch - 226ms/step
Epoch 621/1000
2023-10-25 21:56:18.635 
Epoch 621/1000 
	 loss: 27.2620, MinusLogProbMetric: 27.2620, val_loss: 28.2555, val_MinusLogProbMetric: 28.2555

Epoch 621: val_loss did not improve from 28.25121
196/196 - 45s - loss: 27.2620 - MinusLogProbMetric: 27.2620 - val_loss: 28.2555 - val_MinusLogProbMetric: 28.2555 - lr: 7.8125e-06 - 45s/epoch - 229ms/step
Epoch 622/1000
2023-10-25 21:57:03.294 
Epoch 622/1000 
	 loss: 27.2615, MinusLogProbMetric: 27.2615, val_loss: 28.2550, val_MinusLogProbMetric: 28.2550

Epoch 622: val_loss did not improve from 28.25121
196/196 - 45s - loss: 27.2615 - MinusLogProbMetric: 27.2615 - val_loss: 28.2550 - val_MinusLogProbMetric: 28.2550 - lr: 7.8125e-06 - 45s/epoch - 228ms/step
Epoch 623/1000
2023-10-25 21:57:48.153 
Epoch 623/1000 
	 loss: 27.2625, MinusLogProbMetric: 27.2625, val_loss: 28.2573, val_MinusLogProbMetric: 28.2573

Epoch 623: val_loss did not improve from 28.25121
196/196 - 45s - loss: 27.2625 - MinusLogProbMetric: 27.2625 - val_loss: 28.2573 - val_MinusLogProbMetric: 28.2573 - lr: 7.8125e-06 - 45s/epoch - 229ms/step
Epoch 624/1000
2023-10-25 21:58:32.597 
Epoch 624/1000 
	 loss: 27.2622, MinusLogProbMetric: 27.2622, val_loss: 28.2563, val_MinusLogProbMetric: 28.2563

Epoch 624: val_loss did not improve from 28.25121
196/196 - 44s - loss: 27.2622 - MinusLogProbMetric: 27.2622 - val_loss: 28.2563 - val_MinusLogProbMetric: 28.2563 - lr: 7.8125e-06 - 44s/epoch - 227ms/step
Epoch 625/1000
2023-10-25 21:59:17.197 
Epoch 625/1000 
	 loss: 27.2618, MinusLogProbMetric: 27.2618, val_loss: 28.2598, val_MinusLogProbMetric: 28.2598

Epoch 625: val_loss did not improve from 28.25121
196/196 - 45s - loss: 27.2618 - MinusLogProbMetric: 27.2618 - val_loss: 28.2598 - val_MinusLogProbMetric: 28.2598 - lr: 7.8125e-06 - 45s/epoch - 228ms/step
Epoch 626/1000
2023-10-25 22:00:00.914 
Epoch 626/1000 
	 loss: 27.2621, MinusLogProbMetric: 27.2621, val_loss: 28.2541, val_MinusLogProbMetric: 28.2541

Epoch 626: val_loss did not improve from 28.25121
196/196 - 44s - loss: 27.2621 - MinusLogProbMetric: 27.2621 - val_loss: 28.2541 - val_MinusLogProbMetric: 28.2541 - lr: 7.8125e-06 - 44s/epoch - 223ms/step
Epoch 627/1000
2023-10-25 22:00:44.732 
Epoch 627/1000 
	 loss: 27.2621, MinusLogProbMetric: 27.2621, val_loss: 28.2573, val_MinusLogProbMetric: 28.2573

Epoch 627: val_loss did not improve from 28.25121
196/196 - 44s - loss: 27.2621 - MinusLogProbMetric: 27.2621 - val_loss: 28.2573 - val_MinusLogProbMetric: 28.2573 - lr: 7.8125e-06 - 44s/epoch - 224ms/step
Epoch 628/1000
2023-10-25 22:01:29.176 
Epoch 628/1000 
	 loss: 27.2620, MinusLogProbMetric: 27.2620, val_loss: 28.2595, val_MinusLogProbMetric: 28.2595

Epoch 628: val_loss did not improve from 28.25121
196/196 - 44s - loss: 27.2620 - MinusLogProbMetric: 27.2620 - val_loss: 28.2595 - val_MinusLogProbMetric: 28.2595 - lr: 7.8125e-06 - 44s/epoch - 227ms/step
Epoch 629/1000
2023-10-25 22:02:13.538 
Epoch 629/1000 
	 loss: 27.2622, MinusLogProbMetric: 27.2622, val_loss: 28.2534, val_MinusLogProbMetric: 28.2534

Epoch 629: val_loss did not improve from 28.25121
196/196 - 44s - loss: 27.2622 - MinusLogProbMetric: 27.2622 - val_loss: 28.2534 - val_MinusLogProbMetric: 28.2534 - lr: 7.8125e-06 - 44s/epoch - 226ms/step
Epoch 630/1000
2023-10-25 22:02:58.153 
Epoch 630/1000 
	 loss: 27.2622, MinusLogProbMetric: 27.2622, val_loss: 28.2576, val_MinusLogProbMetric: 28.2576

Epoch 630: val_loss did not improve from 28.25121
196/196 - 45s - loss: 27.2622 - MinusLogProbMetric: 27.2622 - val_loss: 28.2576 - val_MinusLogProbMetric: 28.2576 - lr: 7.8125e-06 - 45s/epoch - 228ms/step
Epoch 631/1000
2023-10-25 22:03:42.642 
Epoch 631/1000 
	 loss: 27.2618, MinusLogProbMetric: 27.2618, val_loss: 28.2566, val_MinusLogProbMetric: 28.2566

Epoch 631: val_loss did not improve from 28.25121
196/196 - 44s - loss: 27.2618 - MinusLogProbMetric: 27.2618 - val_loss: 28.2566 - val_MinusLogProbMetric: 28.2566 - lr: 7.8125e-06 - 44s/epoch - 227ms/step
Epoch 632/1000
2023-10-25 22:04:26.913 
Epoch 632/1000 
	 loss: 27.2619, MinusLogProbMetric: 27.2619, val_loss: 28.2562, val_MinusLogProbMetric: 28.2562

Epoch 632: val_loss did not improve from 28.25121
196/196 - 44s - loss: 27.2619 - MinusLogProbMetric: 27.2619 - val_loss: 28.2562 - val_MinusLogProbMetric: 28.2562 - lr: 7.8125e-06 - 44s/epoch - 226ms/step
Epoch 633/1000
2023-10-25 22:05:11.978 
Epoch 633/1000 
	 loss: 27.2614, MinusLogProbMetric: 27.2614, val_loss: 28.2546, val_MinusLogProbMetric: 28.2546

Epoch 633: val_loss did not improve from 28.25121
196/196 - 45s - loss: 27.2614 - MinusLogProbMetric: 27.2614 - val_loss: 28.2546 - val_MinusLogProbMetric: 28.2546 - lr: 7.8125e-06 - 45s/epoch - 230ms/step
Epoch 634/1000
2023-10-25 22:05:56.890 
Epoch 634/1000 
	 loss: 27.2621, MinusLogProbMetric: 27.2621, val_loss: 28.2563, val_MinusLogProbMetric: 28.2563

Epoch 634: val_loss did not improve from 28.25121
196/196 - 45s - loss: 27.2621 - MinusLogProbMetric: 27.2621 - val_loss: 28.2563 - val_MinusLogProbMetric: 28.2563 - lr: 7.8125e-06 - 45s/epoch - 229ms/step
Epoch 635/1000
2023-10-25 22:06:41.539 
Epoch 635/1000 
	 loss: 27.2624, MinusLogProbMetric: 27.2624, val_loss: 28.2576, val_MinusLogProbMetric: 28.2576

Epoch 635: val_loss did not improve from 28.25121
196/196 - 45s - loss: 27.2624 - MinusLogProbMetric: 27.2624 - val_loss: 28.2576 - val_MinusLogProbMetric: 28.2576 - lr: 7.8125e-06 - 45s/epoch - 228ms/step
Epoch 636/1000
2023-10-25 22:07:26.603 
Epoch 636/1000 
	 loss: 27.2611, MinusLogProbMetric: 27.2611, val_loss: 28.2570, val_MinusLogProbMetric: 28.2570

Epoch 636: val_loss did not improve from 28.25121
196/196 - 45s - loss: 27.2611 - MinusLogProbMetric: 27.2611 - val_loss: 28.2570 - val_MinusLogProbMetric: 28.2570 - lr: 7.8125e-06 - 45s/epoch - 230ms/step
Epoch 637/1000
2023-10-25 22:08:11.623 
Epoch 637/1000 
	 loss: 27.2617, MinusLogProbMetric: 27.2617, val_loss: 28.2536, val_MinusLogProbMetric: 28.2536

Epoch 637: val_loss did not improve from 28.25121
196/196 - 45s - loss: 27.2617 - MinusLogProbMetric: 27.2617 - val_loss: 28.2536 - val_MinusLogProbMetric: 28.2536 - lr: 7.8125e-06 - 45s/epoch - 230ms/step
Epoch 638/1000
2023-10-25 22:08:55.940 
Epoch 638/1000 
	 loss: 27.2619, MinusLogProbMetric: 27.2619, val_loss: 28.2558, val_MinusLogProbMetric: 28.2558

Epoch 638: val_loss did not improve from 28.25121
196/196 - 44s - loss: 27.2619 - MinusLogProbMetric: 27.2619 - val_loss: 28.2558 - val_MinusLogProbMetric: 28.2558 - lr: 7.8125e-06 - 44s/epoch - 226ms/step
Epoch 639/1000
2023-10-25 22:09:40.463 
Epoch 639/1000 
	 loss: 27.2614, MinusLogProbMetric: 27.2614, val_loss: 28.2574, val_MinusLogProbMetric: 28.2574

Epoch 639: val_loss did not improve from 28.25121
196/196 - 45s - loss: 27.2614 - MinusLogProbMetric: 27.2614 - val_loss: 28.2574 - val_MinusLogProbMetric: 28.2574 - lr: 7.8125e-06 - 45s/epoch - 227ms/step
Epoch 640/1000
2023-10-25 22:10:24.836 
Epoch 640/1000 
	 loss: 27.2621, MinusLogProbMetric: 27.2621, val_loss: 28.2572, val_MinusLogProbMetric: 28.2572

Epoch 640: val_loss did not improve from 28.25121
196/196 - 44s - loss: 27.2621 - MinusLogProbMetric: 27.2621 - val_loss: 28.2572 - val_MinusLogProbMetric: 28.2572 - lr: 7.8125e-06 - 44s/epoch - 226ms/step
Epoch 641/1000
2023-10-25 22:11:09.485 
Epoch 641/1000 
	 loss: 27.2620, MinusLogProbMetric: 27.2620, val_loss: 28.2577, val_MinusLogProbMetric: 28.2577

Epoch 641: val_loss did not improve from 28.25121
196/196 - 45s - loss: 27.2620 - MinusLogProbMetric: 27.2620 - val_loss: 28.2577 - val_MinusLogProbMetric: 28.2577 - lr: 7.8125e-06 - 45s/epoch - 228ms/step
Epoch 642/1000
2023-10-25 22:11:54.024 
Epoch 642/1000 
	 loss: 27.2626, MinusLogProbMetric: 27.2626, val_loss: 28.2550, val_MinusLogProbMetric: 28.2550

Epoch 642: val_loss did not improve from 28.25121
196/196 - 45s - loss: 27.2626 - MinusLogProbMetric: 27.2626 - val_loss: 28.2550 - val_MinusLogProbMetric: 28.2550 - lr: 7.8125e-06 - 45s/epoch - 227ms/step
Epoch 643/1000
2023-10-25 22:12:38.869 
Epoch 643/1000 
	 loss: 27.2623, MinusLogProbMetric: 27.2623, val_loss: 28.2581, val_MinusLogProbMetric: 28.2581

Epoch 643: val_loss did not improve from 28.25121
196/196 - 45s - loss: 27.2623 - MinusLogProbMetric: 27.2623 - val_loss: 28.2581 - val_MinusLogProbMetric: 28.2581 - lr: 7.8125e-06 - 45s/epoch - 229ms/step
Epoch 644/1000
2023-10-25 22:13:23.750 
Epoch 644/1000 
	 loss: 27.2614, MinusLogProbMetric: 27.2614, val_loss: 28.2572, val_MinusLogProbMetric: 28.2572

Epoch 644: val_loss did not improve from 28.25121
196/196 - 45s - loss: 27.2614 - MinusLogProbMetric: 27.2614 - val_loss: 28.2572 - val_MinusLogProbMetric: 28.2572 - lr: 7.8125e-06 - 45s/epoch - 229ms/step
Epoch 645/1000
2023-10-25 22:14:08.949 
Epoch 645/1000 
	 loss: 27.2617, MinusLogProbMetric: 27.2617, val_loss: 28.2585, val_MinusLogProbMetric: 28.2585

Epoch 645: val_loss did not improve from 28.25121
196/196 - 45s - loss: 27.2617 - MinusLogProbMetric: 27.2617 - val_loss: 28.2585 - val_MinusLogProbMetric: 28.2585 - lr: 7.8125e-06 - 45s/epoch - 231ms/step
Epoch 646/1000
2023-10-25 22:14:53.845 
Epoch 646/1000 
	 loss: 27.2618, MinusLogProbMetric: 27.2618, val_loss: 28.2599, val_MinusLogProbMetric: 28.2599

Epoch 646: val_loss did not improve from 28.25121
196/196 - 45s - loss: 27.2618 - MinusLogProbMetric: 27.2618 - val_loss: 28.2599 - val_MinusLogProbMetric: 28.2599 - lr: 7.8125e-06 - 45s/epoch - 229ms/step
Epoch 647/1000
2023-10-25 22:15:38.731 
Epoch 647/1000 
	 loss: 27.2619, MinusLogProbMetric: 27.2619, val_loss: 28.2579, val_MinusLogProbMetric: 28.2579

Epoch 647: val_loss did not improve from 28.25121
196/196 - 45s - loss: 27.2619 - MinusLogProbMetric: 27.2619 - val_loss: 28.2579 - val_MinusLogProbMetric: 28.2579 - lr: 7.8125e-06 - 45s/epoch - 229ms/step
Epoch 648/1000
2023-10-25 22:16:23.292 
Epoch 648/1000 
	 loss: 27.2622, MinusLogProbMetric: 27.2622, val_loss: 28.2591, val_MinusLogProbMetric: 28.2591

Epoch 648: val_loss did not improve from 28.25121
196/196 - 45s - loss: 27.2622 - MinusLogProbMetric: 27.2622 - val_loss: 28.2591 - val_MinusLogProbMetric: 28.2591 - lr: 7.8125e-06 - 45s/epoch - 227ms/step
Epoch 649/1000
2023-10-25 22:17:08.297 
Epoch 649/1000 
	 loss: 27.2617, MinusLogProbMetric: 27.2617, val_loss: 28.2555, val_MinusLogProbMetric: 28.2555

Epoch 649: val_loss did not improve from 28.25121
196/196 - 45s - loss: 27.2617 - MinusLogProbMetric: 27.2617 - val_loss: 28.2555 - val_MinusLogProbMetric: 28.2555 - lr: 7.8125e-06 - 45s/epoch - 230ms/step
Epoch 650/1000
2023-10-25 22:17:53.142 
Epoch 650/1000 
	 loss: 27.2617, MinusLogProbMetric: 27.2617, val_loss: 28.2547, val_MinusLogProbMetric: 28.2547

Epoch 650: val_loss did not improve from 28.25121
196/196 - 45s - loss: 27.2617 - MinusLogProbMetric: 27.2617 - val_loss: 28.2547 - val_MinusLogProbMetric: 28.2547 - lr: 7.8125e-06 - 45s/epoch - 229ms/step
Epoch 651/1000
2023-10-25 22:18:37.376 
Epoch 651/1000 
	 loss: 27.2612, MinusLogProbMetric: 27.2612, val_loss: 28.2540, val_MinusLogProbMetric: 28.2540

Epoch 651: val_loss did not improve from 28.25121
196/196 - 44s - loss: 27.2612 - MinusLogProbMetric: 27.2612 - val_loss: 28.2540 - val_MinusLogProbMetric: 28.2540 - lr: 7.8125e-06 - 44s/epoch - 226ms/step
Epoch 652/1000
2023-10-25 22:19:22.190 
Epoch 652/1000 
	 loss: 27.2614, MinusLogProbMetric: 27.2614, val_loss: 28.2565, val_MinusLogProbMetric: 28.2565

Epoch 652: val_loss did not improve from 28.25121
196/196 - 45s - loss: 27.2614 - MinusLogProbMetric: 27.2614 - val_loss: 28.2565 - val_MinusLogProbMetric: 28.2565 - lr: 7.8125e-06 - 45s/epoch - 229ms/step
Epoch 653/1000
2023-10-25 22:20:06.978 
Epoch 653/1000 
	 loss: 27.2614, MinusLogProbMetric: 27.2614, val_loss: 28.2536, val_MinusLogProbMetric: 28.2536

Epoch 653: val_loss did not improve from 28.25121
196/196 - 45s - loss: 27.2614 - MinusLogProbMetric: 27.2614 - val_loss: 28.2536 - val_MinusLogProbMetric: 28.2536 - lr: 7.8125e-06 - 45s/epoch - 229ms/step
Epoch 654/1000
2023-10-25 22:20:51.731 
Epoch 654/1000 
	 loss: 27.2618, MinusLogProbMetric: 27.2618, val_loss: 28.2588, val_MinusLogProbMetric: 28.2588

Epoch 654: val_loss did not improve from 28.25121
196/196 - 45s - loss: 27.2618 - MinusLogProbMetric: 27.2618 - val_loss: 28.2588 - val_MinusLogProbMetric: 28.2588 - lr: 7.8125e-06 - 45s/epoch - 228ms/step
Epoch 655/1000
2023-10-25 22:21:36.339 
Epoch 655/1000 
	 loss: 27.2617, MinusLogProbMetric: 27.2617, val_loss: 28.2554, val_MinusLogProbMetric: 28.2554

Epoch 655: val_loss did not improve from 28.25121
196/196 - 45s - loss: 27.2617 - MinusLogProbMetric: 27.2617 - val_loss: 28.2554 - val_MinusLogProbMetric: 28.2554 - lr: 7.8125e-06 - 45s/epoch - 228ms/step
Epoch 656/1000
2023-10-25 22:22:20.770 
Epoch 656/1000 
	 loss: 27.2617, MinusLogProbMetric: 27.2617, val_loss: 28.2540, val_MinusLogProbMetric: 28.2540

Epoch 656: val_loss did not improve from 28.25121
196/196 - 44s - loss: 27.2617 - MinusLogProbMetric: 27.2617 - val_loss: 28.2540 - val_MinusLogProbMetric: 28.2540 - lr: 7.8125e-06 - 44s/epoch - 227ms/step
Epoch 657/1000
2023-10-25 22:23:05.002 
Epoch 657/1000 
	 loss: 27.2611, MinusLogProbMetric: 27.2611, val_loss: 28.2574, val_MinusLogProbMetric: 28.2574

Epoch 657: val_loss did not improve from 28.25121
196/196 - 44s - loss: 27.2611 - MinusLogProbMetric: 27.2611 - val_loss: 28.2574 - val_MinusLogProbMetric: 28.2574 - lr: 7.8125e-06 - 44s/epoch - 226ms/step
Epoch 658/1000
2023-10-25 22:23:49.688 
Epoch 658/1000 
	 loss: 27.2612, MinusLogProbMetric: 27.2612, val_loss: 28.2547, val_MinusLogProbMetric: 28.2547

Epoch 658: val_loss did not improve from 28.25121
196/196 - 45s - loss: 27.2612 - MinusLogProbMetric: 27.2612 - val_loss: 28.2547 - val_MinusLogProbMetric: 28.2547 - lr: 7.8125e-06 - 45s/epoch - 228ms/step
Epoch 659/1000
2023-10-25 22:24:34.368 
Epoch 659/1000 
	 loss: 27.2612, MinusLogProbMetric: 27.2612, val_loss: 28.2557, val_MinusLogProbMetric: 28.2557

Epoch 659: val_loss did not improve from 28.25121
196/196 - 45s - loss: 27.2612 - MinusLogProbMetric: 27.2612 - val_loss: 28.2557 - val_MinusLogProbMetric: 28.2557 - lr: 7.8125e-06 - 45s/epoch - 228ms/step
Epoch 660/1000
2023-10-25 22:25:18.848 
Epoch 660/1000 
	 loss: 27.2612, MinusLogProbMetric: 27.2612, val_loss: 28.2621, val_MinusLogProbMetric: 28.2621

Epoch 660: val_loss did not improve from 28.25121
196/196 - 44s - loss: 27.2612 - MinusLogProbMetric: 27.2612 - val_loss: 28.2621 - val_MinusLogProbMetric: 28.2621 - lr: 7.8125e-06 - 44s/epoch - 227ms/step
Epoch 661/1000
2023-10-25 22:26:03.153 
Epoch 661/1000 
	 loss: 27.2611, MinusLogProbMetric: 27.2611, val_loss: 28.2552, val_MinusLogProbMetric: 28.2552

Epoch 661: val_loss did not improve from 28.25121
196/196 - 44s - loss: 27.2611 - MinusLogProbMetric: 27.2611 - val_loss: 28.2552 - val_MinusLogProbMetric: 28.2552 - lr: 7.8125e-06 - 44s/epoch - 226ms/step
Epoch 662/1000
2023-10-25 22:26:48.113 
Epoch 662/1000 
	 loss: 27.2610, MinusLogProbMetric: 27.2610, val_loss: 28.2562, val_MinusLogProbMetric: 28.2562

Epoch 662: val_loss did not improve from 28.25121
196/196 - 45s - loss: 27.2610 - MinusLogProbMetric: 27.2610 - val_loss: 28.2562 - val_MinusLogProbMetric: 28.2562 - lr: 7.8125e-06 - 45s/epoch - 229ms/step
Epoch 663/1000
2023-10-25 22:27:32.684 
Epoch 663/1000 
	 loss: 27.2610, MinusLogProbMetric: 27.2610, val_loss: 28.2571, val_MinusLogProbMetric: 28.2571

Epoch 663: val_loss did not improve from 28.25121
196/196 - 45s - loss: 27.2610 - MinusLogProbMetric: 27.2610 - val_loss: 28.2571 - val_MinusLogProbMetric: 28.2571 - lr: 7.8125e-06 - 45s/epoch - 227ms/step
Epoch 664/1000
2023-10-25 22:28:17.524 
Epoch 664/1000 
	 loss: 27.2610, MinusLogProbMetric: 27.2610, val_loss: 28.2571, val_MinusLogProbMetric: 28.2571

Epoch 664: val_loss did not improve from 28.25121
196/196 - 45s - loss: 27.2610 - MinusLogProbMetric: 27.2610 - val_loss: 28.2571 - val_MinusLogProbMetric: 28.2571 - lr: 7.8125e-06 - 45s/epoch - 229ms/step
Epoch 665/1000
2023-10-25 22:29:02.522 
Epoch 665/1000 
	 loss: 27.2610, MinusLogProbMetric: 27.2610, val_loss: 28.2575, val_MinusLogProbMetric: 28.2575

Epoch 665: val_loss did not improve from 28.25121
Restoring model weights from the end of the best epoch: 565.
196/196 - 45s - loss: 27.2610 - MinusLogProbMetric: 27.2610 - val_loss: 28.2575 - val_MinusLogProbMetric: 28.2575 - lr: 7.8125e-06 - 45s/epoch - 232ms/step
Epoch 665: early stopping
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Training succeeded with seed 520.
Model trained in 28032.18 s.

===========
Computing predictions
===========

Computing metrics...
Checking and setting numerical distributions.
Resetting dist_num.
Resetting dist_num.
Metrics computed in 1.05 s.
                ===========
                print("===========
Failed to plot
===========
")
                Exception type: ValueError
                Exception message: Number of rows must be a positive integer, not 0
                Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 481, Func.Name : prediction_function, Message : Plotters.cornerplotter(X_data_test.numpy(),X_data_nf.numpy(),path_to_results,ndims,norm=True) # type: ignore', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/Plotters.py , Line : 156, Func.Name : cornerplotter, Message : figure=corner.corner(target_samples,color=\'red\',bins=n_bins,labels=[r"%s" % s for s in labels],normalize1d=True)', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/corner.py , Line : 235, Func.Name : corner, Message : fig, axes = pl.subplots(K, K, figsize=(dim, dim))', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/pyplot.py , Line : 1502, Func.Name : subplots, Message : axs = fig.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 905, Func.Name : subplots, Message : gs = self.add_gridspec(nrows, ncols, figure=self, **gridspec_kw)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 1527, Func.Name : add_gridspec, Message : gs = GridSpec(nrows=nrows, ncols=ncols, figure=self, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 379, Func.Name : __init__, Message : super().__init__(nrows, ncols,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 49, Func.Name : __init__, Message : raise ValueError(']
                ===========

results.txt saved
results.json saved
Results log saved
Model predictions computed in 1.33 s.
===========
Run 356/720 done in 28039.52 s.
===========

Directory ../../results/CsplineN_new/run_357/ already exists.
Skipping it.
===========
Run 357/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_358/ already exists.
Skipping it.
===========
Run 358/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_359/ already exists.
Skipping it.
===========
Run 359/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_360/ already exists.
Skipping it.
===========
Run 360/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_361/ already exists.
Skipping it.
===========
Run 361/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_362/ already exists.
Skipping it.
===========
Run 362/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_363/ already exists.
Skipping it.
===========
Run 363/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_364/ already exists.
Skipping it.
===========
Run 364/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_365/ already exists.
Skipping it.
===========
Run 365/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_366/ already exists.
Skipping it.
===========
Run 366/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_367/ already exists.
Skipping it.
===========
Run 367/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_368/ already exists.
Skipping it.
===========
Run 368/720 already exists. Skipping it.
===========

===========
Generating train data for run 369.
===========
Train data generated in 0.22 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_369/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 721}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_369/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 0.958336 ,  3.6590893,  8.676455 , ...,  5.9946833,  2.2094154,
         2.624469 ],
       [ 1.2393765,  3.8911932,  8.905665 , ...,  5.783149 ,  1.3067368,
         2.3810356],
       [ 6.815516 ,  2.883391 ,  6.09811  , ...,  2.679101 ,  5.190114 ,
         1.7287396],
       ...,
       [ 5.1098504,  8.570955 ,  5.486371 , ...,  2.6942174,  5.2584124,
         1.4402856],
       [ 1.5225409,  2.876459 ,  6.062782 , ...,  5.341303 , -0.5483551,
         3.446848 ],
       [ 5.6395645,  6.8262954,  6.6988196, ...,  1.2809412,  5.949959 ,
         1.4368113]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_369/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_369
self.data_kwargs: {'seed': 721}
self.x_data: [[ 5.475364    7.6366067   6.593081   ...  0.8530688   8.875271
   1.2600539 ]
 [ 6.624233    2.860935    6.2971916  ...  2.665269    4.2119365
   3.0900779 ]
 [ 6.4123483   2.9544969   6.2372165  ...  3.7006004   4.897939
   1.9067069 ]
 ...
 [ 5.291359    8.643322    5.9392395  ...  0.5393052   7.1813807
   1.3439621 ]
 [ 5.5789056   6.2932816   6.272862   ... -0.29475307  6.0157323
   1.3976022 ]
 [ 1.6532166   2.3152595  10.214214   ...  6.271919   -0.05406091
   3.9072165 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_572"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_573 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_57 (LogProbL  (None,)                  660960    
 ayer)                                                           
                                                                 
=================================================================
Total params: 660,960
Trainable params: 660,960
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_57/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_57'")
self.model: <keras.engine.functional.Functional object at 0x7f57a83af7f0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f589b908c40>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f589b908c40>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f58418a52d0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f57a8332800>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_369/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f57a8332d70>, <keras.callbacks.ModelCheckpoint object at 0x7f57a8332e30>, <keras.callbacks.EarlyStopping object at 0x7f57a83330a0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f57a83330d0>, <keras.callbacks.TerminateOnNaN object at 0x7f57a8332d10>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 0.958336 ,  3.6590893,  8.676455 , ...,  5.9946833,  2.2094154,
         2.624469 ],
       [ 1.2393765,  3.8911932,  8.905665 , ...,  5.783149 ,  1.3067368,
         2.3810356],
       [ 6.815516 ,  2.883391 ,  6.09811  , ...,  2.679101 ,  5.190114 ,
         1.7287396],
       ...,
       [ 5.1098504,  8.570955 ,  5.486371 , ...,  2.6942174,  5.2584124,
         1.4402856],
       [ 1.5225409,  2.876459 ,  6.062782 , ...,  5.341303 , -0.5483551,
         3.446848 ],
       [ 5.6395645,  6.8262954,  6.6988196, ...,  1.2809412,  5.949959 ,
         1.4368113]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_369/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 369/720 with hyperparameters:
timestamp = 2023-10-25 22:29:08.607009
ndims = 64
seed_train = 721
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 660960
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 5.475364    7.6366067   6.593081    5.2213635   4.3851104   6.547967
  4.392531    8.637874    9.379391    3.0568433   8.712279    4.9198484
  5.701744    9.309082    0.31514573  1.1504446  -0.4705431   8.813661
  9.537466    8.269614    8.780608    7.8593726   4.6449614   7.4048796
  1.683444    6.839795    1.5740938   9.839       4.622554    3.4715672
  2.0572462   7.526533    4.5307302   5.4113736   0.1867744   5.891015
  4.975408    5.49995     9.17347     6.8054004   3.9538507   4.336993
  6.759812    0.17014661  6.225843    6.527422    2.054394    1.277253
  3.0545924   3.5868065   4.708149    4.201944    9.617644    1.0881705
  1.922195    1.5136921   6.466659    1.6895405   4.5659165   2.2596717
  1.6085154   0.8530688   8.875271    1.2600539 ]
Epoch 1/1000
2023-10-25 22:30:44.944 
Epoch 1/1000 
	 loss: 794.3638, MinusLogProbMetric: 794.3638, val_loss: 253.7733, val_MinusLogProbMetric: 253.7733

Epoch 1: val_loss improved from inf to 253.77333, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_369/weights/best_weights.h5
196/196 - 97s - loss: 794.3638 - MinusLogProbMetric: 794.3638 - val_loss: 253.7733 - val_MinusLogProbMetric: 253.7733 - lr: 0.0010 - 97s/epoch - 494ms/step
Epoch 2/1000
2023-10-25 22:31:21.143 
Epoch 2/1000 
	 loss: 174.9590, MinusLogProbMetric: 174.9590, val_loss: 143.4892, val_MinusLogProbMetric: 143.4892

Epoch 2: val_loss improved from 253.77333 to 143.48917, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_369/weights/best_weights.h5
196/196 - 36s - loss: 174.9590 - MinusLogProbMetric: 174.9590 - val_loss: 143.4892 - val_MinusLogProbMetric: 143.4892 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 3/1000
2023-10-25 22:31:56.633 
Epoch 3/1000 
	 loss: 118.1745, MinusLogProbMetric: 118.1745, val_loss: 101.3726, val_MinusLogProbMetric: 101.3726

Epoch 3: val_loss improved from 143.48917 to 101.37262, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_369/weights/best_weights.h5
196/196 - 36s - loss: 118.1745 - MinusLogProbMetric: 118.1745 - val_loss: 101.3726 - val_MinusLogProbMetric: 101.3726 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 4/1000
2023-10-25 22:32:32.613 
Epoch 4/1000 
	 loss: 91.9816, MinusLogProbMetric: 91.9816, val_loss: 83.5246, val_MinusLogProbMetric: 83.5246

Epoch 4: val_loss improved from 101.37262 to 83.52460, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_369/weights/best_weights.h5
196/196 - 36s - loss: 91.9816 - MinusLogProbMetric: 91.9816 - val_loss: 83.5246 - val_MinusLogProbMetric: 83.5246 - lr: 0.0010 - 36s/epoch - 184ms/step
Epoch 5/1000
2023-10-25 22:33:08.371 
Epoch 5/1000 
	 loss: 78.7085, MinusLogProbMetric: 78.7085, val_loss: 73.5143, val_MinusLogProbMetric: 73.5143

Epoch 5: val_loss improved from 83.52460 to 73.51425, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_369/weights/best_weights.h5
196/196 - 36s - loss: 78.7085 - MinusLogProbMetric: 78.7085 - val_loss: 73.5143 - val_MinusLogProbMetric: 73.5143 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 6/1000
2023-10-25 22:33:44.183 
Epoch 6/1000 
	 loss: 69.4121, MinusLogProbMetric: 69.4121, val_loss: 64.6992, val_MinusLogProbMetric: 64.6992

Epoch 6: val_loss improved from 73.51425 to 64.69923, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_369/weights/best_weights.h5
196/196 - 36s - loss: 69.4121 - MinusLogProbMetric: 69.4121 - val_loss: 64.6992 - val_MinusLogProbMetric: 64.6992 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 7/1000
2023-10-25 22:34:20.297 
Epoch 7/1000 
	 loss: 62.5164, MinusLogProbMetric: 62.5164, val_loss: 60.5510, val_MinusLogProbMetric: 60.5510

Epoch 7: val_loss improved from 64.69923 to 60.55101, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_369/weights/best_weights.h5
196/196 - 36s - loss: 62.5164 - MinusLogProbMetric: 62.5164 - val_loss: 60.5510 - val_MinusLogProbMetric: 60.5510 - lr: 0.0010 - 36s/epoch - 185ms/step
Epoch 8/1000
2023-10-25 22:34:56.385 
Epoch 8/1000 
	 loss: 57.7365, MinusLogProbMetric: 57.7365, val_loss: 55.2970, val_MinusLogProbMetric: 55.2970

Epoch 8: val_loss improved from 60.55101 to 55.29695, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_369/weights/best_weights.h5
196/196 - 36s - loss: 57.7365 - MinusLogProbMetric: 57.7365 - val_loss: 55.2970 - val_MinusLogProbMetric: 55.2970 - lr: 0.0010 - 36s/epoch - 184ms/step
Epoch 9/1000
2023-10-25 22:35:32.247 
Epoch 9/1000 
	 loss: 54.1945, MinusLogProbMetric: 54.1945, val_loss: 51.7850, val_MinusLogProbMetric: 51.7850

Epoch 9: val_loss improved from 55.29695 to 51.78496, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_369/weights/best_weights.h5
196/196 - 36s - loss: 54.1945 - MinusLogProbMetric: 54.1945 - val_loss: 51.7850 - val_MinusLogProbMetric: 51.7850 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 10/1000
2023-10-25 22:36:08.041 
Epoch 10/1000 
	 loss: 51.2020, MinusLogProbMetric: 51.2020, val_loss: 50.5371, val_MinusLogProbMetric: 50.5371

Epoch 10: val_loss improved from 51.78496 to 50.53712, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_369/weights/best_weights.h5
196/196 - 36s - loss: 51.2020 - MinusLogProbMetric: 51.2020 - val_loss: 50.5371 - val_MinusLogProbMetric: 50.5371 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 11/1000
2023-10-25 22:36:43.545 
Epoch 11/1000 
	 loss: 49.3015, MinusLogProbMetric: 49.3015, val_loss: 48.1792, val_MinusLogProbMetric: 48.1792

Epoch 11: val_loss improved from 50.53712 to 48.17920, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_369/weights/best_weights.h5
196/196 - 36s - loss: 49.3015 - MinusLogProbMetric: 49.3015 - val_loss: 48.1792 - val_MinusLogProbMetric: 48.1792 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 12/1000
2023-10-25 22:37:19.246 
Epoch 12/1000 
	 loss: 47.8754, MinusLogProbMetric: 47.8754, val_loss: 46.2811, val_MinusLogProbMetric: 46.2811

Epoch 12: val_loss improved from 48.17920 to 46.28106, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_369/weights/best_weights.h5
196/196 - 36s - loss: 47.8754 - MinusLogProbMetric: 47.8754 - val_loss: 46.2811 - val_MinusLogProbMetric: 46.2811 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 13/1000
2023-10-25 22:37:54.920 
Epoch 13/1000 
	 loss: 46.1813, MinusLogProbMetric: 46.1813, val_loss: 45.4394, val_MinusLogProbMetric: 45.4394

Epoch 13: val_loss improved from 46.28106 to 45.43945, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_369/weights/best_weights.h5
196/196 - 36s - loss: 46.1813 - MinusLogProbMetric: 46.1813 - val_loss: 45.4394 - val_MinusLogProbMetric: 45.4394 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 14/1000
2023-10-25 22:38:30.859 
Epoch 14/1000 
	 loss: 44.8450, MinusLogProbMetric: 44.8450, val_loss: 44.0396, val_MinusLogProbMetric: 44.0396

Epoch 14: val_loss improved from 45.43945 to 44.03959, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_369/weights/best_weights.h5
196/196 - 36s - loss: 44.8450 - MinusLogProbMetric: 44.8450 - val_loss: 44.0396 - val_MinusLogProbMetric: 44.0396 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 15/1000
2023-10-25 22:39:06.740 
Epoch 15/1000 
	 loss: 43.8934, MinusLogProbMetric: 43.8934, val_loss: 43.3425, val_MinusLogProbMetric: 43.3425

Epoch 15: val_loss improved from 44.03959 to 43.34248, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_369/weights/best_weights.h5
196/196 - 36s - loss: 43.8934 - MinusLogProbMetric: 43.8934 - val_loss: 43.3425 - val_MinusLogProbMetric: 43.3425 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 16/1000
2023-10-25 22:39:42.897 
Epoch 16/1000 
	 loss: 42.8952, MinusLogProbMetric: 42.8952, val_loss: 41.9981, val_MinusLogProbMetric: 41.9981

Epoch 16: val_loss improved from 43.34248 to 41.99806, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_369/weights/best_weights.h5
196/196 - 36s - loss: 42.8952 - MinusLogProbMetric: 42.8952 - val_loss: 41.9981 - val_MinusLogProbMetric: 41.9981 - lr: 0.0010 - 36s/epoch - 184ms/step
Epoch 17/1000
2023-10-25 22:40:18.851 
Epoch 17/1000 
	 loss: 42.4809, MinusLogProbMetric: 42.4809, val_loss: 41.7037, val_MinusLogProbMetric: 41.7037

Epoch 17: val_loss improved from 41.99806 to 41.70368, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_369/weights/best_weights.h5
196/196 - 36s - loss: 42.4809 - MinusLogProbMetric: 42.4809 - val_loss: 41.7037 - val_MinusLogProbMetric: 41.7037 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 18/1000
2023-10-25 22:40:54.686 
Epoch 18/1000 
	 loss: 41.1000, MinusLogProbMetric: 41.1000, val_loss: 40.4445, val_MinusLogProbMetric: 40.4445

Epoch 18: val_loss improved from 41.70368 to 40.44454, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_369/weights/best_weights.h5
196/196 - 36s - loss: 41.1000 - MinusLogProbMetric: 41.1000 - val_loss: 40.4445 - val_MinusLogProbMetric: 40.4445 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 19/1000
2023-10-25 22:41:30.356 
Epoch 19/1000 
	 loss: 40.7228, MinusLogProbMetric: 40.7228, val_loss: 41.8787, val_MinusLogProbMetric: 41.8787

Epoch 19: val_loss did not improve from 40.44454
196/196 - 35s - loss: 40.7228 - MinusLogProbMetric: 40.7228 - val_loss: 41.8787 - val_MinusLogProbMetric: 41.8787 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 20/1000
2023-10-25 22:42:05.639 
Epoch 20/1000 
	 loss: 40.2093, MinusLogProbMetric: 40.2093, val_loss: 40.4293, val_MinusLogProbMetric: 40.4293

Epoch 20: val_loss improved from 40.44454 to 40.42933, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_369/weights/best_weights.h5
196/196 - 36s - loss: 40.2093 - MinusLogProbMetric: 40.2093 - val_loss: 40.4293 - val_MinusLogProbMetric: 40.4293 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 21/1000
2023-10-25 22:42:41.451 
Epoch 21/1000 
	 loss: 39.4779, MinusLogProbMetric: 39.4779, val_loss: 38.6153, val_MinusLogProbMetric: 38.6153

Epoch 21: val_loss improved from 40.42933 to 38.61530, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_369/weights/best_weights.h5
196/196 - 36s - loss: 39.4779 - MinusLogProbMetric: 39.4779 - val_loss: 38.6153 - val_MinusLogProbMetric: 38.6153 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 22/1000
2023-10-25 22:43:17.065 
Epoch 22/1000 
	 loss: 39.0463, MinusLogProbMetric: 39.0463, val_loss: 39.0135, val_MinusLogProbMetric: 39.0135

Epoch 22: val_loss did not improve from 38.61530
196/196 - 35s - loss: 39.0463 - MinusLogProbMetric: 39.0463 - val_loss: 39.0135 - val_MinusLogProbMetric: 39.0135 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 23/1000
2023-10-25 22:43:52.142 
Epoch 23/1000 
	 loss: 38.8736, MinusLogProbMetric: 38.8736, val_loss: 38.6143, val_MinusLogProbMetric: 38.6143

Epoch 23: val_loss improved from 38.61530 to 38.61425, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_369/weights/best_weights.h5
196/196 - 36s - loss: 38.8736 - MinusLogProbMetric: 38.8736 - val_loss: 38.6143 - val_MinusLogProbMetric: 38.6143 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 24/1000
2023-10-25 22:44:28.168 
Epoch 24/1000 
	 loss: 38.2711, MinusLogProbMetric: 38.2711, val_loss: 37.8742, val_MinusLogProbMetric: 37.8742

Epoch 24: val_loss improved from 38.61425 to 37.87419, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_369/weights/best_weights.h5
196/196 - 36s - loss: 38.2711 - MinusLogProbMetric: 38.2711 - val_loss: 37.8742 - val_MinusLogProbMetric: 37.8742 - lr: 0.0010 - 36s/epoch - 184ms/step
Epoch 25/1000
2023-10-25 22:45:04.047 
Epoch 25/1000 
	 loss: 37.7715, MinusLogProbMetric: 37.7715, val_loss: 39.5243, val_MinusLogProbMetric: 39.5243

Epoch 25: val_loss did not improve from 37.87419
196/196 - 35s - loss: 37.7715 - MinusLogProbMetric: 37.7715 - val_loss: 39.5243 - val_MinusLogProbMetric: 39.5243 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 26/1000
2023-10-25 22:45:39.306 
Epoch 26/1000 
	 loss: 38.0296, MinusLogProbMetric: 38.0296, val_loss: 36.5652, val_MinusLogProbMetric: 36.5652

Epoch 26: val_loss improved from 37.87419 to 36.56524, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_369/weights/best_weights.h5
196/196 - 36s - loss: 38.0296 - MinusLogProbMetric: 38.0296 - val_loss: 36.5652 - val_MinusLogProbMetric: 36.5652 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 27/1000
2023-10-25 22:46:15.456 
Epoch 27/1000 
	 loss: 37.2357, MinusLogProbMetric: 37.2357, val_loss: 40.5465, val_MinusLogProbMetric: 40.5465

Epoch 27: val_loss did not improve from 36.56524
196/196 - 36s - loss: 37.2357 - MinusLogProbMetric: 37.2357 - val_loss: 40.5465 - val_MinusLogProbMetric: 40.5465 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 28/1000
2023-10-25 22:46:50.761 
Epoch 28/1000 
	 loss: 36.9485, MinusLogProbMetric: 36.9485, val_loss: 36.6634, val_MinusLogProbMetric: 36.6634

Epoch 28: val_loss did not improve from 36.56524
196/196 - 35s - loss: 36.9485 - MinusLogProbMetric: 36.9485 - val_loss: 36.6634 - val_MinusLogProbMetric: 36.6634 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 29/1000
2023-10-25 22:47:26.096 
Epoch 29/1000 
	 loss: 36.8386, MinusLogProbMetric: 36.8386, val_loss: 37.3703, val_MinusLogProbMetric: 37.3703

Epoch 29: val_loss did not improve from 36.56524
196/196 - 35s - loss: 36.8386 - MinusLogProbMetric: 36.8386 - val_loss: 37.3703 - val_MinusLogProbMetric: 37.3703 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 30/1000
2023-10-25 22:48:01.523 
Epoch 30/1000 
	 loss: 36.1095, MinusLogProbMetric: 36.1095, val_loss: 37.1948, val_MinusLogProbMetric: 37.1948

Epoch 30: val_loss did not improve from 36.56524
196/196 - 35s - loss: 36.1095 - MinusLogProbMetric: 36.1095 - val_loss: 37.1948 - val_MinusLogProbMetric: 37.1948 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 31/1000
2023-10-25 22:48:37.351 
Epoch 31/1000 
	 loss: 36.4014, MinusLogProbMetric: 36.4014, val_loss: 36.2871, val_MinusLogProbMetric: 36.2871

Epoch 31: val_loss improved from 36.56524 to 36.28707, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_369/weights/best_weights.h5
196/196 - 36s - loss: 36.4014 - MinusLogProbMetric: 36.4014 - val_loss: 36.2871 - val_MinusLogProbMetric: 36.2871 - lr: 0.0010 - 36s/epoch - 186ms/step
Epoch 32/1000
2023-10-25 22:49:13.336 
Epoch 32/1000 
	 loss: 36.6015, MinusLogProbMetric: 36.6015, val_loss: 36.6654, val_MinusLogProbMetric: 36.6654

Epoch 32: val_loss did not improve from 36.28707
196/196 - 35s - loss: 36.6015 - MinusLogProbMetric: 36.6015 - val_loss: 36.6654 - val_MinusLogProbMetric: 36.6654 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 33/1000
2023-10-25 22:49:48.714 
Epoch 33/1000 
	 loss: 35.6200, MinusLogProbMetric: 35.6200, val_loss: 35.3436, val_MinusLogProbMetric: 35.3436

Epoch 33: val_loss improved from 36.28707 to 35.34362, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_369/weights/best_weights.h5
196/196 - 36s - loss: 35.6200 - MinusLogProbMetric: 35.6200 - val_loss: 35.3436 - val_MinusLogProbMetric: 35.3436 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 34/1000
2023-10-25 22:50:24.439 
Epoch 34/1000 
	 loss: 35.6508, MinusLogProbMetric: 35.6508, val_loss: 35.4791, val_MinusLogProbMetric: 35.4791

Epoch 34: val_loss did not improve from 35.34362
196/196 - 35s - loss: 35.6508 - MinusLogProbMetric: 35.6508 - val_loss: 35.4791 - val_MinusLogProbMetric: 35.4791 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 35/1000
2023-10-25 22:50:59.827 
Epoch 35/1000 
	 loss: 35.8698, MinusLogProbMetric: 35.8698, val_loss: 35.0137, val_MinusLogProbMetric: 35.0137

Epoch 35: val_loss improved from 35.34362 to 35.01372, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_369/weights/best_weights.h5
196/196 - 36s - loss: 35.8698 - MinusLogProbMetric: 35.8698 - val_loss: 35.0137 - val_MinusLogProbMetric: 35.0137 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 36/1000
2023-10-25 22:51:35.518 
Epoch 36/1000 
	 loss: 35.4145, MinusLogProbMetric: 35.4145, val_loss: 37.3415, val_MinusLogProbMetric: 37.3415

Epoch 36: val_loss did not improve from 35.01372
196/196 - 35s - loss: 35.4145 - MinusLogProbMetric: 35.4145 - val_loss: 37.3415 - val_MinusLogProbMetric: 37.3415 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 37/1000
2023-10-25 22:52:10.973 
Epoch 37/1000 
	 loss: 35.5481, MinusLogProbMetric: 35.5481, val_loss: 36.5090, val_MinusLogProbMetric: 36.5090

Epoch 37: val_loss did not improve from 35.01372
196/196 - 35s - loss: 35.5481 - MinusLogProbMetric: 35.5481 - val_loss: 36.5090 - val_MinusLogProbMetric: 36.5090 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 38/1000
2023-10-25 22:52:46.365 
Epoch 38/1000 
	 loss: 35.0626, MinusLogProbMetric: 35.0626, val_loss: 35.1863, val_MinusLogProbMetric: 35.1863

Epoch 38: val_loss did not improve from 35.01372
196/196 - 35s - loss: 35.0626 - MinusLogProbMetric: 35.0626 - val_loss: 35.1863 - val_MinusLogProbMetric: 35.1863 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 39/1000
2023-10-25 22:53:21.746 
Epoch 39/1000 
	 loss: 35.0432, MinusLogProbMetric: 35.0432, val_loss: 34.2786, val_MinusLogProbMetric: 34.2786

Epoch 39: val_loss improved from 35.01372 to 34.27860, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_369/weights/best_weights.h5
196/196 - 36s - loss: 35.0432 - MinusLogProbMetric: 35.0432 - val_loss: 34.2786 - val_MinusLogProbMetric: 34.2786 - lr: 0.0010 - 36s/epoch - 184ms/step
Epoch 40/1000
2023-10-25 22:53:57.680 
Epoch 40/1000 
	 loss: 34.7091, MinusLogProbMetric: 34.7091, val_loss: 34.0418, val_MinusLogProbMetric: 34.0418

Epoch 40: val_loss improved from 34.27860 to 34.04177, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_369/weights/best_weights.h5
196/196 - 36s - loss: 34.7091 - MinusLogProbMetric: 34.7091 - val_loss: 34.0418 - val_MinusLogProbMetric: 34.0418 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 41/1000
2023-10-25 22:54:33.739 
Epoch 41/1000 
	 loss: 34.7180, MinusLogProbMetric: 34.7180, val_loss: 33.9483, val_MinusLogProbMetric: 33.9483

Epoch 41: val_loss improved from 34.04177 to 33.94827, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_369/weights/best_weights.h5
196/196 - 36s - loss: 34.7180 - MinusLogProbMetric: 34.7180 - val_loss: 33.9483 - val_MinusLogProbMetric: 33.9483 - lr: 0.0010 - 36s/epoch - 184ms/step
Epoch 42/1000
2023-10-25 22:55:09.518 
Epoch 42/1000 
	 loss: 34.6311, MinusLogProbMetric: 34.6311, val_loss: 35.1988, val_MinusLogProbMetric: 35.1988

Epoch 42: val_loss did not improve from 33.94827
196/196 - 35s - loss: 34.6311 - MinusLogProbMetric: 34.6311 - val_loss: 35.1988 - val_MinusLogProbMetric: 35.1988 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 43/1000
2023-10-25 22:55:45.018 
Epoch 43/1000 
	 loss: 34.5592, MinusLogProbMetric: 34.5592, val_loss: 34.1780, val_MinusLogProbMetric: 34.1780

Epoch 43: val_loss did not improve from 33.94827
196/196 - 35s - loss: 34.5592 - MinusLogProbMetric: 34.5592 - val_loss: 34.1780 - val_MinusLogProbMetric: 34.1780 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 44/1000
2023-10-25 22:56:20.323 
Epoch 44/1000 
	 loss: 34.7699, MinusLogProbMetric: 34.7699, val_loss: 34.8255, val_MinusLogProbMetric: 34.8255

Epoch 44: val_loss did not improve from 33.94827
196/196 - 35s - loss: 34.7699 - MinusLogProbMetric: 34.7699 - val_loss: 34.8255 - val_MinusLogProbMetric: 34.8255 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 45/1000
2023-10-25 22:56:56.155 
Epoch 45/1000 
	 loss: 34.7672, MinusLogProbMetric: 34.7672, val_loss: 33.8157, val_MinusLogProbMetric: 33.8157

Epoch 45: val_loss improved from 33.94827 to 33.81571, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_369/weights/best_weights.h5
196/196 - 36s - loss: 34.7672 - MinusLogProbMetric: 34.7672 - val_loss: 33.8157 - val_MinusLogProbMetric: 33.8157 - lr: 0.0010 - 36s/epoch - 186ms/step
Epoch 46/1000
2023-10-25 22:57:31.981 
Epoch 46/1000 
	 loss: 34.0229, MinusLogProbMetric: 34.0229, val_loss: 35.2410, val_MinusLogProbMetric: 35.2410

Epoch 46: val_loss did not improve from 33.81571
196/196 - 35s - loss: 34.0229 - MinusLogProbMetric: 34.0229 - val_loss: 35.2410 - val_MinusLogProbMetric: 35.2410 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 47/1000
2023-10-25 22:58:07.500 
Epoch 47/1000 
	 loss: 33.9963, MinusLogProbMetric: 33.9963, val_loss: 33.0939, val_MinusLogProbMetric: 33.0939

Epoch 47: val_loss improved from 33.81571 to 33.09387, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_369/weights/best_weights.h5
196/196 - 36s - loss: 33.9963 - MinusLogProbMetric: 33.9963 - val_loss: 33.0939 - val_MinusLogProbMetric: 33.0939 - lr: 0.0010 - 36s/epoch - 184ms/step
Epoch 48/1000
2023-10-25 22:58:43.543 
Epoch 48/1000 
	 loss: 33.8444, MinusLogProbMetric: 33.8444, val_loss: 34.0943, val_MinusLogProbMetric: 34.0943

Epoch 48: val_loss did not improve from 33.09387
196/196 - 35s - loss: 33.8444 - MinusLogProbMetric: 33.8444 - val_loss: 34.0943 - val_MinusLogProbMetric: 34.0943 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 49/1000
2023-10-25 22:59:19.193 
Epoch 49/1000 
	 loss: 34.4076, MinusLogProbMetric: 34.4076, val_loss: 34.3371, val_MinusLogProbMetric: 34.3371

Epoch 49: val_loss did not improve from 33.09387
196/196 - 36s - loss: 34.4076 - MinusLogProbMetric: 34.4076 - val_loss: 34.3371 - val_MinusLogProbMetric: 34.3371 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 50/1000
2023-10-25 22:59:54.630 
Epoch 50/1000 
	 loss: 33.8913, MinusLogProbMetric: 33.8913, val_loss: 33.7512, val_MinusLogProbMetric: 33.7512

Epoch 50: val_loss did not improve from 33.09387
196/196 - 35s - loss: 33.8913 - MinusLogProbMetric: 33.8913 - val_loss: 33.7512 - val_MinusLogProbMetric: 33.7512 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 51/1000
2023-10-25 23:00:30.368 
Epoch 51/1000 
	 loss: 33.8432, MinusLogProbMetric: 33.8432, val_loss: 35.0318, val_MinusLogProbMetric: 35.0318

Epoch 51: val_loss did not improve from 33.09387
196/196 - 36s - loss: 33.8432 - MinusLogProbMetric: 33.8432 - val_loss: 35.0318 - val_MinusLogProbMetric: 35.0318 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 52/1000
2023-10-25 23:01:05.534 
Epoch 52/1000 
	 loss: 33.5816, MinusLogProbMetric: 33.5816, val_loss: 33.2817, val_MinusLogProbMetric: 33.2817

Epoch 52: val_loss did not improve from 33.09387
196/196 - 35s - loss: 33.5816 - MinusLogProbMetric: 33.5816 - val_loss: 33.2817 - val_MinusLogProbMetric: 33.2817 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 53/1000
2023-10-25 23:01:41.187 
Epoch 53/1000 
	 loss: 33.5119, MinusLogProbMetric: 33.5119, val_loss: 32.7127, val_MinusLogProbMetric: 32.7127

Epoch 53: val_loss improved from 33.09387 to 32.71273, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_369/weights/best_weights.h5
196/196 - 36s - loss: 33.5119 - MinusLogProbMetric: 33.5119 - val_loss: 32.7127 - val_MinusLogProbMetric: 32.7127 - lr: 0.0010 - 36s/epoch - 185ms/step
Epoch 54/1000
2023-10-25 23:02:17.260 
Epoch 54/1000 
	 loss: 33.8952, MinusLogProbMetric: 33.8952, val_loss: 32.9686, val_MinusLogProbMetric: 32.9686

Epoch 54: val_loss did not improve from 32.71273
196/196 - 35s - loss: 33.8952 - MinusLogProbMetric: 33.8952 - val_loss: 32.9686 - val_MinusLogProbMetric: 32.9686 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 55/1000
2023-10-25 23:02:52.863 
Epoch 55/1000 
	 loss: 33.3902, MinusLogProbMetric: 33.3902, val_loss: 32.4649, val_MinusLogProbMetric: 32.4649

Epoch 55: val_loss improved from 32.71273 to 32.46494, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_369/weights/best_weights.h5
196/196 - 36s - loss: 33.3902 - MinusLogProbMetric: 33.3902 - val_loss: 32.4649 - val_MinusLogProbMetric: 32.4649 - lr: 0.0010 - 36s/epoch - 184ms/step
Epoch 56/1000
2023-10-25 23:03:28.998 
Epoch 56/1000 
	 loss: 33.3347, MinusLogProbMetric: 33.3347, val_loss: 35.0940, val_MinusLogProbMetric: 35.0940

Epoch 56: val_loss did not improve from 32.46494
196/196 - 36s - loss: 33.3347 - MinusLogProbMetric: 33.3347 - val_loss: 35.0940 - val_MinusLogProbMetric: 35.0940 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 57/1000
2023-10-25 23:04:04.490 
Epoch 57/1000 
	 loss: 33.3864, MinusLogProbMetric: 33.3864, val_loss: 32.5028, val_MinusLogProbMetric: 32.5028

Epoch 57: val_loss did not improve from 32.46494
196/196 - 35s - loss: 33.3864 - MinusLogProbMetric: 33.3864 - val_loss: 32.5028 - val_MinusLogProbMetric: 32.5028 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 58/1000
2023-10-25 23:04:40.116 
Epoch 58/1000 
	 loss: 33.4617, MinusLogProbMetric: 33.4617, val_loss: 34.4687, val_MinusLogProbMetric: 34.4687

Epoch 58: val_loss did not improve from 32.46494
196/196 - 36s - loss: 33.4617 - MinusLogProbMetric: 33.4617 - val_loss: 34.4687 - val_MinusLogProbMetric: 34.4687 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 59/1000
2023-10-25 23:05:15.430 
Epoch 59/1000 
	 loss: 33.1106, MinusLogProbMetric: 33.1106, val_loss: 33.1528, val_MinusLogProbMetric: 33.1528

Epoch 59: val_loss did not improve from 32.46494
196/196 - 35s - loss: 33.1106 - MinusLogProbMetric: 33.1106 - val_loss: 33.1528 - val_MinusLogProbMetric: 33.1528 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 60/1000
2023-10-25 23:05:50.618 
Epoch 60/1000 
	 loss: 33.2331, MinusLogProbMetric: 33.2331, val_loss: 41.2296, val_MinusLogProbMetric: 41.2296

Epoch 60: val_loss did not improve from 32.46494
196/196 - 35s - loss: 33.2331 - MinusLogProbMetric: 33.2331 - val_loss: 41.2296 - val_MinusLogProbMetric: 41.2296 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 61/1000
2023-10-25 23:06:25.856 
Epoch 61/1000 
	 loss: 33.1906, MinusLogProbMetric: 33.1906, val_loss: 33.5113, val_MinusLogProbMetric: 33.5113

Epoch 61: val_loss did not improve from 32.46494
196/196 - 35s - loss: 33.1906 - MinusLogProbMetric: 33.1906 - val_loss: 33.5113 - val_MinusLogProbMetric: 33.5113 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 62/1000
2023-10-25 23:07:01.123 
Epoch 62/1000 
	 loss: 32.9635, MinusLogProbMetric: 32.9635, val_loss: 32.6840, val_MinusLogProbMetric: 32.6840

Epoch 62: val_loss did not improve from 32.46494
196/196 - 35s - loss: 32.9635 - MinusLogProbMetric: 32.9635 - val_loss: 32.6840 - val_MinusLogProbMetric: 32.6840 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 63/1000
2023-10-25 23:07:36.135 
Epoch 63/1000 
	 loss: 33.1459, MinusLogProbMetric: 33.1459, val_loss: 33.2500, val_MinusLogProbMetric: 33.2500

Epoch 63: val_loss did not improve from 32.46494
196/196 - 35s - loss: 33.1459 - MinusLogProbMetric: 33.1459 - val_loss: 33.2500 - val_MinusLogProbMetric: 33.2500 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 64/1000
2023-10-25 23:08:11.540 
Epoch 64/1000 
	 loss: 32.8952, MinusLogProbMetric: 32.8952, val_loss: 36.5198, val_MinusLogProbMetric: 36.5198

Epoch 64: val_loss did not improve from 32.46494
196/196 - 35s - loss: 32.8952 - MinusLogProbMetric: 32.8952 - val_loss: 36.5198 - val_MinusLogProbMetric: 36.5198 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 65/1000
2023-10-25 23:08:46.819 
Epoch 65/1000 
	 loss: 33.2992, MinusLogProbMetric: 33.2992, val_loss: 33.5252, val_MinusLogProbMetric: 33.5252

Epoch 65: val_loss did not improve from 32.46494
196/196 - 35s - loss: 33.2992 - MinusLogProbMetric: 33.2992 - val_loss: 33.5252 - val_MinusLogProbMetric: 33.5252 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 66/1000
2023-10-25 23:09:22.338 
Epoch 66/1000 
	 loss: 32.8723, MinusLogProbMetric: 32.8723, val_loss: 33.8858, val_MinusLogProbMetric: 33.8858

Epoch 66: val_loss did not improve from 32.46494
196/196 - 36s - loss: 32.8723 - MinusLogProbMetric: 32.8723 - val_loss: 33.8858 - val_MinusLogProbMetric: 33.8858 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 67/1000
2023-10-25 23:09:58.045 
Epoch 67/1000 
	 loss: 32.8237, MinusLogProbMetric: 32.8237, val_loss: 33.5785, val_MinusLogProbMetric: 33.5785

Epoch 67: val_loss did not improve from 32.46494
196/196 - 36s - loss: 32.8237 - MinusLogProbMetric: 32.8237 - val_loss: 33.5785 - val_MinusLogProbMetric: 33.5785 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 68/1000
2023-10-25 23:10:33.902 
Epoch 68/1000 
	 loss: 33.0122, MinusLogProbMetric: 33.0122, val_loss: 32.8506, val_MinusLogProbMetric: 32.8506

Epoch 68: val_loss did not improve from 32.46494
196/196 - 36s - loss: 33.0122 - MinusLogProbMetric: 33.0122 - val_loss: 32.8506 - val_MinusLogProbMetric: 32.8506 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 69/1000
2023-10-25 23:11:09.515 
Epoch 69/1000 
	 loss: 32.6107, MinusLogProbMetric: 32.6107, val_loss: 33.3572, val_MinusLogProbMetric: 33.3572

Epoch 69: val_loss did not improve from 32.46494
196/196 - 36s - loss: 32.6107 - MinusLogProbMetric: 32.6107 - val_loss: 33.3572 - val_MinusLogProbMetric: 33.3572 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 70/1000
2023-10-25 23:11:45.004 
Epoch 70/1000 
	 loss: 32.7515, MinusLogProbMetric: 32.7515, val_loss: 33.3420, val_MinusLogProbMetric: 33.3420

Epoch 70: val_loss did not improve from 32.46494
196/196 - 35s - loss: 32.7515 - MinusLogProbMetric: 32.7515 - val_loss: 33.3420 - val_MinusLogProbMetric: 33.3420 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 71/1000
2023-10-25 23:12:20.299 
Epoch 71/1000 
	 loss: 32.3067, MinusLogProbMetric: 32.3067, val_loss: 34.2371, val_MinusLogProbMetric: 34.2371

Epoch 71: val_loss did not improve from 32.46494
196/196 - 35s - loss: 32.3067 - MinusLogProbMetric: 32.3067 - val_loss: 34.2371 - val_MinusLogProbMetric: 34.2371 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 72/1000
2023-10-25 23:12:55.603 
Epoch 72/1000 
	 loss: 32.7187, MinusLogProbMetric: 32.7187, val_loss: 31.9434, val_MinusLogProbMetric: 31.9434

Epoch 72: val_loss improved from 32.46494 to 31.94340, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_369/weights/best_weights.h5
196/196 - 36s - loss: 32.7187 - MinusLogProbMetric: 32.7187 - val_loss: 31.9434 - val_MinusLogProbMetric: 31.9434 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 73/1000
2023-10-25 23:13:31.190 
Epoch 73/1000 
	 loss: 32.5582, MinusLogProbMetric: 32.5582, val_loss: 32.3440, val_MinusLogProbMetric: 32.3440

Epoch 73: val_loss did not improve from 31.94340
196/196 - 35s - loss: 32.5582 - MinusLogProbMetric: 32.5582 - val_loss: 32.3440 - val_MinusLogProbMetric: 32.3440 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 74/1000
2023-10-25 23:14:06.784 
Epoch 74/1000 
	 loss: 32.4519, MinusLogProbMetric: 32.4519, val_loss: 32.4207, val_MinusLogProbMetric: 32.4207

Epoch 74: val_loss did not improve from 31.94340
196/196 - 36s - loss: 32.4519 - MinusLogProbMetric: 32.4519 - val_loss: 32.4207 - val_MinusLogProbMetric: 32.4207 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 75/1000
2023-10-25 23:14:42.111 
Epoch 75/1000 
	 loss: 32.3813, MinusLogProbMetric: 32.3813, val_loss: 31.9714, val_MinusLogProbMetric: 31.9714

Epoch 75: val_loss did not improve from 31.94340
196/196 - 35s - loss: 32.3813 - MinusLogProbMetric: 32.3813 - val_loss: 31.9714 - val_MinusLogProbMetric: 31.9714 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 76/1000
2023-10-25 23:15:17.402 
Epoch 76/1000 
	 loss: 32.3570, MinusLogProbMetric: 32.3570, val_loss: 32.8534, val_MinusLogProbMetric: 32.8534

Epoch 76: val_loss did not improve from 31.94340
196/196 - 35s - loss: 32.3570 - MinusLogProbMetric: 32.3570 - val_loss: 32.8534 - val_MinusLogProbMetric: 32.8534 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 77/1000
2023-10-25 23:15:52.949 
Epoch 77/1000 
	 loss: 32.2689, MinusLogProbMetric: 32.2689, val_loss: 32.7992, val_MinusLogProbMetric: 32.7992

Epoch 77: val_loss did not improve from 31.94340
196/196 - 36s - loss: 32.2689 - MinusLogProbMetric: 32.2689 - val_loss: 32.7992 - val_MinusLogProbMetric: 32.7992 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 78/1000
2023-10-25 23:16:28.300 
Epoch 78/1000 
	 loss: 32.4558, MinusLogProbMetric: 32.4558, val_loss: 32.8291, val_MinusLogProbMetric: 32.8291

Epoch 78: val_loss did not improve from 31.94340
196/196 - 35s - loss: 32.4558 - MinusLogProbMetric: 32.4558 - val_loss: 32.8291 - val_MinusLogProbMetric: 32.8291 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 79/1000
2023-10-25 23:17:03.652 
Epoch 79/1000 
	 loss: 32.5021, MinusLogProbMetric: 32.5021, val_loss: 32.1084, val_MinusLogProbMetric: 32.1084

Epoch 79: val_loss did not improve from 31.94340
196/196 - 35s - loss: 32.5021 - MinusLogProbMetric: 32.5021 - val_loss: 32.1084 - val_MinusLogProbMetric: 32.1084 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 80/1000
2023-10-25 23:17:39.248 
Epoch 80/1000 
	 loss: 32.6261, MinusLogProbMetric: 32.6261, val_loss: 32.8385, val_MinusLogProbMetric: 32.8385

Epoch 80: val_loss did not improve from 31.94340
196/196 - 36s - loss: 32.6261 - MinusLogProbMetric: 32.6261 - val_loss: 32.8385 - val_MinusLogProbMetric: 32.8385 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 81/1000
2023-10-25 23:18:14.854 
Epoch 81/1000 
	 loss: 32.1609, MinusLogProbMetric: 32.1609, val_loss: 32.3054, val_MinusLogProbMetric: 32.3054

Epoch 81: val_loss did not improve from 31.94340
196/196 - 36s - loss: 32.1609 - MinusLogProbMetric: 32.1609 - val_loss: 32.3054 - val_MinusLogProbMetric: 32.3054 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 82/1000
2023-10-25 23:18:50.166 
Epoch 82/1000 
	 loss: 32.3834, MinusLogProbMetric: 32.3834, val_loss: 32.2548, val_MinusLogProbMetric: 32.2548

Epoch 82: val_loss did not improve from 31.94340
196/196 - 35s - loss: 32.3834 - MinusLogProbMetric: 32.3834 - val_loss: 32.2548 - val_MinusLogProbMetric: 32.2548 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 83/1000
2023-10-25 23:19:25.364 
Epoch 83/1000 
	 loss: 32.3889, MinusLogProbMetric: 32.3889, val_loss: 31.8916, val_MinusLogProbMetric: 31.8916

Epoch 83: val_loss improved from 31.94340 to 31.89157, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_369/weights/best_weights.h5
196/196 - 36s - loss: 32.3889 - MinusLogProbMetric: 32.3889 - val_loss: 31.8916 - val_MinusLogProbMetric: 31.8916 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 84/1000
2023-10-25 23:20:01.150 
Epoch 84/1000 
	 loss: 32.4563, MinusLogProbMetric: 32.4563, val_loss: 33.8467, val_MinusLogProbMetric: 33.8467

Epoch 84: val_loss did not improve from 31.89157
196/196 - 35s - loss: 32.4563 - MinusLogProbMetric: 32.4563 - val_loss: 33.8467 - val_MinusLogProbMetric: 33.8467 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 85/1000
2023-10-25 23:20:36.743 
Epoch 85/1000 
	 loss: 32.2119, MinusLogProbMetric: 32.2119, val_loss: 31.6680, val_MinusLogProbMetric: 31.6680

Epoch 85: val_loss improved from 31.89157 to 31.66796, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_369/weights/best_weights.h5
196/196 - 36s - loss: 32.2119 - MinusLogProbMetric: 32.2119 - val_loss: 31.6680 - val_MinusLogProbMetric: 31.6680 - lr: 0.0010 - 36s/epoch - 185ms/step
Epoch 86/1000
2023-10-25 23:21:13.162 
Epoch 86/1000 
	 loss: 31.9119, MinusLogProbMetric: 31.9119, val_loss: 32.2931, val_MinusLogProbMetric: 32.2931

Epoch 86: val_loss did not improve from 31.66796
196/196 - 36s - loss: 31.9119 - MinusLogProbMetric: 31.9119 - val_loss: 32.2931 - val_MinusLogProbMetric: 32.2931 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 87/1000
2023-10-25 23:21:48.787 
Epoch 87/1000 
	 loss: 31.9973, MinusLogProbMetric: 31.9973, val_loss: 31.8727, val_MinusLogProbMetric: 31.8727

Epoch 87: val_loss did not improve from 31.66796
196/196 - 36s - loss: 31.9973 - MinusLogProbMetric: 31.9973 - val_loss: 31.8727 - val_MinusLogProbMetric: 31.8727 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 88/1000
2023-10-25 23:22:24.317 
Epoch 88/1000 
	 loss: 32.1947, MinusLogProbMetric: 32.1947, val_loss: 31.9191, val_MinusLogProbMetric: 31.9191

Epoch 88: val_loss did not improve from 31.66796
196/196 - 36s - loss: 32.1947 - MinusLogProbMetric: 32.1947 - val_loss: 31.9191 - val_MinusLogProbMetric: 31.9191 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 89/1000
2023-10-25 23:22:59.806 
Epoch 89/1000 
	 loss: 31.9736, MinusLogProbMetric: 31.9736, val_loss: 32.7382, val_MinusLogProbMetric: 32.7382

Epoch 89: val_loss did not improve from 31.66796
196/196 - 35s - loss: 31.9736 - MinusLogProbMetric: 31.9736 - val_loss: 32.7382 - val_MinusLogProbMetric: 32.7382 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 90/1000
2023-10-25 23:23:35.284 
Epoch 90/1000 
	 loss: 31.9225, MinusLogProbMetric: 31.9225, val_loss: 33.6750, val_MinusLogProbMetric: 33.6750

Epoch 90: val_loss did not improve from 31.66796
196/196 - 35s - loss: 31.9225 - MinusLogProbMetric: 31.9225 - val_loss: 33.6750 - val_MinusLogProbMetric: 33.6750 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 91/1000
2023-10-25 23:24:11.012 
Epoch 91/1000 
	 loss: 31.8517, MinusLogProbMetric: 31.8517, val_loss: 32.7812, val_MinusLogProbMetric: 32.7812

Epoch 91: val_loss did not improve from 31.66796
196/196 - 36s - loss: 31.8517 - MinusLogProbMetric: 31.8517 - val_loss: 32.7812 - val_MinusLogProbMetric: 32.7812 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 92/1000
2023-10-25 23:24:46.142 
Epoch 92/1000 
	 loss: 31.8439, MinusLogProbMetric: 31.8439, val_loss: 32.0059, val_MinusLogProbMetric: 32.0059

Epoch 92: val_loss did not improve from 31.66796
196/196 - 35s - loss: 31.8439 - MinusLogProbMetric: 31.8439 - val_loss: 32.0059 - val_MinusLogProbMetric: 32.0059 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 93/1000
2023-10-25 23:25:21.279 
Epoch 93/1000 
	 loss: 32.0250, MinusLogProbMetric: 32.0250, val_loss: 35.6420, val_MinusLogProbMetric: 35.6420

Epoch 93: val_loss did not improve from 31.66796
196/196 - 35s - loss: 32.0250 - MinusLogProbMetric: 32.0250 - val_loss: 35.6420 - val_MinusLogProbMetric: 35.6420 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 94/1000
2023-10-25 23:25:56.871 
Epoch 94/1000 
	 loss: 31.9400, MinusLogProbMetric: 31.9400, val_loss: 32.3723, val_MinusLogProbMetric: 32.3723

Epoch 94: val_loss did not improve from 31.66796
196/196 - 36s - loss: 31.9400 - MinusLogProbMetric: 31.9400 - val_loss: 32.3723 - val_MinusLogProbMetric: 32.3723 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 95/1000
2023-10-25 23:26:32.192 
Epoch 95/1000 
	 loss: 32.1306, MinusLogProbMetric: 32.1306, val_loss: 31.8106, val_MinusLogProbMetric: 31.8106

Epoch 95: val_loss did not improve from 31.66796
196/196 - 35s - loss: 32.1306 - MinusLogProbMetric: 32.1306 - val_loss: 31.8106 - val_MinusLogProbMetric: 31.8106 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 96/1000
2023-10-25 23:27:07.835 
Epoch 96/1000 
	 loss: 31.6994, MinusLogProbMetric: 31.6994, val_loss: 31.1794, val_MinusLogProbMetric: 31.1794

Epoch 96: val_loss improved from 31.66796 to 31.17943, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_369/weights/best_weights.h5
196/196 - 36s - loss: 31.6994 - MinusLogProbMetric: 31.6994 - val_loss: 31.1794 - val_MinusLogProbMetric: 31.1794 - lr: 0.0010 - 36s/epoch - 185ms/step
Epoch 97/1000
2023-10-25 23:27:43.600 
Epoch 97/1000 
	 loss: 31.5947, MinusLogProbMetric: 31.5947, val_loss: 31.2463, val_MinusLogProbMetric: 31.2463

Epoch 97: val_loss did not improve from 31.17943
196/196 - 35s - loss: 31.5947 - MinusLogProbMetric: 31.5947 - val_loss: 31.2463 - val_MinusLogProbMetric: 31.2463 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 98/1000
2023-10-25 23:28:18.968 
Epoch 98/1000 
	 loss: 31.9505, MinusLogProbMetric: 31.9505, val_loss: 32.9838, val_MinusLogProbMetric: 32.9838

Epoch 98: val_loss did not improve from 31.17943
196/196 - 35s - loss: 31.9505 - MinusLogProbMetric: 31.9505 - val_loss: 32.9838 - val_MinusLogProbMetric: 32.9838 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 99/1000
2023-10-25 23:28:54.508 
Epoch 99/1000 
	 loss: 32.0201, MinusLogProbMetric: 32.0201, val_loss: 31.3687, val_MinusLogProbMetric: 31.3687

Epoch 99: val_loss did not improve from 31.17943
196/196 - 36s - loss: 32.0201 - MinusLogProbMetric: 32.0201 - val_loss: 31.3687 - val_MinusLogProbMetric: 31.3687 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 100/1000
2023-10-25 23:29:30.128 
Epoch 100/1000 
	 loss: 31.4648, MinusLogProbMetric: 31.4648, val_loss: 31.4963, val_MinusLogProbMetric: 31.4963

Epoch 100: val_loss did not improve from 31.17943
196/196 - 36s - loss: 31.4648 - MinusLogProbMetric: 31.4648 - val_loss: 31.4963 - val_MinusLogProbMetric: 31.4963 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 101/1000
2023-10-25 23:30:05.517 
Epoch 101/1000 
	 loss: 31.4523, MinusLogProbMetric: 31.4523, val_loss: 32.4498, val_MinusLogProbMetric: 32.4498

Epoch 101: val_loss did not improve from 31.17943
196/196 - 35s - loss: 31.4523 - MinusLogProbMetric: 31.4523 - val_loss: 32.4498 - val_MinusLogProbMetric: 32.4498 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 102/1000
2023-10-25 23:30:41.325 
Epoch 102/1000 
	 loss: 31.8513, MinusLogProbMetric: 31.8513, val_loss: 31.5721, val_MinusLogProbMetric: 31.5721

Epoch 102: val_loss did not improve from 31.17943
196/196 - 36s - loss: 31.8513 - MinusLogProbMetric: 31.8513 - val_loss: 31.5721 - val_MinusLogProbMetric: 31.5721 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 103/1000
2023-10-25 23:31:16.923 
Epoch 103/1000 
	 loss: 31.5047, MinusLogProbMetric: 31.5047, val_loss: 30.6979, val_MinusLogProbMetric: 30.6979

Epoch 103: val_loss improved from 31.17943 to 30.69787, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_369/weights/best_weights.h5
196/196 - 36s - loss: 31.5047 - MinusLogProbMetric: 31.5047 - val_loss: 30.6979 - val_MinusLogProbMetric: 30.6979 - lr: 0.0010 - 36s/epoch - 184ms/step
Epoch 104/1000
2023-10-25 23:31:52.801 
Epoch 104/1000 
	 loss: 31.8148, MinusLogProbMetric: 31.8148, val_loss: 31.3859, val_MinusLogProbMetric: 31.3859

Epoch 104: val_loss did not improve from 30.69787
196/196 - 35s - loss: 31.8148 - MinusLogProbMetric: 31.8148 - val_loss: 31.3859 - val_MinusLogProbMetric: 31.3859 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 105/1000
2023-10-25 23:32:28.413 
Epoch 105/1000 
	 loss: 31.7248, MinusLogProbMetric: 31.7248, val_loss: 33.0981, val_MinusLogProbMetric: 33.0981

Epoch 105: val_loss did not improve from 30.69787
196/196 - 36s - loss: 31.7248 - MinusLogProbMetric: 31.7248 - val_loss: 33.0981 - val_MinusLogProbMetric: 33.0981 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 106/1000
2023-10-25 23:33:03.819 
Epoch 106/1000 
	 loss: 31.7412, MinusLogProbMetric: 31.7412, val_loss: 30.5881, val_MinusLogProbMetric: 30.5881

Epoch 106: val_loss improved from 30.69787 to 30.58813, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_369/weights/best_weights.h5
196/196 - 36s - loss: 31.7412 - MinusLogProbMetric: 31.7412 - val_loss: 30.5881 - val_MinusLogProbMetric: 30.5881 - lr: 0.0010 - 36s/epoch - 184ms/step
Epoch 107/1000
2023-10-25 23:33:40.020 
Epoch 107/1000 
	 loss: 31.2665, MinusLogProbMetric: 31.2665, val_loss: 32.5434, val_MinusLogProbMetric: 32.5434

Epoch 107: val_loss did not improve from 30.58813
196/196 - 36s - loss: 31.2665 - MinusLogProbMetric: 31.2665 - val_loss: 32.5434 - val_MinusLogProbMetric: 32.5434 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 108/1000
2023-10-25 23:34:15.374 
Epoch 108/1000 
	 loss: 31.5222, MinusLogProbMetric: 31.5222, val_loss: 33.6950, val_MinusLogProbMetric: 33.6950

Epoch 108: val_loss did not improve from 30.58813
196/196 - 35s - loss: 31.5222 - MinusLogProbMetric: 31.5222 - val_loss: 33.6950 - val_MinusLogProbMetric: 33.6950 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 109/1000
2023-10-25 23:34:50.576 
Epoch 109/1000 
	 loss: 32.0311, MinusLogProbMetric: 32.0311, val_loss: 31.2809, val_MinusLogProbMetric: 31.2809

Epoch 109: val_loss did not improve from 30.58813
196/196 - 35s - loss: 32.0311 - MinusLogProbMetric: 32.0311 - val_loss: 31.2809 - val_MinusLogProbMetric: 31.2809 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 110/1000
2023-10-25 23:35:26.086 
Epoch 110/1000 
	 loss: 31.2831, MinusLogProbMetric: 31.2831, val_loss: 31.8141, val_MinusLogProbMetric: 31.8141

Epoch 110: val_loss did not improve from 30.58813
196/196 - 36s - loss: 31.2831 - MinusLogProbMetric: 31.2831 - val_loss: 31.8141 - val_MinusLogProbMetric: 31.8141 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 111/1000
2023-10-25 23:36:01.556 
Epoch 111/1000 
	 loss: 31.2533, MinusLogProbMetric: 31.2533, val_loss: 30.5638, val_MinusLogProbMetric: 30.5638

Epoch 111: val_loss improved from 30.58813 to 30.56379, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_369/weights/best_weights.h5
196/196 - 36s - loss: 31.2533 - MinusLogProbMetric: 31.2533 - val_loss: 30.5638 - val_MinusLogProbMetric: 30.5638 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 112/1000
2023-10-25 23:36:37.409 
Epoch 112/1000 
	 loss: 31.5075, MinusLogProbMetric: 31.5075, val_loss: 31.8700, val_MinusLogProbMetric: 31.8700

Epoch 112: val_loss did not improve from 30.56379
196/196 - 35s - loss: 31.5075 - MinusLogProbMetric: 31.5075 - val_loss: 31.8700 - val_MinusLogProbMetric: 31.8700 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 113/1000
2023-10-25 23:37:12.580 
Epoch 113/1000 
	 loss: 31.2591, MinusLogProbMetric: 31.2591, val_loss: 31.6626, val_MinusLogProbMetric: 31.6626

Epoch 113: val_loss did not improve from 30.56379
196/196 - 35s - loss: 31.2591 - MinusLogProbMetric: 31.2591 - val_loss: 31.6626 - val_MinusLogProbMetric: 31.6626 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 114/1000
2023-10-25 23:37:47.838 
Epoch 114/1000 
	 loss: 31.2341, MinusLogProbMetric: 31.2341, val_loss: 30.6922, val_MinusLogProbMetric: 30.6922

Epoch 114: val_loss did not improve from 30.56379
196/196 - 35s - loss: 31.2341 - MinusLogProbMetric: 31.2341 - val_loss: 30.6922 - val_MinusLogProbMetric: 30.6922 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 115/1000
2023-10-25 23:38:23.146 
Epoch 115/1000 
	 loss: 31.5456, MinusLogProbMetric: 31.5456, val_loss: 31.6483, val_MinusLogProbMetric: 31.6483

Epoch 115: val_loss did not improve from 30.56379
196/196 - 35s - loss: 31.5456 - MinusLogProbMetric: 31.5456 - val_loss: 31.6483 - val_MinusLogProbMetric: 31.6483 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 116/1000
2023-10-25 23:38:58.776 
Epoch 116/1000 
	 loss: 31.1169, MinusLogProbMetric: 31.1169, val_loss: 31.5192, val_MinusLogProbMetric: 31.5192

Epoch 116: val_loss did not improve from 30.56379
196/196 - 36s - loss: 31.1169 - MinusLogProbMetric: 31.1169 - val_loss: 31.5192 - val_MinusLogProbMetric: 31.5192 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 117/1000
2023-10-25 23:39:33.943 
Epoch 117/1000 
	 loss: 31.0876, MinusLogProbMetric: 31.0876, val_loss: 31.3269, val_MinusLogProbMetric: 31.3269

Epoch 117: val_loss did not improve from 30.56379
196/196 - 35s - loss: 31.0876 - MinusLogProbMetric: 31.0876 - val_loss: 31.3269 - val_MinusLogProbMetric: 31.3269 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 118/1000
2023-10-25 23:40:09.541 
Epoch 118/1000 
	 loss: 31.4082, MinusLogProbMetric: 31.4082, val_loss: 30.7995, val_MinusLogProbMetric: 30.7995

Epoch 118: val_loss did not improve from 30.56379
196/196 - 36s - loss: 31.4082 - MinusLogProbMetric: 31.4082 - val_loss: 30.7995 - val_MinusLogProbMetric: 30.7995 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 119/1000
2023-10-25 23:40:44.723 
Epoch 119/1000 
	 loss: 31.3049, MinusLogProbMetric: 31.3049, val_loss: 30.7004, val_MinusLogProbMetric: 30.7004

Epoch 119: val_loss did not improve from 30.56379
196/196 - 35s - loss: 31.3049 - MinusLogProbMetric: 31.3049 - val_loss: 30.7004 - val_MinusLogProbMetric: 30.7004 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 120/1000
2023-10-25 23:41:19.963 
Epoch 120/1000 
	 loss: 31.2264, MinusLogProbMetric: 31.2264, val_loss: 30.9737, val_MinusLogProbMetric: 30.9737

Epoch 120: val_loss did not improve from 30.56379
196/196 - 35s - loss: 31.2264 - MinusLogProbMetric: 31.2264 - val_loss: 30.9737 - val_MinusLogProbMetric: 30.9737 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 121/1000
2023-10-25 23:41:55.566 
Epoch 121/1000 
	 loss: 31.0577, MinusLogProbMetric: 31.0577, val_loss: 33.0934, val_MinusLogProbMetric: 33.0934

Epoch 121: val_loss did not improve from 30.56379
196/196 - 36s - loss: 31.0577 - MinusLogProbMetric: 31.0577 - val_loss: 33.0934 - val_MinusLogProbMetric: 33.0934 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 122/1000
2023-10-25 23:42:30.963 
Epoch 122/1000 
	 loss: 31.2075, MinusLogProbMetric: 31.2075, val_loss: 32.0995, val_MinusLogProbMetric: 32.0995

Epoch 122: val_loss did not improve from 30.56379
196/196 - 35s - loss: 31.2075 - MinusLogProbMetric: 31.2075 - val_loss: 32.0995 - val_MinusLogProbMetric: 32.0995 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 123/1000
2023-10-25 23:43:06.408 
Epoch 123/1000 
	 loss: 31.0658, MinusLogProbMetric: 31.0658, val_loss: 31.4179, val_MinusLogProbMetric: 31.4179

Epoch 123: val_loss did not improve from 30.56379
196/196 - 35s - loss: 31.0658 - MinusLogProbMetric: 31.0658 - val_loss: 31.4179 - val_MinusLogProbMetric: 31.4179 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 124/1000
2023-10-25 23:43:41.911 
Epoch 124/1000 
	 loss: 31.1322, MinusLogProbMetric: 31.1322, val_loss: 30.7572, val_MinusLogProbMetric: 30.7572

Epoch 124: val_loss did not improve from 30.56379
196/196 - 35s - loss: 31.1322 - MinusLogProbMetric: 31.1322 - val_loss: 30.7572 - val_MinusLogProbMetric: 30.7572 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 125/1000
2023-10-25 23:44:17.153 
Epoch 125/1000 
	 loss: 31.1620, MinusLogProbMetric: 31.1620, val_loss: 30.4582, val_MinusLogProbMetric: 30.4582

Epoch 125: val_loss improved from 30.56379 to 30.45817, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_369/weights/best_weights.h5
196/196 - 36s - loss: 31.1620 - MinusLogProbMetric: 31.1620 - val_loss: 30.4582 - val_MinusLogProbMetric: 30.4582 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 126/1000
2023-10-25 23:44:53.087 
Epoch 126/1000 
	 loss: 31.1987, MinusLogProbMetric: 31.1987, val_loss: 31.4410, val_MinusLogProbMetric: 31.4410

Epoch 126: val_loss did not improve from 30.45817
196/196 - 35s - loss: 31.1987 - MinusLogProbMetric: 31.1987 - val_loss: 31.4410 - val_MinusLogProbMetric: 31.4410 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 127/1000
2023-10-25 23:45:28.804 
Epoch 127/1000 
	 loss: 31.0093, MinusLogProbMetric: 31.0093, val_loss: 31.9701, val_MinusLogProbMetric: 31.9701

Epoch 127: val_loss did not improve from 30.45817
196/196 - 36s - loss: 31.0093 - MinusLogProbMetric: 31.0093 - val_loss: 31.9701 - val_MinusLogProbMetric: 31.9701 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 128/1000
2023-10-25 23:46:04.243 
Epoch 128/1000 
	 loss: 30.8491, MinusLogProbMetric: 30.8491, val_loss: 35.6503, val_MinusLogProbMetric: 35.6503

Epoch 128: val_loss did not improve from 30.45817
196/196 - 35s - loss: 30.8491 - MinusLogProbMetric: 30.8491 - val_loss: 35.6503 - val_MinusLogProbMetric: 35.6503 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 129/1000
2023-10-25 23:46:39.691 
Epoch 129/1000 
	 loss: 31.2381, MinusLogProbMetric: 31.2381, val_loss: 31.7850, val_MinusLogProbMetric: 31.7850

Epoch 129: val_loss did not improve from 30.45817
196/196 - 35s - loss: 31.2381 - MinusLogProbMetric: 31.2381 - val_loss: 31.7850 - val_MinusLogProbMetric: 31.7850 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 130/1000
2023-10-25 23:47:14.978 
Epoch 130/1000 
	 loss: 30.9977, MinusLogProbMetric: 30.9977, val_loss: 31.5098, val_MinusLogProbMetric: 31.5098

Epoch 130: val_loss did not improve from 30.45817
196/196 - 35s - loss: 30.9977 - MinusLogProbMetric: 30.9977 - val_loss: 31.5098 - val_MinusLogProbMetric: 31.5098 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 131/1000
2023-10-25 23:47:49.791 
Epoch 131/1000 
	 loss: 31.3776, MinusLogProbMetric: 31.3776, val_loss: 32.5259, val_MinusLogProbMetric: 32.5259

Epoch 131: val_loss did not improve from 30.45817
196/196 - 35s - loss: 31.3776 - MinusLogProbMetric: 31.3776 - val_loss: 32.5259 - val_MinusLogProbMetric: 32.5259 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 132/1000
2023-10-25 23:48:25.464 
Epoch 132/1000 
	 loss: 30.8599, MinusLogProbMetric: 30.8599, val_loss: 30.7905, val_MinusLogProbMetric: 30.7905

Epoch 132: val_loss did not improve from 30.45817
196/196 - 36s - loss: 30.8599 - MinusLogProbMetric: 30.8599 - val_loss: 30.7905 - val_MinusLogProbMetric: 30.7905 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 133/1000
2023-10-25 23:49:01.215 
Epoch 133/1000 
	 loss: 30.8099, MinusLogProbMetric: 30.8099, val_loss: 31.4493, val_MinusLogProbMetric: 31.4493

Epoch 133: val_loss did not improve from 30.45817
196/196 - 36s - loss: 30.8099 - MinusLogProbMetric: 30.8099 - val_loss: 31.4493 - val_MinusLogProbMetric: 31.4493 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 134/1000
2023-10-25 23:49:36.881 
Epoch 134/1000 
	 loss: 31.0554, MinusLogProbMetric: 31.0554, val_loss: 30.8729, val_MinusLogProbMetric: 30.8729

Epoch 134: val_loss did not improve from 30.45817
196/196 - 36s - loss: 31.0554 - MinusLogProbMetric: 31.0554 - val_loss: 30.8729 - val_MinusLogProbMetric: 30.8729 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 135/1000
2023-10-25 23:50:12.414 
Epoch 135/1000 
	 loss: 30.7002, MinusLogProbMetric: 30.7002, val_loss: 30.8035, val_MinusLogProbMetric: 30.8035

Epoch 135: val_loss did not improve from 30.45817
196/196 - 36s - loss: 30.7002 - MinusLogProbMetric: 30.7002 - val_loss: 30.8035 - val_MinusLogProbMetric: 30.8035 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 136/1000
2023-10-25 23:50:48.438 
Epoch 136/1000 
	 loss: 30.9180, MinusLogProbMetric: 30.9180, val_loss: 31.8117, val_MinusLogProbMetric: 31.8117

Epoch 136: val_loss did not improve from 30.45817
196/196 - 36s - loss: 30.9180 - MinusLogProbMetric: 30.9180 - val_loss: 31.8117 - val_MinusLogProbMetric: 31.8117 - lr: 0.0010 - 36s/epoch - 184ms/step
Epoch 137/1000
2023-10-25 23:51:23.976 
Epoch 137/1000 
	 loss: 30.9208, MinusLogProbMetric: 30.9208, val_loss: 31.5004, val_MinusLogProbMetric: 31.5004

Epoch 137: val_loss did not improve from 30.45817
196/196 - 36s - loss: 30.9208 - MinusLogProbMetric: 30.9208 - val_loss: 31.5004 - val_MinusLogProbMetric: 31.5004 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 138/1000
2023-10-25 23:51:59.270 
Epoch 138/1000 
	 loss: 30.9993, MinusLogProbMetric: 30.9993, val_loss: 31.5916, val_MinusLogProbMetric: 31.5916

Epoch 138: val_loss did not improve from 30.45817
196/196 - 35s - loss: 30.9993 - MinusLogProbMetric: 30.9993 - val_loss: 31.5916 - val_MinusLogProbMetric: 31.5916 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 139/1000
2023-10-25 23:52:34.380 
Epoch 139/1000 
	 loss: 30.9035, MinusLogProbMetric: 30.9035, val_loss: 30.7973, val_MinusLogProbMetric: 30.7973

Epoch 139: val_loss did not improve from 30.45817
196/196 - 35s - loss: 30.9035 - MinusLogProbMetric: 30.9035 - val_loss: 30.7973 - val_MinusLogProbMetric: 30.7973 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 140/1000
2023-10-25 23:53:09.529 
Epoch 140/1000 
	 loss: 30.9550, MinusLogProbMetric: 30.9550, val_loss: 31.8173, val_MinusLogProbMetric: 31.8173

Epoch 140: val_loss did not improve from 30.45817
196/196 - 35s - loss: 30.9550 - MinusLogProbMetric: 30.9550 - val_loss: 31.8173 - val_MinusLogProbMetric: 31.8173 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 141/1000
2023-10-25 23:53:45.030 
Epoch 141/1000 
	 loss: 30.6370, MinusLogProbMetric: 30.6370, val_loss: 30.0686, val_MinusLogProbMetric: 30.0686

Epoch 141: val_loss improved from 30.45817 to 30.06855, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_369/weights/best_weights.h5
196/196 - 36s - loss: 30.6370 - MinusLogProbMetric: 30.6370 - val_loss: 30.0686 - val_MinusLogProbMetric: 30.0686 - lr: 0.0010 - 36s/epoch - 184ms/step
Epoch 142/1000
2023-10-25 23:54:20.920 
Epoch 142/1000 
	 loss: 31.0991, MinusLogProbMetric: 31.0991, val_loss: 30.8111, val_MinusLogProbMetric: 30.8111

Epoch 142: val_loss did not improve from 30.06855
196/196 - 35s - loss: 31.0991 - MinusLogProbMetric: 31.0991 - val_loss: 30.8111 - val_MinusLogProbMetric: 30.8111 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 143/1000
2023-10-25 23:54:56.218 
Epoch 143/1000 
	 loss: 30.6554, MinusLogProbMetric: 30.6554, val_loss: 30.2475, val_MinusLogProbMetric: 30.2475

Epoch 143: val_loss did not improve from 30.06855
196/196 - 35s - loss: 30.6554 - MinusLogProbMetric: 30.6554 - val_loss: 30.2475 - val_MinusLogProbMetric: 30.2475 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 144/1000
2023-10-25 23:55:31.518 
Epoch 144/1000 
	 loss: 30.8237, MinusLogProbMetric: 30.8237, val_loss: 30.6198, val_MinusLogProbMetric: 30.6198

Epoch 144: val_loss did not improve from 30.06855
196/196 - 35s - loss: 30.8237 - MinusLogProbMetric: 30.8237 - val_loss: 30.6198 - val_MinusLogProbMetric: 30.6198 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 145/1000
2023-10-25 23:56:07.319 
Epoch 145/1000 
	 loss: 30.6269, MinusLogProbMetric: 30.6269, val_loss: 30.2901, val_MinusLogProbMetric: 30.2901

Epoch 145: val_loss did not improve from 30.06855
196/196 - 36s - loss: 30.6269 - MinusLogProbMetric: 30.6269 - val_loss: 30.2901 - val_MinusLogProbMetric: 30.2901 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 146/1000
2023-10-25 23:56:42.810 
Epoch 146/1000 
	 loss: 30.8129, MinusLogProbMetric: 30.8129, val_loss: 30.9421, val_MinusLogProbMetric: 30.9421

Epoch 146: val_loss did not improve from 30.06855
196/196 - 35s - loss: 30.8129 - MinusLogProbMetric: 30.8129 - val_loss: 30.9421 - val_MinusLogProbMetric: 30.9421 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 147/1000
2023-10-25 23:57:18.171 
Epoch 147/1000 
	 loss: 30.8903, MinusLogProbMetric: 30.8903, val_loss: 30.2774, val_MinusLogProbMetric: 30.2774

Epoch 147: val_loss did not improve from 30.06855
196/196 - 35s - loss: 30.8903 - MinusLogProbMetric: 30.8903 - val_loss: 30.2774 - val_MinusLogProbMetric: 30.2774 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 148/1000
2023-10-25 23:57:53.511 
Epoch 148/1000 
	 loss: 30.9680, MinusLogProbMetric: 30.9680, val_loss: 30.8579, val_MinusLogProbMetric: 30.8579

Epoch 148: val_loss did not improve from 30.06855
196/196 - 35s - loss: 30.9680 - MinusLogProbMetric: 30.9680 - val_loss: 30.8579 - val_MinusLogProbMetric: 30.8579 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 149/1000
2023-10-25 23:58:28.873 
Epoch 149/1000 
	 loss: 30.9248, MinusLogProbMetric: 30.9248, val_loss: 31.6641, val_MinusLogProbMetric: 31.6641

Epoch 149: val_loss did not improve from 30.06855
196/196 - 35s - loss: 30.9248 - MinusLogProbMetric: 30.9248 - val_loss: 31.6641 - val_MinusLogProbMetric: 31.6641 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 150/1000
2023-10-25 23:59:04.575 
Epoch 150/1000 
	 loss: 30.6726, MinusLogProbMetric: 30.6726, val_loss: 31.1107, val_MinusLogProbMetric: 31.1107

Epoch 150: val_loss did not improve from 30.06855
196/196 - 36s - loss: 30.6726 - MinusLogProbMetric: 30.6726 - val_loss: 31.1107 - val_MinusLogProbMetric: 31.1107 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 151/1000
2023-10-25 23:59:40.163 
Epoch 151/1000 
	 loss: 30.8907, MinusLogProbMetric: 30.8907, val_loss: 31.2388, val_MinusLogProbMetric: 31.2388

Epoch 151: val_loss did not improve from 30.06855
196/196 - 36s - loss: 30.8907 - MinusLogProbMetric: 30.8907 - val_loss: 31.2388 - val_MinusLogProbMetric: 31.2388 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 152/1000
2023-10-26 00:00:15.269 
Epoch 152/1000 
	 loss: 30.5884, MinusLogProbMetric: 30.5884, val_loss: 30.6655, val_MinusLogProbMetric: 30.6655

Epoch 152: val_loss did not improve from 30.06855
196/196 - 35s - loss: 30.5884 - MinusLogProbMetric: 30.5884 - val_loss: 30.6655 - val_MinusLogProbMetric: 30.6655 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 153/1000
2023-10-26 00:00:50.492 
Epoch 153/1000 
	 loss: 30.7076, MinusLogProbMetric: 30.7076, val_loss: 30.3246, val_MinusLogProbMetric: 30.3246

Epoch 153: val_loss did not improve from 30.06855
196/196 - 35s - loss: 30.7076 - MinusLogProbMetric: 30.7076 - val_loss: 30.3246 - val_MinusLogProbMetric: 30.3246 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 154/1000
2023-10-26 00:01:19.435 
Epoch 154/1000 
	 loss: 30.5598, MinusLogProbMetric: 30.5598, val_loss: 32.7628, val_MinusLogProbMetric: 32.7628

Epoch 154: val_loss did not improve from 30.06855
196/196 - 29s - loss: 30.5598 - MinusLogProbMetric: 30.5598 - val_loss: 32.7628 - val_MinusLogProbMetric: 32.7628 - lr: 0.0010 - 29s/epoch - 148ms/step
Epoch 155/1000
2023-10-26 00:01:51.598 
Epoch 155/1000 
	 loss: 30.7659, MinusLogProbMetric: 30.7659, val_loss: 33.3568, val_MinusLogProbMetric: 33.3568

Epoch 155: val_loss did not improve from 30.06855
196/196 - 32s - loss: 30.7659 - MinusLogProbMetric: 30.7659 - val_loss: 33.3568 - val_MinusLogProbMetric: 33.3568 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 156/1000
2023-10-26 00:02:26.873 
Epoch 156/1000 
	 loss: 30.8465, MinusLogProbMetric: 30.8465, val_loss: 31.7840, val_MinusLogProbMetric: 31.7840

Epoch 156: val_loss did not improve from 30.06855
196/196 - 35s - loss: 30.8465 - MinusLogProbMetric: 30.8465 - val_loss: 31.7840 - val_MinusLogProbMetric: 31.7840 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 157/1000
2023-10-26 00:03:02.318 
Epoch 157/1000 
	 loss: 30.6673, MinusLogProbMetric: 30.6673, val_loss: 30.4424, val_MinusLogProbMetric: 30.4424

Epoch 157: val_loss did not improve from 30.06855
196/196 - 35s - loss: 30.6673 - MinusLogProbMetric: 30.6673 - val_loss: 30.4424 - val_MinusLogProbMetric: 30.4424 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 158/1000
2023-10-26 00:03:37.662 
Epoch 158/1000 
	 loss: 30.7661, MinusLogProbMetric: 30.7661, val_loss: 30.5820, val_MinusLogProbMetric: 30.5820

Epoch 158: val_loss did not improve from 30.06855
196/196 - 35s - loss: 30.7661 - MinusLogProbMetric: 30.7661 - val_loss: 30.5820 - val_MinusLogProbMetric: 30.5820 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 159/1000
2023-10-26 00:04:12.946 
Epoch 159/1000 
	 loss: 30.7194, MinusLogProbMetric: 30.7194, val_loss: 31.4300, val_MinusLogProbMetric: 31.4300

Epoch 159: val_loss did not improve from 30.06855
196/196 - 35s - loss: 30.7194 - MinusLogProbMetric: 30.7194 - val_loss: 31.4300 - val_MinusLogProbMetric: 31.4300 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 160/1000
2023-10-26 00:04:44.228 
Epoch 160/1000 
	 loss: 30.5902, MinusLogProbMetric: 30.5902, val_loss: 30.5764, val_MinusLogProbMetric: 30.5764

Epoch 160: val_loss did not improve from 30.06855
196/196 - 31s - loss: 30.5902 - MinusLogProbMetric: 30.5902 - val_loss: 30.5764 - val_MinusLogProbMetric: 30.5764 - lr: 0.0010 - 31s/epoch - 160ms/step
Epoch 161/1000
2023-10-26 00:05:14.137 
Epoch 161/1000 
	 loss: 30.8561, MinusLogProbMetric: 30.8561, val_loss: 30.5065, val_MinusLogProbMetric: 30.5065

Epoch 161: val_loss did not improve from 30.06855
196/196 - 30s - loss: 30.8561 - MinusLogProbMetric: 30.8561 - val_loss: 30.5065 - val_MinusLogProbMetric: 30.5065 - lr: 0.0010 - 30s/epoch - 153ms/step
Epoch 162/1000
2023-10-26 00:05:49.608 
Epoch 162/1000 
	 loss: 30.5032, MinusLogProbMetric: 30.5032, val_loss: 30.1349, val_MinusLogProbMetric: 30.1349

Epoch 162: val_loss did not improve from 30.06855
196/196 - 35s - loss: 30.5032 - MinusLogProbMetric: 30.5032 - val_loss: 30.1349 - val_MinusLogProbMetric: 30.1349 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 163/1000
2023-10-26 00:06:25.313 
Epoch 163/1000 
	 loss: 30.3472, MinusLogProbMetric: 30.3472, val_loss: 30.5708, val_MinusLogProbMetric: 30.5708

Epoch 163: val_loss did not improve from 30.06855
196/196 - 36s - loss: 30.3472 - MinusLogProbMetric: 30.3472 - val_loss: 30.5708 - val_MinusLogProbMetric: 30.5708 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 164/1000
2023-10-26 00:07:00.706 
Epoch 164/1000 
	 loss: 30.7129, MinusLogProbMetric: 30.7129, val_loss: 31.0828, val_MinusLogProbMetric: 31.0828

Epoch 164: val_loss did not improve from 30.06855
196/196 - 35s - loss: 30.7129 - MinusLogProbMetric: 30.7129 - val_loss: 31.0828 - val_MinusLogProbMetric: 31.0828 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 165/1000
2023-10-26 00:07:36.342 
Epoch 165/1000 
	 loss: 30.5087, MinusLogProbMetric: 30.5087, val_loss: 30.6746, val_MinusLogProbMetric: 30.6746

Epoch 165: val_loss did not improve from 30.06855
196/196 - 36s - loss: 30.5087 - MinusLogProbMetric: 30.5087 - val_loss: 30.6746 - val_MinusLogProbMetric: 30.6746 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 166/1000
2023-10-26 00:08:11.632 
Epoch 166/1000 
	 loss: 30.7010, MinusLogProbMetric: 30.7010, val_loss: 32.3173, val_MinusLogProbMetric: 32.3173

Epoch 166: val_loss did not improve from 30.06855
196/196 - 35s - loss: 30.7010 - MinusLogProbMetric: 30.7010 - val_loss: 32.3173 - val_MinusLogProbMetric: 32.3173 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 167/1000
2023-10-26 00:08:47.417 
Epoch 167/1000 
	 loss: 30.5188, MinusLogProbMetric: 30.5188, val_loss: 31.1465, val_MinusLogProbMetric: 31.1465

Epoch 167: val_loss did not improve from 30.06855
196/196 - 36s - loss: 30.5188 - MinusLogProbMetric: 30.5188 - val_loss: 31.1465 - val_MinusLogProbMetric: 31.1465 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 168/1000
2023-10-26 00:09:22.722 
Epoch 168/1000 
	 loss: 30.3782, MinusLogProbMetric: 30.3782, val_loss: 30.0032, val_MinusLogProbMetric: 30.0032

Epoch 168: val_loss improved from 30.06855 to 30.00318, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_369/weights/best_weights.h5
196/196 - 36s - loss: 30.3782 - MinusLogProbMetric: 30.3782 - val_loss: 30.0032 - val_MinusLogProbMetric: 30.0032 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 169/1000
2023-10-26 00:09:58.564 
Epoch 169/1000 
	 loss: 30.6456, MinusLogProbMetric: 30.6456, val_loss: 30.1409, val_MinusLogProbMetric: 30.1409

Epoch 169: val_loss did not improve from 30.00318
196/196 - 35s - loss: 30.6456 - MinusLogProbMetric: 30.6456 - val_loss: 30.1409 - val_MinusLogProbMetric: 30.1409 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 170/1000
2023-10-26 00:10:34.160 
Epoch 170/1000 
	 loss: 30.3330, MinusLogProbMetric: 30.3330, val_loss: 31.2180, val_MinusLogProbMetric: 31.2180

Epoch 170: val_loss did not improve from 30.00318
196/196 - 36s - loss: 30.3330 - MinusLogProbMetric: 30.3330 - val_loss: 31.2180 - val_MinusLogProbMetric: 31.2180 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 171/1000
2023-10-26 00:11:09.278 
Epoch 171/1000 
	 loss: 30.3499, MinusLogProbMetric: 30.3499, val_loss: 31.1785, val_MinusLogProbMetric: 31.1785

Epoch 171: val_loss did not improve from 30.00318
196/196 - 35s - loss: 30.3499 - MinusLogProbMetric: 30.3499 - val_loss: 31.1785 - val_MinusLogProbMetric: 31.1785 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 172/1000
2023-10-26 00:11:44.554 
Epoch 172/1000 
	 loss: 30.4248, MinusLogProbMetric: 30.4248, val_loss: 30.6438, val_MinusLogProbMetric: 30.6438

Epoch 172: val_loss did not improve from 30.00318
196/196 - 35s - loss: 30.4248 - MinusLogProbMetric: 30.4248 - val_loss: 30.6438 - val_MinusLogProbMetric: 30.6438 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 173/1000
2023-10-26 00:12:19.753 
Epoch 173/1000 
	 loss: 30.3847, MinusLogProbMetric: 30.3847, val_loss: 29.9418, val_MinusLogProbMetric: 29.9418

Epoch 173: val_loss improved from 30.00318 to 29.94184, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_369/weights/best_weights.h5
196/196 - 36s - loss: 30.3847 - MinusLogProbMetric: 30.3847 - val_loss: 29.9418 - val_MinusLogProbMetric: 29.9418 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 174/1000
2023-10-26 00:12:56.029 
Epoch 174/1000 
	 loss: 30.2227, MinusLogProbMetric: 30.2227, val_loss: 30.7275, val_MinusLogProbMetric: 30.7275

Epoch 174: val_loss did not improve from 29.94184
196/196 - 36s - loss: 30.2227 - MinusLogProbMetric: 30.2227 - val_loss: 30.7275 - val_MinusLogProbMetric: 30.7275 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 175/1000
2023-10-26 00:13:31.468 
Epoch 175/1000 
	 loss: 30.3789, MinusLogProbMetric: 30.3789, val_loss: 32.5047, val_MinusLogProbMetric: 32.5047

Epoch 175: val_loss did not improve from 29.94184
196/196 - 35s - loss: 30.3789 - MinusLogProbMetric: 30.3789 - val_loss: 32.5047 - val_MinusLogProbMetric: 32.5047 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 176/1000
2023-10-26 00:14:06.574 
Epoch 176/1000 
	 loss: 30.4372, MinusLogProbMetric: 30.4372, val_loss: 30.1791, val_MinusLogProbMetric: 30.1791

Epoch 176: val_loss did not improve from 29.94184
196/196 - 35s - loss: 30.4372 - MinusLogProbMetric: 30.4372 - val_loss: 30.1791 - val_MinusLogProbMetric: 30.1791 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 177/1000
2023-10-26 00:14:41.901 
Epoch 177/1000 
	 loss: 31.1000, MinusLogProbMetric: 31.1000, val_loss: 31.2882, val_MinusLogProbMetric: 31.2882

Epoch 177: val_loss did not improve from 29.94184
196/196 - 35s - loss: 31.1000 - MinusLogProbMetric: 31.1000 - val_loss: 31.2882 - val_MinusLogProbMetric: 31.2882 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 178/1000
2023-10-26 00:15:17.282 
Epoch 178/1000 
	 loss: 30.5928, MinusLogProbMetric: 30.5928, val_loss: 30.0599, val_MinusLogProbMetric: 30.0599

Epoch 178: val_loss did not improve from 29.94184
196/196 - 35s - loss: 30.5928 - MinusLogProbMetric: 30.5928 - val_loss: 30.0599 - val_MinusLogProbMetric: 30.0599 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 179/1000
2023-10-26 00:15:52.420 
Epoch 179/1000 
	 loss: 30.2512, MinusLogProbMetric: 30.2512, val_loss: 30.2774, val_MinusLogProbMetric: 30.2774

Epoch 179: val_loss did not improve from 29.94184
196/196 - 35s - loss: 30.2512 - MinusLogProbMetric: 30.2512 - val_loss: 30.2774 - val_MinusLogProbMetric: 30.2774 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 180/1000
2023-10-26 00:16:28.272 
Epoch 180/1000 
	 loss: 30.3806, MinusLogProbMetric: 30.3806, val_loss: 30.6220, val_MinusLogProbMetric: 30.6220

Epoch 180: val_loss did not improve from 29.94184
196/196 - 36s - loss: 30.3806 - MinusLogProbMetric: 30.3806 - val_loss: 30.6220 - val_MinusLogProbMetric: 30.6220 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 181/1000
2023-10-26 00:17:03.520 
Epoch 181/1000 
	 loss: 30.2532, MinusLogProbMetric: 30.2532, val_loss: 30.3384, val_MinusLogProbMetric: 30.3384

Epoch 181: val_loss did not improve from 29.94184
196/196 - 35s - loss: 30.2532 - MinusLogProbMetric: 30.2532 - val_loss: 30.3384 - val_MinusLogProbMetric: 30.3384 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 182/1000
2023-10-26 00:17:39.001 
Epoch 182/1000 
	 loss: 30.4882, MinusLogProbMetric: 30.4882, val_loss: 30.3481, val_MinusLogProbMetric: 30.3481

Epoch 182: val_loss did not improve from 29.94184
196/196 - 35s - loss: 30.4882 - MinusLogProbMetric: 30.4882 - val_loss: 30.3481 - val_MinusLogProbMetric: 30.3481 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 183/1000
2023-10-26 00:18:14.378 
Epoch 183/1000 
	 loss: 30.1944, MinusLogProbMetric: 30.1944, val_loss: 33.7020, val_MinusLogProbMetric: 33.7020

Epoch 183: val_loss did not improve from 29.94184
196/196 - 35s - loss: 30.1944 - MinusLogProbMetric: 30.1944 - val_loss: 33.7020 - val_MinusLogProbMetric: 33.7020 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 184/1000
2023-10-26 00:18:49.817 
Epoch 184/1000 
	 loss: 30.6157, MinusLogProbMetric: 30.6157, val_loss: 31.3384, val_MinusLogProbMetric: 31.3384

Epoch 184: val_loss did not improve from 29.94184
196/196 - 35s - loss: 30.6157 - MinusLogProbMetric: 30.6157 - val_loss: 31.3384 - val_MinusLogProbMetric: 31.3384 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 185/1000
2023-10-26 00:19:25.177 
Epoch 185/1000 
	 loss: 30.1577, MinusLogProbMetric: 30.1577, val_loss: 30.6925, val_MinusLogProbMetric: 30.6925

Epoch 185: val_loss did not improve from 29.94184
196/196 - 35s - loss: 30.1577 - MinusLogProbMetric: 30.1577 - val_loss: 30.6925 - val_MinusLogProbMetric: 30.6925 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 186/1000
2023-10-26 00:20:00.930 
Epoch 186/1000 
	 loss: 30.2041, MinusLogProbMetric: 30.2041, val_loss: 29.9433, val_MinusLogProbMetric: 29.9433

Epoch 186: val_loss did not improve from 29.94184
196/196 - 36s - loss: 30.2041 - MinusLogProbMetric: 30.2041 - val_loss: 29.9433 - val_MinusLogProbMetric: 29.9433 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 187/1000
2023-10-26 00:20:36.301 
Epoch 187/1000 
	 loss: 30.3398, MinusLogProbMetric: 30.3398, val_loss: 31.4868, val_MinusLogProbMetric: 31.4868

Epoch 187: val_loss did not improve from 29.94184
196/196 - 35s - loss: 30.3398 - MinusLogProbMetric: 30.3398 - val_loss: 31.4868 - val_MinusLogProbMetric: 31.4868 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 188/1000
2023-10-26 00:21:11.376 
Epoch 188/1000 
	 loss: 30.2800, MinusLogProbMetric: 30.2800, val_loss: 30.4507, val_MinusLogProbMetric: 30.4507

Epoch 188: val_loss did not improve from 29.94184
196/196 - 35s - loss: 30.2800 - MinusLogProbMetric: 30.2800 - val_loss: 30.4507 - val_MinusLogProbMetric: 30.4507 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 189/1000
2023-10-26 00:21:46.750 
Epoch 189/1000 
	 loss: 30.2404, MinusLogProbMetric: 30.2404, val_loss: 30.2678, val_MinusLogProbMetric: 30.2678

Epoch 189: val_loss did not improve from 29.94184
196/196 - 35s - loss: 30.2404 - MinusLogProbMetric: 30.2404 - val_loss: 30.2678 - val_MinusLogProbMetric: 30.2678 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 190/1000
2023-10-26 00:22:22.268 
Epoch 190/1000 
	 loss: 30.3085, MinusLogProbMetric: 30.3085, val_loss: 30.2950, val_MinusLogProbMetric: 30.2950

Epoch 190: val_loss did not improve from 29.94184
196/196 - 36s - loss: 30.3085 - MinusLogProbMetric: 30.3085 - val_loss: 30.2950 - val_MinusLogProbMetric: 30.2950 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 191/1000
2023-10-26 00:22:57.713 
Epoch 191/1000 
	 loss: 29.9913, MinusLogProbMetric: 29.9913, val_loss: 30.2396, val_MinusLogProbMetric: 30.2396

Epoch 191: val_loss did not improve from 29.94184
196/196 - 35s - loss: 29.9913 - MinusLogProbMetric: 29.9913 - val_loss: 30.2396 - val_MinusLogProbMetric: 30.2396 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 192/1000
2023-10-26 00:23:33.071 
Epoch 192/1000 
	 loss: 30.1186, MinusLogProbMetric: 30.1186, val_loss: 29.5716, val_MinusLogProbMetric: 29.5716

Epoch 192: val_loss improved from 29.94184 to 29.57155, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_369/weights/best_weights.h5
196/196 - 36s - loss: 30.1186 - MinusLogProbMetric: 30.1186 - val_loss: 29.5716 - val_MinusLogProbMetric: 29.5716 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 193/1000
2023-10-26 00:24:09.064 
Epoch 193/1000 
	 loss: 30.2489, MinusLogProbMetric: 30.2489, val_loss: 29.9798, val_MinusLogProbMetric: 29.9798

Epoch 193: val_loss did not improve from 29.57155
196/196 - 35s - loss: 30.2489 - MinusLogProbMetric: 30.2489 - val_loss: 29.9798 - val_MinusLogProbMetric: 29.9798 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 194/1000
2023-10-26 00:24:44.494 
Epoch 194/1000 
	 loss: 30.0527, MinusLogProbMetric: 30.0527, val_loss: 30.8691, val_MinusLogProbMetric: 30.8691

Epoch 194: val_loss did not improve from 29.57155
196/196 - 35s - loss: 30.0527 - MinusLogProbMetric: 30.0527 - val_loss: 30.8691 - val_MinusLogProbMetric: 30.8691 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 195/1000
2023-10-26 00:25:19.748 
Epoch 195/1000 
	 loss: 30.5102, MinusLogProbMetric: 30.5102, val_loss: 29.8173, val_MinusLogProbMetric: 29.8173

Epoch 195: val_loss did not improve from 29.57155
196/196 - 35s - loss: 30.5102 - MinusLogProbMetric: 30.5102 - val_loss: 29.8173 - val_MinusLogProbMetric: 29.8173 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 196/1000
2023-10-26 00:25:55.281 
Epoch 196/1000 
	 loss: 30.2744, MinusLogProbMetric: 30.2744, val_loss: 31.6718, val_MinusLogProbMetric: 31.6718

Epoch 196: val_loss did not improve from 29.57155
196/196 - 36s - loss: 30.2744 - MinusLogProbMetric: 30.2744 - val_loss: 31.6718 - val_MinusLogProbMetric: 31.6718 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 197/1000
2023-10-26 00:26:30.415 
Epoch 197/1000 
	 loss: 30.1896, MinusLogProbMetric: 30.1896, val_loss: 30.3172, val_MinusLogProbMetric: 30.3172

Epoch 197: val_loss did not improve from 29.57155
196/196 - 35s - loss: 30.1896 - MinusLogProbMetric: 30.1896 - val_loss: 30.3172 - val_MinusLogProbMetric: 30.3172 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 198/1000
2023-10-26 00:27:05.408 
Epoch 198/1000 
	 loss: 30.1063, MinusLogProbMetric: 30.1063, val_loss: 30.3616, val_MinusLogProbMetric: 30.3616

Epoch 198: val_loss did not improve from 29.57155
196/196 - 35s - loss: 30.1063 - MinusLogProbMetric: 30.1063 - val_loss: 30.3616 - val_MinusLogProbMetric: 30.3616 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 199/1000
2023-10-26 00:27:40.791 
Epoch 199/1000 
	 loss: 30.3378, MinusLogProbMetric: 30.3378, val_loss: 31.1605, val_MinusLogProbMetric: 31.1605

Epoch 199: val_loss did not improve from 29.57155
196/196 - 35s - loss: 30.3378 - MinusLogProbMetric: 30.3378 - val_loss: 31.1605 - val_MinusLogProbMetric: 31.1605 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 200/1000
2023-10-26 00:28:16.178 
Epoch 200/1000 
	 loss: 30.0669, MinusLogProbMetric: 30.0669, val_loss: 29.7854, val_MinusLogProbMetric: 29.7854

Epoch 200: val_loss did not improve from 29.57155
196/196 - 35s - loss: 30.0669 - MinusLogProbMetric: 30.0669 - val_loss: 29.7854 - val_MinusLogProbMetric: 29.7854 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 201/1000
2023-10-26 00:28:51.943 
Epoch 201/1000 
	 loss: 29.9033, MinusLogProbMetric: 29.9033, val_loss: 29.6230, val_MinusLogProbMetric: 29.6230

Epoch 201: val_loss did not improve from 29.57155
196/196 - 36s - loss: 29.9033 - MinusLogProbMetric: 29.9033 - val_loss: 29.6230 - val_MinusLogProbMetric: 29.6230 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 202/1000
2023-10-26 00:29:27.375 
Epoch 202/1000 
	 loss: 30.0095, MinusLogProbMetric: 30.0095, val_loss: 30.8387, val_MinusLogProbMetric: 30.8387

Epoch 202: val_loss did not improve from 29.57155
196/196 - 35s - loss: 30.0095 - MinusLogProbMetric: 30.0095 - val_loss: 30.8387 - val_MinusLogProbMetric: 30.8387 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 203/1000
2023-10-26 00:30:02.850 
Epoch 203/1000 
	 loss: 30.1565, MinusLogProbMetric: 30.1565, val_loss: 29.6758, val_MinusLogProbMetric: 29.6758

Epoch 203: val_loss did not improve from 29.57155
196/196 - 35s - loss: 30.1565 - MinusLogProbMetric: 30.1565 - val_loss: 29.6758 - val_MinusLogProbMetric: 29.6758 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 204/1000
2023-10-26 00:30:38.242 
Epoch 204/1000 
	 loss: 30.2423, MinusLogProbMetric: 30.2423, val_loss: 30.1446, val_MinusLogProbMetric: 30.1446

Epoch 204: val_loss did not improve from 29.57155
196/196 - 35s - loss: 30.2423 - MinusLogProbMetric: 30.2423 - val_loss: 30.1446 - val_MinusLogProbMetric: 30.1446 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 205/1000
2023-10-26 00:31:13.713 
Epoch 205/1000 
	 loss: 30.1847, MinusLogProbMetric: 30.1847, val_loss: 29.6784, val_MinusLogProbMetric: 29.6784

Epoch 205: val_loss did not improve from 29.57155
196/196 - 35s - loss: 30.1847 - MinusLogProbMetric: 30.1847 - val_loss: 29.6784 - val_MinusLogProbMetric: 29.6784 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 206/1000
2023-10-26 00:31:49.313 
Epoch 206/1000 
	 loss: 30.0776, MinusLogProbMetric: 30.0776, val_loss: 32.0167, val_MinusLogProbMetric: 32.0167

Epoch 206: val_loss did not improve from 29.57155
196/196 - 36s - loss: 30.0776 - MinusLogProbMetric: 30.0776 - val_loss: 32.0167 - val_MinusLogProbMetric: 32.0167 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 207/1000
2023-10-26 00:32:24.760 
Epoch 207/1000 
	 loss: 30.1101, MinusLogProbMetric: 30.1101, val_loss: 30.7700, val_MinusLogProbMetric: 30.7700

Epoch 207: val_loss did not improve from 29.57155
196/196 - 35s - loss: 30.1101 - MinusLogProbMetric: 30.1101 - val_loss: 30.7700 - val_MinusLogProbMetric: 30.7700 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 208/1000
2023-10-26 00:33:00.493 
Epoch 208/1000 
	 loss: 30.0667, MinusLogProbMetric: 30.0667, val_loss: 31.9061, val_MinusLogProbMetric: 31.9061

Epoch 208: val_loss did not improve from 29.57155
196/196 - 36s - loss: 30.0667 - MinusLogProbMetric: 30.0667 - val_loss: 31.9061 - val_MinusLogProbMetric: 31.9061 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 209/1000
2023-10-26 00:33:36.145 
Epoch 209/1000 
	 loss: 29.9464, MinusLogProbMetric: 29.9464, val_loss: 31.1309, val_MinusLogProbMetric: 31.1309

Epoch 209: val_loss did not improve from 29.57155
196/196 - 36s - loss: 29.9464 - MinusLogProbMetric: 29.9464 - val_loss: 31.1309 - val_MinusLogProbMetric: 31.1309 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 210/1000
2023-10-26 00:34:11.788 
Epoch 210/1000 
	 loss: 30.5691, MinusLogProbMetric: 30.5691, val_loss: 30.9600, val_MinusLogProbMetric: 30.9600

Epoch 210: val_loss did not improve from 29.57155
196/196 - 36s - loss: 30.5691 - MinusLogProbMetric: 30.5691 - val_loss: 30.9600 - val_MinusLogProbMetric: 30.9600 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 211/1000
2023-10-26 00:34:47.546 
Epoch 211/1000 
	 loss: 30.0356, MinusLogProbMetric: 30.0356, val_loss: 30.0027, val_MinusLogProbMetric: 30.0027

Epoch 211: val_loss did not improve from 29.57155
196/196 - 36s - loss: 30.0356 - MinusLogProbMetric: 30.0356 - val_loss: 30.0027 - val_MinusLogProbMetric: 30.0027 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 212/1000
2023-10-26 00:35:23.297 
Epoch 212/1000 
	 loss: 30.1800, MinusLogProbMetric: 30.1800, val_loss: 30.3012, val_MinusLogProbMetric: 30.3012

Epoch 212: val_loss did not improve from 29.57155
196/196 - 36s - loss: 30.1800 - MinusLogProbMetric: 30.1800 - val_loss: 30.3012 - val_MinusLogProbMetric: 30.3012 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 213/1000
2023-10-26 00:35:59.047 
Epoch 213/1000 
	 loss: 30.1059, MinusLogProbMetric: 30.1059, val_loss: 29.7524, val_MinusLogProbMetric: 29.7524

Epoch 213: val_loss did not improve from 29.57155
196/196 - 36s - loss: 30.1059 - MinusLogProbMetric: 30.1059 - val_loss: 29.7524 - val_MinusLogProbMetric: 29.7524 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 214/1000
2023-10-26 00:36:34.240 
Epoch 214/1000 
	 loss: 30.0546, MinusLogProbMetric: 30.0546, val_loss: 32.9407, val_MinusLogProbMetric: 32.9407

Epoch 214: val_loss did not improve from 29.57155
196/196 - 35s - loss: 30.0546 - MinusLogProbMetric: 30.0546 - val_loss: 32.9407 - val_MinusLogProbMetric: 32.9407 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 215/1000
2023-10-26 00:37:09.720 
Epoch 215/1000 
	 loss: 30.3034, MinusLogProbMetric: 30.3034, val_loss: 30.0229, val_MinusLogProbMetric: 30.0229

Epoch 215: val_loss did not improve from 29.57155
196/196 - 35s - loss: 30.3034 - MinusLogProbMetric: 30.3034 - val_loss: 30.0229 - val_MinusLogProbMetric: 30.0229 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 216/1000
2023-10-26 00:37:45.024 
Epoch 216/1000 
	 loss: 29.8671, MinusLogProbMetric: 29.8671, val_loss: 31.4552, val_MinusLogProbMetric: 31.4552

Epoch 216: val_loss did not improve from 29.57155
196/196 - 35s - loss: 29.8671 - MinusLogProbMetric: 29.8671 - val_loss: 31.4552 - val_MinusLogProbMetric: 31.4552 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 217/1000
2023-10-26 00:38:20.480 
Epoch 217/1000 
	 loss: 30.0717, MinusLogProbMetric: 30.0717, val_loss: 29.7914, val_MinusLogProbMetric: 29.7914

Epoch 217: val_loss did not improve from 29.57155
196/196 - 35s - loss: 30.0717 - MinusLogProbMetric: 30.0717 - val_loss: 29.7914 - val_MinusLogProbMetric: 29.7914 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 218/1000
2023-10-26 00:38:56.101 
Epoch 218/1000 
	 loss: 29.9817, MinusLogProbMetric: 29.9817, val_loss: 29.7195, val_MinusLogProbMetric: 29.7195

Epoch 218: val_loss did not improve from 29.57155
196/196 - 36s - loss: 29.9817 - MinusLogProbMetric: 29.9817 - val_loss: 29.7195 - val_MinusLogProbMetric: 29.7195 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 219/1000
2023-10-26 00:39:31.616 
Epoch 219/1000 
	 loss: 29.9685, MinusLogProbMetric: 29.9685, val_loss: 34.5789, val_MinusLogProbMetric: 34.5789

Epoch 219: val_loss did not improve from 29.57155
196/196 - 36s - loss: 29.9685 - MinusLogProbMetric: 29.9685 - val_loss: 34.5789 - val_MinusLogProbMetric: 34.5789 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 220/1000
2023-10-26 00:40:07.233 
Epoch 220/1000 
	 loss: 30.0169, MinusLogProbMetric: 30.0169, val_loss: 30.2876, val_MinusLogProbMetric: 30.2876

Epoch 220: val_loss did not improve from 29.57155
196/196 - 36s - loss: 30.0169 - MinusLogProbMetric: 30.0169 - val_loss: 30.2876 - val_MinusLogProbMetric: 30.2876 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 221/1000
2023-10-26 00:40:42.889 
Epoch 221/1000 
	 loss: 30.0019, MinusLogProbMetric: 30.0019, val_loss: 30.4553, val_MinusLogProbMetric: 30.4553

Epoch 221: val_loss did not improve from 29.57155
196/196 - 36s - loss: 30.0019 - MinusLogProbMetric: 30.0019 - val_loss: 30.4553 - val_MinusLogProbMetric: 30.4553 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 222/1000
2023-10-26 00:41:18.386 
Epoch 222/1000 
	 loss: 29.8925, MinusLogProbMetric: 29.8925, val_loss: 30.6816, val_MinusLogProbMetric: 30.6816

Epoch 222: val_loss did not improve from 29.57155
196/196 - 35s - loss: 29.8925 - MinusLogProbMetric: 29.8925 - val_loss: 30.6816 - val_MinusLogProbMetric: 30.6816 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 223/1000
2023-10-26 00:41:54.059 
Epoch 223/1000 
	 loss: 29.8619, MinusLogProbMetric: 29.8619, val_loss: 29.9397, val_MinusLogProbMetric: 29.9397

Epoch 223: val_loss did not improve from 29.57155
196/196 - 36s - loss: 29.8619 - MinusLogProbMetric: 29.8619 - val_loss: 29.9397 - val_MinusLogProbMetric: 29.9397 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 224/1000
2023-10-26 00:42:29.512 
Epoch 224/1000 
	 loss: 30.0142, MinusLogProbMetric: 30.0142, val_loss: 29.8818, val_MinusLogProbMetric: 29.8818

Epoch 224: val_loss did not improve from 29.57155
196/196 - 35s - loss: 30.0142 - MinusLogProbMetric: 30.0142 - val_loss: 29.8818 - val_MinusLogProbMetric: 29.8818 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 225/1000
2023-10-26 00:43:04.661 
Epoch 225/1000 
	 loss: 29.9810, MinusLogProbMetric: 29.9810, val_loss: 29.8232, val_MinusLogProbMetric: 29.8232

Epoch 225: val_loss did not improve from 29.57155
196/196 - 35s - loss: 29.9810 - MinusLogProbMetric: 29.9810 - val_loss: 29.8232 - val_MinusLogProbMetric: 29.8232 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 226/1000
2023-10-26 00:43:40.229 
Epoch 226/1000 
	 loss: 29.9025, MinusLogProbMetric: 29.9025, val_loss: 31.4284, val_MinusLogProbMetric: 31.4284

Epoch 226: val_loss did not improve from 29.57155
196/196 - 36s - loss: 29.9025 - MinusLogProbMetric: 29.9025 - val_loss: 31.4284 - val_MinusLogProbMetric: 31.4284 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 227/1000
2023-10-26 00:44:15.685 
Epoch 227/1000 
	 loss: 30.0354, MinusLogProbMetric: 30.0354, val_loss: 30.9024, val_MinusLogProbMetric: 30.9024

Epoch 227: val_loss did not improve from 29.57155
196/196 - 35s - loss: 30.0354 - MinusLogProbMetric: 30.0354 - val_loss: 30.9024 - val_MinusLogProbMetric: 30.9024 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 228/1000
2023-10-26 00:44:51.212 
Epoch 228/1000 
	 loss: 29.9223, MinusLogProbMetric: 29.9223, val_loss: 29.4566, val_MinusLogProbMetric: 29.4566

Epoch 228: val_loss improved from 29.57155 to 29.45663, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_369/weights/best_weights.h5
196/196 - 36s - loss: 29.9223 - MinusLogProbMetric: 29.9223 - val_loss: 29.4566 - val_MinusLogProbMetric: 29.4566 - lr: 0.0010 - 36s/epoch - 185ms/step
Epoch 229/1000
2023-10-26 00:45:27.232 
Epoch 229/1000 
	 loss: 29.9775, MinusLogProbMetric: 29.9775, val_loss: 30.4575, val_MinusLogProbMetric: 30.4575

Epoch 229: val_loss did not improve from 29.45663
196/196 - 35s - loss: 29.9775 - MinusLogProbMetric: 29.9775 - val_loss: 30.4575 - val_MinusLogProbMetric: 30.4575 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 230/1000
2023-10-26 00:46:02.407 
Epoch 230/1000 
	 loss: 30.1668, MinusLogProbMetric: 30.1668, val_loss: 30.0055, val_MinusLogProbMetric: 30.0055

Epoch 230: val_loss did not improve from 29.45663
196/196 - 35s - loss: 30.1668 - MinusLogProbMetric: 30.1668 - val_loss: 30.0055 - val_MinusLogProbMetric: 30.0055 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 231/1000
2023-10-26 00:46:37.678 
Epoch 231/1000 
	 loss: 29.9245, MinusLogProbMetric: 29.9245, val_loss: 30.1395, val_MinusLogProbMetric: 30.1395

Epoch 231: val_loss did not improve from 29.45663
196/196 - 35s - loss: 29.9245 - MinusLogProbMetric: 29.9245 - val_loss: 30.1395 - val_MinusLogProbMetric: 30.1395 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 232/1000
2023-10-26 00:47:13.346 
Epoch 232/1000 
	 loss: 29.8409, MinusLogProbMetric: 29.8409, val_loss: 30.0348, val_MinusLogProbMetric: 30.0348

Epoch 232: val_loss did not improve from 29.45663
196/196 - 36s - loss: 29.8409 - MinusLogProbMetric: 29.8409 - val_loss: 30.0348 - val_MinusLogProbMetric: 30.0348 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 233/1000
2023-10-26 00:47:48.688 
Epoch 233/1000 
	 loss: 29.7928, MinusLogProbMetric: 29.7928, val_loss: 29.4821, val_MinusLogProbMetric: 29.4821

Epoch 233: val_loss did not improve from 29.45663
196/196 - 35s - loss: 29.7928 - MinusLogProbMetric: 29.7928 - val_loss: 29.4821 - val_MinusLogProbMetric: 29.4821 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 234/1000
2023-10-26 00:48:24.012 
Epoch 234/1000 
	 loss: 29.8332, MinusLogProbMetric: 29.8332, val_loss: 29.4460, val_MinusLogProbMetric: 29.4460

Epoch 234: val_loss improved from 29.45663 to 29.44598, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_369/weights/best_weights.h5
196/196 - 36s - loss: 29.8332 - MinusLogProbMetric: 29.8332 - val_loss: 29.4460 - val_MinusLogProbMetric: 29.4460 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 235/1000
2023-10-26 00:48:59.966 
Epoch 235/1000 
	 loss: 29.9717, MinusLogProbMetric: 29.9717, val_loss: 30.3216, val_MinusLogProbMetric: 30.3216

Epoch 235: val_loss did not improve from 29.44598
196/196 - 35s - loss: 29.9717 - MinusLogProbMetric: 29.9717 - val_loss: 30.3216 - val_MinusLogProbMetric: 30.3216 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 236/1000
2023-10-26 00:49:35.715 
Epoch 236/1000 
	 loss: 29.7344, MinusLogProbMetric: 29.7344, val_loss: 29.5175, val_MinusLogProbMetric: 29.5175

Epoch 236: val_loss did not improve from 29.44598
196/196 - 36s - loss: 29.7344 - MinusLogProbMetric: 29.7344 - val_loss: 29.5175 - val_MinusLogProbMetric: 29.5175 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 237/1000
2023-10-26 00:50:11.381 
Epoch 237/1000 
	 loss: 29.7152, MinusLogProbMetric: 29.7152, val_loss: 30.0302, val_MinusLogProbMetric: 30.0302

Epoch 237: val_loss did not improve from 29.44598
196/196 - 36s - loss: 29.7152 - MinusLogProbMetric: 29.7152 - val_loss: 30.0302 - val_MinusLogProbMetric: 30.0302 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 238/1000
2023-10-26 00:50:46.887 
Epoch 238/1000 
	 loss: 30.0119, MinusLogProbMetric: 30.0119, val_loss: 29.5264, val_MinusLogProbMetric: 29.5264

Epoch 238: val_loss did not improve from 29.44598
196/196 - 36s - loss: 30.0119 - MinusLogProbMetric: 30.0119 - val_loss: 29.5264 - val_MinusLogProbMetric: 29.5264 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 239/1000
2023-10-26 00:51:22.479 
Epoch 239/1000 
	 loss: 29.8512, MinusLogProbMetric: 29.8512, val_loss: 29.4109, val_MinusLogProbMetric: 29.4109

Epoch 239: val_loss improved from 29.44598 to 29.41089, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_369/weights/best_weights.h5
196/196 - 36s - loss: 29.8512 - MinusLogProbMetric: 29.8512 - val_loss: 29.4109 - val_MinusLogProbMetric: 29.4109 - lr: 0.0010 - 36s/epoch - 185ms/step
Epoch 240/1000
2023-10-26 00:51:57.874 
Epoch 240/1000 
	 loss: 29.8515, MinusLogProbMetric: 29.8515, val_loss: 29.8757, val_MinusLogProbMetric: 29.8757

Epoch 240: val_loss did not improve from 29.41089
196/196 - 35s - loss: 29.8515 - MinusLogProbMetric: 29.8515 - val_loss: 29.8757 - val_MinusLogProbMetric: 29.8757 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 241/1000
2023-10-26 00:52:28.143 
Epoch 241/1000 
	 loss: 29.7782, MinusLogProbMetric: 29.7782, val_loss: 29.9436, val_MinusLogProbMetric: 29.9436

Epoch 241: val_loss did not improve from 29.41089
196/196 - 30s - loss: 29.7782 - MinusLogProbMetric: 29.7782 - val_loss: 29.9436 - val_MinusLogProbMetric: 29.9436 - lr: 0.0010 - 30s/epoch - 154ms/step
Epoch 242/1000
2023-10-26 00:53:02.911 
Epoch 242/1000 
	 loss: 29.9923, MinusLogProbMetric: 29.9923, val_loss: 30.7752, val_MinusLogProbMetric: 30.7752

Epoch 242: val_loss did not improve from 29.41089
196/196 - 35s - loss: 29.9923 - MinusLogProbMetric: 29.9923 - val_loss: 30.7752 - val_MinusLogProbMetric: 30.7752 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 243/1000
2023-10-26 00:53:38.198 
Epoch 243/1000 
	 loss: 30.0066, MinusLogProbMetric: 30.0066, val_loss: 31.0204, val_MinusLogProbMetric: 31.0204

Epoch 243: val_loss did not improve from 29.41089
196/196 - 35s - loss: 30.0066 - MinusLogProbMetric: 30.0066 - val_loss: 31.0204 - val_MinusLogProbMetric: 31.0204 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 244/1000
2023-10-26 00:54:13.786 
Epoch 244/1000 
	 loss: 29.8603, MinusLogProbMetric: 29.8603, val_loss: 30.5711, val_MinusLogProbMetric: 30.5711

Epoch 244: val_loss did not improve from 29.41089
196/196 - 36s - loss: 29.8603 - MinusLogProbMetric: 29.8603 - val_loss: 30.5711 - val_MinusLogProbMetric: 30.5711 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 245/1000
2023-10-26 00:54:49.190 
Epoch 245/1000 
	 loss: 29.8056, MinusLogProbMetric: 29.8056, val_loss: 30.1399, val_MinusLogProbMetric: 30.1399

Epoch 245: val_loss did not improve from 29.41089
196/196 - 35s - loss: 29.8056 - MinusLogProbMetric: 29.8056 - val_loss: 30.1399 - val_MinusLogProbMetric: 30.1399 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 246/1000
2023-10-26 00:55:24.515 
Epoch 246/1000 
	 loss: 29.7718, MinusLogProbMetric: 29.7718, val_loss: 30.1795, val_MinusLogProbMetric: 30.1795

Epoch 246: val_loss did not improve from 29.41089
196/196 - 35s - loss: 29.7718 - MinusLogProbMetric: 29.7718 - val_loss: 30.1795 - val_MinusLogProbMetric: 30.1795 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 247/1000
2023-10-26 00:55:59.898 
Epoch 247/1000 
	 loss: 30.0865, MinusLogProbMetric: 30.0865, val_loss: 29.4795, val_MinusLogProbMetric: 29.4795

Epoch 247: val_loss did not improve from 29.41089
196/196 - 35s - loss: 30.0865 - MinusLogProbMetric: 30.0865 - val_loss: 29.4795 - val_MinusLogProbMetric: 29.4795 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 248/1000
2023-10-26 00:56:35.213 
Epoch 248/1000 
	 loss: 29.7106, MinusLogProbMetric: 29.7106, val_loss: 29.4008, val_MinusLogProbMetric: 29.4008

Epoch 248: val_loss improved from 29.41089 to 29.40081, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_369/weights/best_weights.h5
196/196 - 36s - loss: 29.7106 - MinusLogProbMetric: 29.7106 - val_loss: 29.4008 - val_MinusLogProbMetric: 29.4008 - lr: 0.0010 - 36s/epoch - 184ms/step
Epoch 249/1000
2023-10-26 00:57:11.484 
Epoch 249/1000 
	 loss: 29.8380, MinusLogProbMetric: 29.8380, val_loss: 29.5863, val_MinusLogProbMetric: 29.5863

Epoch 249: val_loss did not improve from 29.40081
196/196 - 36s - loss: 29.8380 - MinusLogProbMetric: 29.8380 - val_loss: 29.5863 - val_MinusLogProbMetric: 29.5863 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 250/1000
2023-10-26 00:57:46.895 
Epoch 250/1000 
	 loss: 29.9367, MinusLogProbMetric: 29.9367, val_loss: 29.5495, val_MinusLogProbMetric: 29.5495

Epoch 250: val_loss did not improve from 29.40081
196/196 - 35s - loss: 29.9367 - MinusLogProbMetric: 29.9367 - val_loss: 29.5495 - val_MinusLogProbMetric: 29.5495 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 251/1000
2023-10-26 00:58:22.268 
Epoch 251/1000 
	 loss: 29.9787, MinusLogProbMetric: 29.9787, val_loss: 29.5886, val_MinusLogProbMetric: 29.5886

Epoch 251: val_loss did not improve from 29.40081
196/196 - 35s - loss: 29.9787 - MinusLogProbMetric: 29.9787 - val_loss: 29.5886 - val_MinusLogProbMetric: 29.5886 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 252/1000
2023-10-26 00:58:57.858 
Epoch 252/1000 
	 loss: 29.9902, MinusLogProbMetric: 29.9902, val_loss: 29.7592, val_MinusLogProbMetric: 29.7592

Epoch 252: val_loss did not improve from 29.40081
196/196 - 36s - loss: 29.9902 - MinusLogProbMetric: 29.9902 - val_loss: 29.7592 - val_MinusLogProbMetric: 29.7592 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 253/1000
2023-10-26 00:59:33.413 
Epoch 253/1000 
	 loss: 29.7651, MinusLogProbMetric: 29.7651, val_loss: 29.8184, val_MinusLogProbMetric: 29.8184

Epoch 253: val_loss did not improve from 29.40081
196/196 - 36s - loss: 29.7651 - MinusLogProbMetric: 29.7651 - val_loss: 29.8184 - val_MinusLogProbMetric: 29.8184 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 254/1000
2023-10-26 01:00:08.761 
Epoch 254/1000 
	 loss: 29.7814, MinusLogProbMetric: 29.7814, val_loss: 29.4818, val_MinusLogProbMetric: 29.4818

Epoch 254: val_loss did not improve from 29.40081
196/196 - 35s - loss: 29.7814 - MinusLogProbMetric: 29.7814 - val_loss: 29.4818 - val_MinusLogProbMetric: 29.4818 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 255/1000
2023-10-26 01:00:44.270 
Epoch 255/1000 
	 loss: 29.6569, MinusLogProbMetric: 29.6569, val_loss: 31.3665, val_MinusLogProbMetric: 31.3665

Epoch 255: val_loss did not improve from 29.40081
196/196 - 36s - loss: 29.6569 - MinusLogProbMetric: 29.6569 - val_loss: 31.3665 - val_MinusLogProbMetric: 31.3665 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 256/1000
2023-10-26 01:01:19.344 
Epoch 256/1000 
	 loss: 29.8439, MinusLogProbMetric: 29.8439, val_loss: 29.5815, val_MinusLogProbMetric: 29.5815

Epoch 256: val_loss did not improve from 29.40081
196/196 - 35s - loss: 29.8439 - MinusLogProbMetric: 29.8439 - val_loss: 29.5815 - val_MinusLogProbMetric: 29.5815 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 257/1000
2023-10-26 01:01:54.771 
Epoch 257/1000 
	 loss: 29.7047, MinusLogProbMetric: 29.7047, val_loss: 29.5295, val_MinusLogProbMetric: 29.5295

Epoch 257: val_loss did not improve from 29.40081
196/196 - 35s - loss: 29.7047 - MinusLogProbMetric: 29.7047 - val_loss: 29.5295 - val_MinusLogProbMetric: 29.5295 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 258/1000
2023-10-26 01:02:30.219 
Epoch 258/1000 
	 loss: 29.7826, MinusLogProbMetric: 29.7826, val_loss: 30.3164, val_MinusLogProbMetric: 30.3164

Epoch 258: val_loss did not improve from 29.40081
196/196 - 35s - loss: 29.7826 - MinusLogProbMetric: 29.7826 - val_loss: 30.3164 - val_MinusLogProbMetric: 30.3164 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 259/1000
2023-10-26 01:03:05.878 
Epoch 259/1000 
	 loss: 29.7968, MinusLogProbMetric: 29.7968, val_loss: 29.8983, val_MinusLogProbMetric: 29.8983

Epoch 259: val_loss did not improve from 29.40081
196/196 - 36s - loss: 29.7968 - MinusLogProbMetric: 29.7968 - val_loss: 29.8983 - val_MinusLogProbMetric: 29.8983 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 260/1000
2023-10-26 01:03:41.286 
Epoch 260/1000 
	 loss: 29.7352, MinusLogProbMetric: 29.7352, val_loss: 29.8493, val_MinusLogProbMetric: 29.8493

Epoch 260: val_loss did not improve from 29.40081
196/196 - 35s - loss: 29.7352 - MinusLogProbMetric: 29.7352 - val_loss: 29.8493 - val_MinusLogProbMetric: 29.8493 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 261/1000
2023-10-26 01:04:17.027 
Epoch 261/1000 
	 loss: 29.7710, MinusLogProbMetric: 29.7710, val_loss: 29.7635, val_MinusLogProbMetric: 29.7635

Epoch 261: val_loss did not improve from 29.40081
196/196 - 36s - loss: 29.7710 - MinusLogProbMetric: 29.7710 - val_loss: 29.7635 - val_MinusLogProbMetric: 29.7635 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 262/1000
2023-10-26 01:04:52.456 
Epoch 262/1000 
	 loss: 29.5705, MinusLogProbMetric: 29.5705, val_loss: 29.9414, val_MinusLogProbMetric: 29.9414

Epoch 262: val_loss did not improve from 29.40081
196/196 - 35s - loss: 29.5705 - MinusLogProbMetric: 29.5705 - val_loss: 29.9414 - val_MinusLogProbMetric: 29.9414 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 263/1000
2023-10-26 01:05:28.313 
Epoch 263/1000 
	 loss: 29.9790, MinusLogProbMetric: 29.9790, val_loss: 30.5179, val_MinusLogProbMetric: 30.5179

Epoch 263: val_loss did not improve from 29.40081
196/196 - 36s - loss: 29.9790 - MinusLogProbMetric: 29.9790 - val_loss: 30.5179 - val_MinusLogProbMetric: 30.5179 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 264/1000
2023-10-26 01:06:03.979 
Epoch 264/1000 
	 loss: 29.7059, MinusLogProbMetric: 29.7059, val_loss: 29.6178, val_MinusLogProbMetric: 29.6178

Epoch 264: val_loss did not improve from 29.40081
196/196 - 36s - loss: 29.7059 - MinusLogProbMetric: 29.7059 - val_loss: 29.6178 - val_MinusLogProbMetric: 29.6178 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 265/1000
2023-10-26 01:06:39.457 
Epoch 265/1000 
	 loss: 29.5839, MinusLogProbMetric: 29.5839, val_loss: 33.8295, val_MinusLogProbMetric: 33.8295

Epoch 265: val_loss did not improve from 29.40081
196/196 - 35s - loss: 29.5839 - MinusLogProbMetric: 29.5839 - val_loss: 33.8295 - val_MinusLogProbMetric: 33.8295 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 266/1000
2023-10-26 01:07:15.342 
Epoch 266/1000 
	 loss: 29.9942, MinusLogProbMetric: 29.9942, val_loss: 29.9627, val_MinusLogProbMetric: 29.9627

Epoch 266: val_loss did not improve from 29.40081
196/196 - 36s - loss: 29.9942 - MinusLogProbMetric: 29.9942 - val_loss: 29.9627 - val_MinusLogProbMetric: 29.9627 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 267/1000
2023-10-26 01:07:50.762 
Epoch 267/1000 
	 loss: 29.7444, MinusLogProbMetric: 29.7444, val_loss: 30.0376, val_MinusLogProbMetric: 30.0376

Epoch 267: val_loss did not improve from 29.40081
196/196 - 35s - loss: 29.7444 - MinusLogProbMetric: 29.7444 - val_loss: 30.0376 - val_MinusLogProbMetric: 30.0376 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 268/1000
2023-10-26 01:08:26.324 
Epoch 268/1000 
	 loss: 29.5356, MinusLogProbMetric: 29.5356, val_loss: 31.2120, val_MinusLogProbMetric: 31.2120

Epoch 268: val_loss did not improve from 29.40081
196/196 - 36s - loss: 29.5356 - MinusLogProbMetric: 29.5356 - val_loss: 31.2120 - val_MinusLogProbMetric: 31.2120 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 269/1000
2023-10-26 01:09:01.857 
Epoch 269/1000 
	 loss: 29.7678, MinusLogProbMetric: 29.7678, val_loss: 29.6767, val_MinusLogProbMetric: 29.6767

Epoch 269: val_loss did not improve from 29.40081
196/196 - 36s - loss: 29.7678 - MinusLogProbMetric: 29.7678 - val_loss: 29.6767 - val_MinusLogProbMetric: 29.6767 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 270/1000
2023-10-26 01:09:37.164 
Epoch 270/1000 
	 loss: 29.5038, MinusLogProbMetric: 29.5038, val_loss: 29.2487, val_MinusLogProbMetric: 29.2487

Epoch 270: val_loss improved from 29.40081 to 29.24874, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_369/weights/best_weights.h5
196/196 - 36s - loss: 29.5038 - MinusLogProbMetric: 29.5038 - val_loss: 29.2487 - val_MinusLogProbMetric: 29.2487 - lr: 0.0010 - 36s/epoch - 184ms/step
Epoch 271/1000
2023-10-26 01:10:13.455 
Epoch 271/1000 
	 loss: 29.9026, MinusLogProbMetric: 29.9026, val_loss: 29.9206, val_MinusLogProbMetric: 29.9206

Epoch 271: val_loss did not improve from 29.24874
196/196 - 36s - loss: 29.9026 - MinusLogProbMetric: 29.9026 - val_loss: 29.9206 - val_MinusLogProbMetric: 29.9206 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 272/1000
2023-10-26 01:10:48.883 
Epoch 272/1000 
	 loss: 29.8402, MinusLogProbMetric: 29.8402, val_loss: 31.4625, val_MinusLogProbMetric: 31.4625

Epoch 272: val_loss did not improve from 29.24874
196/196 - 35s - loss: 29.8402 - MinusLogProbMetric: 29.8402 - val_loss: 31.4625 - val_MinusLogProbMetric: 31.4625 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 273/1000
2023-10-26 01:11:24.390 
Epoch 273/1000 
	 loss: 29.7146, MinusLogProbMetric: 29.7146, val_loss: 30.0742, val_MinusLogProbMetric: 30.0742

Epoch 273: val_loss did not improve from 29.24874
196/196 - 36s - loss: 29.7146 - MinusLogProbMetric: 29.7146 - val_loss: 30.0742 - val_MinusLogProbMetric: 30.0742 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 274/1000
2023-10-26 01:12:00.023 
Epoch 274/1000 
	 loss: 29.6169, MinusLogProbMetric: 29.6169, val_loss: 30.1435, val_MinusLogProbMetric: 30.1435

Epoch 274: val_loss did not improve from 29.24874
196/196 - 36s - loss: 29.6169 - MinusLogProbMetric: 29.6169 - val_loss: 30.1435 - val_MinusLogProbMetric: 30.1435 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 275/1000
2023-10-26 01:12:35.672 
Epoch 275/1000 
	 loss: 29.9115, MinusLogProbMetric: 29.9115, val_loss: 29.4695, val_MinusLogProbMetric: 29.4695

Epoch 275: val_loss did not improve from 29.24874
196/196 - 36s - loss: 29.9115 - MinusLogProbMetric: 29.9115 - val_loss: 29.4695 - val_MinusLogProbMetric: 29.4695 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 276/1000
2023-10-26 01:13:10.876 
Epoch 276/1000 
	 loss: 29.7783, MinusLogProbMetric: 29.7783, val_loss: 35.9317, val_MinusLogProbMetric: 35.9317

Epoch 276: val_loss did not improve from 29.24874
196/196 - 35s - loss: 29.7783 - MinusLogProbMetric: 29.7783 - val_loss: 35.9317 - val_MinusLogProbMetric: 35.9317 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 277/1000
2023-10-26 01:13:46.493 
Epoch 277/1000 
	 loss: 29.8127, MinusLogProbMetric: 29.8127, val_loss: 29.3566, val_MinusLogProbMetric: 29.3566

Epoch 277: val_loss did not improve from 29.24874
196/196 - 36s - loss: 29.8127 - MinusLogProbMetric: 29.8127 - val_loss: 29.3566 - val_MinusLogProbMetric: 29.3566 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 278/1000
2023-10-26 01:14:22.100 
Epoch 278/1000 
	 loss: 29.8822, MinusLogProbMetric: 29.8822, val_loss: 31.4974, val_MinusLogProbMetric: 31.4974

Epoch 278: val_loss did not improve from 29.24874
196/196 - 36s - loss: 29.8822 - MinusLogProbMetric: 29.8822 - val_loss: 31.4974 - val_MinusLogProbMetric: 31.4974 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 279/1000
2023-10-26 01:14:57.522 
Epoch 279/1000 
	 loss: 29.6966, MinusLogProbMetric: 29.6966, val_loss: 31.4393, val_MinusLogProbMetric: 31.4393

Epoch 279: val_loss did not improve from 29.24874
196/196 - 35s - loss: 29.6966 - MinusLogProbMetric: 29.6966 - val_loss: 31.4393 - val_MinusLogProbMetric: 31.4393 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 280/1000
2023-10-26 01:15:33.045 
Epoch 280/1000 
	 loss: 29.7159, MinusLogProbMetric: 29.7159, val_loss: 31.2794, val_MinusLogProbMetric: 31.2794

Epoch 280: val_loss did not improve from 29.24874
196/196 - 36s - loss: 29.7159 - MinusLogProbMetric: 29.7159 - val_loss: 31.2794 - val_MinusLogProbMetric: 31.2794 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 281/1000
2023-10-26 01:16:08.240 
Epoch 281/1000 
	 loss: 29.6329, MinusLogProbMetric: 29.6329, val_loss: 30.3194, val_MinusLogProbMetric: 30.3194

Epoch 281: val_loss did not improve from 29.24874
196/196 - 35s - loss: 29.6329 - MinusLogProbMetric: 29.6329 - val_loss: 30.3194 - val_MinusLogProbMetric: 30.3194 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 282/1000
2023-10-26 01:16:43.813 
Epoch 282/1000 
	 loss: 29.7128, MinusLogProbMetric: 29.7128, val_loss: 29.6883, val_MinusLogProbMetric: 29.6883

Epoch 282: val_loss did not improve from 29.24874
196/196 - 36s - loss: 29.7128 - MinusLogProbMetric: 29.7128 - val_loss: 29.6883 - val_MinusLogProbMetric: 29.6883 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 283/1000
2023-10-26 01:17:19.172 
Epoch 283/1000 
	 loss: 29.5669, MinusLogProbMetric: 29.5669, val_loss: 29.8876, val_MinusLogProbMetric: 29.8876

Epoch 283: val_loss did not improve from 29.24874
196/196 - 35s - loss: 29.5669 - MinusLogProbMetric: 29.5669 - val_loss: 29.8876 - val_MinusLogProbMetric: 29.8876 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 284/1000
2023-10-26 01:17:54.819 
Epoch 284/1000 
	 loss: 29.6846, MinusLogProbMetric: 29.6846, val_loss: 30.1081, val_MinusLogProbMetric: 30.1081

Epoch 284: val_loss did not improve from 29.24874
196/196 - 36s - loss: 29.6846 - MinusLogProbMetric: 29.6846 - val_loss: 30.1081 - val_MinusLogProbMetric: 30.1081 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 285/1000
2023-10-26 01:18:30.442 
Epoch 285/1000 
	 loss: 29.8430, MinusLogProbMetric: 29.8430, val_loss: 29.9194, val_MinusLogProbMetric: 29.9194

Epoch 285: val_loss did not improve from 29.24874
196/196 - 36s - loss: 29.8430 - MinusLogProbMetric: 29.8430 - val_loss: 29.9194 - val_MinusLogProbMetric: 29.9194 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 286/1000
2023-10-26 01:19:05.814 
Epoch 286/1000 
	 loss: 29.7251, MinusLogProbMetric: 29.7251, val_loss: 29.7835, val_MinusLogProbMetric: 29.7835

Epoch 286: val_loss did not improve from 29.24874
196/196 - 35s - loss: 29.7251 - MinusLogProbMetric: 29.7251 - val_loss: 29.7835 - val_MinusLogProbMetric: 29.7835 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 287/1000
2023-10-26 01:19:41.203 
Epoch 287/1000 
	 loss: 29.4121, MinusLogProbMetric: 29.4121, val_loss: 29.9496, val_MinusLogProbMetric: 29.9496

Epoch 287: val_loss did not improve from 29.24874
196/196 - 35s - loss: 29.4121 - MinusLogProbMetric: 29.4121 - val_loss: 29.9496 - val_MinusLogProbMetric: 29.9496 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 288/1000
2023-10-26 01:20:17.050 
Epoch 288/1000 
	 loss: 29.6140, MinusLogProbMetric: 29.6140, val_loss: 30.6009, val_MinusLogProbMetric: 30.6009

Epoch 288: val_loss did not improve from 29.24874
196/196 - 36s - loss: 29.6140 - MinusLogProbMetric: 29.6140 - val_loss: 30.6009 - val_MinusLogProbMetric: 30.6009 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 289/1000
2023-10-26 01:20:52.926 
Epoch 289/1000 
	 loss: 29.5546, MinusLogProbMetric: 29.5546, val_loss: 29.2985, val_MinusLogProbMetric: 29.2985

Epoch 289: val_loss did not improve from 29.24874
196/196 - 36s - loss: 29.5546 - MinusLogProbMetric: 29.5546 - val_loss: 29.2985 - val_MinusLogProbMetric: 29.2985 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 290/1000
2023-10-26 01:21:28.729 
Epoch 290/1000 
	 loss: 29.6804, MinusLogProbMetric: 29.6804, val_loss: 29.5232, val_MinusLogProbMetric: 29.5232

Epoch 290: val_loss did not improve from 29.24874
196/196 - 36s - loss: 29.6804 - MinusLogProbMetric: 29.6804 - val_loss: 29.5232 - val_MinusLogProbMetric: 29.5232 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 291/1000
2023-10-26 01:22:04.561 
Epoch 291/1000 
	 loss: 29.6886, MinusLogProbMetric: 29.6886, val_loss: 30.5584, val_MinusLogProbMetric: 30.5584

Epoch 291: val_loss did not improve from 29.24874
196/196 - 36s - loss: 29.6886 - MinusLogProbMetric: 29.6886 - val_loss: 30.5584 - val_MinusLogProbMetric: 30.5584 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 292/1000
2023-10-26 01:22:40.291 
Epoch 292/1000 
	 loss: 29.5343, MinusLogProbMetric: 29.5343, val_loss: 29.4541, val_MinusLogProbMetric: 29.4541

Epoch 292: val_loss did not improve from 29.24874
196/196 - 36s - loss: 29.5343 - MinusLogProbMetric: 29.5343 - val_loss: 29.4541 - val_MinusLogProbMetric: 29.4541 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 293/1000
2023-10-26 01:23:15.569 
Epoch 293/1000 
	 loss: 29.5156, MinusLogProbMetric: 29.5156, val_loss: 29.6443, val_MinusLogProbMetric: 29.6443

Epoch 293: val_loss did not improve from 29.24874
196/196 - 35s - loss: 29.5156 - MinusLogProbMetric: 29.5156 - val_loss: 29.6443 - val_MinusLogProbMetric: 29.6443 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 294/1000
2023-10-26 01:23:50.851 
Epoch 294/1000 
	 loss: 29.4633, MinusLogProbMetric: 29.4633, val_loss: 29.4838, val_MinusLogProbMetric: 29.4838

Epoch 294: val_loss did not improve from 29.24874
196/196 - 35s - loss: 29.4633 - MinusLogProbMetric: 29.4633 - val_loss: 29.4838 - val_MinusLogProbMetric: 29.4838 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 295/1000
2023-10-26 01:24:26.280 
Epoch 295/1000 
	 loss: 29.5597, MinusLogProbMetric: 29.5597, val_loss: 31.3519, val_MinusLogProbMetric: 31.3519

Epoch 295: val_loss did not improve from 29.24874
196/196 - 35s - loss: 29.5597 - MinusLogProbMetric: 29.5597 - val_loss: 31.3519 - val_MinusLogProbMetric: 31.3519 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 296/1000
2023-10-26 01:25:01.583 
Epoch 296/1000 
	 loss: 29.5572, MinusLogProbMetric: 29.5572, val_loss: 29.2695, val_MinusLogProbMetric: 29.2695

Epoch 296: val_loss did not improve from 29.24874
196/196 - 35s - loss: 29.5572 - MinusLogProbMetric: 29.5572 - val_loss: 29.2695 - val_MinusLogProbMetric: 29.2695 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 297/1000
2023-10-26 01:25:37.015 
Epoch 297/1000 
	 loss: 29.4357, MinusLogProbMetric: 29.4357, val_loss: 30.8166, val_MinusLogProbMetric: 30.8166

Epoch 297: val_loss did not improve from 29.24874
196/196 - 35s - loss: 29.4357 - MinusLogProbMetric: 29.4357 - val_loss: 30.8166 - val_MinusLogProbMetric: 30.8166 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 298/1000
2023-10-26 01:26:12.672 
Epoch 298/1000 
	 loss: 29.8880, MinusLogProbMetric: 29.8880, val_loss: 29.7421, val_MinusLogProbMetric: 29.7421

Epoch 298: val_loss did not improve from 29.24874
196/196 - 36s - loss: 29.8880 - MinusLogProbMetric: 29.8880 - val_loss: 29.7421 - val_MinusLogProbMetric: 29.7421 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 299/1000
2023-10-26 01:26:47.923 
Epoch 299/1000 
	 loss: 29.7270, MinusLogProbMetric: 29.7270, val_loss: 29.7992, val_MinusLogProbMetric: 29.7992

Epoch 299: val_loss did not improve from 29.24874
196/196 - 35s - loss: 29.7270 - MinusLogProbMetric: 29.7270 - val_loss: 29.7992 - val_MinusLogProbMetric: 29.7992 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 300/1000
2023-10-26 01:27:23.088 
Epoch 300/1000 
	 loss: 29.3558, MinusLogProbMetric: 29.3558, val_loss: 29.4807, val_MinusLogProbMetric: 29.4807

Epoch 300: val_loss did not improve from 29.24874
196/196 - 35s - loss: 29.3558 - MinusLogProbMetric: 29.3558 - val_loss: 29.4807 - val_MinusLogProbMetric: 29.4807 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 301/1000
2023-10-26 01:27:58.526 
Epoch 301/1000 
	 loss: 29.5890, MinusLogProbMetric: 29.5890, val_loss: 29.4131, val_MinusLogProbMetric: 29.4131

Epoch 301: val_loss did not improve from 29.24874
196/196 - 35s - loss: 29.5890 - MinusLogProbMetric: 29.5890 - val_loss: 29.4131 - val_MinusLogProbMetric: 29.4131 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 302/1000
2023-10-26 01:28:33.616 
Epoch 302/1000 
	 loss: 29.5992, MinusLogProbMetric: 29.5992, val_loss: 29.4589, val_MinusLogProbMetric: 29.4589

Epoch 302: val_loss did not improve from 29.24874
196/196 - 35s - loss: 29.5992 - MinusLogProbMetric: 29.5992 - val_loss: 29.4589 - val_MinusLogProbMetric: 29.4589 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 303/1000
2023-10-26 01:29:09.220 
Epoch 303/1000 
	 loss: 29.3873, MinusLogProbMetric: 29.3873, val_loss: 29.3127, val_MinusLogProbMetric: 29.3127

Epoch 303: val_loss did not improve from 29.24874
196/196 - 36s - loss: 29.3873 - MinusLogProbMetric: 29.3873 - val_loss: 29.3127 - val_MinusLogProbMetric: 29.3127 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 304/1000
2023-10-26 01:29:44.861 
Epoch 304/1000 
	 loss: 29.5261, MinusLogProbMetric: 29.5261, val_loss: 29.6090, val_MinusLogProbMetric: 29.6090

Epoch 304: val_loss did not improve from 29.24874
196/196 - 36s - loss: 29.5261 - MinusLogProbMetric: 29.5261 - val_loss: 29.6090 - val_MinusLogProbMetric: 29.6090 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 305/1000
2023-10-26 01:30:20.130 
Epoch 305/1000 
	 loss: 29.4343, MinusLogProbMetric: 29.4343, val_loss: 30.0489, val_MinusLogProbMetric: 30.0489

Epoch 305: val_loss did not improve from 29.24874
196/196 - 35s - loss: 29.4343 - MinusLogProbMetric: 29.4343 - val_loss: 30.0489 - val_MinusLogProbMetric: 30.0489 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 306/1000
2023-10-26 01:30:55.891 
Epoch 306/1000 
	 loss: 29.7180, MinusLogProbMetric: 29.7180, val_loss: 29.5304, val_MinusLogProbMetric: 29.5304

Epoch 306: val_loss did not improve from 29.24874
196/196 - 36s - loss: 29.7180 - MinusLogProbMetric: 29.7180 - val_loss: 29.5304 - val_MinusLogProbMetric: 29.5304 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 307/1000
2023-10-26 01:31:31.459 
Epoch 307/1000 
	 loss: 29.4582, MinusLogProbMetric: 29.4582, val_loss: 29.4222, val_MinusLogProbMetric: 29.4222

Epoch 307: val_loss did not improve from 29.24874
196/196 - 36s - loss: 29.4582 - MinusLogProbMetric: 29.4582 - val_loss: 29.4222 - val_MinusLogProbMetric: 29.4222 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 308/1000
2023-10-26 01:32:07.322 
Epoch 308/1000 
	 loss: 29.4006, MinusLogProbMetric: 29.4006, val_loss: 29.2134, val_MinusLogProbMetric: 29.2134

Epoch 308: val_loss improved from 29.24874 to 29.21339, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_369/weights/best_weights.h5
196/196 - 37s - loss: 29.4006 - MinusLogProbMetric: 29.4006 - val_loss: 29.2134 - val_MinusLogProbMetric: 29.2134 - lr: 0.0010 - 37s/epoch - 186ms/step
Epoch 309/1000
2023-10-26 01:32:43.545 
Epoch 309/1000 
	 loss: 29.5382, MinusLogProbMetric: 29.5382, val_loss: 29.2825, val_MinusLogProbMetric: 29.2825

Epoch 309: val_loss did not improve from 29.21339
196/196 - 36s - loss: 29.5382 - MinusLogProbMetric: 29.5382 - val_loss: 29.2825 - val_MinusLogProbMetric: 29.2825 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 310/1000
2023-10-26 01:33:19.399 
Epoch 310/1000 
	 loss: 29.4975, MinusLogProbMetric: 29.4975, val_loss: 29.7434, val_MinusLogProbMetric: 29.7434

Epoch 310: val_loss did not improve from 29.21339
196/196 - 36s - loss: 29.4975 - MinusLogProbMetric: 29.4975 - val_loss: 29.7434 - val_MinusLogProbMetric: 29.7434 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 311/1000
2023-10-26 01:33:55.117 
Epoch 311/1000 
	 loss: 29.5877, MinusLogProbMetric: 29.5877, val_loss: 29.1777, val_MinusLogProbMetric: 29.1777

Epoch 311: val_loss improved from 29.21339 to 29.17765, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_369/weights/best_weights.h5
196/196 - 36s - loss: 29.5877 - MinusLogProbMetric: 29.5877 - val_loss: 29.1777 - val_MinusLogProbMetric: 29.1777 - lr: 0.0010 - 36s/epoch - 186ms/step
Epoch 312/1000
2023-10-26 01:34:31.475 
Epoch 312/1000 
	 loss: 29.5735, MinusLogProbMetric: 29.5735, val_loss: 30.3234, val_MinusLogProbMetric: 30.3234

Epoch 312: val_loss did not improve from 29.17765
196/196 - 36s - loss: 29.5735 - MinusLogProbMetric: 29.5735 - val_loss: 30.3234 - val_MinusLogProbMetric: 30.3234 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 313/1000
2023-10-26 01:35:06.889 
Epoch 313/1000 
	 loss: 29.4460, MinusLogProbMetric: 29.4460, val_loss: 29.8718, val_MinusLogProbMetric: 29.8718

Epoch 313: val_loss did not improve from 29.17765
196/196 - 35s - loss: 29.4460 - MinusLogProbMetric: 29.4460 - val_loss: 29.8718 - val_MinusLogProbMetric: 29.8718 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 314/1000
2023-10-26 01:35:42.450 
Epoch 314/1000 
	 loss: 29.4836, MinusLogProbMetric: 29.4836, val_loss: 30.2858, val_MinusLogProbMetric: 30.2858

Epoch 314: val_loss did not improve from 29.17765
196/196 - 36s - loss: 29.4836 - MinusLogProbMetric: 29.4836 - val_loss: 30.2858 - val_MinusLogProbMetric: 30.2858 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 315/1000
2023-10-26 01:36:17.917 
Epoch 315/1000 
	 loss: 29.5356, MinusLogProbMetric: 29.5356, val_loss: 29.4804, val_MinusLogProbMetric: 29.4804

Epoch 315: val_loss did not improve from 29.17765
196/196 - 35s - loss: 29.5356 - MinusLogProbMetric: 29.5356 - val_loss: 29.4804 - val_MinusLogProbMetric: 29.4804 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 316/1000
2023-10-26 01:36:53.592 
Epoch 316/1000 
	 loss: 29.4081, MinusLogProbMetric: 29.4081, val_loss: 29.7970, val_MinusLogProbMetric: 29.7970

Epoch 316: val_loss did not improve from 29.17765
196/196 - 36s - loss: 29.4081 - MinusLogProbMetric: 29.4081 - val_loss: 29.7970 - val_MinusLogProbMetric: 29.7970 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 317/1000
2023-10-26 01:37:29.125 
Epoch 317/1000 
	 loss: 29.4725, MinusLogProbMetric: 29.4725, val_loss: 30.0407, val_MinusLogProbMetric: 30.0407

Epoch 317: val_loss did not improve from 29.17765
196/196 - 36s - loss: 29.4725 - MinusLogProbMetric: 29.4725 - val_loss: 30.0407 - val_MinusLogProbMetric: 30.0407 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 318/1000
2023-10-26 01:37:57.230 
Epoch 318/1000 
	 loss: 29.5805, MinusLogProbMetric: 29.5805, val_loss: 29.4957, val_MinusLogProbMetric: 29.4957

Epoch 318: val_loss did not improve from 29.17765
196/196 - 28s - loss: 29.5805 - MinusLogProbMetric: 29.5805 - val_loss: 29.4957 - val_MinusLogProbMetric: 29.4957 - lr: 0.0010 - 28s/epoch - 143ms/step
Epoch 319/1000
2023-10-26 01:38:26.074 
Epoch 319/1000 
	 loss: 29.4474, MinusLogProbMetric: 29.4474, val_loss: 29.3992, val_MinusLogProbMetric: 29.3992

Epoch 319: val_loss did not improve from 29.17765
196/196 - 29s - loss: 29.4474 - MinusLogProbMetric: 29.4474 - val_loss: 29.3992 - val_MinusLogProbMetric: 29.3992 - lr: 0.0010 - 29s/epoch - 147ms/step
Epoch 320/1000
2023-10-26 01:38:58.974 
Epoch 320/1000 
	 loss: 29.3791, MinusLogProbMetric: 29.3791, val_loss: 29.1575, val_MinusLogProbMetric: 29.1575

Epoch 320: val_loss improved from 29.17765 to 29.15747, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_369/weights/best_weights.h5
196/196 - 33s - loss: 29.3791 - MinusLogProbMetric: 29.3791 - val_loss: 29.1575 - val_MinusLogProbMetric: 29.1575 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 321/1000
2023-10-26 01:39:28.753 
Epoch 321/1000 
	 loss: 29.3173, MinusLogProbMetric: 29.3173, val_loss: 31.1544, val_MinusLogProbMetric: 31.1544

Epoch 321: val_loss did not improve from 29.15747
196/196 - 29s - loss: 29.3173 - MinusLogProbMetric: 29.3173 - val_loss: 31.1544 - val_MinusLogProbMetric: 31.1544 - lr: 0.0010 - 29s/epoch - 149ms/step
Epoch 322/1000
2023-10-26 01:40:00.020 
Epoch 322/1000 
	 loss: 29.5630, MinusLogProbMetric: 29.5630, val_loss: 29.7302, val_MinusLogProbMetric: 29.7302

Epoch 322: val_loss did not improve from 29.15747
196/196 - 31s - loss: 29.5630 - MinusLogProbMetric: 29.5630 - val_loss: 29.7302 - val_MinusLogProbMetric: 29.7302 - lr: 0.0010 - 31s/epoch - 160ms/step
Epoch 323/1000
2023-10-26 01:40:35.657 
Epoch 323/1000 
	 loss: 29.4207, MinusLogProbMetric: 29.4207, val_loss: 29.2609, val_MinusLogProbMetric: 29.2609

Epoch 323: val_loss did not improve from 29.15747
196/196 - 36s - loss: 29.4207 - MinusLogProbMetric: 29.4207 - val_loss: 29.2609 - val_MinusLogProbMetric: 29.2609 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 324/1000
2023-10-26 01:41:11.421 
Epoch 324/1000 
	 loss: 29.3588, MinusLogProbMetric: 29.3588, val_loss: 29.7134, val_MinusLogProbMetric: 29.7134

Epoch 324: val_loss did not improve from 29.15747
196/196 - 36s - loss: 29.3588 - MinusLogProbMetric: 29.3588 - val_loss: 29.7134 - val_MinusLogProbMetric: 29.7134 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 325/1000
2023-10-26 01:41:47.160 
Epoch 325/1000 
	 loss: 29.5298, MinusLogProbMetric: 29.5298, val_loss: 29.0495, val_MinusLogProbMetric: 29.0495

Epoch 325: val_loss improved from 29.15747 to 29.04951, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_369/weights/best_weights.h5
196/196 - 36s - loss: 29.5298 - MinusLogProbMetric: 29.5298 - val_loss: 29.0495 - val_MinusLogProbMetric: 29.0495 - lr: 0.0010 - 36s/epoch - 185ms/step
Epoch 326/1000
2023-10-26 01:42:23.444 
Epoch 326/1000 
	 loss: 29.5634, MinusLogProbMetric: 29.5634, val_loss: 30.0200, val_MinusLogProbMetric: 30.0200

Epoch 326: val_loss did not improve from 29.04951
196/196 - 36s - loss: 29.5634 - MinusLogProbMetric: 29.5634 - val_loss: 30.0200 - val_MinusLogProbMetric: 30.0200 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 327/1000
2023-10-26 01:42:59.014 
Epoch 327/1000 
	 loss: 29.5461, MinusLogProbMetric: 29.5461, val_loss: 29.4265, val_MinusLogProbMetric: 29.4265

Epoch 327: val_loss did not improve from 29.04951
196/196 - 36s - loss: 29.5461 - MinusLogProbMetric: 29.5461 - val_loss: 29.4265 - val_MinusLogProbMetric: 29.4265 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 328/1000
2023-10-26 01:43:34.559 
Epoch 328/1000 
	 loss: 29.3337, MinusLogProbMetric: 29.3337, val_loss: 29.1658, val_MinusLogProbMetric: 29.1658

Epoch 328: val_loss did not improve from 29.04951
196/196 - 36s - loss: 29.3337 - MinusLogProbMetric: 29.3337 - val_loss: 29.1658 - val_MinusLogProbMetric: 29.1658 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 329/1000
2023-10-26 01:44:09.941 
Epoch 329/1000 
	 loss: 29.6805, MinusLogProbMetric: 29.6805, val_loss: 29.6792, val_MinusLogProbMetric: 29.6792

Epoch 329: val_loss did not improve from 29.04951
196/196 - 35s - loss: 29.6805 - MinusLogProbMetric: 29.6805 - val_loss: 29.6792 - val_MinusLogProbMetric: 29.6792 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 330/1000
2023-10-26 01:44:45.452 
Epoch 330/1000 
	 loss: 29.4622, MinusLogProbMetric: 29.4622, val_loss: 30.1622, val_MinusLogProbMetric: 30.1622

Epoch 330: val_loss did not improve from 29.04951
196/196 - 36s - loss: 29.4622 - MinusLogProbMetric: 29.4622 - val_loss: 30.1622 - val_MinusLogProbMetric: 30.1622 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 331/1000
2023-10-26 01:45:20.972 
Epoch 331/1000 
	 loss: 29.2795, MinusLogProbMetric: 29.2795, val_loss: 29.6017, val_MinusLogProbMetric: 29.6017

Epoch 331: val_loss did not improve from 29.04951
196/196 - 36s - loss: 29.2795 - MinusLogProbMetric: 29.2795 - val_loss: 29.6017 - val_MinusLogProbMetric: 29.6017 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 332/1000
2023-10-26 01:45:56.705 
Epoch 332/1000 
	 loss: 29.5959, MinusLogProbMetric: 29.5959, val_loss: 29.6527, val_MinusLogProbMetric: 29.6527

Epoch 332: val_loss did not improve from 29.04951
196/196 - 36s - loss: 29.5959 - MinusLogProbMetric: 29.5959 - val_loss: 29.6527 - val_MinusLogProbMetric: 29.6527 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 333/1000
2023-10-26 01:46:32.442 
Epoch 333/1000 
	 loss: 29.3643, MinusLogProbMetric: 29.3643, val_loss: 29.1947, val_MinusLogProbMetric: 29.1947

Epoch 333: val_loss did not improve from 29.04951
196/196 - 36s - loss: 29.3643 - MinusLogProbMetric: 29.3643 - val_loss: 29.1947 - val_MinusLogProbMetric: 29.1947 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 334/1000
2023-10-26 01:47:08.081 
Epoch 334/1000 
	 loss: 29.3003, MinusLogProbMetric: 29.3003, val_loss: 32.8098, val_MinusLogProbMetric: 32.8098

Epoch 334: val_loss did not improve from 29.04951
196/196 - 36s - loss: 29.3003 - MinusLogProbMetric: 29.3003 - val_loss: 32.8098 - val_MinusLogProbMetric: 32.8098 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 335/1000
2023-10-26 01:47:43.513 
Epoch 335/1000 
	 loss: 29.5339, MinusLogProbMetric: 29.5339, val_loss: 30.5490, val_MinusLogProbMetric: 30.5490

Epoch 335: val_loss did not improve from 29.04951
196/196 - 35s - loss: 29.5339 - MinusLogProbMetric: 29.5339 - val_loss: 30.5490 - val_MinusLogProbMetric: 30.5490 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 336/1000
2023-10-26 01:48:18.951 
Epoch 336/1000 
	 loss: 29.3827, MinusLogProbMetric: 29.3827, val_loss: 29.5404, val_MinusLogProbMetric: 29.5404

Epoch 336: val_loss did not improve from 29.04951
196/196 - 35s - loss: 29.3827 - MinusLogProbMetric: 29.3827 - val_loss: 29.5404 - val_MinusLogProbMetric: 29.5404 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 337/1000
2023-10-26 01:48:54.150 
Epoch 337/1000 
	 loss: 29.5173, MinusLogProbMetric: 29.5173, val_loss: 29.9371, val_MinusLogProbMetric: 29.9371

Epoch 337: val_loss did not improve from 29.04951
196/196 - 35s - loss: 29.5173 - MinusLogProbMetric: 29.5173 - val_loss: 29.9371 - val_MinusLogProbMetric: 29.9371 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 338/1000
2023-10-26 01:49:29.378 
Epoch 338/1000 
	 loss: 29.3587, MinusLogProbMetric: 29.3587, val_loss: 29.7028, val_MinusLogProbMetric: 29.7028

Epoch 338: val_loss did not improve from 29.04951
196/196 - 35s - loss: 29.3587 - MinusLogProbMetric: 29.3587 - val_loss: 29.7028 - val_MinusLogProbMetric: 29.7028 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 339/1000
2023-10-26 01:50:04.807 
Epoch 339/1000 
	 loss: 29.2962, MinusLogProbMetric: 29.2962, val_loss: 29.9826, val_MinusLogProbMetric: 29.9826

Epoch 339: val_loss did not improve from 29.04951
196/196 - 35s - loss: 29.2962 - MinusLogProbMetric: 29.2962 - val_loss: 29.9826 - val_MinusLogProbMetric: 29.9826 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 340/1000
2023-10-26 01:50:39.962 
Epoch 340/1000 
	 loss: 29.6500, MinusLogProbMetric: 29.6500, val_loss: 29.9592, val_MinusLogProbMetric: 29.9592

Epoch 340: val_loss did not improve from 29.04951
196/196 - 35s - loss: 29.6500 - MinusLogProbMetric: 29.6500 - val_loss: 29.9592 - val_MinusLogProbMetric: 29.9592 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 341/1000
2023-10-26 01:51:15.327 
Epoch 341/1000 
	 loss: 29.3048, MinusLogProbMetric: 29.3048, val_loss: 30.0762, val_MinusLogProbMetric: 30.0762

Epoch 341: val_loss did not improve from 29.04951
196/196 - 35s - loss: 29.3048 - MinusLogProbMetric: 29.3048 - val_loss: 30.0762 - val_MinusLogProbMetric: 30.0762 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 342/1000
2023-10-26 01:51:50.567 
Epoch 342/1000 
	 loss: 29.3358, MinusLogProbMetric: 29.3358, val_loss: 29.8404, val_MinusLogProbMetric: 29.8404

Epoch 342: val_loss did not improve from 29.04951
196/196 - 35s - loss: 29.3358 - MinusLogProbMetric: 29.3358 - val_loss: 29.8404 - val_MinusLogProbMetric: 29.8404 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 343/1000
2023-10-26 01:52:26.212 
Epoch 343/1000 
	 loss: 29.3420, MinusLogProbMetric: 29.3420, val_loss: 29.5271, val_MinusLogProbMetric: 29.5271

Epoch 343: val_loss did not improve from 29.04951
196/196 - 36s - loss: 29.3420 - MinusLogProbMetric: 29.3420 - val_loss: 29.5271 - val_MinusLogProbMetric: 29.5271 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 344/1000
2023-10-26 01:53:01.685 
Epoch 344/1000 
	 loss: 29.4619, MinusLogProbMetric: 29.4619, val_loss: 29.5945, val_MinusLogProbMetric: 29.5945

Epoch 344: val_loss did not improve from 29.04951
196/196 - 35s - loss: 29.4619 - MinusLogProbMetric: 29.4619 - val_loss: 29.5945 - val_MinusLogProbMetric: 29.5945 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 345/1000
2023-10-26 01:53:36.992 
Epoch 345/1000 
	 loss: 29.2939, MinusLogProbMetric: 29.2939, val_loss: 29.1031, val_MinusLogProbMetric: 29.1031

Epoch 345: val_loss did not improve from 29.04951
196/196 - 35s - loss: 29.2939 - MinusLogProbMetric: 29.2939 - val_loss: 29.1031 - val_MinusLogProbMetric: 29.1031 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 346/1000
2023-10-26 01:54:12.967 
Epoch 346/1000 
	 loss: 29.4010, MinusLogProbMetric: 29.4010, val_loss: 29.4992, val_MinusLogProbMetric: 29.4992

Epoch 346: val_loss did not improve from 29.04951
196/196 - 36s - loss: 29.4010 - MinusLogProbMetric: 29.4010 - val_loss: 29.4992 - val_MinusLogProbMetric: 29.4992 - lr: 0.0010 - 36s/epoch - 184ms/step
Epoch 347/1000
2023-10-26 01:54:48.551 
Epoch 347/1000 
	 loss: 29.2344, MinusLogProbMetric: 29.2344, val_loss: 29.0539, val_MinusLogProbMetric: 29.0539

Epoch 347: val_loss did not improve from 29.04951
196/196 - 36s - loss: 29.2344 - MinusLogProbMetric: 29.2344 - val_loss: 29.0539 - val_MinusLogProbMetric: 29.0539 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 348/1000
2023-10-26 01:55:24.289 
Epoch 348/1000 
	 loss: 29.3866, MinusLogProbMetric: 29.3866, val_loss: 29.5446, val_MinusLogProbMetric: 29.5446

Epoch 348: val_loss did not improve from 29.04951
196/196 - 36s - loss: 29.3866 - MinusLogProbMetric: 29.3866 - val_loss: 29.5446 - val_MinusLogProbMetric: 29.5446 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 349/1000
2023-10-26 01:55:59.780 
Epoch 349/1000 
	 loss: 29.4005, MinusLogProbMetric: 29.4005, val_loss: 29.8582, val_MinusLogProbMetric: 29.8582

Epoch 349: val_loss did not improve from 29.04951
196/196 - 35s - loss: 29.4005 - MinusLogProbMetric: 29.4005 - val_loss: 29.8582 - val_MinusLogProbMetric: 29.8582 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 350/1000
2023-10-26 01:56:35.323 
Epoch 350/1000 
	 loss: 29.4080, MinusLogProbMetric: 29.4080, val_loss: 29.3000, val_MinusLogProbMetric: 29.3000

Epoch 350: val_loss did not improve from 29.04951
196/196 - 36s - loss: 29.4080 - MinusLogProbMetric: 29.4080 - val_loss: 29.3000 - val_MinusLogProbMetric: 29.3000 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 351/1000
2023-10-26 01:57:10.932 
Epoch 351/1000 
	 loss: 29.2213, MinusLogProbMetric: 29.2213, val_loss: 29.3459, val_MinusLogProbMetric: 29.3459

Epoch 351: val_loss did not improve from 29.04951
196/196 - 36s - loss: 29.2213 - MinusLogProbMetric: 29.2213 - val_loss: 29.3459 - val_MinusLogProbMetric: 29.3459 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 352/1000
2023-10-26 01:57:46.690 
Epoch 352/1000 
	 loss: 29.3404, MinusLogProbMetric: 29.3404, val_loss: 29.0298, val_MinusLogProbMetric: 29.0298

Epoch 352: val_loss improved from 29.04951 to 29.02976, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_369/weights/best_weights.h5
196/196 - 36s - loss: 29.3404 - MinusLogProbMetric: 29.3404 - val_loss: 29.0298 - val_MinusLogProbMetric: 29.0298 - lr: 0.0010 - 36s/epoch - 186ms/step
Epoch 353/1000
2023-10-26 01:58:23.137 
Epoch 353/1000 
	 loss: 29.4192, MinusLogProbMetric: 29.4192, val_loss: 29.4483, val_MinusLogProbMetric: 29.4483

Epoch 353: val_loss did not improve from 29.02976
196/196 - 36s - loss: 29.4192 - MinusLogProbMetric: 29.4192 - val_loss: 29.4483 - val_MinusLogProbMetric: 29.4483 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 354/1000
2023-10-26 01:58:58.662 
Epoch 354/1000 
	 loss: 29.3719, MinusLogProbMetric: 29.3719, val_loss: 30.9623, val_MinusLogProbMetric: 30.9623

Epoch 354: val_loss did not improve from 29.02976
196/196 - 36s - loss: 29.3719 - MinusLogProbMetric: 29.3719 - val_loss: 30.9623 - val_MinusLogProbMetric: 30.9623 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 355/1000
2023-10-26 01:59:33.967 
Epoch 355/1000 
	 loss: 29.4717, MinusLogProbMetric: 29.4717, val_loss: 29.8961, val_MinusLogProbMetric: 29.8961

Epoch 355: val_loss did not improve from 29.02976
196/196 - 35s - loss: 29.4717 - MinusLogProbMetric: 29.4717 - val_loss: 29.8961 - val_MinusLogProbMetric: 29.8961 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 356/1000
2023-10-26 02:00:09.814 
Epoch 356/1000 
	 loss: 29.2819, MinusLogProbMetric: 29.2819, val_loss: 28.9630, val_MinusLogProbMetric: 28.9630

Epoch 356: val_loss improved from 29.02976 to 28.96304, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_369/weights/best_weights.h5
196/196 - 37s - loss: 29.2819 - MinusLogProbMetric: 29.2819 - val_loss: 28.9630 - val_MinusLogProbMetric: 28.9630 - lr: 0.0010 - 37s/epoch - 186ms/step
Epoch 357/1000
2023-10-26 02:00:46.231 
Epoch 357/1000 
	 loss: 29.2339, MinusLogProbMetric: 29.2339, val_loss: 29.8255, val_MinusLogProbMetric: 29.8255

Epoch 357: val_loss did not improve from 28.96304
196/196 - 36s - loss: 29.2339 - MinusLogProbMetric: 29.2339 - val_loss: 29.8255 - val_MinusLogProbMetric: 29.8255 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 358/1000
2023-10-26 02:01:22.195 
Epoch 358/1000 
	 loss: 29.3053, MinusLogProbMetric: 29.3053, val_loss: 29.6465, val_MinusLogProbMetric: 29.6465

Epoch 358: val_loss did not improve from 28.96304
196/196 - 36s - loss: 29.3053 - MinusLogProbMetric: 29.3053 - val_loss: 29.6465 - val_MinusLogProbMetric: 29.6465 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 359/1000
2023-10-26 02:01:57.760 
Epoch 359/1000 
	 loss: 29.3473, MinusLogProbMetric: 29.3473, val_loss: 29.6639, val_MinusLogProbMetric: 29.6639

Epoch 359: val_loss did not improve from 28.96304
196/196 - 36s - loss: 29.3473 - MinusLogProbMetric: 29.3473 - val_loss: 29.6639 - val_MinusLogProbMetric: 29.6639 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 360/1000
2023-10-26 02:02:33.602 
Epoch 360/1000 
	 loss: 29.1718, MinusLogProbMetric: 29.1718, val_loss: 29.2718, val_MinusLogProbMetric: 29.2718

Epoch 360: val_loss did not improve from 28.96304
196/196 - 36s - loss: 29.1718 - MinusLogProbMetric: 29.1718 - val_loss: 29.2718 - val_MinusLogProbMetric: 29.2718 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 361/1000
2023-10-26 02:03:09.217 
Epoch 361/1000 
	 loss: 29.2468, MinusLogProbMetric: 29.2468, val_loss: 29.8377, val_MinusLogProbMetric: 29.8377

Epoch 361: val_loss did not improve from 28.96304
196/196 - 36s - loss: 29.2468 - MinusLogProbMetric: 29.2468 - val_loss: 29.8377 - val_MinusLogProbMetric: 29.8377 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 362/1000
2023-10-26 02:03:45.049 
Epoch 362/1000 
	 loss: 29.4250, MinusLogProbMetric: 29.4250, val_loss: 29.7385, val_MinusLogProbMetric: 29.7385

Epoch 362: val_loss did not improve from 28.96304
196/196 - 36s - loss: 29.4250 - MinusLogProbMetric: 29.4250 - val_loss: 29.7385 - val_MinusLogProbMetric: 29.7385 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 363/1000
2023-10-26 02:04:20.677 
Epoch 363/1000 
	 loss: 29.2172, MinusLogProbMetric: 29.2172, val_loss: 29.9137, val_MinusLogProbMetric: 29.9137

Epoch 363: val_loss did not improve from 28.96304
196/196 - 36s - loss: 29.2172 - MinusLogProbMetric: 29.2172 - val_loss: 29.9137 - val_MinusLogProbMetric: 29.9137 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 364/1000
2023-10-26 02:04:56.434 
Epoch 364/1000 
	 loss: 29.3684, MinusLogProbMetric: 29.3684, val_loss: 29.3327, val_MinusLogProbMetric: 29.3327

Epoch 364: val_loss did not improve from 28.96304
196/196 - 36s - loss: 29.3684 - MinusLogProbMetric: 29.3684 - val_loss: 29.3327 - val_MinusLogProbMetric: 29.3327 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 365/1000
2023-10-26 02:05:31.970 
Epoch 365/1000 
	 loss: 29.2793, MinusLogProbMetric: 29.2793, val_loss: 29.5175, val_MinusLogProbMetric: 29.5175

Epoch 365: val_loss did not improve from 28.96304
196/196 - 36s - loss: 29.2793 - MinusLogProbMetric: 29.2793 - val_loss: 29.5175 - val_MinusLogProbMetric: 29.5175 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 366/1000
2023-10-26 02:06:07.621 
Epoch 366/1000 
	 loss: 29.3461, MinusLogProbMetric: 29.3461, val_loss: 30.8037, val_MinusLogProbMetric: 30.8037

Epoch 366: val_loss did not improve from 28.96304
196/196 - 36s - loss: 29.3461 - MinusLogProbMetric: 29.3461 - val_loss: 30.8037 - val_MinusLogProbMetric: 30.8037 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 367/1000
2023-10-26 02:06:42.940 
Epoch 367/1000 
	 loss: 29.4301, MinusLogProbMetric: 29.4301, val_loss: 29.3752, val_MinusLogProbMetric: 29.3752

Epoch 367: val_loss did not improve from 28.96304
196/196 - 35s - loss: 29.4301 - MinusLogProbMetric: 29.4301 - val_loss: 29.3752 - val_MinusLogProbMetric: 29.3752 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 368/1000
2023-10-26 02:07:18.246 
Epoch 368/1000 
	 loss: 29.4543, MinusLogProbMetric: 29.4543, val_loss: 30.1033, val_MinusLogProbMetric: 30.1033

Epoch 368: val_loss did not improve from 28.96304
196/196 - 35s - loss: 29.4543 - MinusLogProbMetric: 29.4543 - val_loss: 30.1033 - val_MinusLogProbMetric: 30.1033 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 369/1000
2023-10-26 02:07:54.090 
Epoch 369/1000 
	 loss: 29.2805, MinusLogProbMetric: 29.2805, val_loss: 29.4792, val_MinusLogProbMetric: 29.4792

Epoch 369: val_loss did not improve from 28.96304
196/196 - 36s - loss: 29.2805 - MinusLogProbMetric: 29.2805 - val_loss: 29.4792 - val_MinusLogProbMetric: 29.4792 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 370/1000
2023-10-26 02:08:29.987 
Epoch 370/1000 
	 loss: 29.4234, MinusLogProbMetric: 29.4234, val_loss: 31.4578, val_MinusLogProbMetric: 31.4578

Epoch 370: val_loss did not improve from 28.96304
196/196 - 36s - loss: 29.4234 - MinusLogProbMetric: 29.4234 - val_loss: 31.4578 - val_MinusLogProbMetric: 31.4578 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 371/1000
2023-10-26 02:09:05.534 
Epoch 371/1000 
	 loss: 29.1809, MinusLogProbMetric: 29.1809, val_loss: 29.0100, val_MinusLogProbMetric: 29.0100

Epoch 371: val_loss did not improve from 28.96304
196/196 - 36s - loss: 29.1809 - MinusLogProbMetric: 29.1809 - val_loss: 29.0100 - val_MinusLogProbMetric: 29.0100 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 372/1000
2023-10-26 02:09:41.054 
Epoch 372/1000 
	 loss: 29.4348, MinusLogProbMetric: 29.4348, val_loss: 30.6343, val_MinusLogProbMetric: 30.6343

Epoch 372: val_loss did not improve from 28.96304
196/196 - 36s - loss: 29.4348 - MinusLogProbMetric: 29.4348 - val_loss: 30.6343 - val_MinusLogProbMetric: 30.6343 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 373/1000
2023-10-26 02:10:16.694 
Epoch 373/1000 
	 loss: 29.3312, MinusLogProbMetric: 29.3312, val_loss: 29.4912, val_MinusLogProbMetric: 29.4912

Epoch 373: val_loss did not improve from 28.96304
196/196 - 36s - loss: 29.3312 - MinusLogProbMetric: 29.3312 - val_loss: 29.4912 - val_MinusLogProbMetric: 29.4912 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 374/1000
2023-10-26 02:10:52.424 
Epoch 374/1000 
	 loss: 29.3474, MinusLogProbMetric: 29.3474, val_loss: 30.7266, val_MinusLogProbMetric: 30.7266

Epoch 374: val_loss did not improve from 28.96304
196/196 - 36s - loss: 29.3474 - MinusLogProbMetric: 29.3474 - val_loss: 30.7266 - val_MinusLogProbMetric: 30.7266 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 375/1000
2023-10-26 02:11:27.854 
Epoch 375/1000 
	 loss: 29.1923, MinusLogProbMetric: 29.1923, val_loss: 29.2805, val_MinusLogProbMetric: 29.2805

Epoch 375: val_loss did not improve from 28.96304
196/196 - 35s - loss: 29.1923 - MinusLogProbMetric: 29.1923 - val_loss: 29.2805 - val_MinusLogProbMetric: 29.2805 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 376/1000
2023-10-26 02:12:03.347 
Epoch 376/1000 
	 loss: 29.1935, MinusLogProbMetric: 29.1935, val_loss: 29.5461, val_MinusLogProbMetric: 29.5461

Epoch 376: val_loss did not improve from 28.96304
196/196 - 35s - loss: 29.1935 - MinusLogProbMetric: 29.1935 - val_loss: 29.5461 - val_MinusLogProbMetric: 29.5461 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 377/1000
2023-10-26 02:12:38.969 
Epoch 377/1000 
	 loss: 29.2066, MinusLogProbMetric: 29.2066, val_loss: 29.2747, val_MinusLogProbMetric: 29.2747

Epoch 377: val_loss did not improve from 28.96304
196/196 - 36s - loss: 29.2066 - MinusLogProbMetric: 29.2066 - val_loss: 29.2747 - val_MinusLogProbMetric: 29.2747 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 378/1000
2023-10-26 02:13:14.679 
Epoch 378/1000 
	 loss: 29.2544, MinusLogProbMetric: 29.2544, val_loss: 29.6926, val_MinusLogProbMetric: 29.6926

Epoch 378: val_loss did not improve from 28.96304
196/196 - 36s - loss: 29.2544 - MinusLogProbMetric: 29.2544 - val_loss: 29.6926 - val_MinusLogProbMetric: 29.6926 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 379/1000
2023-10-26 02:13:50.305 
Epoch 379/1000 
	 loss: 29.1442, MinusLogProbMetric: 29.1442, val_loss: 30.3121, val_MinusLogProbMetric: 30.3121

Epoch 379: val_loss did not improve from 28.96304
196/196 - 36s - loss: 29.1442 - MinusLogProbMetric: 29.1442 - val_loss: 30.3121 - val_MinusLogProbMetric: 30.3121 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 380/1000
2023-10-26 02:14:25.496 
Epoch 380/1000 
	 loss: 29.3422, MinusLogProbMetric: 29.3422, val_loss: 29.7123, val_MinusLogProbMetric: 29.7123

Epoch 380: val_loss did not improve from 28.96304
196/196 - 35s - loss: 29.3422 - MinusLogProbMetric: 29.3422 - val_loss: 29.7123 - val_MinusLogProbMetric: 29.7123 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 381/1000
2023-10-26 02:15:00.827 
Epoch 381/1000 
	 loss: 29.5410, MinusLogProbMetric: 29.5410, val_loss: 30.0599, val_MinusLogProbMetric: 30.0599

Epoch 381: val_loss did not improve from 28.96304
196/196 - 35s - loss: 29.5410 - MinusLogProbMetric: 29.5410 - val_loss: 30.0599 - val_MinusLogProbMetric: 30.0599 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 382/1000
2023-10-26 02:15:36.383 
Epoch 382/1000 
	 loss: 29.1509, MinusLogProbMetric: 29.1509, val_loss: 30.1226, val_MinusLogProbMetric: 30.1226

Epoch 382: val_loss did not improve from 28.96304
196/196 - 36s - loss: 29.1509 - MinusLogProbMetric: 29.1509 - val_loss: 30.1226 - val_MinusLogProbMetric: 30.1226 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 383/1000
2023-10-26 02:16:11.620 
Epoch 383/1000 
	 loss: 29.1826, MinusLogProbMetric: 29.1826, val_loss: 29.5243, val_MinusLogProbMetric: 29.5243

Epoch 383: val_loss did not improve from 28.96304
196/196 - 35s - loss: 29.1826 - MinusLogProbMetric: 29.1826 - val_loss: 29.5243 - val_MinusLogProbMetric: 29.5243 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 384/1000
2023-10-26 02:16:46.818 
Epoch 384/1000 
	 loss: 29.4626, MinusLogProbMetric: 29.4626, val_loss: 29.2170, val_MinusLogProbMetric: 29.2170

Epoch 384: val_loss did not improve from 28.96304
196/196 - 35s - loss: 29.4626 - MinusLogProbMetric: 29.4626 - val_loss: 29.2170 - val_MinusLogProbMetric: 29.2170 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 385/1000
2023-10-26 02:17:22.733 
Epoch 385/1000 
	 loss: 29.2343, MinusLogProbMetric: 29.2343, val_loss: 29.5102, val_MinusLogProbMetric: 29.5102

Epoch 385: val_loss did not improve from 28.96304
196/196 - 36s - loss: 29.2343 - MinusLogProbMetric: 29.2343 - val_loss: 29.5102 - val_MinusLogProbMetric: 29.5102 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 386/1000
2023-10-26 02:17:58.526 
Epoch 386/1000 
	 loss: 29.1208, MinusLogProbMetric: 29.1208, val_loss: 29.8067, val_MinusLogProbMetric: 29.8067

Epoch 386: val_loss did not improve from 28.96304
196/196 - 36s - loss: 29.1208 - MinusLogProbMetric: 29.1208 - val_loss: 29.8067 - val_MinusLogProbMetric: 29.8067 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 387/1000
2023-10-26 02:18:34.160 
Epoch 387/1000 
	 loss: 29.1648, MinusLogProbMetric: 29.1648, val_loss: 28.9691, val_MinusLogProbMetric: 28.9691

Epoch 387: val_loss did not improve from 28.96304
196/196 - 36s - loss: 29.1648 - MinusLogProbMetric: 29.1648 - val_loss: 28.9691 - val_MinusLogProbMetric: 28.9691 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 388/1000
2023-10-26 02:19:10.037 
Epoch 388/1000 
	 loss: 29.1732, MinusLogProbMetric: 29.1732, val_loss: 29.0487, val_MinusLogProbMetric: 29.0487

Epoch 388: val_loss did not improve from 28.96304
196/196 - 36s - loss: 29.1732 - MinusLogProbMetric: 29.1732 - val_loss: 29.0487 - val_MinusLogProbMetric: 29.0487 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 389/1000
2023-10-26 02:19:45.900 
Epoch 389/1000 
	 loss: 29.3015, MinusLogProbMetric: 29.3015, val_loss: 29.0370, val_MinusLogProbMetric: 29.0370

Epoch 389: val_loss did not improve from 28.96304
196/196 - 36s - loss: 29.3015 - MinusLogProbMetric: 29.3015 - val_loss: 29.0370 - val_MinusLogProbMetric: 29.0370 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 390/1000
2023-10-26 02:20:21.047 
Epoch 390/1000 
	 loss: 29.1331, MinusLogProbMetric: 29.1331, val_loss: 28.9934, val_MinusLogProbMetric: 28.9934

Epoch 390: val_loss did not improve from 28.96304
196/196 - 35s - loss: 29.1331 - MinusLogProbMetric: 29.1331 - val_loss: 28.9934 - val_MinusLogProbMetric: 28.9934 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 391/1000
2023-10-26 02:20:56.233 
Epoch 391/1000 
	 loss: 29.2124, MinusLogProbMetric: 29.2124, val_loss: 31.0874, val_MinusLogProbMetric: 31.0874

Epoch 391: val_loss did not improve from 28.96304
196/196 - 35s - loss: 29.2124 - MinusLogProbMetric: 29.2124 - val_loss: 31.0874 - val_MinusLogProbMetric: 31.0874 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 392/1000
2023-10-26 02:21:31.831 
Epoch 392/1000 
	 loss: 29.4371, MinusLogProbMetric: 29.4371, val_loss: 29.6636, val_MinusLogProbMetric: 29.6636

Epoch 392: val_loss did not improve from 28.96304
196/196 - 36s - loss: 29.4371 - MinusLogProbMetric: 29.4371 - val_loss: 29.6636 - val_MinusLogProbMetric: 29.6636 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 393/1000
2023-10-26 02:22:07.239 
Epoch 393/1000 
	 loss: 29.0900, MinusLogProbMetric: 29.0900, val_loss: 29.5792, val_MinusLogProbMetric: 29.5792

Epoch 393: val_loss did not improve from 28.96304
196/196 - 35s - loss: 29.0900 - MinusLogProbMetric: 29.0900 - val_loss: 29.5792 - val_MinusLogProbMetric: 29.5792 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 394/1000
2023-10-26 02:22:42.859 
Epoch 394/1000 
	 loss: 29.3209, MinusLogProbMetric: 29.3209, val_loss: 29.1274, val_MinusLogProbMetric: 29.1274

Epoch 394: val_loss did not improve from 28.96304
196/196 - 36s - loss: 29.3209 - MinusLogProbMetric: 29.3209 - val_loss: 29.1274 - val_MinusLogProbMetric: 29.1274 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 395/1000
2023-10-26 02:23:18.472 
Epoch 395/1000 
	 loss: 29.4548, MinusLogProbMetric: 29.4548, val_loss: 30.5671, val_MinusLogProbMetric: 30.5671

Epoch 395: val_loss did not improve from 28.96304
196/196 - 36s - loss: 29.4548 - MinusLogProbMetric: 29.4548 - val_loss: 30.5671 - val_MinusLogProbMetric: 30.5671 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 396/1000
2023-10-26 02:23:54.125 
Epoch 396/1000 
	 loss: 29.1950, MinusLogProbMetric: 29.1950, val_loss: 29.0390, val_MinusLogProbMetric: 29.0390

Epoch 396: val_loss did not improve from 28.96304
196/196 - 36s - loss: 29.1950 - MinusLogProbMetric: 29.1950 - val_loss: 29.0390 - val_MinusLogProbMetric: 29.0390 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 397/1000
2023-10-26 02:24:29.412 
Epoch 397/1000 
	 loss: 29.1757, MinusLogProbMetric: 29.1757, val_loss: 28.9966, val_MinusLogProbMetric: 28.9966

Epoch 397: val_loss did not improve from 28.96304
196/196 - 35s - loss: 29.1757 - MinusLogProbMetric: 29.1757 - val_loss: 28.9966 - val_MinusLogProbMetric: 28.9966 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 398/1000
2023-10-26 02:25:04.883 
Epoch 398/1000 
	 loss: 29.1164, MinusLogProbMetric: 29.1164, val_loss: 30.4983, val_MinusLogProbMetric: 30.4983

Epoch 398: val_loss did not improve from 28.96304
196/196 - 35s - loss: 29.1164 - MinusLogProbMetric: 29.1164 - val_loss: 30.4983 - val_MinusLogProbMetric: 30.4983 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 399/1000
2023-10-26 02:25:40.240 
Epoch 399/1000 
	 loss: 29.4168, MinusLogProbMetric: 29.4168, val_loss: 29.5063, val_MinusLogProbMetric: 29.5063

Epoch 399: val_loss did not improve from 28.96304
196/196 - 35s - loss: 29.4168 - MinusLogProbMetric: 29.4168 - val_loss: 29.5063 - val_MinusLogProbMetric: 29.5063 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 400/1000
2023-10-26 02:26:16.210 
Epoch 400/1000 
	 loss: 29.2748, MinusLogProbMetric: 29.2748, val_loss: 29.1874, val_MinusLogProbMetric: 29.1874

Epoch 400: val_loss did not improve from 28.96304
196/196 - 36s - loss: 29.2748 - MinusLogProbMetric: 29.2748 - val_loss: 29.1874 - val_MinusLogProbMetric: 29.1874 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 401/1000
2023-10-26 02:26:51.482 
Epoch 401/1000 
	 loss: 29.3627, MinusLogProbMetric: 29.3627, val_loss: 30.0650, val_MinusLogProbMetric: 30.0650

Epoch 401: val_loss did not improve from 28.96304
196/196 - 35s - loss: 29.3627 - MinusLogProbMetric: 29.3627 - val_loss: 30.0650 - val_MinusLogProbMetric: 30.0650 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 402/1000
2023-10-26 02:27:27.185 
Epoch 402/1000 
	 loss: 29.2443, MinusLogProbMetric: 29.2443, val_loss: 29.1891, val_MinusLogProbMetric: 29.1891

Epoch 402: val_loss did not improve from 28.96304
196/196 - 36s - loss: 29.2443 - MinusLogProbMetric: 29.2443 - val_loss: 29.1891 - val_MinusLogProbMetric: 29.1891 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 403/1000
2023-10-26 02:28:02.781 
Epoch 403/1000 
	 loss: 29.3202, MinusLogProbMetric: 29.3202, val_loss: 29.6357, val_MinusLogProbMetric: 29.6357

Epoch 403: val_loss did not improve from 28.96304
196/196 - 36s - loss: 29.3202 - MinusLogProbMetric: 29.3202 - val_loss: 29.6357 - val_MinusLogProbMetric: 29.6357 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 404/1000
2023-10-26 02:28:38.538 
Epoch 404/1000 
	 loss: 29.1891, MinusLogProbMetric: 29.1891, val_loss: 29.3214, val_MinusLogProbMetric: 29.3214

Epoch 404: val_loss did not improve from 28.96304
196/196 - 36s - loss: 29.1891 - MinusLogProbMetric: 29.1891 - val_loss: 29.3214 - val_MinusLogProbMetric: 29.3214 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 405/1000
2023-10-26 02:29:14.046 
Epoch 405/1000 
	 loss: 29.1137, MinusLogProbMetric: 29.1137, val_loss: 29.4554, val_MinusLogProbMetric: 29.4554

Epoch 405: val_loss did not improve from 28.96304
196/196 - 36s - loss: 29.1137 - MinusLogProbMetric: 29.1137 - val_loss: 29.4554 - val_MinusLogProbMetric: 29.4554 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 406/1000
2023-10-26 02:29:49.972 
Epoch 406/1000 
	 loss: 29.3271, MinusLogProbMetric: 29.3271, val_loss: 29.2191, val_MinusLogProbMetric: 29.2191

Epoch 406: val_loss did not improve from 28.96304
196/196 - 36s - loss: 29.3271 - MinusLogProbMetric: 29.3271 - val_loss: 29.2191 - val_MinusLogProbMetric: 29.2191 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 407/1000
2023-10-26 02:30:25.197 
Epoch 407/1000 
	 loss: 28.4033, MinusLogProbMetric: 28.4033, val_loss: 28.6141, val_MinusLogProbMetric: 28.6141

Epoch 407: val_loss improved from 28.96304 to 28.61410, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_369/weights/best_weights.h5
196/196 - 36s - loss: 28.4033 - MinusLogProbMetric: 28.4033 - val_loss: 28.6141 - val_MinusLogProbMetric: 28.6141 - lr: 5.0000e-04 - 36s/epoch - 184ms/step
Epoch 408/1000
2023-10-26 02:31:01.989 
Epoch 408/1000 
	 loss: 28.3363, MinusLogProbMetric: 28.3363, val_loss: 28.5967, val_MinusLogProbMetric: 28.5967

Epoch 408: val_loss improved from 28.61410 to 28.59675, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_369/weights/best_weights.h5
196/196 - 37s - loss: 28.3363 - MinusLogProbMetric: 28.3363 - val_loss: 28.5967 - val_MinusLogProbMetric: 28.5967 - lr: 5.0000e-04 - 37s/epoch - 187ms/step
Epoch 409/1000
2023-10-26 02:31:38.658 
Epoch 409/1000 
	 loss: 28.4312, MinusLogProbMetric: 28.4312, val_loss: 28.8614, val_MinusLogProbMetric: 28.8614

Epoch 409: val_loss did not improve from 28.59675
196/196 - 36s - loss: 28.4312 - MinusLogProbMetric: 28.4312 - val_loss: 28.8614 - val_MinusLogProbMetric: 28.8614 - lr: 5.0000e-04 - 36s/epoch - 183ms/step
Epoch 410/1000
2023-10-26 02:32:13.955 
Epoch 410/1000 
	 loss: 28.3971, MinusLogProbMetric: 28.3971, val_loss: 28.4952, val_MinusLogProbMetric: 28.4952

Epoch 410: val_loss improved from 28.59675 to 28.49523, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_369/weights/best_weights.h5
196/196 - 36s - loss: 28.3971 - MinusLogProbMetric: 28.3971 - val_loss: 28.4952 - val_MinusLogProbMetric: 28.4952 - lr: 5.0000e-04 - 36s/epoch - 183ms/step
Epoch 411/1000
2023-10-26 02:32:50.363 
Epoch 411/1000 
	 loss: 28.3723, MinusLogProbMetric: 28.3723, val_loss: 28.5406, val_MinusLogProbMetric: 28.5406

Epoch 411: val_loss did not improve from 28.49523
196/196 - 36s - loss: 28.3723 - MinusLogProbMetric: 28.3723 - val_loss: 28.5406 - val_MinusLogProbMetric: 28.5406 - lr: 5.0000e-04 - 36s/epoch - 183ms/step
Epoch 412/1000
2023-10-26 02:33:26.544 
Epoch 412/1000 
	 loss: 28.4459, MinusLogProbMetric: 28.4459, val_loss: 28.5564, val_MinusLogProbMetric: 28.5564

Epoch 412: val_loss did not improve from 28.49523
196/196 - 36s - loss: 28.4459 - MinusLogProbMetric: 28.4459 - val_loss: 28.5564 - val_MinusLogProbMetric: 28.5564 - lr: 5.0000e-04 - 36s/epoch - 185ms/step
Epoch 413/1000
2023-10-26 02:34:01.925 
Epoch 413/1000 
	 loss: 28.4239, MinusLogProbMetric: 28.4239, val_loss: 28.6155, val_MinusLogProbMetric: 28.6155

Epoch 413: val_loss did not improve from 28.49523
196/196 - 35s - loss: 28.4239 - MinusLogProbMetric: 28.4239 - val_loss: 28.6155 - val_MinusLogProbMetric: 28.6155 - lr: 5.0000e-04 - 35s/epoch - 180ms/step
Epoch 414/1000
2023-10-26 02:34:37.601 
Epoch 414/1000 
	 loss: 28.5432, MinusLogProbMetric: 28.5432, val_loss: 29.0291, val_MinusLogProbMetric: 29.0291

Epoch 414: val_loss did not improve from 28.49523
196/196 - 36s - loss: 28.5432 - MinusLogProbMetric: 28.5432 - val_loss: 29.0291 - val_MinusLogProbMetric: 29.0291 - lr: 5.0000e-04 - 36s/epoch - 182ms/step
Epoch 415/1000
2023-10-26 02:35:13.271 
Epoch 415/1000 
	 loss: 28.4526, MinusLogProbMetric: 28.4526, val_loss: 29.7876, val_MinusLogProbMetric: 29.7876

Epoch 415: val_loss did not improve from 28.49523
196/196 - 36s - loss: 28.4526 - MinusLogProbMetric: 28.4526 - val_loss: 29.7876 - val_MinusLogProbMetric: 29.7876 - lr: 5.0000e-04 - 36s/epoch - 182ms/step
Epoch 416/1000
2023-10-26 02:35:48.732 
Epoch 416/1000 
	 loss: 28.4654, MinusLogProbMetric: 28.4654, val_loss: 28.7204, val_MinusLogProbMetric: 28.7204

Epoch 416: val_loss did not improve from 28.49523
196/196 - 35s - loss: 28.4654 - MinusLogProbMetric: 28.4654 - val_loss: 28.7204 - val_MinusLogProbMetric: 28.7204 - lr: 5.0000e-04 - 35s/epoch - 181ms/step
Epoch 417/1000
2023-10-26 02:36:24.182 
Epoch 417/1000 
	 loss: 28.3856, MinusLogProbMetric: 28.3856, val_loss: 28.4730, val_MinusLogProbMetric: 28.4730

Epoch 417: val_loss improved from 28.49523 to 28.47298, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_369/weights/best_weights.h5
196/196 - 36s - loss: 28.3856 - MinusLogProbMetric: 28.3856 - val_loss: 28.4730 - val_MinusLogProbMetric: 28.4730 - lr: 5.0000e-04 - 36s/epoch - 185ms/step
Epoch 418/1000
2023-10-26 02:37:00.546 
Epoch 418/1000 
	 loss: 28.6179, MinusLogProbMetric: 28.6179, val_loss: 29.0053, val_MinusLogProbMetric: 29.0053

Epoch 418: val_loss did not improve from 28.47298
196/196 - 36s - loss: 28.6179 - MinusLogProbMetric: 28.6179 - val_loss: 29.0053 - val_MinusLogProbMetric: 29.0053 - lr: 5.0000e-04 - 36s/epoch - 182ms/step
Epoch 419/1000
2023-10-26 02:37:36.376 
Epoch 419/1000 
	 loss: 28.3225, MinusLogProbMetric: 28.3225, val_loss: 28.4596, val_MinusLogProbMetric: 28.4596

Epoch 419: val_loss improved from 28.47298 to 28.45963, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_369/weights/best_weights.h5
196/196 - 37s - loss: 28.3225 - MinusLogProbMetric: 28.3225 - val_loss: 28.4596 - val_MinusLogProbMetric: 28.4596 - lr: 5.0000e-04 - 37s/epoch - 187ms/step
Epoch 420/1000
2023-10-26 02:38:12.616 
Epoch 420/1000 
	 loss: 28.6996, MinusLogProbMetric: 28.6996, val_loss: 29.4756, val_MinusLogProbMetric: 29.4756

Epoch 420: val_loss did not improve from 28.45963
196/196 - 35s - loss: 28.6996 - MinusLogProbMetric: 28.6996 - val_loss: 29.4756 - val_MinusLogProbMetric: 29.4756 - lr: 5.0000e-04 - 35s/epoch - 181ms/step
Epoch 421/1000
2023-10-26 02:38:47.989 
Epoch 421/1000 
	 loss: 28.5605, MinusLogProbMetric: 28.5605, val_loss: 28.9267, val_MinusLogProbMetric: 28.9267

Epoch 421: val_loss did not improve from 28.45963
196/196 - 35s - loss: 28.5605 - MinusLogProbMetric: 28.5605 - val_loss: 28.9267 - val_MinusLogProbMetric: 28.9267 - lr: 5.0000e-04 - 35s/epoch - 180ms/step
Epoch 422/1000
2023-10-26 02:39:23.535 
Epoch 422/1000 
	 loss: 28.4511, MinusLogProbMetric: 28.4511, val_loss: 28.3933, val_MinusLogProbMetric: 28.3933

Epoch 422: val_loss improved from 28.45963 to 28.39330, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_369/weights/best_weights.h5
196/196 - 36s - loss: 28.4511 - MinusLogProbMetric: 28.4511 - val_loss: 28.3933 - val_MinusLogProbMetric: 28.3933 - lr: 5.0000e-04 - 36s/epoch - 185ms/step
Epoch 423/1000
2023-10-26 02:39:59.941 
Epoch 423/1000 
	 loss: 28.4476, MinusLogProbMetric: 28.4476, val_loss: 28.4441, val_MinusLogProbMetric: 28.4441

Epoch 423: val_loss did not improve from 28.39330
196/196 - 36s - loss: 28.4476 - MinusLogProbMetric: 28.4476 - val_loss: 28.4441 - val_MinusLogProbMetric: 28.4441 - lr: 5.0000e-04 - 36s/epoch - 182ms/step
Epoch 424/1000
2023-10-26 02:40:35.426 
Epoch 424/1000 
	 loss: 28.3871, MinusLogProbMetric: 28.3871, val_loss: 28.8656, val_MinusLogProbMetric: 28.8656

Epoch 424: val_loss did not improve from 28.39330
196/196 - 35s - loss: 28.3871 - MinusLogProbMetric: 28.3871 - val_loss: 28.8656 - val_MinusLogProbMetric: 28.8656 - lr: 5.0000e-04 - 35s/epoch - 181ms/step
Epoch 425/1000
2023-10-26 02:41:11.560 
Epoch 425/1000 
	 loss: 28.5587, MinusLogProbMetric: 28.5587, val_loss: 28.5184, val_MinusLogProbMetric: 28.5184

Epoch 425: val_loss did not improve from 28.39330
196/196 - 36s - loss: 28.5587 - MinusLogProbMetric: 28.5587 - val_loss: 28.5184 - val_MinusLogProbMetric: 28.5184 - lr: 5.0000e-04 - 36s/epoch - 184ms/step
Epoch 426/1000
2023-10-26 02:41:47.104 
Epoch 426/1000 
	 loss: 28.5374, MinusLogProbMetric: 28.5374, val_loss: 29.2196, val_MinusLogProbMetric: 29.2196

Epoch 426: val_loss did not improve from 28.39330
196/196 - 36s - loss: 28.5374 - MinusLogProbMetric: 28.5374 - val_loss: 29.2196 - val_MinusLogProbMetric: 29.2196 - lr: 5.0000e-04 - 36s/epoch - 181ms/step
Epoch 427/1000
2023-10-26 02:42:23.127 
Epoch 427/1000 
	 loss: 28.4101, MinusLogProbMetric: 28.4101, val_loss: 28.5043, val_MinusLogProbMetric: 28.5043

Epoch 427: val_loss did not improve from 28.39330
196/196 - 36s - loss: 28.4101 - MinusLogProbMetric: 28.4101 - val_loss: 28.5043 - val_MinusLogProbMetric: 28.5043 - lr: 5.0000e-04 - 36s/epoch - 184ms/step
Epoch 428/1000
2023-10-26 02:42:58.922 
Epoch 428/1000 
	 loss: 28.4774, MinusLogProbMetric: 28.4774, val_loss: 28.6661, val_MinusLogProbMetric: 28.6661

Epoch 428: val_loss did not improve from 28.39330
196/196 - 36s - loss: 28.4774 - MinusLogProbMetric: 28.4774 - val_loss: 28.6661 - val_MinusLogProbMetric: 28.6661 - lr: 5.0000e-04 - 36s/epoch - 183ms/step
Epoch 429/1000
2023-10-26 02:43:34.342 
Epoch 429/1000 
	 loss: 28.4191, MinusLogProbMetric: 28.4191, val_loss: 28.7668, val_MinusLogProbMetric: 28.7668

Epoch 429: val_loss did not improve from 28.39330
196/196 - 35s - loss: 28.4191 - MinusLogProbMetric: 28.4191 - val_loss: 28.7668 - val_MinusLogProbMetric: 28.7668 - lr: 5.0000e-04 - 35s/epoch - 181ms/step
Epoch 430/1000
2023-10-26 02:44:09.977 
Epoch 430/1000 
	 loss: 28.5003, MinusLogProbMetric: 28.5003, val_loss: 28.9925, val_MinusLogProbMetric: 28.9925

Epoch 430: val_loss did not improve from 28.39330
196/196 - 36s - loss: 28.5003 - MinusLogProbMetric: 28.5003 - val_loss: 28.9925 - val_MinusLogProbMetric: 28.9925 - lr: 5.0000e-04 - 36s/epoch - 182ms/step
Epoch 431/1000
2023-10-26 02:44:45.371 
Epoch 431/1000 
	 loss: 28.3659, MinusLogProbMetric: 28.3659, val_loss: 28.6686, val_MinusLogProbMetric: 28.6686

Epoch 431: val_loss did not improve from 28.39330
196/196 - 35s - loss: 28.3659 - MinusLogProbMetric: 28.3659 - val_loss: 28.6686 - val_MinusLogProbMetric: 28.6686 - lr: 5.0000e-04 - 35s/epoch - 181ms/step
Epoch 432/1000
2023-10-26 02:45:21.024 
Epoch 432/1000 
	 loss: 28.4107, MinusLogProbMetric: 28.4107, val_loss: 29.3599, val_MinusLogProbMetric: 29.3599

Epoch 432: val_loss did not improve from 28.39330
196/196 - 36s - loss: 28.4107 - MinusLogProbMetric: 28.4107 - val_loss: 29.3599 - val_MinusLogProbMetric: 29.3599 - lr: 5.0000e-04 - 36s/epoch - 182ms/step
Epoch 433/1000
2023-10-26 02:45:56.490 
Epoch 433/1000 
	 loss: 28.5166, MinusLogProbMetric: 28.5166, val_loss: 28.7597, val_MinusLogProbMetric: 28.7597

Epoch 433: val_loss did not improve from 28.39330
196/196 - 35s - loss: 28.5166 - MinusLogProbMetric: 28.5166 - val_loss: 28.7597 - val_MinusLogProbMetric: 28.7597 - lr: 5.0000e-04 - 35s/epoch - 181ms/step
Epoch 434/1000
2023-10-26 02:46:32.383 
Epoch 434/1000 
	 loss: 28.3224, MinusLogProbMetric: 28.3224, val_loss: 28.8047, val_MinusLogProbMetric: 28.8047

Epoch 434: val_loss did not improve from 28.39330
196/196 - 36s - loss: 28.3224 - MinusLogProbMetric: 28.3224 - val_loss: 28.8047 - val_MinusLogProbMetric: 28.8047 - lr: 5.0000e-04 - 36s/epoch - 183ms/step
Epoch 435/1000
2023-10-26 02:47:07.946 
Epoch 435/1000 
	 loss: 28.5537, MinusLogProbMetric: 28.5537, val_loss: 28.5133, val_MinusLogProbMetric: 28.5133

Epoch 435: val_loss did not improve from 28.39330
196/196 - 36s - loss: 28.5537 - MinusLogProbMetric: 28.5537 - val_loss: 28.5133 - val_MinusLogProbMetric: 28.5133 - lr: 5.0000e-04 - 36s/epoch - 181ms/step
Epoch 436/1000
2023-10-26 02:47:43.655 
Epoch 436/1000 
	 loss: 28.5430, MinusLogProbMetric: 28.5430, val_loss: 29.0692, val_MinusLogProbMetric: 29.0692

Epoch 436: val_loss did not improve from 28.39330
196/196 - 36s - loss: 28.5430 - MinusLogProbMetric: 28.5430 - val_loss: 29.0692 - val_MinusLogProbMetric: 29.0692 - lr: 5.0000e-04 - 36s/epoch - 182ms/step
Epoch 437/1000
2023-10-26 02:48:19.023 
Epoch 437/1000 
	 loss: 28.6567, MinusLogProbMetric: 28.6567, val_loss: 28.9106, val_MinusLogProbMetric: 28.9106

Epoch 437: val_loss did not improve from 28.39330
196/196 - 35s - loss: 28.6567 - MinusLogProbMetric: 28.6567 - val_loss: 28.9106 - val_MinusLogProbMetric: 28.9106 - lr: 5.0000e-04 - 35s/epoch - 180ms/step
Epoch 438/1000
2023-10-26 02:48:54.545 
Epoch 438/1000 
	 loss: 28.5640, MinusLogProbMetric: 28.5640, val_loss: 28.5121, val_MinusLogProbMetric: 28.5121

Epoch 438: val_loss did not improve from 28.39330
196/196 - 36s - loss: 28.5640 - MinusLogProbMetric: 28.5640 - val_loss: 28.5121 - val_MinusLogProbMetric: 28.5121 - lr: 5.0000e-04 - 36s/epoch - 181ms/step
Epoch 439/1000
2023-10-26 02:49:29.650 
Epoch 439/1000 
	 loss: 28.5351, MinusLogProbMetric: 28.5351, val_loss: 28.4026, val_MinusLogProbMetric: 28.4026

Epoch 439: val_loss did not improve from 28.39330
196/196 - 35s - loss: 28.5351 - MinusLogProbMetric: 28.5351 - val_loss: 28.4026 - val_MinusLogProbMetric: 28.4026 - lr: 5.0000e-04 - 35s/epoch - 179ms/step
Epoch 440/1000
2023-10-26 02:50:05.100 
Epoch 440/1000 
	 loss: 28.5098, MinusLogProbMetric: 28.5098, val_loss: 28.4748, val_MinusLogProbMetric: 28.4748

Epoch 440: val_loss did not improve from 28.39330
196/196 - 35s - loss: 28.5098 - MinusLogProbMetric: 28.5098 - val_loss: 28.4748 - val_MinusLogProbMetric: 28.4748 - lr: 5.0000e-04 - 35s/epoch - 181ms/step
Epoch 441/1000
2023-10-26 02:50:40.322 
Epoch 441/1000 
	 loss: 28.3485, MinusLogProbMetric: 28.3485, val_loss: 28.6287, val_MinusLogProbMetric: 28.6287

Epoch 441: val_loss did not improve from 28.39330
196/196 - 35s - loss: 28.3485 - MinusLogProbMetric: 28.3485 - val_loss: 28.6287 - val_MinusLogProbMetric: 28.6287 - lr: 5.0000e-04 - 35s/epoch - 180ms/step
Epoch 442/1000
2023-10-26 02:51:15.851 
Epoch 442/1000 
	 loss: 28.3796, MinusLogProbMetric: 28.3796, val_loss: 28.5340, val_MinusLogProbMetric: 28.5340

Epoch 442: val_loss did not improve from 28.39330
196/196 - 36s - loss: 28.3796 - MinusLogProbMetric: 28.3796 - val_loss: 28.5340 - val_MinusLogProbMetric: 28.5340 - lr: 5.0000e-04 - 36s/epoch - 181ms/step
Epoch 443/1000
2023-10-26 02:51:51.452 
Epoch 443/1000 
	 loss: 28.6316, MinusLogProbMetric: 28.6316, val_loss: 28.9629, val_MinusLogProbMetric: 28.9629

Epoch 443: val_loss did not improve from 28.39330
196/196 - 36s - loss: 28.6316 - MinusLogProbMetric: 28.6316 - val_loss: 28.9629 - val_MinusLogProbMetric: 28.9629 - lr: 5.0000e-04 - 36s/epoch - 182ms/step
Epoch 444/1000
2023-10-26 02:52:26.697 
Epoch 444/1000 
	 loss: 28.4524, MinusLogProbMetric: 28.4524, val_loss: 28.5368, val_MinusLogProbMetric: 28.5368

Epoch 444: val_loss did not improve from 28.39330
196/196 - 35s - loss: 28.4524 - MinusLogProbMetric: 28.4524 - val_loss: 28.5368 - val_MinusLogProbMetric: 28.5368 - lr: 5.0000e-04 - 35s/epoch - 180ms/step
Epoch 445/1000
2023-10-26 02:53:02.159 
Epoch 445/1000 
	 loss: 28.4215, MinusLogProbMetric: 28.4215, val_loss: 28.4972, val_MinusLogProbMetric: 28.4972

Epoch 445: val_loss did not improve from 28.39330
196/196 - 35s - loss: 28.4215 - MinusLogProbMetric: 28.4215 - val_loss: 28.4972 - val_MinusLogProbMetric: 28.4972 - lr: 5.0000e-04 - 35s/epoch - 181ms/step
Epoch 446/1000
2023-10-26 02:53:37.826 
Epoch 446/1000 
	 loss: 28.6052, MinusLogProbMetric: 28.6052, val_loss: 28.9455, val_MinusLogProbMetric: 28.9455

Epoch 446: val_loss did not improve from 28.39330
196/196 - 36s - loss: 28.6052 - MinusLogProbMetric: 28.6052 - val_loss: 28.9455 - val_MinusLogProbMetric: 28.9455 - lr: 5.0000e-04 - 36s/epoch - 182ms/step
Epoch 447/1000
2023-10-26 02:54:13.243 
Epoch 447/1000 
	 loss: 28.5312, MinusLogProbMetric: 28.5312, val_loss: 28.6114, val_MinusLogProbMetric: 28.6114

Epoch 447: val_loss did not improve from 28.39330
196/196 - 35s - loss: 28.5312 - MinusLogProbMetric: 28.5312 - val_loss: 28.6114 - val_MinusLogProbMetric: 28.6114 - lr: 5.0000e-04 - 35s/epoch - 181ms/step
Epoch 448/1000
2023-10-26 02:54:48.655 
Epoch 448/1000 
	 loss: 28.3125, MinusLogProbMetric: 28.3125, val_loss: 28.6938, val_MinusLogProbMetric: 28.6938

Epoch 448: val_loss did not improve from 28.39330
196/196 - 35s - loss: 28.3125 - MinusLogProbMetric: 28.3125 - val_loss: 28.6938 - val_MinusLogProbMetric: 28.6938 - lr: 5.0000e-04 - 35s/epoch - 181ms/step
Epoch 449/1000
2023-10-26 02:55:24.095 
Epoch 449/1000 
	 loss: 28.5637, MinusLogProbMetric: 28.5637, val_loss: 28.9974, val_MinusLogProbMetric: 28.9974

Epoch 449: val_loss did not improve from 28.39330
196/196 - 35s - loss: 28.5637 - MinusLogProbMetric: 28.5637 - val_loss: 28.9974 - val_MinusLogProbMetric: 28.9974 - lr: 5.0000e-04 - 35s/epoch - 181ms/step
Epoch 450/1000
2023-10-26 02:55:59.509 
Epoch 450/1000 
	 loss: 28.3542, MinusLogProbMetric: 28.3542, val_loss: 28.7675, val_MinusLogProbMetric: 28.7675

Epoch 450: val_loss did not improve from 28.39330
196/196 - 35s - loss: 28.3542 - MinusLogProbMetric: 28.3542 - val_loss: 28.7675 - val_MinusLogProbMetric: 28.7675 - lr: 5.0000e-04 - 35s/epoch - 181ms/step
Epoch 451/1000
2023-10-26 02:56:34.597 
Epoch 451/1000 
	 loss: 28.4366, MinusLogProbMetric: 28.4366, val_loss: 29.9576, val_MinusLogProbMetric: 29.9576

Epoch 451: val_loss did not improve from 28.39330
196/196 - 35s - loss: 28.4366 - MinusLogProbMetric: 28.4366 - val_loss: 29.9576 - val_MinusLogProbMetric: 29.9576 - lr: 5.0000e-04 - 35s/epoch - 179ms/step
Epoch 452/1000
2023-10-26 02:57:09.877 
Epoch 452/1000 
	 loss: 28.4280, MinusLogProbMetric: 28.4280, val_loss: 28.6322, val_MinusLogProbMetric: 28.6322

Epoch 452: val_loss did not improve from 28.39330
196/196 - 35s - loss: 28.4280 - MinusLogProbMetric: 28.4280 - val_loss: 28.6322 - val_MinusLogProbMetric: 28.6322 - lr: 5.0000e-04 - 35s/epoch - 180ms/step
Epoch 453/1000
2023-10-26 02:57:45.053 
Epoch 453/1000 
	 loss: 28.3109, MinusLogProbMetric: 28.3109, val_loss: 28.8280, val_MinusLogProbMetric: 28.8280

Epoch 453: val_loss did not improve from 28.39330
196/196 - 35s - loss: 28.3109 - MinusLogProbMetric: 28.3109 - val_loss: 28.8280 - val_MinusLogProbMetric: 28.8280 - lr: 5.0000e-04 - 35s/epoch - 179ms/step
Epoch 454/1000
2023-10-26 02:58:20.575 
Epoch 454/1000 
	 loss: 28.3760, MinusLogProbMetric: 28.3760, val_loss: 28.4991, val_MinusLogProbMetric: 28.4991

Epoch 454: val_loss did not improve from 28.39330
196/196 - 36s - loss: 28.3760 - MinusLogProbMetric: 28.3760 - val_loss: 28.4991 - val_MinusLogProbMetric: 28.4991 - lr: 5.0000e-04 - 36s/epoch - 181ms/step
Epoch 455/1000
2023-10-26 02:58:55.887 
Epoch 455/1000 
	 loss: 28.5251, MinusLogProbMetric: 28.5251, val_loss: 28.4179, val_MinusLogProbMetric: 28.4179

Epoch 455: val_loss did not improve from 28.39330
196/196 - 35s - loss: 28.5251 - MinusLogProbMetric: 28.5251 - val_loss: 28.4179 - val_MinusLogProbMetric: 28.4179 - lr: 5.0000e-04 - 35s/epoch - 180ms/step
Epoch 456/1000
2023-10-26 02:59:31.285 
Epoch 456/1000 
	 loss: 28.3544, MinusLogProbMetric: 28.3544, val_loss: 28.8098, val_MinusLogProbMetric: 28.8098

Epoch 456: val_loss did not improve from 28.39330
196/196 - 35s - loss: 28.3544 - MinusLogProbMetric: 28.3544 - val_loss: 28.8098 - val_MinusLogProbMetric: 28.8098 - lr: 5.0000e-04 - 35s/epoch - 181ms/step
Epoch 457/1000
2023-10-26 03:00:06.670 
Epoch 457/1000 
	 loss: 28.4661, MinusLogProbMetric: 28.4661, val_loss: 28.5687, val_MinusLogProbMetric: 28.5687

Epoch 457: val_loss did not improve from 28.39330
196/196 - 35s - loss: 28.4661 - MinusLogProbMetric: 28.4661 - val_loss: 28.5687 - val_MinusLogProbMetric: 28.5687 - lr: 5.0000e-04 - 35s/epoch - 181ms/step
Epoch 458/1000
2023-10-26 03:00:41.828 
Epoch 458/1000 
	 loss: 28.5343, MinusLogProbMetric: 28.5343, val_loss: 28.5509, val_MinusLogProbMetric: 28.5509

Epoch 458: val_loss did not improve from 28.39330
196/196 - 35s - loss: 28.5343 - MinusLogProbMetric: 28.5343 - val_loss: 28.5509 - val_MinusLogProbMetric: 28.5509 - lr: 5.0000e-04 - 35s/epoch - 179ms/step
Epoch 459/1000
2023-10-26 03:01:17.363 
Epoch 459/1000 
	 loss: 28.4098, MinusLogProbMetric: 28.4098, val_loss: 28.5119, val_MinusLogProbMetric: 28.5119

Epoch 459: val_loss did not improve from 28.39330
196/196 - 36s - loss: 28.4098 - MinusLogProbMetric: 28.4098 - val_loss: 28.5119 - val_MinusLogProbMetric: 28.5119 - lr: 5.0000e-04 - 36s/epoch - 181ms/step
Epoch 460/1000
2023-10-26 03:01:52.985 
Epoch 460/1000 
	 loss: 28.5339, MinusLogProbMetric: 28.5339, val_loss: 28.9492, val_MinusLogProbMetric: 28.9492

Epoch 460: val_loss did not improve from 28.39330
196/196 - 36s - loss: 28.5339 - MinusLogProbMetric: 28.5339 - val_loss: 28.9492 - val_MinusLogProbMetric: 28.9492 - lr: 5.0000e-04 - 36s/epoch - 182ms/step
Epoch 461/1000
2023-10-26 03:02:28.379 
Epoch 461/1000 
	 loss: 28.7043, MinusLogProbMetric: 28.7043, val_loss: 28.7676, val_MinusLogProbMetric: 28.7676

Epoch 461: val_loss did not improve from 28.39330
196/196 - 35s - loss: 28.7043 - MinusLogProbMetric: 28.7043 - val_loss: 28.7676 - val_MinusLogProbMetric: 28.7676 - lr: 5.0000e-04 - 35s/epoch - 181ms/step
Epoch 462/1000
2023-10-26 03:03:04.187 
Epoch 462/1000 
	 loss: 28.4105, MinusLogProbMetric: 28.4105, val_loss: 28.6704, val_MinusLogProbMetric: 28.6704

Epoch 462: val_loss did not improve from 28.39330
196/196 - 36s - loss: 28.4105 - MinusLogProbMetric: 28.4105 - val_loss: 28.6704 - val_MinusLogProbMetric: 28.6704 - lr: 5.0000e-04 - 36s/epoch - 183ms/step
Epoch 463/1000
2023-10-26 03:03:39.784 
Epoch 463/1000 
	 loss: 28.3653, MinusLogProbMetric: 28.3653, val_loss: 28.5253, val_MinusLogProbMetric: 28.5253

Epoch 463: val_loss did not improve from 28.39330
196/196 - 36s - loss: 28.3653 - MinusLogProbMetric: 28.3653 - val_loss: 28.5253 - val_MinusLogProbMetric: 28.5253 - lr: 5.0000e-04 - 36s/epoch - 182ms/step
Epoch 464/1000
2023-10-26 03:04:15.016 
Epoch 464/1000 
	 loss: 28.4203, MinusLogProbMetric: 28.4203, val_loss: 28.4291, val_MinusLogProbMetric: 28.4291

Epoch 464: val_loss did not improve from 28.39330
196/196 - 35s - loss: 28.4203 - MinusLogProbMetric: 28.4203 - val_loss: 28.4291 - val_MinusLogProbMetric: 28.4291 - lr: 5.0000e-04 - 35s/epoch - 180ms/step
Epoch 465/1000
2023-10-26 03:04:50.299 
Epoch 465/1000 
	 loss: 28.5306, MinusLogProbMetric: 28.5306, val_loss: 28.6698, val_MinusLogProbMetric: 28.6698

Epoch 465: val_loss did not improve from 28.39330
196/196 - 35s - loss: 28.5306 - MinusLogProbMetric: 28.5306 - val_loss: 28.6698 - val_MinusLogProbMetric: 28.6698 - lr: 5.0000e-04 - 35s/epoch - 180ms/step
Epoch 466/1000
2023-10-26 03:05:26.180 
Epoch 466/1000 
	 loss: 28.5407, MinusLogProbMetric: 28.5407, val_loss: 29.5359, val_MinusLogProbMetric: 29.5359

Epoch 466: val_loss did not improve from 28.39330
196/196 - 36s - loss: 28.5407 - MinusLogProbMetric: 28.5407 - val_loss: 29.5359 - val_MinusLogProbMetric: 29.5359 - lr: 5.0000e-04 - 36s/epoch - 183ms/step
Epoch 467/1000
2023-10-26 03:06:01.121 
Epoch 467/1000 
	 loss: 28.5591, MinusLogProbMetric: 28.5591, val_loss: 28.4894, val_MinusLogProbMetric: 28.4894

Epoch 467: val_loss did not improve from 28.39330
196/196 - 35s - loss: 28.5591 - MinusLogProbMetric: 28.5591 - val_loss: 28.4894 - val_MinusLogProbMetric: 28.4894 - lr: 5.0000e-04 - 35s/epoch - 178ms/step
Epoch 468/1000
2023-10-26 03:06:36.642 
Epoch 468/1000 
	 loss: 28.4069, MinusLogProbMetric: 28.4069, val_loss: 28.4566, val_MinusLogProbMetric: 28.4566

Epoch 468: val_loss did not improve from 28.39330
196/196 - 36s - loss: 28.4069 - MinusLogProbMetric: 28.4069 - val_loss: 28.4566 - val_MinusLogProbMetric: 28.4566 - lr: 5.0000e-04 - 36s/epoch - 181ms/step
Epoch 469/1000
2023-10-26 03:07:12.120 
Epoch 469/1000 
	 loss: 28.4154, MinusLogProbMetric: 28.4154, val_loss: 28.3747, val_MinusLogProbMetric: 28.3747

Epoch 469: val_loss improved from 28.39330 to 28.37474, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_369/weights/best_weights.h5
196/196 - 36s - loss: 28.4154 - MinusLogProbMetric: 28.4154 - val_loss: 28.3747 - val_MinusLogProbMetric: 28.3747 - lr: 5.0000e-04 - 36s/epoch - 185ms/step
Epoch 470/1000
2023-10-26 03:07:48.860 
Epoch 470/1000 
	 loss: 28.3708, MinusLogProbMetric: 28.3708, val_loss: 28.9705, val_MinusLogProbMetric: 28.9705

Epoch 470: val_loss did not improve from 28.37474
196/196 - 36s - loss: 28.3708 - MinusLogProbMetric: 28.3708 - val_loss: 28.9705 - val_MinusLogProbMetric: 28.9705 - lr: 5.0000e-04 - 36s/epoch - 184ms/step
Epoch 471/1000
2023-10-26 03:08:24.627 
Epoch 471/1000 
	 loss: 28.6792, MinusLogProbMetric: 28.6792, val_loss: 28.9351, val_MinusLogProbMetric: 28.9351

Epoch 471: val_loss did not improve from 28.37474
196/196 - 36s - loss: 28.6792 - MinusLogProbMetric: 28.6792 - val_loss: 28.9351 - val_MinusLogProbMetric: 28.9351 - lr: 5.0000e-04 - 36s/epoch - 182ms/step
Epoch 472/1000
2023-10-26 03:08:59.667 
Epoch 472/1000 
	 loss: 28.3084, MinusLogProbMetric: 28.3084, val_loss: 28.4238, val_MinusLogProbMetric: 28.4238

Epoch 472: val_loss did not improve from 28.37474
196/196 - 35s - loss: 28.3084 - MinusLogProbMetric: 28.3084 - val_loss: 28.4238 - val_MinusLogProbMetric: 28.4238 - lr: 5.0000e-04 - 35s/epoch - 179ms/step
Epoch 473/1000
2023-10-26 03:09:34.274 
Epoch 473/1000 
	 loss: 28.5415, MinusLogProbMetric: 28.5415, val_loss: 28.8778, val_MinusLogProbMetric: 28.8778

Epoch 473: val_loss did not improve from 28.37474
196/196 - 35s - loss: 28.5415 - MinusLogProbMetric: 28.5415 - val_loss: 28.8778 - val_MinusLogProbMetric: 28.8778 - lr: 5.0000e-04 - 35s/epoch - 177ms/step
Epoch 474/1000
2023-10-26 03:10:08.091 
Epoch 474/1000 
	 loss: 28.4747, MinusLogProbMetric: 28.4747, val_loss: 28.6452, val_MinusLogProbMetric: 28.6452

Epoch 474: val_loss did not improve from 28.37474
196/196 - 34s - loss: 28.4747 - MinusLogProbMetric: 28.4747 - val_loss: 28.6452 - val_MinusLogProbMetric: 28.6452 - lr: 5.0000e-04 - 34s/epoch - 173ms/step
Epoch 475/1000
2023-10-26 03:10:41.127 
Epoch 475/1000 
	 loss: 28.5065, MinusLogProbMetric: 28.5065, val_loss: 28.6847, val_MinusLogProbMetric: 28.6847

Epoch 475: val_loss did not improve from 28.37474
196/196 - 33s - loss: 28.5065 - MinusLogProbMetric: 28.5065 - val_loss: 28.6847 - val_MinusLogProbMetric: 28.6847 - lr: 5.0000e-04 - 33s/epoch - 169ms/step
Epoch 476/1000
2023-10-26 03:11:15.408 
Epoch 476/1000 
	 loss: 28.4186, MinusLogProbMetric: 28.4186, val_loss: 28.6571, val_MinusLogProbMetric: 28.6571

Epoch 476: val_loss did not improve from 28.37474
196/196 - 34s - loss: 28.4186 - MinusLogProbMetric: 28.4186 - val_loss: 28.6571 - val_MinusLogProbMetric: 28.6571 - lr: 5.0000e-04 - 34s/epoch - 175ms/step
Epoch 477/1000
2023-10-26 03:11:49.942 
Epoch 477/1000 
	 loss: 28.3371, MinusLogProbMetric: 28.3371, val_loss: 28.4662, val_MinusLogProbMetric: 28.4662

Epoch 477: val_loss did not improve from 28.37474
196/196 - 35s - loss: 28.3371 - MinusLogProbMetric: 28.3371 - val_loss: 28.4662 - val_MinusLogProbMetric: 28.4662 - lr: 5.0000e-04 - 35s/epoch - 176ms/step
Epoch 478/1000
2023-10-26 03:12:24.631 
Epoch 478/1000 
	 loss: 28.4581, MinusLogProbMetric: 28.4581, val_loss: 28.6901, val_MinusLogProbMetric: 28.6901

Epoch 478: val_loss did not improve from 28.37474
196/196 - 35s - loss: 28.4581 - MinusLogProbMetric: 28.4581 - val_loss: 28.6901 - val_MinusLogProbMetric: 28.6901 - lr: 5.0000e-04 - 35s/epoch - 177ms/step
Epoch 479/1000
2023-10-26 03:12:59.735 
Epoch 479/1000 
	 loss: 28.3856, MinusLogProbMetric: 28.3856, val_loss: 28.5799, val_MinusLogProbMetric: 28.5799

Epoch 479: val_loss did not improve from 28.37474
196/196 - 35s - loss: 28.3856 - MinusLogProbMetric: 28.3856 - val_loss: 28.5799 - val_MinusLogProbMetric: 28.5799 - lr: 5.0000e-04 - 35s/epoch - 179ms/step
Epoch 480/1000
2023-10-26 03:13:35.135 
Epoch 480/1000 
	 loss: 28.4518, MinusLogProbMetric: 28.4518, val_loss: 28.5542, val_MinusLogProbMetric: 28.5542

Epoch 480: val_loss did not improve from 28.37474
196/196 - 35s - loss: 28.4518 - MinusLogProbMetric: 28.4518 - val_loss: 28.5542 - val_MinusLogProbMetric: 28.5542 - lr: 5.0000e-04 - 35s/epoch - 181ms/step
Epoch 481/1000
2023-10-26 03:14:10.902 
Epoch 481/1000 
	 loss: 28.5590, MinusLogProbMetric: 28.5590, val_loss: 29.1613, val_MinusLogProbMetric: 29.1613

Epoch 481: val_loss did not improve from 28.37474
196/196 - 36s - loss: 28.5590 - MinusLogProbMetric: 28.5590 - val_loss: 29.1613 - val_MinusLogProbMetric: 29.1613 - lr: 5.0000e-04 - 36s/epoch - 182ms/step
Epoch 482/1000
2023-10-26 03:14:46.355 
Epoch 482/1000 
	 loss: 28.4583, MinusLogProbMetric: 28.4583, val_loss: 28.7328, val_MinusLogProbMetric: 28.7328

Epoch 482: val_loss did not improve from 28.37474
196/196 - 35s - loss: 28.4583 - MinusLogProbMetric: 28.4583 - val_loss: 28.7328 - val_MinusLogProbMetric: 28.7328 - lr: 5.0000e-04 - 35s/epoch - 181ms/step
Epoch 483/1000
2023-10-26 03:15:21.962 
Epoch 483/1000 
	 loss: 28.4867, MinusLogProbMetric: 28.4867, val_loss: 29.4129, val_MinusLogProbMetric: 29.4129

Epoch 483: val_loss did not improve from 28.37474
196/196 - 36s - loss: 28.4867 - MinusLogProbMetric: 28.4867 - val_loss: 29.4129 - val_MinusLogProbMetric: 29.4129 - lr: 5.0000e-04 - 36s/epoch - 182ms/step
Epoch 484/1000
2023-10-26 03:15:57.255 
Epoch 484/1000 
	 loss: 28.4755, MinusLogProbMetric: 28.4755, val_loss: 28.4332, val_MinusLogProbMetric: 28.4332

Epoch 484: val_loss did not improve from 28.37474
196/196 - 35s - loss: 28.4755 - MinusLogProbMetric: 28.4755 - val_loss: 28.4332 - val_MinusLogProbMetric: 28.4332 - lr: 5.0000e-04 - 35s/epoch - 180ms/step
Epoch 485/1000
2023-10-26 03:16:32.844 
Epoch 485/1000 
	 loss: 28.5541, MinusLogProbMetric: 28.5541, val_loss: 28.4421, val_MinusLogProbMetric: 28.4421

Epoch 485: val_loss did not improve from 28.37474
196/196 - 36s - loss: 28.5541 - MinusLogProbMetric: 28.5541 - val_loss: 28.4421 - val_MinusLogProbMetric: 28.4421 - lr: 5.0000e-04 - 36s/epoch - 182ms/step
Epoch 486/1000
2023-10-26 03:17:08.176 
Epoch 486/1000 
	 loss: 28.5455, MinusLogProbMetric: 28.5455, val_loss: 28.5229, val_MinusLogProbMetric: 28.5229

Epoch 486: val_loss did not improve from 28.37474
196/196 - 35s - loss: 28.5455 - MinusLogProbMetric: 28.5455 - val_loss: 28.5229 - val_MinusLogProbMetric: 28.5229 - lr: 5.0000e-04 - 35s/epoch - 180ms/step
Epoch 487/1000
2023-10-26 03:17:43.804 
Epoch 487/1000 
	 loss: 28.4494, MinusLogProbMetric: 28.4494, val_loss: 28.5691, val_MinusLogProbMetric: 28.5691

Epoch 487: val_loss did not improve from 28.37474
196/196 - 36s - loss: 28.4494 - MinusLogProbMetric: 28.4494 - val_loss: 28.5691 - val_MinusLogProbMetric: 28.5691 - lr: 5.0000e-04 - 36s/epoch - 182ms/step
Epoch 488/1000
2023-10-26 03:18:19.278 
Epoch 488/1000 
	 loss: 28.4419, MinusLogProbMetric: 28.4419, val_loss: 28.6482, val_MinusLogProbMetric: 28.6482

Epoch 488: val_loss did not improve from 28.37474
196/196 - 35s - loss: 28.4419 - MinusLogProbMetric: 28.4419 - val_loss: 28.6482 - val_MinusLogProbMetric: 28.6482 - lr: 5.0000e-04 - 35s/epoch - 181ms/step
Epoch 489/1000
2023-10-26 03:18:55.251 
Epoch 489/1000 
	 loss: 28.4193, MinusLogProbMetric: 28.4193, val_loss: 28.4563, val_MinusLogProbMetric: 28.4563

Epoch 489: val_loss did not improve from 28.37474
196/196 - 36s - loss: 28.4193 - MinusLogProbMetric: 28.4193 - val_loss: 28.4563 - val_MinusLogProbMetric: 28.4563 - lr: 5.0000e-04 - 36s/epoch - 184ms/step
Epoch 490/1000
2023-10-26 03:19:30.793 
Epoch 490/1000 
	 loss: 28.3627, MinusLogProbMetric: 28.3627, val_loss: 28.4952, val_MinusLogProbMetric: 28.4952

Epoch 490: val_loss did not improve from 28.37474
196/196 - 36s - loss: 28.3627 - MinusLogProbMetric: 28.3627 - val_loss: 28.4952 - val_MinusLogProbMetric: 28.4952 - lr: 5.0000e-04 - 36s/epoch - 181ms/step
Epoch 491/1000
2023-10-26 03:20:06.001 
Epoch 491/1000 
	 loss: 28.3804, MinusLogProbMetric: 28.3804, val_loss: 28.7541, val_MinusLogProbMetric: 28.7541

Epoch 491: val_loss did not improve from 28.37474
196/196 - 35s - loss: 28.3804 - MinusLogProbMetric: 28.3804 - val_loss: 28.7541 - val_MinusLogProbMetric: 28.7541 - lr: 5.0000e-04 - 35s/epoch - 180ms/step
Epoch 492/1000
2023-10-26 03:20:41.652 
Epoch 492/1000 
	 loss: 28.5147, MinusLogProbMetric: 28.5147, val_loss: 29.6024, val_MinusLogProbMetric: 29.6024

Epoch 492: val_loss did not improve from 28.37474
196/196 - 36s - loss: 28.5147 - MinusLogProbMetric: 28.5147 - val_loss: 29.6024 - val_MinusLogProbMetric: 29.6024 - lr: 5.0000e-04 - 36s/epoch - 182ms/step
Epoch 493/1000
2023-10-26 03:21:17.295 
Epoch 493/1000 
	 loss: 28.5490, MinusLogProbMetric: 28.5490, val_loss: 29.4647, val_MinusLogProbMetric: 29.4647

Epoch 493: val_loss did not improve from 28.37474
196/196 - 36s - loss: 28.5490 - MinusLogProbMetric: 28.5490 - val_loss: 29.4647 - val_MinusLogProbMetric: 29.4647 - lr: 5.0000e-04 - 36s/epoch - 182ms/step
Epoch 494/1000
2023-10-26 03:21:53.232 
Epoch 494/1000 
	 loss: 28.6677, MinusLogProbMetric: 28.6677, val_loss: 28.8579, val_MinusLogProbMetric: 28.8579

Epoch 494: val_loss did not improve from 28.37474
196/196 - 36s - loss: 28.6677 - MinusLogProbMetric: 28.6677 - val_loss: 28.8579 - val_MinusLogProbMetric: 28.8579 - lr: 5.0000e-04 - 36s/epoch - 183ms/step
Epoch 495/1000
2023-10-26 03:22:29.084 
Epoch 495/1000 
	 loss: 28.4538, MinusLogProbMetric: 28.4538, val_loss: 28.4909, val_MinusLogProbMetric: 28.4909

Epoch 495: val_loss did not improve from 28.37474
196/196 - 36s - loss: 28.4538 - MinusLogProbMetric: 28.4538 - val_loss: 28.4909 - val_MinusLogProbMetric: 28.4909 - lr: 5.0000e-04 - 36s/epoch - 183ms/step
Epoch 496/1000
2023-10-26 03:23:05.125 
Epoch 496/1000 
	 loss: 28.3579, MinusLogProbMetric: 28.3579, val_loss: 28.7356, val_MinusLogProbMetric: 28.7356

Epoch 496: val_loss did not improve from 28.37474
196/196 - 36s - loss: 28.3579 - MinusLogProbMetric: 28.3579 - val_loss: 28.7356 - val_MinusLogProbMetric: 28.7356 - lr: 5.0000e-04 - 36s/epoch - 184ms/step
Epoch 497/1000
2023-10-26 03:23:40.859 
Epoch 497/1000 
	 loss: 28.5607, MinusLogProbMetric: 28.5607, val_loss: 28.3637, val_MinusLogProbMetric: 28.3637

Epoch 497: val_loss improved from 28.37474 to 28.36374, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_369/weights/best_weights.h5
196/196 - 36s - loss: 28.5607 - MinusLogProbMetric: 28.5607 - val_loss: 28.3637 - val_MinusLogProbMetric: 28.3637 - lr: 5.0000e-04 - 36s/epoch - 185ms/step
Epoch 498/1000
2023-10-26 03:24:16.991 
Epoch 498/1000 
	 loss: 28.3340, MinusLogProbMetric: 28.3340, val_loss: 28.4859, val_MinusLogProbMetric: 28.4859

Epoch 498: val_loss did not improve from 28.36374
196/196 - 36s - loss: 28.3340 - MinusLogProbMetric: 28.3340 - val_loss: 28.4859 - val_MinusLogProbMetric: 28.4859 - lr: 5.0000e-04 - 36s/epoch - 181ms/step
Epoch 499/1000
2023-10-26 03:24:52.859 
Epoch 499/1000 
	 loss: 28.3881, MinusLogProbMetric: 28.3881, val_loss: 28.8251, val_MinusLogProbMetric: 28.8251

Epoch 499: val_loss did not improve from 28.36374
196/196 - 36s - loss: 28.3881 - MinusLogProbMetric: 28.3881 - val_loss: 28.8251 - val_MinusLogProbMetric: 28.8251 - lr: 5.0000e-04 - 36s/epoch - 183ms/step
Epoch 500/1000
2023-10-26 03:25:28.152 
Epoch 500/1000 
	 loss: 28.4259, MinusLogProbMetric: 28.4259, val_loss: 28.4594, val_MinusLogProbMetric: 28.4594

Epoch 500: val_loss did not improve from 28.36374
196/196 - 35s - loss: 28.4259 - MinusLogProbMetric: 28.4259 - val_loss: 28.4594 - val_MinusLogProbMetric: 28.4594 - lr: 5.0000e-04 - 35s/epoch - 180ms/step
Epoch 501/1000
2023-10-26 03:26:03.811 
Epoch 501/1000 
	 loss: 28.6412, MinusLogProbMetric: 28.6412, val_loss: 28.6234, val_MinusLogProbMetric: 28.6234

Epoch 501: val_loss did not improve from 28.36374
196/196 - 36s - loss: 28.6412 - MinusLogProbMetric: 28.6412 - val_loss: 28.6234 - val_MinusLogProbMetric: 28.6234 - lr: 5.0000e-04 - 36s/epoch - 182ms/step
Epoch 502/1000
2023-10-26 03:26:39.349 
Epoch 502/1000 
	 loss: 28.3917, MinusLogProbMetric: 28.3917, val_loss: 28.7944, val_MinusLogProbMetric: 28.7944

Epoch 502: val_loss did not improve from 28.36374
196/196 - 36s - loss: 28.3917 - MinusLogProbMetric: 28.3917 - val_loss: 28.7944 - val_MinusLogProbMetric: 28.7944 - lr: 5.0000e-04 - 36s/epoch - 181ms/step
Epoch 503/1000
2023-10-26 03:27:14.526 
Epoch 503/1000 
	 loss: 28.4113, MinusLogProbMetric: 28.4113, val_loss: 28.5343, val_MinusLogProbMetric: 28.5343

Epoch 503: val_loss did not improve from 28.36374
196/196 - 35s - loss: 28.4113 - MinusLogProbMetric: 28.4113 - val_loss: 28.5343 - val_MinusLogProbMetric: 28.5343 - lr: 5.0000e-04 - 35s/epoch - 179ms/step
Epoch 504/1000
2023-10-26 03:27:49.942 
Epoch 504/1000 
	 loss: 28.5415, MinusLogProbMetric: 28.5415, val_loss: 28.4817, val_MinusLogProbMetric: 28.4817

Epoch 504: val_loss did not improve from 28.36374
196/196 - 35s - loss: 28.5415 - MinusLogProbMetric: 28.5415 - val_loss: 28.4817 - val_MinusLogProbMetric: 28.4817 - lr: 5.0000e-04 - 35s/epoch - 181ms/step
Epoch 505/1000
2023-10-26 03:28:24.950 
Epoch 505/1000 
	 loss: 28.4732, MinusLogProbMetric: 28.4732, val_loss: 28.5369, val_MinusLogProbMetric: 28.5369

Epoch 505: val_loss did not improve from 28.36374
196/196 - 35s - loss: 28.4732 - MinusLogProbMetric: 28.4732 - val_loss: 28.5369 - val_MinusLogProbMetric: 28.5369 - lr: 5.0000e-04 - 35s/epoch - 179ms/step
Epoch 506/1000
2023-10-26 03:29:00.297 
Epoch 506/1000 
	 loss: 28.5868, MinusLogProbMetric: 28.5868, val_loss: 28.5684, val_MinusLogProbMetric: 28.5684

Epoch 506: val_loss did not improve from 28.36374
196/196 - 35s - loss: 28.5868 - MinusLogProbMetric: 28.5868 - val_loss: 28.5684 - val_MinusLogProbMetric: 28.5684 - lr: 5.0000e-04 - 35s/epoch - 180ms/step
Epoch 507/1000
2023-10-26 03:29:35.785 
Epoch 507/1000 
	 loss: 28.4941, MinusLogProbMetric: 28.4941, val_loss: 28.4256, val_MinusLogProbMetric: 28.4256

Epoch 507: val_loss did not improve from 28.36374
196/196 - 35s - loss: 28.4941 - MinusLogProbMetric: 28.4941 - val_loss: 28.4256 - val_MinusLogProbMetric: 28.4256 - lr: 5.0000e-04 - 35s/epoch - 181ms/step
Epoch 508/1000
2023-10-26 03:30:11.157 
Epoch 508/1000 
	 loss: 28.2869, MinusLogProbMetric: 28.2869, val_loss: 28.6312, val_MinusLogProbMetric: 28.6312

Epoch 508: val_loss did not improve from 28.36374
196/196 - 35s - loss: 28.2869 - MinusLogProbMetric: 28.2869 - val_loss: 28.6312 - val_MinusLogProbMetric: 28.6312 - lr: 5.0000e-04 - 35s/epoch - 180ms/step
Epoch 509/1000
2023-10-26 03:30:46.392 
Epoch 509/1000 
	 loss: 28.3610, MinusLogProbMetric: 28.3610, val_loss: 28.6564, val_MinusLogProbMetric: 28.6564

Epoch 509: val_loss did not improve from 28.36374
196/196 - 35s - loss: 28.3610 - MinusLogProbMetric: 28.3610 - val_loss: 28.6564 - val_MinusLogProbMetric: 28.6564 - lr: 5.0000e-04 - 35s/epoch - 180ms/step
Epoch 510/1000
2023-10-26 03:31:21.690 
Epoch 510/1000 
	 loss: 28.3215, MinusLogProbMetric: 28.3215, val_loss: 28.3614, val_MinusLogProbMetric: 28.3614

Epoch 510: val_loss improved from 28.36374 to 28.36143, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_369/weights/best_weights.h5
196/196 - 36s - loss: 28.3215 - MinusLogProbMetric: 28.3215 - val_loss: 28.3614 - val_MinusLogProbMetric: 28.3614 - lr: 5.0000e-04 - 36s/epoch - 184ms/step
Epoch 511/1000
2023-10-26 03:31:57.895 
Epoch 511/1000 
	 loss: 28.4357, MinusLogProbMetric: 28.4357, val_loss: 28.7463, val_MinusLogProbMetric: 28.7463

Epoch 511: val_loss did not improve from 28.36143
196/196 - 36s - loss: 28.4357 - MinusLogProbMetric: 28.4357 - val_loss: 28.7463 - val_MinusLogProbMetric: 28.7463 - lr: 5.0000e-04 - 36s/epoch - 181ms/step
Epoch 512/1000
2023-10-26 03:32:33.818 
Epoch 512/1000 
	 loss: 28.7404, MinusLogProbMetric: 28.7404, val_loss: 29.1512, val_MinusLogProbMetric: 29.1512

Epoch 512: val_loss did not improve from 28.36143
196/196 - 36s - loss: 28.7404 - MinusLogProbMetric: 28.7404 - val_loss: 29.1512 - val_MinusLogProbMetric: 29.1512 - lr: 5.0000e-04 - 36s/epoch - 183ms/step
Epoch 513/1000
2023-10-26 03:33:09.540 
Epoch 513/1000 
	 loss: 28.4058, MinusLogProbMetric: 28.4058, val_loss: 28.5368, val_MinusLogProbMetric: 28.5368

Epoch 513: val_loss did not improve from 28.36143
196/196 - 36s - loss: 28.4058 - MinusLogProbMetric: 28.4058 - val_loss: 28.5368 - val_MinusLogProbMetric: 28.5368 - lr: 5.0000e-04 - 36s/epoch - 182ms/step
Epoch 514/1000
2023-10-26 03:33:45.422 
Epoch 514/1000 
	 loss: 28.4788, MinusLogProbMetric: 28.4788, val_loss: 28.5845, val_MinusLogProbMetric: 28.5845

Epoch 514: val_loss did not improve from 28.36143
196/196 - 36s - loss: 28.4788 - MinusLogProbMetric: 28.4788 - val_loss: 28.5845 - val_MinusLogProbMetric: 28.5845 - lr: 5.0000e-04 - 36s/epoch - 183ms/step
Epoch 515/1000
2023-10-26 03:34:20.776 
Epoch 515/1000 
	 loss: 28.3139, MinusLogProbMetric: 28.3139, val_loss: 28.8184, val_MinusLogProbMetric: 28.8184

Epoch 515: val_loss did not improve from 28.36143
196/196 - 35s - loss: 28.3139 - MinusLogProbMetric: 28.3139 - val_loss: 28.8184 - val_MinusLogProbMetric: 28.8184 - lr: 5.0000e-04 - 35s/epoch - 180ms/step
Epoch 516/1000
2023-10-26 03:34:56.414 
Epoch 516/1000 
	 loss: 28.3363, MinusLogProbMetric: 28.3363, val_loss: 28.7261, val_MinusLogProbMetric: 28.7261

Epoch 516: val_loss did not improve from 28.36143
196/196 - 36s - loss: 28.3363 - MinusLogProbMetric: 28.3363 - val_loss: 28.7261 - val_MinusLogProbMetric: 28.7261 - lr: 5.0000e-04 - 36s/epoch - 182ms/step
Epoch 517/1000
2023-10-26 03:35:31.818 
Epoch 517/1000 
	 loss: 28.4905, MinusLogProbMetric: 28.4905, val_loss: 28.3867, val_MinusLogProbMetric: 28.3867

Epoch 517: val_loss did not improve from 28.36143
196/196 - 35s - loss: 28.4905 - MinusLogProbMetric: 28.4905 - val_loss: 28.3867 - val_MinusLogProbMetric: 28.3867 - lr: 5.0000e-04 - 35s/epoch - 181ms/step
Epoch 518/1000
2023-10-26 03:36:07.283 
Epoch 518/1000 
	 loss: 28.4739, MinusLogProbMetric: 28.4739, val_loss: 28.7291, val_MinusLogProbMetric: 28.7291

Epoch 518: val_loss did not improve from 28.36143
196/196 - 35s - loss: 28.4739 - MinusLogProbMetric: 28.4739 - val_loss: 28.7291 - val_MinusLogProbMetric: 28.7291 - lr: 5.0000e-04 - 35s/epoch - 181ms/step
Epoch 519/1000
2023-10-26 03:36:42.805 
Epoch 519/1000 
	 loss: 28.4757, MinusLogProbMetric: 28.4757, val_loss: 28.3815, val_MinusLogProbMetric: 28.3815

Epoch 519: val_loss did not improve from 28.36143
196/196 - 36s - loss: 28.4757 - MinusLogProbMetric: 28.4757 - val_loss: 28.3815 - val_MinusLogProbMetric: 28.3815 - lr: 5.0000e-04 - 36s/epoch - 181ms/step
Epoch 520/1000
2023-10-26 03:37:18.348 
Epoch 520/1000 
	 loss: 28.3329, MinusLogProbMetric: 28.3329, val_loss: 28.4929, val_MinusLogProbMetric: 28.4929

Epoch 520: val_loss did not improve from 28.36143
196/196 - 36s - loss: 28.3329 - MinusLogProbMetric: 28.3329 - val_loss: 28.4929 - val_MinusLogProbMetric: 28.4929 - lr: 5.0000e-04 - 36s/epoch - 181ms/step
Epoch 521/1000
2023-10-26 03:37:53.883 
Epoch 521/1000 
	 loss: 28.4290, MinusLogProbMetric: 28.4290, val_loss: 28.3389, val_MinusLogProbMetric: 28.3389

Epoch 521: val_loss improved from 28.36143 to 28.33889, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_369/weights/best_weights.h5
196/196 - 36s - loss: 28.4290 - MinusLogProbMetric: 28.4290 - val_loss: 28.3389 - val_MinusLogProbMetric: 28.3389 - lr: 5.0000e-04 - 36s/epoch - 185ms/step
Epoch 522/1000
2023-10-26 03:38:29.840 
Epoch 522/1000 
	 loss: 28.3622, MinusLogProbMetric: 28.3622, val_loss: 28.9704, val_MinusLogProbMetric: 28.9704

Epoch 522: val_loss did not improve from 28.33889
196/196 - 35s - loss: 28.3622 - MinusLogProbMetric: 28.3622 - val_loss: 28.9704 - val_MinusLogProbMetric: 28.9704 - lr: 5.0000e-04 - 35s/epoch - 180ms/step
Epoch 523/1000
2023-10-26 03:39:05.360 
Epoch 523/1000 
	 loss: 28.3828, MinusLogProbMetric: 28.3828, val_loss: 28.4663, val_MinusLogProbMetric: 28.4663

Epoch 523: val_loss did not improve from 28.33889
196/196 - 36s - loss: 28.3828 - MinusLogProbMetric: 28.3828 - val_loss: 28.4663 - val_MinusLogProbMetric: 28.4663 - lr: 5.0000e-04 - 36s/epoch - 181ms/step
Epoch 524/1000
2023-10-26 03:39:40.763 
Epoch 524/1000 
	 loss: 28.5560, MinusLogProbMetric: 28.5560, val_loss: 28.5886, val_MinusLogProbMetric: 28.5886

Epoch 524: val_loss did not improve from 28.33889
196/196 - 35s - loss: 28.5560 - MinusLogProbMetric: 28.5560 - val_loss: 28.5886 - val_MinusLogProbMetric: 28.5886 - lr: 5.0000e-04 - 35s/epoch - 181ms/step
Epoch 525/1000
2023-10-26 03:40:15.906 
Epoch 525/1000 
	 loss: 28.2770, MinusLogProbMetric: 28.2770, val_loss: 29.1209, val_MinusLogProbMetric: 29.1209

Epoch 525: val_loss did not improve from 28.33889
196/196 - 35s - loss: 28.2770 - MinusLogProbMetric: 28.2770 - val_loss: 29.1209 - val_MinusLogProbMetric: 29.1209 - lr: 5.0000e-04 - 35s/epoch - 179ms/step
Epoch 526/1000
2023-10-26 03:40:51.077 
Epoch 526/1000 
	 loss: 28.2853, MinusLogProbMetric: 28.2853, val_loss: 28.6263, val_MinusLogProbMetric: 28.6263

Epoch 526: val_loss did not improve from 28.33889
196/196 - 35s - loss: 28.2853 - MinusLogProbMetric: 28.2853 - val_loss: 28.6263 - val_MinusLogProbMetric: 28.6263 - lr: 5.0000e-04 - 35s/epoch - 179ms/step
Epoch 527/1000
2023-10-26 03:41:26.137 
Epoch 527/1000 
	 loss: 28.5604, MinusLogProbMetric: 28.5604, val_loss: 28.4517, val_MinusLogProbMetric: 28.4517

Epoch 527: val_loss did not improve from 28.33889
196/196 - 35s - loss: 28.5604 - MinusLogProbMetric: 28.5604 - val_loss: 28.4517 - val_MinusLogProbMetric: 28.4517 - lr: 5.0000e-04 - 35s/epoch - 179ms/step
Epoch 528/1000
2023-10-26 03:42:01.641 
Epoch 528/1000 
	 loss: 28.5298, MinusLogProbMetric: 28.5298, val_loss: 29.9176, val_MinusLogProbMetric: 29.9176

Epoch 528: val_loss did not improve from 28.33889
196/196 - 36s - loss: 28.5298 - MinusLogProbMetric: 28.5298 - val_loss: 29.9176 - val_MinusLogProbMetric: 29.9176 - lr: 5.0000e-04 - 36s/epoch - 181ms/step
Epoch 529/1000
2023-10-26 03:42:36.721 
Epoch 529/1000 
	 loss: 28.7162, MinusLogProbMetric: 28.7162, val_loss: 28.5737, val_MinusLogProbMetric: 28.5737

Epoch 529: val_loss did not improve from 28.33889
196/196 - 35s - loss: 28.7162 - MinusLogProbMetric: 28.7162 - val_loss: 28.5737 - val_MinusLogProbMetric: 28.5737 - lr: 5.0000e-04 - 35s/epoch - 179ms/step
Epoch 530/1000
2023-10-26 03:43:12.096 
Epoch 530/1000 
	 loss: 28.4332, MinusLogProbMetric: 28.4332, val_loss: 28.5202, val_MinusLogProbMetric: 28.5202

Epoch 530: val_loss did not improve from 28.33889
196/196 - 35s - loss: 28.4332 - MinusLogProbMetric: 28.4332 - val_loss: 28.5202 - val_MinusLogProbMetric: 28.5202 - lr: 5.0000e-04 - 35s/epoch - 180ms/step
Epoch 531/1000
2023-10-26 03:43:47.345 
Epoch 531/1000 
	 loss: 28.3093, MinusLogProbMetric: 28.3093, val_loss: 28.6289, val_MinusLogProbMetric: 28.6289

Epoch 531: val_loss did not improve from 28.33889
196/196 - 35s - loss: 28.3093 - MinusLogProbMetric: 28.3093 - val_loss: 28.6289 - val_MinusLogProbMetric: 28.6289 - lr: 5.0000e-04 - 35s/epoch - 180ms/step
Epoch 532/1000
2023-10-26 03:44:22.782 
Epoch 532/1000 
	 loss: 28.7160, MinusLogProbMetric: 28.7160, val_loss: 28.7805, val_MinusLogProbMetric: 28.7805

Epoch 532: val_loss did not improve from 28.33889
196/196 - 35s - loss: 28.7160 - MinusLogProbMetric: 28.7160 - val_loss: 28.7805 - val_MinusLogProbMetric: 28.7805 - lr: 5.0000e-04 - 35s/epoch - 181ms/step
Epoch 533/1000
2023-10-26 03:44:57.962 
Epoch 533/1000 
	 loss: 28.3380, MinusLogProbMetric: 28.3380, val_loss: 28.4884, val_MinusLogProbMetric: 28.4884

Epoch 533: val_loss did not improve from 28.33889
196/196 - 35s - loss: 28.3380 - MinusLogProbMetric: 28.3380 - val_loss: 28.4884 - val_MinusLogProbMetric: 28.4884 - lr: 5.0000e-04 - 35s/epoch - 179ms/step
Epoch 534/1000
2023-10-26 03:45:33.298 
Epoch 534/1000 
	 loss: 28.3048, MinusLogProbMetric: 28.3048, val_loss: 28.5570, val_MinusLogProbMetric: 28.5570

Epoch 534: val_loss did not improve from 28.33889
196/196 - 35s - loss: 28.3048 - MinusLogProbMetric: 28.3048 - val_loss: 28.5570 - val_MinusLogProbMetric: 28.5570 - lr: 5.0000e-04 - 35s/epoch - 180ms/step
Epoch 535/1000
2023-10-26 03:46:08.675 
Epoch 535/1000 
	 loss: 28.5793, MinusLogProbMetric: 28.5793, val_loss: 28.3811, val_MinusLogProbMetric: 28.3811

Epoch 535: val_loss did not improve from 28.33889
196/196 - 35s - loss: 28.5793 - MinusLogProbMetric: 28.5793 - val_loss: 28.3811 - val_MinusLogProbMetric: 28.3811 - lr: 5.0000e-04 - 35s/epoch - 180ms/step
Epoch 536/1000
2023-10-26 03:46:44.064 
Epoch 536/1000 
	 loss: 28.6676, MinusLogProbMetric: 28.6676, val_loss: 28.5882, val_MinusLogProbMetric: 28.5882

Epoch 536: val_loss did not improve from 28.33889
196/196 - 35s - loss: 28.6676 - MinusLogProbMetric: 28.6676 - val_loss: 28.5882 - val_MinusLogProbMetric: 28.5882 - lr: 5.0000e-04 - 35s/epoch - 181ms/step
Epoch 537/1000
2023-10-26 03:47:19.557 
Epoch 537/1000 
	 loss: 28.4782, MinusLogProbMetric: 28.4782, val_loss: 28.4447, val_MinusLogProbMetric: 28.4447

Epoch 537: val_loss did not improve from 28.33889
196/196 - 35s - loss: 28.4782 - MinusLogProbMetric: 28.4782 - val_loss: 28.4447 - val_MinusLogProbMetric: 28.4447 - lr: 5.0000e-04 - 35s/epoch - 181ms/step
Epoch 538/1000
2023-10-26 03:47:55.444 
Epoch 538/1000 
	 loss: 28.4615, MinusLogProbMetric: 28.4615, val_loss: 28.4718, val_MinusLogProbMetric: 28.4718

Epoch 538: val_loss did not improve from 28.33889
196/196 - 36s - loss: 28.4615 - MinusLogProbMetric: 28.4615 - val_loss: 28.4718 - val_MinusLogProbMetric: 28.4718 - lr: 5.0000e-04 - 36s/epoch - 183ms/step
Epoch 539/1000
2023-10-26 03:48:30.824 
Epoch 539/1000 
	 loss: 28.3071, MinusLogProbMetric: 28.3071, val_loss: 28.5097, val_MinusLogProbMetric: 28.5097

Epoch 539: val_loss did not improve from 28.33889
196/196 - 35s - loss: 28.3071 - MinusLogProbMetric: 28.3071 - val_loss: 28.5097 - val_MinusLogProbMetric: 28.5097 - lr: 5.0000e-04 - 35s/epoch - 180ms/step
Epoch 540/1000
2023-10-26 03:49:05.910 
Epoch 540/1000 
	 loss: 28.4342, MinusLogProbMetric: 28.4342, val_loss: 28.8510, val_MinusLogProbMetric: 28.8510

Epoch 540: val_loss did not improve from 28.33889
196/196 - 35s - loss: 28.4342 - MinusLogProbMetric: 28.4342 - val_loss: 28.8510 - val_MinusLogProbMetric: 28.8510 - lr: 5.0000e-04 - 35s/epoch - 179ms/step
Epoch 541/1000
2023-10-26 03:49:41.287 
Epoch 541/1000 
	 loss: 28.4819, MinusLogProbMetric: 28.4819, val_loss: 28.5978, val_MinusLogProbMetric: 28.5978

Epoch 541: val_loss did not improve from 28.33889
196/196 - 35s - loss: 28.4819 - MinusLogProbMetric: 28.4819 - val_loss: 28.5978 - val_MinusLogProbMetric: 28.5978 - lr: 5.0000e-04 - 35s/epoch - 180ms/step
Epoch 542/1000
2023-10-26 03:50:16.788 
Epoch 542/1000 
	 loss: 28.2982, MinusLogProbMetric: 28.2982, val_loss: 28.4801, val_MinusLogProbMetric: 28.4801

Epoch 542: val_loss did not improve from 28.33889
196/196 - 35s - loss: 28.2982 - MinusLogProbMetric: 28.2982 - val_loss: 28.4801 - val_MinusLogProbMetric: 28.4801 - lr: 5.0000e-04 - 35s/epoch - 181ms/step
Epoch 543/1000
2023-10-26 03:50:52.468 
Epoch 543/1000 
	 loss: 28.4157, MinusLogProbMetric: 28.4157, val_loss: 28.5301, val_MinusLogProbMetric: 28.5301

Epoch 543: val_loss did not improve from 28.33889
196/196 - 36s - loss: 28.4157 - MinusLogProbMetric: 28.4157 - val_loss: 28.5301 - val_MinusLogProbMetric: 28.5301 - lr: 5.0000e-04 - 36s/epoch - 182ms/step
Epoch 544/1000
2023-10-26 03:51:28.172 
Epoch 544/1000 
	 loss: 28.5766, MinusLogProbMetric: 28.5766, val_loss: 29.4306, val_MinusLogProbMetric: 29.4306

Epoch 544: val_loss did not improve from 28.33889
196/196 - 36s - loss: 28.5766 - MinusLogProbMetric: 28.5766 - val_loss: 29.4306 - val_MinusLogProbMetric: 29.4306 - lr: 5.0000e-04 - 36s/epoch - 182ms/step
Epoch 545/1000
2023-10-26 03:52:03.522 
Epoch 545/1000 
	 loss: 28.5591, MinusLogProbMetric: 28.5591, val_loss: 28.3809, val_MinusLogProbMetric: 28.3809

Epoch 545: val_loss did not improve from 28.33889
196/196 - 35s - loss: 28.5591 - MinusLogProbMetric: 28.5591 - val_loss: 28.3809 - val_MinusLogProbMetric: 28.3809 - lr: 5.0000e-04 - 35s/epoch - 180ms/step
Epoch 546/1000
2023-10-26 03:52:39.156 
Epoch 546/1000 
	 loss: 28.3477, MinusLogProbMetric: 28.3477, val_loss: 28.7432, val_MinusLogProbMetric: 28.7432

Epoch 546: val_loss did not improve from 28.33889
196/196 - 36s - loss: 28.3477 - MinusLogProbMetric: 28.3477 - val_loss: 28.7432 - val_MinusLogProbMetric: 28.7432 - lr: 5.0000e-04 - 36s/epoch - 182ms/step
Epoch 547/1000
2023-10-26 03:53:14.954 
Epoch 547/1000 
	 loss: 28.4015, MinusLogProbMetric: 28.4015, val_loss: 28.5461, val_MinusLogProbMetric: 28.5461

Epoch 547: val_loss did not improve from 28.33889
196/196 - 36s - loss: 28.4015 - MinusLogProbMetric: 28.4015 - val_loss: 28.5461 - val_MinusLogProbMetric: 28.5461 - lr: 5.0000e-04 - 36s/epoch - 183ms/step
Epoch 548/1000
2023-10-26 03:53:50.843 
Epoch 548/1000 
	 loss: 28.5759, MinusLogProbMetric: 28.5759, val_loss: 28.9397, val_MinusLogProbMetric: 28.9397

Epoch 548: val_loss did not improve from 28.33889
196/196 - 36s - loss: 28.5759 - MinusLogProbMetric: 28.5759 - val_loss: 28.9397 - val_MinusLogProbMetric: 28.9397 - lr: 5.0000e-04 - 36s/epoch - 183ms/step
Epoch 549/1000
2023-10-26 03:54:26.620 
Epoch 549/1000 
	 loss: 28.3129, MinusLogProbMetric: 28.3129, val_loss: 28.4848, val_MinusLogProbMetric: 28.4848

Epoch 549: val_loss did not improve from 28.33889
196/196 - 36s - loss: 28.3129 - MinusLogProbMetric: 28.3129 - val_loss: 28.4848 - val_MinusLogProbMetric: 28.4848 - lr: 5.0000e-04 - 36s/epoch - 183ms/step
Epoch 550/1000
2023-10-26 03:55:02.143 
Epoch 550/1000 
	 loss: 28.4342, MinusLogProbMetric: 28.4342, val_loss: 28.3576, val_MinusLogProbMetric: 28.3576

Epoch 550: val_loss did not improve from 28.33889
196/196 - 36s - loss: 28.4342 - MinusLogProbMetric: 28.4342 - val_loss: 28.3576 - val_MinusLogProbMetric: 28.3576 - lr: 5.0000e-04 - 36s/epoch - 181ms/step
Epoch 551/1000
2023-10-26 03:55:37.695 
Epoch 551/1000 
	 loss: 28.3673, MinusLogProbMetric: 28.3673, val_loss: 29.0886, val_MinusLogProbMetric: 29.0886

Epoch 551: val_loss did not improve from 28.33889
196/196 - 36s - loss: 28.3673 - MinusLogProbMetric: 28.3673 - val_loss: 29.0886 - val_MinusLogProbMetric: 29.0886 - lr: 5.0000e-04 - 36s/epoch - 181ms/step
Epoch 552/1000
2023-10-26 03:56:13.341 
Epoch 552/1000 
	 loss: 28.4414, MinusLogProbMetric: 28.4414, val_loss: 28.2844, val_MinusLogProbMetric: 28.2844

Epoch 552: val_loss improved from 28.33889 to 28.28438, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_369/weights/best_weights.h5
196/196 - 36s - loss: 28.4414 - MinusLogProbMetric: 28.4414 - val_loss: 28.2844 - val_MinusLogProbMetric: 28.2844 - lr: 5.0000e-04 - 36s/epoch - 185ms/step
Epoch 553/1000
2023-10-26 03:56:49.519 
Epoch 553/1000 
	 loss: 28.4603, MinusLogProbMetric: 28.4603, val_loss: 28.5664, val_MinusLogProbMetric: 28.5664

Epoch 553: val_loss did not improve from 28.28438
196/196 - 36s - loss: 28.4603 - MinusLogProbMetric: 28.4603 - val_loss: 28.5664 - val_MinusLogProbMetric: 28.5664 - lr: 5.0000e-04 - 36s/epoch - 181ms/step
Epoch 554/1000
2023-10-26 03:57:24.832 
Epoch 554/1000 
	 loss: 28.3766, MinusLogProbMetric: 28.3766, val_loss: 28.5753, val_MinusLogProbMetric: 28.5753

Epoch 554: val_loss did not improve from 28.28438
196/196 - 35s - loss: 28.3766 - MinusLogProbMetric: 28.3766 - val_loss: 28.5753 - val_MinusLogProbMetric: 28.5753 - lr: 5.0000e-04 - 35s/epoch - 180ms/step
Epoch 555/1000
2023-10-26 03:58:00.606 
Epoch 555/1000 
	 loss: 28.3880, MinusLogProbMetric: 28.3880, val_loss: 28.4031, val_MinusLogProbMetric: 28.4031

Epoch 555: val_loss did not improve from 28.28438
196/196 - 36s - loss: 28.3880 - MinusLogProbMetric: 28.3880 - val_loss: 28.4031 - val_MinusLogProbMetric: 28.4031 - lr: 5.0000e-04 - 36s/epoch - 183ms/step
Epoch 556/1000
2023-10-26 03:58:36.563 
Epoch 556/1000 
	 loss: 28.5559, MinusLogProbMetric: 28.5559, val_loss: 28.5287, val_MinusLogProbMetric: 28.5287

Epoch 556: val_loss did not improve from 28.28438
196/196 - 36s - loss: 28.5559 - MinusLogProbMetric: 28.5559 - val_loss: 28.5287 - val_MinusLogProbMetric: 28.5287 - lr: 5.0000e-04 - 36s/epoch - 183ms/step
Epoch 557/1000
2023-10-26 03:59:12.428 
Epoch 557/1000 
	 loss: 28.4609, MinusLogProbMetric: 28.4609, val_loss: 28.5891, val_MinusLogProbMetric: 28.5891

Epoch 557: val_loss did not improve from 28.28438
196/196 - 36s - loss: 28.4609 - MinusLogProbMetric: 28.4609 - val_loss: 28.5891 - val_MinusLogProbMetric: 28.5891 - lr: 5.0000e-04 - 36s/epoch - 183ms/step
Epoch 558/1000
2023-10-26 03:59:48.268 
Epoch 558/1000 
	 loss: 28.4254, MinusLogProbMetric: 28.4254, val_loss: 28.7464, val_MinusLogProbMetric: 28.7464

Epoch 558: val_loss did not improve from 28.28438
196/196 - 36s - loss: 28.4254 - MinusLogProbMetric: 28.4254 - val_loss: 28.7464 - val_MinusLogProbMetric: 28.7464 - lr: 5.0000e-04 - 36s/epoch - 183ms/step
Epoch 559/1000
2023-10-26 04:00:23.784 
Epoch 559/1000 
	 loss: 28.5158, MinusLogProbMetric: 28.5158, val_loss: 28.8364, val_MinusLogProbMetric: 28.8364

Epoch 559: val_loss did not improve from 28.28438
196/196 - 36s - loss: 28.5158 - MinusLogProbMetric: 28.5158 - val_loss: 28.8364 - val_MinusLogProbMetric: 28.8364 - lr: 5.0000e-04 - 36s/epoch - 181ms/step
Epoch 560/1000
2023-10-26 04:00:59.346 
Epoch 560/1000 
	 loss: 28.6831, MinusLogProbMetric: 28.6831, val_loss: 28.9254, val_MinusLogProbMetric: 28.9254

Epoch 560: val_loss did not improve from 28.28438
196/196 - 36s - loss: 28.6831 - MinusLogProbMetric: 28.6831 - val_loss: 28.9254 - val_MinusLogProbMetric: 28.9254 - lr: 5.0000e-04 - 36s/epoch - 181ms/step
Epoch 561/1000
2023-10-26 04:01:35.089 
Epoch 561/1000 
	 loss: 28.5435, MinusLogProbMetric: 28.5435, val_loss: 28.6216, val_MinusLogProbMetric: 28.6216

Epoch 561: val_loss did not improve from 28.28438
196/196 - 36s - loss: 28.5435 - MinusLogProbMetric: 28.5435 - val_loss: 28.6216 - val_MinusLogProbMetric: 28.6216 - lr: 5.0000e-04 - 36s/epoch - 182ms/step
Epoch 562/1000
2023-10-26 04:02:10.874 
Epoch 562/1000 
	 loss: 28.3255, MinusLogProbMetric: 28.3255, val_loss: 28.3358, val_MinusLogProbMetric: 28.3358

Epoch 562: val_loss did not improve from 28.28438
196/196 - 36s - loss: 28.3255 - MinusLogProbMetric: 28.3255 - val_loss: 28.3358 - val_MinusLogProbMetric: 28.3358 - lr: 5.0000e-04 - 36s/epoch - 183ms/step
Epoch 563/1000
2023-10-26 04:02:46.641 
Epoch 563/1000 
	 loss: 28.5630, MinusLogProbMetric: 28.5630, val_loss: 28.5459, val_MinusLogProbMetric: 28.5459

Epoch 563: val_loss did not improve from 28.28438
196/196 - 36s - loss: 28.5630 - MinusLogProbMetric: 28.5630 - val_loss: 28.5459 - val_MinusLogProbMetric: 28.5459 - lr: 5.0000e-04 - 36s/epoch - 182ms/step
Epoch 564/1000
2023-10-26 04:03:22.248 
Epoch 564/1000 
	 loss: 28.3877, MinusLogProbMetric: 28.3877, val_loss: 29.2778, val_MinusLogProbMetric: 29.2778

Epoch 564: val_loss did not improve from 28.28438
196/196 - 36s - loss: 28.3877 - MinusLogProbMetric: 28.3877 - val_loss: 29.2778 - val_MinusLogProbMetric: 29.2778 - lr: 5.0000e-04 - 36s/epoch - 182ms/step
Epoch 565/1000
2023-10-26 04:03:57.725 
Epoch 565/1000 
	 loss: 28.5226, MinusLogProbMetric: 28.5226, val_loss: 28.5071, val_MinusLogProbMetric: 28.5071

Epoch 565: val_loss did not improve from 28.28438
196/196 - 35s - loss: 28.5226 - MinusLogProbMetric: 28.5226 - val_loss: 28.5071 - val_MinusLogProbMetric: 28.5071 - lr: 5.0000e-04 - 35s/epoch - 181ms/step
Epoch 566/1000
2023-10-26 04:04:33.864 
Epoch 566/1000 
	 loss: 28.4072, MinusLogProbMetric: 28.4072, val_loss: 28.5648, val_MinusLogProbMetric: 28.5648

Epoch 566: val_loss did not improve from 28.28438
196/196 - 36s - loss: 28.4072 - MinusLogProbMetric: 28.4072 - val_loss: 28.5648 - val_MinusLogProbMetric: 28.5648 - lr: 5.0000e-04 - 36s/epoch - 184ms/step
Epoch 567/1000
2023-10-26 04:05:09.642 
Epoch 567/1000 
	 loss: 28.3702, MinusLogProbMetric: 28.3702, val_loss: 28.3641, val_MinusLogProbMetric: 28.3641

Epoch 567: val_loss did not improve from 28.28438
196/196 - 36s - loss: 28.3702 - MinusLogProbMetric: 28.3702 - val_loss: 28.3641 - val_MinusLogProbMetric: 28.3641 - lr: 5.0000e-04 - 36s/epoch - 183ms/step
Epoch 568/1000
2023-10-26 04:05:45.269 
Epoch 568/1000 
	 loss: 28.3681, MinusLogProbMetric: 28.3681, val_loss: 28.6647, val_MinusLogProbMetric: 28.6647

Epoch 568: val_loss did not improve from 28.28438
196/196 - 36s - loss: 28.3681 - MinusLogProbMetric: 28.3681 - val_loss: 28.6647 - val_MinusLogProbMetric: 28.6647 - lr: 5.0000e-04 - 36s/epoch - 182ms/step
Epoch 569/1000
2023-10-26 04:06:20.472 
Epoch 569/1000 
	 loss: 28.4800, MinusLogProbMetric: 28.4800, val_loss: 28.7094, val_MinusLogProbMetric: 28.7094

Epoch 569: val_loss did not improve from 28.28438
196/196 - 35s - loss: 28.4800 - MinusLogProbMetric: 28.4800 - val_loss: 28.7094 - val_MinusLogProbMetric: 28.7094 - lr: 5.0000e-04 - 35s/epoch - 180ms/step
Epoch 570/1000
2023-10-26 04:06:56.018 
Epoch 570/1000 
	 loss: 28.2718, MinusLogProbMetric: 28.2718, val_loss: 28.5412, val_MinusLogProbMetric: 28.5412

Epoch 570: val_loss did not improve from 28.28438
196/196 - 36s - loss: 28.2718 - MinusLogProbMetric: 28.2718 - val_loss: 28.5412 - val_MinusLogProbMetric: 28.5412 - lr: 5.0000e-04 - 36s/epoch - 181ms/step
Epoch 571/1000
2023-10-26 04:07:31.503 
Epoch 571/1000 
	 loss: 28.3725, MinusLogProbMetric: 28.3725, val_loss: 29.4728, val_MinusLogProbMetric: 29.4728

Epoch 571: val_loss did not improve from 28.28438
196/196 - 35s - loss: 28.3725 - MinusLogProbMetric: 28.3725 - val_loss: 29.4728 - val_MinusLogProbMetric: 29.4728 - lr: 5.0000e-04 - 35s/epoch - 181ms/step
Epoch 572/1000
2023-10-26 04:08:06.932 
Epoch 572/1000 
	 loss: 28.5454, MinusLogProbMetric: 28.5454, val_loss: 28.5544, val_MinusLogProbMetric: 28.5544

Epoch 572: val_loss did not improve from 28.28438
196/196 - 35s - loss: 28.5454 - MinusLogProbMetric: 28.5454 - val_loss: 28.5544 - val_MinusLogProbMetric: 28.5544 - lr: 5.0000e-04 - 35s/epoch - 181ms/step
Epoch 573/1000
2023-10-26 04:08:42.631 
Epoch 573/1000 
	 loss: 28.3891, MinusLogProbMetric: 28.3891, val_loss: 29.0306, val_MinusLogProbMetric: 29.0306

Epoch 573: val_loss did not improve from 28.28438
196/196 - 36s - loss: 28.3891 - MinusLogProbMetric: 28.3891 - val_loss: 29.0306 - val_MinusLogProbMetric: 29.0306 - lr: 5.0000e-04 - 36s/epoch - 182ms/step
Epoch 574/1000
2023-10-26 04:09:18.539 
Epoch 574/1000 
	 loss: 28.2522, MinusLogProbMetric: 28.2522, val_loss: 28.7090, val_MinusLogProbMetric: 28.7090

Epoch 574: val_loss did not improve from 28.28438
196/196 - 36s - loss: 28.2522 - MinusLogProbMetric: 28.2522 - val_loss: 28.7090 - val_MinusLogProbMetric: 28.7090 - lr: 5.0000e-04 - 36s/epoch - 183ms/step
Epoch 575/1000
2023-10-26 04:09:54.282 
Epoch 575/1000 
	 loss: 28.4492, MinusLogProbMetric: 28.4492, val_loss: 28.3918, val_MinusLogProbMetric: 28.3918

Epoch 575: val_loss did not improve from 28.28438
196/196 - 36s - loss: 28.4492 - MinusLogProbMetric: 28.4492 - val_loss: 28.3918 - val_MinusLogProbMetric: 28.3918 - lr: 5.0000e-04 - 36s/epoch - 182ms/step
Epoch 576/1000
2023-10-26 04:10:30.158 
Epoch 576/1000 
	 loss: 28.3710, MinusLogProbMetric: 28.3710, val_loss: 28.4793, val_MinusLogProbMetric: 28.4793

Epoch 576: val_loss did not improve from 28.28438
196/196 - 36s - loss: 28.3710 - MinusLogProbMetric: 28.3710 - val_loss: 28.4793 - val_MinusLogProbMetric: 28.4793 - lr: 5.0000e-04 - 36s/epoch - 183ms/step
Epoch 577/1000
2023-10-26 04:11:05.696 
Epoch 577/1000 
	 loss: 28.4022, MinusLogProbMetric: 28.4022, val_loss: 28.5756, val_MinusLogProbMetric: 28.5756

Epoch 577: val_loss did not improve from 28.28438
196/196 - 36s - loss: 28.4022 - MinusLogProbMetric: 28.4022 - val_loss: 28.5756 - val_MinusLogProbMetric: 28.5756 - lr: 5.0000e-04 - 36s/epoch - 181ms/step
Epoch 578/1000
2023-10-26 04:11:41.145 
Epoch 578/1000 
	 loss: 28.3449, MinusLogProbMetric: 28.3449, val_loss: 28.5381, val_MinusLogProbMetric: 28.5381

Epoch 578: val_loss did not improve from 28.28438
196/196 - 35s - loss: 28.3449 - MinusLogProbMetric: 28.3449 - val_loss: 28.5381 - val_MinusLogProbMetric: 28.5381 - lr: 5.0000e-04 - 35s/epoch - 181ms/step
Epoch 579/1000
2023-10-26 04:12:15.140 
Epoch 579/1000 
	 loss: 28.4889, MinusLogProbMetric: 28.4889, val_loss: 28.5483, val_MinusLogProbMetric: 28.5483

Epoch 579: val_loss did not improve from 28.28438
196/196 - 34s - loss: 28.4889 - MinusLogProbMetric: 28.4889 - val_loss: 28.5483 - val_MinusLogProbMetric: 28.5483 - lr: 5.0000e-04 - 34s/epoch - 173ms/step
Epoch 580/1000
2023-10-26 04:12:49.041 
Epoch 580/1000 
	 loss: 28.4017, MinusLogProbMetric: 28.4017, val_loss: 28.4838, val_MinusLogProbMetric: 28.4838

Epoch 580: val_loss did not improve from 28.28438
196/196 - 34s - loss: 28.4017 - MinusLogProbMetric: 28.4017 - val_loss: 28.4838 - val_MinusLogProbMetric: 28.4838 - lr: 5.0000e-04 - 34s/epoch - 173ms/step
Epoch 581/1000
2023-10-26 04:13:23.772 
Epoch 581/1000 
	 loss: 28.4592, MinusLogProbMetric: 28.4592, val_loss: 28.5243, val_MinusLogProbMetric: 28.5243

Epoch 581: val_loss did not improve from 28.28438
196/196 - 35s - loss: 28.4592 - MinusLogProbMetric: 28.4592 - val_loss: 28.5243 - val_MinusLogProbMetric: 28.5243 - lr: 5.0000e-04 - 35s/epoch - 177ms/step
Epoch 582/1000
2023-10-26 04:13:55.895 
Epoch 582/1000 
	 loss: 28.2878, MinusLogProbMetric: 28.2878, val_loss: 28.5049, val_MinusLogProbMetric: 28.5049

Epoch 582: val_loss did not improve from 28.28438
196/196 - 32s - loss: 28.2878 - MinusLogProbMetric: 28.2878 - val_loss: 28.5049 - val_MinusLogProbMetric: 28.5049 - lr: 5.0000e-04 - 32s/epoch - 164ms/step
Epoch 583/1000
2023-10-26 04:14:30.667 
Epoch 583/1000 
	 loss: 28.4758, MinusLogProbMetric: 28.4758, val_loss: 28.3846, val_MinusLogProbMetric: 28.3846

Epoch 583: val_loss did not improve from 28.28438
196/196 - 35s - loss: 28.4758 - MinusLogProbMetric: 28.4758 - val_loss: 28.3846 - val_MinusLogProbMetric: 28.3846 - lr: 5.0000e-04 - 35s/epoch - 177ms/step
Epoch 584/1000
2023-10-26 04:15:05.796 
Epoch 584/1000 
	 loss: 28.4173, MinusLogProbMetric: 28.4173, val_loss: 28.3211, val_MinusLogProbMetric: 28.3211

Epoch 584: val_loss did not improve from 28.28438
196/196 - 35s - loss: 28.4173 - MinusLogProbMetric: 28.4173 - val_loss: 28.3211 - val_MinusLogProbMetric: 28.3211 - lr: 5.0000e-04 - 35s/epoch - 179ms/step
Epoch 585/1000
2023-10-26 04:15:40.359 
Epoch 585/1000 
	 loss: 28.5734, MinusLogProbMetric: 28.5734, val_loss: 28.5152, val_MinusLogProbMetric: 28.5152

Epoch 585: val_loss did not improve from 28.28438
196/196 - 35s - loss: 28.5734 - MinusLogProbMetric: 28.5734 - val_loss: 28.5152 - val_MinusLogProbMetric: 28.5152 - lr: 5.0000e-04 - 35s/epoch - 176ms/step
Epoch 586/1000
2023-10-26 04:16:16.272 
Epoch 586/1000 
	 loss: 28.3827, MinusLogProbMetric: 28.3827, val_loss: 28.3692, val_MinusLogProbMetric: 28.3692

Epoch 586: val_loss did not improve from 28.28438
196/196 - 36s - loss: 28.3827 - MinusLogProbMetric: 28.3827 - val_loss: 28.3692 - val_MinusLogProbMetric: 28.3692 - lr: 5.0000e-04 - 36s/epoch - 183ms/step
Epoch 587/1000
2023-10-26 04:16:52.087 
Epoch 587/1000 
	 loss: 28.2541, MinusLogProbMetric: 28.2541, val_loss: 28.3584, val_MinusLogProbMetric: 28.3584

Epoch 587: val_loss did not improve from 28.28438
196/196 - 36s - loss: 28.2541 - MinusLogProbMetric: 28.2541 - val_loss: 28.3584 - val_MinusLogProbMetric: 28.3584 - lr: 5.0000e-04 - 36s/epoch - 183ms/step
Epoch 588/1000
2023-10-26 04:17:26.915 
Epoch 588/1000 
	 loss: 28.4845, MinusLogProbMetric: 28.4845, val_loss: 28.3878, val_MinusLogProbMetric: 28.3878

Epoch 588: val_loss did not improve from 28.28438
196/196 - 35s - loss: 28.4845 - MinusLogProbMetric: 28.4845 - val_loss: 28.3878 - val_MinusLogProbMetric: 28.3878 - lr: 5.0000e-04 - 35s/epoch - 178ms/step
Epoch 589/1000
2023-10-26 04:18:02.607 
Epoch 589/1000 
	 loss: 28.4242, MinusLogProbMetric: 28.4242, val_loss: 28.6242, val_MinusLogProbMetric: 28.6242

Epoch 589: val_loss did not improve from 28.28438
196/196 - 36s - loss: 28.4242 - MinusLogProbMetric: 28.4242 - val_loss: 28.6242 - val_MinusLogProbMetric: 28.6242 - lr: 5.0000e-04 - 36s/epoch - 182ms/step
Epoch 590/1000
2023-10-26 04:18:37.934 
Epoch 590/1000 
	 loss: 28.2940, MinusLogProbMetric: 28.2940, val_loss: 29.4834, val_MinusLogProbMetric: 29.4834

Epoch 590: val_loss did not improve from 28.28438
196/196 - 35s - loss: 28.2940 - MinusLogProbMetric: 28.2940 - val_loss: 29.4834 - val_MinusLogProbMetric: 29.4834 - lr: 5.0000e-04 - 35s/epoch - 180ms/step
Epoch 591/1000
2023-10-26 04:19:13.565 
Epoch 591/1000 
	 loss: 28.6732, MinusLogProbMetric: 28.6732, val_loss: 28.3622, val_MinusLogProbMetric: 28.3622

Epoch 591: val_loss did not improve from 28.28438
196/196 - 36s - loss: 28.6732 - MinusLogProbMetric: 28.6732 - val_loss: 28.3622 - val_MinusLogProbMetric: 28.3622 - lr: 5.0000e-04 - 36s/epoch - 182ms/step
Epoch 592/1000
2023-10-26 04:19:48.735 
Epoch 592/1000 
	 loss: 28.5783, MinusLogProbMetric: 28.5783, val_loss: 28.7991, val_MinusLogProbMetric: 28.7991

Epoch 592: val_loss did not improve from 28.28438
196/196 - 35s - loss: 28.5783 - MinusLogProbMetric: 28.5783 - val_loss: 28.7991 - val_MinusLogProbMetric: 28.7991 - lr: 5.0000e-04 - 35s/epoch - 179ms/step
Epoch 593/1000
2023-10-26 04:20:24.047 
Epoch 593/1000 
	 loss: 28.4001, MinusLogProbMetric: 28.4001, val_loss: 28.6374, val_MinusLogProbMetric: 28.6374

Epoch 593: val_loss did not improve from 28.28438
196/196 - 35s - loss: 28.4001 - MinusLogProbMetric: 28.4001 - val_loss: 28.6374 - val_MinusLogProbMetric: 28.6374 - lr: 5.0000e-04 - 35s/epoch - 180ms/step
Epoch 594/1000
2023-10-26 04:20:59.246 
Epoch 594/1000 
	 loss: 28.4022, MinusLogProbMetric: 28.4022, val_loss: 28.5912, val_MinusLogProbMetric: 28.5912

Epoch 594: val_loss did not improve from 28.28438
196/196 - 35s - loss: 28.4022 - MinusLogProbMetric: 28.4022 - val_loss: 28.5912 - val_MinusLogProbMetric: 28.5912 - lr: 5.0000e-04 - 35s/epoch - 180ms/step
Epoch 595/1000
2023-10-26 04:21:34.461 
Epoch 595/1000 
	 loss: 28.4407, MinusLogProbMetric: 28.4407, val_loss: 28.4118, val_MinusLogProbMetric: 28.4118

Epoch 595: val_loss did not improve from 28.28438
196/196 - 35s - loss: 28.4407 - MinusLogProbMetric: 28.4407 - val_loss: 28.4118 - val_MinusLogProbMetric: 28.4118 - lr: 5.0000e-04 - 35s/epoch - 180ms/step
Epoch 596/1000
2023-10-26 04:22:09.704 
Epoch 596/1000 
	 loss: 28.4183, MinusLogProbMetric: 28.4183, val_loss: 28.6924, val_MinusLogProbMetric: 28.6924

Epoch 596: val_loss did not improve from 28.28438
196/196 - 35s - loss: 28.4183 - MinusLogProbMetric: 28.4183 - val_loss: 28.6924 - val_MinusLogProbMetric: 28.6924 - lr: 5.0000e-04 - 35s/epoch - 180ms/step
Epoch 597/1000
2023-10-26 04:22:44.830 
Epoch 597/1000 
	 loss: 28.3466, MinusLogProbMetric: 28.3466, val_loss: 28.4130, val_MinusLogProbMetric: 28.4130

Epoch 597: val_loss did not improve from 28.28438
196/196 - 35s - loss: 28.3466 - MinusLogProbMetric: 28.3466 - val_loss: 28.4130 - val_MinusLogProbMetric: 28.4130 - lr: 5.0000e-04 - 35s/epoch - 179ms/step
Epoch 598/1000
2023-10-26 04:23:16.441 
Epoch 598/1000 
	 loss: 28.4609, MinusLogProbMetric: 28.4609, val_loss: 28.7914, val_MinusLogProbMetric: 28.7914

Epoch 598: val_loss did not improve from 28.28438
196/196 - 32s - loss: 28.4609 - MinusLogProbMetric: 28.4609 - val_loss: 28.7914 - val_MinusLogProbMetric: 28.7914 - lr: 5.0000e-04 - 32s/epoch - 161ms/step
Epoch 599/1000
2023-10-26 04:23:46.895 
Epoch 599/1000 
	 loss: 28.4101, MinusLogProbMetric: 28.4101, val_loss: 28.6127, val_MinusLogProbMetric: 28.6127

Epoch 599: val_loss did not improve from 28.28438
196/196 - 30s - loss: 28.4101 - MinusLogProbMetric: 28.4101 - val_loss: 28.6127 - val_MinusLogProbMetric: 28.6127 - lr: 5.0000e-04 - 30s/epoch - 155ms/step
Epoch 600/1000
2023-10-26 04:24:17.857 
Epoch 600/1000 
	 loss: 28.3931, MinusLogProbMetric: 28.3931, val_loss: 28.4998, val_MinusLogProbMetric: 28.4998

Epoch 600: val_loss did not improve from 28.28438
196/196 - 31s - loss: 28.3931 - MinusLogProbMetric: 28.3931 - val_loss: 28.4998 - val_MinusLogProbMetric: 28.4998 - lr: 5.0000e-04 - 31s/epoch - 158ms/step
Epoch 601/1000
2023-10-26 04:24:53.747 
Epoch 601/1000 
	 loss: 28.2317, MinusLogProbMetric: 28.2317, val_loss: 28.8562, val_MinusLogProbMetric: 28.8562

Epoch 601: val_loss did not improve from 28.28438
196/196 - 36s - loss: 28.2317 - MinusLogProbMetric: 28.2317 - val_loss: 28.8562 - val_MinusLogProbMetric: 28.8562 - lr: 5.0000e-04 - 36s/epoch - 183ms/step
Epoch 602/1000
2023-10-26 04:25:27.120 
Epoch 602/1000 
	 loss: 28.5802, MinusLogProbMetric: 28.5802, val_loss: 28.8086, val_MinusLogProbMetric: 28.8086

Epoch 602: val_loss did not improve from 28.28438
196/196 - 33s - loss: 28.5802 - MinusLogProbMetric: 28.5802 - val_loss: 28.8086 - val_MinusLogProbMetric: 28.8086 - lr: 5.0000e-04 - 33s/epoch - 170ms/step
Epoch 603/1000
2023-10-26 04:25:57.493 
Epoch 603/1000 
	 loss: 28.0185, MinusLogProbMetric: 28.0185, val_loss: 28.2467, val_MinusLogProbMetric: 28.2467

Epoch 603: val_loss improved from 28.28438 to 28.24668, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_369/weights/best_weights.h5
196/196 - 31s - loss: 28.0185 - MinusLogProbMetric: 28.0185 - val_loss: 28.2467 - val_MinusLogProbMetric: 28.2467 - lr: 2.5000e-04 - 31s/epoch - 158ms/step
Epoch 604/1000
2023-10-26 04:26:28.169 
Epoch 604/1000 
	 loss: 27.9554, MinusLogProbMetric: 27.9554, val_loss: 28.2148, val_MinusLogProbMetric: 28.2148

Epoch 604: val_loss improved from 28.24668 to 28.21475, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_369/weights/best_weights.h5
196/196 - 31s - loss: 27.9554 - MinusLogProbMetric: 27.9554 - val_loss: 28.2148 - val_MinusLogProbMetric: 28.2148 - lr: 2.5000e-04 - 31s/epoch - 156ms/step
Epoch 605/1000
2023-10-26 04:27:01.217 
Epoch 605/1000 
	 loss: 27.9547, MinusLogProbMetric: 27.9547, val_loss: 28.1829, val_MinusLogProbMetric: 28.1829

Epoch 605: val_loss improved from 28.21475 to 28.18287, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_369/weights/best_weights.h5
196/196 - 33s - loss: 27.9547 - MinusLogProbMetric: 27.9547 - val_loss: 28.1829 - val_MinusLogProbMetric: 28.1829 - lr: 2.5000e-04 - 33s/epoch - 170ms/step
Epoch 606/1000
2023-10-26 04:27:36.820 
Epoch 606/1000 
	 loss: 27.9732, MinusLogProbMetric: 27.9732, val_loss: 28.3512, val_MinusLogProbMetric: 28.3512

Epoch 606: val_loss did not improve from 28.18287
196/196 - 35s - loss: 27.9732 - MinusLogProbMetric: 27.9732 - val_loss: 28.3512 - val_MinusLogProbMetric: 28.3512 - lr: 2.5000e-04 - 35s/epoch - 178ms/step
Epoch 607/1000
2023-10-26 04:28:07.527 
Epoch 607/1000 
	 loss: 27.9830, MinusLogProbMetric: 27.9830, val_loss: 28.2217, val_MinusLogProbMetric: 28.2217

Epoch 607: val_loss did not improve from 28.18287
196/196 - 31s - loss: 27.9830 - MinusLogProbMetric: 27.9830 - val_loss: 28.2217 - val_MinusLogProbMetric: 28.2217 - lr: 2.5000e-04 - 31s/epoch - 157ms/step
Epoch 608/1000
2023-10-26 04:28:37.106 
Epoch 608/1000 
	 loss: 28.0701, MinusLogProbMetric: 28.0701, val_loss: 28.3188, val_MinusLogProbMetric: 28.3188

Epoch 608: val_loss did not improve from 28.18287
196/196 - 30s - loss: 28.0701 - MinusLogProbMetric: 28.0701 - val_loss: 28.3188 - val_MinusLogProbMetric: 28.3188 - lr: 2.5000e-04 - 30s/epoch - 151ms/step
Epoch 609/1000
2023-10-26 04:29:06.855 
Epoch 609/1000 
	 loss: 28.0832, MinusLogProbMetric: 28.0832, val_loss: 28.2069, val_MinusLogProbMetric: 28.2069

Epoch 609: val_loss did not improve from 28.18287
196/196 - 30s - loss: 28.0832 - MinusLogProbMetric: 28.0832 - val_loss: 28.2069 - val_MinusLogProbMetric: 28.2069 - lr: 2.5000e-04 - 30s/epoch - 152ms/step
Epoch 610/1000
2023-10-26 04:29:40.672 
Epoch 610/1000 
	 loss: 28.0234, MinusLogProbMetric: 28.0234, val_loss: 28.2013, val_MinusLogProbMetric: 28.2013

Epoch 610: val_loss did not improve from 28.18287
196/196 - 34s - loss: 28.0234 - MinusLogProbMetric: 28.0234 - val_loss: 28.2013 - val_MinusLogProbMetric: 28.2013 - lr: 2.5000e-04 - 34s/epoch - 173ms/step
Epoch 611/1000
2023-10-26 04:30:10.651 
Epoch 611/1000 
	 loss: 28.0362, MinusLogProbMetric: 28.0362, val_loss: 28.2002, val_MinusLogProbMetric: 28.2002

Epoch 611: val_loss did not improve from 28.18287
196/196 - 30s - loss: 28.0362 - MinusLogProbMetric: 28.0362 - val_loss: 28.2002 - val_MinusLogProbMetric: 28.2002 - lr: 2.5000e-04 - 30s/epoch - 153ms/step
Epoch 612/1000
2023-10-26 04:30:41.126 
Epoch 612/1000 
	 loss: 27.9950, MinusLogProbMetric: 27.9950, val_loss: 28.3119, val_MinusLogProbMetric: 28.3119

Epoch 612: val_loss did not improve from 28.18287
196/196 - 30s - loss: 27.9950 - MinusLogProbMetric: 27.9950 - val_loss: 28.3119 - val_MinusLogProbMetric: 28.3119 - lr: 2.5000e-04 - 30s/epoch - 155ms/step
Epoch 613/1000
2023-10-26 04:31:11.568 
Epoch 613/1000 
	 loss: 27.9739, MinusLogProbMetric: 27.9739, val_loss: 28.2934, val_MinusLogProbMetric: 28.2934

Epoch 613: val_loss did not improve from 28.18287
196/196 - 30s - loss: 27.9739 - MinusLogProbMetric: 27.9739 - val_loss: 28.2934 - val_MinusLogProbMetric: 28.2934 - lr: 2.5000e-04 - 30s/epoch - 155ms/step
Epoch 614/1000
2023-10-26 04:31:44.660 
Epoch 614/1000 
	 loss: 27.9934, MinusLogProbMetric: 27.9934, val_loss: 28.1631, val_MinusLogProbMetric: 28.1631

Epoch 614: val_loss improved from 28.18287 to 28.16307, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_369/weights/best_weights.h5
196/196 - 34s - loss: 27.9934 - MinusLogProbMetric: 27.9934 - val_loss: 28.1631 - val_MinusLogProbMetric: 28.1631 - lr: 2.5000e-04 - 34s/epoch - 172ms/step
Epoch 615/1000
2023-10-26 04:32:17.898 
Epoch 615/1000 
	 loss: 27.9882, MinusLogProbMetric: 27.9882, val_loss: 28.1537, val_MinusLogProbMetric: 28.1537

Epoch 615: val_loss improved from 28.16307 to 28.15372, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_369/weights/best_weights.h5
196/196 - 33s - loss: 27.9882 - MinusLogProbMetric: 27.9882 - val_loss: 28.1537 - val_MinusLogProbMetric: 28.1537 - lr: 2.5000e-04 - 33s/epoch - 170ms/step
Epoch 616/1000
2023-10-26 04:32:49.067 
Epoch 616/1000 
	 loss: 28.0614, MinusLogProbMetric: 28.0614, val_loss: 28.2003, val_MinusLogProbMetric: 28.2003

Epoch 616: val_loss did not improve from 28.15372
196/196 - 31s - loss: 28.0614 - MinusLogProbMetric: 28.0614 - val_loss: 28.2003 - val_MinusLogProbMetric: 28.2003 - lr: 2.5000e-04 - 31s/epoch - 156ms/step
Epoch 617/1000
2023-10-26 04:33:19.074 
Epoch 617/1000 
	 loss: 27.9873, MinusLogProbMetric: 27.9873, val_loss: 28.2308, val_MinusLogProbMetric: 28.2308

Epoch 617: val_loss did not improve from 28.15372
196/196 - 30s - loss: 27.9873 - MinusLogProbMetric: 27.9873 - val_loss: 28.2308 - val_MinusLogProbMetric: 28.2308 - lr: 2.5000e-04 - 30s/epoch - 153ms/step
Epoch 618/1000
2023-10-26 04:33:50.405 
Epoch 618/1000 
	 loss: 28.1149, MinusLogProbMetric: 28.1149, val_loss: 28.2781, val_MinusLogProbMetric: 28.2781

Epoch 618: val_loss did not improve from 28.15372
196/196 - 31s - loss: 28.1149 - MinusLogProbMetric: 28.1149 - val_loss: 28.2781 - val_MinusLogProbMetric: 28.2781 - lr: 2.5000e-04 - 31s/epoch - 160ms/step
Epoch 619/1000
2023-10-26 04:34:25.494 
Epoch 619/1000 
	 loss: 28.0218, MinusLogProbMetric: 28.0218, val_loss: 28.3485, val_MinusLogProbMetric: 28.3485

Epoch 619: val_loss did not improve from 28.15372
196/196 - 35s - loss: 28.0218 - MinusLogProbMetric: 28.0218 - val_loss: 28.3485 - val_MinusLogProbMetric: 28.3485 - lr: 2.5000e-04 - 35s/epoch - 179ms/step
Epoch 620/1000
2023-10-26 04:34:58.720 
Epoch 620/1000 
	 loss: 28.1407, MinusLogProbMetric: 28.1407, val_loss: 28.2181, val_MinusLogProbMetric: 28.2181

Epoch 620: val_loss did not improve from 28.15372
196/196 - 33s - loss: 28.1407 - MinusLogProbMetric: 28.1407 - val_loss: 28.2181 - val_MinusLogProbMetric: 28.2181 - lr: 2.5000e-04 - 33s/epoch - 170ms/step
Epoch 621/1000
2023-10-26 04:35:29.043 
Epoch 621/1000 
	 loss: 27.9939, MinusLogProbMetric: 27.9939, val_loss: 29.0073, val_MinusLogProbMetric: 29.0073

Epoch 621: val_loss did not improve from 28.15372
196/196 - 30s - loss: 27.9939 - MinusLogProbMetric: 27.9939 - val_loss: 29.0073 - val_MinusLogProbMetric: 29.0073 - lr: 2.5000e-04 - 30s/epoch - 155ms/step
Epoch 622/1000
2023-10-26 04:35:59.468 
Epoch 622/1000 
	 loss: 28.0973, MinusLogProbMetric: 28.0973, val_loss: 28.6580, val_MinusLogProbMetric: 28.6580

Epoch 622: val_loss did not improve from 28.15372
196/196 - 30s - loss: 28.0973 - MinusLogProbMetric: 28.0973 - val_loss: 28.6580 - val_MinusLogProbMetric: 28.6580 - lr: 2.5000e-04 - 30s/epoch - 155ms/step
Epoch 623/1000
2023-10-26 04:36:34.655 
Epoch 623/1000 
	 loss: 28.0982, MinusLogProbMetric: 28.0982, val_loss: 28.4583, val_MinusLogProbMetric: 28.4583

Epoch 623: val_loss did not improve from 28.15372
196/196 - 35s - loss: 28.0982 - MinusLogProbMetric: 28.0982 - val_loss: 28.4583 - val_MinusLogProbMetric: 28.4583 - lr: 2.5000e-04 - 35s/epoch - 179ms/step
Epoch 624/1000
2023-10-26 04:37:08.224 
Epoch 624/1000 
	 loss: 27.9862, MinusLogProbMetric: 27.9862, val_loss: 28.5880, val_MinusLogProbMetric: 28.5880

Epoch 624: val_loss did not improve from 28.15372
196/196 - 34s - loss: 27.9862 - MinusLogProbMetric: 27.9862 - val_loss: 28.5880 - val_MinusLogProbMetric: 28.5880 - lr: 2.5000e-04 - 34s/epoch - 171ms/step
Epoch 625/1000
2023-10-26 04:37:38.335 
Epoch 625/1000 
	 loss: 28.1671, MinusLogProbMetric: 28.1671, val_loss: 28.2427, val_MinusLogProbMetric: 28.2427

Epoch 625: val_loss did not improve from 28.15372
196/196 - 30s - loss: 28.1671 - MinusLogProbMetric: 28.1671 - val_loss: 28.2427 - val_MinusLogProbMetric: 28.2427 - lr: 2.5000e-04 - 30s/epoch - 154ms/step
Epoch 626/1000
2023-10-26 04:38:08.497 
Epoch 626/1000 
	 loss: 27.9741, MinusLogProbMetric: 27.9741, val_loss: 28.2416, val_MinusLogProbMetric: 28.2416

Epoch 626: val_loss did not improve from 28.15372
196/196 - 30s - loss: 27.9741 - MinusLogProbMetric: 27.9741 - val_loss: 28.2416 - val_MinusLogProbMetric: 28.2416 - lr: 2.5000e-04 - 30s/epoch - 154ms/step
Epoch 627/1000
2023-10-26 04:38:40.404 
Epoch 627/1000 
	 loss: 27.9985, MinusLogProbMetric: 27.9985, val_loss: 28.3648, val_MinusLogProbMetric: 28.3648

Epoch 627: val_loss did not improve from 28.15372
196/196 - 32s - loss: 27.9985 - MinusLogProbMetric: 27.9985 - val_loss: 28.3648 - val_MinusLogProbMetric: 28.3648 - lr: 2.5000e-04 - 32s/epoch - 163ms/step
Epoch 628/1000
2023-10-26 04:39:15.564 
Epoch 628/1000 
	 loss: 28.0163, MinusLogProbMetric: 28.0163, val_loss: 28.1669, val_MinusLogProbMetric: 28.1669

Epoch 628: val_loss did not improve from 28.15372
196/196 - 35s - loss: 28.0163 - MinusLogProbMetric: 28.0163 - val_loss: 28.1669 - val_MinusLogProbMetric: 28.1669 - lr: 2.5000e-04 - 35s/epoch - 179ms/step
Epoch 629/1000
2023-10-26 04:39:47.833 
Epoch 629/1000 
	 loss: 28.1511, MinusLogProbMetric: 28.1511, val_loss: 28.4685, val_MinusLogProbMetric: 28.4685

Epoch 629: val_loss did not improve from 28.15372
196/196 - 32s - loss: 28.1511 - MinusLogProbMetric: 28.1511 - val_loss: 28.4685 - val_MinusLogProbMetric: 28.4685 - lr: 2.5000e-04 - 32s/epoch - 165ms/step
Epoch 630/1000
2023-10-26 04:40:17.977 
Epoch 630/1000 
	 loss: 27.9431, MinusLogProbMetric: 27.9431, val_loss: 28.2370, val_MinusLogProbMetric: 28.2370

Epoch 630: val_loss did not improve from 28.15372
196/196 - 30s - loss: 27.9431 - MinusLogProbMetric: 27.9431 - val_loss: 28.2370 - val_MinusLogProbMetric: 28.2370 - lr: 2.5000e-04 - 30s/epoch - 154ms/step
Epoch 631/1000
2023-10-26 04:40:47.812 
Epoch 631/1000 
	 loss: 28.1074, MinusLogProbMetric: 28.1074, val_loss: 28.2826, val_MinusLogProbMetric: 28.2826

Epoch 631: val_loss did not improve from 28.15372
196/196 - 30s - loss: 28.1074 - MinusLogProbMetric: 28.1074 - val_loss: 28.2826 - val_MinusLogProbMetric: 28.2826 - lr: 2.5000e-04 - 30s/epoch - 152ms/step
Epoch 632/1000
2023-10-26 04:41:21.904 
Epoch 632/1000 
	 loss: 27.9956, MinusLogProbMetric: 27.9956, val_loss: 28.2287, val_MinusLogProbMetric: 28.2287

Epoch 632: val_loss did not improve from 28.15372
196/196 - 34s - loss: 27.9956 - MinusLogProbMetric: 27.9956 - val_loss: 28.2287 - val_MinusLogProbMetric: 28.2287 - lr: 2.5000e-04 - 34s/epoch - 174ms/step
Epoch 633/1000
2023-10-26 04:41:56.454 
Epoch 633/1000 
	 loss: 28.0149, MinusLogProbMetric: 28.0149, val_loss: 28.1084, val_MinusLogProbMetric: 28.1084

Epoch 633: val_loss improved from 28.15372 to 28.10838, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_369/weights/best_weights.h5
196/196 - 35s - loss: 28.0149 - MinusLogProbMetric: 28.0149 - val_loss: 28.1084 - val_MinusLogProbMetric: 28.1084 - lr: 2.5000e-04 - 35s/epoch - 179ms/step
Epoch 634/1000
2023-10-26 04:42:27.951 
Epoch 634/1000 
	 loss: 28.0974, MinusLogProbMetric: 28.0974, val_loss: 28.2873, val_MinusLogProbMetric: 28.2873

Epoch 634: val_loss did not improve from 28.10838
196/196 - 31s - loss: 28.0974 - MinusLogProbMetric: 28.0974 - val_loss: 28.2873 - val_MinusLogProbMetric: 28.2873 - lr: 2.5000e-04 - 31s/epoch - 158ms/step
Epoch 635/1000
2023-10-26 04:42:58.742 
Epoch 635/1000 
	 loss: 28.3533, MinusLogProbMetric: 28.3533, val_loss: 28.5724, val_MinusLogProbMetric: 28.5724

Epoch 635: val_loss did not improve from 28.10838
196/196 - 31s - loss: 28.3533 - MinusLogProbMetric: 28.3533 - val_loss: 28.5724 - val_MinusLogProbMetric: 28.5724 - lr: 2.5000e-04 - 31s/epoch - 157ms/step
Epoch 636/1000
2023-10-26 04:43:29.765 
Epoch 636/1000 
	 loss: 28.0285, MinusLogProbMetric: 28.0285, val_loss: 28.2326, val_MinusLogProbMetric: 28.2326

Epoch 636: val_loss did not improve from 28.10838
196/196 - 31s - loss: 28.0285 - MinusLogProbMetric: 28.0285 - val_loss: 28.2326 - val_MinusLogProbMetric: 28.2326 - lr: 2.5000e-04 - 31s/epoch - 158ms/step
Epoch 637/1000
2023-10-26 04:44:04.998 
Epoch 637/1000 
	 loss: 28.1902, MinusLogProbMetric: 28.1902, val_loss: 28.5686, val_MinusLogProbMetric: 28.5686

Epoch 637: val_loss did not improve from 28.10838
196/196 - 35s - loss: 28.1902 - MinusLogProbMetric: 28.1902 - val_loss: 28.5686 - val_MinusLogProbMetric: 28.5686 - lr: 2.5000e-04 - 35s/epoch - 180ms/step
Epoch 638/1000
2023-10-26 04:44:39.348 
Epoch 638/1000 
	 loss: 27.9927, MinusLogProbMetric: 27.9927, val_loss: 28.3659, val_MinusLogProbMetric: 28.3659

Epoch 638: val_loss did not improve from 28.10838
196/196 - 34s - loss: 27.9927 - MinusLogProbMetric: 27.9927 - val_loss: 28.3659 - val_MinusLogProbMetric: 28.3659 - lr: 2.5000e-04 - 34s/epoch - 175ms/step
Epoch 639/1000
2023-10-26 04:45:09.738 
Epoch 639/1000 
	 loss: 27.9958, MinusLogProbMetric: 27.9958, val_loss: 28.4667, val_MinusLogProbMetric: 28.4667

Epoch 639: val_loss did not improve from 28.10838
196/196 - 30s - loss: 27.9958 - MinusLogProbMetric: 27.9958 - val_loss: 28.4667 - val_MinusLogProbMetric: 28.4667 - lr: 2.5000e-04 - 30s/epoch - 155ms/step
Epoch 640/1000
2023-10-26 04:45:39.770 
Epoch 640/1000 
	 loss: 28.0592, MinusLogProbMetric: 28.0592, val_loss: 28.4683, val_MinusLogProbMetric: 28.4683

Epoch 640: val_loss did not improve from 28.10838
196/196 - 30s - loss: 28.0592 - MinusLogProbMetric: 28.0592 - val_loss: 28.4683 - val_MinusLogProbMetric: 28.4683 - lr: 2.5000e-04 - 30s/epoch - 153ms/step
Epoch 641/1000
2023-10-26 04:46:12.683 
Epoch 641/1000 
	 loss: 28.0225, MinusLogProbMetric: 28.0225, val_loss: 28.1299, val_MinusLogProbMetric: 28.1299

Epoch 641: val_loss did not improve from 28.10838
196/196 - 33s - loss: 28.0225 - MinusLogProbMetric: 28.0225 - val_loss: 28.1299 - val_MinusLogProbMetric: 28.1299 - lr: 2.5000e-04 - 33s/epoch - 168ms/step
Epoch 642/1000
2023-10-26 04:46:45.738 
Epoch 642/1000 
	 loss: 28.0840, MinusLogProbMetric: 28.0840, val_loss: 29.8029, val_MinusLogProbMetric: 29.8029

Epoch 642: val_loss did not improve from 28.10838
196/196 - 33s - loss: 28.0840 - MinusLogProbMetric: 28.0840 - val_loss: 29.8029 - val_MinusLogProbMetric: 29.8029 - lr: 2.5000e-04 - 33s/epoch - 169ms/step
Epoch 643/1000
2023-10-26 04:47:15.516 
Epoch 643/1000 
	 loss: 27.9983, MinusLogProbMetric: 27.9983, val_loss: 28.1166, val_MinusLogProbMetric: 28.1166

Epoch 643: val_loss did not improve from 28.10838
196/196 - 30s - loss: 27.9983 - MinusLogProbMetric: 27.9983 - val_loss: 28.1166 - val_MinusLogProbMetric: 28.1166 - lr: 2.5000e-04 - 30s/epoch - 152ms/step
Epoch 644/1000
2023-10-26 04:47:45.488 
Epoch 644/1000 
	 loss: 27.9954, MinusLogProbMetric: 27.9954, val_loss: 28.1819, val_MinusLogProbMetric: 28.1819

Epoch 644: val_loss did not improve from 28.10838
196/196 - 30s - loss: 27.9954 - MinusLogProbMetric: 27.9954 - val_loss: 28.1819 - val_MinusLogProbMetric: 28.1819 - lr: 2.5000e-04 - 30s/epoch - 153ms/step
Epoch 645/1000
2023-10-26 04:48:15.723 
Epoch 645/1000 
	 loss: 27.9688, MinusLogProbMetric: 27.9688, val_loss: 28.2379, val_MinusLogProbMetric: 28.2379

Epoch 645: val_loss did not improve from 28.10838
196/196 - 30s - loss: 27.9688 - MinusLogProbMetric: 27.9688 - val_loss: 28.2379 - val_MinusLogProbMetric: 28.2379 - lr: 2.5000e-04 - 30s/epoch - 154ms/step
Epoch 646/1000
2023-10-26 04:48:50.374 
Epoch 646/1000 
	 loss: 28.0476, MinusLogProbMetric: 28.0476, val_loss: 28.3035, val_MinusLogProbMetric: 28.3035

Epoch 646: val_loss did not improve from 28.10838
196/196 - 35s - loss: 28.0476 - MinusLogProbMetric: 28.0476 - val_loss: 28.3035 - val_MinusLogProbMetric: 28.3035 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 647/1000
2023-10-26 04:49:22.606 
Epoch 647/1000 
	 loss: 28.1528, MinusLogProbMetric: 28.1528, val_loss: 28.0847, val_MinusLogProbMetric: 28.0847

Epoch 647: val_loss improved from 28.10838 to 28.08474, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_369/weights/best_weights.h5
196/196 - 33s - loss: 28.1528 - MinusLogProbMetric: 28.1528 - val_loss: 28.0847 - val_MinusLogProbMetric: 28.0847 - lr: 2.5000e-04 - 33s/epoch - 167ms/step
Epoch 648/1000
2023-10-26 04:49:53.484 
Epoch 648/1000 
	 loss: 27.9466, MinusLogProbMetric: 27.9466, val_loss: 28.1845, val_MinusLogProbMetric: 28.1845

Epoch 648: val_loss did not improve from 28.08474
196/196 - 30s - loss: 27.9466 - MinusLogProbMetric: 27.9466 - val_loss: 28.1845 - val_MinusLogProbMetric: 28.1845 - lr: 2.5000e-04 - 30s/epoch - 155ms/step
Epoch 649/1000
2023-10-26 04:50:24.020 
Epoch 649/1000 
	 loss: 28.1863, MinusLogProbMetric: 28.1863, val_loss: 28.1543, val_MinusLogProbMetric: 28.1543

Epoch 649: val_loss did not improve from 28.08474
196/196 - 31s - loss: 28.1863 - MinusLogProbMetric: 28.1863 - val_loss: 28.1543 - val_MinusLogProbMetric: 28.1543 - lr: 2.5000e-04 - 31s/epoch - 156ms/step
Epoch 650/1000
2023-10-26 04:50:57.554 
Epoch 650/1000 
	 loss: 27.9620, MinusLogProbMetric: 27.9620, val_loss: 28.2188, val_MinusLogProbMetric: 28.2188

Epoch 650: val_loss did not improve from 28.08474
196/196 - 34s - loss: 27.9620 - MinusLogProbMetric: 27.9620 - val_loss: 28.2188 - val_MinusLogProbMetric: 28.2188 - lr: 2.5000e-04 - 34s/epoch - 171ms/step
Epoch 651/1000
2023-10-26 04:51:32.285 
Epoch 651/1000 
	 loss: 27.9742, MinusLogProbMetric: 27.9742, val_loss: 28.3335, val_MinusLogProbMetric: 28.3335

Epoch 651: val_loss did not improve from 28.08474
196/196 - 35s - loss: 27.9742 - MinusLogProbMetric: 27.9742 - val_loss: 28.3335 - val_MinusLogProbMetric: 28.3335 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 652/1000
2023-10-26 04:52:04.779 
Epoch 652/1000 
	 loss: 28.0727, MinusLogProbMetric: 28.0727, val_loss: 28.5306, val_MinusLogProbMetric: 28.5306

Epoch 652: val_loss did not improve from 28.08474
196/196 - 32s - loss: 28.0727 - MinusLogProbMetric: 28.0727 - val_loss: 28.5306 - val_MinusLogProbMetric: 28.5306 - lr: 2.5000e-04 - 32s/epoch - 166ms/step
Epoch 653/1000
2023-10-26 04:52:34.598 
Epoch 653/1000 
	 loss: 28.0394, MinusLogProbMetric: 28.0394, val_loss: 28.5990, val_MinusLogProbMetric: 28.5990

Epoch 653: val_loss did not improve from 28.08474
196/196 - 30s - loss: 28.0394 - MinusLogProbMetric: 28.0394 - val_loss: 28.5990 - val_MinusLogProbMetric: 28.5990 - lr: 2.5000e-04 - 30s/epoch - 152ms/step
Epoch 654/1000
2023-10-26 04:53:04.160 
Epoch 654/1000 
	 loss: 28.1307, MinusLogProbMetric: 28.1307, val_loss: 28.1663, val_MinusLogProbMetric: 28.1663

Epoch 654: val_loss did not improve from 28.08474
196/196 - 30s - loss: 28.1307 - MinusLogProbMetric: 28.1307 - val_loss: 28.1663 - val_MinusLogProbMetric: 28.1663 - lr: 2.5000e-04 - 30s/epoch - 151ms/step
Epoch 655/1000
2023-10-26 04:53:34.301 
Epoch 655/1000 
	 loss: 28.1007, MinusLogProbMetric: 28.1007, val_loss: 29.0111, val_MinusLogProbMetric: 29.0111

Epoch 655: val_loss did not improve from 28.08474
196/196 - 30s - loss: 28.1007 - MinusLogProbMetric: 28.1007 - val_loss: 29.0111 - val_MinusLogProbMetric: 29.0111 - lr: 2.5000e-04 - 30s/epoch - 154ms/step
Epoch 656/1000
2023-10-26 04:54:05.672 
Epoch 656/1000 
	 loss: 28.0694, MinusLogProbMetric: 28.0694, val_loss: 28.2084, val_MinusLogProbMetric: 28.2084

Epoch 656: val_loss did not improve from 28.08474
196/196 - 31s - loss: 28.0694 - MinusLogProbMetric: 28.0694 - val_loss: 28.2084 - val_MinusLogProbMetric: 28.2084 - lr: 2.5000e-04 - 31s/epoch - 160ms/step
Epoch 657/1000
2023-10-26 04:54:39.257 
Epoch 657/1000 
	 loss: 28.0393, MinusLogProbMetric: 28.0393, val_loss: 28.1505, val_MinusLogProbMetric: 28.1505

Epoch 657: val_loss did not improve from 28.08474
196/196 - 34s - loss: 28.0393 - MinusLogProbMetric: 28.0393 - val_loss: 28.1505 - val_MinusLogProbMetric: 28.1505 - lr: 2.5000e-04 - 34s/epoch - 171ms/step
Epoch 658/1000
2023-10-26 04:55:09.850 
Epoch 658/1000 
	 loss: 27.9663, MinusLogProbMetric: 27.9663, val_loss: 28.3486, val_MinusLogProbMetric: 28.3486

Epoch 658: val_loss did not improve from 28.08474
196/196 - 31s - loss: 27.9663 - MinusLogProbMetric: 27.9663 - val_loss: 28.3486 - val_MinusLogProbMetric: 28.3486 - lr: 2.5000e-04 - 31s/epoch - 156ms/step
Epoch 659/1000
2023-10-26 04:55:39.228 
Epoch 659/1000 
	 loss: 28.0527, MinusLogProbMetric: 28.0527, val_loss: 28.1405, val_MinusLogProbMetric: 28.1405

Epoch 659: val_loss did not improve from 28.08474
196/196 - 29s - loss: 28.0527 - MinusLogProbMetric: 28.0527 - val_loss: 28.1405 - val_MinusLogProbMetric: 28.1405 - lr: 2.5000e-04 - 29s/epoch - 150ms/step
Epoch 660/1000
2023-10-26 04:56:09.063 
Epoch 660/1000 
	 loss: 27.9458, MinusLogProbMetric: 27.9458, val_loss: 28.3882, val_MinusLogProbMetric: 28.3882

Epoch 660: val_loss did not improve from 28.08474
196/196 - 30s - loss: 27.9458 - MinusLogProbMetric: 27.9458 - val_loss: 28.3882 - val_MinusLogProbMetric: 28.3882 - lr: 2.5000e-04 - 30s/epoch - 152ms/step
Epoch 661/1000
2023-10-26 04:56:40.178 
Epoch 661/1000 
	 loss: 28.0676, MinusLogProbMetric: 28.0676, val_loss: 28.1629, val_MinusLogProbMetric: 28.1629

Epoch 661: val_loss did not improve from 28.08474
196/196 - 31s - loss: 28.0676 - MinusLogProbMetric: 28.0676 - val_loss: 28.1629 - val_MinusLogProbMetric: 28.1629 - lr: 2.5000e-04 - 31s/epoch - 159ms/step
Epoch 662/1000
2023-10-26 04:57:15.616 
Epoch 662/1000 
	 loss: 28.1221, MinusLogProbMetric: 28.1221, val_loss: 28.0901, val_MinusLogProbMetric: 28.0901

Epoch 662: val_loss did not improve from 28.08474
196/196 - 35s - loss: 28.1221 - MinusLogProbMetric: 28.1221 - val_loss: 28.0901 - val_MinusLogProbMetric: 28.0901 - lr: 2.5000e-04 - 35s/epoch - 181ms/step
Epoch 663/1000
2023-10-26 04:57:48.118 
Epoch 663/1000 
	 loss: 28.0123, MinusLogProbMetric: 28.0123, val_loss: 28.1058, val_MinusLogProbMetric: 28.1058

Epoch 663: val_loss did not improve from 28.08474
196/196 - 32s - loss: 28.0123 - MinusLogProbMetric: 28.0123 - val_loss: 28.1058 - val_MinusLogProbMetric: 28.1058 - lr: 2.5000e-04 - 32s/epoch - 166ms/step
Epoch 664/1000
2023-10-26 04:58:23.402 
Epoch 664/1000 
	 loss: 28.0546, MinusLogProbMetric: 28.0546, val_loss: 28.1809, val_MinusLogProbMetric: 28.1809

Epoch 664: val_loss did not improve from 28.08474
196/196 - 35s - loss: 28.0546 - MinusLogProbMetric: 28.0546 - val_loss: 28.1809 - val_MinusLogProbMetric: 28.1809 - lr: 2.5000e-04 - 35s/epoch - 180ms/step
Epoch 665/1000
2023-10-26 04:58:56.320 
Epoch 665/1000 
	 loss: 28.0630, MinusLogProbMetric: 28.0630, val_loss: 28.1929, val_MinusLogProbMetric: 28.1929

Epoch 665: val_loss did not improve from 28.08474
196/196 - 33s - loss: 28.0630 - MinusLogProbMetric: 28.0630 - val_loss: 28.1929 - val_MinusLogProbMetric: 28.1929 - lr: 2.5000e-04 - 33s/epoch - 168ms/step
Epoch 666/1000
2023-10-26 04:59:29.418 
Epoch 666/1000 
	 loss: 28.0489, MinusLogProbMetric: 28.0489, val_loss: 28.2052, val_MinusLogProbMetric: 28.2052

Epoch 666: val_loss did not improve from 28.08474
196/196 - 33s - loss: 28.0489 - MinusLogProbMetric: 28.0489 - val_loss: 28.2052 - val_MinusLogProbMetric: 28.2052 - lr: 2.5000e-04 - 33s/epoch - 169ms/step
Epoch 667/1000
2023-10-26 05:00:00.019 
Epoch 667/1000 
	 loss: 27.9597, MinusLogProbMetric: 27.9597, val_loss: 28.2437, val_MinusLogProbMetric: 28.2437

Epoch 667: val_loss did not improve from 28.08474
196/196 - 31s - loss: 27.9597 - MinusLogProbMetric: 27.9597 - val_loss: 28.2437 - val_MinusLogProbMetric: 28.2437 - lr: 2.5000e-04 - 31s/epoch - 156ms/step
Epoch 668/1000
2023-10-26 05:00:29.984 
Epoch 668/1000 
	 loss: 28.0958, MinusLogProbMetric: 28.0958, val_loss: 28.2624, val_MinusLogProbMetric: 28.2624

Epoch 668: val_loss did not improve from 28.08474
196/196 - 30s - loss: 28.0958 - MinusLogProbMetric: 28.0958 - val_loss: 28.2624 - val_MinusLogProbMetric: 28.2624 - lr: 2.5000e-04 - 30s/epoch - 153ms/step
Epoch 669/1000
2023-10-26 05:01:03.664 
Epoch 669/1000 
	 loss: 27.9954, MinusLogProbMetric: 27.9954, val_loss: 28.3167, val_MinusLogProbMetric: 28.3167

Epoch 669: val_loss did not improve from 28.08474
196/196 - 34s - loss: 27.9954 - MinusLogProbMetric: 27.9954 - val_loss: 28.3167 - val_MinusLogProbMetric: 28.3167 - lr: 2.5000e-04 - 34s/epoch - 172ms/step
Epoch 670/1000
2023-10-26 05:01:37.698 
Epoch 670/1000 
	 loss: 28.0705, MinusLogProbMetric: 28.0705, val_loss: 28.1608, val_MinusLogProbMetric: 28.1608

Epoch 670: val_loss did not improve from 28.08474
196/196 - 34s - loss: 28.0705 - MinusLogProbMetric: 28.0705 - val_loss: 28.1608 - val_MinusLogProbMetric: 28.1608 - lr: 2.5000e-04 - 34s/epoch - 174ms/step
Epoch 671/1000
2023-10-26 05:02:08.216 
Epoch 671/1000 
	 loss: 27.9980, MinusLogProbMetric: 27.9980, val_loss: 28.3209, val_MinusLogProbMetric: 28.3209

Epoch 671: val_loss did not improve from 28.08474
196/196 - 31s - loss: 27.9980 - MinusLogProbMetric: 27.9980 - val_loss: 28.3209 - val_MinusLogProbMetric: 28.3209 - lr: 2.5000e-04 - 31s/epoch - 156ms/step
Epoch 672/1000
2023-10-26 05:02:38.663 
Epoch 672/1000 
	 loss: 28.0080, MinusLogProbMetric: 28.0080, val_loss: 28.2932, val_MinusLogProbMetric: 28.2932

Epoch 672: val_loss did not improve from 28.08474
196/196 - 30s - loss: 28.0080 - MinusLogProbMetric: 28.0080 - val_loss: 28.2932 - val_MinusLogProbMetric: 28.2932 - lr: 2.5000e-04 - 30s/epoch - 155ms/step
Epoch 673/1000
2023-10-26 05:03:08.915 
Epoch 673/1000 
	 loss: 28.1049, MinusLogProbMetric: 28.1049, val_loss: 28.2320, val_MinusLogProbMetric: 28.2320

Epoch 673: val_loss did not improve from 28.08474
196/196 - 30s - loss: 28.1049 - MinusLogProbMetric: 28.1049 - val_loss: 28.2320 - val_MinusLogProbMetric: 28.2320 - lr: 2.5000e-04 - 30s/epoch - 154ms/step
Epoch 674/1000
2023-10-26 05:03:39.676 
Epoch 674/1000 
	 loss: 28.0048, MinusLogProbMetric: 28.0048, val_loss: 28.4020, val_MinusLogProbMetric: 28.4020

Epoch 674: val_loss did not improve from 28.08474
196/196 - 31s - loss: 28.0048 - MinusLogProbMetric: 28.0048 - val_loss: 28.4020 - val_MinusLogProbMetric: 28.4020 - lr: 2.5000e-04 - 31s/epoch - 157ms/step
Epoch 675/1000
2023-10-26 05:04:14.673 
Epoch 675/1000 
	 loss: 28.1466, MinusLogProbMetric: 28.1466, val_loss: 28.6676, val_MinusLogProbMetric: 28.6676

Epoch 675: val_loss did not improve from 28.08474
196/196 - 35s - loss: 28.1466 - MinusLogProbMetric: 28.1466 - val_loss: 28.6676 - val_MinusLogProbMetric: 28.6676 - lr: 2.5000e-04 - 35s/epoch - 179ms/step
Epoch 676/1000
2023-10-26 05:04:47.677 
Epoch 676/1000 
	 loss: 28.0515, MinusLogProbMetric: 28.0515, val_loss: 28.1399, val_MinusLogProbMetric: 28.1399

Epoch 676: val_loss did not improve from 28.08474
196/196 - 33s - loss: 28.0515 - MinusLogProbMetric: 28.0515 - val_loss: 28.1399 - val_MinusLogProbMetric: 28.1399 - lr: 2.5000e-04 - 33s/epoch - 168ms/step
Epoch 677/1000
2023-10-26 05:05:21.130 
Epoch 677/1000 
	 loss: 27.9428, MinusLogProbMetric: 27.9428, val_loss: 28.1693, val_MinusLogProbMetric: 28.1693

Epoch 677: val_loss did not improve from 28.08474
196/196 - 33s - loss: 27.9428 - MinusLogProbMetric: 27.9428 - val_loss: 28.1693 - val_MinusLogProbMetric: 28.1693 - lr: 2.5000e-04 - 33s/epoch - 171ms/step
Epoch 678/1000
2023-10-26 05:05:52.431 
Epoch 678/1000 
	 loss: 28.2071, MinusLogProbMetric: 28.2071, val_loss: 28.3948, val_MinusLogProbMetric: 28.3948

Epoch 678: val_loss did not improve from 28.08474
196/196 - 31s - loss: 28.2071 - MinusLogProbMetric: 28.2071 - val_loss: 28.3948 - val_MinusLogProbMetric: 28.3948 - lr: 2.5000e-04 - 31s/epoch - 160ms/step
Epoch 679/1000
2023-10-26 05:06:22.805 
Epoch 679/1000 
	 loss: 28.0952, MinusLogProbMetric: 28.0952, val_loss: 28.3944, val_MinusLogProbMetric: 28.3944

Epoch 679: val_loss did not improve from 28.08474
196/196 - 30s - loss: 28.0952 - MinusLogProbMetric: 28.0952 - val_loss: 28.3944 - val_MinusLogProbMetric: 28.3944 - lr: 2.5000e-04 - 30s/epoch - 155ms/step
Epoch 680/1000
2023-10-26 05:06:53.692 
Epoch 680/1000 
	 loss: 28.0596, MinusLogProbMetric: 28.0596, val_loss: 28.2357, val_MinusLogProbMetric: 28.2357

Epoch 680: val_loss did not improve from 28.08474
196/196 - 31s - loss: 28.0596 - MinusLogProbMetric: 28.0596 - val_loss: 28.2357 - val_MinusLogProbMetric: 28.2357 - lr: 2.5000e-04 - 31s/epoch - 158ms/step
Epoch 681/1000
2023-10-26 05:07:29.638 
Epoch 681/1000 
	 loss: 28.0522, MinusLogProbMetric: 28.0522, val_loss: 29.1851, val_MinusLogProbMetric: 29.1851

Epoch 681: val_loss did not improve from 28.08474
196/196 - 36s - loss: 28.0522 - MinusLogProbMetric: 28.0522 - val_loss: 29.1851 - val_MinusLogProbMetric: 29.1851 - lr: 2.5000e-04 - 36s/epoch - 183ms/step
Epoch 682/1000
2023-10-26 05:08:03.545 
Epoch 682/1000 
	 loss: 28.0788, MinusLogProbMetric: 28.0788, val_loss: 28.2987, val_MinusLogProbMetric: 28.2987

Epoch 682: val_loss did not improve from 28.08474
196/196 - 34s - loss: 28.0788 - MinusLogProbMetric: 28.0788 - val_loss: 28.2987 - val_MinusLogProbMetric: 28.2987 - lr: 2.5000e-04 - 34s/epoch - 173ms/step
Epoch 683/1000
2023-10-26 05:08:36.103 
Epoch 683/1000 
	 loss: 28.0079, MinusLogProbMetric: 28.0079, val_loss: 28.4050, val_MinusLogProbMetric: 28.4050

Epoch 683: val_loss did not improve from 28.08474
196/196 - 33s - loss: 28.0079 - MinusLogProbMetric: 28.0079 - val_loss: 28.4050 - val_MinusLogProbMetric: 28.4050 - lr: 2.5000e-04 - 33s/epoch - 166ms/step
Epoch 684/1000
2023-10-26 05:09:06.463 
Epoch 684/1000 
	 loss: 28.1486, MinusLogProbMetric: 28.1486, val_loss: 28.2464, val_MinusLogProbMetric: 28.2464

Epoch 684: val_loss did not improve from 28.08474
196/196 - 30s - loss: 28.1486 - MinusLogProbMetric: 28.1486 - val_loss: 28.2464 - val_MinusLogProbMetric: 28.2464 - lr: 2.5000e-04 - 30s/epoch - 155ms/step
Epoch 685/1000
2023-10-26 05:09:37.913 
Epoch 685/1000 
	 loss: 27.9451, MinusLogProbMetric: 27.9451, val_loss: 28.2577, val_MinusLogProbMetric: 28.2577

Epoch 685: val_loss did not improve from 28.08474
196/196 - 31s - loss: 27.9451 - MinusLogProbMetric: 27.9451 - val_loss: 28.2577 - val_MinusLogProbMetric: 28.2577 - lr: 2.5000e-04 - 31s/epoch - 160ms/step
Epoch 686/1000
2023-10-26 05:10:08.148 
Epoch 686/1000 
	 loss: 27.9828, MinusLogProbMetric: 27.9828, val_loss: 28.3595, val_MinusLogProbMetric: 28.3595

Epoch 686: val_loss did not improve from 28.08474
196/196 - 30s - loss: 27.9828 - MinusLogProbMetric: 27.9828 - val_loss: 28.3595 - val_MinusLogProbMetric: 28.3595 - lr: 2.5000e-04 - 30s/epoch - 154ms/step
Epoch 687/1000
2023-10-26 05:10:42.399 
Epoch 687/1000 
	 loss: 27.9781, MinusLogProbMetric: 27.9781, val_loss: 28.1150, val_MinusLogProbMetric: 28.1150

Epoch 687: val_loss did not improve from 28.08474
196/196 - 34s - loss: 27.9781 - MinusLogProbMetric: 27.9781 - val_loss: 28.1150 - val_MinusLogProbMetric: 28.1150 - lr: 2.5000e-04 - 34s/epoch - 175ms/step
Epoch 688/1000
2023-10-26 05:11:17.290 
Epoch 688/1000 
	 loss: 28.0319, MinusLogProbMetric: 28.0319, val_loss: 28.7464, val_MinusLogProbMetric: 28.7464

Epoch 688: val_loss did not improve from 28.08474
196/196 - 35s - loss: 28.0319 - MinusLogProbMetric: 28.0319 - val_loss: 28.7464 - val_MinusLogProbMetric: 28.7464 - lr: 2.5000e-04 - 35s/epoch - 178ms/step
Epoch 689/1000
2023-10-26 05:11:48.613 
Epoch 689/1000 
	 loss: 27.9614, MinusLogProbMetric: 27.9614, val_loss: 28.1897, val_MinusLogProbMetric: 28.1897

Epoch 689: val_loss did not improve from 28.08474
196/196 - 31s - loss: 27.9614 - MinusLogProbMetric: 27.9614 - val_loss: 28.1897 - val_MinusLogProbMetric: 28.1897 - lr: 2.5000e-04 - 31s/epoch - 160ms/step
Epoch 690/1000
2023-10-26 05:12:19.313 
Epoch 690/1000 
	 loss: 28.0264, MinusLogProbMetric: 28.0264, val_loss: 28.2267, val_MinusLogProbMetric: 28.2267

Epoch 690: val_loss did not improve from 28.08474
196/196 - 31s - loss: 28.0264 - MinusLogProbMetric: 28.0264 - val_loss: 28.2267 - val_MinusLogProbMetric: 28.2267 - lr: 2.5000e-04 - 31s/epoch - 157ms/step
Epoch 691/1000
2023-10-26 05:12:49.717 
Epoch 691/1000 
	 loss: 27.9960, MinusLogProbMetric: 27.9960, val_loss: 28.2888, val_MinusLogProbMetric: 28.2888

Epoch 691: val_loss did not improve from 28.08474
196/196 - 30s - loss: 27.9960 - MinusLogProbMetric: 27.9960 - val_loss: 28.2888 - val_MinusLogProbMetric: 28.2888 - lr: 2.5000e-04 - 30s/epoch - 155ms/step
Epoch 692/1000
2023-10-26 05:13:20.506 
Epoch 692/1000 
	 loss: 27.9873, MinusLogProbMetric: 27.9873, val_loss: 28.8708, val_MinusLogProbMetric: 28.8708

Epoch 692: val_loss did not improve from 28.08474
196/196 - 31s - loss: 27.9873 - MinusLogProbMetric: 27.9873 - val_loss: 28.8708 - val_MinusLogProbMetric: 28.8708 - lr: 2.5000e-04 - 31s/epoch - 157ms/step
Epoch 693/1000
2023-10-26 05:13:55.497 
Epoch 693/1000 
	 loss: 27.9506, MinusLogProbMetric: 27.9506, val_loss: 28.4285, val_MinusLogProbMetric: 28.4285

Epoch 693: val_loss did not improve from 28.08474
196/196 - 35s - loss: 27.9506 - MinusLogProbMetric: 27.9506 - val_loss: 28.4285 - val_MinusLogProbMetric: 28.4285 - lr: 2.5000e-04 - 35s/epoch - 179ms/step
Epoch 694/1000
2023-10-26 05:14:30.907 
Epoch 694/1000 
	 loss: 28.0640, MinusLogProbMetric: 28.0640, val_loss: 28.1267, val_MinusLogProbMetric: 28.1267

Epoch 694: val_loss did not improve from 28.08474
196/196 - 35s - loss: 28.0640 - MinusLogProbMetric: 28.0640 - val_loss: 28.1267 - val_MinusLogProbMetric: 28.1267 - lr: 2.5000e-04 - 35s/epoch - 181ms/step
Epoch 695/1000
2023-10-26 05:15:04.720 
Epoch 695/1000 
	 loss: 27.9851, MinusLogProbMetric: 27.9851, val_loss: 28.6229, val_MinusLogProbMetric: 28.6229

Epoch 695: val_loss did not improve from 28.08474
196/196 - 34s - loss: 27.9851 - MinusLogProbMetric: 27.9851 - val_loss: 28.6229 - val_MinusLogProbMetric: 28.6229 - lr: 2.5000e-04 - 34s/epoch - 172ms/step
Epoch 696/1000
2023-10-26 05:15:35.665 
Epoch 696/1000 
	 loss: 28.1075, MinusLogProbMetric: 28.1075, val_loss: 29.1998, val_MinusLogProbMetric: 29.1998

Epoch 696: val_loss did not improve from 28.08474
196/196 - 31s - loss: 28.1075 - MinusLogProbMetric: 28.1075 - val_loss: 29.1998 - val_MinusLogProbMetric: 29.1998 - lr: 2.5000e-04 - 31s/epoch - 158ms/step
Epoch 697/1000
2023-10-26 05:16:05.947 
Epoch 697/1000 
	 loss: 28.0384, MinusLogProbMetric: 28.0384, val_loss: 28.3216, val_MinusLogProbMetric: 28.3216

Epoch 697: val_loss did not improve from 28.08474
196/196 - 30s - loss: 28.0384 - MinusLogProbMetric: 28.0384 - val_loss: 28.3216 - val_MinusLogProbMetric: 28.3216 - lr: 2.5000e-04 - 30s/epoch - 154ms/step
Epoch 698/1000
2023-10-26 05:16:36.420 
Epoch 698/1000 
	 loss: 27.8221, MinusLogProbMetric: 27.8221, val_loss: 28.1015, val_MinusLogProbMetric: 28.1015

Epoch 698: val_loss did not improve from 28.08474
196/196 - 30s - loss: 27.8221 - MinusLogProbMetric: 27.8221 - val_loss: 28.1015 - val_MinusLogProbMetric: 28.1015 - lr: 1.2500e-04 - 30s/epoch - 155ms/step
Epoch 699/1000
2023-10-26 05:17:10.044 
Epoch 699/1000 
	 loss: 27.8260, MinusLogProbMetric: 27.8260, val_loss: 28.0680, val_MinusLogProbMetric: 28.0680

Epoch 699: val_loss improved from 28.08474 to 28.06804, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_369/weights/best_weights.h5
196/196 - 34s - loss: 27.8260 - MinusLogProbMetric: 27.8260 - val_loss: 28.0680 - val_MinusLogProbMetric: 28.0680 - lr: 1.2500e-04 - 34s/epoch - 175ms/step
Epoch 700/1000
2023-10-26 05:17:46.326 
Epoch 700/1000 
	 loss: 27.8185, MinusLogProbMetric: 27.8185, val_loss: 28.0534, val_MinusLogProbMetric: 28.0534

Epoch 700: val_loss improved from 28.06804 to 28.05338, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_369/weights/best_weights.h5
196/196 - 36s - loss: 27.8185 - MinusLogProbMetric: 27.8185 - val_loss: 28.0534 - val_MinusLogProbMetric: 28.0534 - lr: 1.2500e-04 - 36s/epoch - 184ms/step
Epoch 701/1000
2023-10-26 05:18:19.276 
Epoch 701/1000 
	 loss: 27.8111, MinusLogProbMetric: 27.8111, val_loss: 28.0552, val_MinusLogProbMetric: 28.0552

Epoch 701: val_loss did not improve from 28.05338
196/196 - 32s - loss: 27.8111 - MinusLogProbMetric: 27.8111 - val_loss: 28.0552 - val_MinusLogProbMetric: 28.0552 - lr: 1.2500e-04 - 32s/epoch - 166ms/step
Epoch 702/1000
2023-10-26 05:18:49.737 
Epoch 702/1000 
	 loss: 27.7958, MinusLogProbMetric: 27.7958, val_loss: 28.0658, val_MinusLogProbMetric: 28.0658

Epoch 702: val_loss did not improve from 28.05338
196/196 - 30s - loss: 27.7958 - MinusLogProbMetric: 27.7958 - val_loss: 28.0658 - val_MinusLogProbMetric: 28.0658 - lr: 1.2500e-04 - 30s/epoch - 155ms/step
Epoch 703/1000
2023-10-26 05:19:20.025 
Epoch 703/1000 
	 loss: 27.8262, MinusLogProbMetric: 27.8262, val_loss: 28.0937, val_MinusLogProbMetric: 28.0937

Epoch 703: val_loss did not improve from 28.05338
196/196 - 30s - loss: 27.8262 - MinusLogProbMetric: 27.8262 - val_loss: 28.0937 - val_MinusLogProbMetric: 28.0937 - lr: 1.2500e-04 - 30s/epoch - 155ms/step
Epoch 704/1000
2023-10-26 05:19:51.379 
Epoch 704/1000 
	 loss: 27.8488, MinusLogProbMetric: 27.8488, val_loss: 28.3523, val_MinusLogProbMetric: 28.3523

Epoch 704: val_loss did not improve from 28.05338
196/196 - 31s - loss: 27.8488 - MinusLogProbMetric: 27.8488 - val_loss: 28.3523 - val_MinusLogProbMetric: 28.3523 - lr: 1.2500e-04 - 31s/epoch - 160ms/step
Epoch 705/1000
2023-10-26 05:20:24.090 
Epoch 705/1000 
	 loss: 27.8228, MinusLogProbMetric: 27.8228, val_loss: 28.0438, val_MinusLogProbMetric: 28.0438

Epoch 705: val_loss improved from 28.05338 to 28.04380, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_369/weights/best_weights.h5
196/196 - 33s - loss: 27.8228 - MinusLogProbMetric: 27.8228 - val_loss: 28.0438 - val_MinusLogProbMetric: 28.0438 - lr: 1.2500e-04 - 33s/epoch - 170ms/step
Epoch 706/1000
2023-10-26 05:20:59.922 
Epoch 706/1000 
	 loss: 27.7988, MinusLogProbMetric: 27.7988, val_loss: 28.0871, val_MinusLogProbMetric: 28.0871

Epoch 706: val_loss did not improve from 28.04380
196/196 - 35s - loss: 27.7988 - MinusLogProbMetric: 27.7988 - val_loss: 28.0871 - val_MinusLogProbMetric: 28.0871 - lr: 1.2500e-04 - 35s/epoch - 180ms/step
Epoch 707/1000
2023-10-26 05:21:35.401 
Epoch 707/1000 
	 loss: 27.7941, MinusLogProbMetric: 27.7941, val_loss: 28.0126, val_MinusLogProbMetric: 28.0126

Epoch 707: val_loss improved from 28.04380 to 28.01265, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_369/weights/best_weights.h5
196/196 - 36s - loss: 27.7941 - MinusLogProbMetric: 27.7941 - val_loss: 28.0126 - val_MinusLogProbMetric: 28.0126 - lr: 1.2500e-04 - 36s/epoch - 184ms/step
Epoch 708/1000
2023-10-26 05:22:06.180 
Epoch 708/1000 
	 loss: 27.7909, MinusLogProbMetric: 27.7909, val_loss: 28.0497, val_MinusLogProbMetric: 28.0497

Epoch 708: val_loss did not improve from 28.01265
196/196 - 30s - loss: 27.7909 - MinusLogProbMetric: 27.7909 - val_loss: 28.0497 - val_MinusLogProbMetric: 28.0497 - lr: 1.2500e-04 - 30s/epoch - 154ms/step
Epoch 709/1000
2023-10-26 05:22:36.473 
Epoch 709/1000 
	 loss: 27.8090, MinusLogProbMetric: 27.8090, val_loss: 28.1515, val_MinusLogProbMetric: 28.1515

Epoch 709: val_loss did not improve from 28.01265
196/196 - 30s - loss: 27.8090 - MinusLogProbMetric: 27.8090 - val_loss: 28.1515 - val_MinusLogProbMetric: 28.1515 - lr: 1.2500e-04 - 30s/epoch - 155ms/step
Epoch 710/1000
2023-10-26 05:23:07.350 
Epoch 710/1000 
	 loss: 27.7984, MinusLogProbMetric: 27.7984, val_loss: 28.0252, val_MinusLogProbMetric: 28.0252

Epoch 710: val_loss did not improve from 28.01265
196/196 - 31s - loss: 27.7984 - MinusLogProbMetric: 27.7984 - val_loss: 28.0252 - val_MinusLogProbMetric: 28.0252 - lr: 1.2500e-04 - 31s/epoch - 158ms/step
Epoch 711/1000
2023-10-26 05:23:38.405 
Epoch 711/1000 
	 loss: 27.8045, MinusLogProbMetric: 27.8045, val_loss: 28.2434, val_MinusLogProbMetric: 28.2434

Epoch 711: val_loss did not improve from 28.01265
196/196 - 31s - loss: 27.8045 - MinusLogProbMetric: 27.8045 - val_loss: 28.2434 - val_MinusLogProbMetric: 28.2434 - lr: 1.2500e-04 - 31s/epoch - 158ms/step
Epoch 712/1000
2023-10-26 05:24:14.435 
Epoch 712/1000 
	 loss: 27.8184, MinusLogProbMetric: 27.8184, val_loss: 28.0695, val_MinusLogProbMetric: 28.0695

Epoch 712: val_loss did not improve from 28.01265
196/196 - 36s - loss: 27.8184 - MinusLogProbMetric: 27.8184 - val_loss: 28.0695 - val_MinusLogProbMetric: 28.0695 - lr: 1.2500e-04 - 36s/epoch - 184ms/step
Epoch 713/1000
2023-10-26 05:24:48.100 
Epoch 713/1000 
	 loss: 27.7833, MinusLogProbMetric: 27.7833, val_loss: 28.0719, val_MinusLogProbMetric: 28.0719

Epoch 713: val_loss did not improve from 28.01265
196/196 - 34s - loss: 27.7833 - MinusLogProbMetric: 27.7833 - val_loss: 28.0719 - val_MinusLogProbMetric: 28.0719 - lr: 1.2500e-04 - 34s/epoch - 172ms/step
Epoch 714/1000
2023-10-26 05:25:20.020 
Epoch 714/1000 
	 loss: 27.8504, MinusLogProbMetric: 27.8504, val_loss: 28.0146, val_MinusLogProbMetric: 28.0146

Epoch 714: val_loss did not improve from 28.01265
196/196 - 32s - loss: 27.8504 - MinusLogProbMetric: 27.8504 - val_loss: 28.0146 - val_MinusLogProbMetric: 28.0146 - lr: 1.2500e-04 - 32s/epoch - 163ms/step
Epoch 715/1000
2023-10-26 05:25:50.388 
Epoch 715/1000 
	 loss: 27.8090, MinusLogProbMetric: 27.8090, val_loss: 28.0036, val_MinusLogProbMetric: 28.0036

Epoch 715: val_loss improved from 28.01265 to 28.00361, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_369/weights/best_weights.h5
196/196 - 31s - loss: 27.8090 - MinusLogProbMetric: 27.8090 - val_loss: 28.0036 - val_MinusLogProbMetric: 28.0036 - lr: 1.2500e-04 - 31s/epoch - 157ms/step
Epoch 716/1000
2023-10-26 05:26:21.478 
Epoch 716/1000 
	 loss: 27.7808, MinusLogProbMetric: 27.7808, val_loss: 28.0837, val_MinusLogProbMetric: 28.0837

Epoch 716: val_loss did not improve from 28.00361
196/196 - 31s - loss: 27.7808 - MinusLogProbMetric: 27.7808 - val_loss: 28.0837 - val_MinusLogProbMetric: 28.0837 - lr: 1.2500e-04 - 31s/epoch - 156ms/step
Epoch 717/1000
2023-10-26 05:26:50.516 
Epoch 717/1000 
	 loss: 27.8437, MinusLogProbMetric: 27.8437, val_loss: 28.0660, val_MinusLogProbMetric: 28.0660

Epoch 717: val_loss did not improve from 28.00361
196/196 - 29s - loss: 27.8437 - MinusLogProbMetric: 27.8437 - val_loss: 28.0660 - val_MinusLogProbMetric: 28.0660 - lr: 1.2500e-04 - 29s/epoch - 148ms/step
Epoch 718/1000
2023-10-26 05:27:18.565 
Epoch 718/1000 
	 loss: 27.7994, MinusLogProbMetric: 27.7994, val_loss: 28.0816, val_MinusLogProbMetric: 28.0816

Epoch 718: val_loss did not improve from 28.00361
196/196 - 28s - loss: 27.7994 - MinusLogProbMetric: 27.7994 - val_loss: 28.0816 - val_MinusLogProbMetric: 28.0816 - lr: 1.2500e-04 - 28s/epoch - 143ms/step
Epoch 719/1000
2023-10-26 05:27:46.921 
Epoch 719/1000 
	 loss: 27.7958, MinusLogProbMetric: 27.7958, val_loss: 28.0401, val_MinusLogProbMetric: 28.0401

Epoch 719: val_loss did not improve from 28.00361
196/196 - 28s - loss: 27.7958 - MinusLogProbMetric: 27.7958 - val_loss: 28.0401 - val_MinusLogProbMetric: 28.0401 - lr: 1.2500e-04 - 28s/epoch - 145ms/step
Epoch 720/1000
2023-10-26 05:28:16.258 
Epoch 720/1000 
	 loss: 27.8621, MinusLogProbMetric: 27.8621, val_loss: 28.0806, val_MinusLogProbMetric: 28.0806

Epoch 720: val_loss did not improve from 28.00361
196/196 - 29s - loss: 27.8621 - MinusLogProbMetric: 27.8621 - val_loss: 28.0806 - val_MinusLogProbMetric: 28.0806 - lr: 1.2500e-04 - 29s/epoch - 150ms/step
Epoch 721/1000
2023-10-26 05:28:48.817 
Epoch 721/1000 
	 loss: 27.8426, MinusLogProbMetric: 27.8426, val_loss: 28.0576, val_MinusLogProbMetric: 28.0576

Epoch 721: val_loss did not improve from 28.00361
196/196 - 33s - loss: 27.8426 - MinusLogProbMetric: 27.8426 - val_loss: 28.0576 - val_MinusLogProbMetric: 28.0576 - lr: 1.2500e-04 - 33s/epoch - 166ms/step
Epoch 722/1000
2023-10-26 05:29:20.282 
Epoch 722/1000 
	 loss: 27.8799, MinusLogProbMetric: 27.8799, val_loss: 28.0322, val_MinusLogProbMetric: 28.0322

Epoch 722: val_loss did not improve from 28.00361
196/196 - 31s - loss: 27.8799 - MinusLogProbMetric: 27.8799 - val_loss: 28.0322 - val_MinusLogProbMetric: 28.0322 - lr: 1.2500e-04 - 31s/epoch - 161ms/step
Epoch 723/1000
2023-10-26 05:29:48.911 
Epoch 723/1000 
	 loss: 27.8720, MinusLogProbMetric: 27.8720, val_loss: 28.1124, val_MinusLogProbMetric: 28.1124

Epoch 723: val_loss did not improve from 28.00361
196/196 - 29s - loss: 27.8720 - MinusLogProbMetric: 27.8720 - val_loss: 28.1124 - val_MinusLogProbMetric: 28.1124 - lr: 1.2500e-04 - 29s/epoch - 146ms/step
Epoch 724/1000
2023-10-26 05:30:16.686 
Epoch 724/1000 
	 loss: 27.8559, MinusLogProbMetric: 27.8559, val_loss: 28.0416, val_MinusLogProbMetric: 28.0416

Epoch 724: val_loss did not improve from 28.00361
196/196 - 28s - loss: 27.8559 - MinusLogProbMetric: 27.8559 - val_loss: 28.0416 - val_MinusLogProbMetric: 28.0416 - lr: 1.2500e-04 - 28s/epoch - 142ms/step
Epoch 725/1000
2023-10-26 05:30:45.125 
Epoch 725/1000 
	 loss: 27.8792, MinusLogProbMetric: 27.8792, val_loss: 28.1048, val_MinusLogProbMetric: 28.1048

Epoch 725: val_loss did not improve from 28.00361
196/196 - 28s - loss: 27.8792 - MinusLogProbMetric: 27.8792 - val_loss: 28.1048 - val_MinusLogProbMetric: 28.1048 - lr: 1.2500e-04 - 28s/epoch - 145ms/step
Epoch 726/1000
2023-10-26 05:31:14.915 
Epoch 726/1000 
	 loss: 27.8411, MinusLogProbMetric: 27.8411, val_loss: 28.4947, val_MinusLogProbMetric: 28.4947

Epoch 726: val_loss did not improve from 28.00361
196/196 - 30s - loss: 27.8411 - MinusLogProbMetric: 27.8411 - val_loss: 28.4947 - val_MinusLogProbMetric: 28.4947 - lr: 1.2500e-04 - 30s/epoch - 152ms/step
Epoch 727/1000
2023-10-26 05:31:47.081 
Epoch 727/1000 
	 loss: 27.8366, MinusLogProbMetric: 27.8366, val_loss: 28.1040, val_MinusLogProbMetric: 28.1040

Epoch 727: val_loss did not improve from 28.00361
196/196 - 32s - loss: 27.8366 - MinusLogProbMetric: 27.8366 - val_loss: 28.1040 - val_MinusLogProbMetric: 28.1040 - lr: 1.2500e-04 - 32s/epoch - 164ms/step
Epoch 728/1000
2023-10-26 05:32:19.102 
Epoch 728/1000 
	 loss: 27.8386, MinusLogProbMetric: 27.8386, val_loss: 28.1256, val_MinusLogProbMetric: 28.1256

Epoch 728: val_loss did not improve from 28.00361
196/196 - 32s - loss: 27.8386 - MinusLogProbMetric: 27.8386 - val_loss: 28.1256 - val_MinusLogProbMetric: 28.1256 - lr: 1.2500e-04 - 32s/epoch - 163ms/step
Epoch 729/1000
2023-10-26 05:32:52.376 
Epoch 729/1000 
	 loss: 27.8336, MinusLogProbMetric: 27.8336, val_loss: 28.0520, val_MinusLogProbMetric: 28.0520

Epoch 729: val_loss did not improve from 28.00361
196/196 - 33s - loss: 27.8336 - MinusLogProbMetric: 27.8336 - val_loss: 28.0520 - val_MinusLogProbMetric: 28.0520 - lr: 1.2500e-04 - 33s/epoch - 170ms/step
Epoch 730/1000
2023-10-26 05:33:21.051 
Epoch 730/1000 
	 loss: 27.8326, MinusLogProbMetric: 27.8326, val_loss: 28.0646, val_MinusLogProbMetric: 28.0646

Epoch 730: val_loss did not improve from 28.00361
196/196 - 29s - loss: 27.8326 - MinusLogProbMetric: 27.8326 - val_loss: 28.0646 - val_MinusLogProbMetric: 28.0646 - lr: 1.2500e-04 - 29s/epoch - 146ms/step
Epoch 731/1000
2023-10-26 05:33:49.554 
Epoch 731/1000 
	 loss: 27.8322, MinusLogProbMetric: 27.8322, val_loss: 28.0708, val_MinusLogProbMetric: 28.0708

Epoch 731: val_loss did not improve from 28.00361
196/196 - 28s - loss: 27.8322 - MinusLogProbMetric: 27.8322 - val_loss: 28.0708 - val_MinusLogProbMetric: 28.0708 - lr: 1.2500e-04 - 28s/epoch - 145ms/step
Epoch 732/1000
2023-10-26 05:34:17.119 
Epoch 732/1000 
	 loss: 27.8129, MinusLogProbMetric: 27.8129, val_loss: 28.0760, val_MinusLogProbMetric: 28.0760

Epoch 732: val_loss did not improve from 28.00361
196/196 - 28s - loss: 27.8129 - MinusLogProbMetric: 27.8129 - val_loss: 28.0760 - val_MinusLogProbMetric: 28.0760 - lr: 1.2500e-04 - 28s/epoch - 141ms/step
Epoch 733/1000
2023-10-26 05:34:45.135 
Epoch 733/1000 
	 loss: 27.8422, MinusLogProbMetric: 27.8422, val_loss: 28.1873, val_MinusLogProbMetric: 28.1873

Epoch 733: val_loss did not improve from 28.00361
196/196 - 28s - loss: 27.8422 - MinusLogProbMetric: 27.8422 - val_loss: 28.1873 - val_MinusLogProbMetric: 28.1873 - lr: 1.2500e-04 - 28s/epoch - 143ms/step
Epoch 734/1000
2023-10-26 05:35:14.882 
Epoch 734/1000 
	 loss: 27.8463, MinusLogProbMetric: 27.8463, val_loss: 28.1446, val_MinusLogProbMetric: 28.1446

Epoch 734: val_loss did not improve from 28.00361
196/196 - 30s - loss: 27.8463 - MinusLogProbMetric: 27.8463 - val_loss: 28.1446 - val_MinusLogProbMetric: 28.1446 - lr: 1.2500e-04 - 30s/epoch - 152ms/step
Epoch 735/1000
2023-10-26 05:35:45.987 
Epoch 735/1000 
	 loss: 27.8154, MinusLogProbMetric: 27.8154, val_loss: 28.1775, val_MinusLogProbMetric: 28.1775

Epoch 735: val_loss did not improve from 28.00361
196/196 - 31s - loss: 27.8154 - MinusLogProbMetric: 27.8154 - val_loss: 28.1775 - val_MinusLogProbMetric: 28.1775 - lr: 1.2500e-04 - 31s/epoch - 159ms/step
Epoch 736/1000
2023-10-26 05:36:20.515 
Epoch 736/1000 
	 loss: 27.7977, MinusLogProbMetric: 27.7977, val_loss: 28.0685, val_MinusLogProbMetric: 28.0685

Epoch 736: val_loss did not improve from 28.00361
196/196 - 35s - loss: 27.7977 - MinusLogProbMetric: 27.7977 - val_loss: 28.0685 - val_MinusLogProbMetric: 28.0685 - lr: 1.2500e-04 - 35s/epoch - 176ms/step
Epoch 737/1000
2023-10-26 05:36:52.072 
Epoch 737/1000 
	 loss: 27.8229, MinusLogProbMetric: 27.8229, val_loss: 28.1642, val_MinusLogProbMetric: 28.1642

Epoch 737: val_loss did not improve from 28.00361
196/196 - 32s - loss: 27.8229 - MinusLogProbMetric: 27.8229 - val_loss: 28.1642 - val_MinusLogProbMetric: 28.1642 - lr: 1.2500e-04 - 32s/epoch - 161ms/step
Epoch 738/1000
2023-10-26 05:37:25.249 
Epoch 738/1000 
	 loss: 27.8202, MinusLogProbMetric: 27.8202, val_loss: 28.0170, val_MinusLogProbMetric: 28.0170

Epoch 738: val_loss did not improve from 28.00361
196/196 - 33s - loss: 27.8202 - MinusLogProbMetric: 27.8202 - val_loss: 28.0170 - val_MinusLogProbMetric: 28.0170 - lr: 1.2500e-04 - 33s/epoch - 169ms/step
Epoch 739/1000
2023-10-26 05:37:54.860 
Epoch 739/1000 
	 loss: 27.8124, MinusLogProbMetric: 27.8124, val_loss: 27.9982, val_MinusLogProbMetric: 27.9982

Epoch 739: val_loss improved from 28.00361 to 27.99823, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_369/weights/best_weights.h5
196/196 - 30s - loss: 27.8124 - MinusLogProbMetric: 27.8124 - val_loss: 27.9982 - val_MinusLogProbMetric: 27.9982 - lr: 1.2500e-04 - 30s/epoch - 153ms/step
Epoch 740/1000
2023-10-26 05:38:23.358 
Epoch 740/1000 
	 loss: 27.7855, MinusLogProbMetric: 27.7855, val_loss: 28.0253, val_MinusLogProbMetric: 28.0253

Epoch 740: val_loss did not improve from 27.99823
196/196 - 28s - loss: 27.7855 - MinusLogProbMetric: 27.7855 - val_loss: 28.0253 - val_MinusLogProbMetric: 28.0253 - lr: 1.2500e-04 - 28s/epoch - 143ms/step
Epoch 741/1000
2023-10-26 05:38:51.169 
Epoch 741/1000 
	 loss: 27.8163, MinusLogProbMetric: 27.8163, val_loss: 28.0799, val_MinusLogProbMetric: 28.0799

Epoch 741: val_loss did not improve from 27.99823
196/196 - 28s - loss: 27.8163 - MinusLogProbMetric: 27.8163 - val_loss: 28.0799 - val_MinusLogProbMetric: 28.0799 - lr: 1.2500e-04 - 28s/epoch - 142ms/step
Epoch 742/1000
2023-10-26 05:39:18.983 
Epoch 742/1000 
	 loss: 27.7942, MinusLogProbMetric: 27.7942, val_loss: 28.0745, val_MinusLogProbMetric: 28.0745

Epoch 742: val_loss did not improve from 27.99823
196/196 - 28s - loss: 27.7942 - MinusLogProbMetric: 27.7942 - val_loss: 28.0745 - val_MinusLogProbMetric: 28.0745 - lr: 1.2500e-04 - 28s/epoch - 142ms/step
Epoch 743/1000
2023-10-26 05:39:47.915 
Epoch 743/1000 
	 loss: 27.8511, MinusLogProbMetric: 27.8511, val_loss: 28.2042, val_MinusLogProbMetric: 28.2042

Epoch 743: val_loss did not improve from 27.99823
196/196 - 29s - loss: 27.8511 - MinusLogProbMetric: 27.8511 - val_loss: 28.2042 - val_MinusLogProbMetric: 28.2042 - lr: 1.2500e-04 - 29s/epoch - 148ms/step
Epoch 744/1000
2023-10-26 05:40:18.048 
Epoch 744/1000 
	 loss: 27.8568, MinusLogProbMetric: 27.8568, val_loss: 28.2759, val_MinusLogProbMetric: 28.2759

Epoch 744: val_loss did not improve from 27.99823
196/196 - 30s - loss: 27.8568 - MinusLogProbMetric: 27.8568 - val_loss: 28.2759 - val_MinusLogProbMetric: 28.2759 - lr: 1.2500e-04 - 30s/epoch - 154ms/step
Epoch 745/1000
2023-10-26 05:40:48.725 
Epoch 745/1000 
	 loss: 27.8404, MinusLogProbMetric: 27.8404, val_loss: 28.1855, val_MinusLogProbMetric: 28.1855

Epoch 745: val_loss did not improve from 27.99823
196/196 - 31s - loss: 27.8404 - MinusLogProbMetric: 27.8404 - val_loss: 28.1855 - val_MinusLogProbMetric: 28.1855 - lr: 1.2500e-04 - 31s/epoch - 156ms/step
Epoch 746/1000
2023-10-26 05:41:16.121 
Epoch 746/1000 
	 loss: 27.8299, MinusLogProbMetric: 27.8299, val_loss: 27.9976, val_MinusLogProbMetric: 27.9976

Epoch 746: val_loss improved from 27.99823 to 27.99762, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_369/weights/best_weights.h5
196/196 - 28s - loss: 27.8299 - MinusLogProbMetric: 27.8299 - val_loss: 27.9976 - val_MinusLogProbMetric: 27.9976 - lr: 1.2500e-04 - 28s/epoch - 143ms/step
Epoch 747/1000
2023-10-26 05:41:44.566 
Epoch 747/1000 
	 loss: 27.7933, MinusLogProbMetric: 27.7933, val_loss: 28.1470, val_MinusLogProbMetric: 28.1470

Epoch 747: val_loss did not improve from 27.99762
196/196 - 28s - loss: 27.7933 - MinusLogProbMetric: 27.7933 - val_loss: 28.1470 - val_MinusLogProbMetric: 28.1470 - lr: 1.2500e-04 - 28s/epoch - 142ms/step
Epoch 748/1000
2023-10-26 05:42:12.681 
Epoch 748/1000 
	 loss: 27.8022, MinusLogProbMetric: 27.8022, val_loss: 28.0727, val_MinusLogProbMetric: 28.0727

Epoch 748: val_loss did not improve from 27.99762
196/196 - 28s - loss: 27.8022 - MinusLogProbMetric: 27.8022 - val_loss: 28.0727 - val_MinusLogProbMetric: 28.0727 - lr: 1.2500e-04 - 28s/epoch - 143ms/step
Epoch 749/1000
2023-10-26 05:42:40.668 
Epoch 749/1000 
	 loss: 27.8065, MinusLogProbMetric: 27.8065, val_loss: 28.0305, val_MinusLogProbMetric: 28.0305

Epoch 749: val_loss did not improve from 27.99762
196/196 - 28s - loss: 27.8065 - MinusLogProbMetric: 27.8065 - val_loss: 28.0305 - val_MinusLogProbMetric: 28.0305 - lr: 1.2500e-04 - 28s/epoch - 143ms/step
Epoch 750/1000
2023-10-26 05:43:13.396 
Epoch 750/1000 
	 loss: 27.8156, MinusLogProbMetric: 27.8156, val_loss: 28.0991, val_MinusLogProbMetric: 28.0991

Epoch 750: val_loss did not improve from 27.99762
196/196 - 33s - loss: 27.8156 - MinusLogProbMetric: 27.8156 - val_loss: 28.0991 - val_MinusLogProbMetric: 28.0991 - lr: 1.2500e-04 - 33s/epoch - 167ms/step
Epoch 751/1000
2023-10-26 05:43:48.509 
Epoch 751/1000 
	 loss: 27.8633, MinusLogProbMetric: 27.8633, val_loss: 28.3712, val_MinusLogProbMetric: 28.3712

Epoch 751: val_loss did not improve from 27.99762
196/196 - 35s - loss: 27.8633 - MinusLogProbMetric: 27.8633 - val_loss: 28.3712 - val_MinusLogProbMetric: 28.3712 - lr: 1.2500e-04 - 35s/epoch - 179ms/step
Epoch 752/1000
2023-10-26 05:44:24.514 
Epoch 752/1000 
	 loss: 27.8721, MinusLogProbMetric: 27.8721, val_loss: 28.1195, val_MinusLogProbMetric: 28.1195

Epoch 752: val_loss did not improve from 27.99762
196/196 - 36s - loss: 27.8721 - MinusLogProbMetric: 27.8721 - val_loss: 28.1195 - val_MinusLogProbMetric: 28.1195 - lr: 1.2500e-04 - 36s/epoch - 184ms/step
Epoch 753/1000
2023-10-26 05:44:58.094 
Epoch 753/1000 
	 loss: 27.8258, MinusLogProbMetric: 27.8258, val_loss: 28.1103, val_MinusLogProbMetric: 28.1103

Epoch 753: val_loss did not improve from 27.99762
196/196 - 34s - loss: 27.8258 - MinusLogProbMetric: 27.8258 - val_loss: 28.1103 - val_MinusLogProbMetric: 28.1103 - lr: 1.2500e-04 - 34s/epoch - 171ms/step
Epoch 754/1000
2023-10-26 05:45:34.243 
Epoch 754/1000 
	 loss: 27.8406, MinusLogProbMetric: 27.8406, val_loss: 28.0676, val_MinusLogProbMetric: 28.0676

Epoch 754: val_loss did not improve from 27.99762
196/196 - 36s - loss: 27.8406 - MinusLogProbMetric: 27.8406 - val_loss: 28.0676 - val_MinusLogProbMetric: 28.0676 - lr: 1.2500e-04 - 36s/epoch - 184ms/step
Epoch 755/1000
2023-10-26 05:46:10.327 
Epoch 755/1000 
	 loss: 27.7791, MinusLogProbMetric: 27.7791, val_loss: 28.0226, val_MinusLogProbMetric: 28.0226

Epoch 755: val_loss did not improve from 27.99762
196/196 - 36s - loss: 27.7791 - MinusLogProbMetric: 27.7791 - val_loss: 28.0226 - val_MinusLogProbMetric: 28.0226 - lr: 1.2500e-04 - 36s/epoch - 184ms/step
Epoch 756/1000
2023-10-26 05:46:46.196 
Epoch 756/1000 
	 loss: 27.8042, MinusLogProbMetric: 27.8042, val_loss: 28.0703, val_MinusLogProbMetric: 28.0703

Epoch 756: val_loss did not improve from 27.99762
196/196 - 36s - loss: 27.8042 - MinusLogProbMetric: 27.8042 - val_loss: 28.0703 - val_MinusLogProbMetric: 28.0703 - lr: 1.2500e-04 - 36s/epoch - 183ms/step
Epoch 757/1000
2023-10-26 05:47:21.930 
Epoch 757/1000 
	 loss: 27.8033, MinusLogProbMetric: 27.8033, val_loss: 28.0411, val_MinusLogProbMetric: 28.0411

Epoch 757: val_loss did not improve from 27.99762
196/196 - 36s - loss: 27.8033 - MinusLogProbMetric: 27.8033 - val_loss: 28.0411 - val_MinusLogProbMetric: 28.0411 - lr: 1.2500e-04 - 36s/epoch - 182ms/step
Epoch 758/1000
2023-10-26 05:47:57.052 
Epoch 758/1000 
	 loss: 27.7790, MinusLogProbMetric: 27.7790, val_loss: 28.0872, val_MinusLogProbMetric: 28.0872

Epoch 758: val_loss did not improve from 27.99762
196/196 - 35s - loss: 27.7790 - MinusLogProbMetric: 27.7790 - val_loss: 28.0872 - val_MinusLogProbMetric: 28.0872 - lr: 1.2500e-04 - 35s/epoch - 179ms/step
Epoch 759/1000
2023-10-26 05:48:32.903 
Epoch 759/1000 
	 loss: 27.8016, MinusLogProbMetric: 27.8016, val_loss: 28.0193, val_MinusLogProbMetric: 28.0193

Epoch 759: val_loss did not improve from 27.99762
196/196 - 36s - loss: 27.8016 - MinusLogProbMetric: 27.8016 - val_loss: 28.0193 - val_MinusLogProbMetric: 28.0193 - lr: 1.2500e-04 - 36s/epoch - 183ms/step
Epoch 760/1000
2023-10-26 05:49:08.745 
Epoch 760/1000 
	 loss: 27.8371, MinusLogProbMetric: 27.8371, val_loss: 28.0125, val_MinusLogProbMetric: 28.0125

Epoch 760: val_loss did not improve from 27.99762
196/196 - 36s - loss: 27.8371 - MinusLogProbMetric: 27.8371 - val_loss: 28.0125 - val_MinusLogProbMetric: 28.0125 - lr: 1.2500e-04 - 36s/epoch - 183ms/step
Epoch 761/1000
2023-10-26 05:49:44.947 
Epoch 761/1000 
	 loss: 27.7771, MinusLogProbMetric: 27.7771, val_loss: 28.1133, val_MinusLogProbMetric: 28.1133

Epoch 761: val_loss did not improve from 27.99762
196/196 - 36s - loss: 27.7771 - MinusLogProbMetric: 27.7771 - val_loss: 28.1133 - val_MinusLogProbMetric: 28.1133 - lr: 1.2500e-04 - 36s/epoch - 185ms/step
Epoch 762/1000
2023-10-26 05:50:20.861 
Epoch 762/1000 
	 loss: 27.8231, MinusLogProbMetric: 27.8231, val_loss: 28.1834, val_MinusLogProbMetric: 28.1834

Epoch 762: val_loss did not improve from 27.99762
196/196 - 36s - loss: 27.8231 - MinusLogProbMetric: 27.8231 - val_loss: 28.1834 - val_MinusLogProbMetric: 28.1834 - lr: 1.2500e-04 - 36s/epoch - 183ms/step
Epoch 763/1000
2023-10-26 05:50:56.666 
Epoch 763/1000 
	 loss: 27.8190, MinusLogProbMetric: 27.8190, val_loss: 28.0864, val_MinusLogProbMetric: 28.0864

Epoch 763: val_loss did not improve from 27.99762
196/196 - 36s - loss: 27.8190 - MinusLogProbMetric: 27.8190 - val_loss: 28.0864 - val_MinusLogProbMetric: 28.0864 - lr: 1.2500e-04 - 36s/epoch - 183ms/step
Epoch 764/1000
2023-10-26 05:51:32.528 
Epoch 764/1000 
	 loss: 27.7831, MinusLogProbMetric: 27.7831, val_loss: 28.0221, val_MinusLogProbMetric: 28.0221

Epoch 764: val_loss did not improve from 27.99762
196/196 - 36s - loss: 27.7831 - MinusLogProbMetric: 27.7831 - val_loss: 28.0221 - val_MinusLogProbMetric: 28.0221 - lr: 1.2500e-04 - 36s/epoch - 183ms/step
Epoch 765/1000
2023-10-26 05:52:08.123 
Epoch 765/1000 
	 loss: 27.7942, MinusLogProbMetric: 27.7942, val_loss: 28.1248, val_MinusLogProbMetric: 28.1248

Epoch 765: val_loss did not improve from 27.99762
196/196 - 36s - loss: 27.7942 - MinusLogProbMetric: 27.7942 - val_loss: 28.1248 - val_MinusLogProbMetric: 28.1248 - lr: 1.2500e-04 - 36s/epoch - 182ms/step
Epoch 766/1000
2023-10-26 05:52:43.795 
Epoch 766/1000 
	 loss: 27.8321, MinusLogProbMetric: 27.8321, val_loss: 28.0557, val_MinusLogProbMetric: 28.0557

Epoch 766: val_loss did not improve from 27.99762
196/196 - 36s - loss: 27.8321 - MinusLogProbMetric: 27.8321 - val_loss: 28.0557 - val_MinusLogProbMetric: 28.0557 - lr: 1.2500e-04 - 36s/epoch - 182ms/step
Epoch 767/1000
2023-10-26 05:53:19.682 
Epoch 767/1000 
	 loss: 27.7786, MinusLogProbMetric: 27.7786, val_loss: 28.0891, val_MinusLogProbMetric: 28.0891

Epoch 767: val_loss did not improve from 27.99762
196/196 - 36s - loss: 27.7786 - MinusLogProbMetric: 27.7786 - val_loss: 28.0891 - val_MinusLogProbMetric: 28.0891 - lr: 1.2500e-04 - 36s/epoch - 183ms/step
Epoch 768/1000
2023-10-26 05:53:55.359 
Epoch 768/1000 
	 loss: 27.7869, MinusLogProbMetric: 27.7869, val_loss: 28.1236, val_MinusLogProbMetric: 28.1236

Epoch 768: val_loss did not improve from 27.99762
196/196 - 36s - loss: 27.7869 - MinusLogProbMetric: 27.7869 - val_loss: 28.1236 - val_MinusLogProbMetric: 28.1236 - lr: 1.2500e-04 - 36s/epoch - 182ms/step
Epoch 769/1000
2023-10-26 05:54:31.295 
Epoch 769/1000 
	 loss: 27.7904, MinusLogProbMetric: 27.7904, val_loss: 28.2510, val_MinusLogProbMetric: 28.2510

Epoch 769: val_loss did not improve from 27.99762
196/196 - 36s - loss: 27.7904 - MinusLogProbMetric: 27.7904 - val_loss: 28.2510 - val_MinusLogProbMetric: 28.2510 - lr: 1.2500e-04 - 36s/epoch - 183ms/step
Epoch 770/1000
2023-10-26 05:55:06.835 
Epoch 770/1000 
	 loss: 27.7770, MinusLogProbMetric: 27.7770, val_loss: 28.0202, val_MinusLogProbMetric: 28.0202

Epoch 770: val_loss did not improve from 27.99762
196/196 - 36s - loss: 27.7770 - MinusLogProbMetric: 27.7770 - val_loss: 28.0202 - val_MinusLogProbMetric: 28.0202 - lr: 1.2500e-04 - 36s/epoch - 181ms/step
Epoch 771/1000
2023-10-26 05:55:42.559 
Epoch 771/1000 
	 loss: 27.7703, MinusLogProbMetric: 27.7703, val_loss: 28.0193, val_MinusLogProbMetric: 28.0193

Epoch 771: val_loss did not improve from 27.99762
196/196 - 36s - loss: 27.7703 - MinusLogProbMetric: 27.7703 - val_loss: 28.0193 - val_MinusLogProbMetric: 28.0193 - lr: 1.2500e-04 - 36s/epoch - 182ms/step
Epoch 772/1000
2023-10-26 05:56:18.025 
Epoch 772/1000 
	 loss: 27.7847, MinusLogProbMetric: 27.7847, val_loss: 28.1125, val_MinusLogProbMetric: 28.1125

Epoch 772: val_loss did not improve from 27.99762
196/196 - 35s - loss: 27.7847 - MinusLogProbMetric: 27.7847 - val_loss: 28.1125 - val_MinusLogProbMetric: 28.1125 - lr: 1.2500e-04 - 35s/epoch - 181ms/step
Epoch 773/1000
2023-10-26 05:56:53.759 
Epoch 773/1000 
	 loss: 27.7567, MinusLogProbMetric: 27.7567, val_loss: 28.0072, val_MinusLogProbMetric: 28.0072

Epoch 773: val_loss did not improve from 27.99762
196/196 - 36s - loss: 27.7567 - MinusLogProbMetric: 27.7567 - val_loss: 28.0072 - val_MinusLogProbMetric: 28.0072 - lr: 1.2500e-04 - 36s/epoch - 182ms/step
Epoch 774/1000
2023-10-26 05:57:29.433 
Epoch 774/1000 
	 loss: 27.7751, MinusLogProbMetric: 27.7751, val_loss: 27.9954, val_MinusLogProbMetric: 27.9954

Epoch 774: val_loss improved from 27.99762 to 27.99537, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_369/weights/best_weights.h5
196/196 - 36s - loss: 27.7751 - MinusLogProbMetric: 27.7751 - val_loss: 27.9954 - val_MinusLogProbMetric: 27.9954 - lr: 1.2500e-04 - 36s/epoch - 186ms/step
Epoch 775/1000
2023-10-26 05:58:06.248 
Epoch 775/1000 
	 loss: 27.7900, MinusLogProbMetric: 27.7900, val_loss: 27.9990, val_MinusLogProbMetric: 27.9990

Epoch 775: val_loss did not improve from 27.99537
196/196 - 36s - loss: 27.7900 - MinusLogProbMetric: 27.7900 - val_loss: 27.9990 - val_MinusLogProbMetric: 27.9990 - lr: 1.2500e-04 - 36s/epoch - 184ms/step
Epoch 776/1000
2023-10-26 05:58:42.079 
Epoch 776/1000 
	 loss: 27.7738, MinusLogProbMetric: 27.7738, val_loss: 27.9861, val_MinusLogProbMetric: 27.9861

Epoch 776: val_loss improved from 27.99537 to 27.98608, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_369/weights/best_weights.h5
196/196 - 37s - loss: 27.7738 - MinusLogProbMetric: 27.7738 - val_loss: 27.9861 - val_MinusLogProbMetric: 27.9861 - lr: 1.2500e-04 - 37s/epoch - 187ms/step
Epoch 777/1000
2023-10-26 05:59:18.485 
Epoch 777/1000 
	 loss: 27.7510, MinusLogProbMetric: 27.7510, val_loss: 28.0786, val_MinusLogProbMetric: 28.0786

Epoch 777: val_loss did not improve from 27.98608
196/196 - 36s - loss: 27.7510 - MinusLogProbMetric: 27.7510 - val_loss: 28.0786 - val_MinusLogProbMetric: 28.0786 - lr: 1.2500e-04 - 36s/epoch - 182ms/step
Epoch 778/1000
2023-10-26 05:59:54.390 
Epoch 778/1000 
	 loss: 27.7737, MinusLogProbMetric: 27.7737, val_loss: 28.0433, val_MinusLogProbMetric: 28.0433

Epoch 778: val_loss did not improve from 27.98608
196/196 - 36s - loss: 27.7737 - MinusLogProbMetric: 27.7737 - val_loss: 28.0433 - val_MinusLogProbMetric: 28.0433 - lr: 1.2500e-04 - 36s/epoch - 183ms/step
Epoch 779/1000
2023-10-26 06:00:29.887 
Epoch 779/1000 
	 loss: 27.7837, MinusLogProbMetric: 27.7837, val_loss: 28.0020, val_MinusLogProbMetric: 28.0020

Epoch 779: val_loss did not improve from 27.98608
196/196 - 35s - loss: 27.7837 - MinusLogProbMetric: 27.7837 - val_loss: 28.0020 - val_MinusLogProbMetric: 28.0020 - lr: 1.2500e-04 - 35s/epoch - 181ms/step
Epoch 780/1000
2023-10-26 06:01:06.046 
Epoch 780/1000 
	 loss: 27.7808, MinusLogProbMetric: 27.7808, val_loss: 28.0092, val_MinusLogProbMetric: 28.0092

Epoch 780: val_loss did not improve from 27.98608
196/196 - 36s - loss: 27.7808 - MinusLogProbMetric: 27.7808 - val_loss: 28.0092 - val_MinusLogProbMetric: 28.0092 - lr: 1.2500e-04 - 36s/epoch - 184ms/step
Epoch 781/1000
2023-10-26 06:01:41.785 
Epoch 781/1000 
	 loss: 27.7699, MinusLogProbMetric: 27.7699, val_loss: 28.0147, val_MinusLogProbMetric: 28.0147

Epoch 781: val_loss did not improve from 27.98608
196/196 - 36s - loss: 27.7699 - MinusLogProbMetric: 27.7699 - val_loss: 28.0147 - val_MinusLogProbMetric: 28.0147 - lr: 1.2500e-04 - 36s/epoch - 182ms/step
Epoch 782/1000
2023-10-26 06:02:17.462 
Epoch 782/1000 
	 loss: 27.7689, MinusLogProbMetric: 27.7689, val_loss: 28.0133, val_MinusLogProbMetric: 28.0133

Epoch 782: val_loss did not improve from 27.98608
196/196 - 36s - loss: 27.7689 - MinusLogProbMetric: 27.7689 - val_loss: 28.0133 - val_MinusLogProbMetric: 28.0133 - lr: 1.2500e-04 - 36s/epoch - 182ms/step
Epoch 783/1000
2023-10-26 06:02:53.330 
Epoch 783/1000 
	 loss: 27.7657, MinusLogProbMetric: 27.7657, val_loss: 27.9846, val_MinusLogProbMetric: 27.9846

Epoch 783: val_loss improved from 27.98608 to 27.98459, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_369/weights/best_weights.h5
196/196 - 36s - loss: 27.7657 - MinusLogProbMetric: 27.7657 - val_loss: 27.9846 - val_MinusLogProbMetric: 27.9846 - lr: 1.2500e-04 - 36s/epoch - 186ms/step
Epoch 784/1000
2023-10-26 06:03:29.636 
Epoch 784/1000 
	 loss: 27.7590, MinusLogProbMetric: 27.7590, val_loss: 28.0119, val_MinusLogProbMetric: 28.0119

Epoch 784: val_loss did not improve from 27.98459
196/196 - 36s - loss: 27.7590 - MinusLogProbMetric: 27.7590 - val_loss: 28.0119 - val_MinusLogProbMetric: 28.0119 - lr: 1.2500e-04 - 36s/epoch - 182ms/step
Epoch 785/1000
2023-10-26 06:04:05.447 
Epoch 785/1000 
	 loss: 27.7671, MinusLogProbMetric: 27.7671, val_loss: 28.0004, val_MinusLogProbMetric: 28.0004

Epoch 785: val_loss did not improve from 27.98459
196/196 - 36s - loss: 27.7671 - MinusLogProbMetric: 27.7671 - val_loss: 28.0004 - val_MinusLogProbMetric: 28.0004 - lr: 1.2500e-04 - 36s/epoch - 183ms/step
Epoch 786/1000
2023-10-26 06:04:41.042 
Epoch 786/1000 
	 loss: 27.7744, MinusLogProbMetric: 27.7744, val_loss: 28.0479, val_MinusLogProbMetric: 28.0479

Epoch 786: val_loss did not improve from 27.98459
196/196 - 36s - loss: 27.7744 - MinusLogProbMetric: 27.7744 - val_loss: 28.0479 - val_MinusLogProbMetric: 28.0479 - lr: 1.2500e-04 - 36s/epoch - 182ms/step
Epoch 787/1000
2023-10-26 06:05:16.517 
Epoch 787/1000 
	 loss: 27.7605, MinusLogProbMetric: 27.7605, val_loss: 28.0400, val_MinusLogProbMetric: 28.0400

Epoch 787: val_loss did not improve from 27.98459
196/196 - 35s - loss: 27.7605 - MinusLogProbMetric: 27.7605 - val_loss: 28.0400 - val_MinusLogProbMetric: 28.0400 - lr: 1.2500e-04 - 35s/epoch - 181ms/step
Epoch 788/1000
2023-10-26 06:05:52.063 
Epoch 788/1000 
	 loss: 27.7839, MinusLogProbMetric: 27.7839, val_loss: 28.0370, val_MinusLogProbMetric: 28.0370

Epoch 788: val_loss did not improve from 27.98459
196/196 - 36s - loss: 27.7839 - MinusLogProbMetric: 27.7839 - val_loss: 28.0370 - val_MinusLogProbMetric: 28.0370 - lr: 1.2500e-04 - 36s/epoch - 181ms/step
Epoch 789/1000
2023-10-26 06:06:27.894 
Epoch 789/1000 
	 loss: 27.8086, MinusLogProbMetric: 27.8086, val_loss: 28.0244, val_MinusLogProbMetric: 28.0244

Epoch 789: val_loss did not improve from 27.98459
196/196 - 36s - loss: 27.8086 - MinusLogProbMetric: 27.8086 - val_loss: 28.0244 - val_MinusLogProbMetric: 28.0244 - lr: 1.2500e-04 - 36s/epoch - 183ms/step
Epoch 790/1000
2023-10-26 06:07:03.071 
Epoch 790/1000 
	 loss: 27.8099, MinusLogProbMetric: 27.8099, val_loss: 28.1297, val_MinusLogProbMetric: 28.1297

Epoch 790: val_loss did not improve from 27.98459
196/196 - 35s - loss: 27.8099 - MinusLogProbMetric: 27.8099 - val_loss: 28.1297 - val_MinusLogProbMetric: 28.1297 - lr: 1.2500e-04 - 35s/epoch - 179ms/step
Epoch 791/1000
2023-10-26 06:07:38.595 
Epoch 791/1000 
	 loss: 27.7804, MinusLogProbMetric: 27.7804, val_loss: 28.0209, val_MinusLogProbMetric: 28.0209

Epoch 791: val_loss did not improve from 27.98459
196/196 - 36s - loss: 27.7804 - MinusLogProbMetric: 27.7804 - val_loss: 28.0209 - val_MinusLogProbMetric: 28.0209 - lr: 1.2500e-04 - 36s/epoch - 181ms/step
Epoch 792/1000
2023-10-26 06:08:14.782 
Epoch 792/1000 
	 loss: 27.7672, MinusLogProbMetric: 27.7672, val_loss: 28.0468, val_MinusLogProbMetric: 28.0468

Epoch 792: val_loss did not improve from 27.98459
196/196 - 36s - loss: 27.7672 - MinusLogProbMetric: 27.7672 - val_loss: 28.0468 - val_MinusLogProbMetric: 28.0468 - lr: 1.2500e-04 - 36s/epoch - 185ms/step
Epoch 793/1000
2023-10-26 06:08:50.491 
Epoch 793/1000 
	 loss: 27.7835, MinusLogProbMetric: 27.7835, val_loss: 28.0171, val_MinusLogProbMetric: 28.0171

Epoch 793: val_loss did not improve from 27.98459
196/196 - 36s - loss: 27.7835 - MinusLogProbMetric: 27.7835 - val_loss: 28.0171 - val_MinusLogProbMetric: 28.0171 - lr: 1.2500e-04 - 36s/epoch - 182ms/step
Epoch 794/1000
2023-10-26 06:09:25.984 
Epoch 794/1000 
	 loss: 27.7808, MinusLogProbMetric: 27.7808, val_loss: 27.9981, val_MinusLogProbMetric: 27.9981

Epoch 794: val_loss did not improve from 27.98459
196/196 - 35s - loss: 27.7808 - MinusLogProbMetric: 27.7808 - val_loss: 27.9981 - val_MinusLogProbMetric: 27.9981 - lr: 1.2500e-04 - 35s/epoch - 181ms/step
Epoch 795/1000
2023-10-26 06:10:01.606 
Epoch 795/1000 
	 loss: 27.8088, MinusLogProbMetric: 27.8088, val_loss: 28.0495, val_MinusLogProbMetric: 28.0495

Epoch 795: val_loss did not improve from 27.98459
196/196 - 36s - loss: 27.8088 - MinusLogProbMetric: 27.8088 - val_loss: 28.0495 - val_MinusLogProbMetric: 28.0495 - lr: 1.2500e-04 - 36s/epoch - 182ms/step
Epoch 796/1000
2023-10-26 06:10:37.410 
Epoch 796/1000 
	 loss: 27.7710, MinusLogProbMetric: 27.7710, val_loss: 28.0132, val_MinusLogProbMetric: 28.0132

Epoch 796: val_loss did not improve from 27.98459
196/196 - 36s - loss: 27.7710 - MinusLogProbMetric: 27.7710 - val_loss: 28.0132 - val_MinusLogProbMetric: 28.0132 - lr: 1.2500e-04 - 36s/epoch - 183ms/step
Epoch 797/1000
2023-10-26 06:11:13.118 
Epoch 797/1000 
	 loss: 27.8025, MinusLogProbMetric: 27.8025, val_loss: 28.0145, val_MinusLogProbMetric: 28.0145

Epoch 797: val_loss did not improve from 27.98459
196/196 - 36s - loss: 27.8025 - MinusLogProbMetric: 27.8025 - val_loss: 28.0145 - val_MinusLogProbMetric: 28.0145 - lr: 1.2500e-04 - 36s/epoch - 182ms/step
Epoch 798/1000
2023-10-26 06:11:48.831 
Epoch 798/1000 
	 loss: 27.7757, MinusLogProbMetric: 27.7757, val_loss: 28.0004, val_MinusLogProbMetric: 28.0004

Epoch 798: val_loss did not improve from 27.98459
196/196 - 36s - loss: 27.7757 - MinusLogProbMetric: 27.7757 - val_loss: 28.0004 - val_MinusLogProbMetric: 28.0004 - lr: 1.2500e-04 - 36s/epoch - 182ms/step
Epoch 799/1000
2023-10-26 06:12:24.711 
Epoch 799/1000 
	 loss: 27.7899, MinusLogProbMetric: 27.7899, val_loss: 27.9905, val_MinusLogProbMetric: 27.9905

Epoch 799: val_loss did not improve from 27.98459
196/196 - 36s - loss: 27.7899 - MinusLogProbMetric: 27.7899 - val_loss: 27.9905 - val_MinusLogProbMetric: 27.9905 - lr: 1.2500e-04 - 36s/epoch - 183ms/step
Epoch 800/1000
2023-10-26 06:13:00.520 
Epoch 800/1000 
	 loss: 27.7979, MinusLogProbMetric: 27.7979, val_loss: 28.0099, val_MinusLogProbMetric: 28.0099

Epoch 800: val_loss did not improve from 27.98459
196/196 - 36s - loss: 27.7979 - MinusLogProbMetric: 27.7979 - val_loss: 28.0099 - val_MinusLogProbMetric: 28.0099 - lr: 1.2500e-04 - 36s/epoch - 183ms/step
Epoch 801/1000
2023-10-26 06:13:35.880 
Epoch 801/1000 
	 loss: 27.7591, MinusLogProbMetric: 27.7591, val_loss: 28.0375, val_MinusLogProbMetric: 28.0375

Epoch 801: val_loss did not improve from 27.98459
196/196 - 35s - loss: 27.7591 - MinusLogProbMetric: 27.7591 - val_loss: 28.0375 - val_MinusLogProbMetric: 28.0375 - lr: 1.2500e-04 - 35s/epoch - 180ms/step
Epoch 802/1000
2023-10-26 06:14:11.203 
Epoch 802/1000 
	 loss: 27.7698, MinusLogProbMetric: 27.7698, val_loss: 28.0607, val_MinusLogProbMetric: 28.0607

Epoch 802: val_loss did not improve from 27.98459
196/196 - 35s - loss: 27.7698 - MinusLogProbMetric: 27.7698 - val_loss: 28.0607 - val_MinusLogProbMetric: 28.0607 - lr: 1.2500e-04 - 35s/epoch - 180ms/step
Epoch 803/1000
2023-10-26 06:14:46.946 
Epoch 803/1000 
	 loss: 27.7945, MinusLogProbMetric: 27.7945, val_loss: 28.0662, val_MinusLogProbMetric: 28.0662

Epoch 803: val_loss did not improve from 27.98459
196/196 - 36s - loss: 27.7945 - MinusLogProbMetric: 27.7945 - val_loss: 28.0662 - val_MinusLogProbMetric: 28.0662 - lr: 1.2500e-04 - 36s/epoch - 182ms/step
Epoch 804/1000
2023-10-26 06:15:22.522 
Epoch 804/1000 
	 loss: 27.8170, MinusLogProbMetric: 27.8170, val_loss: 28.0372, val_MinusLogProbMetric: 28.0372

Epoch 804: val_loss did not improve from 27.98459
196/196 - 36s - loss: 27.8170 - MinusLogProbMetric: 27.8170 - val_loss: 28.0372 - val_MinusLogProbMetric: 28.0372 - lr: 1.2500e-04 - 36s/epoch - 181ms/step
Epoch 805/1000
2023-10-26 06:15:58.145 
Epoch 805/1000 
	 loss: 27.8263, MinusLogProbMetric: 27.8263, val_loss: 27.9934, val_MinusLogProbMetric: 27.9934

Epoch 805: val_loss did not improve from 27.98459
196/196 - 36s - loss: 27.8263 - MinusLogProbMetric: 27.8263 - val_loss: 27.9934 - val_MinusLogProbMetric: 27.9934 - lr: 1.2500e-04 - 36s/epoch - 182ms/step
Epoch 806/1000
2023-10-26 06:16:33.726 
Epoch 806/1000 
	 loss: 27.8804, MinusLogProbMetric: 27.8804, val_loss: 28.5690, val_MinusLogProbMetric: 28.5690

Epoch 806: val_loss did not improve from 27.98459
196/196 - 36s - loss: 27.8804 - MinusLogProbMetric: 27.8804 - val_loss: 28.5690 - val_MinusLogProbMetric: 28.5690 - lr: 1.2500e-04 - 36s/epoch - 182ms/step
Epoch 807/1000
2023-10-26 06:17:09.415 
Epoch 807/1000 
	 loss: 27.8847, MinusLogProbMetric: 27.8847, val_loss: 28.1230, val_MinusLogProbMetric: 28.1230

Epoch 807: val_loss did not improve from 27.98459
196/196 - 36s - loss: 27.8847 - MinusLogProbMetric: 27.8847 - val_loss: 28.1230 - val_MinusLogProbMetric: 28.1230 - lr: 1.2500e-04 - 36s/epoch - 182ms/step
Epoch 808/1000
2023-10-26 06:17:45.203 
Epoch 808/1000 
	 loss: 27.8587, MinusLogProbMetric: 27.8587, val_loss: 28.1138, val_MinusLogProbMetric: 28.1138

Epoch 808: val_loss did not improve from 27.98459
196/196 - 36s - loss: 27.8587 - MinusLogProbMetric: 27.8587 - val_loss: 28.1138 - val_MinusLogProbMetric: 28.1138 - lr: 1.2500e-04 - 36s/epoch - 183ms/step
Epoch 809/1000
2023-10-26 06:18:20.948 
Epoch 809/1000 
	 loss: 27.8811, MinusLogProbMetric: 27.8811, val_loss: 28.3710, val_MinusLogProbMetric: 28.3710

Epoch 809: val_loss did not improve from 27.98459
196/196 - 36s - loss: 27.8811 - MinusLogProbMetric: 27.8811 - val_loss: 28.3710 - val_MinusLogProbMetric: 28.3710 - lr: 1.2500e-04 - 36s/epoch - 182ms/step
Epoch 810/1000
2023-10-26 06:18:56.389 
Epoch 810/1000 
	 loss: 27.8965, MinusLogProbMetric: 27.8965, val_loss: 28.1264, val_MinusLogProbMetric: 28.1264

Epoch 810: val_loss did not improve from 27.98459
196/196 - 35s - loss: 27.8965 - MinusLogProbMetric: 27.8965 - val_loss: 28.1264 - val_MinusLogProbMetric: 28.1264 - lr: 1.2500e-04 - 35s/epoch - 181ms/step
Epoch 811/1000
2023-10-26 06:19:31.766 
Epoch 811/1000 
	 loss: 27.9057, MinusLogProbMetric: 27.9057, val_loss: 28.2182, val_MinusLogProbMetric: 28.2182

Epoch 811: val_loss did not improve from 27.98459
196/196 - 35s - loss: 27.9057 - MinusLogProbMetric: 27.9057 - val_loss: 28.2182 - val_MinusLogProbMetric: 28.2182 - lr: 1.2500e-04 - 35s/epoch - 180ms/step
Epoch 812/1000
2023-10-26 06:20:07.194 
Epoch 812/1000 
	 loss: 27.8789, MinusLogProbMetric: 27.8789, val_loss: 28.0649, val_MinusLogProbMetric: 28.0649

Epoch 812: val_loss did not improve from 27.98459
196/196 - 35s - loss: 27.8789 - MinusLogProbMetric: 27.8789 - val_loss: 28.0649 - val_MinusLogProbMetric: 28.0649 - lr: 1.2500e-04 - 35s/epoch - 181ms/step
Epoch 813/1000
2023-10-26 06:20:42.551 
Epoch 813/1000 
	 loss: 27.8434, MinusLogProbMetric: 27.8434, val_loss: 28.0783, val_MinusLogProbMetric: 28.0783

Epoch 813: val_loss did not improve from 27.98459
196/196 - 35s - loss: 27.8434 - MinusLogProbMetric: 27.8434 - val_loss: 28.0783 - val_MinusLogProbMetric: 28.0783 - lr: 1.2500e-04 - 35s/epoch - 180ms/step
Epoch 814/1000
2023-10-26 06:21:18.362 
Epoch 814/1000 
	 loss: 27.8289, MinusLogProbMetric: 27.8289, val_loss: 28.3569, val_MinusLogProbMetric: 28.3569

Epoch 814: val_loss did not improve from 27.98459
196/196 - 36s - loss: 27.8289 - MinusLogProbMetric: 27.8289 - val_loss: 28.3569 - val_MinusLogProbMetric: 28.3569 - lr: 1.2500e-04 - 36s/epoch - 183ms/step
Epoch 815/1000
2023-10-26 06:21:53.927 
Epoch 815/1000 
	 loss: 27.8331, MinusLogProbMetric: 27.8331, val_loss: 28.3260, val_MinusLogProbMetric: 28.3260

Epoch 815: val_loss did not improve from 27.98459
196/196 - 36s - loss: 27.8331 - MinusLogProbMetric: 27.8331 - val_loss: 28.3260 - val_MinusLogProbMetric: 28.3260 - lr: 1.2500e-04 - 36s/epoch - 181ms/step
Epoch 816/1000
2023-10-26 06:22:29.503 
Epoch 816/1000 
	 loss: 27.8608, MinusLogProbMetric: 27.8608, val_loss: 28.2156, val_MinusLogProbMetric: 28.2156

Epoch 816: val_loss did not improve from 27.98459
196/196 - 36s - loss: 27.8608 - MinusLogProbMetric: 27.8608 - val_loss: 28.2156 - val_MinusLogProbMetric: 28.2156 - lr: 1.2500e-04 - 36s/epoch - 181ms/step
Epoch 817/1000
2023-10-26 06:23:05.018 
Epoch 817/1000 
	 loss: 27.8179, MinusLogProbMetric: 27.8179, val_loss: 28.0667, val_MinusLogProbMetric: 28.0667

Epoch 817: val_loss did not improve from 27.98459
196/196 - 36s - loss: 27.8179 - MinusLogProbMetric: 27.8179 - val_loss: 28.0667 - val_MinusLogProbMetric: 28.0667 - lr: 1.2500e-04 - 36s/epoch - 181ms/step
Epoch 818/1000
2023-10-26 06:23:40.830 
Epoch 818/1000 
	 loss: 27.7856, MinusLogProbMetric: 27.7856, val_loss: 28.2182, val_MinusLogProbMetric: 28.2182

Epoch 818: val_loss did not improve from 27.98459
196/196 - 36s - loss: 27.7856 - MinusLogProbMetric: 27.7856 - val_loss: 28.2182 - val_MinusLogProbMetric: 28.2182 - lr: 1.2500e-04 - 36s/epoch - 183ms/step
Epoch 819/1000
2023-10-26 06:24:16.434 
Epoch 819/1000 
	 loss: 27.9019, MinusLogProbMetric: 27.9019, val_loss: 28.0182, val_MinusLogProbMetric: 28.0182

Epoch 819: val_loss did not improve from 27.98459
196/196 - 36s - loss: 27.9019 - MinusLogProbMetric: 27.9019 - val_loss: 28.0182 - val_MinusLogProbMetric: 28.0182 - lr: 1.2500e-04 - 36s/epoch - 182ms/step
Epoch 820/1000
2023-10-26 06:24:51.775 
Epoch 820/1000 
	 loss: 27.8007, MinusLogProbMetric: 27.8007, val_loss: 28.3332, val_MinusLogProbMetric: 28.3332

Epoch 820: val_loss did not improve from 27.98459
196/196 - 35s - loss: 27.8007 - MinusLogProbMetric: 27.8007 - val_loss: 28.3332 - val_MinusLogProbMetric: 28.3332 - lr: 1.2500e-04 - 35s/epoch - 180ms/step
Epoch 821/1000
2023-10-26 06:25:27.417 
Epoch 821/1000 
	 loss: 27.7866, MinusLogProbMetric: 27.7866, val_loss: 28.0370, val_MinusLogProbMetric: 28.0370

Epoch 821: val_loss did not improve from 27.98459
196/196 - 36s - loss: 27.7866 - MinusLogProbMetric: 27.7866 - val_loss: 28.0370 - val_MinusLogProbMetric: 28.0370 - lr: 1.2500e-04 - 36s/epoch - 182ms/step
Epoch 822/1000
2023-10-26 06:26:03.184 
Epoch 822/1000 
	 loss: 27.7760, MinusLogProbMetric: 27.7760, val_loss: 28.1071, val_MinusLogProbMetric: 28.1071

Epoch 822: val_loss did not improve from 27.98459
196/196 - 36s - loss: 27.7760 - MinusLogProbMetric: 27.7760 - val_loss: 28.1071 - val_MinusLogProbMetric: 28.1071 - lr: 1.2500e-04 - 36s/epoch - 182ms/step
Epoch 823/1000
2023-10-26 06:26:38.766 
Epoch 823/1000 
	 loss: 27.8461, MinusLogProbMetric: 27.8461, val_loss: 28.1972, val_MinusLogProbMetric: 28.1972

Epoch 823: val_loss did not improve from 27.98459
196/196 - 36s - loss: 27.8461 - MinusLogProbMetric: 27.8461 - val_loss: 28.1972 - val_MinusLogProbMetric: 28.1972 - lr: 1.2500e-04 - 36s/epoch - 182ms/step
Epoch 824/1000
2023-10-26 06:27:14.514 
Epoch 824/1000 
	 loss: 27.8120, MinusLogProbMetric: 27.8120, val_loss: 28.0266, val_MinusLogProbMetric: 28.0266

Epoch 824: val_loss did not improve from 27.98459
196/196 - 36s - loss: 27.8120 - MinusLogProbMetric: 27.8120 - val_loss: 28.0266 - val_MinusLogProbMetric: 28.0266 - lr: 1.2500e-04 - 36s/epoch - 182ms/step
Epoch 825/1000
2023-10-26 06:27:49.794 
Epoch 825/1000 
	 loss: 27.8344, MinusLogProbMetric: 27.8344, val_loss: 28.2120, val_MinusLogProbMetric: 28.2120

Epoch 825: val_loss did not improve from 27.98459
196/196 - 35s - loss: 27.8344 - MinusLogProbMetric: 27.8344 - val_loss: 28.2120 - val_MinusLogProbMetric: 28.2120 - lr: 1.2500e-04 - 35s/epoch - 180ms/step
Epoch 826/1000
2023-10-26 06:28:25.183 
Epoch 826/1000 
	 loss: 27.7946, MinusLogProbMetric: 27.7946, val_loss: 28.0929, val_MinusLogProbMetric: 28.0929

Epoch 826: val_loss did not improve from 27.98459
196/196 - 35s - loss: 27.7946 - MinusLogProbMetric: 27.7946 - val_loss: 28.0929 - val_MinusLogProbMetric: 28.0929 - lr: 1.2500e-04 - 35s/epoch - 181ms/step
Epoch 827/1000
2023-10-26 06:29:00.960 
Epoch 827/1000 
	 loss: 27.8454, MinusLogProbMetric: 27.8454, val_loss: 28.3700, val_MinusLogProbMetric: 28.3700

Epoch 827: val_loss did not improve from 27.98459
196/196 - 36s - loss: 27.8454 - MinusLogProbMetric: 27.8454 - val_loss: 28.3700 - val_MinusLogProbMetric: 28.3700 - lr: 1.2500e-04 - 36s/epoch - 183ms/step
Epoch 828/1000
2023-10-26 06:29:36.346 
Epoch 828/1000 
	 loss: 27.8654, MinusLogProbMetric: 27.8654, val_loss: 28.2038, val_MinusLogProbMetric: 28.2038

Epoch 828: val_loss did not improve from 27.98459
196/196 - 35s - loss: 27.8654 - MinusLogProbMetric: 27.8654 - val_loss: 28.2038 - val_MinusLogProbMetric: 28.2038 - lr: 1.2500e-04 - 35s/epoch - 181ms/step
Epoch 829/1000
2023-10-26 06:30:11.857 
Epoch 829/1000 
	 loss: 27.8290, MinusLogProbMetric: 27.8290, val_loss: 28.1581, val_MinusLogProbMetric: 28.1581

Epoch 829: val_loss did not improve from 27.98459
196/196 - 36s - loss: 27.8290 - MinusLogProbMetric: 27.8290 - val_loss: 28.1581 - val_MinusLogProbMetric: 28.1581 - lr: 1.2500e-04 - 36s/epoch - 181ms/step
Epoch 830/1000
2023-10-26 06:30:47.607 
Epoch 830/1000 
	 loss: 27.8152, MinusLogProbMetric: 27.8152, val_loss: 28.4289, val_MinusLogProbMetric: 28.4289

Epoch 830: val_loss did not improve from 27.98459
196/196 - 36s - loss: 27.8152 - MinusLogProbMetric: 27.8152 - val_loss: 28.4289 - val_MinusLogProbMetric: 28.4289 - lr: 1.2500e-04 - 36s/epoch - 182ms/step
Epoch 831/1000
2023-10-26 06:31:23.085 
Epoch 831/1000 
	 loss: 27.8505, MinusLogProbMetric: 27.8505, val_loss: 28.0278, val_MinusLogProbMetric: 28.0278

Epoch 831: val_loss did not improve from 27.98459
196/196 - 35s - loss: 27.8505 - MinusLogProbMetric: 27.8505 - val_loss: 28.0278 - val_MinusLogProbMetric: 28.0278 - lr: 1.2500e-04 - 35s/epoch - 181ms/step
Epoch 832/1000
2023-10-26 06:31:58.491 
Epoch 832/1000 
	 loss: 27.7918, MinusLogProbMetric: 27.7918, val_loss: 28.1196, val_MinusLogProbMetric: 28.1196

Epoch 832: val_loss did not improve from 27.98459
196/196 - 35s - loss: 27.7918 - MinusLogProbMetric: 27.7918 - val_loss: 28.1196 - val_MinusLogProbMetric: 28.1196 - lr: 1.2500e-04 - 35s/epoch - 181ms/step
Epoch 833/1000
2023-10-26 06:32:34.075 
Epoch 833/1000 
	 loss: 27.8383, MinusLogProbMetric: 27.8383, val_loss: 28.0501, val_MinusLogProbMetric: 28.0501

Epoch 833: val_loss did not improve from 27.98459
196/196 - 36s - loss: 27.8383 - MinusLogProbMetric: 27.8383 - val_loss: 28.0501 - val_MinusLogProbMetric: 28.0501 - lr: 1.2500e-04 - 36s/epoch - 182ms/step
Epoch 834/1000
2023-10-26 06:33:09.634 
Epoch 834/1000 
	 loss: 27.7297, MinusLogProbMetric: 27.7297, val_loss: 28.0024, val_MinusLogProbMetric: 28.0024

Epoch 834: val_loss did not improve from 27.98459
196/196 - 36s - loss: 27.7297 - MinusLogProbMetric: 27.7297 - val_loss: 28.0024 - val_MinusLogProbMetric: 28.0024 - lr: 6.2500e-05 - 36s/epoch - 181ms/step
Epoch 835/1000
2023-10-26 06:33:45.146 
Epoch 835/1000 
	 loss: 27.7234, MinusLogProbMetric: 27.7234, val_loss: 28.0082, val_MinusLogProbMetric: 28.0082

Epoch 835: val_loss did not improve from 27.98459
196/196 - 36s - loss: 27.7234 - MinusLogProbMetric: 27.7234 - val_loss: 28.0082 - val_MinusLogProbMetric: 28.0082 - lr: 6.2500e-05 - 36s/epoch - 181ms/step
Epoch 836/1000
2023-10-26 06:34:20.759 
Epoch 836/1000 
	 loss: 27.7285, MinusLogProbMetric: 27.7285, val_loss: 27.9724, val_MinusLogProbMetric: 27.9724

Epoch 836: val_loss improved from 27.98459 to 27.97235, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_369/weights/best_weights.h5
196/196 - 36s - loss: 27.7285 - MinusLogProbMetric: 27.7285 - val_loss: 27.9724 - val_MinusLogProbMetric: 27.9724 - lr: 6.2500e-05 - 36s/epoch - 185ms/step
Epoch 837/1000
2023-10-26 06:34:57.335 
Epoch 837/1000 
	 loss: 27.7426, MinusLogProbMetric: 27.7426, val_loss: 27.9653, val_MinusLogProbMetric: 27.9653

Epoch 837: val_loss improved from 27.97235 to 27.96530, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_369/weights/best_weights.h5
196/196 - 37s - loss: 27.7426 - MinusLogProbMetric: 27.7426 - val_loss: 27.9653 - val_MinusLogProbMetric: 27.9653 - lr: 6.2500e-05 - 37s/epoch - 186ms/step
Epoch 838/1000
2023-10-26 06:35:33.569 
Epoch 838/1000 
	 loss: 27.7258, MinusLogProbMetric: 27.7258, val_loss: 27.9615, val_MinusLogProbMetric: 27.9615

Epoch 838: val_loss improved from 27.96530 to 27.96146, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_369/weights/best_weights.h5
196/196 - 36s - loss: 27.7258 - MinusLogProbMetric: 27.7258 - val_loss: 27.9615 - val_MinusLogProbMetric: 27.9615 - lr: 6.2500e-05 - 36s/epoch - 185ms/step
Epoch 839/1000
2023-10-26 06:36:10.089 
Epoch 839/1000 
	 loss: 27.7393, MinusLogProbMetric: 27.7393, val_loss: 27.9590, val_MinusLogProbMetric: 27.9590

Epoch 839: val_loss improved from 27.96146 to 27.95904, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_369/weights/best_weights.h5
196/196 - 37s - loss: 27.7393 - MinusLogProbMetric: 27.7393 - val_loss: 27.9590 - val_MinusLogProbMetric: 27.9590 - lr: 6.2500e-05 - 37s/epoch - 186ms/step
Epoch 840/1000
2023-10-26 06:36:46.437 
Epoch 840/1000 
	 loss: 27.7227, MinusLogProbMetric: 27.7227, val_loss: 28.0433, val_MinusLogProbMetric: 28.0433

Epoch 840: val_loss did not improve from 27.95904
196/196 - 36s - loss: 27.7227 - MinusLogProbMetric: 27.7227 - val_loss: 28.0433 - val_MinusLogProbMetric: 28.0433 - lr: 6.2500e-05 - 36s/epoch - 182ms/step
Epoch 841/1000
2023-10-26 06:37:22.366 
Epoch 841/1000 
	 loss: 27.7453, MinusLogProbMetric: 27.7453, val_loss: 27.9997, val_MinusLogProbMetric: 27.9997

Epoch 841: val_loss did not improve from 27.95904
196/196 - 36s - loss: 27.7453 - MinusLogProbMetric: 27.7453 - val_loss: 27.9997 - val_MinusLogProbMetric: 27.9997 - lr: 6.2500e-05 - 36s/epoch - 183ms/step
Epoch 842/1000
2023-10-26 06:37:58.294 
Epoch 842/1000 
	 loss: 27.7222, MinusLogProbMetric: 27.7222, val_loss: 28.0083, val_MinusLogProbMetric: 28.0083

Epoch 842: val_loss did not improve from 27.95904
196/196 - 36s - loss: 27.7222 - MinusLogProbMetric: 27.7222 - val_loss: 28.0083 - val_MinusLogProbMetric: 28.0083 - lr: 6.2500e-05 - 36s/epoch - 183ms/step
Epoch 843/1000
2023-10-26 06:38:33.885 
Epoch 843/1000 
	 loss: 27.7279, MinusLogProbMetric: 27.7279, val_loss: 27.9636, val_MinusLogProbMetric: 27.9636

Epoch 843: val_loss did not improve from 27.95904
196/196 - 36s - loss: 27.7279 - MinusLogProbMetric: 27.7279 - val_loss: 27.9636 - val_MinusLogProbMetric: 27.9636 - lr: 6.2500e-05 - 36s/epoch - 182ms/step
Epoch 844/1000
2023-10-26 06:39:09.504 
Epoch 844/1000 
	 loss: 27.7243, MinusLogProbMetric: 27.7243, val_loss: 27.9606, val_MinusLogProbMetric: 27.9606

Epoch 844: val_loss did not improve from 27.95904
196/196 - 36s - loss: 27.7243 - MinusLogProbMetric: 27.7243 - val_loss: 27.9606 - val_MinusLogProbMetric: 27.9606 - lr: 6.2500e-05 - 36s/epoch - 182ms/step
Epoch 845/1000
2023-10-26 06:39:45.022 
Epoch 845/1000 
	 loss: 27.7126, MinusLogProbMetric: 27.7126, val_loss: 27.9600, val_MinusLogProbMetric: 27.9600

Epoch 845: val_loss did not improve from 27.95904
196/196 - 36s - loss: 27.7126 - MinusLogProbMetric: 27.7126 - val_loss: 27.9600 - val_MinusLogProbMetric: 27.9600 - lr: 6.2500e-05 - 36s/epoch - 181ms/step
Epoch 846/1000
2023-10-26 06:40:20.744 
Epoch 846/1000 
	 loss: 27.7299, MinusLogProbMetric: 27.7299, val_loss: 28.0047, val_MinusLogProbMetric: 28.0047

Epoch 846: val_loss did not improve from 27.95904
196/196 - 36s - loss: 27.7299 - MinusLogProbMetric: 27.7299 - val_loss: 28.0047 - val_MinusLogProbMetric: 28.0047 - lr: 6.2500e-05 - 36s/epoch - 182ms/step
Epoch 847/1000
2023-10-26 06:40:56.188 
Epoch 847/1000 
	 loss: 27.7254, MinusLogProbMetric: 27.7254, val_loss: 28.0890, val_MinusLogProbMetric: 28.0890

Epoch 847: val_loss did not improve from 27.95904
196/196 - 35s - loss: 27.7254 - MinusLogProbMetric: 27.7254 - val_loss: 28.0890 - val_MinusLogProbMetric: 28.0890 - lr: 6.2500e-05 - 35s/epoch - 181ms/step
Epoch 848/1000
2023-10-26 06:41:31.867 
Epoch 848/1000 
	 loss: 27.7307, MinusLogProbMetric: 27.7307, val_loss: 27.9759, val_MinusLogProbMetric: 27.9759

Epoch 848: val_loss did not improve from 27.95904
196/196 - 36s - loss: 27.7307 - MinusLogProbMetric: 27.7307 - val_loss: 27.9759 - val_MinusLogProbMetric: 27.9759 - lr: 6.2500e-05 - 36s/epoch - 182ms/step
Epoch 849/1000
2023-10-26 06:42:07.718 
Epoch 849/1000 
	 loss: 27.7170, MinusLogProbMetric: 27.7170, val_loss: 27.9674, val_MinusLogProbMetric: 27.9674

Epoch 849: val_loss did not improve from 27.95904
196/196 - 36s - loss: 27.7170 - MinusLogProbMetric: 27.7170 - val_loss: 27.9674 - val_MinusLogProbMetric: 27.9674 - lr: 6.2500e-05 - 36s/epoch - 183ms/step
Epoch 850/1000
2023-10-26 06:42:43.312 
Epoch 850/1000 
	 loss: 27.7260, MinusLogProbMetric: 27.7260, val_loss: 27.9935, val_MinusLogProbMetric: 27.9935

Epoch 850: val_loss did not improve from 27.95904
196/196 - 36s - loss: 27.7260 - MinusLogProbMetric: 27.7260 - val_loss: 27.9935 - val_MinusLogProbMetric: 27.9935 - lr: 6.2500e-05 - 36s/epoch - 182ms/step
Epoch 851/1000
2023-10-26 06:43:19.196 
Epoch 851/1000 
	 loss: 27.7117, MinusLogProbMetric: 27.7117, val_loss: 27.9821, val_MinusLogProbMetric: 27.9821

Epoch 851: val_loss did not improve from 27.95904
196/196 - 36s - loss: 27.7117 - MinusLogProbMetric: 27.7117 - val_loss: 27.9821 - val_MinusLogProbMetric: 27.9821 - lr: 6.2500e-05 - 36s/epoch - 183ms/step
Epoch 852/1000
2023-10-26 06:43:54.790 
Epoch 852/1000 
	 loss: 27.7280, MinusLogProbMetric: 27.7280, val_loss: 27.9827, val_MinusLogProbMetric: 27.9827

Epoch 852: val_loss did not improve from 27.95904
196/196 - 36s - loss: 27.7280 - MinusLogProbMetric: 27.7280 - val_loss: 27.9827 - val_MinusLogProbMetric: 27.9827 - lr: 6.2500e-05 - 36s/epoch - 182ms/step
Epoch 853/1000
2023-10-26 06:44:30.710 
Epoch 853/1000 
	 loss: 27.7139, MinusLogProbMetric: 27.7139, val_loss: 27.9505, val_MinusLogProbMetric: 27.9505

Epoch 853: val_loss improved from 27.95904 to 27.95050, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_369/weights/best_weights.h5
196/196 - 37s - loss: 27.7139 - MinusLogProbMetric: 27.7139 - val_loss: 27.9505 - val_MinusLogProbMetric: 27.9505 - lr: 6.2500e-05 - 37s/epoch - 187ms/step
Epoch 854/1000
2023-10-26 06:45:07.101 
Epoch 854/1000 
	 loss: 27.7350, MinusLogProbMetric: 27.7350, val_loss: 27.9701, val_MinusLogProbMetric: 27.9701

Epoch 854: val_loss did not improve from 27.95050
196/196 - 36s - loss: 27.7350 - MinusLogProbMetric: 27.7350 - val_loss: 27.9701 - val_MinusLogProbMetric: 27.9701 - lr: 6.2500e-05 - 36s/epoch - 182ms/step
Epoch 855/1000
2023-10-26 06:45:42.996 
Epoch 855/1000 
	 loss: 27.7301, MinusLogProbMetric: 27.7301, val_loss: 27.9583, val_MinusLogProbMetric: 27.9583

Epoch 855: val_loss did not improve from 27.95050
196/196 - 36s - loss: 27.7301 - MinusLogProbMetric: 27.7301 - val_loss: 27.9583 - val_MinusLogProbMetric: 27.9583 - lr: 6.2500e-05 - 36s/epoch - 183ms/step
Epoch 856/1000
2023-10-26 06:46:18.835 
Epoch 856/1000 
	 loss: 27.7326, MinusLogProbMetric: 27.7326, val_loss: 27.9713, val_MinusLogProbMetric: 27.9713

Epoch 856: val_loss did not improve from 27.95050
196/196 - 36s - loss: 27.7326 - MinusLogProbMetric: 27.7326 - val_loss: 27.9713 - val_MinusLogProbMetric: 27.9713 - lr: 6.2500e-05 - 36s/epoch - 183ms/step
Epoch 857/1000
2023-10-26 06:46:54.653 
Epoch 857/1000 
	 loss: 27.7160, MinusLogProbMetric: 27.7160, val_loss: 27.9745, val_MinusLogProbMetric: 27.9745

Epoch 857: val_loss did not improve from 27.95050
196/196 - 36s - loss: 27.7160 - MinusLogProbMetric: 27.7160 - val_loss: 27.9745 - val_MinusLogProbMetric: 27.9745 - lr: 6.2500e-05 - 36s/epoch - 183ms/step
Epoch 858/1000
2023-10-26 06:47:30.791 
Epoch 858/1000 
	 loss: 27.7446, MinusLogProbMetric: 27.7446, val_loss: 28.0411, val_MinusLogProbMetric: 28.0411

Epoch 858: val_loss did not improve from 27.95050
196/196 - 36s - loss: 27.7446 - MinusLogProbMetric: 27.7446 - val_loss: 28.0411 - val_MinusLogProbMetric: 28.0411 - lr: 6.2500e-05 - 36s/epoch - 184ms/step
Epoch 859/1000
2023-10-26 06:48:06.878 
Epoch 859/1000 
	 loss: 27.7113, MinusLogProbMetric: 27.7113, val_loss: 27.9595, val_MinusLogProbMetric: 27.9595

Epoch 859: val_loss did not improve from 27.95050
196/196 - 36s - loss: 27.7113 - MinusLogProbMetric: 27.7113 - val_loss: 27.9595 - val_MinusLogProbMetric: 27.9595 - lr: 6.2500e-05 - 36s/epoch - 184ms/step
Epoch 860/1000
2023-10-26 06:48:42.406 
Epoch 860/1000 
	 loss: 27.7378, MinusLogProbMetric: 27.7378, val_loss: 27.9589, val_MinusLogProbMetric: 27.9589

Epoch 860: val_loss did not improve from 27.95050
196/196 - 36s - loss: 27.7378 - MinusLogProbMetric: 27.7378 - val_loss: 27.9589 - val_MinusLogProbMetric: 27.9589 - lr: 6.2500e-05 - 36s/epoch - 181ms/step
Epoch 861/1000
2023-10-26 06:49:18.062 
Epoch 861/1000 
	 loss: 27.7152, MinusLogProbMetric: 27.7152, val_loss: 27.9921, val_MinusLogProbMetric: 27.9921

Epoch 861: val_loss did not improve from 27.95050
196/196 - 36s - loss: 27.7152 - MinusLogProbMetric: 27.7152 - val_loss: 27.9921 - val_MinusLogProbMetric: 27.9921 - lr: 6.2500e-05 - 36s/epoch - 182ms/step
Epoch 862/1000
2023-10-26 06:49:53.756 
Epoch 862/1000 
	 loss: 27.7234, MinusLogProbMetric: 27.7234, val_loss: 28.0562, val_MinusLogProbMetric: 28.0562

Epoch 862: val_loss did not improve from 27.95050
196/196 - 36s - loss: 27.7234 - MinusLogProbMetric: 27.7234 - val_loss: 28.0562 - val_MinusLogProbMetric: 28.0562 - lr: 6.2500e-05 - 36s/epoch - 182ms/step
Epoch 863/1000
2023-10-26 06:50:29.390 
Epoch 863/1000 
	 loss: 27.7154, MinusLogProbMetric: 27.7154, val_loss: 27.9702, val_MinusLogProbMetric: 27.9702

Epoch 863: val_loss did not improve from 27.95050
196/196 - 36s - loss: 27.7154 - MinusLogProbMetric: 27.7154 - val_loss: 27.9702 - val_MinusLogProbMetric: 27.9702 - lr: 6.2500e-05 - 36s/epoch - 182ms/step
Epoch 864/1000
2023-10-26 06:51:05.909 
Epoch 864/1000 
	 loss: 27.7344, MinusLogProbMetric: 27.7344, val_loss: 27.9683, val_MinusLogProbMetric: 27.9683

Epoch 864: val_loss did not improve from 27.95050
196/196 - 37s - loss: 27.7344 - MinusLogProbMetric: 27.7344 - val_loss: 27.9683 - val_MinusLogProbMetric: 27.9683 - lr: 6.2500e-05 - 37s/epoch - 186ms/step
Epoch 865/1000
2023-10-26 06:51:41.026 
Epoch 865/1000 
	 loss: 27.7075, MinusLogProbMetric: 27.7075, val_loss: 27.9725, val_MinusLogProbMetric: 27.9725

Epoch 865: val_loss did not improve from 27.95050
196/196 - 35s - loss: 27.7075 - MinusLogProbMetric: 27.7075 - val_loss: 27.9725 - val_MinusLogProbMetric: 27.9725 - lr: 6.2500e-05 - 35s/epoch - 179ms/step
Epoch 866/1000
2023-10-26 06:52:13.960 
Epoch 866/1000 
	 loss: 27.7249, MinusLogProbMetric: 27.7249, val_loss: 28.0484, val_MinusLogProbMetric: 28.0484

Epoch 866: val_loss did not improve from 27.95050
196/196 - 33s - loss: 27.7249 - MinusLogProbMetric: 27.7249 - val_loss: 28.0484 - val_MinusLogProbMetric: 28.0484 - lr: 6.2500e-05 - 33s/epoch - 168ms/step
Epoch 867/1000
2023-10-26 06:52:48.115 
Epoch 867/1000 
	 loss: 27.7313, MinusLogProbMetric: 27.7313, val_loss: 28.0037, val_MinusLogProbMetric: 28.0037

Epoch 867: val_loss did not improve from 27.95050
196/196 - 34s - loss: 27.7313 - MinusLogProbMetric: 27.7313 - val_loss: 28.0037 - val_MinusLogProbMetric: 28.0037 - lr: 6.2500e-05 - 34s/epoch - 174ms/step
Epoch 868/1000
2023-10-26 06:53:22.515 
Epoch 868/1000 
	 loss: 27.7065, MinusLogProbMetric: 27.7065, val_loss: 27.9819, val_MinusLogProbMetric: 27.9819

Epoch 868: val_loss did not improve from 27.95050
196/196 - 34s - loss: 27.7065 - MinusLogProbMetric: 27.7065 - val_loss: 27.9819 - val_MinusLogProbMetric: 27.9819 - lr: 6.2500e-05 - 34s/epoch - 175ms/step
Epoch 869/1000
2023-10-26 06:53:57.198 
Epoch 869/1000 
	 loss: 27.7111, MinusLogProbMetric: 27.7111, val_loss: 27.9618, val_MinusLogProbMetric: 27.9618

Epoch 869: val_loss did not improve from 27.95050
196/196 - 35s - loss: 27.7111 - MinusLogProbMetric: 27.7111 - val_loss: 27.9618 - val_MinusLogProbMetric: 27.9618 - lr: 6.2500e-05 - 35s/epoch - 177ms/step
Epoch 870/1000
2023-10-26 06:54:32.447 
Epoch 870/1000 
	 loss: 27.7237, MinusLogProbMetric: 27.7237, val_loss: 28.0145, val_MinusLogProbMetric: 28.0145

Epoch 870: val_loss did not improve from 27.95050
196/196 - 35s - loss: 27.7237 - MinusLogProbMetric: 27.7237 - val_loss: 28.0145 - val_MinusLogProbMetric: 28.0145 - lr: 6.2500e-05 - 35s/epoch - 180ms/step
Epoch 871/1000
2023-10-26 06:55:08.212 
Epoch 871/1000 
	 loss: 27.7300, MinusLogProbMetric: 27.7300, val_loss: 27.9711, val_MinusLogProbMetric: 27.9711

Epoch 871: val_loss did not improve from 27.95050
196/196 - 36s - loss: 27.7300 - MinusLogProbMetric: 27.7300 - val_loss: 27.9711 - val_MinusLogProbMetric: 27.9711 - lr: 6.2500e-05 - 36s/epoch - 182ms/step
Epoch 872/1000
2023-10-26 06:55:40.440 
Epoch 872/1000 
	 loss: 27.7038, MinusLogProbMetric: 27.7038, val_loss: 28.0136, val_MinusLogProbMetric: 28.0136

Epoch 872: val_loss did not improve from 27.95050
196/196 - 32s - loss: 27.7038 - MinusLogProbMetric: 27.7038 - val_loss: 28.0136 - val_MinusLogProbMetric: 28.0136 - lr: 6.2500e-05 - 32s/epoch - 164ms/step
Epoch 873/1000
2023-10-26 06:56:14.860 
Epoch 873/1000 
	 loss: 27.7022, MinusLogProbMetric: 27.7022, val_loss: 27.9692, val_MinusLogProbMetric: 27.9692

Epoch 873: val_loss did not improve from 27.95050
196/196 - 34s - loss: 27.7022 - MinusLogProbMetric: 27.7022 - val_loss: 27.9692 - val_MinusLogProbMetric: 27.9692 - lr: 6.2500e-05 - 34s/epoch - 176ms/step
Epoch 874/1000
2023-10-26 06:56:49.175 
Epoch 874/1000 
	 loss: 27.6988, MinusLogProbMetric: 27.6988, val_loss: 28.0197, val_MinusLogProbMetric: 28.0197

Epoch 874: val_loss did not improve from 27.95050
196/196 - 34s - loss: 27.6988 - MinusLogProbMetric: 27.6988 - val_loss: 28.0197 - val_MinusLogProbMetric: 28.0197 - lr: 6.2500e-05 - 34s/epoch - 175ms/step
Epoch 875/1000
2023-10-26 06:57:22.302 
Epoch 875/1000 
	 loss: 27.7436, MinusLogProbMetric: 27.7436, val_loss: 27.9577, val_MinusLogProbMetric: 27.9577

Epoch 875: val_loss did not improve from 27.95050
196/196 - 33s - loss: 27.7436 - MinusLogProbMetric: 27.7436 - val_loss: 27.9577 - val_MinusLogProbMetric: 27.9577 - lr: 6.2500e-05 - 33s/epoch - 169ms/step
Epoch 876/1000
2023-10-26 06:57:53.866 
Epoch 876/1000 
	 loss: 27.7091, MinusLogProbMetric: 27.7091, val_loss: 28.0054, val_MinusLogProbMetric: 28.0054

Epoch 876: val_loss did not improve from 27.95050
196/196 - 32s - loss: 27.7091 - MinusLogProbMetric: 27.7091 - val_loss: 28.0054 - val_MinusLogProbMetric: 28.0054 - lr: 6.2500e-05 - 32s/epoch - 161ms/step
Epoch 877/1000
2023-10-26 06:58:28.367 
Epoch 877/1000 
	 loss: 27.7266, MinusLogProbMetric: 27.7266, val_loss: 28.0926, val_MinusLogProbMetric: 28.0926

Epoch 877: val_loss did not improve from 27.95050
196/196 - 34s - loss: 27.7266 - MinusLogProbMetric: 27.7266 - val_loss: 28.0926 - val_MinusLogProbMetric: 28.0926 - lr: 6.2500e-05 - 34s/epoch - 176ms/step
Epoch 878/1000
2023-10-26 06:59:03.495 
Epoch 878/1000 
	 loss: 27.7097, MinusLogProbMetric: 27.7097, val_loss: 27.9999, val_MinusLogProbMetric: 27.9999

Epoch 878: val_loss did not improve from 27.95050
196/196 - 35s - loss: 27.7097 - MinusLogProbMetric: 27.7097 - val_loss: 27.9999 - val_MinusLogProbMetric: 27.9999 - lr: 6.2500e-05 - 35s/epoch - 179ms/step
Epoch 879/1000
2023-10-26 06:59:39.014 
Epoch 879/1000 
	 loss: 27.7184, MinusLogProbMetric: 27.7184, val_loss: 28.0278, val_MinusLogProbMetric: 28.0278

Epoch 879: val_loss did not improve from 27.95050
196/196 - 36s - loss: 27.7184 - MinusLogProbMetric: 27.7184 - val_loss: 28.0278 - val_MinusLogProbMetric: 28.0278 - lr: 6.2500e-05 - 36s/epoch - 181ms/step
Epoch 880/1000
2023-10-26 07:00:14.189 
Epoch 880/1000 
	 loss: 27.7266, MinusLogProbMetric: 27.7266, val_loss: 28.0274, val_MinusLogProbMetric: 28.0274

Epoch 880: val_loss did not improve from 27.95050
196/196 - 35s - loss: 27.7266 - MinusLogProbMetric: 27.7266 - val_loss: 28.0274 - val_MinusLogProbMetric: 28.0274 - lr: 6.2500e-05 - 35s/epoch - 179ms/step
Epoch 881/1000
2023-10-26 07:00:49.397 
Epoch 881/1000 
	 loss: 27.7201, MinusLogProbMetric: 27.7201, val_loss: 27.9946, val_MinusLogProbMetric: 27.9946

Epoch 881: val_loss did not improve from 27.95050
196/196 - 35s - loss: 27.7201 - MinusLogProbMetric: 27.7201 - val_loss: 27.9946 - val_MinusLogProbMetric: 27.9946 - lr: 6.2500e-05 - 35s/epoch - 180ms/step
Epoch 882/1000
2023-10-26 07:01:24.771 
Epoch 882/1000 
	 loss: 27.7134, MinusLogProbMetric: 27.7134, val_loss: 27.9794, val_MinusLogProbMetric: 27.9794

Epoch 882: val_loss did not improve from 27.95050
196/196 - 35s - loss: 27.7134 - MinusLogProbMetric: 27.7134 - val_loss: 27.9794 - val_MinusLogProbMetric: 27.9794 - lr: 6.2500e-05 - 35s/epoch - 180ms/step
Epoch 883/1000
2023-10-26 07:02:00.018 
Epoch 883/1000 
	 loss: 27.7062, MinusLogProbMetric: 27.7062, val_loss: 28.0176, val_MinusLogProbMetric: 28.0176

Epoch 883: val_loss did not improve from 27.95050
196/196 - 35s - loss: 27.7062 - MinusLogProbMetric: 27.7062 - val_loss: 28.0176 - val_MinusLogProbMetric: 28.0176 - lr: 6.2500e-05 - 35s/epoch - 180ms/step
Epoch 884/1000
2023-10-26 07:02:35.204 
Epoch 884/1000 
	 loss: 27.7226, MinusLogProbMetric: 27.7226, val_loss: 28.0953, val_MinusLogProbMetric: 28.0953

Epoch 884: val_loss did not improve from 27.95050
196/196 - 35s - loss: 27.7226 - MinusLogProbMetric: 27.7226 - val_loss: 28.0953 - val_MinusLogProbMetric: 28.0953 - lr: 6.2500e-05 - 35s/epoch - 180ms/step
Epoch 885/1000
2023-10-26 07:03:10.504 
Epoch 885/1000 
	 loss: 27.7088, MinusLogProbMetric: 27.7088, val_loss: 28.0745, val_MinusLogProbMetric: 28.0745

Epoch 885: val_loss did not improve from 27.95050
196/196 - 35s - loss: 27.7088 - MinusLogProbMetric: 27.7088 - val_loss: 28.0745 - val_MinusLogProbMetric: 28.0745 - lr: 6.2500e-05 - 35s/epoch - 180ms/step
Epoch 886/1000
2023-10-26 07:03:45.386 
Epoch 886/1000 
	 loss: 27.7530, MinusLogProbMetric: 27.7530, val_loss: 28.0605, val_MinusLogProbMetric: 28.0605

Epoch 886: val_loss did not improve from 27.95050
196/196 - 35s - loss: 27.7530 - MinusLogProbMetric: 27.7530 - val_loss: 28.0605 - val_MinusLogProbMetric: 28.0605 - lr: 6.2500e-05 - 35s/epoch - 178ms/step
Epoch 887/1000
2023-10-26 07:04:21.000 
Epoch 887/1000 
	 loss: 27.7219, MinusLogProbMetric: 27.7219, val_loss: 27.9790, val_MinusLogProbMetric: 27.9790

Epoch 887: val_loss did not improve from 27.95050
196/196 - 36s - loss: 27.7219 - MinusLogProbMetric: 27.7219 - val_loss: 27.9790 - val_MinusLogProbMetric: 27.9790 - lr: 6.2500e-05 - 36s/epoch - 182ms/step
Epoch 888/1000
2023-10-26 07:04:56.479 
Epoch 888/1000 
	 loss: 27.7096, MinusLogProbMetric: 27.7096, val_loss: 28.0044, val_MinusLogProbMetric: 28.0044

Epoch 888: val_loss did not improve from 27.95050
196/196 - 35s - loss: 27.7096 - MinusLogProbMetric: 27.7096 - val_loss: 28.0044 - val_MinusLogProbMetric: 28.0044 - lr: 6.2500e-05 - 35s/epoch - 181ms/step
Epoch 889/1000
2023-10-26 07:05:31.568 
Epoch 889/1000 
	 loss: 27.7157, MinusLogProbMetric: 27.7157, val_loss: 28.1190, val_MinusLogProbMetric: 28.1190

Epoch 889: val_loss did not improve from 27.95050
196/196 - 35s - loss: 27.7157 - MinusLogProbMetric: 27.7157 - val_loss: 28.1190 - val_MinusLogProbMetric: 28.1190 - lr: 6.2500e-05 - 35s/epoch - 179ms/step
Epoch 890/1000
2023-10-26 07:06:06.612 
Epoch 890/1000 
	 loss: 27.7347, MinusLogProbMetric: 27.7347, val_loss: 27.9967, val_MinusLogProbMetric: 27.9967

Epoch 890: val_loss did not improve from 27.95050
196/196 - 35s - loss: 27.7347 - MinusLogProbMetric: 27.7347 - val_loss: 27.9967 - val_MinusLogProbMetric: 27.9967 - lr: 6.2500e-05 - 35s/epoch - 179ms/step
Epoch 891/1000
2023-10-26 07:06:41.735 
Epoch 891/1000 
	 loss: 27.7087, MinusLogProbMetric: 27.7087, val_loss: 28.0168, val_MinusLogProbMetric: 28.0168

Epoch 891: val_loss did not improve from 27.95050
196/196 - 35s - loss: 27.7087 - MinusLogProbMetric: 27.7087 - val_loss: 28.0168 - val_MinusLogProbMetric: 28.0168 - lr: 6.2500e-05 - 35s/epoch - 179ms/step
Epoch 892/1000
2023-10-26 07:07:17.200 
Epoch 892/1000 
	 loss: 27.7276, MinusLogProbMetric: 27.7276, val_loss: 28.0126, val_MinusLogProbMetric: 28.0126

Epoch 892: val_loss did not improve from 27.95050
196/196 - 35s - loss: 27.7276 - MinusLogProbMetric: 27.7276 - val_loss: 28.0126 - val_MinusLogProbMetric: 28.0126 - lr: 6.2500e-05 - 35s/epoch - 181ms/step
Epoch 893/1000
2023-10-26 07:07:52.424 
Epoch 893/1000 
	 loss: 27.7283, MinusLogProbMetric: 27.7283, val_loss: 28.0645, val_MinusLogProbMetric: 28.0645

Epoch 893: val_loss did not improve from 27.95050
196/196 - 35s - loss: 27.7283 - MinusLogProbMetric: 27.7283 - val_loss: 28.0645 - val_MinusLogProbMetric: 28.0645 - lr: 6.2500e-05 - 35s/epoch - 180ms/step
Epoch 894/1000
2023-10-26 07:08:27.735 
Epoch 894/1000 
	 loss: 27.7131, MinusLogProbMetric: 27.7131, val_loss: 28.0283, val_MinusLogProbMetric: 28.0283

Epoch 894: val_loss did not improve from 27.95050
196/196 - 35s - loss: 27.7131 - MinusLogProbMetric: 27.7131 - val_loss: 28.0283 - val_MinusLogProbMetric: 28.0283 - lr: 6.2500e-05 - 35s/epoch - 180ms/step
Epoch 895/1000
2023-10-26 07:09:03.012 
Epoch 895/1000 
	 loss: 27.7382, MinusLogProbMetric: 27.7382, val_loss: 28.1347, val_MinusLogProbMetric: 28.1347

Epoch 895: val_loss did not improve from 27.95050
196/196 - 35s - loss: 27.7382 - MinusLogProbMetric: 27.7382 - val_loss: 28.1347 - val_MinusLogProbMetric: 28.1347 - lr: 6.2500e-05 - 35s/epoch - 180ms/step
Epoch 896/1000
2023-10-26 07:09:37.779 
Epoch 896/1000 
	 loss: 27.7366, MinusLogProbMetric: 27.7366, val_loss: 28.0318, val_MinusLogProbMetric: 28.0318

Epoch 896: val_loss did not improve from 27.95050
196/196 - 35s - loss: 27.7366 - MinusLogProbMetric: 27.7366 - val_loss: 28.0318 - val_MinusLogProbMetric: 28.0318 - lr: 6.2500e-05 - 35s/epoch - 177ms/step
Epoch 897/1000
2023-10-26 07:10:13.086 
Epoch 897/1000 
	 loss: 27.7422, MinusLogProbMetric: 27.7422, val_loss: 28.0491, val_MinusLogProbMetric: 28.0491

Epoch 897: val_loss did not improve from 27.95050
196/196 - 35s - loss: 27.7422 - MinusLogProbMetric: 27.7422 - val_loss: 28.0491 - val_MinusLogProbMetric: 28.0491 - lr: 6.2500e-05 - 35s/epoch - 180ms/step
Epoch 898/1000
2023-10-26 07:10:48.219 
Epoch 898/1000 
	 loss: 27.7461, MinusLogProbMetric: 27.7461, val_loss: 28.1045, val_MinusLogProbMetric: 28.1045

Epoch 898: val_loss did not improve from 27.95050
196/196 - 35s - loss: 27.7461 - MinusLogProbMetric: 27.7461 - val_loss: 28.1045 - val_MinusLogProbMetric: 28.1045 - lr: 6.2500e-05 - 35s/epoch - 179ms/step
Epoch 899/1000
2023-10-26 07:11:23.194 
Epoch 899/1000 
	 loss: 27.7385, MinusLogProbMetric: 27.7385, val_loss: 27.9814, val_MinusLogProbMetric: 27.9814

Epoch 899: val_loss did not improve from 27.95050
196/196 - 35s - loss: 27.7385 - MinusLogProbMetric: 27.7385 - val_loss: 27.9814 - val_MinusLogProbMetric: 27.9814 - lr: 6.2500e-05 - 35s/epoch - 178ms/step
Epoch 900/1000
2023-10-26 07:11:58.040 
Epoch 900/1000 
	 loss: 27.7389, MinusLogProbMetric: 27.7389, val_loss: 27.9714, val_MinusLogProbMetric: 27.9714

Epoch 900: val_loss did not improve from 27.95050
196/196 - 35s - loss: 27.7389 - MinusLogProbMetric: 27.7389 - val_loss: 27.9714 - val_MinusLogProbMetric: 27.9714 - lr: 6.2500e-05 - 35s/epoch - 178ms/step
Epoch 901/1000
2023-10-26 07:12:33.154 
Epoch 901/1000 
	 loss: 27.7224, MinusLogProbMetric: 27.7224, val_loss: 27.9812, val_MinusLogProbMetric: 27.9812

Epoch 901: val_loss did not improve from 27.95050
196/196 - 35s - loss: 27.7224 - MinusLogProbMetric: 27.7224 - val_loss: 27.9812 - val_MinusLogProbMetric: 27.9812 - lr: 6.2500e-05 - 35s/epoch - 179ms/step
Epoch 902/1000
2023-10-26 07:13:08.272 
Epoch 902/1000 
	 loss: 27.7373, MinusLogProbMetric: 27.7373, val_loss: 28.1523, val_MinusLogProbMetric: 28.1523

Epoch 902: val_loss did not improve from 27.95050
196/196 - 35s - loss: 27.7373 - MinusLogProbMetric: 27.7373 - val_loss: 28.1523 - val_MinusLogProbMetric: 28.1523 - lr: 6.2500e-05 - 35s/epoch - 179ms/step
Epoch 903/1000
2023-10-26 07:13:43.102 
Epoch 903/1000 
	 loss: 27.7228, MinusLogProbMetric: 27.7228, val_loss: 27.9835, val_MinusLogProbMetric: 27.9835

Epoch 903: val_loss did not improve from 27.95050
196/196 - 35s - loss: 27.7228 - MinusLogProbMetric: 27.7228 - val_loss: 27.9835 - val_MinusLogProbMetric: 27.9835 - lr: 6.2500e-05 - 35s/epoch - 178ms/step
Epoch 904/1000
2023-10-26 07:14:18.135 
Epoch 904/1000 
	 loss: 27.6825, MinusLogProbMetric: 27.6825, val_loss: 27.9463, val_MinusLogProbMetric: 27.9463

Epoch 904: val_loss improved from 27.95050 to 27.94629, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_369/weights/best_weights.h5
196/196 - 36s - loss: 27.6825 - MinusLogProbMetric: 27.6825 - val_loss: 27.9463 - val_MinusLogProbMetric: 27.9463 - lr: 3.1250e-05 - 36s/epoch - 181ms/step
Epoch 905/1000
2023-10-26 07:14:53.551 
Epoch 905/1000 
	 loss: 27.6862, MinusLogProbMetric: 27.6862, val_loss: 27.9474, val_MinusLogProbMetric: 27.9474

Epoch 905: val_loss did not improve from 27.94629
196/196 - 35s - loss: 27.6862 - MinusLogProbMetric: 27.6862 - val_loss: 27.9474 - val_MinusLogProbMetric: 27.9474 - lr: 3.1250e-05 - 35s/epoch - 178ms/step
Epoch 906/1000
2023-10-26 07:15:28.698 
Epoch 906/1000 
	 loss: 27.6828, MinusLogProbMetric: 27.6828, val_loss: 27.9630, val_MinusLogProbMetric: 27.9630

Epoch 906: val_loss did not improve from 27.94629
196/196 - 35s - loss: 27.6828 - MinusLogProbMetric: 27.6828 - val_loss: 27.9630 - val_MinusLogProbMetric: 27.9630 - lr: 3.1250e-05 - 35s/epoch - 179ms/step
Epoch 907/1000
2023-10-26 07:16:03.262 
Epoch 907/1000 
	 loss: 27.6810, MinusLogProbMetric: 27.6810, val_loss: 28.0349, val_MinusLogProbMetric: 28.0349

Epoch 907: val_loss did not improve from 27.94629
196/196 - 35s - loss: 27.6810 - MinusLogProbMetric: 27.6810 - val_loss: 28.0349 - val_MinusLogProbMetric: 28.0349 - lr: 3.1250e-05 - 35s/epoch - 176ms/step
Epoch 908/1000
2023-10-26 07:16:38.304 
Epoch 908/1000 
	 loss: 27.6858, MinusLogProbMetric: 27.6858, val_loss: 27.9595, val_MinusLogProbMetric: 27.9595

Epoch 908: val_loss did not improve from 27.94629
196/196 - 35s - loss: 27.6858 - MinusLogProbMetric: 27.6858 - val_loss: 27.9595 - val_MinusLogProbMetric: 27.9595 - lr: 3.1250e-05 - 35s/epoch - 179ms/step
Epoch 909/1000
2023-10-26 07:17:13.345 
Epoch 909/1000 
	 loss: 27.6820, MinusLogProbMetric: 27.6820, val_loss: 27.9471, val_MinusLogProbMetric: 27.9471

Epoch 909: val_loss did not improve from 27.94629
196/196 - 35s - loss: 27.6820 - MinusLogProbMetric: 27.6820 - val_loss: 27.9471 - val_MinusLogProbMetric: 27.9471 - lr: 3.1250e-05 - 35s/epoch - 179ms/step
Epoch 910/1000
2023-10-26 07:17:48.316 
Epoch 910/1000 
	 loss: 27.6781, MinusLogProbMetric: 27.6781, val_loss: 27.9372, val_MinusLogProbMetric: 27.9372

Epoch 910: val_loss improved from 27.94629 to 27.93724, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_369/weights/best_weights.h5
196/196 - 36s - loss: 27.6781 - MinusLogProbMetric: 27.6781 - val_loss: 27.9372 - val_MinusLogProbMetric: 27.9372 - lr: 3.1250e-05 - 36s/epoch - 181ms/step
Epoch 911/1000
2023-10-26 07:18:23.828 
Epoch 911/1000 
	 loss: 27.6787, MinusLogProbMetric: 27.6787, val_loss: 27.9717, val_MinusLogProbMetric: 27.9717

Epoch 911: val_loss did not improve from 27.93724
196/196 - 35s - loss: 27.6787 - MinusLogProbMetric: 27.6787 - val_loss: 27.9717 - val_MinusLogProbMetric: 27.9717 - lr: 3.1250e-05 - 35s/epoch - 178ms/step
Epoch 912/1000
2023-10-26 07:18:58.998 
Epoch 912/1000 
	 loss: 27.6780, MinusLogProbMetric: 27.6780, val_loss: 27.9709, val_MinusLogProbMetric: 27.9709

Epoch 912: val_loss did not improve from 27.93724
196/196 - 35s - loss: 27.6780 - MinusLogProbMetric: 27.6780 - val_loss: 27.9709 - val_MinusLogProbMetric: 27.9709 - lr: 3.1250e-05 - 35s/epoch - 179ms/step
Epoch 913/1000
2023-10-26 07:19:34.413 
Epoch 913/1000 
	 loss: 27.6781, MinusLogProbMetric: 27.6781, val_loss: 27.9903, val_MinusLogProbMetric: 27.9903

Epoch 913: val_loss did not improve from 27.93724
196/196 - 35s - loss: 27.6781 - MinusLogProbMetric: 27.6781 - val_loss: 27.9903 - val_MinusLogProbMetric: 27.9903 - lr: 3.1250e-05 - 35s/epoch - 181ms/step
Epoch 914/1000
2023-10-26 07:20:09.631 
Epoch 914/1000 
	 loss: 27.6823, MinusLogProbMetric: 27.6823, val_loss: 27.9694, val_MinusLogProbMetric: 27.9694

Epoch 914: val_loss did not improve from 27.93724
196/196 - 35s - loss: 27.6823 - MinusLogProbMetric: 27.6823 - val_loss: 27.9694 - val_MinusLogProbMetric: 27.9694 - lr: 3.1250e-05 - 35s/epoch - 180ms/step
Epoch 915/1000
2023-10-26 07:20:44.684 
Epoch 915/1000 
	 loss: 27.6809, MinusLogProbMetric: 27.6809, val_loss: 27.9795, val_MinusLogProbMetric: 27.9795

Epoch 915: val_loss did not improve from 27.93724
196/196 - 35s - loss: 27.6809 - MinusLogProbMetric: 27.6809 - val_loss: 27.9795 - val_MinusLogProbMetric: 27.9795 - lr: 3.1250e-05 - 35s/epoch - 179ms/step
Epoch 916/1000
2023-10-26 07:21:19.829 
Epoch 916/1000 
	 loss: 27.6800, MinusLogProbMetric: 27.6800, val_loss: 27.9526, val_MinusLogProbMetric: 27.9526

Epoch 916: val_loss did not improve from 27.93724
196/196 - 35s - loss: 27.6800 - MinusLogProbMetric: 27.6800 - val_loss: 27.9526 - val_MinusLogProbMetric: 27.9526 - lr: 3.1250e-05 - 35s/epoch - 179ms/step
Epoch 917/1000
2023-10-26 07:21:54.869 
Epoch 917/1000 
	 loss: 27.6896, MinusLogProbMetric: 27.6896, val_loss: 27.9592, val_MinusLogProbMetric: 27.9592

Epoch 917: val_loss did not improve from 27.93724
196/196 - 35s - loss: 27.6896 - MinusLogProbMetric: 27.6896 - val_loss: 27.9592 - val_MinusLogProbMetric: 27.9592 - lr: 3.1250e-05 - 35s/epoch - 179ms/step
Epoch 918/1000
2023-10-26 07:22:29.910 
Epoch 918/1000 
	 loss: 27.6968, MinusLogProbMetric: 27.6968, val_loss: 27.9617, val_MinusLogProbMetric: 27.9617

Epoch 918: val_loss did not improve from 27.93724
196/196 - 35s - loss: 27.6968 - MinusLogProbMetric: 27.6968 - val_loss: 27.9617 - val_MinusLogProbMetric: 27.9617 - lr: 3.1250e-05 - 35s/epoch - 179ms/step
Epoch 919/1000
2023-10-26 07:23:05.122 
Epoch 919/1000 
	 loss: 27.6894, MinusLogProbMetric: 27.6894, val_loss: 27.9448, val_MinusLogProbMetric: 27.9448

Epoch 919: val_loss did not improve from 27.93724
196/196 - 35s - loss: 27.6894 - MinusLogProbMetric: 27.6894 - val_loss: 27.9448 - val_MinusLogProbMetric: 27.9448 - lr: 3.1250e-05 - 35s/epoch - 180ms/step
Epoch 920/1000
2023-10-26 07:23:40.066 
Epoch 920/1000 
	 loss: 27.6836, MinusLogProbMetric: 27.6836, val_loss: 27.9761, val_MinusLogProbMetric: 27.9761

Epoch 920: val_loss did not improve from 27.93724
196/196 - 35s - loss: 27.6836 - MinusLogProbMetric: 27.6836 - val_loss: 27.9761 - val_MinusLogProbMetric: 27.9761 - lr: 3.1250e-05 - 35s/epoch - 178ms/step
Epoch 921/1000
2023-10-26 07:24:14.785 
Epoch 921/1000 
	 loss: 27.6835, MinusLogProbMetric: 27.6835, val_loss: 27.9640, val_MinusLogProbMetric: 27.9640

Epoch 921: val_loss did not improve from 27.93724
196/196 - 35s - loss: 27.6835 - MinusLogProbMetric: 27.6835 - val_loss: 27.9640 - val_MinusLogProbMetric: 27.9640 - lr: 3.1250e-05 - 35s/epoch - 177ms/step
Epoch 922/1000
2023-10-26 07:24:49.794 
Epoch 922/1000 
	 loss: 27.6831, MinusLogProbMetric: 27.6831, val_loss: 27.9418, val_MinusLogProbMetric: 27.9418

Epoch 922: val_loss did not improve from 27.93724
196/196 - 35s - loss: 27.6831 - MinusLogProbMetric: 27.6831 - val_loss: 27.9418 - val_MinusLogProbMetric: 27.9418 - lr: 3.1250e-05 - 35s/epoch - 179ms/step
Epoch 923/1000
2023-10-26 07:25:24.670 
Epoch 923/1000 
	 loss: 27.6796, MinusLogProbMetric: 27.6796, val_loss: 27.9557, val_MinusLogProbMetric: 27.9557

Epoch 923: val_loss did not improve from 27.93724
196/196 - 35s - loss: 27.6796 - MinusLogProbMetric: 27.6796 - val_loss: 27.9557 - val_MinusLogProbMetric: 27.9557 - lr: 3.1250e-05 - 35s/epoch - 178ms/step
Epoch 924/1000
2023-10-26 07:25:59.848 
Epoch 924/1000 
	 loss: 27.6809, MinusLogProbMetric: 27.6809, val_loss: 27.9647, val_MinusLogProbMetric: 27.9647

Epoch 924: val_loss did not improve from 27.93724
196/196 - 35s - loss: 27.6809 - MinusLogProbMetric: 27.6809 - val_loss: 27.9647 - val_MinusLogProbMetric: 27.9647 - lr: 3.1250e-05 - 35s/epoch - 179ms/step
Epoch 925/1000
2023-10-26 07:26:34.707 
Epoch 925/1000 
	 loss: 27.6828, MinusLogProbMetric: 27.6828, val_loss: 27.9516, val_MinusLogProbMetric: 27.9516

Epoch 925: val_loss did not improve from 27.93724
196/196 - 35s - loss: 27.6828 - MinusLogProbMetric: 27.6828 - val_loss: 27.9516 - val_MinusLogProbMetric: 27.9516 - lr: 3.1250e-05 - 35s/epoch - 178ms/step
Epoch 926/1000
2023-10-26 07:27:09.708 
Epoch 926/1000 
	 loss: 27.6805, MinusLogProbMetric: 27.6805, val_loss: 27.9328, val_MinusLogProbMetric: 27.9328

Epoch 926: val_loss improved from 27.93724 to 27.93282, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_369/weights/best_weights.h5
196/196 - 36s - loss: 27.6805 - MinusLogProbMetric: 27.6805 - val_loss: 27.9328 - val_MinusLogProbMetric: 27.9328 - lr: 3.1250e-05 - 36s/epoch - 182ms/step
Epoch 927/1000
2023-10-26 07:27:45.341 
Epoch 927/1000 
	 loss: 27.6764, MinusLogProbMetric: 27.6764, val_loss: 27.9931, val_MinusLogProbMetric: 27.9931

Epoch 927: val_loss did not improve from 27.93282
196/196 - 35s - loss: 27.6764 - MinusLogProbMetric: 27.6764 - val_loss: 27.9931 - val_MinusLogProbMetric: 27.9931 - lr: 3.1250e-05 - 35s/epoch - 179ms/step
Epoch 928/1000
2023-10-26 07:28:20.300 
Epoch 928/1000 
	 loss: 27.6840, MinusLogProbMetric: 27.6840, val_loss: 27.9373, val_MinusLogProbMetric: 27.9373

Epoch 928: val_loss did not improve from 27.93282
196/196 - 35s - loss: 27.6840 - MinusLogProbMetric: 27.6840 - val_loss: 27.9373 - val_MinusLogProbMetric: 27.9373 - lr: 3.1250e-05 - 35s/epoch - 178ms/step
Epoch 929/1000
2023-10-26 07:28:55.493 
Epoch 929/1000 
	 loss: 27.6822, MinusLogProbMetric: 27.6822, val_loss: 27.9692, val_MinusLogProbMetric: 27.9692

Epoch 929: val_loss did not improve from 27.93282
196/196 - 35s - loss: 27.6822 - MinusLogProbMetric: 27.6822 - val_loss: 27.9692 - val_MinusLogProbMetric: 27.9692 - lr: 3.1250e-05 - 35s/epoch - 180ms/step
Epoch 930/1000
2023-10-26 07:29:30.440 
Epoch 930/1000 
	 loss: 27.6793, MinusLogProbMetric: 27.6793, val_loss: 27.9651, val_MinusLogProbMetric: 27.9651

Epoch 930: val_loss did not improve from 27.93282
196/196 - 35s - loss: 27.6793 - MinusLogProbMetric: 27.6793 - val_loss: 27.9651 - val_MinusLogProbMetric: 27.9651 - lr: 3.1250e-05 - 35s/epoch - 178ms/step
Epoch 931/1000
2023-10-26 07:30:05.335 
Epoch 931/1000 
	 loss: 27.6786, MinusLogProbMetric: 27.6786, val_loss: 27.9574, val_MinusLogProbMetric: 27.9574

Epoch 931: val_loss did not improve from 27.93282
196/196 - 35s - loss: 27.6786 - MinusLogProbMetric: 27.6786 - val_loss: 27.9574 - val_MinusLogProbMetric: 27.9574 - lr: 3.1250e-05 - 35s/epoch - 178ms/step
Epoch 932/1000
2023-10-26 07:30:39.745 
Epoch 932/1000 
	 loss: 27.6829, MinusLogProbMetric: 27.6829, val_loss: 27.9541, val_MinusLogProbMetric: 27.9541

Epoch 932: val_loss did not improve from 27.93282
196/196 - 34s - loss: 27.6829 - MinusLogProbMetric: 27.6829 - val_loss: 27.9541 - val_MinusLogProbMetric: 27.9541 - lr: 3.1250e-05 - 34s/epoch - 176ms/step
Epoch 933/1000
2023-10-26 07:31:14.373 
Epoch 933/1000 
	 loss: 27.6790, MinusLogProbMetric: 27.6790, val_loss: 27.9361, val_MinusLogProbMetric: 27.9361

Epoch 933: val_loss did not improve from 27.93282
196/196 - 35s - loss: 27.6790 - MinusLogProbMetric: 27.6790 - val_loss: 27.9361 - val_MinusLogProbMetric: 27.9361 - lr: 3.1250e-05 - 35s/epoch - 177ms/step
Epoch 934/1000
2023-10-26 07:31:47.156 
Epoch 934/1000 
	 loss: 27.6758, MinusLogProbMetric: 27.6758, val_loss: 27.9356, val_MinusLogProbMetric: 27.9356

Epoch 934: val_loss did not improve from 27.93282
196/196 - 33s - loss: 27.6758 - MinusLogProbMetric: 27.6758 - val_loss: 27.9356 - val_MinusLogProbMetric: 27.9356 - lr: 3.1250e-05 - 33s/epoch - 167ms/step
Epoch 935/1000
2023-10-26 07:32:21.780 
Epoch 935/1000 
	 loss: 27.6766, MinusLogProbMetric: 27.6766, val_loss: 27.9577, val_MinusLogProbMetric: 27.9577

Epoch 935: val_loss did not improve from 27.93282
196/196 - 35s - loss: 27.6766 - MinusLogProbMetric: 27.6766 - val_loss: 27.9577 - val_MinusLogProbMetric: 27.9577 - lr: 3.1250e-05 - 35s/epoch - 177ms/step
Epoch 936/1000
2023-10-26 07:32:57.298 
Epoch 936/1000 
	 loss: 27.6781, MinusLogProbMetric: 27.6781, val_loss: 27.9395, val_MinusLogProbMetric: 27.9395

Epoch 936: val_loss did not improve from 27.93282
196/196 - 36s - loss: 27.6781 - MinusLogProbMetric: 27.6781 - val_loss: 27.9395 - val_MinusLogProbMetric: 27.9395 - lr: 3.1250e-05 - 36s/epoch - 181ms/step
Epoch 937/1000
2023-10-26 07:33:32.726 
Epoch 937/1000 
	 loss: 27.6822, MinusLogProbMetric: 27.6822, val_loss: 27.9406, val_MinusLogProbMetric: 27.9406

Epoch 937: val_loss did not improve from 27.93282
196/196 - 35s - loss: 27.6822 - MinusLogProbMetric: 27.6822 - val_loss: 27.9406 - val_MinusLogProbMetric: 27.9406 - lr: 3.1250e-05 - 35s/epoch - 181ms/step
Epoch 938/1000
2023-10-26 07:34:07.741 
Epoch 938/1000 
	 loss: 27.6753, MinusLogProbMetric: 27.6753, val_loss: 27.9374, val_MinusLogProbMetric: 27.9374

Epoch 938: val_loss did not improve from 27.93282
196/196 - 35s - loss: 27.6753 - MinusLogProbMetric: 27.6753 - val_loss: 27.9374 - val_MinusLogProbMetric: 27.9374 - lr: 3.1250e-05 - 35s/epoch - 179ms/step
Epoch 939/1000
2023-10-26 07:34:43.232 
Epoch 939/1000 
	 loss: 27.6756, MinusLogProbMetric: 27.6756, val_loss: 27.9437, val_MinusLogProbMetric: 27.9437

Epoch 939: val_loss did not improve from 27.93282
196/196 - 35s - loss: 27.6756 - MinusLogProbMetric: 27.6756 - val_loss: 27.9437 - val_MinusLogProbMetric: 27.9437 - lr: 3.1250e-05 - 35s/epoch - 181ms/step
Epoch 940/1000
2023-10-26 07:35:18.334 
Epoch 940/1000 
	 loss: 27.6743, MinusLogProbMetric: 27.6743, val_loss: 27.9808, val_MinusLogProbMetric: 27.9808

Epoch 940: val_loss did not improve from 27.93282
196/196 - 35s - loss: 27.6743 - MinusLogProbMetric: 27.6743 - val_loss: 27.9808 - val_MinusLogProbMetric: 27.9808 - lr: 3.1250e-05 - 35s/epoch - 179ms/step
Epoch 941/1000
2023-10-26 07:35:53.330 
Epoch 941/1000 
	 loss: 27.6788, MinusLogProbMetric: 27.6788, val_loss: 27.9457, val_MinusLogProbMetric: 27.9457

Epoch 941: val_loss did not improve from 27.93282
196/196 - 35s - loss: 27.6788 - MinusLogProbMetric: 27.6788 - val_loss: 27.9457 - val_MinusLogProbMetric: 27.9457 - lr: 3.1250e-05 - 35s/epoch - 179ms/step
Epoch 942/1000
2023-10-26 07:36:28.862 
Epoch 942/1000 
	 loss: 27.6757, MinusLogProbMetric: 27.6757, val_loss: 27.9612, val_MinusLogProbMetric: 27.9612

Epoch 942: val_loss did not improve from 27.93282
196/196 - 36s - loss: 27.6757 - MinusLogProbMetric: 27.6757 - val_loss: 27.9612 - val_MinusLogProbMetric: 27.9612 - lr: 3.1250e-05 - 36s/epoch - 181ms/step
Epoch 943/1000
2023-10-26 07:37:04.406 
Epoch 943/1000 
	 loss: 27.6787, MinusLogProbMetric: 27.6787, val_loss: 27.9423, val_MinusLogProbMetric: 27.9423

Epoch 943: val_loss did not improve from 27.93282
196/196 - 36s - loss: 27.6787 - MinusLogProbMetric: 27.6787 - val_loss: 27.9423 - val_MinusLogProbMetric: 27.9423 - lr: 3.1250e-05 - 36s/epoch - 181ms/step
Epoch 944/1000
2023-10-26 07:37:39.700 
Epoch 944/1000 
	 loss: 27.6784, MinusLogProbMetric: 27.6784, val_loss: 27.9382, val_MinusLogProbMetric: 27.9382

Epoch 944: val_loss did not improve from 27.93282
196/196 - 35s - loss: 27.6784 - MinusLogProbMetric: 27.6784 - val_loss: 27.9382 - val_MinusLogProbMetric: 27.9382 - lr: 3.1250e-05 - 35s/epoch - 180ms/step
Epoch 945/1000
2023-10-26 07:38:14.640 
Epoch 945/1000 
	 loss: 27.6770, MinusLogProbMetric: 27.6770, val_loss: 27.9389, val_MinusLogProbMetric: 27.9389

Epoch 945: val_loss did not improve from 27.93282
196/196 - 35s - loss: 27.6770 - MinusLogProbMetric: 27.6770 - val_loss: 27.9389 - val_MinusLogProbMetric: 27.9389 - lr: 3.1250e-05 - 35s/epoch - 178ms/step
Epoch 946/1000
2023-10-26 07:38:49.850 
Epoch 946/1000 
	 loss: 27.6773, MinusLogProbMetric: 27.6773, val_loss: 27.9391, val_MinusLogProbMetric: 27.9391

Epoch 946: val_loss did not improve from 27.93282
196/196 - 35s - loss: 27.6773 - MinusLogProbMetric: 27.6773 - val_loss: 27.9391 - val_MinusLogProbMetric: 27.9391 - lr: 3.1250e-05 - 35s/epoch - 180ms/step
Epoch 947/1000
2023-10-26 07:39:24.968 
Epoch 947/1000 
	 loss: 27.6772, MinusLogProbMetric: 27.6772, val_loss: 27.9700, val_MinusLogProbMetric: 27.9700

Epoch 947: val_loss did not improve from 27.93282
196/196 - 35s - loss: 27.6772 - MinusLogProbMetric: 27.6772 - val_loss: 27.9700 - val_MinusLogProbMetric: 27.9700 - lr: 3.1250e-05 - 35s/epoch - 179ms/step
Epoch 948/1000
2023-10-26 07:40:00.639 
Epoch 948/1000 
	 loss: 27.6835, MinusLogProbMetric: 27.6835, val_loss: 27.9567, val_MinusLogProbMetric: 27.9567

Epoch 948: val_loss did not improve from 27.93282
196/196 - 36s - loss: 27.6835 - MinusLogProbMetric: 27.6835 - val_loss: 27.9567 - val_MinusLogProbMetric: 27.9567 - lr: 3.1250e-05 - 36s/epoch - 182ms/step
Epoch 949/1000
2023-10-26 07:40:35.819 
Epoch 949/1000 
	 loss: 27.6797, MinusLogProbMetric: 27.6797, val_loss: 27.9547, val_MinusLogProbMetric: 27.9547

Epoch 949: val_loss did not improve from 27.93282
196/196 - 35s - loss: 27.6797 - MinusLogProbMetric: 27.6797 - val_loss: 27.9547 - val_MinusLogProbMetric: 27.9547 - lr: 3.1250e-05 - 35s/epoch - 179ms/step
Epoch 950/1000
2023-10-26 07:41:11.040 
Epoch 950/1000 
	 loss: 27.6813, MinusLogProbMetric: 27.6813, val_loss: 27.9804, val_MinusLogProbMetric: 27.9804

Epoch 950: val_loss did not improve from 27.93282
196/196 - 35s - loss: 27.6813 - MinusLogProbMetric: 27.6813 - val_loss: 27.9804 - val_MinusLogProbMetric: 27.9804 - lr: 3.1250e-05 - 35s/epoch - 180ms/step
Epoch 951/1000
2023-10-26 07:41:46.787 
Epoch 951/1000 
	 loss: 27.6823, MinusLogProbMetric: 27.6823, val_loss: 27.9511, val_MinusLogProbMetric: 27.9511

Epoch 951: val_loss did not improve from 27.93282
196/196 - 36s - loss: 27.6823 - MinusLogProbMetric: 27.6823 - val_loss: 27.9511 - val_MinusLogProbMetric: 27.9511 - lr: 3.1250e-05 - 36s/epoch - 182ms/step
Epoch 952/1000
2023-10-26 07:42:22.024 
Epoch 952/1000 
	 loss: 27.6781, MinusLogProbMetric: 27.6781, val_loss: 27.9820, val_MinusLogProbMetric: 27.9820

Epoch 952: val_loss did not improve from 27.93282
196/196 - 35s - loss: 27.6781 - MinusLogProbMetric: 27.6781 - val_loss: 27.9820 - val_MinusLogProbMetric: 27.9820 - lr: 3.1250e-05 - 35s/epoch - 180ms/step
Epoch 953/1000
2023-10-26 07:42:57.485 
Epoch 953/1000 
	 loss: 27.6841, MinusLogProbMetric: 27.6841, val_loss: 27.9449, val_MinusLogProbMetric: 27.9449

Epoch 953: val_loss did not improve from 27.93282
196/196 - 35s - loss: 27.6841 - MinusLogProbMetric: 27.6841 - val_loss: 27.9449 - val_MinusLogProbMetric: 27.9449 - lr: 3.1250e-05 - 35s/epoch - 181ms/step
Epoch 954/1000
2023-10-26 07:43:32.869 
Epoch 954/1000 
	 loss: 27.6803, MinusLogProbMetric: 27.6803, val_loss: 27.9447, val_MinusLogProbMetric: 27.9447

Epoch 954: val_loss did not improve from 27.93282
196/196 - 35s - loss: 27.6803 - MinusLogProbMetric: 27.6803 - val_loss: 27.9447 - val_MinusLogProbMetric: 27.9447 - lr: 3.1250e-05 - 35s/epoch - 181ms/step
Epoch 955/1000
2023-10-26 07:44:08.502 
Epoch 955/1000 
	 loss: 27.6834, MinusLogProbMetric: 27.6834, val_loss: 27.9425, val_MinusLogProbMetric: 27.9425

Epoch 955: val_loss did not improve from 27.93282
196/196 - 36s - loss: 27.6834 - MinusLogProbMetric: 27.6834 - val_loss: 27.9425 - val_MinusLogProbMetric: 27.9425 - lr: 3.1250e-05 - 36s/epoch - 182ms/step
Epoch 956/1000
2023-10-26 07:44:43.943 
Epoch 956/1000 
	 loss: 27.6781, MinusLogProbMetric: 27.6781, val_loss: 27.9437, val_MinusLogProbMetric: 27.9437

Epoch 956: val_loss did not improve from 27.93282
196/196 - 35s - loss: 27.6781 - MinusLogProbMetric: 27.6781 - val_loss: 27.9437 - val_MinusLogProbMetric: 27.9437 - lr: 3.1250e-05 - 35s/epoch - 181ms/step
Epoch 957/1000
2023-10-26 07:45:19.056 
Epoch 957/1000 
	 loss: 27.6742, MinusLogProbMetric: 27.6742, val_loss: 27.9429, val_MinusLogProbMetric: 27.9429

Epoch 957: val_loss did not improve from 27.93282
196/196 - 35s - loss: 27.6742 - MinusLogProbMetric: 27.6742 - val_loss: 27.9429 - val_MinusLogProbMetric: 27.9429 - lr: 3.1250e-05 - 35s/epoch - 179ms/step
Epoch 958/1000
2023-10-26 07:45:54.338 
Epoch 958/1000 
	 loss: 27.6866, MinusLogProbMetric: 27.6866, val_loss: 27.9870, val_MinusLogProbMetric: 27.9870

Epoch 958: val_loss did not improve from 27.93282
196/196 - 35s - loss: 27.6866 - MinusLogProbMetric: 27.6866 - val_loss: 27.9870 - val_MinusLogProbMetric: 27.9870 - lr: 3.1250e-05 - 35s/epoch - 180ms/step
Epoch 959/1000
2023-10-26 07:46:29.714 
Epoch 959/1000 
	 loss: 27.6763, MinusLogProbMetric: 27.6763, val_loss: 27.9546, val_MinusLogProbMetric: 27.9546

Epoch 959: val_loss did not improve from 27.93282
196/196 - 35s - loss: 27.6763 - MinusLogProbMetric: 27.6763 - val_loss: 27.9546 - val_MinusLogProbMetric: 27.9546 - lr: 3.1250e-05 - 35s/epoch - 180ms/step
Epoch 960/1000
2023-10-26 07:47:05.203 
Epoch 960/1000 
	 loss: 27.6807, MinusLogProbMetric: 27.6807, val_loss: 27.9793, val_MinusLogProbMetric: 27.9793

Epoch 960: val_loss did not improve from 27.93282
196/196 - 35s - loss: 27.6807 - MinusLogProbMetric: 27.6807 - val_loss: 27.9793 - val_MinusLogProbMetric: 27.9793 - lr: 3.1250e-05 - 35s/epoch - 181ms/step
Epoch 961/1000
2023-10-26 07:47:40.554 
Epoch 961/1000 
	 loss: 27.6821, MinusLogProbMetric: 27.6821, val_loss: 27.9387, val_MinusLogProbMetric: 27.9387

Epoch 961: val_loss did not improve from 27.93282
196/196 - 35s - loss: 27.6821 - MinusLogProbMetric: 27.6821 - val_loss: 27.9387 - val_MinusLogProbMetric: 27.9387 - lr: 3.1250e-05 - 35s/epoch - 180ms/step
Epoch 962/1000
2023-10-26 07:48:15.952 
Epoch 962/1000 
	 loss: 27.6736, MinusLogProbMetric: 27.6736, val_loss: 27.9880, val_MinusLogProbMetric: 27.9880

Epoch 962: val_loss did not improve from 27.93282
196/196 - 35s - loss: 27.6736 - MinusLogProbMetric: 27.6736 - val_loss: 27.9880 - val_MinusLogProbMetric: 27.9880 - lr: 3.1250e-05 - 35s/epoch - 181ms/step
Epoch 963/1000
2023-10-26 07:48:51.450 
Epoch 963/1000 
	 loss: 27.6816, MinusLogProbMetric: 27.6816, val_loss: 27.9358, val_MinusLogProbMetric: 27.9358

Epoch 963: val_loss did not improve from 27.93282
196/196 - 35s - loss: 27.6816 - MinusLogProbMetric: 27.6816 - val_loss: 27.9358 - val_MinusLogProbMetric: 27.9358 - lr: 3.1250e-05 - 35s/epoch - 181ms/step
Epoch 964/1000
2023-10-26 07:49:26.712 
Epoch 964/1000 
	 loss: 27.6761, MinusLogProbMetric: 27.6761, val_loss: 27.9705, val_MinusLogProbMetric: 27.9705

Epoch 964: val_loss did not improve from 27.93282
196/196 - 35s - loss: 27.6761 - MinusLogProbMetric: 27.6761 - val_loss: 27.9705 - val_MinusLogProbMetric: 27.9705 - lr: 3.1250e-05 - 35s/epoch - 180ms/step
Epoch 965/1000
2023-10-26 07:50:02.045 
Epoch 965/1000 
	 loss: 27.6785, MinusLogProbMetric: 27.6785, val_loss: 27.9448, val_MinusLogProbMetric: 27.9448

Epoch 965: val_loss did not improve from 27.93282
196/196 - 35s - loss: 27.6785 - MinusLogProbMetric: 27.6785 - val_loss: 27.9448 - val_MinusLogProbMetric: 27.9448 - lr: 3.1250e-05 - 35s/epoch - 180ms/step
Epoch 966/1000
2023-10-26 07:50:37.157 
Epoch 966/1000 
	 loss: 27.6782, MinusLogProbMetric: 27.6782, val_loss: 27.9531, val_MinusLogProbMetric: 27.9531

Epoch 966: val_loss did not improve from 27.93282
196/196 - 35s - loss: 27.6782 - MinusLogProbMetric: 27.6782 - val_loss: 27.9531 - val_MinusLogProbMetric: 27.9531 - lr: 3.1250e-05 - 35s/epoch - 179ms/step
Epoch 967/1000
2023-10-26 07:51:11.385 
Epoch 967/1000 
	 loss: 27.6804, MinusLogProbMetric: 27.6804, val_loss: 27.9448, val_MinusLogProbMetric: 27.9448

Epoch 967: val_loss did not improve from 27.93282
196/196 - 34s - loss: 27.6804 - MinusLogProbMetric: 27.6804 - val_loss: 27.9448 - val_MinusLogProbMetric: 27.9448 - lr: 3.1250e-05 - 34s/epoch - 175ms/step
Epoch 968/1000
2023-10-26 07:51:42.453 
Epoch 968/1000 
	 loss: 27.6829, MinusLogProbMetric: 27.6829, val_loss: 27.9698, val_MinusLogProbMetric: 27.9698

Epoch 968: val_loss did not improve from 27.93282
196/196 - 31s - loss: 27.6829 - MinusLogProbMetric: 27.6829 - val_loss: 27.9698 - val_MinusLogProbMetric: 27.9698 - lr: 3.1250e-05 - 31s/epoch - 158ms/step
Epoch 969/1000
2023-10-26 07:52:17.251 
Epoch 969/1000 
	 loss: 27.6796, MinusLogProbMetric: 27.6796, val_loss: 27.9655, val_MinusLogProbMetric: 27.9655

Epoch 969: val_loss did not improve from 27.93282
196/196 - 35s - loss: 27.6796 - MinusLogProbMetric: 27.6796 - val_loss: 27.9655 - val_MinusLogProbMetric: 27.9655 - lr: 3.1250e-05 - 35s/epoch - 178ms/step
Epoch 970/1000
2023-10-26 07:52:51.948 
Epoch 970/1000 
	 loss: 27.6827, MinusLogProbMetric: 27.6827, val_loss: 27.9412, val_MinusLogProbMetric: 27.9412

Epoch 970: val_loss did not improve from 27.93282
196/196 - 35s - loss: 27.6827 - MinusLogProbMetric: 27.6827 - val_loss: 27.9412 - val_MinusLogProbMetric: 27.9412 - lr: 3.1250e-05 - 35s/epoch - 177ms/step
Epoch 971/1000
2023-10-26 07:53:27.111 
Epoch 971/1000 
	 loss: 27.6774, MinusLogProbMetric: 27.6774, val_loss: 27.9454, val_MinusLogProbMetric: 27.9454

Epoch 971: val_loss did not improve from 27.93282
196/196 - 35s - loss: 27.6774 - MinusLogProbMetric: 27.6774 - val_loss: 27.9454 - val_MinusLogProbMetric: 27.9454 - lr: 3.1250e-05 - 35s/epoch - 179ms/step
Epoch 972/1000
2023-10-26 07:54:02.383 
Epoch 972/1000 
	 loss: 27.6762, MinusLogProbMetric: 27.6762, val_loss: 27.9695, val_MinusLogProbMetric: 27.9695

Epoch 972: val_loss did not improve from 27.93282
196/196 - 35s - loss: 27.6762 - MinusLogProbMetric: 27.6762 - val_loss: 27.9695 - val_MinusLogProbMetric: 27.9695 - lr: 3.1250e-05 - 35s/epoch - 180ms/step
Epoch 973/1000
2023-10-26 07:54:37.053 
Epoch 973/1000 
	 loss: 27.6781, MinusLogProbMetric: 27.6781, val_loss: 27.9435, val_MinusLogProbMetric: 27.9435

Epoch 973: val_loss did not improve from 27.93282
196/196 - 35s - loss: 27.6781 - MinusLogProbMetric: 27.6781 - val_loss: 27.9435 - val_MinusLogProbMetric: 27.9435 - lr: 3.1250e-05 - 35s/epoch - 177ms/step
Epoch 974/1000
2023-10-26 07:55:12.318 
Epoch 974/1000 
	 loss: 27.6731, MinusLogProbMetric: 27.6731, val_loss: 27.9350, val_MinusLogProbMetric: 27.9350

Epoch 974: val_loss did not improve from 27.93282
196/196 - 35s - loss: 27.6731 - MinusLogProbMetric: 27.6731 - val_loss: 27.9350 - val_MinusLogProbMetric: 27.9350 - lr: 3.1250e-05 - 35s/epoch - 180ms/step
Epoch 975/1000
2023-10-26 07:55:47.253 
Epoch 975/1000 
	 loss: 27.6747, MinusLogProbMetric: 27.6747, val_loss: 27.9857, val_MinusLogProbMetric: 27.9857

Epoch 975: val_loss did not improve from 27.93282
196/196 - 35s - loss: 27.6747 - MinusLogProbMetric: 27.6747 - val_loss: 27.9857 - val_MinusLogProbMetric: 27.9857 - lr: 3.1250e-05 - 35s/epoch - 178ms/step
Epoch 976/1000
2023-10-26 07:56:22.380 
Epoch 976/1000 
	 loss: 27.6764, MinusLogProbMetric: 27.6764, val_loss: 27.9455, val_MinusLogProbMetric: 27.9455

Epoch 976: val_loss did not improve from 27.93282
196/196 - 35s - loss: 27.6764 - MinusLogProbMetric: 27.6764 - val_loss: 27.9455 - val_MinusLogProbMetric: 27.9455 - lr: 3.1250e-05 - 35s/epoch - 179ms/step
Epoch 977/1000
2023-10-26 07:56:57.497 
Epoch 977/1000 
	 loss: 27.6644, MinusLogProbMetric: 27.6644, val_loss: 27.9313, val_MinusLogProbMetric: 27.9313

Epoch 977: val_loss improved from 27.93282 to 27.93133, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_369/weights/best_weights.h5
196/196 - 36s - loss: 27.6644 - MinusLogProbMetric: 27.6644 - val_loss: 27.9313 - val_MinusLogProbMetric: 27.9313 - lr: 1.5625e-05 - 36s/epoch - 183ms/step
Epoch 978/1000
2023-10-26 07:57:33.046 
Epoch 978/1000 
	 loss: 27.6650, MinusLogProbMetric: 27.6650, val_loss: 27.9492, val_MinusLogProbMetric: 27.9492

Epoch 978: val_loss did not improve from 27.93133
196/196 - 35s - loss: 27.6650 - MinusLogProbMetric: 27.6650 - val_loss: 27.9492 - val_MinusLogProbMetric: 27.9492 - lr: 1.5625e-05 - 35s/epoch - 178ms/step
Epoch 979/1000
2023-10-26 07:58:05.700 
Epoch 979/1000 
	 loss: 27.6684, MinusLogProbMetric: 27.6684, val_loss: 27.9344, val_MinusLogProbMetric: 27.9344

Epoch 979: val_loss did not improve from 27.93133
196/196 - 33s - loss: 27.6684 - MinusLogProbMetric: 27.6684 - val_loss: 27.9344 - val_MinusLogProbMetric: 27.9344 - lr: 1.5625e-05 - 33s/epoch - 167ms/step
Epoch 980/1000
2023-10-26 07:58:33.368 
Epoch 980/1000 
	 loss: 27.6636, MinusLogProbMetric: 27.6636, val_loss: 27.9331, val_MinusLogProbMetric: 27.9331

Epoch 980: val_loss did not improve from 27.93133
196/196 - 28s - loss: 27.6636 - MinusLogProbMetric: 27.6636 - val_loss: 27.9331 - val_MinusLogProbMetric: 27.9331 - lr: 1.5625e-05 - 28s/epoch - 141ms/step
Epoch 981/1000
2023-10-26 07:59:01.214 
Epoch 981/1000 
	 loss: 27.6634, MinusLogProbMetric: 27.6634, val_loss: 27.9347, val_MinusLogProbMetric: 27.9347

Epoch 981: val_loss did not improve from 27.93133
196/196 - 28s - loss: 27.6634 - MinusLogProbMetric: 27.6634 - val_loss: 27.9347 - val_MinusLogProbMetric: 27.9347 - lr: 1.5625e-05 - 28s/epoch - 142ms/step
Epoch 982/1000
2023-10-26 07:59:29.140 
Epoch 982/1000 
	 loss: 27.6643, MinusLogProbMetric: 27.6643, val_loss: 27.9396, val_MinusLogProbMetric: 27.9396

Epoch 982: val_loss did not improve from 27.93133
196/196 - 28s - loss: 27.6643 - MinusLogProbMetric: 27.6643 - val_loss: 27.9396 - val_MinusLogProbMetric: 27.9396 - lr: 1.5625e-05 - 28s/epoch - 142ms/step
Epoch 983/1000
2023-10-26 07:59:57.814 
Epoch 983/1000 
	 loss: 27.6650, MinusLogProbMetric: 27.6650, val_loss: 27.9314, val_MinusLogProbMetric: 27.9314

Epoch 983: val_loss did not improve from 27.93133
196/196 - 29s - loss: 27.6650 - MinusLogProbMetric: 27.6650 - val_loss: 27.9314 - val_MinusLogProbMetric: 27.9314 - lr: 1.5625e-05 - 29s/epoch - 146ms/step
Epoch 984/1000
2023-10-26 08:00:29.126 
Epoch 984/1000 
	 loss: 27.6646, MinusLogProbMetric: 27.6646, val_loss: 27.9299, val_MinusLogProbMetric: 27.9299

Epoch 984: val_loss improved from 27.93133 to 27.92991, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_369/weights/best_weights.h5
196/196 - 32s - loss: 27.6646 - MinusLogProbMetric: 27.6646 - val_loss: 27.9299 - val_MinusLogProbMetric: 27.9299 - lr: 1.5625e-05 - 32s/epoch - 163ms/step
Epoch 985/1000
2023-10-26 08:01:05.170 
Epoch 985/1000 
	 loss: 27.6633, MinusLogProbMetric: 27.6633, val_loss: 27.9449, val_MinusLogProbMetric: 27.9449

Epoch 985: val_loss did not improve from 27.92991
196/196 - 35s - loss: 27.6633 - MinusLogProbMetric: 27.6633 - val_loss: 27.9449 - val_MinusLogProbMetric: 27.9449 - lr: 1.5625e-05 - 35s/epoch - 180ms/step
Epoch 986/1000
2023-10-26 08:01:40.768 
Epoch 986/1000 
	 loss: 27.6647, MinusLogProbMetric: 27.6647, val_loss: 27.9318, val_MinusLogProbMetric: 27.9318

Epoch 986: val_loss did not improve from 27.92991
196/196 - 36s - loss: 27.6647 - MinusLogProbMetric: 27.6647 - val_loss: 27.9318 - val_MinusLogProbMetric: 27.9318 - lr: 1.5625e-05 - 36s/epoch - 182ms/step
Epoch 987/1000
2023-10-26 08:02:13.556 
Epoch 987/1000 
	 loss: 27.6640, MinusLogProbMetric: 27.6640, val_loss: 27.9279, val_MinusLogProbMetric: 27.9279

Epoch 987: val_loss improved from 27.92991 to 27.92790, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_369/weights/best_weights.h5
196/196 - 33s - loss: 27.6640 - MinusLogProbMetric: 27.6640 - val_loss: 27.9279 - val_MinusLogProbMetric: 27.9279 - lr: 1.5625e-05 - 33s/epoch - 171ms/step
Epoch 988/1000
2023-10-26 08:02:49.665 
Epoch 988/1000 
	 loss: 27.6653, MinusLogProbMetric: 27.6653, val_loss: 27.9322, val_MinusLogProbMetric: 27.9322

Epoch 988: val_loss did not improve from 27.92790
196/196 - 35s - loss: 27.6653 - MinusLogProbMetric: 27.6653 - val_loss: 27.9322 - val_MinusLogProbMetric: 27.9322 - lr: 1.5625e-05 - 35s/epoch - 181ms/step
Epoch 989/1000
2023-10-26 08:03:25.351 
Epoch 989/1000 
	 loss: 27.6640, MinusLogProbMetric: 27.6640, val_loss: 27.9326, val_MinusLogProbMetric: 27.9326

Epoch 989: val_loss did not improve from 27.92790
196/196 - 36s - loss: 27.6640 - MinusLogProbMetric: 27.6640 - val_loss: 27.9326 - val_MinusLogProbMetric: 27.9326 - lr: 1.5625e-05 - 36s/epoch - 182ms/step
Epoch 990/1000
2023-10-26 08:04:00.823 
Epoch 990/1000 
	 loss: 27.6650, MinusLogProbMetric: 27.6650, val_loss: 27.9305, val_MinusLogProbMetric: 27.9305

Epoch 990: val_loss did not improve from 27.92790
196/196 - 35s - loss: 27.6650 - MinusLogProbMetric: 27.6650 - val_loss: 27.9305 - val_MinusLogProbMetric: 27.9305 - lr: 1.5625e-05 - 35s/epoch - 181ms/step
Epoch 991/1000
2023-10-26 08:04:36.502 
Epoch 991/1000 
	 loss: 27.6661, MinusLogProbMetric: 27.6661, val_loss: 27.9287, val_MinusLogProbMetric: 27.9287

Epoch 991: val_loss did not improve from 27.92790
196/196 - 36s - loss: 27.6661 - MinusLogProbMetric: 27.6661 - val_loss: 27.9287 - val_MinusLogProbMetric: 27.9287 - lr: 1.5625e-05 - 36s/epoch - 182ms/step
Epoch 992/1000
2023-10-26 08:05:11.670 
Epoch 992/1000 
	 loss: 27.6651, MinusLogProbMetric: 27.6651, val_loss: 27.9512, val_MinusLogProbMetric: 27.9512

Epoch 992: val_loss did not improve from 27.92790
196/196 - 35s - loss: 27.6651 - MinusLogProbMetric: 27.6651 - val_loss: 27.9512 - val_MinusLogProbMetric: 27.9512 - lr: 1.5625e-05 - 35s/epoch - 179ms/step
Epoch 993/1000
2023-10-26 08:05:47.159 
Epoch 993/1000 
	 loss: 27.6656, MinusLogProbMetric: 27.6656, val_loss: 27.9340, val_MinusLogProbMetric: 27.9340

Epoch 993: val_loss did not improve from 27.92790
196/196 - 35s - loss: 27.6656 - MinusLogProbMetric: 27.6656 - val_loss: 27.9340 - val_MinusLogProbMetric: 27.9340 - lr: 1.5625e-05 - 35s/epoch - 181ms/step
Epoch 994/1000
2023-10-26 08:06:22.279 
Epoch 994/1000 
	 loss: 27.6645, MinusLogProbMetric: 27.6645, val_loss: 27.9354, val_MinusLogProbMetric: 27.9354

Epoch 994: val_loss did not improve from 27.92790
196/196 - 35s - loss: 27.6645 - MinusLogProbMetric: 27.6645 - val_loss: 27.9354 - val_MinusLogProbMetric: 27.9354 - lr: 1.5625e-05 - 35s/epoch - 179ms/step
Epoch 995/1000
2023-10-26 08:06:57.604 
Epoch 995/1000 
	 loss: 27.6643, MinusLogProbMetric: 27.6643, val_loss: 27.9313, val_MinusLogProbMetric: 27.9313

Epoch 995: val_loss did not improve from 27.92790
196/196 - 35s - loss: 27.6643 - MinusLogProbMetric: 27.6643 - val_loss: 27.9313 - val_MinusLogProbMetric: 27.9313 - lr: 1.5625e-05 - 35s/epoch - 180ms/step
Epoch 996/1000
2023-10-26 08:07:32.975 
Epoch 996/1000 
	 loss: 27.6695, MinusLogProbMetric: 27.6695, val_loss: 27.9311, val_MinusLogProbMetric: 27.9311

Epoch 996: val_loss did not improve from 27.92790
196/196 - 35s - loss: 27.6695 - MinusLogProbMetric: 27.6695 - val_loss: 27.9311 - val_MinusLogProbMetric: 27.9311 - lr: 1.5625e-05 - 35s/epoch - 180ms/step
Epoch 997/1000
2023-10-26 08:08:08.456 
Epoch 997/1000 
	 loss: 27.6659, MinusLogProbMetric: 27.6659, val_loss: 27.9465, val_MinusLogProbMetric: 27.9465

Epoch 997: val_loss did not improve from 27.92790
196/196 - 35s - loss: 27.6659 - MinusLogProbMetric: 27.6659 - val_loss: 27.9465 - val_MinusLogProbMetric: 27.9465 - lr: 1.5625e-05 - 35s/epoch - 181ms/step
Epoch 998/1000
2023-10-26 08:08:43.841 
Epoch 998/1000 
	 loss: 27.6673, MinusLogProbMetric: 27.6673, val_loss: 27.9386, val_MinusLogProbMetric: 27.9386

Epoch 998: val_loss did not improve from 27.92790
196/196 - 35s - loss: 27.6673 - MinusLogProbMetric: 27.6673 - val_loss: 27.9386 - val_MinusLogProbMetric: 27.9386 - lr: 1.5625e-05 - 35s/epoch - 181ms/step
Epoch 999/1000
2023-10-26 08:09:19.001 
Epoch 999/1000 
	 loss: 27.6676, MinusLogProbMetric: 27.6676, val_loss: 27.9290, val_MinusLogProbMetric: 27.9290

Epoch 999: val_loss did not improve from 27.92790
196/196 - 35s - loss: 27.6676 - MinusLogProbMetric: 27.6676 - val_loss: 27.9290 - val_MinusLogProbMetric: 27.9290 - lr: 1.5625e-05 - 35s/epoch - 179ms/step
Epoch 1000/1000
2023-10-26 08:09:54.210 
Epoch 1000/1000 
	 loss: 27.6669, MinusLogProbMetric: 27.6669, val_loss: 27.9383, val_MinusLogProbMetric: 27.9383

Epoch 1000: val_loss did not improve from 27.92790
196/196 - 35s - loss: 27.6669 - MinusLogProbMetric: 27.6669 - val_loss: 27.9383 - val_MinusLogProbMetric: 27.9383 - lr: 1.5625e-05 - 35s/epoch - 180ms/step
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Training succeeded with seed 721.
Model trained in 34845.70 s.

===========
Computing predictions
===========

Computing metrics...
Checking and setting numerical distributions.
Resetting dist_num.
Resetting dist_num.
Metrics computed in 0.77 s.
                ===========
                print("===========
Failed to plot
===========
")
                Exception type: ValueError
                Exception message: Number of rows must be a positive integer, not 0
                Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 481, Func.Name : prediction_function, Message : Plotters.cornerplotter(X_data_test.numpy(),X_data_nf.numpy(),path_to_results,ndims,norm=True) # type: ignore', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/Plotters.py , Line : 156, Func.Name : cornerplotter, Message : figure=corner.corner(target_samples,color=\'red\',bins=n_bins,labels=[r"%s" % s for s in labels],normalize1d=True)', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/corner.py , Line : 235, Func.Name : corner, Message : fig, axes = pl.subplots(K, K, figsize=(dim, dim))', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/pyplot.py , Line : 1502, Func.Name : subplots, Message : axs = fig.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 905, Func.Name : subplots, Message : gs = self.add_gridspec(nrows, ncols, figure=self, **gridspec_kw)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 1527, Func.Name : add_gridspec, Message : gs = GridSpec(nrows=nrows, ncols=ncols, figure=self, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 379, Func.Name : __init__, Message : super().__init__(nrows, ncols,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 49, Func.Name : __init__, Message : raise ValueError(']
                ===========

results.txt saved
results.json saved
Results log saved
Model predictions computed in 0.98 s.
===========
Run 369/720 done in 34850.85 s.
===========

Directory ../../results/CsplineN_new/run_370/ already exists.
Skipping it.
===========
Run 370/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_371/ already exists.
Skipping it.
===========
Run 371/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_372/ already exists.
Skipping it.
===========
Run 372/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_373/ already exists.
Skipping it.
===========
Run 373/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_374/ already exists.
Skipping it.
===========
Run 374/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_375/ already exists.
Skipping it.
===========
Run 375/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_376/ already exists.
Skipping it.
===========
Run 376/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_377/ already exists.
Skipping it.
===========
Run 377/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_378/ already exists.
Skipping it.
===========
Run 378/720 already exists. Skipping it.
===========

===========
Generating train data for run 379.
===========
Train data generated in 0.23 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_379/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_379/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_379/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_379
self.data_kwargs: {'seed': 869}
self.x_data: [[ 6.514898    2.847786    6.255653   ...  3.6510358   4.425651
   2.1592255 ]
 [ 6.9207306   2.8537555   6.075634   ...  3.3624277   3.311129
   1.6233965 ]
 [ 6.4017973   3.0749886   6.172165   ...  2.955763    4.1152744
   1.3646827 ]
 ...
 [ 1.2767403   5.669613    7.5353622  ...  6.398686   -0.23885484
   2.5814815 ]
 [ 6.5017424   2.903797    6.162057   ...  3.9584637   2.3525772
   1.5695707 ]
 [ 6.560207    2.9788766   6.2347674  ...  3.186725    4.042962
   1.7694858 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_578"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_579 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_58 (LogProbL  (None,)                  908640    
 ayer)                                                           
                                                                 
=================================================================
Total params: 908,640
Trainable params: 908,640
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_58/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_58'")
self.model: <keras.engine.functional.Functional object at 0x7f58350b67d0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f57b03a9570>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f57b03a9570>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f554d72d5d0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f5827b5d810>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_379/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f5827b5dd80>, <keras.callbacks.ModelCheckpoint object at 0x7f5827b5de40>, <keras.callbacks.EarlyStopping object at 0x7f5827b5e0b0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f5827b5e0e0>, <keras.callbacks.TerminateOnNaN object at 0x7f5827b5dd20>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_379/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 379/720 with hyperparameters:
timestamp = 2023-10-26 08:10:00.315446
ndims = 64
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 908640
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 6.514898    2.847786    6.255653    4.3194256   1.4973812   3.1341035
  5.0228505   6.1194158   5.916825    6.0264635   6.508691    4.724786
  9.036075    2.7496574   4.5507812   9.340518    7.6978188   7.6711087
  0.8399726   9.2822895   6.8571315   9.722106    2.197008    8.719921
  2.8436491   6.131179    1.4479048   8.114116    7.6586547   6.3509684
  4.0610533   0.6821992   8.149007    5.037877    7.3046746   9.524578
  9.765952    8.719091    0.35098916  4.6243057   7.163706    1.6523881
  5.6271005   0.58072436  2.70707    -0.32627347  7.868687    2.5209217
  4.148258    8.0176115   6.9998198   0.4588151   2.8661127   6.7648273
  5.7348933   2.2856503   9.749809    4.471522    5.854378    4.4448867
  7.1426215   3.6510358   4.425651    2.1592255 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 08:11:14.794 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6785.3379, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 74s - loss: nan - MinusLogProbMetric: 6785.3379 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 74s/epoch - 380ms/step
The loss history contains NaN values.
Training failed: trying again with seed 235536 and lr 0.0003333333333333333.
===========
Generating train data for run 379.
===========
Train data generated in 0.17 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_379/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_379/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_379/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_379
self.data_kwargs: {'seed': 869}
self.x_data: [[ 6.514898    2.847786    6.255653   ...  3.6510358   4.425651
   2.1592255 ]
 [ 6.9207306   2.8537555   6.075634   ...  3.3624277   3.311129
   1.6233965 ]
 [ 6.4017973   3.0749886   6.172165   ...  2.955763    4.1152744
   1.3646827 ]
 ...
 [ 1.2767403   5.669613    7.5353622  ...  6.398686   -0.23885484
   2.5814815 ]
 [ 6.5017424   2.903797    6.162057   ...  3.9584637   2.3525772
   1.5695707 ]
 [ 6.560207    2.9788766   6.2347674  ...  3.186725    4.042962
   1.7694858 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_584"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_585 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_59 (LogProbL  (None,)                  908640    
 ayer)                                                           
                                                                 
=================================================================
Total params: 908,640
Trainable params: 908,640
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_59/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_59'")
self.model: <keras.engine.functional.Functional object at 0x7f5735dbbf10>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f553c935420>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f553c935420>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f57609f0490>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f5735c1f490>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_379/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f5735c1fa00>, <keras.callbacks.ModelCheckpoint object at 0x7f5735c1fac0>, <keras.callbacks.EarlyStopping object at 0x7f5735c1fd30>, <keras.callbacks.ReduceLROnPlateau object at 0x7f5735c1fd60>, <keras.callbacks.TerminateOnNaN object at 0x7f5735c1f9a0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_379/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 379/720 with hyperparameters:
timestamp = 2023-10-26 08:11:20.775938
ndims = 64
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 908640
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 6.514898    2.847786    6.255653    4.3194256   1.4973812   3.1341035
  5.0228505   6.1194158   5.916825    6.0264635   6.508691    4.724786
  9.036075    2.7496574   4.5507812   9.340518    7.6978188   7.6711087
  0.8399726   9.2822895   6.8571315   9.722106    2.197008    8.719921
  2.8436491   6.131179    1.4479048   8.114116    7.6586547   6.3509684
  4.0610533   0.6821992   8.149007    5.037877    7.3046746   9.524578
  9.765952    8.719091    0.35098916  4.6243057   7.163706    1.6523881
  5.6271005   0.58072436  2.70707    -0.32627347  7.868687    2.5209217
  4.148258    8.0176115   6.9998198   0.4588151   2.8661127   6.7648273
  5.7348933   2.2856503   9.749809    4.471522    5.854378    4.4448867
  7.1426215   3.6510358   4.425651    2.1592255 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 08:12:59.302 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6785.3379, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 98s - loss: nan - MinusLogProbMetric: 6785.3379 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 98s/epoch - 502ms/step
The loss history contains NaN values.
Training failed: trying again with seed 235536 and lr 0.0001111111111111111.
===========
Generating train data for run 379.
===========
Train data generated in 0.33 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_379/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_379/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_379/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_379
self.data_kwargs: {'seed': 869}
self.x_data: [[ 6.514898    2.847786    6.255653   ...  3.6510358   4.425651
   2.1592255 ]
 [ 6.9207306   2.8537555   6.075634   ...  3.3624277   3.311129
   1.6233965 ]
 [ 6.4017973   3.0749886   6.172165   ...  2.955763    4.1152744
   1.3646827 ]
 ...
 [ 1.2767403   5.669613    7.5353622  ...  6.398686   -0.23885484
   2.5814815 ]
 [ 6.5017424   2.903797    6.162057   ...  3.9584637   2.3525772
   1.5695707 ]
 [ 6.560207    2.9788766   6.2347674  ...  3.186725    4.042962
   1.7694858 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_590"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_591 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_60 (LogProbL  (None,)                  908640    
 ayer)                                                           
                                                                 
=================================================================
Total params: 908,640
Trainable params: 908,640
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_60/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_60'")
self.model: <keras.engine.functional.Functional object at 0x7f554f966980>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f57c8ed16f0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f57c8ed16f0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f577954e560>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f5ae07189d0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_379/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f5ae071ad40>, <keras.callbacks.ModelCheckpoint object at 0x7f5ae071be20>, <keras.callbacks.EarlyStopping object at 0x7f5ae0718640>, <keras.callbacks.ReduceLROnPlateau object at 0x7f5ae071aaa0>, <keras.callbacks.TerminateOnNaN object at 0x7f5ae071b3a0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_379/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 379/720 with hyperparameters:
timestamp = 2023-10-26 08:13:06.742946
ndims = 64
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 908640
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 6.514898    2.847786    6.255653    4.3194256   1.4973812   3.1341035
  5.0228505   6.1194158   5.916825    6.0264635   6.508691    4.724786
  9.036075    2.7496574   4.5507812   9.340518    7.6978188   7.6711087
  0.8399726   9.2822895   6.8571315   9.722106    2.197008    8.719921
  2.8436491   6.131179    1.4479048   8.114116    7.6586547   6.3509684
  4.0610533   0.6821992   8.149007    5.037877    7.3046746   9.524578
  9.765952    8.719091    0.35098916  4.6243057   7.163706    1.6523881
  5.6271005   0.58072436  2.70707    -0.32627347  7.868687    2.5209217
  4.148258    8.0176115   6.9998198   0.4588151   2.8661127   6.7648273
  5.7348933   2.2856503   9.749809    4.471522    5.854378    4.4448867
  7.1426215   3.6510358   4.425651    2.1592255 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 08:14:30.532 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6785.3379, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 84s - loss: nan - MinusLogProbMetric: 6785.3379 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.1111e-04 - 84s/epoch - 426ms/step
The loss history contains NaN values.
Training failed: trying again with seed 235536 and lr 3.703703703703703e-05.
===========
Generating train data for run 379.
===========
Train data generated in 0.37 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_379/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_379/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_379/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_379
self.data_kwargs: {'seed': 869}
self.x_data: [[ 6.514898    2.847786    6.255653   ...  3.6510358   4.425651
   2.1592255 ]
 [ 6.9207306   2.8537555   6.075634   ...  3.3624277   3.311129
   1.6233965 ]
 [ 6.4017973   3.0749886   6.172165   ...  2.955763    4.1152744
   1.3646827 ]
 ...
 [ 1.2767403   5.669613    7.5353622  ...  6.398686   -0.23885484
   2.5814815 ]
 [ 6.5017424   2.903797    6.162057   ...  3.9584637   2.3525772
   1.5695707 ]
 [ 6.560207    2.9788766   6.2347674  ...  3.186725    4.042962
   1.7694858 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_596"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_597 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_61 (LogProbL  (None,)                  908640    
 ayer)                                                           
                                                                 
=================================================================
Total params: 908,640
Trainable params: 908,640
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_61/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_61'")
self.model: <keras.engine.functional.Functional object at 0x7f57c88dfe80>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f58981749d0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f58981749d0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f58de9de7a0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f5f8e57b670>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_379/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f5f8e57bbe0>, <keras.callbacks.ModelCheckpoint object at 0x7f5f8e57bca0>, <keras.callbacks.EarlyStopping object at 0x7f5f8e57bf10>, <keras.callbacks.ReduceLROnPlateau object at 0x7f5f8e57bf40>, <keras.callbacks.TerminateOnNaN object at 0x7f5f8e57bb80>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_379/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 379/720 with hyperparameters:
timestamp = 2023-10-26 08:14:37.668041
ndims = 64
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 908640
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 3.703703703703703e-05...
Train first sample: [ 6.514898    2.847786    6.255653    4.3194256   1.4973812   3.1341035
  5.0228505   6.1194158   5.916825    6.0264635   6.508691    4.724786
  9.036075    2.7496574   4.5507812   9.340518    7.6978188   7.6711087
  0.8399726   9.2822895   6.8571315   9.722106    2.197008    8.719921
  2.8436491   6.131179    1.4479048   8.114116    7.6586547   6.3509684
  4.0610533   0.6821992   8.149007    5.037877    7.3046746   9.524578
  9.765952    8.719091    0.35098916  4.6243057   7.163706    1.6523881
  5.6271005   0.58072436  2.70707    -0.32627347  7.868687    2.5209217
  4.148258    8.0176115   6.9998198   0.4588151   2.8661127   6.7648273
  5.7348933   2.2856503   9.749809    4.471522    5.854378    4.4448867
  7.1426215   3.6510358   4.425651    2.1592255 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 08:16:01.264 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6785.3379, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 83s - loss: nan - MinusLogProbMetric: 6785.3379 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.7037e-05 - 83s/epoch - 426ms/step
The loss history contains NaN values.
Training failed: trying again with seed 235536 and lr 1.2345679012345677e-05.
===========
Generating train data for run 379.
===========
Train data generated in 0.30 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_379/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_379/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_379/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_379
self.data_kwargs: {'seed': 869}
self.x_data: [[ 6.514898    2.847786    6.255653   ...  3.6510358   4.425651
   2.1592255 ]
 [ 6.9207306   2.8537555   6.075634   ...  3.3624277   3.311129
   1.6233965 ]
 [ 6.4017973   3.0749886   6.172165   ...  2.955763    4.1152744
   1.3646827 ]
 ...
 [ 1.2767403   5.669613    7.5353622  ...  6.398686   -0.23885484
   2.5814815 ]
 [ 6.5017424   2.903797    6.162057   ...  3.9584637   2.3525772
   1.5695707 ]
 [ 6.560207    2.9788766   6.2347674  ...  3.186725    4.042962
   1.7694858 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_602"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_603 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_62 (LogProbL  (None,)                  908640    
 ayer)                                                           
                                                                 
=================================================================
Total params: 908,640
Trainable params: 908,640
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_62/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_62'")
self.model: <keras.engine.functional.Functional object at 0x7f55264affd0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f552c2d9870>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f552c2d9870>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f553c9cdea0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f55548815a0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_379/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f5554881b10>, <keras.callbacks.ModelCheckpoint object at 0x7f5554881bd0>, <keras.callbacks.EarlyStopping object at 0x7f5554881e40>, <keras.callbacks.ReduceLROnPlateau object at 0x7f5554881e70>, <keras.callbacks.TerminateOnNaN object at 0x7f5554881ab0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_379/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 379/720 with hyperparameters:
timestamp = 2023-10-26 08:16:07.889804
ndims = 64
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 908640
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.2345679012345677e-05...
Train first sample: [ 6.514898    2.847786    6.255653    4.3194256   1.4973812   3.1341035
  5.0228505   6.1194158   5.916825    6.0264635   6.508691    4.724786
  9.036075    2.7496574   4.5507812   9.340518    7.6978188   7.6711087
  0.8399726   9.2822895   6.8571315   9.722106    2.197008    8.719921
  2.8436491   6.131179    1.4479048   8.114116    7.6586547   6.3509684
  4.0610533   0.6821992   8.149007    5.037877    7.3046746   9.524578
  9.765952    8.719091    0.35098916  4.6243057   7.163706    1.6523881
  5.6271005   0.58072436  2.70707    -0.32627347  7.868687    2.5209217
  4.148258    8.0176115   6.9998198   0.4588151   2.8661127   6.7648273
  5.7348933   2.2856503   9.749809    4.471522    5.854378    4.4448867
  7.1426215   3.6510358   4.425651    2.1592255 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 08:17:49.083 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6785.3379, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 101s - loss: nan - MinusLogProbMetric: 6785.3379 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.2346e-05 - 101s/epoch - 515ms/step
The loss history contains NaN values.
Training failed: trying again with seed 235536 and lr 4.115226337448558e-06.
===========
Generating train data for run 379.
===========
Train data generated in 0.35 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_379/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_379/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_379/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_379
self.data_kwargs: {'seed': 869}
self.x_data: [[ 6.514898    2.847786    6.255653   ...  3.6510358   4.425651
   2.1592255 ]
 [ 6.9207306   2.8537555   6.075634   ...  3.3624277   3.311129
   1.6233965 ]
 [ 6.4017973   3.0749886   6.172165   ...  2.955763    4.1152744
   1.3646827 ]
 ...
 [ 1.2767403   5.669613    7.5353622  ...  6.398686   -0.23885484
   2.5814815 ]
 [ 6.5017424   2.903797    6.162057   ...  3.9584637   2.3525772
   1.5695707 ]
 [ 6.560207    2.9788766   6.2347674  ...  3.186725    4.042962
   1.7694858 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_608"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_609 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_63 (LogProbL  (None,)                  908640    
 ayer)                                                           
                                                                 
=================================================================
Total params: 908,640
Trainable params: 908,640
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_63/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_63'")
self.model: <keras.engine.functional.Functional object at 0x7f58dfcb2380>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f5c0040eef0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f5c0040eef0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f554f9f9ab0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f57ed95bcd0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_379/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f57ed95bd60>, <keras.callbacks.ModelCheckpoint object at 0x7f57ed95a8f0>, <keras.callbacks.EarlyStopping object at 0x7f57ed95b190>, <keras.callbacks.ReduceLROnPlateau object at 0x7f57ed95af50>, <keras.callbacks.TerminateOnNaN object at 0x7f57ed958af0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_379/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 379/720 with hyperparameters:
timestamp = 2023-10-26 08:17:56.540879
ndims = 64
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 908640
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.115226337448558e-06...
Train first sample: [ 6.514898    2.847786    6.255653    4.3194256   1.4973812   3.1341035
  5.0228505   6.1194158   5.916825    6.0264635   6.508691    4.724786
  9.036075    2.7496574   4.5507812   9.340518    7.6978188   7.6711087
  0.8399726   9.2822895   6.8571315   9.722106    2.197008    8.719921
  2.8436491   6.131179    1.4479048   8.114116    7.6586547   6.3509684
  4.0610533   0.6821992   8.149007    5.037877    7.3046746   9.524578
  9.765952    8.719091    0.35098916  4.6243057   7.163706    1.6523881
  5.6271005   0.58072436  2.70707    -0.32627347  7.868687    2.5209217
  4.148258    8.0176115   6.9998198   0.4588151   2.8661127   6.7648273
  5.7348933   2.2856503   9.749809    4.471522    5.854378    4.4448867
  7.1426215   3.6510358   4.425651    2.1592255 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 08:19:25.035 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6785.3379, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 88s - loss: nan - MinusLogProbMetric: 6785.3379 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.1152e-06 - 88s/epoch - 451ms/step
The loss history contains NaN values.
Training failed: trying again with seed 235536 and lr 1.3717421124828526e-06.
===========
Generating train data for run 379.
===========
Train data generated in 0.37 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_379/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_379/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_379/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_379
self.data_kwargs: {'seed': 869}
self.x_data: [[ 6.514898    2.847786    6.255653   ...  3.6510358   4.425651
   2.1592255 ]
 [ 6.9207306   2.8537555   6.075634   ...  3.3624277   3.311129
   1.6233965 ]
 [ 6.4017973   3.0749886   6.172165   ...  2.955763    4.1152744
   1.3646827 ]
 ...
 [ 1.2767403   5.669613    7.5353622  ...  6.398686   -0.23885484
   2.5814815 ]
 [ 6.5017424   2.903797    6.162057   ...  3.9584637   2.3525772
   1.5695707 ]
 [ 6.560207    2.9788766   6.2347674  ...  3.186725    4.042962
   1.7694858 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_614"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_615 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_64 (LogProbL  (None,)                  908640    
 ayer)                                                           
                                                                 
=================================================================
Total params: 908,640
Trainable params: 908,640
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_64/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_64'")
self.model: <keras.engine.functional.Functional object at 0x7f6026e73430>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f58369a1de0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f58369a1de0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f589a33e350>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f5ff561eef0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_379/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f5ff561f460>, <keras.callbacks.ModelCheckpoint object at 0x7f5ff561f520>, <keras.callbacks.EarlyStopping object at 0x7f5ff561f790>, <keras.callbacks.ReduceLROnPlateau object at 0x7f5ff561f7c0>, <keras.callbacks.TerminateOnNaN object at 0x7f5ff561f400>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_379/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 379/720 with hyperparameters:
timestamp = 2023-10-26 08:19:32.052493
ndims = 64
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 908640
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.3717421124828526e-06...
Train first sample: [ 6.514898    2.847786    6.255653    4.3194256   1.4973812   3.1341035
  5.0228505   6.1194158   5.916825    6.0264635   6.508691    4.724786
  9.036075    2.7496574   4.5507812   9.340518    7.6978188   7.6711087
  0.8399726   9.2822895   6.8571315   9.722106    2.197008    8.719921
  2.8436491   6.131179    1.4479048   8.114116    7.6586547   6.3509684
  4.0610533   0.6821992   8.149007    5.037877    7.3046746   9.524578
  9.765952    8.719091    0.35098916  4.6243057   7.163706    1.6523881
  5.6271005   0.58072436  2.70707    -0.32627347  7.868687    2.5209217
  4.148258    8.0176115   6.9998198   0.4588151   2.8661127   6.7648273
  5.7348933   2.2856503   9.749809    4.471522    5.854378    4.4448867
  7.1426215   3.6510358   4.425651    2.1592255 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 08:20:51.802 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6785.3379, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 80s - loss: nan - MinusLogProbMetric: 6785.3379 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.3717e-06 - 80s/epoch - 406ms/step
The loss history contains NaN values.
Training failed: trying again with seed 235536 and lr 4.572473708276175e-07.
===========
Generating train data for run 379.
===========
Train data generated in 0.38 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_379/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_379/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_379/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_379
self.data_kwargs: {'seed': 869}
self.x_data: [[ 6.514898    2.847786    6.255653   ...  3.6510358   4.425651
   2.1592255 ]
 [ 6.9207306   2.8537555   6.075634   ...  3.3624277   3.311129
   1.6233965 ]
 [ 6.4017973   3.0749886   6.172165   ...  2.955763    4.1152744
   1.3646827 ]
 ...
 [ 1.2767403   5.669613    7.5353622  ...  6.398686   -0.23885484
   2.5814815 ]
 [ 6.5017424   2.903797    6.162057   ...  3.9584637   2.3525772
   1.5695707 ]
 [ 6.560207    2.9788766   6.2347674  ...  3.186725    4.042962
   1.7694858 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_620"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_621 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_65 (LogProbL  (None,)                  908640    
 ayer)                                                           
                                                                 
=================================================================
Total params: 908,640
Trainable params: 908,640
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_65/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_65'")
self.model: <keras.engine.functional.Functional object at 0x7f552c79e230>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f589be19c60>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f589be19c60>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f551454e1d0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f55356784f0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_379/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f5535678a60>, <keras.callbacks.ModelCheckpoint object at 0x7f5535678b20>, <keras.callbacks.EarlyStopping object at 0x7f5535678d90>, <keras.callbacks.ReduceLROnPlateau object at 0x7f5535678dc0>, <keras.callbacks.TerminateOnNaN object at 0x7f5535678a00>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_379/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 379/720 with hyperparameters:
timestamp = 2023-10-26 08:20:58.757704
ndims = 64
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 908640
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.572473708276175e-07...
Train first sample: [ 6.514898    2.847786    6.255653    4.3194256   1.4973812   3.1341035
  5.0228505   6.1194158   5.916825    6.0264635   6.508691    4.724786
  9.036075    2.7496574   4.5507812   9.340518    7.6978188   7.6711087
  0.8399726   9.2822895   6.8571315   9.722106    2.197008    8.719921
  2.8436491   6.131179    1.4479048   8.114116    7.6586547   6.3509684
  4.0610533   0.6821992   8.149007    5.037877    7.3046746   9.524578
  9.765952    8.719091    0.35098916  4.6243057   7.163706    1.6523881
  5.6271005   0.58072436  2.70707    -0.32627347  7.868687    2.5209217
  4.148258    8.0176115   6.9998198   0.4588151   2.8661127   6.7648273
  5.7348933   2.2856503   9.749809    4.471522    5.854378    4.4448867
  7.1426215   3.6510358   4.425651    2.1592255 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 08:22:39.283 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6785.3379, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 100s - loss: nan - MinusLogProbMetric: 6785.3379 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.5725e-07 - 100s/epoch - 512ms/step
The loss history contains NaN values.
Training failed: trying again with seed 235536 and lr 1.524157902758725e-07.
===========
Generating train data for run 379.
===========
Train data generated in 0.49 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_379/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_379/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_379/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_379
self.data_kwargs: {'seed': 869}
self.x_data: [[ 6.514898    2.847786    6.255653   ...  3.6510358   4.425651
   2.1592255 ]
 [ 6.9207306   2.8537555   6.075634   ...  3.3624277   3.311129
   1.6233965 ]
 [ 6.4017973   3.0749886   6.172165   ...  2.955763    4.1152744
   1.3646827 ]
 ...
 [ 1.2767403   5.669613    7.5353622  ...  6.398686   -0.23885484
   2.5814815 ]
 [ 6.5017424   2.903797    6.162057   ...  3.9584637   2.3525772
   1.5695707 ]
 [ 6.560207    2.9788766   6.2347674  ...  3.186725    4.042962
   1.7694858 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_626"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_627 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_66 (LogProbL  (None,)                  908640    
 ayer)                                                           
                                                                 
=================================================================
Total params: 908,640
Trainable params: 908,640
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_66/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_66'")
self.model: <keras.engine.functional.Functional object at 0x7f582708c730>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f576d516da0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f576d516da0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f57bde220e0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f57e87c8190>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_379/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f57b1a14610>, <keras.callbacks.ModelCheckpoint object at 0x7f57b1a15960>, <keras.callbacks.EarlyStopping object at 0x7f57b1a14100>, <keras.callbacks.ReduceLROnPlateau object at 0x7f57b1a14eb0>, <keras.callbacks.TerminateOnNaN object at 0x7f57b1a14a60>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_379/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 379/720 with hyperparameters:
timestamp = 2023-10-26 08:22:47.114045
ndims = 64
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 908640
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.524157902758725e-07...
Train first sample: [ 6.514898    2.847786    6.255653    4.3194256   1.4973812   3.1341035
  5.0228505   6.1194158   5.916825    6.0264635   6.508691    4.724786
  9.036075    2.7496574   4.5507812   9.340518    7.6978188   7.6711087
  0.8399726   9.2822895   6.8571315   9.722106    2.197008    8.719921
  2.8436491   6.131179    1.4479048   8.114116    7.6586547   6.3509684
  4.0610533   0.6821992   8.149007    5.037877    7.3046746   9.524578
  9.765952    8.719091    0.35098916  4.6243057   7.163706    1.6523881
  5.6271005   0.58072436  2.70707    -0.32627347  7.868687    2.5209217
  4.148258    8.0176115   6.9998198   0.4588151   2.8661127   6.7648273
  5.7348933   2.2856503   9.749809    4.471522    5.854378    4.4448867
  7.1426215   3.6510358   4.425651    2.1592255 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 08:24:12.705 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6785.3379, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 86s - loss: nan - MinusLogProbMetric: 6785.3379 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.5242e-07 - 86s/epoch - 436ms/step
The loss history contains NaN values.
Training failed: trying again with seed 235536 and lr 5.0805263425290834e-08.
===========
Generating train data for run 379.
===========
Train data generated in 0.32 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_379/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_379/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_379/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_379
self.data_kwargs: {'seed': 869}
self.x_data: [[ 6.514898    2.847786    6.255653   ...  3.6510358   4.425651
   2.1592255 ]
 [ 6.9207306   2.8537555   6.075634   ...  3.3624277   3.311129
   1.6233965 ]
 [ 6.4017973   3.0749886   6.172165   ...  2.955763    4.1152744
   1.3646827 ]
 ...
 [ 1.2767403   5.669613    7.5353622  ...  6.398686   -0.23885484
   2.5814815 ]
 [ 6.5017424   2.903797    6.162057   ...  3.9584637   2.3525772
   1.5695707 ]
 [ 6.560207    2.9788766   6.2347674  ...  3.186725    4.042962
   1.7694858 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_632"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_633 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_67 (LogProbL  (None,)                  908640    
 ayer)                                                           
                                                                 
=================================================================
Total params: 908,640
Trainable params: 908,640
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_67/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_67'")
self.model: <keras.engine.functional.Functional object at 0x7f57b11f7c10>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f57bc9e9d20>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f57bc9e9d20>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f5826ca5720>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f593f612080>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_379/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f593f6125f0>, <keras.callbacks.ModelCheckpoint object at 0x7f593f6126b0>, <keras.callbacks.EarlyStopping object at 0x7f593f612920>, <keras.callbacks.ReduceLROnPlateau object at 0x7f593f612950>, <keras.callbacks.TerminateOnNaN object at 0x7f593f612590>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_379/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 379/720 with hyperparameters:
timestamp = 2023-10-26 08:24:19.941273
ndims = 64
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 908640
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 5.0805263425290834e-08...
Train first sample: [ 6.514898    2.847786    6.255653    4.3194256   1.4973812   3.1341035
  5.0228505   6.1194158   5.916825    6.0264635   6.508691    4.724786
  9.036075    2.7496574   4.5507812   9.340518    7.6978188   7.6711087
  0.8399726   9.2822895   6.8571315   9.722106    2.197008    8.719921
  2.8436491   6.131179    1.4479048   8.114116    7.6586547   6.3509684
  4.0610533   0.6821992   8.149007    5.037877    7.3046746   9.524578
  9.765952    8.719091    0.35098916  4.6243057   7.163706    1.6523881
  5.6271005   0.58072436  2.70707    -0.32627347  7.868687    2.5209217
  4.148258    8.0176115   6.9998198   0.4588151   2.8661127   6.7648273
  5.7348933   2.2856503   9.749809    4.471522    5.854378    4.4448867
  7.1426215   3.6510358   4.425651    2.1592255 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 08:25:40.401 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6785.3379, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 80s - loss: nan - MinusLogProbMetric: 6785.3379 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 5.0805e-08 - 80s/epoch - 410ms/step
The loss history contains NaN values.
Training failed: trying again with seed 235536 and lr 1.6935087808430278e-08.
===========
Generating train data for run 379.
===========
Train data generated in 0.30 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_379/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_379/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_379/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_379
self.data_kwargs: {'seed': 869}
self.x_data: [[ 6.514898    2.847786    6.255653   ...  3.6510358   4.425651
   2.1592255 ]
 [ 6.9207306   2.8537555   6.075634   ...  3.3624277   3.311129
   1.6233965 ]
 [ 6.4017973   3.0749886   6.172165   ...  2.955763    4.1152744
   1.3646827 ]
 ...
 [ 1.2767403   5.669613    7.5353622  ...  6.398686   -0.23885484
   2.5814815 ]
 [ 6.5017424   2.903797    6.162057   ...  3.9584637   2.3525772
   1.5695707 ]
 [ 6.560207    2.9788766   6.2347674  ...  3.186725    4.042962
   1.7694858 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_638"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_639 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_68 (LogProbL  (None,)                  908640    
 ayer)                                                           
                                                                 
=================================================================
Total params: 908,640
Trainable params: 908,640
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_68/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_68'")
self.model: <keras.engine.functional.Functional object at 0x7f553c383df0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f555c9e4ee0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f555c9e4ee0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f5544ad0af0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f553c37f5e0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_379/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f553c37fb50>, <keras.callbacks.ModelCheckpoint object at 0x7f553c37fc10>, <keras.callbacks.EarlyStopping object at 0x7f553c37fe80>, <keras.callbacks.ReduceLROnPlateau object at 0x7f553c37feb0>, <keras.callbacks.TerminateOnNaN object at 0x7f553c37faf0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_379/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 379/720 with hyperparameters:
timestamp = 2023-10-26 08:25:47.023304
ndims = 64
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 908640
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.6935087808430278e-08...
Train first sample: [ 6.514898    2.847786    6.255653    4.3194256   1.4973812   3.1341035
  5.0228505   6.1194158   5.916825    6.0264635   6.508691    4.724786
  9.036075    2.7496574   4.5507812   9.340518    7.6978188   7.6711087
  0.8399726   9.2822895   6.8571315   9.722106    2.197008    8.719921
  2.8436491   6.131179    1.4479048   8.114116    7.6586547   6.3509684
  4.0610533   0.6821992   8.149007    5.037877    7.3046746   9.524578
  9.765952    8.719091    0.35098916  4.6243057   7.163706    1.6523881
  5.6271005   0.58072436  2.70707    -0.32627347  7.868687    2.5209217
  4.148258    8.0176115   6.9998198   0.4588151   2.8661127   6.7648273
  5.7348933   2.2856503   9.749809    4.471522    5.854378    4.4448867
  7.1426215   3.6510358   4.425651    2.1592255 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 08:27:21.506 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6785.3379, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 94s - loss: nan - MinusLogProbMetric: 6785.3379 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.6935e-08 - 94s/epoch - 481ms/step
The loss history contains NaN values.
Training failed: trying again with seed 235536 and lr 5.645029269476759e-09.
===========
Run 379/720 failed.
Exception type: Exception
Exception message: Training failed for the maximum number of retry.
Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 649, Func.Name : <module>, Message : hyperparams_dict, NFObject, seed_train, training_time = train_function(seeds = [seed_train, seed_test, seed_dist, seed_metrics],', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 322, Func.Name : train_function, Message : raise Exception("Training failed for the maximum number of retry.")']
===========

===========
Generating train data for run 380.
===========
Train data generated in 0.38 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_380/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_380/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_380/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_380
self.data_kwargs: {'seed': 869}
self.x_data: [[ 6.514898    2.847786    6.255653   ...  3.6510358   4.425651
   2.1592255 ]
 [ 6.9207306   2.8537555   6.075634   ...  3.3624277   3.311129
   1.6233965 ]
 [ 6.4017973   3.0749886   6.172165   ...  2.955763    4.1152744
   1.3646827 ]
 ...
 [ 1.2767403   5.669613    7.5353622  ...  6.398686   -0.23885484
   2.5814815 ]
 [ 6.5017424   2.903797    6.162057   ...  3.9584637   2.3525772
   1.5695707 ]
 [ 6.560207    2.9788766   6.2347674  ...  3.186725    4.042962
   1.7694858 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_644"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_645 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_69 (LogProbL  (None,)                  2139360   
 ayer)                                                           
                                                                 
=================================================================
Total params: 2,139,360
Trainable params: 2,139,360
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_69/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_69'")
self.model: <keras.engine.functional.Functional object at 0x7f58a02cf910>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f5891fd23e0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f5891fd23e0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f553c9cd9f0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f582536c8b0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_380/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f582536f6a0>, <keras.callbacks.ModelCheckpoint object at 0x7f582536f9a0>, <keras.callbacks.EarlyStopping object at 0x7f582536cc10>, <keras.callbacks.ReduceLROnPlateau object at 0x7f582536fd00>, <keras.callbacks.TerminateOnNaN object at 0x7f582536f0d0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_380/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 380/720 with hyperparameters:
timestamp = 2023-10-26 08:27:28.845401
ndims = 64
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 2139360
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 6.514898    2.847786    6.255653    4.3194256   1.4973812   3.1341035
  5.0228505   6.1194158   5.916825    6.0264635   6.508691    4.724786
  9.036075    2.7496574   4.5507812   9.340518    7.6978188   7.6711087
  0.8399726   9.2822895   6.8571315   9.722106    2.197008    8.719921
  2.8436491   6.131179    1.4479048   8.114116    7.6586547   6.3509684
  4.0610533   0.6821992   8.149007    5.037877    7.3046746   9.524578
  9.765952    8.719091    0.35098916  4.6243057   7.163706    1.6523881
  5.6271005   0.58072436  2.70707    -0.32627347  7.868687    2.5209217
  4.148258    8.0176115   6.9998198   0.4588151   2.8661127   6.7648273
  5.7348933   2.2856503   9.749809    4.471522    5.854378    4.4448867
  7.1426215   3.6510358   4.425651    2.1592255 ]
Epoch 1/1000
LLVM ERROR: Unable to allocate section memory!
