2023-09-26 13:23:37.119641: Importing os...
2023-09-26 13:23:37.119711: Importing sys...
2023-09-26 13:23:37.119727: Importing and initializing argparse...
Visible devices: [0]
2023-09-26 13:23:37.137966: Importing timer from timeit...
2023-09-26 13:23:37.138565: Setting env variables for tf import (only device [0] will be available)...
2023-09-26 13:23:37.138611: Importing numpy...
2023-09-26 13:23:37.312083: Importing pandas...
2023-09-26 13:23:37.494658: Importing shutil...
2023-09-26 13:23:37.494683: Importing subprocess...
2023-09-26 13:23:37.494690: Importing tensorflow...
Tensorflow version: 2.12.0
2023-09-26 13:23:39.594851: Importing tensorflow_probability...
Tensorflow probability version: 0.20.1
2023-09-26 13:23:39.992976: Importing textwrap...
2023-09-26 13:23:39.993003: Importing timeit...
2023-09-26 13:23:39.993014: Importing traceback...
2023-09-26 13:23:39.993020: Importing typing...
2023-09-26 13:23:39.993031: Setting tf configs...
2023-09-26 13:23:40.094902: Importing custom module...
Successfully loaded GPU model: NVIDIA A40
2023-09-26 13:23:40.864523: All modues imported successfully.
Directory ../../results/CsplineN_new/ already exists.
Directory ../../results/CsplineN_new/run_1/ already exists.
Skipping it.
===========
Run 1/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_2/ already exists.
Skipping it.
===========
Run 2/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_3/ already exists.
Skipping it.
===========
Run 3/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_4/ already exists.
Skipping it.
===========
Run 4/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_5/ already exists.
Skipping it.
===========
Run 5/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_6/ already exists.
Skipping it.
===========
Run 6/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_7/ already exists.
Skipping it.
===========
Run 7/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_8/ already exists.
Skipping it.
===========
Run 8/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_9/ already exists.
Skipping it.
===========
Run 9/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_10/ already exists.
Skipping it.
===========
Run 10/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_11/ already exists.
Skipping it.
===========
Run 11/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_12/ already exists.
Skipping it.
===========
Run 12/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_13/ already exists.
Skipping it.
===========
Run 13/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_14/ already exists.
Skipping it.
===========
Run 14/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_15/ already exists.
Skipping it.
===========
Run 15/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_16/ already exists.
Skipping it.
===========
Run 16/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_17/ already exists.
Skipping it.
===========
Run 17/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_18/ already exists.
Skipping it.
===========
Run 18/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_19/ already exists.
Skipping it.
===========
Run 19/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_20/ already exists.
Skipping it.
===========
Run 20/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_21/ already exists.
Skipping it.
===========
Run 21/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_22/ already exists.
Skipping it.
===========
Run 22/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_23/ already exists.
Skipping it.
===========
Run 23/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_24/ already exists.
Skipping it.
===========
Run 24/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_25/ already exists.
Skipping it.
===========
Run 25/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_26/ already exists.
Skipping it.
===========
Run 26/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_27/ already exists.
Skipping it.
===========
Run 27/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_28/ already exists.
Skipping it.
===========
Run 28/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_29/ already exists.
Skipping it.
===========
Run 29/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_30/ already exists.
Skipping it.
===========
Run 30/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_31/ already exists.
Skipping it.
===========
Run 31/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_32/ already exists.
Skipping it.
===========
Run 32/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_33/ already exists.
Skipping it.
===========
Run 33/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_34/ already exists.
Skipping it.
===========
Run 34/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_35/ already exists.
Skipping it.
===========
Run 35/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_36/ already exists.
Skipping it.
===========
Run 36/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_37/ already exists.
Skipping it.
===========
Run 37/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_38/ already exists.
Skipping it.
===========
Run 38/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_39/ already exists.
Skipping it.
===========
Run 39/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_40/ already exists.
Skipping it.
===========
Run 40/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_41/ already exists.
Skipping it.
===========
Run 41/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_42/ already exists.
Skipping it.
===========
Run 42/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_43/ already exists.
Skipping it.
===========
Run 43/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_44/ already exists.
Skipping it.
===========
Run 44/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_45/ already exists.
Skipping it.
===========
Run 45/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_46/ already exists.
Skipping it.
===========
Run 46/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_47/ already exists.
Skipping it.
===========
Run 47/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_48/ already exists.
Skipping it.
===========
Run 48/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_49/ already exists.
Skipping it.
===========
Run 49/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_50/ already exists.
Skipping it.
===========
Run 50/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_51/ already exists.
Skipping it.
===========
Run 51/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_52/ already exists.
Skipping it.
===========
Run 52/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_53/ already exists.
Skipping it.
===========
Run 53/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_54/ already exists.
Skipping it.
===========
Run 54/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_55/ already exists.
Skipping it.
===========
Run 55/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_56/ already exists.
Skipping it.
===========
Run 56/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_57/ already exists.
Skipping it.
===========
Run 57/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_58/ already exists.
Skipping it.
===========
Run 58/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_59/ already exists.
Skipping it.
===========
Run 59/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_60/ already exists.
Skipping it.
===========
Run 60/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_61/ already exists.
Skipping it.
===========
Run 61/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_62/ already exists.
Skipping it.
===========
Run 62/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_63/ already exists.
Skipping it.
===========
Run 63/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_64/ already exists.
Skipping it.
===========
Run 64/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_65/ already exists.
Skipping it.
===========
Run 65/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_66/ already exists.
Skipping it.
===========
Run 66/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_67/ already exists.
Skipping it.
===========
Run 67/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_68/ already exists.
Skipping it.
===========
Run 68/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_69/ already exists.
Skipping it.
===========
Run 69/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_70/ already exists.
Skipping it.
===========
Run 70/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_71/ already exists.
Skipping it.
===========
Run 71/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_72/ already exists.
Skipping it.
===========
Run 72/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_73/ already exists.
Skipping it.
===========
Run 73/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_74/ already exists.
Skipping it.
===========
Run 74/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_75/ already exists.
Skipping it.
===========
Run 75/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_76/ already exists.
Skipping it.
===========
Run 76/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_77/ already exists.
Skipping it.
===========
Run 77/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_78/ already exists.
Skipping it.
===========
Run 78/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_79/ already exists.
Skipping it.
===========
Run 79/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_80/ already exists.
Skipping it.
===========
Run 80/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_81/ already exists.
Skipping it.
===========
Run 81/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_82/ already exists.
Skipping it.
===========
Run 82/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_83/ already exists.
Skipping it.
===========
Run 83/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_84/ already exists.
Skipping it.
===========
Run 84/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_85/ already exists.
Skipping it.
===========
Run 85/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_86/ already exists.
Skipping it.
===========
Run 86/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_87/ already exists.
Skipping it.
===========
Run 87/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_88/ already exists.
Skipping it.
===========
Run 88/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_89/ already exists.
Skipping it.
===========
Run 89/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_90/ already exists.
Skipping it.
===========
Run 90/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_91/ already exists.
Skipping it.
===========
Run 91/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_92/ already exists.
Skipping it.
===========
Run 92/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_93/ already exists.
Skipping it.
===========
Run 93/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_94/ already exists.
Skipping it.
===========
Run 94/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_95/ already exists.
Skipping it.
===========
Run 95/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_96/ already exists.
Skipping it.
===========
Run 96/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_97/ already exists.
Skipping it.
===========
Run 97/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_98/ already exists.
Skipping it.
===========
Run 98/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_99/ already exists.
Skipping it.
===========
Run 99/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_100/ already exists.
Skipping it.
===========
Run 100/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_101/ already exists.
Skipping it.
===========
Run 101/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_102/ already exists.
Skipping it.
===========
Run 102/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_103/ already exists.
Skipping it.
===========
Run 103/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_104/ already exists.
Skipping it.
===========
Run 104/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_105/ already exists.
Skipping it.
===========
Run 105/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_106/ already exists.
Skipping it.
===========
Run 106/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_107/ already exists.
Skipping it.
===========
Run 107/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_108/ already exists.
Skipping it.
===========
Run 108/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_109/ already exists.
Skipping it.
===========
Run 109/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_110/ already exists.
Skipping it.
===========
Run 110/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_111/ already exists.
Skipping it.
===========
Run 111/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_112/ already exists.
Skipping it.
===========
Run 112/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_113/ already exists.
Skipping it.
===========
Run 113/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_114/ already exists.
Skipping it.
===========
Run 114/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_115/ already exists.
Skipping it.
===========
Run 115/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_116/ already exists.
Skipping it.
===========
Run 116/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_117/ already exists.
Skipping it.
===========
Run 117/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_118/ already exists.
Skipping it.
===========
Run 118/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_119/ already exists.
Skipping it.
===========
Run 119/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_120/ already exists.
Skipping it.
===========
Run 120/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_121/ already exists.
Skipping it.
===========
Run 121/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_122/ already exists.
Skipping it.
===========
Run 122/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_123/ already exists.
Skipping it.
===========
Run 123/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_124/ already exists.
Skipping it.
===========
Run 124/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_125/ already exists.
Skipping it.
===========
Run 125/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_126/ already exists.
Skipping it.
===========
Run 126/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_127/ already exists.
Skipping it.
===========
Run 127/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_128/ already exists.
Skipping it.
===========
Run 128/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_129/ already exists.
Skipping it.
===========
Run 129/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_130/ already exists.
Skipping it.
===========
Run 130/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_131/ already exists.
Skipping it.
===========
Run 131/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_132/ already exists.
Skipping it.
===========
Run 132/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_133/ already exists.
Skipping it.
===========
Run 133/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_134/ already exists.
Skipping it.
===========
Run 134/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_135/ already exists.
Skipping it.
===========
Run 135/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_136/ already exists.
Skipping it.
===========
Run 136/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_137/ already exists.
Skipping it.
===========
Run 137/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_138/ already exists.
Skipping it.
===========
Run 138/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_139/ already exists.
Skipping it.
===========
Run 139/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_140/ already exists.
Skipping it.
===========
Run 140/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_141/ already exists.
Skipping it.
===========
Run 141/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_142/ already exists.
Skipping it.
===========
Run 142/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_143/ already exists.
Skipping it.
===========
Run 143/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_144/ already exists.
Skipping it.
===========
Run 144/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_145/ already exists.
Skipping it.
===========
Run 145/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_146/ already exists.
Skipping it.
===========
Run 146/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_147/ already exists.
Skipping it.
===========
Run 147/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_148/ already exists.
Skipping it.
===========
Run 148/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_149/ already exists.
Skipping it.
===========
Run 149/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_150/ already exists.
Skipping it.
===========
Run 150/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_151/ already exists.
Skipping it.
===========
Run 151/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_152/ already exists.
Skipping it.
===========
Run 152/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_153/ already exists.
Skipping it.
===========
Run 153/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_154/ already exists.
Skipping it.
===========
Run 154/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_155/ already exists.
Skipping it.
===========
Run 155/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_156/ already exists.
Skipping it.
===========
Run 156/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_157/ already exists.
Skipping it.
===========
Run 157/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_158/ already exists.
Skipping it.
===========
Run 158/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_159/ already exists.
Skipping it.
===========
Run 159/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_160/ already exists.
Skipping it.
===========
Run 160/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_161/ already exists.
Skipping it.
===========
Run 161/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_162/ already exists.
Skipping it.
===========
Run 162/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_163/ already exists.
Skipping it.
===========
Run 163/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_164/ already exists.
Skipping it.
===========
Run 164/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_165/ already exists.
Skipping it.
===========
Run 165/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_166/ already exists.
Skipping it.
===========
Run 166/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_167/ already exists.
Skipping it.
===========
Run 167/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_168/ already exists.
Skipping it.
===========
Run 168/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_169/ already exists.
Skipping it.
===========
Run 169/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_170/ already exists.
Skipping it.
===========
Run 170/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_171/ already exists.
Skipping it.
===========
Run 171/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_172/ already exists.
Skipping it.
===========
Run 172/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_173/ already exists.
Skipping it.
===========
Run 173/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_174/ already exists.
Skipping it.
===========
Run 174/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_175/ already exists.
Skipping it.
===========
Run 175/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_176/ already exists.
Skipping it.
===========
Run 176/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_177/ already exists.
Skipping it.
===========
Run 177/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_178/ already exists.
Skipping it.
===========
Run 178/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_179/ already exists.
Skipping it.
===========
Run 179/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_180/ already exists.
Skipping it.
===========
Run 180/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_181/ already exists.
Skipping it.
===========
Run 181/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_182/ already exists.
Skipping it.
===========
Run 182/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_183/ already exists.
Skipping it.
===========
Run 183/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_184/ already exists.
Skipping it.
===========
Run 184/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_185/ already exists.
Skipping it.
===========
Run 185/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_186/ already exists.
Skipping it.
===========
Run 186/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_187/ already exists.
Skipping it.
===========
Run 187/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_188/ already exists.
Skipping it.
===========
Run 188/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_189/ already exists.
Skipping it.
===========
Run 189/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_190/ already exists.
Skipping it.
===========
Run 190/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_191/ already exists.
Skipping it.
===========
Run 191/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_192/ already exists.
Skipping it.
===========
Run 192/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_193/ already exists.
Skipping it.
===========
Run 193/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_194/ already exists.
Skipping it.
===========
Run 194/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_195/ already exists.
Skipping it.
===========
Run 195/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_196/ already exists.
Skipping it.
===========
Run 196/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_197/ already exists.
Skipping it.
===========
Run 197/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_198/ already exists.
Skipping it.
===========
Run 198/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_199/ already exists.
Skipping it.
===========
Run 199/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_200/ already exists.
Skipping it.
===========
Run 200/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_201/ already exists.
Skipping it.
===========
Run 201/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_202/ already exists.
Skipping it.
===========
Run 202/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_203/ already exists.
Skipping it.
===========
Run 203/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_204/ already exists.
Skipping it.
===========
Run 204/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_205/ already exists.
Skipping it.
===========
Run 205/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_206/ already exists.
Skipping it.
===========
Run 206/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_207/ already exists.
Skipping it.
===========
Run 207/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_208/ already exists.
Skipping it.
===========
Run 208/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_209/ already exists.
Skipping it.
===========
Run 209/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_210/ already exists.
Skipping it.
===========
Run 210/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_211/ already exists.
Skipping it.
===========
Run 211/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_212/ already exists.
Skipping it.
===========
Run 212/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_213/ already exists.
Skipping it.
===========
Run 213/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_214/ already exists.
Skipping it.
===========
Run 214/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_215/ already exists.
Skipping it.
===========
Run 215/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_216/ already exists.
Skipping it.
===========
Run 216/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_217/ already exists.
Skipping it.
===========
Run 217/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_218/ already exists.
Skipping it.
===========
Run 218/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_219/ already exists.
Skipping it.
===========
Run 219/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_220/ already exists.
Skipping it.
===========
Run 220/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_221/ already exists.
Skipping it.
===========
Run 221/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_222/ already exists.
Skipping it.
===========
Run 222/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_223/ already exists.
Skipping it.
===========
Run 223/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_224/ already exists.
Skipping it.
===========
Run 224/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_225/ already exists.
Skipping it.
===========
Run 225/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_226/ already exists.
Skipping it.
===========
Run 226/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_227/ already exists.
Skipping it.
===========
Run 227/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_228/ already exists.
Skipping it.
===========
Run 228/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_229/ already exists.
Skipping it.
===========
Run 229/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_230/ already exists.
Skipping it.
===========
Run 230/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_231/ already exists.
Skipping it.
===========
Run 231/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_232/ already exists.
Skipping it.
===========
Run 232/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_233/ already exists.
Skipping it.
===========
Run 233/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_234/ already exists.
Skipping it.
===========
Run 234/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_235/ already exists.
Skipping it.
===========
Run 235/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_236/ already exists.
Skipping it.
===========
Run 236/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_237/ already exists.
Skipping it.
===========
Run 237/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_238/ already exists.
Skipping it.
===========
Run 238/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_239/ already exists.
Skipping it.
===========
Run 239/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_240/ already exists.
Skipping it.
===========
Run 240/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_241/ already exists.
Skipping it.
===========
Run 241/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_242/ already exists.
Skipping it.
===========
Run 242/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_243/ already exists.
Skipping it.
===========
Run 243/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_244/ already exists.
Skipping it.
===========
Run 244/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_245/ already exists.
Skipping it.
===========
Run 245/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_246/ already exists.
Skipping it.
===========
Run 246/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_247/ already exists.
Skipping it.
===========
Run 247/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_248/ already exists.
Skipping it.
===========
Run 248/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_249/ already exists.
Skipping it.
===========
Run 249/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_250/ already exists.
Skipping it.
===========
Run 250/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_251/ already exists.
Skipping it.
===========
Run 251/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_252/ already exists.
Skipping it.
===========
Run 252/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_253/ already exists.
Skipping it.
===========
Run 253/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_254/ already exists.
Skipping it.
===========
Run 254/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_255/ already exists.
Skipping it.
===========
Run 255/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_256/ already exists.
Skipping it.
===========
Run 256/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_257/ already exists.
Skipping it.
===========
Run 257/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_258/ already exists.
Skipping it.
===========
Run 258/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_259/ already exists.
Skipping it.
===========
Run 259/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_260/ already exists.
Skipping it.
===========
Run 260/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_261/ already exists.
Skipping it.
===========
Run 261/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_262/ already exists.
Skipping it.
===========
Run 262/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_263/ already exists.
Skipping it.
===========
Run 263/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_264/ already exists.
Skipping it.
===========
Run 264/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_265/ already exists.
Skipping it.
===========
Run 265/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_266/ already exists.
Skipping it.
===========
Run 266/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_267/ already exists.
Skipping it.
===========
Run 267/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_268/ already exists.
Skipping it.
===========
Run 268/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_269/ already exists.
Skipping it.
===========
Run 269/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_270/ already exists.
Skipping it.
===========
Run 270/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_271/ already exists.
Skipping it.
===========
Run 271/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_272/ already exists.
Skipping it.
===========
Run 272/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_273/ already exists.
Skipping it.
===========
Run 273/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_274/ already exists.
Skipping it.
===========
Run 274/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_275/ already exists.
Skipping it.
===========
Run 275/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_276/ already exists.
Skipping it.
===========
Run 276/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_277/ already exists.
Skipping it.
===========
Run 277/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_278/ already exists.
Skipping it.
===========
Run 278/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_279/ already exists.
Skipping it.
===========
Run 279/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_280/ already exists.
Skipping it.
===========
Run 280/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_281/ already exists.
Skipping it.
===========
Run 281/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_282/ already exists.
Skipping it.
===========
Run 282/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_283/ already exists.
Skipping it.
===========
Run 283/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_284/ already exists.
Skipping it.
===========
Run 284/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_285/ already exists.
Skipping it.
===========
Run 285/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_286/ already exists.
Skipping it.
===========
Run 286/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_287/ already exists.
Skipping it.
===========
Run 287/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_288/ already exists.
Skipping it.
===========
Run 288/720 already exists. Skipping it.
===========

===========
Generating train data for run 289.
===========
Train data generated in 0.11 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_289/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 721}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_289/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.3570018 ,  5.6703625 ,  0.48762453, ...,  0.76730525,
         6.4537516 ,  1.4289263 ],
       [ 4.8933635 ,  4.954244  , -0.7742728 , ...,  1.033045  ,
         7.5179    ,  1.3446053 ],
       [ 2.6192126 ,  3.6659784 ,  9.3581505 , ...,  6.4096394 ,
         2.8983781 ,  1.8163822 ],
       ...,
       [ 4.4286733 ,  5.582822  , -0.08033401, ...,  1.0755457 ,
         6.482341  ,  1.3259909 ],
       [ 5.2667904 ,  5.78094   ,  0.52473783, ...,  1.3584392 ,
         6.6371064 ,  1.4527634 ],
       [ 1.3711529 ,  3.985786  ,  8.434673  , ...,  6.650886  ,
         2.8764064 ,  2.071397  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_289/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_289
self.data_kwargs: {'seed': 721}
self.x_data: [[ 5.4479074  7.1622143  6.6614265 ...  3.6775346  2.6042771  7.6607866]
 [ 1.558796   3.7643342  7.765504  ...  7.3985186  2.2847533  2.1737251]
 [ 2.023865   3.5498223  9.736043  ...  7.378399   2.9893079  1.7681804]
 ...
 [ 3.157689   3.8140697  9.128405  ...  7.3280826  3.3431854  1.6145309]
 [ 4.9035525  5.63367    1.3061054 ...  1.7336241  7.263643   1.3624198]
 [ 4.873677   6.0271535 -0.4844559 ...  1.4305246  6.6029577  1.4994687]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_5"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_6 (InputLayer)        [(None, 32)]              0         
                                                                 
 log_prob_layer (LogProbLaye  (None,)                  413360    
 r)                                                              
                                                                 
=================================================================
Total params: 413,360
Trainable params: 413,360
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer'")
self.model: <keras.engine.functional.Functional object at 0x7f80485e9840>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f80503c1480>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f80503c1480>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f8050381ae0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f805011d0c0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_289/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f805011f280>, <keras.callbacks.ModelCheckpoint object at 0x7f805011dd50>, <keras.callbacks.EarlyStopping object at 0x7f805011eef0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f805011fdc0>, <keras.callbacks.TerminateOnNaN object at 0x7f80501606a0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.3570018 ,  5.6703625 ,  0.48762453, ...,  0.76730525,
         6.4537516 ,  1.4289263 ],
       [ 4.8933635 ,  4.954244  , -0.7742728 , ...,  1.033045  ,
         7.5179    ,  1.3446053 ],
       [ 2.6192126 ,  3.6659784 ,  9.3581505 , ...,  6.4096394 ,
         2.8983781 ,  1.8163822 ],
       ...,
       [ 4.4286733 ,  5.582822  , -0.08033401, ...,  1.0755457 ,
         6.482341  ,  1.3259909 ],
       [ 5.2667904 ,  5.78094   ,  0.52473783, ...,  1.3584392 ,
         6.6371064 ,  1.4527634 ],
       [ 1.3711529 ,  3.985786  ,  8.434673  , ...,  6.650886  ,
         2.8764064 ,  2.071397  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_289/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 289/720 with hyperparameters:
timestamp = 2023-09-26 13:23:44.593975
ndims = 32
seed_train = 721
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 413360
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 5.4479074  7.1622143  6.6614265  5.4466543  4.679343   6.5534453
  4.5869303  8.297877   9.431402   3.191049   9.163281   4.112924
  5.872972  10.008474   0.4502598  1.3920732 -0.6747763  8.487896
  9.224454   7.913118   8.449968   7.6997313  5.9028535  7.439646
  2.3174791  6.975212   1.6619607  9.750696   4.816901   3.6775346
  2.6042771  7.6607866]
Epoch 1/1000
2023-09-26 13:25:05.619 
Epoch 1/1000 
	 loss: 184.8499, MinusLogProbMetric: 184.8499, val_loss: 41.7437, val_MinusLogProbMetric: 41.7437

Epoch 1: val_loss improved from inf to 41.74368, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_289/weights/best_weights.h5
196/196 - 81s - loss: 184.8499 - MinusLogProbMetric: 184.8499 - val_loss: 41.7437 - val_MinusLogProbMetric: 41.7437 - lr: 0.0010 - 81s/epoch - 415ms/step
Epoch 2/1000
2023-09-26 13:25:35.309 
Epoch 2/1000 
	 loss: 35.3231, MinusLogProbMetric: 35.3231, val_loss: 30.5938, val_MinusLogProbMetric: 30.5938

Epoch 2: val_loss improved from 41.74368 to 30.59381, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_289/weights/best_weights.h5
196/196 - 30s - loss: 35.3231 - MinusLogProbMetric: 35.3231 - val_loss: 30.5938 - val_MinusLogProbMetric: 30.5938 - lr: 0.0010 - 30s/epoch - 151ms/step
Epoch 3/1000
2023-09-26 13:26:04.429 
Epoch 3/1000 
	 loss: 28.5525, MinusLogProbMetric: 28.5525, val_loss: 27.7114, val_MinusLogProbMetric: 27.7114

Epoch 3: val_loss improved from 30.59381 to 27.71136, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_289/weights/best_weights.h5
196/196 - 29s - loss: 28.5525 - MinusLogProbMetric: 28.5525 - val_loss: 27.7114 - val_MinusLogProbMetric: 27.7114 - lr: 0.0010 - 29s/epoch - 149ms/step
Epoch 4/1000
2023-09-26 13:26:32.416 
Epoch 4/1000 
	 loss: 25.9946, MinusLogProbMetric: 25.9946, val_loss: 24.6541, val_MinusLogProbMetric: 24.6541

Epoch 4: val_loss improved from 27.71136 to 24.65409, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_289/weights/best_weights.h5
196/196 - 29s - loss: 25.9946 - MinusLogProbMetric: 25.9946 - val_loss: 24.6541 - val_MinusLogProbMetric: 24.6541 - lr: 0.0010 - 29s/epoch - 146ms/step
Epoch 5/1000
2023-09-26 13:27:00.827 
Epoch 5/1000 
	 loss: 24.5634, MinusLogProbMetric: 24.5634, val_loss: 23.9175, val_MinusLogProbMetric: 23.9175

Epoch 5: val_loss improved from 24.65409 to 23.91753, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_289/weights/best_weights.h5
196/196 - 28s - loss: 24.5634 - MinusLogProbMetric: 24.5634 - val_loss: 23.9175 - val_MinusLogProbMetric: 23.9175 - lr: 0.0010 - 28s/epoch - 141ms/step
Epoch 6/1000
2023-09-26 13:27:28.513 
Epoch 6/1000 
	 loss: 23.8296, MinusLogProbMetric: 23.8296, val_loss: 23.8973, val_MinusLogProbMetric: 23.8973

Epoch 6: val_loss improved from 23.91753 to 23.89731, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_289/weights/best_weights.h5
196/196 - 28s - loss: 23.8296 - MinusLogProbMetric: 23.8296 - val_loss: 23.8973 - val_MinusLogProbMetric: 23.8973 - lr: 0.0010 - 28s/epoch - 142ms/step
Epoch 7/1000
2023-09-26 13:27:56.469 
Epoch 7/1000 
	 loss: 23.1718, MinusLogProbMetric: 23.1718, val_loss: 23.1771, val_MinusLogProbMetric: 23.1771

Epoch 7: val_loss improved from 23.89731 to 23.17709, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_289/weights/best_weights.h5
196/196 - 28s - loss: 23.1718 - MinusLogProbMetric: 23.1718 - val_loss: 23.1771 - val_MinusLogProbMetric: 23.1771 - lr: 0.0010 - 28s/epoch - 143ms/step
Epoch 8/1000
2023-09-26 13:28:23.906 
Epoch 8/1000 
	 loss: 22.6363, MinusLogProbMetric: 22.6363, val_loss: 22.8171, val_MinusLogProbMetric: 22.8171

Epoch 8: val_loss improved from 23.17709 to 22.81712, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_289/weights/best_weights.h5
196/196 - 27s - loss: 22.6363 - MinusLogProbMetric: 22.6363 - val_loss: 22.8171 - val_MinusLogProbMetric: 22.8171 - lr: 0.0010 - 27s/epoch - 140ms/step
Epoch 9/1000
2023-09-26 13:28:56.233 
Epoch 9/1000 
	 loss: 22.0081, MinusLogProbMetric: 22.0081, val_loss: 22.3631, val_MinusLogProbMetric: 22.3631

Epoch 9: val_loss improved from 22.81712 to 22.36305, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_289/weights/best_weights.h5
196/196 - 32s - loss: 22.0081 - MinusLogProbMetric: 22.0081 - val_loss: 22.3631 - val_MinusLogProbMetric: 22.3631 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 10/1000
2023-09-26 13:29:30.848 
Epoch 10/1000 
	 loss: 22.1057, MinusLogProbMetric: 22.1057, val_loss: 22.4642, val_MinusLogProbMetric: 22.4642

Epoch 10: val_loss did not improve from 22.36305
196/196 - 34s - loss: 22.1057 - MinusLogProbMetric: 22.1057 - val_loss: 22.4642 - val_MinusLogProbMetric: 22.4642 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 11/1000
2023-09-26 13:30:04.641 
Epoch 11/1000 
	 loss: 21.4646, MinusLogProbMetric: 21.4646, val_loss: 21.5412, val_MinusLogProbMetric: 21.5412

Epoch 11: val_loss improved from 22.36305 to 21.54117, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_289/weights/best_weights.h5
196/196 - 35s - loss: 21.4646 - MinusLogProbMetric: 21.4646 - val_loss: 21.5412 - val_MinusLogProbMetric: 21.5412 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 12/1000
2023-09-26 13:30:36.544 
Epoch 12/1000 
	 loss: 21.4507, MinusLogProbMetric: 21.4507, val_loss: 22.0373, val_MinusLogProbMetric: 22.0373

Epoch 12: val_loss did not improve from 21.54117
196/196 - 31s - loss: 21.4507 - MinusLogProbMetric: 21.4507 - val_loss: 22.0373 - val_MinusLogProbMetric: 22.0373 - lr: 0.0010 - 31s/epoch - 159ms/step
Epoch 13/1000
2023-09-26 13:31:10.873 
Epoch 13/1000 
	 loss: 21.1113, MinusLogProbMetric: 21.1113, val_loss: 21.2914, val_MinusLogProbMetric: 21.2914

Epoch 13: val_loss improved from 21.54117 to 21.29142, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_289/weights/best_weights.h5
196/196 - 35s - loss: 21.1113 - MinusLogProbMetric: 21.1113 - val_loss: 21.2914 - val_MinusLogProbMetric: 21.2914 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 14/1000
2023-09-26 13:31:45.519 
Epoch 14/1000 
	 loss: 20.9282, MinusLogProbMetric: 20.9282, val_loss: 20.5716, val_MinusLogProbMetric: 20.5716

Epoch 14: val_loss improved from 21.29142 to 20.57164, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_289/weights/best_weights.h5
196/196 - 35s - loss: 20.9282 - MinusLogProbMetric: 20.9282 - val_loss: 20.5716 - val_MinusLogProbMetric: 20.5716 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 15/1000
2023-09-26 13:32:20.481 
Epoch 15/1000 
	 loss: 20.6794, MinusLogProbMetric: 20.6794, val_loss: 20.1541, val_MinusLogProbMetric: 20.1541

Epoch 15: val_loss improved from 20.57164 to 20.15410, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_289/weights/best_weights.h5
196/196 - 35s - loss: 20.6794 - MinusLogProbMetric: 20.6794 - val_loss: 20.1541 - val_MinusLogProbMetric: 20.1541 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 16/1000
2023-09-26 13:32:55.313 
Epoch 16/1000 
	 loss: 20.5929, MinusLogProbMetric: 20.5929, val_loss: 21.1741, val_MinusLogProbMetric: 21.1741

Epoch 16: val_loss did not improve from 20.15410
196/196 - 34s - loss: 20.5929 - MinusLogProbMetric: 20.5929 - val_loss: 21.1741 - val_MinusLogProbMetric: 21.1741 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 17/1000
2023-09-26 13:33:29.438 
Epoch 17/1000 
	 loss: 20.4696, MinusLogProbMetric: 20.4696, val_loss: 23.1801, val_MinusLogProbMetric: 23.1801

Epoch 17: val_loss did not improve from 20.15410
196/196 - 34s - loss: 20.4696 - MinusLogProbMetric: 20.4696 - val_loss: 23.1801 - val_MinusLogProbMetric: 23.1801 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 18/1000
2023-09-26 13:34:03.444 
Epoch 18/1000 
	 loss: 20.4364, MinusLogProbMetric: 20.4364, val_loss: 19.7620, val_MinusLogProbMetric: 19.7620

Epoch 18: val_loss improved from 20.15410 to 19.76204, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_289/weights/best_weights.h5
196/196 - 35s - loss: 20.4364 - MinusLogProbMetric: 20.4364 - val_loss: 19.7620 - val_MinusLogProbMetric: 19.7620 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 19/1000
2023-09-26 13:34:37.904 
Epoch 19/1000 
	 loss: 20.1220, MinusLogProbMetric: 20.1220, val_loss: 20.0388, val_MinusLogProbMetric: 20.0388

Epoch 19: val_loss did not improve from 19.76204
196/196 - 34s - loss: 20.1220 - MinusLogProbMetric: 20.1220 - val_loss: 20.0388 - val_MinusLogProbMetric: 20.0388 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 20/1000
2023-09-26 13:35:12.043 
Epoch 20/1000 
	 loss: 20.2761, MinusLogProbMetric: 20.2761, val_loss: 20.1105, val_MinusLogProbMetric: 20.1105

Epoch 20: val_loss did not improve from 19.76204
196/196 - 34s - loss: 20.2761 - MinusLogProbMetric: 20.2761 - val_loss: 20.1105 - val_MinusLogProbMetric: 20.1105 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 21/1000
2023-09-26 13:35:46.363 
Epoch 21/1000 
	 loss: 20.0488, MinusLogProbMetric: 20.0488, val_loss: 19.6488, val_MinusLogProbMetric: 19.6488

Epoch 21: val_loss improved from 19.76204 to 19.64883, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_289/weights/best_weights.h5
196/196 - 35s - loss: 20.0488 - MinusLogProbMetric: 20.0488 - val_loss: 19.6488 - val_MinusLogProbMetric: 19.6488 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 22/1000
2023-09-26 13:36:21.094 
Epoch 22/1000 
	 loss: 19.8201, MinusLogProbMetric: 19.8201, val_loss: 20.7959, val_MinusLogProbMetric: 20.7959

Epoch 22: val_loss did not improve from 19.64883
196/196 - 34s - loss: 19.8201 - MinusLogProbMetric: 19.8201 - val_loss: 20.7959 - val_MinusLogProbMetric: 20.7959 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 23/1000
2023-09-26 13:36:55.394 
Epoch 23/1000 
	 loss: 19.7361, MinusLogProbMetric: 19.7361, val_loss: 19.3767, val_MinusLogProbMetric: 19.3767

Epoch 23: val_loss improved from 19.64883 to 19.37673, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_289/weights/best_weights.h5
196/196 - 35s - loss: 19.7361 - MinusLogProbMetric: 19.7361 - val_loss: 19.3767 - val_MinusLogProbMetric: 19.3767 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 24/1000
2023-09-26 13:37:30.161 
Epoch 24/1000 
	 loss: 19.7461, MinusLogProbMetric: 19.7461, val_loss: 20.2617, val_MinusLogProbMetric: 20.2617

Epoch 24: val_loss did not improve from 19.37673
196/196 - 34s - loss: 19.7461 - MinusLogProbMetric: 19.7461 - val_loss: 20.2617 - val_MinusLogProbMetric: 20.2617 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 25/1000
2023-09-26 13:38:04.315 
Epoch 25/1000 
	 loss: 19.6046, MinusLogProbMetric: 19.6046, val_loss: 19.2935, val_MinusLogProbMetric: 19.2935

Epoch 25: val_loss improved from 19.37673 to 19.29348, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_289/weights/best_weights.h5
196/196 - 35s - loss: 19.6046 - MinusLogProbMetric: 19.6046 - val_loss: 19.2935 - val_MinusLogProbMetric: 19.2935 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 26/1000
2023-09-26 13:38:39.224 
Epoch 26/1000 
	 loss: 19.4921, MinusLogProbMetric: 19.4921, val_loss: 19.9682, val_MinusLogProbMetric: 19.9682

Epoch 26: val_loss did not improve from 19.29348
196/196 - 34s - loss: 19.4921 - MinusLogProbMetric: 19.4921 - val_loss: 19.9682 - val_MinusLogProbMetric: 19.9682 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 27/1000
2023-09-26 13:39:13.346 
Epoch 27/1000 
	 loss: 19.4667, MinusLogProbMetric: 19.4667, val_loss: 19.4367, val_MinusLogProbMetric: 19.4367

Epoch 27: val_loss did not improve from 19.29348
196/196 - 34s - loss: 19.4667 - MinusLogProbMetric: 19.4667 - val_loss: 19.4367 - val_MinusLogProbMetric: 19.4367 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 28/1000
2023-09-26 13:39:47.671 
Epoch 28/1000 
	 loss: 19.4125, MinusLogProbMetric: 19.4125, val_loss: 20.0109, val_MinusLogProbMetric: 20.0109

Epoch 28: val_loss did not improve from 19.29348
196/196 - 34s - loss: 19.4125 - MinusLogProbMetric: 19.4125 - val_loss: 20.0109 - val_MinusLogProbMetric: 20.0109 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 29/1000
2023-09-26 13:40:21.608 
Epoch 29/1000 
	 loss: 19.4711, MinusLogProbMetric: 19.4711, val_loss: 19.5679, val_MinusLogProbMetric: 19.5679

Epoch 29: val_loss did not improve from 19.29348
196/196 - 34s - loss: 19.4711 - MinusLogProbMetric: 19.4711 - val_loss: 19.5679 - val_MinusLogProbMetric: 19.5679 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 30/1000
2023-09-26 13:40:55.209 
Epoch 30/1000 
	 loss: 19.5486, MinusLogProbMetric: 19.5486, val_loss: 19.5420, val_MinusLogProbMetric: 19.5420

Epoch 30: val_loss did not improve from 19.29348
196/196 - 34s - loss: 19.5486 - MinusLogProbMetric: 19.5486 - val_loss: 19.5420 - val_MinusLogProbMetric: 19.5420 - lr: 0.0010 - 34s/epoch - 171ms/step
Epoch 31/1000
2023-09-26 13:41:29.305 
Epoch 31/1000 
	 loss: 19.2535, MinusLogProbMetric: 19.2535, val_loss: 19.2829, val_MinusLogProbMetric: 19.2829

Epoch 31: val_loss improved from 19.29348 to 19.28294, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_289/weights/best_weights.h5
196/196 - 35s - loss: 19.2535 - MinusLogProbMetric: 19.2535 - val_loss: 19.2829 - val_MinusLogProbMetric: 19.2829 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 32/1000
2023-09-26 13:42:04.022 
Epoch 32/1000 
	 loss: 19.1422, MinusLogProbMetric: 19.1422, val_loss: 19.1273, val_MinusLogProbMetric: 19.1273

Epoch 32: val_loss improved from 19.28294 to 19.12725, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_289/weights/best_weights.h5
196/196 - 35s - loss: 19.1422 - MinusLogProbMetric: 19.1422 - val_loss: 19.1273 - val_MinusLogProbMetric: 19.1273 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 33/1000
2023-09-26 13:42:38.739 
Epoch 33/1000 
	 loss: 19.1448, MinusLogProbMetric: 19.1448, val_loss: 21.1985, val_MinusLogProbMetric: 21.1985

Epoch 33: val_loss did not improve from 19.12725
196/196 - 34s - loss: 19.1448 - MinusLogProbMetric: 19.1448 - val_loss: 21.1985 - val_MinusLogProbMetric: 21.1985 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 34/1000
2023-09-26 13:43:12.897 
Epoch 34/1000 
	 loss: 19.1806, MinusLogProbMetric: 19.1806, val_loss: 18.7517, val_MinusLogProbMetric: 18.7517

Epoch 34: val_loss improved from 19.12725 to 18.75167, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_289/weights/best_weights.h5
196/196 - 35s - loss: 19.1806 - MinusLogProbMetric: 19.1806 - val_loss: 18.7517 - val_MinusLogProbMetric: 18.7517 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 35/1000
2023-09-26 13:43:47.576 
Epoch 35/1000 
	 loss: 19.1959, MinusLogProbMetric: 19.1959, val_loss: 19.1544, val_MinusLogProbMetric: 19.1544

Epoch 35: val_loss did not improve from 18.75167
196/196 - 34s - loss: 19.1959 - MinusLogProbMetric: 19.1959 - val_loss: 19.1544 - val_MinusLogProbMetric: 19.1544 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 36/1000
2023-09-26 13:44:21.455 
Epoch 36/1000 
	 loss: 19.0348, MinusLogProbMetric: 19.0348, val_loss: 19.6363, val_MinusLogProbMetric: 19.6363

Epoch 36: val_loss did not improve from 18.75167
196/196 - 34s - loss: 19.0348 - MinusLogProbMetric: 19.0348 - val_loss: 19.6363 - val_MinusLogProbMetric: 19.6363 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 37/1000
2023-09-26 13:44:55.492 
Epoch 37/1000 
	 loss: 19.0307, MinusLogProbMetric: 19.0307, val_loss: 18.7382, val_MinusLogProbMetric: 18.7382

Epoch 37: val_loss improved from 18.75167 to 18.73819, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_289/weights/best_weights.h5
196/196 - 35s - loss: 19.0307 - MinusLogProbMetric: 19.0307 - val_loss: 18.7382 - val_MinusLogProbMetric: 18.7382 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 38/1000
2023-09-26 13:45:30.821 
Epoch 38/1000 
	 loss: 18.9227, MinusLogProbMetric: 18.9227, val_loss: 18.4789, val_MinusLogProbMetric: 18.4789

Epoch 38: val_loss improved from 18.73819 to 18.47894, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_289/weights/best_weights.h5
196/196 - 35s - loss: 18.9227 - MinusLogProbMetric: 18.9227 - val_loss: 18.4789 - val_MinusLogProbMetric: 18.4789 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 39/1000
2023-09-26 13:46:05.555 
Epoch 39/1000 
	 loss: 18.9992, MinusLogProbMetric: 18.9992, val_loss: 18.7813, val_MinusLogProbMetric: 18.7813

Epoch 39: val_loss did not improve from 18.47894
196/196 - 34s - loss: 18.9992 - MinusLogProbMetric: 18.9992 - val_loss: 18.7813 - val_MinusLogProbMetric: 18.7813 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 40/1000
2023-09-26 13:46:39.729 
Epoch 40/1000 
	 loss: 18.9591, MinusLogProbMetric: 18.9591, val_loss: 18.8478, val_MinusLogProbMetric: 18.8478

Epoch 40: val_loss did not improve from 18.47894
196/196 - 34s - loss: 18.9591 - MinusLogProbMetric: 18.9591 - val_loss: 18.8478 - val_MinusLogProbMetric: 18.8478 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 41/1000
2023-09-26 13:47:13.655 
Epoch 41/1000 
	 loss: 18.8758, MinusLogProbMetric: 18.8758, val_loss: 19.0483, val_MinusLogProbMetric: 19.0483

Epoch 41: val_loss did not improve from 18.47894
196/196 - 34s - loss: 18.8758 - MinusLogProbMetric: 18.8758 - val_loss: 19.0483 - val_MinusLogProbMetric: 19.0483 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 42/1000
2023-09-26 13:47:47.888 
Epoch 42/1000 
	 loss: 18.7077, MinusLogProbMetric: 18.7077, val_loss: 20.0163, val_MinusLogProbMetric: 20.0163

Epoch 42: val_loss did not improve from 18.47894
196/196 - 34s - loss: 18.7077 - MinusLogProbMetric: 18.7077 - val_loss: 20.0163 - val_MinusLogProbMetric: 20.0163 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 43/1000
2023-09-26 13:48:22.073 
Epoch 43/1000 
	 loss: 18.9778, MinusLogProbMetric: 18.9778, val_loss: 18.8885, val_MinusLogProbMetric: 18.8885

Epoch 43: val_loss did not improve from 18.47894
196/196 - 34s - loss: 18.9778 - MinusLogProbMetric: 18.9778 - val_loss: 18.8885 - val_MinusLogProbMetric: 18.8885 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 44/1000
2023-09-26 13:48:56.035 
Epoch 44/1000 
	 loss: 18.9206, MinusLogProbMetric: 18.9206, val_loss: 19.0368, val_MinusLogProbMetric: 19.0368

Epoch 44: val_loss did not improve from 18.47894
196/196 - 34s - loss: 18.9206 - MinusLogProbMetric: 18.9206 - val_loss: 19.0368 - val_MinusLogProbMetric: 19.0368 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 45/1000
2023-09-26 13:49:30.036 
Epoch 45/1000 
	 loss: 18.7105, MinusLogProbMetric: 18.7105, val_loss: 19.4683, val_MinusLogProbMetric: 19.4683

Epoch 45: val_loss did not improve from 18.47894
196/196 - 34s - loss: 18.7105 - MinusLogProbMetric: 18.7105 - val_loss: 19.4683 - val_MinusLogProbMetric: 19.4683 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 46/1000
2023-09-26 13:50:04.465 
Epoch 46/1000 
	 loss: 18.6790, MinusLogProbMetric: 18.6790, val_loss: 18.8204, val_MinusLogProbMetric: 18.8204

Epoch 46: val_loss did not improve from 18.47894
196/196 - 34s - loss: 18.6790 - MinusLogProbMetric: 18.6790 - val_loss: 18.8204 - val_MinusLogProbMetric: 18.8204 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 47/1000
2023-09-26 13:50:38.605 
Epoch 47/1000 
	 loss: 18.5114, MinusLogProbMetric: 18.5114, val_loss: 19.1759, val_MinusLogProbMetric: 19.1759

Epoch 47: val_loss did not improve from 18.47894
196/196 - 34s - loss: 18.5114 - MinusLogProbMetric: 18.5114 - val_loss: 19.1759 - val_MinusLogProbMetric: 19.1759 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 48/1000
2023-09-26 13:51:12.465 
Epoch 48/1000 
	 loss: 18.7048, MinusLogProbMetric: 18.7048, val_loss: 19.2065, val_MinusLogProbMetric: 19.2065

Epoch 48: val_loss did not improve from 18.47894
196/196 - 34s - loss: 18.7048 - MinusLogProbMetric: 18.7048 - val_loss: 19.2065 - val_MinusLogProbMetric: 19.2065 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 49/1000
2023-09-26 13:51:46.623 
Epoch 49/1000 
	 loss: 18.7319, MinusLogProbMetric: 18.7319, val_loss: 19.2396, val_MinusLogProbMetric: 19.2396

Epoch 49: val_loss did not improve from 18.47894
196/196 - 34s - loss: 18.7319 - MinusLogProbMetric: 18.7319 - val_loss: 19.2396 - val_MinusLogProbMetric: 19.2396 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 50/1000
2023-09-26 13:52:20.869 
Epoch 50/1000 
	 loss: 18.4788, MinusLogProbMetric: 18.4788, val_loss: 19.0141, val_MinusLogProbMetric: 19.0141

Epoch 50: val_loss did not improve from 18.47894
196/196 - 34s - loss: 18.4788 - MinusLogProbMetric: 18.4788 - val_loss: 19.0141 - val_MinusLogProbMetric: 19.0141 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 51/1000
2023-09-26 13:52:55.024 
Epoch 51/1000 
	 loss: 18.5600, MinusLogProbMetric: 18.5600, val_loss: 18.6301, val_MinusLogProbMetric: 18.6301

Epoch 51: val_loss did not improve from 18.47894
196/196 - 34s - loss: 18.5600 - MinusLogProbMetric: 18.5600 - val_loss: 18.6301 - val_MinusLogProbMetric: 18.6301 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 52/1000
2023-09-26 13:53:29.356 
Epoch 52/1000 
	 loss: 18.4610, MinusLogProbMetric: 18.4610, val_loss: 18.6763, val_MinusLogProbMetric: 18.6763

Epoch 52: val_loss did not improve from 18.47894
196/196 - 34s - loss: 18.4610 - MinusLogProbMetric: 18.4610 - val_loss: 18.6763 - val_MinusLogProbMetric: 18.6763 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 53/1000
2023-09-26 13:54:03.459 
Epoch 53/1000 
	 loss: 18.5005, MinusLogProbMetric: 18.5005, val_loss: 18.5106, val_MinusLogProbMetric: 18.5106

Epoch 53: val_loss did not improve from 18.47894
196/196 - 34s - loss: 18.5005 - MinusLogProbMetric: 18.5005 - val_loss: 18.5106 - val_MinusLogProbMetric: 18.5106 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 54/1000
2023-09-26 13:54:38.111 
Epoch 54/1000 
	 loss: 18.4515, MinusLogProbMetric: 18.4515, val_loss: 18.8151, val_MinusLogProbMetric: 18.8151

Epoch 54: val_loss did not improve from 18.47894
196/196 - 35s - loss: 18.4515 - MinusLogProbMetric: 18.4515 - val_loss: 18.8151 - val_MinusLogProbMetric: 18.8151 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 55/1000
2023-09-26 13:55:11.985 
Epoch 55/1000 
	 loss: 18.5321, MinusLogProbMetric: 18.5321, val_loss: 18.3310, val_MinusLogProbMetric: 18.3310

Epoch 55: val_loss improved from 18.47894 to 18.33101, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_289/weights/best_weights.h5
196/196 - 34s - loss: 18.5321 - MinusLogProbMetric: 18.5321 - val_loss: 18.3310 - val_MinusLogProbMetric: 18.3310 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 56/1000
2023-09-26 13:55:46.739 
Epoch 56/1000 
	 loss: 18.4891, MinusLogProbMetric: 18.4891, val_loss: 18.5450, val_MinusLogProbMetric: 18.5450

Epoch 56: val_loss did not improve from 18.33101
196/196 - 34s - loss: 18.4891 - MinusLogProbMetric: 18.4891 - val_loss: 18.5450 - val_MinusLogProbMetric: 18.5450 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 57/1000
2023-09-26 13:56:20.747 
Epoch 57/1000 
	 loss: 18.5062, MinusLogProbMetric: 18.5062, val_loss: 18.6961, val_MinusLogProbMetric: 18.6961

Epoch 57: val_loss did not improve from 18.33101
196/196 - 34s - loss: 18.5062 - MinusLogProbMetric: 18.5062 - val_loss: 18.6961 - val_MinusLogProbMetric: 18.6961 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 58/1000
2023-09-26 13:56:55.165 
Epoch 58/1000 
	 loss: 18.3693, MinusLogProbMetric: 18.3693, val_loss: 18.6923, val_MinusLogProbMetric: 18.6923

Epoch 58: val_loss did not improve from 18.33101
196/196 - 34s - loss: 18.3693 - MinusLogProbMetric: 18.3693 - val_loss: 18.6923 - val_MinusLogProbMetric: 18.6923 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 59/1000
2023-09-26 13:57:29.581 
Epoch 59/1000 
	 loss: 18.6213, MinusLogProbMetric: 18.6213, val_loss: 19.5091, val_MinusLogProbMetric: 19.5091

Epoch 59: val_loss did not improve from 18.33101
196/196 - 34s - loss: 18.6213 - MinusLogProbMetric: 18.6213 - val_loss: 19.5091 - val_MinusLogProbMetric: 19.5091 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 60/1000
2023-09-26 13:58:03.630 
Epoch 60/1000 
	 loss: 18.3580, MinusLogProbMetric: 18.3580, val_loss: 18.2219, val_MinusLogProbMetric: 18.2219

Epoch 60: val_loss improved from 18.33101 to 18.22186, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_289/weights/best_weights.h5
196/196 - 35s - loss: 18.3580 - MinusLogProbMetric: 18.3580 - val_loss: 18.2219 - val_MinusLogProbMetric: 18.2219 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 61/1000
2023-09-26 13:58:38.713 
Epoch 61/1000 
	 loss: 18.3798, MinusLogProbMetric: 18.3798, val_loss: 18.8867, val_MinusLogProbMetric: 18.8867

Epoch 61: val_loss did not improve from 18.22186
196/196 - 34s - loss: 18.3798 - MinusLogProbMetric: 18.3798 - val_loss: 18.8867 - val_MinusLogProbMetric: 18.8867 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 62/1000
2023-09-26 13:59:12.974 
Epoch 62/1000 
	 loss: 18.3318, MinusLogProbMetric: 18.3318, val_loss: 18.4906, val_MinusLogProbMetric: 18.4906

Epoch 62: val_loss did not improve from 18.22186
196/196 - 34s - loss: 18.3318 - MinusLogProbMetric: 18.3318 - val_loss: 18.4906 - val_MinusLogProbMetric: 18.4906 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 63/1000
2023-09-26 13:59:47.128 
Epoch 63/1000 
	 loss: 18.3216, MinusLogProbMetric: 18.3216, val_loss: 18.2991, val_MinusLogProbMetric: 18.2991

Epoch 63: val_loss did not improve from 18.22186
196/196 - 34s - loss: 18.3216 - MinusLogProbMetric: 18.3216 - val_loss: 18.2991 - val_MinusLogProbMetric: 18.2991 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 64/1000
2023-09-26 14:00:21.239 
Epoch 64/1000 
	 loss: 18.2811, MinusLogProbMetric: 18.2811, val_loss: 18.2621, val_MinusLogProbMetric: 18.2621

Epoch 64: val_loss did not improve from 18.22186
196/196 - 34s - loss: 18.2811 - MinusLogProbMetric: 18.2811 - val_loss: 18.2621 - val_MinusLogProbMetric: 18.2621 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 65/1000
2023-09-26 14:00:55.320 
Epoch 65/1000 
	 loss: 18.2588, MinusLogProbMetric: 18.2588, val_loss: 18.4571, val_MinusLogProbMetric: 18.4571

Epoch 65: val_loss did not improve from 18.22186
196/196 - 34s - loss: 18.2588 - MinusLogProbMetric: 18.2588 - val_loss: 18.4571 - val_MinusLogProbMetric: 18.4571 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 66/1000
2023-09-26 14:01:29.513 
Epoch 66/1000 
	 loss: 18.3561, MinusLogProbMetric: 18.3561, val_loss: 18.1175, val_MinusLogProbMetric: 18.1175

Epoch 66: val_loss improved from 18.22186 to 18.11751, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_289/weights/best_weights.h5
196/196 - 35s - loss: 18.3561 - MinusLogProbMetric: 18.3561 - val_loss: 18.1175 - val_MinusLogProbMetric: 18.1175 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 67/1000
2023-09-26 14:02:04.430 
Epoch 67/1000 
	 loss: 18.1825, MinusLogProbMetric: 18.1825, val_loss: 18.6759, val_MinusLogProbMetric: 18.6759

Epoch 67: val_loss did not improve from 18.11751
196/196 - 34s - loss: 18.1825 - MinusLogProbMetric: 18.1825 - val_loss: 18.6759 - val_MinusLogProbMetric: 18.6759 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 68/1000
2023-09-26 14:02:38.304 
Epoch 68/1000 
	 loss: 18.2119, MinusLogProbMetric: 18.2119, val_loss: 18.5241, val_MinusLogProbMetric: 18.5241

Epoch 68: val_loss did not improve from 18.11751
196/196 - 34s - loss: 18.2119 - MinusLogProbMetric: 18.2119 - val_loss: 18.5241 - val_MinusLogProbMetric: 18.5241 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 69/1000
2023-09-26 14:03:12.311 
Epoch 69/1000 
	 loss: 18.0791, MinusLogProbMetric: 18.0791, val_loss: 18.4689, val_MinusLogProbMetric: 18.4689

Epoch 69: val_loss did not improve from 18.11751
196/196 - 34s - loss: 18.0791 - MinusLogProbMetric: 18.0791 - val_loss: 18.4689 - val_MinusLogProbMetric: 18.4689 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 70/1000
2023-09-26 14:03:46.323 
Epoch 70/1000 
	 loss: 18.1883, MinusLogProbMetric: 18.1883, val_loss: 18.9352, val_MinusLogProbMetric: 18.9352

Epoch 70: val_loss did not improve from 18.11751
196/196 - 34s - loss: 18.1883 - MinusLogProbMetric: 18.1883 - val_loss: 18.9352 - val_MinusLogProbMetric: 18.9352 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 71/1000
2023-09-26 14:04:20.561 
Epoch 71/1000 
	 loss: 18.2512, MinusLogProbMetric: 18.2512, val_loss: 18.7180, val_MinusLogProbMetric: 18.7180

Epoch 71: val_loss did not improve from 18.11751
196/196 - 34s - loss: 18.2512 - MinusLogProbMetric: 18.2512 - val_loss: 18.7180 - val_MinusLogProbMetric: 18.7180 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 72/1000
2023-09-26 14:04:55.035 
Epoch 72/1000 
	 loss: 18.1205, MinusLogProbMetric: 18.1205, val_loss: 18.3188, val_MinusLogProbMetric: 18.3188

Epoch 72: val_loss did not improve from 18.11751
196/196 - 34s - loss: 18.1205 - MinusLogProbMetric: 18.1205 - val_loss: 18.3188 - val_MinusLogProbMetric: 18.3188 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 73/1000
2023-09-26 14:05:29.368 
Epoch 73/1000 
	 loss: 18.2262, MinusLogProbMetric: 18.2262, val_loss: 18.1141, val_MinusLogProbMetric: 18.1141

Epoch 73: val_loss improved from 18.11751 to 18.11413, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_289/weights/best_weights.h5
196/196 - 35s - loss: 18.2262 - MinusLogProbMetric: 18.2262 - val_loss: 18.1141 - val_MinusLogProbMetric: 18.1141 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 74/1000
2023-09-26 14:06:03.900 
Epoch 74/1000 
	 loss: 18.0624, MinusLogProbMetric: 18.0624, val_loss: 18.6908, val_MinusLogProbMetric: 18.6908

Epoch 74: val_loss did not improve from 18.11413
196/196 - 34s - loss: 18.0624 - MinusLogProbMetric: 18.0624 - val_loss: 18.6908 - val_MinusLogProbMetric: 18.6908 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 75/1000
2023-09-26 14:06:38.063 
Epoch 75/1000 
	 loss: 18.0872, MinusLogProbMetric: 18.0872, val_loss: 18.3240, val_MinusLogProbMetric: 18.3240

Epoch 75: val_loss did not improve from 18.11413
196/196 - 34s - loss: 18.0872 - MinusLogProbMetric: 18.0872 - val_loss: 18.3240 - val_MinusLogProbMetric: 18.3240 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 76/1000
2023-09-26 14:07:12.170 
Epoch 76/1000 
	 loss: 17.9943, MinusLogProbMetric: 17.9943, val_loss: 18.9377, val_MinusLogProbMetric: 18.9377

Epoch 76: val_loss did not improve from 18.11413
196/196 - 34s - loss: 17.9943 - MinusLogProbMetric: 17.9943 - val_loss: 18.9377 - val_MinusLogProbMetric: 18.9377 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 77/1000
2023-09-26 14:07:46.680 
Epoch 77/1000 
	 loss: 18.2314, MinusLogProbMetric: 18.2314, val_loss: 18.1639, val_MinusLogProbMetric: 18.1639

Epoch 77: val_loss did not improve from 18.11413
196/196 - 35s - loss: 18.2314 - MinusLogProbMetric: 18.2314 - val_loss: 18.1639 - val_MinusLogProbMetric: 18.1639 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 78/1000
2023-09-26 14:08:20.850 
Epoch 78/1000 
	 loss: 18.0643, MinusLogProbMetric: 18.0643, val_loss: 17.9504, val_MinusLogProbMetric: 17.9504

Epoch 78: val_loss improved from 18.11413 to 17.95043, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_289/weights/best_weights.h5
196/196 - 35s - loss: 18.0643 - MinusLogProbMetric: 18.0643 - val_loss: 17.9504 - val_MinusLogProbMetric: 17.9504 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 79/1000
2023-09-26 14:08:55.307 
Epoch 79/1000 
	 loss: 17.9496, MinusLogProbMetric: 17.9496, val_loss: 18.5086, val_MinusLogProbMetric: 18.5086

Epoch 79: val_loss did not improve from 17.95043
196/196 - 34s - loss: 17.9496 - MinusLogProbMetric: 17.9496 - val_loss: 18.5086 - val_MinusLogProbMetric: 18.5086 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 80/1000
2023-09-26 14:09:29.311 
Epoch 80/1000 
	 loss: 18.0100, MinusLogProbMetric: 18.0100, val_loss: 19.4910, val_MinusLogProbMetric: 19.4910

Epoch 80: val_loss did not improve from 17.95043
196/196 - 34s - loss: 18.0100 - MinusLogProbMetric: 18.0100 - val_loss: 19.4910 - val_MinusLogProbMetric: 19.4910 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 81/1000
2023-09-26 14:10:03.150 
Epoch 81/1000 
	 loss: 18.0767, MinusLogProbMetric: 18.0767, val_loss: 18.4910, val_MinusLogProbMetric: 18.4910

Epoch 81: val_loss did not improve from 17.95043
196/196 - 34s - loss: 18.0767 - MinusLogProbMetric: 18.0767 - val_loss: 18.4910 - val_MinusLogProbMetric: 18.4910 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 82/1000
2023-09-26 14:10:36.963 
Epoch 82/1000 
	 loss: 17.8999, MinusLogProbMetric: 17.8999, val_loss: 20.3295, val_MinusLogProbMetric: 20.3295

Epoch 82: val_loss did not improve from 17.95043
196/196 - 34s - loss: 17.8999 - MinusLogProbMetric: 17.8999 - val_loss: 20.3295 - val_MinusLogProbMetric: 20.3295 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 83/1000
2023-09-26 14:11:10.905 
Epoch 83/1000 
	 loss: 18.0473, MinusLogProbMetric: 18.0473, val_loss: 18.1217, val_MinusLogProbMetric: 18.1217

Epoch 83: val_loss did not improve from 17.95043
196/196 - 34s - loss: 18.0473 - MinusLogProbMetric: 18.0473 - val_loss: 18.1217 - val_MinusLogProbMetric: 18.1217 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 84/1000
2023-09-26 14:11:45.104 
Epoch 84/1000 
	 loss: 17.9634, MinusLogProbMetric: 17.9634, val_loss: 17.9329, val_MinusLogProbMetric: 17.9329

Epoch 84: val_loss improved from 17.95043 to 17.93285, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_289/weights/best_weights.h5
196/196 - 35s - loss: 17.9634 - MinusLogProbMetric: 17.9634 - val_loss: 17.9329 - val_MinusLogProbMetric: 17.9329 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 85/1000
2023-09-26 14:12:19.526 
Epoch 85/1000 
	 loss: 17.9332, MinusLogProbMetric: 17.9332, val_loss: 17.6932, val_MinusLogProbMetric: 17.6932

Epoch 85: val_loss improved from 17.93285 to 17.69317, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_289/weights/best_weights.h5
196/196 - 34s - loss: 17.9332 - MinusLogProbMetric: 17.9332 - val_loss: 17.6932 - val_MinusLogProbMetric: 17.6932 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 86/1000
2023-09-26 14:12:54.185 
Epoch 86/1000 
	 loss: 17.8546, MinusLogProbMetric: 17.8546, val_loss: 18.6039, val_MinusLogProbMetric: 18.6039

Epoch 86: val_loss did not improve from 17.69317
196/196 - 34s - loss: 17.8546 - MinusLogProbMetric: 17.8546 - val_loss: 18.6039 - val_MinusLogProbMetric: 18.6039 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 87/1000
2023-09-26 14:13:28.294 
Epoch 87/1000 
	 loss: 17.9304, MinusLogProbMetric: 17.9304, val_loss: 17.7487, val_MinusLogProbMetric: 17.7487

Epoch 87: val_loss did not improve from 17.69317
196/196 - 34s - loss: 17.9304 - MinusLogProbMetric: 17.9304 - val_loss: 17.7487 - val_MinusLogProbMetric: 17.7487 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 88/1000
2023-09-26 14:14:02.352 
Epoch 88/1000 
	 loss: 17.8778, MinusLogProbMetric: 17.8778, val_loss: 18.6122, val_MinusLogProbMetric: 18.6122

Epoch 88: val_loss did not improve from 17.69317
196/196 - 34s - loss: 17.8778 - MinusLogProbMetric: 17.8778 - val_loss: 18.6122 - val_MinusLogProbMetric: 18.6122 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 89/1000
2023-09-26 14:14:36.285 
Epoch 89/1000 
	 loss: 17.9720, MinusLogProbMetric: 17.9720, val_loss: 18.7145, val_MinusLogProbMetric: 18.7145

Epoch 89: val_loss did not improve from 17.69317
196/196 - 34s - loss: 17.9720 - MinusLogProbMetric: 17.9720 - val_loss: 18.7145 - val_MinusLogProbMetric: 18.7145 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 90/1000
2023-09-26 14:15:10.584 
Epoch 90/1000 
	 loss: 17.8255, MinusLogProbMetric: 17.8255, val_loss: 18.1338, val_MinusLogProbMetric: 18.1338

Epoch 90: val_loss did not improve from 17.69317
196/196 - 34s - loss: 17.8255 - MinusLogProbMetric: 17.8255 - val_loss: 18.1338 - val_MinusLogProbMetric: 18.1338 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 91/1000
2023-09-26 14:15:44.867 
Epoch 91/1000 
	 loss: 17.8021, MinusLogProbMetric: 17.8021, val_loss: 17.9482, val_MinusLogProbMetric: 17.9482

Epoch 91: val_loss did not improve from 17.69317
196/196 - 34s - loss: 17.8021 - MinusLogProbMetric: 17.8021 - val_loss: 17.9482 - val_MinusLogProbMetric: 17.9482 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 92/1000
2023-09-26 14:16:19.097 
Epoch 92/1000 
	 loss: 17.7414, MinusLogProbMetric: 17.7414, val_loss: 20.9791, val_MinusLogProbMetric: 20.9791

Epoch 92: val_loss did not improve from 17.69317
196/196 - 34s - loss: 17.7414 - MinusLogProbMetric: 17.7414 - val_loss: 20.9791 - val_MinusLogProbMetric: 20.9791 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 93/1000
2023-09-26 14:16:53.052 
Epoch 93/1000 
	 loss: 17.8999, MinusLogProbMetric: 17.8999, val_loss: 19.0915, val_MinusLogProbMetric: 19.0915

Epoch 93: val_loss did not improve from 17.69317
196/196 - 34s - loss: 17.8999 - MinusLogProbMetric: 17.8999 - val_loss: 19.0915 - val_MinusLogProbMetric: 19.0915 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 94/1000
2023-09-26 14:17:27.096 
Epoch 94/1000 
	 loss: 17.7898, MinusLogProbMetric: 17.7898, val_loss: 17.9076, val_MinusLogProbMetric: 17.9076

Epoch 94: val_loss did not improve from 17.69317
196/196 - 34s - loss: 17.7898 - MinusLogProbMetric: 17.7898 - val_loss: 17.9076 - val_MinusLogProbMetric: 17.9076 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 95/1000
2023-09-26 14:18:00.902 
Epoch 95/1000 
	 loss: 17.9797, MinusLogProbMetric: 17.9797, val_loss: 17.5791, val_MinusLogProbMetric: 17.5791

Epoch 95: val_loss improved from 17.69317 to 17.57909, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_289/weights/best_weights.h5
196/196 - 34s - loss: 17.9797 - MinusLogProbMetric: 17.9797 - val_loss: 17.5791 - val_MinusLogProbMetric: 17.5791 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 96/1000
2023-09-26 14:18:35.065 
Epoch 96/1000 
	 loss: 17.7970, MinusLogProbMetric: 17.7970, val_loss: 17.6893, val_MinusLogProbMetric: 17.6893

Epoch 96: val_loss did not improve from 17.57909
196/196 - 34s - loss: 17.7970 - MinusLogProbMetric: 17.7970 - val_loss: 17.6893 - val_MinusLogProbMetric: 17.6893 - lr: 0.0010 - 34s/epoch - 171ms/step
Epoch 97/1000
2023-09-26 14:19:08.840 
Epoch 97/1000 
	 loss: 17.7806, MinusLogProbMetric: 17.7806, val_loss: 17.9515, val_MinusLogProbMetric: 17.9515

Epoch 97: val_loss did not improve from 17.57909
196/196 - 34s - loss: 17.7806 - MinusLogProbMetric: 17.7806 - val_loss: 17.9515 - val_MinusLogProbMetric: 17.9515 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 98/1000
2023-09-26 14:19:43.563 
Epoch 98/1000 
	 loss: 17.7187, MinusLogProbMetric: 17.7187, val_loss: 17.7472, val_MinusLogProbMetric: 17.7472

Epoch 98: val_loss did not improve from 17.57909
196/196 - 35s - loss: 17.7187 - MinusLogProbMetric: 17.7187 - val_loss: 17.7472 - val_MinusLogProbMetric: 17.7472 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 99/1000
2023-09-26 14:20:18.741 
Epoch 99/1000 
	 loss: 17.9976, MinusLogProbMetric: 17.9976, val_loss: 17.7390, val_MinusLogProbMetric: 17.7390

Epoch 99: val_loss did not improve from 17.57909
196/196 - 35s - loss: 17.9976 - MinusLogProbMetric: 17.9976 - val_loss: 17.7390 - val_MinusLogProbMetric: 17.7390 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 100/1000
2023-09-26 14:20:55.304 
Epoch 100/1000 
	 loss: 17.7156, MinusLogProbMetric: 17.7156, val_loss: 18.0070, val_MinusLogProbMetric: 18.0070

Epoch 100: val_loss did not improve from 17.57909
196/196 - 37s - loss: 17.7156 - MinusLogProbMetric: 17.7156 - val_loss: 18.0070 - val_MinusLogProbMetric: 18.0070 - lr: 0.0010 - 37s/epoch - 187ms/step
Epoch 101/1000
2023-09-26 14:21:32.213 
Epoch 101/1000 
	 loss: 17.8532, MinusLogProbMetric: 17.8532, val_loss: 18.3688, val_MinusLogProbMetric: 18.3688

Epoch 101: val_loss did not improve from 17.57909
196/196 - 37s - loss: 17.8532 - MinusLogProbMetric: 17.8532 - val_loss: 18.3688 - val_MinusLogProbMetric: 18.3688 - lr: 0.0010 - 37s/epoch - 188ms/step
Epoch 102/1000
2023-09-26 14:22:07.909 
Epoch 102/1000 
	 loss: 17.7942, MinusLogProbMetric: 17.7942, val_loss: 17.7640, val_MinusLogProbMetric: 17.7640

Epoch 102: val_loss did not improve from 17.57909
196/196 - 36s - loss: 17.7942 - MinusLogProbMetric: 17.7942 - val_loss: 17.7640 - val_MinusLogProbMetric: 17.7640 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 103/1000
2023-09-26 14:22:45.058 
Epoch 103/1000 
	 loss: 17.6624, MinusLogProbMetric: 17.6624, val_loss: 17.8873, val_MinusLogProbMetric: 17.8873

Epoch 103: val_loss did not improve from 17.57909
196/196 - 37s - loss: 17.6624 - MinusLogProbMetric: 17.6624 - val_loss: 17.8873 - val_MinusLogProbMetric: 17.8873 - lr: 0.0010 - 37s/epoch - 190ms/step
Epoch 104/1000
2023-09-26 14:23:20.307 
Epoch 104/1000 
	 loss: 17.8322, MinusLogProbMetric: 17.8322, val_loss: 17.6149, val_MinusLogProbMetric: 17.6149

Epoch 104: val_loss did not improve from 17.57909
196/196 - 35s - loss: 17.8322 - MinusLogProbMetric: 17.8322 - val_loss: 17.6149 - val_MinusLogProbMetric: 17.6149 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 105/1000
2023-09-26 14:23:56.314 
Epoch 105/1000 
	 loss: 17.7004, MinusLogProbMetric: 17.7004, val_loss: 17.9641, val_MinusLogProbMetric: 17.9641

Epoch 105: val_loss did not improve from 17.57909
196/196 - 36s - loss: 17.7004 - MinusLogProbMetric: 17.7004 - val_loss: 17.9641 - val_MinusLogProbMetric: 17.9641 - lr: 0.0010 - 36s/epoch - 184ms/step
Epoch 106/1000
2023-09-26 14:24:32.131 
Epoch 106/1000 
	 loss: 17.7674, MinusLogProbMetric: 17.7674, val_loss: 18.0018, val_MinusLogProbMetric: 18.0018

Epoch 106: val_loss did not improve from 17.57909
196/196 - 36s - loss: 17.7674 - MinusLogProbMetric: 17.7674 - val_loss: 18.0018 - val_MinusLogProbMetric: 18.0018 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 107/1000
2023-09-26 14:25:09.022 
Epoch 107/1000 
	 loss: 17.7355, MinusLogProbMetric: 17.7355, val_loss: 17.5129, val_MinusLogProbMetric: 17.5129

Epoch 107: val_loss improved from 17.57909 to 17.51286, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_289/weights/best_weights.h5
196/196 - 37s - loss: 17.7355 - MinusLogProbMetric: 17.7355 - val_loss: 17.5129 - val_MinusLogProbMetric: 17.5129 - lr: 0.0010 - 37s/epoch - 191ms/step
Epoch 108/1000
2023-09-26 14:25:45.958 
Epoch 108/1000 
	 loss: 17.7045, MinusLogProbMetric: 17.7045, val_loss: 17.6675, val_MinusLogProbMetric: 17.6675

Epoch 108: val_loss did not improve from 17.51286
196/196 - 36s - loss: 17.7045 - MinusLogProbMetric: 17.7045 - val_loss: 17.6675 - val_MinusLogProbMetric: 17.6675 - lr: 0.0010 - 36s/epoch - 185ms/step
Epoch 109/1000
2023-09-26 14:26:22.037 
Epoch 109/1000 
	 loss: 17.7984, MinusLogProbMetric: 17.7984, val_loss: 17.8637, val_MinusLogProbMetric: 17.8637

Epoch 109: val_loss did not improve from 17.51286
196/196 - 36s - loss: 17.7984 - MinusLogProbMetric: 17.7984 - val_loss: 17.8637 - val_MinusLogProbMetric: 17.8637 - lr: 0.0010 - 36s/epoch - 184ms/step
Epoch 110/1000
2023-09-26 14:26:58.598 
Epoch 110/1000 
	 loss: 17.6483, MinusLogProbMetric: 17.6483, val_loss: 19.7334, val_MinusLogProbMetric: 19.7334

Epoch 110: val_loss did not improve from 17.51286
196/196 - 37s - loss: 17.6483 - MinusLogProbMetric: 17.6483 - val_loss: 19.7334 - val_MinusLogProbMetric: 19.7334 - lr: 0.0010 - 37s/epoch - 187ms/step
Epoch 111/1000
2023-09-26 14:27:34.904 
Epoch 111/1000 
	 loss: 17.7793, MinusLogProbMetric: 17.7793, val_loss: 17.5373, val_MinusLogProbMetric: 17.5373

Epoch 111: val_loss did not improve from 17.51286
196/196 - 36s - loss: 17.7793 - MinusLogProbMetric: 17.7793 - val_loss: 17.5373 - val_MinusLogProbMetric: 17.5373 - lr: 0.0010 - 36s/epoch - 185ms/step
Epoch 112/1000
2023-09-26 14:28:10.664 
Epoch 112/1000 
	 loss: 17.5845, MinusLogProbMetric: 17.5845, val_loss: 17.7053, val_MinusLogProbMetric: 17.7053

Epoch 112: val_loss did not improve from 17.51286
196/196 - 36s - loss: 17.5845 - MinusLogProbMetric: 17.5845 - val_loss: 17.7053 - val_MinusLogProbMetric: 17.7053 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 113/1000
2023-09-26 14:28:45.768 
Epoch 113/1000 
	 loss: 17.7600, MinusLogProbMetric: 17.7600, val_loss: 17.5925, val_MinusLogProbMetric: 17.5925

Epoch 113: val_loss did not improve from 17.51286
196/196 - 35s - loss: 17.7600 - MinusLogProbMetric: 17.7600 - val_loss: 17.5925 - val_MinusLogProbMetric: 17.5925 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 114/1000
2023-09-26 14:29:22.640 
Epoch 114/1000 
	 loss: 17.5714, MinusLogProbMetric: 17.5714, val_loss: 17.6533, val_MinusLogProbMetric: 17.6533

Epoch 114: val_loss did not improve from 17.51286
196/196 - 37s - loss: 17.5714 - MinusLogProbMetric: 17.5714 - val_loss: 17.6533 - val_MinusLogProbMetric: 17.6533 - lr: 0.0010 - 37s/epoch - 188ms/step
Epoch 115/1000
2023-09-26 14:29:57.719 
Epoch 115/1000 
	 loss: 17.6619, MinusLogProbMetric: 17.6619, val_loss: 17.7900, val_MinusLogProbMetric: 17.7900

Epoch 115: val_loss did not improve from 17.51286
196/196 - 35s - loss: 17.6619 - MinusLogProbMetric: 17.6619 - val_loss: 17.7900 - val_MinusLogProbMetric: 17.7900 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 116/1000
2023-09-26 14:30:32.554 
Epoch 116/1000 
	 loss: 17.5674, MinusLogProbMetric: 17.5674, val_loss: 17.5470, val_MinusLogProbMetric: 17.5470

Epoch 116: val_loss did not improve from 17.51286
196/196 - 35s - loss: 17.5674 - MinusLogProbMetric: 17.5674 - val_loss: 17.5470 - val_MinusLogProbMetric: 17.5470 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 117/1000
2023-09-26 14:31:06.992 
Epoch 117/1000 
	 loss: 17.5482, MinusLogProbMetric: 17.5482, val_loss: 17.4709, val_MinusLogProbMetric: 17.4709

Epoch 117: val_loss improved from 17.51286 to 17.47094, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_289/weights/best_weights.h5
196/196 - 35s - loss: 17.5482 - MinusLogProbMetric: 17.5482 - val_loss: 17.4709 - val_MinusLogProbMetric: 17.4709 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 118/1000
2023-09-26 14:31:42.196 
Epoch 118/1000 
	 loss: 17.8507, MinusLogProbMetric: 17.8507, val_loss: 17.6647, val_MinusLogProbMetric: 17.6647

Epoch 118: val_loss did not improve from 17.47094
196/196 - 35s - loss: 17.8507 - MinusLogProbMetric: 17.8507 - val_loss: 17.6647 - val_MinusLogProbMetric: 17.6647 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 119/1000
2023-09-26 14:32:16.503 
Epoch 119/1000 
	 loss: 17.5688, MinusLogProbMetric: 17.5688, val_loss: 17.8069, val_MinusLogProbMetric: 17.8069

Epoch 119: val_loss did not improve from 17.47094
196/196 - 34s - loss: 17.5688 - MinusLogProbMetric: 17.5688 - val_loss: 17.8069 - val_MinusLogProbMetric: 17.8069 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 120/1000
2023-09-26 14:32:50.928 
Epoch 120/1000 
	 loss: 17.5656, MinusLogProbMetric: 17.5656, val_loss: 17.5284, val_MinusLogProbMetric: 17.5284

Epoch 120: val_loss did not improve from 17.47094
196/196 - 34s - loss: 17.5656 - MinusLogProbMetric: 17.5656 - val_loss: 17.5284 - val_MinusLogProbMetric: 17.5284 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 121/1000
2023-09-26 14:33:25.017 
Epoch 121/1000 
	 loss: 17.5297, MinusLogProbMetric: 17.5297, val_loss: 17.8609, val_MinusLogProbMetric: 17.8609

Epoch 121: val_loss did not improve from 17.47094
196/196 - 34s - loss: 17.5297 - MinusLogProbMetric: 17.5297 - val_loss: 17.8609 - val_MinusLogProbMetric: 17.8609 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 122/1000
2023-09-26 14:33:59.079 
Epoch 122/1000 
	 loss: 17.5033, MinusLogProbMetric: 17.5033, val_loss: 17.4350, val_MinusLogProbMetric: 17.4350

Epoch 122: val_loss improved from 17.47094 to 17.43499, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_289/weights/best_weights.h5
196/196 - 35s - loss: 17.5033 - MinusLogProbMetric: 17.5033 - val_loss: 17.4350 - val_MinusLogProbMetric: 17.4350 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 123/1000
2023-09-26 14:34:33.665 
Epoch 123/1000 
	 loss: 17.7303, MinusLogProbMetric: 17.7303, val_loss: 17.9899, val_MinusLogProbMetric: 17.9899

Epoch 123: val_loss did not improve from 17.43499
196/196 - 34s - loss: 17.7303 - MinusLogProbMetric: 17.7303 - val_loss: 17.9899 - val_MinusLogProbMetric: 17.9899 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 124/1000
2023-09-26 14:35:07.607 
Epoch 124/1000 
	 loss: 17.6521, MinusLogProbMetric: 17.6521, val_loss: 17.8287, val_MinusLogProbMetric: 17.8287

Epoch 124: val_loss did not improve from 17.43499
196/196 - 34s - loss: 17.6521 - MinusLogProbMetric: 17.6521 - val_loss: 17.8287 - val_MinusLogProbMetric: 17.8287 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 125/1000
2023-09-26 14:35:41.542 
Epoch 125/1000 
	 loss: 17.6829, MinusLogProbMetric: 17.6829, val_loss: 17.9989, val_MinusLogProbMetric: 17.9989

Epoch 125: val_loss did not improve from 17.43499
196/196 - 34s - loss: 17.6829 - MinusLogProbMetric: 17.6829 - val_loss: 17.9989 - val_MinusLogProbMetric: 17.9989 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 126/1000
2023-09-26 14:36:15.483 
Epoch 126/1000 
	 loss: 17.6126, MinusLogProbMetric: 17.6126, val_loss: 17.6505, val_MinusLogProbMetric: 17.6505

Epoch 126: val_loss did not improve from 17.43499
196/196 - 34s - loss: 17.6126 - MinusLogProbMetric: 17.6126 - val_loss: 17.6505 - val_MinusLogProbMetric: 17.6505 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 127/1000
2023-09-26 14:36:49.364 
Epoch 127/1000 
	 loss: 17.4812, MinusLogProbMetric: 17.4812, val_loss: 17.5790, val_MinusLogProbMetric: 17.5790

Epoch 127: val_loss did not improve from 17.43499
196/196 - 34s - loss: 17.4812 - MinusLogProbMetric: 17.4812 - val_loss: 17.5790 - val_MinusLogProbMetric: 17.5790 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 128/1000
2023-09-26 14:37:23.314 
Epoch 128/1000 
	 loss: 17.4779, MinusLogProbMetric: 17.4779, val_loss: 17.2582, val_MinusLogProbMetric: 17.2582

Epoch 128: val_loss improved from 17.43499 to 17.25822, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_289/weights/best_weights.h5
196/196 - 35s - loss: 17.4779 - MinusLogProbMetric: 17.4779 - val_loss: 17.2582 - val_MinusLogProbMetric: 17.2582 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 129/1000
2023-09-26 14:37:58.192 
Epoch 129/1000 
	 loss: 17.4405, MinusLogProbMetric: 17.4405, val_loss: 17.7229, val_MinusLogProbMetric: 17.7229

Epoch 129: val_loss did not improve from 17.25822
196/196 - 34s - loss: 17.4405 - MinusLogProbMetric: 17.4405 - val_loss: 17.7229 - val_MinusLogProbMetric: 17.7229 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 130/1000
2023-09-26 14:38:32.450 
Epoch 130/1000 
	 loss: 17.5205, MinusLogProbMetric: 17.5205, val_loss: 17.4058, val_MinusLogProbMetric: 17.4058

Epoch 130: val_loss did not improve from 17.25822
196/196 - 34s - loss: 17.5205 - MinusLogProbMetric: 17.5205 - val_loss: 17.4058 - val_MinusLogProbMetric: 17.4058 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 131/1000
2023-09-26 14:39:06.573 
Epoch 131/1000 
	 loss: 17.5347, MinusLogProbMetric: 17.5347, val_loss: 18.0869, val_MinusLogProbMetric: 18.0869

Epoch 131: val_loss did not improve from 17.25822
196/196 - 34s - loss: 17.5347 - MinusLogProbMetric: 17.5347 - val_loss: 18.0869 - val_MinusLogProbMetric: 18.0869 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 132/1000
2023-09-26 14:39:40.495 
Epoch 132/1000 
	 loss: 17.4347, MinusLogProbMetric: 17.4347, val_loss: 18.0944, val_MinusLogProbMetric: 18.0944

Epoch 132: val_loss did not improve from 17.25822
196/196 - 34s - loss: 17.4347 - MinusLogProbMetric: 17.4347 - val_loss: 18.0944 - val_MinusLogProbMetric: 18.0944 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 133/1000
2023-09-26 14:40:14.625 
Epoch 133/1000 
	 loss: 17.4695, MinusLogProbMetric: 17.4695, val_loss: 17.4806, val_MinusLogProbMetric: 17.4806

Epoch 133: val_loss did not improve from 17.25822
196/196 - 34s - loss: 17.4695 - MinusLogProbMetric: 17.4695 - val_loss: 17.4806 - val_MinusLogProbMetric: 17.4806 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 134/1000
2023-09-26 14:40:48.651 
Epoch 134/1000 
	 loss: 17.4862, MinusLogProbMetric: 17.4862, val_loss: 18.2759, val_MinusLogProbMetric: 18.2759

Epoch 134: val_loss did not improve from 17.25822
196/196 - 34s - loss: 17.4862 - MinusLogProbMetric: 17.4862 - val_loss: 18.2759 - val_MinusLogProbMetric: 18.2759 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 135/1000
2023-09-26 14:41:22.574 
Epoch 135/1000 
	 loss: 17.4032, MinusLogProbMetric: 17.4032, val_loss: 17.5516, val_MinusLogProbMetric: 17.5516

Epoch 135: val_loss did not improve from 17.25822
196/196 - 34s - loss: 17.4032 - MinusLogProbMetric: 17.4032 - val_loss: 17.5516 - val_MinusLogProbMetric: 17.5516 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 136/1000
2023-09-26 14:41:56.501 
Epoch 136/1000 
	 loss: 17.4971, MinusLogProbMetric: 17.4971, val_loss: 17.5418, val_MinusLogProbMetric: 17.5418

Epoch 136: val_loss did not improve from 17.25822
196/196 - 34s - loss: 17.4971 - MinusLogProbMetric: 17.4971 - val_loss: 17.5418 - val_MinusLogProbMetric: 17.5418 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 137/1000
2023-09-26 14:42:30.442 
Epoch 137/1000 
	 loss: 17.5306, MinusLogProbMetric: 17.5306, val_loss: 17.4859, val_MinusLogProbMetric: 17.4859

Epoch 137: val_loss did not improve from 17.25822
196/196 - 34s - loss: 17.5306 - MinusLogProbMetric: 17.5306 - val_loss: 17.4859 - val_MinusLogProbMetric: 17.4859 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 138/1000
2023-09-26 14:43:04.337 
Epoch 138/1000 
	 loss: 17.3985, MinusLogProbMetric: 17.3985, val_loss: 18.7730, val_MinusLogProbMetric: 18.7730

Epoch 138: val_loss did not improve from 17.25822
196/196 - 34s - loss: 17.3985 - MinusLogProbMetric: 17.3985 - val_loss: 18.7730 - val_MinusLogProbMetric: 18.7730 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 139/1000
2023-09-26 14:43:38.558 
Epoch 139/1000 
	 loss: 17.5196, MinusLogProbMetric: 17.5196, val_loss: 17.6836, val_MinusLogProbMetric: 17.6836

Epoch 139: val_loss did not improve from 17.25822
196/196 - 34s - loss: 17.5196 - MinusLogProbMetric: 17.5196 - val_loss: 17.6836 - val_MinusLogProbMetric: 17.6836 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 140/1000
2023-09-26 14:44:12.530 
Epoch 140/1000 
	 loss: 17.4195, MinusLogProbMetric: 17.4195, val_loss: 17.5973, val_MinusLogProbMetric: 17.5973

Epoch 140: val_loss did not improve from 17.25822
196/196 - 34s - loss: 17.4195 - MinusLogProbMetric: 17.4195 - val_loss: 17.5973 - val_MinusLogProbMetric: 17.5973 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 141/1000
2023-09-26 14:44:46.781 
Epoch 141/1000 
	 loss: 17.3831, MinusLogProbMetric: 17.3831, val_loss: 17.5605, val_MinusLogProbMetric: 17.5605

Epoch 141: val_loss did not improve from 17.25822
196/196 - 34s - loss: 17.3831 - MinusLogProbMetric: 17.3831 - val_loss: 17.5605 - val_MinusLogProbMetric: 17.5605 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 142/1000
2023-09-26 14:45:21.126 
Epoch 142/1000 
	 loss: 17.3639, MinusLogProbMetric: 17.3639, val_loss: 17.9446, val_MinusLogProbMetric: 17.9446

Epoch 142: val_loss did not improve from 17.25822
196/196 - 34s - loss: 17.3639 - MinusLogProbMetric: 17.3639 - val_loss: 17.9446 - val_MinusLogProbMetric: 17.9446 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 143/1000
2023-09-26 14:45:55.251 
Epoch 143/1000 
	 loss: 17.5320, MinusLogProbMetric: 17.5320, val_loss: 17.5830, val_MinusLogProbMetric: 17.5830

Epoch 143: val_loss did not improve from 17.25822
196/196 - 34s - loss: 17.5320 - MinusLogProbMetric: 17.5320 - val_loss: 17.5830 - val_MinusLogProbMetric: 17.5830 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 144/1000
2023-09-26 14:46:29.062 
Epoch 144/1000 
	 loss: 17.4976, MinusLogProbMetric: 17.4976, val_loss: 18.5213, val_MinusLogProbMetric: 18.5213

Epoch 144: val_loss did not improve from 17.25822
196/196 - 34s - loss: 17.4976 - MinusLogProbMetric: 17.4976 - val_loss: 18.5213 - val_MinusLogProbMetric: 18.5213 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 145/1000
2023-09-26 14:47:03.352 
Epoch 145/1000 
	 loss: 17.4388, MinusLogProbMetric: 17.4388, val_loss: 17.3732, val_MinusLogProbMetric: 17.3732

Epoch 145: val_loss did not improve from 17.25822
196/196 - 34s - loss: 17.4388 - MinusLogProbMetric: 17.4388 - val_loss: 17.3732 - val_MinusLogProbMetric: 17.3732 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 146/1000
2023-09-26 14:47:37.536 
Epoch 146/1000 
	 loss: 17.4291, MinusLogProbMetric: 17.4291, val_loss: 17.8615, val_MinusLogProbMetric: 17.8615

Epoch 146: val_loss did not improve from 17.25822
196/196 - 34s - loss: 17.4291 - MinusLogProbMetric: 17.4291 - val_loss: 17.8615 - val_MinusLogProbMetric: 17.8615 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 147/1000
2023-09-26 14:48:11.565 
Epoch 147/1000 
	 loss: 17.4228, MinusLogProbMetric: 17.4228, val_loss: 17.6454, val_MinusLogProbMetric: 17.6454

Epoch 147: val_loss did not improve from 17.25822
196/196 - 34s - loss: 17.4228 - MinusLogProbMetric: 17.4228 - val_loss: 17.6454 - val_MinusLogProbMetric: 17.6454 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 148/1000
2023-09-26 14:48:45.375 
Epoch 148/1000 
	 loss: 17.3894, MinusLogProbMetric: 17.3894, val_loss: 17.2393, val_MinusLogProbMetric: 17.2393

Epoch 148: val_loss improved from 17.25822 to 17.23935, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_289/weights/best_weights.h5
196/196 - 34s - loss: 17.3894 - MinusLogProbMetric: 17.3894 - val_loss: 17.2393 - val_MinusLogProbMetric: 17.2393 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 149/1000
2023-09-26 14:49:19.704 
Epoch 149/1000 
	 loss: 17.4500, MinusLogProbMetric: 17.4500, val_loss: 17.6577, val_MinusLogProbMetric: 17.6577

Epoch 149: val_loss did not improve from 17.23935
196/196 - 34s - loss: 17.4500 - MinusLogProbMetric: 17.4500 - val_loss: 17.6577 - val_MinusLogProbMetric: 17.6577 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 150/1000
2023-09-26 14:49:53.894 
Epoch 150/1000 
	 loss: 17.4087, MinusLogProbMetric: 17.4087, val_loss: 18.5775, val_MinusLogProbMetric: 18.5775

Epoch 150: val_loss did not improve from 17.23935
196/196 - 34s - loss: 17.4087 - MinusLogProbMetric: 17.4087 - val_loss: 18.5775 - val_MinusLogProbMetric: 18.5775 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 151/1000
2023-09-26 14:50:27.896 
Epoch 151/1000 
	 loss: 17.3531, MinusLogProbMetric: 17.3531, val_loss: 17.3204, val_MinusLogProbMetric: 17.3204

Epoch 151: val_loss did not improve from 17.23935
196/196 - 34s - loss: 17.3531 - MinusLogProbMetric: 17.3531 - val_loss: 17.3204 - val_MinusLogProbMetric: 17.3204 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 152/1000
2023-09-26 14:51:01.792 
Epoch 152/1000 
	 loss: 17.2853, MinusLogProbMetric: 17.2853, val_loss: 17.4107, val_MinusLogProbMetric: 17.4107

Epoch 152: val_loss did not improve from 17.23935
196/196 - 34s - loss: 17.2853 - MinusLogProbMetric: 17.2853 - val_loss: 17.4107 - val_MinusLogProbMetric: 17.4107 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 153/1000
2023-09-26 14:51:35.589 
Epoch 153/1000 
	 loss: 17.3258, MinusLogProbMetric: 17.3258, val_loss: 18.3378, val_MinusLogProbMetric: 18.3378

Epoch 153: val_loss did not improve from 17.23935
196/196 - 34s - loss: 17.3258 - MinusLogProbMetric: 17.3258 - val_loss: 18.3378 - val_MinusLogProbMetric: 18.3378 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 154/1000
2023-09-26 14:52:09.685 
Epoch 154/1000 
	 loss: 17.4262, MinusLogProbMetric: 17.4262, val_loss: 17.5057, val_MinusLogProbMetric: 17.5057

Epoch 154: val_loss did not improve from 17.23935
196/196 - 34s - loss: 17.4262 - MinusLogProbMetric: 17.4262 - val_loss: 17.5057 - val_MinusLogProbMetric: 17.5057 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 155/1000
2023-09-26 14:52:43.505 
Epoch 155/1000 
	 loss: 17.3289, MinusLogProbMetric: 17.3289, val_loss: 17.9719, val_MinusLogProbMetric: 17.9719

Epoch 155: val_loss did not improve from 17.23935
196/196 - 34s - loss: 17.3289 - MinusLogProbMetric: 17.3289 - val_loss: 17.9719 - val_MinusLogProbMetric: 17.9719 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 156/1000
2023-09-26 14:53:17.473 
Epoch 156/1000 
	 loss: 17.3333, MinusLogProbMetric: 17.3333, val_loss: 17.5855, val_MinusLogProbMetric: 17.5855

Epoch 156: val_loss did not improve from 17.23935
196/196 - 34s - loss: 17.3333 - MinusLogProbMetric: 17.3333 - val_loss: 17.5855 - val_MinusLogProbMetric: 17.5855 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 157/1000
2023-09-26 14:53:51.673 
Epoch 157/1000 
	 loss: 17.4148, MinusLogProbMetric: 17.4148, val_loss: 17.9565, val_MinusLogProbMetric: 17.9565

Epoch 157: val_loss did not improve from 17.23935
196/196 - 34s - loss: 17.4148 - MinusLogProbMetric: 17.4148 - val_loss: 17.9565 - val_MinusLogProbMetric: 17.9565 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 158/1000
2023-09-26 14:54:25.521 
Epoch 158/1000 
	 loss: 17.3035, MinusLogProbMetric: 17.3035, val_loss: 17.8871, val_MinusLogProbMetric: 17.8871

Epoch 158: val_loss did not improve from 17.23935
196/196 - 34s - loss: 17.3035 - MinusLogProbMetric: 17.3035 - val_loss: 17.8871 - val_MinusLogProbMetric: 17.8871 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 159/1000
2023-09-26 14:54:59.518 
Epoch 159/1000 
	 loss: 17.2383, MinusLogProbMetric: 17.2383, val_loss: 17.2689, val_MinusLogProbMetric: 17.2689

Epoch 159: val_loss did not improve from 17.23935
196/196 - 34s - loss: 17.2383 - MinusLogProbMetric: 17.2383 - val_loss: 17.2689 - val_MinusLogProbMetric: 17.2689 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 160/1000
2023-09-26 14:55:33.579 
Epoch 160/1000 
	 loss: 17.3452, MinusLogProbMetric: 17.3452, val_loss: 17.3961, val_MinusLogProbMetric: 17.3961

Epoch 160: val_loss did not improve from 17.23935
196/196 - 34s - loss: 17.3452 - MinusLogProbMetric: 17.3452 - val_loss: 17.3961 - val_MinusLogProbMetric: 17.3961 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 161/1000
2023-09-26 14:56:07.647 
Epoch 161/1000 
	 loss: 17.3440, MinusLogProbMetric: 17.3440, val_loss: 17.3224, val_MinusLogProbMetric: 17.3224

Epoch 161: val_loss did not improve from 17.23935
196/196 - 34s - loss: 17.3440 - MinusLogProbMetric: 17.3440 - val_loss: 17.3224 - val_MinusLogProbMetric: 17.3224 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 162/1000
2023-09-26 14:56:41.415 
Epoch 162/1000 
	 loss: 17.3121, MinusLogProbMetric: 17.3121, val_loss: 17.9159, val_MinusLogProbMetric: 17.9159

Epoch 162: val_loss did not improve from 17.23935
196/196 - 34s - loss: 17.3121 - MinusLogProbMetric: 17.3121 - val_loss: 17.9159 - val_MinusLogProbMetric: 17.9159 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 163/1000
2023-09-26 14:57:15.268 
Epoch 163/1000 
	 loss: 17.2882, MinusLogProbMetric: 17.2882, val_loss: 17.4868, val_MinusLogProbMetric: 17.4868

Epoch 163: val_loss did not improve from 17.23935
196/196 - 34s - loss: 17.2882 - MinusLogProbMetric: 17.2882 - val_loss: 17.4868 - val_MinusLogProbMetric: 17.4868 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 164/1000
2023-09-26 14:57:49.196 
Epoch 164/1000 
	 loss: 17.3814, MinusLogProbMetric: 17.3814, val_loss: 18.1330, val_MinusLogProbMetric: 18.1330

Epoch 164: val_loss did not improve from 17.23935
196/196 - 34s - loss: 17.3814 - MinusLogProbMetric: 17.3814 - val_loss: 18.1330 - val_MinusLogProbMetric: 18.1330 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 165/1000
2023-09-26 14:58:23.166 
Epoch 165/1000 
	 loss: 17.3348, MinusLogProbMetric: 17.3348, val_loss: 17.3676, val_MinusLogProbMetric: 17.3676

Epoch 165: val_loss did not improve from 17.23935
196/196 - 34s - loss: 17.3348 - MinusLogProbMetric: 17.3348 - val_loss: 17.3676 - val_MinusLogProbMetric: 17.3676 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 166/1000
2023-09-26 14:58:57.426 
Epoch 166/1000 
	 loss: 17.2156, MinusLogProbMetric: 17.2156, val_loss: 17.6640, val_MinusLogProbMetric: 17.6640

Epoch 166: val_loss did not improve from 17.23935
196/196 - 34s - loss: 17.2156 - MinusLogProbMetric: 17.2156 - val_loss: 17.6640 - val_MinusLogProbMetric: 17.6640 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 167/1000
2023-09-26 14:59:31.300 
Epoch 167/1000 
	 loss: 17.2591, MinusLogProbMetric: 17.2591, val_loss: 17.3624, val_MinusLogProbMetric: 17.3624

Epoch 167: val_loss did not improve from 17.23935
196/196 - 34s - loss: 17.2591 - MinusLogProbMetric: 17.2591 - val_loss: 17.3624 - val_MinusLogProbMetric: 17.3624 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 168/1000
2023-09-26 15:00:05.288 
Epoch 168/1000 
	 loss: 17.2897, MinusLogProbMetric: 17.2897, val_loss: 17.6368, val_MinusLogProbMetric: 17.6368

Epoch 168: val_loss did not improve from 17.23935
196/196 - 34s - loss: 17.2897 - MinusLogProbMetric: 17.2897 - val_loss: 17.6368 - val_MinusLogProbMetric: 17.6368 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 169/1000
2023-09-26 15:00:39.438 
Epoch 169/1000 
	 loss: 17.2371, MinusLogProbMetric: 17.2371, val_loss: 18.7362, val_MinusLogProbMetric: 18.7362

Epoch 169: val_loss did not improve from 17.23935
196/196 - 34s - loss: 17.2371 - MinusLogProbMetric: 17.2371 - val_loss: 18.7362 - val_MinusLogProbMetric: 18.7362 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 170/1000
2023-09-26 15:01:13.524 
Epoch 170/1000 
	 loss: 17.3059, MinusLogProbMetric: 17.3059, val_loss: 17.6372, val_MinusLogProbMetric: 17.6372

Epoch 170: val_loss did not improve from 17.23935
196/196 - 34s - loss: 17.3059 - MinusLogProbMetric: 17.3059 - val_loss: 17.6372 - val_MinusLogProbMetric: 17.6372 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 171/1000
2023-09-26 15:01:47.604 
Epoch 171/1000 
	 loss: 17.2701, MinusLogProbMetric: 17.2701, val_loss: 18.4382, val_MinusLogProbMetric: 18.4382

Epoch 171: val_loss did not improve from 17.23935
196/196 - 34s - loss: 17.2701 - MinusLogProbMetric: 17.2701 - val_loss: 18.4382 - val_MinusLogProbMetric: 18.4382 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 172/1000
2023-09-26 15:02:21.637 
Epoch 172/1000 
	 loss: 17.2706, MinusLogProbMetric: 17.2706, val_loss: 17.6987, val_MinusLogProbMetric: 17.6987

Epoch 172: val_loss did not improve from 17.23935
196/196 - 34s - loss: 17.2706 - MinusLogProbMetric: 17.2706 - val_loss: 17.6987 - val_MinusLogProbMetric: 17.6987 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 173/1000
2023-09-26 15:02:55.757 
Epoch 173/1000 
	 loss: 17.2379, MinusLogProbMetric: 17.2379, val_loss: 17.5489, val_MinusLogProbMetric: 17.5489

Epoch 173: val_loss did not improve from 17.23935
196/196 - 34s - loss: 17.2379 - MinusLogProbMetric: 17.2379 - val_loss: 17.5489 - val_MinusLogProbMetric: 17.5489 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 174/1000
2023-09-26 15:03:29.758 
Epoch 174/1000 
	 loss: 17.3346, MinusLogProbMetric: 17.3346, val_loss: 17.9188, val_MinusLogProbMetric: 17.9188

Epoch 174: val_loss did not improve from 17.23935
196/196 - 34s - loss: 17.3346 - MinusLogProbMetric: 17.3346 - val_loss: 17.9188 - val_MinusLogProbMetric: 17.9188 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 175/1000
2023-09-26 15:04:04.243 
Epoch 175/1000 
	 loss: 17.2034, MinusLogProbMetric: 17.2034, val_loss: 17.3186, val_MinusLogProbMetric: 17.3186

Epoch 175: val_loss did not improve from 17.23935
196/196 - 34s - loss: 17.2034 - MinusLogProbMetric: 17.2034 - val_loss: 17.3186 - val_MinusLogProbMetric: 17.3186 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 176/1000
2023-09-26 15:04:38.226 
Epoch 176/1000 
	 loss: 17.2717, MinusLogProbMetric: 17.2717, val_loss: 17.8046, val_MinusLogProbMetric: 17.8046

Epoch 176: val_loss did not improve from 17.23935
196/196 - 34s - loss: 17.2717 - MinusLogProbMetric: 17.2717 - val_loss: 17.8046 - val_MinusLogProbMetric: 17.8046 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 177/1000
2023-09-26 15:05:12.502 
Epoch 177/1000 
	 loss: 17.1594, MinusLogProbMetric: 17.1594, val_loss: 17.6142, val_MinusLogProbMetric: 17.6142

Epoch 177: val_loss did not improve from 17.23935
196/196 - 34s - loss: 17.1594 - MinusLogProbMetric: 17.1594 - val_loss: 17.6142 - val_MinusLogProbMetric: 17.6142 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 178/1000
2023-09-26 15:05:46.624 
Epoch 178/1000 
	 loss: 17.2087, MinusLogProbMetric: 17.2087, val_loss: 17.6536, val_MinusLogProbMetric: 17.6536

Epoch 178: val_loss did not improve from 17.23935
196/196 - 34s - loss: 17.2087 - MinusLogProbMetric: 17.2087 - val_loss: 17.6536 - val_MinusLogProbMetric: 17.6536 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 179/1000
2023-09-26 15:06:20.894 
Epoch 179/1000 
	 loss: 17.2543, MinusLogProbMetric: 17.2543, val_loss: 17.5217, val_MinusLogProbMetric: 17.5217

Epoch 179: val_loss did not improve from 17.23935
196/196 - 34s - loss: 17.2543 - MinusLogProbMetric: 17.2543 - val_loss: 17.5217 - val_MinusLogProbMetric: 17.5217 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 180/1000
2023-09-26 15:06:54.792 
Epoch 180/1000 
	 loss: 17.4349, MinusLogProbMetric: 17.4349, val_loss: 17.2707, val_MinusLogProbMetric: 17.2707

Epoch 180: val_loss did not improve from 17.23935
196/196 - 34s - loss: 17.4349 - MinusLogProbMetric: 17.4349 - val_loss: 17.2707 - val_MinusLogProbMetric: 17.2707 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 181/1000
2023-09-26 15:07:28.761 
Epoch 181/1000 
	 loss: 17.2071, MinusLogProbMetric: 17.2071, val_loss: 17.3323, val_MinusLogProbMetric: 17.3323

Epoch 181: val_loss did not improve from 17.23935
196/196 - 34s - loss: 17.2071 - MinusLogProbMetric: 17.2071 - val_loss: 17.3323 - val_MinusLogProbMetric: 17.3323 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 182/1000
2023-09-26 15:08:02.885 
Epoch 182/1000 
	 loss: 17.4125, MinusLogProbMetric: 17.4125, val_loss: 18.6704, val_MinusLogProbMetric: 18.6704

Epoch 182: val_loss did not improve from 17.23935
196/196 - 34s - loss: 17.4125 - MinusLogProbMetric: 17.4125 - val_loss: 18.6704 - val_MinusLogProbMetric: 18.6704 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 183/1000
2023-09-26 15:08:36.860 
Epoch 183/1000 
	 loss: 17.2296, MinusLogProbMetric: 17.2296, val_loss: 17.9004, val_MinusLogProbMetric: 17.9004

Epoch 183: val_loss did not improve from 17.23935
196/196 - 34s - loss: 17.2296 - MinusLogProbMetric: 17.2296 - val_loss: 17.9004 - val_MinusLogProbMetric: 17.9004 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 184/1000
2023-09-26 15:09:11.034 
Epoch 184/1000 
	 loss: 17.2223, MinusLogProbMetric: 17.2223, val_loss: 17.8060, val_MinusLogProbMetric: 17.8060

Epoch 184: val_loss did not improve from 17.23935
196/196 - 34s - loss: 17.2223 - MinusLogProbMetric: 17.2223 - val_loss: 17.8060 - val_MinusLogProbMetric: 17.8060 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 185/1000
2023-09-26 15:09:44.970 
Epoch 185/1000 
	 loss: 17.2129, MinusLogProbMetric: 17.2129, val_loss: 17.3356, val_MinusLogProbMetric: 17.3356

Epoch 185: val_loss did not improve from 17.23935
196/196 - 34s - loss: 17.2129 - MinusLogProbMetric: 17.2129 - val_loss: 17.3356 - val_MinusLogProbMetric: 17.3356 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 186/1000
2023-09-26 15:10:19.179 
Epoch 186/1000 
	 loss: 17.1741, MinusLogProbMetric: 17.1741, val_loss: 17.7466, val_MinusLogProbMetric: 17.7466

Epoch 186: val_loss did not improve from 17.23935
196/196 - 34s - loss: 17.1741 - MinusLogProbMetric: 17.1741 - val_loss: 17.7466 - val_MinusLogProbMetric: 17.7466 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 187/1000
2023-09-26 15:10:53.429 
Epoch 187/1000 
	 loss: 17.2393, MinusLogProbMetric: 17.2393, val_loss: 17.4600, val_MinusLogProbMetric: 17.4600

Epoch 187: val_loss did not improve from 17.23935
196/196 - 34s - loss: 17.2393 - MinusLogProbMetric: 17.2393 - val_loss: 17.4600 - val_MinusLogProbMetric: 17.4600 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 188/1000
2023-09-26 15:11:27.847 
Epoch 188/1000 
	 loss: 17.1053, MinusLogProbMetric: 17.1053, val_loss: 17.2912, val_MinusLogProbMetric: 17.2912

Epoch 188: val_loss did not improve from 17.23935
196/196 - 34s - loss: 17.1053 - MinusLogProbMetric: 17.1053 - val_loss: 17.2912 - val_MinusLogProbMetric: 17.2912 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 189/1000
2023-09-26 15:12:01.816 
Epoch 189/1000 
	 loss: 17.1485, MinusLogProbMetric: 17.1485, val_loss: 17.6795, val_MinusLogProbMetric: 17.6795

Epoch 189: val_loss did not improve from 17.23935
196/196 - 34s - loss: 17.1485 - MinusLogProbMetric: 17.1485 - val_loss: 17.6795 - val_MinusLogProbMetric: 17.6795 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 190/1000
2023-09-26 15:12:35.664 
Epoch 190/1000 
	 loss: 17.1496, MinusLogProbMetric: 17.1496, val_loss: 17.2308, val_MinusLogProbMetric: 17.2308

Epoch 190: val_loss improved from 17.23935 to 17.23079, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_289/weights/best_weights.h5
196/196 - 34s - loss: 17.1496 - MinusLogProbMetric: 17.1496 - val_loss: 17.2308 - val_MinusLogProbMetric: 17.2308 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 191/1000
2023-09-26 15:13:10.322 
Epoch 191/1000 
	 loss: 17.1197, MinusLogProbMetric: 17.1197, val_loss: 17.9447, val_MinusLogProbMetric: 17.9447

Epoch 191: val_loss did not improve from 17.23079
196/196 - 34s - loss: 17.1197 - MinusLogProbMetric: 17.1197 - val_loss: 17.9447 - val_MinusLogProbMetric: 17.9447 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 192/1000
2023-09-26 15:13:44.316 
Epoch 192/1000 
	 loss: 17.1023, MinusLogProbMetric: 17.1023, val_loss: 17.5276, val_MinusLogProbMetric: 17.5276

Epoch 192: val_loss did not improve from 17.23079
196/196 - 34s - loss: 17.1023 - MinusLogProbMetric: 17.1023 - val_loss: 17.5276 - val_MinusLogProbMetric: 17.5276 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 193/1000
2023-09-26 15:14:18.324 
Epoch 193/1000 
	 loss: 17.1463, MinusLogProbMetric: 17.1463, val_loss: 17.4707, val_MinusLogProbMetric: 17.4707

Epoch 193: val_loss did not improve from 17.23079
196/196 - 34s - loss: 17.1463 - MinusLogProbMetric: 17.1463 - val_loss: 17.4707 - val_MinusLogProbMetric: 17.4707 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 194/1000
2023-09-26 15:14:52.517 
Epoch 194/1000 
	 loss: 17.1280, MinusLogProbMetric: 17.1280, val_loss: 17.4186, val_MinusLogProbMetric: 17.4186

Epoch 194: val_loss did not improve from 17.23079
196/196 - 34s - loss: 17.1280 - MinusLogProbMetric: 17.1280 - val_loss: 17.4186 - val_MinusLogProbMetric: 17.4186 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 195/1000
2023-09-26 15:15:26.537 
Epoch 195/1000 
	 loss: 17.0951, MinusLogProbMetric: 17.0951, val_loss: 18.9578, val_MinusLogProbMetric: 18.9578

Epoch 195: val_loss did not improve from 17.23079
196/196 - 34s - loss: 17.0951 - MinusLogProbMetric: 17.0951 - val_loss: 18.9578 - val_MinusLogProbMetric: 18.9578 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 196/1000
2023-09-26 15:16:00.748 
Epoch 196/1000 
	 loss: 17.2373, MinusLogProbMetric: 17.2373, val_loss: 17.1837, val_MinusLogProbMetric: 17.1837

Epoch 196: val_loss improved from 17.23079 to 17.18372, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_289/weights/best_weights.h5
196/196 - 35s - loss: 17.2373 - MinusLogProbMetric: 17.2373 - val_loss: 17.1837 - val_MinusLogProbMetric: 17.1837 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 197/1000
2023-09-26 15:16:35.522 
Epoch 197/1000 
	 loss: 17.1107, MinusLogProbMetric: 17.1107, val_loss: 17.1781, val_MinusLogProbMetric: 17.1781

Epoch 197: val_loss improved from 17.18372 to 17.17811, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_289/weights/best_weights.h5
196/196 - 35s - loss: 17.1107 - MinusLogProbMetric: 17.1107 - val_loss: 17.1781 - val_MinusLogProbMetric: 17.1781 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 198/1000
2023-09-26 15:17:10.385 
Epoch 198/1000 
	 loss: 17.1337, MinusLogProbMetric: 17.1337, val_loss: 17.6901, val_MinusLogProbMetric: 17.6901

Epoch 198: val_loss did not improve from 17.17811
196/196 - 34s - loss: 17.1337 - MinusLogProbMetric: 17.1337 - val_loss: 17.6901 - val_MinusLogProbMetric: 17.6901 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 199/1000
2023-09-26 15:17:44.690 
Epoch 199/1000 
	 loss: 17.1719, MinusLogProbMetric: 17.1719, val_loss: 17.3579, val_MinusLogProbMetric: 17.3579

Epoch 199: val_loss did not improve from 17.17811
196/196 - 34s - loss: 17.1719 - MinusLogProbMetric: 17.1719 - val_loss: 17.3579 - val_MinusLogProbMetric: 17.3579 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 200/1000
2023-09-26 15:18:18.663 
Epoch 200/1000 
	 loss: 17.1756, MinusLogProbMetric: 17.1756, val_loss: 18.3470, val_MinusLogProbMetric: 18.3470

Epoch 200: val_loss did not improve from 17.17811
196/196 - 34s - loss: 17.1756 - MinusLogProbMetric: 17.1756 - val_loss: 18.3470 - val_MinusLogProbMetric: 18.3470 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 201/1000
2023-09-26 15:18:52.049 
Epoch 201/1000 
	 loss: 17.1118, MinusLogProbMetric: 17.1118, val_loss: 17.1283, val_MinusLogProbMetric: 17.1283

Epoch 201: val_loss improved from 17.17811 to 17.12828, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_289/weights/best_weights.h5
196/196 - 34s - loss: 17.1118 - MinusLogProbMetric: 17.1118 - val_loss: 17.1283 - val_MinusLogProbMetric: 17.1283 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 202/1000
2023-09-26 15:19:26.967 
Epoch 202/1000 
	 loss: 17.1387, MinusLogProbMetric: 17.1387, val_loss: 17.3845, val_MinusLogProbMetric: 17.3845

Epoch 202: val_loss did not improve from 17.12828
196/196 - 34s - loss: 17.1387 - MinusLogProbMetric: 17.1387 - val_loss: 17.3845 - val_MinusLogProbMetric: 17.3845 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 203/1000
2023-09-26 15:20:01.804 
Epoch 203/1000 
	 loss: 17.1379, MinusLogProbMetric: 17.1379, val_loss: 17.2803, val_MinusLogProbMetric: 17.2803

Epoch 203: val_loss did not improve from 17.12828
196/196 - 35s - loss: 17.1379 - MinusLogProbMetric: 17.1379 - val_loss: 17.2803 - val_MinusLogProbMetric: 17.2803 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 204/1000
2023-09-26 15:20:36.860 
Epoch 204/1000 
	 loss: 17.1549, MinusLogProbMetric: 17.1549, val_loss: 17.2758, val_MinusLogProbMetric: 17.2758

Epoch 204: val_loss did not improve from 17.12828
196/196 - 35s - loss: 17.1549 - MinusLogProbMetric: 17.1549 - val_loss: 17.2758 - val_MinusLogProbMetric: 17.2758 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 205/1000
2023-09-26 15:21:11.872 
Epoch 205/1000 
	 loss: 17.1832, MinusLogProbMetric: 17.1832, val_loss: 17.4428, val_MinusLogProbMetric: 17.4428

Epoch 205: val_loss did not improve from 17.12828
196/196 - 35s - loss: 17.1832 - MinusLogProbMetric: 17.1832 - val_loss: 17.4428 - val_MinusLogProbMetric: 17.4428 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 206/1000
2023-09-26 15:21:45.884 
Epoch 206/1000 
	 loss: 17.0930, MinusLogProbMetric: 17.0930, val_loss: 17.2417, val_MinusLogProbMetric: 17.2417

Epoch 206: val_loss did not improve from 17.12828
196/196 - 34s - loss: 17.0930 - MinusLogProbMetric: 17.0930 - val_loss: 17.2417 - val_MinusLogProbMetric: 17.2417 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 207/1000
2023-09-26 15:22:19.547 
Epoch 207/1000 
	 loss: 17.1158, MinusLogProbMetric: 17.1158, val_loss: 17.2114, val_MinusLogProbMetric: 17.2114

Epoch 207: val_loss did not improve from 17.12828
196/196 - 34s - loss: 17.1158 - MinusLogProbMetric: 17.1158 - val_loss: 17.2114 - val_MinusLogProbMetric: 17.2114 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 208/1000
2023-09-26 15:22:53.210 
Epoch 208/1000 
	 loss: 17.1239, MinusLogProbMetric: 17.1239, val_loss: 17.2340, val_MinusLogProbMetric: 17.2340

Epoch 208: val_loss did not improve from 17.12828
196/196 - 34s - loss: 17.1239 - MinusLogProbMetric: 17.1239 - val_loss: 17.2340 - val_MinusLogProbMetric: 17.2340 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 209/1000
2023-09-26 15:23:27.091 
Epoch 209/1000 
	 loss: 17.0587, MinusLogProbMetric: 17.0587, val_loss: 17.3322, val_MinusLogProbMetric: 17.3322

Epoch 209: val_loss did not improve from 17.12828
196/196 - 34s - loss: 17.0587 - MinusLogProbMetric: 17.0587 - val_loss: 17.3322 - val_MinusLogProbMetric: 17.3322 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 210/1000
2023-09-26 15:24:01.232 
Epoch 210/1000 
	 loss: 17.0513, MinusLogProbMetric: 17.0513, val_loss: 17.1672, val_MinusLogProbMetric: 17.1672

Epoch 210: val_loss did not improve from 17.12828
196/196 - 34s - loss: 17.0513 - MinusLogProbMetric: 17.0513 - val_loss: 17.1672 - val_MinusLogProbMetric: 17.1672 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 211/1000
2023-09-26 15:24:35.476 
Epoch 211/1000 
	 loss: 17.0781, MinusLogProbMetric: 17.0781, val_loss: 17.2376, val_MinusLogProbMetric: 17.2376

Epoch 211: val_loss did not improve from 17.12828
196/196 - 34s - loss: 17.0781 - MinusLogProbMetric: 17.0781 - val_loss: 17.2376 - val_MinusLogProbMetric: 17.2376 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 212/1000
2023-09-26 15:25:08.318 
Epoch 212/1000 
	 loss: 17.0995, MinusLogProbMetric: 17.0995, val_loss: 17.2150, val_MinusLogProbMetric: 17.2150

Epoch 212: val_loss did not improve from 17.12828
196/196 - 33s - loss: 17.0995 - MinusLogProbMetric: 17.0995 - val_loss: 17.2150 - val_MinusLogProbMetric: 17.2150 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 213/1000
2023-09-26 15:25:40.351 
Epoch 213/1000 
	 loss: 17.1160, MinusLogProbMetric: 17.1160, val_loss: 17.6127, val_MinusLogProbMetric: 17.6127

Epoch 213: val_loss did not improve from 17.12828
196/196 - 32s - loss: 17.1160 - MinusLogProbMetric: 17.1160 - val_loss: 17.6127 - val_MinusLogProbMetric: 17.6127 - lr: 0.0010 - 32s/epoch - 163ms/step
Epoch 214/1000
2023-09-26 15:26:11.748 
Epoch 214/1000 
	 loss: 17.0591, MinusLogProbMetric: 17.0591, val_loss: 17.4661, val_MinusLogProbMetric: 17.4661

Epoch 214: val_loss did not improve from 17.12828
196/196 - 31s - loss: 17.0591 - MinusLogProbMetric: 17.0591 - val_loss: 17.4661 - val_MinusLogProbMetric: 17.4661 - lr: 0.0010 - 31s/epoch - 160ms/step
Epoch 215/1000
2023-09-26 15:26:41.261 
Epoch 215/1000 
	 loss: 17.0432, MinusLogProbMetric: 17.0432, val_loss: 17.4091, val_MinusLogProbMetric: 17.4091

Epoch 215: val_loss did not improve from 17.12828
196/196 - 30s - loss: 17.0432 - MinusLogProbMetric: 17.0432 - val_loss: 17.4091 - val_MinusLogProbMetric: 17.4091 - lr: 0.0010 - 30s/epoch - 151ms/step
Epoch 216/1000
2023-09-26 15:27:08.955 
Epoch 216/1000 
	 loss: 17.0606, MinusLogProbMetric: 17.0606, val_loss: 17.1217, val_MinusLogProbMetric: 17.1217

Epoch 216: val_loss improved from 17.12828 to 17.12172, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_289/weights/best_weights.h5
196/196 - 28s - loss: 17.0606 - MinusLogProbMetric: 17.0606 - val_loss: 17.1217 - val_MinusLogProbMetric: 17.1217 - lr: 0.0010 - 28s/epoch - 144ms/step
Epoch 217/1000
2023-09-26 15:27:40.949 
Epoch 217/1000 
	 loss: 17.0844, MinusLogProbMetric: 17.0844, val_loss: 17.4788, val_MinusLogProbMetric: 17.4788

Epoch 217: val_loss did not improve from 17.12172
196/196 - 32s - loss: 17.0844 - MinusLogProbMetric: 17.0844 - val_loss: 17.4788 - val_MinusLogProbMetric: 17.4788 - lr: 0.0010 - 32s/epoch - 161ms/step
Epoch 218/1000
2023-09-26 15:28:15.145 
Epoch 218/1000 
	 loss: 17.0742, MinusLogProbMetric: 17.0742, val_loss: 17.4879, val_MinusLogProbMetric: 17.4879

Epoch 218: val_loss did not improve from 17.12172
196/196 - 34s - loss: 17.0742 - MinusLogProbMetric: 17.0742 - val_loss: 17.4879 - val_MinusLogProbMetric: 17.4879 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 219/1000
2023-09-26 15:28:46.662 
Epoch 219/1000 
	 loss: 17.1038, MinusLogProbMetric: 17.1038, val_loss: 17.4279, val_MinusLogProbMetric: 17.4279

Epoch 219: val_loss did not improve from 17.12172
196/196 - 32s - loss: 17.1038 - MinusLogProbMetric: 17.1038 - val_loss: 17.4279 - val_MinusLogProbMetric: 17.4279 - lr: 0.0010 - 32s/epoch - 161ms/step
Epoch 220/1000
2023-09-26 15:29:17.413 
Epoch 220/1000 
	 loss: 17.0178, MinusLogProbMetric: 17.0178, val_loss: 17.5027, val_MinusLogProbMetric: 17.5027

Epoch 220: val_loss did not improve from 17.12172
196/196 - 31s - loss: 17.0178 - MinusLogProbMetric: 17.0178 - val_loss: 17.5027 - val_MinusLogProbMetric: 17.5027 - lr: 0.0010 - 31s/epoch - 157ms/step
Epoch 221/1000
2023-09-26 15:29:47.370 
Epoch 221/1000 
	 loss: 17.1137, MinusLogProbMetric: 17.1137, val_loss: 17.5990, val_MinusLogProbMetric: 17.5990

Epoch 221: val_loss did not improve from 17.12172
196/196 - 30s - loss: 17.1137 - MinusLogProbMetric: 17.1137 - val_loss: 17.5990 - val_MinusLogProbMetric: 17.5990 - lr: 0.0010 - 30s/epoch - 153ms/step
Epoch 222/1000
2023-09-26 15:30:17.980 
Epoch 222/1000 
	 loss: 17.0196, MinusLogProbMetric: 17.0196, val_loss: 17.2902, val_MinusLogProbMetric: 17.2902

Epoch 222: val_loss did not improve from 17.12172
196/196 - 31s - loss: 17.0196 - MinusLogProbMetric: 17.0196 - val_loss: 17.2902 - val_MinusLogProbMetric: 17.2902 - lr: 0.0010 - 31s/epoch - 156ms/step
Epoch 223/1000
2023-09-26 15:30:48.003 
Epoch 223/1000 
	 loss: 17.1725, MinusLogProbMetric: 17.1725, val_loss: 17.0714, val_MinusLogProbMetric: 17.0714

Epoch 223: val_loss improved from 17.12172 to 17.07141, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_289/weights/best_weights.h5
196/196 - 31s - loss: 17.1725 - MinusLogProbMetric: 17.1725 - val_loss: 17.0714 - val_MinusLogProbMetric: 17.0714 - lr: 0.0010 - 31s/epoch - 156ms/step
Epoch 224/1000
2023-09-26 15:31:22.973 
Epoch 224/1000 
	 loss: 17.0270, MinusLogProbMetric: 17.0270, val_loss: 17.1117, val_MinusLogProbMetric: 17.1117

Epoch 224: val_loss did not improve from 17.07141
196/196 - 34s - loss: 17.0270 - MinusLogProbMetric: 17.0270 - val_loss: 17.1117 - val_MinusLogProbMetric: 17.1117 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 225/1000
2023-09-26 15:31:56.909 
Epoch 225/1000 
	 loss: 17.0394, MinusLogProbMetric: 17.0394, val_loss: 17.1939, val_MinusLogProbMetric: 17.1939

Epoch 225: val_loss did not improve from 17.07141
196/196 - 34s - loss: 17.0394 - MinusLogProbMetric: 17.0394 - val_loss: 17.1939 - val_MinusLogProbMetric: 17.1939 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 226/1000
2023-09-26 15:32:28.514 
Epoch 226/1000 
	 loss: 17.0331, MinusLogProbMetric: 17.0331, val_loss: 17.1553, val_MinusLogProbMetric: 17.1553

Epoch 226: val_loss did not improve from 17.07141
196/196 - 32s - loss: 17.0331 - MinusLogProbMetric: 17.0331 - val_loss: 17.1553 - val_MinusLogProbMetric: 17.1553 - lr: 0.0010 - 32s/epoch - 161ms/step
Epoch 227/1000
2023-09-26 15:33:01.374 
Epoch 227/1000 
	 loss: 17.0680, MinusLogProbMetric: 17.0680, val_loss: 17.0921, val_MinusLogProbMetric: 17.0921

Epoch 227: val_loss did not improve from 17.07141
196/196 - 33s - loss: 17.0680 - MinusLogProbMetric: 17.0680 - val_loss: 17.0921 - val_MinusLogProbMetric: 17.0921 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 228/1000
2023-09-26 15:33:33.959 
Epoch 228/1000 
	 loss: 17.0268, MinusLogProbMetric: 17.0268, val_loss: 17.0685, val_MinusLogProbMetric: 17.0685

Epoch 228: val_loss improved from 17.07141 to 17.06850, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_289/weights/best_weights.h5
196/196 - 33s - loss: 17.0268 - MinusLogProbMetric: 17.0268 - val_loss: 17.0685 - val_MinusLogProbMetric: 17.0685 - lr: 0.0010 - 33s/epoch - 169ms/step
Epoch 229/1000
2023-09-26 15:34:07.413 
Epoch 229/1000 
	 loss: 17.0400, MinusLogProbMetric: 17.0400, val_loss: 17.2369, val_MinusLogProbMetric: 17.2369

Epoch 229: val_loss did not improve from 17.06850
196/196 - 33s - loss: 17.0400 - MinusLogProbMetric: 17.0400 - val_loss: 17.2369 - val_MinusLogProbMetric: 17.2369 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 230/1000
2023-09-26 15:34:41.040 
Epoch 230/1000 
	 loss: 17.0645, MinusLogProbMetric: 17.0645, val_loss: 17.3831, val_MinusLogProbMetric: 17.3831

Epoch 230: val_loss did not improve from 17.06850
196/196 - 34s - loss: 17.0645 - MinusLogProbMetric: 17.0645 - val_loss: 17.3831 - val_MinusLogProbMetric: 17.3831 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 231/1000
2023-09-26 15:35:15.183 
Epoch 231/1000 
	 loss: 17.0144, MinusLogProbMetric: 17.0144, val_loss: 17.4259, val_MinusLogProbMetric: 17.4259

Epoch 231: val_loss did not improve from 17.06850
196/196 - 34s - loss: 17.0144 - MinusLogProbMetric: 17.0144 - val_loss: 17.4259 - val_MinusLogProbMetric: 17.4259 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 232/1000
2023-09-26 15:35:48.956 
Epoch 232/1000 
	 loss: 17.0542, MinusLogProbMetric: 17.0542, val_loss: 17.5974, val_MinusLogProbMetric: 17.5974

Epoch 232: val_loss did not improve from 17.06850
196/196 - 34s - loss: 17.0542 - MinusLogProbMetric: 17.0542 - val_loss: 17.5974 - val_MinusLogProbMetric: 17.5974 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 233/1000
2023-09-26 15:36:23.337 
Epoch 233/1000 
	 loss: 17.0202, MinusLogProbMetric: 17.0202, val_loss: 17.2106, val_MinusLogProbMetric: 17.2106

Epoch 233: val_loss did not improve from 17.06850
196/196 - 34s - loss: 17.0202 - MinusLogProbMetric: 17.0202 - val_loss: 17.2106 - val_MinusLogProbMetric: 17.2106 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 234/1000
2023-09-26 15:36:57.305 
Epoch 234/1000 
	 loss: 17.0619, MinusLogProbMetric: 17.0619, val_loss: 17.4902, val_MinusLogProbMetric: 17.4902

Epoch 234: val_loss did not improve from 17.06850
196/196 - 34s - loss: 17.0619 - MinusLogProbMetric: 17.0619 - val_loss: 17.4902 - val_MinusLogProbMetric: 17.4902 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 235/1000
2023-09-26 15:37:31.473 
Epoch 235/1000 
	 loss: 17.0129, MinusLogProbMetric: 17.0129, val_loss: 17.5709, val_MinusLogProbMetric: 17.5709

Epoch 235: val_loss did not improve from 17.06850
196/196 - 34s - loss: 17.0129 - MinusLogProbMetric: 17.0129 - val_loss: 17.5709 - val_MinusLogProbMetric: 17.5709 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 236/1000
2023-09-26 15:38:05.571 
Epoch 236/1000 
	 loss: 17.0035, MinusLogProbMetric: 17.0035, val_loss: 17.5461, val_MinusLogProbMetric: 17.5461

Epoch 236: val_loss did not improve from 17.06850
196/196 - 34s - loss: 17.0035 - MinusLogProbMetric: 17.0035 - val_loss: 17.5461 - val_MinusLogProbMetric: 17.5461 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 237/1000
2023-09-26 15:38:39.662 
Epoch 237/1000 
	 loss: 17.0430, MinusLogProbMetric: 17.0430, val_loss: 17.1045, val_MinusLogProbMetric: 17.1045

Epoch 237: val_loss did not improve from 17.06850
196/196 - 34s - loss: 17.0430 - MinusLogProbMetric: 17.0430 - val_loss: 17.1045 - val_MinusLogProbMetric: 17.1045 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 238/1000
2023-09-26 15:39:13.502 
Epoch 238/1000 
	 loss: 16.9485, MinusLogProbMetric: 16.9485, val_loss: 17.0597, val_MinusLogProbMetric: 17.0597

Epoch 238: val_loss improved from 17.06850 to 17.05974, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_289/weights/best_weights.h5
196/196 - 34s - loss: 16.9485 - MinusLogProbMetric: 16.9485 - val_loss: 17.0597 - val_MinusLogProbMetric: 17.0597 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 239/1000
2023-09-26 15:39:48.507 
Epoch 239/1000 
	 loss: 17.0376, MinusLogProbMetric: 17.0376, val_loss: 17.2928, val_MinusLogProbMetric: 17.2928

Epoch 239: val_loss did not improve from 17.05974
196/196 - 34s - loss: 17.0376 - MinusLogProbMetric: 17.0376 - val_loss: 17.2928 - val_MinusLogProbMetric: 17.2928 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 240/1000
2023-09-26 15:40:22.487 
Epoch 240/1000 
	 loss: 17.0204, MinusLogProbMetric: 17.0204, val_loss: 17.6905, val_MinusLogProbMetric: 17.6905

Epoch 240: val_loss did not improve from 17.05974
196/196 - 34s - loss: 17.0204 - MinusLogProbMetric: 17.0204 - val_loss: 17.6905 - val_MinusLogProbMetric: 17.6905 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 241/1000
2023-09-26 15:40:56.580 
Epoch 241/1000 
	 loss: 17.0089, MinusLogProbMetric: 17.0089, val_loss: 17.8772, val_MinusLogProbMetric: 17.8772

Epoch 241: val_loss did not improve from 17.05974
196/196 - 34s - loss: 17.0089 - MinusLogProbMetric: 17.0089 - val_loss: 17.8772 - val_MinusLogProbMetric: 17.8772 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 242/1000
2023-09-26 15:41:30.509 
Epoch 242/1000 
	 loss: 17.0699, MinusLogProbMetric: 17.0699, val_loss: 17.4365, val_MinusLogProbMetric: 17.4365

Epoch 242: val_loss did not improve from 17.05974
196/196 - 34s - loss: 17.0699 - MinusLogProbMetric: 17.0699 - val_loss: 17.4365 - val_MinusLogProbMetric: 17.4365 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 243/1000
2023-09-26 15:42:05.070 
Epoch 243/1000 
	 loss: 16.9487, MinusLogProbMetric: 16.9487, val_loss: 17.0886, val_MinusLogProbMetric: 17.0886

Epoch 243: val_loss did not improve from 17.05974
196/196 - 35s - loss: 16.9487 - MinusLogProbMetric: 16.9487 - val_loss: 17.0886 - val_MinusLogProbMetric: 17.0886 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 244/1000
2023-09-26 15:42:39.900 
Epoch 244/1000 
	 loss: 16.9738, MinusLogProbMetric: 16.9738, val_loss: 17.1710, val_MinusLogProbMetric: 17.1710

Epoch 244: val_loss did not improve from 17.05974
196/196 - 35s - loss: 16.9738 - MinusLogProbMetric: 16.9738 - val_loss: 17.1710 - val_MinusLogProbMetric: 17.1710 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 245/1000
2023-09-26 15:43:14.255 
Epoch 245/1000 
	 loss: 16.9932, MinusLogProbMetric: 16.9932, val_loss: 17.1130, val_MinusLogProbMetric: 17.1130

Epoch 245: val_loss did not improve from 17.05974
196/196 - 34s - loss: 16.9932 - MinusLogProbMetric: 16.9932 - val_loss: 17.1130 - val_MinusLogProbMetric: 17.1130 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 246/1000
2023-09-26 15:43:49.061 
Epoch 246/1000 
	 loss: 16.9965, MinusLogProbMetric: 16.9965, val_loss: 17.1547, val_MinusLogProbMetric: 17.1547

Epoch 246: val_loss did not improve from 17.05974
196/196 - 35s - loss: 16.9965 - MinusLogProbMetric: 16.9965 - val_loss: 17.1547 - val_MinusLogProbMetric: 17.1547 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 247/1000
2023-09-26 15:44:23.432 
Epoch 247/1000 
	 loss: 16.9827, MinusLogProbMetric: 16.9827, val_loss: 17.8128, val_MinusLogProbMetric: 17.8128

Epoch 247: val_loss did not improve from 17.05974
196/196 - 34s - loss: 16.9827 - MinusLogProbMetric: 16.9827 - val_loss: 17.8128 - val_MinusLogProbMetric: 17.8128 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 248/1000
2023-09-26 15:44:57.289 
Epoch 248/1000 
	 loss: 17.0294, MinusLogProbMetric: 17.0294, val_loss: 17.2787, val_MinusLogProbMetric: 17.2787

Epoch 248: val_loss did not improve from 17.05974
196/196 - 34s - loss: 17.0294 - MinusLogProbMetric: 17.0294 - val_loss: 17.2787 - val_MinusLogProbMetric: 17.2787 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 249/1000
2023-09-26 15:45:31.170 
Epoch 249/1000 
	 loss: 16.9909, MinusLogProbMetric: 16.9909, val_loss: 17.7556, val_MinusLogProbMetric: 17.7556

Epoch 249: val_loss did not improve from 17.05974
196/196 - 34s - loss: 16.9909 - MinusLogProbMetric: 16.9909 - val_loss: 17.7556 - val_MinusLogProbMetric: 17.7556 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 250/1000
2023-09-26 15:46:04.782 
Epoch 250/1000 
	 loss: 17.0386, MinusLogProbMetric: 17.0386, val_loss: 17.4835, val_MinusLogProbMetric: 17.4835

Epoch 250: val_loss did not improve from 17.05974
196/196 - 34s - loss: 17.0386 - MinusLogProbMetric: 17.0386 - val_loss: 17.4835 - val_MinusLogProbMetric: 17.4835 - lr: 0.0010 - 34s/epoch - 171ms/step
Epoch 251/1000
2023-09-26 15:46:38.872 
Epoch 251/1000 
	 loss: 17.0403, MinusLogProbMetric: 17.0403, val_loss: 17.4433, val_MinusLogProbMetric: 17.4433

Epoch 251: val_loss did not improve from 17.05974
196/196 - 34s - loss: 17.0403 - MinusLogProbMetric: 17.0403 - val_loss: 17.4433 - val_MinusLogProbMetric: 17.4433 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 252/1000
2023-09-26 15:47:12.766 
Epoch 252/1000 
	 loss: 16.9547, MinusLogProbMetric: 16.9547, val_loss: 17.0508, val_MinusLogProbMetric: 17.0508

Epoch 252: val_loss improved from 17.05974 to 17.05084, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_289/weights/best_weights.h5
196/196 - 34s - loss: 16.9547 - MinusLogProbMetric: 16.9547 - val_loss: 17.0508 - val_MinusLogProbMetric: 17.0508 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 253/1000
2023-09-26 15:47:47.369 
Epoch 253/1000 
	 loss: 16.9877, MinusLogProbMetric: 16.9877, val_loss: 17.2371, val_MinusLogProbMetric: 17.2371

Epoch 253: val_loss did not improve from 17.05084
196/196 - 34s - loss: 16.9877 - MinusLogProbMetric: 16.9877 - val_loss: 17.2371 - val_MinusLogProbMetric: 17.2371 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 254/1000
2023-09-26 15:48:21.386 
Epoch 254/1000 
	 loss: 16.9729, MinusLogProbMetric: 16.9729, val_loss: 17.1377, val_MinusLogProbMetric: 17.1377

Epoch 254: val_loss did not improve from 17.05084
196/196 - 34s - loss: 16.9729 - MinusLogProbMetric: 16.9729 - val_loss: 17.1377 - val_MinusLogProbMetric: 17.1377 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 255/1000
2023-09-26 15:48:55.804 
Epoch 255/1000 
	 loss: 16.9405, MinusLogProbMetric: 16.9405, val_loss: 18.0956, val_MinusLogProbMetric: 18.0956

Epoch 255: val_loss did not improve from 17.05084
196/196 - 34s - loss: 16.9405 - MinusLogProbMetric: 16.9405 - val_loss: 18.0956 - val_MinusLogProbMetric: 18.0956 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 256/1000
2023-09-26 15:49:29.570 
Epoch 256/1000 
	 loss: 16.9492, MinusLogProbMetric: 16.9492, val_loss: 17.1738, val_MinusLogProbMetric: 17.1738

Epoch 256: val_loss did not improve from 17.05084
196/196 - 34s - loss: 16.9492 - MinusLogProbMetric: 16.9492 - val_loss: 17.1738 - val_MinusLogProbMetric: 17.1738 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 257/1000
2023-09-26 15:50:03.821 
Epoch 257/1000 
	 loss: 16.9456, MinusLogProbMetric: 16.9456, val_loss: 17.3226, val_MinusLogProbMetric: 17.3226

Epoch 257: val_loss did not improve from 17.05084
196/196 - 34s - loss: 16.9456 - MinusLogProbMetric: 16.9456 - val_loss: 17.3226 - val_MinusLogProbMetric: 17.3226 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 258/1000
2023-09-26 15:50:37.519 
Epoch 258/1000 
	 loss: 17.0028, MinusLogProbMetric: 17.0028, val_loss: 17.2527, val_MinusLogProbMetric: 17.2527

Epoch 258: val_loss did not improve from 17.05084
196/196 - 34s - loss: 17.0028 - MinusLogProbMetric: 17.0028 - val_loss: 17.2527 - val_MinusLogProbMetric: 17.2527 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 259/1000
2023-09-26 15:51:11.312 
Epoch 259/1000 
	 loss: 16.9612, MinusLogProbMetric: 16.9612, val_loss: 17.0424, val_MinusLogProbMetric: 17.0424

Epoch 259: val_loss improved from 17.05084 to 17.04237, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_289/weights/best_weights.h5
196/196 - 34s - loss: 16.9612 - MinusLogProbMetric: 16.9612 - val_loss: 17.0424 - val_MinusLogProbMetric: 17.0424 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 260/1000
2023-09-26 15:51:45.692 
Epoch 260/1000 
	 loss: 16.9499, MinusLogProbMetric: 16.9499, val_loss: 17.5015, val_MinusLogProbMetric: 17.5015

Epoch 260: val_loss did not improve from 17.04237
196/196 - 34s - loss: 16.9499 - MinusLogProbMetric: 16.9499 - val_loss: 17.5015 - val_MinusLogProbMetric: 17.5015 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 261/1000
2023-09-26 15:52:19.469 
Epoch 261/1000 
	 loss: 17.0229, MinusLogProbMetric: 17.0229, val_loss: 17.7808, val_MinusLogProbMetric: 17.7808

Epoch 261: val_loss did not improve from 17.04237
196/196 - 34s - loss: 17.0229 - MinusLogProbMetric: 17.0229 - val_loss: 17.7808 - val_MinusLogProbMetric: 17.7808 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 262/1000
2023-09-26 15:52:53.446 
Epoch 262/1000 
	 loss: 16.9256, MinusLogProbMetric: 16.9256, val_loss: 17.1898, val_MinusLogProbMetric: 17.1898

Epoch 262: val_loss did not improve from 17.04237
196/196 - 34s - loss: 16.9256 - MinusLogProbMetric: 16.9256 - val_loss: 17.1898 - val_MinusLogProbMetric: 17.1898 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 263/1000
2023-09-26 15:53:27.187 
Epoch 263/1000 
	 loss: 17.0472, MinusLogProbMetric: 17.0472, val_loss: 17.0804, val_MinusLogProbMetric: 17.0804

Epoch 263: val_loss did not improve from 17.04237
196/196 - 34s - loss: 17.0472 - MinusLogProbMetric: 17.0472 - val_loss: 17.0804 - val_MinusLogProbMetric: 17.0804 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 264/1000
2023-09-26 15:54:01.345 
Epoch 264/1000 
	 loss: 16.9428, MinusLogProbMetric: 16.9428, val_loss: 17.1768, val_MinusLogProbMetric: 17.1768

Epoch 264: val_loss did not improve from 17.04237
196/196 - 34s - loss: 16.9428 - MinusLogProbMetric: 16.9428 - val_loss: 17.1768 - val_MinusLogProbMetric: 17.1768 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 265/1000
2023-09-26 15:54:34.953 
Epoch 265/1000 
	 loss: 16.9249, MinusLogProbMetric: 16.9249, val_loss: 17.4239, val_MinusLogProbMetric: 17.4239

Epoch 265: val_loss did not improve from 17.04237
196/196 - 34s - loss: 16.9249 - MinusLogProbMetric: 16.9249 - val_loss: 17.4239 - val_MinusLogProbMetric: 17.4239 - lr: 0.0010 - 34s/epoch - 171ms/step
Epoch 266/1000
2023-09-26 15:55:08.626 
Epoch 266/1000 
	 loss: 16.9847, MinusLogProbMetric: 16.9847, val_loss: 17.2723, val_MinusLogProbMetric: 17.2723

Epoch 266: val_loss did not improve from 17.04237
196/196 - 34s - loss: 16.9847 - MinusLogProbMetric: 16.9847 - val_loss: 17.2723 - val_MinusLogProbMetric: 17.2723 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 267/1000
2023-09-26 15:55:42.540 
Epoch 267/1000 
	 loss: 16.9127, MinusLogProbMetric: 16.9127, val_loss: 17.2535, val_MinusLogProbMetric: 17.2535

Epoch 267: val_loss did not improve from 17.04237
196/196 - 34s - loss: 16.9127 - MinusLogProbMetric: 16.9127 - val_loss: 17.2535 - val_MinusLogProbMetric: 17.2535 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 268/1000
2023-09-26 15:56:16.580 
Epoch 268/1000 
	 loss: 16.9063, MinusLogProbMetric: 16.9063, val_loss: 17.3427, val_MinusLogProbMetric: 17.3427

Epoch 268: val_loss did not improve from 17.04237
196/196 - 34s - loss: 16.9063 - MinusLogProbMetric: 16.9063 - val_loss: 17.3427 - val_MinusLogProbMetric: 17.3427 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 269/1000
2023-09-26 15:56:46.869 
Epoch 269/1000 
	 loss: 16.9333, MinusLogProbMetric: 16.9333, val_loss: 17.3787, val_MinusLogProbMetric: 17.3787

Epoch 269: val_loss did not improve from 17.04237
196/196 - 30s - loss: 16.9333 - MinusLogProbMetric: 16.9333 - val_loss: 17.3787 - val_MinusLogProbMetric: 17.3787 - lr: 0.0010 - 30s/epoch - 154ms/step
Epoch 270/1000
2023-09-26 15:57:15.652 
Epoch 270/1000 
	 loss: 16.9331, MinusLogProbMetric: 16.9331, val_loss: 17.5367, val_MinusLogProbMetric: 17.5367

Epoch 270: val_loss did not improve from 17.04237
196/196 - 29s - loss: 16.9331 - MinusLogProbMetric: 16.9331 - val_loss: 17.5367 - val_MinusLogProbMetric: 17.5367 - lr: 0.0010 - 29s/epoch - 147ms/step
Epoch 271/1000
2023-09-26 15:57:45.346 
Epoch 271/1000 
	 loss: 16.9851, MinusLogProbMetric: 16.9851, val_loss: 17.1045, val_MinusLogProbMetric: 17.1045

Epoch 271: val_loss did not improve from 17.04237
196/196 - 30s - loss: 16.9851 - MinusLogProbMetric: 16.9851 - val_loss: 17.1045 - val_MinusLogProbMetric: 17.1045 - lr: 0.0010 - 30s/epoch - 151ms/step
Epoch 272/1000
2023-09-26 15:58:18.005 
Epoch 272/1000 
	 loss: 16.9240, MinusLogProbMetric: 16.9240, val_loss: 17.4811, val_MinusLogProbMetric: 17.4811

Epoch 272: val_loss did not improve from 17.04237
196/196 - 33s - loss: 16.9240 - MinusLogProbMetric: 16.9240 - val_loss: 17.4811 - val_MinusLogProbMetric: 17.4811 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 273/1000
2023-09-26 15:58:51.783 
Epoch 273/1000 
	 loss: 16.9184, MinusLogProbMetric: 16.9184, val_loss: 17.0766, val_MinusLogProbMetric: 17.0766

Epoch 273: val_loss did not improve from 17.04237
196/196 - 34s - loss: 16.9184 - MinusLogProbMetric: 16.9184 - val_loss: 17.0766 - val_MinusLogProbMetric: 17.0766 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 274/1000
2023-09-26 15:59:25.058 
Epoch 274/1000 
	 loss: 16.9962, MinusLogProbMetric: 16.9962, val_loss: 17.2634, val_MinusLogProbMetric: 17.2634

Epoch 274: val_loss did not improve from 17.04237
196/196 - 33s - loss: 16.9962 - MinusLogProbMetric: 16.9962 - val_loss: 17.2634 - val_MinusLogProbMetric: 17.2634 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 275/1000
2023-09-26 15:59:58.723 
Epoch 275/1000 
	 loss: 16.9048, MinusLogProbMetric: 16.9048, val_loss: 17.1030, val_MinusLogProbMetric: 17.1030

Epoch 275: val_loss did not improve from 17.04237
196/196 - 34s - loss: 16.9048 - MinusLogProbMetric: 16.9048 - val_loss: 17.1030 - val_MinusLogProbMetric: 17.1030 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 276/1000
2023-09-26 16:00:32.583 
Epoch 276/1000 
	 loss: 16.9004, MinusLogProbMetric: 16.9004, val_loss: 17.2944, val_MinusLogProbMetric: 17.2944

Epoch 276: val_loss did not improve from 17.04237
196/196 - 34s - loss: 16.9004 - MinusLogProbMetric: 16.9004 - val_loss: 17.2944 - val_MinusLogProbMetric: 17.2944 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 277/1000
2023-09-26 16:01:06.578 
Epoch 277/1000 
	 loss: 16.9144, MinusLogProbMetric: 16.9144, val_loss: 17.8024, val_MinusLogProbMetric: 17.8024

Epoch 277: val_loss did not improve from 17.04237
196/196 - 34s - loss: 16.9144 - MinusLogProbMetric: 16.9144 - val_loss: 17.8024 - val_MinusLogProbMetric: 17.8024 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 278/1000
2023-09-26 16:01:40.600 
Epoch 278/1000 
	 loss: 16.8849, MinusLogProbMetric: 16.8849, val_loss: 17.5441, val_MinusLogProbMetric: 17.5441

Epoch 278: val_loss did not improve from 17.04237
196/196 - 34s - loss: 16.8849 - MinusLogProbMetric: 16.8849 - val_loss: 17.5441 - val_MinusLogProbMetric: 17.5441 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 279/1000
2023-09-26 16:02:14.373 
Epoch 279/1000 
	 loss: 16.9093, MinusLogProbMetric: 16.9093, val_loss: 17.1976, val_MinusLogProbMetric: 17.1976

Epoch 279: val_loss did not improve from 17.04237
196/196 - 34s - loss: 16.9093 - MinusLogProbMetric: 16.9093 - val_loss: 17.1976 - val_MinusLogProbMetric: 17.1976 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 280/1000
2023-09-26 16:02:47.902 
Epoch 280/1000 
	 loss: 16.9569, MinusLogProbMetric: 16.9569, val_loss: 17.6030, val_MinusLogProbMetric: 17.6030

Epoch 280: val_loss did not improve from 17.04237
196/196 - 34s - loss: 16.9569 - MinusLogProbMetric: 16.9569 - val_loss: 17.6030 - val_MinusLogProbMetric: 17.6030 - lr: 0.0010 - 34s/epoch - 171ms/step
Epoch 281/1000
2023-09-26 16:03:21.730 
Epoch 281/1000 
	 loss: 16.9189, MinusLogProbMetric: 16.9189, val_loss: 17.2087, val_MinusLogProbMetric: 17.2087

Epoch 281: val_loss did not improve from 17.04237
196/196 - 34s - loss: 16.9189 - MinusLogProbMetric: 16.9189 - val_loss: 17.2087 - val_MinusLogProbMetric: 17.2087 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 282/1000
2023-09-26 16:03:55.569 
Epoch 282/1000 
	 loss: 16.8791, MinusLogProbMetric: 16.8791, val_loss: 17.1427, val_MinusLogProbMetric: 17.1427

Epoch 282: val_loss did not improve from 17.04237
196/196 - 34s - loss: 16.8791 - MinusLogProbMetric: 16.8791 - val_loss: 17.1427 - val_MinusLogProbMetric: 17.1427 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 283/1000
2023-09-26 16:04:29.454 
Epoch 283/1000 
	 loss: 16.9510, MinusLogProbMetric: 16.9510, val_loss: 17.3284, val_MinusLogProbMetric: 17.3284

Epoch 283: val_loss did not improve from 17.04237
196/196 - 34s - loss: 16.9510 - MinusLogProbMetric: 16.9510 - val_loss: 17.3284 - val_MinusLogProbMetric: 17.3284 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 284/1000
2023-09-26 16:05:03.467 
Epoch 284/1000 
	 loss: 16.9136, MinusLogProbMetric: 16.9136, val_loss: 17.2269, val_MinusLogProbMetric: 17.2269

Epoch 284: val_loss did not improve from 17.04237
196/196 - 34s - loss: 16.9136 - MinusLogProbMetric: 16.9136 - val_loss: 17.2269 - val_MinusLogProbMetric: 17.2269 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 285/1000
2023-09-26 16:05:37.269 
Epoch 285/1000 
	 loss: 16.9290, MinusLogProbMetric: 16.9290, val_loss: 17.7709, val_MinusLogProbMetric: 17.7709

Epoch 285: val_loss did not improve from 17.04237
196/196 - 34s - loss: 16.9290 - MinusLogProbMetric: 16.9290 - val_loss: 17.7709 - val_MinusLogProbMetric: 17.7709 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 286/1000
2023-09-26 16:06:11.116 
Epoch 286/1000 
	 loss: 16.8886, MinusLogProbMetric: 16.8886, val_loss: 17.2942, val_MinusLogProbMetric: 17.2942

Epoch 286: val_loss did not improve from 17.04237
196/196 - 34s - loss: 16.8886 - MinusLogProbMetric: 16.8886 - val_loss: 17.2942 - val_MinusLogProbMetric: 17.2942 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 287/1000
2023-09-26 16:06:44.954 
Epoch 287/1000 
	 loss: 16.9277, MinusLogProbMetric: 16.9277, val_loss: 17.3927, val_MinusLogProbMetric: 17.3927

Epoch 287: val_loss did not improve from 17.04237
196/196 - 34s - loss: 16.9277 - MinusLogProbMetric: 16.9277 - val_loss: 17.3927 - val_MinusLogProbMetric: 17.3927 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 288/1000
2023-09-26 16:07:18.889 
Epoch 288/1000 
	 loss: 16.9556, MinusLogProbMetric: 16.9556, val_loss: 17.1080, val_MinusLogProbMetric: 17.1080

Epoch 288: val_loss did not improve from 17.04237
196/196 - 34s - loss: 16.9556 - MinusLogProbMetric: 16.9556 - val_loss: 17.1080 - val_MinusLogProbMetric: 17.1080 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 289/1000
2023-09-26 16:07:52.729 
Epoch 289/1000 
	 loss: 16.9049, MinusLogProbMetric: 16.9049, val_loss: 17.2001, val_MinusLogProbMetric: 17.2001

Epoch 289: val_loss did not improve from 17.04237
196/196 - 34s - loss: 16.9049 - MinusLogProbMetric: 16.9049 - val_loss: 17.2001 - val_MinusLogProbMetric: 17.2001 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 290/1000
2023-09-26 16:08:26.476 
Epoch 290/1000 
	 loss: 16.8675, MinusLogProbMetric: 16.8675, val_loss: 17.2164, val_MinusLogProbMetric: 17.2164

Epoch 290: val_loss did not improve from 17.04237
196/196 - 34s - loss: 16.8675 - MinusLogProbMetric: 16.8675 - val_loss: 17.2164 - val_MinusLogProbMetric: 17.2164 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 291/1000
2023-09-26 16:09:00.472 
Epoch 291/1000 
	 loss: 16.9545, MinusLogProbMetric: 16.9545, val_loss: 17.0896, val_MinusLogProbMetric: 17.0896

Epoch 291: val_loss did not improve from 17.04237
196/196 - 34s - loss: 16.9545 - MinusLogProbMetric: 16.9545 - val_loss: 17.0896 - val_MinusLogProbMetric: 17.0896 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 292/1000
2023-09-26 16:09:34.467 
Epoch 292/1000 
	 loss: 16.8379, MinusLogProbMetric: 16.8379, val_loss: 17.2181, val_MinusLogProbMetric: 17.2181

Epoch 292: val_loss did not improve from 17.04237
196/196 - 34s - loss: 16.8379 - MinusLogProbMetric: 16.8379 - val_loss: 17.2181 - val_MinusLogProbMetric: 17.2181 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 293/1000
2023-09-26 16:10:08.437 
Epoch 293/1000 
	 loss: 16.8650, MinusLogProbMetric: 16.8650, val_loss: 17.1020, val_MinusLogProbMetric: 17.1020

Epoch 293: val_loss did not improve from 17.04237
196/196 - 34s - loss: 16.8650 - MinusLogProbMetric: 16.8650 - val_loss: 17.1020 - val_MinusLogProbMetric: 17.1020 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 294/1000
2023-09-26 16:10:42.547 
Epoch 294/1000 
	 loss: 16.9259, MinusLogProbMetric: 16.9259, val_loss: 17.6738, val_MinusLogProbMetric: 17.6738

Epoch 294: val_loss did not improve from 17.04237
196/196 - 34s - loss: 16.9259 - MinusLogProbMetric: 16.9259 - val_loss: 17.6738 - val_MinusLogProbMetric: 17.6738 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 295/1000
2023-09-26 16:11:16.005 
Epoch 295/1000 
	 loss: 16.9223, MinusLogProbMetric: 16.9223, val_loss: 16.9942, val_MinusLogProbMetric: 16.9942

Epoch 295: val_loss improved from 17.04237 to 16.99417, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_289/weights/best_weights.h5
196/196 - 34s - loss: 16.9223 - MinusLogProbMetric: 16.9223 - val_loss: 16.9942 - val_MinusLogProbMetric: 16.9942 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 296/1000
2023-09-26 16:11:50.474 
Epoch 296/1000 
	 loss: 16.9299, MinusLogProbMetric: 16.9299, val_loss: 17.2760, val_MinusLogProbMetric: 17.2760

Epoch 296: val_loss did not improve from 16.99417
196/196 - 34s - loss: 16.9299 - MinusLogProbMetric: 16.9299 - val_loss: 17.2760 - val_MinusLogProbMetric: 17.2760 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 297/1000
2023-09-26 16:12:24.100 
Epoch 297/1000 
	 loss: 16.8770, MinusLogProbMetric: 16.8770, val_loss: 17.1689, val_MinusLogProbMetric: 17.1689

Epoch 297: val_loss did not improve from 16.99417
196/196 - 34s - loss: 16.8770 - MinusLogProbMetric: 16.8770 - val_loss: 17.1689 - val_MinusLogProbMetric: 17.1689 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 298/1000
2023-09-26 16:12:57.898 
Epoch 298/1000 
	 loss: 16.8702, MinusLogProbMetric: 16.8702, val_loss: 17.5123, val_MinusLogProbMetric: 17.5123

Epoch 298: val_loss did not improve from 16.99417
196/196 - 34s - loss: 16.8702 - MinusLogProbMetric: 16.8702 - val_loss: 17.5123 - val_MinusLogProbMetric: 17.5123 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 299/1000
2023-09-26 16:13:31.897 
Epoch 299/1000 
	 loss: 16.8521, MinusLogProbMetric: 16.8521, val_loss: 17.1519, val_MinusLogProbMetric: 17.1519

Epoch 299: val_loss did not improve from 16.99417
196/196 - 34s - loss: 16.8521 - MinusLogProbMetric: 16.8521 - val_loss: 17.1519 - val_MinusLogProbMetric: 17.1519 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 300/1000
2023-09-26 16:14:05.892 
Epoch 300/1000 
	 loss: 16.8675, MinusLogProbMetric: 16.8675, val_loss: 17.0610, val_MinusLogProbMetric: 17.0610

Epoch 300: val_loss did not improve from 16.99417
196/196 - 34s - loss: 16.8675 - MinusLogProbMetric: 16.8675 - val_loss: 17.0610 - val_MinusLogProbMetric: 17.0610 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 301/1000
2023-09-26 16:14:39.976 
Epoch 301/1000 
	 loss: 16.8419, MinusLogProbMetric: 16.8419, val_loss: 17.4217, val_MinusLogProbMetric: 17.4217

Epoch 301: val_loss did not improve from 16.99417
196/196 - 34s - loss: 16.8419 - MinusLogProbMetric: 16.8419 - val_loss: 17.4217 - val_MinusLogProbMetric: 17.4217 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 302/1000
2023-09-26 16:15:14.022 
Epoch 302/1000 
	 loss: 16.8579, MinusLogProbMetric: 16.8579, val_loss: 17.3314, val_MinusLogProbMetric: 17.3314

Epoch 302: val_loss did not improve from 16.99417
196/196 - 34s - loss: 16.8579 - MinusLogProbMetric: 16.8579 - val_loss: 17.3314 - val_MinusLogProbMetric: 17.3314 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 303/1000
2023-09-26 16:15:48.079 
Epoch 303/1000 
	 loss: 16.8465, MinusLogProbMetric: 16.8465, val_loss: 17.8446, val_MinusLogProbMetric: 17.8446

Epoch 303: val_loss did not improve from 16.99417
196/196 - 34s - loss: 16.8465 - MinusLogProbMetric: 16.8465 - val_loss: 17.8446 - val_MinusLogProbMetric: 17.8446 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 304/1000
2023-09-26 16:16:22.483 
Epoch 304/1000 
	 loss: 16.8841, MinusLogProbMetric: 16.8841, val_loss: 17.1804, val_MinusLogProbMetric: 17.1804

Epoch 304: val_loss did not improve from 16.99417
196/196 - 34s - loss: 16.8841 - MinusLogProbMetric: 16.8841 - val_loss: 17.1804 - val_MinusLogProbMetric: 17.1804 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 305/1000
2023-09-26 16:16:56.914 
Epoch 305/1000 
	 loss: 16.8593, MinusLogProbMetric: 16.8593, val_loss: 17.2829, val_MinusLogProbMetric: 17.2829

Epoch 305: val_loss did not improve from 16.99417
196/196 - 34s - loss: 16.8593 - MinusLogProbMetric: 16.8593 - val_loss: 17.2829 - val_MinusLogProbMetric: 17.2829 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 306/1000
2023-09-26 16:17:31.185 
Epoch 306/1000 
	 loss: 16.9107, MinusLogProbMetric: 16.9107, val_loss: 17.3349, val_MinusLogProbMetric: 17.3349

Epoch 306: val_loss did not improve from 16.99417
196/196 - 34s - loss: 16.9107 - MinusLogProbMetric: 16.9107 - val_loss: 17.3349 - val_MinusLogProbMetric: 17.3349 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 307/1000
2023-09-26 16:18:05.298 
Epoch 307/1000 
	 loss: 16.8924, MinusLogProbMetric: 16.8924, val_loss: 17.1899, val_MinusLogProbMetric: 17.1899

Epoch 307: val_loss did not improve from 16.99417
196/196 - 34s - loss: 16.8924 - MinusLogProbMetric: 16.8924 - val_loss: 17.1899 - val_MinusLogProbMetric: 17.1899 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 308/1000
2023-09-26 16:18:39.511 
Epoch 308/1000 
	 loss: 16.8412, MinusLogProbMetric: 16.8412, val_loss: 18.3851, val_MinusLogProbMetric: 18.3851

Epoch 308: val_loss did not improve from 16.99417
196/196 - 34s - loss: 16.8412 - MinusLogProbMetric: 16.8412 - val_loss: 18.3851 - val_MinusLogProbMetric: 18.3851 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 309/1000
2023-09-26 16:19:13.683 
Epoch 309/1000 
	 loss: 16.8715, MinusLogProbMetric: 16.8715, val_loss: 17.9757, val_MinusLogProbMetric: 17.9757

Epoch 309: val_loss did not improve from 16.99417
196/196 - 34s - loss: 16.8715 - MinusLogProbMetric: 16.8715 - val_loss: 17.9757 - val_MinusLogProbMetric: 17.9757 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 310/1000
2023-09-26 16:19:48.072 
Epoch 310/1000 
	 loss: 16.8511, MinusLogProbMetric: 16.8511, val_loss: 17.2190, val_MinusLogProbMetric: 17.2190

Epoch 310: val_loss did not improve from 16.99417
196/196 - 34s - loss: 16.8511 - MinusLogProbMetric: 16.8511 - val_loss: 17.2190 - val_MinusLogProbMetric: 17.2190 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 311/1000
2023-09-26 16:20:22.281 
Epoch 311/1000 
	 loss: 16.8192, MinusLogProbMetric: 16.8192, val_loss: 17.2108, val_MinusLogProbMetric: 17.2108

Epoch 311: val_loss did not improve from 16.99417
196/196 - 34s - loss: 16.8192 - MinusLogProbMetric: 16.8192 - val_loss: 17.2108 - val_MinusLogProbMetric: 17.2108 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 312/1000
2023-09-26 16:20:56.345 
Epoch 312/1000 
	 loss: 16.8532, MinusLogProbMetric: 16.8532, val_loss: 16.9618, val_MinusLogProbMetric: 16.9618

Epoch 312: val_loss improved from 16.99417 to 16.96177, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_289/weights/best_weights.h5
196/196 - 35s - loss: 16.8532 - MinusLogProbMetric: 16.8532 - val_loss: 16.9618 - val_MinusLogProbMetric: 16.9618 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 313/1000
2023-09-26 16:21:30.813 
Epoch 313/1000 
	 loss: 16.8560, MinusLogProbMetric: 16.8560, val_loss: 17.3967, val_MinusLogProbMetric: 17.3967

Epoch 313: val_loss did not improve from 16.96177
196/196 - 34s - loss: 16.8560 - MinusLogProbMetric: 16.8560 - val_loss: 17.3967 - val_MinusLogProbMetric: 17.3967 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 314/1000
2023-09-26 16:22:05.164 
Epoch 314/1000 
	 loss: 16.8604, MinusLogProbMetric: 16.8604, val_loss: 17.6745, val_MinusLogProbMetric: 17.6745

Epoch 314: val_loss did not improve from 16.96177
196/196 - 34s - loss: 16.8604 - MinusLogProbMetric: 16.8604 - val_loss: 17.6745 - val_MinusLogProbMetric: 17.6745 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 315/1000
2023-09-26 16:22:39.572 
Epoch 315/1000 
	 loss: 16.8563, MinusLogProbMetric: 16.8563, val_loss: 17.2732, val_MinusLogProbMetric: 17.2732

Epoch 315: val_loss did not improve from 16.96177
196/196 - 34s - loss: 16.8563 - MinusLogProbMetric: 16.8563 - val_loss: 17.2732 - val_MinusLogProbMetric: 17.2732 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 316/1000
2023-09-26 16:23:13.687 
Epoch 316/1000 
	 loss: 16.8110, MinusLogProbMetric: 16.8110, val_loss: 17.0269, val_MinusLogProbMetric: 17.0269

Epoch 316: val_loss did not improve from 16.96177
196/196 - 34s - loss: 16.8110 - MinusLogProbMetric: 16.8110 - val_loss: 17.0269 - val_MinusLogProbMetric: 17.0269 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 317/1000
2023-09-26 16:23:47.852 
Epoch 317/1000 
	 loss: 16.8307, MinusLogProbMetric: 16.8307, val_loss: 17.1882, val_MinusLogProbMetric: 17.1882

Epoch 317: val_loss did not improve from 16.96177
196/196 - 34s - loss: 16.8307 - MinusLogProbMetric: 16.8307 - val_loss: 17.1882 - val_MinusLogProbMetric: 17.1882 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 318/1000
2023-09-26 16:24:22.074 
Epoch 318/1000 
	 loss: 16.8362, MinusLogProbMetric: 16.8362, val_loss: 17.2637, val_MinusLogProbMetric: 17.2637

Epoch 318: val_loss did not improve from 16.96177
196/196 - 34s - loss: 16.8362 - MinusLogProbMetric: 16.8362 - val_loss: 17.2637 - val_MinusLogProbMetric: 17.2637 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 319/1000
2023-09-26 16:24:56.307 
Epoch 319/1000 
	 loss: 16.8471, MinusLogProbMetric: 16.8471, val_loss: 17.1969, val_MinusLogProbMetric: 17.1969

Epoch 319: val_loss did not improve from 16.96177
196/196 - 34s - loss: 16.8471 - MinusLogProbMetric: 16.8471 - val_loss: 17.1969 - val_MinusLogProbMetric: 17.1969 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 320/1000
2023-09-26 16:25:30.414 
Epoch 320/1000 
	 loss: 16.8510, MinusLogProbMetric: 16.8510, val_loss: 17.0699, val_MinusLogProbMetric: 17.0699

Epoch 320: val_loss did not improve from 16.96177
196/196 - 34s - loss: 16.8510 - MinusLogProbMetric: 16.8510 - val_loss: 17.0699 - val_MinusLogProbMetric: 17.0699 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 321/1000
2023-09-26 16:26:05.075 
Epoch 321/1000 
	 loss: 16.8497, MinusLogProbMetric: 16.8497, val_loss: 17.4422, val_MinusLogProbMetric: 17.4422

Epoch 321: val_loss did not improve from 16.96177
196/196 - 35s - loss: 16.8497 - MinusLogProbMetric: 16.8497 - val_loss: 17.4422 - val_MinusLogProbMetric: 17.4422 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 322/1000
2023-09-26 16:26:38.872 
Epoch 322/1000 
	 loss: 16.8347, MinusLogProbMetric: 16.8347, val_loss: 17.3830, val_MinusLogProbMetric: 17.3830

Epoch 322: val_loss did not improve from 16.96177
196/196 - 34s - loss: 16.8347 - MinusLogProbMetric: 16.8347 - val_loss: 17.3830 - val_MinusLogProbMetric: 17.3830 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 323/1000
2023-09-26 16:27:13.224 
Epoch 323/1000 
	 loss: 16.8484, MinusLogProbMetric: 16.8484, val_loss: 17.0486, val_MinusLogProbMetric: 17.0486

Epoch 323: val_loss did not improve from 16.96177
196/196 - 34s - loss: 16.8484 - MinusLogProbMetric: 16.8484 - val_loss: 17.0486 - val_MinusLogProbMetric: 17.0486 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 324/1000
2023-09-26 16:27:47.381 
Epoch 324/1000 
	 loss: 16.7889, MinusLogProbMetric: 16.7889, val_loss: 17.7081, val_MinusLogProbMetric: 17.7081

Epoch 324: val_loss did not improve from 16.96177
196/196 - 34s - loss: 16.7889 - MinusLogProbMetric: 16.7889 - val_loss: 17.7081 - val_MinusLogProbMetric: 17.7081 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 325/1000
2023-09-26 16:28:21.469 
Epoch 325/1000 
	 loss: 16.8350, MinusLogProbMetric: 16.8350, val_loss: 17.0933, val_MinusLogProbMetric: 17.0933

Epoch 325: val_loss did not improve from 16.96177
196/196 - 34s - loss: 16.8350 - MinusLogProbMetric: 16.8350 - val_loss: 17.0933 - val_MinusLogProbMetric: 17.0933 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 326/1000
2023-09-26 16:28:55.451 
Epoch 326/1000 
	 loss: 16.8527, MinusLogProbMetric: 16.8527, val_loss: 17.0687, val_MinusLogProbMetric: 17.0687

Epoch 326: val_loss did not improve from 16.96177
196/196 - 34s - loss: 16.8527 - MinusLogProbMetric: 16.8527 - val_loss: 17.0687 - val_MinusLogProbMetric: 17.0687 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 327/1000
2023-09-26 16:29:29.841 
Epoch 327/1000 
	 loss: 16.7967, MinusLogProbMetric: 16.7967, val_loss: 17.0187, val_MinusLogProbMetric: 17.0187

Epoch 327: val_loss did not improve from 16.96177
196/196 - 34s - loss: 16.7967 - MinusLogProbMetric: 16.7967 - val_loss: 17.0187 - val_MinusLogProbMetric: 17.0187 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 328/1000
2023-09-26 16:30:03.585 
Epoch 328/1000 
	 loss: 16.8277, MinusLogProbMetric: 16.8277, val_loss: 17.0504, val_MinusLogProbMetric: 17.0504

Epoch 328: val_loss did not improve from 16.96177
196/196 - 34s - loss: 16.8277 - MinusLogProbMetric: 16.8277 - val_loss: 17.0504 - val_MinusLogProbMetric: 17.0504 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 329/1000
2023-09-26 16:30:37.787 
Epoch 329/1000 
	 loss: 16.8511, MinusLogProbMetric: 16.8511, val_loss: 17.2123, val_MinusLogProbMetric: 17.2123

Epoch 329: val_loss did not improve from 16.96177
196/196 - 34s - loss: 16.8511 - MinusLogProbMetric: 16.8511 - val_loss: 17.2123 - val_MinusLogProbMetric: 17.2123 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 330/1000
2023-09-26 16:31:11.842 
Epoch 330/1000 
	 loss: 16.7989, MinusLogProbMetric: 16.7989, val_loss: 17.5648, val_MinusLogProbMetric: 17.5648

Epoch 330: val_loss did not improve from 16.96177
196/196 - 34s - loss: 16.7989 - MinusLogProbMetric: 16.7989 - val_loss: 17.5648 - val_MinusLogProbMetric: 17.5648 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 331/1000
2023-09-26 16:31:46.373 
Epoch 331/1000 
	 loss: 16.8602, MinusLogProbMetric: 16.8602, val_loss: 17.0429, val_MinusLogProbMetric: 17.0429

Epoch 331: val_loss did not improve from 16.96177
196/196 - 35s - loss: 16.8602 - MinusLogProbMetric: 16.8602 - val_loss: 17.0429 - val_MinusLogProbMetric: 17.0429 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 332/1000
2023-09-26 16:32:20.963 
Epoch 332/1000 
	 loss: 16.8511, MinusLogProbMetric: 16.8511, val_loss: 17.2062, val_MinusLogProbMetric: 17.2062

Epoch 332: val_loss did not improve from 16.96177
196/196 - 35s - loss: 16.8511 - MinusLogProbMetric: 16.8511 - val_loss: 17.2062 - val_MinusLogProbMetric: 17.2062 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 333/1000
2023-09-26 16:32:55.030 
Epoch 333/1000 
	 loss: 16.7961, MinusLogProbMetric: 16.7961, val_loss: 17.1437, val_MinusLogProbMetric: 17.1437

Epoch 333: val_loss did not improve from 16.96177
196/196 - 34s - loss: 16.7961 - MinusLogProbMetric: 16.7961 - val_loss: 17.1437 - val_MinusLogProbMetric: 17.1437 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 334/1000
2023-09-26 16:33:29.347 
Epoch 334/1000 
	 loss: 16.8992, MinusLogProbMetric: 16.8992, val_loss: 17.3288, val_MinusLogProbMetric: 17.3288

Epoch 334: val_loss did not improve from 16.96177
196/196 - 34s - loss: 16.8992 - MinusLogProbMetric: 16.8992 - val_loss: 17.3288 - val_MinusLogProbMetric: 17.3288 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 335/1000
2023-09-26 16:34:03.254 
Epoch 335/1000 
	 loss: 16.7768, MinusLogProbMetric: 16.7768, val_loss: 17.2104, val_MinusLogProbMetric: 17.2104

Epoch 335: val_loss did not improve from 16.96177
196/196 - 34s - loss: 16.7768 - MinusLogProbMetric: 16.7768 - val_loss: 17.2104 - val_MinusLogProbMetric: 17.2104 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 336/1000
2023-09-26 16:34:37.594 
Epoch 336/1000 
	 loss: 16.8404, MinusLogProbMetric: 16.8404, val_loss: 18.6810, val_MinusLogProbMetric: 18.6810

Epoch 336: val_loss did not improve from 16.96177
196/196 - 34s - loss: 16.8404 - MinusLogProbMetric: 16.8404 - val_loss: 18.6810 - val_MinusLogProbMetric: 18.6810 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 337/1000
2023-09-26 16:35:11.885 
Epoch 337/1000 
	 loss: 16.9024, MinusLogProbMetric: 16.9024, val_loss: 17.0711, val_MinusLogProbMetric: 17.0711

Epoch 337: val_loss did not improve from 16.96177
196/196 - 34s - loss: 16.9024 - MinusLogProbMetric: 16.9024 - val_loss: 17.0711 - val_MinusLogProbMetric: 17.0711 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 338/1000
2023-09-26 16:35:45.901 
Epoch 338/1000 
	 loss: 16.7967, MinusLogProbMetric: 16.7967, val_loss: 17.6324, val_MinusLogProbMetric: 17.6324

Epoch 338: val_loss did not improve from 16.96177
196/196 - 34s - loss: 16.7967 - MinusLogProbMetric: 16.7967 - val_loss: 17.6324 - val_MinusLogProbMetric: 17.6324 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 339/1000
2023-09-26 16:36:19.940 
Epoch 339/1000 
	 loss: 16.8066, MinusLogProbMetric: 16.8066, val_loss: 17.3784, val_MinusLogProbMetric: 17.3784

Epoch 339: val_loss did not improve from 16.96177
196/196 - 34s - loss: 16.8066 - MinusLogProbMetric: 16.8066 - val_loss: 17.3784 - val_MinusLogProbMetric: 17.3784 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 340/1000
2023-09-26 16:36:54.271 
Epoch 340/1000 
	 loss: 16.7919, MinusLogProbMetric: 16.7919, val_loss: 17.7771, val_MinusLogProbMetric: 17.7771

Epoch 340: val_loss did not improve from 16.96177
196/196 - 34s - loss: 16.7919 - MinusLogProbMetric: 16.7919 - val_loss: 17.7771 - val_MinusLogProbMetric: 17.7771 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 341/1000
2023-09-26 16:37:28.183 
Epoch 341/1000 
	 loss: 16.8125, MinusLogProbMetric: 16.8125, val_loss: 17.0972, val_MinusLogProbMetric: 17.0972

Epoch 341: val_loss did not improve from 16.96177
196/196 - 34s - loss: 16.8125 - MinusLogProbMetric: 16.8125 - val_loss: 17.0972 - val_MinusLogProbMetric: 17.0972 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 342/1000
2023-09-26 16:38:02.025 
Epoch 342/1000 
	 loss: 16.8169, MinusLogProbMetric: 16.8169, val_loss: 17.0794, val_MinusLogProbMetric: 17.0794

Epoch 342: val_loss did not improve from 16.96177
196/196 - 34s - loss: 16.8169 - MinusLogProbMetric: 16.8169 - val_loss: 17.0794 - val_MinusLogProbMetric: 17.0794 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 343/1000
2023-09-26 16:38:35.831 
Epoch 343/1000 
	 loss: 16.8170, MinusLogProbMetric: 16.8170, val_loss: 17.0351, val_MinusLogProbMetric: 17.0351

Epoch 343: val_loss did not improve from 16.96177
196/196 - 34s - loss: 16.8170 - MinusLogProbMetric: 16.8170 - val_loss: 17.0351 - val_MinusLogProbMetric: 17.0351 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 344/1000
2023-09-26 16:39:09.813 
Epoch 344/1000 
	 loss: 16.7753, MinusLogProbMetric: 16.7753, val_loss: 17.3542, val_MinusLogProbMetric: 17.3542

Epoch 344: val_loss did not improve from 16.96177
196/196 - 34s - loss: 16.7753 - MinusLogProbMetric: 16.7753 - val_loss: 17.3542 - val_MinusLogProbMetric: 17.3542 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 345/1000
2023-09-26 16:39:43.708 
Epoch 345/1000 
	 loss: 16.8322, MinusLogProbMetric: 16.8322, val_loss: 17.1017, val_MinusLogProbMetric: 17.1017

Epoch 345: val_loss did not improve from 16.96177
196/196 - 34s - loss: 16.8322 - MinusLogProbMetric: 16.8322 - val_loss: 17.1017 - val_MinusLogProbMetric: 17.1017 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 346/1000
2023-09-26 16:40:18.168 
Epoch 346/1000 
	 loss: 16.8058, MinusLogProbMetric: 16.8058, val_loss: 17.0744, val_MinusLogProbMetric: 17.0744

Epoch 346: val_loss did not improve from 16.96177
196/196 - 34s - loss: 16.8058 - MinusLogProbMetric: 16.8058 - val_loss: 17.0744 - val_MinusLogProbMetric: 17.0744 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 347/1000
2023-09-26 16:40:52.117 
Epoch 347/1000 
	 loss: 16.8107, MinusLogProbMetric: 16.8107, val_loss: 17.1872, val_MinusLogProbMetric: 17.1872

Epoch 347: val_loss did not improve from 16.96177
196/196 - 34s - loss: 16.8107 - MinusLogProbMetric: 16.8107 - val_loss: 17.1872 - val_MinusLogProbMetric: 17.1872 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 348/1000
2023-09-26 16:41:26.464 
Epoch 348/1000 
	 loss: 16.8160, MinusLogProbMetric: 16.8160, val_loss: 17.3637, val_MinusLogProbMetric: 17.3637

Epoch 348: val_loss did not improve from 16.96177
196/196 - 34s - loss: 16.8160 - MinusLogProbMetric: 16.8160 - val_loss: 17.3637 - val_MinusLogProbMetric: 17.3637 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 349/1000
2023-09-26 16:42:00.256 
Epoch 349/1000 
	 loss: 16.8299, MinusLogProbMetric: 16.8299, val_loss: 17.3808, val_MinusLogProbMetric: 17.3808

Epoch 349: val_loss did not improve from 16.96177
196/196 - 34s - loss: 16.8299 - MinusLogProbMetric: 16.8299 - val_loss: 17.3808 - val_MinusLogProbMetric: 17.3808 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 350/1000
2023-09-26 16:42:34.209 
Epoch 350/1000 
	 loss: 16.7941, MinusLogProbMetric: 16.7941, val_loss: 17.2940, val_MinusLogProbMetric: 17.2940

Epoch 350: val_loss did not improve from 16.96177
196/196 - 34s - loss: 16.7941 - MinusLogProbMetric: 16.7941 - val_loss: 17.2940 - val_MinusLogProbMetric: 17.2940 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 351/1000
2023-09-26 16:43:08.566 
Epoch 351/1000 
	 loss: 16.8088, MinusLogProbMetric: 16.8088, val_loss: 17.2662, val_MinusLogProbMetric: 17.2662

Epoch 351: val_loss did not improve from 16.96177
196/196 - 34s - loss: 16.8088 - MinusLogProbMetric: 16.8088 - val_loss: 17.2662 - val_MinusLogProbMetric: 17.2662 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 352/1000
2023-09-26 16:43:43.089 
Epoch 352/1000 
	 loss: 16.7515, MinusLogProbMetric: 16.7515, val_loss: 17.1830, val_MinusLogProbMetric: 17.1830

Epoch 352: val_loss did not improve from 16.96177
196/196 - 35s - loss: 16.7515 - MinusLogProbMetric: 16.7515 - val_loss: 17.1830 - val_MinusLogProbMetric: 17.1830 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 353/1000
2023-09-26 16:44:16.955 
Epoch 353/1000 
	 loss: 16.8277, MinusLogProbMetric: 16.8277, val_loss: 17.3644, val_MinusLogProbMetric: 17.3644

Epoch 353: val_loss did not improve from 16.96177
196/196 - 34s - loss: 16.8277 - MinusLogProbMetric: 16.8277 - val_loss: 17.3644 - val_MinusLogProbMetric: 17.3644 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 354/1000
2023-09-26 16:44:50.819 
Epoch 354/1000 
	 loss: 16.8152, MinusLogProbMetric: 16.8152, val_loss: 17.3132, val_MinusLogProbMetric: 17.3132

Epoch 354: val_loss did not improve from 16.96177
196/196 - 34s - loss: 16.8152 - MinusLogProbMetric: 16.8152 - val_loss: 17.3132 - val_MinusLogProbMetric: 17.3132 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 355/1000
2023-09-26 16:45:24.961 
Epoch 355/1000 
	 loss: 16.7689, MinusLogProbMetric: 16.7689, val_loss: 17.1505, val_MinusLogProbMetric: 17.1505

Epoch 355: val_loss did not improve from 16.96177
196/196 - 34s - loss: 16.7689 - MinusLogProbMetric: 16.7689 - val_loss: 17.1505 - val_MinusLogProbMetric: 17.1505 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 356/1000
2023-09-26 16:45:59.092 
Epoch 356/1000 
	 loss: 16.8850, MinusLogProbMetric: 16.8850, val_loss: 17.0492, val_MinusLogProbMetric: 17.0492

Epoch 356: val_loss did not improve from 16.96177
196/196 - 34s - loss: 16.8850 - MinusLogProbMetric: 16.8850 - val_loss: 17.0492 - val_MinusLogProbMetric: 17.0492 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 357/1000
2023-09-26 16:46:32.977 
Epoch 357/1000 
	 loss: 16.7575, MinusLogProbMetric: 16.7575, val_loss: 17.2583, val_MinusLogProbMetric: 17.2583

Epoch 357: val_loss did not improve from 16.96177
196/196 - 34s - loss: 16.7575 - MinusLogProbMetric: 16.7575 - val_loss: 17.2583 - val_MinusLogProbMetric: 17.2583 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 358/1000
2023-09-26 16:47:07.121 
Epoch 358/1000 
	 loss: 16.7578, MinusLogProbMetric: 16.7578, val_loss: 17.0080, val_MinusLogProbMetric: 17.0080

Epoch 358: val_loss did not improve from 16.96177
196/196 - 34s - loss: 16.7578 - MinusLogProbMetric: 16.7578 - val_loss: 17.0080 - val_MinusLogProbMetric: 17.0080 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 359/1000
2023-09-26 16:47:41.125 
Epoch 359/1000 
	 loss: 16.8327, MinusLogProbMetric: 16.8327, val_loss: 17.1551, val_MinusLogProbMetric: 17.1551

Epoch 359: val_loss did not improve from 16.96177
196/196 - 34s - loss: 16.8327 - MinusLogProbMetric: 16.8327 - val_loss: 17.1551 - val_MinusLogProbMetric: 17.1551 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 360/1000
2023-09-26 16:48:15.145 
Epoch 360/1000 
	 loss: 16.8095, MinusLogProbMetric: 16.8095, val_loss: 17.0622, val_MinusLogProbMetric: 17.0622

Epoch 360: val_loss did not improve from 16.96177
196/196 - 34s - loss: 16.8095 - MinusLogProbMetric: 16.8095 - val_loss: 17.0622 - val_MinusLogProbMetric: 17.0622 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 361/1000
2023-09-26 16:48:49.254 
Epoch 361/1000 
	 loss: 16.7880, MinusLogProbMetric: 16.7880, val_loss: 17.1646, val_MinusLogProbMetric: 17.1646

Epoch 361: val_loss did not improve from 16.96177
196/196 - 34s - loss: 16.7880 - MinusLogProbMetric: 16.7880 - val_loss: 17.1646 - val_MinusLogProbMetric: 17.1646 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 362/1000
2023-09-26 16:49:23.485 
Epoch 362/1000 
	 loss: 16.7512, MinusLogProbMetric: 16.7512, val_loss: 16.9829, val_MinusLogProbMetric: 16.9829

Epoch 362: val_loss did not improve from 16.96177
196/196 - 34s - loss: 16.7512 - MinusLogProbMetric: 16.7512 - val_loss: 16.9829 - val_MinusLogProbMetric: 16.9829 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 363/1000
2023-09-26 16:49:57.695 
Epoch 363/1000 
	 loss: 16.5298, MinusLogProbMetric: 16.5298, val_loss: 16.9565, val_MinusLogProbMetric: 16.9565

Epoch 363: val_loss improved from 16.96177 to 16.95651, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_289/weights/best_weights.h5
196/196 - 35s - loss: 16.5298 - MinusLogProbMetric: 16.5298 - val_loss: 16.9565 - val_MinusLogProbMetric: 16.9565 - lr: 5.0000e-04 - 35s/epoch - 177ms/step
Epoch 364/1000
2023-09-26 16:50:32.395 
Epoch 364/1000 
	 loss: 16.5215, MinusLogProbMetric: 16.5215, val_loss: 17.0303, val_MinusLogProbMetric: 17.0303

Epoch 364: val_loss did not improve from 16.95651
196/196 - 34s - loss: 16.5215 - MinusLogProbMetric: 16.5215 - val_loss: 17.0303 - val_MinusLogProbMetric: 17.0303 - lr: 5.0000e-04 - 34s/epoch - 174ms/step
Epoch 365/1000
2023-09-26 16:51:06.504 
Epoch 365/1000 
	 loss: 16.5202, MinusLogProbMetric: 16.5202, val_loss: 17.0360, val_MinusLogProbMetric: 17.0360

Epoch 365: val_loss did not improve from 16.95651
196/196 - 34s - loss: 16.5202 - MinusLogProbMetric: 16.5202 - val_loss: 17.0360 - val_MinusLogProbMetric: 17.0360 - lr: 5.0000e-04 - 34s/epoch - 174ms/step
Epoch 366/1000
2023-09-26 16:51:40.493 
Epoch 366/1000 
	 loss: 16.5406, MinusLogProbMetric: 16.5406, val_loss: 16.9641, val_MinusLogProbMetric: 16.9641

Epoch 366: val_loss did not improve from 16.95651
196/196 - 34s - loss: 16.5406 - MinusLogProbMetric: 16.5406 - val_loss: 16.9641 - val_MinusLogProbMetric: 16.9641 - lr: 5.0000e-04 - 34s/epoch - 173ms/step
Epoch 367/1000
2023-09-26 16:52:14.560 
Epoch 367/1000 
	 loss: 16.5520, MinusLogProbMetric: 16.5520, val_loss: 16.9384, val_MinusLogProbMetric: 16.9384

Epoch 367: val_loss improved from 16.95651 to 16.93841, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_289/weights/best_weights.h5
196/196 - 35s - loss: 16.5520 - MinusLogProbMetric: 16.5520 - val_loss: 16.9384 - val_MinusLogProbMetric: 16.9384 - lr: 5.0000e-04 - 35s/epoch - 177ms/step
Epoch 368/1000
2023-09-26 16:52:49.412 
Epoch 368/1000 
	 loss: 16.5212, MinusLogProbMetric: 16.5212, val_loss: 16.9572, val_MinusLogProbMetric: 16.9572

Epoch 368: val_loss did not improve from 16.93841
196/196 - 34s - loss: 16.5212 - MinusLogProbMetric: 16.5212 - val_loss: 16.9572 - val_MinusLogProbMetric: 16.9572 - lr: 5.0000e-04 - 34s/epoch - 175ms/step
Epoch 369/1000
2023-09-26 16:53:23.604 
Epoch 369/1000 
	 loss: 16.5187, MinusLogProbMetric: 16.5187, val_loss: 17.0078, val_MinusLogProbMetric: 17.0078

Epoch 369: val_loss did not improve from 16.93841
196/196 - 34s - loss: 16.5187 - MinusLogProbMetric: 16.5187 - val_loss: 17.0078 - val_MinusLogProbMetric: 17.0078 - lr: 5.0000e-04 - 34s/epoch - 174ms/step
Epoch 370/1000
2023-09-26 16:53:57.994 
Epoch 370/1000 
	 loss: 16.5437, MinusLogProbMetric: 16.5437, val_loss: 16.9664, val_MinusLogProbMetric: 16.9664

Epoch 370: val_loss did not improve from 16.93841
196/196 - 34s - loss: 16.5437 - MinusLogProbMetric: 16.5437 - val_loss: 16.9664 - val_MinusLogProbMetric: 16.9664 - lr: 5.0000e-04 - 34s/epoch - 175ms/step
Epoch 371/1000
2023-09-26 16:54:32.042 
Epoch 371/1000 
	 loss: 16.5249, MinusLogProbMetric: 16.5249, val_loss: 17.1030, val_MinusLogProbMetric: 17.1030

Epoch 371: val_loss did not improve from 16.93841
196/196 - 34s - loss: 16.5249 - MinusLogProbMetric: 16.5249 - val_loss: 17.1030 - val_MinusLogProbMetric: 17.1030 - lr: 5.0000e-04 - 34s/epoch - 174ms/step
Epoch 372/1000
2023-09-26 16:55:06.284 
Epoch 372/1000 
	 loss: 16.5549, MinusLogProbMetric: 16.5549, val_loss: 16.9125, val_MinusLogProbMetric: 16.9125

Epoch 372: val_loss improved from 16.93841 to 16.91251, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_289/weights/best_weights.h5
196/196 - 35s - loss: 16.5549 - MinusLogProbMetric: 16.5549 - val_loss: 16.9125 - val_MinusLogProbMetric: 16.9125 - lr: 5.0000e-04 - 35s/epoch - 177ms/step
Epoch 373/1000
2023-09-26 16:55:40.563 
Epoch 373/1000 
	 loss: 16.5251, MinusLogProbMetric: 16.5251, val_loss: 16.9406, val_MinusLogProbMetric: 16.9406

Epoch 373: val_loss did not improve from 16.91251
196/196 - 34s - loss: 16.5251 - MinusLogProbMetric: 16.5251 - val_loss: 16.9406 - val_MinusLogProbMetric: 16.9406 - lr: 5.0000e-04 - 34s/epoch - 172ms/step
Epoch 374/1000
2023-09-26 16:56:14.526 
Epoch 374/1000 
	 loss: 16.5134, MinusLogProbMetric: 16.5134, val_loss: 17.0173, val_MinusLogProbMetric: 17.0173

Epoch 374: val_loss did not improve from 16.91251
196/196 - 34s - loss: 16.5134 - MinusLogProbMetric: 16.5134 - val_loss: 17.0173 - val_MinusLogProbMetric: 17.0173 - lr: 5.0000e-04 - 34s/epoch - 173ms/step
Epoch 375/1000
2023-09-26 16:56:48.670 
Epoch 375/1000 
	 loss: 16.5206, MinusLogProbMetric: 16.5206, val_loss: 17.0589, val_MinusLogProbMetric: 17.0589

Epoch 375: val_loss did not improve from 16.91251
196/196 - 34s - loss: 16.5206 - MinusLogProbMetric: 16.5206 - val_loss: 17.0589 - val_MinusLogProbMetric: 17.0589 - lr: 5.0000e-04 - 34s/epoch - 174ms/step
Epoch 376/1000
2023-09-26 16:57:23.016 
Epoch 376/1000 
	 loss: 16.5466, MinusLogProbMetric: 16.5466, val_loss: 16.8895, val_MinusLogProbMetric: 16.8895

Epoch 376: val_loss improved from 16.91251 to 16.88949, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_289/weights/best_weights.h5
196/196 - 35s - loss: 16.5466 - MinusLogProbMetric: 16.5466 - val_loss: 16.8895 - val_MinusLogProbMetric: 16.8895 - lr: 5.0000e-04 - 35s/epoch - 179ms/step
Epoch 377/1000
2023-09-26 16:57:57.736 
Epoch 377/1000 
	 loss: 16.5351, MinusLogProbMetric: 16.5351, val_loss: 17.1393, val_MinusLogProbMetric: 17.1393

Epoch 377: val_loss did not improve from 16.88949
196/196 - 34s - loss: 16.5351 - MinusLogProbMetric: 16.5351 - val_loss: 17.1393 - val_MinusLogProbMetric: 17.1393 - lr: 5.0000e-04 - 34s/epoch - 174ms/step
Epoch 378/1000
2023-09-26 16:58:31.831 
Epoch 378/1000 
	 loss: 16.5209, MinusLogProbMetric: 16.5209, val_loss: 16.8781, val_MinusLogProbMetric: 16.8781

Epoch 378: val_loss improved from 16.88949 to 16.87806, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_289/weights/best_weights.h5
196/196 - 35s - loss: 16.5209 - MinusLogProbMetric: 16.5209 - val_loss: 16.8781 - val_MinusLogProbMetric: 16.8781 - lr: 5.0000e-04 - 35s/epoch - 177ms/step
Epoch 379/1000
2023-09-26 16:59:06.543 
Epoch 379/1000 
	 loss: 16.5264, MinusLogProbMetric: 16.5264, val_loss: 17.0473, val_MinusLogProbMetric: 17.0473

Epoch 379: val_loss did not improve from 16.87806
196/196 - 34s - loss: 16.5264 - MinusLogProbMetric: 16.5264 - val_loss: 17.0473 - val_MinusLogProbMetric: 17.0473 - lr: 5.0000e-04 - 34s/epoch - 174ms/step
Epoch 380/1000
2023-09-26 16:59:40.237 
Epoch 380/1000 
	 loss: 16.5511, MinusLogProbMetric: 16.5511, val_loss: 16.9376, val_MinusLogProbMetric: 16.9376

Epoch 380: val_loss did not improve from 16.87806
196/196 - 34s - loss: 16.5511 - MinusLogProbMetric: 16.5511 - val_loss: 16.9376 - val_MinusLogProbMetric: 16.9376 - lr: 5.0000e-04 - 34s/epoch - 172ms/step
Epoch 381/1000
2023-09-26 17:00:14.552 
Epoch 381/1000 
	 loss: 16.5086, MinusLogProbMetric: 16.5086, val_loss: 16.8459, val_MinusLogProbMetric: 16.8459

Epoch 381: val_loss improved from 16.87806 to 16.84587, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_289/weights/best_weights.h5
196/196 - 35s - loss: 16.5086 - MinusLogProbMetric: 16.5086 - val_loss: 16.8459 - val_MinusLogProbMetric: 16.8459 - lr: 5.0000e-04 - 35s/epoch - 178ms/step
Epoch 382/1000
2023-09-26 17:00:49.152 
Epoch 382/1000 
	 loss: 16.5138, MinusLogProbMetric: 16.5138, val_loss: 16.9124, val_MinusLogProbMetric: 16.9124

Epoch 382: val_loss did not improve from 16.84587
196/196 - 34s - loss: 16.5138 - MinusLogProbMetric: 16.5138 - val_loss: 16.9124 - val_MinusLogProbMetric: 16.9124 - lr: 5.0000e-04 - 34s/epoch - 174ms/step
Epoch 383/1000
2023-09-26 17:01:23.354 
Epoch 383/1000 
	 loss: 16.5230, MinusLogProbMetric: 16.5230, val_loss: 16.9361, val_MinusLogProbMetric: 16.9361

Epoch 383: val_loss did not improve from 16.84587
196/196 - 34s - loss: 16.5230 - MinusLogProbMetric: 16.5230 - val_loss: 16.9361 - val_MinusLogProbMetric: 16.9361 - lr: 5.0000e-04 - 34s/epoch - 174ms/step
Epoch 384/1000
2023-09-26 17:01:57.458 
Epoch 384/1000 
	 loss: 16.5230, MinusLogProbMetric: 16.5230, val_loss: 16.9749, val_MinusLogProbMetric: 16.9749

Epoch 384: val_loss did not improve from 16.84587
196/196 - 34s - loss: 16.5230 - MinusLogProbMetric: 16.5230 - val_loss: 16.9749 - val_MinusLogProbMetric: 16.9749 - lr: 5.0000e-04 - 34s/epoch - 174ms/step
Epoch 385/1000
2023-09-26 17:02:31.629 
Epoch 385/1000 
	 loss: 16.5061, MinusLogProbMetric: 16.5061, val_loss: 17.0281, val_MinusLogProbMetric: 17.0281

Epoch 385: val_loss did not improve from 16.84587
196/196 - 34s - loss: 16.5061 - MinusLogProbMetric: 16.5061 - val_loss: 17.0281 - val_MinusLogProbMetric: 17.0281 - lr: 5.0000e-04 - 34s/epoch - 174ms/step
Epoch 386/1000
2023-09-26 17:03:05.906 
Epoch 386/1000 
	 loss: 16.5148, MinusLogProbMetric: 16.5148, val_loss: 16.8694, val_MinusLogProbMetric: 16.8694

Epoch 386: val_loss did not improve from 16.84587
196/196 - 34s - loss: 16.5148 - MinusLogProbMetric: 16.5148 - val_loss: 16.8694 - val_MinusLogProbMetric: 16.8694 - lr: 5.0000e-04 - 34s/epoch - 175ms/step
Epoch 387/1000
2023-09-26 17:03:40.267 
Epoch 387/1000 
	 loss: 16.5133, MinusLogProbMetric: 16.5133, val_loss: 16.9014, val_MinusLogProbMetric: 16.9014

Epoch 387: val_loss did not improve from 16.84587
196/196 - 34s - loss: 16.5133 - MinusLogProbMetric: 16.5133 - val_loss: 16.9014 - val_MinusLogProbMetric: 16.9014 - lr: 5.0000e-04 - 34s/epoch - 175ms/step
Epoch 388/1000
2023-09-26 17:04:14.602 
Epoch 388/1000 
	 loss: 16.5325, MinusLogProbMetric: 16.5325, val_loss: 16.9999, val_MinusLogProbMetric: 16.9999

Epoch 388: val_loss did not improve from 16.84587
196/196 - 34s - loss: 16.5325 - MinusLogProbMetric: 16.5325 - val_loss: 16.9999 - val_MinusLogProbMetric: 16.9999 - lr: 5.0000e-04 - 34s/epoch - 175ms/step
Epoch 389/1000
2023-09-26 17:04:48.711 
Epoch 389/1000 
	 loss: 16.5070, MinusLogProbMetric: 16.5070, val_loss: 16.9221, val_MinusLogProbMetric: 16.9221

Epoch 389: val_loss did not improve from 16.84587
196/196 - 34s - loss: 16.5070 - MinusLogProbMetric: 16.5070 - val_loss: 16.9221 - val_MinusLogProbMetric: 16.9221 - lr: 5.0000e-04 - 34s/epoch - 174ms/step
Epoch 390/1000
2023-09-26 17:05:22.645 
Epoch 390/1000 
	 loss: 16.5196, MinusLogProbMetric: 16.5196, val_loss: 17.0569, val_MinusLogProbMetric: 17.0569

Epoch 390: val_loss did not improve from 16.84587
196/196 - 34s - loss: 16.5196 - MinusLogProbMetric: 16.5196 - val_loss: 17.0569 - val_MinusLogProbMetric: 17.0569 - lr: 5.0000e-04 - 34s/epoch - 173ms/step
Epoch 391/1000
2023-09-26 17:05:56.870 
Epoch 391/1000 
	 loss: 16.5254, MinusLogProbMetric: 16.5254, val_loss: 17.0583, val_MinusLogProbMetric: 17.0583

Epoch 391: val_loss did not improve from 16.84587
196/196 - 34s - loss: 16.5254 - MinusLogProbMetric: 16.5254 - val_loss: 17.0583 - val_MinusLogProbMetric: 17.0583 - lr: 5.0000e-04 - 34s/epoch - 175ms/step
Epoch 392/1000
2023-09-26 17:06:31.341 
Epoch 392/1000 
	 loss: 16.5253, MinusLogProbMetric: 16.5253, val_loss: 17.0247, val_MinusLogProbMetric: 17.0247

Epoch 392: val_loss did not improve from 16.84587
196/196 - 34s - loss: 16.5253 - MinusLogProbMetric: 16.5253 - val_loss: 17.0247 - val_MinusLogProbMetric: 17.0247 - lr: 5.0000e-04 - 34s/epoch - 176ms/step
Epoch 393/1000
2023-09-26 17:07:05.285 
Epoch 393/1000 
	 loss: 16.5159, MinusLogProbMetric: 16.5159, val_loss: 16.8601, val_MinusLogProbMetric: 16.8601

Epoch 393: val_loss did not improve from 16.84587
196/196 - 34s - loss: 16.5159 - MinusLogProbMetric: 16.5159 - val_loss: 16.8601 - val_MinusLogProbMetric: 16.8601 - lr: 5.0000e-04 - 34s/epoch - 173ms/step
Epoch 394/1000
2023-09-26 17:07:39.629 
Epoch 394/1000 
	 loss: 16.5453, MinusLogProbMetric: 16.5453, val_loss: 17.0159, val_MinusLogProbMetric: 17.0159

Epoch 394: val_loss did not improve from 16.84587
196/196 - 34s - loss: 16.5453 - MinusLogProbMetric: 16.5453 - val_loss: 17.0159 - val_MinusLogProbMetric: 17.0159 - lr: 5.0000e-04 - 34s/epoch - 175ms/step
Epoch 395/1000
2023-09-26 17:08:13.465 
Epoch 395/1000 
	 loss: 16.5233, MinusLogProbMetric: 16.5233, val_loss: 16.9501, val_MinusLogProbMetric: 16.9501

Epoch 395: val_loss did not improve from 16.84587
196/196 - 34s - loss: 16.5233 - MinusLogProbMetric: 16.5233 - val_loss: 16.9501 - val_MinusLogProbMetric: 16.9501 - lr: 5.0000e-04 - 34s/epoch - 173ms/step
Epoch 396/1000
2023-09-26 17:08:47.557 
Epoch 396/1000 
	 loss: 16.5152, MinusLogProbMetric: 16.5152, val_loss: 16.9348, val_MinusLogProbMetric: 16.9348

Epoch 396: val_loss did not improve from 16.84587
196/196 - 34s - loss: 16.5152 - MinusLogProbMetric: 16.5152 - val_loss: 16.9348 - val_MinusLogProbMetric: 16.9348 - lr: 5.0000e-04 - 34s/epoch - 174ms/step
Epoch 397/1000
2023-09-26 17:09:21.536 
Epoch 397/1000 
	 loss: 16.5381, MinusLogProbMetric: 16.5381, val_loss: 16.9015, val_MinusLogProbMetric: 16.9015

Epoch 397: val_loss did not improve from 16.84587
196/196 - 34s - loss: 16.5381 - MinusLogProbMetric: 16.5381 - val_loss: 16.9015 - val_MinusLogProbMetric: 16.9015 - lr: 5.0000e-04 - 34s/epoch - 173ms/step
Epoch 398/1000
2023-09-26 17:09:55.913 
Epoch 398/1000 
	 loss: 16.5202, MinusLogProbMetric: 16.5202, val_loss: 16.9396, val_MinusLogProbMetric: 16.9396

Epoch 398: val_loss did not improve from 16.84587
196/196 - 34s - loss: 16.5202 - MinusLogProbMetric: 16.5202 - val_loss: 16.9396 - val_MinusLogProbMetric: 16.9396 - lr: 5.0000e-04 - 34s/epoch - 175ms/step
Epoch 399/1000
2023-09-26 17:10:30.147 
Epoch 399/1000 
	 loss: 16.5189, MinusLogProbMetric: 16.5189, val_loss: 17.0181, val_MinusLogProbMetric: 17.0181

Epoch 399: val_loss did not improve from 16.84587
196/196 - 34s - loss: 16.5189 - MinusLogProbMetric: 16.5189 - val_loss: 17.0181 - val_MinusLogProbMetric: 17.0181 - lr: 5.0000e-04 - 34s/epoch - 175ms/step
Epoch 400/1000
2023-09-26 17:11:04.324 
Epoch 400/1000 
	 loss: 16.5342, MinusLogProbMetric: 16.5342, val_loss: 16.9351, val_MinusLogProbMetric: 16.9351

Epoch 400: val_loss did not improve from 16.84587
196/196 - 34s - loss: 16.5342 - MinusLogProbMetric: 16.5342 - val_loss: 16.9351 - val_MinusLogProbMetric: 16.9351 - lr: 5.0000e-04 - 34s/epoch - 174ms/step
Epoch 401/1000
2023-09-26 17:11:38.310 
Epoch 401/1000 
	 loss: 16.5147, MinusLogProbMetric: 16.5147, val_loss: 16.9119, val_MinusLogProbMetric: 16.9119

Epoch 401: val_loss did not improve from 16.84587
196/196 - 34s - loss: 16.5147 - MinusLogProbMetric: 16.5147 - val_loss: 16.9119 - val_MinusLogProbMetric: 16.9119 - lr: 5.0000e-04 - 34s/epoch - 173ms/step
Epoch 402/1000
2023-09-26 17:12:12.514 
Epoch 402/1000 
	 loss: 16.5040, MinusLogProbMetric: 16.5040, val_loss: 16.8838, val_MinusLogProbMetric: 16.8838

Epoch 402: val_loss did not improve from 16.84587
196/196 - 34s - loss: 16.5040 - MinusLogProbMetric: 16.5040 - val_loss: 16.8838 - val_MinusLogProbMetric: 16.8838 - lr: 5.0000e-04 - 34s/epoch - 174ms/step
Epoch 403/1000
2023-09-26 17:12:46.183 
Epoch 403/1000 
	 loss: 16.5129, MinusLogProbMetric: 16.5129, val_loss: 17.0157, val_MinusLogProbMetric: 17.0157

Epoch 403: val_loss did not improve from 16.84587
196/196 - 34s - loss: 16.5129 - MinusLogProbMetric: 16.5129 - val_loss: 17.0157 - val_MinusLogProbMetric: 17.0157 - lr: 5.0000e-04 - 34s/epoch - 172ms/step
Epoch 404/1000
2023-09-26 17:13:20.006 
Epoch 404/1000 
	 loss: 16.5161, MinusLogProbMetric: 16.5161, val_loss: 16.9656, val_MinusLogProbMetric: 16.9656

Epoch 404: val_loss did not improve from 16.84587
196/196 - 34s - loss: 16.5161 - MinusLogProbMetric: 16.5161 - val_loss: 16.9656 - val_MinusLogProbMetric: 16.9656 - lr: 5.0000e-04 - 34s/epoch - 173ms/step
Epoch 405/1000
2023-09-26 17:13:53.578 
Epoch 405/1000 
	 loss: 16.5017, MinusLogProbMetric: 16.5017, val_loss: 16.9375, val_MinusLogProbMetric: 16.9375

Epoch 405: val_loss did not improve from 16.84587
196/196 - 34s - loss: 16.5017 - MinusLogProbMetric: 16.5017 - val_loss: 16.9375 - val_MinusLogProbMetric: 16.9375 - lr: 5.0000e-04 - 34s/epoch - 171ms/step
Epoch 406/1000
2023-09-26 17:14:27.874 
Epoch 406/1000 
	 loss: 16.5079, MinusLogProbMetric: 16.5079, val_loss: 16.8682, val_MinusLogProbMetric: 16.8682

Epoch 406: val_loss did not improve from 16.84587
196/196 - 34s - loss: 16.5079 - MinusLogProbMetric: 16.5079 - val_loss: 16.8682 - val_MinusLogProbMetric: 16.8682 - lr: 5.0000e-04 - 34s/epoch - 175ms/step
Epoch 407/1000
2023-09-26 17:15:01.734 
Epoch 407/1000 
	 loss: 16.5238, MinusLogProbMetric: 16.5238, val_loss: 16.9687, val_MinusLogProbMetric: 16.9687

Epoch 407: val_loss did not improve from 16.84587
196/196 - 34s - loss: 16.5238 - MinusLogProbMetric: 16.5238 - val_loss: 16.9687 - val_MinusLogProbMetric: 16.9687 - lr: 5.0000e-04 - 34s/epoch - 173ms/step
Epoch 408/1000
2023-09-26 17:15:35.817 
Epoch 408/1000 
	 loss: 16.5000, MinusLogProbMetric: 16.5000, val_loss: 16.8773, val_MinusLogProbMetric: 16.8773

Epoch 408: val_loss did not improve from 16.84587
196/196 - 34s - loss: 16.5000 - MinusLogProbMetric: 16.5000 - val_loss: 16.8773 - val_MinusLogProbMetric: 16.8773 - lr: 5.0000e-04 - 34s/epoch - 174ms/step
Epoch 409/1000
2023-09-26 17:16:09.716 
Epoch 409/1000 
	 loss: 16.5246, MinusLogProbMetric: 16.5246, val_loss: 17.1413, val_MinusLogProbMetric: 17.1413

Epoch 409: val_loss did not improve from 16.84587
196/196 - 34s - loss: 16.5246 - MinusLogProbMetric: 16.5246 - val_loss: 17.1413 - val_MinusLogProbMetric: 17.1413 - lr: 5.0000e-04 - 34s/epoch - 173ms/step
Epoch 410/1000
2023-09-26 17:16:44.071 
Epoch 410/1000 
	 loss: 16.5127, MinusLogProbMetric: 16.5127, val_loss: 17.1210, val_MinusLogProbMetric: 17.1210

Epoch 410: val_loss did not improve from 16.84587
196/196 - 34s - loss: 16.5127 - MinusLogProbMetric: 16.5127 - val_loss: 17.1210 - val_MinusLogProbMetric: 17.1210 - lr: 5.0000e-04 - 34s/epoch - 175ms/step
Epoch 411/1000
2023-09-26 17:17:18.124 
Epoch 411/1000 
	 loss: 16.5227, MinusLogProbMetric: 16.5227, val_loss: 16.9349, val_MinusLogProbMetric: 16.9349

Epoch 411: val_loss did not improve from 16.84587
196/196 - 34s - loss: 16.5227 - MinusLogProbMetric: 16.5227 - val_loss: 16.9349 - val_MinusLogProbMetric: 16.9349 - lr: 5.0000e-04 - 34s/epoch - 174ms/step
Epoch 412/1000
2023-09-26 17:17:52.176 
Epoch 412/1000 
	 loss: 16.5134, MinusLogProbMetric: 16.5134, val_loss: 17.0468, val_MinusLogProbMetric: 17.0468

Epoch 412: val_loss did not improve from 16.84587
196/196 - 34s - loss: 16.5134 - MinusLogProbMetric: 16.5134 - val_loss: 17.0468 - val_MinusLogProbMetric: 17.0468 - lr: 5.0000e-04 - 34s/epoch - 174ms/step
Epoch 413/1000
2023-09-26 17:18:26.371 
Epoch 413/1000 
	 loss: 16.5004, MinusLogProbMetric: 16.5004, val_loss: 16.8549, val_MinusLogProbMetric: 16.8549

Epoch 413: val_loss did not improve from 16.84587
196/196 - 34s - loss: 16.5004 - MinusLogProbMetric: 16.5004 - val_loss: 16.8549 - val_MinusLogProbMetric: 16.8549 - lr: 5.0000e-04 - 34s/epoch - 174ms/step
Epoch 414/1000
2023-09-26 17:19:00.249 
Epoch 414/1000 
	 loss: 16.5209, MinusLogProbMetric: 16.5209, val_loss: 16.9886, val_MinusLogProbMetric: 16.9886

Epoch 414: val_loss did not improve from 16.84587
196/196 - 34s - loss: 16.5209 - MinusLogProbMetric: 16.5209 - val_loss: 16.9886 - val_MinusLogProbMetric: 16.9886 - lr: 5.0000e-04 - 34s/epoch - 173ms/step
Epoch 415/1000
2023-09-26 17:19:34.223 
Epoch 415/1000 
	 loss: 16.5167, MinusLogProbMetric: 16.5167, val_loss: 16.9392, val_MinusLogProbMetric: 16.9392

Epoch 415: val_loss did not improve from 16.84587
196/196 - 34s - loss: 16.5167 - MinusLogProbMetric: 16.5167 - val_loss: 16.9392 - val_MinusLogProbMetric: 16.9392 - lr: 5.0000e-04 - 34s/epoch - 173ms/step
Epoch 416/1000
2023-09-26 17:20:08.357 
Epoch 416/1000 
	 loss: 16.5005, MinusLogProbMetric: 16.5005, val_loss: 16.8721, val_MinusLogProbMetric: 16.8721

Epoch 416: val_loss did not improve from 16.84587
196/196 - 34s - loss: 16.5005 - MinusLogProbMetric: 16.5005 - val_loss: 16.8721 - val_MinusLogProbMetric: 16.8721 - lr: 5.0000e-04 - 34s/epoch - 174ms/step
Epoch 417/1000
2023-09-26 17:20:42.468 
Epoch 417/1000 
	 loss: 16.5168, MinusLogProbMetric: 16.5168, val_loss: 16.9362, val_MinusLogProbMetric: 16.9362

Epoch 417: val_loss did not improve from 16.84587
196/196 - 34s - loss: 16.5168 - MinusLogProbMetric: 16.5168 - val_loss: 16.9362 - val_MinusLogProbMetric: 16.9362 - lr: 5.0000e-04 - 34s/epoch - 174ms/step
Epoch 418/1000
2023-09-26 17:21:16.511 
Epoch 418/1000 
	 loss: 16.5183, MinusLogProbMetric: 16.5183, val_loss: 17.0020, val_MinusLogProbMetric: 17.0020

Epoch 418: val_loss did not improve from 16.84587
196/196 - 34s - loss: 16.5183 - MinusLogProbMetric: 16.5183 - val_loss: 17.0020 - val_MinusLogProbMetric: 17.0020 - lr: 5.0000e-04 - 34s/epoch - 174ms/step
Epoch 419/1000
2023-09-26 17:21:50.523 
Epoch 419/1000 
	 loss: 16.5130, MinusLogProbMetric: 16.5130, val_loss: 16.9193, val_MinusLogProbMetric: 16.9193

Epoch 419: val_loss did not improve from 16.84587
196/196 - 34s - loss: 16.5130 - MinusLogProbMetric: 16.5130 - val_loss: 16.9193 - val_MinusLogProbMetric: 16.9193 - lr: 5.0000e-04 - 34s/epoch - 174ms/step
Epoch 420/1000
2023-09-26 17:22:24.617 
Epoch 420/1000 
	 loss: 16.5062, MinusLogProbMetric: 16.5062, val_loss: 16.9286, val_MinusLogProbMetric: 16.9286

Epoch 420: val_loss did not improve from 16.84587
196/196 - 34s - loss: 16.5062 - MinusLogProbMetric: 16.5062 - val_loss: 16.9286 - val_MinusLogProbMetric: 16.9286 - lr: 5.0000e-04 - 34s/epoch - 174ms/step
Epoch 421/1000
2023-09-26 17:22:59.083 
Epoch 421/1000 
	 loss: 16.5341, MinusLogProbMetric: 16.5341, val_loss: 16.9329, val_MinusLogProbMetric: 16.9329

Epoch 421: val_loss did not improve from 16.84587
196/196 - 34s - loss: 16.5341 - MinusLogProbMetric: 16.5341 - val_loss: 16.9329 - val_MinusLogProbMetric: 16.9329 - lr: 5.0000e-04 - 34s/epoch - 176ms/step
Epoch 422/1000
2023-09-26 17:23:33.162 
Epoch 422/1000 
	 loss: 16.5101, MinusLogProbMetric: 16.5101, val_loss: 16.8808, val_MinusLogProbMetric: 16.8808

Epoch 422: val_loss did not improve from 16.84587
196/196 - 34s - loss: 16.5101 - MinusLogProbMetric: 16.5101 - val_loss: 16.8808 - val_MinusLogProbMetric: 16.8808 - lr: 5.0000e-04 - 34s/epoch - 174ms/step
Epoch 423/1000
2023-09-26 17:24:07.267 
Epoch 423/1000 
	 loss: 16.5009, MinusLogProbMetric: 16.5009, val_loss: 16.9241, val_MinusLogProbMetric: 16.9241

Epoch 423: val_loss did not improve from 16.84587
196/196 - 34s - loss: 16.5009 - MinusLogProbMetric: 16.5009 - val_loss: 16.9241 - val_MinusLogProbMetric: 16.9241 - lr: 5.0000e-04 - 34s/epoch - 174ms/step
Epoch 424/1000
2023-09-26 17:24:41.376 
Epoch 424/1000 
	 loss: 16.4812, MinusLogProbMetric: 16.4812, val_loss: 16.9518, val_MinusLogProbMetric: 16.9518

Epoch 424: val_loss did not improve from 16.84587
196/196 - 34s - loss: 16.4812 - MinusLogProbMetric: 16.4812 - val_loss: 16.9518 - val_MinusLogProbMetric: 16.9518 - lr: 5.0000e-04 - 34s/epoch - 174ms/step
Epoch 425/1000
2023-09-26 17:25:15.636 
Epoch 425/1000 
	 loss: 16.5152, MinusLogProbMetric: 16.5152, val_loss: 17.0208, val_MinusLogProbMetric: 17.0208

Epoch 425: val_loss did not improve from 16.84587
196/196 - 34s - loss: 16.5152 - MinusLogProbMetric: 16.5152 - val_loss: 17.0208 - val_MinusLogProbMetric: 17.0208 - lr: 5.0000e-04 - 34s/epoch - 175ms/step
Epoch 426/1000
2023-09-26 17:25:49.797 
Epoch 426/1000 
	 loss: 16.5142, MinusLogProbMetric: 16.5142, val_loss: 16.9323, val_MinusLogProbMetric: 16.9323

Epoch 426: val_loss did not improve from 16.84587
196/196 - 34s - loss: 16.5142 - MinusLogProbMetric: 16.5142 - val_loss: 16.9323 - val_MinusLogProbMetric: 16.9323 - lr: 5.0000e-04 - 34s/epoch - 174ms/step
Epoch 427/1000
2023-09-26 17:26:24.079 
Epoch 427/1000 
	 loss: 16.5208, MinusLogProbMetric: 16.5208, val_loss: 17.0242, val_MinusLogProbMetric: 17.0242

Epoch 427: val_loss did not improve from 16.84587
196/196 - 34s - loss: 16.5208 - MinusLogProbMetric: 16.5208 - val_loss: 17.0242 - val_MinusLogProbMetric: 17.0242 - lr: 5.0000e-04 - 34s/epoch - 175ms/step
Epoch 428/1000
2023-09-26 17:26:58.278 
Epoch 428/1000 
	 loss: 16.4996, MinusLogProbMetric: 16.4996, val_loss: 16.9646, val_MinusLogProbMetric: 16.9646

Epoch 428: val_loss did not improve from 16.84587
196/196 - 34s - loss: 16.4996 - MinusLogProbMetric: 16.4996 - val_loss: 16.9646 - val_MinusLogProbMetric: 16.9646 - lr: 5.0000e-04 - 34s/epoch - 174ms/step
Epoch 429/1000
2023-09-26 17:27:32.448 
Epoch 429/1000 
	 loss: 16.5170, MinusLogProbMetric: 16.5170, val_loss: 16.9320, val_MinusLogProbMetric: 16.9320

Epoch 429: val_loss did not improve from 16.84587
196/196 - 34s - loss: 16.5170 - MinusLogProbMetric: 16.5170 - val_loss: 16.9320 - val_MinusLogProbMetric: 16.9320 - lr: 5.0000e-04 - 34s/epoch - 174ms/step
Epoch 430/1000
2023-09-26 17:28:06.642 
Epoch 430/1000 
	 loss: 16.5062, MinusLogProbMetric: 16.5062, val_loss: 17.0352, val_MinusLogProbMetric: 17.0352

Epoch 430: val_loss did not improve from 16.84587
196/196 - 34s - loss: 16.5062 - MinusLogProbMetric: 16.5062 - val_loss: 17.0352 - val_MinusLogProbMetric: 17.0352 - lr: 5.0000e-04 - 34s/epoch - 174ms/step
Epoch 431/1000
2023-09-26 17:28:40.257 
Epoch 431/1000 
	 loss: 16.5029, MinusLogProbMetric: 16.5029, val_loss: 17.0757, val_MinusLogProbMetric: 17.0757

Epoch 431: val_loss did not improve from 16.84587
196/196 - 34s - loss: 16.5029 - MinusLogProbMetric: 16.5029 - val_loss: 17.0757 - val_MinusLogProbMetric: 17.0757 - lr: 5.0000e-04 - 34s/epoch - 171ms/step
Epoch 432/1000
2023-09-26 17:29:14.362 
Epoch 432/1000 
	 loss: 16.4221, MinusLogProbMetric: 16.4221, val_loss: 16.8446, val_MinusLogProbMetric: 16.8446

Epoch 432: val_loss improved from 16.84587 to 16.84461, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_289/weights/best_weights.h5
196/196 - 35s - loss: 16.4221 - MinusLogProbMetric: 16.4221 - val_loss: 16.8446 - val_MinusLogProbMetric: 16.8446 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 433/1000
2023-09-26 17:29:48.784 
Epoch 433/1000 
	 loss: 16.4117, MinusLogProbMetric: 16.4117, val_loss: 16.8631, val_MinusLogProbMetric: 16.8631

Epoch 433: val_loss did not improve from 16.84461
196/196 - 34s - loss: 16.4117 - MinusLogProbMetric: 16.4117 - val_loss: 16.8631 - val_MinusLogProbMetric: 16.8631 - lr: 2.5000e-04 - 34s/epoch - 173ms/step
Epoch 434/1000
2023-09-26 17:30:22.884 
Epoch 434/1000 
	 loss: 16.4085, MinusLogProbMetric: 16.4085, val_loss: 16.8636, val_MinusLogProbMetric: 16.8636

Epoch 434: val_loss did not improve from 16.84461
196/196 - 34s - loss: 16.4085 - MinusLogProbMetric: 16.4085 - val_loss: 16.8636 - val_MinusLogProbMetric: 16.8636 - lr: 2.5000e-04 - 34s/epoch - 174ms/step
Epoch 435/1000
2023-09-26 17:30:57.332 
Epoch 435/1000 
	 loss: 16.4031, MinusLogProbMetric: 16.4031, val_loss: 16.8551, val_MinusLogProbMetric: 16.8551

Epoch 435: val_loss did not improve from 16.84461
196/196 - 34s - loss: 16.4031 - MinusLogProbMetric: 16.4031 - val_loss: 16.8551 - val_MinusLogProbMetric: 16.8551 - lr: 2.5000e-04 - 34s/epoch - 176ms/step
Epoch 436/1000
2023-09-26 17:31:31.415 
Epoch 436/1000 
	 loss: 16.4143, MinusLogProbMetric: 16.4143, val_loss: 16.8716, val_MinusLogProbMetric: 16.8716

Epoch 436: val_loss did not improve from 16.84461
196/196 - 34s - loss: 16.4143 - MinusLogProbMetric: 16.4143 - val_loss: 16.8716 - val_MinusLogProbMetric: 16.8716 - lr: 2.5000e-04 - 34s/epoch - 174ms/step
Epoch 437/1000
2023-09-26 17:32:05.229 
Epoch 437/1000 
	 loss: 16.4171, MinusLogProbMetric: 16.4171, val_loss: 16.8569, val_MinusLogProbMetric: 16.8569

Epoch 437: val_loss did not improve from 16.84461
196/196 - 34s - loss: 16.4171 - MinusLogProbMetric: 16.4171 - val_loss: 16.8569 - val_MinusLogProbMetric: 16.8569 - lr: 2.5000e-04 - 34s/epoch - 172ms/step
Epoch 438/1000
2023-09-26 17:32:39.830 
Epoch 438/1000 
	 loss: 16.4037, MinusLogProbMetric: 16.4037, val_loss: 16.8544, val_MinusLogProbMetric: 16.8544

Epoch 438: val_loss did not improve from 16.84461
196/196 - 35s - loss: 16.4037 - MinusLogProbMetric: 16.4037 - val_loss: 16.8544 - val_MinusLogProbMetric: 16.8544 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 439/1000
2023-09-26 17:33:13.650 
Epoch 439/1000 
	 loss: 16.4085, MinusLogProbMetric: 16.4085, val_loss: 16.8502, val_MinusLogProbMetric: 16.8502

Epoch 439: val_loss did not improve from 16.84461
196/196 - 34s - loss: 16.4085 - MinusLogProbMetric: 16.4085 - val_loss: 16.8502 - val_MinusLogProbMetric: 16.8502 - lr: 2.5000e-04 - 34s/epoch - 173ms/step
Epoch 440/1000
2023-09-26 17:33:47.458 
Epoch 440/1000 
	 loss: 16.4221, MinusLogProbMetric: 16.4221, val_loss: 16.8559, val_MinusLogProbMetric: 16.8559

Epoch 440: val_loss did not improve from 16.84461
196/196 - 34s - loss: 16.4221 - MinusLogProbMetric: 16.4221 - val_loss: 16.8559 - val_MinusLogProbMetric: 16.8559 - lr: 2.5000e-04 - 34s/epoch - 172ms/step
Epoch 441/1000
2023-09-26 17:34:21.289 
Epoch 441/1000 
	 loss: 16.3986, MinusLogProbMetric: 16.3986, val_loss: 16.8655, val_MinusLogProbMetric: 16.8655

Epoch 441: val_loss did not improve from 16.84461
196/196 - 34s - loss: 16.3986 - MinusLogProbMetric: 16.3986 - val_loss: 16.8655 - val_MinusLogProbMetric: 16.8655 - lr: 2.5000e-04 - 34s/epoch - 173ms/step
Epoch 442/1000
2023-09-26 17:34:55.249 
Epoch 442/1000 
	 loss: 16.4148, MinusLogProbMetric: 16.4148, val_loss: 16.8929, val_MinusLogProbMetric: 16.8929

Epoch 442: val_loss did not improve from 16.84461
196/196 - 34s - loss: 16.4148 - MinusLogProbMetric: 16.4148 - val_loss: 16.8929 - val_MinusLogProbMetric: 16.8929 - lr: 2.5000e-04 - 34s/epoch - 173ms/step
Epoch 443/1000
2023-09-26 17:35:29.087 
Epoch 443/1000 
	 loss: 16.4083, MinusLogProbMetric: 16.4083, val_loss: 16.9187, val_MinusLogProbMetric: 16.9187

Epoch 443: val_loss did not improve from 16.84461
196/196 - 34s - loss: 16.4083 - MinusLogProbMetric: 16.4083 - val_loss: 16.9187 - val_MinusLogProbMetric: 16.9187 - lr: 2.5000e-04 - 34s/epoch - 173ms/step
Epoch 444/1000
2023-09-26 17:36:03.022 
Epoch 444/1000 
	 loss: 16.4043, MinusLogProbMetric: 16.4043, val_loss: 16.8997, val_MinusLogProbMetric: 16.8997

Epoch 444: val_loss did not improve from 16.84461
196/196 - 34s - loss: 16.4043 - MinusLogProbMetric: 16.4043 - val_loss: 16.8997 - val_MinusLogProbMetric: 16.8997 - lr: 2.5000e-04 - 34s/epoch - 173ms/step
Epoch 445/1000
2023-09-26 17:36:36.949 
Epoch 445/1000 
	 loss: 16.4013, MinusLogProbMetric: 16.4013, val_loss: 16.8949, val_MinusLogProbMetric: 16.8949

Epoch 445: val_loss did not improve from 16.84461
196/196 - 34s - loss: 16.4013 - MinusLogProbMetric: 16.4013 - val_loss: 16.8949 - val_MinusLogProbMetric: 16.8949 - lr: 2.5000e-04 - 34s/epoch - 173ms/step
Epoch 446/1000
2023-09-26 17:37:10.991 
Epoch 446/1000 
	 loss: 16.4056, MinusLogProbMetric: 16.4056, val_loss: 16.8667, val_MinusLogProbMetric: 16.8667

Epoch 446: val_loss did not improve from 16.84461
196/196 - 34s - loss: 16.4056 - MinusLogProbMetric: 16.4056 - val_loss: 16.8667 - val_MinusLogProbMetric: 16.8667 - lr: 2.5000e-04 - 34s/epoch - 174ms/step
Epoch 447/1000
2023-09-26 17:37:45.037 
Epoch 447/1000 
	 loss: 16.4074, MinusLogProbMetric: 16.4074, val_loss: 16.8958, val_MinusLogProbMetric: 16.8958

Epoch 447: val_loss did not improve from 16.84461
196/196 - 34s - loss: 16.4074 - MinusLogProbMetric: 16.4074 - val_loss: 16.8958 - val_MinusLogProbMetric: 16.8958 - lr: 2.5000e-04 - 34s/epoch - 174ms/step
Epoch 448/1000
2023-09-26 17:38:19.452 
Epoch 448/1000 
	 loss: 16.4201, MinusLogProbMetric: 16.4201, val_loss: 16.9204, val_MinusLogProbMetric: 16.9204

Epoch 448: val_loss did not improve from 16.84461
196/196 - 34s - loss: 16.4201 - MinusLogProbMetric: 16.4201 - val_loss: 16.9204 - val_MinusLogProbMetric: 16.9204 - lr: 2.5000e-04 - 34s/epoch - 176ms/step
Epoch 449/1000
2023-09-26 17:38:53.438 
Epoch 449/1000 
	 loss: 16.4107, MinusLogProbMetric: 16.4107, val_loss: 16.8839, val_MinusLogProbMetric: 16.8839

Epoch 449: val_loss did not improve from 16.84461
196/196 - 34s - loss: 16.4107 - MinusLogProbMetric: 16.4107 - val_loss: 16.8839 - val_MinusLogProbMetric: 16.8839 - lr: 2.5000e-04 - 34s/epoch - 173ms/step
Epoch 450/1000
2023-09-26 17:39:27.640 
Epoch 450/1000 
	 loss: 16.4207, MinusLogProbMetric: 16.4207, val_loss: 16.8850, val_MinusLogProbMetric: 16.8850

Epoch 450: val_loss did not improve from 16.84461
196/196 - 34s - loss: 16.4207 - MinusLogProbMetric: 16.4207 - val_loss: 16.8850 - val_MinusLogProbMetric: 16.8850 - lr: 2.5000e-04 - 34s/epoch - 174ms/step
Epoch 451/1000
2023-09-26 17:40:01.596 
Epoch 451/1000 
	 loss: 16.4132, MinusLogProbMetric: 16.4132, val_loss: 16.8702, val_MinusLogProbMetric: 16.8702

Epoch 451: val_loss did not improve from 16.84461
196/196 - 34s - loss: 16.4132 - MinusLogProbMetric: 16.4132 - val_loss: 16.8702 - val_MinusLogProbMetric: 16.8702 - lr: 2.5000e-04 - 34s/epoch - 173ms/step
Epoch 452/1000
2023-09-26 17:40:35.547 
Epoch 452/1000 
	 loss: 16.4032, MinusLogProbMetric: 16.4032, val_loss: 16.9730, val_MinusLogProbMetric: 16.9730

Epoch 452: val_loss did not improve from 16.84461
196/196 - 34s - loss: 16.4032 - MinusLogProbMetric: 16.4032 - val_loss: 16.9730 - val_MinusLogProbMetric: 16.9730 - lr: 2.5000e-04 - 34s/epoch - 173ms/step
Epoch 453/1000
2023-09-26 17:41:09.492 
Epoch 453/1000 
	 loss: 16.4133, MinusLogProbMetric: 16.4133, val_loss: 16.8838, val_MinusLogProbMetric: 16.8838

Epoch 453: val_loss did not improve from 16.84461
196/196 - 34s - loss: 16.4133 - MinusLogProbMetric: 16.4133 - val_loss: 16.8838 - val_MinusLogProbMetric: 16.8838 - lr: 2.5000e-04 - 34s/epoch - 173ms/step
Epoch 454/1000
2023-09-26 17:41:43.570 
Epoch 454/1000 
	 loss: 16.4154, MinusLogProbMetric: 16.4154, val_loss: 16.8507, val_MinusLogProbMetric: 16.8507

Epoch 454: val_loss did not improve from 16.84461
196/196 - 34s - loss: 16.4154 - MinusLogProbMetric: 16.4154 - val_loss: 16.8507 - val_MinusLogProbMetric: 16.8507 - lr: 2.5000e-04 - 34s/epoch - 174ms/step
Epoch 455/1000
2023-09-26 17:42:17.913 
Epoch 455/1000 
	 loss: 16.4113, MinusLogProbMetric: 16.4113, val_loss: 16.9011, val_MinusLogProbMetric: 16.9011

Epoch 455: val_loss did not improve from 16.84461
196/196 - 34s - loss: 16.4113 - MinusLogProbMetric: 16.4113 - val_loss: 16.9011 - val_MinusLogProbMetric: 16.9011 - lr: 2.5000e-04 - 34s/epoch - 175ms/step
Epoch 456/1000
2023-09-26 17:42:52.127 
Epoch 456/1000 
	 loss: 16.4071, MinusLogProbMetric: 16.4071, val_loss: 16.9027, val_MinusLogProbMetric: 16.9027

Epoch 456: val_loss did not improve from 16.84461
196/196 - 34s - loss: 16.4071 - MinusLogProbMetric: 16.4071 - val_loss: 16.9027 - val_MinusLogProbMetric: 16.9027 - lr: 2.5000e-04 - 34s/epoch - 175ms/step
Epoch 457/1000
2023-09-26 17:43:26.233 
Epoch 457/1000 
	 loss: 16.4187, MinusLogProbMetric: 16.4187, val_loss: 16.8560, val_MinusLogProbMetric: 16.8560

Epoch 457: val_loss did not improve from 16.84461
196/196 - 34s - loss: 16.4187 - MinusLogProbMetric: 16.4187 - val_loss: 16.8560 - val_MinusLogProbMetric: 16.8560 - lr: 2.5000e-04 - 34s/epoch - 174ms/step
Epoch 458/1000
2023-09-26 17:44:00.575 
Epoch 458/1000 
	 loss: 16.4082, MinusLogProbMetric: 16.4082, val_loss: 16.9042, val_MinusLogProbMetric: 16.9042

Epoch 458: val_loss did not improve from 16.84461
196/196 - 34s - loss: 16.4082 - MinusLogProbMetric: 16.4082 - val_loss: 16.9042 - val_MinusLogProbMetric: 16.9042 - lr: 2.5000e-04 - 34s/epoch - 175ms/step
Epoch 459/1000
2023-09-26 17:44:34.467 
Epoch 459/1000 
	 loss: 16.4051, MinusLogProbMetric: 16.4051, val_loss: 16.8706, val_MinusLogProbMetric: 16.8706

Epoch 459: val_loss did not improve from 16.84461
196/196 - 34s - loss: 16.4051 - MinusLogProbMetric: 16.4051 - val_loss: 16.8706 - val_MinusLogProbMetric: 16.8706 - lr: 2.5000e-04 - 34s/epoch - 173ms/step
Epoch 460/1000
2023-09-26 17:45:08.400 
Epoch 460/1000 
	 loss: 16.4026, MinusLogProbMetric: 16.4026, val_loss: 16.8886, val_MinusLogProbMetric: 16.8886

Epoch 460: val_loss did not improve from 16.84461
196/196 - 34s - loss: 16.4026 - MinusLogProbMetric: 16.4026 - val_loss: 16.8886 - val_MinusLogProbMetric: 16.8886 - lr: 2.5000e-04 - 34s/epoch - 173ms/step
Epoch 461/1000
2023-09-26 17:45:42.532 
Epoch 461/1000 
	 loss: 16.3982, MinusLogProbMetric: 16.3982, val_loss: 16.8713, val_MinusLogProbMetric: 16.8713

Epoch 461: val_loss did not improve from 16.84461
196/196 - 34s - loss: 16.3982 - MinusLogProbMetric: 16.3982 - val_loss: 16.8713 - val_MinusLogProbMetric: 16.8713 - lr: 2.5000e-04 - 34s/epoch - 174ms/step
Epoch 462/1000
2023-09-26 17:46:16.533 
Epoch 462/1000 
	 loss: 16.4166, MinusLogProbMetric: 16.4166, val_loss: 16.9208, val_MinusLogProbMetric: 16.9208

Epoch 462: val_loss did not improve from 16.84461
196/196 - 34s - loss: 16.4166 - MinusLogProbMetric: 16.4166 - val_loss: 16.9208 - val_MinusLogProbMetric: 16.9208 - lr: 2.5000e-04 - 34s/epoch - 173ms/step
Epoch 463/1000
2023-09-26 17:46:50.739 
Epoch 463/1000 
	 loss: 16.4068, MinusLogProbMetric: 16.4068, val_loss: 16.8594, val_MinusLogProbMetric: 16.8594

Epoch 463: val_loss did not improve from 16.84461
196/196 - 34s - loss: 16.4068 - MinusLogProbMetric: 16.4068 - val_loss: 16.8594 - val_MinusLogProbMetric: 16.8594 - lr: 2.5000e-04 - 34s/epoch - 174ms/step
Epoch 464/1000
2023-09-26 17:47:24.771 
Epoch 464/1000 
	 loss: 16.4036, MinusLogProbMetric: 16.4036, val_loss: 16.9564, val_MinusLogProbMetric: 16.9564

Epoch 464: val_loss did not improve from 16.84461
196/196 - 34s - loss: 16.4036 - MinusLogProbMetric: 16.4036 - val_loss: 16.9564 - val_MinusLogProbMetric: 16.9564 - lr: 2.5000e-04 - 34s/epoch - 174ms/step
Epoch 465/1000
2023-09-26 17:47:59.050 
Epoch 465/1000 
	 loss: 16.4083, MinusLogProbMetric: 16.4083, val_loss: 16.8585, val_MinusLogProbMetric: 16.8585

Epoch 465: val_loss did not improve from 16.84461
196/196 - 34s - loss: 16.4083 - MinusLogProbMetric: 16.4083 - val_loss: 16.8585 - val_MinusLogProbMetric: 16.8585 - lr: 2.5000e-04 - 34s/epoch - 175ms/step
Epoch 466/1000
2023-09-26 17:48:32.680 
Epoch 466/1000 
	 loss: 16.4050, MinusLogProbMetric: 16.4050, val_loss: 16.8557, val_MinusLogProbMetric: 16.8557

Epoch 466: val_loss did not improve from 16.84461
196/196 - 34s - loss: 16.4050 - MinusLogProbMetric: 16.4050 - val_loss: 16.8557 - val_MinusLogProbMetric: 16.8557 - lr: 2.5000e-04 - 34s/epoch - 172ms/step
Epoch 467/1000
2023-09-26 17:49:07.014 
Epoch 467/1000 
	 loss: 16.4070, MinusLogProbMetric: 16.4070, val_loss: 16.8931, val_MinusLogProbMetric: 16.8931

Epoch 467: val_loss did not improve from 16.84461
196/196 - 34s - loss: 16.4070 - MinusLogProbMetric: 16.4070 - val_loss: 16.8931 - val_MinusLogProbMetric: 16.8931 - lr: 2.5000e-04 - 34s/epoch - 175ms/step
Epoch 468/1000
2023-09-26 17:49:40.841 
Epoch 468/1000 
	 loss: 16.4045, MinusLogProbMetric: 16.4045, val_loss: 16.8707, val_MinusLogProbMetric: 16.8707

Epoch 468: val_loss did not improve from 16.84461
196/196 - 34s - loss: 16.4045 - MinusLogProbMetric: 16.4045 - val_loss: 16.8707 - val_MinusLogProbMetric: 16.8707 - lr: 2.5000e-04 - 34s/epoch - 173ms/step
Epoch 469/1000
2023-09-26 17:50:15.491 
Epoch 469/1000 
	 loss: 16.4149, MinusLogProbMetric: 16.4149, val_loss: 16.8743, val_MinusLogProbMetric: 16.8743

Epoch 469: val_loss did not improve from 16.84461
196/196 - 35s - loss: 16.4149 - MinusLogProbMetric: 16.4149 - val_loss: 16.8743 - val_MinusLogProbMetric: 16.8743 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 470/1000
2023-09-26 17:50:49.861 
Epoch 470/1000 
	 loss: 16.3998, MinusLogProbMetric: 16.3998, val_loss: 16.8515, val_MinusLogProbMetric: 16.8515

Epoch 470: val_loss did not improve from 16.84461
196/196 - 34s - loss: 16.3998 - MinusLogProbMetric: 16.3998 - val_loss: 16.8515 - val_MinusLogProbMetric: 16.8515 - lr: 2.5000e-04 - 34s/epoch - 175ms/step
Epoch 471/1000
2023-09-26 17:51:24.429 
Epoch 471/1000 
	 loss: 16.3984, MinusLogProbMetric: 16.3984, val_loss: 16.8467, val_MinusLogProbMetric: 16.8467

Epoch 471: val_loss did not improve from 16.84461
196/196 - 35s - loss: 16.3984 - MinusLogProbMetric: 16.3984 - val_loss: 16.8467 - val_MinusLogProbMetric: 16.8467 - lr: 2.5000e-04 - 35s/epoch - 176ms/step
Epoch 472/1000
2023-09-26 17:51:58.471 
Epoch 472/1000 
	 loss: 16.4013, MinusLogProbMetric: 16.4013, val_loss: 16.8537, val_MinusLogProbMetric: 16.8537

Epoch 472: val_loss did not improve from 16.84461
196/196 - 34s - loss: 16.4013 - MinusLogProbMetric: 16.4013 - val_loss: 16.8537 - val_MinusLogProbMetric: 16.8537 - lr: 2.5000e-04 - 34s/epoch - 174ms/step
Epoch 473/1000
2023-09-26 17:52:32.518 
Epoch 473/1000 
	 loss: 16.4056, MinusLogProbMetric: 16.4056, val_loss: 16.9450, val_MinusLogProbMetric: 16.9450

Epoch 473: val_loss did not improve from 16.84461
196/196 - 34s - loss: 16.4056 - MinusLogProbMetric: 16.4056 - val_loss: 16.9450 - val_MinusLogProbMetric: 16.9450 - lr: 2.5000e-04 - 34s/epoch - 174ms/step
Epoch 474/1000
2023-09-26 17:53:06.294 
Epoch 474/1000 
	 loss: 16.4020, MinusLogProbMetric: 16.4020, val_loss: 16.8833, val_MinusLogProbMetric: 16.8833

Epoch 474: val_loss did not improve from 16.84461
196/196 - 34s - loss: 16.4020 - MinusLogProbMetric: 16.4020 - val_loss: 16.8833 - val_MinusLogProbMetric: 16.8833 - lr: 2.5000e-04 - 34s/epoch - 172ms/step
Epoch 475/1000
2023-09-26 17:53:40.155 
Epoch 475/1000 
	 loss: 16.4044, MinusLogProbMetric: 16.4044, val_loss: 16.9020, val_MinusLogProbMetric: 16.9020

Epoch 475: val_loss did not improve from 16.84461
196/196 - 34s - loss: 16.4044 - MinusLogProbMetric: 16.4044 - val_loss: 16.9020 - val_MinusLogProbMetric: 16.9020 - lr: 2.5000e-04 - 34s/epoch - 173ms/step
Epoch 476/1000
2023-09-26 17:54:14.371 
Epoch 476/1000 
	 loss: 16.4011, MinusLogProbMetric: 16.4011, val_loss: 16.8844, val_MinusLogProbMetric: 16.8844

Epoch 476: val_loss did not improve from 16.84461
196/196 - 34s - loss: 16.4011 - MinusLogProbMetric: 16.4011 - val_loss: 16.8844 - val_MinusLogProbMetric: 16.8844 - lr: 2.5000e-04 - 34s/epoch - 175ms/step
Epoch 477/1000
2023-09-26 17:54:48.588 
Epoch 477/1000 
	 loss: 16.4018, MinusLogProbMetric: 16.4018, val_loss: 16.8469, val_MinusLogProbMetric: 16.8469

Epoch 477: val_loss did not improve from 16.84461
196/196 - 34s - loss: 16.4018 - MinusLogProbMetric: 16.4018 - val_loss: 16.8469 - val_MinusLogProbMetric: 16.8469 - lr: 2.5000e-04 - 34s/epoch - 175ms/step
Epoch 478/1000
2023-09-26 17:55:22.890 
Epoch 478/1000 
	 loss: 16.4036, MinusLogProbMetric: 16.4036, val_loss: 16.8518, val_MinusLogProbMetric: 16.8518

Epoch 478: val_loss did not improve from 16.84461
196/196 - 34s - loss: 16.4036 - MinusLogProbMetric: 16.4036 - val_loss: 16.8518 - val_MinusLogProbMetric: 16.8518 - lr: 2.5000e-04 - 34s/epoch - 175ms/step
Epoch 479/1000
2023-09-26 17:55:56.835 
Epoch 479/1000 
	 loss: 16.3915, MinusLogProbMetric: 16.3915, val_loss: 16.8738, val_MinusLogProbMetric: 16.8738

Epoch 479: val_loss did not improve from 16.84461
196/196 - 34s - loss: 16.3915 - MinusLogProbMetric: 16.3915 - val_loss: 16.8738 - val_MinusLogProbMetric: 16.8738 - lr: 2.5000e-04 - 34s/epoch - 173ms/step
Epoch 480/1000
2023-09-26 17:56:30.873 
Epoch 480/1000 
	 loss: 16.4036, MinusLogProbMetric: 16.4036, val_loss: 16.8648, val_MinusLogProbMetric: 16.8648

Epoch 480: val_loss did not improve from 16.84461
196/196 - 34s - loss: 16.4036 - MinusLogProbMetric: 16.4036 - val_loss: 16.8648 - val_MinusLogProbMetric: 16.8648 - lr: 2.5000e-04 - 34s/epoch - 174ms/step
Epoch 481/1000
2023-09-26 17:57:04.793 
Epoch 481/1000 
	 loss: 16.4040, MinusLogProbMetric: 16.4040, val_loss: 16.9248, val_MinusLogProbMetric: 16.9248

Epoch 481: val_loss did not improve from 16.84461
196/196 - 34s - loss: 16.4040 - MinusLogProbMetric: 16.4040 - val_loss: 16.9248 - val_MinusLogProbMetric: 16.9248 - lr: 2.5000e-04 - 34s/epoch - 173ms/step
Epoch 482/1000
2023-09-26 17:57:38.915 
Epoch 482/1000 
	 loss: 16.3996, MinusLogProbMetric: 16.3996, val_loss: 16.8816, val_MinusLogProbMetric: 16.8816

Epoch 482: val_loss did not improve from 16.84461
196/196 - 34s - loss: 16.3996 - MinusLogProbMetric: 16.3996 - val_loss: 16.8816 - val_MinusLogProbMetric: 16.8816 - lr: 2.5000e-04 - 34s/epoch - 174ms/step
Epoch 483/1000
2023-09-26 17:58:12.761 
Epoch 483/1000 
	 loss: 16.3702, MinusLogProbMetric: 16.3702, val_loss: 16.8664, val_MinusLogProbMetric: 16.8664

Epoch 483: val_loss did not improve from 16.84461
196/196 - 34s - loss: 16.3702 - MinusLogProbMetric: 16.3702 - val_loss: 16.8664 - val_MinusLogProbMetric: 16.8664 - lr: 1.2500e-04 - 34s/epoch - 173ms/step
Epoch 484/1000
2023-09-26 17:58:46.818 
Epoch 484/1000 
	 loss: 16.3616, MinusLogProbMetric: 16.3616, val_loss: 16.8385, val_MinusLogProbMetric: 16.8385

Epoch 484: val_loss improved from 16.84461 to 16.83853, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_289/weights/best_weights.h5
196/196 - 35s - loss: 16.3616 - MinusLogProbMetric: 16.3616 - val_loss: 16.8385 - val_MinusLogProbMetric: 16.8385 - lr: 1.2500e-04 - 35s/epoch - 176ms/step
Epoch 485/1000
2023-09-26 17:59:21.116 
Epoch 485/1000 
	 loss: 16.3628, MinusLogProbMetric: 16.3628, val_loss: 16.8396, val_MinusLogProbMetric: 16.8396

Epoch 485: val_loss did not improve from 16.83853
196/196 - 34s - loss: 16.3628 - MinusLogProbMetric: 16.3628 - val_loss: 16.8396 - val_MinusLogProbMetric: 16.8396 - lr: 1.2500e-04 - 34s/epoch - 173ms/step
Epoch 486/1000
2023-09-26 17:59:55.664 
Epoch 486/1000 
	 loss: 16.3647, MinusLogProbMetric: 16.3647, val_loss: 16.8564, val_MinusLogProbMetric: 16.8564

Epoch 486: val_loss did not improve from 16.83853
196/196 - 35s - loss: 16.3647 - MinusLogProbMetric: 16.3647 - val_loss: 16.8564 - val_MinusLogProbMetric: 16.8564 - lr: 1.2500e-04 - 35s/epoch - 176ms/step
Epoch 487/1000
2023-09-26 18:00:30.007 
Epoch 487/1000 
	 loss: 16.3629, MinusLogProbMetric: 16.3629, val_loss: 16.8472, val_MinusLogProbMetric: 16.8472

Epoch 487: val_loss did not improve from 16.83853
196/196 - 34s - loss: 16.3629 - MinusLogProbMetric: 16.3629 - val_loss: 16.8472 - val_MinusLogProbMetric: 16.8472 - lr: 1.2500e-04 - 34s/epoch - 175ms/step
Epoch 488/1000
2023-09-26 18:01:03.885 
Epoch 488/1000 
	 loss: 16.3603, MinusLogProbMetric: 16.3603, val_loss: 16.8485, val_MinusLogProbMetric: 16.8485

Epoch 488: val_loss did not improve from 16.83853
196/196 - 34s - loss: 16.3603 - MinusLogProbMetric: 16.3603 - val_loss: 16.8485 - val_MinusLogProbMetric: 16.8485 - lr: 1.2500e-04 - 34s/epoch - 173ms/step
Epoch 489/1000
2023-09-26 18:01:37.578 
Epoch 489/1000 
	 loss: 16.3638, MinusLogProbMetric: 16.3638, val_loss: 16.8520, val_MinusLogProbMetric: 16.8520

Epoch 489: val_loss did not improve from 16.83853
196/196 - 34s - loss: 16.3638 - MinusLogProbMetric: 16.3638 - val_loss: 16.8520 - val_MinusLogProbMetric: 16.8520 - lr: 1.2500e-04 - 34s/epoch - 172ms/step
Epoch 490/1000
2023-09-26 18:02:11.282 
Epoch 490/1000 
	 loss: 16.3611, MinusLogProbMetric: 16.3611, val_loss: 16.8455, val_MinusLogProbMetric: 16.8455

Epoch 490: val_loss did not improve from 16.83853
196/196 - 34s - loss: 16.3611 - MinusLogProbMetric: 16.3611 - val_loss: 16.8455 - val_MinusLogProbMetric: 16.8455 - lr: 1.2500e-04 - 34s/epoch - 172ms/step
Epoch 491/1000
2023-09-26 18:02:44.847 
Epoch 491/1000 
	 loss: 16.3628, MinusLogProbMetric: 16.3628, val_loss: 16.8587, val_MinusLogProbMetric: 16.8587

Epoch 491: val_loss did not improve from 16.83853
196/196 - 34s - loss: 16.3628 - MinusLogProbMetric: 16.3628 - val_loss: 16.8587 - val_MinusLogProbMetric: 16.8587 - lr: 1.2500e-04 - 34s/epoch - 171ms/step
Epoch 492/1000
2023-09-26 18:03:18.500 
Epoch 492/1000 
	 loss: 16.3595, MinusLogProbMetric: 16.3595, val_loss: 16.8650, val_MinusLogProbMetric: 16.8650

Epoch 492: val_loss did not improve from 16.83853
196/196 - 34s - loss: 16.3595 - MinusLogProbMetric: 16.3595 - val_loss: 16.8650 - val_MinusLogProbMetric: 16.8650 - lr: 1.2500e-04 - 34s/epoch - 172ms/step
Epoch 493/1000
2023-09-26 18:03:52.291 
Epoch 493/1000 
	 loss: 16.3673, MinusLogProbMetric: 16.3673, val_loss: 16.8709, val_MinusLogProbMetric: 16.8709

Epoch 493: val_loss did not improve from 16.83853
196/196 - 34s - loss: 16.3673 - MinusLogProbMetric: 16.3673 - val_loss: 16.8709 - val_MinusLogProbMetric: 16.8709 - lr: 1.2500e-04 - 34s/epoch - 172ms/step
Epoch 494/1000
2023-09-26 18:04:26.357 
Epoch 494/1000 
	 loss: 16.3651, MinusLogProbMetric: 16.3651, val_loss: 16.8495, val_MinusLogProbMetric: 16.8495

Epoch 494: val_loss did not improve from 16.83853
196/196 - 34s - loss: 16.3651 - MinusLogProbMetric: 16.3651 - val_loss: 16.8495 - val_MinusLogProbMetric: 16.8495 - lr: 1.2500e-04 - 34s/epoch - 174ms/step
Epoch 495/1000
2023-09-26 18:05:00.325 
Epoch 495/1000 
	 loss: 16.3726, MinusLogProbMetric: 16.3726, val_loss: 16.8459, val_MinusLogProbMetric: 16.8459

Epoch 495: val_loss did not improve from 16.83853
196/196 - 34s - loss: 16.3726 - MinusLogProbMetric: 16.3726 - val_loss: 16.8459 - val_MinusLogProbMetric: 16.8459 - lr: 1.2500e-04 - 34s/epoch - 173ms/step
Epoch 496/1000
2023-09-26 18:05:34.486 
Epoch 496/1000 
	 loss: 16.3601, MinusLogProbMetric: 16.3601, val_loss: 16.8589, val_MinusLogProbMetric: 16.8589

Epoch 496: val_loss did not improve from 16.83853
196/196 - 34s - loss: 16.3601 - MinusLogProbMetric: 16.3601 - val_loss: 16.8589 - val_MinusLogProbMetric: 16.8589 - lr: 1.2500e-04 - 34s/epoch - 174ms/step
Epoch 497/1000
2023-09-26 18:06:08.773 
Epoch 497/1000 
	 loss: 16.3586, MinusLogProbMetric: 16.3586, val_loss: 16.8538, val_MinusLogProbMetric: 16.8538

Epoch 497: val_loss did not improve from 16.83853
196/196 - 34s - loss: 16.3586 - MinusLogProbMetric: 16.3586 - val_loss: 16.8538 - val_MinusLogProbMetric: 16.8538 - lr: 1.2500e-04 - 34s/epoch - 175ms/step
Epoch 498/1000
2023-09-26 18:06:42.730 
Epoch 498/1000 
	 loss: 16.3641, MinusLogProbMetric: 16.3641, val_loss: 16.8570, val_MinusLogProbMetric: 16.8570

Epoch 498: val_loss did not improve from 16.83853
196/196 - 34s - loss: 16.3641 - MinusLogProbMetric: 16.3641 - val_loss: 16.8570 - val_MinusLogProbMetric: 16.8570 - lr: 1.2500e-04 - 34s/epoch - 173ms/step
Epoch 499/1000
2023-09-26 18:07:16.917 
Epoch 499/1000 
	 loss: 16.3634, MinusLogProbMetric: 16.3634, val_loss: 16.8425, val_MinusLogProbMetric: 16.8425

Epoch 499: val_loss did not improve from 16.83853
196/196 - 34s - loss: 16.3634 - MinusLogProbMetric: 16.3634 - val_loss: 16.8425 - val_MinusLogProbMetric: 16.8425 - lr: 1.2500e-04 - 34s/epoch - 174ms/step
Epoch 500/1000
2023-09-26 18:07:50.874 
Epoch 500/1000 
	 loss: 16.3675, MinusLogProbMetric: 16.3675, val_loss: 16.8368, val_MinusLogProbMetric: 16.8368

Epoch 500: val_loss improved from 16.83853 to 16.83680, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_289/weights/best_weights.h5
196/196 - 34s - loss: 16.3675 - MinusLogProbMetric: 16.3675 - val_loss: 16.8368 - val_MinusLogProbMetric: 16.8368 - lr: 1.2500e-04 - 34s/epoch - 176ms/step
Epoch 501/1000
2023-09-26 18:08:25.517 
Epoch 501/1000 
	 loss: 16.3604, MinusLogProbMetric: 16.3604, val_loss: 16.8616, val_MinusLogProbMetric: 16.8616

Epoch 501: val_loss did not improve from 16.83680
196/196 - 34s - loss: 16.3604 - MinusLogProbMetric: 16.3604 - val_loss: 16.8616 - val_MinusLogProbMetric: 16.8616 - lr: 1.2500e-04 - 34s/epoch - 174ms/step
Epoch 502/1000
2023-09-26 18:08:59.814 
Epoch 502/1000 
	 loss: 16.3620, MinusLogProbMetric: 16.3620, val_loss: 16.8461, val_MinusLogProbMetric: 16.8461

Epoch 502: val_loss did not improve from 16.83680
196/196 - 34s - loss: 16.3620 - MinusLogProbMetric: 16.3620 - val_loss: 16.8461 - val_MinusLogProbMetric: 16.8461 - lr: 1.2500e-04 - 34s/epoch - 175ms/step
Epoch 503/1000
2023-09-26 18:09:33.720 
Epoch 503/1000 
	 loss: 16.3571, MinusLogProbMetric: 16.3571, val_loss: 16.8484, val_MinusLogProbMetric: 16.8484

Epoch 503: val_loss did not improve from 16.83680
196/196 - 34s - loss: 16.3571 - MinusLogProbMetric: 16.3571 - val_loss: 16.8484 - val_MinusLogProbMetric: 16.8484 - lr: 1.2500e-04 - 34s/epoch - 173ms/step
Epoch 504/1000
2023-09-26 18:10:07.518 
Epoch 504/1000 
	 loss: 16.3613, MinusLogProbMetric: 16.3613, val_loss: 16.8467, val_MinusLogProbMetric: 16.8467

Epoch 504: val_loss did not improve from 16.83680
196/196 - 34s - loss: 16.3613 - MinusLogProbMetric: 16.3613 - val_loss: 16.8467 - val_MinusLogProbMetric: 16.8467 - lr: 1.2500e-04 - 34s/epoch - 172ms/step
Epoch 505/1000
2023-09-26 18:10:41.447 
Epoch 505/1000 
	 loss: 16.3628, MinusLogProbMetric: 16.3628, val_loss: 16.8474, val_MinusLogProbMetric: 16.8474

Epoch 505: val_loss did not improve from 16.83680
196/196 - 34s - loss: 16.3628 - MinusLogProbMetric: 16.3628 - val_loss: 16.8474 - val_MinusLogProbMetric: 16.8474 - lr: 1.2500e-04 - 34s/epoch - 173ms/step
Epoch 506/1000
2023-09-26 18:11:15.455 
Epoch 506/1000 
	 loss: 16.3597, MinusLogProbMetric: 16.3597, val_loss: 16.8490, val_MinusLogProbMetric: 16.8490

Epoch 506: val_loss did not improve from 16.83680
196/196 - 34s - loss: 16.3597 - MinusLogProbMetric: 16.3597 - val_loss: 16.8490 - val_MinusLogProbMetric: 16.8490 - lr: 1.2500e-04 - 34s/epoch - 173ms/step
Epoch 507/1000
2023-09-26 18:11:49.107 
Epoch 507/1000 
	 loss: 16.3602, MinusLogProbMetric: 16.3602, val_loss: 16.8467, val_MinusLogProbMetric: 16.8467

Epoch 507: val_loss did not improve from 16.83680
196/196 - 34s - loss: 16.3602 - MinusLogProbMetric: 16.3602 - val_loss: 16.8467 - val_MinusLogProbMetric: 16.8467 - lr: 1.2500e-04 - 34s/epoch - 172ms/step
Epoch 508/1000
2023-09-26 18:12:23.392 
Epoch 508/1000 
	 loss: 16.3613, MinusLogProbMetric: 16.3613, val_loss: 16.8524, val_MinusLogProbMetric: 16.8524

Epoch 508: val_loss did not improve from 16.83680
196/196 - 34s - loss: 16.3613 - MinusLogProbMetric: 16.3613 - val_loss: 16.8524 - val_MinusLogProbMetric: 16.8524 - lr: 1.2500e-04 - 34s/epoch - 175ms/step
Epoch 509/1000
2023-09-26 18:12:57.320 
Epoch 509/1000 
	 loss: 16.3652, MinusLogProbMetric: 16.3652, val_loss: 16.8825, val_MinusLogProbMetric: 16.8825

Epoch 509: val_loss did not improve from 16.83680
196/196 - 34s - loss: 16.3652 - MinusLogProbMetric: 16.3652 - val_loss: 16.8825 - val_MinusLogProbMetric: 16.8825 - lr: 1.2500e-04 - 34s/epoch - 173ms/step
Epoch 510/1000
2023-09-26 18:13:31.677 
Epoch 510/1000 
	 loss: 16.3620, MinusLogProbMetric: 16.3620, val_loss: 16.8698, val_MinusLogProbMetric: 16.8698

Epoch 510: val_loss did not improve from 16.83680
196/196 - 34s - loss: 16.3620 - MinusLogProbMetric: 16.3620 - val_loss: 16.8698 - val_MinusLogProbMetric: 16.8698 - lr: 1.2500e-04 - 34s/epoch - 175ms/step
Epoch 511/1000
2023-09-26 18:14:05.580 
Epoch 511/1000 
	 loss: 16.3598, MinusLogProbMetric: 16.3598, val_loss: 16.8596, val_MinusLogProbMetric: 16.8596

Epoch 511: val_loss did not improve from 16.83680
196/196 - 34s - loss: 16.3598 - MinusLogProbMetric: 16.3598 - val_loss: 16.8596 - val_MinusLogProbMetric: 16.8596 - lr: 1.2500e-04 - 34s/epoch - 173ms/step
Epoch 512/1000
2023-09-26 18:14:39.754 
Epoch 512/1000 
	 loss: 16.3640, MinusLogProbMetric: 16.3640, val_loss: 16.8422, val_MinusLogProbMetric: 16.8422

Epoch 512: val_loss did not improve from 16.83680
196/196 - 34s - loss: 16.3640 - MinusLogProbMetric: 16.3640 - val_loss: 16.8422 - val_MinusLogProbMetric: 16.8422 - lr: 1.2500e-04 - 34s/epoch - 174ms/step
Epoch 513/1000
2023-09-26 18:15:13.895 
Epoch 513/1000 
	 loss: 16.3613, MinusLogProbMetric: 16.3613, val_loss: 16.8508, val_MinusLogProbMetric: 16.8508

Epoch 513: val_loss did not improve from 16.83680
196/196 - 34s - loss: 16.3613 - MinusLogProbMetric: 16.3613 - val_loss: 16.8508 - val_MinusLogProbMetric: 16.8508 - lr: 1.2500e-04 - 34s/epoch - 174ms/step
Epoch 514/1000
2023-09-26 18:15:47.800 
Epoch 514/1000 
	 loss: 16.3632, MinusLogProbMetric: 16.3632, val_loss: 16.8684, val_MinusLogProbMetric: 16.8684

Epoch 514: val_loss did not improve from 16.83680
196/196 - 34s - loss: 16.3632 - MinusLogProbMetric: 16.3632 - val_loss: 16.8684 - val_MinusLogProbMetric: 16.8684 - lr: 1.2500e-04 - 34s/epoch - 173ms/step
Epoch 515/1000
2023-09-26 18:16:21.936 
Epoch 515/1000 
	 loss: 16.3611, MinusLogProbMetric: 16.3611, val_loss: 16.8548, val_MinusLogProbMetric: 16.8548

Epoch 515: val_loss did not improve from 16.83680
196/196 - 34s - loss: 16.3611 - MinusLogProbMetric: 16.3611 - val_loss: 16.8548 - val_MinusLogProbMetric: 16.8548 - lr: 1.2500e-04 - 34s/epoch - 174ms/step
Epoch 516/1000
2023-09-26 18:16:55.695 
Epoch 516/1000 
	 loss: 16.3611, MinusLogProbMetric: 16.3611, val_loss: 16.8452, val_MinusLogProbMetric: 16.8452

Epoch 516: val_loss did not improve from 16.83680
196/196 - 34s - loss: 16.3611 - MinusLogProbMetric: 16.3611 - val_loss: 16.8452 - val_MinusLogProbMetric: 16.8452 - lr: 1.2500e-04 - 34s/epoch - 172ms/step
Epoch 517/1000
2023-09-26 18:17:29.542 
Epoch 517/1000 
	 loss: 16.3645, MinusLogProbMetric: 16.3645, val_loss: 16.8453, val_MinusLogProbMetric: 16.8453

Epoch 517: val_loss did not improve from 16.83680
196/196 - 34s - loss: 16.3645 - MinusLogProbMetric: 16.3645 - val_loss: 16.8453 - val_MinusLogProbMetric: 16.8453 - lr: 1.2500e-04 - 34s/epoch - 173ms/step
Epoch 518/1000
2023-09-26 18:18:03.431 
Epoch 518/1000 
	 loss: 16.3580, MinusLogProbMetric: 16.3580, val_loss: 16.8498, val_MinusLogProbMetric: 16.8498

Epoch 518: val_loss did not improve from 16.83680
196/196 - 34s - loss: 16.3580 - MinusLogProbMetric: 16.3580 - val_loss: 16.8498 - val_MinusLogProbMetric: 16.8498 - lr: 1.2500e-04 - 34s/epoch - 173ms/step
Epoch 519/1000
2023-09-26 18:18:37.704 
Epoch 519/1000 
	 loss: 16.3703, MinusLogProbMetric: 16.3703, val_loss: 16.8680, val_MinusLogProbMetric: 16.8680

Epoch 519: val_loss did not improve from 16.83680
196/196 - 34s - loss: 16.3703 - MinusLogProbMetric: 16.3703 - val_loss: 16.8680 - val_MinusLogProbMetric: 16.8680 - lr: 1.2500e-04 - 34s/epoch - 175ms/step
Epoch 520/1000
2023-09-26 18:19:11.608 
Epoch 520/1000 
	 loss: 16.3605, MinusLogProbMetric: 16.3605, val_loss: 16.8443, val_MinusLogProbMetric: 16.8443

Epoch 520: val_loss did not improve from 16.83680
196/196 - 34s - loss: 16.3605 - MinusLogProbMetric: 16.3605 - val_loss: 16.8443 - val_MinusLogProbMetric: 16.8443 - lr: 1.2500e-04 - 34s/epoch - 173ms/step
Epoch 521/1000
2023-09-26 18:19:45.376 
Epoch 521/1000 
	 loss: 16.3618, MinusLogProbMetric: 16.3618, val_loss: 16.8610, val_MinusLogProbMetric: 16.8610

Epoch 521: val_loss did not improve from 16.83680
196/196 - 34s - loss: 16.3618 - MinusLogProbMetric: 16.3618 - val_loss: 16.8610 - val_MinusLogProbMetric: 16.8610 - lr: 1.2500e-04 - 34s/epoch - 172ms/step
Epoch 522/1000
2023-09-26 18:20:19.229 
Epoch 522/1000 
	 loss: 16.3601, MinusLogProbMetric: 16.3601, val_loss: 16.8411, val_MinusLogProbMetric: 16.8411

Epoch 522: val_loss did not improve from 16.83680
196/196 - 34s - loss: 16.3601 - MinusLogProbMetric: 16.3601 - val_loss: 16.8411 - val_MinusLogProbMetric: 16.8411 - lr: 1.2500e-04 - 34s/epoch - 173ms/step
Epoch 523/1000
2023-09-26 18:20:53.182 
Epoch 523/1000 
	 loss: 16.3608, MinusLogProbMetric: 16.3608, val_loss: 16.8344, val_MinusLogProbMetric: 16.8344

Epoch 523: val_loss improved from 16.83680 to 16.83441, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_289/weights/best_weights.h5
196/196 - 35s - loss: 16.3608 - MinusLogProbMetric: 16.3608 - val_loss: 16.8344 - val_MinusLogProbMetric: 16.8344 - lr: 1.2500e-04 - 35s/epoch - 176ms/step
Epoch 524/1000
2023-09-26 18:21:27.419 
Epoch 524/1000 
	 loss: 16.3606, MinusLogProbMetric: 16.3606, val_loss: 16.8514, val_MinusLogProbMetric: 16.8514

Epoch 524: val_loss did not improve from 16.83441
196/196 - 34s - loss: 16.3606 - MinusLogProbMetric: 16.3606 - val_loss: 16.8514 - val_MinusLogProbMetric: 16.8514 - lr: 1.2500e-04 - 34s/epoch - 172ms/step
Epoch 525/1000
2023-09-26 18:22:01.269 
Epoch 525/1000 
	 loss: 16.3594, MinusLogProbMetric: 16.3594, val_loss: 16.8552, val_MinusLogProbMetric: 16.8552

Epoch 525: val_loss did not improve from 16.83441
196/196 - 34s - loss: 16.3594 - MinusLogProbMetric: 16.3594 - val_loss: 16.8552 - val_MinusLogProbMetric: 16.8552 - lr: 1.2500e-04 - 34s/epoch - 173ms/step
Epoch 526/1000
2023-09-26 18:22:35.425 
Epoch 526/1000 
	 loss: 16.3621, MinusLogProbMetric: 16.3621, val_loss: 16.8589, val_MinusLogProbMetric: 16.8589

Epoch 526: val_loss did not improve from 16.83441
196/196 - 34s - loss: 16.3621 - MinusLogProbMetric: 16.3621 - val_loss: 16.8589 - val_MinusLogProbMetric: 16.8589 - lr: 1.2500e-04 - 34s/epoch - 174ms/step
Epoch 527/1000
2023-09-26 18:23:09.794 
Epoch 527/1000 
	 loss: 16.3619, MinusLogProbMetric: 16.3619, val_loss: 16.8500, val_MinusLogProbMetric: 16.8500

Epoch 527: val_loss did not improve from 16.83441
196/196 - 34s - loss: 16.3619 - MinusLogProbMetric: 16.3619 - val_loss: 16.8500 - val_MinusLogProbMetric: 16.8500 - lr: 1.2500e-04 - 34s/epoch - 175ms/step
Epoch 528/1000
2023-09-26 18:23:44.138 
Epoch 528/1000 
	 loss: 16.3586, MinusLogProbMetric: 16.3586, val_loss: 16.9052, val_MinusLogProbMetric: 16.9052

Epoch 528: val_loss did not improve from 16.83441
196/196 - 34s - loss: 16.3586 - MinusLogProbMetric: 16.3586 - val_loss: 16.9052 - val_MinusLogProbMetric: 16.9052 - lr: 1.2500e-04 - 34s/epoch - 175ms/step
Epoch 529/1000
2023-09-26 18:24:17.981 
Epoch 529/1000 
	 loss: 16.3620, MinusLogProbMetric: 16.3620, val_loss: 16.8437, val_MinusLogProbMetric: 16.8437

Epoch 529: val_loss did not improve from 16.83441
196/196 - 34s - loss: 16.3620 - MinusLogProbMetric: 16.3620 - val_loss: 16.8437 - val_MinusLogProbMetric: 16.8437 - lr: 1.2500e-04 - 34s/epoch - 173ms/step
Epoch 530/1000
2023-09-26 18:24:52.123 
Epoch 530/1000 
	 loss: 16.3583, MinusLogProbMetric: 16.3583, val_loss: 16.8538, val_MinusLogProbMetric: 16.8538

Epoch 530: val_loss did not improve from 16.83441
196/196 - 34s - loss: 16.3583 - MinusLogProbMetric: 16.3583 - val_loss: 16.8538 - val_MinusLogProbMetric: 16.8538 - lr: 1.2500e-04 - 34s/epoch - 174ms/step
Epoch 531/1000
2023-09-26 18:25:26.135 
Epoch 531/1000 
	 loss: 16.3595, MinusLogProbMetric: 16.3595, val_loss: 16.8492, val_MinusLogProbMetric: 16.8492

Epoch 531: val_loss did not improve from 16.83441
196/196 - 34s - loss: 16.3595 - MinusLogProbMetric: 16.3595 - val_loss: 16.8492 - val_MinusLogProbMetric: 16.8492 - lr: 1.2500e-04 - 34s/epoch - 174ms/step
Epoch 532/1000
2023-09-26 18:26:00.016 
Epoch 532/1000 
	 loss: 16.3568, MinusLogProbMetric: 16.3568, val_loss: 16.8746, val_MinusLogProbMetric: 16.8746

Epoch 532: val_loss did not improve from 16.83441
196/196 - 34s - loss: 16.3568 - MinusLogProbMetric: 16.3568 - val_loss: 16.8746 - val_MinusLogProbMetric: 16.8746 - lr: 1.2500e-04 - 34s/epoch - 173ms/step
Epoch 533/1000
2023-09-26 18:26:34.237 
Epoch 533/1000 
	 loss: 16.3602, MinusLogProbMetric: 16.3602, val_loss: 16.8692, val_MinusLogProbMetric: 16.8692

Epoch 533: val_loss did not improve from 16.83441
196/196 - 34s - loss: 16.3602 - MinusLogProbMetric: 16.3602 - val_loss: 16.8692 - val_MinusLogProbMetric: 16.8692 - lr: 1.2500e-04 - 34s/epoch - 175ms/step
Epoch 534/1000
2023-09-26 18:27:08.376 
Epoch 534/1000 
	 loss: 16.3590, MinusLogProbMetric: 16.3590, val_loss: 16.8443, val_MinusLogProbMetric: 16.8443

Epoch 534: val_loss did not improve from 16.83441
196/196 - 34s - loss: 16.3590 - MinusLogProbMetric: 16.3590 - val_loss: 16.8443 - val_MinusLogProbMetric: 16.8443 - lr: 1.2500e-04 - 34s/epoch - 174ms/step
Epoch 535/1000
2023-09-26 18:27:42.683 
Epoch 535/1000 
	 loss: 16.3581, MinusLogProbMetric: 16.3581, val_loss: 16.8539, val_MinusLogProbMetric: 16.8539

Epoch 535: val_loss did not improve from 16.83441
196/196 - 34s - loss: 16.3581 - MinusLogProbMetric: 16.3581 - val_loss: 16.8539 - val_MinusLogProbMetric: 16.8539 - lr: 1.2500e-04 - 34s/epoch - 175ms/step
Epoch 536/1000
2023-09-26 18:28:16.844 
Epoch 536/1000 
	 loss: 16.3585, MinusLogProbMetric: 16.3585, val_loss: 16.8415, val_MinusLogProbMetric: 16.8415

Epoch 536: val_loss did not improve from 16.83441
196/196 - 34s - loss: 16.3585 - MinusLogProbMetric: 16.3585 - val_loss: 16.8415 - val_MinusLogProbMetric: 16.8415 - lr: 1.2500e-04 - 34s/epoch - 174ms/step
Epoch 537/1000
2023-09-26 18:28:51.088 
Epoch 537/1000 
	 loss: 16.3594, MinusLogProbMetric: 16.3594, val_loss: 16.8453, val_MinusLogProbMetric: 16.8453

Epoch 537: val_loss did not improve from 16.83441
196/196 - 34s - loss: 16.3594 - MinusLogProbMetric: 16.3594 - val_loss: 16.8453 - val_MinusLogProbMetric: 16.8453 - lr: 1.2500e-04 - 34s/epoch - 175ms/step
Epoch 538/1000
2023-09-26 18:29:25.240 
Epoch 538/1000 
	 loss: 16.3620, MinusLogProbMetric: 16.3620, val_loss: 16.8475, val_MinusLogProbMetric: 16.8475

Epoch 538: val_loss did not improve from 16.83441
196/196 - 34s - loss: 16.3620 - MinusLogProbMetric: 16.3620 - val_loss: 16.8475 - val_MinusLogProbMetric: 16.8475 - lr: 1.2500e-04 - 34s/epoch - 174ms/step
Epoch 539/1000
2023-09-26 18:29:59.415 
Epoch 539/1000 
	 loss: 16.3590, MinusLogProbMetric: 16.3590, val_loss: 16.8576, val_MinusLogProbMetric: 16.8576

Epoch 539: val_loss did not improve from 16.83441
196/196 - 34s - loss: 16.3590 - MinusLogProbMetric: 16.3590 - val_loss: 16.8576 - val_MinusLogProbMetric: 16.8576 - lr: 1.2500e-04 - 34s/epoch - 174ms/step
Epoch 540/1000
2023-09-26 18:30:33.343 
Epoch 540/1000 
	 loss: 16.3650, MinusLogProbMetric: 16.3650, val_loss: 16.8862, val_MinusLogProbMetric: 16.8862

Epoch 540: val_loss did not improve from 16.83441
196/196 - 34s - loss: 16.3650 - MinusLogProbMetric: 16.3650 - val_loss: 16.8862 - val_MinusLogProbMetric: 16.8862 - lr: 1.2500e-04 - 34s/epoch - 173ms/step
Epoch 541/1000
2023-09-26 18:31:07.593 
Epoch 541/1000 
	 loss: 16.3602, MinusLogProbMetric: 16.3602, val_loss: 16.8420, val_MinusLogProbMetric: 16.8420

Epoch 541: val_loss did not improve from 16.83441
196/196 - 34s - loss: 16.3602 - MinusLogProbMetric: 16.3602 - val_loss: 16.8420 - val_MinusLogProbMetric: 16.8420 - lr: 1.2500e-04 - 34s/epoch - 175ms/step
Epoch 542/1000
2023-09-26 18:31:41.303 
Epoch 542/1000 
	 loss: 16.3564, MinusLogProbMetric: 16.3564, val_loss: 16.8497, val_MinusLogProbMetric: 16.8497

Epoch 542: val_loss did not improve from 16.83441
196/196 - 34s - loss: 16.3564 - MinusLogProbMetric: 16.3564 - val_loss: 16.8497 - val_MinusLogProbMetric: 16.8497 - lr: 1.2500e-04 - 34s/epoch - 172ms/step
Epoch 543/1000
2023-09-26 18:32:15.198 
Epoch 543/1000 
	 loss: 16.3521, MinusLogProbMetric: 16.3521, val_loss: 16.8364, val_MinusLogProbMetric: 16.8364

Epoch 543: val_loss did not improve from 16.83441
196/196 - 34s - loss: 16.3521 - MinusLogProbMetric: 16.3521 - val_loss: 16.8364 - val_MinusLogProbMetric: 16.8364 - lr: 1.2500e-04 - 34s/epoch - 173ms/step
Epoch 544/1000
2023-09-26 18:32:49.359 
Epoch 544/1000 
	 loss: 16.3562, MinusLogProbMetric: 16.3562, val_loss: 16.8510, val_MinusLogProbMetric: 16.8510

Epoch 544: val_loss did not improve from 16.83441
196/196 - 34s - loss: 16.3562 - MinusLogProbMetric: 16.3562 - val_loss: 16.8510 - val_MinusLogProbMetric: 16.8510 - lr: 1.2500e-04 - 34s/epoch - 174ms/step
Epoch 545/1000
2023-09-26 18:33:23.071 
Epoch 545/1000 
	 loss: 16.3599, MinusLogProbMetric: 16.3599, val_loss: 16.8515, val_MinusLogProbMetric: 16.8515

Epoch 545: val_loss did not improve from 16.83441
196/196 - 34s - loss: 16.3599 - MinusLogProbMetric: 16.3599 - val_loss: 16.8515 - val_MinusLogProbMetric: 16.8515 - lr: 1.2500e-04 - 34s/epoch - 172ms/step
Epoch 546/1000
2023-09-26 18:33:57.356 
Epoch 546/1000 
	 loss: 16.3521, MinusLogProbMetric: 16.3521, val_loss: 16.8523, val_MinusLogProbMetric: 16.8523

Epoch 546: val_loss did not improve from 16.83441
196/196 - 34s - loss: 16.3521 - MinusLogProbMetric: 16.3521 - val_loss: 16.8523 - val_MinusLogProbMetric: 16.8523 - lr: 1.2500e-04 - 34s/epoch - 175ms/step
Epoch 547/1000
2023-09-26 18:34:31.303 
Epoch 547/1000 
	 loss: 16.3554, MinusLogProbMetric: 16.3554, val_loss: 16.8502, val_MinusLogProbMetric: 16.8502

Epoch 547: val_loss did not improve from 16.83441
196/196 - 34s - loss: 16.3554 - MinusLogProbMetric: 16.3554 - val_loss: 16.8502 - val_MinusLogProbMetric: 16.8502 - lr: 1.2500e-04 - 34s/epoch - 173ms/step
Epoch 548/1000
2023-09-26 18:35:05.626 
Epoch 548/1000 
	 loss: 16.3548, MinusLogProbMetric: 16.3548, val_loss: 16.8410, val_MinusLogProbMetric: 16.8410

Epoch 548: val_loss did not improve from 16.83441
196/196 - 34s - loss: 16.3548 - MinusLogProbMetric: 16.3548 - val_loss: 16.8410 - val_MinusLogProbMetric: 16.8410 - lr: 1.2500e-04 - 34s/epoch - 175ms/step
Epoch 549/1000
2023-09-26 18:35:39.538 
Epoch 549/1000 
	 loss: 16.3563, MinusLogProbMetric: 16.3563, val_loss: 16.8715, val_MinusLogProbMetric: 16.8715

Epoch 549: val_loss did not improve from 16.83441
196/196 - 34s - loss: 16.3563 - MinusLogProbMetric: 16.3563 - val_loss: 16.8715 - val_MinusLogProbMetric: 16.8715 - lr: 1.2500e-04 - 34s/epoch - 173ms/step
Epoch 550/1000
2023-09-26 18:36:13.676 
Epoch 550/1000 
	 loss: 16.3597, MinusLogProbMetric: 16.3597, val_loss: 16.8490, val_MinusLogProbMetric: 16.8490

Epoch 550: val_loss did not improve from 16.83441
196/196 - 34s - loss: 16.3597 - MinusLogProbMetric: 16.3597 - val_loss: 16.8490 - val_MinusLogProbMetric: 16.8490 - lr: 1.2500e-04 - 34s/epoch - 174ms/step
Epoch 551/1000
2023-09-26 18:36:47.917 
Epoch 551/1000 
	 loss: 16.3626, MinusLogProbMetric: 16.3626, val_loss: 16.8366, val_MinusLogProbMetric: 16.8366

Epoch 551: val_loss did not improve from 16.83441
196/196 - 34s - loss: 16.3626 - MinusLogProbMetric: 16.3626 - val_loss: 16.8366 - val_MinusLogProbMetric: 16.8366 - lr: 1.2500e-04 - 34s/epoch - 175ms/step
Epoch 552/1000
2023-09-26 18:37:22.148 
Epoch 552/1000 
	 loss: 16.3553, MinusLogProbMetric: 16.3553, val_loss: 16.8420, val_MinusLogProbMetric: 16.8420

Epoch 552: val_loss did not improve from 16.83441
196/196 - 34s - loss: 16.3553 - MinusLogProbMetric: 16.3553 - val_loss: 16.8420 - val_MinusLogProbMetric: 16.8420 - lr: 1.2500e-04 - 34s/epoch - 175ms/step
Epoch 553/1000
2023-09-26 18:37:56.262 
Epoch 553/1000 
	 loss: 16.3594, MinusLogProbMetric: 16.3594, val_loss: 16.8440, val_MinusLogProbMetric: 16.8440

Epoch 553: val_loss did not improve from 16.83441
196/196 - 34s - loss: 16.3594 - MinusLogProbMetric: 16.3594 - val_loss: 16.8440 - val_MinusLogProbMetric: 16.8440 - lr: 1.2500e-04 - 34s/epoch - 174ms/step
Epoch 554/1000
2023-09-26 18:38:30.361 
Epoch 554/1000 
	 loss: 16.3568, MinusLogProbMetric: 16.3568, val_loss: 16.9268, val_MinusLogProbMetric: 16.9268

Epoch 554: val_loss did not improve from 16.83441
196/196 - 34s - loss: 16.3568 - MinusLogProbMetric: 16.3568 - val_loss: 16.9268 - val_MinusLogProbMetric: 16.9268 - lr: 1.2500e-04 - 34s/epoch - 174ms/step
Epoch 555/1000
2023-09-26 18:39:04.441 
Epoch 555/1000 
	 loss: 16.3555, MinusLogProbMetric: 16.3555, val_loss: 16.8482, val_MinusLogProbMetric: 16.8482

Epoch 555: val_loss did not improve from 16.83441
196/196 - 34s - loss: 16.3555 - MinusLogProbMetric: 16.3555 - val_loss: 16.8482 - val_MinusLogProbMetric: 16.8482 - lr: 1.2500e-04 - 34s/epoch - 174ms/step
Epoch 556/1000
2023-09-26 18:39:38.686 
Epoch 556/1000 
	 loss: 16.3544, MinusLogProbMetric: 16.3544, val_loss: 16.8844, val_MinusLogProbMetric: 16.8844

Epoch 556: val_loss did not improve from 16.83441
196/196 - 34s - loss: 16.3544 - MinusLogProbMetric: 16.3544 - val_loss: 16.8844 - val_MinusLogProbMetric: 16.8844 - lr: 1.2500e-04 - 34s/epoch - 175ms/step
Epoch 557/1000
2023-09-26 18:40:13.043 
Epoch 557/1000 
	 loss: 16.3557, MinusLogProbMetric: 16.3557, val_loss: 16.8441, val_MinusLogProbMetric: 16.8441

Epoch 557: val_loss did not improve from 16.83441
196/196 - 34s - loss: 16.3557 - MinusLogProbMetric: 16.3557 - val_loss: 16.8441 - val_MinusLogProbMetric: 16.8441 - lr: 1.2500e-04 - 34s/epoch - 175ms/step
Epoch 558/1000
2023-09-26 18:40:47.011 
Epoch 558/1000 
	 loss: 16.3520, MinusLogProbMetric: 16.3520, val_loss: 16.8770, val_MinusLogProbMetric: 16.8770

Epoch 558: val_loss did not improve from 16.83441
196/196 - 34s - loss: 16.3520 - MinusLogProbMetric: 16.3520 - val_loss: 16.8770 - val_MinusLogProbMetric: 16.8770 - lr: 1.2500e-04 - 34s/epoch - 173ms/step
Epoch 559/1000
2023-09-26 18:41:21.090 
Epoch 559/1000 
	 loss: 16.3557, MinusLogProbMetric: 16.3557, val_loss: 16.8489, val_MinusLogProbMetric: 16.8489

Epoch 559: val_loss did not improve from 16.83441
196/196 - 34s - loss: 16.3557 - MinusLogProbMetric: 16.3557 - val_loss: 16.8489 - val_MinusLogProbMetric: 16.8489 - lr: 1.2500e-04 - 34s/epoch - 174ms/step
Epoch 560/1000
2023-09-26 18:41:55.235 
Epoch 560/1000 
	 loss: 16.3581, MinusLogProbMetric: 16.3581, val_loss: 16.9024, val_MinusLogProbMetric: 16.9024

Epoch 560: val_loss did not improve from 16.83441
196/196 - 34s - loss: 16.3581 - MinusLogProbMetric: 16.3581 - val_loss: 16.9024 - val_MinusLogProbMetric: 16.9024 - lr: 1.2500e-04 - 34s/epoch - 174ms/step
Epoch 561/1000
2023-09-26 18:42:29.224 
Epoch 561/1000 
	 loss: 16.3585, MinusLogProbMetric: 16.3585, val_loss: 16.8513, val_MinusLogProbMetric: 16.8513

Epoch 561: val_loss did not improve from 16.83441
196/196 - 34s - loss: 16.3585 - MinusLogProbMetric: 16.3585 - val_loss: 16.8513 - val_MinusLogProbMetric: 16.8513 - lr: 1.2500e-04 - 34s/epoch - 173ms/step
Epoch 562/1000
2023-09-26 18:43:03.376 
Epoch 562/1000 
	 loss: 16.3550, MinusLogProbMetric: 16.3550, val_loss: 16.8856, val_MinusLogProbMetric: 16.8856

Epoch 562: val_loss did not improve from 16.83441
196/196 - 34s - loss: 16.3550 - MinusLogProbMetric: 16.3550 - val_loss: 16.8856 - val_MinusLogProbMetric: 16.8856 - lr: 1.2500e-04 - 34s/epoch - 174ms/step
Epoch 563/1000
2023-09-26 18:43:37.545 
Epoch 563/1000 
	 loss: 16.3542, MinusLogProbMetric: 16.3542, val_loss: 16.8519, val_MinusLogProbMetric: 16.8519

Epoch 563: val_loss did not improve from 16.83441
196/196 - 34s - loss: 16.3542 - MinusLogProbMetric: 16.3542 - val_loss: 16.8519 - val_MinusLogProbMetric: 16.8519 - lr: 1.2500e-04 - 34s/epoch - 174ms/step
Epoch 564/1000
2023-09-26 18:44:11.740 
Epoch 564/1000 
	 loss: 16.3538, MinusLogProbMetric: 16.3538, val_loss: 16.8480, val_MinusLogProbMetric: 16.8480

Epoch 564: val_loss did not improve from 16.83441
196/196 - 34s - loss: 16.3538 - MinusLogProbMetric: 16.3538 - val_loss: 16.8480 - val_MinusLogProbMetric: 16.8480 - lr: 1.2500e-04 - 34s/epoch - 174ms/step
Epoch 565/1000
2023-09-26 18:44:45.818 
Epoch 565/1000 
	 loss: 16.3519, MinusLogProbMetric: 16.3519, val_loss: 16.8556, val_MinusLogProbMetric: 16.8556

Epoch 565: val_loss did not improve from 16.83441
196/196 - 34s - loss: 16.3519 - MinusLogProbMetric: 16.3519 - val_loss: 16.8556 - val_MinusLogProbMetric: 16.8556 - lr: 1.2500e-04 - 34s/epoch - 174ms/step
Epoch 566/1000
2023-09-26 18:45:20.156 
Epoch 566/1000 
	 loss: 16.3518, MinusLogProbMetric: 16.3518, val_loss: 16.8555, val_MinusLogProbMetric: 16.8555

Epoch 566: val_loss did not improve from 16.83441
196/196 - 34s - loss: 16.3518 - MinusLogProbMetric: 16.3518 - val_loss: 16.8555 - val_MinusLogProbMetric: 16.8555 - lr: 1.2500e-04 - 34s/epoch - 175ms/step
Epoch 567/1000
2023-09-26 18:45:54.407 
Epoch 567/1000 
	 loss: 16.3610, MinusLogProbMetric: 16.3610, val_loss: 16.8670, val_MinusLogProbMetric: 16.8670

Epoch 567: val_loss did not improve from 16.83441
196/196 - 34s - loss: 16.3610 - MinusLogProbMetric: 16.3610 - val_loss: 16.8670 - val_MinusLogProbMetric: 16.8670 - lr: 1.2500e-04 - 34s/epoch - 175ms/step
Epoch 568/1000
2023-09-26 18:46:28.807 
Epoch 568/1000 
	 loss: 16.3596, MinusLogProbMetric: 16.3596, val_loss: 16.8736, val_MinusLogProbMetric: 16.8736

Epoch 568: val_loss did not improve from 16.83441
196/196 - 34s - loss: 16.3596 - MinusLogProbMetric: 16.3596 - val_loss: 16.8736 - val_MinusLogProbMetric: 16.8736 - lr: 1.2500e-04 - 34s/epoch - 175ms/step
Epoch 569/1000
2023-09-26 18:47:03.157 
Epoch 569/1000 
	 loss: 16.3564, MinusLogProbMetric: 16.3564, val_loss: 16.8461, val_MinusLogProbMetric: 16.8461

Epoch 569: val_loss did not improve from 16.83441
196/196 - 34s - loss: 16.3564 - MinusLogProbMetric: 16.3564 - val_loss: 16.8461 - val_MinusLogProbMetric: 16.8461 - lr: 1.2500e-04 - 34s/epoch - 175ms/step
Epoch 570/1000
2023-09-26 18:47:37.540 
Epoch 570/1000 
	 loss: 16.3555, MinusLogProbMetric: 16.3555, val_loss: 16.8529, val_MinusLogProbMetric: 16.8529

Epoch 570: val_loss did not improve from 16.83441
196/196 - 34s - loss: 16.3555 - MinusLogProbMetric: 16.3555 - val_loss: 16.8529 - val_MinusLogProbMetric: 16.8529 - lr: 1.2500e-04 - 34s/epoch - 175ms/step
Epoch 571/1000
2023-09-26 18:48:12.071 
Epoch 571/1000 
	 loss: 16.3543, MinusLogProbMetric: 16.3543, val_loss: 16.9165, val_MinusLogProbMetric: 16.9165

Epoch 571: val_loss did not improve from 16.83441
196/196 - 35s - loss: 16.3543 - MinusLogProbMetric: 16.3543 - val_loss: 16.9165 - val_MinusLogProbMetric: 16.9165 - lr: 1.2500e-04 - 35s/epoch - 176ms/step
Epoch 572/1000
2023-09-26 18:48:46.285 
Epoch 572/1000 
	 loss: 16.3550, MinusLogProbMetric: 16.3550, val_loss: 16.8876, val_MinusLogProbMetric: 16.8876

Epoch 572: val_loss did not improve from 16.83441
196/196 - 34s - loss: 16.3550 - MinusLogProbMetric: 16.3550 - val_loss: 16.8876 - val_MinusLogProbMetric: 16.8876 - lr: 1.2500e-04 - 34s/epoch - 175ms/step
Epoch 573/1000
2023-09-26 18:49:20.451 
Epoch 573/1000 
	 loss: 16.3553, MinusLogProbMetric: 16.3553, val_loss: 16.8522, val_MinusLogProbMetric: 16.8522

Epoch 573: val_loss did not improve from 16.83441
196/196 - 34s - loss: 16.3553 - MinusLogProbMetric: 16.3553 - val_loss: 16.8522 - val_MinusLogProbMetric: 16.8522 - lr: 1.2500e-04 - 34s/epoch - 174ms/step
Epoch 574/1000
2023-09-26 18:49:54.490 
Epoch 574/1000 
	 loss: 16.3363, MinusLogProbMetric: 16.3363, val_loss: 16.8400, val_MinusLogProbMetric: 16.8400

Epoch 574: val_loss did not improve from 16.83441
196/196 - 34s - loss: 16.3363 - MinusLogProbMetric: 16.3363 - val_loss: 16.8400 - val_MinusLogProbMetric: 16.8400 - lr: 6.2500e-05 - 34s/epoch - 174ms/step
Epoch 575/1000
2023-09-26 18:50:28.707 
Epoch 575/1000 
	 loss: 16.3385, MinusLogProbMetric: 16.3385, val_loss: 16.8468, val_MinusLogProbMetric: 16.8468

Epoch 575: val_loss did not improve from 16.83441
196/196 - 34s - loss: 16.3385 - MinusLogProbMetric: 16.3385 - val_loss: 16.8468 - val_MinusLogProbMetric: 16.8468 - lr: 6.2500e-05 - 34s/epoch - 175ms/step
Epoch 576/1000
2023-09-26 18:51:02.876 
Epoch 576/1000 
	 loss: 16.3350, MinusLogProbMetric: 16.3350, val_loss: 16.8433, val_MinusLogProbMetric: 16.8433

Epoch 576: val_loss did not improve from 16.83441
196/196 - 34s - loss: 16.3350 - MinusLogProbMetric: 16.3350 - val_loss: 16.8433 - val_MinusLogProbMetric: 16.8433 - lr: 6.2500e-05 - 34s/epoch - 174ms/step
Epoch 577/1000
2023-09-26 18:51:37.181 
Epoch 577/1000 
	 loss: 16.3366, MinusLogProbMetric: 16.3366, val_loss: 16.8461, val_MinusLogProbMetric: 16.8461

Epoch 577: val_loss did not improve from 16.83441
196/196 - 34s - loss: 16.3366 - MinusLogProbMetric: 16.3366 - val_loss: 16.8461 - val_MinusLogProbMetric: 16.8461 - lr: 6.2500e-05 - 34s/epoch - 175ms/step
Epoch 578/1000
2023-09-26 18:52:11.347 
Epoch 578/1000 
	 loss: 16.3349, MinusLogProbMetric: 16.3349, val_loss: 16.8386, val_MinusLogProbMetric: 16.8386

Epoch 578: val_loss did not improve from 16.83441
196/196 - 34s - loss: 16.3349 - MinusLogProbMetric: 16.3349 - val_loss: 16.8386 - val_MinusLogProbMetric: 16.8386 - lr: 6.2500e-05 - 34s/epoch - 174ms/step
Epoch 579/1000
2023-09-26 18:52:45.450 
Epoch 579/1000 
	 loss: 16.3372, MinusLogProbMetric: 16.3372, val_loss: 16.8396, val_MinusLogProbMetric: 16.8396

Epoch 579: val_loss did not improve from 16.83441
196/196 - 34s - loss: 16.3372 - MinusLogProbMetric: 16.3372 - val_loss: 16.8396 - val_MinusLogProbMetric: 16.8396 - lr: 6.2500e-05 - 34s/epoch - 174ms/step
Epoch 580/1000
2023-09-26 18:53:19.455 
Epoch 580/1000 
	 loss: 16.3367, MinusLogProbMetric: 16.3367, val_loss: 16.8434, val_MinusLogProbMetric: 16.8434

Epoch 580: val_loss did not improve from 16.83441
196/196 - 34s - loss: 16.3367 - MinusLogProbMetric: 16.3367 - val_loss: 16.8434 - val_MinusLogProbMetric: 16.8434 - lr: 6.2500e-05 - 34s/epoch - 173ms/step
Epoch 581/1000
2023-09-26 18:53:53.293 
Epoch 581/1000 
	 loss: 16.3369, MinusLogProbMetric: 16.3369, val_loss: 16.8398, val_MinusLogProbMetric: 16.8398

Epoch 581: val_loss did not improve from 16.83441
196/196 - 34s - loss: 16.3369 - MinusLogProbMetric: 16.3369 - val_loss: 16.8398 - val_MinusLogProbMetric: 16.8398 - lr: 6.2500e-05 - 34s/epoch - 173ms/step
Epoch 582/1000
2023-09-26 18:54:27.163 
Epoch 582/1000 
	 loss: 16.3361, MinusLogProbMetric: 16.3361, val_loss: 16.8393, val_MinusLogProbMetric: 16.8393

Epoch 582: val_loss did not improve from 16.83441
196/196 - 34s - loss: 16.3361 - MinusLogProbMetric: 16.3361 - val_loss: 16.8393 - val_MinusLogProbMetric: 16.8393 - lr: 6.2500e-05 - 34s/epoch - 173ms/step
Epoch 583/1000
2023-09-26 18:55:01.184 
Epoch 583/1000 
	 loss: 16.3348, MinusLogProbMetric: 16.3348, val_loss: 16.8371, val_MinusLogProbMetric: 16.8371

Epoch 583: val_loss did not improve from 16.83441
196/196 - 34s - loss: 16.3348 - MinusLogProbMetric: 16.3348 - val_loss: 16.8371 - val_MinusLogProbMetric: 16.8371 - lr: 6.2500e-05 - 34s/epoch - 174ms/step
Epoch 584/1000
2023-09-26 18:55:35.145 
Epoch 584/1000 
	 loss: 16.3379, MinusLogProbMetric: 16.3379, val_loss: 16.8453, val_MinusLogProbMetric: 16.8453

Epoch 584: val_loss did not improve from 16.83441
196/196 - 34s - loss: 16.3379 - MinusLogProbMetric: 16.3379 - val_loss: 16.8453 - val_MinusLogProbMetric: 16.8453 - lr: 6.2500e-05 - 34s/epoch - 173ms/step
Epoch 585/1000
2023-09-26 18:56:09.326 
Epoch 585/1000 
	 loss: 16.3378, MinusLogProbMetric: 16.3378, val_loss: 16.8672, val_MinusLogProbMetric: 16.8672

Epoch 585: val_loss did not improve from 16.83441
196/196 - 34s - loss: 16.3378 - MinusLogProbMetric: 16.3378 - val_loss: 16.8672 - val_MinusLogProbMetric: 16.8672 - lr: 6.2500e-05 - 34s/epoch - 174ms/step
Epoch 586/1000
2023-09-26 18:56:43.192 
Epoch 586/1000 
	 loss: 16.3384, MinusLogProbMetric: 16.3384, val_loss: 16.8353, val_MinusLogProbMetric: 16.8353

Epoch 586: val_loss did not improve from 16.83441
196/196 - 34s - loss: 16.3384 - MinusLogProbMetric: 16.3384 - val_loss: 16.8353 - val_MinusLogProbMetric: 16.8353 - lr: 6.2500e-05 - 34s/epoch - 173ms/step
Epoch 587/1000
2023-09-26 18:57:17.301 
Epoch 587/1000 
	 loss: 16.3380, MinusLogProbMetric: 16.3380, val_loss: 16.8415, val_MinusLogProbMetric: 16.8415

Epoch 587: val_loss did not improve from 16.83441
196/196 - 34s - loss: 16.3380 - MinusLogProbMetric: 16.3380 - val_loss: 16.8415 - val_MinusLogProbMetric: 16.8415 - lr: 6.2500e-05 - 34s/epoch - 174ms/step
Epoch 588/1000
2023-09-26 18:57:51.194 
Epoch 588/1000 
	 loss: 16.3356, MinusLogProbMetric: 16.3356, val_loss: 16.8519, val_MinusLogProbMetric: 16.8519

Epoch 588: val_loss did not improve from 16.83441
196/196 - 34s - loss: 16.3356 - MinusLogProbMetric: 16.3356 - val_loss: 16.8519 - val_MinusLogProbMetric: 16.8519 - lr: 6.2500e-05 - 34s/epoch - 173ms/step
Epoch 589/1000
2023-09-26 18:58:24.942 
Epoch 589/1000 
	 loss: 16.3342, MinusLogProbMetric: 16.3342, val_loss: 16.8385, val_MinusLogProbMetric: 16.8385

Epoch 589: val_loss did not improve from 16.83441
196/196 - 34s - loss: 16.3342 - MinusLogProbMetric: 16.3342 - val_loss: 16.8385 - val_MinusLogProbMetric: 16.8385 - lr: 6.2500e-05 - 34s/epoch - 172ms/step
Epoch 590/1000
2023-09-26 18:58:58.756 
Epoch 590/1000 
	 loss: 16.3359, MinusLogProbMetric: 16.3359, val_loss: 16.8491, val_MinusLogProbMetric: 16.8491

Epoch 590: val_loss did not improve from 16.83441
196/196 - 34s - loss: 16.3359 - MinusLogProbMetric: 16.3359 - val_loss: 16.8491 - val_MinusLogProbMetric: 16.8491 - lr: 6.2500e-05 - 34s/epoch - 173ms/step
Epoch 591/1000
2023-09-26 18:59:32.743 
Epoch 591/1000 
	 loss: 16.3365, MinusLogProbMetric: 16.3365, val_loss: 16.8378, val_MinusLogProbMetric: 16.8378

Epoch 591: val_loss did not improve from 16.83441
196/196 - 34s - loss: 16.3365 - MinusLogProbMetric: 16.3365 - val_loss: 16.8378 - val_MinusLogProbMetric: 16.8378 - lr: 6.2500e-05 - 34s/epoch - 173ms/step
Epoch 592/1000
2023-09-26 19:00:06.782 
Epoch 592/1000 
	 loss: 16.3363, MinusLogProbMetric: 16.3363, val_loss: 16.8463, val_MinusLogProbMetric: 16.8463

Epoch 592: val_loss did not improve from 16.83441
196/196 - 34s - loss: 16.3363 - MinusLogProbMetric: 16.3363 - val_loss: 16.8463 - val_MinusLogProbMetric: 16.8463 - lr: 6.2500e-05 - 34s/epoch - 174ms/step
Epoch 593/1000
2023-09-26 19:00:40.431 
Epoch 593/1000 
	 loss: 16.3357, MinusLogProbMetric: 16.3357, val_loss: 16.8419, val_MinusLogProbMetric: 16.8419

Epoch 593: val_loss did not improve from 16.83441
196/196 - 34s - loss: 16.3357 - MinusLogProbMetric: 16.3357 - val_loss: 16.8419 - val_MinusLogProbMetric: 16.8419 - lr: 6.2500e-05 - 34s/epoch - 172ms/step
Epoch 594/1000
2023-09-26 19:01:14.466 
Epoch 594/1000 
	 loss: 16.3347, MinusLogProbMetric: 16.3347, val_loss: 16.8458, val_MinusLogProbMetric: 16.8458

Epoch 594: val_loss did not improve from 16.83441
196/196 - 34s - loss: 16.3347 - MinusLogProbMetric: 16.3347 - val_loss: 16.8458 - val_MinusLogProbMetric: 16.8458 - lr: 6.2500e-05 - 34s/epoch - 174ms/step
Epoch 595/1000
2023-09-26 19:01:47.846 
Epoch 595/1000 
	 loss: 16.3386, MinusLogProbMetric: 16.3386, val_loss: 16.8394, val_MinusLogProbMetric: 16.8394

Epoch 595: val_loss did not improve from 16.83441
196/196 - 33s - loss: 16.3386 - MinusLogProbMetric: 16.3386 - val_loss: 16.8394 - val_MinusLogProbMetric: 16.8394 - lr: 6.2500e-05 - 33s/epoch - 170ms/step
Epoch 596/1000
2023-09-26 19:02:21.666 
Epoch 596/1000 
	 loss: 16.3355, MinusLogProbMetric: 16.3355, val_loss: 16.8474, val_MinusLogProbMetric: 16.8474

Epoch 596: val_loss did not improve from 16.83441
196/196 - 34s - loss: 16.3355 - MinusLogProbMetric: 16.3355 - val_loss: 16.8474 - val_MinusLogProbMetric: 16.8474 - lr: 6.2500e-05 - 34s/epoch - 173ms/step
Epoch 597/1000
2023-09-26 19:02:55.547 
Epoch 597/1000 
	 loss: 16.3368, MinusLogProbMetric: 16.3368, val_loss: 16.8399, val_MinusLogProbMetric: 16.8399

Epoch 597: val_loss did not improve from 16.83441
196/196 - 34s - loss: 16.3368 - MinusLogProbMetric: 16.3368 - val_loss: 16.8399 - val_MinusLogProbMetric: 16.8399 - lr: 6.2500e-05 - 34s/epoch - 173ms/step
Epoch 598/1000
2023-09-26 19:03:29.886 
Epoch 598/1000 
	 loss: 16.3350, MinusLogProbMetric: 16.3350, val_loss: 16.8499, val_MinusLogProbMetric: 16.8499

Epoch 598: val_loss did not improve from 16.83441
196/196 - 34s - loss: 16.3350 - MinusLogProbMetric: 16.3350 - val_loss: 16.8499 - val_MinusLogProbMetric: 16.8499 - lr: 6.2500e-05 - 34s/epoch - 175ms/step
Epoch 599/1000
2023-09-26 19:04:03.732 
Epoch 599/1000 
	 loss: 16.3365, MinusLogProbMetric: 16.3365, val_loss: 16.8373, val_MinusLogProbMetric: 16.8373

Epoch 599: val_loss did not improve from 16.83441
196/196 - 34s - loss: 16.3365 - MinusLogProbMetric: 16.3365 - val_loss: 16.8373 - val_MinusLogProbMetric: 16.8373 - lr: 6.2500e-05 - 34s/epoch - 173ms/step
Epoch 600/1000
2023-09-26 19:04:37.544 
Epoch 600/1000 
	 loss: 16.3342, MinusLogProbMetric: 16.3342, val_loss: 16.8460, val_MinusLogProbMetric: 16.8460

Epoch 600: val_loss did not improve from 16.83441
196/196 - 34s - loss: 16.3342 - MinusLogProbMetric: 16.3342 - val_loss: 16.8460 - val_MinusLogProbMetric: 16.8460 - lr: 6.2500e-05 - 34s/epoch - 172ms/step
Epoch 601/1000
2023-09-26 19:05:11.578 
Epoch 601/1000 
	 loss: 16.3363, MinusLogProbMetric: 16.3363, val_loss: 16.8392, val_MinusLogProbMetric: 16.8392

Epoch 601: val_loss did not improve from 16.83441
196/196 - 34s - loss: 16.3363 - MinusLogProbMetric: 16.3363 - val_loss: 16.8392 - val_MinusLogProbMetric: 16.8392 - lr: 6.2500e-05 - 34s/epoch - 174ms/step
Epoch 602/1000
2023-09-26 19:05:45.477 
Epoch 602/1000 
	 loss: 16.3351, MinusLogProbMetric: 16.3351, val_loss: 16.8448, val_MinusLogProbMetric: 16.8448

Epoch 602: val_loss did not improve from 16.83441
196/196 - 34s - loss: 16.3351 - MinusLogProbMetric: 16.3351 - val_loss: 16.8448 - val_MinusLogProbMetric: 16.8448 - lr: 6.2500e-05 - 34s/epoch - 173ms/step
Epoch 603/1000
2023-09-26 19:06:19.381 
Epoch 603/1000 
	 loss: 16.3342, MinusLogProbMetric: 16.3342, val_loss: 16.8478, val_MinusLogProbMetric: 16.8478

Epoch 603: val_loss did not improve from 16.83441
196/196 - 34s - loss: 16.3342 - MinusLogProbMetric: 16.3342 - val_loss: 16.8478 - val_MinusLogProbMetric: 16.8478 - lr: 6.2500e-05 - 34s/epoch - 173ms/step
Epoch 604/1000
2023-09-26 19:06:53.426 
Epoch 604/1000 
	 loss: 16.3338, MinusLogProbMetric: 16.3338, val_loss: 16.8418, val_MinusLogProbMetric: 16.8418

Epoch 604: val_loss did not improve from 16.83441
196/196 - 34s - loss: 16.3338 - MinusLogProbMetric: 16.3338 - val_loss: 16.8418 - val_MinusLogProbMetric: 16.8418 - lr: 6.2500e-05 - 34s/epoch - 174ms/step
Epoch 605/1000
2023-09-26 19:07:27.267 
Epoch 605/1000 
	 loss: 16.3366, MinusLogProbMetric: 16.3366, val_loss: 16.8462, val_MinusLogProbMetric: 16.8462

Epoch 605: val_loss did not improve from 16.83441
196/196 - 34s - loss: 16.3366 - MinusLogProbMetric: 16.3366 - val_loss: 16.8462 - val_MinusLogProbMetric: 16.8462 - lr: 6.2500e-05 - 34s/epoch - 173ms/step
Epoch 606/1000
2023-09-26 19:08:01.216 
Epoch 606/1000 
	 loss: 16.3370, MinusLogProbMetric: 16.3370, val_loss: 16.8571, val_MinusLogProbMetric: 16.8571

Epoch 606: val_loss did not improve from 16.83441
196/196 - 34s - loss: 16.3370 - MinusLogProbMetric: 16.3370 - val_loss: 16.8571 - val_MinusLogProbMetric: 16.8571 - lr: 6.2500e-05 - 34s/epoch - 173ms/step
Epoch 607/1000
2023-09-26 19:08:35.128 
Epoch 607/1000 
	 loss: 16.3379, MinusLogProbMetric: 16.3379, val_loss: 16.8394, val_MinusLogProbMetric: 16.8394

Epoch 607: val_loss did not improve from 16.83441
196/196 - 34s - loss: 16.3379 - MinusLogProbMetric: 16.3379 - val_loss: 16.8394 - val_MinusLogProbMetric: 16.8394 - lr: 6.2500e-05 - 34s/epoch - 173ms/step
Epoch 608/1000
2023-09-26 19:09:08.849 
Epoch 608/1000 
	 loss: 16.3340, MinusLogProbMetric: 16.3340, val_loss: 16.8448, val_MinusLogProbMetric: 16.8448

Epoch 608: val_loss did not improve from 16.83441
196/196 - 34s - loss: 16.3340 - MinusLogProbMetric: 16.3340 - val_loss: 16.8448 - val_MinusLogProbMetric: 16.8448 - lr: 6.2500e-05 - 34s/epoch - 172ms/step
Epoch 609/1000
2023-09-26 19:09:42.864 
Epoch 609/1000 
	 loss: 16.3350, MinusLogProbMetric: 16.3350, val_loss: 16.8393, val_MinusLogProbMetric: 16.8393

Epoch 609: val_loss did not improve from 16.83441
196/196 - 34s - loss: 16.3350 - MinusLogProbMetric: 16.3350 - val_loss: 16.8393 - val_MinusLogProbMetric: 16.8393 - lr: 6.2500e-05 - 34s/epoch - 174ms/step
Epoch 610/1000
2023-09-26 19:10:17.070 
Epoch 610/1000 
	 loss: 16.3354, MinusLogProbMetric: 16.3354, val_loss: 16.8403, val_MinusLogProbMetric: 16.8403

Epoch 610: val_loss did not improve from 16.83441
196/196 - 34s - loss: 16.3354 - MinusLogProbMetric: 16.3354 - val_loss: 16.8403 - val_MinusLogProbMetric: 16.8403 - lr: 6.2500e-05 - 34s/epoch - 175ms/step
Epoch 611/1000
2023-09-26 19:10:50.960 
Epoch 611/1000 
	 loss: 16.3338, MinusLogProbMetric: 16.3338, val_loss: 16.8431, val_MinusLogProbMetric: 16.8431

Epoch 611: val_loss did not improve from 16.83441
196/196 - 34s - loss: 16.3338 - MinusLogProbMetric: 16.3338 - val_loss: 16.8431 - val_MinusLogProbMetric: 16.8431 - lr: 6.2500e-05 - 34s/epoch - 173ms/step
Epoch 612/1000
2023-09-26 19:11:24.791 
Epoch 612/1000 
	 loss: 16.3351, MinusLogProbMetric: 16.3351, val_loss: 16.8404, val_MinusLogProbMetric: 16.8404

Epoch 612: val_loss did not improve from 16.83441
196/196 - 34s - loss: 16.3351 - MinusLogProbMetric: 16.3351 - val_loss: 16.8404 - val_MinusLogProbMetric: 16.8404 - lr: 6.2500e-05 - 34s/epoch - 173ms/step
Epoch 613/1000
2023-09-26 19:11:58.541 
Epoch 613/1000 
	 loss: 16.3332, MinusLogProbMetric: 16.3332, val_loss: 16.8390, val_MinusLogProbMetric: 16.8390

Epoch 613: val_loss did not improve from 16.83441
196/196 - 34s - loss: 16.3332 - MinusLogProbMetric: 16.3332 - val_loss: 16.8390 - val_MinusLogProbMetric: 16.8390 - lr: 6.2500e-05 - 34s/epoch - 172ms/step
Epoch 614/1000
2023-09-26 19:12:32.314 
Epoch 614/1000 
	 loss: 16.3341, MinusLogProbMetric: 16.3341, val_loss: 16.8363, val_MinusLogProbMetric: 16.8363

Epoch 614: val_loss did not improve from 16.83441
196/196 - 34s - loss: 16.3341 - MinusLogProbMetric: 16.3341 - val_loss: 16.8363 - val_MinusLogProbMetric: 16.8363 - lr: 6.2500e-05 - 34s/epoch - 172ms/step
Epoch 615/1000
2023-09-26 19:13:06.384 
Epoch 615/1000 
	 loss: 16.3368, MinusLogProbMetric: 16.3368, val_loss: 16.8382, val_MinusLogProbMetric: 16.8382

Epoch 615: val_loss did not improve from 16.83441
196/196 - 34s - loss: 16.3368 - MinusLogProbMetric: 16.3368 - val_loss: 16.8382 - val_MinusLogProbMetric: 16.8382 - lr: 6.2500e-05 - 34s/epoch - 174ms/step
Epoch 616/1000
2023-09-26 19:13:40.378 
Epoch 616/1000 
	 loss: 16.3358, MinusLogProbMetric: 16.3358, val_loss: 16.8534, val_MinusLogProbMetric: 16.8534

Epoch 616: val_loss did not improve from 16.83441
196/196 - 34s - loss: 16.3358 - MinusLogProbMetric: 16.3358 - val_loss: 16.8534 - val_MinusLogProbMetric: 16.8534 - lr: 6.2500e-05 - 34s/epoch - 173ms/step
Epoch 617/1000
2023-09-26 19:14:14.561 
Epoch 617/1000 
	 loss: 16.3340, MinusLogProbMetric: 16.3340, val_loss: 16.8420, val_MinusLogProbMetric: 16.8420

Epoch 617: val_loss did not improve from 16.83441
196/196 - 34s - loss: 16.3340 - MinusLogProbMetric: 16.3340 - val_loss: 16.8420 - val_MinusLogProbMetric: 16.8420 - lr: 6.2500e-05 - 34s/epoch - 174ms/step
Epoch 618/1000
2023-09-26 19:14:48.766 
Epoch 618/1000 
	 loss: 16.3353, MinusLogProbMetric: 16.3353, val_loss: 16.8482, val_MinusLogProbMetric: 16.8482

Epoch 618: val_loss did not improve from 16.83441
196/196 - 34s - loss: 16.3353 - MinusLogProbMetric: 16.3353 - val_loss: 16.8482 - val_MinusLogProbMetric: 16.8482 - lr: 6.2500e-05 - 34s/epoch - 174ms/step
Epoch 619/1000
2023-09-26 19:15:22.807 
Epoch 619/1000 
	 loss: 16.3351, MinusLogProbMetric: 16.3351, val_loss: 16.8471, val_MinusLogProbMetric: 16.8471

Epoch 619: val_loss did not improve from 16.83441
196/196 - 34s - loss: 16.3351 - MinusLogProbMetric: 16.3351 - val_loss: 16.8471 - val_MinusLogProbMetric: 16.8471 - lr: 6.2500e-05 - 34s/epoch - 174ms/step
Epoch 620/1000
2023-09-26 19:15:56.802 
Epoch 620/1000 
	 loss: 16.3340, MinusLogProbMetric: 16.3340, val_loss: 16.8560, val_MinusLogProbMetric: 16.8560

Epoch 620: val_loss did not improve from 16.83441
196/196 - 34s - loss: 16.3340 - MinusLogProbMetric: 16.3340 - val_loss: 16.8560 - val_MinusLogProbMetric: 16.8560 - lr: 6.2500e-05 - 34s/epoch - 173ms/step
Epoch 621/1000
2023-09-26 19:16:31.037 
Epoch 621/1000 
	 loss: 16.3347, MinusLogProbMetric: 16.3347, val_loss: 16.8428, val_MinusLogProbMetric: 16.8428

Epoch 621: val_loss did not improve from 16.83441
196/196 - 34s - loss: 16.3347 - MinusLogProbMetric: 16.3347 - val_loss: 16.8428 - val_MinusLogProbMetric: 16.8428 - lr: 6.2500e-05 - 34s/epoch - 175ms/step
Epoch 622/1000
2023-09-26 19:17:04.902 
Epoch 622/1000 
	 loss: 16.3325, MinusLogProbMetric: 16.3325, val_loss: 16.8511, val_MinusLogProbMetric: 16.8511

Epoch 622: val_loss did not improve from 16.83441
196/196 - 34s - loss: 16.3325 - MinusLogProbMetric: 16.3325 - val_loss: 16.8511 - val_MinusLogProbMetric: 16.8511 - lr: 6.2500e-05 - 34s/epoch - 173ms/step
Epoch 623/1000
2023-09-26 19:17:38.789 
Epoch 623/1000 
	 loss: 16.3351, MinusLogProbMetric: 16.3351, val_loss: 16.8511, val_MinusLogProbMetric: 16.8511

Epoch 623: val_loss did not improve from 16.83441
Restoring model weights from the end of the best epoch: 523.
196/196 - 34s - loss: 16.3351 - MinusLogProbMetric: 16.3351 - val_loss: 16.8511 - val_MinusLogProbMetric: 16.8511 - lr: 6.2500e-05 - 34s/epoch - 175ms/step
Epoch 623: early stopping
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.

------------------------------------------
Starting LR metric calculation...
Running TF LR calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
LR metric calculation completed in 12.87771980999969 seconds.

------------------------------------------
Starting KS tests calculation...
Running TF KS tests...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
KS tests calculation completed in 9.61191138898721 seconds.

------------------------------------------
Starting SWD metric calculation...
Running TF SWD calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
SWD metric calculation completed in 7.242469129967503 seconds.

------------------------------------------
Starting FN metric calculation...
Running TF FN calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
FN metric calculation completed in 7.220432296977378 seconds.
Training succeeded with seed 721.
Model trained in 21234.62 s.

===========
Computing predictions
===========

Computing metrics...
Metrics computed in 38.32 s.
                ===========
                print("===========
Failed to plot
===========
")
                Exception type: ValueError
                Exception message: Number of rows must be a positive integer, not 0
                Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN.py , Line : 469, Func.Name : prediction_function, Message : Plotters.cornerplotter(X_data_test.numpy(),X_data_nf.numpy(),path_to_results,ndims,norm=True) # type: ignore', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/Plotters.py , Line : 156, Func.Name : cornerplotter, Message : figure=corner.corner(target_samples,color=\'red\',bins=n_bins,labels=[r"%s" % s for s in labels],normalize1d=True)', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/corner.py , Line : 235, Func.Name : corner, Message : fig, axes = pl.subplots(K, K, figsize=(dim, dim))', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/pyplot.py , Line : 1502, Func.Name : subplots, Message : axs = fig.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 905, Func.Name : subplots, Message : gs = self.add_gridspec(nrows, ncols, figure=self, **gridspec_kw)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 1527, Func.Name : add_gridspec, Message : gs = GridSpec(nrows=nrows, ncols=ncols, figure=self, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 379, Func.Name : __init__, Message : super().__init__(nrows, ncols,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 49, Func.Name : __init__, Message : raise ValueError(']
                ===========

results.txt saved
results.json saved
Results log saved
Model predictions computed in 38.85 s.
===========
Run 289/720 done in 21276.44 s.
===========

Directory ../../results/CsplineN_new/run_290/ already exists.
Skipping it.
===========
Run 290/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_291/ already exists.
Skipping it.
===========
Run 291/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_292/ already exists.
Skipping it.
===========
Run 292/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_293/ already exists.
Skipping it.
===========
Run 293/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_294/ already exists.
Skipping it.
===========
Run 294/720 already exists. Skipping it.
===========

===========
Generating train data for run 295.
===========
Train data generated in 0.24 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_295/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 721}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_295/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.3570018 ,  5.6703625 ,  0.48762453, ...,  0.76730525,
         6.4537516 ,  1.4289263 ],
       [ 4.8933635 ,  4.954244  , -0.7742728 , ...,  1.033045  ,
         7.5179    ,  1.3446053 ],
       [ 2.6192126 ,  3.6659784 ,  9.3581505 , ...,  6.4096394 ,
         2.8983781 ,  1.8163822 ],
       ...,
       [ 4.4286733 ,  5.582822  , -0.08033401, ...,  1.0755457 ,
         6.482341  ,  1.3259909 ],
       [ 5.2667904 ,  5.78094   ,  0.52473783, ...,  1.3584392 ,
         6.6371064 ,  1.4527634 ],
       [ 1.3711529 ,  3.985786  ,  8.434673  , ...,  6.650886  ,
         2.8764064 ,  2.071397  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_295/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_295
self.data_kwargs: {'seed': 721}
self.x_data: [[ 5.4479074  7.1622143  6.6614265 ...  3.6775346  2.6042771  7.6607866]
 [ 1.558796   3.7643342  7.765504  ...  7.3985186  2.2847533  2.1737251]
 [ 2.023865   3.5498223  9.736043  ...  7.378399   2.9893079  1.7681804]
 ...
 [ 3.157689   3.8140697  9.128405  ...  7.3280826  3.3431854  1.6145309]
 [ 4.9035525  5.63367    1.3061054 ...  1.7336241  7.263643   1.3624198]
 [ 4.873677   6.0271535 -0.4844559 ...  1.4305246  6.6029577  1.4994687]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_16"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_17 (InputLayer)       [(None, 32)]              0         
                                                                 
 log_prob_layer_1 (LogProbLa  (None,)                  1074400   
 yer)                                                            
                                                                 
=================================================================
Total params: 1,074,400
Trainable params: 1,074,400
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_1/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_1'")
self.model: <keras.engine.functional.Functional object at 0x7f7e3068c940>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f7de838dcf0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f7de838dcf0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f7e40184970>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f7de825fee0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f7de82a8490>, <keras.callbacks.ModelCheckpoint object at 0x7f7de82a8550>, <keras.callbacks.EarlyStopping object at 0x7f7de82a87c0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f7de82a87f0>, <keras.callbacks.TerminateOnNaN object at 0x7f7de82a8430>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.3570018 ,  5.6703625 ,  0.48762453, ...,  0.76730525,
         6.4537516 ,  1.4289263 ],
       [ 4.8933635 ,  4.954244  , -0.7742728 , ...,  1.033045  ,
         7.5179    ,  1.3446053 ],
       [ 2.6192126 ,  3.6659784 ,  9.3581505 , ...,  6.4096394 ,
         2.8983781 ,  1.8163822 ],
       ...,
       [ 4.4286733 ,  5.582822  , -0.08033401, ...,  1.0755457 ,
         6.482341  ,  1.3259909 ],
       [ 5.2667904 ,  5.78094   ,  0.52473783, ...,  1.3584392 ,
         6.6371064 ,  1.4527634 ],
       [ 1.3711529 ,  3.985786  ,  8.434673  , ...,  6.650886  ,
         2.8764064 ,  2.071397  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_295/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 295/720 with hyperparameters:
timestamp = 2023-09-26 19:18:29.485434
ndims = 32
seed_train = 721
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1074400
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 5.4479074  7.1622143  6.6614265  5.4466543  4.679343   6.5534453
  4.5869303  8.297877   9.431402   3.191049   9.163281   4.112924
  5.872972  10.008474   0.4502598  1.3920732 -0.6747763  8.487896
  9.224454   7.913118   8.449968   7.6997313  5.9028535  7.439646
  2.3174791  6.975212   1.6619607  9.750696   4.816901   3.6775346
  2.6042771  7.6607866]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 23: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-26 19:21:22.045 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 2937.1819, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 172s - loss: nan - MinusLogProbMetric: 2937.1819 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 172s/epoch - 880ms/step
The loss history contains NaN values.
Training failed: trying again with seed 821433 and lr 0.0003333333333333333.
===========
Generating train data for run 295.
===========
Train data generated in 0.22 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_295/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 721}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_295/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.3570018 ,  5.6703625 ,  0.48762453, ...,  0.76730525,
         6.4537516 ,  1.4289263 ],
       [ 4.8933635 ,  4.954244  , -0.7742728 , ...,  1.033045  ,
         7.5179    ,  1.3446053 ],
       [ 2.6192126 ,  3.6659784 ,  9.3581505 , ...,  6.4096394 ,
         2.8983781 ,  1.8163822 ],
       ...,
       [ 4.4286733 ,  5.582822  , -0.08033401, ...,  1.0755457 ,
         6.482341  ,  1.3259909 ],
       [ 5.2667904 ,  5.78094   ,  0.52473783, ...,  1.3584392 ,
         6.6371064 ,  1.4527634 ],
       [ 1.3711529 ,  3.985786  ,  8.434673  , ...,  6.650886  ,
         2.8764064 ,  2.071397  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_295/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_295
self.data_kwargs: {'seed': 721}
self.x_data: [[ 5.4479074  7.1622143  6.6614265 ...  3.6775346  2.6042771  7.6607866]
 [ 1.558796   3.7643342  7.765504  ...  7.3985186  2.2847533  2.1737251]
 [ 2.023865   3.5498223  9.736043  ...  7.378399   2.9893079  1.7681804]
 ...
 [ 3.157689   3.8140697  9.128405  ...  7.3280826  3.3431854  1.6145309]
 [ 4.9035525  5.63367    1.3061054 ...  1.7336241  7.263643   1.3624198]
 [ 4.873677   6.0271535 -0.4844559 ...  1.4305246  6.6029577  1.4994687]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_27"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_28 (InputLayer)       [(None, 32)]              0         
                                                                 
 log_prob_layer_2 (LogProbLa  (None,)                  1074400   
 yer)                                                            
                                                                 
=================================================================
Total params: 1,074,400
Trainable params: 1,074,400
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_2/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_2'")
self.model: <keras.engine.functional.Functional object at 0x7f7d292da6e0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f7d28a8a200>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f7d28a8a200>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f7d28aa8910>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f7d28aa9bd0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f7d28aaa140>, <keras.callbacks.ModelCheckpoint object at 0x7f7d28aaa200>, <keras.callbacks.EarlyStopping object at 0x7f7d28aaa470>, <keras.callbacks.ReduceLROnPlateau object at 0x7f7d28aaa4a0>, <keras.callbacks.TerminateOnNaN object at 0x7f7d28aaa0e0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.3570018 ,  5.6703625 ,  0.48762453, ...,  0.76730525,
         6.4537516 ,  1.4289263 ],
       [ 4.8933635 ,  4.954244  , -0.7742728 , ...,  1.033045  ,
         7.5179    ,  1.3446053 ],
       [ 2.6192126 ,  3.6659784 ,  9.3581505 , ...,  6.4096394 ,
         2.8983781 ,  1.8163822 ],
       ...,
       [ 4.4286733 ,  5.582822  , -0.08033401, ...,  1.0755457 ,
         6.482341  ,  1.3259909 ],
       [ 5.2667904 ,  5.78094   ,  0.52473783, ...,  1.3584392 ,
         6.6371064 ,  1.4527634 ],
       [ 1.3711529 ,  3.985786  ,  8.434673  , ...,  6.650886  ,
         2.8764064 ,  2.071397  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_295/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 295/720 with hyperparameters:
timestamp = 2023-09-26 19:21:36.033164
ndims = 32
seed_train = 721
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1074400
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 5.4479074  7.1622143  6.6614265  5.4466543  4.679343   6.5534453
  4.5869303  8.297877   9.431402   3.191049   9.163281   4.112924
  5.872972  10.008474   0.4502598  1.3920732 -0.6747763  8.487896
  9.224454   7.913118   8.449968   7.6997313  5.9028535  7.439646
  2.3174791  6.975212   1.6619607  9.750696   4.816901   3.6775346
  2.6042771  7.6607866]
Epoch 1/1000
2023-09-26 19:25:39.120 
Epoch 1/1000 
	 loss: 1175.5356, MinusLogProbMetric: 1175.5356, val_loss: 787.1153, val_MinusLogProbMetric: 787.1153

Epoch 1: val_loss improved from inf to 787.11530, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 244s - loss: 1175.5356 - MinusLogProbMetric: 1175.5356 - val_loss: 787.1153 - val_MinusLogProbMetric: 787.1153 - lr: 3.3333e-04 - 244s/epoch - 1s/step
Epoch 2/1000
2023-09-26 19:27:00.772 
Epoch 2/1000 
	 loss: 502.8647, MinusLogProbMetric: 502.8647, val_loss: 437.8643, val_MinusLogProbMetric: 437.8643

Epoch 2: val_loss improved from 787.11530 to 437.86426, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 81s - loss: 502.8647 - MinusLogProbMetric: 502.8647 - val_loss: 437.8643 - val_MinusLogProbMetric: 437.8643 - lr: 3.3333e-04 - 81s/epoch - 415ms/step
Epoch 3/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 71: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-26 19:27:34.813 
Epoch 3/1000 
	 loss: nan, MinusLogProbMetric: 589.2546, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 3: val_loss did not improve from 437.86426
196/196 - 33s - loss: nan - MinusLogProbMetric: 589.2546 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 33s/epoch - 167ms/step
The loss history contains NaN values.
Training failed: trying again with seed 821433 and lr 0.0001111111111111111.
===========
Generating train data for run 295.
===========
Train data generated in 0.27 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_295/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 721}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_295/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.3570018 ,  5.6703625 ,  0.48762453, ...,  0.76730525,
         6.4537516 ,  1.4289263 ],
       [ 4.8933635 ,  4.954244  , -0.7742728 , ...,  1.033045  ,
         7.5179    ,  1.3446053 ],
       [ 2.6192126 ,  3.6659784 ,  9.3581505 , ...,  6.4096394 ,
         2.8983781 ,  1.8163822 ],
       ...,
       [ 4.4286733 ,  5.582822  , -0.08033401, ...,  1.0755457 ,
         6.482341  ,  1.3259909 ],
       [ 5.2667904 ,  5.78094   ,  0.52473783, ...,  1.3584392 ,
         6.6371064 ,  1.4527634 ],
       [ 1.3711529 ,  3.985786  ,  8.434673  , ...,  6.650886  ,
         2.8764064 ,  2.071397  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_295/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_295
self.data_kwargs: {'seed': 721}
self.x_data: [[ 5.4479074  7.1622143  6.6614265 ...  3.6775346  2.6042771  7.6607866]
 [ 1.558796   3.7643342  7.765504  ...  7.3985186  2.2847533  2.1737251]
 [ 2.023865   3.5498223  9.736043  ...  7.378399   2.9893079  1.7681804]
 ...
 [ 3.157689   3.8140697  9.128405  ...  7.3280826  3.3431854  1.6145309]
 [ 4.9035525  5.63367    1.3061054 ...  1.7336241  7.263643   1.3624198]
 [ 4.873677   6.0271535 -0.4844559 ...  1.4305246  6.6029577  1.4994687]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_38"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_39 (InputLayer)       [(None, 32)]              0         
                                                                 
 log_prob_layer_3 (LogProbLa  (None,)                  1074400   
 yer)                                                            
                                                                 
=================================================================
Total params: 1,074,400
Trainable params: 1,074,400
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_3/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_3'")
self.model: <keras.engine.functional.Functional object at 0x7f7c904aa590>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f7d0861b250>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f7d0861b250>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f7c2849c190>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f7c68414cd0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f7c68415240>, <keras.callbacks.ModelCheckpoint object at 0x7f7c68415300>, <keras.callbacks.EarlyStopping object at 0x7f7c68415570>, <keras.callbacks.ReduceLROnPlateau object at 0x7f7c684155a0>, <keras.callbacks.TerminateOnNaN object at 0x7f7c684151e0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.3570018 ,  5.6703625 ,  0.48762453, ...,  0.76730525,
         6.4537516 ,  1.4289263 ],
       [ 4.8933635 ,  4.954244  , -0.7742728 , ...,  1.033045  ,
         7.5179    ,  1.3446053 ],
       [ 2.6192126 ,  3.6659784 ,  9.3581505 , ...,  6.4096394 ,
         2.8983781 ,  1.8163822 ],
       ...,
       [ 4.4286733 ,  5.582822  , -0.08033401, ...,  1.0755457 ,
         6.482341  ,  1.3259909 ],
       [ 5.2667904 ,  5.78094   ,  0.52473783, ...,  1.3584392 ,
         6.6371064 ,  1.4527634 ],
       [ 1.3711529 ,  3.985786  ,  8.434673  , ...,  6.650886  ,
         2.8764064 ,  2.071397  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 295/720 with hyperparameters:
timestamp = 2023-09-26 19:27:48.947961
ndims = 32
seed_train = 721
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1074400
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 5.4479074  7.1622143  6.6614265  5.4466543  4.679343   6.5534453
  4.5869303  8.297877   9.431402   3.191049   9.163281   4.112924
  5.872972  10.008474   0.4502598  1.3920732 -0.6747763  8.487896
  9.224454   7.913118   8.449968   7.6997313  5.9028535  7.439646
  2.3174791  6.975212   1.6619607  9.750696   4.816901   3.6775346
  2.6042771  7.6607866]
Epoch 1/1000
2023-09-26 19:31:53.013 
Epoch 1/1000 
	 loss: 453.7546, MinusLogProbMetric: 453.7546, val_loss: 395.5433, val_MinusLogProbMetric: 395.5433

Epoch 1: val_loss improved from inf to 395.54330, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 245s - loss: 453.7546 - MinusLogProbMetric: 453.7546 - val_loss: 395.5433 - val_MinusLogProbMetric: 395.5433 - lr: 1.1111e-04 - 245s/epoch - 1s/step
Epoch 2/1000
2023-09-26 19:33:14.293 
Epoch 2/1000 
	 loss: 391.1916, MinusLogProbMetric: 391.1916, val_loss: 371.7320, val_MinusLogProbMetric: 371.7320

Epoch 2: val_loss improved from 395.54330 to 371.73196, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 81s - loss: 391.1916 - MinusLogProbMetric: 391.1916 - val_loss: 371.7320 - val_MinusLogProbMetric: 371.7320 - lr: 1.1111e-04 - 81s/epoch - 413ms/step
Epoch 3/1000
2023-09-26 19:34:35.004 
Epoch 3/1000 
	 loss: 257.7143, MinusLogProbMetric: 257.7143, val_loss: 364.6262, val_MinusLogProbMetric: 364.6262

Epoch 3: val_loss improved from 371.73196 to 364.62619, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 81s - loss: 257.7143 - MinusLogProbMetric: 257.7143 - val_loss: 364.6262 - val_MinusLogProbMetric: 364.6262 - lr: 1.1111e-04 - 81s/epoch - 412ms/step
Epoch 4/1000
2023-09-26 19:35:55.600 
Epoch 4/1000 
	 loss: 188.1942, MinusLogProbMetric: 188.1942, val_loss: 175.0318, val_MinusLogProbMetric: 175.0318

Epoch 4: val_loss improved from 364.62619 to 175.03185, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 81s - loss: 188.1942 - MinusLogProbMetric: 188.1942 - val_loss: 175.0318 - val_MinusLogProbMetric: 175.0318 - lr: 1.1111e-04 - 81s/epoch - 412ms/step
Epoch 5/1000
2023-09-26 19:37:16.493 
Epoch 5/1000 
	 loss: 169.1002, MinusLogProbMetric: 169.1002, val_loss: 130.1837, val_MinusLogProbMetric: 130.1837

Epoch 5: val_loss improved from 175.03185 to 130.18372, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 81s - loss: 169.1002 - MinusLogProbMetric: 169.1002 - val_loss: 130.1837 - val_MinusLogProbMetric: 130.1837 - lr: 1.1111e-04 - 81s/epoch - 413ms/step
Epoch 6/1000
2023-09-26 19:38:37.393 
Epoch 6/1000 
	 loss: 164.3282, MinusLogProbMetric: 164.3282, val_loss: 160.4202, val_MinusLogProbMetric: 160.4202

Epoch 6: val_loss did not improve from 130.18372
196/196 - 80s - loss: 164.3282 - MinusLogProbMetric: 164.3282 - val_loss: 160.4202 - val_MinusLogProbMetric: 160.4202 - lr: 1.1111e-04 - 80s/epoch - 406ms/step
Epoch 7/1000
2023-09-26 19:39:57.093 
Epoch 7/1000 
	 loss: 146.6623, MinusLogProbMetric: 146.6623, val_loss: 136.2222, val_MinusLogProbMetric: 136.2222

Epoch 7: val_loss did not improve from 130.18372
196/196 - 80s - loss: 146.6623 - MinusLogProbMetric: 146.6623 - val_loss: 136.2222 - val_MinusLogProbMetric: 136.2222 - lr: 1.1111e-04 - 80s/epoch - 407ms/step
Epoch 8/1000
2023-09-26 19:41:16.073 
Epoch 8/1000 
	 loss: 131.9775, MinusLogProbMetric: 131.9775, val_loss: 126.0906, val_MinusLogProbMetric: 126.0906

Epoch 8: val_loss improved from 130.18372 to 126.09055, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 80s - loss: 131.9775 - MinusLogProbMetric: 131.9775 - val_loss: 126.0906 - val_MinusLogProbMetric: 126.0906 - lr: 1.1111e-04 - 80s/epoch - 410ms/step
Epoch 9/1000
2023-09-26 19:42:36.263 
Epoch 9/1000 
	 loss: 111.5909, MinusLogProbMetric: 111.5909, val_loss: 78.5812, val_MinusLogProbMetric: 78.5812

Epoch 9: val_loss improved from 126.09055 to 78.58122, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 80s - loss: 111.5909 - MinusLogProbMetric: 111.5909 - val_loss: 78.5812 - val_MinusLogProbMetric: 78.5812 - lr: 1.1111e-04 - 80s/epoch - 410ms/step
Epoch 10/1000
2023-09-26 19:43:56.732 
Epoch 10/1000 
	 loss: 71.7622, MinusLogProbMetric: 71.7622, val_loss: 65.7505, val_MinusLogProbMetric: 65.7505

Epoch 10: val_loss improved from 78.58122 to 65.75054, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 80s - loss: 71.7622 - MinusLogProbMetric: 71.7622 - val_loss: 65.7505 - val_MinusLogProbMetric: 65.7505 - lr: 1.1111e-04 - 80s/epoch - 409ms/step
Epoch 11/1000
2023-09-26 19:45:16.708 
Epoch 11/1000 
	 loss: 64.5149, MinusLogProbMetric: 64.5149, val_loss: 60.2484, val_MinusLogProbMetric: 60.2484

Epoch 11: val_loss improved from 65.75054 to 60.24842, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 80s - loss: 64.5149 - MinusLogProbMetric: 64.5149 - val_loss: 60.2484 - val_MinusLogProbMetric: 60.2484 - lr: 1.1111e-04 - 80s/epoch - 408ms/step
Epoch 12/1000
2023-09-26 19:46:37.117 
Epoch 12/1000 
	 loss: 59.1160, MinusLogProbMetric: 59.1160, val_loss: 55.9704, val_MinusLogProbMetric: 55.9704

Epoch 12: val_loss improved from 60.24842 to 55.97038, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 81s - loss: 59.1160 - MinusLogProbMetric: 59.1160 - val_loss: 55.9704 - val_MinusLogProbMetric: 55.9704 - lr: 1.1111e-04 - 81s/epoch - 411ms/step
Epoch 13/1000
2023-09-26 19:47:57.799 
Epoch 13/1000 
	 loss: 55.8097, MinusLogProbMetric: 55.8097, val_loss: 54.4303, val_MinusLogProbMetric: 54.4303

Epoch 13: val_loss improved from 55.97038 to 54.43033, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 81s - loss: 55.8097 - MinusLogProbMetric: 55.8097 - val_loss: 54.4303 - val_MinusLogProbMetric: 54.4303 - lr: 1.1111e-04 - 81s/epoch - 411ms/step
Epoch 14/1000
2023-09-26 19:49:18.307 
Epoch 14/1000 
	 loss: 52.9725, MinusLogProbMetric: 52.9725, val_loss: 50.2863, val_MinusLogProbMetric: 50.2863

Epoch 14: val_loss improved from 54.43033 to 50.28628, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 81s - loss: 52.9725 - MinusLogProbMetric: 52.9725 - val_loss: 50.2863 - val_MinusLogProbMetric: 50.2863 - lr: 1.1111e-04 - 81s/epoch - 411ms/step
Epoch 15/1000
2023-09-26 19:50:39.254 
Epoch 15/1000 
	 loss: 48.9065, MinusLogProbMetric: 48.9065, val_loss: 47.1843, val_MinusLogProbMetric: 47.1843

Epoch 15: val_loss improved from 50.28628 to 47.18429, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 81s - loss: 48.9065 - MinusLogProbMetric: 48.9065 - val_loss: 47.1843 - val_MinusLogProbMetric: 47.1843 - lr: 1.1111e-04 - 81s/epoch - 413ms/step
Epoch 16/1000
2023-09-26 19:51:59.908 
Epoch 16/1000 
	 loss: 46.8042, MinusLogProbMetric: 46.8042, val_loss: 46.9885, val_MinusLogProbMetric: 46.9885

Epoch 16: val_loss improved from 47.18429 to 46.98846, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 81s - loss: 46.8042 - MinusLogProbMetric: 46.8042 - val_loss: 46.9885 - val_MinusLogProbMetric: 46.9885 - lr: 1.1111e-04 - 81s/epoch - 412ms/step
Epoch 17/1000
2023-09-26 19:53:21.006 
Epoch 17/1000 
	 loss: 170.3429, MinusLogProbMetric: 170.3429, val_loss: 128.9480, val_MinusLogProbMetric: 128.9480

Epoch 17: val_loss did not improve from 46.98846
196/196 - 80s - loss: 170.3429 - MinusLogProbMetric: 170.3429 - val_loss: 128.9480 - val_MinusLogProbMetric: 128.9480 - lr: 1.1111e-04 - 80s/epoch - 406ms/step
Epoch 18/1000
2023-09-26 19:54:40.110 
Epoch 18/1000 
	 loss: 106.1125, MinusLogProbMetric: 106.1125, val_loss: 87.7789, val_MinusLogProbMetric: 87.7789

Epoch 18: val_loss did not improve from 46.98846
196/196 - 79s - loss: 106.1125 - MinusLogProbMetric: 106.1125 - val_loss: 87.7789 - val_MinusLogProbMetric: 87.7789 - lr: 1.1111e-04 - 79s/epoch - 404ms/step
Epoch 19/1000
2023-09-26 19:55:59.991 
Epoch 19/1000 
	 loss: 78.3463, MinusLogProbMetric: 78.3463, val_loss: 72.3106, val_MinusLogProbMetric: 72.3106

Epoch 19: val_loss did not improve from 46.98846
196/196 - 80s - loss: 78.3463 - MinusLogProbMetric: 78.3463 - val_loss: 72.3106 - val_MinusLogProbMetric: 72.3106 - lr: 1.1111e-04 - 80s/epoch - 408ms/step
Epoch 20/1000
2023-09-26 19:57:19.457 
Epoch 20/1000 
	 loss: 69.6830, MinusLogProbMetric: 69.6830, val_loss: 64.1836, val_MinusLogProbMetric: 64.1836

Epoch 20: val_loss did not improve from 46.98846
196/196 - 79s - loss: 69.6830 - MinusLogProbMetric: 69.6830 - val_loss: 64.1836 - val_MinusLogProbMetric: 64.1836 - lr: 1.1111e-04 - 79s/epoch - 405ms/step
Epoch 21/1000
2023-09-26 19:58:39.827 
Epoch 21/1000 
	 loss: 71.7832, MinusLogProbMetric: 71.7832, val_loss: 73.0580, val_MinusLogProbMetric: 73.0580

Epoch 21: val_loss did not improve from 46.98846
196/196 - 80s - loss: 71.7832 - MinusLogProbMetric: 71.7832 - val_loss: 73.0580 - val_MinusLogProbMetric: 73.0580 - lr: 1.1111e-04 - 80s/epoch - 410ms/step
Epoch 22/1000
2023-09-26 19:59:59.519 
Epoch 22/1000 
	 loss: 63.5132, MinusLogProbMetric: 63.5132, val_loss: 58.4499, val_MinusLogProbMetric: 58.4499

Epoch 22: val_loss did not improve from 46.98846
196/196 - 80s - loss: 63.5132 - MinusLogProbMetric: 63.5132 - val_loss: 58.4499 - val_MinusLogProbMetric: 58.4499 - lr: 1.1111e-04 - 80s/epoch - 407ms/step
Epoch 23/1000
2023-09-26 20:01:20.131 
Epoch 23/1000 
	 loss: 56.2911, MinusLogProbMetric: 56.2911, val_loss: 53.8244, val_MinusLogProbMetric: 53.8244

Epoch 23: val_loss did not improve from 46.98846
196/196 - 81s - loss: 56.2911 - MinusLogProbMetric: 56.2911 - val_loss: 53.8244 - val_MinusLogProbMetric: 53.8244 - lr: 1.1111e-04 - 81s/epoch - 411ms/step
Epoch 24/1000
2023-09-26 20:02:39.808 
Epoch 24/1000 
	 loss: 58.5071, MinusLogProbMetric: 58.5071, val_loss: 59.8765, val_MinusLogProbMetric: 59.8765

Epoch 24: val_loss did not improve from 46.98846
196/196 - 80s - loss: 58.5071 - MinusLogProbMetric: 58.5071 - val_loss: 59.8765 - val_MinusLogProbMetric: 59.8765 - lr: 1.1111e-04 - 80s/epoch - 406ms/step
Epoch 25/1000
2023-09-26 20:03:59.429 
Epoch 25/1000 
	 loss: 51.5326, MinusLogProbMetric: 51.5326, val_loss: 48.3202, val_MinusLogProbMetric: 48.3202

Epoch 25: val_loss did not improve from 46.98846
196/196 - 80s - loss: 51.5326 - MinusLogProbMetric: 51.5326 - val_loss: 48.3202 - val_MinusLogProbMetric: 48.3202 - lr: 1.1111e-04 - 80s/epoch - 406ms/step
Epoch 26/1000
2023-09-26 20:05:19.324 
Epoch 26/1000 
	 loss: 47.1687, MinusLogProbMetric: 47.1687, val_loss: 45.4514, val_MinusLogProbMetric: 45.4514

Epoch 26: val_loss improved from 46.98846 to 45.45137, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 81s - loss: 47.1687 - MinusLogProbMetric: 47.1687 - val_loss: 45.4514 - val_MinusLogProbMetric: 45.4514 - lr: 1.1111e-04 - 81s/epoch - 415ms/step
Epoch 27/1000
2023-09-26 20:06:39.910 
Epoch 27/1000 
	 loss: 44.9966, MinusLogProbMetric: 44.9966, val_loss: 44.7737, val_MinusLogProbMetric: 44.7737

Epoch 27: val_loss improved from 45.45137 to 44.77370, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 81s - loss: 44.9966 - MinusLogProbMetric: 44.9966 - val_loss: 44.7737 - val_MinusLogProbMetric: 44.7737 - lr: 1.1111e-04 - 81s/epoch - 411ms/step
Epoch 28/1000
2023-09-26 20:08:01.507 
Epoch 28/1000 
	 loss: 44.2085, MinusLogProbMetric: 44.2085, val_loss: 43.4158, val_MinusLogProbMetric: 43.4158

Epoch 28: val_loss improved from 44.77370 to 43.41581, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 81s - loss: 44.2085 - MinusLogProbMetric: 44.2085 - val_loss: 43.4158 - val_MinusLogProbMetric: 43.4158 - lr: 1.1111e-04 - 81s/epoch - 415ms/step
Epoch 29/1000
2023-09-26 20:09:22.377 
Epoch 29/1000 
	 loss: 42.1797, MinusLogProbMetric: 42.1797, val_loss: 41.3435, val_MinusLogProbMetric: 41.3435

Epoch 29: val_loss improved from 43.41581 to 41.34354, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 81s - loss: 42.1797 - MinusLogProbMetric: 42.1797 - val_loss: 41.3435 - val_MinusLogProbMetric: 41.3435 - lr: 1.1111e-04 - 81s/epoch - 413ms/step
Epoch 30/1000
2023-09-26 20:10:43.736 
Epoch 30/1000 
	 loss: 40.6789, MinusLogProbMetric: 40.6789, val_loss: 40.6881, val_MinusLogProbMetric: 40.6881

Epoch 30: val_loss improved from 41.34354 to 40.68813, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 81s - loss: 40.6789 - MinusLogProbMetric: 40.6789 - val_loss: 40.6881 - val_MinusLogProbMetric: 40.6881 - lr: 1.1111e-04 - 81s/epoch - 415ms/step
Epoch 31/1000
2023-09-26 20:12:05.086 
Epoch 31/1000 
	 loss: 39.1823, MinusLogProbMetric: 39.1823, val_loss: 38.7193, val_MinusLogProbMetric: 38.7193

Epoch 31: val_loss improved from 40.68813 to 38.71930, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 82s - loss: 39.1823 - MinusLogProbMetric: 39.1823 - val_loss: 38.7193 - val_MinusLogProbMetric: 38.7193 - lr: 1.1111e-04 - 82s/epoch - 416ms/step
Epoch 32/1000
2023-09-26 20:13:25.929 
Epoch 32/1000 
	 loss: 38.2574, MinusLogProbMetric: 38.2574, val_loss: 37.9614, val_MinusLogProbMetric: 37.9614

Epoch 32: val_loss improved from 38.71930 to 37.96141, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 81s - loss: 38.2574 - MinusLogProbMetric: 38.2574 - val_loss: 37.9614 - val_MinusLogProbMetric: 37.9614 - lr: 1.1111e-04 - 81s/epoch - 412ms/step
Epoch 33/1000
2023-09-26 20:14:46.504 
Epoch 33/1000 
	 loss: 50.1667, MinusLogProbMetric: 50.1667, val_loss: 40.1918, val_MinusLogProbMetric: 40.1918

Epoch 33: val_loss did not improve from 37.96141
196/196 - 79s - loss: 50.1667 - MinusLogProbMetric: 50.1667 - val_loss: 40.1918 - val_MinusLogProbMetric: 40.1918 - lr: 1.1111e-04 - 79s/epoch - 404ms/step
Epoch 34/1000
2023-09-26 20:16:06.283 
Epoch 34/1000 
	 loss: 40.8674, MinusLogProbMetric: 40.8674, val_loss: 37.7826, val_MinusLogProbMetric: 37.7826

Epoch 34: val_loss improved from 37.96141 to 37.78259, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 81s - loss: 40.8674 - MinusLogProbMetric: 40.8674 - val_loss: 37.7826 - val_MinusLogProbMetric: 37.7826 - lr: 1.1111e-04 - 81s/epoch - 414ms/step
Epoch 35/1000
2023-09-26 20:17:27.378 
Epoch 35/1000 
	 loss: 37.0196, MinusLogProbMetric: 37.0196, val_loss: 36.5131, val_MinusLogProbMetric: 36.5131

Epoch 35: val_loss improved from 37.78259 to 36.51313, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 81s - loss: 37.0196 - MinusLogProbMetric: 37.0196 - val_loss: 36.5131 - val_MinusLogProbMetric: 36.5131 - lr: 1.1111e-04 - 81s/epoch - 413ms/step
Epoch 36/1000
2023-09-26 20:18:47.205 
Epoch 36/1000 
	 loss: 36.4526, MinusLogProbMetric: 36.4526, val_loss: 37.2808, val_MinusLogProbMetric: 37.2808

Epoch 36: val_loss did not improve from 36.51313
196/196 - 79s - loss: 36.4526 - MinusLogProbMetric: 36.4526 - val_loss: 37.2808 - val_MinusLogProbMetric: 37.2808 - lr: 1.1111e-04 - 79s/epoch - 401ms/step
Epoch 37/1000
2023-09-26 20:20:06.984 
Epoch 37/1000 
	 loss: 35.2816, MinusLogProbMetric: 35.2816, val_loss: 34.2152, val_MinusLogProbMetric: 34.2152

Epoch 37: val_loss improved from 36.51313 to 34.21519, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 81s - loss: 35.2816 - MinusLogProbMetric: 35.2816 - val_loss: 34.2152 - val_MinusLogProbMetric: 34.2152 - lr: 1.1111e-04 - 81s/epoch - 415ms/step
Epoch 38/1000
2023-09-26 20:21:27.957 
Epoch 38/1000 
	 loss: 33.5906, MinusLogProbMetric: 33.5906, val_loss: 33.5898, val_MinusLogProbMetric: 33.5898

Epoch 38: val_loss improved from 34.21519 to 33.58981, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 81s - loss: 33.5906 - MinusLogProbMetric: 33.5906 - val_loss: 33.5898 - val_MinusLogProbMetric: 33.5898 - lr: 1.1111e-04 - 81s/epoch - 412ms/step
Epoch 39/1000
2023-09-26 20:22:49.182 
Epoch 39/1000 
	 loss: 33.1173, MinusLogProbMetric: 33.1173, val_loss: 33.0454, val_MinusLogProbMetric: 33.0454

Epoch 39: val_loss improved from 33.58981 to 33.04545, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 81s - loss: 33.1173 - MinusLogProbMetric: 33.1173 - val_loss: 33.0454 - val_MinusLogProbMetric: 33.0454 - lr: 1.1111e-04 - 81s/epoch - 415ms/step
Epoch 40/1000
2023-09-26 20:24:10.292 
Epoch 40/1000 
	 loss: 32.3332, MinusLogProbMetric: 32.3332, val_loss: 32.0633, val_MinusLogProbMetric: 32.0633

Epoch 40: val_loss improved from 33.04545 to 32.06328, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 81s - loss: 32.3332 - MinusLogProbMetric: 32.3332 - val_loss: 32.0633 - val_MinusLogProbMetric: 32.0633 - lr: 1.1111e-04 - 81s/epoch - 414ms/step
Epoch 41/1000
2023-09-26 20:25:31.748 
Epoch 41/1000 
	 loss: 31.7681, MinusLogProbMetric: 31.7681, val_loss: 32.0507, val_MinusLogProbMetric: 32.0507

Epoch 41: val_loss improved from 32.06328 to 32.05072, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 81s - loss: 31.7681 - MinusLogProbMetric: 31.7681 - val_loss: 32.0507 - val_MinusLogProbMetric: 32.0507 - lr: 1.1111e-04 - 81s/epoch - 415ms/step
Epoch 42/1000
2023-09-26 20:26:52.467 
Epoch 42/1000 
	 loss: 31.3998, MinusLogProbMetric: 31.3998, val_loss: 31.4120, val_MinusLogProbMetric: 31.4120

Epoch 42: val_loss improved from 32.05072 to 31.41200, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 81s - loss: 31.3998 - MinusLogProbMetric: 31.3998 - val_loss: 31.4120 - val_MinusLogProbMetric: 31.4120 - lr: 1.1111e-04 - 81s/epoch - 412ms/step
Epoch 43/1000
2023-09-26 20:28:12.859 
Epoch 43/1000 
	 loss: 31.0562, MinusLogProbMetric: 31.0562, val_loss: 31.0050, val_MinusLogProbMetric: 31.0050

Epoch 43: val_loss improved from 31.41200 to 31.00500, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 80s - loss: 31.0562 - MinusLogProbMetric: 31.0562 - val_loss: 31.0050 - val_MinusLogProbMetric: 31.0050 - lr: 1.1111e-04 - 80s/epoch - 410ms/step
Epoch 44/1000
2023-09-26 20:29:33.972 
Epoch 44/1000 
	 loss: 30.6748, MinusLogProbMetric: 30.6748, val_loss: 30.5215, val_MinusLogProbMetric: 30.5215

Epoch 44: val_loss improved from 31.00500 to 30.52152, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 81s - loss: 30.6748 - MinusLogProbMetric: 30.6748 - val_loss: 30.5215 - val_MinusLogProbMetric: 30.5215 - lr: 1.1111e-04 - 81s/epoch - 414ms/step
Epoch 45/1000
2023-09-26 20:30:54.929 
Epoch 45/1000 
	 loss: 30.3317, MinusLogProbMetric: 30.3317, val_loss: 30.2129, val_MinusLogProbMetric: 30.2129

Epoch 45: val_loss improved from 30.52152 to 30.21291, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 81s - loss: 30.3317 - MinusLogProbMetric: 30.3317 - val_loss: 30.2129 - val_MinusLogProbMetric: 30.2129 - lr: 1.1111e-04 - 81s/epoch - 412ms/step
Epoch 46/1000
2023-09-26 20:32:15.823 
Epoch 46/1000 
	 loss: 29.9682, MinusLogProbMetric: 29.9682, val_loss: 30.1520, val_MinusLogProbMetric: 30.1520

Epoch 46: val_loss improved from 30.21291 to 30.15204, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 81s - loss: 29.9682 - MinusLogProbMetric: 29.9682 - val_loss: 30.1520 - val_MinusLogProbMetric: 30.1520 - lr: 1.1111e-04 - 81s/epoch - 412ms/step
Epoch 47/1000
2023-09-26 20:33:36.386 
Epoch 47/1000 
	 loss: 30.8064, MinusLogProbMetric: 30.8064, val_loss: 33.2837, val_MinusLogProbMetric: 33.2837

Epoch 47: val_loss did not improve from 30.15204
196/196 - 79s - loss: 30.8064 - MinusLogProbMetric: 30.8064 - val_loss: 33.2837 - val_MinusLogProbMetric: 33.2837 - lr: 1.1111e-04 - 79s/epoch - 405ms/step
Epoch 48/1000
2023-09-26 20:34:56.396 
Epoch 48/1000 
	 loss: 30.5477, MinusLogProbMetric: 30.5477, val_loss: 29.5669, val_MinusLogProbMetric: 29.5669

Epoch 48: val_loss improved from 30.15204 to 29.56690, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 81s - loss: 30.5477 - MinusLogProbMetric: 30.5477 - val_loss: 29.5669 - val_MinusLogProbMetric: 29.5669 - lr: 1.1111e-04 - 81s/epoch - 414ms/step
Epoch 49/1000
2023-09-26 20:36:17.171 
Epoch 49/1000 
	 loss: 29.2468, MinusLogProbMetric: 29.2468, val_loss: 29.1053, val_MinusLogProbMetric: 29.1053

Epoch 49: val_loss improved from 29.56690 to 29.10529, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 81s - loss: 29.2468 - MinusLogProbMetric: 29.2468 - val_loss: 29.1053 - val_MinusLogProbMetric: 29.1053 - lr: 1.1111e-04 - 81s/epoch - 413ms/step
Epoch 50/1000
2023-09-26 20:37:37.732 
Epoch 50/1000 
	 loss: 28.8104, MinusLogProbMetric: 28.8104, val_loss: 28.6955, val_MinusLogProbMetric: 28.6955

Epoch 50: val_loss improved from 29.10529 to 28.69553, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 81s - loss: 28.8104 - MinusLogProbMetric: 28.8104 - val_loss: 28.6955 - val_MinusLogProbMetric: 28.6955 - lr: 1.1111e-04 - 81s/epoch - 411ms/step
Epoch 51/1000
2023-09-26 20:38:58.577 
Epoch 51/1000 
	 loss: 28.5825, MinusLogProbMetric: 28.5825, val_loss: 28.3581, val_MinusLogProbMetric: 28.3581

Epoch 51: val_loss improved from 28.69553 to 28.35807, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 81s - loss: 28.5825 - MinusLogProbMetric: 28.5825 - val_loss: 28.3581 - val_MinusLogProbMetric: 28.3581 - lr: 1.1111e-04 - 81s/epoch - 412ms/step
Epoch 52/1000
2023-09-26 20:40:18.933 
Epoch 52/1000 
	 loss: 28.3950, MinusLogProbMetric: 28.3950, val_loss: 28.4244, val_MinusLogProbMetric: 28.4244

Epoch 52: val_loss did not improve from 28.35807
196/196 - 79s - loss: 28.3950 - MinusLogProbMetric: 28.3950 - val_loss: 28.4244 - val_MinusLogProbMetric: 28.4244 - lr: 1.1111e-04 - 79s/epoch - 403ms/step
Epoch 53/1000
2023-09-26 20:41:38.627 
Epoch 53/1000 
	 loss: 28.0713, MinusLogProbMetric: 28.0713, val_loss: 28.0618, val_MinusLogProbMetric: 28.0618

Epoch 53: val_loss improved from 28.35807 to 28.06185, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 81s - loss: 28.0713 - MinusLogProbMetric: 28.0713 - val_loss: 28.0618 - val_MinusLogProbMetric: 28.0618 - lr: 1.1111e-04 - 81s/epoch - 414ms/step
Epoch 54/1000
2023-09-26 20:43:00.049 
Epoch 54/1000 
	 loss: 27.9312, MinusLogProbMetric: 27.9312, val_loss: 27.9823, val_MinusLogProbMetric: 27.9823

Epoch 54: val_loss improved from 28.06185 to 27.98228, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 81s - loss: 27.9312 - MinusLogProbMetric: 27.9312 - val_loss: 27.9823 - val_MinusLogProbMetric: 27.9823 - lr: 1.1111e-04 - 81s/epoch - 415ms/step
Epoch 55/1000
2023-09-26 20:44:21.094 
Epoch 55/1000 
	 loss: 43.8831, MinusLogProbMetric: 43.8831, val_loss: 64.1764, val_MinusLogProbMetric: 64.1764

Epoch 55: val_loss did not improve from 27.98228
196/196 - 80s - loss: 43.8831 - MinusLogProbMetric: 43.8831 - val_loss: 64.1764 - val_MinusLogProbMetric: 64.1764 - lr: 1.1111e-04 - 80s/epoch - 407ms/step
Epoch 56/1000
2023-09-26 20:45:40.221 
Epoch 56/1000 
	 loss: 47.9677, MinusLogProbMetric: 47.9677, val_loss: 40.8431, val_MinusLogProbMetric: 40.8431

Epoch 56: val_loss did not improve from 27.98228
196/196 - 79s - loss: 47.9677 - MinusLogProbMetric: 47.9677 - val_loss: 40.8431 - val_MinusLogProbMetric: 40.8431 - lr: 1.1111e-04 - 79s/epoch - 404ms/step
Epoch 57/1000
2023-09-26 20:47:00.264 
Epoch 57/1000 
	 loss: 37.9596, MinusLogProbMetric: 37.9596, val_loss: 35.8101, val_MinusLogProbMetric: 35.8101

Epoch 57: val_loss did not improve from 27.98228
196/196 - 80s - loss: 37.9596 - MinusLogProbMetric: 37.9596 - val_loss: 35.8101 - val_MinusLogProbMetric: 35.8101 - lr: 1.1111e-04 - 80s/epoch - 408ms/step
Epoch 58/1000
2023-09-26 20:48:19.980 
Epoch 58/1000 
	 loss: 34.4644, MinusLogProbMetric: 34.4644, val_loss: 33.5965, val_MinusLogProbMetric: 33.5965

Epoch 58: val_loss did not improve from 27.98228
196/196 - 80s - loss: 34.4644 - MinusLogProbMetric: 34.4644 - val_loss: 33.5965 - val_MinusLogProbMetric: 33.5965 - lr: 1.1111e-04 - 80s/epoch - 407ms/step
Epoch 59/1000
2023-09-26 20:49:39.567 
Epoch 59/1000 
	 loss: 32.3704, MinusLogProbMetric: 32.3704, val_loss: 31.7363, val_MinusLogProbMetric: 31.7363

Epoch 59: val_loss did not improve from 27.98228
196/196 - 80s - loss: 32.3704 - MinusLogProbMetric: 32.3704 - val_loss: 31.7363 - val_MinusLogProbMetric: 31.7363 - lr: 1.1111e-04 - 80s/epoch - 406ms/step
Epoch 60/1000
2023-09-26 20:50:59.471 
Epoch 60/1000 
	 loss: 31.0423, MinusLogProbMetric: 31.0423, val_loss: 31.4135, val_MinusLogProbMetric: 31.4135

Epoch 60: val_loss did not improve from 27.98228
196/196 - 80s - loss: 31.0423 - MinusLogProbMetric: 31.0423 - val_loss: 31.4135 - val_MinusLogProbMetric: 31.4135 - lr: 1.1111e-04 - 80s/epoch - 408ms/step
Epoch 61/1000
2023-09-26 20:52:19.706 
Epoch 61/1000 
	 loss: 30.0980, MinusLogProbMetric: 30.0980, val_loss: 30.1910, val_MinusLogProbMetric: 30.1910

Epoch 61: val_loss did not improve from 27.98228
196/196 - 80s - loss: 30.0980 - MinusLogProbMetric: 30.0980 - val_loss: 30.1910 - val_MinusLogProbMetric: 30.1910 - lr: 1.1111e-04 - 80s/epoch - 409ms/step
Epoch 62/1000
2023-09-26 20:53:38.915 
Epoch 62/1000 
	 loss: 29.4343, MinusLogProbMetric: 29.4343, val_loss: 29.1725, val_MinusLogProbMetric: 29.1725

Epoch 62: val_loss did not improve from 27.98228
196/196 - 79s - loss: 29.4343 - MinusLogProbMetric: 29.4343 - val_loss: 29.1725 - val_MinusLogProbMetric: 29.1725 - lr: 1.1111e-04 - 79s/epoch - 404ms/step
Epoch 63/1000
2023-09-26 20:54:58.495 
Epoch 63/1000 
	 loss: 28.8008, MinusLogProbMetric: 28.8008, val_loss: 28.6548, val_MinusLogProbMetric: 28.6548

Epoch 63: val_loss did not improve from 27.98228
196/196 - 80s - loss: 28.8008 - MinusLogProbMetric: 28.8008 - val_loss: 28.6548 - val_MinusLogProbMetric: 28.6548 - lr: 1.1111e-04 - 80s/epoch - 406ms/step
Epoch 64/1000
2023-09-26 20:56:17.780 
Epoch 64/1000 
	 loss: 28.4823, MinusLogProbMetric: 28.4823, val_loss: 28.2572, val_MinusLogProbMetric: 28.2572

Epoch 64: val_loss did not improve from 27.98228
196/196 - 79s - loss: 28.4823 - MinusLogProbMetric: 28.4823 - val_loss: 28.2572 - val_MinusLogProbMetric: 28.2572 - lr: 1.1111e-04 - 79s/epoch - 404ms/step
Epoch 65/1000
2023-09-26 20:57:35.830 
Epoch 65/1000 
	 loss: 27.9001, MinusLogProbMetric: 27.9001, val_loss: 27.5108, val_MinusLogProbMetric: 27.5108

Epoch 65: val_loss improved from 27.98228 to 27.51080, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 79s - loss: 27.9001 - MinusLogProbMetric: 27.9001 - val_loss: 27.5108 - val_MinusLogProbMetric: 27.5108 - lr: 1.1111e-04 - 79s/epoch - 405ms/step
Epoch 66/1000
2023-09-26 20:58:56.953 
Epoch 66/1000 
	 loss: 27.3804, MinusLogProbMetric: 27.3804, val_loss: 27.3700, val_MinusLogProbMetric: 27.3700

Epoch 66: val_loss improved from 27.51080 to 27.36997, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 81s - loss: 27.3804 - MinusLogProbMetric: 27.3804 - val_loss: 27.3700 - val_MinusLogProbMetric: 27.3700 - lr: 1.1111e-04 - 81s/epoch - 413ms/step
Epoch 67/1000
2023-09-26 21:00:17.514 
Epoch 67/1000 
	 loss: 27.2038, MinusLogProbMetric: 27.2038, val_loss: 27.5372, val_MinusLogProbMetric: 27.5372

Epoch 67: val_loss did not improve from 27.36997
196/196 - 79s - loss: 27.2038 - MinusLogProbMetric: 27.2038 - val_loss: 27.5372 - val_MinusLogProbMetric: 27.5372 - lr: 1.1111e-04 - 79s/epoch - 405ms/step
Epoch 68/1000
2023-09-26 21:01:36.849 
Epoch 68/1000 
	 loss: 27.1228, MinusLogProbMetric: 27.1228, val_loss: 26.9842, val_MinusLogProbMetric: 26.9842

Epoch 68: val_loss improved from 27.36997 to 26.98416, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 81s - loss: 27.1228 - MinusLogProbMetric: 27.1228 - val_loss: 26.9842 - val_MinusLogProbMetric: 26.9842 - lr: 1.1111e-04 - 81s/epoch - 411ms/step
Epoch 69/1000
2023-09-26 21:02:57.453 
Epoch 69/1000 
	 loss: 26.5031, MinusLogProbMetric: 26.5031, val_loss: 27.1828, val_MinusLogProbMetric: 27.1828

Epoch 69: val_loss did not improve from 26.98416
196/196 - 79s - loss: 26.5031 - MinusLogProbMetric: 26.5031 - val_loss: 27.1828 - val_MinusLogProbMetric: 27.1828 - lr: 1.1111e-04 - 79s/epoch - 405ms/step
Epoch 70/1000
2023-09-26 21:04:17.166 
Epoch 70/1000 
	 loss: 26.3908, MinusLogProbMetric: 26.3908, val_loss: 26.3077, val_MinusLogProbMetric: 26.3077

Epoch 70: val_loss improved from 26.98416 to 26.30773, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 81s - loss: 26.3908 - MinusLogProbMetric: 26.3908 - val_loss: 26.3077 - val_MinusLogProbMetric: 26.3077 - lr: 1.1111e-04 - 81s/epoch - 414ms/step
Epoch 71/1000
2023-09-26 21:05:38.369 
Epoch 71/1000 
	 loss: 26.1250, MinusLogProbMetric: 26.1250, val_loss: 25.9139, val_MinusLogProbMetric: 25.9139

Epoch 71: val_loss improved from 26.30773 to 25.91389, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 81s - loss: 26.1250 - MinusLogProbMetric: 26.1250 - val_loss: 25.9139 - val_MinusLogProbMetric: 25.9139 - lr: 1.1111e-04 - 81s/epoch - 414ms/step
Epoch 72/1000
2023-09-26 21:06:59.889 
Epoch 72/1000 
	 loss: 25.9242, MinusLogProbMetric: 25.9242, val_loss: 25.8593, val_MinusLogProbMetric: 25.8593

Epoch 72: val_loss improved from 25.91389 to 25.85928, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 81s - loss: 25.9242 - MinusLogProbMetric: 25.9242 - val_loss: 25.8593 - val_MinusLogProbMetric: 25.8593 - lr: 1.1111e-04 - 81s/epoch - 416ms/step
Epoch 73/1000
2023-09-26 21:08:20.937 
Epoch 73/1000 
	 loss: 25.6940, MinusLogProbMetric: 25.6940, val_loss: 25.7111, val_MinusLogProbMetric: 25.7111

Epoch 73: val_loss improved from 25.85928 to 25.71108, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 81s - loss: 25.6940 - MinusLogProbMetric: 25.6940 - val_loss: 25.7111 - val_MinusLogProbMetric: 25.7111 - lr: 1.1111e-04 - 81s/epoch - 413ms/step
Epoch 74/1000
2023-09-26 21:09:42.057 
Epoch 74/1000 
	 loss: 25.6135, MinusLogProbMetric: 25.6135, val_loss: 25.5929, val_MinusLogProbMetric: 25.5929

Epoch 74: val_loss improved from 25.71108 to 25.59287, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 81s - loss: 25.6135 - MinusLogProbMetric: 25.6135 - val_loss: 25.5929 - val_MinusLogProbMetric: 25.5929 - lr: 1.1111e-04 - 81s/epoch - 415ms/step
Epoch 75/1000
2023-09-26 21:11:02.376 
Epoch 75/1000 
	 loss: 25.3269, MinusLogProbMetric: 25.3269, val_loss: 25.4832, val_MinusLogProbMetric: 25.4832

Epoch 75: val_loss improved from 25.59287 to 25.48316, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 80s - loss: 25.3269 - MinusLogProbMetric: 25.3269 - val_loss: 25.4832 - val_MinusLogProbMetric: 25.4832 - lr: 1.1111e-04 - 80s/epoch - 409ms/step
Epoch 76/1000
2023-09-26 21:12:22.972 
Epoch 76/1000 
	 loss: 25.4147, MinusLogProbMetric: 25.4147, val_loss: 25.5790, val_MinusLogProbMetric: 25.5790

Epoch 76: val_loss did not improve from 25.48316
196/196 - 79s - loss: 25.4147 - MinusLogProbMetric: 25.4147 - val_loss: 25.5790 - val_MinusLogProbMetric: 25.5790 - lr: 1.1111e-04 - 79s/epoch - 405ms/step
Epoch 77/1000
2023-09-26 21:13:41.906 
Epoch 77/1000 
	 loss: 25.0724, MinusLogProbMetric: 25.0724, val_loss: 25.2052, val_MinusLogProbMetric: 25.2052

Epoch 77: val_loss improved from 25.48316 to 25.20517, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 80s - loss: 25.0724 - MinusLogProbMetric: 25.0724 - val_loss: 25.2052 - val_MinusLogProbMetric: 25.2052 - lr: 1.1111e-04 - 80s/epoch - 409ms/step
Epoch 78/1000
2023-09-26 21:15:03.080 
Epoch 78/1000 
	 loss: 24.9441, MinusLogProbMetric: 24.9441, val_loss: 24.7969, val_MinusLogProbMetric: 24.7969

Epoch 78: val_loss improved from 25.20517 to 24.79686, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 81s - loss: 24.9441 - MinusLogProbMetric: 24.9441 - val_loss: 24.7969 - val_MinusLogProbMetric: 24.7969 - lr: 1.1111e-04 - 81s/epoch - 415ms/step
Epoch 79/1000
2023-09-26 21:16:23.912 
Epoch 79/1000 
	 loss: 24.7272, MinusLogProbMetric: 24.7272, val_loss: 24.8748, val_MinusLogProbMetric: 24.8748

Epoch 79: val_loss did not improve from 24.79686
196/196 - 80s - loss: 24.7272 - MinusLogProbMetric: 24.7272 - val_loss: 24.8748 - val_MinusLogProbMetric: 24.8748 - lr: 1.1111e-04 - 80s/epoch - 406ms/step
Epoch 80/1000
2023-09-26 21:17:43.983 
Epoch 80/1000 
	 loss: 25.5575, MinusLogProbMetric: 25.5575, val_loss: 24.7631, val_MinusLogProbMetric: 24.7631

Epoch 80: val_loss improved from 24.79686 to 24.76311, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 81s - loss: 25.5575 - MinusLogProbMetric: 25.5575 - val_loss: 24.7631 - val_MinusLogProbMetric: 24.7631 - lr: 1.1111e-04 - 81s/epoch - 415ms/step
Epoch 81/1000
2023-09-26 21:19:04.245 
Epoch 81/1000 
	 loss: 24.5478, MinusLogProbMetric: 24.5478, val_loss: 24.5260, val_MinusLogProbMetric: 24.5260

Epoch 81: val_loss improved from 24.76311 to 24.52595, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 80s - loss: 24.5478 - MinusLogProbMetric: 24.5478 - val_loss: 24.5260 - val_MinusLogProbMetric: 24.5260 - lr: 1.1111e-04 - 80s/epoch - 408ms/step
Epoch 82/1000
2023-09-26 21:20:25.045 
Epoch 82/1000 
	 loss: 24.4102, MinusLogProbMetric: 24.4102, val_loss: 24.3604, val_MinusLogProbMetric: 24.3604

Epoch 82: val_loss improved from 24.52595 to 24.36035, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 81s - loss: 24.4102 - MinusLogProbMetric: 24.4102 - val_loss: 24.3604 - val_MinusLogProbMetric: 24.3604 - lr: 1.1111e-04 - 81s/epoch - 413ms/step
Epoch 83/1000
2023-09-26 21:21:46.222 
Epoch 83/1000 
	 loss: 24.2216, MinusLogProbMetric: 24.2216, val_loss: 24.1159, val_MinusLogProbMetric: 24.1159

Epoch 83: val_loss improved from 24.36035 to 24.11589, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 81s - loss: 24.2216 - MinusLogProbMetric: 24.2216 - val_loss: 24.1159 - val_MinusLogProbMetric: 24.1159 - lr: 1.1111e-04 - 81s/epoch - 415ms/step
Epoch 84/1000
2023-09-26 21:23:06.693 
Epoch 84/1000 
	 loss: 24.1157, MinusLogProbMetric: 24.1157, val_loss: 24.0292, val_MinusLogProbMetric: 24.0292

Epoch 84: val_loss improved from 24.11589 to 24.02922, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 80s - loss: 24.1157 - MinusLogProbMetric: 24.1157 - val_loss: 24.0292 - val_MinusLogProbMetric: 24.0292 - lr: 1.1111e-04 - 80s/epoch - 411ms/step
Epoch 85/1000
2023-09-26 21:24:27.596 
Epoch 85/1000 
	 loss: 23.9940, MinusLogProbMetric: 23.9940, val_loss: 24.2749, val_MinusLogProbMetric: 24.2749

Epoch 85: val_loss did not improve from 24.02922
196/196 - 79s - loss: 23.9940 - MinusLogProbMetric: 23.9940 - val_loss: 24.2749 - val_MinusLogProbMetric: 24.2749 - lr: 1.1111e-04 - 79s/epoch - 406ms/step
Epoch 86/1000
2023-09-26 21:25:47.708 
Epoch 86/1000 
	 loss: 24.0048, MinusLogProbMetric: 24.0048, val_loss: 24.0083, val_MinusLogProbMetric: 24.0083

Epoch 86: val_loss improved from 24.02922 to 24.00828, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 81s - loss: 24.0048 - MinusLogProbMetric: 24.0048 - val_loss: 24.0083 - val_MinusLogProbMetric: 24.0083 - lr: 1.1111e-04 - 81s/epoch - 415ms/step
Epoch 87/1000
2023-09-26 21:27:08.694 
Epoch 87/1000 
	 loss: 23.7722, MinusLogProbMetric: 23.7722, val_loss: 23.8737, val_MinusLogProbMetric: 23.8737

Epoch 87: val_loss improved from 24.00828 to 23.87371, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 81s - loss: 23.7722 - MinusLogProbMetric: 23.7722 - val_loss: 23.8737 - val_MinusLogProbMetric: 23.8737 - lr: 1.1111e-04 - 81s/epoch - 413ms/step
Epoch 88/1000
2023-09-26 21:28:29.355 
Epoch 88/1000 
	 loss: 23.7119, MinusLogProbMetric: 23.7119, val_loss: 23.8471, val_MinusLogProbMetric: 23.8471

Epoch 88: val_loss improved from 23.87371 to 23.84713, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 81s - loss: 23.7119 - MinusLogProbMetric: 23.7119 - val_loss: 23.8471 - val_MinusLogProbMetric: 23.8471 - lr: 1.1111e-04 - 81s/epoch - 413ms/step
Epoch 89/1000
2023-09-26 21:29:50.679 
Epoch 89/1000 
	 loss: 23.6531, MinusLogProbMetric: 23.6531, val_loss: 23.6830, val_MinusLogProbMetric: 23.6830

Epoch 89: val_loss improved from 23.84713 to 23.68302, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 81s - loss: 23.6531 - MinusLogProbMetric: 23.6531 - val_loss: 23.6830 - val_MinusLogProbMetric: 23.6830 - lr: 1.1111e-04 - 81s/epoch - 414ms/step
Epoch 90/1000
2023-09-26 21:31:12.056 
Epoch 90/1000 
	 loss: 23.4878, MinusLogProbMetric: 23.4878, val_loss: 23.4771, val_MinusLogProbMetric: 23.4771

Epoch 90: val_loss improved from 23.68302 to 23.47706, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 81s - loss: 23.4878 - MinusLogProbMetric: 23.4878 - val_loss: 23.4771 - val_MinusLogProbMetric: 23.4771 - lr: 1.1111e-04 - 81s/epoch - 415ms/step
Epoch 91/1000
2023-09-26 21:32:33.386 
Epoch 91/1000 
	 loss: 23.4234, MinusLogProbMetric: 23.4234, val_loss: 23.5479, val_MinusLogProbMetric: 23.5479

Epoch 91: val_loss did not improve from 23.47706
196/196 - 80s - loss: 23.4234 - MinusLogProbMetric: 23.4234 - val_loss: 23.5479 - val_MinusLogProbMetric: 23.5479 - lr: 1.1111e-04 - 80s/epoch - 409ms/step
Epoch 92/1000
2023-09-26 21:33:53.727 
Epoch 92/1000 
	 loss: 23.4034, MinusLogProbMetric: 23.4034, val_loss: 24.1102, val_MinusLogProbMetric: 24.1102

Epoch 92: val_loss did not improve from 23.47706
196/196 - 80s - loss: 23.4034 - MinusLogProbMetric: 23.4034 - val_loss: 24.1102 - val_MinusLogProbMetric: 24.1102 - lr: 1.1111e-04 - 80s/epoch - 410ms/step
Epoch 93/1000
2023-09-26 21:35:13.544 
Epoch 93/1000 
	 loss: 23.2559, MinusLogProbMetric: 23.2559, val_loss: 23.6517, val_MinusLogProbMetric: 23.6517

Epoch 93: val_loss did not improve from 23.47706
196/196 - 80s - loss: 23.2559 - MinusLogProbMetric: 23.2559 - val_loss: 23.6517 - val_MinusLogProbMetric: 23.6517 - lr: 1.1111e-04 - 80s/epoch - 407ms/step
Epoch 94/1000
2023-09-26 21:36:33.014 
Epoch 94/1000 
	 loss: 23.1926, MinusLogProbMetric: 23.1926, val_loss: 23.3778, val_MinusLogProbMetric: 23.3778

Epoch 94: val_loss improved from 23.47706 to 23.37775, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 81s - loss: 23.1926 - MinusLogProbMetric: 23.1926 - val_loss: 23.3778 - val_MinusLogProbMetric: 23.3778 - lr: 1.1111e-04 - 81s/epoch - 412ms/step
Epoch 95/1000
2023-09-26 21:37:53.820 
Epoch 95/1000 
	 loss: 23.1807, MinusLogProbMetric: 23.1807, val_loss: 23.2269, val_MinusLogProbMetric: 23.2269

Epoch 95: val_loss improved from 23.37775 to 23.22694, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 81s - loss: 23.1807 - MinusLogProbMetric: 23.1807 - val_loss: 23.2269 - val_MinusLogProbMetric: 23.2269 - lr: 1.1111e-04 - 81s/epoch - 412ms/step
Epoch 96/1000
2023-09-26 21:39:15.256 
Epoch 96/1000 
	 loss: 23.0598, MinusLogProbMetric: 23.0598, val_loss: 22.9243, val_MinusLogProbMetric: 22.9243

Epoch 96: val_loss improved from 23.22694 to 22.92430, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 82s - loss: 23.0598 - MinusLogProbMetric: 23.0598 - val_loss: 22.9243 - val_MinusLogProbMetric: 22.9243 - lr: 1.1111e-04 - 82s/epoch - 416ms/step
Epoch 97/1000
2023-09-26 21:40:35.748 
Epoch 97/1000 
	 loss: 22.9713, MinusLogProbMetric: 22.9713, val_loss: 23.1172, val_MinusLogProbMetric: 23.1172

Epoch 97: val_loss did not improve from 22.92430
196/196 - 79s - loss: 22.9713 - MinusLogProbMetric: 22.9713 - val_loss: 23.1172 - val_MinusLogProbMetric: 23.1172 - lr: 1.1111e-04 - 79s/epoch - 404ms/step
Epoch 98/1000
2023-09-26 21:41:55.532 
Epoch 98/1000 
	 loss: 22.9797, MinusLogProbMetric: 22.9797, val_loss: 22.9435, val_MinusLogProbMetric: 22.9435

Epoch 98: val_loss did not improve from 22.92430
196/196 - 80s - loss: 22.9797 - MinusLogProbMetric: 22.9797 - val_loss: 22.9435 - val_MinusLogProbMetric: 22.9435 - lr: 1.1111e-04 - 80s/epoch - 407ms/step
Epoch 99/1000
2023-09-26 21:43:15.584 
Epoch 99/1000 
	 loss: 22.8280, MinusLogProbMetric: 22.8280, val_loss: 22.7923, val_MinusLogProbMetric: 22.7923

Epoch 99: val_loss improved from 22.92430 to 22.79225, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 81s - loss: 22.8280 - MinusLogProbMetric: 22.8280 - val_loss: 22.7923 - val_MinusLogProbMetric: 22.7923 - lr: 1.1111e-04 - 81s/epoch - 414ms/step
Epoch 100/1000
2023-09-26 21:44:36.745 
Epoch 100/1000 
	 loss: 22.7724, MinusLogProbMetric: 22.7724, val_loss: 23.0717, val_MinusLogProbMetric: 23.0717

Epoch 100: val_loss did not improve from 22.79225
196/196 - 80s - loss: 22.7724 - MinusLogProbMetric: 22.7724 - val_loss: 23.0717 - val_MinusLogProbMetric: 23.0717 - lr: 1.1111e-04 - 80s/epoch - 408ms/step
Epoch 101/1000
2023-09-26 21:45:56.791 
Epoch 101/1000 
	 loss: 22.7070, MinusLogProbMetric: 22.7070, val_loss: 22.7567, val_MinusLogProbMetric: 22.7567

Epoch 101: val_loss improved from 22.79225 to 22.75675, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 81s - loss: 22.7070 - MinusLogProbMetric: 22.7070 - val_loss: 22.7567 - val_MinusLogProbMetric: 22.7567 - lr: 1.1111e-04 - 81s/epoch - 414ms/step
Epoch 102/1000
2023-09-26 21:47:17.871 
Epoch 102/1000 
	 loss: 22.8068, MinusLogProbMetric: 22.8068, val_loss: 22.7059, val_MinusLogProbMetric: 22.7059

Epoch 102: val_loss improved from 22.75675 to 22.70592, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 81s - loss: 22.8068 - MinusLogProbMetric: 22.8068 - val_loss: 22.7059 - val_MinusLogProbMetric: 22.7059 - lr: 1.1111e-04 - 81s/epoch - 415ms/step
Epoch 103/1000
2023-09-26 21:48:39.058 
Epoch 103/1000 
	 loss: 22.6632, MinusLogProbMetric: 22.6632, val_loss: 23.2391, val_MinusLogProbMetric: 23.2391

Epoch 103: val_loss did not improve from 22.70592
196/196 - 80s - loss: 22.6632 - MinusLogProbMetric: 22.6632 - val_loss: 23.2391 - val_MinusLogProbMetric: 23.2391 - lr: 1.1111e-04 - 80s/epoch - 407ms/step
Epoch 104/1000
2023-09-26 21:49:58.795 
Epoch 104/1000 
	 loss: 22.5090, MinusLogProbMetric: 22.5090, val_loss: 22.7084, val_MinusLogProbMetric: 22.7084

Epoch 104: val_loss did not improve from 22.70592
196/196 - 80s - loss: 22.5090 - MinusLogProbMetric: 22.5090 - val_loss: 22.7084 - val_MinusLogProbMetric: 22.7084 - lr: 1.1111e-04 - 80s/epoch - 407ms/step
Epoch 105/1000
2023-09-26 21:51:18.362 
Epoch 105/1000 
	 loss: 22.4735, MinusLogProbMetric: 22.4735, val_loss: 22.3901, val_MinusLogProbMetric: 22.3901

Epoch 105: val_loss improved from 22.70592 to 22.39006, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 81s - loss: 22.4735 - MinusLogProbMetric: 22.4735 - val_loss: 22.3901 - val_MinusLogProbMetric: 22.3901 - lr: 1.1111e-04 - 81s/epoch - 413ms/step
Epoch 106/1000
2023-09-26 21:52:39.416 
Epoch 106/1000 
	 loss: 22.3619, MinusLogProbMetric: 22.3619, val_loss: 22.7424, val_MinusLogProbMetric: 22.7424

Epoch 106: val_loss did not improve from 22.39006
196/196 - 80s - loss: 22.3619 - MinusLogProbMetric: 22.3619 - val_loss: 22.7424 - val_MinusLogProbMetric: 22.7424 - lr: 1.1111e-04 - 80s/epoch - 406ms/step
Epoch 107/1000
2023-09-26 21:53:59.095 
Epoch 107/1000 
	 loss: 22.4082, MinusLogProbMetric: 22.4082, val_loss: 22.4753, val_MinusLogProbMetric: 22.4753

Epoch 107: val_loss did not improve from 22.39006
196/196 - 80s - loss: 22.4082 - MinusLogProbMetric: 22.4082 - val_loss: 22.4753 - val_MinusLogProbMetric: 22.4753 - lr: 1.1111e-04 - 80s/epoch - 407ms/step
Epoch 108/1000
2023-09-26 21:55:18.492 
Epoch 108/1000 
	 loss: 22.2230, MinusLogProbMetric: 22.2230, val_loss: 22.3540, val_MinusLogProbMetric: 22.3540

Epoch 108: val_loss improved from 22.39006 to 22.35398, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 81s - loss: 22.2230 - MinusLogProbMetric: 22.2230 - val_loss: 22.3540 - val_MinusLogProbMetric: 22.3540 - lr: 1.1111e-04 - 81s/epoch - 412ms/step
Epoch 109/1000
2023-09-26 21:56:39.880 
Epoch 109/1000 
	 loss: 22.2611, MinusLogProbMetric: 22.2611, val_loss: 22.7358, val_MinusLogProbMetric: 22.7358

Epoch 109: val_loss did not improve from 22.35398
196/196 - 80s - loss: 22.2611 - MinusLogProbMetric: 22.2611 - val_loss: 22.7358 - val_MinusLogProbMetric: 22.7358 - lr: 1.1111e-04 - 80s/epoch - 408ms/step
Epoch 110/1000
2023-09-26 21:57:59.966 
Epoch 110/1000 
	 loss: 22.2002, MinusLogProbMetric: 22.2002, val_loss: 22.5796, val_MinusLogProbMetric: 22.5796

Epoch 110: val_loss did not improve from 22.35398
196/196 - 80s - loss: 22.2002 - MinusLogProbMetric: 22.2002 - val_loss: 22.5796 - val_MinusLogProbMetric: 22.5796 - lr: 1.1111e-04 - 80s/epoch - 409ms/step
Epoch 111/1000
2023-09-26 21:59:19.338 
Epoch 111/1000 
	 loss: 22.4411, MinusLogProbMetric: 22.4411, val_loss: 22.1539, val_MinusLogProbMetric: 22.1539

Epoch 111: val_loss improved from 22.35398 to 22.15395, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 81s - loss: 22.4411 - MinusLogProbMetric: 22.4411 - val_loss: 22.1539 - val_MinusLogProbMetric: 22.1539 - lr: 1.1111e-04 - 81s/epoch - 412ms/step
Epoch 112/1000
2023-09-26 22:00:41.476 
Epoch 112/1000 
	 loss: 22.1421, MinusLogProbMetric: 22.1421, val_loss: 22.7064, val_MinusLogProbMetric: 22.7064

Epoch 112: val_loss did not improve from 22.15395
196/196 - 81s - loss: 22.1421 - MinusLogProbMetric: 22.1421 - val_loss: 22.7064 - val_MinusLogProbMetric: 22.7064 - lr: 1.1111e-04 - 81s/epoch - 412ms/step
Epoch 113/1000
2023-09-26 22:02:02.763 
Epoch 113/1000 
	 loss: 22.0491, MinusLogProbMetric: 22.0491, val_loss: 22.0216, val_MinusLogProbMetric: 22.0216

Epoch 113: val_loss improved from 22.15395 to 22.02159, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 82s - loss: 22.0491 - MinusLogProbMetric: 22.0491 - val_loss: 22.0216 - val_MinusLogProbMetric: 22.0216 - lr: 1.1111e-04 - 82s/epoch - 420ms/step
Epoch 114/1000
2023-09-26 22:03:23.477 
Epoch 114/1000 
	 loss: 22.0173, MinusLogProbMetric: 22.0173, val_loss: 21.9691, val_MinusLogProbMetric: 21.9691

Epoch 114: val_loss improved from 22.02159 to 21.96912, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 81s - loss: 22.0173 - MinusLogProbMetric: 22.0173 - val_loss: 21.9691 - val_MinusLogProbMetric: 21.9691 - lr: 1.1111e-04 - 81s/epoch - 413ms/step
Epoch 115/1000
2023-09-26 22:04:43.879 
Epoch 115/1000 
	 loss: 21.9156, MinusLogProbMetric: 21.9156, val_loss: 21.9026, val_MinusLogProbMetric: 21.9026

Epoch 115: val_loss improved from 21.96912 to 21.90261, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 80s - loss: 21.9156 - MinusLogProbMetric: 21.9156 - val_loss: 21.9026 - val_MinusLogProbMetric: 21.9026 - lr: 1.1111e-04 - 80s/epoch - 410ms/step
Epoch 116/1000
2023-09-26 22:06:04.411 
Epoch 116/1000 
	 loss: 21.8928, MinusLogProbMetric: 21.8928, val_loss: 21.8571, val_MinusLogProbMetric: 21.8571

Epoch 116: val_loss improved from 21.90261 to 21.85707, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 80s - loss: 21.8928 - MinusLogProbMetric: 21.8928 - val_loss: 21.8571 - val_MinusLogProbMetric: 21.8571 - lr: 1.1111e-04 - 80s/epoch - 410ms/step
Epoch 117/1000
2023-09-26 22:07:25.232 
Epoch 117/1000 
	 loss: 22.0726, MinusLogProbMetric: 22.0726, val_loss: 22.2949, val_MinusLogProbMetric: 22.2949

Epoch 117: val_loss did not improve from 21.85707
196/196 - 80s - loss: 22.0726 - MinusLogProbMetric: 22.0726 - val_loss: 22.2949 - val_MinusLogProbMetric: 22.2949 - lr: 1.1111e-04 - 80s/epoch - 406ms/step
Epoch 118/1000
2023-09-26 22:08:45.474 
Epoch 118/1000 
	 loss: 21.8479, MinusLogProbMetric: 21.8479, val_loss: 21.9669, val_MinusLogProbMetric: 21.9669

Epoch 118: val_loss did not improve from 21.85707
196/196 - 80s - loss: 21.8479 - MinusLogProbMetric: 21.8479 - val_loss: 21.9669 - val_MinusLogProbMetric: 21.9669 - lr: 1.1111e-04 - 80s/epoch - 409ms/step
Epoch 119/1000
2023-09-26 22:10:05.652 
Epoch 119/1000 
	 loss: 21.8008, MinusLogProbMetric: 21.8008, val_loss: 22.3348, val_MinusLogProbMetric: 22.3348

Epoch 119: val_loss did not improve from 21.85707
196/196 - 80s - loss: 21.8008 - MinusLogProbMetric: 21.8008 - val_loss: 22.3348 - val_MinusLogProbMetric: 22.3348 - lr: 1.1111e-04 - 80s/epoch - 409ms/step
Epoch 120/1000
2023-09-26 22:11:24.952 
Epoch 120/1000 
	 loss: 21.7426, MinusLogProbMetric: 21.7426, val_loss: 21.7684, val_MinusLogProbMetric: 21.7684

Epoch 120: val_loss improved from 21.85707 to 21.76839, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 80s - loss: 21.7426 - MinusLogProbMetric: 21.7426 - val_loss: 21.7684 - val_MinusLogProbMetric: 21.7684 - lr: 1.1111e-04 - 80s/epoch - 410ms/step
Epoch 121/1000
2023-09-26 22:12:46.276 
Epoch 121/1000 
	 loss: 21.6132, MinusLogProbMetric: 21.6132, val_loss: 21.8625, val_MinusLogProbMetric: 21.8625

Epoch 121: val_loss did not improve from 21.76839
196/196 - 80s - loss: 21.6132 - MinusLogProbMetric: 21.6132 - val_loss: 21.8625 - val_MinusLogProbMetric: 21.8625 - lr: 1.1111e-04 - 80s/epoch - 409ms/step
Epoch 122/1000
2023-09-26 22:14:06.236 
Epoch 122/1000 
	 loss: 21.7186, MinusLogProbMetric: 21.7186, val_loss: 21.9085, val_MinusLogProbMetric: 21.9085

Epoch 122: val_loss did not improve from 21.76839
196/196 - 80s - loss: 21.7186 - MinusLogProbMetric: 21.7186 - val_loss: 21.9085 - val_MinusLogProbMetric: 21.9085 - lr: 1.1111e-04 - 80s/epoch - 408ms/step
Epoch 123/1000
2023-09-26 22:15:25.335 
Epoch 123/1000 
	 loss: 21.5889, MinusLogProbMetric: 21.5889, val_loss: 21.4921, val_MinusLogProbMetric: 21.4921

Epoch 123: val_loss improved from 21.76839 to 21.49212, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 80s - loss: 21.5889 - MinusLogProbMetric: 21.5889 - val_loss: 21.4921 - val_MinusLogProbMetric: 21.4921 - lr: 1.1111e-04 - 80s/epoch - 411ms/step
Epoch 124/1000
2023-09-26 22:16:45.527 
Epoch 124/1000 
	 loss: 21.5147, MinusLogProbMetric: 21.5147, val_loss: 21.5456, val_MinusLogProbMetric: 21.5456

Epoch 124: val_loss did not improve from 21.49212
196/196 - 79s - loss: 21.5147 - MinusLogProbMetric: 21.5147 - val_loss: 21.5456 - val_MinusLogProbMetric: 21.5456 - lr: 1.1111e-04 - 79s/epoch - 402ms/step
Epoch 125/1000
2023-09-26 22:18:04.329 
Epoch 125/1000 
	 loss: 21.4462, MinusLogProbMetric: 21.4462, val_loss: 21.5884, val_MinusLogProbMetric: 21.5884

Epoch 125: val_loss did not improve from 21.49212
196/196 - 79s - loss: 21.4462 - MinusLogProbMetric: 21.4462 - val_loss: 21.5884 - val_MinusLogProbMetric: 21.5884 - lr: 1.1111e-04 - 79s/epoch - 402ms/step
Epoch 126/1000
2023-09-26 22:19:23.135 
Epoch 126/1000 
	 loss: 21.4990, MinusLogProbMetric: 21.4990, val_loss: 21.5266, val_MinusLogProbMetric: 21.5266

Epoch 126: val_loss did not improve from 21.49212
196/196 - 79s - loss: 21.4990 - MinusLogProbMetric: 21.4990 - val_loss: 21.5266 - val_MinusLogProbMetric: 21.5266 - lr: 1.1111e-04 - 79s/epoch - 402ms/step
Epoch 127/1000
2023-09-26 22:20:41.543 
Epoch 127/1000 
	 loss: 21.3587, MinusLogProbMetric: 21.3587, val_loss: 21.3773, val_MinusLogProbMetric: 21.3773

Epoch 127: val_loss improved from 21.49212 to 21.37731, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 80s - loss: 21.3587 - MinusLogProbMetric: 21.3587 - val_loss: 21.3773 - val_MinusLogProbMetric: 21.3773 - lr: 1.1111e-04 - 80s/epoch - 407ms/step
Epoch 128/1000
2023-09-26 22:22:01.340 
Epoch 128/1000 
	 loss: 21.3274, MinusLogProbMetric: 21.3274, val_loss: 21.3734, val_MinusLogProbMetric: 21.3734

Epoch 128: val_loss improved from 21.37731 to 21.37341, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 80s - loss: 21.3274 - MinusLogProbMetric: 21.3274 - val_loss: 21.3734 - val_MinusLogProbMetric: 21.3734 - lr: 1.1111e-04 - 80s/epoch - 407ms/step
Epoch 129/1000
2023-09-26 22:23:21.415 
Epoch 129/1000 
	 loss: 21.5071, MinusLogProbMetric: 21.5071, val_loss: 21.8762, val_MinusLogProbMetric: 21.8762

Epoch 129: val_loss did not improve from 21.37341
196/196 - 79s - loss: 21.5071 - MinusLogProbMetric: 21.5071 - val_loss: 21.8762 - val_MinusLogProbMetric: 21.8762 - lr: 1.1111e-04 - 79s/epoch - 402ms/step
Epoch 130/1000
2023-09-26 22:24:39.709 
Epoch 130/1000 
	 loss: 21.2736, MinusLogProbMetric: 21.2736, val_loss: 21.3922, val_MinusLogProbMetric: 21.3922

Epoch 130: val_loss did not improve from 21.37341
196/196 - 78s - loss: 21.2736 - MinusLogProbMetric: 21.2736 - val_loss: 21.3922 - val_MinusLogProbMetric: 21.3922 - lr: 1.1111e-04 - 78s/epoch - 399ms/step
Epoch 131/1000
2023-09-26 22:25:58.392 
Epoch 131/1000 
	 loss: 21.3919, MinusLogProbMetric: 21.3919, val_loss: 21.5155, val_MinusLogProbMetric: 21.5155

Epoch 131: val_loss did not improve from 21.37341
196/196 - 79s - loss: 21.3919 - MinusLogProbMetric: 21.3919 - val_loss: 21.5155 - val_MinusLogProbMetric: 21.5155 - lr: 1.1111e-04 - 79s/epoch - 401ms/step
Epoch 132/1000
2023-09-26 22:27:16.994 
Epoch 132/1000 
	 loss: 21.2234, MinusLogProbMetric: 21.2234, val_loss: 21.2102, val_MinusLogProbMetric: 21.2102

Epoch 132: val_loss improved from 21.37341 to 21.21025, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 80s - loss: 21.2234 - MinusLogProbMetric: 21.2234 - val_loss: 21.2102 - val_MinusLogProbMetric: 21.2102 - lr: 1.1111e-04 - 80s/epoch - 407ms/step
Epoch 133/1000
2023-09-26 22:28:36.030 
Epoch 133/1000 
	 loss: 21.2904, MinusLogProbMetric: 21.2904, val_loss: 22.3123, val_MinusLogProbMetric: 22.3123

Epoch 133: val_loss did not improve from 21.21025
196/196 - 78s - loss: 21.2904 - MinusLogProbMetric: 21.2904 - val_loss: 22.3123 - val_MinusLogProbMetric: 22.3123 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 134/1000
2023-09-26 22:29:54.591 
Epoch 134/1000 
	 loss: 21.2875, MinusLogProbMetric: 21.2875, val_loss: 21.1981, val_MinusLogProbMetric: 21.1981

Epoch 134: val_loss improved from 21.21025 to 21.19811, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 80s - loss: 21.2875 - MinusLogProbMetric: 21.2875 - val_loss: 21.1981 - val_MinusLogProbMetric: 21.1981 - lr: 1.1111e-04 - 80s/epoch - 408ms/step
Epoch 135/1000
2023-09-26 22:31:14.665 
Epoch 135/1000 
	 loss: 21.1283, MinusLogProbMetric: 21.1283, val_loss: 21.2132, val_MinusLogProbMetric: 21.2132

Epoch 135: val_loss did not improve from 21.19811
196/196 - 79s - loss: 21.1283 - MinusLogProbMetric: 21.1283 - val_loss: 21.2132 - val_MinusLogProbMetric: 21.2132 - lr: 1.1111e-04 - 79s/epoch - 402ms/step
Epoch 136/1000
2023-09-26 22:32:30.946 
Epoch 136/1000 
	 loss: 21.1525, MinusLogProbMetric: 21.1525, val_loss: 21.6047, val_MinusLogProbMetric: 21.6047

Epoch 136: val_loss did not improve from 21.19811
196/196 - 76s - loss: 21.1525 - MinusLogProbMetric: 21.1525 - val_loss: 21.6047 - val_MinusLogProbMetric: 21.6047 - lr: 1.1111e-04 - 76s/epoch - 389ms/step
Epoch 137/1000
2023-09-26 22:33:39.276 
Epoch 137/1000 
	 loss: 21.2151, MinusLogProbMetric: 21.2151, val_loss: 21.2853, val_MinusLogProbMetric: 21.2853

Epoch 137: val_loss did not improve from 21.19811
196/196 - 68s - loss: 21.2151 - MinusLogProbMetric: 21.2151 - val_loss: 21.2853 - val_MinusLogProbMetric: 21.2853 - lr: 1.1111e-04 - 68s/epoch - 349ms/step
Epoch 138/1000
2023-09-26 22:34:50.883 
Epoch 138/1000 
	 loss: 21.0010, MinusLogProbMetric: 21.0010, val_loss: 21.9295, val_MinusLogProbMetric: 21.9295

Epoch 138: val_loss did not improve from 21.19811
196/196 - 72s - loss: 21.0010 - MinusLogProbMetric: 21.0010 - val_loss: 21.9295 - val_MinusLogProbMetric: 21.9295 - lr: 1.1111e-04 - 72s/epoch - 365ms/step
Epoch 139/1000
2023-09-26 22:36:04.936 
Epoch 139/1000 
	 loss: 21.0691, MinusLogProbMetric: 21.0691, val_loss: 21.1285, val_MinusLogProbMetric: 21.1285

Epoch 139: val_loss improved from 21.19811 to 21.12847, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 75s - loss: 21.0691 - MinusLogProbMetric: 21.0691 - val_loss: 21.1285 - val_MinusLogProbMetric: 21.1285 - lr: 1.1111e-04 - 75s/epoch - 383ms/step
Epoch 140/1000
2023-09-26 22:37:12.837 
Epoch 140/1000 
	 loss: 20.9036, MinusLogProbMetric: 20.9036, val_loss: 20.9542, val_MinusLogProbMetric: 20.9542

Epoch 140: val_loss improved from 21.12847 to 20.95419, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 68s - loss: 20.9036 - MinusLogProbMetric: 20.9036 - val_loss: 20.9542 - val_MinusLogProbMetric: 20.9542 - lr: 1.1111e-04 - 68s/epoch - 346ms/step
Epoch 141/1000
2023-09-26 22:38:28.013 
Epoch 141/1000 
	 loss: 20.8703, MinusLogProbMetric: 20.8703, val_loss: 21.6510, val_MinusLogProbMetric: 21.6510

Epoch 141: val_loss did not improve from 20.95419
196/196 - 74s - loss: 20.8703 - MinusLogProbMetric: 20.8703 - val_loss: 21.6510 - val_MinusLogProbMetric: 21.6510 - lr: 1.1111e-04 - 74s/epoch - 378ms/step
Epoch 142/1000
2023-09-26 22:39:45.334 
Epoch 142/1000 
	 loss: 20.8893, MinusLogProbMetric: 20.8893, val_loss: 20.9213, val_MinusLogProbMetric: 20.9213

Epoch 142: val_loss improved from 20.95419 to 20.92135, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 79s - loss: 20.8893 - MinusLogProbMetric: 20.8893 - val_loss: 20.9213 - val_MinusLogProbMetric: 20.9213 - lr: 1.1111e-04 - 79s/epoch - 401ms/step
Epoch 143/1000
2023-09-26 22:41:04.356 
Epoch 143/1000 
	 loss: 20.9250, MinusLogProbMetric: 20.9250, val_loss: 21.2667, val_MinusLogProbMetric: 21.2667

Epoch 143: val_loss did not improve from 20.92135
196/196 - 78s - loss: 20.9250 - MinusLogProbMetric: 20.9250 - val_loss: 21.2667 - val_MinusLogProbMetric: 21.2667 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 144/1000
2023-09-26 22:42:21.611 
Epoch 144/1000 
	 loss: 21.0180, MinusLogProbMetric: 21.0180, val_loss: 21.2294, val_MinusLogProbMetric: 21.2294

Epoch 144: val_loss did not improve from 20.92135
196/196 - 77s - loss: 21.0180 - MinusLogProbMetric: 21.0180 - val_loss: 21.2294 - val_MinusLogProbMetric: 21.2294 - lr: 1.1111e-04 - 77s/epoch - 394ms/step
Epoch 145/1000
2023-09-26 22:43:39.357 
Epoch 145/1000 
	 loss: 20.8117, MinusLogProbMetric: 20.8117, val_loss: 21.1126, val_MinusLogProbMetric: 21.1126

Epoch 145: val_loss did not improve from 20.92135
196/196 - 78s - loss: 20.8117 - MinusLogProbMetric: 20.8117 - val_loss: 21.1126 - val_MinusLogProbMetric: 21.1126 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 146/1000
2023-09-26 22:44:56.610 
Epoch 146/1000 
	 loss: 20.7679, MinusLogProbMetric: 20.7679, val_loss: 20.8989, val_MinusLogProbMetric: 20.8989

Epoch 146: val_loss improved from 20.92135 to 20.89885, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 78s - loss: 20.7679 - MinusLogProbMetric: 20.7679 - val_loss: 20.8989 - val_MinusLogProbMetric: 20.8989 - lr: 1.1111e-04 - 78s/epoch - 400ms/step
Epoch 147/1000
2023-09-26 22:46:15.111 
Epoch 147/1000 
	 loss: 20.9830, MinusLogProbMetric: 20.9830, val_loss: 21.0897, val_MinusLogProbMetric: 21.0897

Epoch 147: val_loss did not improve from 20.89885
196/196 - 77s - loss: 20.9830 - MinusLogProbMetric: 20.9830 - val_loss: 21.0897 - val_MinusLogProbMetric: 21.0897 - lr: 1.1111e-04 - 77s/epoch - 394ms/step
Epoch 148/1000
2023-09-26 22:47:33.039 
Epoch 148/1000 
	 loss: 20.7130, MinusLogProbMetric: 20.7130, val_loss: 20.8766, val_MinusLogProbMetric: 20.8766

Epoch 148: val_loss improved from 20.89885 to 20.87656, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 79s - loss: 20.7130 - MinusLogProbMetric: 20.7130 - val_loss: 20.8766 - val_MinusLogProbMetric: 20.8766 - lr: 1.1111e-04 - 79s/epoch - 404ms/step
Epoch 149/1000
2023-09-26 22:48:51.967 
Epoch 149/1000 
	 loss: 20.6803, MinusLogProbMetric: 20.6803, val_loss: 20.8533, val_MinusLogProbMetric: 20.8533

Epoch 149: val_loss improved from 20.87656 to 20.85332, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 79s - loss: 20.6803 - MinusLogProbMetric: 20.6803 - val_loss: 20.8533 - val_MinusLogProbMetric: 20.8533 - lr: 1.1111e-04 - 79s/epoch - 402ms/step
Epoch 150/1000
2023-09-26 22:50:09.822 
Epoch 150/1000 
	 loss: 20.6909, MinusLogProbMetric: 20.6909, val_loss: 20.7491, val_MinusLogProbMetric: 20.7491

Epoch 150: val_loss improved from 20.85332 to 20.74912, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 78s - loss: 20.6909 - MinusLogProbMetric: 20.6909 - val_loss: 20.7491 - val_MinusLogProbMetric: 20.7491 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 151/1000
2023-09-26 22:51:28.589 
Epoch 151/1000 
	 loss: 20.6304, MinusLogProbMetric: 20.6304, val_loss: 20.6191, val_MinusLogProbMetric: 20.6191

Epoch 151: val_loss improved from 20.74912 to 20.61913, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 79s - loss: 20.6304 - MinusLogProbMetric: 20.6304 - val_loss: 20.6191 - val_MinusLogProbMetric: 20.6191 - lr: 1.1111e-04 - 79s/epoch - 401ms/step
Epoch 152/1000
2023-09-26 22:52:46.829 
Epoch 152/1000 
	 loss: 20.5864, MinusLogProbMetric: 20.5864, val_loss: 20.9528, val_MinusLogProbMetric: 20.9528

Epoch 152: val_loss did not improve from 20.61913
196/196 - 77s - loss: 20.5864 - MinusLogProbMetric: 20.5864 - val_loss: 20.9528 - val_MinusLogProbMetric: 20.9528 - lr: 1.1111e-04 - 77s/epoch - 393ms/step
Epoch 153/1000
2023-09-26 22:54:04.413 
Epoch 153/1000 
	 loss: 20.5761, MinusLogProbMetric: 20.5761, val_loss: 20.7227, val_MinusLogProbMetric: 20.7227

Epoch 153: val_loss did not improve from 20.61913
196/196 - 78s - loss: 20.5761 - MinusLogProbMetric: 20.5761 - val_loss: 20.7227 - val_MinusLogProbMetric: 20.7227 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 154/1000
2023-09-26 22:55:22.057 
Epoch 154/1000 
	 loss: 20.5108, MinusLogProbMetric: 20.5108, val_loss: 20.4333, val_MinusLogProbMetric: 20.4333

Epoch 154: val_loss improved from 20.61913 to 20.43335, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 79s - loss: 20.5108 - MinusLogProbMetric: 20.5108 - val_loss: 20.4333 - val_MinusLogProbMetric: 20.4333 - lr: 1.1111e-04 - 79s/epoch - 402ms/step
Epoch 155/1000
2023-09-26 22:56:40.725 
Epoch 155/1000 
	 loss: 20.5583, MinusLogProbMetric: 20.5583, val_loss: 20.7521, val_MinusLogProbMetric: 20.7521

Epoch 155: val_loss did not improve from 20.43335
196/196 - 78s - loss: 20.5583 - MinusLogProbMetric: 20.5583 - val_loss: 20.7521 - val_MinusLogProbMetric: 20.7521 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 156/1000
2023-09-26 22:57:57.981 
Epoch 156/1000 
	 loss: 20.5066, MinusLogProbMetric: 20.5066, val_loss: 20.7115, val_MinusLogProbMetric: 20.7115

Epoch 156: val_loss did not improve from 20.43335
196/196 - 77s - loss: 20.5066 - MinusLogProbMetric: 20.5066 - val_loss: 20.7115 - val_MinusLogProbMetric: 20.7115 - lr: 1.1111e-04 - 77s/epoch - 394ms/step
Epoch 157/1000
2023-09-26 22:59:15.378 
Epoch 157/1000 
	 loss: 20.4541, MinusLogProbMetric: 20.4541, val_loss: 20.9326, val_MinusLogProbMetric: 20.9326

Epoch 157: val_loss did not improve from 20.43335
196/196 - 77s - loss: 20.4541 - MinusLogProbMetric: 20.4541 - val_loss: 20.9326 - val_MinusLogProbMetric: 20.9326 - lr: 1.1111e-04 - 77s/epoch - 395ms/step
Epoch 158/1000
2023-09-26 23:00:32.300 
Epoch 158/1000 
	 loss: 20.4314, MinusLogProbMetric: 20.4314, val_loss: 20.8425, val_MinusLogProbMetric: 20.8425

Epoch 158: val_loss did not improve from 20.43335
196/196 - 77s - loss: 20.4314 - MinusLogProbMetric: 20.4314 - val_loss: 20.8425 - val_MinusLogProbMetric: 20.8425 - lr: 1.1111e-04 - 77s/epoch - 392ms/step
Epoch 159/1000
2023-09-26 23:01:49.435 
Epoch 159/1000 
	 loss: 20.4358, MinusLogProbMetric: 20.4358, val_loss: 21.0627, val_MinusLogProbMetric: 21.0627

Epoch 159: val_loss did not improve from 20.43335
196/196 - 77s - loss: 20.4358 - MinusLogProbMetric: 20.4358 - val_loss: 21.0627 - val_MinusLogProbMetric: 21.0627 - lr: 1.1111e-04 - 77s/epoch - 394ms/step
Epoch 160/1000
2023-09-26 23:03:06.922 
Epoch 160/1000 
	 loss: 20.3995, MinusLogProbMetric: 20.3995, val_loss: 20.4887, val_MinusLogProbMetric: 20.4887

Epoch 160: val_loss did not improve from 20.43335
196/196 - 77s - loss: 20.3995 - MinusLogProbMetric: 20.3995 - val_loss: 20.4887 - val_MinusLogProbMetric: 20.4887 - lr: 1.1111e-04 - 77s/epoch - 395ms/step
Epoch 161/1000
2023-09-26 23:04:24.117 
Epoch 161/1000 
	 loss: 20.3700, MinusLogProbMetric: 20.3700, val_loss: 20.7496, val_MinusLogProbMetric: 20.7496

Epoch 161: val_loss did not improve from 20.43335
196/196 - 77s - loss: 20.3700 - MinusLogProbMetric: 20.3700 - val_loss: 20.7496 - val_MinusLogProbMetric: 20.7496 - lr: 1.1111e-04 - 77s/epoch - 394ms/step
Epoch 162/1000
2023-09-26 23:05:41.921 
Epoch 162/1000 
	 loss: 20.3312, MinusLogProbMetric: 20.3312, val_loss: 20.8648, val_MinusLogProbMetric: 20.8648

Epoch 162: val_loss did not improve from 20.43335
196/196 - 78s - loss: 20.3312 - MinusLogProbMetric: 20.3312 - val_loss: 20.8648 - val_MinusLogProbMetric: 20.8648 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 163/1000
2023-09-26 23:06:58.812 
Epoch 163/1000 
	 loss: 20.3347, MinusLogProbMetric: 20.3347, val_loss: 20.3003, val_MinusLogProbMetric: 20.3003

Epoch 163: val_loss improved from 20.43335 to 20.30026, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 78s - loss: 20.3347 - MinusLogProbMetric: 20.3347 - val_loss: 20.3003 - val_MinusLogProbMetric: 20.3003 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 164/1000
2023-09-26 23:08:17.685 
Epoch 164/1000 
	 loss: 20.4169, MinusLogProbMetric: 20.4169, val_loss: 20.8162, val_MinusLogProbMetric: 20.8162

Epoch 164: val_loss did not improve from 20.30026
196/196 - 78s - loss: 20.4169 - MinusLogProbMetric: 20.4169 - val_loss: 20.8162 - val_MinusLogProbMetric: 20.8162 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 165/1000
2023-09-26 23:09:34.800 
Epoch 165/1000 
	 loss: 20.2907, MinusLogProbMetric: 20.2907, val_loss: 20.6890, val_MinusLogProbMetric: 20.6890

Epoch 165: val_loss did not improve from 20.30026
196/196 - 77s - loss: 20.2907 - MinusLogProbMetric: 20.2907 - val_loss: 20.6890 - val_MinusLogProbMetric: 20.6890 - lr: 1.1111e-04 - 77s/epoch - 393ms/step
Epoch 166/1000
2023-09-26 23:10:52.469 
Epoch 166/1000 
	 loss: 20.3894, MinusLogProbMetric: 20.3894, val_loss: 20.9233, val_MinusLogProbMetric: 20.9233

Epoch 166: val_loss did not improve from 20.30026
196/196 - 78s - loss: 20.3894 - MinusLogProbMetric: 20.3894 - val_loss: 20.9233 - val_MinusLogProbMetric: 20.9233 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 167/1000
2023-09-26 23:12:09.159 
Epoch 167/1000 
	 loss: 20.2509, MinusLogProbMetric: 20.2509, val_loss: 20.5257, val_MinusLogProbMetric: 20.5257

Epoch 167: val_loss did not improve from 20.30026
196/196 - 77s - loss: 20.2509 - MinusLogProbMetric: 20.2509 - val_loss: 20.5257 - val_MinusLogProbMetric: 20.5257 - lr: 1.1111e-04 - 77s/epoch - 391ms/step
Epoch 168/1000
2023-09-26 23:13:26.215 
Epoch 168/1000 
	 loss: 20.2332, MinusLogProbMetric: 20.2332, val_loss: 20.3050, val_MinusLogProbMetric: 20.3050

Epoch 168: val_loss did not improve from 20.30026
196/196 - 77s - loss: 20.2332 - MinusLogProbMetric: 20.2332 - val_loss: 20.3050 - val_MinusLogProbMetric: 20.3050 - lr: 1.1111e-04 - 77s/epoch - 393ms/step
Epoch 169/1000
2023-09-26 23:14:43.195 
Epoch 169/1000 
	 loss: 20.2125, MinusLogProbMetric: 20.2125, val_loss: 20.8786, val_MinusLogProbMetric: 20.8786

Epoch 169: val_loss did not improve from 20.30026
196/196 - 77s - loss: 20.2125 - MinusLogProbMetric: 20.2125 - val_loss: 20.8786 - val_MinusLogProbMetric: 20.8786 - lr: 1.1111e-04 - 77s/epoch - 393ms/step
Epoch 170/1000
2023-09-26 23:16:00.729 
Epoch 170/1000 
	 loss: 20.2467, MinusLogProbMetric: 20.2467, val_loss: 22.2998, val_MinusLogProbMetric: 22.2998

Epoch 170: val_loss did not improve from 20.30026
196/196 - 78s - loss: 20.2467 - MinusLogProbMetric: 20.2467 - val_loss: 22.2998 - val_MinusLogProbMetric: 22.2998 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 171/1000
2023-09-26 23:17:17.948 
Epoch 171/1000 
	 loss: 20.2586, MinusLogProbMetric: 20.2586, val_loss: 20.8593, val_MinusLogProbMetric: 20.8593

Epoch 171: val_loss did not improve from 20.30026
196/196 - 77s - loss: 20.2586 - MinusLogProbMetric: 20.2586 - val_loss: 20.8593 - val_MinusLogProbMetric: 20.8593 - lr: 1.1111e-04 - 77s/epoch - 394ms/step
Epoch 172/1000
2023-09-26 23:18:34.644 
Epoch 172/1000 
	 loss: 20.2749, MinusLogProbMetric: 20.2749, val_loss: 20.3676, val_MinusLogProbMetric: 20.3676

Epoch 172: val_loss did not improve from 20.30026
196/196 - 77s - loss: 20.2749 - MinusLogProbMetric: 20.2749 - val_loss: 20.3676 - val_MinusLogProbMetric: 20.3676 - lr: 1.1111e-04 - 77s/epoch - 391ms/step
Epoch 173/1000
2023-09-26 23:19:51.751 
Epoch 173/1000 
	 loss: 20.1771, MinusLogProbMetric: 20.1771, val_loss: 20.5964, val_MinusLogProbMetric: 20.5964

Epoch 173: val_loss did not improve from 20.30026
196/196 - 77s - loss: 20.1771 - MinusLogProbMetric: 20.1771 - val_loss: 20.5964 - val_MinusLogProbMetric: 20.5964 - lr: 1.1111e-04 - 77s/epoch - 393ms/step
Epoch 174/1000
2023-09-26 23:21:09.060 
Epoch 174/1000 
	 loss: 20.1593, MinusLogProbMetric: 20.1593, val_loss: 20.1863, val_MinusLogProbMetric: 20.1863

Epoch 174: val_loss improved from 20.30026 to 20.18629, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 79s - loss: 20.1593 - MinusLogProbMetric: 20.1593 - val_loss: 20.1863 - val_MinusLogProbMetric: 20.1863 - lr: 1.1111e-04 - 79s/epoch - 401ms/step
Epoch 175/1000
2023-09-26 23:22:27.983 
Epoch 175/1000 
	 loss: 20.0402, MinusLogProbMetric: 20.0402, val_loss: 20.1443, val_MinusLogProbMetric: 20.1443

Epoch 175: val_loss improved from 20.18629 to 20.14434, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 79s - loss: 20.0402 - MinusLogProbMetric: 20.0402 - val_loss: 20.1443 - val_MinusLogProbMetric: 20.1443 - lr: 1.1111e-04 - 79s/epoch - 402ms/step
Epoch 176/1000
2023-09-26 23:23:46.104 
Epoch 176/1000 
	 loss: 20.0742, MinusLogProbMetric: 20.0742, val_loss: 20.2560, val_MinusLogProbMetric: 20.2560

Epoch 176: val_loss did not improve from 20.14434
196/196 - 77s - loss: 20.0742 - MinusLogProbMetric: 20.0742 - val_loss: 20.2560 - val_MinusLogProbMetric: 20.2560 - lr: 1.1111e-04 - 77s/epoch - 393ms/step
Epoch 177/1000
2023-09-26 23:25:03.646 
Epoch 177/1000 
	 loss: 20.0335, MinusLogProbMetric: 20.0335, val_loss: 20.1474, val_MinusLogProbMetric: 20.1474

Epoch 177: val_loss did not improve from 20.14434
196/196 - 78s - loss: 20.0335 - MinusLogProbMetric: 20.0335 - val_loss: 20.1474 - val_MinusLogProbMetric: 20.1474 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 178/1000
2023-09-26 23:26:20.779 
Epoch 178/1000 
	 loss: 20.0335, MinusLogProbMetric: 20.0335, val_loss: 20.4012, val_MinusLogProbMetric: 20.4012

Epoch 178: val_loss did not improve from 20.14434
196/196 - 77s - loss: 20.0335 - MinusLogProbMetric: 20.0335 - val_loss: 20.4012 - val_MinusLogProbMetric: 20.4012 - lr: 1.1111e-04 - 77s/epoch - 394ms/step
Epoch 179/1000
2023-09-26 23:27:38.366 
Epoch 179/1000 
	 loss: 20.0190, MinusLogProbMetric: 20.0190, val_loss: 20.3231, val_MinusLogProbMetric: 20.3231

Epoch 179: val_loss did not improve from 20.14434
196/196 - 78s - loss: 20.0190 - MinusLogProbMetric: 20.0190 - val_loss: 20.3231 - val_MinusLogProbMetric: 20.3231 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 180/1000
2023-09-26 23:28:55.623 
Epoch 180/1000 
	 loss: 20.0867, MinusLogProbMetric: 20.0867, val_loss: 20.1017, val_MinusLogProbMetric: 20.1017

Epoch 180: val_loss improved from 20.14434 to 20.10169, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 78s - loss: 20.0867 - MinusLogProbMetric: 20.0867 - val_loss: 20.1017 - val_MinusLogProbMetric: 20.1017 - lr: 1.1111e-04 - 78s/epoch - 400ms/step
Epoch 181/1000
2023-09-26 23:30:14.589 
Epoch 181/1000 
	 loss: 19.9931, MinusLogProbMetric: 19.9931, val_loss: 20.2496, val_MinusLogProbMetric: 20.2496

Epoch 181: val_loss did not improve from 20.10169
196/196 - 78s - loss: 19.9931 - MinusLogProbMetric: 19.9931 - val_loss: 20.2496 - val_MinusLogProbMetric: 20.2496 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 182/1000
2023-09-26 23:31:32.091 
Epoch 182/1000 
	 loss: 19.9909, MinusLogProbMetric: 19.9909, val_loss: 20.1135, val_MinusLogProbMetric: 20.1135

Epoch 182: val_loss did not improve from 20.10169
196/196 - 77s - loss: 19.9909 - MinusLogProbMetric: 19.9909 - val_loss: 20.1135 - val_MinusLogProbMetric: 20.1135 - lr: 1.1111e-04 - 77s/epoch - 395ms/step
Epoch 183/1000
2023-09-26 23:32:50.030 
Epoch 183/1000 
	 loss: 19.9392, MinusLogProbMetric: 19.9392, val_loss: 20.1540, val_MinusLogProbMetric: 20.1540

Epoch 183: val_loss did not improve from 20.10169
196/196 - 78s - loss: 19.9392 - MinusLogProbMetric: 19.9392 - val_loss: 20.1540 - val_MinusLogProbMetric: 20.1540 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 184/1000
2023-09-26 23:34:07.987 
Epoch 184/1000 
	 loss: 19.9664, MinusLogProbMetric: 19.9664, val_loss: 20.1151, val_MinusLogProbMetric: 20.1151

Epoch 184: val_loss did not improve from 20.10169
196/196 - 78s - loss: 19.9664 - MinusLogProbMetric: 19.9664 - val_loss: 20.1151 - val_MinusLogProbMetric: 20.1151 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 185/1000
2023-09-26 23:35:25.601 
Epoch 185/1000 
	 loss: 19.8683, MinusLogProbMetric: 19.8683, val_loss: 20.0665, val_MinusLogProbMetric: 20.0665

Epoch 185: val_loss improved from 20.10169 to 20.06647, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 79s - loss: 19.8683 - MinusLogProbMetric: 19.8683 - val_loss: 20.0665 - val_MinusLogProbMetric: 20.0665 - lr: 1.1111e-04 - 79s/epoch - 403ms/step
Epoch 186/1000
2023-09-26 23:36:44.031 
Epoch 186/1000 
	 loss: 19.8894, MinusLogProbMetric: 19.8894, val_loss: 20.1462, val_MinusLogProbMetric: 20.1462

Epoch 186: val_loss did not improve from 20.06647
196/196 - 77s - loss: 19.8894 - MinusLogProbMetric: 19.8894 - val_loss: 20.1462 - val_MinusLogProbMetric: 20.1462 - lr: 1.1111e-04 - 77s/epoch - 393ms/step
Epoch 187/1000
2023-09-26 23:38:01.669 
Epoch 187/1000 
	 loss: 19.8562, MinusLogProbMetric: 19.8562, val_loss: 19.9324, val_MinusLogProbMetric: 19.9324

Epoch 187: val_loss improved from 20.06647 to 19.93236, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 79s - loss: 19.8562 - MinusLogProbMetric: 19.8562 - val_loss: 19.9324 - val_MinusLogProbMetric: 19.9324 - lr: 1.1111e-04 - 79s/epoch - 402ms/step
Epoch 188/1000
2023-09-26 23:39:19.525 
Epoch 188/1000 
	 loss: 19.8526, MinusLogProbMetric: 19.8526, val_loss: 19.9217, val_MinusLogProbMetric: 19.9217

Epoch 188: val_loss improved from 19.93236 to 19.92169, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 78s - loss: 19.8526 - MinusLogProbMetric: 19.8526 - val_loss: 19.9217 - val_MinusLogProbMetric: 19.9217 - lr: 1.1111e-04 - 78s/epoch - 399ms/step
Epoch 189/1000
2023-09-26 23:40:38.204 
Epoch 189/1000 
	 loss: 19.7831, MinusLogProbMetric: 19.7831, val_loss: 19.9861, val_MinusLogProbMetric: 19.9861

Epoch 189: val_loss did not improve from 19.92169
196/196 - 77s - loss: 19.7831 - MinusLogProbMetric: 19.7831 - val_loss: 19.9861 - val_MinusLogProbMetric: 19.9861 - lr: 1.1111e-04 - 77s/epoch - 394ms/step
Epoch 190/1000
2023-09-26 23:41:55.189 
Epoch 190/1000 
	 loss: 19.8033, MinusLogProbMetric: 19.8033, val_loss: 19.9383, val_MinusLogProbMetric: 19.9383

Epoch 190: val_loss did not improve from 19.92169
196/196 - 77s - loss: 19.8033 - MinusLogProbMetric: 19.8033 - val_loss: 19.9383 - val_MinusLogProbMetric: 19.9383 - lr: 1.1111e-04 - 77s/epoch - 393ms/step
Epoch 191/1000
2023-09-26 23:43:12.555 
Epoch 191/1000 
	 loss: 19.8608, MinusLogProbMetric: 19.8608, val_loss: 20.0596, val_MinusLogProbMetric: 20.0596

Epoch 191: val_loss did not improve from 19.92169
196/196 - 77s - loss: 19.8608 - MinusLogProbMetric: 19.8608 - val_loss: 20.0596 - val_MinusLogProbMetric: 20.0596 - lr: 1.1111e-04 - 77s/epoch - 395ms/step
Epoch 192/1000
2023-09-26 23:44:29.890 
Epoch 192/1000 
	 loss: 19.8287, MinusLogProbMetric: 19.8287, val_loss: 20.0329, val_MinusLogProbMetric: 20.0329

Epoch 192: val_loss did not improve from 19.92169
196/196 - 77s - loss: 19.8287 - MinusLogProbMetric: 19.8287 - val_loss: 20.0329 - val_MinusLogProbMetric: 20.0329 - lr: 1.1111e-04 - 77s/epoch - 395ms/step
Epoch 193/1000
2023-09-26 23:45:47.364 
Epoch 193/1000 
	 loss: 19.7607, MinusLogProbMetric: 19.7607, val_loss: 20.0209, val_MinusLogProbMetric: 20.0209

Epoch 193: val_loss did not improve from 19.92169
196/196 - 77s - loss: 19.7607 - MinusLogProbMetric: 19.7607 - val_loss: 20.0209 - val_MinusLogProbMetric: 20.0209 - lr: 1.1111e-04 - 77s/epoch - 395ms/step
Epoch 194/1000
2023-09-26 23:47:03.857 
Epoch 194/1000 
	 loss: 19.7453, MinusLogProbMetric: 19.7453, val_loss: 19.8737, val_MinusLogProbMetric: 19.8737

Epoch 194: val_loss improved from 19.92169 to 19.87371, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 78s - loss: 19.7453 - MinusLogProbMetric: 19.7453 - val_loss: 19.8737 - val_MinusLogProbMetric: 19.8737 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 195/1000
2023-09-26 23:48:22.384 
Epoch 195/1000 
	 loss: 19.7404, MinusLogProbMetric: 19.7404, val_loss: 19.8175, val_MinusLogProbMetric: 19.8175

Epoch 195: val_loss improved from 19.87371 to 19.81748, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 78s - loss: 19.7404 - MinusLogProbMetric: 19.7404 - val_loss: 19.8175 - val_MinusLogProbMetric: 19.8175 - lr: 1.1111e-04 - 78s/epoch - 400ms/step
Epoch 196/1000
2023-09-26 23:49:40.886 
Epoch 196/1000 
	 loss: 19.7630, MinusLogProbMetric: 19.7630, val_loss: 19.9395, val_MinusLogProbMetric: 19.9395

Epoch 196: val_loss did not improve from 19.81748
196/196 - 77s - loss: 19.7630 - MinusLogProbMetric: 19.7630 - val_loss: 19.9395 - val_MinusLogProbMetric: 19.9395 - lr: 1.1111e-04 - 77s/epoch - 395ms/step
Epoch 197/1000
2023-09-26 23:50:58.074 
Epoch 197/1000 
	 loss: 19.7603, MinusLogProbMetric: 19.7603, val_loss: 19.9126, val_MinusLogProbMetric: 19.9126

Epoch 197: val_loss did not improve from 19.81748
196/196 - 77s - loss: 19.7603 - MinusLogProbMetric: 19.7603 - val_loss: 19.9126 - val_MinusLogProbMetric: 19.9126 - lr: 1.1111e-04 - 77s/epoch - 394ms/step
Epoch 198/1000
2023-09-26 23:52:15.465 
Epoch 198/1000 
	 loss: 19.6856, MinusLogProbMetric: 19.6856, val_loss: 20.0706, val_MinusLogProbMetric: 20.0706

Epoch 198: val_loss did not improve from 19.81748
196/196 - 77s - loss: 19.6856 - MinusLogProbMetric: 19.6856 - val_loss: 20.0706 - val_MinusLogProbMetric: 20.0706 - lr: 1.1111e-04 - 77s/epoch - 395ms/step
Epoch 199/1000
2023-09-26 23:53:32.189 
Epoch 199/1000 
	 loss: 19.6362, MinusLogProbMetric: 19.6362, val_loss: 19.7771, val_MinusLogProbMetric: 19.7771

Epoch 199: val_loss improved from 19.81748 to 19.77711, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 78s - loss: 19.6362 - MinusLogProbMetric: 19.6362 - val_loss: 19.7771 - val_MinusLogProbMetric: 19.7771 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 200/1000
2023-09-26 23:54:49.914 
Epoch 200/1000 
	 loss: 19.7113, MinusLogProbMetric: 19.7113, val_loss: 19.8545, val_MinusLogProbMetric: 19.8545

Epoch 200: val_loss did not improve from 19.77711
196/196 - 77s - loss: 19.7113 - MinusLogProbMetric: 19.7113 - val_loss: 19.8545 - val_MinusLogProbMetric: 19.8545 - lr: 1.1111e-04 - 77s/epoch - 391ms/step
Epoch 201/1000
2023-09-26 23:56:06.914 
Epoch 201/1000 
	 loss: 19.6145, MinusLogProbMetric: 19.6145, val_loss: 19.9547, val_MinusLogProbMetric: 19.9547

Epoch 201: val_loss did not improve from 19.77711
196/196 - 77s - loss: 19.6145 - MinusLogProbMetric: 19.6145 - val_loss: 19.9547 - val_MinusLogProbMetric: 19.9547 - lr: 1.1111e-04 - 77s/epoch - 393ms/step
Epoch 202/1000
2023-09-26 23:57:24.497 
Epoch 202/1000 
	 loss: 19.6068, MinusLogProbMetric: 19.6068, val_loss: 19.8759, val_MinusLogProbMetric: 19.8759

Epoch 202: val_loss did not improve from 19.77711
196/196 - 78s - loss: 19.6068 - MinusLogProbMetric: 19.6068 - val_loss: 19.8759 - val_MinusLogProbMetric: 19.8759 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 203/1000
2023-09-26 23:58:41.133 
Epoch 203/1000 
	 loss: 19.6219, MinusLogProbMetric: 19.6219, val_loss: 19.8010, val_MinusLogProbMetric: 19.8010

Epoch 203: val_loss did not improve from 19.77711
196/196 - 77s - loss: 19.6219 - MinusLogProbMetric: 19.6219 - val_loss: 19.8010 - val_MinusLogProbMetric: 19.8010 - lr: 1.1111e-04 - 77s/epoch - 391ms/step
Epoch 204/1000
2023-09-26 23:59:57.774 
Epoch 204/1000 
	 loss: 19.6891, MinusLogProbMetric: 19.6891, val_loss: 19.7772, val_MinusLogProbMetric: 19.7772

Epoch 204: val_loss did not improve from 19.77711
196/196 - 77s - loss: 19.6891 - MinusLogProbMetric: 19.6891 - val_loss: 19.7772 - val_MinusLogProbMetric: 19.7772 - lr: 1.1111e-04 - 77s/epoch - 391ms/step
Epoch 205/1000
2023-09-27 00:01:14.916 
Epoch 205/1000 
	 loss: 19.5780, MinusLogProbMetric: 19.5780, val_loss: 19.7476, val_MinusLogProbMetric: 19.7476

Epoch 205: val_loss improved from 19.77711 to 19.74758, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 78s - loss: 19.5780 - MinusLogProbMetric: 19.5780 - val_loss: 19.7476 - val_MinusLogProbMetric: 19.7476 - lr: 1.1111e-04 - 78s/epoch - 400ms/step
Epoch 206/1000
2023-09-27 00:02:32.896 
Epoch 206/1000 
	 loss: 19.5350, MinusLogProbMetric: 19.5350, val_loss: 19.6594, val_MinusLogProbMetric: 19.6594

Epoch 206: val_loss improved from 19.74758 to 19.65936, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 78s - loss: 19.5350 - MinusLogProbMetric: 19.5350 - val_loss: 19.6594 - val_MinusLogProbMetric: 19.6594 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 207/1000
2023-09-27 00:03:51.180 
Epoch 207/1000 
	 loss: 19.5734, MinusLogProbMetric: 19.5734, val_loss: 19.7824, val_MinusLogProbMetric: 19.7824

Epoch 207: val_loss did not improve from 19.65936
196/196 - 77s - loss: 19.5734 - MinusLogProbMetric: 19.5734 - val_loss: 19.7824 - val_MinusLogProbMetric: 19.7824 - lr: 1.1111e-04 - 77s/epoch - 393ms/step
Epoch 208/1000
2023-09-27 00:05:08.111 
Epoch 208/1000 
	 loss: 19.5556, MinusLogProbMetric: 19.5556, val_loss: 19.8570, val_MinusLogProbMetric: 19.8570

Epoch 208: val_loss did not improve from 19.65936
196/196 - 77s - loss: 19.5556 - MinusLogProbMetric: 19.5556 - val_loss: 19.8570 - val_MinusLogProbMetric: 19.8570 - lr: 1.1111e-04 - 77s/epoch - 392ms/step
Epoch 209/1000
2023-09-27 00:06:25.361 
Epoch 209/1000 
	 loss: 19.5841, MinusLogProbMetric: 19.5841, val_loss: 19.7077, val_MinusLogProbMetric: 19.7077

Epoch 209: val_loss did not improve from 19.65936
196/196 - 77s - loss: 19.5841 - MinusLogProbMetric: 19.5841 - val_loss: 19.7077 - val_MinusLogProbMetric: 19.7077 - lr: 1.1111e-04 - 77s/epoch - 394ms/step
Epoch 210/1000
2023-09-27 00:07:42.101 
Epoch 210/1000 
	 loss: 19.5248, MinusLogProbMetric: 19.5248, val_loss: 19.7373, val_MinusLogProbMetric: 19.7373

Epoch 210: val_loss did not improve from 19.65936
196/196 - 77s - loss: 19.5248 - MinusLogProbMetric: 19.5248 - val_loss: 19.7373 - val_MinusLogProbMetric: 19.7373 - lr: 1.1111e-04 - 77s/epoch - 392ms/step
Epoch 211/1000
2023-09-27 00:08:59.216 
Epoch 211/1000 
	 loss: 19.5481, MinusLogProbMetric: 19.5481, val_loss: 19.6385, val_MinusLogProbMetric: 19.6385

Epoch 211: val_loss improved from 19.65936 to 19.63855, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 78s - loss: 19.5481 - MinusLogProbMetric: 19.5481 - val_loss: 19.6385 - val_MinusLogProbMetric: 19.6385 - lr: 1.1111e-04 - 78s/epoch - 399ms/step
Epoch 212/1000
2023-09-27 00:10:17.019 
Epoch 212/1000 
	 loss: 20.3051, MinusLogProbMetric: 20.3051, val_loss: 19.6605, val_MinusLogProbMetric: 19.6605

Epoch 212: val_loss did not improve from 19.63855
196/196 - 77s - loss: 20.3051 - MinusLogProbMetric: 20.3051 - val_loss: 19.6605 - val_MinusLogProbMetric: 19.6605 - lr: 1.1111e-04 - 77s/epoch - 391ms/step
Epoch 213/1000
2023-09-27 00:11:33.989 
Epoch 213/1000 
	 loss: 19.4936, MinusLogProbMetric: 19.4936, val_loss: 19.7126, val_MinusLogProbMetric: 19.7126

Epoch 213: val_loss did not improve from 19.63855
196/196 - 77s - loss: 19.4936 - MinusLogProbMetric: 19.4936 - val_loss: 19.7126 - val_MinusLogProbMetric: 19.7126 - lr: 1.1111e-04 - 77s/epoch - 393ms/step
Epoch 214/1000
2023-09-27 00:12:50.690 
Epoch 214/1000 
	 loss: 19.4343, MinusLogProbMetric: 19.4343, val_loss: 19.6046, val_MinusLogProbMetric: 19.6046

Epoch 214: val_loss improved from 19.63855 to 19.60455, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 78s - loss: 19.4343 - MinusLogProbMetric: 19.4343 - val_loss: 19.6046 - val_MinusLogProbMetric: 19.6046 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 215/1000
2023-09-27 00:14:09.691 
Epoch 215/1000 
	 loss: 19.4333, MinusLogProbMetric: 19.4333, val_loss: 19.6466, val_MinusLogProbMetric: 19.6466

Epoch 215: val_loss did not improve from 19.60455
196/196 - 78s - loss: 19.4333 - MinusLogProbMetric: 19.4333 - val_loss: 19.6466 - val_MinusLogProbMetric: 19.6466 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 216/1000
2023-09-27 00:15:26.938 
Epoch 216/1000 
	 loss: 19.4214, MinusLogProbMetric: 19.4214, val_loss: 19.8582, val_MinusLogProbMetric: 19.8582

Epoch 216: val_loss did not improve from 19.60455
196/196 - 77s - loss: 19.4214 - MinusLogProbMetric: 19.4214 - val_loss: 19.8582 - val_MinusLogProbMetric: 19.8582 - lr: 1.1111e-04 - 77s/epoch - 394ms/step
Epoch 217/1000
2023-09-27 00:16:44.223 
Epoch 217/1000 
	 loss: 19.5034, MinusLogProbMetric: 19.5034, val_loss: 19.7001, val_MinusLogProbMetric: 19.7001

Epoch 217: val_loss did not improve from 19.60455
196/196 - 77s - loss: 19.5034 - MinusLogProbMetric: 19.5034 - val_loss: 19.7001 - val_MinusLogProbMetric: 19.7001 - lr: 1.1111e-04 - 77s/epoch - 394ms/step
Epoch 218/1000
2023-09-27 00:18:01.232 
Epoch 218/1000 
	 loss: 19.3750, MinusLogProbMetric: 19.3750, val_loss: 19.4739, val_MinusLogProbMetric: 19.4739

Epoch 218: val_loss improved from 19.60455 to 19.47390, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 78s - loss: 19.3750 - MinusLogProbMetric: 19.3750 - val_loss: 19.4739 - val_MinusLogProbMetric: 19.4739 - lr: 1.1111e-04 - 78s/epoch - 400ms/step
Epoch 219/1000
2023-09-27 00:19:19.697 
Epoch 219/1000 
	 loss: 19.3831, MinusLogProbMetric: 19.3831, val_loss: 19.5445, val_MinusLogProbMetric: 19.5445

Epoch 219: val_loss did not improve from 19.47390
196/196 - 77s - loss: 19.3831 - MinusLogProbMetric: 19.3831 - val_loss: 19.5445 - val_MinusLogProbMetric: 19.5445 - lr: 1.1111e-04 - 77s/epoch - 393ms/step
Epoch 220/1000
2023-09-27 00:20:36.280 
Epoch 220/1000 
	 loss: 19.3271, MinusLogProbMetric: 19.3271, val_loss: 19.7572, val_MinusLogProbMetric: 19.7572

Epoch 220: val_loss did not improve from 19.47390
196/196 - 77s - loss: 19.3271 - MinusLogProbMetric: 19.3271 - val_loss: 19.7572 - val_MinusLogProbMetric: 19.7572 - lr: 1.1111e-04 - 77s/epoch - 391ms/step
Epoch 221/1000
2023-09-27 00:21:53.411 
Epoch 221/1000 
	 loss: 19.4330, MinusLogProbMetric: 19.4330, val_loss: 19.7454, val_MinusLogProbMetric: 19.7454

Epoch 221: val_loss did not improve from 19.47390
196/196 - 77s - loss: 19.4330 - MinusLogProbMetric: 19.4330 - val_loss: 19.7454 - val_MinusLogProbMetric: 19.7454 - lr: 1.1111e-04 - 77s/epoch - 394ms/step
Epoch 222/1000
2023-09-27 00:23:10.294 
Epoch 222/1000 
	 loss: 19.3321, MinusLogProbMetric: 19.3321, val_loss: 19.5253, val_MinusLogProbMetric: 19.5253

Epoch 222: val_loss did not improve from 19.47390
196/196 - 77s - loss: 19.3321 - MinusLogProbMetric: 19.3321 - val_loss: 19.5253 - val_MinusLogProbMetric: 19.5253 - lr: 1.1111e-04 - 77s/epoch - 392ms/step
Epoch 223/1000
2023-09-27 00:24:22.663 
Epoch 223/1000 
	 loss: 19.3608, MinusLogProbMetric: 19.3608, val_loss: 19.4925, val_MinusLogProbMetric: 19.4925

Epoch 223: val_loss did not improve from 19.47390
196/196 - 72s - loss: 19.3608 - MinusLogProbMetric: 19.3608 - val_loss: 19.4925 - val_MinusLogProbMetric: 19.4925 - lr: 1.1111e-04 - 72s/epoch - 369ms/step
Epoch 224/1000
2023-09-27 00:25:28.636 
Epoch 224/1000 
	 loss: 19.3610, MinusLogProbMetric: 19.3610, val_loss: 19.5683, val_MinusLogProbMetric: 19.5683

Epoch 224: val_loss did not improve from 19.47390
196/196 - 66s - loss: 19.3610 - MinusLogProbMetric: 19.3610 - val_loss: 19.5683 - val_MinusLogProbMetric: 19.5683 - lr: 1.1111e-04 - 66s/epoch - 337ms/step
Epoch 225/1000
2023-09-27 00:26:46.828 
Epoch 225/1000 
	 loss: 19.3192, MinusLogProbMetric: 19.3192, val_loss: 19.6179, val_MinusLogProbMetric: 19.6179

Epoch 225: val_loss did not improve from 19.47390
196/196 - 78s - loss: 19.3192 - MinusLogProbMetric: 19.3192 - val_loss: 19.6179 - val_MinusLogProbMetric: 19.6179 - lr: 1.1111e-04 - 78s/epoch - 399ms/step
Epoch 226/1000
2023-09-27 00:28:06.242 
Epoch 226/1000 
	 loss: 19.3132, MinusLogProbMetric: 19.3132, val_loss: 19.5374, val_MinusLogProbMetric: 19.5374

Epoch 226: val_loss did not improve from 19.47390
196/196 - 79s - loss: 19.3132 - MinusLogProbMetric: 19.3132 - val_loss: 19.5374 - val_MinusLogProbMetric: 19.5374 - lr: 1.1111e-04 - 79s/epoch - 405ms/step
Epoch 227/1000
2023-09-27 00:29:25.762 
Epoch 227/1000 
	 loss: 19.3241, MinusLogProbMetric: 19.3241, val_loss: 19.4097, val_MinusLogProbMetric: 19.4097

Epoch 227: val_loss improved from 19.47390 to 19.40971, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 81s - loss: 19.3241 - MinusLogProbMetric: 19.3241 - val_loss: 19.4097 - val_MinusLogProbMetric: 19.4097 - lr: 1.1111e-04 - 81s/epoch - 413ms/step
Epoch 228/1000
2023-09-27 00:30:47.176 
Epoch 228/1000 
	 loss: 19.2872, MinusLogProbMetric: 19.2872, val_loss: 19.3729, val_MinusLogProbMetric: 19.3729

Epoch 228: val_loss improved from 19.40971 to 19.37288, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 81s - loss: 19.2872 - MinusLogProbMetric: 19.2872 - val_loss: 19.3729 - val_MinusLogProbMetric: 19.3729 - lr: 1.1111e-04 - 81s/epoch - 415ms/step
Epoch 229/1000
2023-09-27 00:32:08.594 
Epoch 229/1000 
	 loss: 19.2682, MinusLogProbMetric: 19.2682, val_loss: 19.6578, val_MinusLogProbMetric: 19.6578

Epoch 229: val_loss did not improve from 19.37288
196/196 - 80s - loss: 19.2682 - MinusLogProbMetric: 19.2682 - val_loss: 19.6578 - val_MinusLogProbMetric: 19.6578 - lr: 1.1111e-04 - 80s/epoch - 409ms/step
Epoch 230/1000
2023-09-27 00:33:28.045 
Epoch 230/1000 
	 loss: 19.3094, MinusLogProbMetric: 19.3094, val_loss: 19.6168, val_MinusLogProbMetric: 19.6168

Epoch 230: val_loss did not improve from 19.37288
196/196 - 79s - loss: 19.3094 - MinusLogProbMetric: 19.3094 - val_loss: 19.6168 - val_MinusLogProbMetric: 19.6168 - lr: 1.1111e-04 - 79s/epoch - 405ms/step
Epoch 231/1000
2023-09-27 00:34:48.023 
Epoch 231/1000 
	 loss: 19.4372, MinusLogProbMetric: 19.4372, val_loss: 19.2931, val_MinusLogProbMetric: 19.2931

Epoch 231: val_loss improved from 19.37288 to 19.29315, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 82s - loss: 19.4372 - MinusLogProbMetric: 19.4372 - val_loss: 19.2931 - val_MinusLogProbMetric: 19.2931 - lr: 1.1111e-04 - 82s/epoch - 417ms/step
Epoch 232/1000
2023-09-27 00:36:08.610 
Epoch 232/1000 
	 loss: 19.2447, MinusLogProbMetric: 19.2447, val_loss: 19.3210, val_MinusLogProbMetric: 19.3210

Epoch 232: val_loss did not improve from 19.29315
196/196 - 79s - loss: 19.2447 - MinusLogProbMetric: 19.2447 - val_loss: 19.3210 - val_MinusLogProbMetric: 19.3210 - lr: 1.1111e-04 - 79s/epoch - 402ms/step
Epoch 233/1000
2023-09-27 00:37:27.404 
Epoch 233/1000 
	 loss: 19.2487, MinusLogProbMetric: 19.2487, val_loss: 19.7414, val_MinusLogProbMetric: 19.7414

Epoch 233: val_loss did not improve from 19.29315
196/196 - 79s - loss: 19.2487 - MinusLogProbMetric: 19.2487 - val_loss: 19.7414 - val_MinusLogProbMetric: 19.7414 - lr: 1.1111e-04 - 79s/epoch - 402ms/step
Epoch 234/1000
2023-09-27 00:38:45.588 
Epoch 234/1000 
	 loss: 19.2396, MinusLogProbMetric: 19.2396, val_loss: 19.3853, val_MinusLogProbMetric: 19.3853

Epoch 234: val_loss did not improve from 19.29315
196/196 - 78s - loss: 19.2396 - MinusLogProbMetric: 19.2396 - val_loss: 19.3853 - val_MinusLogProbMetric: 19.3853 - lr: 1.1111e-04 - 78s/epoch - 399ms/step
Epoch 235/1000
2023-09-27 00:40:03.250 
Epoch 235/1000 
	 loss: 19.2166, MinusLogProbMetric: 19.2166, val_loss: 20.3350, val_MinusLogProbMetric: 20.3350

Epoch 235: val_loss did not improve from 19.29315
196/196 - 78s - loss: 19.2166 - MinusLogProbMetric: 19.2166 - val_loss: 20.3350 - val_MinusLogProbMetric: 20.3350 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 236/1000
2023-09-27 00:41:21.814 
Epoch 236/1000 
	 loss: 19.2988, MinusLogProbMetric: 19.2988, val_loss: 19.6183, val_MinusLogProbMetric: 19.6183

Epoch 236: val_loss did not improve from 19.29315
196/196 - 79s - loss: 19.2988 - MinusLogProbMetric: 19.2988 - val_loss: 19.6183 - val_MinusLogProbMetric: 19.6183 - lr: 1.1111e-04 - 79s/epoch - 401ms/step
Epoch 237/1000
2023-09-27 00:42:39.538 
Epoch 237/1000 
	 loss: 19.2204, MinusLogProbMetric: 19.2204, val_loss: 19.4118, val_MinusLogProbMetric: 19.4118

Epoch 237: val_loss did not improve from 19.29315
196/196 - 78s - loss: 19.2204 - MinusLogProbMetric: 19.2204 - val_loss: 19.4118 - val_MinusLogProbMetric: 19.4118 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 238/1000
2023-09-27 00:43:57.562 
Epoch 238/1000 
	 loss: 19.1958, MinusLogProbMetric: 19.1958, val_loss: 19.4714, val_MinusLogProbMetric: 19.4714

Epoch 238: val_loss did not improve from 19.29315
196/196 - 78s - loss: 19.1958 - MinusLogProbMetric: 19.1958 - val_loss: 19.4714 - val_MinusLogProbMetric: 19.4714 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 239/1000
2023-09-27 00:45:15.802 
Epoch 239/1000 
	 loss: 19.1357, MinusLogProbMetric: 19.1357, val_loss: 19.4090, val_MinusLogProbMetric: 19.4090

Epoch 239: val_loss did not improve from 19.29315
196/196 - 78s - loss: 19.1357 - MinusLogProbMetric: 19.1357 - val_loss: 19.4090 - val_MinusLogProbMetric: 19.4090 - lr: 1.1111e-04 - 78s/epoch - 399ms/step
Epoch 240/1000
2023-09-27 00:46:34.189 
Epoch 240/1000 
	 loss: 19.1746, MinusLogProbMetric: 19.1746, val_loss: 19.2231, val_MinusLogProbMetric: 19.2231

Epoch 240: val_loss improved from 19.29315 to 19.22311, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 80s - loss: 19.1746 - MinusLogProbMetric: 19.1746 - val_loss: 19.2231 - val_MinusLogProbMetric: 19.2231 - lr: 1.1111e-04 - 80s/epoch - 409ms/step
Epoch 241/1000
2023-09-27 00:47:54.140 
Epoch 241/1000 
	 loss: 19.2153, MinusLogProbMetric: 19.2153, val_loss: 19.2302, val_MinusLogProbMetric: 19.2302

Epoch 241: val_loss did not improve from 19.22311
196/196 - 78s - loss: 19.2153 - MinusLogProbMetric: 19.2153 - val_loss: 19.2302 - val_MinusLogProbMetric: 19.2302 - lr: 1.1111e-04 - 78s/epoch - 399ms/step
Epoch 242/1000
2023-09-27 00:49:13.118 
Epoch 242/1000 
	 loss: 19.1632, MinusLogProbMetric: 19.1632, val_loss: 19.3096, val_MinusLogProbMetric: 19.3096

Epoch 242: val_loss did not improve from 19.22311
196/196 - 79s - loss: 19.1632 - MinusLogProbMetric: 19.1632 - val_loss: 19.3096 - val_MinusLogProbMetric: 19.3096 - lr: 1.1111e-04 - 79s/epoch - 403ms/step
Epoch 243/1000
2023-09-27 00:50:31.598 
Epoch 243/1000 
	 loss: 19.1389, MinusLogProbMetric: 19.1389, val_loss: 19.3465, val_MinusLogProbMetric: 19.3465

Epoch 243: val_loss did not improve from 19.22311
196/196 - 78s - loss: 19.1389 - MinusLogProbMetric: 19.1389 - val_loss: 19.3465 - val_MinusLogProbMetric: 19.3465 - lr: 1.1111e-04 - 78s/epoch - 400ms/step
Epoch 244/1000
2023-09-27 00:51:49.360 
Epoch 244/1000 
	 loss: 21.4454, MinusLogProbMetric: 21.4454, val_loss: 20.6790, val_MinusLogProbMetric: 20.6790

Epoch 244: val_loss did not improve from 19.22311
196/196 - 78s - loss: 21.4454 - MinusLogProbMetric: 21.4454 - val_loss: 20.6790 - val_MinusLogProbMetric: 20.6790 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 245/1000
2023-09-27 00:53:07.053 
Epoch 245/1000 
	 loss: 20.2561, MinusLogProbMetric: 20.2561, val_loss: 20.2676, val_MinusLogProbMetric: 20.2676

Epoch 245: val_loss did not improve from 19.22311
196/196 - 78s - loss: 20.2561 - MinusLogProbMetric: 20.2561 - val_loss: 20.2676 - val_MinusLogProbMetric: 20.2676 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 246/1000
2023-09-27 00:54:24.551 
Epoch 246/1000 
	 loss: 20.0077, MinusLogProbMetric: 20.0077, val_loss: 19.8738, val_MinusLogProbMetric: 19.8738

Epoch 246: val_loss did not improve from 19.22311
196/196 - 77s - loss: 20.0077 - MinusLogProbMetric: 20.0077 - val_loss: 19.8738 - val_MinusLogProbMetric: 19.8738 - lr: 1.1111e-04 - 77s/epoch - 395ms/step
Epoch 247/1000
2023-09-27 00:55:42.400 
Epoch 247/1000 
	 loss: 19.7761, MinusLogProbMetric: 19.7761, val_loss: 21.2080, val_MinusLogProbMetric: 21.2080

Epoch 247: val_loss did not improve from 19.22311
196/196 - 78s - loss: 19.7761 - MinusLogProbMetric: 19.7761 - val_loss: 21.2080 - val_MinusLogProbMetric: 21.2080 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 248/1000
2023-09-27 00:56:59.765 
Epoch 248/1000 
	 loss: 19.6716, MinusLogProbMetric: 19.6716, val_loss: 19.6550, val_MinusLogProbMetric: 19.6550

Epoch 248: val_loss did not improve from 19.22311
196/196 - 77s - loss: 19.6716 - MinusLogProbMetric: 19.6716 - val_loss: 19.6550 - val_MinusLogProbMetric: 19.6550 - lr: 1.1111e-04 - 77s/epoch - 395ms/step
Epoch 249/1000
2023-09-27 00:58:17.757 
Epoch 249/1000 
	 loss: 20.2167, MinusLogProbMetric: 20.2167, val_loss: 19.6396, val_MinusLogProbMetric: 19.6396

Epoch 249: val_loss did not improve from 19.22311
196/196 - 78s - loss: 20.2167 - MinusLogProbMetric: 20.2167 - val_loss: 19.6396 - val_MinusLogProbMetric: 19.6396 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 250/1000
2023-09-27 00:59:35.423 
Epoch 250/1000 
	 loss: 19.5316, MinusLogProbMetric: 19.5316, val_loss: 19.4544, val_MinusLogProbMetric: 19.4544

Epoch 250: val_loss did not improve from 19.22311
196/196 - 78s - loss: 19.5316 - MinusLogProbMetric: 19.5316 - val_loss: 19.4544 - val_MinusLogProbMetric: 19.4544 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 251/1000
2023-09-27 01:00:53.194 
Epoch 251/1000 
	 loss: 19.3656, MinusLogProbMetric: 19.3656, val_loss: 19.3909, val_MinusLogProbMetric: 19.3909

Epoch 251: val_loss did not improve from 19.22311
196/196 - 78s - loss: 19.3656 - MinusLogProbMetric: 19.3656 - val_loss: 19.3909 - val_MinusLogProbMetric: 19.3909 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 252/1000
2023-09-27 01:02:10.422 
Epoch 252/1000 
	 loss: 19.1855, MinusLogProbMetric: 19.1855, val_loss: 19.4883, val_MinusLogProbMetric: 19.4883

Epoch 252: val_loss did not improve from 19.22311
196/196 - 77s - loss: 19.1855 - MinusLogProbMetric: 19.1855 - val_loss: 19.4883 - val_MinusLogProbMetric: 19.4883 - lr: 1.1111e-04 - 77s/epoch - 394ms/step
Epoch 253/1000
2023-09-27 01:03:28.066 
Epoch 253/1000 
	 loss: 19.1483, MinusLogProbMetric: 19.1483, val_loss: 19.2951, val_MinusLogProbMetric: 19.2951

Epoch 253: val_loss did not improve from 19.22311
196/196 - 78s - loss: 19.1483 - MinusLogProbMetric: 19.1483 - val_loss: 19.2951 - val_MinusLogProbMetric: 19.2951 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 254/1000
2023-09-27 01:04:45.550 
Epoch 254/1000 
	 loss: 19.1335, MinusLogProbMetric: 19.1335, val_loss: 19.3399, val_MinusLogProbMetric: 19.3399

Epoch 254: val_loss did not improve from 19.22311
196/196 - 77s - loss: 19.1335 - MinusLogProbMetric: 19.1335 - val_loss: 19.3399 - val_MinusLogProbMetric: 19.3399 - lr: 1.1111e-04 - 77s/epoch - 395ms/step
Epoch 255/1000
2023-09-27 01:06:03.122 
Epoch 255/1000 
	 loss: 19.0942, MinusLogProbMetric: 19.0942, val_loss: 19.2872, val_MinusLogProbMetric: 19.2872

Epoch 255: val_loss did not improve from 19.22311
196/196 - 78s - loss: 19.0942 - MinusLogProbMetric: 19.0942 - val_loss: 19.2872 - val_MinusLogProbMetric: 19.2872 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 256/1000
2023-09-27 01:07:20.775 
Epoch 256/1000 
	 loss: 19.0718, MinusLogProbMetric: 19.0718, val_loss: 19.1078, val_MinusLogProbMetric: 19.1078

Epoch 256: val_loss improved from 19.22311 to 19.10776, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 79s - loss: 19.0718 - MinusLogProbMetric: 19.0718 - val_loss: 19.1078 - val_MinusLogProbMetric: 19.1078 - lr: 1.1111e-04 - 79s/epoch - 403ms/step
Epoch 257/1000
2023-09-27 01:08:39.669 
Epoch 257/1000 
	 loss: 19.0192, MinusLogProbMetric: 19.0192, val_loss: 19.1710, val_MinusLogProbMetric: 19.1710

Epoch 257: val_loss did not improve from 19.10776
196/196 - 78s - loss: 19.0192 - MinusLogProbMetric: 19.0192 - val_loss: 19.1710 - val_MinusLogProbMetric: 19.1710 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 258/1000
2023-09-27 01:09:57.518 
Epoch 258/1000 
	 loss: 19.0182, MinusLogProbMetric: 19.0182, val_loss: 19.2252, val_MinusLogProbMetric: 19.2252

Epoch 258: val_loss did not improve from 19.10776
196/196 - 78s - loss: 19.0182 - MinusLogProbMetric: 19.0182 - val_loss: 19.2252 - val_MinusLogProbMetric: 19.2252 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 259/1000
2023-09-27 01:11:14.965 
Epoch 259/1000 
	 loss: 19.0458, MinusLogProbMetric: 19.0458, val_loss: 19.1138, val_MinusLogProbMetric: 19.1138

Epoch 259: val_loss did not improve from 19.10776
196/196 - 77s - loss: 19.0458 - MinusLogProbMetric: 19.0458 - val_loss: 19.1138 - val_MinusLogProbMetric: 19.1138 - lr: 1.1111e-04 - 77s/epoch - 395ms/step
Epoch 260/1000
2023-09-27 01:12:32.385 
Epoch 260/1000 
	 loss: 19.0190, MinusLogProbMetric: 19.0190, val_loss: 19.2147, val_MinusLogProbMetric: 19.2147

Epoch 260: val_loss did not improve from 19.10776
196/196 - 77s - loss: 19.0190 - MinusLogProbMetric: 19.0190 - val_loss: 19.2147 - val_MinusLogProbMetric: 19.2147 - lr: 1.1111e-04 - 77s/epoch - 395ms/step
Epoch 261/1000
2023-09-27 01:13:50.085 
Epoch 261/1000 
	 loss: 19.0361, MinusLogProbMetric: 19.0361, val_loss: 19.3220, val_MinusLogProbMetric: 19.3220

Epoch 261: val_loss did not improve from 19.10776
196/196 - 78s - loss: 19.0361 - MinusLogProbMetric: 19.0361 - val_loss: 19.3220 - val_MinusLogProbMetric: 19.3220 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 262/1000
2023-09-27 01:15:07.730 
Epoch 262/1000 
	 loss: 18.9626, MinusLogProbMetric: 18.9626, val_loss: 19.2862, val_MinusLogProbMetric: 19.2862

Epoch 262: val_loss did not improve from 19.10776
196/196 - 78s - loss: 18.9626 - MinusLogProbMetric: 18.9626 - val_loss: 19.2862 - val_MinusLogProbMetric: 19.2862 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 263/1000
2023-09-27 01:16:25.451 
Epoch 263/1000 
	 loss: 19.0012, MinusLogProbMetric: 19.0012, val_loss: 19.1006, val_MinusLogProbMetric: 19.1006

Epoch 263: val_loss improved from 19.10776 to 19.10059, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 79s - loss: 19.0012 - MinusLogProbMetric: 19.0012 - val_loss: 19.1006 - val_MinusLogProbMetric: 19.1006 - lr: 1.1111e-04 - 79s/epoch - 406ms/step
Epoch 264/1000
2023-09-27 01:17:45.086 
Epoch 264/1000 
	 loss: 18.9461, MinusLogProbMetric: 18.9461, val_loss: 19.1095, val_MinusLogProbMetric: 19.1095

Epoch 264: val_loss did not improve from 19.10059
196/196 - 78s - loss: 18.9461 - MinusLogProbMetric: 18.9461 - val_loss: 19.1095 - val_MinusLogProbMetric: 19.1095 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 265/1000
2023-09-27 01:19:02.294 
Epoch 265/1000 
	 loss: 18.9918, MinusLogProbMetric: 18.9918, val_loss: 19.0860, val_MinusLogProbMetric: 19.0860

Epoch 265: val_loss improved from 19.10059 to 19.08595, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 79s - loss: 18.9918 - MinusLogProbMetric: 18.9918 - val_loss: 19.0860 - val_MinusLogProbMetric: 19.0860 - lr: 1.1111e-04 - 79s/epoch - 402ms/step
Epoch 266/1000
2023-09-27 01:20:21.306 
Epoch 266/1000 
	 loss: 19.0278, MinusLogProbMetric: 19.0278, val_loss: 19.0418, val_MinusLogProbMetric: 19.0418

Epoch 266: val_loss improved from 19.08595 to 19.04180, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 79s - loss: 19.0278 - MinusLogProbMetric: 19.0278 - val_loss: 19.0418 - val_MinusLogProbMetric: 19.0418 - lr: 1.1111e-04 - 79s/epoch - 402ms/step
Epoch 267/1000
2023-09-27 01:21:39.912 
Epoch 267/1000 
	 loss: 18.9220, MinusLogProbMetric: 18.9220, val_loss: 19.2310, val_MinusLogProbMetric: 19.2310

Epoch 267: val_loss did not improve from 19.04180
196/196 - 77s - loss: 18.9220 - MinusLogProbMetric: 18.9220 - val_loss: 19.2310 - val_MinusLogProbMetric: 19.2310 - lr: 1.1111e-04 - 77s/epoch - 394ms/step
Epoch 268/1000
2023-09-27 01:22:57.271 
Epoch 268/1000 
	 loss: 18.9583, MinusLogProbMetric: 18.9583, val_loss: 19.1759, val_MinusLogProbMetric: 19.1759

Epoch 268: val_loss did not improve from 19.04180
196/196 - 77s - loss: 18.9583 - MinusLogProbMetric: 18.9583 - val_loss: 19.1759 - val_MinusLogProbMetric: 19.1759 - lr: 1.1111e-04 - 77s/epoch - 395ms/step
Epoch 269/1000
2023-09-27 01:24:14.643 
Epoch 269/1000 
	 loss: 18.9387, MinusLogProbMetric: 18.9387, val_loss: 19.1233, val_MinusLogProbMetric: 19.1233

Epoch 269: val_loss did not improve from 19.04180
196/196 - 77s - loss: 18.9387 - MinusLogProbMetric: 18.9387 - val_loss: 19.1233 - val_MinusLogProbMetric: 19.1233 - lr: 1.1111e-04 - 77s/epoch - 395ms/step
Epoch 270/1000
2023-09-27 01:25:32.448 
Epoch 270/1000 
	 loss: 18.8791, MinusLogProbMetric: 18.8791, val_loss: 19.0181, val_MinusLogProbMetric: 19.0181

Epoch 270: val_loss improved from 19.04180 to 19.01810, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 79s - loss: 18.8791 - MinusLogProbMetric: 18.8791 - val_loss: 19.0181 - val_MinusLogProbMetric: 19.0181 - lr: 1.1111e-04 - 79s/epoch - 405ms/step
Epoch 271/1000
2023-09-27 01:26:51.776 
Epoch 271/1000 
	 loss: 18.8841, MinusLogProbMetric: 18.8841, val_loss: 19.3261, val_MinusLogProbMetric: 19.3261

Epoch 271: val_loss did not improve from 19.01810
196/196 - 78s - loss: 18.8841 - MinusLogProbMetric: 18.8841 - val_loss: 19.3261 - val_MinusLogProbMetric: 19.3261 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 272/1000
2023-09-27 01:28:09.480 
Epoch 272/1000 
	 loss: 18.8838, MinusLogProbMetric: 18.8838, val_loss: 19.3764, val_MinusLogProbMetric: 19.3764

Epoch 272: val_loss did not improve from 19.01810
196/196 - 78s - loss: 18.8838 - MinusLogProbMetric: 18.8838 - val_loss: 19.3764 - val_MinusLogProbMetric: 19.3764 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 273/1000
2023-09-27 01:29:26.930 
Epoch 273/1000 
	 loss: 18.8891, MinusLogProbMetric: 18.8891, val_loss: 19.1556, val_MinusLogProbMetric: 19.1556

Epoch 273: val_loss did not improve from 19.01810
196/196 - 77s - loss: 18.8891 - MinusLogProbMetric: 18.8891 - val_loss: 19.1556 - val_MinusLogProbMetric: 19.1556 - lr: 1.1111e-04 - 77s/epoch - 395ms/step
Epoch 274/1000
2023-09-27 01:30:44.555 
Epoch 274/1000 
	 loss: 18.8551, MinusLogProbMetric: 18.8551, val_loss: 19.1471, val_MinusLogProbMetric: 19.1471

Epoch 274: val_loss did not improve from 19.01810
196/196 - 78s - loss: 18.8551 - MinusLogProbMetric: 18.8551 - val_loss: 19.1471 - val_MinusLogProbMetric: 19.1471 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 275/1000
2023-09-27 01:32:01.985 
Epoch 275/1000 
	 loss: 18.8317, MinusLogProbMetric: 18.8317, val_loss: 19.0887, val_MinusLogProbMetric: 19.0887

Epoch 275: val_loss did not improve from 19.01810
196/196 - 77s - loss: 18.8317 - MinusLogProbMetric: 18.8317 - val_loss: 19.0887 - val_MinusLogProbMetric: 19.0887 - lr: 1.1111e-04 - 77s/epoch - 395ms/step
Epoch 276/1000
2023-09-27 01:33:20.113 
Epoch 276/1000 
	 loss: 18.9000, MinusLogProbMetric: 18.9000, val_loss: 19.1909, val_MinusLogProbMetric: 19.1909

Epoch 276: val_loss did not improve from 19.01810
196/196 - 78s - loss: 18.9000 - MinusLogProbMetric: 18.9000 - val_loss: 19.1909 - val_MinusLogProbMetric: 19.1909 - lr: 1.1111e-04 - 78s/epoch - 399ms/step
Epoch 277/1000
2023-09-27 01:34:38.103 
Epoch 277/1000 
	 loss: 18.8796, MinusLogProbMetric: 18.8796, val_loss: 19.4190, val_MinusLogProbMetric: 19.4190

Epoch 277: val_loss did not improve from 19.01810
196/196 - 78s - loss: 18.8796 - MinusLogProbMetric: 18.8796 - val_loss: 19.4190 - val_MinusLogProbMetric: 19.4190 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 278/1000
2023-09-27 01:35:55.892 
Epoch 278/1000 
	 loss: 18.8585, MinusLogProbMetric: 18.8585, val_loss: 18.9473, val_MinusLogProbMetric: 18.9473

Epoch 278: val_loss improved from 19.01810 to 18.94731, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 79s - loss: 18.8585 - MinusLogProbMetric: 18.8585 - val_loss: 18.9473 - val_MinusLogProbMetric: 18.9473 - lr: 1.1111e-04 - 79s/epoch - 404ms/step
Epoch 279/1000
2023-09-27 01:37:14.423 
Epoch 279/1000 
	 loss: 18.8467, MinusLogProbMetric: 18.8467, val_loss: 19.0946, val_MinusLogProbMetric: 19.0946

Epoch 279: val_loss did not improve from 18.94731
196/196 - 77s - loss: 18.8467 - MinusLogProbMetric: 18.8467 - val_loss: 19.0946 - val_MinusLogProbMetric: 19.0946 - lr: 1.1111e-04 - 77s/epoch - 393ms/step
Epoch 280/1000
2023-09-27 01:38:31.942 
Epoch 280/1000 
	 loss: 18.8221, MinusLogProbMetric: 18.8221, val_loss: 18.9279, val_MinusLogProbMetric: 18.9279

Epoch 280: val_loss improved from 18.94731 to 18.92794, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 79s - loss: 18.8221 - MinusLogProbMetric: 18.8221 - val_loss: 18.9279 - val_MinusLogProbMetric: 18.9279 - lr: 1.1111e-04 - 79s/epoch - 403ms/step
Epoch 281/1000
2023-09-27 01:39:50.725 
Epoch 281/1000 
	 loss: 18.8220, MinusLogProbMetric: 18.8220, val_loss: 19.0437, val_MinusLogProbMetric: 19.0437

Epoch 281: val_loss did not improve from 18.92794
196/196 - 77s - loss: 18.8220 - MinusLogProbMetric: 18.8220 - val_loss: 19.0437 - val_MinusLogProbMetric: 19.0437 - lr: 1.1111e-04 - 77s/epoch - 395ms/step
Epoch 282/1000
2023-09-27 01:41:08.728 
Epoch 282/1000 
	 loss: 18.8630, MinusLogProbMetric: 18.8630, val_loss: 19.2013, val_MinusLogProbMetric: 19.2013

Epoch 282: val_loss did not improve from 18.92794
196/196 - 78s - loss: 18.8630 - MinusLogProbMetric: 18.8630 - val_loss: 19.2013 - val_MinusLogProbMetric: 19.2013 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 283/1000
2023-09-27 01:42:26.288 
Epoch 283/1000 
	 loss: 18.8592, MinusLogProbMetric: 18.8592, val_loss: 18.9724, val_MinusLogProbMetric: 18.9724

Epoch 283: val_loss did not improve from 18.92794
196/196 - 78s - loss: 18.8592 - MinusLogProbMetric: 18.8592 - val_loss: 18.9724 - val_MinusLogProbMetric: 18.9724 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 284/1000
2023-09-27 01:43:43.491 
Epoch 284/1000 
	 loss: 18.7930, MinusLogProbMetric: 18.7930, val_loss: 18.8951, val_MinusLogProbMetric: 18.8951

Epoch 284: val_loss improved from 18.92794 to 18.89507, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 79s - loss: 18.7930 - MinusLogProbMetric: 18.7930 - val_loss: 18.8951 - val_MinusLogProbMetric: 18.8951 - lr: 1.1111e-04 - 79s/epoch - 402ms/step
Epoch 285/1000
2023-09-27 01:45:03.139 
Epoch 285/1000 
	 loss: 18.7661, MinusLogProbMetric: 18.7661, val_loss: 19.0417, val_MinusLogProbMetric: 19.0417

Epoch 285: val_loss did not improve from 18.89507
196/196 - 78s - loss: 18.7661 - MinusLogProbMetric: 18.7661 - val_loss: 19.0417 - val_MinusLogProbMetric: 19.0417 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 286/1000
2023-09-27 01:46:21.388 
Epoch 286/1000 
	 loss: 18.7593, MinusLogProbMetric: 18.7593, val_loss: 19.2138, val_MinusLogProbMetric: 19.2138

Epoch 286: val_loss did not improve from 18.89507
196/196 - 78s - loss: 18.7593 - MinusLogProbMetric: 18.7593 - val_loss: 19.2138 - val_MinusLogProbMetric: 19.2138 - lr: 1.1111e-04 - 78s/epoch - 399ms/step
Epoch 287/1000
2023-09-27 01:47:39.047 
Epoch 287/1000 
	 loss: 18.7703, MinusLogProbMetric: 18.7703, val_loss: 18.9566, val_MinusLogProbMetric: 18.9566

Epoch 287: val_loss did not improve from 18.89507
196/196 - 78s - loss: 18.7703 - MinusLogProbMetric: 18.7703 - val_loss: 18.9566 - val_MinusLogProbMetric: 18.9566 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 288/1000
2023-09-27 01:48:56.962 
Epoch 288/1000 
	 loss: 18.7720, MinusLogProbMetric: 18.7720, val_loss: 18.9302, val_MinusLogProbMetric: 18.9302

Epoch 288: val_loss did not improve from 18.89507
196/196 - 78s - loss: 18.7720 - MinusLogProbMetric: 18.7720 - val_loss: 18.9302 - val_MinusLogProbMetric: 18.9302 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 289/1000
2023-09-27 01:50:14.259 
Epoch 289/1000 
	 loss: 18.8014, MinusLogProbMetric: 18.8014, val_loss: 19.0839, val_MinusLogProbMetric: 19.0839

Epoch 289: val_loss did not improve from 18.89507
196/196 - 77s - loss: 18.8014 - MinusLogProbMetric: 18.8014 - val_loss: 19.0839 - val_MinusLogProbMetric: 19.0839 - lr: 1.1111e-04 - 77s/epoch - 394ms/step
Epoch 290/1000
2023-09-27 01:51:31.309 
Epoch 290/1000 
	 loss: 19.0545, MinusLogProbMetric: 19.0545, val_loss: 19.2072, val_MinusLogProbMetric: 19.2072

Epoch 290: val_loss did not improve from 18.89507
196/196 - 77s - loss: 19.0545 - MinusLogProbMetric: 19.0545 - val_loss: 19.2072 - val_MinusLogProbMetric: 19.2072 - lr: 1.1111e-04 - 77s/epoch - 393ms/step
Epoch 291/1000
2023-09-27 01:52:49.051 
Epoch 291/1000 
	 loss: 18.7482, MinusLogProbMetric: 18.7482, val_loss: 18.9057, val_MinusLogProbMetric: 18.9057

Epoch 291: val_loss did not improve from 18.89507
196/196 - 78s - loss: 18.7482 - MinusLogProbMetric: 18.7482 - val_loss: 18.9057 - val_MinusLogProbMetric: 18.9057 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 292/1000
2023-09-27 01:54:07.138 
Epoch 292/1000 
	 loss: 18.7373, MinusLogProbMetric: 18.7373, val_loss: 19.0460, val_MinusLogProbMetric: 19.0460

Epoch 292: val_loss did not improve from 18.89507
196/196 - 78s - loss: 18.7373 - MinusLogProbMetric: 18.7373 - val_loss: 19.0460 - val_MinusLogProbMetric: 19.0460 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 293/1000
2023-09-27 01:55:25.938 
Epoch 293/1000 
	 loss: 18.7897, MinusLogProbMetric: 18.7897, val_loss: 18.9439, val_MinusLogProbMetric: 18.9439

Epoch 293: val_loss did not improve from 18.89507
196/196 - 79s - loss: 18.7897 - MinusLogProbMetric: 18.7897 - val_loss: 18.9439 - val_MinusLogProbMetric: 18.9439 - lr: 1.1111e-04 - 79s/epoch - 402ms/step
Epoch 294/1000
2023-09-27 01:56:43.199 
Epoch 294/1000 
	 loss: 18.7204, MinusLogProbMetric: 18.7204, val_loss: 18.8434, val_MinusLogProbMetric: 18.8434

Epoch 294: val_loss improved from 18.89507 to 18.84340, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 79s - loss: 18.7204 - MinusLogProbMetric: 18.7204 - val_loss: 18.8434 - val_MinusLogProbMetric: 18.8434 - lr: 1.1111e-04 - 79s/epoch - 402ms/step
Epoch 295/1000
2023-09-27 01:58:01.817 
Epoch 295/1000 
	 loss: 18.7528, MinusLogProbMetric: 18.7528, val_loss: 18.8954, val_MinusLogProbMetric: 18.8954

Epoch 295: val_loss did not improve from 18.84340
196/196 - 77s - loss: 18.7528 - MinusLogProbMetric: 18.7528 - val_loss: 18.8954 - val_MinusLogProbMetric: 18.8954 - lr: 1.1111e-04 - 77s/epoch - 393ms/step
Epoch 296/1000
2023-09-27 01:59:19.851 
Epoch 296/1000 
	 loss: 18.7287, MinusLogProbMetric: 18.7287, val_loss: 18.9756, val_MinusLogProbMetric: 18.9756

Epoch 296: val_loss did not improve from 18.84340
196/196 - 78s - loss: 18.7287 - MinusLogProbMetric: 18.7287 - val_loss: 18.9756 - val_MinusLogProbMetric: 18.9756 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 297/1000
2023-09-27 02:00:37.732 
Epoch 297/1000 
	 loss: 18.7826, MinusLogProbMetric: 18.7826, val_loss: 19.0035, val_MinusLogProbMetric: 19.0035

Epoch 297: val_loss did not improve from 18.84340
196/196 - 78s - loss: 18.7826 - MinusLogProbMetric: 18.7826 - val_loss: 19.0035 - val_MinusLogProbMetric: 19.0035 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 298/1000
2023-09-27 02:01:55.708 
Epoch 298/1000 
	 loss: 18.8650, MinusLogProbMetric: 18.8650, val_loss: 19.0245, val_MinusLogProbMetric: 19.0245

Epoch 298: val_loss did not improve from 18.84340
196/196 - 78s - loss: 18.8650 - MinusLogProbMetric: 18.8650 - val_loss: 19.0245 - val_MinusLogProbMetric: 19.0245 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 299/1000
2023-09-27 02:03:13.730 
Epoch 299/1000 
	 loss: 18.7028, MinusLogProbMetric: 18.7028, val_loss: 18.9830, val_MinusLogProbMetric: 18.9830

Epoch 299: val_loss did not improve from 18.84340
196/196 - 78s - loss: 18.7028 - MinusLogProbMetric: 18.7028 - val_loss: 18.9830 - val_MinusLogProbMetric: 18.9830 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 300/1000
2023-09-27 02:04:31.816 
Epoch 300/1000 
	 loss: 18.8439, MinusLogProbMetric: 18.8439, val_loss: 19.9993, val_MinusLogProbMetric: 19.9993

Epoch 300: val_loss did not improve from 18.84340
196/196 - 78s - loss: 18.8439 - MinusLogProbMetric: 18.8439 - val_loss: 19.9993 - val_MinusLogProbMetric: 19.9993 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 301/1000
2023-09-27 02:05:49.353 
Epoch 301/1000 
	 loss: 18.7469, MinusLogProbMetric: 18.7469, val_loss: 18.9515, val_MinusLogProbMetric: 18.9515

Epoch 301: val_loss did not improve from 18.84340
196/196 - 78s - loss: 18.7469 - MinusLogProbMetric: 18.7469 - val_loss: 18.9515 - val_MinusLogProbMetric: 18.9515 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 302/1000
2023-09-27 02:07:07.031 
Epoch 302/1000 
	 loss: 18.8336, MinusLogProbMetric: 18.8336, val_loss: 19.0755, val_MinusLogProbMetric: 19.0755

Epoch 302: val_loss did not improve from 18.84340
196/196 - 78s - loss: 18.8336 - MinusLogProbMetric: 18.8336 - val_loss: 19.0755 - val_MinusLogProbMetric: 19.0755 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 303/1000
2023-09-27 02:08:24.795 
Epoch 303/1000 
	 loss: 18.7024, MinusLogProbMetric: 18.7024, val_loss: 18.8971, val_MinusLogProbMetric: 18.8971

Epoch 303: val_loss did not improve from 18.84340
196/196 - 78s - loss: 18.7024 - MinusLogProbMetric: 18.7024 - val_loss: 18.8971 - val_MinusLogProbMetric: 18.8971 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 304/1000
2023-09-27 02:09:43.072 
Epoch 304/1000 
	 loss: 18.6638, MinusLogProbMetric: 18.6638, val_loss: 18.8835, val_MinusLogProbMetric: 18.8835

Epoch 304: val_loss did not improve from 18.84340
196/196 - 78s - loss: 18.6638 - MinusLogProbMetric: 18.6638 - val_loss: 18.8835 - val_MinusLogProbMetric: 18.8835 - lr: 1.1111e-04 - 78s/epoch - 399ms/step
Epoch 305/1000
2023-09-27 02:11:01.189 
Epoch 305/1000 
	 loss: 18.6415, MinusLogProbMetric: 18.6415, val_loss: 18.7863, val_MinusLogProbMetric: 18.7863

Epoch 305: val_loss improved from 18.84340 to 18.78628, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 80s - loss: 18.6415 - MinusLogProbMetric: 18.6415 - val_loss: 18.7863 - val_MinusLogProbMetric: 18.7863 - lr: 1.1111e-04 - 80s/epoch - 407ms/step
Epoch 306/1000
2023-09-27 02:12:20.703 
Epoch 306/1000 
	 loss: 18.6282, MinusLogProbMetric: 18.6282, val_loss: 18.8356, val_MinusLogProbMetric: 18.8356

Epoch 306: val_loss did not improve from 18.78628
196/196 - 78s - loss: 18.6282 - MinusLogProbMetric: 18.6282 - val_loss: 18.8356 - val_MinusLogProbMetric: 18.8356 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 307/1000
2023-09-27 02:13:37.952 
Epoch 307/1000 
	 loss: 18.9598, MinusLogProbMetric: 18.9598, val_loss: 18.8856, val_MinusLogProbMetric: 18.8856

Epoch 307: val_loss did not improve from 18.78628
196/196 - 77s - loss: 18.9598 - MinusLogProbMetric: 18.9598 - val_loss: 18.8856 - val_MinusLogProbMetric: 18.8856 - lr: 1.1111e-04 - 77s/epoch - 394ms/step
Epoch 308/1000
2023-09-27 02:14:55.144 
Epoch 308/1000 
	 loss: 18.6674, MinusLogProbMetric: 18.6674, val_loss: 18.7894, val_MinusLogProbMetric: 18.7894

Epoch 308: val_loss did not improve from 18.78628
196/196 - 77s - loss: 18.6674 - MinusLogProbMetric: 18.6674 - val_loss: 18.7894 - val_MinusLogProbMetric: 18.7894 - lr: 1.1111e-04 - 77s/epoch - 394ms/step
Epoch 309/1000
2023-09-27 02:16:13.464 
Epoch 309/1000 
	 loss: 19.3751, MinusLogProbMetric: 19.3751, val_loss: 19.0924, val_MinusLogProbMetric: 19.0924

Epoch 309: val_loss did not improve from 18.78628
196/196 - 78s - loss: 19.3751 - MinusLogProbMetric: 19.3751 - val_loss: 19.0924 - val_MinusLogProbMetric: 19.0924 - lr: 1.1111e-04 - 78s/epoch - 400ms/step
Epoch 310/1000
2023-09-27 02:17:31.434 
Epoch 310/1000 
	 loss: 18.6521, MinusLogProbMetric: 18.6521, val_loss: 19.0065, val_MinusLogProbMetric: 19.0065

Epoch 310: val_loss did not improve from 18.78628
196/196 - 78s - loss: 18.6521 - MinusLogProbMetric: 18.6521 - val_loss: 19.0065 - val_MinusLogProbMetric: 19.0065 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 311/1000
2023-09-27 02:18:49.354 
Epoch 311/1000 
	 loss: 18.6453, MinusLogProbMetric: 18.6453, val_loss: 18.8662, val_MinusLogProbMetric: 18.8662

Epoch 311: val_loss did not improve from 18.78628
196/196 - 78s - loss: 18.6453 - MinusLogProbMetric: 18.6453 - val_loss: 18.8662 - val_MinusLogProbMetric: 18.8662 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 312/1000
2023-09-27 02:20:07.483 
Epoch 312/1000 
	 loss: 18.6131, MinusLogProbMetric: 18.6131, val_loss: 18.8865, val_MinusLogProbMetric: 18.8865

Epoch 312: val_loss did not improve from 18.78628
196/196 - 78s - loss: 18.6131 - MinusLogProbMetric: 18.6131 - val_loss: 18.8865 - val_MinusLogProbMetric: 18.8865 - lr: 1.1111e-04 - 78s/epoch - 399ms/step
Epoch 313/1000
2023-09-27 02:21:25.062 
Epoch 313/1000 
	 loss: 18.6043, MinusLogProbMetric: 18.6043, val_loss: 19.3020, val_MinusLogProbMetric: 19.3020

Epoch 313: val_loss did not improve from 18.78628
196/196 - 78s - loss: 18.6043 - MinusLogProbMetric: 18.6043 - val_loss: 19.3020 - val_MinusLogProbMetric: 19.3020 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 314/1000
2023-09-27 02:22:42.985 
Epoch 314/1000 
	 loss: 18.6508, MinusLogProbMetric: 18.6508, val_loss: 18.6969, val_MinusLogProbMetric: 18.6969

Epoch 314: val_loss improved from 18.78628 to 18.69689, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 80s - loss: 18.6508 - MinusLogProbMetric: 18.6508 - val_loss: 18.6969 - val_MinusLogProbMetric: 18.6969 - lr: 1.1111e-04 - 80s/epoch - 406ms/step
Epoch 315/1000
2023-09-27 02:24:02.577 
Epoch 315/1000 
	 loss: 18.6204, MinusLogProbMetric: 18.6204, val_loss: 18.8515, val_MinusLogProbMetric: 18.8515

Epoch 315: val_loss did not improve from 18.69689
196/196 - 78s - loss: 18.6204 - MinusLogProbMetric: 18.6204 - val_loss: 18.8515 - val_MinusLogProbMetric: 18.8515 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 316/1000
2023-09-27 02:25:20.382 
Epoch 316/1000 
	 loss: 18.5746, MinusLogProbMetric: 18.5746, val_loss: 18.7650, val_MinusLogProbMetric: 18.7650

Epoch 316: val_loss did not improve from 18.69689
196/196 - 78s - loss: 18.5746 - MinusLogProbMetric: 18.5746 - val_loss: 18.7650 - val_MinusLogProbMetric: 18.7650 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 317/1000
2023-09-27 02:26:38.693 
Epoch 317/1000 
	 loss: 18.5919, MinusLogProbMetric: 18.5919, val_loss: 18.8432, val_MinusLogProbMetric: 18.8432

Epoch 317: val_loss did not improve from 18.69689
196/196 - 78s - loss: 18.5919 - MinusLogProbMetric: 18.5919 - val_loss: 18.8432 - val_MinusLogProbMetric: 18.8432 - lr: 1.1111e-04 - 78s/epoch - 400ms/step
Epoch 318/1000
2023-09-27 02:27:56.272 
Epoch 318/1000 
	 loss: 18.7922, MinusLogProbMetric: 18.7922, val_loss: 20.7686, val_MinusLogProbMetric: 20.7686

Epoch 318: val_loss did not improve from 18.69689
196/196 - 78s - loss: 18.7922 - MinusLogProbMetric: 18.7922 - val_loss: 20.7686 - val_MinusLogProbMetric: 20.7686 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 319/1000
2023-09-27 02:29:14.124 
Epoch 319/1000 
	 loss: 18.7154, MinusLogProbMetric: 18.7154, val_loss: 18.8750, val_MinusLogProbMetric: 18.8750

Epoch 319: val_loss did not improve from 18.69689
196/196 - 78s - loss: 18.7154 - MinusLogProbMetric: 18.7154 - val_loss: 18.8750 - val_MinusLogProbMetric: 18.8750 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 320/1000
2023-09-27 02:30:32.010 
Epoch 320/1000 
	 loss: 18.5711, MinusLogProbMetric: 18.5711, val_loss: 19.0395, val_MinusLogProbMetric: 19.0395

Epoch 320: val_loss did not improve from 18.69689
196/196 - 78s - loss: 18.5711 - MinusLogProbMetric: 18.5711 - val_loss: 19.0395 - val_MinusLogProbMetric: 19.0395 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 321/1000
2023-09-27 02:31:48.585 
Epoch 321/1000 
	 loss: 18.5811, MinusLogProbMetric: 18.5811, val_loss: 18.5498, val_MinusLogProbMetric: 18.5498

Epoch 321: val_loss improved from 18.69689 to 18.54984, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 78s - loss: 18.5811 - MinusLogProbMetric: 18.5811 - val_loss: 18.5498 - val_MinusLogProbMetric: 18.5498 - lr: 1.1111e-04 - 78s/epoch - 399ms/step
Epoch 322/1000
2023-09-27 02:33:00.214 
Epoch 322/1000 
	 loss: 18.5073, MinusLogProbMetric: 18.5073, val_loss: 18.7223, val_MinusLogProbMetric: 18.7223

Epoch 322: val_loss did not improve from 18.54984
196/196 - 70s - loss: 18.5073 - MinusLogProbMetric: 18.5073 - val_loss: 18.7223 - val_MinusLogProbMetric: 18.7223 - lr: 1.1111e-04 - 70s/epoch - 357ms/step
Epoch 323/1000
2023-09-27 02:34:16.819 
Epoch 323/1000 
	 loss: 18.5653, MinusLogProbMetric: 18.5653, val_loss: 18.8536, val_MinusLogProbMetric: 18.8536

Epoch 323: val_loss did not improve from 18.54984
196/196 - 77s - loss: 18.5653 - MinusLogProbMetric: 18.5653 - val_loss: 18.8536 - val_MinusLogProbMetric: 18.8536 - lr: 1.1111e-04 - 77s/epoch - 391ms/step
Epoch 324/1000
2023-09-27 02:35:33.732 
Epoch 324/1000 
	 loss: 18.5600, MinusLogProbMetric: 18.5600, val_loss: 19.2440, val_MinusLogProbMetric: 19.2440

Epoch 324: val_loss did not improve from 18.54984
196/196 - 77s - loss: 18.5600 - MinusLogProbMetric: 18.5600 - val_loss: 19.2440 - val_MinusLogProbMetric: 19.2440 - lr: 1.1111e-04 - 77s/epoch - 392ms/step
Epoch 325/1000
2023-09-27 02:36:51.349 
Epoch 325/1000 
	 loss: 18.6053, MinusLogProbMetric: 18.6053, val_loss: 18.7620, val_MinusLogProbMetric: 18.7620

Epoch 325: val_loss did not improve from 18.54984
196/196 - 78s - loss: 18.6053 - MinusLogProbMetric: 18.6053 - val_loss: 18.7620 - val_MinusLogProbMetric: 18.7620 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 326/1000
2023-09-27 02:38:09.079 
Epoch 326/1000 
	 loss: 18.6931, MinusLogProbMetric: 18.6931, val_loss: 18.7407, val_MinusLogProbMetric: 18.7407

Epoch 326: val_loss did not improve from 18.54984
196/196 - 78s - loss: 18.6931 - MinusLogProbMetric: 18.6931 - val_loss: 18.7407 - val_MinusLogProbMetric: 18.7407 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 327/1000
2023-09-27 02:39:27.391 
Epoch 327/1000 
	 loss: 18.5254, MinusLogProbMetric: 18.5254, val_loss: 18.6681, val_MinusLogProbMetric: 18.6681

Epoch 327: val_loss did not improve from 18.54984
196/196 - 78s - loss: 18.5254 - MinusLogProbMetric: 18.5254 - val_loss: 18.6681 - val_MinusLogProbMetric: 18.6681 - lr: 1.1111e-04 - 78s/epoch - 400ms/step
Epoch 328/1000
2023-09-27 02:40:45.011 
Epoch 328/1000 
	 loss: 18.6523, MinusLogProbMetric: 18.6523, val_loss: 18.7298, val_MinusLogProbMetric: 18.7298

Epoch 328: val_loss did not improve from 18.54984
196/196 - 78s - loss: 18.6523 - MinusLogProbMetric: 18.6523 - val_loss: 18.7298 - val_MinusLogProbMetric: 18.7298 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 329/1000
2023-09-27 02:42:02.645 
Epoch 329/1000 
	 loss: 18.4672, MinusLogProbMetric: 18.4672, val_loss: 18.8825, val_MinusLogProbMetric: 18.8825

Epoch 329: val_loss did not improve from 18.54984
196/196 - 78s - loss: 18.4672 - MinusLogProbMetric: 18.4672 - val_loss: 18.8825 - val_MinusLogProbMetric: 18.8825 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 330/1000
2023-09-27 02:43:20.538 
Epoch 330/1000 
	 loss: 18.4623, MinusLogProbMetric: 18.4623, val_loss: 18.6713, val_MinusLogProbMetric: 18.6713

Epoch 330: val_loss did not improve from 18.54984
196/196 - 78s - loss: 18.4623 - MinusLogProbMetric: 18.4623 - val_loss: 18.6713 - val_MinusLogProbMetric: 18.6713 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 331/1000
2023-09-27 02:44:38.705 
Epoch 331/1000 
	 loss: 18.5154, MinusLogProbMetric: 18.5154, val_loss: 19.3641, val_MinusLogProbMetric: 19.3641

Epoch 331: val_loss did not improve from 18.54984
196/196 - 78s - loss: 18.5154 - MinusLogProbMetric: 18.5154 - val_loss: 19.3641 - val_MinusLogProbMetric: 19.3641 - lr: 1.1111e-04 - 78s/epoch - 399ms/step
Epoch 332/1000
2023-09-27 02:45:57.270 
Epoch 332/1000 
	 loss: 18.5537, MinusLogProbMetric: 18.5537, val_loss: 18.6059, val_MinusLogProbMetric: 18.6059

Epoch 332: val_loss did not improve from 18.54984
196/196 - 79s - loss: 18.5537 - MinusLogProbMetric: 18.5537 - val_loss: 18.6059 - val_MinusLogProbMetric: 18.6059 - lr: 1.1111e-04 - 79s/epoch - 401ms/step
Epoch 333/1000
2023-09-27 02:47:15.797 
Epoch 333/1000 
	 loss: 18.4711, MinusLogProbMetric: 18.4711, val_loss: 18.9285, val_MinusLogProbMetric: 18.9285

Epoch 333: val_loss did not improve from 18.54984
196/196 - 79s - loss: 18.4711 - MinusLogProbMetric: 18.4711 - val_loss: 18.9285 - val_MinusLogProbMetric: 18.9285 - lr: 1.1111e-04 - 79s/epoch - 401ms/step
Epoch 334/1000
2023-09-27 02:48:33.514 
Epoch 334/1000 
	 loss: 18.5206, MinusLogProbMetric: 18.5206, val_loss: 18.8037, val_MinusLogProbMetric: 18.8037

Epoch 334: val_loss did not improve from 18.54984
196/196 - 78s - loss: 18.5206 - MinusLogProbMetric: 18.5206 - val_loss: 18.8037 - val_MinusLogProbMetric: 18.8037 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 335/1000
2023-09-27 02:49:51.185 
Epoch 335/1000 
	 loss: 18.5123, MinusLogProbMetric: 18.5123, val_loss: 18.6574, val_MinusLogProbMetric: 18.6574

Epoch 335: val_loss did not improve from 18.54984
196/196 - 78s - loss: 18.5123 - MinusLogProbMetric: 18.5123 - val_loss: 18.6574 - val_MinusLogProbMetric: 18.6574 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 336/1000
2023-09-27 02:51:09.498 
Epoch 336/1000 
	 loss: 18.5284, MinusLogProbMetric: 18.5284, val_loss: 18.7059, val_MinusLogProbMetric: 18.7059

Epoch 336: val_loss did not improve from 18.54984
196/196 - 78s - loss: 18.5284 - MinusLogProbMetric: 18.5284 - val_loss: 18.7059 - val_MinusLogProbMetric: 18.7059 - lr: 1.1111e-04 - 78s/epoch - 400ms/step
Epoch 337/1000
2023-09-27 02:52:27.686 
Epoch 337/1000 
	 loss: 18.5284, MinusLogProbMetric: 18.5284, val_loss: 18.5922, val_MinusLogProbMetric: 18.5922

Epoch 337: val_loss did not improve from 18.54984
196/196 - 78s - loss: 18.5284 - MinusLogProbMetric: 18.5284 - val_loss: 18.5922 - val_MinusLogProbMetric: 18.5922 - lr: 1.1111e-04 - 78s/epoch - 399ms/step
Epoch 338/1000
2023-09-27 02:53:45.917 
Epoch 338/1000 
	 loss: 18.5196, MinusLogProbMetric: 18.5196, val_loss: 18.7969, val_MinusLogProbMetric: 18.7969

Epoch 338: val_loss did not improve from 18.54984
196/196 - 78s - loss: 18.5196 - MinusLogProbMetric: 18.5196 - val_loss: 18.7969 - val_MinusLogProbMetric: 18.7969 - lr: 1.1111e-04 - 78s/epoch - 399ms/step
Epoch 339/1000
2023-09-27 02:55:03.839 
Epoch 339/1000 
	 loss: 18.4714, MinusLogProbMetric: 18.4714, val_loss: 18.5612, val_MinusLogProbMetric: 18.5612

Epoch 339: val_loss did not improve from 18.54984
196/196 - 78s - loss: 18.4714 - MinusLogProbMetric: 18.4714 - val_loss: 18.5612 - val_MinusLogProbMetric: 18.5612 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 340/1000
2023-09-27 02:56:21.806 
Epoch 340/1000 
	 loss: 18.5076, MinusLogProbMetric: 18.5076, val_loss: 18.7029, val_MinusLogProbMetric: 18.7029

Epoch 340: val_loss did not improve from 18.54984
196/196 - 78s - loss: 18.5076 - MinusLogProbMetric: 18.5076 - val_loss: 18.7029 - val_MinusLogProbMetric: 18.7029 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 341/1000
2023-09-27 02:57:39.457 
Epoch 341/1000 
	 loss: 18.5836, MinusLogProbMetric: 18.5836, val_loss: 18.5636, val_MinusLogProbMetric: 18.5636

Epoch 341: val_loss did not improve from 18.54984
196/196 - 78s - loss: 18.5836 - MinusLogProbMetric: 18.5836 - val_loss: 18.5636 - val_MinusLogProbMetric: 18.5636 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 342/1000
2023-09-27 02:58:56.837 
Epoch 342/1000 
	 loss: 18.4672, MinusLogProbMetric: 18.4672, val_loss: 18.6329, val_MinusLogProbMetric: 18.6329

Epoch 342: val_loss did not improve from 18.54984
196/196 - 77s - loss: 18.4672 - MinusLogProbMetric: 18.4672 - val_loss: 18.6329 - val_MinusLogProbMetric: 18.6329 - lr: 1.1111e-04 - 77s/epoch - 395ms/step
Epoch 343/1000
2023-09-27 03:00:14.351 
Epoch 343/1000 
	 loss: 18.4187, MinusLogProbMetric: 18.4187, val_loss: 18.6341, val_MinusLogProbMetric: 18.6341

Epoch 343: val_loss did not improve from 18.54984
196/196 - 78s - loss: 18.4187 - MinusLogProbMetric: 18.4187 - val_loss: 18.6341 - val_MinusLogProbMetric: 18.6341 - lr: 1.1111e-04 - 78s/epoch - 395ms/step
Epoch 344/1000
2023-09-27 03:01:31.905 
Epoch 344/1000 
	 loss: 18.4500, MinusLogProbMetric: 18.4500, val_loss: 18.6503, val_MinusLogProbMetric: 18.6503

Epoch 344: val_loss did not improve from 18.54984
196/196 - 78s - loss: 18.4500 - MinusLogProbMetric: 18.4500 - val_loss: 18.6503 - val_MinusLogProbMetric: 18.6503 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 345/1000
2023-09-27 03:02:49.363 
Epoch 345/1000 
	 loss: 18.4673, MinusLogProbMetric: 18.4673, val_loss: 18.6409, val_MinusLogProbMetric: 18.6409

Epoch 345: val_loss did not improve from 18.54984
196/196 - 77s - loss: 18.4673 - MinusLogProbMetric: 18.4673 - val_loss: 18.6409 - val_MinusLogProbMetric: 18.6409 - lr: 1.1111e-04 - 77s/epoch - 395ms/step
Epoch 346/1000
2023-09-27 03:04:07.250 
Epoch 346/1000 
	 loss: 18.4624, MinusLogProbMetric: 18.4624, val_loss: 18.5167, val_MinusLogProbMetric: 18.5167

Epoch 346: val_loss improved from 18.54984 to 18.51672, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 79s - loss: 18.4624 - MinusLogProbMetric: 18.4624 - val_loss: 18.5167 - val_MinusLogProbMetric: 18.5167 - lr: 1.1111e-04 - 79s/epoch - 405ms/step
Epoch 347/1000
2023-09-27 03:05:26.106 
Epoch 347/1000 
	 loss: 18.4356, MinusLogProbMetric: 18.4356, val_loss: 18.4502, val_MinusLogProbMetric: 18.4502

Epoch 347: val_loss improved from 18.51672 to 18.45022, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 79s - loss: 18.4356 - MinusLogProbMetric: 18.4356 - val_loss: 18.4502 - val_MinusLogProbMetric: 18.4502 - lr: 1.1111e-04 - 79s/epoch - 402ms/step
Epoch 348/1000
2023-09-27 03:06:45.122 
Epoch 348/1000 
	 loss: 18.4401, MinusLogProbMetric: 18.4401, val_loss: 18.6116, val_MinusLogProbMetric: 18.6116

Epoch 348: val_loss did not improve from 18.45022
196/196 - 78s - loss: 18.4401 - MinusLogProbMetric: 18.4401 - val_loss: 18.6116 - val_MinusLogProbMetric: 18.6116 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 349/1000
2023-09-27 03:08:02.531 
Epoch 349/1000 
	 loss: 18.6361, MinusLogProbMetric: 18.6361, val_loss: 18.4850, val_MinusLogProbMetric: 18.4850

Epoch 349: val_loss did not improve from 18.45022
196/196 - 77s - loss: 18.6361 - MinusLogProbMetric: 18.6361 - val_loss: 18.4850 - val_MinusLogProbMetric: 18.4850 - lr: 1.1111e-04 - 77s/epoch - 395ms/step
Epoch 350/1000
2023-09-27 03:09:20.370 
Epoch 350/1000 
	 loss: 18.4319, MinusLogProbMetric: 18.4319, val_loss: 18.4943, val_MinusLogProbMetric: 18.4943

Epoch 350: val_loss did not improve from 18.45022
196/196 - 78s - loss: 18.4319 - MinusLogProbMetric: 18.4319 - val_loss: 18.4943 - val_MinusLogProbMetric: 18.4943 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 351/1000
2023-09-27 03:10:38.543 
Epoch 351/1000 
	 loss: 18.4128, MinusLogProbMetric: 18.4128, val_loss: 18.5668, val_MinusLogProbMetric: 18.5668

Epoch 351: val_loss did not improve from 18.45022
196/196 - 78s - loss: 18.4128 - MinusLogProbMetric: 18.4128 - val_loss: 18.5668 - val_MinusLogProbMetric: 18.5668 - lr: 1.1111e-04 - 78s/epoch - 399ms/step
Epoch 352/1000
2023-09-27 03:11:56.150 
Epoch 352/1000 
	 loss: 18.4137, MinusLogProbMetric: 18.4137, val_loss: 18.6888, val_MinusLogProbMetric: 18.6888

Epoch 352: val_loss did not improve from 18.45022
196/196 - 78s - loss: 18.4137 - MinusLogProbMetric: 18.4137 - val_loss: 18.6888 - val_MinusLogProbMetric: 18.6888 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 353/1000
2023-09-27 03:13:14.034 
Epoch 353/1000 
	 loss: 18.3977, MinusLogProbMetric: 18.3977, val_loss: 18.5908, val_MinusLogProbMetric: 18.5908

Epoch 353: val_loss did not improve from 18.45022
196/196 - 78s - loss: 18.3977 - MinusLogProbMetric: 18.3977 - val_loss: 18.5908 - val_MinusLogProbMetric: 18.5908 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 354/1000
2023-09-27 03:14:31.338 
Epoch 354/1000 
	 loss: 18.3960, MinusLogProbMetric: 18.3960, val_loss: 18.6696, val_MinusLogProbMetric: 18.6696

Epoch 354: val_loss did not improve from 18.45022
196/196 - 77s - loss: 18.3960 - MinusLogProbMetric: 18.3960 - val_loss: 18.6696 - val_MinusLogProbMetric: 18.6696 - lr: 1.1111e-04 - 77s/epoch - 394ms/step
Epoch 355/1000
2023-09-27 03:15:49.107 
Epoch 355/1000 
	 loss: 18.3518, MinusLogProbMetric: 18.3518, val_loss: 18.4933, val_MinusLogProbMetric: 18.4933

Epoch 355: val_loss did not improve from 18.45022
196/196 - 78s - loss: 18.3518 - MinusLogProbMetric: 18.3518 - val_loss: 18.4933 - val_MinusLogProbMetric: 18.4933 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 356/1000
2023-09-27 03:17:06.691 
Epoch 356/1000 
	 loss: 18.4140, MinusLogProbMetric: 18.4140, val_loss: 18.4213, val_MinusLogProbMetric: 18.4213

Epoch 356: val_loss improved from 18.45022 to 18.42134, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 79s - loss: 18.4140 - MinusLogProbMetric: 18.4140 - val_loss: 18.4213 - val_MinusLogProbMetric: 18.4213 - lr: 1.1111e-04 - 79s/epoch - 402ms/step
Epoch 357/1000
2023-09-27 03:18:25.405 
Epoch 357/1000 
	 loss: 18.3574, MinusLogProbMetric: 18.3574, val_loss: 18.5581, val_MinusLogProbMetric: 18.5581

Epoch 357: val_loss did not improve from 18.42134
196/196 - 78s - loss: 18.3574 - MinusLogProbMetric: 18.3574 - val_loss: 18.5581 - val_MinusLogProbMetric: 18.5581 - lr: 1.1111e-04 - 78s/epoch - 395ms/step
Epoch 358/1000
2023-09-27 03:19:42.749 
Epoch 358/1000 
	 loss: 18.4279, MinusLogProbMetric: 18.4279, val_loss: 18.6088, val_MinusLogProbMetric: 18.6088

Epoch 358: val_loss did not improve from 18.42134
196/196 - 77s - loss: 18.4279 - MinusLogProbMetric: 18.4279 - val_loss: 18.6088 - val_MinusLogProbMetric: 18.6088 - lr: 1.1111e-04 - 77s/epoch - 395ms/step
Epoch 359/1000
2023-09-27 03:21:00.130 
Epoch 359/1000 
	 loss: 18.4589, MinusLogProbMetric: 18.4589, val_loss: 18.4973, val_MinusLogProbMetric: 18.4973

Epoch 359: val_loss did not improve from 18.42134
196/196 - 77s - loss: 18.4589 - MinusLogProbMetric: 18.4589 - val_loss: 18.4973 - val_MinusLogProbMetric: 18.4973 - lr: 1.1111e-04 - 77s/epoch - 395ms/step
Epoch 360/1000
2023-09-27 03:22:18.060 
Epoch 360/1000 
	 loss: 18.3384, MinusLogProbMetric: 18.3384, val_loss: 18.4874, val_MinusLogProbMetric: 18.4874

Epoch 360: val_loss did not improve from 18.42134
196/196 - 78s - loss: 18.3384 - MinusLogProbMetric: 18.3384 - val_loss: 18.4874 - val_MinusLogProbMetric: 18.4874 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 361/1000
2023-09-27 03:23:35.505 
Epoch 361/1000 
	 loss: 18.3270, MinusLogProbMetric: 18.3270, val_loss: 18.4138, val_MinusLogProbMetric: 18.4138

Epoch 361: val_loss improved from 18.42134 to 18.41378, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 79s - loss: 18.3270 - MinusLogProbMetric: 18.3270 - val_loss: 18.4138 - val_MinusLogProbMetric: 18.4138 - lr: 1.1111e-04 - 79s/epoch - 404ms/step
Epoch 362/1000
2023-09-27 03:24:55.327 
Epoch 362/1000 
	 loss: 18.4270, MinusLogProbMetric: 18.4270, val_loss: 18.6522, val_MinusLogProbMetric: 18.6522

Epoch 362: val_loss did not improve from 18.41378
196/196 - 78s - loss: 18.4270 - MinusLogProbMetric: 18.4270 - val_loss: 18.6522 - val_MinusLogProbMetric: 18.6522 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 363/1000
2023-09-27 03:26:13.508 
Epoch 363/1000 
	 loss: 18.7055, MinusLogProbMetric: 18.7055, val_loss: 18.4883, val_MinusLogProbMetric: 18.4883

Epoch 363: val_loss did not improve from 18.41378
196/196 - 78s - loss: 18.7055 - MinusLogProbMetric: 18.7055 - val_loss: 18.4883 - val_MinusLogProbMetric: 18.4883 - lr: 1.1111e-04 - 78s/epoch - 399ms/step
Epoch 364/1000
2023-09-27 03:27:31.650 
Epoch 364/1000 
	 loss: 18.3492, MinusLogProbMetric: 18.3492, val_loss: 18.5545, val_MinusLogProbMetric: 18.5545

Epoch 364: val_loss did not improve from 18.41378
196/196 - 78s - loss: 18.3492 - MinusLogProbMetric: 18.3492 - val_loss: 18.5545 - val_MinusLogProbMetric: 18.5545 - lr: 1.1111e-04 - 78s/epoch - 399ms/step
Epoch 365/1000
2023-09-27 03:28:49.817 
Epoch 365/1000 
	 loss: 18.3290, MinusLogProbMetric: 18.3290, val_loss: 18.8810, val_MinusLogProbMetric: 18.8810

Epoch 365: val_loss did not improve from 18.41378
196/196 - 78s - loss: 18.3290 - MinusLogProbMetric: 18.3290 - val_loss: 18.8810 - val_MinusLogProbMetric: 18.8810 - lr: 1.1111e-04 - 78s/epoch - 399ms/step
Epoch 366/1000
2023-09-27 03:30:07.473 
Epoch 366/1000 
	 loss: 18.3925, MinusLogProbMetric: 18.3925, val_loss: 18.4192, val_MinusLogProbMetric: 18.4192

Epoch 366: val_loss did not improve from 18.41378
196/196 - 78s - loss: 18.3925 - MinusLogProbMetric: 18.3925 - val_loss: 18.4192 - val_MinusLogProbMetric: 18.4192 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 367/1000
2023-09-27 03:31:24.727 
Epoch 367/1000 
	 loss: 18.2944, MinusLogProbMetric: 18.2944, val_loss: 18.5342, val_MinusLogProbMetric: 18.5342

Epoch 367: val_loss did not improve from 18.41378
196/196 - 77s - loss: 18.2944 - MinusLogProbMetric: 18.2944 - val_loss: 18.5342 - val_MinusLogProbMetric: 18.5342 - lr: 1.1111e-04 - 77s/epoch - 394ms/step
Epoch 368/1000
2023-09-27 03:32:42.616 
Epoch 368/1000 
	 loss: 18.3666, MinusLogProbMetric: 18.3666, val_loss: 18.4014, val_MinusLogProbMetric: 18.4014

Epoch 368: val_loss improved from 18.41378 to 18.40142, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 79s - loss: 18.3666 - MinusLogProbMetric: 18.3666 - val_loss: 18.4014 - val_MinusLogProbMetric: 18.4014 - lr: 1.1111e-04 - 79s/epoch - 405ms/step
Epoch 369/1000
2023-09-27 03:34:02.385 
Epoch 369/1000 
	 loss: 18.4452, MinusLogProbMetric: 18.4452, val_loss: 18.5409, val_MinusLogProbMetric: 18.5409

Epoch 369: val_loss did not improve from 18.40142
196/196 - 78s - loss: 18.4452 - MinusLogProbMetric: 18.4452 - val_loss: 18.5409 - val_MinusLogProbMetric: 18.5409 - lr: 1.1111e-04 - 78s/epoch - 399ms/step
Epoch 370/1000
2023-09-27 03:35:20.543 
Epoch 370/1000 
	 loss: 18.3534, MinusLogProbMetric: 18.3534, val_loss: 18.5070, val_MinusLogProbMetric: 18.5070

Epoch 370: val_loss did not improve from 18.40142
196/196 - 78s - loss: 18.3534 - MinusLogProbMetric: 18.3534 - val_loss: 18.5070 - val_MinusLogProbMetric: 18.5070 - lr: 1.1111e-04 - 78s/epoch - 399ms/step
Epoch 371/1000
2023-09-27 03:36:38.911 
Epoch 371/1000 
	 loss: 18.3664, MinusLogProbMetric: 18.3664, val_loss: 18.8859, val_MinusLogProbMetric: 18.8859

Epoch 371: val_loss did not improve from 18.40142
196/196 - 78s - loss: 18.3664 - MinusLogProbMetric: 18.3664 - val_loss: 18.8859 - val_MinusLogProbMetric: 18.8859 - lr: 1.1111e-04 - 78s/epoch - 400ms/step
Epoch 372/1000
2023-09-27 03:37:57.563 
Epoch 372/1000 
	 loss: 18.2921, MinusLogProbMetric: 18.2921, val_loss: 18.4995, val_MinusLogProbMetric: 18.4995

Epoch 372: val_loss did not improve from 18.40142
196/196 - 79s - loss: 18.2921 - MinusLogProbMetric: 18.2921 - val_loss: 18.4995 - val_MinusLogProbMetric: 18.4995 - lr: 1.1111e-04 - 79s/epoch - 401ms/step
Epoch 373/1000
2023-09-27 03:39:16.134 
Epoch 373/1000 
	 loss: 18.2703, MinusLogProbMetric: 18.2703, val_loss: 18.5364, val_MinusLogProbMetric: 18.5364

Epoch 373: val_loss did not improve from 18.40142
196/196 - 79s - loss: 18.2703 - MinusLogProbMetric: 18.2703 - val_loss: 18.5364 - val_MinusLogProbMetric: 18.5364 - lr: 1.1111e-04 - 79s/epoch - 401ms/step
Epoch 374/1000
2023-09-27 03:40:33.616 
Epoch 374/1000 
	 loss: 18.3364, MinusLogProbMetric: 18.3364, val_loss: 18.7737, val_MinusLogProbMetric: 18.7737

Epoch 374: val_loss did not improve from 18.40142
196/196 - 77s - loss: 18.3364 - MinusLogProbMetric: 18.3364 - val_loss: 18.7737 - val_MinusLogProbMetric: 18.7737 - lr: 1.1111e-04 - 77s/epoch - 395ms/step
Epoch 375/1000
2023-09-27 03:41:51.645 
Epoch 375/1000 
	 loss: 18.3058, MinusLogProbMetric: 18.3058, val_loss: 18.7746, val_MinusLogProbMetric: 18.7746

Epoch 375: val_loss did not improve from 18.40142
196/196 - 78s - loss: 18.3058 - MinusLogProbMetric: 18.3058 - val_loss: 18.7746 - val_MinusLogProbMetric: 18.7746 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 376/1000
2023-09-27 03:43:09.440 
Epoch 376/1000 
	 loss: 18.3556, MinusLogProbMetric: 18.3556, val_loss: 18.5571, val_MinusLogProbMetric: 18.5571

Epoch 376: val_loss did not improve from 18.40142
196/196 - 78s - loss: 18.3556 - MinusLogProbMetric: 18.3556 - val_loss: 18.5571 - val_MinusLogProbMetric: 18.5571 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 377/1000
2023-09-27 03:44:27.039 
Epoch 377/1000 
	 loss: 18.3840, MinusLogProbMetric: 18.3840, val_loss: 18.6940, val_MinusLogProbMetric: 18.6940

Epoch 377: val_loss did not improve from 18.40142
196/196 - 78s - loss: 18.3840 - MinusLogProbMetric: 18.3840 - val_loss: 18.6940 - val_MinusLogProbMetric: 18.6940 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 378/1000
2023-09-27 03:45:44.601 
Epoch 378/1000 
	 loss: 18.2900, MinusLogProbMetric: 18.2900, val_loss: 18.3858, val_MinusLogProbMetric: 18.3858

Epoch 378: val_loss improved from 18.40142 to 18.38579, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 79s - loss: 18.2900 - MinusLogProbMetric: 18.2900 - val_loss: 18.3858 - val_MinusLogProbMetric: 18.3858 - lr: 1.1111e-04 - 79s/epoch - 403ms/step
Epoch 379/1000
2023-09-27 03:47:03.873 
Epoch 379/1000 
	 loss: 18.3167, MinusLogProbMetric: 18.3167, val_loss: 18.4849, val_MinusLogProbMetric: 18.4849

Epoch 379: val_loss did not improve from 18.38579
196/196 - 78s - loss: 18.3167 - MinusLogProbMetric: 18.3167 - val_loss: 18.4849 - val_MinusLogProbMetric: 18.4849 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 380/1000
2023-09-27 03:48:21.273 
Epoch 380/1000 
	 loss: 18.3575, MinusLogProbMetric: 18.3575, val_loss: 18.8812, val_MinusLogProbMetric: 18.8812

Epoch 380: val_loss did not improve from 18.38579
196/196 - 77s - loss: 18.3575 - MinusLogProbMetric: 18.3575 - val_loss: 18.8812 - val_MinusLogProbMetric: 18.8812 - lr: 1.1111e-04 - 77s/epoch - 395ms/step
Epoch 381/1000
2023-09-27 03:49:39.116 
Epoch 381/1000 
	 loss: 18.2912, MinusLogProbMetric: 18.2912, val_loss: 18.5281, val_MinusLogProbMetric: 18.5281

Epoch 381: val_loss did not improve from 18.38579
196/196 - 78s - loss: 18.2912 - MinusLogProbMetric: 18.2912 - val_loss: 18.5281 - val_MinusLogProbMetric: 18.5281 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 382/1000
2023-09-27 03:50:56.684 
Epoch 382/1000 
	 loss: 18.3909, MinusLogProbMetric: 18.3909, val_loss: 18.3655, val_MinusLogProbMetric: 18.3655

Epoch 382: val_loss improved from 18.38579 to 18.36547, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 79s - loss: 18.3909 - MinusLogProbMetric: 18.3909 - val_loss: 18.3655 - val_MinusLogProbMetric: 18.3655 - lr: 1.1111e-04 - 79s/epoch - 404ms/step
Epoch 383/1000
2023-09-27 03:52:16.066 
Epoch 383/1000 
	 loss: 18.8052, MinusLogProbMetric: 18.8052, val_loss: 18.4708, val_MinusLogProbMetric: 18.4708

Epoch 383: val_loss did not improve from 18.36547
196/196 - 78s - loss: 18.8052 - MinusLogProbMetric: 18.8052 - val_loss: 18.4708 - val_MinusLogProbMetric: 18.4708 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 384/1000
2023-09-27 03:53:33.937 
Epoch 384/1000 
	 loss: 18.2986, MinusLogProbMetric: 18.2986, val_loss: 18.5675, val_MinusLogProbMetric: 18.5675

Epoch 384: val_loss did not improve from 18.36547
196/196 - 78s - loss: 18.2986 - MinusLogProbMetric: 18.2986 - val_loss: 18.5675 - val_MinusLogProbMetric: 18.5675 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 385/1000
2023-09-27 03:54:51.646 
Epoch 385/1000 
	 loss: 18.2561, MinusLogProbMetric: 18.2561, val_loss: 18.4941, val_MinusLogProbMetric: 18.4941

Epoch 385: val_loss did not improve from 18.36547
196/196 - 78s - loss: 18.2561 - MinusLogProbMetric: 18.2561 - val_loss: 18.4941 - val_MinusLogProbMetric: 18.4941 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 386/1000
2023-09-27 03:56:09.354 
Epoch 386/1000 
	 loss: 18.2716, MinusLogProbMetric: 18.2716, val_loss: 18.5449, val_MinusLogProbMetric: 18.5449

Epoch 386: val_loss did not improve from 18.36547
196/196 - 78s - loss: 18.2716 - MinusLogProbMetric: 18.2716 - val_loss: 18.5449 - val_MinusLogProbMetric: 18.5449 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 387/1000
2023-09-27 03:57:27.396 
Epoch 387/1000 
	 loss: 18.3129, MinusLogProbMetric: 18.3129, val_loss: 18.6311, val_MinusLogProbMetric: 18.6311

Epoch 387: val_loss did not improve from 18.36547
196/196 - 78s - loss: 18.3129 - MinusLogProbMetric: 18.3129 - val_loss: 18.6311 - val_MinusLogProbMetric: 18.6311 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 388/1000
2023-09-27 03:58:45.262 
Epoch 388/1000 
	 loss: 18.2636, MinusLogProbMetric: 18.2636, val_loss: 18.5903, val_MinusLogProbMetric: 18.5903

Epoch 388: val_loss did not improve from 18.36547
196/196 - 78s - loss: 18.2636 - MinusLogProbMetric: 18.2636 - val_loss: 18.5903 - val_MinusLogProbMetric: 18.5903 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 389/1000
2023-09-27 04:00:02.692 
Epoch 389/1000 
	 loss: 18.2548, MinusLogProbMetric: 18.2548, val_loss: 18.4006, val_MinusLogProbMetric: 18.4006

Epoch 389: val_loss did not improve from 18.36547
196/196 - 77s - loss: 18.2548 - MinusLogProbMetric: 18.2548 - val_loss: 18.4006 - val_MinusLogProbMetric: 18.4006 - lr: 1.1111e-04 - 77s/epoch - 395ms/step
Epoch 390/1000
2023-09-27 04:01:20.468 
Epoch 390/1000 
	 loss: 18.2575, MinusLogProbMetric: 18.2575, val_loss: 18.5144, val_MinusLogProbMetric: 18.5144

Epoch 390: val_loss did not improve from 18.36547
196/196 - 78s - loss: 18.2575 - MinusLogProbMetric: 18.2575 - val_loss: 18.5144 - val_MinusLogProbMetric: 18.5144 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 391/1000
2023-09-27 04:02:38.140 
Epoch 391/1000 
	 loss: 18.2370, MinusLogProbMetric: 18.2370, val_loss: 18.3305, val_MinusLogProbMetric: 18.3305

Epoch 391: val_loss improved from 18.36547 to 18.33054, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 79s - loss: 18.2370 - MinusLogProbMetric: 18.2370 - val_loss: 18.3305 - val_MinusLogProbMetric: 18.3305 - lr: 1.1111e-04 - 79s/epoch - 403ms/step
Epoch 392/1000
2023-09-27 04:03:57.279 
Epoch 392/1000 
	 loss: 18.2479, MinusLogProbMetric: 18.2479, val_loss: 18.4053, val_MinusLogProbMetric: 18.4053

Epoch 392: val_loss did not improve from 18.33054
196/196 - 78s - loss: 18.2479 - MinusLogProbMetric: 18.2479 - val_loss: 18.4053 - val_MinusLogProbMetric: 18.4053 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 393/1000
2023-09-27 04:05:14.640 
Epoch 393/1000 
	 loss: 18.3543, MinusLogProbMetric: 18.3543, val_loss: 18.4028, val_MinusLogProbMetric: 18.4028

Epoch 393: val_loss did not improve from 18.33054
196/196 - 77s - loss: 18.3543 - MinusLogProbMetric: 18.3543 - val_loss: 18.4028 - val_MinusLogProbMetric: 18.4028 - lr: 1.1111e-04 - 77s/epoch - 395ms/step
Epoch 394/1000
2023-09-27 04:06:32.621 
Epoch 394/1000 
	 loss: 18.1966, MinusLogProbMetric: 18.1966, val_loss: 18.5045, val_MinusLogProbMetric: 18.5045

Epoch 394: val_loss did not improve from 18.33054
196/196 - 78s - loss: 18.1966 - MinusLogProbMetric: 18.1966 - val_loss: 18.5045 - val_MinusLogProbMetric: 18.5045 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 395/1000
2023-09-27 04:07:50.622 
Epoch 395/1000 
	 loss: 18.2385, MinusLogProbMetric: 18.2385, val_loss: 18.3247, val_MinusLogProbMetric: 18.3247

Epoch 395: val_loss improved from 18.33054 to 18.32470, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 79s - loss: 18.2385 - MinusLogProbMetric: 18.2385 - val_loss: 18.3247 - val_MinusLogProbMetric: 18.3247 - lr: 1.1111e-04 - 79s/epoch - 405ms/step
Epoch 396/1000
2023-09-27 04:09:09.473 
Epoch 396/1000 
	 loss: 18.2687, MinusLogProbMetric: 18.2687, val_loss: 18.3522, val_MinusLogProbMetric: 18.3522

Epoch 396: val_loss did not improve from 18.32470
196/196 - 78s - loss: 18.2687 - MinusLogProbMetric: 18.2687 - val_loss: 18.3522 - val_MinusLogProbMetric: 18.3522 - lr: 1.1111e-04 - 78s/epoch - 395ms/step
Epoch 397/1000
2023-09-27 04:10:27.591 
Epoch 397/1000 
	 loss: 18.1721, MinusLogProbMetric: 18.1721, val_loss: 18.7988, val_MinusLogProbMetric: 18.7988

Epoch 397: val_loss did not improve from 18.32470
196/196 - 78s - loss: 18.1721 - MinusLogProbMetric: 18.1721 - val_loss: 18.7988 - val_MinusLogProbMetric: 18.7988 - lr: 1.1111e-04 - 78s/epoch - 399ms/step
Epoch 398/1000
2023-09-27 04:11:45.427 
Epoch 398/1000 
	 loss: 18.2533, MinusLogProbMetric: 18.2533, val_loss: 18.7923, val_MinusLogProbMetric: 18.7923

Epoch 398: val_loss did not improve from 18.32470
196/196 - 78s - loss: 18.2533 - MinusLogProbMetric: 18.2533 - val_loss: 18.7923 - val_MinusLogProbMetric: 18.7923 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 399/1000
2023-09-27 04:13:03.419 
Epoch 399/1000 
	 loss: 18.2482, MinusLogProbMetric: 18.2482, val_loss: 18.8256, val_MinusLogProbMetric: 18.8256

Epoch 399: val_loss did not improve from 18.32470
196/196 - 78s - loss: 18.2482 - MinusLogProbMetric: 18.2482 - val_loss: 18.8256 - val_MinusLogProbMetric: 18.8256 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 400/1000
2023-09-27 04:14:20.824 
Epoch 400/1000 
	 loss: 18.2509, MinusLogProbMetric: 18.2509, val_loss: 18.2906, val_MinusLogProbMetric: 18.2906

Epoch 400: val_loss improved from 18.32470 to 18.29058, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 79s - loss: 18.2509 - MinusLogProbMetric: 18.2509 - val_loss: 18.2906 - val_MinusLogProbMetric: 18.2906 - lr: 1.1111e-04 - 79s/epoch - 401ms/step
Epoch 401/1000
2023-09-27 04:15:39.838 
Epoch 401/1000 
	 loss: 18.2132, MinusLogProbMetric: 18.2132, val_loss: 18.5535, val_MinusLogProbMetric: 18.5535

Epoch 401: val_loss did not improve from 18.29058
196/196 - 78s - loss: 18.2132 - MinusLogProbMetric: 18.2132 - val_loss: 18.5535 - val_MinusLogProbMetric: 18.5535 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 402/1000
2023-09-27 04:16:57.914 
Epoch 402/1000 
	 loss: 18.4650, MinusLogProbMetric: 18.4650, val_loss: 18.3163, val_MinusLogProbMetric: 18.3163

Epoch 402: val_loss did not improve from 18.29058
196/196 - 78s - loss: 18.4650 - MinusLogProbMetric: 18.4650 - val_loss: 18.3163 - val_MinusLogProbMetric: 18.3163 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 403/1000
2023-09-27 04:18:15.511 
Epoch 403/1000 
	 loss: 18.2237, MinusLogProbMetric: 18.2237, val_loss: 18.7352, val_MinusLogProbMetric: 18.7352

Epoch 403: val_loss did not improve from 18.29058
196/196 - 78s - loss: 18.2237 - MinusLogProbMetric: 18.2237 - val_loss: 18.7352 - val_MinusLogProbMetric: 18.7352 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 404/1000
2023-09-27 04:19:34.182 
Epoch 404/1000 
	 loss: 18.2183, MinusLogProbMetric: 18.2183, val_loss: 18.9319, val_MinusLogProbMetric: 18.9319

Epoch 404: val_loss did not improve from 18.29058
196/196 - 79s - loss: 18.2183 - MinusLogProbMetric: 18.2183 - val_loss: 18.9319 - val_MinusLogProbMetric: 18.9319 - lr: 1.1111e-04 - 79s/epoch - 401ms/step
Epoch 405/1000
2023-09-27 04:20:51.810 
Epoch 405/1000 
	 loss: 18.2299, MinusLogProbMetric: 18.2299, val_loss: 18.2827, val_MinusLogProbMetric: 18.2827

Epoch 405: val_loss improved from 18.29058 to 18.28267, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 79s - loss: 18.2299 - MinusLogProbMetric: 18.2299 - val_loss: 18.2827 - val_MinusLogProbMetric: 18.2827 - lr: 1.1111e-04 - 79s/epoch - 403ms/step
Epoch 406/1000
2023-09-27 04:22:11.730 
Epoch 406/1000 
	 loss: 18.1602, MinusLogProbMetric: 18.1602, val_loss: 18.2227, val_MinusLogProbMetric: 18.2227

Epoch 406: val_loss improved from 18.28267 to 18.22268, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 80s - loss: 18.1602 - MinusLogProbMetric: 18.1602 - val_loss: 18.2227 - val_MinusLogProbMetric: 18.2227 - lr: 1.1111e-04 - 80s/epoch - 408ms/step
Epoch 407/1000
2023-09-27 04:23:31.140 
Epoch 407/1000 
	 loss: 18.1648, MinusLogProbMetric: 18.1648, val_loss: 18.5283, val_MinusLogProbMetric: 18.5283

Epoch 407: val_loss did not improve from 18.22268
196/196 - 78s - loss: 18.1648 - MinusLogProbMetric: 18.1648 - val_loss: 18.5283 - val_MinusLogProbMetric: 18.5283 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 408/1000
2023-09-27 04:24:49.441 
Epoch 408/1000 
	 loss: 18.1535, MinusLogProbMetric: 18.1535, val_loss: 18.2583, val_MinusLogProbMetric: 18.2583

Epoch 408: val_loss did not improve from 18.22268
196/196 - 78s - loss: 18.1535 - MinusLogProbMetric: 18.1535 - val_loss: 18.2583 - val_MinusLogProbMetric: 18.2583 - lr: 1.1111e-04 - 78s/epoch - 399ms/step
Epoch 409/1000
2023-09-27 04:26:08.402 
Epoch 409/1000 
	 loss: 18.1567, MinusLogProbMetric: 18.1567, val_loss: 18.3349, val_MinusLogProbMetric: 18.3349

Epoch 409: val_loss did not improve from 18.22268
196/196 - 79s - loss: 18.1567 - MinusLogProbMetric: 18.1567 - val_loss: 18.3349 - val_MinusLogProbMetric: 18.3349 - lr: 1.1111e-04 - 79s/epoch - 403ms/step
Epoch 410/1000
2023-09-27 04:27:27.752 
Epoch 410/1000 
	 loss: 18.1670, MinusLogProbMetric: 18.1670, val_loss: 18.3488, val_MinusLogProbMetric: 18.3488

Epoch 410: val_loss did not improve from 18.22268
196/196 - 79s - loss: 18.1670 - MinusLogProbMetric: 18.1670 - val_loss: 18.3488 - val_MinusLogProbMetric: 18.3488 - lr: 1.1111e-04 - 79s/epoch - 405ms/step
Epoch 411/1000
2023-09-27 04:28:45.622 
Epoch 411/1000 
	 loss: 18.1496, MinusLogProbMetric: 18.1496, val_loss: 18.2236, val_MinusLogProbMetric: 18.2236

Epoch 411: val_loss did not improve from 18.22268
196/196 - 78s - loss: 18.1496 - MinusLogProbMetric: 18.1496 - val_loss: 18.2236 - val_MinusLogProbMetric: 18.2236 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 412/1000
2023-09-27 04:30:04.097 
Epoch 412/1000 
	 loss: 18.2304, MinusLogProbMetric: 18.2304, val_loss: 18.3600, val_MinusLogProbMetric: 18.3600

Epoch 412: val_loss did not improve from 18.22268
196/196 - 78s - loss: 18.2304 - MinusLogProbMetric: 18.2304 - val_loss: 18.3600 - val_MinusLogProbMetric: 18.3600 - lr: 1.1111e-04 - 78s/epoch - 400ms/step
Epoch 413/1000
2023-09-27 04:31:21.913 
Epoch 413/1000 
	 loss: 18.1510, MinusLogProbMetric: 18.1510, val_loss: 18.4259, val_MinusLogProbMetric: 18.4259

Epoch 413: val_loss did not improve from 18.22268
196/196 - 78s - loss: 18.1510 - MinusLogProbMetric: 18.1510 - val_loss: 18.4259 - val_MinusLogProbMetric: 18.4259 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 414/1000
2023-09-27 04:32:39.753 
Epoch 414/1000 
	 loss: 18.2534, MinusLogProbMetric: 18.2534, val_loss: 18.6534, val_MinusLogProbMetric: 18.6534

Epoch 414: val_loss did not improve from 18.22268
196/196 - 78s - loss: 18.2534 - MinusLogProbMetric: 18.2534 - val_loss: 18.6534 - val_MinusLogProbMetric: 18.6534 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 415/1000
2023-09-27 04:33:57.671 
Epoch 415/1000 
	 loss: 18.2014, MinusLogProbMetric: 18.2014, val_loss: 18.2627, val_MinusLogProbMetric: 18.2627

Epoch 415: val_loss did not improve from 18.22268
196/196 - 78s - loss: 18.2014 - MinusLogProbMetric: 18.2014 - val_loss: 18.2627 - val_MinusLogProbMetric: 18.2627 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 416/1000
2023-09-27 04:35:15.624 
Epoch 416/1000 
	 loss: 18.1439, MinusLogProbMetric: 18.1439, val_loss: 18.3028, val_MinusLogProbMetric: 18.3028

Epoch 416: val_loss did not improve from 18.22268
196/196 - 78s - loss: 18.1439 - MinusLogProbMetric: 18.1439 - val_loss: 18.3028 - val_MinusLogProbMetric: 18.3028 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 417/1000
2023-09-27 04:36:33.309 
Epoch 417/1000 
	 loss: 18.3675, MinusLogProbMetric: 18.3675, val_loss: 18.2930, val_MinusLogProbMetric: 18.2930

Epoch 417: val_loss did not improve from 18.22268
196/196 - 78s - loss: 18.3675 - MinusLogProbMetric: 18.3675 - val_loss: 18.2930 - val_MinusLogProbMetric: 18.2930 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 418/1000
2023-09-27 04:37:51.685 
Epoch 418/1000 
	 loss: 18.1304, MinusLogProbMetric: 18.1304, val_loss: 18.3730, val_MinusLogProbMetric: 18.3730

Epoch 418: val_loss did not improve from 18.22268
196/196 - 78s - loss: 18.1304 - MinusLogProbMetric: 18.1304 - val_loss: 18.3730 - val_MinusLogProbMetric: 18.3730 - lr: 1.1111e-04 - 78s/epoch - 400ms/step
Epoch 419/1000
2023-09-27 04:39:09.415 
Epoch 419/1000 
	 loss: 18.1779, MinusLogProbMetric: 18.1779, val_loss: 18.4324, val_MinusLogProbMetric: 18.4324

Epoch 419: val_loss did not improve from 18.22268
196/196 - 78s - loss: 18.1779 - MinusLogProbMetric: 18.1779 - val_loss: 18.4324 - val_MinusLogProbMetric: 18.4324 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 420/1000
2023-09-27 04:40:27.351 
Epoch 420/1000 
	 loss: 18.1898, MinusLogProbMetric: 18.1898, val_loss: 18.3411, val_MinusLogProbMetric: 18.3411

Epoch 420: val_loss did not improve from 18.22268
196/196 - 78s - loss: 18.1898 - MinusLogProbMetric: 18.1898 - val_loss: 18.3411 - val_MinusLogProbMetric: 18.3411 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 421/1000
2023-09-27 04:41:45.528 
Epoch 421/1000 
	 loss: 18.2397, MinusLogProbMetric: 18.2397, val_loss: 18.6337, val_MinusLogProbMetric: 18.6337

Epoch 421: val_loss did not improve from 18.22268
196/196 - 78s - loss: 18.2397 - MinusLogProbMetric: 18.2397 - val_loss: 18.6337 - val_MinusLogProbMetric: 18.6337 - lr: 1.1111e-04 - 78s/epoch - 399ms/step
Epoch 422/1000
2023-09-27 04:43:03.572 
Epoch 422/1000 
	 loss: 18.1610, MinusLogProbMetric: 18.1610, val_loss: 18.2307, val_MinusLogProbMetric: 18.2307

Epoch 422: val_loss did not improve from 18.22268
196/196 - 78s - loss: 18.1610 - MinusLogProbMetric: 18.1610 - val_loss: 18.2307 - val_MinusLogProbMetric: 18.2307 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 423/1000
2023-09-27 04:44:21.192 
Epoch 423/1000 
	 loss: 18.1562, MinusLogProbMetric: 18.1562, val_loss: 18.2970, val_MinusLogProbMetric: 18.2970

Epoch 423: val_loss did not improve from 18.22268
196/196 - 78s - loss: 18.1562 - MinusLogProbMetric: 18.1562 - val_loss: 18.2970 - val_MinusLogProbMetric: 18.2970 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 424/1000
2023-09-27 04:45:38.998 
Epoch 424/1000 
	 loss: 18.1224, MinusLogProbMetric: 18.1224, val_loss: 18.5584, val_MinusLogProbMetric: 18.5584

Epoch 424: val_loss did not improve from 18.22268
196/196 - 78s - loss: 18.1224 - MinusLogProbMetric: 18.1224 - val_loss: 18.5584 - val_MinusLogProbMetric: 18.5584 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 425/1000
2023-09-27 04:46:56.401 
Epoch 425/1000 
	 loss: 18.1073, MinusLogProbMetric: 18.1073, val_loss: 18.3455, val_MinusLogProbMetric: 18.3455

Epoch 425: val_loss did not improve from 18.22268
196/196 - 77s - loss: 18.1073 - MinusLogProbMetric: 18.1073 - val_loss: 18.3455 - val_MinusLogProbMetric: 18.3455 - lr: 1.1111e-04 - 77s/epoch - 395ms/step
Epoch 426/1000
2023-09-27 04:48:14.023 
Epoch 426/1000 
	 loss: 18.1373, MinusLogProbMetric: 18.1373, val_loss: 18.4783, val_MinusLogProbMetric: 18.4783

Epoch 426: val_loss did not improve from 18.22268
196/196 - 78s - loss: 18.1373 - MinusLogProbMetric: 18.1373 - val_loss: 18.4783 - val_MinusLogProbMetric: 18.4783 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 427/1000
2023-09-27 04:49:31.725 
Epoch 427/1000 
	 loss: 18.1541, MinusLogProbMetric: 18.1541, val_loss: 18.3861, val_MinusLogProbMetric: 18.3861

Epoch 427: val_loss did not improve from 18.22268
196/196 - 78s - loss: 18.1541 - MinusLogProbMetric: 18.1541 - val_loss: 18.3861 - val_MinusLogProbMetric: 18.3861 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 428/1000
2023-09-27 04:50:49.673 
Epoch 428/1000 
	 loss: 18.1503, MinusLogProbMetric: 18.1503, val_loss: 18.5490, val_MinusLogProbMetric: 18.5490

Epoch 428: val_loss did not improve from 18.22268
196/196 - 78s - loss: 18.1503 - MinusLogProbMetric: 18.1503 - val_loss: 18.5490 - val_MinusLogProbMetric: 18.5490 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 429/1000
2023-09-27 04:52:07.231 
Epoch 429/1000 
	 loss: 18.6324, MinusLogProbMetric: 18.6324, val_loss: 18.5402, val_MinusLogProbMetric: 18.5402

Epoch 429: val_loss did not improve from 18.22268
196/196 - 78s - loss: 18.6324 - MinusLogProbMetric: 18.6324 - val_loss: 18.5402 - val_MinusLogProbMetric: 18.5402 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 430/1000
2023-09-27 04:53:25.279 
Epoch 430/1000 
	 loss: 18.0834, MinusLogProbMetric: 18.0834, val_loss: 18.2446, val_MinusLogProbMetric: 18.2446

Epoch 430: val_loss did not improve from 18.22268
196/196 - 78s - loss: 18.0834 - MinusLogProbMetric: 18.0834 - val_loss: 18.2446 - val_MinusLogProbMetric: 18.2446 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 431/1000
2023-09-27 04:54:43.120 
Epoch 431/1000 
	 loss: 18.2570, MinusLogProbMetric: 18.2570, val_loss: 18.2852, val_MinusLogProbMetric: 18.2852

Epoch 431: val_loss did not improve from 18.22268
196/196 - 78s - loss: 18.2570 - MinusLogProbMetric: 18.2570 - val_loss: 18.2852 - val_MinusLogProbMetric: 18.2852 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 432/1000
2023-09-27 04:56:00.673 
Epoch 432/1000 
	 loss: 18.1752, MinusLogProbMetric: 18.1752, val_loss: 18.4591, val_MinusLogProbMetric: 18.4591

Epoch 432: val_loss did not improve from 18.22268
196/196 - 78s - loss: 18.1752 - MinusLogProbMetric: 18.1752 - val_loss: 18.4591 - val_MinusLogProbMetric: 18.4591 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 433/1000
2023-09-27 04:57:17.402 
Epoch 433/1000 
	 loss: 18.1268, MinusLogProbMetric: 18.1268, val_loss: 18.3034, val_MinusLogProbMetric: 18.3034

Epoch 433: val_loss did not improve from 18.22268
196/196 - 77s - loss: 18.1268 - MinusLogProbMetric: 18.1268 - val_loss: 18.3034 - val_MinusLogProbMetric: 18.3034 - lr: 1.1111e-04 - 77s/epoch - 391ms/step
Epoch 434/1000
2023-09-27 04:58:26.377 
Epoch 434/1000 
	 loss: 18.1434, MinusLogProbMetric: 18.1434, val_loss: 18.2517, val_MinusLogProbMetric: 18.2517

Epoch 434: val_loss did not improve from 18.22268
196/196 - 69s - loss: 18.1434 - MinusLogProbMetric: 18.1434 - val_loss: 18.2517 - val_MinusLogProbMetric: 18.2517 - lr: 1.1111e-04 - 69s/epoch - 352ms/step
Epoch 435/1000
2023-09-27 04:59:42.294 
Epoch 435/1000 
	 loss: 18.1040, MinusLogProbMetric: 18.1040, val_loss: 18.2804, val_MinusLogProbMetric: 18.2804

Epoch 435: val_loss did not improve from 18.22268
196/196 - 76s - loss: 18.1040 - MinusLogProbMetric: 18.1040 - val_loss: 18.2804 - val_MinusLogProbMetric: 18.2804 - lr: 1.1111e-04 - 76s/epoch - 387ms/step
Epoch 436/1000
2023-09-27 05:01:00.066 
Epoch 436/1000 
	 loss: 18.1784, MinusLogProbMetric: 18.1784, val_loss: 18.3133, val_MinusLogProbMetric: 18.3133

Epoch 436: val_loss did not improve from 18.22268
196/196 - 78s - loss: 18.1784 - MinusLogProbMetric: 18.1784 - val_loss: 18.3133 - val_MinusLogProbMetric: 18.3133 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 437/1000
2023-09-27 05:02:17.397 
Epoch 437/1000 
	 loss: 18.1040, MinusLogProbMetric: 18.1040, val_loss: 18.3958, val_MinusLogProbMetric: 18.3958

Epoch 437: val_loss did not improve from 18.22268
196/196 - 77s - loss: 18.1040 - MinusLogProbMetric: 18.1040 - val_loss: 18.3958 - val_MinusLogProbMetric: 18.3958 - lr: 1.1111e-04 - 77s/epoch - 395ms/step
Epoch 438/1000
2023-09-27 05:03:34.941 
Epoch 438/1000 
	 loss: 18.1248, MinusLogProbMetric: 18.1248, val_loss: 18.3694, val_MinusLogProbMetric: 18.3694

Epoch 438: val_loss did not improve from 18.22268
196/196 - 78s - loss: 18.1248 - MinusLogProbMetric: 18.1248 - val_loss: 18.3694 - val_MinusLogProbMetric: 18.3694 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 439/1000
2023-09-27 05:04:52.516 
Epoch 439/1000 
	 loss: 18.2714, MinusLogProbMetric: 18.2714, val_loss: 18.2916, val_MinusLogProbMetric: 18.2916

Epoch 439: val_loss did not improve from 18.22268
196/196 - 78s - loss: 18.2714 - MinusLogProbMetric: 18.2714 - val_loss: 18.2916 - val_MinusLogProbMetric: 18.2916 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 440/1000
2023-09-27 05:06:10.549 
Epoch 440/1000 
	 loss: 18.0843, MinusLogProbMetric: 18.0843, val_loss: 18.2239, val_MinusLogProbMetric: 18.2239

Epoch 440: val_loss did not improve from 18.22268
196/196 - 78s - loss: 18.0843 - MinusLogProbMetric: 18.0843 - val_loss: 18.2239 - val_MinusLogProbMetric: 18.2239 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 441/1000
2023-09-27 05:07:28.141 
Epoch 441/1000 
	 loss: 18.3207, MinusLogProbMetric: 18.3207, val_loss: 18.3166, val_MinusLogProbMetric: 18.3166

Epoch 441: val_loss did not improve from 18.22268
196/196 - 78s - loss: 18.3207 - MinusLogProbMetric: 18.3207 - val_loss: 18.3166 - val_MinusLogProbMetric: 18.3166 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 442/1000
2023-09-27 05:08:45.817 
Epoch 442/1000 
	 loss: 18.0588, MinusLogProbMetric: 18.0588, val_loss: 18.2589, val_MinusLogProbMetric: 18.2589

Epoch 442: val_loss did not improve from 18.22268
196/196 - 78s - loss: 18.0588 - MinusLogProbMetric: 18.0588 - val_loss: 18.2589 - val_MinusLogProbMetric: 18.2589 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 443/1000
2023-09-27 05:10:03.161 
Epoch 443/1000 
	 loss: 18.0752, MinusLogProbMetric: 18.0752, val_loss: 18.4508, val_MinusLogProbMetric: 18.4508

Epoch 443: val_loss did not improve from 18.22268
196/196 - 77s - loss: 18.0752 - MinusLogProbMetric: 18.0752 - val_loss: 18.4508 - val_MinusLogProbMetric: 18.4508 - lr: 1.1111e-04 - 77s/epoch - 395ms/step
Epoch 444/1000
2023-09-27 05:11:21.062 
Epoch 444/1000 
	 loss: 18.1983, MinusLogProbMetric: 18.1983, val_loss: 18.4598, val_MinusLogProbMetric: 18.4598

Epoch 444: val_loss did not improve from 18.22268
196/196 - 78s - loss: 18.1983 - MinusLogProbMetric: 18.1983 - val_loss: 18.4598 - val_MinusLogProbMetric: 18.4598 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 445/1000
2023-09-27 05:12:38.371 
Epoch 445/1000 
	 loss: 18.0954, MinusLogProbMetric: 18.0954, val_loss: 18.4253, val_MinusLogProbMetric: 18.4253

Epoch 445: val_loss did not improve from 18.22268
196/196 - 77s - loss: 18.0954 - MinusLogProbMetric: 18.0954 - val_loss: 18.4253 - val_MinusLogProbMetric: 18.4253 - lr: 1.1111e-04 - 77s/epoch - 394ms/step
Epoch 446/1000
2023-09-27 05:13:56.380 
Epoch 446/1000 
	 loss: 18.0311, MinusLogProbMetric: 18.0311, val_loss: 18.4642, val_MinusLogProbMetric: 18.4642

Epoch 446: val_loss did not improve from 18.22268
196/196 - 78s - loss: 18.0311 - MinusLogProbMetric: 18.0311 - val_loss: 18.4642 - val_MinusLogProbMetric: 18.4642 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 447/1000
2023-09-27 05:15:14.104 
Epoch 447/1000 
	 loss: 18.0672, MinusLogProbMetric: 18.0672, val_loss: 18.1350, val_MinusLogProbMetric: 18.1350

Epoch 447: val_loss improved from 18.22268 to 18.13505, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 79s - loss: 18.0672 - MinusLogProbMetric: 18.0672 - val_loss: 18.1350 - val_MinusLogProbMetric: 18.1350 - lr: 1.1111e-04 - 79s/epoch - 403ms/step
Epoch 448/1000
2023-09-27 05:16:32.859 
Epoch 448/1000 
	 loss: 18.0790, MinusLogProbMetric: 18.0790, val_loss: 18.4541, val_MinusLogProbMetric: 18.4541

Epoch 448: val_loss did not improve from 18.13505
196/196 - 77s - loss: 18.0790 - MinusLogProbMetric: 18.0790 - val_loss: 18.4541 - val_MinusLogProbMetric: 18.4541 - lr: 1.1111e-04 - 77s/epoch - 395ms/step
Epoch 449/1000
2023-09-27 05:17:50.264 
Epoch 449/1000 
	 loss: 18.0701, MinusLogProbMetric: 18.0701, val_loss: 18.2194, val_MinusLogProbMetric: 18.2194

Epoch 449: val_loss did not improve from 18.13505
196/196 - 77s - loss: 18.0701 - MinusLogProbMetric: 18.0701 - val_loss: 18.2194 - val_MinusLogProbMetric: 18.2194 - lr: 1.1111e-04 - 77s/epoch - 395ms/step
Epoch 450/1000
2023-09-27 05:19:08.463 
Epoch 450/1000 
	 loss: 18.0906, MinusLogProbMetric: 18.0906, val_loss: 18.9075, val_MinusLogProbMetric: 18.9075

Epoch 450: val_loss did not improve from 18.13505
196/196 - 78s - loss: 18.0906 - MinusLogProbMetric: 18.0906 - val_loss: 18.9075 - val_MinusLogProbMetric: 18.9075 - lr: 1.1111e-04 - 78s/epoch - 399ms/step
Epoch 451/1000
2023-09-27 05:20:26.010 
Epoch 451/1000 
	 loss: 18.1135, MinusLogProbMetric: 18.1135, val_loss: 18.2396, val_MinusLogProbMetric: 18.2396

Epoch 451: val_loss did not improve from 18.13505
196/196 - 78s - loss: 18.1135 - MinusLogProbMetric: 18.1135 - val_loss: 18.2396 - val_MinusLogProbMetric: 18.2396 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 452/1000
2023-09-27 05:21:41.387 
Epoch 452/1000 
	 loss: 18.0789, MinusLogProbMetric: 18.0789, val_loss: 18.3515, val_MinusLogProbMetric: 18.3515

Epoch 452: val_loss did not improve from 18.13505
196/196 - 75s - loss: 18.0789 - MinusLogProbMetric: 18.0789 - val_loss: 18.3515 - val_MinusLogProbMetric: 18.3515 - lr: 1.1111e-04 - 75s/epoch - 385ms/step
Epoch 453/1000
2023-09-27 05:22:52.709 
Epoch 453/1000 
	 loss: 17.9958, MinusLogProbMetric: 17.9958, val_loss: 18.2123, val_MinusLogProbMetric: 18.2123

Epoch 453: val_loss did not improve from 18.13505
196/196 - 71s - loss: 17.9958 - MinusLogProbMetric: 17.9958 - val_loss: 18.2123 - val_MinusLogProbMetric: 18.2123 - lr: 1.1111e-04 - 71s/epoch - 364ms/step
Epoch 454/1000
2023-09-27 05:24:07.452 
Epoch 454/1000 
	 loss: 18.0474, MinusLogProbMetric: 18.0474, val_loss: 18.2378, val_MinusLogProbMetric: 18.2378

Epoch 454: val_loss did not improve from 18.13505
196/196 - 75s - loss: 18.0474 - MinusLogProbMetric: 18.0474 - val_loss: 18.2378 - val_MinusLogProbMetric: 18.2378 - lr: 1.1111e-04 - 75s/epoch - 381ms/step
Epoch 455/1000
2023-09-27 05:25:23.447 
Epoch 455/1000 
	 loss: 18.0591, MinusLogProbMetric: 18.0591, val_loss: 18.2340, val_MinusLogProbMetric: 18.2340

Epoch 455: val_loss did not improve from 18.13505
196/196 - 76s - loss: 18.0591 - MinusLogProbMetric: 18.0591 - val_loss: 18.2340 - val_MinusLogProbMetric: 18.2340 - lr: 1.1111e-04 - 76s/epoch - 388ms/step
Epoch 456/1000
2023-09-27 05:26:41.209 
Epoch 456/1000 
	 loss: 18.1773, MinusLogProbMetric: 18.1773, val_loss: 18.1863, val_MinusLogProbMetric: 18.1863

Epoch 456: val_loss did not improve from 18.13505
196/196 - 78s - loss: 18.1773 - MinusLogProbMetric: 18.1773 - val_loss: 18.1863 - val_MinusLogProbMetric: 18.1863 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 457/1000
2023-09-27 05:27:58.532 
Epoch 457/1000 
	 loss: 18.0974, MinusLogProbMetric: 18.0974, val_loss: 18.1479, val_MinusLogProbMetric: 18.1479

Epoch 457: val_loss did not improve from 18.13505
196/196 - 77s - loss: 18.0974 - MinusLogProbMetric: 18.0974 - val_loss: 18.1479 - val_MinusLogProbMetric: 18.1479 - lr: 1.1111e-04 - 77s/epoch - 394ms/step
Epoch 458/1000
2023-09-27 05:29:15.332 
Epoch 458/1000 
	 loss: 17.9810, MinusLogProbMetric: 17.9810, val_loss: 18.2575, val_MinusLogProbMetric: 18.2575

Epoch 458: val_loss did not improve from 18.13505
196/196 - 77s - loss: 17.9810 - MinusLogProbMetric: 17.9810 - val_loss: 18.2575 - val_MinusLogProbMetric: 18.2575 - lr: 1.1111e-04 - 77s/epoch - 392ms/step
Epoch 459/1000
2023-09-27 05:30:32.541 
Epoch 459/1000 
	 loss: 18.1973, MinusLogProbMetric: 18.1973, val_loss: 18.4139, val_MinusLogProbMetric: 18.4139

Epoch 459: val_loss did not improve from 18.13505
196/196 - 77s - loss: 18.1973 - MinusLogProbMetric: 18.1973 - val_loss: 18.4139 - val_MinusLogProbMetric: 18.4139 - lr: 1.1111e-04 - 77s/epoch - 394ms/step
Epoch 460/1000
2023-09-27 05:31:49.112 
Epoch 460/1000 
	 loss: 18.0351, MinusLogProbMetric: 18.0351, val_loss: 18.2656, val_MinusLogProbMetric: 18.2656

Epoch 460: val_loss did not improve from 18.13505
196/196 - 77s - loss: 18.0351 - MinusLogProbMetric: 18.0351 - val_loss: 18.2656 - val_MinusLogProbMetric: 18.2656 - lr: 1.1111e-04 - 77s/epoch - 391ms/step
Epoch 461/1000
2023-09-27 05:33:02.490 
Epoch 461/1000 
	 loss: 18.0728, MinusLogProbMetric: 18.0728, val_loss: 18.4020, val_MinusLogProbMetric: 18.4020

Epoch 461: val_loss did not improve from 18.13505
196/196 - 73s - loss: 18.0728 - MinusLogProbMetric: 18.0728 - val_loss: 18.4020 - val_MinusLogProbMetric: 18.4020 - lr: 1.1111e-04 - 73s/epoch - 374ms/step
Epoch 462/1000
2023-09-27 05:34:11.265 
Epoch 462/1000 
	 loss: 18.0641, MinusLogProbMetric: 18.0641, val_loss: 18.2364, val_MinusLogProbMetric: 18.2364

Epoch 462: val_loss did not improve from 18.13505
196/196 - 69s - loss: 18.0641 - MinusLogProbMetric: 18.0641 - val_loss: 18.2364 - val_MinusLogProbMetric: 18.2364 - lr: 1.1111e-04 - 69s/epoch - 351ms/step
Epoch 463/1000
2023-09-27 05:35:24.728 
Epoch 463/1000 
	 loss: 18.0369, MinusLogProbMetric: 18.0369, val_loss: 19.1604, val_MinusLogProbMetric: 19.1604

Epoch 463: val_loss did not improve from 18.13505
196/196 - 73s - loss: 18.0369 - MinusLogProbMetric: 18.0369 - val_loss: 19.1604 - val_MinusLogProbMetric: 19.1604 - lr: 1.1111e-04 - 73s/epoch - 375ms/step
Epoch 464/1000
2023-09-27 05:36:30.802 
Epoch 464/1000 
	 loss: 18.1147, MinusLogProbMetric: 18.1147, val_loss: 18.1954, val_MinusLogProbMetric: 18.1954

Epoch 464: val_loss did not improve from 18.13505
196/196 - 66s - loss: 18.1147 - MinusLogProbMetric: 18.1147 - val_loss: 18.1954 - val_MinusLogProbMetric: 18.1954 - lr: 1.1111e-04 - 66s/epoch - 337ms/step
Epoch 465/1000
2023-09-27 05:37:47.889 
Epoch 465/1000 
	 loss: 18.0018, MinusLogProbMetric: 18.0018, val_loss: 18.3391, val_MinusLogProbMetric: 18.3391

Epoch 465: val_loss did not improve from 18.13505
196/196 - 77s - loss: 18.0018 - MinusLogProbMetric: 18.0018 - val_loss: 18.3391 - val_MinusLogProbMetric: 18.3391 - lr: 1.1111e-04 - 77s/epoch - 393ms/step
Epoch 466/1000
2023-09-27 05:39:04.640 
Epoch 466/1000 
	 loss: 18.0469, MinusLogProbMetric: 18.0469, val_loss: 18.2397, val_MinusLogProbMetric: 18.2397

Epoch 466: val_loss did not improve from 18.13505
196/196 - 77s - loss: 18.0469 - MinusLogProbMetric: 18.0469 - val_loss: 18.2397 - val_MinusLogProbMetric: 18.2397 - lr: 1.1111e-04 - 77s/epoch - 392ms/step
Epoch 467/1000
2023-09-27 05:40:22.204 
Epoch 467/1000 
	 loss: 18.1177, MinusLogProbMetric: 18.1177, val_loss: 18.0866, val_MinusLogProbMetric: 18.0866

Epoch 467: val_loss improved from 18.13505 to 18.08658, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 79s - loss: 18.1177 - MinusLogProbMetric: 18.1177 - val_loss: 18.0866 - val_MinusLogProbMetric: 18.0866 - lr: 1.1111e-04 - 79s/epoch - 402ms/step
Epoch 468/1000
2023-09-27 05:41:41.063 
Epoch 468/1000 
	 loss: 17.9768, MinusLogProbMetric: 17.9768, val_loss: 18.2444, val_MinusLogProbMetric: 18.2444

Epoch 468: val_loss did not improve from 18.08658
196/196 - 78s - loss: 17.9768 - MinusLogProbMetric: 17.9768 - val_loss: 18.2444 - val_MinusLogProbMetric: 18.2444 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 469/1000
2023-09-27 05:42:58.237 
Epoch 469/1000 
	 loss: 18.1161, MinusLogProbMetric: 18.1161, val_loss: 18.2462, val_MinusLogProbMetric: 18.2462

Epoch 469: val_loss did not improve from 18.08658
196/196 - 77s - loss: 18.1161 - MinusLogProbMetric: 18.1161 - val_loss: 18.2462 - val_MinusLogProbMetric: 18.2462 - lr: 1.1111e-04 - 77s/epoch - 394ms/step
Epoch 470/1000
2023-09-27 05:44:15.031 
Epoch 470/1000 
	 loss: 18.0139, MinusLogProbMetric: 18.0139, val_loss: 18.2014, val_MinusLogProbMetric: 18.2014

Epoch 470: val_loss did not improve from 18.08658
196/196 - 77s - loss: 18.0139 - MinusLogProbMetric: 18.0139 - val_loss: 18.2014 - val_MinusLogProbMetric: 18.2014 - lr: 1.1111e-04 - 77s/epoch - 392ms/step
Epoch 471/1000
2023-09-27 05:45:32.361 
Epoch 471/1000 
	 loss: 17.9772, MinusLogProbMetric: 17.9772, val_loss: 18.2809, val_MinusLogProbMetric: 18.2809

Epoch 471: val_loss did not improve from 18.08658
196/196 - 77s - loss: 17.9772 - MinusLogProbMetric: 17.9772 - val_loss: 18.2809 - val_MinusLogProbMetric: 18.2809 - lr: 1.1111e-04 - 77s/epoch - 395ms/step
Epoch 472/1000
2023-09-27 05:46:50.276 
Epoch 472/1000 
	 loss: 18.0564, MinusLogProbMetric: 18.0564, val_loss: 18.2278, val_MinusLogProbMetric: 18.2278

Epoch 472: val_loss did not improve from 18.08658
196/196 - 78s - loss: 18.0564 - MinusLogProbMetric: 18.0564 - val_loss: 18.2278 - val_MinusLogProbMetric: 18.2278 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 473/1000
2023-09-27 05:48:07.362 
Epoch 473/1000 
	 loss: 17.9985, MinusLogProbMetric: 17.9985, val_loss: 18.1292, val_MinusLogProbMetric: 18.1292

Epoch 473: val_loss did not improve from 18.08658
196/196 - 77s - loss: 17.9985 - MinusLogProbMetric: 17.9985 - val_loss: 18.1292 - val_MinusLogProbMetric: 18.1292 - lr: 1.1111e-04 - 77s/epoch - 393ms/step
Epoch 474/1000
2023-09-27 05:49:24.432 
Epoch 474/1000 
	 loss: 18.1106, MinusLogProbMetric: 18.1106, val_loss: 18.1425, val_MinusLogProbMetric: 18.1425

Epoch 474: val_loss did not improve from 18.08658
196/196 - 77s - loss: 18.1106 - MinusLogProbMetric: 18.1106 - val_loss: 18.1425 - val_MinusLogProbMetric: 18.1425 - lr: 1.1111e-04 - 77s/epoch - 393ms/step
Epoch 475/1000
2023-09-27 05:50:42.327 
Epoch 475/1000 
	 loss: 18.0139, MinusLogProbMetric: 18.0139, val_loss: 18.2121, val_MinusLogProbMetric: 18.2121

Epoch 475: val_loss did not improve from 18.08658
196/196 - 78s - loss: 18.0139 - MinusLogProbMetric: 18.0139 - val_loss: 18.2121 - val_MinusLogProbMetric: 18.2121 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 476/1000
2023-09-27 05:51:59.787 
Epoch 476/1000 
	 loss: 17.9610, MinusLogProbMetric: 17.9610, val_loss: 18.0773, val_MinusLogProbMetric: 18.0773

Epoch 476: val_loss improved from 18.08658 to 18.07731, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 79s - loss: 17.9610 - MinusLogProbMetric: 17.9610 - val_loss: 18.0773 - val_MinusLogProbMetric: 18.0773 - lr: 1.1111e-04 - 79s/epoch - 404ms/step
Epoch 477/1000
2023-09-27 05:53:19.784 
Epoch 477/1000 
	 loss: 17.9842, MinusLogProbMetric: 17.9842, val_loss: 18.3177, val_MinusLogProbMetric: 18.3177

Epoch 477: val_loss did not improve from 18.07731
196/196 - 78s - loss: 17.9842 - MinusLogProbMetric: 17.9842 - val_loss: 18.3177 - val_MinusLogProbMetric: 18.3177 - lr: 1.1111e-04 - 78s/epoch - 399ms/step
Epoch 478/1000
2023-09-27 05:54:37.301 
Epoch 478/1000 
	 loss: 17.9834, MinusLogProbMetric: 17.9834, val_loss: 18.2906, val_MinusLogProbMetric: 18.2906

Epoch 478: val_loss did not improve from 18.07731
196/196 - 78s - loss: 17.9834 - MinusLogProbMetric: 17.9834 - val_loss: 18.2906 - val_MinusLogProbMetric: 18.2906 - lr: 1.1111e-04 - 78s/epoch - 395ms/step
Epoch 479/1000
2023-09-27 05:55:54.445 
Epoch 479/1000 
	 loss: 17.9453, MinusLogProbMetric: 17.9453, val_loss: 18.2774, val_MinusLogProbMetric: 18.2774

Epoch 479: val_loss did not improve from 18.07731
196/196 - 77s - loss: 17.9453 - MinusLogProbMetric: 17.9453 - val_loss: 18.2774 - val_MinusLogProbMetric: 18.2774 - lr: 1.1111e-04 - 77s/epoch - 394ms/step
Epoch 480/1000
2023-09-27 05:57:12.344 
Epoch 480/1000 
	 loss: 17.9583, MinusLogProbMetric: 17.9583, val_loss: 18.4753, val_MinusLogProbMetric: 18.4753

Epoch 480: val_loss did not improve from 18.07731
196/196 - 78s - loss: 17.9583 - MinusLogProbMetric: 17.9583 - val_loss: 18.4753 - val_MinusLogProbMetric: 18.4753 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 481/1000
2023-09-27 05:58:30.499 
Epoch 481/1000 
	 loss: 18.0493, MinusLogProbMetric: 18.0493, val_loss: 18.2462, val_MinusLogProbMetric: 18.2462

Epoch 481: val_loss did not improve from 18.07731
196/196 - 78s - loss: 18.0493 - MinusLogProbMetric: 18.0493 - val_loss: 18.2462 - val_MinusLogProbMetric: 18.2462 - lr: 1.1111e-04 - 78s/epoch - 399ms/step
Epoch 482/1000
2023-09-27 05:59:47.794 
Epoch 482/1000 
	 loss: 18.0321, MinusLogProbMetric: 18.0321, val_loss: 18.2173, val_MinusLogProbMetric: 18.2173

Epoch 482: val_loss did not improve from 18.07731
196/196 - 77s - loss: 18.0321 - MinusLogProbMetric: 18.0321 - val_loss: 18.2173 - val_MinusLogProbMetric: 18.2173 - lr: 1.1111e-04 - 77s/epoch - 394ms/step
Epoch 483/1000
2023-09-27 06:01:05.429 
Epoch 483/1000 
	 loss: 18.1309, MinusLogProbMetric: 18.1309, val_loss: 18.3689, val_MinusLogProbMetric: 18.3689

Epoch 483: val_loss did not improve from 18.07731
196/196 - 78s - loss: 18.1309 - MinusLogProbMetric: 18.1309 - val_loss: 18.3689 - val_MinusLogProbMetric: 18.3689 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 484/1000
2023-09-27 06:02:22.835 
Epoch 484/1000 
	 loss: 17.9813, MinusLogProbMetric: 17.9813, val_loss: 18.3559, val_MinusLogProbMetric: 18.3559

Epoch 484: val_loss did not improve from 18.07731
196/196 - 77s - loss: 17.9813 - MinusLogProbMetric: 17.9813 - val_loss: 18.3559 - val_MinusLogProbMetric: 18.3559 - lr: 1.1111e-04 - 77s/epoch - 395ms/step
Epoch 485/1000
2023-09-27 06:03:40.448 
Epoch 485/1000 
	 loss: 17.9544, MinusLogProbMetric: 17.9544, val_loss: 18.1348, val_MinusLogProbMetric: 18.1348

Epoch 485: val_loss did not improve from 18.07731
196/196 - 78s - loss: 17.9544 - MinusLogProbMetric: 17.9544 - val_loss: 18.1348 - val_MinusLogProbMetric: 18.1348 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 486/1000
2023-09-27 06:04:58.737 
Epoch 486/1000 
	 loss: 17.9991, MinusLogProbMetric: 17.9991, val_loss: 18.5569, val_MinusLogProbMetric: 18.5569

Epoch 486: val_loss did not improve from 18.07731
196/196 - 78s - loss: 17.9991 - MinusLogProbMetric: 17.9991 - val_loss: 18.5569 - val_MinusLogProbMetric: 18.5569 - lr: 1.1111e-04 - 78s/epoch - 399ms/step
Epoch 487/1000
2023-09-27 06:06:16.349 
Epoch 487/1000 
	 loss: 17.9910, MinusLogProbMetric: 17.9910, val_loss: 18.2414, val_MinusLogProbMetric: 18.2414

Epoch 487: val_loss did not improve from 18.07731
196/196 - 78s - loss: 17.9910 - MinusLogProbMetric: 17.9910 - val_loss: 18.2414 - val_MinusLogProbMetric: 18.2414 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 488/1000
2023-09-27 06:07:34.437 
Epoch 488/1000 
	 loss: 17.9702, MinusLogProbMetric: 17.9702, val_loss: 18.1798, val_MinusLogProbMetric: 18.1798

Epoch 488: val_loss did not improve from 18.07731
196/196 - 78s - loss: 17.9702 - MinusLogProbMetric: 17.9702 - val_loss: 18.1798 - val_MinusLogProbMetric: 18.1798 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 489/1000
2023-09-27 06:08:51.989 
Epoch 489/1000 
	 loss: 17.9399, MinusLogProbMetric: 17.9399, val_loss: 18.0612, val_MinusLogProbMetric: 18.0612

Epoch 489: val_loss improved from 18.07731 to 18.06120, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 79s - loss: 17.9399 - MinusLogProbMetric: 17.9399 - val_loss: 18.0612 - val_MinusLogProbMetric: 18.0612 - lr: 1.1111e-04 - 79s/epoch - 404ms/step
Epoch 490/1000
2023-09-27 06:10:11.599 
Epoch 490/1000 
	 loss: 17.9625, MinusLogProbMetric: 17.9625, val_loss: 18.2894, val_MinusLogProbMetric: 18.2894

Epoch 490: val_loss did not improve from 18.06120
196/196 - 78s - loss: 17.9625 - MinusLogProbMetric: 17.9625 - val_loss: 18.2894 - val_MinusLogProbMetric: 18.2894 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 491/1000
2023-09-27 06:11:29.422 
Epoch 491/1000 
	 loss: 18.0954, MinusLogProbMetric: 18.0954, val_loss: 18.1100, val_MinusLogProbMetric: 18.1100

Epoch 491: val_loss did not improve from 18.06120
196/196 - 78s - loss: 18.0954 - MinusLogProbMetric: 18.0954 - val_loss: 18.1100 - val_MinusLogProbMetric: 18.1100 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 492/1000
2023-09-27 06:12:46.811 
Epoch 492/1000 
	 loss: 18.0913, MinusLogProbMetric: 18.0913, val_loss: 18.4928, val_MinusLogProbMetric: 18.4928

Epoch 492: val_loss did not improve from 18.06120
196/196 - 77s - loss: 18.0913 - MinusLogProbMetric: 18.0913 - val_loss: 18.4928 - val_MinusLogProbMetric: 18.4928 - lr: 1.1111e-04 - 77s/epoch - 395ms/step
Epoch 493/1000
2023-09-27 06:14:04.628 
Epoch 493/1000 
	 loss: 18.0419, MinusLogProbMetric: 18.0419, val_loss: 18.0399, val_MinusLogProbMetric: 18.0399

Epoch 493: val_loss improved from 18.06120 to 18.03988, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 79s - loss: 18.0419 - MinusLogProbMetric: 18.0419 - val_loss: 18.0399 - val_MinusLogProbMetric: 18.0399 - lr: 1.1111e-04 - 79s/epoch - 405ms/step
Epoch 494/1000
2023-09-27 06:15:23.385 
Epoch 494/1000 
	 loss: 17.9652, MinusLogProbMetric: 17.9652, val_loss: 18.4748, val_MinusLogProbMetric: 18.4748

Epoch 494: val_loss did not improve from 18.03988
196/196 - 77s - loss: 17.9652 - MinusLogProbMetric: 17.9652 - val_loss: 18.4748 - val_MinusLogProbMetric: 18.4748 - lr: 1.1111e-04 - 77s/epoch - 394ms/step
Epoch 495/1000
2023-09-27 06:16:40.981 
Epoch 495/1000 
	 loss: 17.9642, MinusLogProbMetric: 17.9642, val_loss: 18.1681, val_MinusLogProbMetric: 18.1681

Epoch 495: val_loss did not improve from 18.03988
196/196 - 78s - loss: 17.9642 - MinusLogProbMetric: 17.9642 - val_loss: 18.1681 - val_MinusLogProbMetric: 18.1681 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 496/1000
2023-09-27 06:17:58.671 
Epoch 496/1000 
	 loss: 17.9317, MinusLogProbMetric: 17.9317, val_loss: 18.1959, val_MinusLogProbMetric: 18.1959

Epoch 496: val_loss did not improve from 18.03988
196/196 - 78s - loss: 17.9317 - MinusLogProbMetric: 17.9317 - val_loss: 18.1959 - val_MinusLogProbMetric: 18.1959 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 497/1000
2023-09-27 06:19:16.181 
Epoch 497/1000 
	 loss: 18.0256, MinusLogProbMetric: 18.0256, val_loss: 18.0584, val_MinusLogProbMetric: 18.0584

Epoch 497: val_loss did not improve from 18.03988
196/196 - 78s - loss: 18.0256 - MinusLogProbMetric: 18.0256 - val_loss: 18.0584 - val_MinusLogProbMetric: 18.0584 - lr: 1.1111e-04 - 78s/epoch - 395ms/step
Epoch 498/1000
2023-09-27 06:20:33.463 
Epoch 498/1000 
	 loss: 17.9482, MinusLogProbMetric: 17.9482, val_loss: 18.2359, val_MinusLogProbMetric: 18.2359

Epoch 498: val_loss did not improve from 18.03988
196/196 - 77s - loss: 17.9482 - MinusLogProbMetric: 17.9482 - val_loss: 18.2359 - val_MinusLogProbMetric: 18.2359 - lr: 1.1111e-04 - 77s/epoch - 394ms/step
Epoch 499/1000
2023-09-27 06:21:51.659 
Epoch 499/1000 
	 loss: 17.9508, MinusLogProbMetric: 17.9508, val_loss: 18.0938, val_MinusLogProbMetric: 18.0938

Epoch 499: val_loss did not improve from 18.03988
196/196 - 78s - loss: 17.9508 - MinusLogProbMetric: 17.9508 - val_loss: 18.0938 - val_MinusLogProbMetric: 18.0938 - lr: 1.1111e-04 - 78s/epoch - 399ms/step
Epoch 500/1000
2023-09-27 06:23:09.166 
Epoch 500/1000 
	 loss: 17.9543, MinusLogProbMetric: 17.9543, val_loss: 18.0555, val_MinusLogProbMetric: 18.0555

Epoch 500: val_loss did not improve from 18.03988
196/196 - 78s - loss: 17.9543 - MinusLogProbMetric: 17.9543 - val_loss: 18.0555 - val_MinusLogProbMetric: 18.0555 - lr: 1.1111e-04 - 78s/epoch - 395ms/step
Epoch 501/1000
2023-09-27 06:24:26.860 
Epoch 501/1000 
	 loss: 17.9393, MinusLogProbMetric: 17.9393, val_loss: 18.1395, val_MinusLogProbMetric: 18.1395

Epoch 501: val_loss did not improve from 18.03988
196/196 - 78s - loss: 17.9393 - MinusLogProbMetric: 17.9393 - val_loss: 18.1395 - val_MinusLogProbMetric: 18.1395 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 502/1000
2023-09-27 06:25:44.814 
Epoch 502/1000 
	 loss: 17.9548, MinusLogProbMetric: 17.9548, val_loss: 18.2264, val_MinusLogProbMetric: 18.2264

Epoch 502: val_loss did not improve from 18.03988
196/196 - 78s - loss: 17.9548 - MinusLogProbMetric: 17.9548 - val_loss: 18.2264 - val_MinusLogProbMetric: 18.2264 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 503/1000
2023-09-27 06:27:02.864 
Epoch 503/1000 
	 loss: 17.9731, MinusLogProbMetric: 17.9731, val_loss: 18.0886, val_MinusLogProbMetric: 18.0886

Epoch 503: val_loss did not improve from 18.03988
196/196 - 78s - loss: 17.9731 - MinusLogProbMetric: 17.9731 - val_loss: 18.0886 - val_MinusLogProbMetric: 18.0886 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 504/1000
2023-09-27 06:28:20.820 
Epoch 504/1000 
	 loss: 18.0312, MinusLogProbMetric: 18.0312, val_loss: 18.0570, val_MinusLogProbMetric: 18.0570

Epoch 504: val_loss did not improve from 18.03988
196/196 - 78s - loss: 18.0312 - MinusLogProbMetric: 18.0312 - val_loss: 18.0570 - val_MinusLogProbMetric: 18.0570 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 505/1000
2023-09-27 06:29:38.513 
Epoch 505/1000 
	 loss: 17.9086, MinusLogProbMetric: 17.9086, val_loss: 18.3908, val_MinusLogProbMetric: 18.3908

Epoch 505: val_loss did not improve from 18.03988
196/196 - 78s - loss: 17.9086 - MinusLogProbMetric: 17.9086 - val_loss: 18.3908 - val_MinusLogProbMetric: 18.3908 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 506/1000
2023-09-27 06:30:56.593 
Epoch 506/1000 
	 loss: 18.0773, MinusLogProbMetric: 18.0773, val_loss: 18.0592, val_MinusLogProbMetric: 18.0592

Epoch 506: val_loss did not improve from 18.03988
196/196 - 78s - loss: 18.0773 - MinusLogProbMetric: 18.0773 - val_loss: 18.0592 - val_MinusLogProbMetric: 18.0592 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 507/1000
2023-09-27 06:32:14.455 
Epoch 507/1000 
	 loss: 17.8848, MinusLogProbMetric: 17.8848, val_loss: 18.2327, val_MinusLogProbMetric: 18.2327

Epoch 507: val_loss did not improve from 18.03988
196/196 - 78s - loss: 17.8848 - MinusLogProbMetric: 17.8848 - val_loss: 18.2327 - val_MinusLogProbMetric: 18.2327 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 508/1000
2023-09-27 06:33:31.818 
Epoch 508/1000 
	 loss: 17.9873, MinusLogProbMetric: 17.9873, val_loss: 18.1859, val_MinusLogProbMetric: 18.1859

Epoch 508: val_loss did not improve from 18.03988
196/196 - 77s - loss: 17.9873 - MinusLogProbMetric: 17.9873 - val_loss: 18.1859 - val_MinusLogProbMetric: 18.1859 - lr: 1.1111e-04 - 77s/epoch - 395ms/step
Epoch 509/1000
2023-09-27 06:34:49.571 
Epoch 509/1000 
	 loss: 17.9049, MinusLogProbMetric: 17.9049, val_loss: 18.0453, val_MinusLogProbMetric: 18.0453

Epoch 509: val_loss did not improve from 18.03988
196/196 - 78s - loss: 17.9049 - MinusLogProbMetric: 17.9049 - val_loss: 18.0453 - val_MinusLogProbMetric: 18.0453 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 510/1000
2023-09-27 06:36:07.065 
Epoch 510/1000 
	 loss: 17.9087, MinusLogProbMetric: 17.9087, val_loss: 18.0453, val_MinusLogProbMetric: 18.0453

Epoch 510: val_loss did not improve from 18.03988
196/196 - 77s - loss: 17.9087 - MinusLogProbMetric: 17.9087 - val_loss: 18.0453 - val_MinusLogProbMetric: 18.0453 - lr: 1.1111e-04 - 77s/epoch - 395ms/step
Epoch 511/1000
2023-09-27 06:37:24.923 
Epoch 511/1000 
	 loss: 18.0123, MinusLogProbMetric: 18.0123, val_loss: 18.1317, val_MinusLogProbMetric: 18.1317

Epoch 511: val_loss did not improve from 18.03988
196/196 - 78s - loss: 18.0123 - MinusLogProbMetric: 18.0123 - val_loss: 18.1317 - val_MinusLogProbMetric: 18.1317 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 512/1000
2023-09-27 06:38:42.101 
Epoch 512/1000 
	 loss: 17.8807, MinusLogProbMetric: 17.8807, val_loss: 18.0877, val_MinusLogProbMetric: 18.0877

Epoch 512: val_loss did not improve from 18.03988
196/196 - 77s - loss: 17.8807 - MinusLogProbMetric: 17.8807 - val_loss: 18.0877 - val_MinusLogProbMetric: 18.0877 - lr: 1.1111e-04 - 77s/epoch - 394ms/step
Epoch 513/1000
2023-09-27 06:39:59.393 
Epoch 513/1000 
	 loss: 17.8930, MinusLogProbMetric: 17.8930, val_loss: 18.2639, val_MinusLogProbMetric: 18.2639

Epoch 513: val_loss did not improve from 18.03988
196/196 - 77s - loss: 17.8930 - MinusLogProbMetric: 17.8930 - val_loss: 18.2639 - val_MinusLogProbMetric: 18.2639 - lr: 1.1111e-04 - 77s/epoch - 394ms/step
Epoch 514/1000
2023-09-27 06:41:17.109 
Epoch 514/1000 
	 loss: 18.0711, MinusLogProbMetric: 18.0711, val_loss: 18.6451, val_MinusLogProbMetric: 18.6451

Epoch 514: val_loss did not improve from 18.03988
196/196 - 78s - loss: 18.0711 - MinusLogProbMetric: 18.0711 - val_loss: 18.6451 - val_MinusLogProbMetric: 18.6451 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 515/1000
2023-09-27 06:42:35.262 
Epoch 515/1000 
	 loss: 17.9287, MinusLogProbMetric: 17.9287, val_loss: 18.0402, val_MinusLogProbMetric: 18.0402

Epoch 515: val_loss did not improve from 18.03988
196/196 - 78s - loss: 17.9287 - MinusLogProbMetric: 17.9287 - val_loss: 18.0402 - val_MinusLogProbMetric: 18.0402 - lr: 1.1111e-04 - 78s/epoch - 399ms/step
Epoch 516/1000
2023-09-27 06:43:53.032 
Epoch 516/1000 
	 loss: 18.5655, MinusLogProbMetric: 18.5655, val_loss: 18.1945, val_MinusLogProbMetric: 18.1945

Epoch 516: val_loss did not improve from 18.03988
196/196 - 78s - loss: 18.5655 - MinusLogProbMetric: 18.5655 - val_loss: 18.1945 - val_MinusLogProbMetric: 18.1945 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 517/1000
2023-09-27 06:45:11.169 
Epoch 517/1000 
	 loss: 17.9228, MinusLogProbMetric: 17.9228, val_loss: 18.2428, val_MinusLogProbMetric: 18.2428

Epoch 517: val_loss did not improve from 18.03988
196/196 - 78s - loss: 17.9228 - MinusLogProbMetric: 17.9228 - val_loss: 18.2428 - val_MinusLogProbMetric: 18.2428 - lr: 1.1111e-04 - 78s/epoch - 399ms/step
Epoch 518/1000
2023-09-27 06:46:28.813 
Epoch 518/1000 
	 loss: 17.8836, MinusLogProbMetric: 17.8836, val_loss: 18.1526, val_MinusLogProbMetric: 18.1526

Epoch 518: val_loss did not improve from 18.03988
196/196 - 78s - loss: 17.8836 - MinusLogProbMetric: 17.8836 - val_loss: 18.1526 - val_MinusLogProbMetric: 18.1526 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 519/1000
2023-09-27 06:47:45.456 
Epoch 519/1000 
	 loss: 17.8971, MinusLogProbMetric: 17.8971, val_loss: 18.0692, val_MinusLogProbMetric: 18.0692

Epoch 519: val_loss did not improve from 18.03988
196/196 - 77s - loss: 17.8971 - MinusLogProbMetric: 17.8971 - val_loss: 18.0692 - val_MinusLogProbMetric: 18.0692 - lr: 1.1111e-04 - 77s/epoch - 391ms/step
Epoch 520/1000
2023-09-27 06:49:03.095 
Epoch 520/1000 
	 loss: 17.8788, MinusLogProbMetric: 17.8788, val_loss: 19.8027, val_MinusLogProbMetric: 19.8027

Epoch 520: val_loss did not improve from 18.03988
196/196 - 78s - loss: 17.8788 - MinusLogProbMetric: 17.8788 - val_loss: 19.8027 - val_MinusLogProbMetric: 19.8027 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 521/1000
2023-09-27 06:50:20.897 
Epoch 521/1000 
	 loss: 18.1291, MinusLogProbMetric: 18.1291, val_loss: 18.2223, val_MinusLogProbMetric: 18.2223

Epoch 521: val_loss did not improve from 18.03988
196/196 - 78s - loss: 18.1291 - MinusLogProbMetric: 18.1291 - val_loss: 18.2223 - val_MinusLogProbMetric: 18.2223 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 522/1000
2023-09-27 06:51:38.483 
Epoch 522/1000 
	 loss: 17.9270, MinusLogProbMetric: 17.9270, val_loss: 18.4534, val_MinusLogProbMetric: 18.4534

Epoch 522: val_loss did not improve from 18.03988
196/196 - 78s - loss: 17.9270 - MinusLogProbMetric: 17.9270 - val_loss: 18.4534 - val_MinusLogProbMetric: 18.4534 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 523/1000
2023-09-27 06:52:55.950 
Epoch 523/1000 
	 loss: 17.8901, MinusLogProbMetric: 17.8901, val_loss: 18.1405, val_MinusLogProbMetric: 18.1405

Epoch 523: val_loss did not improve from 18.03988
196/196 - 77s - loss: 17.8901 - MinusLogProbMetric: 17.8901 - val_loss: 18.1405 - val_MinusLogProbMetric: 18.1405 - lr: 1.1111e-04 - 77s/epoch - 395ms/step
Epoch 524/1000
2023-09-27 06:54:13.503 
Epoch 524/1000 
	 loss: 17.9682, MinusLogProbMetric: 17.9682, val_loss: 18.1826, val_MinusLogProbMetric: 18.1826

Epoch 524: val_loss did not improve from 18.03988
196/196 - 78s - loss: 17.9682 - MinusLogProbMetric: 17.9682 - val_loss: 18.1826 - val_MinusLogProbMetric: 18.1826 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 525/1000
2023-09-27 06:55:30.789 
Epoch 525/1000 
	 loss: 18.1968, MinusLogProbMetric: 18.1968, val_loss: 18.1914, val_MinusLogProbMetric: 18.1914

Epoch 525: val_loss did not improve from 18.03988
196/196 - 77s - loss: 18.1968 - MinusLogProbMetric: 18.1968 - val_loss: 18.1914 - val_MinusLogProbMetric: 18.1914 - lr: 1.1111e-04 - 77s/epoch - 394ms/step
Epoch 526/1000
2023-09-27 06:56:48.474 
Epoch 526/1000 
	 loss: 17.8927, MinusLogProbMetric: 17.8927, val_loss: 18.1925, val_MinusLogProbMetric: 18.1925

Epoch 526: val_loss did not improve from 18.03988
196/196 - 78s - loss: 17.8927 - MinusLogProbMetric: 17.8927 - val_loss: 18.1925 - val_MinusLogProbMetric: 18.1925 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 527/1000
2023-09-27 06:58:05.958 
Epoch 527/1000 
	 loss: 18.0080, MinusLogProbMetric: 18.0080, val_loss: 18.1060, val_MinusLogProbMetric: 18.1060

Epoch 527: val_loss did not improve from 18.03988
196/196 - 77s - loss: 18.0080 - MinusLogProbMetric: 18.0080 - val_loss: 18.1060 - val_MinusLogProbMetric: 18.1060 - lr: 1.1111e-04 - 77s/epoch - 395ms/step
Epoch 528/1000
2023-09-27 06:59:23.552 
Epoch 528/1000 
	 loss: 17.8781, MinusLogProbMetric: 17.8781, val_loss: 18.3690, val_MinusLogProbMetric: 18.3690

Epoch 528: val_loss did not improve from 18.03988
196/196 - 78s - loss: 17.8781 - MinusLogProbMetric: 17.8781 - val_loss: 18.3690 - val_MinusLogProbMetric: 18.3690 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 529/1000
2023-09-27 07:00:40.845 
Epoch 529/1000 
	 loss: 17.8789, MinusLogProbMetric: 17.8789, val_loss: 18.0651, val_MinusLogProbMetric: 18.0651

Epoch 529: val_loss did not improve from 18.03988
196/196 - 77s - loss: 17.8789 - MinusLogProbMetric: 17.8789 - val_loss: 18.0651 - val_MinusLogProbMetric: 18.0651 - lr: 1.1111e-04 - 77s/epoch - 394ms/step
Epoch 530/1000
2023-09-27 07:01:57.723 
Epoch 530/1000 
	 loss: 17.8277, MinusLogProbMetric: 17.8277, val_loss: 18.0837, val_MinusLogProbMetric: 18.0837

Epoch 530: val_loss did not improve from 18.03988
196/196 - 77s - loss: 17.8277 - MinusLogProbMetric: 17.8277 - val_loss: 18.0837 - val_MinusLogProbMetric: 18.0837 - lr: 1.1111e-04 - 77s/epoch - 392ms/step
Epoch 531/1000
2023-09-27 07:03:15.355 
Epoch 531/1000 
	 loss: 17.8638, MinusLogProbMetric: 17.8638, val_loss: 18.5185, val_MinusLogProbMetric: 18.5185

Epoch 531: val_loss did not improve from 18.03988
196/196 - 78s - loss: 17.8638 - MinusLogProbMetric: 17.8638 - val_loss: 18.5185 - val_MinusLogProbMetric: 18.5185 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 532/1000
2023-09-27 07:04:32.721 
Epoch 532/1000 
	 loss: 17.8581, MinusLogProbMetric: 17.8581, val_loss: 18.0813, val_MinusLogProbMetric: 18.0813

Epoch 532: val_loss did not improve from 18.03988
196/196 - 77s - loss: 17.8581 - MinusLogProbMetric: 17.8581 - val_loss: 18.0813 - val_MinusLogProbMetric: 18.0813 - lr: 1.1111e-04 - 77s/epoch - 395ms/step
Epoch 533/1000
2023-09-27 07:05:50.396 
Epoch 533/1000 
	 loss: 17.8675, MinusLogProbMetric: 17.8675, val_loss: 18.1062, val_MinusLogProbMetric: 18.1062

Epoch 533: val_loss did not improve from 18.03988
196/196 - 78s - loss: 17.8675 - MinusLogProbMetric: 17.8675 - val_loss: 18.1062 - val_MinusLogProbMetric: 18.1062 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 534/1000
2023-09-27 07:07:07.749 
Epoch 534/1000 
	 loss: 17.8448, MinusLogProbMetric: 17.8448, val_loss: 18.0661, val_MinusLogProbMetric: 18.0661

Epoch 534: val_loss did not improve from 18.03988
196/196 - 77s - loss: 17.8448 - MinusLogProbMetric: 17.8448 - val_loss: 18.0661 - val_MinusLogProbMetric: 18.0661 - lr: 1.1111e-04 - 77s/epoch - 395ms/step
Epoch 535/1000
2023-09-27 07:08:25.676 
Epoch 535/1000 
	 loss: 17.8804, MinusLogProbMetric: 17.8804, val_loss: 18.6229, val_MinusLogProbMetric: 18.6229

Epoch 535: val_loss did not improve from 18.03988
196/196 - 78s - loss: 17.8804 - MinusLogProbMetric: 17.8804 - val_loss: 18.6229 - val_MinusLogProbMetric: 18.6229 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 536/1000
2023-09-27 07:09:43.044 
Epoch 536/1000 
	 loss: 17.8795, MinusLogProbMetric: 17.8795, val_loss: 22.2549, val_MinusLogProbMetric: 22.2549

Epoch 536: val_loss did not improve from 18.03988
196/196 - 77s - loss: 17.8795 - MinusLogProbMetric: 17.8795 - val_loss: 22.2549 - val_MinusLogProbMetric: 22.2549 - lr: 1.1111e-04 - 77s/epoch - 395ms/step
Epoch 537/1000
2023-09-27 07:11:00.711 
Epoch 537/1000 
	 loss: 18.2266, MinusLogProbMetric: 18.2266, val_loss: 18.0465, val_MinusLogProbMetric: 18.0465

Epoch 537: val_loss did not improve from 18.03988
196/196 - 78s - loss: 18.2266 - MinusLogProbMetric: 18.2266 - val_loss: 18.0465 - val_MinusLogProbMetric: 18.0465 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 538/1000
2023-09-27 07:12:18.008 
Epoch 538/1000 
	 loss: 17.8795, MinusLogProbMetric: 17.8795, val_loss: 17.8882, val_MinusLogProbMetric: 17.8882

Epoch 538: val_loss improved from 18.03988 to 17.88820, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 79s - loss: 17.8795 - MinusLogProbMetric: 17.8795 - val_loss: 17.8882 - val_MinusLogProbMetric: 17.8882 - lr: 1.1111e-04 - 79s/epoch - 401ms/step
Epoch 539/1000
2023-09-27 07:13:36.814 
Epoch 539/1000 
	 loss: 17.8490, MinusLogProbMetric: 17.8490, val_loss: 18.1798, val_MinusLogProbMetric: 18.1798

Epoch 539: val_loss did not improve from 17.88820
196/196 - 77s - loss: 17.8490 - MinusLogProbMetric: 17.8490 - val_loss: 18.1798 - val_MinusLogProbMetric: 18.1798 - lr: 1.1111e-04 - 77s/epoch - 395ms/step
Epoch 540/1000
2023-09-27 07:14:54.446 
Epoch 540/1000 
	 loss: 17.8315, MinusLogProbMetric: 17.8315, val_loss: 18.0065, val_MinusLogProbMetric: 18.0065

Epoch 540: val_loss did not improve from 17.88820
196/196 - 78s - loss: 17.8315 - MinusLogProbMetric: 17.8315 - val_loss: 18.0065 - val_MinusLogProbMetric: 18.0065 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 541/1000
2023-09-27 07:16:12.163 
Epoch 541/1000 
	 loss: 17.8135, MinusLogProbMetric: 17.8135, val_loss: 17.9966, val_MinusLogProbMetric: 17.9966

Epoch 541: val_loss did not improve from 17.88820
196/196 - 78s - loss: 17.8135 - MinusLogProbMetric: 17.8135 - val_loss: 17.9966 - val_MinusLogProbMetric: 17.9966 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 542/1000
2023-09-27 07:17:28.437 
Epoch 542/1000 
	 loss: 17.9371, MinusLogProbMetric: 17.9371, val_loss: 18.5880, val_MinusLogProbMetric: 18.5880

Epoch 542: val_loss did not improve from 17.88820
196/196 - 76s - loss: 17.9371 - MinusLogProbMetric: 17.9371 - val_loss: 18.5880 - val_MinusLogProbMetric: 18.5880 - lr: 1.1111e-04 - 76s/epoch - 389ms/step
Epoch 543/1000
2023-09-27 07:18:38.348 
Epoch 543/1000 
	 loss: 17.9447, MinusLogProbMetric: 17.9447, val_loss: 18.1015, val_MinusLogProbMetric: 18.1015

Epoch 543: val_loss did not improve from 17.88820
196/196 - 70s - loss: 17.9447 - MinusLogProbMetric: 17.9447 - val_loss: 18.1015 - val_MinusLogProbMetric: 18.1015 - lr: 1.1111e-04 - 70s/epoch - 357ms/step
Epoch 544/1000
2023-09-27 07:19:50.497 
Epoch 544/1000 
	 loss: 17.8375, MinusLogProbMetric: 17.8375, val_loss: 17.8859, val_MinusLogProbMetric: 17.8859

Epoch 544: val_loss improved from 17.88820 to 17.88586, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 74s - loss: 17.8375 - MinusLogProbMetric: 17.8375 - val_loss: 17.8859 - val_MinusLogProbMetric: 17.8859 - lr: 1.1111e-04 - 74s/epoch - 376ms/step
Epoch 545/1000
2023-09-27 07:21:03.282 
Epoch 545/1000 
	 loss: 17.8043, MinusLogProbMetric: 17.8043, val_loss: 18.1645, val_MinusLogProbMetric: 18.1645

Epoch 545: val_loss did not improve from 17.88586
196/196 - 71s - loss: 17.8043 - MinusLogProbMetric: 17.8043 - val_loss: 18.1645 - val_MinusLogProbMetric: 18.1645 - lr: 1.1111e-04 - 71s/epoch - 364ms/step
Epoch 546/1000
2023-09-27 07:22:12.767 
Epoch 546/1000 
	 loss: 17.8595, MinusLogProbMetric: 17.8595, val_loss: 18.0891, val_MinusLogProbMetric: 18.0891

Epoch 546: val_loss did not improve from 17.88586
196/196 - 69s - loss: 17.8595 - MinusLogProbMetric: 17.8595 - val_loss: 18.0891 - val_MinusLogProbMetric: 18.0891 - lr: 1.1111e-04 - 69s/epoch - 355ms/step
Epoch 547/1000
2023-09-27 07:23:29.439 
Epoch 547/1000 
	 loss: 17.8936, MinusLogProbMetric: 17.8936, val_loss: 17.9720, val_MinusLogProbMetric: 17.9720

Epoch 547: val_loss did not improve from 17.88586
196/196 - 77s - loss: 17.8936 - MinusLogProbMetric: 17.8936 - val_loss: 17.9720 - val_MinusLogProbMetric: 17.9720 - lr: 1.1111e-04 - 77s/epoch - 391ms/step
Epoch 548/1000
2023-09-27 07:24:45.918 
Epoch 548/1000 
	 loss: 18.0116, MinusLogProbMetric: 18.0116, val_loss: 18.1564, val_MinusLogProbMetric: 18.1564

Epoch 548: val_loss did not improve from 17.88586
196/196 - 76s - loss: 18.0116 - MinusLogProbMetric: 18.0116 - val_loss: 18.1564 - val_MinusLogProbMetric: 18.1564 - lr: 1.1111e-04 - 76s/epoch - 390ms/step
Epoch 549/1000
2023-09-27 07:26:03.127 
Epoch 549/1000 
	 loss: 17.9243, MinusLogProbMetric: 17.9243, val_loss: 35.3922, val_MinusLogProbMetric: 35.3922

Epoch 549: val_loss did not improve from 17.88586
196/196 - 77s - loss: 17.9243 - MinusLogProbMetric: 17.9243 - val_loss: 35.3922 - val_MinusLogProbMetric: 35.3922 - lr: 1.1111e-04 - 77s/epoch - 394ms/step
Epoch 550/1000
2023-09-27 07:27:20.056 
Epoch 550/1000 
	 loss: 19.3429, MinusLogProbMetric: 19.3429, val_loss: 18.1334, val_MinusLogProbMetric: 18.1334

Epoch 550: val_loss did not improve from 17.88586
196/196 - 77s - loss: 19.3429 - MinusLogProbMetric: 19.3429 - val_loss: 18.1334 - val_MinusLogProbMetric: 18.1334 - lr: 1.1111e-04 - 77s/epoch - 392ms/step
Epoch 551/1000
2023-09-27 07:28:37.461 
Epoch 551/1000 
	 loss: 18.0635, MinusLogProbMetric: 18.0635, val_loss: 18.1041, val_MinusLogProbMetric: 18.1041

Epoch 551: val_loss did not improve from 17.88586
196/196 - 77s - loss: 18.0635 - MinusLogProbMetric: 18.0635 - val_loss: 18.1041 - val_MinusLogProbMetric: 18.1041 - lr: 1.1111e-04 - 77s/epoch - 395ms/step
Epoch 552/1000
2023-09-27 07:29:55.269 
Epoch 552/1000 
	 loss: 17.8885, MinusLogProbMetric: 17.8885, val_loss: 17.9773, val_MinusLogProbMetric: 17.9773

Epoch 552: val_loss did not improve from 17.88586
196/196 - 78s - loss: 17.8885 - MinusLogProbMetric: 17.8885 - val_loss: 17.9773 - val_MinusLogProbMetric: 17.9773 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 553/1000
2023-09-27 07:31:13.172 
Epoch 553/1000 
	 loss: 17.9152, MinusLogProbMetric: 17.9152, val_loss: 17.9389, val_MinusLogProbMetric: 17.9389

Epoch 553: val_loss did not improve from 17.88586
196/196 - 78s - loss: 17.9152 - MinusLogProbMetric: 17.9152 - val_loss: 17.9389 - val_MinusLogProbMetric: 17.9389 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 554/1000
2023-09-27 07:32:30.747 
Epoch 554/1000 
	 loss: 17.7968, MinusLogProbMetric: 17.7968, val_loss: 18.1300, val_MinusLogProbMetric: 18.1300

Epoch 554: val_loss did not improve from 17.88586
196/196 - 78s - loss: 17.7968 - MinusLogProbMetric: 17.7968 - val_loss: 18.1300 - val_MinusLogProbMetric: 18.1300 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 555/1000
2023-09-27 07:33:48.368 
Epoch 555/1000 
	 loss: 17.8155, MinusLogProbMetric: 17.8155, val_loss: 18.0522, val_MinusLogProbMetric: 18.0522

Epoch 555: val_loss did not improve from 17.88586
196/196 - 78s - loss: 17.8155 - MinusLogProbMetric: 17.8155 - val_loss: 18.0522 - val_MinusLogProbMetric: 18.0522 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 556/1000
2023-09-27 07:35:05.732 
Epoch 556/1000 
	 loss: 17.8446, MinusLogProbMetric: 17.8446, val_loss: 18.5778, val_MinusLogProbMetric: 18.5778

Epoch 556: val_loss did not improve from 17.88586
196/196 - 77s - loss: 17.8446 - MinusLogProbMetric: 17.8446 - val_loss: 18.5778 - val_MinusLogProbMetric: 18.5778 - lr: 1.1111e-04 - 77s/epoch - 395ms/step
Epoch 557/1000
2023-09-27 07:36:22.899 
Epoch 557/1000 
	 loss: 18.0729, MinusLogProbMetric: 18.0729, val_loss: 18.6606, val_MinusLogProbMetric: 18.6606

Epoch 557: val_loss did not improve from 17.88586
196/196 - 77s - loss: 18.0729 - MinusLogProbMetric: 18.0729 - val_loss: 18.6606 - val_MinusLogProbMetric: 18.6606 - lr: 1.1111e-04 - 77s/epoch - 394ms/step
Epoch 558/1000
2023-09-27 07:37:40.668 
Epoch 558/1000 
	 loss: 17.8779, MinusLogProbMetric: 17.8779, val_loss: 18.2046, val_MinusLogProbMetric: 18.2046

Epoch 558: val_loss did not improve from 17.88586
196/196 - 78s - loss: 17.8779 - MinusLogProbMetric: 17.8779 - val_loss: 18.2046 - val_MinusLogProbMetric: 18.2046 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 559/1000
2023-09-27 07:38:57.768 
Epoch 559/1000 
	 loss: 17.8395, MinusLogProbMetric: 17.8395, val_loss: 17.9763, val_MinusLogProbMetric: 17.9763

Epoch 559: val_loss did not improve from 17.88586
196/196 - 77s - loss: 17.8395 - MinusLogProbMetric: 17.8395 - val_loss: 17.9763 - val_MinusLogProbMetric: 17.9763 - lr: 1.1111e-04 - 77s/epoch - 393ms/step
Epoch 560/1000
2023-09-27 07:40:15.002 
Epoch 560/1000 
	 loss: 17.8243, MinusLogProbMetric: 17.8243, val_loss: 17.8741, val_MinusLogProbMetric: 17.8741

Epoch 560: val_loss improved from 17.88586 to 17.87409, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 79s - loss: 17.8243 - MinusLogProbMetric: 17.8243 - val_loss: 17.8741 - val_MinusLogProbMetric: 17.8741 - lr: 1.1111e-04 - 79s/epoch - 403ms/step
Epoch 561/1000
2023-09-27 07:41:34.336 
Epoch 561/1000 
	 loss: 17.8734, MinusLogProbMetric: 17.8734, val_loss: 18.0532, val_MinusLogProbMetric: 18.0532

Epoch 561: val_loss did not improve from 17.87409
196/196 - 78s - loss: 17.8734 - MinusLogProbMetric: 17.8734 - val_loss: 18.0532 - val_MinusLogProbMetric: 18.0532 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 562/1000
2023-09-27 07:42:52.376 
Epoch 562/1000 
	 loss: 17.7842, MinusLogProbMetric: 17.7842, val_loss: 17.8523, val_MinusLogProbMetric: 17.8523

Epoch 562: val_loss improved from 17.87409 to 17.85227, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 80s - loss: 17.7842 - MinusLogProbMetric: 17.7842 - val_loss: 17.8523 - val_MinusLogProbMetric: 17.8523 - lr: 1.1111e-04 - 80s/epoch - 406ms/step
Epoch 563/1000
2023-09-27 07:44:11.695 
Epoch 563/1000 
	 loss: 17.7765, MinusLogProbMetric: 17.7765, val_loss: 18.3615, val_MinusLogProbMetric: 18.3615

Epoch 563: val_loss did not improve from 17.85227
196/196 - 78s - loss: 17.7765 - MinusLogProbMetric: 17.7765 - val_loss: 18.3615 - val_MinusLogProbMetric: 18.3615 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 564/1000
2023-09-27 07:45:29.407 
Epoch 564/1000 
	 loss: 17.8499, MinusLogProbMetric: 17.8499, val_loss: 18.0026, val_MinusLogProbMetric: 18.0026

Epoch 564: val_loss did not improve from 17.85227
196/196 - 78s - loss: 17.8499 - MinusLogProbMetric: 17.8499 - val_loss: 18.0026 - val_MinusLogProbMetric: 18.0026 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 565/1000
2023-09-27 07:46:48.759 
Epoch 565/1000 
	 loss: 17.9377, MinusLogProbMetric: 17.9377, val_loss: 18.1005, val_MinusLogProbMetric: 18.1005

Epoch 565: val_loss did not improve from 17.85227
196/196 - 79s - loss: 17.9377 - MinusLogProbMetric: 17.9377 - val_loss: 18.1005 - val_MinusLogProbMetric: 18.1005 - lr: 1.1111e-04 - 79s/epoch - 405ms/step
Epoch 566/1000
2023-09-27 07:48:07.634 
Epoch 566/1000 
	 loss: 17.8937, MinusLogProbMetric: 17.8937, val_loss: 18.3162, val_MinusLogProbMetric: 18.3162

Epoch 566: val_loss did not improve from 17.85227
196/196 - 79s - loss: 17.8937 - MinusLogProbMetric: 17.8937 - val_loss: 18.3162 - val_MinusLogProbMetric: 18.3162 - lr: 1.1111e-04 - 79s/epoch - 402ms/step
Epoch 567/1000
2023-09-27 07:49:25.999 
Epoch 567/1000 
	 loss: 17.8399, MinusLogProbMetric: 17.8399, val_loss: 18.1206, val_MinusLogProbMetric: 18.1206

Epoch 567: val_loss did not improve from 17.85227
196/196 - 78s - loss: 17.8399 - MinusLogProbMetric: 17.8399 - val_loss: 18.1206 - val_MinusLogProbMetric: 18.1206 - lr: 1.1111e-04 - 78s/epoch - 400ms/step
Epoch 568/1000
2023-09-27 07:50:45.142 
Epoch 568/1000 
	 loss: 17.8652, MinusLogProbMetric: 17.8652, val_loss: 18.2178, val_MinusLogProbMetric: 18.2178

Epoch 568: val_loss did not improve from 17.85227
196/196 - 79s - loss: 17.8652 - MinusLogProbMetric: 17.8652 - val_loss: 18.2178 - val_MinusLogProbMetric: 18.2178 - lr: 1.1111e-04 - 79s/epoch - 404ms/step
Epoch 569/1000
2023-09-27 07:52:05.195 
Epoch 569/1000 
	 loss: 17.9028, MinusLogProbMetric: 17.9028, val_loss: 17.9527, val_MinusLogProbMetric: 17.9527

Epoch 569: val_loss did not improve from 17.85227
196/196 - 80s - loss: 17.9028 - MinusLogProbMetric: 17.9028 - val_loss: 17.9527 - val_MinusLogProbMetric: 17.9527 - lr: 1.1111e-04 - 80s/epoch - 408ms/step
Epoch 570/1000
2023-09-27 07:53:24.568 
Epoch 570/1000 
	 loss: 17.7752, MinusLogProbMetric: 17.7752, val_loss: 17.9250, val_MinusLogProbMetric: 17.9250

Epoch 570: val_loss did not improve from 17.85227
196/196 - 79s - loss: 17.7752 - MinusLogProbMetric: 17.7752 - val_loss: 17.9250 - val_MinusLogProbMetric: 17.9250 - lr: 1.1111e-04 - 79s/epoch - 405ms/step
Epoch 571/1000
2023-09-27 07:54:43.112 
Epoch 571/1000 
	 loss: 17.7675, MinusLogProbMetric: 17.7675, val_loss: 17.9576, val_MinusLogProbMetric: 17.9576

Epoch 571: val_loss did not improve from 17.85227
196/196 - 79s - loss: 17.7675 - MinusLogProbMetric: 17.7675 - val_loss: 17.9576 - val_MinusLogProbMetric: 17.9576 - lr: 1.1111e-04 - 79s/epoch - 401ms/step
Epoch 572/1000
2023-09-27 07:56:00.720 
Epoch 572/1000 
	 loss: 17.8919, MinusLogProbMetric: 17.8919, val_loss: 18.0195, val_MinusLogProbMetric: 18.0195

Epoch 572: val_loss did not improve from 17.85227
196/196 - 78s - loss: 17.8919 - MinusLogProbMetric: 17.8919 - val_loss: 18.0195 - val_MinusLogProbMetric: 18.0195 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 573/1000
2023-09-27 07:57:17.938 
Epoch 573/1000 
	 loss: 17.8190, MinusLogProbMetric: 17.8190, val_loss: 18.2086, val_MinusLogProbMetric: 18.2086

Epoch 573: val_loss did not improve from 17.85227
196/196 - 77s - loss: 17.8190 - MinusLogProbMetric: 17.8190 - val_loss: 18.2086 - val_MinusLogProbMetric: 18.2086 - lr: 1.1111e-04 - 77s/epoch - 394ms/step
Epoch 574/1000
2023-09-27 07:58:36.058 
Epoch 574/1000 
	 loss: 17.9482, MinusLogProbMetric: 17.9482, val_loss: 17.9634, val_MinusLogProbMetric: 17.9634

Epoch 574: val_loss did not improve from 17.85227
196/196 - 78s - loss: 17.9482 - MinusLogProbMetric: 17.9482 - val_loss: 17.9634 - val_MinusLogProbMetric: 17.9634 - lr: 1.1111e-04 - 78s/epoch - 399ms/step
Epoch 575/1000
2023-09-27 07:59:53.739 
Epoch 575/1000 
	 loss: 17.8020, MinusLogProbMetric: 17.8020, val_loss: 18.0415, val_MinusLogProbMetric: 18.0415

Epoch 575: val_loss did not improve from 17.85227
196/196 - 78s - loss: 17.8020 - MinusLogProbMetric: 17.8020 - val_loss: 18.0415 - val_MinusLogProbMetric: 18.0415 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 576/1000
2023-09-27 08:01:11.718 
Epoch 576/1000 
	 loss: 17.7487, MinusLogProbMetric: 17.7487, val_loss: 17.9290, val_MinusLogProbMetric: 17.9290

Epoch 576: val_loss did not improve from 17.85227
196/196 - 78s - loss: 17.7487 - MinusLogProbMetric: 17.7487 - val_loss: 17.9290 - val_MinusLogProbMetric: 17.9290 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 577/1000
2023-09-27 08:02:28.762 
Epoch 577/1000 
	 loss: 17.8502, MinusLogProbMetric: 17.8502, val_loss: 18.0518, val_MinusLogProbMetric: 18.0518

Epoch 577: val_loss did not improve from 17.85227
196/196 - 77s - loss: 17.8502 - MinusLogProbMetric: 17.8502 - val_loss: 18.0518 - val_MinusLogProbMetric: 18.0518 - lr: 1.1111e-04 - 77s/epoch - 393ms/step
Epoch 578/1000
2023-09-27 08:03:45.648 
Epoch 578/1000 
	 loss: 17.8062, MinusLogProbMetric: 17.8062, val_loss: 17.8728, val_MinusLogProbMetric: 17.8728

Epoch 578: val_loss did not improve from 17.85227
196/196 - 77s - loss: 17.8062 - MinusLogProbMetric: 17.8062 - val_loss: 17.8728 - val_MinusLogProbMetric: 17.8728 - lr: 1.1111e-04 - 77s/epoch - 392ms/step
Epoch 579/1000
2023-09-27 08:05:03.120 
Epoch 579/1000 
	 loss: 17.7460, MinusLogProbMetric: 17.7460, val_loss: 17.9132, val_MinusLogProbMetric: 17.9132

Epoch 579: val_loss did not improve from 17.85227
196/196 - 77s - loss: 17.7460 - MinusLogProbMetric: 17.7460 - val_loss: 17.9132 - val_MinusLogProbMetric: 17.9132 - lr: 1.1111e-04 - 77s/epoch - 395ms/step
Epoch 580/1000
2023-09-27 08:06:21.656 
Epoch 580/1000 
	 loss: 17.8608, MinusLogProbMetric: 17.8608, val_loss: 18.5506, val_MinusLogProbMetric: 18.5506

Epoch 580: val_loss did not improve from 17.85227
196/196 - 79s - loss: 17.8608 - MinusLogProbMetric: 17.8608 - val_loss: 18.5506 - val_MinusLogProbMetric: 18.5506 - lr: 1.1111e-04 - 79s/epoch - 401ms/step
Epoch 581/1000
2023-09-27 08:07:38.808 
Epoch 581/1000 
	 loss: 17.8355, MinusLogProbMetric: 17.8355, val_loss: 17.8927, val_MinusLogProbMetric: 17.8927

Epoch 581: val_loss did not improve from 17.85227
196/196 - 77s - loss: 17.8355 - MinusLogProbMetric: 17.8355 - val_loss: 17.8927 - val_MinusLogProbMetric: 17.8927 - lr: 1.1111e-04 - 77s/epoch - 394ms/step
Epoch 582/1000
2023-09-27 08:08:56.573 
Epoch 582/1000 
	 loss: 17.8234, MinusLogProbMetric: 17.8234, val_loss: 17.9449, val_MinusLogProbMetric: 17.9449

Epoch 582: val_loss did not improve from 17.85227
196/196 - 78s - loss: 17.8234 - MinusLogProbMetric: 17.8234 - val_loss: 17.9449 - val_MinusLogProbMetric: 17.9449 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 583/1000
2023-09-27 08:10:14.041 
Epoch 583/1000 
	 loss: 17.7683, MinusLogProbMetric: 17.7683, val_loss: 17.9751, val_MinusLogProbMetric: 17.9751

Epoch 583: val_loss did not improve from 17.85227
196/196 - 77s - loss: 17.7683 - MinusLogProbMetric: 17.7683 - val_loss: 17.9751 - val_MinusLogProbMetric: 17.9751 - lr: 1.1111e-04 - 77s/epoch - 395ms/step
Epoch 584/1000
2023-09-27 08:11:31.127 
Epoch 584/1000 
	 loss: 17.7599, MinusLogProbMetric: 17.7599, val_loss: 18.1772, val_MinusLogProbMetric: 18.1772

Epoch 584: val_loss did not improve from 17.85227
196/196 - 77s - loss: 17.7599 - MinusLogProbMetric: 17.7599 - val_loss: 18.1772 - val_MinusLogProbMetric: 18.1772 - lr: 1.1111e-04 - 77s/epoch - 393ms/step
Epoch 585/1000
2023-09-27 08:12:48.314 
Epoch 585/1000 
	 loss: 17.7799, MinusLogProbMetric: 17.7799, val_loss: 18.0154, val_MinusLogProbMetric: 18.0154

Epoch 585: val_loss did not improve from 17.85227
196/196 - 77s - loss: 17.7799 - MinusLogProbMetric: 17.7799 - val_loss: 18.0154 - val_MinusLogProbMetric: 18.0154 - lr: 1.1111e-04 - 77s/epoch - 394ms/step
Epoch 586/1000
2023-09-27 08:14:05.795 
Epoch 586/1000 
	 loss: 17.8468, MinusLogProbMetric: 17.8468, val_loss: 18.3784, val_MinusLogProbMetric: 18.3784

Epoch 586: val_loss did not improve from 17.85227
196/196 - 77s - loss: 17.8468 - MinusLogProbMetric: 17.8468 - val_loss: 18.3784 - val_MinusLogProbMetric: 18.3784 - lr: 1.1111e-04 - 77s/epoch - 395ms/step
Epoch 587/1000
2023-09-27 08:15:23.265 
Epoch 587/1000 
	 loss: 17.8158, MinusLogProbMetric: 17.8158, val_loss: 18.0737, val_MinusLogProbMetric: 18.0737

Epoch 587: val_loss did not improve from 17.85227
196/196 - 77s - loss: 17.8158 - MinusLogProbMetric: 17.8158 - val_loss: 18.0737 - val_MinusLogProbMetric: 18.0737 - lr: 1.1111e-04 - 77s/epoch - 395ms/step
Epoch 588/1000
2023-09-27 08:16:40.846 
Epoch 588/1000 
	 loss: 17.7351, MinusLogProbMetric: 17.7351, val_loss: 17.9628, val_MinusLogProbMetric: 17.9628

Epoch 588: val_loss did not improve from 17.85227
196/196 - 78s - loss: 17.7351 - MinusLogProbMetric: 17.7351 - val_loss: 17.9628 - val_MinusLogProbMetric: 17.9628 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 589/1000
2023-09-27 08:17:58.328 
Epoch 589/1000 
	 loss: 17.7852, MinusLogProbMetric: 17.7852, val_loss: 18.0033, val_MinusLogProbMetric: 18.0033

Epoch 589: val_loss did not improve from 17.85227
196/196 - 77s - loss: 17.7852 - MinusLogProbMetric: 17.7852 - val_loss: 18.0033 - val_MinusLogProbMetric: 18.0033 - lr: 1.1111e-04 - 77s/epoch - 395ms/step
Epoch 590/1000
2023-09-27 08:19:15.832 
Epoch 590/1000 
	 loss: 17.7940, MinusLogProbMetric: 17.7940, val_loss: 18.1128, val_MinusLogProbMetric: 18.1128

Epoch 590: val_loss did not improve from 17.85227
196/196 - 78s - loss: 17.7940 - MinusLogProbMetric: 17.7940 - val_loss: 18.1128 - val_MinusLogProbMetric: 18.1128 - lr: 1.1111e-04 - 78s/epoch - 395ms/step
Epoch 591/1000
2023-09-27 08:20:32.981 
Epoch 591/1000 
	 loss: 17.7514, MinusLogProbMetric: 17.7514, val_loss: 17.9237, val_MinusLogProbMetric: 17.9237

Epoch 591: val_loss did not improve from 17.85227
196/196 - 77s - loss: 17.7514 - MinusLogProbMetric: 17.7514 - val_loss: 17.9237 - val_MinusLogProbMetric: 17.9237 - lr: 1.1111e-04 - 77s/epoch - 394ms/step
Epoch 592/1000
2023-09-27 08:21:50.632 
Epoch 592/1000 
	 loss: 17.7448, MinusLogProbMetric: 17.7448, val_loss: 17.9787, val_MinusLogProbMetric: 17.9787

Epoch 592: val_loss did not improve from 17.85227
196/196 - 78s - loss: 17.7448 - MinusLogProbMetric: 17.7448 - val_loss: 17.9787 - val_MinusLogProbMetric: 17.9787 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 593/1000
2023-09-27 08:23:07.277 
Epoch 593/1000 
	 loss: 17.8843, MinusLogProbMetric: 17.8843, val_loss: 17.9740, val_MinusLogProbMetric: 17.9740

Epoch 593: val_loss did not improve from 17.85227
196/196 - 77s - loss: 17.8843 - MinusLogProbMetric: 17.8843 - val_loss: 17.9740 - val_MinusLogProbMetric: 17.9740 - lr: 1.1111e-04 - 77s/epoch - 391ms/step
Epoch 594/1000
2023-09-27 08:24:24.807 
Epoch 594/1000 
	 loss: 17.7983, MinusLogProbMetric: 17.7983, val_loss: 18.0196, val_MinusLogProbMetric: 18.0196

Epoch 594: val_loss did not improve from 17.85227
196/196 - 78s - loss: 17.7983 - MinusLogProbMetric: 17.7983 - val_loss: 18.0196 - val_MinusLogProbMetric: 18.0196 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 595/1000
2023-09-27 08:25:42.269 
Epoch 595/1000 
	 loss: 17.7920, MinusLogProbMetric: 17.7920, val_loss: 17.9255, val_MinusLogProbMetric: 17.9255

Epoch 595: val_loss did not improve from 17.85227
196/196 - 77s - loss: 17.7920 - MinusLogProbMetric: 17.7920 - val_loss: 17.9255 - val_MinusLogProbMetric: 17.9255 - lr: 1.1111e-04 - 77s/epoch - 395ms/step
Epoch 596/1000
2023-09-27 08:26:59.474 
Epoch 596/1000 
	 loss: 17.7450, MinusLogProbMetric: 17.7450, val_loss: 18.0027, val_MinusLogProbMetric: 18.0027

Epoch 596: val_loss did not improve from 17.85227
196/196 - 77s - loss: 17.7450 - MinusLogProbMetric: 17.7450 - val_loss: 18.0027 - val_MinusLogProbMetric: 18.0027 - lr: 1.1111e-04 - 77s/epoch - 394ms/step
Epoch 597/1000
2023-09-27 08:28:16.517 
Epoch 597/1000 
	 loss: 17.7365, MinusLogProbMetric: 17.7365, val_loss: 18.0919, val_MinusLogProbMetric: 18.0919

Epoch 597: val_loss did not improve from 17.85227
196/196 - 77s - loss: 17.7365 - MinusLogProbMetric: 17.7365 - val_loss: 18.0919 - val_MinusLogProbMetric: 18.0919 - lr: 1.1111e-04 - 77s/epoch - 393ms/step
Epoch 598/1000
2023-09-27 08:29:34.560 
Epoch 598/1000 
	 loss: 17.7620, MinusLogProbMetric: 17.7620, val_loss: 17.9954, val_MinusLogProbMetric: 17.9954

Epoch 598: val_loss did not improve from 17.85227
196/196 - 78s - loss: 17.7620 - MinusLogProbMetric: 17.7620 - val_loss: 17.9954 - val_MinusLogProbMetric: 17.9954 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 599/1000
2023-09-27 08:30:52.252 
Epoch 599/1000 
	 loss: 17.8027, MinusLogProbMetric: 17.8027, val_loss: 17.9482, val_MinusLogProbMetric: 17.9482

Epoch 599: val_loss did not improve from 17.85227
196/196 - 78s - loss: 17.8027 - MinusLogProbMetric: 17.8027 - val_loss: 17.9482 - val_MinusLogProbMetric: 17.9482 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 600/1000
2023-09-27 08:32:09.924 
Epoch 600/1000 
	 loss: 17.7735, MinusLogProbMetric: 17.7735, val_loss: 17.8261, val_MinusLogProbMetric: 17.8261

Epoch 600: val_loss improved from 17.85227 to 17.82607, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 79s - loss: 17.7735 - MinusLogProbMetric: 17.7735 - val_loss: 17.8261 - val_MinusLogProbMetric: 17.8261 - lr: 1.1111e-04 - 79s/epoch - 404ms/step
Epoch 601/1000
2023-09-27 08:33:28.743 
Epoch 601/1000 
	 loss: 17.8376, MinusLogProbMetric: 17.8376, val_loss: 18.0454, val_MinusLogProbMetric: 18.0454

Epoch 601: val_loss did not improve from 17.82607
196/196 - 77s - loss: 17.8376 - MinusLogProbMetric: 17.8376 - val_loss: 18.0454 - val_MinusLogProbMetric: 18.0454 - lr: 1.1111e-04 - 77s/epoch - 394ms/step
Epoch 602/1000
2023-09-27 08:34:46.253 
Epoch 602/1000 
	 loss: 17.7750, MinusLogProbMetric: 17.7750, val_loss: 17.9320, val_MinusLogProbMetric: 17.9320

Epoch 602: val_loss did not improve from 17.82607
196/196 - 78s - loss: 17.7750 - MinusLogProbMetric: 17.7750 - val_loss: 17.9320 - val_MinusLogProbMetric: 17.9320 - lr: 1.1111e-04 - 78s/epoch - 395ms/step
Epoch 603/1000
2023-09-27 08:36:03.849 
Epoch 603/1000 
	 loss: 17.7179, MinusLogProbMetric: 17.7179, val_loss: 17.9285, val_MinusLogProbMetric: 17.9285

Epoch 603: val_loss did not improve from 17.82607
196/196 - 78s - loss: 17.7179 - MinusLogProbMetric: 17.7179 - val_loss: 17.9285 - val_MinusLogProbMetric: 17.9285 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 604/1000
2023-09-27 08:37:22.060 
Epoch 604/1000 
	 loss: 17.8336, MinusLogProbMetric: 17.8336, val_loss: 17.8870, val_MinusLogProbMetric: 17.8870

Epoch 604: val_loss did not improve from 17.82607
196/196 - 78s - loss: 17.8336 - MinusLogProbMetric: 17.8336 - val_loss: 17.8870 - val_MinusLogProbMetric: 17.8870 - lr: 1.1111e-04 - 78s/epoch - 399ms/step
Epoch 605/1000
2023-09-27 08:38:40.111 
Epoch 605/1000 
	 loss: 17.7004, MinusLogProbMetric: 17.7004, val_loss: 18.2592, val_MinusLogProbMetric: 18.2592

Epoch 605: val_loss did not improve from 17.82607
196/196 - 78s - loss: 17.7004 - MinusLogProbMetric: 17.7004 - val_loss: 18.2592 - val_MinusLogProbMetric: 18.2592 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 606/1000
2023-09-27 08:39:57.595 
Epoch 606/1000 
	 loss: 17.7538, MinusLogProbMetric: 17.7538, val_loss: 17.9533, val_MinusLogProbMetric: 17.9533

Epoch 606: val_loss did not improve from 17.82607
196/196 - 77s - loss: 17.7538 - MinusLogProbMetric: 17.7538 - val_loss: 17.9533 - val_MinusLogProbMetric: 17.9533 - lr: 1.1111e-04 - 77s/epoch - 395ms/step
Epoch 607/1000
2023-09-27 08:41:15.341 
Epoch 607/1000 
	 loss: 17.8156, MinusLogProbMetric: 17.8156, val_loss: 18.0520, val_MinusLogProbMetric: 18.0520

Epoch 607: val_loss did not improve from 17.82607
196/196 - 78s - loss: 17.8156 - MinusLogProbMetric: 17.8156 - val_loss: 18.0520 - val_MinusLogProbMetric: 18.0520 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 608/1000
2023-09-27 08:42:32.728 
Epoch 608/1000 
	 loss: 17.7824, MinusLogProbMetric: 17.7824, val_loss: 17.7922, val_MinusLogProbMetric: 17.7922

Epoch 608: val_loss improved from 17.82607 to 17.79222, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 79s - loss: 17.7824 - MinusLogProbMetric: 17.7824 - val_loss: 17.7922 - val_MinusLogProbMetric: 17.7922 - lr: 1.1111e-04 - 79s/epoch - 402ms/step
Epoch 609/1000
2023-09-27 08:43:52.231 
Epoch 609/1000 
	 loss: 17.6858, MinusLogProbMetric: 17.6858, val_loss: 17.8585, val_MinusLogProbMetric: 17.8585

Epoch 609: val_loss did not improve from 17.79222
196/196 - 78s - loss: 17.6858 - MinusLogProbMetric: 17.6858 - val_loss: 17.8585 - val_MinusLogProbMetric: 17.8585 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 610/1000
2023-09-27 08:45:09.692 
Epoch 610/1000 
	 loss: 17.7152, MinusLogProbMetric: 17.7152, val_loss: 20.7095, val_MinusLogProbMetric: 20.7095

Epoch 610: val_loss did not improve from 17.79222
196/196 - 77s - loss: 17.7152 - MinusLogProbMetric: 17.7152 - val_loss: 20.7095 - val_MinusLogProbMetric: 20.7095 - lr: 1.1111e-04 - 77s/epoch - 395ms/step
Epoch 611/1000
2023-09-27 08:46:27.525 
Epoch 611/1000 
	 loss: 17.9052, MinusLogProbMetric: 17.9052, val_loss: 17.8096, val_MinusLogProbMetric: 17.8096

Epoch 611: val_loss did not improve from 17.79222
196/196 - 78s - loss: 17.9052 - MinusLogProbMetric: 17.9052 - val_loss: 17.8096 - val_MinusLogProbMetric: 17.8096 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 612/1000
2023-09-27 08:47:45.511 
Epoch 612/1000 
	 loss: 18.1973, MinusLogProbMetric: 18.1973, val_loss: 18.1069, val_MinusLogProbMetric: 18.1069

Epoch 612: val_loss did not improve from 17.79222
196/196 - 78s - loss: 18.1973 - MinusLogProbMetric: 18.1973 - val_loss: 18.1069 - val_MinusLogProbMetric: 18.1069 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 613/1000
2023-09-27 08:49:04.076 
Epoch 613/1000 
	 loss: 17.7601, MinusLogProbMetric: 17.7601, val_loss: 17.9015, val_MinusLogProbMetric: 17.9015

Epoch 613: val_loss did not improve from 17.79222
196/196 - 79s - loss: 17.7601 - MinusLogProbMetric: 17.7601 - val_loss: 17.9015 - val_MinusLogProbMetric: 17.9015 - lr: 1.1111e-04 - 79s/epoch - 401ms/step
Epoch 614/1000
2023-09-27 08:50:21.109 
Epoch 614/1000 
	 loss: 17.7320, MinusLogProbMetric: 17.7320, val_loss: 17.9439, val_MinusLogProbMetric: 17.9439

Epoch 614: val_loss did not improve from 17.79222
196/196 - 77s - loss: 17.7320 - MinusLogProbMetric: 17.7320 - val_loss: 17.9439 - val_MinusLogProbMetric: 17.9439 - lr: 1.1111e-04 - 77s/epoch - 393ms/step
Epoch 615/1000
2023-09-27 08:51:38.473 
Epoch 615/1000 
	 loss: 17.7777, MinusLogProbMetric: 17.7777, val_loss: 18.0075, val_MinusLogProbMetric: 18.0075

Epoch 615: val_loss did not improve from 17.79222
196/196 - 77s - loss: 17.7777 - MinusLogProbMetric: 17.7777 - val_loss: 18.0075 - val_MinusLogProbMetric: 18.0075 - lr: 1.1111e-04 - 77s/epoch - 395ms/step
Epoch 616/1000
2023-09-27 08:52:55.446 
Epoch 616/1000 
	 loss: 17.7279, MinusLogProbMetric: 17.7279, val_loss: 17.9422, val_MinusLogProbMetric: 17.9422

Epoch 616: val_loss did not improve from 17.79222
196/196 - 77s - loss: 17.7279 - MinusLogProbMetric: 17.7279 - val_loss: 17.9422 - val_MinusLogProbMetric: 17.9422 - lr: 1.1111e-04 - 77s/epoch - 393ms/step
Epoch 617/1000
2023-09-27 08:54:13.175 
Epoch 617/1000 
	 loss: 17.7392, MinusLogProbMetric: 17.7392, val_loss: 17.9202, val_MinusLogProbMetric: 17.9202

Epoch 617: val_loss did not improve from 17.79222
196/196 - 78s - loss: 17.7392 - MinusLogProbMetric: 17.7392 - val_loss: 17.9202 - val_MinusLogProbMetric: 17.9202 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 618/1000
2023-09-27 08:55:30.276 
Epoch 618/1000 
	 loss: 17.9960, MinusLogProbMetric: 17.9960, val_loss: 18.0740, val_MinusLogProbMetric: 18.0740

Epoch 618: val_loss did not improve from 17.79222
196/196 - 77s - loss: 17.9960 - MinusLogProbMetric: 17.9960 - val_loss: 18.0740 - val_MinusLogProbMetric: 18.0740 - lr: 1.1111e-04 - 77s/epoch - 393ms/step
Epoch 619/1000
2023-09-27 08:56:44.411 
Epoch 619/1000 
	 loss: 17.6743, MinusLogProbMetric: 17.6743, val_loss: 18.0965, val_MinusLogProbMetric: 18.0965

Epoch 619: val_loss did not improve from 17.79222
196/196 - 74s - loss: 17.6743 - MinusLogProbMetric: 17.6743 - val_loss: 18.0965 - val_MinusLogProbMetric: 18.0965 - lr: 1.1111e-04 - 74s/epoch - 378ms/step
Epoch 620/1000
2023-09-27 08:57:56.963 
Epoch 620/1000 
	 loss: 17.7182, MinusLogProbMetric: 17.7182, val_loss: 17.8858, val_MinusLogProbMetric: 17.8858

Epoch 620: val_loss did not improve from 17.79222
196/196 - 73s - loss: 17.7182 - MinusLogProbMetric: 17.7182 - val_loss: 17.8858 - val_MinusLogProbMetric: 17.8858 - lr: 1.1111e-04 - 73s/epoch - 370ms/step
Epoch 621/1000
2023-09-27 08:59:15.341 
Epoch 621/1000 
	 loss: 17.6905, MinusLogProbMetric: 17.6905, val_loss: 17.8548, val_MinusLogProbMetric: 17.8548

Epoch 621: val_loss did not improve from 17.79222
196/196 - 78s - loss: 17.6905 - MinusLogProbMetric: 17.6905 - val_loss: 17.8548 - val_MinusLogProbMetric: 17.8548 - lr: 1.1111e-04 - 78s/epoch - 400ms/step
Epoch 622/1000
2023-09-27 09:00:31.277 
Epoch 622/1000 
	 loss: 17.7146, MinusLogProbMetric: 17.7146, val_loss: 17.9936, val_MinusLogProbMetric: 17.9936

Epoch 622: val_loss did not improve from 17.79222
196/196 - 76s - loss: 17.7146 - MinusLogProbMetric: 17.7146 - val_loss: 17.9936 - val_MinusLogProbMetric: 17.9936 - lr: 1.1111e-04 - 76s/epoch - 387ms/step
Epoch 623/1000
2023-09-27 09:01:43.985 
Epoch 623/1000 
	 loss: 17.7556, MinusLogProbMetric: 17.7556, val_loss: 17.9287, val_MinusLogProbMetric: 17.9287

Epoch 623: val_loss did not improve from 17.79222
196/196 - 73s - loss: 17.7556 - MinusLogProbMetric: 17.7556 - val_loss: 17.9287 - val_MinusLogProbMetric: 17.9287 - lr: 1.1111e-04 - 73s/epoch - 371ms/step
Epoch 624/1000
2023-09-27 09:03:01.376 
Epoch 624/1000 
	 loss: 17.7632, MinusLogProbMetric: 17.7632, val_loss: 17.7324, val_MinusLogProbMetric: 17.7324

Epoch 624: val_loss improved from 17.79222 to 17.73237, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 79s - loss: 17.7632 - MinusLogProbMetric: 17.7632 - val_loss: 17.7324 - val_MinusLogProbMetric: 17.7324 - lr: 1.1111e-04 - 79s/epoch - 403ms/step
Epoch 625/1000
2023-09-27 09:04:20.900 
Epoch 625/1000 
	 loss: 17.7195, MinusLogProbMetric: 17.7195, val_loss: 17.9741, val_MinusLogProbMetric: 17.9741

Epoch 625: val_loss did not improve from 17.73237
196/196 - 78s - loss: 17.7195 - MinusLogProbMetric: 17.7195 - val_loss: 17.9741 - val_MinusLogProbMetric: 17.9741 - lr: 1.1111e-04 - 78s/epoch - 398ms/step
Epoch 626/1000
2023-09-27 09:05:38.295 
Epoch 626/1000 
	 loss: 17.7280, MinusLogProbMetric: 17.7280, val_loss: 17.7491, val_MinusLogProbMetric: 17.7491

Epoch 626: val_loss did not improve from 17.73237
196/196 - 77s - loss: 17.7280 - MinusLogProbMetric: 17.7280 - val_loss: 17.7491 - val_MinusLogProbMetric: 17.7491 - lr: 1.1111e-04 - 77s/epoch - 395ms/step
Epoch 627/1000
2023-09-27 09:06:55.866 
Epoch 627/1000 
	 loss: 17.6738, MinusLogProbMetric: 17.6738, val_loss: 17.8910, val_MinusLogProbMetric: 17.8910

Epoch 627: val_loss did not improve from 17.73237
196/196 - 78s - loss: 17.6738 - MinusLogProbMetric: 17.6738 - val_loss: 17.8910 - val_MinusLogProbMetric: 17.8910 - lr: 1.1111e-04 - 78s/epoch - 396ms/step
Epoch 628/1000
2023-09-27 09:08:12.925 
Epoch 628/1000 
	 loss: 17.9440, MinusLogProbMetric: 17.9440, val_loss: 21.8760, val_MinusLogProbMetric: 21.8760

Epoch 628: val_loss did not improve from 17.73237
196/196 - 77s - loss: 17.9440 - MinusLogProbMetric: 17.9440 - val_loss: 21.8760 - val_MinusLogProbMetric: 21.8760 - lr: 1.1111e-04 - 77s/epoch - 393ms/step
Epoch 629/1000
2023-09-27 09:09:29.428 
Epoch 629/1000 
	 loss: 17.9315, MinusLogProbMetric: 17.9315, val_loss: 17.7654, val_MinusLogProbMetric: 17.7654

Epoch 629: val_loss did not improve from 17.73237
196/196 - 76s - loss: 17.9315 - MinusLogProbMetric: 17.9315 - val_loss: 17.7654 - val_MinusLogProbMetric: 17.7654 - lr: 1.1111e-04 - 76s/epoch - 390ms/step
Epoch 630/1000
2023-09-27 09:10:46.069 
Epoch 630/1000 
	 loss: 18.2410, MinusLogProbMetric: 18.2410, val_loss: 17.8912, val_MinusLogProbMetric: 17.8912

Epoch 630: val_loss did not improve from 17.73237
196/196 - 77s - loss: 18.2410 - MinusLogProbMetric: 18.2410 - val_loss: 17.8912 - val_MinusLogProbMetric: 17.8912 - lr: 1.1111e-04 - 77s/epoch - 391ms/step
Epoch 631/1000
2023-09-27 09:12:03.578 
Epoch 631/1000 
	 loss: 17.6722, MinusLogProbMetric: 17.6722, val_loss: 17.8852, val_MinusLogProbMetric: 17.8852

Epoch 631: val_loss did not improve from 17.73237
196/196 - 78s - loss: 17.6722 - MinusLogProbMetric: 17.6722 - val_loss: 17.8852 - val_MinusLogProbMetric: 17.8852 - lr: 1.1111e-04 - 78s/epoch - 395ms/step
Epoch 632/1000
2023-09-27 09:13:20.496 
Epoch 632/1000 
	 loss: 17.6663, MinusLogProbMetric: 17.6663, val_loss: 17.8151, val_MinusLogProbMetric: 17.8151

Epoch 632: val_loss did not improve from 17.73237
196/196 - 77s - loss: 17.6663 - MinusLogProbMetric: 17.6663 - val_loss: 17.8151 - val_MinusLogProbMetric: 17.8151 - lr: 1.1111e-04 - 77s/epoch - 392ms/step
Epoch 633/1000
2023-09-27 09:14:37.515 
Epoch 633/1000 
	 loss: 17.6722, MinusLogProbMetric: 17.6722, val_loss: 17.9453, val_MinusLogProbMetric: 17.9453

Epoch 633: val_loss did not improve from 17.73237
196/196 - 77s - loss: 17.6722 - MinusLogProbMetric: 17.6722 - val_loss: 17.9453 - val_MinusLogProbMetric: 17.9453 - lr: 1.1111e-04 - 77s/epoch - 393ms/step
Epoch 634/1000
2023-09-27 09:15:54.137 
Epoch 634/1000 
	 loss: 17.6834, MinusLogProbMetric: 17.6834, val_loss: 17.8023, val_MinusLogProbMetric: 17.8023

Epoch 634: val_loss did not improve from 17.73237
196/196 - 77s - loss: 17.6834 - MinusLogProbMetric: 17.6834 - val_loss: 17.8023 - val_MinusLogProbMetric: 17.8023 - lr: 1.1111e-04 - 77s/epoch - 391ms/step
Epoch 635/1000
2023-09-27 09:17:10.418 
Epoch 635/1000 
	 loss: 17.6735, MinusLogProbMetric: 17.6735, val_loss: 17.8714, val_MinusLogProbMetric: 17.8714

Epoch 635: val_loss did not improve from 17.73237
196/196 - 76s - loss: 17.6735 - MinusLogProbMetric: 17.6735 - val_loss: 17.8714 - val_MinusLogProbMetric: 17.8714 - lr: 1.1111e-04 - 76s/epoch - 389ms/step
Epoch 636/1000
2023-09-27 09:18:25.025 
Epoch 636/1000 
	 loss: 17.6942, MinusLogProbMetric: 17.6942, val_loss: 18.0687, val_MinusLogProbMetric: 18.0687

Epoch 636: val_loss did not improve from 17.73237
196/196 - 75s - loss: 17.6942 - MinusLogProbMetric: 17.6942 - val_loss: 18.0687 - val_MinusLogProbMetric: 18.0687 - lr: 1.1111e-04 - 75s/epoch - 381ms/step
Epoch 637/1000
2023-09-27 09:19:38.918 
Epoch 637/1000 
	 loss: 18.0224, MinusLogProbMetric: 18.0224, val_loss: 19.6268, val_MinusLogProbMetric: 19.6268

Epoch 637: val_loss did not improve from 17.73237
196/196 - 74s - loss: 18.0224 - MinusLogProbMetric: 18.0224 - val_loss: 19.6268 - val_MinusLogProbMetric: 19.6268 - lr: 1.1111e-04 - 74s/epoch - 377ms/step
Epoch 638/1000
2023-09-27 09:20:55.303 
Epoch 638/1000 
	 loss: 17.8965, MinusLogProbMetric: 17.8965, val_loss: 17.8506, val_MinusLogProbMetric: 17.8506

Epoch 638: val_loss did not improve from 17.73237
196/196 - 76s - loss: 17.8965 - MinusLogProbMetric: 17.8965 - val_loss: 17.8506 - val_MinusLogProbMetric: 17.8506 - lr: 1.1111e-04 - 76s/epoch - 390ms/step
Epoch 639/1000
2023-09-27 09:22:08.396 
Epoch 639/1000 
	 loss: 17.7579, MinusLogProbMetric: 17.7579, val_loss: 17.9000, val_MinusLogProbMetric: 17.9000

Epoch 639: val_loss did not improve from 17.73237
196/196 - 73s - loss: 17.7579 - MinusLogProbMetric: 17.7579 - val_loss: 17.9000 - val_MinusLogProbMetric: 17.9000 - lr: 1.1111e-04 - 73s/epoch - 373ms/step
Epoch 640/1000
2023-09-27 09:23:15.126 
Epoch 640/1000 
	 loss: 17.6706, MinusLogProbMetric: 17.6706, val_loss: 18.1180, val_MinusLogProbMetric: 18.1180

Epoch 640: val_loss did not improve from 17.73237
196/196 - 67s - loss: 17.6706 - MinusLogProbMetric: 17.6706 - val_loss: 18.1180 - val_MinusLogProbMetric: 18.1180 - lr: 1.1111e-04 - 67s/epoch - 340ms/step
Epoch 641/1000
2023-09-27 09:24:17.298 
Epoch 641/1000 
	 loss: 17.7071, MinusLogProbMetric: 17.7071, val_loss: 17.8030, val_MinusLogProbMetric: 17.8030

Epoch 641: val_loss did not improve from 17.73237
196/196 - 62s - loss: 17.7071 - MinusLogProbMetric: 17.7071 - val_loss: 17.8030 - val_MinusLogProbMetric: 17.8030 - lr: 1.1111e-04 - 62s/epoch - 317ms/step
Epoch 642/1000
2023-09-27 09:25:28.820 
Epoch 642/1000 
	 loss: 17.7038, MinusLogProbMetric: 17.7038, val_loss: 17.8041, val_MinusLogProbMetric: 17.8041

Epoch 642: val_loss did not improve from 17.73237
196/196 - 72s - loss: 17.7038 - MinusLogProbMetric: 17.7038 - val_loss: 17.8041 - val_MinusLogProbMetric: 17.8041 - lr: 1.1111e-04 - 72s/epoch - 365ms/step
Epoch 643/1000
2023-09-27 09:26:33.333 
Epoch 643/1000 
	 loss: 17.6874, MinusLogProbMetric: 17.6874, val_loss: 18.3689, val_MinusLogProbMetric: 18.3689

Epoch 643: val_loss did not improve from 17.73237
196/196 - 65s - loss: 17.6874 - MinusLogProbMetric: 17.6874 - val_loss: 18.3689 - val_MinusLogProbMetric: 18.3689 - lr: 1.1111e-04 - 65s/epoch - 329ms/step
Epoch 644/1000
2023-09-27 09:27:38.760 
Epoch 644/1000 
	 loss: 17.6813, MinusLogProbMetric: 17.6813, val_loss: 17.7605, val_MinusLogProbMetric: 17.7605

Epoch 644: val_loss did not improve from 17.73237
196/196 - 65s - loss: 17.6813 - MinusLogProbMetric: 17.6813 - val_loss: 17.7605 - val_MinusLogProbMetric: 17.7605 - lr: 1.1111e-04 - 65s/epoch - 334ms/step
Epoch 645/1000
2023-09-27 09:28:53.205 
Epoch 645/1000 
	 loss: 17.7425, MinusLogProbMetric: 17.7425, val_loss: 17.7880, val_MinusLogProbMetric: 17.7880

Epoch 645: val_loss did not improve from 17.73237
196/196 - 74s - loss: 17.7425 - MinusLogProbMetric: 17.7425 - val_loss: 17.7880 - val_MinusLogProbMetric: 17.7880 - lr: 1.1111e-04 - 74s/epoch - 380ms/step
Epoch 646/1000
2023-09-27 09:30:09.026 
Epoch 646/1000 
	 loss: 17.6691, MinusLogProbMetric: 17.6691, val_loss: 17.9113, val_MinusLogProbMetric: 17.9113

Epoch 646: val_loss did not improve from 17.73237
196/196 - 76s - loss: 17.6691 - MinusLogProbMetric: 17.6691 - val_loss: 17.9113 - val_MinusLogProbMetric: 17.9113 - lr: 1.1111e-04 - 76s/epoch - 387ms/step
Epoch 647/1000
2023-09-27 09:31:25.764 
Epoch 647/1000 
	 loss: 17.6790, MinusLogProbMetric: 17.6790, val_loss: 17.8691, val_MinusLogProbMetric: 17.8691

Epoch 647: val_loss did not improve from 17.73237
196/196 - 77s - loss: 17.6790 - MinusLogProbMetric: 17.6790 - val_loss: 17.8691 - val_MinusLogProbMetric: 17.8691 - lr: 1.1111e-04 - 77s/epoch - 391ms/step
Epoch 648/1000
2023-09-27 09:32:42.566 
Epoch 648/1000 
	 loss: 17.6517, MinusLogProbMetric: 17.6517, val_loss: 17.8927, val_MinusLogProbMetric: 17.8927

Epoch 648: val_loss did not improve from 17.73237
196/196 - 77s - loss: 17.6517 - MinusLogProbMetric: 17.6517 - val_loss: 17.8927 - val_MinusLogProbMetric: 17.8927 - lr: 1.1111e-04 - 77s/epoch - 392ms/step
Epoch 649/1000
2023-09-27 09:33:59.961 
Epoch 649/1000 
	 loss: 17.6652, MinusLogProbMetric: 17.6652, val_loss: 17.9181, val_MinusLogProbMetric: 17.9181

Epoch 649: val_loss did not improve from 17.73237
196/196 - 77s - loss: 17.6652 - MinusLogProbMetric: 17.6652 - val_loss: 17.9181 - val_MinusLogProbMetric: 17.9181 - lr: 1.1111e-04 - 77s/epoch - 395ms/step
Epoch 650/1000
2023-09-27 09:35:17.124 
Epoch 650/1000 
	 loss: 17.6858, MinusLogProbMetric: 17.6858, val_loss: 17.9680, val_MinusLogProbMetric: 17.9680

Epoch 650: val_loss did not improve from 17.73237
196/196 - 77s - loss: 17.6858 - MinusLogProbMetric: 17.6858 - val_loss: 17.9680 - val_MinusLogProbMetric: 17.9680 - lr: 1.1111e-04 - 77s/epoch - 394ms/step
Epoch 651/1000
2023-09-27 09:36:34.494 
Epoch 651/1000 
	 loss: 17.6868, MinusLogProbMetric: 17.6868, val_loss: 17.8047, val_MinusLogProbMetric: 17.8047

Epoch 651: val_loss did not improve from 17.73237
196/196 - 77s - loss: 17.6868 - MinusLogProbMetric: 17.6868 - val_loss: 17.8047 - val_MinusLogProbMetric: 17.8047 - lr: 1.1111e-04 - 77s/epoch - 395ms/step
Epoch 652/1000
2023-09-27 09:37:51.040 
Epoch 652/1000 
	 loss: 17.6958, MinusLogProbMetric: 17.6958, val_loss: 17.8879, val_MinusLogProbMetric: 17.8879

Epoch 652: val_loss did not improve from 17.73237
196/196 - 77s - loss: 17.6958 - MinusLogProbMetric: 17.6958 - val_loss: 17.8879 - val_MinusLogProbMetric: 17.8879 - lr: 1.1111e-04 - 77s/epoch - 391ms/step
Epoch 653/1000
2023-09-27 09:39:07.244 
Epoch 653/1000 
	 loss: 17.7919, MinusLogProbMetric: 17.7919, val_loss: 18.0067, val_MinusLogProbMetric: 18.0067

Epoch 653: val_loss did not improve from 17.73237
196/196 - 76s - loss: 17.7919 - MinusLogProbMetric: 17.7919 - val_loss: 18.0067 - val_MinusLogProbMetric: 18.0067 - lr: 1.1111e-04 - 76s/epoch - 389ms/step
Epoch 654/1000
2023-09-27 09:40:24.173 
Epoch 654/1000 
	 loss: 17.8849, MinusLogProbMetric: 17.8849, val_loss: 17.8319, val_MinusLogProbMetric: 17.8319

Epoch 654: val_loss did not improve from 17.73237
196/196 - 77s - loss: 17.8849 - MinusLogProbMetric: 17.8849 - val_loss: 17.8319 - val_MinusLogProbMetric: 17.8319 - lr: 1.1111e-04 - 77s/epoch - 392ms/step
Epoch 655/1000
2023-09-27 09:41:41.253 
Epoch 655/1000 
	 loss: 17.7002, MinusLogProbMetric: 17.7002, val_loss: 17.8745, val_MinusLogProbMetric: 17.8745

Epoch 655: val_loss did not improve from 17.73237
196/196 - 77s - loss: 17.7002 - MinusLogProbMetric: 17.7002 - val_loss: 17.8745 - val_MinusLogProbMetric: 17.8745 - lr: 1.1111e-04 - 77s/epoch - 393ms/step
Epoch 656/1000
2023-09-27 09:42:58.453 
Epoch 656/1000 
	 loss: 17.6911, MinusLogProbMetric: 17.6911, val_loss: 17.7565, val_MinusLogProbMetric: 17.7565

Epoch 656: val_loss did not improve from 17.73237
196/196 - 77s - loss: 17.6911 - MinusLogProbMetric: 17.6911 - val_loss: 17.7565 - val_MinusLogProbMetric: 17.7565 - lr: 1.1111e-04 - 77s/epoch - 394ms/step
Epoch 657/1000
2023-09-27 09:44:15.365 
Epoch 657/1000 
	 loss: 17.6570, MinusLogProbMetric: 17.6570, val_loss: 17.8335, val_MinusLogProbMetric: 17.8335

Epoch 657: val_loss did not improve from 17.73237
196/196 - 77s - loss: 17.6570 - MinusLogProbMetric: 17.6570 - val_loss: 17.8335 - val_MinusLogProbMetric: 17.8335 - lr: 1.1111e-04 - 77s/epoch - 392ms/step
Epoch 658/1000
2023-09-27 09:45:32.427 
Epoch 658/1000 
	 loss: 17.6728, MinusLogProbMetric: 17.6728, val_loss: 17.8982, val_MinusLogProbMetric: 17.8982

Epoch 658: val_loss did not improve from 17.73237
196/196 - 77s - loss: 17.6728 - MinusLogProbMetric: 17.6728 - val_loss: 17.8982 - val_MinusLogProbMetric: 17.8982 - lr: 1.1111e-04 - 77s/epoch - 393ms/step
Epoch 659/1000
2023-09-27 09:46:49.676 
Epoch 659/1000 
	 loss: 17.6388, MinusLogProbMetric: 17.6388, val_loss: 18.0421, val_MinusLogProbMetric: 18.0421

Epoch 659: val_loss did not improve from 17.73237
196/196 - 77s - loss: 17.6388 - MinusLogProbMetric: 17.6388 - val_loss: 18.0421 - val_MinusLogProbMetric: 18.0421 - lr: 1.1111e-04 - 77s/epoch - 394ms/step
Epoch 660/1000
2023-09-27 09:48:06.689 
Epoch 660/1000 
	 loss: 17.6885, MinusLogProbMetric: 17.6885, val_loss: 17.8677, val_MinusLogProbMetric: 17.8677

Epoch 660: val_loss did not improve from 17.73237
196/196 - 77s - loss: 17.6885 - MinusLogProbMetric: 17.6885 - val_loss: 17.8677 - val_MinusLogProbMetric: 17.8677 - lr: 1.1111e-04 - 77s/epoch - 393ms/step
Epoch 661/1000
2023-09-27 09:49:24.482 
Epoch 661/1000 
	 loss: 17.6575, MinusLogProbMetric: 17.6575, val_loss: 17.8140, val_MinusLogProbMetric: 17.8140

Epoch 661: val_loss did not improve from 17.73237
196/196 - 78s - loss: 17.6575 - MinusLogProbMetric: 17.6575 - val_loss: 17.8140 - val_MinusLogProbMetric: 17.8140 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 662/1000
2023-09-27 09:50:42.366 
Epoch 662/1000 
	 loss: 17.6546, MinusLogProbMetric: 17.6546, val_loss: 17.9342, val_MinusLogProbMetric: 17.9342

Epoch 662: val_loss did not improve from 17.73237
196/196 - 78s - loss: 17.6546 - MinusLogProbMetric: 17.6546 - val_loss: 17.9342 - val_MinusLogProbMetric: 17.9342 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 663/1000
2023-09-27 09:51:59.871 
Epoch 663/1000 
	 loss: 17.6932, MinusLogProbMetric: 17.6932, val_loss: 17.7813, val_MinusLogProbMetric: 17.7813

Epoch 663: val_loss did not improve from 17.73237
196/196 - 78s - loss: 17.6932 - MinusLogProbMetric: 17.6932 - val_loss: 17.7813 - val_MinusLogProbMetric: 17.7813 - lr: 1.1111e-04 - 78s/epoch - 395ms/step
Epoch 664/1000
2023-09-27 09:53:16.934 
Epoch 664/1000 
	 loss: 17.6294, MinusLogProbMetric: 17.6294, val_loss: 18.0161, val_MinusLogProbMetric: 18.0161

Epoch 664: val_loss did not improve from 17.73237
196/196 - 77s - loss: 17.6294 - MinusLogProbMetric: 17.6294 - val_loss: 18.0161 - val_MinusLogProbMetric: 18.0161 - lr: 1.1111e-04 - 77s/epoch - 393ms/step
Epoch 665/1000
2023-09-27 09:54:34.167 
Epoch 665/1000 
	 loss: 17.6370, MinusLogProbMetric: 17.6370, val_loss: 17.8051, val_MinusLogProbMetric: 17.8051

Epoch 665: val_loss did not improve from 17.73237
196/196 - 77s - loss: 17.6370 - MinusLogProbMetric: 17.6370 - val_loss: 17.8051 - val_MinusLogProbMetric: 17.8051 - lr: 1.1111e-04 - 77s/epoch - 394ms/step
Epoch 666/1000
2023-09-27 09:55:51.444 
Epoch 666/1000 
	 loss: 17.6734, MinusLogProbMetric: 17.6734, val_loss: 17.8767, val_MinusLogProbMetric: 17.8767

Epoch 666: val_loss did not improve from 17.73237
196/196 - 77s - loss: 17.6734 - MinusLogProbMetric: 17.6734 - val_loss: 17.8767 - val_MinusLogProbMetric: 17.8767 - lr: 1.1111e-04 - 77s/epoch - 394ms/step
Epoch 667/1000
2023-09-27 09:57:07.949 
Epoch 667/1000 
	 loss: 17.6551, MinusLogProbMetric: 17.6551, val_loss: 17.9663, val_MinusLogProbMetric: 17.9663

Epoch 667: val_loss did not improve from 17.73237
196/196 - 77s - loss: 17.6551 - MinusLogProbMetric: 17.6551 - val_loss: 17.9663 - val_MinusLogProbMetric: 17.9663 - lr: 1.1111e-04 - 77s/epoch - 390ms/step
Epoch 668/1000
2023-09-27 09:58:24.821 
Epoch 668/1000 
	 loss: 17.6310, MinusLogProbMetric: 17.6310, val_loss: 18.0101, val_MinusLogProbMetric: 18.0101

Epoch 668: val_loss did not improve from 17.73237
196/196 - 77s - loss: 17.6310 - MinusLogProbMetric: 17.6310 - val_loss: 18.0101 - val_MinusLogProbMetric: 18.0101 - lr: 1.1111e-04 - 77s/epoch - 392ms/step
Epoch 669/1000
2023-09-27 09:59:41.470 
Epoch 669/1000 
	 loss: 17.6460, MinusLogProbMetric: 17.6460, val_loss: 17.8241, val_MinusLogProbMetric: 17.8241

Epoch 669: val_loss did not improve from 17.73237
196/196 - 77s - loss: 17.6460 - MinusLogProbMetric: 17.6460 - val_loss: 17.8241 - val_MinusLogProbMetric: 17.8241 - lr: 1.1111e-04 - 77s/epoch - 391ms/step
Epoch 670/1000
2023-09-27 10:00:57.994 
Epoch 670/1000 
	 loss: 17.6631, MinusLogProbMetric: 17.6631, val_loss: 17.7581, val_MinusLogProbMetric: 17.7581

Epoch 670: val_loss did not improve from 17.73237
196/196 - 77s - loss: 17.6631 - MinusLogProbMetric: 17.6631 - val_loss: 17.7581 - val_MinusLogProbMetric: 17.7581 - lr: 1.1111e-04 - 77s/epoch - 390ms/step
Epoch 671/1000
2023-09-27 10:02:14.904 
Epoch 671/1000 
	 loss: 17.7250, MinusLogProbMetric: 17.7250, val_loss: 17.8533, val_MinusLogProbMetric: 17.8533

Epoch 671: val_loss did not improve from 17.73237
196/196 - 77s - loss: 17.7250 - MinusLogProbMetric: 17.7250 - val_loss: 17.8533 - val_MinusLogProbMetric: 17.8533 - lr: 1.1111e-04 - 77s/epoch - 392ms/step
Epoch 672/1000
2023-09-27 10:03:31.592 
Epoch 672/1000 
	 loss: 17.6783, MinusLogProbMetric: 17.6783, val_loss: 17.9173, val_MinusLogProbMetric: 17.9173

Epoch 672: val_loss did not improve from 17.73237
196/196 - 77s - loss: 17.6783 - MinusLogProbMetric: 17.6783 - val_loss: 17.9173 - val_MinusLogProbMetric: 17.9173 - lr: 1.1111e-04 - 77s/epoch - 391ms/step
Epoch 673/1000
2023-09-27 10:04:48.250 
Epoch 673/1000 
	 loss: 17.7183, MinusLogProbMetric: 17.7183, val_loss: 18.4523, val_MinusLogProbMetric: 18.4523

Epoch 673: val_loss did not improve from 17.73237
196/196 - 77s - loss: 17.7183 - MinusLogProbMetric: 17.7183 - val_loss: 18.4523 - val_MinusLogProbMetric: 18.4523 - lr: 1.1111e-04 - 77s/epoch - 391ms/step
Epoch 674/1000
2023-09-27 10:06:05.356 
Epoch 674/1000 
	 loss: 17.7157, MinusLogProbMetric: 17.7157, val_loss: 18.7806, val_MinusLogProbMetric: 18.7806

Epoch 674: val_loss did not improve from 17.73237
196/196 - 77s - loss: 17.7157 - MinusLogProbMetric: 17.7157 - val_loss: 18.7806 - val_MinusLogProbMetric: 18.7806 - lr: 1.1111e-04 - 77s/epoch - 393ms/step
Epoch 675/1000
2023-09-27 10:07:14.325 
Epoch 675/1000 
	 loss: 17.5193, MinusLogProbMetric: 17.5193, val_loss: 17.7340, val_MinusLogProbMetric: 17.7340

Epoch 675: val_loss did not improve from 17.73237
196/196 - 69s - loss: 17.5193 - MinusLogProbMetric: 17.5193 - val_loss: 17.7340 - val_MinusLogProbMetric: 17.7340 - lr: 5.5556e-05 - 69s/epoch - 352ms/step
Epoch 676/1000
2023-09-27 10:08:23.827 
Epoch 676/1000 
	 loss: 17.4736, MinusLogProbMetric: 17.4736, val_loss: 17.6365, val_MinusLogProbMetric: 17.6365

Epoch 676: val_loss improved from 17.73237 to 17.63648, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 71s - loss: 17.4736 - MinusLogProbMetric: 17.4736 - val_loss: 17.6365 - val_MinusLogProbMetric: 17.6365 - lr: 5.5556e-05 - 71s/epoch - 363ms/step
Epoch 677/1000
2023-09-27 10:09:42.813 
Epoch 677/1000 
	 loss: 17.4762, MinusLogProbMetric: 17.4762, val_loss: 17.6099, val_MinusLogProbMetric: 17.6099

Epoch 677: val_loss improved from 17.63648 to 17.60991, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 79s - loss: 17.4762 - MinusLogProbMetric: 17.4762 - val_loss: 17.6099 - val_MinusLogProbMetric: 17.6099 - lr: 5.5556e-05 - 79s/epoch - 403ms/step
Epoch 678/1000
2023-09-27 10:11:02.085 
Epoch 678/1000 
	 loss: 17.4617, MinusLogProbMetric: 17.4617, val_loss: 17.7048, val_MinusLogProbMetric: 17.7048

Epoch 678: val_loss did not improve from 17.60991
196/196 - 78s - loss: 17.4617 - MinusLogProbMetric: 17.4617 - val_loss: 17.7048 - val_MinusLogProbMetric: 17.7048 - lr: 5.5556e-05 - 78s/epoch - 396ms/step
Epoch 679/1000
2023-09-27 10:12:20.056 
Epoch 679/1000 
	 loss: 17.4919, MinusLogProbMetric: 17.4919, val_loss: 17.7237, val_MinusLogProbMetric: 17.7237

Epoch 679: val_loss did not improve from 17.60991
196/196 - 78s - loss: 17.4919 - MinusLogProbMetric: 17.4919 - val_loss: 17.7237 - val_MinusLogProbMetric: 17.7237 - lr: 5.5556e-05 - 78s/epoch - 398ms/step
Epoch 680/1000
2023-09-27 10:13:38.711 
Epoch 680/1000 
	 loss: 17.5084, MinusLogProbMetric: 17.5084, val_loss: 17.7029, val_MinusLogProbMetric: 17.7029

Epoch 680: val_loss did not improve from 17.60991
196/196 - 79s - loss: 17.5084 - MinusLogProbMetric: 17.5084 - val_loss: 17.7029 - val_MinusLogProbMetric: 17.7029 - lr: 5.5556e-05 - 79s/epoch - 401ms/step
Epoch 681/1000
2023-09-27 10:14:56.846 
Epoch 681/1000 
	 loss: 17.4757, MinusLogProbMetric: 17.4757, val_loss: 17.7835, val_MinusLogProbMetric: 17.7835

Epoch 681: val_loss did not improve from 17.60991
196/196 - 78s - loss: 17.4757 - MinusLogProbMetric: 17.4757 - val_loss: 17.7835 - val_MinusLogProbMetric: 17.7835 - lr: 5.5556e-05 - 78s/epoch - 399ms/step
Epoch 682/1000
2023-09-27 10:16:14.780 
Epoch 682/1000 
	 loss: 17.6825, MinusLogProbMetric: 17.6825, val_loss: 19.2261, val_MinusLogProbMetric: 19.2261

Epoch 682: val_loss did not improve from 17.60991
196/196 - 78s - loss: 17.6825 - MinusLogProbMetric: 17.6825 - val_loss: 19.2261 - val_MinusLogProbMetric: 19.2261 - lr: 5.5556e-05 - 78s/epoch - 398ms/step
Epoch 683/1000
2023-09-27 10:17:32.437 
Epoch 683/1000 
	 loss: 17.5619, MinusLogProbMetric: 17.5619, val_loss: 17.6133, val_MinusLogProbMetric: 17.6133

Epoch 683: val_loss did not improve from 17.60991
196/196 - 78s - loss: 17.5619 - MinusLogProbMetric: 17.5619 - val_loss: 17.6133 - val_MinusLogProbMetric: 17.6133 - lr: 5.5556e-05 - 78s/epoch - 396ms/step
Epoch 684/1000
2023-09-27 10:18:50.792 
Epoch 684/1000 
	 loss: 17.5137, MinusLogProbMetric: 17.5137, val_loss: 17.6625, val_MinusLogProbMetric: 17.6625

Epoch 684: val_loss did not improve from 17.60991
196/196 - 78s - loss: 17.5137 - MinusLogProbMetric: 17.5137 - val_loss: 17.6625 - val_MinusLogProbMetric: 17.6625 - lr: 5.5556e-05 - 78s/epoch - 400ms/step
Epoch 685/1000
2023-09-27 10:20:09.383 
Epoch 685/1000 
	 loss: 17.5026, MinusLogProbMetric: 17.5026, val_loss: 17.6190, val_MinusLogProbMetric: 17.6190

Epoch 685: val_loss did not improve from 17.60991
196/196 - 79s - loss: 17.5026 - MinusLogProbMetric: 17.5026 - val_loss: 17.6190 - val_MinusLogProbMetric: 17.6190 - lr: 5.5556e-05 - 79s/epoch - 401ms/step
Epoch 686/1000
2023-09-27 10:21:28.115 
Epoch 686/1000 
	 loss: 17.4601, MinusLogProbMetric: 17.4601, val_loss: 17.6239, val_MinusLogProbMetric: 17.6239

Epoch 686: val_loss did not improve from 17.60991
196/196 - 79s - loss: 17.4601 - MinusLogProbMetric: 17.4601 - val_loss: 17.6239 - val_MinusLogProbMetric: 17.6239 - lr: 5.5556e-05 - 79s/epoch - 402ms/step
Epoch 687/1000
2023-09-27 10:22:45.974 
Epoch 687/1000 
	 loss: 17.4537, MinusLogProbMetric: 17.4537, val_loss: 17.5988, val_MinusLogProbMetric: 17.5988

Epoch 687: val_loss improved from 17.60991 to 17.59879, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 79s - loss: 17.4537 - MinusLogProbMetric: 17.4537 - val_loss: 17.5988 - val_MinusLogProbMetric: 17.5988 - lr: 5.5556e-05 - 79s/epoch - 405ms/step
Epoch 688/1000
2023-09-27 10:24:05.934 
Epoch 688/1000 
	 loss: 17.4525, MinusLogProbMetric: 17.4525, val_loss: 18.0098, val_MinusLogProbMetric: 18.0098

Epoch 688: val_loss did not improve from 17.59879
196/196 - 78s - loss: 17.4525 - MinusLogProbMetric: 17.4525 - val_loss: 18.0098 - val_MinusLogProbMetric: 18.0098 - lr: 5.5556e-05 - 78s/epoch - 400ms/step
Epoch 689/1000
2023-09-27 10:25:24.103 
Epoch 689/1000 
	 loss: 17.6444, MinusLogProbMetric: 17.6444, val_loss: 17.7108, val_MinusLogProbMetric: 17.7108

Epoch 689: val_loss did not improve from 17.59879
196/196 - 78s - loss: 17.6444 - MinusLogProbMetric: 17.6444 - val_loss: 17.7108 - val_MinusLogProbMetric: 17.7108 - lr: 5.5556e-05 - 78s/epoch - 399ms/step
Epoch 690/1000
2023-09-27 10:26:41.932 
Epoch 690/1000 
	 loss: 17.5141, MinusLogProbMetric: 17.5141, val_loss: 17.6190, val_MinusLogProbMetric: 17.6190

Epoch 690: val_loss did not improve from 17.59879
196/196 - 78s - loss: 17.5141 - MinusLogProbMetric: 17.5141 - val_loss: 17.6190 - val_MinusLogProbMetric: 17.6190 - lr: 5.5556e-05 - 78s/epoch - 397ms/step
Epoch 691/1000
2023-09-27 10:27:59.451 
Epoch 691/1000 
	 loss: 17.4372, MinusLogProbMetric: 17.4372, val_loss: 17.6631, val_MinusLogProbMetric: 17.6631

Epoch 691: val_loss did not improve from 17.59879
196/196 - 78s - loss: 17.4372 - MinusLogProbMetric: 17.4372 - val_loss: 17.6631 - val_MinusLogProbMetric: 17.6631 - lr: 5.5556e-05 - 78s/epoch - 395ms/step
Epoch 692/1000
2023-09-27 10:29:17.253 
Epoch 692/1000 
	 loss: 17.4814, MinusLogProbMetric: 17.4814, val_loss: 17.5885, val_MinusLogProbMetric: 17.5885

Epoch 692: val_loss improved from 17.59879 to 17.58850, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 80s - loss: 17.4814 - MinusLogProbMetric: 17.4814 - val_loss: 17.5885 - val_MinusLogProbMetric: 17.5885 - lr: 5.5556e-05 - 80s/epoch - 406ms/step
Epoch 693/1000
2023-09-27 10:30:36.621 
Epoch 693/1000 
	 loss: 17.4411, MinusLogProbMetric: 17.4411, val_loss: 17.6175, val_MinusLogProbMetric: 17.6175

Epoch 693: val_loss did not improve from 17.58850
196/196 - 78s - loss: 17.4411 - MinusLogProbMetric: 17.4411 - val_loss: 17.6175 - val_MinusLogProbMetric: 17.6175 - lr: 5.5556e-05 - 78s/epoch - 396ms/step
Epoch 694/1000
2023-09-27 10:31:54.701 
Epoch 694/1000 
	 loss: 17.4625, MinusLogProbMetric: 17.4625, val_loss: 17.6363, val_MinusLogProbMetric: 17.6363

Epoch 694: val_loss did not improve from 17.58850
196/196 - 78s - loss: 17.4625 - MinusLogProbMetric: 17.4625 - val_loss: 17.6363 - val_MinusLogProbMetric: 17.6363 - lr: 5.5556e-05 - 78s/epoch - 398ms/step
Epoch 695/1000
2023-09-27 10:33:13.307 
Epoch 695/1000 
	 loss: 17.4649, MinusLogProbMetric: 17.4649, val_loss: 17.7145, val_MinusLogProbMetric: 17.7145

Epoch 695: val_loss did not improve from 17.58850
196/196 - 79s - loss: 17.4649 - MinusLogProbMetric: 17.4649 - val_loss: 17.7145 - val_MinusLogProbMetric: 17.7145 - lr: 5.5556e-05 - 79s/epoch - 401ms/step
Epoch 696/1000
2023-09-27 10:34:31.385 
Epoch 696/1000 
	 loss: 17.4469, MinusLogProbMetric: 17.4469, val_loss: 17.5964, val_MinusLogProbMetric: 17.5964

Epoch 696: val_loss did not improve from 17.58850
196/196 - 78s - loss: 17.4469 - MinusLogProbMetric: 17.4469 - val_loss: 17.5964 - val_MinusLogProbMetric: 17.5964 - lr: 5.5556e-05 - 78s/epoch - 398ms/step
Epoch 697/1000
2023-09-27 10:35:49.430 
Epoch 697/1000 
	 loss: 17.5027, MinusLogProbMetric: 17.5027, val_loss: 17.6308, val_MinusLogProbMetric: 17.6308

Epoch 697: val_loss did not improve from 17.58850
196/196 - 78s - loss: 17.5027 - MinusLogProbMetric: 17.5027 - val_loss: 17.6308 - val_MinusLogProbMetric: 17.6308 - lr: 5.5556e-05 - 78s/epoch - 398ms/step
Epoch 698/1000
2023-09-27 10:37:07.681 
Epoch 698/1000 
	 loss: 17.4545, MinusLogProbMetric: 17.4545, val_loss: 17.6748, val_MinusLogProbMetric: 17.6748

Epoch 698: val_loss did not improve from 17.58850
196/196 - 78s - loss: 17.4545 - MinusLogProbMetric: 17.4545 - val_loss: 17.6748 - val_MinusLogProbMetric: 17.6748 - lr: 5.5556e-05 - 78s/epoch - 399ms/step
Epoch 699/1000
2023-09-27 10:38:25.730 
Epoch 699/1000 
	 loss: 17.4408, MinusLogProbMetric: 17.4408, val_loss: 17.7621, val_MinusLogProbMetric: 17.7621

Epoch 699: val_loss did not improve from 17.58850
196/196 - 78s - loss: 17.4408 - MinusLogProbMetric: 17.4408 - val_loss: 17.7621 - val_MinusLogProbMetric: 17.7621 - lr: 5.5556e-05 - 78s/epoch - 398ms/step
Epoch 700/1000
2023-09-27 10:39:43.595 
Epoch 700/1000 
	 loss: 17.4621, MinusLogProbMetric: 17.4621, val_loss: 17.6500, val_MinusLogProbMetric: 17.6500

Epoch 700: val_loss did not improve from 17.58850
196/196 - 78s - loss: 17.4621 - MinusLogProbMetric: 17.4621 - val_loss: 17.6500 - val_MinusLogProbMetric: 17.6500 - lr: 5.5556e-05 - 78s/epoch - 397ms/step
Epoch 701/1000
2023-09-27 10:41:01.260 
Epoch 701/1000 
	 loss: 17.4537, MinusLogProbMetric: 17.4537, val_loss: 17.6109, val_MinusLogProbMetric: 17.6109

Epoch 701: val_loss did not improve from 17.58850
196/196 - 78s - loss: 17.4537 - MinusLogProbMetric: 17.4537 - val_loss: 17.6109 - val_MinusLogProbMetric: 17.6109 - lr: 5.5556e-05 - 78s/epoch - 396ms/step
Epoch 702/1000
2023-09-27 10:42:19.113 
Epoch 702/1000 
	 loss: 17.6561, MinusLogProbMetric: 17.6561, val_loss: 17.9689, val_MinusLogProbMetric: 17.9689

Epoch 702: val_loss did not improve from 17.58850
196/196 - 78s - loss: 17.6561 - MinusLogProbMetric: 17.6561 - val_loss: 17.9689 - val_MinusLogProbMetric: 17.9689 - lr: 5.5556e-05 - 78s/epoch - 397ms/step
Epoch 703/1000
2023-09-27 10:43:36.396 
Epoch 703/1000 
	 loss: 17.4508, MinusLogProbMetric: 17.4508, val_loss: 17.6139, val_MinusLogProbMetric: 17.6139

Epoch 703: val_loss did not improve from 17.58850
196/196 - 77s - loss: 17.4508 - MinusLogProbMetric: 17.4508 - val_loss: 17.6139 - val_MinusLogProbMetric: 17.6139 - lr: 5.5556e-05 - 77s/epoch - 394ms/step
Epoch 704/1000
2023-09-27 10:44:54.368 
Epoch 704/1000 
	 loss: 17.4354, MinusLogProbMetric: 17.4354, val_loss: 17.6003, val_MinusLogProbMetric: 17.6003

Epoch 704: val_loss did not improve from 17.58850
196/196 - 78s - loss: 17.4354 - MinusLogProbMetric: 17.4354 - val_loss: 17.6003 - val_MinusLogProbMetric: 17.6003 - lr: 5.5556e-05 - 78s/epoch - 398ms/step
Epoch 705/1000
2023-09-27 10:46:12.375 
Epoch 705/1000 
	 loss: 17.4656, MinusLogProbMetric: 17.4656, val_loss: 17.6789, val_MinusLogProbMetric: 17.6789

Epoch 705: val_loss did not improve from 17.58850
196/196 - 78s - loss: 17.4656 - MinusLogProbMetric: 17.4656 - val_loss: 17.6789 - val_MinusLogProbMetric: 17.6789 - lr: 5.5556e-05 - 78s/epoch - 398ms/step
Epoch 706/1000
2023-09-27 10:47:30.448 
Epoch 706/1000 
	 loss: 17.4639, MinusLogProbMetric: 17.4639, val_loss: 17.5773, val_MinusLogProbMetric: 17.5773

Epoch 706: val_loss improved from 17.58850 to 17.57729, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 80s - loss: 17.4639 - MinusLogProbMetric: 17.4639 - val_loss: 17.5773 - val_MinusLogProbMetric: 17.5773 - lr: 5.5556e-05 - 80s/epoch - 407ms/step
Epoch 707/1000
2023-09-27 10:48:49.728 
Epoch 707/1000 
	 loss: 17.4454, MinusLogProbMetric: 17.4454, val_loss: 17.8394, val_MinusLogProbMetric: 17.8394

Epoch 707: val_loss did not improve from 17.57729
196/196 - 78s - loss: 17.4454 - MinusLogProbMetric: 17.4454 - val_loss: 17.8394 - val_MinusLogProbMetric: 17.8394 - lr: 5.5556e-05 - 78s/epoch - 396ms/step
Epoch 708/1000
2023-09-27 10:50:07.307 
Epoch 708/1000 
	 loss: 17.4770, MinusLogProbMetric: 17.4770, val_loss: 17.6182, val_MinusLogProbMetric: 17.6182

Epoch 708: val_loss did not improve from 17.57729
196/196 - 78s - loss: 17.4770 - MinusLogProbMetric: 17.4770 - val_loss: 17.6182 - val_MinusLogProbMetric: 17.6182 - lr: 5.5556e-05 - 78s/epoch - 396ms/step
Epoch 709/1000
2023-09-27 10:51:25.311 
Epoch 709/1000 
	 loss: 17.4589, MinusLogProbMetric: 17.4589, val_loss: 17.6723, val_MinusLogProbMetric: 17.6723

Epoch 709: val_loss did not improve from 17.57729
196/196 - 78s - loss: 17.4589 - MinusLogProbMetric: 17.4589 - val_loss: 17.6723 - val_MinusLogProbMetric: 17.6723 - lr: 5.5556e-05 - 78s/epoch - 398ms/step
Epoch 710/1000
2023-09-27 10:52:42.941 
Epoch 710/1000 
	 loss: 17.5203, MinusLogProbMetric: 17.5203, val_loss: 17.8252, val_MinusLogProbMetric: 17.8252

Epoch 710: val_loss did not improve from 17.57729
196/196 - 78s - loss: 17.5203 - MinusLogProbMetric: 17.5203 - val_loss: 17.8252 - val_MinusLogProbMetric: 17.8252 - lr: 5.5556e-05 - 78s/epoch - 396ms/step
Epoch 711/1000
2023-09-27 10:54:00.764 
Epoch 711/1000 
	 loss: 17.4905, MinusLogProbMetric: 17.4905, val_loss: 17.7095, val_MinusLogProbMetric: 17.7095

Epoch 711: val_loss did not improve from 17.57729
196/196 - 78s - loss: 17.4905 - MinusLogProbMetric: 17.4905 - val_loss: 17.7095 - val_MinusLogProbMetric: 17.7095 - lr: 5.5556e-05 - 78s/epoch - 397ms/step
Epoch 712/1000
2023-09-27 10:55:19.032 
Epoch 712/1000 
	 loss: 17.4936, MinusLogProbMetric: 17.4936, val_loss: 17.6772, val_MinusLogProbMetric: 17.6772

Epoch 712: val_loss did not improve from 17.57729
196/196 - 78s - loss: 17.4936 - MinusLogProbMetric: 17.4936 - val_loss: 17.6772 - val_MinusLogProbMetric: 17.6772 - lr: 5.5556e-05 - 78s/epoch - 399ms/step
Epoch 713/1000
2023-09-27 10:56:37.247 
Epoch 713/1000 
	 loss: 17.4447, MinusLogProbMetric: 17.4447, val_loss: 17.6347, val_MinusLogProbMetric: 17.6347

Epoch 713: val_loss did not improve from 17.57729
196/196 - 78s - loss: 17.4447 - MinusLogProbMetric: 17.4447 - val_loss: 17.6347 - val_MinusLogProbMetric: 17.6347 - lr: 5.5556e-05 - 78s/epoch - 399ms/step
Epoch 714/1000
2023-09-27 10:57:55.413 
Epoch 714/1000 
	 loss: 17.5034, MinusLogProbMetric: 17.5034, val_loss: 17.6374, val_MinusLogProbMetric: 17.6374

Epoch 714: val_loss did not improve from 17.57729
196/196 - 78s - loss: 17.5034 - MinusLogProbMetric: 17.5034 - val_loss: 17.6374 - val_MinusLogProbMetric: 17.6374 - lr: 5.5556e-05 - 78s/epoch - 399ms/step
Epoch 715/1000
2023-09-27 10:59:13.343 
Epoch 715/1000 
	 loss: 17.5310, MinusLogProbMetric: 17.5310, val_loss: 17.6170, val_MinusLogProbMetric: 17.6170

Epoch 715: val_loss did not improve from 17.57729
196/196 - 78s - loss: 17.5310 - MinusLogProbMetric: 17.5310 - val_loss: 17.6170 - val_MinusLogProbMetric: 17.6170 - lr: 5.5556e-05 - 78s/epoch - 398ms/step
Epoch 716/1000
2023-09-27 11:00:30.842 
Epoch 716/1000 
	 loss: 17.4486, MinusLogProbMetric: 17.4486, val_loss: 17.5491, val_MinusLogProbMetric: 17.5491

Epoch 716: val_loss improved from 17.57729 to 17.54909, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 79s - loss: 17.4486 - MinusLogProbMetric: 17.4486 - val_loss: 17.5491 - val_MinusLogProbMetric: 17.5491 - lr: 5.5556e-05 - 79s/epoch - 403ms/step
Epoch 717/1000
2023-09-27 11:01:50.096 
Epoch 717/1000 
	 loss: 17.4425, MinusLogProbMetric: 17.4425, val_loss: 17.6008, val_MinusLogProbMetric: 17.6008

Epoch 717: val_loss did not improve from 17.54909
196/196 - 78s - loss: 17.4425 - MinusLogProbMetric: 17.4425 - val_loss: 17.6008 - val_MinusLogProbMetric: 17.6008 - lr: 5.5556e-05 - 78s/epoch - 396ms/step
Epoch 718/1000
2023-09-27 11:03:07.944 
Epoch 718/1000 
	 loss: 17.4664, MinusLogProbMetric: 17.4664, val_loss: 17.7012, val_MinusLogProbMetric: 17.7012

Epoch 718: val_loss did not improve from 17.54909
196/196 - 78s - loss: 17.4664 - MinusLogProbMetric: 17.4664 - val_loss: 17.7012 - val_MinusLogProbMetric: 17.7012 - lr: 5.5556e-05 - 78s/epoch - 397ms/step
Epoch 719/1000
2023-09-27 11:04:25.413 
Epoch 719/1000 
	 loss: 17.4538, MinusLogProbMetric: 17.4538, val_loss: 17.6742, val_MinusLogProbMetric: 17.6742

Epoch 719: val_loss did not improve from 17.54909
196/196 - 77s - loss: 17.4538 - MinusLogProbMetric: 17.4538 - val_loss: 17.6742 - val_MinusLogProbMetric: 17.6742 - lr: 5.5556e-05 - 77s/epoch - 395ms/step
Epoch 720/1000
2023-09-27 11:05:43.489 
Epoch 720/1000 
	 loss: 17.4954, MinusLogProbMetric: 17.4954, val_loss: 17.7074, val_MinusLogProbMetric: 17.7074

Epoch 720: val_loss did not improve from 17.54909
196/196 - 78s - loss: 17.4954 - MinusLogProbMetric: 17.4954 - val_loss: 17.7074 - val_MinusLogProbMetric: 17.7074 - lr: 5.5556e-05 - 78s/epoch - 398ms/step
Epoch 721/1000
2023-09-27 11:07:01.434 
Epoch 721/1000 
	 loss: 17.4762, MinusLogProbMetric: 17.4762, val_loss: 17.7053, val_MinusLogProbMetric: 17.7053

Epoch 721: val_loss did not improve from 17.54909
196/196 - 78s - loss: 17.4762 - MinusLogProbMetric: 17.4762 - val_loss: 17.7053 - val_MinusLogProbMetric: 17.7053 - lr: 5.5556e-05 - 78s/epoch - 398ms/step
Epoch 722/1000
2023-09-27 11:08:19.488 
Epoch 722/1000 
	 loss: 17.4521, MinusLogProbMetric: 17.4521, val_loss: 17.6582, val_MinusLogProbMetric: 17.6582

Epoch 722: val_loss did not improve from 17.54909
196/196 - 78s - loss: 17.4521 - MinusLogProbMetric: 17.4521 - val_loss: 17.6582 - val_MinusLogProbMetric: 17.6582 - lr: 5.5556e-05 - 78s/epoch - 398ms/step
Epoch 723/1000
2023-09-27 11:09:36.849 
Epoch 723/1000 
	 loss: 17.4504, MinusLogProbMetric: 17.4504, val_loss: 17.6427, val_MinusLogProbMetric: 17.6427

Epoch 723: val_loss did not improve from 17.54909
196/196 - 77s - loss: 17.4504 - MinusLogProbMetric: 17.4504 - val_loss: 17.6427 - val_MinusLogProbMetric: 17.6427 - lr: 5.5556e-05 - 77s/epoch - 395ms/step
Epoch 724/1000
2023-09-27 11:10:54.198 
Epoch 724/1000 
	 loss: 17.4409, MinusLogProbMetric: 17.4409, val_loss: 17.6085, val_MinusLogProbMetric: 17.6085

Epoch 724: val_loss did not improve from 17.54909
196/196 - 77s - loss: 17.4409 - MinusLogProbMetric: 17.4409 - val_loss: 17.6085 - val_MinusLogProbMetric: 17.6085 - lr: 5.5556e-05 - 77s/epoch - 395ms/step
Epoch 725/1000
2023-09-27 11:12:06.708 
Epoch 725/1000 
	 loss: 18.1025, MinusLogProbMetric: 18.1025, val_loss: 17.5982, val_MinusLogProbMetric: 17.5982

Epoch 725: val_loss did not improve from 17.54909
196/196 - 73s - loss: 18.1025 - MinusLogProbMetric: 18.1025 - val_loss: 17.5982 - val_MinusLogProbMetric: 17.5982 - lr: 5.5556e-05 - 73s/epoch - 370ms/step
Epoch 726/1000
2023-09-27 11:13:24.492 
Epoch 726/1000 
	 loss: 17.4290, MinusLogProbMetric: 17.4290, val_loss: 17.6241, val_MinusLogProbMetric: 17.6241

Epoch 726: val_loss did not improve from 17.54909
196/196 - 78s - loss: 17.4290 - MinusLogProbMetric: 17.4290 - val_loss: 17.6241 - val_MinusLogProbMetric: 17.6241 - lr: 5.5556e-05 - 78s/epoch - 397ms/step
Epoch 727/1000
2023-09-27 11:14:42.757 
Epoch 727/1000 
	 loss: 17.4413, MinusLogProbMetric: 17.4413, val_loss: 17.6116, val_MinusLogProbMetric: 17.6116

Epoch 727: val_loss did not improve from 17.54909
196/196 - 78s - loss: 17.4413 - MinusLogProbMetric: 17.4413 - val_loss: 17.6116 - val_MinusLogProbMetric: 17.6116 - lr: 5.5556e-05 - 78s/epoch - 399ms/step
Epoch 728/1000
2023-09-27 11:16:00.793 
Epoch 728/1000 
	 loss: 17.4403, MinusLogProbMetric: 17.4403, val_loss: 17.6977, val_MinusLogProbMetric: 17.6977

Epoch 728: val_loss did not improve from 17.54909
196/196 - 78s - loss: 17.4403 - MinusLogProbMetric: 17.4403 - val_loss: 17.6977 - val_MinusLogProbMetric: 17.6977 - lr: 5.5556e-05 - 78s/epoch - 398ms/step
Epoch 729/1000
2023-09-27 11:17:18.451 
Epoch 729/1000 
	 loss: 17.4271, MinusLogProbMetric: 17.4271, val_loss: 17.6931, val_MinusLogProbMetric: 17.6931

Epoch 729: val_loss did not improve from 17.54909
196/196 - 78s - loss: 17.4271 - MinusLogProbMetric: 17.4271 - val_loss: 17.6931 - val_MinusLogProbMetric: 17.6931 - lr: 5.5556e-05 - 78s/epoch - 396ms/step
Epoch 730/1000
2023-09-27 11:18:36.925 
Epoch 730/1000 
	 loss: 17.4548, MinusLogProbMetric: 17.4548, val_loss: 17.6514, val_MinusLogProbMetric: 17.6514

Epoch 730: val_loss did not improve from 17.54909
196/196 - 78s - loss: 17.4548 - MinusLogProbMetric: 17.4548 - val_loss: 17.6514 - val_MinusLogProbMetric: 17.6514 - lr: 5.5556e-05 - 78s/epoch - 400ms/step
Epoch 731/1000
2023-09-27 11:19:54.642 
Epoch 731/1000 
	 loss: 17.6929, MinusLogProbMetric: 17.6929, val_loss: 17.6414, val_MinusLogProbMetric: 17.6414

Epoch 731: val_loss did not improve from 17.54909
196/196 - 78s - loss: 17.6929 - MinusLogProbMetric: 17.6929 - val_loss: 17.6414 - val_MinusLogProbMetric: 17.6414 - lr: 5.5556e-05 - 78s/epoch - 396ms/step
Epoch 732/1000
2023-09-27 11:21:12.691 
Epoch 732/1000 
	 loss: 17.4583, MinusLogProbMetric: 17.4583, val_loss: 17.6123, val_MinusLogProbMetric: 17.6123

Epoch 732: val_loss did not improve from 17.54909
196/196 - 78s - loss: 17.4583 - MinusLogProbMetric: 17.4583 - val_loss: 17.6123 - val_MinusLogProbMetric: 17.6123 - lr: 5.5556e-05 - 78s/epoch - 398ms/step
Epoch 733/1000
2023-09-27 11:22:30.785 
Epoch 733/1000 
	 loss: 17.4272, MinusLogProbMetric: 17.4272, val_loss: 17.6635, val_MinusLogProbMetric: 17.6635

Epoch 733: val_loss did not improve from 17.54909
196/196 - 78s - loss: 17.4272 - MinusLogProbMetric: 17.4272 - val_loss: 17.6635 - val_MinusLogProbMetric: 17.6635 - lr: 5.5556e-05 - 78s/epoch - 398ms/step
Epoch 734/1000
2023-09-27 11:23:48.140 
Epoch 734/1000 
	 loss: 17.6319, MinusLogProbMetric: 17.6319, val_loss: 17.6000, val_MinusLogProbMetric: 17.6000

Epoch 734: val_loss did not improve from 17.54909
196/196 - 77s - loss: 17.6319 - MinusLogProbMetric: 17.6319 - val_loss: 17.6000 - val_MinusLogProbMetric: 17.6000 - lr: 5.5556e-05 - 77s/epoch - 395ms/step
Epoch 735/1000
2023-09-27 11:25:06.578 
Epoch 735/1000 
	 loss: 17.4405, MinusLogProbMetric: 17.4405, val_loss: 17.6303, val_MinusLogProbMetric: 17.6303

Epoch 735: val_loss did not improve from 17.54909
196/196 - 78s - loss: 17.4405 - MinusLogProbMetric: 17.4405 - val_loss: 17.6303 - val_MinusLogProbMetric: 17.6303 - lr: 5.5556e-05 - 78s/epoch - 400ms/step
Epoch 736/1000
2023-09-27 11:26:25.238 
Epoch 736/1000 
	 loss: 17.4246, MinusLogProbMetric: 17.4246, val_loss: 17.7576, val_MinusLogProbMetric: 17.7576

Epoch 736: val_loss did not improve from 17.54909
196/196 - 79s - loss: 17.4246 - MinusLogProbMetric: 17.4246 - val_loss: 17.7576 - val_MinusLogProbMetric: 17.7576 - lr: 5.5556e-05 - 79s/epoch - 401ms/step
Epoch 737/1000
2023-09-27 11:27:44.091 
Epoch 737/1000 
	 loss: 17.4250, MinusLogProbMetric: 17.4250, val_loss: 17.6322, val_MinusLogProbMetric: 17.6322

Epoch 737: val_loss did not improve from 17.54909
196/196 - 79s - loss: 17.4250 - MinusLogProbMetric: 17.4250 - val_loss: 17.6322 - val_MinusLogProbMetric: 17.6322 - lr: 5.5556e-05 - 79s/epoch - 402ms/step
Epoch 738/1000
2023-09-27 11:29:02.148 
Epoch 738/1000 
	 loss: 17.6224, MinusLogProbMetric: 17.6224, val_loss: 17.6227, val_MinusLogProbMetric: 17.6227

Epoch 738: val_loss did not improve from 17.54909
196/196 - 78s - loss: 17.6224 - MinusLogProbMetric: 17.6224 - val_loss: 17.6227 - val_MinusLogProbMetric: 17.6227 - lr: 5.5556e-05 - 78s/epoch - 398ms/step
Epoch 739/1000
2023-09-27 11:30:21.107 
Epoch 739/1000 
	 loss: 17.4108, MinusLogProbMetric: 17.4108, val_loss: 17.6121, val_MinusLogProbMetric: 17.6121

Epoch 739: val_loss did not improve from 17.54909
196/196 - 79s - loss: 17.4108 - MinusLogProbMetric: 17.4108 - val_loss: 17.6121 - val_MinusLogProbMetric: 17.6121 - lr: 5.5556e-05 - 79s/epoch - 403ms/step
Epoch 740/1000
2023-09-27 11:31:39.783 
Epoch 740/1000 
	 loss: 17.4521, MinusLogProbMetric: 17.4521, val_loss: 17.7525, val_MinusLogProbMetric: 17.7525

Epoch 740: val_loss did not improve from 17.54909
196/196 - 79s - loss: 17.4521 - MinusLogProbMetric: 17.4521 - val_loss: 17.7525 - val_MinusLogProbMetric: 17.7525 - lr: 5.5556e-05 - 79s/epoch - 401ms/step
Epoch 741/1000
2023-09-27 11:32:57.376 
Epoch 741/1000 
	 loss: 17.4531, MinusLogProbMetric: 17.4531, val_loss: 17.9348, val_MinusLogProbMetric: 17.9348

Epoch 741: val_loss did not improve from 17.54909
196/196 - 78s - loss: 17.4531 - MinusLogProbMetric: 17.4531 - val_loss: 17.9348 - val_MinusLogProbMetric: 17.9348 - lr: 5.5556e-05 - 78s/epoch - 396ms/step
Epoch 742/1000
2023-09-27 11:34:15.902 
Epoch 742/1000 
	 loss: 17.4430, MinusLogProbMetric: 17.4430, val_loss: 17.5913, val_MinusLogProbMetric: 17.5913

Epoch 742: val_loss did not improve from 17.54909
196/196 - 79s - loss: 17.4430 - MinusLogProbMetric: 17.4430 - val_loss: 17.5913 - val_MinusLogProbMetric: 17.5913 - lr: 5.5556e-05 - 79s/epoch - 401ms/step
Epoch 743/1000
2023-09-27 11:35:33.677 
Epoch 743/1000 
	 loss: 17.4129, MinusLogProbMetric: 17.4129, val_loss: 17.6346, val_MinusLogProbMetric: 17.6346

Epoch 743: val_loss did not improve from 17.54909
196/196 - 78s - loss: 17.4129 - MinusLogProbMetric: 17.4129 - val_loss: 17.6346 - val_MinusLogProbMetric: 17.6346 - lr: 5.5556e-05 - 78s/epoch - 397ms/step
Epoch 744/1000
2023-09-27 11:36:51.894 
Epoch 744/1000 
	 loss: 17.4521, MinusLogProbMetric: 17.4521, val_loss: 17.6731, val_MinusLogProbMetric: 17.6731

Epoch 744: val_loss did not improve from 17.54909
196/196 - 78s - loss: 17.4521 - MinusLogProbMetric: 17.4521 - val_loss: 17.6731 - val_MinusLogProbMetric: 17.6731 - lr: 5.5556e-05 - 78s/epoch - 399ms/step
Epoch 745/1000
2023-09-27 11:38:09.827 
Epoch 745/1000 
	 loss: 17.9363, MinusLogProbMetric: 17.9363, val_loss: 17.5963, val_MinusLogProbMetric: 17.5963

Epoch 745: val_loss did not improve from 17.54909
196/196 - 78s - loss: 17.9363 - MinusLogProbMetric: 17.9363 - val_loss: 17.5963 - val_MinusLogProbMetric: 17.5963 - lr: 5.5556e-05 - 78s/epoch - 398ms/step
Epoch 746/1000
2023-09-27 11:39:27.622 
Epoch 746/1000 
	 loss: 17.4287, MinusLogProbMetric: 17.4287, val_loss: 17.6984, val_MinusLogProbMetric: 17.6984

Epoch 746: val_loss did not improve from 17.54909
196/196 - 78s - loss: 17.4287 - MinusLogProbMetric: 17.4287 - val_loss: 17.6984 - val_MinusLogProbMetric: 17.6984 - lr: 5.5556e-05 - 78s/epoch - 397ms/step
Epoch 747/1000
2023-09-27 11:40:45.546 
Epoch 747/1000 
	 loss: 17.4437, MinusLogProbMetric: 17.4437, val_loss: 17.7858, val_MinusLogProbMetric: 17.7858

Epoch 747: val_loss did not improve from 17.54909
196/196 - 78s - loss: 17.4437 - MinusLogProbMetric: 17.4437 - val_loss: 17.7858 - val_MinusLogProbMetric: 17.7858 - lr: 5.5556e-05 - 78s/epoch - 398ms/step
Epoch 748/1000
2023-09-27 11:42:03.492 
Epoch 748/1000 
	 loss: 17.4475, MinusLogProbMetric: 17.4475, val_loss: 17.6399, val_MinusLogProbMetric: 17.6399

Epoch 748: val_loss did not improve from 17.54909
196/196 - 78s - loss: 17.4475 - MinusLogProbMetric: 17.4475 - val_loss: 17.6399 - val_MinusLogProbMetric: 17.6399 - lr: 5.5556e-05 - 78s/epoch - 398ms/step
Epoch 749/1000
2023-09-27 11:43:21.424 
Epoch 749/1000 
	 loss: 17.4294, MinusLogProbMetric: 17.4294, val_loss: 17.6732, val_MinusLogProbMetric: 17.6732

Epoch 749: val_loss did not improve from 17.54909
196/196 - 78s - loss: 17.4294 - MinusLogProbMetric: 17.4294 - val_loss: 17.6732 - val_MinusLogProbMetric: 17.6732 - lr: 5.5556e-05 - 78s/epoch - 398ms/step
Epoch 750/1000
2023-09-27 11:44:38.894 
Epoch 750/1000 
	 loss: 17.4123, MinusLogProbMetric: 17.4123, val_loss: 17.6656, val_MinusLogProbMetric: 17.6656

Epoch 750: val_loss did not improve from 17.54909
196/196 - 77s - loss: 17.4123 - MinusLogProbMetric: 17.4123 - val_loss: 17.6656 - val_MinusLogProbMetric: 17.6656 - lr: 5.5556e-05 - 77s/epoch - 395ms/step
Epoch 751/1000
2023-09-27 11:45:57.304 
Epoch 751/1000 
	 loss: 17.4364, MinusLogProbMetric: 17.4364, val_loss: 17.6831, val_MinusLogProbMetric: 17.6831

Epoch 751: val_loss did not improve from 17.54909
196/196 - 78s - loss: 17.4364 - MinusLogProbMetric: 17.4364 - val_loss: 17.6831 - val_MinusLogProbMetric: 17.6831 - lr: 5.5556e-05 - 78s/epoch - 400ms/step
Epoch 752/1000
2023-09-27 11:47:15.599 
Epoch 752/1000 
	 loss: 17.4236, MinusLogProbMetric: 17.4236, val_loss: 17.5784, val_MinusLogProbMetric: 17.5784

Epoch 752: val_loss did not improve from 17.54909
196/196 - 78s - loss: 17.4236 - MinusLogProbMetric: 17.4236 - val_loss: 17.5784 - val_MinusLogProbMetric: 17.5784 - lr: 5.5556e-05 - 78s/epoch - 399ms/step
Epoch 753/1000
2023-09-27 11:48:33.667 
Epoch 753/1000 
	 loss: 17.4097, MinusLogProbMetric: 17.4097, val_loss: 17.6124, val_MinusLogProbMetric: 17.6124

Epoch 753: val_loss did not improve from 17.54909
196/196 - 78s - loss: 17.4097 - MinusLogProbMetric: 17.4097 - val_loss: 17.6124 - val_MinusLogProbMetric: 17.6124 - lr: 5.5556e-05 - 78s/epoch - 398ms/step
Epoch 754/1000
2023-09-27 11:49:52.263 
Epoch 754/1000 
	 loss: 17.5467, MinusLogProbMetric: 17.5467, val_loss: 17.6104, val_MinusLogProbMetric: 17.6104

Epoch 754: val_loss did not improve from 17.54909
196/196 - 79s - loss: 17.5467 - MinusLogProbMetric: 17.5467 - val_loss: 17.6104 - val_MinusLogProbMetric: 17.6104 - lr: 5.5556e-05 - 79s/epoch - 401ms/step
Epoch 755/1000
2023-09-27 11:51:10.595 
Epoch 755/1000 
	 loss: 17.4228, MinusLogProbMetric: 17.4228, val_loss: 17.5991, val_MinusLogProbMetric: 17.5991

Epoch 755: val_loss did not improve from 17.54909
196/196 - 78s - loss: 17.4228 - MinusLogProbMetric: 17.4228 - val_loss: 17.5991 - val_MinusLogProbMetric: 17.5991 - lr: 5.5556e-05 - 78s/epoch - 400ms/step
Epoch 756/1000
2023-09-27 11:52:29.056 
Epoch 756/1000 
	 loss: 17.4218, MinusLogProbMetric: 17.4218, val_loss: 17.6395, val_MinusLogProbMetric: 17.6395

Epoch 756: val_loss did not improve from 17.54909
196/196 - 78s - loss: 17.4218 - MinusLogProbMetric: 17.4218 - val_loss: 17.6395 - val_MinusLogProbMetric: 17.6395 - lr: 5.5556e-05 - 78s/epoch - 400ms/step
Epoch 757/1000
2023-09-27 11:53:47.207 
Epoch 757/1000 
	 loss: 17.4712, MinusLogProbMetric: 17.4712, val_loss: 17.6114, val_MinusLogProbMetric: 17.6114

Epoch 757: val_loss did not improve from 17.54909
196/196 - 78s - loss: 17.4712 - MinusLogProbMetric: 17.4712 - val_loss: 17.6114 - val_MinusLogProbMetric: 17.6114 - lr: 5.5556e-05 - 78s/epoch - 399ms/step
Epoch 758/1000
2023-09-27 11:55:04.980 
Epoch 758/1000 
	 loss: 17.4213, MinusLogProbMetric: 17.4213, val_loss: 17.5948, val_MinusLogProbMetric: 17.5948

Epoch 758: val_loss did not improve from 17.54909
196/196 - 78s - loss: 17.4213 - MinusLogProbMetric: 17.4213 - val_loss: 17.5948 - val_MinusLogProbMetric: 17.5948 - lr: 5.5556e-05 - 78s/epoch - 397ms/step
Epoch 759/1000
2023-09-27 11:56:22.865 
Epoch 759/1000 
	 loss: 17.3922, MinusLogProbMetric: 17.3922, val_loss: 17.6083, val_MinusLogProbMetric: 17.6083

Epoch 759: val_loss did not improve from 17.54909
196/196 - 78s - loss: 17.3922 - MinusLogProbMetric: 17.3922 - val_loss: 17.6083 - val_MinusLogProbMetric: 17.6083 - lr: 5.5556e-05 - 78s/epoch - 397ms/step
Epoch 760/1000
2023-09-27 11:57:41.113 
Epoch 760/1000 
	 loss: 17.4882, MinusLogProbMetric: 17.4882, val_loss: 17.6255, val_MinusLogProbMetric: 17.6255

Epoch 760: val_loss did not improve from 17.54909
196/196 - 78s - loss: 17.4882 - MinusLogProbMetric: 17.4882 - val_loss: 17.6255 - val_MinusLogProbMetric: 17.6255 - lr: 5.5556e-05 - 78s/epoch - 399ms/step
Epoch 761/1000
2023-09-27 11:58:59.636 
Epoch 761/1000 
	 loss: 17.4236, MinusLogProbMetric: 17.4236, val_loss: 17.6542, val_MinusLogProbMetric: 17.6542

Epoch 761: val_loss did not improve from 17.54909
196/196 - 79s - loss: 17.4236 - MinusLogProbMetric: 17.4236 - val_loss: 17.6542 - val_MinusLogProbMetric: 17.6542 - lr: 5.5556e-05 - 79s/epoch - 401ms/step
Epoch 762/1000
2023-09-27 12:00:17.922 
Epoch 762/1000 
	 loss: 17.4524, MinusLogProbMetric: 17.4524, val_loss: 17.6621, val_MinusLogProbMetric: 17.6621

Epoch 762: val_loss did not improve from 17.54909
196/196 - 78s - loss: 17.4524 - MinusLogProbMetric: 17.4524 - val_loss: 17.6621 - val_MinusLogProbMetric: 17.6621 - lr: 5.5556e-05 - 78s/epoch - 399ms/step
Epoch 763/1000
2023-09-27 12:01:35.695 
Epoch 763/1000 
	 loss: 17.4164, MinusLogProbMetric: 17.4164, val_loss: 17.6093, val_MinusLogProbMetric: 17.6093

Epoch 763: val_loss did not improve from 17.54909
196/196 - 78s - loss: 17.4164 - MinusLogProbMetric: 17.4164 - val_loss: 17.6093 - val_MinusLogProbMetric: 17.6093 - lr: 5.5556e-05 - 78s/epoch - 397ms/step
Epoch 764/1000
2023-09-27 12:02:54.058 
Epoch 764/1000 
	 loss: 17.4225, MinusLogProbMetric: 17.4225, val_loss: 17.6357, val_MinusLogProbMetric: 17.6357

Epoch 764: val_loss did not improve from 17.54909
196/196 - 78s - loss: 17.4225 - MinusLogProbMetric: 17.4225 - val_loss: 17.6357 - val_MinusLogProbMetric: 17.6357 - lr: 5.5556e-05 - 78s/epoch - 400ms/step
Epoch 765/1000
2023-09-27 12:04:11.837 
Epoch 765/1000 
	 loss: 17.7178, MinusLogProbMetric: 17.7178, val_loss: 20.1666, val_MinusLogProbMetric: 20.1666

Epoch 765: val_loss did not improve from 17.54909
196/196 - 78s - loss: 17.7178 - MinusLogProbMetric: 17.7178 - val_loss: 20.1666 - val_MinusLogProbMetric: 20.1666 - lr: 5.5556e-05 - 78s/epoch - 397ms/step
Epoch 766/1000
2023-09-27 12:05:29.848 
Epoch 766/1000 
	 loss: 17.5786, MinusLogProbMetric: 17.5786, val_loss: 17.5928, val_MinusLogProbMetric: 17.5928

Epoch 766: val_loss did not improve from 17.54909
196/196 - 78s - loss: 17.5786 - MinusLogProbMetric: 17.5786 - val_loss: 17.5928 - val_MinusLogProbMetric: 17.5928 - lr: 5.5556e-05 - 78s/epoch - 398ms/step
Epoch 767/1000
2023-09-27 12:06:47.994 
Epoch 767/1000 
	 loss: 17.3301, MinusLogProbMetric: 17.3301, val_loss: 17.5187, val_MinusLogProbMetric: 17.5187

Epoch 767: val_loss improved from 17.54909 to 17.51872, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 79s - loss: 17.3301 - MinusLogProbMetric: 17.3301 - val_loss: 17.5187 - val_MinusLogProbMetric: 17.5187 - lr: 2.7778e-05 - 79s/epoch - 405ms/step
Epoch 768/1000
2023-09-27 12:08:07.720 
Epoch 768/1000 
	 loss: 17.3323, MinusLogProbMetric: 17.3323, val_loss: 17.5363, val_MinusLogProbMetric: 17.5363

Epoch 768: val_loss did not improve from 17.51872
196/196 - 78s - loss: 17.3323 - MinusLogProbMetric: 17.3323 - val_loss: 17.5363 - val_MinusLogProbMetric: 17.5363 - lr: 2.7778e-05 - 78s/epoch - 400ms/step
Epoch 769/1000
2023-09-27 12:09:25.656 
Epoch 769/1000 
	 loss: 17.3407, MinusLogProbMetric: 17.3407, val_loss: 17.5445, val_MinusLogProbMetric: 17.5445

Epoch 769: val_loss did not improve from 17.51872
196/196 - 78s - loss: 17.3407 - MinusLogProbMetric: 17.3407 - val_loss: 17.5445 - val_MinusLogProbMetric: 17.5445 - lr: 2.7778e-05 - 78s/epoch - 398ms/step
Epoch 770/1000
2023-09-27 12:10:43.618 
Epoch 770/1000 
	 loss: 17.3322, MinusLogProbMetric: 17.3322, val_loss: 17.5823, val_MinusLogProbMetric: 17.5823

Epoch 770: val_loss did not improve from 17.51872
196/196 - 78s - loss: 17.3322 - MinusLogProbMetric: 17.3322 - val_loss: 17.5823 - val_MinusLogProbMetric: 17.5823 - lr: 2.7778e-05 - 78s/epoch - 398ms/step
Epoch 771/1000
2023-09-27 12:12:01.700 
Epoch 771/1000 
	 loss: 17.4034, MinusLogProbMetric: 17.4034, val_loss: 17.5310, val_MinusLogProbMetric: 17.5310

Epoch 771: val_loss did not improve from 17.51872
196/196 - 78s - loss: 17.4034 - MinusLogProbMetric: 17.4034 - val_loss: 17.5310 - val_MinusLogProbMetric: 17.5310 - lr: 2.7778e-05 - 78s/epoch - 398ms/step
Epoch 772/1000
2023-09-27 12:13:19.387 
Epoch 772/1000 
	 loss: 17.3372, MinusLogProbMetric: 17.3372, val_loss: 17.5247, val_MinusLogProbMetric: 17.5247

Epoch 772: val_loss did not improve from 17.51872
196/196 - 78s - loss: 17.3372 - MinusLogProbMetric: 17.3372 - val_loss: 17.5247 - val_MinusLogProbMetric: 17.5247 - lr: 2.7778e-05 - 78s/epoch - 396ms/step
Epoch 773/1000
2023-09-27 12:14:37.320 
Epoch 773/1000 
	 loss: 17.3398, MinusLogProbMetric: 17.3398, val_loss: 17.5168, val_MinusLogProbMetric: 17.5168

Epoch 773: val_loss improved from 17.51872 to 17.51682, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 80s - loss: 17.3398 - MinusLogProbMetric: 17.3398 - val_loss: 17.5168 - val_MinusLogProbMetric: 17.5168 - lr: 2.7778e-05 - 80s/epoch - 407ms/step
Epoch 774/1000
2023-09-27 12:15:57.425 
Epoch 774/1000 
	 loss: 17.3359, MinusLogProbMetric: 17.3359, val_loss: 17.5266, val_MinusLogProbMetric: 17.5266

Epoch 774: val_loss did not improve from 17.51682
196/196 - 78s - loss: 17.3359 - MinusLogProbMetric: 17.3359 - val_loss: 17.5266 - val_MinusLogProbMetric: 17.5266 - lr: 2.7778e-05 - 78s/epoch - 400ms/step
Epoch 775/1000
2023-09-27 12:17:15.145 
Epoch 775/1000 
	 loss: 17.3334, MinusLogProbMetric: 17.3334, val_loss: 17.5456, val_MinusLogProbMetric: 17.5456

Epoch 775: val_loss did not improve from 17.51682
196/196 - 78s - loss: 17.3334 - MinusLogProbMetric: 17.3334 - val_loss: 17.5456 - val_MinusLogProbMetric: 17.5456 - lr: 2.7778e-05 - 78s/epoch - 397ms/step
Epoch 776/1000
2023-09-27 12:18:32.289 
Epoch 776/1000 
	 loss: 17.3318, MinusLogProbMetric: 17.3318, val_loss: 17.5668, val_MinusLogProbMetric: 17.5668

Epoch 776: val_loss did not improve from 17.51682
196/196 - 77s - loss: 17.3318 - MinusLogProbMetric: 17.3318 - val_loss: 17.5668 - val_MinusLogProbMetric: 17.5668 - lr: 2.7778e-05 - 77s/epoch - 394ms/step
Epoch 777/1000
2023-09-27 12:19:50.558 
Epoch 777/1000 
	 loss: 17.3339, MinusLogProbMetric: 17.3339, val_loss: 17.5411, val_MinusLogProbMetric: 17.5411

Epoch 777: val_loss did not improve from 17.51682
196/196 - 78s - loss: 17.3339 - MinusLogProbMetric: 17.3339 - val_loss: 17.5411 - val_MinusLogProbMetric: 17.5411 - lr: 2.7778e-05 - 78s/epoch - 399ms/step
Epoch 778/1000
2023-09-27 12:21:08.157 
Epoch 778/1000 
	 loss: 17.3654, MinusLogProbMetric: 17.3654, val_loss: 17.6415, val_MinusLogProbMetric: 17.6415

Epoch 778: val_loss did not improve from 17.51682
196/196 - 78s - loss: 17.3654 - MinusLogProbMetric: 17.3654 - val_loss: 17.6415 - val_MinusLogProbMetric: 17.6415 - lr: 2.7778e-05 - 78s/epoch - 396ms/step
Epoch 779/1000
2023-09-27 12:22:26.763 
Epoch 779/1000 
	 loss: 17.3609, MinusLogProbMetric: 17.3609, val_loss: 17.5600, val_MinusLogProbMetric: 17.5600

Epoch 779: val_loss did not improve from 17.51682
196/196 - 79s - loss: 17.3609 - MinusLogProbMetric: 17.3609 - val_loss: 17.5600 - val_MinusLogProbMetric: 17.5600 - lr: 2.7778e-05 - 79s/epoch - 401ms/step
Epoch 780/1000
2023-09-27 12:23:44.512 
Epoch 780/1000 
	 loss: 17.3306, MinusLogProbMetric: 17.3306, val_loss: 17.7174, val_MinusLogProbMetric: 17.7174

Epoch 780: val_loss did not improve from 17.51682
196/196 - 78s - loss: 17.3306 - MinusLogProbMetric: 17.3306 - val_loss: 17.7174 - val_MinusLogProbMetric: 17.7174 - lr: 2.7778e-05 - 78s/epoch - 397ms/step
Epoch 781/1000
2023-09-27 12:25:02.591 
Epoch 781/1000 
	 loss: 17.3643, MinusLogProbMetric: 17.3643, val_loss: 17.5233, val_MinusLogProbMetric: 17.5233

Epoch 781: val_loss did not improve from 17.51682
196/196 - 78s - loss: 17.3643 - MinusLogProbMetric: 17.3643 - val_loss: 17.5233 - val_MinusLogProbMetric: 17.5233 - lr: 2.7778e-05 - 78s/epoch - 398ms/step
Epoch 782/1000
2023-09-27 12:26:20.666 
Epoch 782/1000 
	 loss: 17.3367, MinusLogProbMetric: 17.3367, val_loss: 17.5352, val_MinusLogProbMetric: 17.5352

Epoch 782: val_loss did not improve from 17.51682
196/196 - 78s - loss: 17.3367 - MinusLogProbMetric: 17.3367 - val_loss: 17.5352 - val_MinusLogProbMetric: 17.5352 - lr: 2.7778e-05 - 78s/epoch - 398ms/step
Epoch 783/1000
2023-09-27 12:27:38.694 
Epoch 783/1000 
	 loss: 17.3489, MinusLogProbMetric: 17.3489, val_loss: 17.5588, val_MinusLogProbMetric: 17.5588

Epoch 783: val_loss did not improve from 17.51682
196/196 - 78s - loss: 17.3489 - MinusLogProbMetric: 17.3489 - val_loss: 17.5588 - val_MinusLogProbMetric: 17.5588 - lr: 2.7778e-05 - 78s/epoch - 398ms/step
Epoch 784/1000
2023-09-27 12:28:56.757 
Epoch 784/1000 
	 loss: 17.3359, MinusLogProbMetric: 17.3359, val_loss: 17.5314, val_MinusLogProbMetric: 17.5314

Epoch 784: val_loss did not improve from 17.51682
196/196 - 78s - loss: 17.3359 - MinusLogProbMetric: 17.3359 - val_loss: 17.5314 - val_MinusLogProbMetric: 17.5314 - lr: 2.7778e-05 - 78s/epoch - 398ms/step
Epoch 785/1000
2023-09-27 12:30:14.489 
Epoch 785/1000 
	 loss: 17.3268, MinusLogProbMetric: 17.3268, val_loss: 17.5410, val_MinusLogProbMetric: 17.5410

Epoch 785: val_loss did not improve from 17.51682
196/196 - 78s - loss: 17.3268 - MinusLogProbMetric: 17.3268 - val_loss: 17.5410 - val_MinusLogProbMetric: 17.5410 - lr: 2.7778e-05 - 78s/epoch - 397ms/step
Epoch 786/1000
2023-09-27 12:31:32.744 
Epoch 786/1000 
	 loss: 17.3299, MinusLogProbMetric: 17.3299, val_loss: 17.5274, val_MinusLogProbMetric: 17.5274

Epoch 786: val_loss did not improve from 17.51682
196/196 - 78s - loss: 17.3299 - MinusLogProbMetric: 17.3299 - val_loss: 17.5274 - val_MinusLogProbMetric: 17.5274 - lr: 2.7778e-05 - 78s/epoch - 399ms/step
Epoch 787/1000
2023-09-27 12:32:50.867 
Epoch 787/1000 
	 loss: 17.3462, MinusLogProbMetric: 17.3462, val_loss: 17.5444, val_MinusLogProbMetric: 17.5444

Epoch 787: val_loss did not improve from 17.51682
196/196 - 78s - loss: 17.3462 - MinusLogProbMetric: 17.3462 - val_loss: 17.5444 - val_MinusLogProbMetric: 17.5444 - lr: 2.7778e-05 - 78s/epoch - 399ms/step
Epoch 788/1000
2023-09-27 12:34:09.401 
Epoch 788/1000 
	 loss: 17.3419, MinusLogProbMetric: 17.3419, val_loss: 17.5455, val_MinusLogProbMetric: 17.5455

Epoch 788: val_loss did not improve from 17.51682
196/196 - 79s - loss: 17.3419 - MinusLogProbMetric: 17.3419 - val_loss: 17.5455 - val_MinusLogProbMetric: 17.5455 - lr: 2.7778e-05 - 79s/epoch - 401ms/step
Epoch 789/1000
2023-09-27 12:35:27.615 
Epoch 789/1000 
	 loss: 17.3321, MinusLogProbMetric: 17.3321, val_loss: 17.5418, val_MinusLogProbMetric: 17.5418

Epoch 789: val_loss did not improve from 17.51682
196/196 - 78s - loss: 17.3321 - MinusLogProbMetric: 17.3321 - val_loss: 17.5418 - val_MinusLogProbMetric: 17.5418 - lr: 2.7778e-05 - 78s/epoch - 399ms/step
Epoch 790/1000
2023-09-27 12:36:45.741 
Epoch 790/1000 
	 loss: 17.3349, MinusLogProbMetric: 17.3349, val_loss: 17.5602, val_MinusLogProbMetric: 17.5602

Epoch 790: val_loss did not improve from 17.51682
196/196 - 78s - loss: 17.3349 - MinusLogProbMetric: 17.3349 - val_loss: 17.5602 - val_MinusLogProbMetric: 17.5602 - lr: 2.7778e-05 - 78s/epoch - 399ms/step
Epoch 791/1000
2023-09-27 12:38:03.763 
Epoch 791/1000 
	 loss: 17.3418, MinusLogProbMetric: 17.3418, val_loss: 17.5948, val_MinusLogProbMetric: 17.5948

Epoch 791: val_loss did not improve from 17.51682
196/196 - 78s - loss: 17.3418 - MinusLogProbMetric: 17.3418 - val_loss: 17.5948 - val_MinusLogProbMetric: 17.5948 - lr: 2.7778e-05 - 78s/epoch - 398ms/step
Epoch 792/1000
2023-09-27 12:39:21.858 
Epoch 792/1000 
	 loss: 17.3360, MinusLogProbMetric: 17.3360, val_loss: 17.5373, val_MinusLogProbMetric: 17.5373

Epoch 792: val_loss did not improve from 17.51682
196/196 - 78s - loss: 17.3360 - MinusLogProbMetric: 17.3360 - val_loss: 17.5373 - val_MinusLogProbMetric: 17.5373 - lr: 2.7778e-05 - 78s/epoch - 398ms/step
Epoch 793/1000
2023-09-27 12:40:39.345 
Epoch 793/1000 
	 loss: 17.3312, MinusLogProbMetric: 17.3312, val_loss: 17.5302, val_MinusLogProbMetric: 17.5302

Epoch 793: val_loss did not improve from 17.51682
196/196 - 77s - loss: 17.3312 - MinusLogProbMetric: 17.3312 - val_loss: 17.5302 - val_MinusLogProbMetric: 17.5302 - lr: 2.7778e-05 - 77s/epoch - 395ms/step
Epoch 794/1000
2023-09-27 12:41:57.444 
Epoch 794/1000 
	 loss: 17.3291, MinusLogProbMetric: 17.3291, val_loss: 17.5581, val_MinusLogProbMetric: 17.5581

Epoch 794: val_loss did not improve from 17.51682
196/196 - 78s - loss: 17.3291 - MinusLogProbMetric: 17.3291 - val_loss: 17.5581 - val_MinusLogProbMetric: 17.5581 - lr: 2.7778e-05 - 78s/epoch - 398ms/step
Epoch 795/1000
2023-09-27 12:43:15.718 
Epoch 795/1000 
	 loss: 17.3780, MinusLogProbMetric: 17.3780, val_loss: 17.5783, val_MinusLogProbMetric: 17.5783

Epoch 795: val_loss did not improve from 17.51682
196/196 - 78s - loss: 17.3780 - MinusLogProbMetric: 17.3780 - val_loss: 17.5783 - val_MinusLogProbMetric: 17.5783 - lr: 2.7778e-05 - 78s/epoch - 399ms/step
Epoch 796/1000
2023-09-27 12:44:34.397 
Epoch 796/1000 
	 loss: 17.3287, MinusLogProbMetric: 17.3287, val_loss: 17.5053, val_MinusLogProbMetric: 17.5053

Epoch 796: val_loss improved from 17.51682 to 17.50531, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 80s - loss: 17.3287 - MinusLogProbMetric: 17.3287 - val_loss: 17.5053 - val_MinusLogProbMetric: 17.5053 - lr: 2.7778e-05 - 80s/epoch - 410ms/step
Epoch 797/1000
2023-09-27 12:45:53.592 
Epoch 797/1000 
	 loss: 17.3306, MinusLogProbMetric: 17.3306, val_loss: 17.5443, val_MinusLogProbMetric: 17.5443

Epoch 797: val_loss did not improve from 17.50531
196/196 - 77s - loss: 17.3306 - MinusLogProbMetric: 17.3306 - val_loss: 17.5443 - val_MinusLogProbMetric: 17.5443 - lr: 2.7778e-05 - 77s/epoch - 395ms/step
Epoch 798/1000
2023-09-27 12:47:11.456 
Epoch 798/1000 
	 loss: 17.3462, MinusLogProbMetric: 17.3462, val_loss: 17.5517, val_MinusLogProbMetric: 17.5517

Epoch 798: val_loss did not improve from 17.50531
196/196 - 78s - loss: 17.3462 - MinusLogProbMetric: 17.3462 - val_loss: 17.5517 - val_MinusLogProbMetric: 17.5517 - lr: 2.7778e-05 - 78s/epoch - 397ms/step
Epoch 799/1000
2023-09-27 12:48:29.484 
Epoch 799/1000 
	 loss: 17.3330, MinusLogProbMetric: 17.3330, val_loss: 17.5619, val_MinusLogProbMetric: 17.5619

Epoch 799: val_loss did not improve from 17.50531
196/196 - 78s - loss: 17.3330 - MinusLogProbMetric: 17.3330 - val_loss: 17.5619 - val_MinusLogProbMetric: 17.5619 - lr: 2.7778e-05 - 78s/epoch - 398ms/step
Epoch 800/1000
2023-09-27 12:49:47.063 
Epoch 800/1000 
	 loss: 17.3732, MinusLogProbMetric: 17.3732, val_loss: 17.5486, val_MinusLogProbMetric: 17.5486

Epoch 800: val_loss did not improve from 17.50531
196/196 - 78s - loss: 17.3732 - MinusLogProbMetric: 17.3732 - val_loss: 17.5486 - val_MinusLogProbMetric: 17.5486 - lr: 2.7778e-05 - 78s/epoch - 396ms/step
Epoch 801/1000
2023-09-27 12:51:02.090 
Epoch 801/1000 
	 loss: 17.3927, MinusLogProbMetric: 17.3927, val_loss: 17.6900, val_MinusLogProbMetric: 17.6900

Epoch 801: val_loss did not improve from 17.50531
196/196 - 75s - loss: 17.3927 - MinusLogProbMetric: 17.3927 - val_loss: 17.6900 - val_MinusLogProbMetric: 17.6900 - lr: 2.7778e-05 - 75s/epoch - 383ms/step
Epoch 802/1000
2023-09-27 12:52:16.576 
Epoch 802/1000 
	 loss: 17.3331, MinusLogProbMetric: 17.3331, val_loss: 17.5823, val_MinusLogProbMetric: 17.5823

Epoch 802: val_loss did not improve from 17.50531
196/196 - 74s - loss: 17.3331 - MinusLogProbMetric: 17.3331 - val_loss: 17.5823 - val_MinusLogProbMetric: 17.5823 - lr: 2.7778e-05 - 74s/epoch - 380ms/step
Epoch 803/1000
2023-09-27 12:53:34.610 
Epoch 803/1000 
	 loss: 17.3214, MinusLogProbMetric: 17.3214, val_loss: 17.5531, val_MinusLogProbMetric: 17.5531

Epoch 803: val_loss did not improve from 17.50531
196/196 - 78s - loss: 17.3214 - MinusLogProbMetric: 17.3214 - val_loss: 17.5531 - val_MinusLogProbMetric: 17.5531 - lr: 2.7778e-05 - 78s/epoch - 398ms/step
Epoch 804/1000
2023-09-27 12:54:52.160 
Epoch 804/1000 
	 loss: 17.3215, MinusLogProbMetric: 17.3215, val_loss: 17.5627, val_MinusLogProbMetric: 17.5627

Epoch 804: val_loss did not improve from 17.50531
196/196 - 78s - loss: 17.3215 - MinusLogProbMetric: 17.3215 - val_loss: 17.5627 - val_MinusLogProbMetric: 17.5627 - lr: 2.7778e-05 - 78s/epoch - 396ms/step
Epoch 805/1000
2023-09-27 12:56:10.465 
Epoch 805/1000 
	 loss: 17.3262, MinusLogProbMetric: 17.3262, val_loss: 17.5680, val_MinusLogProbMetric: 17.5680

Epoch 805: val_loss did not improve from 17.50531
196/196 - 78s - loss: 17.3262 - MinusLogProbMetric: 17.3262 - val_loss: 17.5680 - val_MinusLogProbMetric: 17.5680 - lr: 2.7778e-05 - 78s/epoch - 400ms/step
Epoch 806/1000
2023-09-27 12:57:28.275 
Epoch 806/1000 
	 loss: 17.3344, MinusLogProbMetric: 17.3344, val_loss: 17.5547, val_MinusLogProbMetric: 17.5547

Epoch 806: val_loss did not improve from 17.50531
196/196 - 78s - loss: 17.3344 - MinusLogProbMetric: 17.3344 - val_loss: 17.5547 - val_MinusLogProbMetric: 17.5547 - lr: 2.7778e-05 - 78s/epoch - 397ms/step
Epoch 807/1000
2023-09-27 12:58:45.860 
Epoch 807/1000 
	 loss: 17.3341, MinusLogProbMetric: 17.3341, val_loss: 17.5093, val_MinusLogProbMetric: 17.5093

Epoch 807: val_loss did not improve from 17.50531
196/196 - 78s - loss: 17.3341 - MinusLogProbMetric: 17.3341 - val_loss: 17.5093 - val_MinusLogProbMetric: 17.5093 - lr: 2.7778e-05 - 78s/epoch - 396ms/step
Epoch 808/1000
2023-09-27 13:00:02.998 
Epoch 808/1000 
	 loss: 17.3262, MinusLogProbMetric: 17.3262, val_loss: 17.5397, val_MinusLogProbMetric: 17.5397

Epoch 808: val_loss did not improve from 17.50531
196/196 - 77s - loss: 17.3262 - MinusLogProbMetric: 17.3262 - val_loss: 17.5397 - val_MinusLogProbMetric: 17.5397 - lr: 2.7778e-05 - 77s/epoch - 394ms/step
Epoch 809/1000
2023-09-27 13:01:21.205 
Epoch 809/1000 
	 loss: 17.3237, MinusLogProbMetric: 17.3237, val_loss: 17.5216, val_MinusLogProbMetric: 17.5216

Epoch 809: val_loss did not improve from 17.50531
196/196 - 78s - loss: 17.3237 - MinusLogProbMetric: 17.3237 - val_loss: 17.5216 - val_MinusLogProbMetric: 17.5216 - lr: 2.7778e-05 - 78s/epoch - 399ms/step
Epoch 810/1000
2023-09-27 13:02:38.776 
Epoch 810/1000 
	 loss: 17.3448, MinusLogProbMetric: 17.3448, val_loss: 17.5323, val_MinusLogProbMetric: 17.5323

Epoch 810: val_loss did not improve from 17.50531
196/196 - 78s - loss: 17.3448 - MinusLogProbMetric: 17.3448 - val_loss: 17.5323 - val_MinusLogProbMetric: 17.5323 - lr: 2.7778e-05 - 78s/epoch - 396ms/step
Epoch 811/1000
2023-09-27 13:03:56.283 
Epoch 811/1000 
	 loss: 17.4470, MinusLogProbMetric: 17.4470, val_loss: 17.5873, val_MinusLogProbMetric: 17.5873

Epoch 811: val_loss did not improve from 17.50531
196/196 - 78s - loss: 17.4470 - MinusLogProbMetric: 17.4470 - val_loss: 17.5873 - val_MinusLogProbMetric: 17.5873 - lr: 2.7778e-05 - 78s/epoch - 395ms/step
Epoch 812/1000
2023-09-27 13:05:13.549 
Epoch 812/1000 
	 loss: 17.3303, MinusLogProbMetric: 17.3303, val_loss: 17.5409, val_MinusLogProbMetric: 17.5409

Epoch 812: val_loss did not improve from 17.50531
196/196 - 77s - loss: 17.3303 - MinusLogProbMetric: 17.3303 - val_loss: 17.5409 - val_MinusLogProbMetric: 17.5409 - lr: 2.7778e-05 - 77s/epoch - 394ms/step
Epoch 813/1000
2023-09-27 13:06:30.944 
Epoch 813/1000 
	 loss: 17.4484, MinusLogProbMetric: 17.4484, val_loss: 17.5458, val_MinusLogProbMetric: 17.5458

Epoch 813: val_loss did not improve from 17.50531
196/196 - 77s - loss: 17.4484 - MinusLogProbMetric: 17.4484 - val_loss: 17.5458 - val_MinusLogProbMetric: 17.5458 - lr: 2.7778e-05 - 77s/epoch - 395ms/step
Epoch 814/1000
2023-09-27 13:07:49.379 
Epoch 814/1000 
	 loss: 17.3357, MinusLogProbMetric: 17.3357, val_loss: 17.5239, val_MinusLogProbMetric: 17.5239

Epoch 814: val_loss did not improve from 17.50531
196/196 - 78s - loss: 17.3357 - MinusLogProbMetric: 17.3357 - val_loss: 17.5239 - val_MinusLogProbMetric: 17.5239 - lr: 2.7778e-05 - 78s/epoch - 400ms/step
Epoch 815/1000
2023-09-27 13:09:07.168 
Epoch 815/1000 
	 loss: 17.3278, MinusLogProbMetric: 17.3278, val_loss: 17.5278, val_MinusLogProbMetric: 17.5278

Epoch 815: val_loss did not improve from 17.50531
196/196 - 78s - loss: 17.3278 - MinusLogProbMetric: 17.3278 - val_loss: 17.5278 - val_MinusLogProbMetric: 17.5278 - lr: 2.7778e-05 - 78s/epoch - 397ms/step
Epoch 816/1000
2023-09-27 13:10:25.126 
Epoch 816/1000 
	 loss: 17.3262, MinusLogProbMetric: 17.3262, val_loss: 17.5560, val_MinusLogProbMetric: 17.5560

Epoch 816: val_loss did not improve from 17.50531
196/196 - 78s - loss: 17.3262 - MinusLogProbMetric: 17.3262 - val_loss: 17.5560 - val_MinusLogProbMetric: 17.5560 - lr: 2.7778e-05 - 78s/epoch - 398ms/step
Epoch 817/1000
2023-09-27 13:11:42.619 
Epoch 817/1000 
	 loss: 17.3570, MinusLogProbMetric: 17.3570, val_loss: 17.5205, val_MinusLogProbMetric: 17.5205

Epoch 817: val_loss did not improve from 17.50531
196/196 - 77s - loss: 17.3570 - MinusLogProbMetric: 17.3570 - val_loss: 17.5205 - val_MinusLogProbMetric: 17.5205 - lr: 2.7778e-05 - 77s/epoch - 395ms/step
Epoch 818/1000
2023-09-27 13:13:00.774 
Epoch 818/1000 
	 loss: 17.3202, MinusLogProbMetric: 17.3202, val_loss: 17.5523, val_MinusLogProbMetric: 17.5523

Epoch 818: val_loss did not improve from 17.50531
196/196 - 78s - loss: 17.3202 - MinusLogProbMetric: 17.3202 - val_loss: 17.5523 - val_MinusLogProbMetric: 17.5523 - lr: 2.7778e-05 - 78s/epoch - 399ms/step
Epoch 819/1000
2023-09-27 13:14:18.192 
Epoch 819/1000 
	 loss: 17.3319, MinusLogProbMetric: 17.3319, val_loss: 17.5322, val_MinusLogProbMetric: 17.5322

Epoch 819: val_loss did not improve from 17.50531
196/196 - 77s - loss: 17.3319 - MinusLogProbMetric: 17.3319 - val_loss: 17.5322 - val_MinusLogProbMetric: 17.5322 - lr: 2.7778e-05 - 77s/epoch - 395ms/step
Epoch 820/1000
2023-09-27 13:15:35.913 
Epoch 820/1000 
	 loss: 17.3209, MinusLogProbMetric: 17.3209, val_loss: 17.5489, val_MinusLogProbMetric: 17.5489

Epoch 820: val_loss did not improve from 17.50531
196/196 - 78s - loss: 17.3209 - MinusLogProbMetric: 17.3209 - val_loss: 17.5489 - val_MinusLogProbMetric: 17.5489 - lr: 2.7778e-05 - 78s/epoch - 397ms/step
Epoch 821/1000
2023-09-27 13:16:53.351 
Epoch 821/1000 
	 loss: 17.3287, MinusLogProbMetric: 17.3287, val_loss: 17.5142, val_MinusLogProbMetric: 17.5142

Epoch 821: val_loss did not improve from 17.50531
196/196 - 77s - loss: 17.3287 - MinusLogProbMetric: 17.3287 - val_loss: 17.5142 - val_MinusLogProbMetric: 17.5142 - lr: 2.7778e-05 - 77s/epoch - 395ms/step
Epoch 822/1000
2023-09-27 13:18:11.701 
Epoch 822/1000 
	 loss: 17.3244, MinusLogProbMetric: 17.3244, val_loss: 17.5093, val_MinusLogProbMetric: 17.5093

Epoch 822: val_loss did not improve from 17.50531
196/196 - 78s - loss: 17.3244 - MinusLogProbMetric: 17.3244 - val_loss: 17.5093 - val_MinusLogProbMetric: 17.5093 - lr: 2.7778e-05 - 78s/epoch - 400ms/step
Epoch 823/1000
2023-09-27 13:19:29.409 
Epoch 823/1000 
	 loss: 17.3146, MinusLogProbMetric: 17.3146, val_loss: 17.5313, val_MinusLogProbMetric: 17.5313

Epoch 823: val_loss did not improve from 17.50531
196/196 - 78s - loss: 17.3146 - MinusLogProbMetric: 17.3146 - val_loss: 17.5313 - val_MinusLogProbMetric: 17.5313 - lr: 2.7778e-05 - 78s/epoch - 396ms/step
Epoch 824/1000
2023-09-27 13:20:46.828 
Epoch 824/1000 
	 loss: 17.3595, MinusLogProbMetric: 17.3595, val_loss: 17.6488, val_MinusLogProbMetric: 17.6488

Epoch 824: val_loss did not improve from 17.50531
196/196 - 77s - loss: 17.3595 - MinusLogProbMetric: 17.3595 - val_loss: 17.6488 - val_MinusLogProbMetric: 17.6488 - lr: 2.7778e-05 - 77s/epoch - 395ms/step
Epoch 825/1000
2023-09-27 13:22:04.787 
Epoch 825/1000 
	 loss: 17.3331, MinusLogProbMetric: 17.3331, val_loss: 17.5145, val_MinusLogProbMetric: 17.5145

Epoch 825: val_loss did not improve from 17.50531
196/196 - 78s - loss: 17.3331 - MinusLogProbMetric: 17.3331 - val_loss: 17.5145 - val_MinusLogProbMetric: 17.5145 - lr: 2.7778e-05 - 78s/epoch - 398ms/step
Epoch 826/1000
2023-09-27 13:23:22.358 
Epoch 826/1000 
	 loss: 17.3368, MinusLogProbMetric: 17.3368, val_loss: 17.5033, val_MinusLogProbMetric: 17.5033

Epoch 826: val_loss improved from 17.50531 to 17.50334, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 79s - loss: 17.3368 - MinusLogProbMetric: 17.3368 - val_loss: 17.5033 - val_MinusLogProbMetric: 17.5033 - lr: 2.7778e-05 - 79s/epoch - 404ms/step
Epoch 827/1000
2023-09-27 13:24:41.840 
Epoch 827/1000 
	 loss: 17.3345, MinusLogProbMetric: 17.3345, val_loss: 17.5116, val_MinusLogProbMetric: 17.5116

Epoch 827: val_loss did not improve from 17.50334
196/196 - 78s - loss: 17.3345 - MinusLogProbMetric: 17.3345 - val_loss: 17.5116 - val_MinusLogProbMetric: 17.5116 - lr: 2.7778e-05 - 78s/epoch - 398ms/step
Epoch 828/1000
2023-09-27 13:25:59.569 
Epoch 828/1000 
	 loss: 17.3234, MinusLogProbMetric: 17.3234, val_loss: 17.5110, val_MinusLogProbMetric: 17.5110

Epoch 828: val_loss did not improve from 17.50334
196/196 - 78s - loss: 17.3234 - MinusLogProbMetric: 17.3234 - val_loss: 17.5110 - val_MinusLogProbMetric: 17.5110 - lr: 2.7778e-05 - 78s/epoch - 397ms/step
Epoch 829/1000
2023-09-27 13:27:17.852 
Epoch 829/1000 
	 loss: 17.3282, MinusLogProbMetric: 17.3282, val_loss: 17.5179, val_MinusLogProbMetric: 17.5179

Epoch 829: val_loss did not improve from 17.50334
196/196 - 78s - loss: 17.3282 - MinusLogProbMetric: 17.3282 - val_loss: 17.5179 - val_MinusLogProbMetric: 17.5179 - lr: 2.7778e-05 - 78s/epoch - 399ms/step
Epoch 830/1000
2023-09-27 13:28:35.440 
Epoch 830/1000 
	 loss: 17.3715, MinusLogProbMetric: 17.3715, val_loss: 17.5258, val_MinusLogProbMetric: 17.5258

Epoch 830: val_loss did not improve from 17.50334
196/196 - 78s - loss: 17.3715 - MinusLogProbMetric: 17.3715 - val_loss: 17.5258 - val_MinusLogProbMetric: 17.5258 - lr: 2.7778e-05 - 78s/epoch - 396ms/step
Epoch 831/1000
2023-09-27 13:29:53.563 
Epoch 831/1000 
	 loss: 17.3157, MinusLogProbMetric: 17.3157, val_loss: 17.4976, val_MinusLogProbMetric: 17.4976

Epoch 831: val_loss improved from 17.50334 to 17.49756, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 79s - loss: 17.3157 - MinusLogProbMetric: 17.3157 - val_loss: 17.4976 - val_MinusLogProbMetric: 17.4976 - lr: 2.7778e-05 - 79s/epoch - 405ms/step
Epoch 832/1000
2023-09-27 13:31:12.584 
Epoch 832/1000 
	 loss: 17.3204, MinusLogProbMetric: 17.3204, val_loss: 17.5064, val_MinusLogProbMetric: 17.5064

Epoch 832: val_loss did not improve from 17.49756
196/196 - 78s - loss: 17.3204 - MinusLogProbMetric: 17.3204 - val_loss: 17.5064 - val_MinusLogProbMetric: 17.5064 - lr: 2.7778e-05 - 78s/epoch - 397ms/step
Epoch 833/1000
2023-09-27 13:32:29.997 
Epoch 833/1000 
	 loss: 17.3130, MinusLogProbMetric: 17.3130, val_loss: 17.5180, val_MinusLogProbMetric: 17.5180

Epoch 833: val_loss did not improve from 17.49756
196/196 - 77s - loss: 17.3130 - MinusLogProbMetric: 17.3130 - val_loss: 17.5180 - val_MinusLogProbMetric: 17.5180 - lr: 2.7778e-05 - 77s/epoch - 395ms/step
Epoch 834/1000
2023-09-27 13:33:47.974 
Epoch 834/1000 
	 loss: 17.3227, MinusLogProbMetric: 17.3227, val_loss: 17.5326, val_MinusLogProbMetric: 17.5326

Epoch 834: val_loss did not improve from 17.49756
196/196 - 78s - loss: 17.3227 - MinusLogProbMetric: 17.3227 - val_loss: 17.5326 - val_MinusLogProbMetric: 17.5326 - lr: 2.7778e-05 - 78s/epoch - 398ms/step
Epoch 835/1000
2023-09-27 13:35:06.231 
Epoch 835/1000 
	 loss: 17.3199, MinusLogProbMetric: 17.3199, val_loss: 17.5573, val_MinusLogProbMetric: 17.5573

Epoch 835: val_loss did not improve from 17.49756
196/196 - 78s - loss: 17.3199 - MinusLogProbMetric: 17.3199 - val_loss: 17.5573 - val_MinusLogProbMetric: 17.5573 - lr: 2.7778e-05 - 78s/epoch - 399ms/step
Epoch 836/1000
2023-09-27 13:36:23.580 
Epoch 836/1000 
	 loss: 17.3421, MinusLogProbMetric: 17.3421, val_loss: 17.5431, val_MinusLogProbMetric: 17.5431

Epoch 836: val_loss did not improve from 17.49756
196/196 - 77s - loss: 17.3421 - MinusLogProbMetric: 17.3421 - val_loss: 17.5431 - val_MinusLogProbMetric: 17.5431 - lr: 2.7778e-05 - 77s/epoch - 395ms/step
Epoch 837/1000
2023-09-27 13:37:40.992 
Epoch 837/1000 
	 loss: 17.3091, MinusLogProbMetric: 17.3091, val_loss: 17.5156, val_MinusLogProbMetric: 17.5156

Epoch 837: val_loss did not improve from 17.49756
196/196 - 77s - loss: 17.3091 - MinusLogProbMetric: 17.3091 - val_loss: 17.5156 - val_MinusLogProbMetric: 17.5156 - lr: 2.7778e-05 - 77s/epoch - 395ms/step
Epoch 838/1000
2023-09-27 13:38:58.762 
Epoch 838/1000 
	 loss: 17.3413, MinusLogProbMetric: 17.3413, val_loss: 17.5191, val_MinusLogProbMetric: 17.5191

Epoch 838: val_loss did not improve from 17.49756
196/196 - 78s - loss: 17.3413 - MinusLogProbMetric: 17.3413 - val_loss: 17.5191 - val_MinusLogProbMetric: 17.5191 - lr: 2.7778e-05 - 78s/epoch - 397ms/step
Epoch 839/1000
2023-09-27 13:40:16.025 
Epoch 839/1000 
	 loss: 17.3195, MinusLogProbMetric: 17.3195, val_loss: 17.5499, val_MinusLogProbMetric: 17.5499

Epoch 839: val_loss did not improve from 17.49756
196/196 - 77s - loss: 17.3195 - MinusLogProbMetric: 17.3195 - val_loss: 17.5499 - val_MinusLogProbMetric: 17.5499 - lr: 2.7778e-05 - 77s/epoch - 394ms/step
Epoch 840/1000
2023-09-27 13:41:33.891 
Epoch 840/1000 
	 loss: 17.3239, MinusLogProbMetric: 17.3239, val_loss: 17.6376, val_MinusLogProbMetric: 17.6376

Epoch 840: val_loss did not improve from 17.49756
196/196 - 78s - loss: 17.3239 - MinusLogProbMetric: 17.3239 - val_loss: 17.6376 - val_MinusLogProbMetric: 17.6376 - lr: 2.7778e-05 - 78s/epoch - 397ms/step
Epoch 841/1000
2023-09-27 13:42:51.344 
Epoch 841/1000 
	 loss: 17.3106, MinusLogProbMetric: 17.3106, val_loss: 17.4868, val_MinusLogProbMetric: 17.4868

Epoch 841: val_loss improved from 17.49756 to 17.48681, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 79s - loss: 17.3106 - MinusLogProbMetric: 17.3106 - val_loss: 17.4868 - val_MinusLogProbMetric: 17.4868 - lr: 2.7778e-05 - 79s/epoch - 402ms/step
Epoch 842/1000
2023-09-27 13:44:09.943 
Epoch 842/1000 
	 loss: 17.3111, MinusLogProbMetric: 17.3111, val_loss: 17.5132, val_MinusLogProbMetric: 17.5132

Epoch 842: val_loss did not improve from 17.48681
196/196 - 77s - loss: 17.3111 - MinusLogProbMetric: 17.3111 - val_loss: 17.5132 - val_MinusLogProbMetric: 17.5132 - lr: 2.7778e-05 - 77s/epoch - 395ms/step
Epoch 843/1000
2023-09-27 13:45:18.514 
Epoch 843/1000 
	 loss: 17.3149, MinusLogProbMetric: 17.3149, val_loss: 17.5696, val_MinusLogProbMetric: 17.5696

Epoch 843: val_loss did not improve from 17.48681
196/196 - 69s - loss: 17.3149 - MinusLogProbMetric: 17.3149 - val_loss: 17.5696 - val_MinusLogProbMetric: 17.5696 - lr: 2.7778e-05 - 69s/epoch - 350ms/step
Epoch 844/1000
2023-09-27 13:46:27.907 
Epoch 844/1000 
	 loss: 17.3363, MinusLogProbMetric: 17.3363, val_loss: 17.8753, val_MinusLogProbMetric: 17.8753

Epoch 844: val_loss did not improve from 17.48681
196/196 - 69s - loss: 17.3363 - MinusLogProbMetric: 17.3363 - val_loss: 17.8753 - val_MinusLogProbMetric: 17.8753 - lr: 2.7778e-05 - 69s/epoch - 354ms/step
Epoch 845/1000
2023-09-27 13:47:44.993 
Epoch 845/1000 
	 loss: 17.3297, MinusLogProbMetric: 17.3297, val_loss: 17.5280, val_MinusLogProbMetric: 17.5280

Epoch 845: val_loss did not improve from 17.48681
196/196 - 77s - loss: 17.3297 - MinusLogProbMetric: 17.3297 - val_loss: 17.5280 - val_MinusLogProbMetric: 17.5280 - lr: 2.7778e-05 - 77s/epoch - 393ms/step
Epoch 846/1000
2023-09-27 13:49:02.533 
Epoch 846/1000 
	 loss: 17.3156, MinusLogProbMetric: 17.3156, val_loss: 17.5093, val_MinusLogProbMetric: 17.5093

Epoch 846: val_loss did not improve from 17.48681
196/196 - 78s - loss: 17.3156 - MinusLogProbMetric: 17.3156 - val_loss: 17.5093 - val_MinusLogProbMetric: 17.5093 - lr: 2.7778e-05 - 78s/epoch - 396ms/step
Epoch 847/1000
2023-09-27 13:50:20.968 
Epoch 847/1000 
	 loss: 17.3272, MinusLogProbMetric: 17.3272, val_loss: 17.5260, val_MinusLogProbMetric: 17.5260

Epoch 847: val_loss did not improve from 17.48681
196/196 - 78s - loss: 17.3272 - MinusLogProbMetric: 17.3272 - val_loss: 17.5260 - val_MinusLogProbMetric: 17.5260 - lr: 2.7778e-05 - 78s/epoch - 400ms/step
Epoch 848/1000
2023-09-27 13:51:38.762 
Epoch 848/1000 
	 loss: 17.3195, MinusLogProbMetric: 17.3195, val_loss: 17.5214, val_MinusLogProbMetric: 17.5214

Epoch 848: val_loss did not improve from 17.48681
196/196 - 78s - loss: 17.3195 - MinusLogProbMetric: 17.3195 - val_loss: 17.5214 - val_MinusLogProbMetric: 17.5214 - lr: 2.7778e-05 - 78s/epoch - 397ms/step
Epoch 849/1000
2023-09-27 13:52:56.600 
Epoch 849/1000 
	 loss: 17.3179, MinusLogProbMetric: 17.3179, val_loss: 17.5195, val_MinusLogProbMetric: 17.5195

Epoch 849: val_loss did not improve from 17.48681
196/196 - 78s - loss: 17.3179 - MinusLogProbMetric: 17.3179 - val_loss: 17.5195 - val_MinusLogProbMetric: 17.5195 - lr: 2.7778e-05 - 78s/epoch - 397ms/step
Epoch 850/1000
2023-09-27 13:54:14.078 
Epoch 850/1000 
	 loss: 17.3210, MinusLogProbMetric: 17.3210, val_loss: 17.4916, val_MinusLogProbMetric: 17.4916

Epoch 850: val_loss did not improve from 17.48681
196/196 - 77s - loss: 17.3210 - MinusLogProbMetric: 17.3210 - val_loss: 17.4916 - val_MinusLogProbMetric: 17.4916 - lr: 2.7778e-05 - 77s/epoch - 395ms/step
Epoch 851/1000
2023-09-27 13:55:31.739 
Epoch 851/1000 
	 loss: 17.3054, MinusLogProbMetric: 17.3054, val_loss: 17.5104, val_MinusLogProbMetric: 17.5104

Epoch 851: val_loss did not improve from 17.48681
196/196 - 78s - loss: 17.3054 - MinusLogProbMetric: 17.3054 - val_loss: 17.5104 - val_MinusLogProbMetric: 17.5104 - lr: 2.7778e-05 - 78s/epoch - 396ms/step
Epoch 852/1000
2023-09-27 13:56:49.114 
Epoch 852/1000 
	 loss: 17.3078, MinusLogProbMetric: 17.3078, val_loss: 17.5104, val_MinusLogProbMetric: 17.5104

Epoch 852: val_loss did not improve from 17.48681
196/196 - 77s - loss: 17.3078 - MinusLogProbMetric: 17.3078 - val_loss: 17.5104 - val_MinusLogProbMetric: 17.5104 - lr: 2.7778e-05 - 77s/epoch - 395ms/step
Epoch 853/1000
2023-09-27 13:58:06.491 
Epoch 853/1000 
	 loss: 17.4209, MinusLogProbMetric: 17.4209, val_loss: 17.5117, val_MinusLogProbMetric: 17.5117

Epoch 853: val_loss did not improve from 17.48681
196/196 - 77s - loss: 17.4209 - MinusLogProbMetric: 17.4209 - val_loss: 17.5117 - val_MinusLogProbMetric: 17.5117 - lr: 2.7778e-05 - 77s/epoch - 395ms/step
Epoch 854/1000
2023-09-27 13:59:23.803 
Epoch 854/1000 
	 loss: 17.3113, MinusLogProbMetric: 17.3113, val_loss: 17.5022, val_MinusLogProbMetric: 17.5022

Epoch 854: val_loss did not improve from 17.48681
196/196 - 77s - loss: 17.3113 - MinusLogProbMetric: 17.3113 - val_loss: 17.5022 - val_MinusLogProbMetric: 17.5022 - lr: 2.7778e-05 - 77s/epoch - 394ms/step
Epoch 855/1000
2023-09-27 14:00:41.469 
Epoch 855/1000 
	 loss: 17.6095, MinusLogProbMetric: 17.6095, val_loss: 17.6861, val_MinusLogProbMetric: 17.6861

Epoch 855: val_loss did not improve from 17.48681
196/196 - 78s - loss: 17.6095 - MinusLogProbMetric: 17.6095 - val_loss: 17.6861 - val_MinusLogProbMetric: 17.6861 - lr: 2.7778e-05 - 78s/epoch - 396ms/step
Epoch 856/1000
2023-09-27 14:01:59.308 
Epoch 856/1000 
	 loss: 17.3170, MinusLogProbMetric: 17.3170, val_loss: 17.5334, val_MinusLogProbMetric: 17.5334

Epoch 856: val_loss did not improve from 17.48681
196/196 - 78s - loss: 17.3170 - MinusLogProbMetric: 17.3170 - val_loss: 17.5334 - val_MinusLogProbMetric: 17.5334 - lr: 2.7778e-05 - 78s/epoch - 397ms/step
Epoch 857/1000
2023-09-27 14:03:16.756 
Epoch 857/1000 
	 loss: 17.3118, MinusLogProbMetric: 17.3118, val_loss: 17.5439, val_MinusLogProbMetric: 17.5439

Epoch 857: val_loss did not improve from 17.48681
196/196 - 77s - loss: 17.3118 - MinusLogProbMetric: 17.3118 - val_loss: 17.5439 - val_MinusLogProbMetric: 17.5439 - lr: 2.7778e-05 - 77s/epoch - 395ms/step
Epoch 858/1000
2023-09-27 14:04:34.327 
Epoch 858/1000 
	 loss: 17.3629, MinusLogProbMetric: 17.3629, val_loss: 17.5061, val_MinusLogProbMetric: 17.5061

Epoch 858: val_loss did not improve from 17.48681
196/196 - 78s - loss: 17.3629 - MinusLogProbMetric: 17.3629 - val_loss: 17.5061 - val_MinusLogProbMetric: 17.5061 - lr: 2.7778e-05 - 78s/epoch - 396ms/step
Epoch 859/1000
2023-09-27 14:05:52.123 
Epoch 859/1000 
	 loss: 17.3524, MinusLogProbMetric: 17.3524, val_loss: 17.4972, val_MinusLogProbMetric: 17.4972

Epoch 859: val_loss did not improve from 17.48681
196/196 - 78s - loss: 17.3524 - MinusLogProbMetric: 17.3524 - val_loss: 17.4972 - val_MinusLogProbMetric: 17.4972 - lr: 2.7778e-05 - 78s/epoch - 397ms/step
Epoch 860/1000
2023-09-27 14:07:09.845 
Epoch 860/1000 
	 loss: 17.3101, MinusLogProbMetric: 17.3101, val_loss: 17.5449, val_MinusLogProbMetric: 17.5449

Epoch 860: val_loss did not improve from 17.48681
196/196 - 78s - loss: 17.3101 - MinusLogProbMetric: 17.3101 - val_loss: 17.5449 - val_MinusLogProbMetric: 17.5449 - lr: 2.7778e-05 - 78s/epoch - 397ms/step
Epoch 861/1000
2023-09-27 14:08:27.427 
Epoch 861/1000 
	 loss: 17.4093, MinusLogProbMetric: 17.4093, val_loss: 17.5113, val_MinusLogProbMetric: 17.5113

Epoch 861: val_loss did not improve from 17.48681
196/196 - 78s - loss: 17.4093 - MinusLogProbMetric: 17.4093 - val_loss: 17.5113 - val_MinusLogProbMetric: 17.5113 - lr: 2.7778e-05 - 78s/epoch - 396ms/step
Epoch 862/1000
2023-09-27 14:09:44.897 
Epoch 862/1000 
	 loss: 17.3164, MinusLogProbMetric: 17.3164, val_loss: 17.5098, val_MinusLogProbMetric: 17.5098

Epoch 862: val_loss did not improve from 17.48681
196/196 - 77s - loss: 17.3164 - MinusLogProbMetric: 17.3164 - val_loss: 17.5098 - val_MinusLogProbMetric: 17.5098 - lr: 2.7778e-05 - 77s/epoch - 395ms/step
Epoch 863/1000
2023-09-27 14:11:03.192 
Epoch 863/1000 
	 loss: 17.3101, MinusLogProbMetric: 17.3101, val_loss: 17.5133, val_MinusLogProbMetric: 17.5133

Epoch 863: val_loss did not improve from 17.48681
196/196 - 78s - loss: 17.3101 - MinusLogProbMetric: 17.3101 - val_loss: 17.5133 - val_MinusLogProbMetric: 17.5133 - lr: 2.7778e-05 - 78s/epoch - 399ms/step
Epoch 864/1000
2023-09-27 14:12:21.141 
Epoch 864/1000 
	 loss: 17.3126, MinusLogProbMetric: 17.3126, val_loss: 17.5701, val_MinusLogProbMetric: 17.5701

Epoch 864: val_loss did not improve from 17.48681
196/196 - 78s - loss: 17.3126 - MinusLogProbMetric: 17.3126 - val_loss: 17.5701 - val_MinusLogProbMetric: 17.5701 - lr: 2.7778e-05 - 78s/epoch - 398ms/step
Epoch 865/1000
2023-09-27 14:13:38.857 
Epoch 865/1000 
	 loss: 17.3459, MinusLogProbMetric: 17.3459, val_loss: 17.5771, val_MinusLogProbMetric: 17.5771

Epoch 865: val_loss did not improve from 17.48681
196/196 - 78s - loss: 17.3459 - MinusLogProbMetric: 17.3459 - val_loss: 17.5771 - val_MinusLogProbMetric: 17.5771 - lr: 2.7778e-05 - 78s/epoch - 396ms/step
Epoch 866/1000
2023-09-27 14:14:56.745 
Epoch 866/1000 
	 loss: 17.3162, MinusLogProbMetric: 17.3162, val_loss: 17.5247, val_MinusLogProbMetric: 17.5247

Epoch 866: val_loss did not improve from 17.48681
196/196 - 78s - loss: 17.3162 - MinusLogProbMetric: 17.3162 - val_loss: 17.5247 - val_MinusLogProbMetric: 17.5247 - lr: 2.7778e-05 - 78s/epoch - 397ms/step
Epoch 867/1000
2023-09-27 14:16:14.475 
Epoch 867/1000 
	 loss: 17.3094, MinusLogProbMetric: 17.3094, val_loss: 17.5233, val_MinusLogProbMetric: 17.5233

Epoch 867: val_loss did not improve from 17.48681
196/196 - 78s - loss: 17.3094 - MinusLogProbMetric: 17.3094 - val_loss: 17.5233 - val_MinusLogProbMetric: 17.5233 - lr: 2.7778e-05 - 78s/epoch - 397ms/step
Epoch 868/1000
2023-09-27 14:17:32.910 
Epoch 868/1000 
	 loss: 17.3183, MinusLogProbMetric: 17.3183, val_loss: 17.4887, val_MinusLogProbMetric: 17.4887

Epoch 868: val_loss did not improve from 17.48681
196/196 - 78s - loss: 17.3183 - MinusLogProbMetric: 17.3183 - val_loss: 17.4887 - val_MinusLogProbMetric: 17.4887 - lr: 2.7778e-05 - 78s/epoch - 400ms/step
Epoch 869/1000
2023-09-27 14:18:50.986 
Epoch 869/1000 
	 loss: 17.3100, MinusLogProbMetric: 17.3100, val_loss: 17.5379, val_MinusLogProbMetric: 17.5379

Epoch 869: val_loss did not improve from 17.48681
196/196 - 78s - loss: 17.3100 - MinusLogProbMetric: 17.3100 - val_loss: 17.5379 - val_MinusLogProbMetric: 17.5379 - lr: 2.7778e-05 - 78s/epoch - 398ms/step
Epoch 870/1000
2023-09-27 14:20:08.427 
Epoch 870/1000 
	 loss: 17.4455, MinusLogProbMetric: 17.4455, val_loss: 17.5065, val_MinusLogProbMetric: 17.5065

Epoch 870: val_loss did not improve from 17.48681
196/196 - 77s - loss: 17.4455 - MinusLogProbMetric: 17.4455 - val_loss: 17.5065 - val_MinusLogProbMetric: 17.5065 - lr: 2.7778e-05 - 77s/epoch - 395ms/step
Epoch 871/1000
2023-09-27 14:21:26.011 
Epoch 871/1000 
	 loss: 17.3085, MinusLogProbMetric: 17.3085, val_loss: 17.5286, val_MinusLogProbMetric: 17.5286

Epoch 871: val_loss did not improve from 17.48681
196/196 - 78s - loss: 17.3085 - MinusLogProbMetric: 17.3085 - val_loss: 17.5286 - val_MinusLogProbMetric: 17.5286 - lr: 2.7778e-05 - 78s/epoch - 396ms/step
Epoch 872/1000
2023-09-27 14:22:44.667 
Epoch 872/1000 
	 loss: 17.3118, MinusLogProbMetric: 17.3118, val_loss: 17.5285, val_MinusLogProbMetric: 17.5285

Epoch 872: val_loss did not improve from 17.48681
196/196 - 79s - loss: 17.3118 - MinusLogProbMetric: 17.3118 - val_loss: 17.5285 - val_MinusLogProbMetric: 17.5285 - lr: 2.7778e-05 - 79s/epoch - 401ms/step
Epoch 873/1000
2023-09-27 14:24:02.793 
Epoch 873/1000 
	 loss: 17.3280, MinusLogProbMetric: 17.3280, val_loss: 17.5460, val_MinusLogProbMetric: 17.5460

Epoch 873: val_loss did not improve from 17.48681
196/196 - 78s - loss: 17.3280 - MinusLogProbMetric: 17.3280 - val_loss: 17.5460 - val_MinusLogProbMetric: 17.5460 - lr: 2.7778e-05 - 78s/epoch - 399ms/step
Epoch 874/1000
2023-09-27 14:25:20.683 
Epoch 874/1000 
	 loss: 17.3062, MinusLogProbMetric: 17.3062, val_loss: 17.5137, val_MinusLogProbMetric: 17.5137

Epoch 874: val_loss did not improve from 17.48681
196/196 - 78s - loss: 17.3062 - MinusLogProbMetric: 17.3062 - val_loss: 17.5137 - val_MinusLogProbMetric: 17.5137 - lr: 2.7778e-05 - 78s/epoch - 397ms/step
Epoch 875/1000
2023-09-27 14:26:38.260 
Epoch 875/1000 
	 loss: 17.3017, MinusLogProbMetric: 17.3017, val_loss: 17.4939, val_MinusLogProbMetric: 17.4939

Epoch 875: val_loss did not improve from 17.48681
196/196 - 78s - loss: 17.3017 - MinusLogProbMetric: 17.3017 - val_loss: 17.4939 - val_MinusLogProbMetric: 17.4939 - lr: 2.7778e-05 - 78s/epoch - 396ms/step
Epoch 876/1000
2023-09-27 14:27:55.861 
Epoch 876/1000 
	 loss: 17.3047, MinusLogProbMetric: 17.3047, val_loss: 17.5046, val_MinusLogProbMetric: 17.5046

Epoch 876: val_loss did not improve from 17.48681
196/196 - 78s - loss: 17.3047 - MinusLogProbMetric: 17.3047 - val_loss: 17.5046 - val_MinusLogProbMetric: 17.5046 - lr: 2.7778e-05 - 78s/epoch - 396ms/step
Epoch 877/1000
2023-09-27 14:29:13.520 
Epoch 877/1000 
	 loss: 17.3054, MinusLogProbMetric: 17.3054, val_loss: 17.4922, val_MinusLogProbMetric: 17.4922

Epoch 877: val_loss did not improve from 17.48681
196/196 - 78s - loss: 17.3054 - MinusLogProbMetric: 17.3054 - val_loss: 17.4922 - val_MinusLogProbMetric: 17.4922 - lr: 2.7778e-05 - 78s/epoch - 396ms/step
Epoch 878/1000
2023-09-27 14:30:31.232 
Epoch 878/1000 
	 loss: 17.3030, MinusLogProbMetric: 17.3030, val_loss: 17.4823, val_MinusLogProbMetric: 17.4823

Epoch 878: val_loss improved from 17.48681 to 17.48229, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 79s - loss: 17.3030 - MinusLogProbMetric: 17.3030 - val_loss: 17.4823 - val_MinusLogProbMetric: 17.4823 - lr: 2.7778e-05 - 79s/epoch - 404ms/step
Epoch 879/1000
2023-09-27 14:31:49.949 
Epoch 879/1000 
	 loss: 17.2887, MinusLogProbMetric: 17.2887, val_loss: 17.5478, val_MinusLogProbMetric: 17.5478

Epoch 879: val_loss did not improve from 17.48229
196/196 - 77s - loss: 17.2887 - MinusLogProbMetric: 17.2887 - val_loss: 17.5478 - val_MinusLogProbMetric: 17.5478 - lr: 2.7778e-05 - 77s/epoch - 394ms/step
Epoch 880/1000
2023-09-27 14:33:07.887 
Epoch 880/1000 
	 loss: 17.3100, MinusLogProbMetric: 17.3100, val_loss: 17.4856, val_MinusLogProbMetric: 17.4856

Epoch 880: val_loss did not improve from 17.48229
196/196 - 78s - loss: 17.3100 - MinusLogProbMetric: 17.3100 - val_loss: 17.4856 - val_MinusLogProbMetric: 17.4856 - lr: 2.7778e-05 - 78s/epoch - 398ms/step
Epoch 881/1000
2023-09-27 14:34:25.534 
Epoch 881/1000 
	 loss: 17.3033, MinusLogProbMetric: 17.3033, val_loss: 17.5267, val_MinusLogProbMetric: 17.5267

Epoch 881: val_loss did not improve from 17.48229
196/196 - 78s - loss: 17.3033 - MinusLogProbMetric: 17.3033 - val_loss: 17.5267 - val_MinusLogProbMetric: 17.5267 - lr: 2.7778e-05 - 78s/epoch - 396ms/step
Epoch 882/1000
2023-09-27 14:35:43.313 
Epoch 882/1000 
	 loss: 17.2966, MinusLogProbMetric: 17.2966, val_loss: 17.5004, val_MinusLogProbMetric: 17.5004

Epoch 882: val_loss did not improve from 17.48229
196/196 - 78s - loss: 17.2966 - MinusLogProbMetric: 17.2966 - val_loss: 17.5004 - val_MinusLogProbMetric: 17.5004 - lr: 2.7778e-05 - 78s/epoch - 397ms/step
Epoch 883/1000
2023-09-27 14:37:01.067 
Epoch 883/1000 
	 loss: 17.3109, MinusLogProbMetric: 17.3109, val_loss: 17.5201, val_MinusLogProbMetric: 17.5201

Epoch 883: val_loss did not improve from 17.48229
196/196 - 78s - loss: 17.3109 - MinusLogProbMetric: 17.3109 - val_loss: 17.5201 - val_MinusLogProbMetric: 17.5201 - lr: 2.7778e-05 - 78s/epoch - 397ms/step
Epoch 884/1000
2023-09-27 14:38:18.390 
Epoch 884/1000 
	 loss: 17.2985, MinusLogProbMetric: 17.2985, val_loss: 17.4815, val_MinusLogProbMetric: 17.4815

Epoch 884: val_loss improved from 17.48229 to 17.48148, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 79s - loss: 17.2985 - MinusLogProbMetric: 17.2985 - val_loss: 17.4815 - val_MinusLogProbMetric: 17.4815 - lr: 2.7778e-05 - 79s/epoch - 403ms/step
Epoch 885/1000
2023-09-27 14:39:37.680 
Epoch 885/1000 
	 loss: 17.2951, MinusLogProbMetric: 17.2951, val_loss: 17.5253, val_MinusLogProbMetric: 17.5253

Epoch 885: val_loss did not improve from 17.48148
196/196 - 78s - loss: 17.2951 - MinusLogProbMetric: 17.2951 - val_loss: 17.5253 - val_MinusLogProbMetric: 17.5253 - lr: 2.7778e-05 - 78s/epoch - 396ms/step
Epoch 886/1000
2023-09-27 14:40:55.081 
Epoch 886/1000 
	 loss: 17.3048, MinusLogProbMetric: 17.3048, val_loss: 17.4828, val_MinusLogProbMetric: 17.4828

Epoch 886: val_loss did not improve from 17.48148
196/196 - 77s - loss: 17.3048 - MinusLogProbMetric: 17.3048 - val_loss: 17.4828 - val_MinusLogProbMetric: 17.4828 - lr: 2.7778e-05 - 77s/epoch - 395ms/step
Epoch 887/1000
2023-09-27 14:42:13.018 
Epoch 887/1000 
	 loss: 17.4573, MinusLogProbMetric: 17.4573, val_loss: 17.5196, val_MinusLogProbMetric: 17.5196

Epoch 887: val_loss did not improve from 17.48148
196/196 - 78s - loss: 17.4573 - MinusLogProbMetric: 17.4573 - val_loss: 17.5196 - val_MinusLogProbMetric: 17.5196 - lr: 2.7778e-05 - 78s/epoch - 398ms/step
Epoch 888/1000
2023-09-27 14:43:30.966 
Epoch 888/1000 
	 loss: 17.3115, MinusLogProbMetric: 17.3115, val_loss: 17.5064, val_MinusLogProbMetric: 17.5064

Epoch 888: val_loss did not improve from 17.48148
196/196 - 78s - loss: 17.3115 - MinusLogProbMetric: 17.3115 - val_loss: 17.5064 - val_MinusLogProbMetric: 17.5064 - lr: 2.7778e-05 - 78s/epoch - 398ms/step
Epoch 889/1000
2023-09-27 14:44:49.140 
Epoch 889/1000 
	 loss: 17.3051, MinusLogProbMetric: 17.3051, val_loss: 17.5203, val_MinusLogProbMetric: 17.5203

Epoch 889: val_loss did not improve from 17.48148
196/196 - 78s - loss: 17.3051 - MinusLogProbMetric: 17.3051 - val_loss: 17.5203 - val_MinusLogProbMetric: 17.5203 - lr: 2.7778e-05 - 78s/epoch - 399ms/step
Epoch 890/1000
2023-09-27 14:46:06.686 
Epoch 890/1000 
	 loss: 17.2985, MinusLogProbMetric: 17.2985, val_loss: 17.5086, val_MinusLogProbMetric: 17.5086

Epoch 890: val_loss did not improve from 17.48148
196/196 - 78s - loss: 17.2985 - MinusLogProbMetric: 17.2985 - val_loss: 17.5086 - val_MinusLogProbMetric: 17.5086 - lr: 2.7778e-05 - 78s/epoch - 396ms/step
Epoch 891/1000
2023-09-27 14:47:23.935 
Epoch 891/1000 
	 loss: 17.2961, MinusLogProbMetric: 17.2961, val_loss: 17.5601, val_MinusLogProbMetric: 17.5601

Epoch 891: val_loss did not improve from 17.48148
196/196 - 77s - loss: 17.2961 - MinusLogProbMetric: 17.2961 - val_loss: 17.5601 - val_MinusLogProbMetric: 17.5601 - lr: 2.7778e-05 - 77s/epoch - 394ms/step
Epoch 892/1000
2023-09-27 14:48:41.533 
Epoch 892/1000 
	 loss: 17.3129, MinusLogProbMetric: 17.3129, val_loss: 17.5073, val_MinusLogProbMetric: 17.5073

Epoch 892: val_loss did not improve from 17.48148
196/196 - 78s - loss: 17.3129 - MinusLogProbMetric: 17.3129 - val_loss: 17.5073 - val_MinusLogProbMetric: 17.5073 - lr: 2.7778e-05 - 78s/epoch - 396ms/step
Epoch 893/1000
2023-09-27 14:49:59.721 
Epoch 893/1000 
	 loss: 17.3065, MinusLogProbMetric: 17.3065, val_loss: 17.5113, val_MinusLogProbMetric: 17.5113

Epoch 893: val_loss did not improve from 17.48148
196/196 - 78s - loss: 17.3065 - MinusLogProbMetric: 17.3065 - val_loss: 17.5113 - val_MinusLogProbMetric: 17.5113 - lr: 2.7778e-05 - 78s/epoch - 399ms/step
Epoch 894/1000
2023-09-27 14:51:17.304 
Epoch 894/1000 
	 loss: 17.3035, MinusLogProbMetric: 17.3035, val_loss: 17.5085, val_MinusLogProbMetric: 17.5085

Epoch 894: val_loss did not improve from 17.48148
196/196 - 78s - loss: 17.3035 - MinusLogProbMetric: 17.3035 - val_loss: 17.5085 - val_MinusLogProbMetric: 17.5085 - lr: 2.7778e-05 - 78s/epoch - 396ms/step
Epoch 895/1000
2023-09-27 14:52:35.261 
Epoch 895/1000 
	 loss: 17.2941, MinusLogProbMetric: 17.2941, val_loss: 17.5245, val_MinusLogProbMetric: 17.5245

Epoch 895: val_loss did not improve from 17.48148
196/196 - 78s - loss: 17.2941 - MinusLogProbMetric: 17.2941 - val_loss: 17.5245 - val_MinusLogProbMetric: 17.5245 - lr: 2.7778e-05 - 78s/epoch - 398ms/step
Epoch 896/1000
2023-09-27 14:53:53.222 
Epoch 896/1000 
	 loss: 17.3022, MinusLogProbMetric: 17.3022, val_loss: 17.4911, val_MinusLogProbMetric: 17.4911

Epoch 896: val_loss did not improve from 17.48148
196/196 - 78s - loss: 17.3022 - MinusLogProbMetric: 17.3022 - val_loss: 17.4911 - val_MinusLogProbMetric: 17.4911 - lr: 2.7778e-05 - 78s/epoch - 398ms/step
Epoch 897/1000
2023-09-27 14:55:11.105 
Epoch 897/1000 
	 loss: 17.3706, MinusLogProbMetric: 17.3706, val_loss: 17.5293, val_MinusLogProbMetric: 17.5293

Epoch 897: val_loss did not improve from 17.48148
196/196 - 78s - loss: 17.3706 - MinusLogProbMetric: 17.3706 - val_loss: 17.5293 - val_MinusLogProbMetric: 17.5293 - lr: 2.7778e-05 - 78s/epoch - 397ms/step
Epoch 898/1000
2023-09-27 14:56:29.026 
Epoch 898/1000 
	 loss: 17.3032, MinusLogProbMetric: 17.3032, val_loss: 17.5143, val_MinusLogProbMetric: 17.5143

Epoch 898: val_loss did not improve from 17.48148
196/196 - 78s - loss: 17.3032 - MinusLogProbMetric: 17.3032 - val_loss: 17.5143 - val_MinusLogProbMetric: 17.5143 - lr: 2.7778e-05 - 78s/epoch - 398ms/step
Epoch 899/1000
2023-09-27 14:57:46.891 
Epoch 899/1000 
	 loss: 17.3006, MinusLogProbMetric: 17.3006, val_loss: 17.4702, val_MinusLogProbMetric: 17.4702

Epoch 899: val_loss improved from 17.48148 to 17.47022, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 80s - loss: 17.3006 - MinusLogProbMetric: 17.3006 - val_loss: 17.4702 - val_MinusLogProbMetric: 17.4702 - lr: 2.7778e-05 - 80s/epoch - 407ms/step
Epoch 900/1000
2023-09-27 14:59:06.489 
Epoch 900/1000 
	 loss: 17.3044, MinusLogProbMetric: 17.3044, val_loss: 17.5970, val_MinusLogProbMetric: 17.5970

Epoch 900: val_loss did not improve from 17.47022
196/196 - 78s - loss: 17.3044 - MinusLogProbMetric: 17.3044 - val_loss: 17.5970 - val_MinusLogProbMetric: 17.5970 - lr: 2.7778e-05 - 78s/epoch - 397ms/step
Epoch 901/1000
2023-09-27 15:00:24.525 
Epoch 901/1000 
	 loss: 17.3034, MinusLogProbMetric: 17.3034, val_loss: 17.4961, val_MinusLogProbMetric: 17.4961

Epoch 901: val_loss did not improve from 17.47022
196/196 - 78s - loss: 17.3034 - MinusLogProbMetric: 17.3034 - val_loss: 17.4961 - val_MinusLogProbMetric: 17.4961 - lr: 2.7778e-05 - 78s/epoch - 398ms/step
Epoch 902/1000
2023-09-27 15:01:41.656 
Epoch 902/1000 
	 loss: 17.2955, MinusLogProbMetric: 17.2955, val_loss: 17.5034, val_MinusLogProbMetric: 17.5034

Epoch 902: val_loss did not improve from 17.47022
196/196 - 77s - loss: 17.2955 - MinusLogProbMetric: 17.2955 - val_loss: 17.5034 - val_MinusLogProbMetric: 17.5034 - lr: 2.7778e-05 - 77s/epoch - 394ms/step
Epoch 903/1000
2023-09-27 15:02:59.759 
Epoch 903/1000 
	 loss: 17.3588, MinusLogProbMetric: 17.3588, val_loss: 17.5139, val_MinusLogProbMetric: 17.5139

Epoch 903: val_loss did not improve from 17.47022
196/196 - 78s - loss: 17.3588 - MinusLogProbMetric: 17.3588 - val_loss: 17.5139 - val_MinusLogProbMetric: 17.5139 - lr: 2.7778e-05 - 78s/epoch - 398ms/step
Epoch 904/1000
2023-09-27 15:04:17.302 
Epoch 904/1000 
	 loss: 17.3062, MinusLogProbMetric: 17.3062, val_loss: 17.5293, val_MinusLogProbMetric: 17.5293

Epoch 904: val_loss did not improve from 17.47022
196/196 - 78s - loss: 17.3062 - MinusLogProbMetric: 17.3062 - val_loss: 17.5293 - val_MinusLogProbMetric: 17.5293 - lr: 2.7778e-05 - 78s/epoch - 396ms/step
Epoch 905/1000
2023-09-27 15:05:35.237 
Epoch 905/1000 
	 loss: 17.4226, MinusLogProbMetric: 17.4226, val_loss: 17.5210, val_MinusLogProbMetric: 17.5210

Epoch 905: val_loss did not improve from 17.47022
196/196 - 78s - loss: 17.4226 - MinusLogProbMetric: 17.4226 - val_loss: 17.5210 - val_MinusLogProbMetric: 17.5210 - lr: 2.7778e-05 - 78s/epoch - 398ms/step
Epoch 906/1000
2023-09-27 15:06:52.691 
Epoch 906/1000 
	 loss: 17.2965, MinusLogProbMetric: 17.2965, val_loss: 17.5385, val_MinusLogProbMetric: 17.5385

Epoch 906: val_loss did not improve from 17.47022
196/196 - 77s - loss: 17.2965 - MinusLogProbMetric: 17.2965 - val_loss: 17.5385 - val_MinusLogProbMetric: 17.5385 - lr: 2.7778e-05 - 77s/epoch - 395ms/step
Epoch 907/1000
2023-09-27 15:08:10.470 
Epoch 907/1000 
	 loss: 17.3197, MinusLogProbMetric: 17.3197, val_loss: 17.4962, val_MinusLogProbMetric: 17.4962

Epoch 907: val_loss did not improve from 17.47022
196/196 - 78s - loss: 17.3197 - MinusLogProbMetric: 17.3197 - val_loss: 17.4962 - val_MinusLogProbMetric: 17.4962 - lr: 2.7778e-05 - 78s/epoch - 397ms/step
Epoch 908/1000
2023-09-27 15:09:28.191 
Epoch 908/1000 
	 loss: 17.2899, MinusLogProbMetric: 17.2899, val_loss: 17.5028, val_MinusLogProbMetric: 17.5028

Epoch 908: val_loss did not improve from 17.47022
196/196 - 78s - loss: 17.2899 - MinusLogProbMetric: 17.2899 - val_loss: 17.5028 - val_MinusLogProbMetric: 17.5028 - lr: 2.7778e-05 - 78s/epoch - 397ms/step
Epoch 909/1000
2023-09-27 15:10:45.918 
Epoch 909/1000 
	 loss: 17.3116, MinusLogProbMetric: 17.3116, val_loss: 17.5283, val_MinusLogProbMetric: 17.5283

Epoch 909: val_loss did not improve from 17.47022
196/196 - 78s - loss: 17.3116 - MinusLogProbMetric: 17.3116 - val_loss: 17.5283 - val_MinusLogProbMetric: 17.5283 - lr: 2.7778e-05 - 78s/epoch - 397ms/step
Epoch 910/1000
2023-09-27 15:12:03.513 
Epoch 910/1000 
	 loss: 17.3073, MinusLogProbMetric: 17.3073, val_loss: 17.4925, val_MinusLogProbMetric: 17.4925

Epoch 910: val_loss did not improve from 17.47022
196/196 - 78s - loss: 17.3073 - MinusLogProbMetric: 17.3073 - val_loss: 17.4925 - val_MinusLogProbMetric: 17.4925 - lr: 2.7778e-05 - 78s/epoch - 396ms/step
Epoch 911/1000
2023-09-27 15:13:20.827 
Epoch 911/1000 
	 loss: 17.3002, MinusLogProbMetric: 17.3002, val_loss: 17.5007, val_MinusLogProbMetric: 17.5007

Epoch 911: val_loss did not improve from 17.47022
196/196 - 77s - loss: 17.3002 - MinusLogProbMetric: 17.3002 - val_loss: 17.5007 - val_MinusLogProbMetric: 17.5007 - lr: 2.7778e-05 - 77s/epoch - 394ms/step
Epoch 912/1000
2023-09-27 15:14:38.350 
Epoch 912/1000 
	 loss: 17.3079, MinusLogProbMetric: 17.3079, val_loss: 17.5146, val_MinusLogProbMetric: 17.5146

Epoch 912: val_loss did not improve from 17.47022
196/196 - 78s - loss: 17.3079 - MinusLogProbMetric: 17.3079 - val_loss: 17.5146 - val_MinusLogProbMetric: 17.5146 - lr: 2.7778e-05 - 78s/epoch - 396ms/step
Epoch 913/1000
2023-09-27 15:15:56.773 
Epoch 913/1000 
	 loss: 17.2967, MinusLogProbMetric: 17.2967, val_loss: 17.5713, val_MinusLogProbMetric: 17.5713

Epoch 913: val_loss did not improve from 17.47022
196/196 - 78s - loss: 17.2967 - MinusLogProbMetric: 17.2967 - val_loss: 17.5713 - val_MinusLogProbMetric: 17.5713 - lr: 2.7778e-05 - 78s/epoch - 400ms/step
Epoch 914/1000
2023-09-27 15:17:14.425 
Epoch 914/1000 
	 loss: 17.3199, MinusLogProbMetric: 17.3199, val_loss: 17.4844, val_MinusLogProbMetric: 17.4844

Epoch 914: val_loss did not improve from 17.47022
196/196 - 78s - loss: 17.3199 - MinusLogProbMetric: 17.3199 - val_loss: 17.4844 - val_MinusLogProbMetric: 17.4844 - lr: 2.7778e-05 - 78s/epoch - 396ms/step
Epoch 915/1000
2023-09-27 15:18:32.968 
Epoch 915/1000 
	 loss: 17.2897, MinusLogProbMetric: 17.2897, val_loss: 17.4781, val_MinusLogProbMetric: 17.4781

Epoch 915: val_loss did not improve from 17.47022
196/196 - 79s - loss: 17.2897 - MinusLogProbMetric: 17.2897 - val_loss: 17.4781 - val_MinusLogProbMetric: 17.4781 - lr: 2.7778e-05 - 79s/epoch - 401ms/step
Epoch 916/1000
2023-09-27 15:19:51.430 
Epoch 916/1000 
	 loss: 17.3329, MinusLogProbMetric: 17.3329, val_loss: 17.4857, val_MinusLogProbMetric: 17.4857

Epoch 916: val_loss did not improve from 17.47022
196/196 - 78s - loss: 17.3329 - MinusLogProbMetric: 17.3329 - val_loss: 17.4857 - val_MinusLogProbMetric: 17.4857 - lr: 2.7778e-05 - 78s/epoch - 400ms/step
Epoch 917/1000
2023-09-27 15:21:11.809 
Epoch 917/1000 
	 loss: 17.2990, MinusLogProbMetric: 17.2990, val_loss: 17.4682, val_MinusLogProbMetric: 17.4682

Epoch 917: val_loss improved from 17.47022 to 17.46825, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 82s - loss: 17.2990 - MinusLogProbMetric: 17.2990 - val_loss: 17.4682 - val_MinusLogProbMetric: 17.4682 - lr: 2.7778e-05 - 82s/epoch - 418ms/step
Epoch 918/1000
2023-09-27 15:22:31.921 
Epoch 918/1000 
	 loss: 17.2896, MinusLogProbMetric: 17.2896, val_loss: 17.4831, val_MinusLogProbMetric: 17.4831

Epoch 918: val_loss did not improve from 17.46825
196/196 - 79s - loss: 17.2896 - MinusLogProbMetric: 17.2896 - val_loss: 17.4831 - val_MinusLogProbMetric: 17.4831 - lr: 2.7778e-05 - 79s/epoch - 401ms/step
Epoch 919/1000
2023-09-27 15:23:49.761 
Epoch 919/1000 
	 loss: 17.3235, MinusLogProbMetric: 17.3235, val_loss: 17.5054, val_MinusLogProbMetric: 17.5054

Epoch 919: val_loss did not improve from 17.46825
196/196 - 78s - loss: 17.3235 - MinusLogProbMetric: 17.3235 - val_loss: 17.5054 - val_MinusLogProbMetric: 17.5054 - lr: 2.7778e-05 - 78s/epoch - 397ms/step
Epoch 920/1000
2023-09-27 15:25:07.625 
Epoch 920/1000 
	 loss: 17.2907, MinusLogProbMetric: 17.2907, val_loss: 17.5672, val_MinusLogProbMetric: 17.5672

Epoch 920: val_loss did not improve from 17.46825
196/196 - 78s - loss: 17.2907 - MinusLogProbMetric: 17.2907 - val_loss: 17.5672 - val_MinusLogProbMetric: 17.5672 - lr: 2.7778e-05 - 78s/epoch - 397ms/step
Epoch 921/1000
2023-09-27 15:26:25.497 
Epoch 921/1000 
	 loss: 17.2958, MinusLogProbMetric: 17.2958, val_loss: 17.4698, val_MinusLogProbMetric: 17.4698

Epoch 921: val_loss did not improve from 17.46825
196/196 - 78s - loss: 17.2958 - MinusLogProbMetric: 17.2958 - val_loss: 17.4698 - val_MinusLogProbMetric: 17.4698 - lr: 2.7778e-05 - 78s/epoch - 397ms/step
Epoch 922/1000
2023-09-27 15:27:43.232 
Epoch 922/1000 
	 loss: 17.3090, MinusLogProbMetric: 17.3090, val_loss: 17.5310, val_MinusLogProbMetric: 17.5310

Epoch 922: val_loss did not improve from 17.46825
196/196 - 78s - loss: 17.3090 - MinusLogProbMetric: 17.3090 - val_loss: 17.5310 - val_MinusLogProbMetric: 17.5310 - lr: 2.7778e-05 - 78s/epoch - 397ms/step
Epoch 923/1000
2023-09-27 15:29:00.547 
Epoch 923/1000 
	 loss: 17.3011, MinusLogProbMetric: 17.3011, val_loss: 17.4790, val_MinusLogProbMetric: 17.4790

Epoch 923: val_loss did not improve from 17.46825
196/196 - 77s - loss: 17.3011 - MinusLogProbMetric: 17.3011 - val_loss: 17.4790 - val_MinusLogProbMetric: 17.4790 - lr: 2.7778e-05 - 77s/epoch - 394ms/step
Epoch 924/1000
2023-09-27 15:30:19.249 
Epoch 924/1000 
	 loss: 17.3023, MinusLogProbMetric: 17.3023, val_loss: 17.4956, val_MinusLogProbMetric: 17.4956

Epoch 924: val_loss did not improve from 17.46825
196/196 - 79s - loss: 17.3023 - MinusLogProbMetric: 17.3023 - val_loss: 17.4956 - val_MinusLogProbMetric: 17.4956 - lr: 2.7778e-05 - 79s/epoch - 402ms/step
Epoch 925/1000
2023-09-27 15:31:36.481 
Epoch 925/1000 
	 loss: 17.2845, MinusLogProbMetric: 17.2845, val_loss: 17.6104, val_MinusLogProbMetric: 17.6104

Epoch 925: val_loss did not improve from 17.46825
196/196 - 77s - loss: 17.2845 - MinusLogProbMetric: 17.2845 - val_loss: 17.6104 - val_MinusLogProbMetric: 17.6104 - lr: 2.7778e-05 - 77s/epoch - 394ms/step
Epoch 926/1000
2023-09-27 15:32:54.112 
Epoch 926/1000 
	 loss: 17.3163, MinusLogProbMetric: 17.3163, val_loss: 17.4800, val_MinusLogProbMetric: 17.4800

Epoch 926: val_loss did not improve from 17.46825
196/196 - 78s - loss: 17.3163 - MinusLogProbMetric: 17.3163 - val_loss: 17.4800 - val_MinusLogProbMetric: 17.4800 - lr: 2.7778e-05 - 78s/epoch - 396ms/step
Epoch 927/1000
2023-09-27 15:34:08.930 
Epoch 927/1000 
	 loss: 17.4564, MinusLogProbMetric: 17.4564, val_loss: 17.4820, val_MinusLogProbMetric: 17.4820

Epoch 927: val_loss did not improve from 17.46825
196/196 - 75s - loss: 17.4564 - MinusLogProbMetric: 17.4564 - val_loss: 17.4820 - val_MinusLogProbMetric: 17.4820 - lr: 2.7778e-05 - 75s/epoch - 382ms/step
Epoch 928/1000
2023-09-27 15:35:14.927 
Epoch 928/1000 
	 loss: 17.2861, MinusLogProbMetric: 17.2861, val_loss: 17.4972, val_MinusLogProbMetric: 17.4972

Epoch 928: val_loss did not improve from 17.46825
196/196 - 66s - loss: 17.2861 - MinusLogProbMetric: 17.2861 - val_loss: 17.4972 - val_MinusLogProbMetric: 17.4972 - lr: 2.7778e-05 - 66s/epoch - 337ms/step
Epoch 929/1000
2023-09-27 15:36:23.297 
Epoch 929/1000 
	 loss: 17.2991, MinusLogProbMetric: 17.2991, val_loss: 17.4805, val_MinusLogProbMetric: 17.4805

Epoch 929: val_loss did not improve from 17.46825
196/196 - 68s - loss: 17.2991 - MinusLogProbMetric: 17.2991 - val_loss: 17.4805 - val_MinusLogProbMetric: 17.4805 - lr: 2.7778e-05 - 68s/epoch - 349ms/step
Epoch 930/1000
2023-09-27 15:37:31.883 
Epoch 930/1000 
	 loss: 17.2954, MinusLogProbMetric: 17.2954, val_loss: 17.5191, val_MinusLogProbMetric: 17.5191

Epoch 930: val_loss did not improve from 17.46825
196/196 - 69s - loss: 17.2954 - MinusLogProbMetric: 17.2954 - val_loss: 17.5191 - val_MinusLogProbMetric: 17.5191 - lr: 2.7778e-05 - 69s/epoch - 350ms/step
Epoch 931/1000
2023-09-27 15:38:37.863 
Epoch 931/1000 
	 loss: 17.2804, MinusLogProbMetric: 17.2804, val_loss: 17.6618, val_MinusLogProbMetric: 17.6618

Epoch 931: val_loss did not improve from 17.46825
196/196 - 66s - loss: 17.2804 - MinusLogProbMetric: 17.2804 - val_loss: 17.6618 - val_MinusLogProbMetric: 17.6618 - lr: 2.7778e-05 - 66s/epoch - 337ms/step
Epoch 932/1000
2023-09-27 15:39:53.660 
Epoch 932/1000 
	 loss: 17.2853, MinusLogProbMetric: 17.2853, val_loss: 17.4741, val_MinusLogProbMetric: 17.4741

Epoch 932: val_loss did not improve from 17.46825
196/196 - 76s - loss: 17.2853 - MinusLogProbMetric: 17.2853 - val_loss: 17.4741 - val_MinusLogProbMetric: 17.4741 - lr: 2.7778e-05 - 76s/epoch - 387ms/step
Epoch 933/1000
2023-09-27 15:41:11.451 
Epoch 933/1000 
	 loss: 17.2804, MinusLogProbMetric: 17.2804, val_loss: 17.4846, val_MinusLogProbMetric: 17.4846

Epoch 933: val_loss did not improve from 17.46825
196/196 - 78s - loss: 17.2804 - MinusLogProbMetric: 17.2804 - val_loss: 17.4846 - val_MinusLogProbMetric: 17.4846 - lr: 2.7778e-05 - 78s/epoch - 397ms/step
Epoch 934/1000
2023-09-27 15:42:28.955 
Epoch 934/1000 
	 loss: 17.3086, MinusLogProbMetric: 17.3086, val_loss: 17.4813, val_MinusLogProbMetric: 17.4813

Epoch 934: val_loss did not improve from 17.46825
196/196 - 78s - loss: 17.3086 - MinusLogProbMetric: 17.3086 - val_loss: 17.4813 - val_MinusLogProbMetric: 17.4813 - lr: 2.7778e-05 - 78s/epoch - 395ms/step
Epoch 935/1000
2023-09-27 15:43:46.556 
Epoch 935/1000 
	 loss: 17.2790, MinusLogProbMetric: 17.2790, val_loss: 17.5330, val_MinusLogProbMetric: 17.5330

Epoch 935: val_loss did not improve from 17.46825
196/196 - 78s - loss: 17.2790 - MinusLogProbMetric: 17.2790 - val_loss: 17.5330 - val_MinusLogProbMetric: 17.5330 - lr: 2.7778e-05 - 78s/epoch - 396ms/step
Epoch 936/1000
2023-09-27 15:45:04.134 
Epoch 936/1000 
	 loss: 17.2975, MinusLogProbMetric: 17.2975, val_loss: 17.4885, val_MinusLogProbMetric: 17.4885

Epoch 936: val_loss did not improve from 17.46825
196/196 - 78s - loss: 17.2975 - MinusLogProbMetric: 17.2975 - val_loss: 17.4885 - val_MinusLogProbMetric: 17.4885 - lr: 2.7778e-05 - 78s/epoch - 396ms/step
Epoch 937/1000
2023-09-27 15:46:21.849 
Epoch 937/1000 
	 loss: 17.3018, MinusLogProbMetric: 17.3018, val_loss: 17.5086, val_MinusLogProbMetric: 17.5086

Epoch 937: val_loss did not improve from 17.46825
196/196 - 78s - loss: 17.3018 - MinusLogProbMetric: 17.3018 - val_loss: 17.5086 - val_MinusLogProbMetric: 17.5086 - lr: 2.7778e-05 - 78s/epoch - 396ms/step
Epoch 938/1000
2023-09-27 15:47:39.515 
Epoch 938/1000 
	 loss: 17.2915, MinusLogProbMetric: 17.2915, val_loss: 17.5647, val_MinusLogProbMetric: 17.5647

Epoch 938: val_loss did not improve from 17.46825
196/196 - 78s - loss: 17.2915 - MinusLogProbMetric: 17.2915 - val_loss: 17.5647 - val_MinusLogProbMetric: 17.5647 - lr: 2.7778e-05 - 78s/epoch - 396ms/step
Epoch 939/1000
2023-09-27 15:48:46.388 
Epoch 939/1000 
	 loss: 17.3407, MinusLogProbMetric: 17.3407, val_loss: 17.5759, val_MinusLogProbMetric: 17.5759

Epoch 939: val_loss did not improve from 17.46825
196/196 - 67s - loss: 17.3407 - MinusLogProbMetric: 17.3407 - val_loss: 17.5759 - val_MinusLogProbMetric: 17.5759 - lr: 2.7778e-05 - 67s/epoch - 341ms/step
Epoch 940/1000
2023-09-27 15:49:55.030 
Epoch 940/1000 
	 loss: 17.3002, MinusLogProbMetric: 17.3002, val_loss: 17.5062, val_MinusLogProbMetric: 17.5062

Epoch 940: val_loss did not improve from 17.46825
196/196 - 69s - loss: 17.3002 - MinusLogProbMetric: 17.3002 - val_loss: 17.5062 - val_MinusLogProbMetric: 17.5062 - lr: 2.7778e-05 - 69s/epoch - 350ms/step
Epoch 941/1000
2023-09-27 15:51:03.750 
Epoch 941/1000 
	 loss: 17.2948, MinusLogProbMetric: 17.2948, val_loss: 17.4897, val_MinusLogProbMetric: 17.4897

Epoch 941: val_loss did not improve from 17.46825
196/196 - 69s - loss: 17.2948 - MinusLogProbMetric: 17.2948 - val_loss: 17.4897 - val_MinusLogProbMetric: 17.4897 - lr: 2.7778e-05 - 69s/epoch - 351ms/step
Epoch 942/1000
2023-09-27 15:52:06.987 
Epoch 942/1000 
	 loss: 17.3235, MinusLogProbMetric: 17.3235, val_loss: 17.4944, val_MinusLogProbMetric: 17.4944

Epoch 942: val_loss did not improve from 17.46825
196/196 - 63s - loss: 17.3235 - MinusLogProbMetric: 17.3235 - val_loss: 17.4944 - val_MinusLogProbMetric: 17.4944 - lr: 2.7778e-05 - 63s/epoch - 323ms/step
Epoch 943/1000
2023-09-27 15:53:24.278 
Epoch 943/1000 
	 loss: 17.3209, MinusLogProbMetric: 17.3209, val_loss: 17.5087, val_MinusLogProbMetric: 17.5087

Epoch 943: val_loss did not improve from 17.46825
196/196 - 77s - loss: 17.3209 - MinusLogProbMetric: 17.3209 - val_loss: 17.5087 - val_MinusLogProbMetric: 17.5087 - lr: 2.7778e-05 - 77s/epoch - 394ms/step
Epoch 944/1000
2023-09-27 15:54:38.502 
Epoch 944/1000 
	 loss: 17.2858, MinusLogProbMetric: 17.2858, val_loss: 17.5048, val_MinusLogProbMetric: 17.5048

Epoch 944: val_loss did not improve from 17.46825
196/196 - 74s - loss: 17.2858 - MinusLogProbMetric: 17.2858 - val_loss: 17.5048 - val_MinusLogProbMetric: 17.5048 - lr: 2.7778e-05 - 74s/epoch - 379ms/step
Epoch 945/1000
2023-09-27 15:55:50.457 
Epoch 945/1000 
	 loss: 17.2811, MinusLogProbMetric: 17.2811, val_loss: 17.4727, val_MinusLogProbMetric: 17.4727

Epoch 945: val_loss did not improve from 17.46825
196/196 - 72s - loss: 17.2811 - MinusLogProbMetric: 17.2811 - val_loss: 17.4727 - val_MinusLogProbMetric: 17.4727 - lr: 2.7778e-05 - 72s/epoch - 367ms/step
Epoch 946/1000
2023-09-27 15:57:06.553 
Epoch 946/1000 
	 loss: 17.3214, MinusLogProbMetric: 17.3214, val_loss: 18.3828, val_MinusLogProbMetric: 18.3828

Epoch 946: val_loss did not improve from 17.46825
196/196 - 76s - loss: 17.3214 - MinusLogProbMetric: 17.3214 - val_loss: 18.3828 - val_MinusLogProbMetric: 18.3828 - lr: 2.7778e-05 - 76s/epoch - 388ms/step
Epoch 947/1000
2023-09-27 15:58:24.430 
Epoch 947/1000 
	 loss: 17.3320, MinusLogProbMetric: 17.3320, val_loss: 17.4677, val_MinusLogProbMetric: 17.4677

Epoch 947: val_loss improved from 17.46825 to 17.46769, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 79s - loss: 17.3320 - MinusLogProbMetric: 17.3320 - val_loss: 17.4677 - val_MinusLogProbMetric: 17.4677 - lr: 2.7778e-05 - 79s/epoch - 405ms/step
Epoch 948/1000
2023-09-27 15:59:43.359 
Epoch 948/1000 
	 loss: 17.3675, MinusLogProbMetric: 17.3675, val_loss: 17.4964, val_MinusLogProbMetric: 17.4964

Epoch 948: val_loss did not improve from 17.46769
196/196 - 77s - loss: 17.3675 - MinusLogProbMetric: 17.3675 - val_loss: 17.4964 - val_MinusLogProbMetric: 17.4964 - lr: 2.7778e-05 - 77s/epoch - 395ms/step
Epoch 949/1000
2023-09-27 16:00:54.240 
Epoch 949/1000 
	 loss: 17.3460, MinusLogProbMetric: 17.3460, val_loss: 17.5022, val_MinusLogProbMetric: 17.5022

Epoch 949: val_loss did not improve from 17.46769
196/196 - 71s - loss: 17.3460 - MinusLogProbMetric: 17.3460 - val_loss: 17.5022 - val_MinusLogProbMetric: 17.5022 - lr: 2.7778e-05 - 71s/epoch - 362ms/step
Epoch 950/1000
2023-09-27 16:02:09.226 
Epoch 950/1000 
	 loss: 17.2873, MinusLogProbMetric: 17.2873, val_loss: 19.1193, val_MinusLogProbMetric: 19.1193

Epoch 950: val_loss did not improve from 17.46769
196/196 - 75s - loss: 17.2873 - MinusLogProbMetric: 17.2873 - val_loss: 19.1193 - val_MinusLogProbMetric: 19.1193 - lr: 2.7778e-05 - 75s/epoch - 383ms/step
Epoch 951/1000
2023-09-27 16:03:26.631 
Epoch 951/1000 
	 loss: 17.4695, MinusLogProbMetric: 17.4695, val_loss: 17.4810, val_MinusLogProbMetric: 17.4810

Epoch 951: val_loss did not improve from 17.46769
196/196 - 77s - loss: 17.4695 - MinusLogProbMetric: 17.4695 - val_loss: 17.4810 - val_MinusLogProbMetric: 17.4810 - lr: 2.7778e-05 - 77s/epoch - 395ms/step
Epoch 952/1000
2023-09-27 16:04:44.240 
Epoch 952/1000 
	 loss: 17.2771, MinusLogProbMetric: 17.2771, val_loss: 17.4718, val_MinusLogProbMetric: 17.4718

Epoch 952: val_loss did not improve from 17.46769
196/196 - 78s - loss: 17.2771 - MinusLogProbMetric: 17.2771 - val_loss: 17.4718 - val_MinusLogProbMetric: 17.4718 - lr: 2.7778e-05 - 78s/epoch - 396ms/step
Epoch 953/1000
2023-09-27 16:06:02.254 
Epoch 953/1000 
	 loss: 17.2828, MinusLogProbMetric: 17.2828, val_loss: 17.4749, val_MinusLogProbMetric: 17.4749

Epoch 953: val_loss did not improve from 17.46769
196/196 - 78s - loss: 17.2828 - MinusLogProbMetric: 17.2828 - val_loss: 17.4749 - val_MinusLogProbMetric: 17.4749 - lr: 2.7778e-05 - 78s/epoch - 398ms/step
Epoch 954/1000
2023-09-27 16:07:17.645 
Epoch 954/1000 
	 loss: 17.2853, MinusLogProbMetric: 17.2853, val_loss: 17.5029, val_MinusLogProbMetric: 17.5029

Epoch 954: val_loss did not improve from 17.46769
196/196 - 75s - loss: 17.2853 - MinusLogProbMetric: 17.2853 - val_loss: 17.5029 - val_MinusLogProbMetric: 17.5029 - lr: 2.7778e-05 - 75s/epoch - 385ms/step
Epoch 955/1000
2023-09-27 16:08:29.410 
Epoch 955/1000 
	 loss: 17.2916, MinusLogProbMetric: 17.2916, val_loss: 17.5382, val_MinusLogProbMetric: 17.5382

Epoch 955: val_loss did not improve from 17.46769
196/196 - 72s - loss: 17.2916 - MinusLogProbMetric: 17.2916 - val_loss: 17.5382 - val_MinusLogProbMetric: 17.5382 - lr: 2.7778e-05 - 72s/epoch - 366ms/step
Epoch 956/1000
2023-09-27 16:09:40.842 
Epoch 956/1000 
	 loss: 17.2792, MinusLogProbMetric: 17.2792, val_loss: 17.4987, val_MinusLogProbMetric: 17.4987

Epoch 956: val_loss did not improve from 17.46769
196/196 - 71s - loss: 17.2792 - MinusLogProbMetric: 17.2792 - val_loss: 17.4987 - val_MinusLogProbMetric: 17.4987 - lr: 2.7778e-05 - 71s/epoch - 364ms/step
Epoch 957/1000
2023-09-27 16:10:54.229 
Epoch 957/1000 
	 loss: 17.3784, MinusLogProbMetric: 17.3784, val_loss: 17.5220, val_MinusLogProbMetric: 17.5220

Epoch 957: val_loss did not improve from 17.46769
196/196 - 73s - loss: 17.3784 - MinusLogProbMetric: 17.3784 - val_loss: 17.5220 - val_MinusLogProbMetric: 17.5220 - lr: 2.7778e-05 - 73s/epoch - 374ms/step
Epoch 958/1000
2023-09-27 16:12:08.288 
Epoch 958/1000 
	 loss: 17.3497, MinusLogProbMetric: 17.3497, val_loss: 17.4979, val_MinusLogProbMetric: 17.4979

Epoch 958: val_loss did not improve from 17.46769
196/196 - 74s - loss: 17.3497 - MinusLogProbMetric: 17.3497 - val_loss: 17.4979 - val_MinusLogProbMetric: 17.4979 - lr: 2.7778e-05 - 74s/epoch - 378ms/step
Epoch 959/1000
2023-09-27 16:13:22.764 
Epoch 959/1000 
	 loss: 17.2865, MinusLogProbMetric: 17.2865, val_loss: 17.4920, val_MinusLogProbMetric: 17.4920

Epoch 959: val_loss did not improve from 17.46769
196/196 - 74s - loss: 17.2865 - MinusLogProbMetric: 17.2865 - val_loss: 17.4920 - val_MinusLogProbMetric: 17.4920 - lr: 2.7778e-05 - 74s/epoch - 380ms/step
Epoch 960/1000
2023-09-27 16:14:34.884 
Epoch 960/1000 
	 loss: 17.2919, MinusLogProbMetric: 17.2919, val_loss: 17.4689, val_MinusLogProbMetric: 17.4689

Epoch 960: val_loss did not improve from 17.46769
196/196 - 72s - loss: 17.2919 - MinusLogProbMetric: 17.2919 - val_loss: 17.4689 - val_MinusLogProbMetric: 17.4689 - lr: 2.7778e-05 - 72s/epoch - 368ms/step
Epoch 961/1000
2023-09-27 16:15:47.847 
Epoch 961/1000 
	 loss: 17.3489, MinusLogProbMetric: 17.3489, val_loss: 17.5116, val_MinusLogProbMetric: 17.5116

Epoch 961: val_loss did not improve from 17.46769
196/196 - 73s - loss: 17.3489 - MinusLogProbMetric: 17.3489 - val_loss: 17.5116 - val_MinusLogProbMetric: 17.5116 - lr: 2.7778e-05 - 73s/epoch - 372ms/step
Epoch 962/1000
2023-09-27 16:17:04.781 
Epoch 962/1000 
	 loss: 17.2925, MinusLogProbMetric: 17.2925, val_loss: 17.5094, val_MinusLogProbMetric: 17.5094

Epoch 962: val_loss did not improve from 17.46769
196/196 - 77s - loss: 17.2925 - MinusLogProbMetric: 17.2925 - val_loss: 17.5094 - val_MinusLogProbMetric: 17.5094 - lr: 2.7778e-05 - 77s/epoch - 392ms/step
Epoch 963/1000
2023-09-27 16:18:19.891 
Epoch 963/1000 
	 loss: 17.2880, MinusLogProbMetric: 17.2880, val_loss: 17.5708, val_MinusLogProbMetric: 17.5708

Epoch 963: val_loss did not improve from 17.46769
196/196 - 75s - loss: 17.2880 - MinusLogProbMetric: 17.2880 - val_loss: 17.5708 - val_MinusLogProbMetric: 17.5708 - lr: 2.7778e-05 - 75s/epoch - 383ms/step
Epoch 964/1000
2023-09-27 16:19:28.086 
Epoch 964/1000 
	 loss: 17.2933, MinusLogProbMetric: 17.2933, val_loss: 17.5449, val_MinusLogProbMetric: 17.5449

Epoch 964: val_loss did not improve from 17.46769
196/196 - 68s - loss: 17.2933 - MinusLogProbMetric: 17.2933 - val_loss: 17.5449 - val_MinusLogProbMetric: 17.5449 - lr: 2.7778e-05 - 68s/epoch - 348ms/step
Epoch 965/1000
2023-09-27 16:20:44.322 
Epoch 965/1000 
	 loss: 17.3068, MinusLogProbMetric: 17.3068, val_loss: 17.4800, val_MinusLogProbMetric: 17.4800

Epoch 965: val_loss did not improve from 17.46769
196/196 - 76s - loss: 17.3068 - MinusLogProbMetric: 17.3068 - val_loss: 17.4800 - val_MinusLogProbMetric: 17.4800 - lr: 2.7778e-05 - 76s/epoch - 389ms/step
Epoch 966/1000
2023-09-27 16:21:59.668 
Epoch 966/1000 
	 loss: 17.2796, MinusLogProbMetric: 17.2796, val_loss: 17.4788, val_MinusLogProbMetric: 17.4788

Epoch 966: val_loss did not improve from 17.46769
196/196 - 75s - loss: 17.2796 - MinusLogProbMetric: 17.2796 - val_loss: 17.4788 - val_MinusLogProbMetric: 17.4788 - lr: 2.7778e-05 - 75s/epoch - 384ms/step
Epoch 967/1000
2023-09-27 16:23:07.621 
Epoch 967/1000 
	 loss: 17.2883, MinusLogProbMetric: 17.2883, val_loss: 17.4827, val_MinusLogProbMetric: 17.4827

Epoch 967: val_loss did not improve from 17.46769
196/196 - 68s - loss: 17.2883 - MinusLogProbMetric: 17.2883 - val_loss: 17.4827 - val_MinusLogProbMetric: 17.4827 - lr: 2.7778e-05 - 68s/epoch - 347ms/step
Epoch 968/1000
2023-09-27 16:24:24.581 
Epoch 968/1000 
	 loss: 17.2736, MinusLogProbMetric: 17.2736, val_loss: 17.4863, val_MinusLogProbMetric: 17.4863

Epoch 968: val_loss did not improve from 17.46769
196/196 - 77s - loss: 17.2736 - MinusLogProbMetric: 17.2736 - val_loss: 17.4863 - val_MinusLogProbMetric: 17.4863 - lr: 2.7778e-05 - 77s/epoch - 393ms/step
Epoch 969/1000
2023-09-27 16:25:41.544 
Epoch 969/1000 
	 loss: 17.2909, MinusLogProbMetric: 17.2909, val_loss: 17.5280, val_MinusLogProbMetric: 17.5280

Epoch 969: val_loss did not improve from 17.46769
196/196 - 77s - loss: 17.2909 - MinusLogProbMetric: 17.2909 - val_loss: 17.5280 - val_MinusLogProbMetric: 17.5280 - lr: 2.7778e-05 - 77s/epoch - 393ms/step
Epoch 970/1000
2023-09-27 16:26:59.015 
Epoch 970/1000 
	 loss: 17.3107, MinusLogProbMetric: 17.3107, val_loss: 17.5480, val_MinusLogProbMetric: 17.5480

Epoch 970: val_loss did not improve from 17.46769
196/196 - 77s - loss: 17.3107 - MinusLogProbMetric: 17.3107 - val_loss: 17.5480 - val_MinusLogProbMetric: 17.5480 - lr: 2.7778e-05 - 77s/epoch - 395ms/step
Epoch 971/1000
2023-09-27 16:28:16.741 
Epoch 971/1000 
	 loss: 17.3098, MinusLogProbMetric: 17.3098, val_loss: 17.4789, val_MinusLogProbMetric: 17.4789

Epoch 971: val_loss did not improve from 17.46769
196/196 - 78s - loss: 17.3098 - MinusLogProbMetric: 17.3098 - val_loss: 17.4789 - val_MinusLogProbMetric: 17.4789 - lr: 2.7778e-05 - 78s/epoch - 397ms/step
Epoch 972/1000
2023-09-27 16:29:33.968 
Epoch 972/1000 
	 loss: 17.2776, MinusLogProbMetric: 17.2776, val_loss: 17.4871, val_MinusLogProbMetric: 17.4871

Epoch 972: val_loss did not improve from 17.46769
196/196 - 77s - loss: 17.2776 - MinusLogProbMetric: 17.2776 - val_loss: 17.4871 - val_MinusLogProbMetric: 17.4871 - lr: 2.7778e-05 - 77s/epoch - 394ms/step
Epoch 973/1000
2023-09-27 16:30:50.708 
Epoch 973/1000 
	 loss: 17.2730, MinusLogProbMetric: 17.2730, val_loss: 17.4644, val_MinusLogProbMetric: 17.4644

Epoch 973: val_loss improved from 17.46769 to 17.46444, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 78s - loss: 17.2730 - MinusLogProbMetric: 17.2730 - val_loss: 17.4644 - val_MinusLogProbMetric: 17.4644 - lr: 2.7778e-05 - 78s/epoch - 399ms/step
Epoch 974/1000
2023-09-27 16:31:57.933 
Epoch 974/1000 
	 loss: 17.2808, MinusLogProbMetric: 17.2808, val_loss: 17.4906, val_MinusLogProbMetric: 17.4906

Epoch 974: val_loss did not improve from 17.46444
196/196 - 66s - loss: 17.2808 - MinusLogProbMetric: 17.2808 - val_loss: 17.4906 - val_MinusLogProbMetric: 17.4906 - lr: 2.7778e-05 - 66s/epoch - 335ms/step
Epoch 975/1000
2023-09-27 16:33:11.972 
Epoch 975/1000 
	 loss: 17.2897, MinusLogProbMetric: 17.2897, val_loss: 17.4795, val_MinusLogProbMetric: 17.4795

Epoch 975: val_loss did not improve from 17.46444
196/196 - 74s - loss: 17.2897 - MinusLogProbMetric: 17.2897 - val_loss: 17.4795 - val_MinusLogProbMetric: 17.4795 - lr: 2.7778e-05 - 74s/epoch - 378ms/step
Epoch 976/1000
2023-09-27 16:34:24.882 
Epoch 976/1000 
	 loss: 17.2764, MinusLogProbMetric: 17.2764, val_loss: 17.4920, val_MinusLogProbMetric: 17.4920

Epoch 976: val_loss did not improve from 17.46444
196/196 - 73s - loss: 17.2764 - MinusLogProbMetric: 17.2764 - val_loss: 17.4920 - val_MinusLogProbMetric: 17.4920 - lr: 2.7778e-05 - 73s/epoch - 372ms/step
Epoch 977/1000
2023-09-27 16:35:41.080 
Epoch 977/1000 
	 loss: 17.2813, MinusLogProbMetric: 17.2813, val_loss: 17.4641, val_MinusLogProbMetric: 17.4641

Epoch 977: val_loss improved from 17.46444 to 17.46413, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 78s - loss: 17.2813 - MinusLogProbMetric: 17.2813 - val_loss: 17.4641 - val_MinusLogProbMetric: 17.4641 - lr: 2.7778e-05 - 78s/epoch - 396ms/step
Epoch 978/1000
2023-09-27 16:36:59.830 
Epoch 978/1000 
	 loss: 17.2858, MinusLogProbMetric: 17.2858, val_loss: 17.4975, val_MinusLogProbMetric: 17.4975

Epoch 978: val_loss did not improve from 17.46413
196/196 - 77s - loss: 17.2858 - MinusLogProbMetric: 17.2858 - val_loss: 17.4975 - val_MinusLogProbMetric: 17.4975 - lr: 2.7778e-05 - 77s/epoch - 395ms/step
Epoch 979/1000
2023-09-27 16:38:17.701 
Epoch 979/1000 
	 loss: 17.6945, MinusLogProbMetric: 17.6945, val_loss: 22.0785, val_MinusLogProbMetric: 22.0785

Epoch 979: val_loss did not improve from 17.46413
196/196 - 78s - loss: 17.6945 - MinusLogProbMetric: 17.6945 - val_loss: 22.0785 - val_MinusLogProbMetric: 22.0785 - lr: 2.7778e-05 - 78s/epoch - 397ms/step
Epoch 980/1000
2023-09-27 16:39:35.042 
Epoch 980/1000 
	 loss: 17.7274, MinusLogProbMetric: 17.7274, val_loss: 17.5070, val_MinusLogProbMetric: 17.5070

Epoch 980: val_loss did not improve from 17.46413
196/196 - 77s - loss: 17.7274 - MinusLogProbMetric: 17.7274 - val_loss: 17.5070 - val_MinusLogProbMetric: 17.5070 - lr: 2.7778e-05 - 77s/epoch - 395ms/step
Epoch 981/1000
2023-09-27 16:40:52.485 
Epoch 981/1000 
	 loss: 17.3806, MinusLogProbMetric: 17.3806, val_loss: 17.4941, val_MinusLogProbMetric: 17.4941

Epoch 981: val_loss did not improve from 17.46413
196/196 - 77s - loss: 17.3806 - MinusLogProbMetric: 17.3806 - val_loss: 17.4941 - val_MinusLogProbMetric: 17.4941 - lr: 2.7778e-05 - 77s/epoch - 395ms/step
Epoch 982/1000
2023-09-27 16:42:09.797 
Epoch 982/1000 
	 loss: 17.2750, MinusLogProbMetric: 17.2750, val_loss: 17.4937, val_MinusLogProbMetric: 17.4937

Epoch 982: val_loss did not improve from 17.46413
196/196 - 77s - loss: 17.2750 - MinusLogProbMetric: 17.2750 - val_loss: 17.4937 - val_MinusLogProbMetric: 17.4937 - lr: 2.7778e-05 - 77s/epoch - 394ms/step
Epoch 983/1000
2023-09-27 16:43:27.251 
Epoch 983/1000 
	 loss: 17.2758, MinusLogProbMetric: 17.2758, val_loss: 17.4723, val_MinusLogProbMetric: 17.4723

Epoch 983: val_loss did not improve from 17.46413
196/196 - 77s - loss: 17.2758 - MinusLogProbMetric: 17.2758 - val_loss: 17.4723 - val_MinusLogProbMetric: 17.4723 - lr: 2.7778e-05 - 77s/epoch - 395ms/step
Epoch 984/1000
2023-09-27 16:44:44.130 
Epoch 984/1000 
	 loss: 17.2705, MinusLogProbMetric: 17.2705, val_loss: 17.4679, val_MinusLogProbMetric: 17.4679

Epoch 984: val_loss did not improve from 17.46413
196/196 - 77s - loss: 17.2705 - MinusLogProbMetric: 17.2705 - val_loss: 17.4679 - val_MinusLogProbMetric: 17.4679 - lr: 2.7778e-05 - 77s/epoch - 392ms/step
Epoch 985/1000
2023-09-27 16:46:01.428 
Epoch 985/1000 
	 loss: 17.2732, MinusLogProbMetric: 17.2732, val_loss: 17.4943, val_MinusLogProbMetric: 17.4943

Epoch 985: val_loss did not improve from 17.46413
196/196 - 77s - loss: 17.2732 - MinusLogProbMetric: 17.2732 - val_loss: 17.4943 - val_MinusLogProbMetric: 17.4943 - lr: 2.7778e-05 - 77s/epoch - 394ms/step
Epoch 986/1000
2023-09-27 16:47:18.480 
Epoch 986/1000 
	 loss: 17.2746, MinusLogProbMetric: 17.2746, val_loss: 17.4541, val_MinusLogProbMetric: 17.4541

Epoch 986: val_loss improved from 17.46413 to 17.45410, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 78s - loss: 17.2746 - MinusLogProbMetric: 17.2746 - val_loss: 17.4541 - val_MinusLogProbMetric: 17.4541 - lr: 2.7778e-05 - 78s/epoch - 400ms/step
Epoch 987/1000
2023-09-27 16:48:37.514 
Epoch 987/1000 
	 loss: 17.2748, MinusLogProbMetric: 17.2748, val_loss: 17.4876, val_MinusLogProbMetric: 17.4876

Epoch 987: val_loss did not improve from 17.45410
196/196 - 78s - loss: 17.2748 - MinusLogProbMetric: 17.2748 - val_loss: 17.4876 - val_MinusLogProbMetric: 17.4876 - lr: 2.7778e-05 - 78s/epoch - 396ms/step
Epoch 988/1000
2023-09-27 16:49:54.944 
Epoch 988/1000 
	 loss: 17.2992, MinusLogProbMetric: 17.2992, val_loss: 17.4641, val_MinusLogProbMetric: 17.4641

Epoch 988: val_loss did not improve from 17.45410
196/196 - 77s - loss: 17.2992 - MinusLogProbMetric: 17.2992 - val_loss: 17.4641 - val_MinusLogProbMetric: 17.4641 - lr: 2.7778e-05 - 77s/epoch - 395ms/step
Epoch 989/1000
2023-09-27 16:51:12.594 
Epoch 989/1000 
	 loss: 17.3088, MinusLogProbMetric: 17.3088, val_loss: 17.4765, val_MinusLogProbMetric: 17.4765

Epoch 989: val_loss did not improve from 17.45410
196/196 - 78s - loss: 17.3088 - MinusLogProbMetric: 17.3088 - val_loss: 17.4765 - val_MinusLogProbMetric: 17.4765 - lr: 2.7778e-05 - 78s/epoch - 396ms/step
Epoch 990/1000
2023-09-27 16:52:30.109 
Epoch 990/1000 
	 loss: 17.3567, MinusLogProbMetric: 17.3567, val_loss: 17.6562, val_MinusLogProbMetric: 17.6562

Epoch 990: val_loss did not improve from 17.45410
196/196 - 78s - loss: 17.3567 - MinusLogProbMetric: 17.3567 - val_loss: 17.6562 - val_MinusLogProbMetric: 17.6562 - lr: 2.7778e-05 - 78s/epoch - 395ms/step
Epoch 991/1000
2023-09-27 16:53:47.263 
Epoch 991/1000 
	 loss: 17.2988, MinusLogProbMetric: 17.2988, val_loss: 17.5024, val_MinusLogProbMetric: 17.5024

Epoch 991: val_loss did not improve from 17.45410
196/196 - 77s - loss: 17.2988 - MinusLogProbMetric: 17.2988 - val_loss: 17.5024 - val_MinusLogProbMetric: 17.5024 - lr: 2.7778e-05 - 77s/epoch - 394ms/step
Epoch 992/1000
2023-09-27 16:55:04.954 
Epoch 992/1000 
	 loss: 17.2769, MinusLogProbMetric: 17.2769, val_loss: 17.4863, val_MinusLogProbMetric: 17.4863

Epoch 992: val_loss did not improve from 17.45410
196/196 - 78s - loss: 17.2769 - MinusLogProbMetric: 17.2769 - val_loss: 17.4863 - val_MinusLogProbMetric: 17.4863 - lr: 2.7778e-05 - 78s/epoch - 396ms/step
Epoch 993/1000
2023-09-27 16:56:22.720 
Epoch 993/1000 
	 loss: 17.2746, MinusLogProbMetric: 17.2746, val_loss: 17.4820, val_MinusLogProbMetric: 17.4820

Epoch 993: val_loss did not improve from 17.45410
196/196 - 78s - loss: 17.2746 - MinusLogProbMetric: 17.2746 - val_loss: 17.4820 - val_MinusLogProbMetric: 17.4820 - lr: 2.7778e-05 - 78s/epoch - 397ms/step
Epoch 994/1000
2023-09-27 16:57:39.990 
Epoch 994/1000 
	 loss: 17.3766, MinusLogProbMetric: 17.3766, val_loss: 17.5197, val_MinusLogProbMetric: 17.5197

Epoch 994: val_loss did not improve from 17.45410
196/196 - 77s - loss: 17.3766 - MinusLogProbMetric: 17.3766 - val_loss: 17.5197 - val_MinusLogProbMetric: 17.5197 - lr: 2.7778e-05 - 77s/epoch - 394ms/step
Epoch 995/1000
2023-09-27 16:58:57.290 
Epoch 995/1000 
	 loss: 17.2667, MinusLogProbMetric: 17.2667, val_loss: 17.4805, val_MinusLogProbMetric: 17.4805

Epoch 995: val_loss did not improve from 17.45410
196/196 - 77s - loss: 17.2667 - MinusLogProbMetric: 17.2667 - val_loss: 17.4805 - val_MinusLogProbMetric: 17.4805 - lr: 2.7778e-05 - 77s/epoch - 394ms/step
Epoch 996/1000
2023-09-27 17:00:14.127 
Epoch 996/1000 
	 loss: 17.2720, MinusLogProbMetric: 17.2720, val_loss: 17.4988, val_MinusLogProbMetric: 17.4988

Epoch 996: val_loss did not improve from 17.45410
196/196 - 77s - loss: 17.2720 - MinusLogProbMetric: 17.2720 - val_loss: 17.4988 - val_MinusLogProbMetric: 17.4988 - lr: 2.7778e-05 - 77s/epoch - 392ms/step
Epoch 997/1000
2023-09-27 17:01:31.517 
Epoch 997/1000 
	 loss: 17.2728, MinusLogProbMetric: 17.2728, val_loss: 17.4909, val_MinusLogProbMetric: 17.4909

Epoch 997: val_loss did not improve from 17.45410
196/196 - 77s - loss: 17.2728 - MinusLogProbMetric: 17.2728 - val_loss: 17.4909 - val_MinusLogProbMetric: 17.4909 - lr: 2.7778e-05 - 77s/epoch - 395ms/step
Epoch 998/1000
2023-09-27 17:02:49.110 
Epoch 998/1000 
	 loss: 17.2769, MinusLogProbMetric: 17.2769, val_loss: 17.5000, val_MinusLogProbMetric: 17.5000

Epoch 998: val_loss did not improve from 17.45410
196/196 - 78s - loss: 17.2769 - MinusLogProbMetric: 17.2769 - val_loss: 17.5000 - val_MinusLogProbMetric: 17.5000 - lr: 2.7778e-05 - 78s/epoch - 396ms/step
Epoch 999/1000
2023-09-27 17:04:05.697 
Epoch 999/1000 
	 loss: 17.3001, MinusLogProbMetric: 17.3001, val_loss: 17.5322, val_MinusLogProbMetric: 17.5322

Epoch 999: val_loss did not improve from 17.45410
196/196 - 77s - loss: 17.3001 - MinusLogProbMetric: 17.3001 - val_loss: 17.5322 - val_MinusLogProbMetric: 17.5322 - lr: 2.7778e-05 - 77s/epoch - 391ms/step
Epoch 1000/1000
2023-09-27 17:05:23.070 
Epoch 1000/1000 
	 loss: 17.2784, MinusLogProbMetric: 17.2784, val_loss: 17.4834, val_MinusLogProbMetric: 17.4834

Epoch 1000: val_loss did not improve from 17.45410
196/196 - 77s - loss: 17.2784 - MinusLogProbMetric: 17.2784 - val_loss: 17.4834 - val_MinusLogProbMetric: 17.4834 - lr: 2.7778e-05 - 77s/epoch - 395ms/step
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.

------------------------------------------
Starting LR metric calculation...
Running TF LR calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
Warning: Removed a fraction of alternative samples due to non-finite log probabilities.
Warning: Removed a fraction of alternative samples due to non-finite log probabilities.
Warning: Removed a fraction of alternative samples due to non-finite log probabilities.
Warning: Removed a fraction of alternative samples due to non-finite log probabilities.
Warning: Removed a fraction of alternative samples due to non-finite log probabilities.
Warning: Removed a fraction of alternative samples due to non-finite log probabilities.
Warning: Removed a fraction of alternative samples due to non-finite log probabilities.
Warning: Removed a fraction of alternative samples due to non-finite log probabilities.
Warning: Removed a fraction of alternative samples due to non-finite log probabilities.
LR metric calculation completed in 31.175489582004957 seconds.

------------------------------------------
Starting KS tests calculation...
Running TF KS tests...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
KS tests calculation completed in 15.585181379050482 seconds.

------------------------------------------
Starting SWD metric calculation...
Running TF SWD calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
SWD metric calculation completed in 13.685430126963183 seconds.

------------------------------------------
Starting FN metric calculation...
Running TF FN calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
FN metric calculation completed in 15.877920489001554 seconds.
Training succeeded with seed 721.
Model trained in 77854.26 s.

===========
Computing predictions
===========

Computing metrics...
Metrics computed in 77.99 s.
                ===========
                print("===========
Failed to plot
===========
")
                Exception type: ValueError
                Exception message: Number of rows must be a positive integer, not 0
                Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN.py , Line : 469, Func.Name : prediction_function, Message : Plotters.cornerplotter(X_data_test.numpy(),X_data_nf.numpy(),path_to_results,ndims,norm=True) # type: ignore', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/Plotters.py , Line : 156, Func.Name : cornerplotter, Message : figure=corner.corner(target_samples,color=\'red\',bins=n_bins,labels=[r"%s" % s for s in labels],normalize1d=True)', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/corner.py , Line : 235, Func.Name : corner, Message : fig, axes = pl.subplots(K, K, figsize=(dim, dim))', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/pyplot.py , Line : 1502, Func.Name : subplots, Message : axs = fig.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 905, Func.Name : subplots, Message : gs = self.add_gridspec(nrows, ncols, figure=self, **gridspec_kw)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 1527, Func.Name : add_gridspec, Message : gs = GridSpec(nrows=nrows, ncols=ncols, figure=self, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 379, Func.Name : __init__, Message : super().__init__(nrows, ncols,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 49, Func.Name : __init__, Message : raise ValueError(']
                ===========

results.txt saved
results.json saved
Results log saved
Model predictions computed in 78.21 s.
===========
Run 295/720 done in 78503.38 s.
===========

Directory ../../results/CsplineN_new/run_296/ already exists.
Skipping it.
===========
Run 296/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_297/ already exists.
Skipping it.
===========
Run 297/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_298/ already exists.
Skipping it.
===========
Run 298/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_299/ already exists.
Skipping it.
===========
Run 299/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_300/ already exists.
Skipping it.
===========
Run 300/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_301/ already exists.
Skipping it.
===========
Run 301/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_302/ already exists.
Skipping it.
===========
Run 302/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_303/ already exists.
Skipping it.
===========
Run 303/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_304/ already exists.
Skipping it.
===========
Run 304/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_305/ already exists.
Skipping it.
===========
Run 305/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_306/ already exists.
Skipping it.
===========
Run 306/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_307/ already exists.
Skipping it.
===========
Run 307/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_308/ already exists.
Skipping it.
===========
Run 308/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_309/ already exists.
Skipping it.
===========
Run 309/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_310/ already exists.
Skipping it.
===========
Run 310/720 already exists. Skipping it.
===========

===========
Generating train data for run 311.
===========
Train data generated in 0.25 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_311/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 926}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_311/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[1.5383615 , 4.21851   , 9.147865  , ..., 7.328855  , 2.4535477 ,
        2.0265868 ],
       [1.2026695 , 3.5675488 , 8.490738  , ..., 7.486889  , 2.9732862 ,
        1.6526531 ],
       [5.7808385 , 5.6486273 , 0.85161066, ..., 0.44565296, 6.0336885 ,
        1.4597642 ],
       ...,
       [3.869998  , 5.7079043 , 0.7277752 , ..., 1.4709862 , 7.5385275 ,
        1.2852083 ],
       [2.111317  , 3.3553224 , 7.6223764 , ..., 7.4778986 , 3.6354768 ,
        1.7641349 ],
       [0.83836687, 3.4177158 , 7.4230075 , ..., 7.223582  , 3.3779047 ,
        2.0014815 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_311/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_311
self.data_kwargs: {'seed': 926}
self.x_data: [[1.4696891  3.165189   9.115545   ... 7.2440553  3.018408   2.0407877 ]
 [2.9008398  3.8232424  7.129506   ... 7.411926   2.8866556  1.4983263 ]
 [4.669435   5.457821   0.04349925 ... 0.866001   6.611358   1.4323243 ]
 ...
 [4.261785   5.5317554  0.77893746 ... 0.60558414 5.4824862  1.3011014 ]
 [4.4981847  5.716866   0.72522354 ... 0.9913055  5.7512865  1.5265256 ]
 [5.479814   7.150977   6.3064384  ... 3.1490364  2.6668518  8.143393  ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_49"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_50 (InputLayer)       [(None, 32)]              0         
                                                                 
 log_prob_layer_4 (LogProbLa  (None,)                  1074400   
 yer)                                                            
                                                                 
=================================================================
Total params: 1,074,400
Trainable params: 1,074,400
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_4/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_4'")
self.model: <keras.engine.functional.Functional object at 0x7f7d0a69fc10>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f7d0a375d20>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f7d0a375d20>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f7d0a10f550>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f7d0a13d870>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f7d0a13dde0>, <keras.callbacks.ModelCheckpoint object at 0x7f7d0a13dea0>, <keras.callbacks.EarlyStopping object at 0x7f7d0a13e110>, <keras.callbacks.ReduceLROnPlateau object at 0x7f7d0a13e140>, <keras.callbacks.TerminateOnNaN object at 0x7f7d0a13dd80>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[1.5383615 , 4.21851   , 9.147865  , ..., 7.328855  , 2.4535477 ,
        2.0265868 ],
       [1.2026695 , 3.5675488 , 8.490738  , ..., 7.486889  , 2.9732862 ,
        1.6526531 ],
       [5.7808385 , 5.6486273 , 0.85161066, ..., 0.44565296, 6.0336885 ,
        1.4597642 ],
       ...,
       [3.869998  , 5.7079043 , 0.7277752 , ..., 1.4709862 , 7.5385275 ,
        1.2852083 ],
       [2.111317  , 3.3553224 , 7.6223764 , ..., 7.4778986 , 3.6354768 ,
        1.7641349 ],
       [0.83836687, 3.4177158 , 7.4230075 , ..., 7.223582  , 3.3779047 ,
        2.0014815 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_311/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 311/720 with hyperparameters:
timestamp = 2023-09-27 17:06:51.781664
ndims = 32
seed_train = 926
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1074400
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 1.4696891   3.165189    9.115545    1.2257975   9.89761    -0.04518861
  9.761367    5.046433   10.037439    6.270509    6.6800056   0.37080207
  3.2512977   1.1872      2.5658634   0.9389908   3.4711475   5.1744766
  0.38911742  6.94769     5.6341734   2.616604    4.513363    1.4770697
  4.2721643   9.222334    2.548921    6.22901     1.4673232   7.2440553
  3.018408    2.0407877 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 19: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-27 17:09:38.883 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 3066.8796, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 167s - loss: nan - MinusLogProbMetric: 3066.8796 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 167s/epoch - 852ms/step
The loss history contains NaN values.
Training failed: trying again with seed 638742 and lr 0.0003333333333333333.
===========
Generating train data for run 311.
===========
Train data generated in 0.30 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_311/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 926}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_311/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[1.5383615 , 4.21851   , 9.147865  , ..., 7.328855  , 2.4535477 ,
        2.0265868 ],
       [1.2026695 , 3.5675488 , 8.490738  , ..., 7.486889  , 2.9732862 ,
        1.6526531 ],
       [5.7808385 , 5.6486273 , 0.85161066, ..., 0.44565296, 6.0336885 ,
        1.4597642 ],
       ...,
       [3.869998  , 5.7079043 , 0.7277752 , ..., 1.4709862 , 7.5385275 ,
        1.2852083 ],
       [2.111317  , 3.3553224 , 7.6223764 , ..., 7.4778986 , 3.6354768 ,
        1.7641349 ],
       [0.83836687, 3.4177158 , 7.4230075 , ..., 7.223582  , 3.3779047 ,
        2.0014815 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_311/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_311
self.data_kwargs: {'seed': 926}
self.x_data: [[1.4696891  3.165189   9.115545   ... 7.2440553  3.018408   2.0407877 ]
 [2.9008398  3.8232424  7.129506   ... 7.411926   2.8866556  1.4983263 ]
 [4.669435   5.457821   0.04349925 ... 0.866001   6.611358   1.4323243 ]
 ...
 [4.261785   5.5317554  0.77893746 ... 0.60558414 5.4824862  1.3011014 ]
 [4.4981847  5.716866   0.72522354 ... 0.9913055  5.7512865  1.5265256 ]
 [5.479814   7.150977   6.3064384  ... 3.1490364  2.6668518  8.143393  ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_60"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_61 (InputLayer)       [(None, 32)]              0         
                                                                 
 log_prob_layer_5 (LogProbLa  (None,)                  1074400   
 yer)                                                            
                                                                 
=================================================================
Total params: 1,074,400
Trainable params: 1,074,400
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_5/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_5'")
self.model: <keras.engine.functional.Functional object at 0x7f7d0966f2e0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f7b38482080>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f7b38482080>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f7d092fc610>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f7d091521d0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f7d09152740>, <keras.callbacks.ModelCheckpoint object at 0x7f7d09152800>, <keras.callbacks.EarlyStopping object at 0x7f7d09152a70>, <keras.callbacks.ReduceLROnPlateau object at 0x7f7d09152aa0>, <keras.callbacks.TerminateOnNaN object at 0x7f7d091526e0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[1.5383615 , 4.21851   , 9.147865  , ..., 7.328855  , 2.4535477 ,
        2.0265868 ],
       [1.2026695 , 3.5675488 , 8.490738  , ..., 7.486889  , 2.9732862 ,
        1.6526531 ],
       [5.7808385 , 5.6486273 , 0.85161066, ..., 0.44565296, 6.0336885 ,
        1.4597642 ],
       ...,
       [3.869998  , 5.7079043 , 0.7277752 , ..., 1.4709862 , 7.5385275 ,
        1.2852083 ],
       [2.111317  , 3.3553224 , 7.6223764 , ..., 7.4778986 , 3.6354768 ,
        1.7641349 ],
       [0.83836687, 3.4177158 , 7.4230075 , ..., 7.223582  , 3.3779047 ,
        2.0014815 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_311/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 311/720 with hyperparameters:
timestamp = 2023-09-27 17:09:51.161442
ndims = 32
seed_train = 926
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1074400
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 1.4696891   3.165189    9.115545    1.2257975   9.89761    -0.04518861
  9.761367    5.046433   10.037439    6.270509    6.6800056   0.37080207
  3.2512977   1.1872      2.5658634   0.9389908   3.4711475   5.1744766
  0.38911742  6.94769     5.6341734   2.616604    4.513363    1.4770697
  4.2721643   9.222334    2.548921    6.22901     1.4673232   7.2440553
  3.018408    2.0407877 ]
Epoch 1/1000
2023-09-27 17:13:52.663 
Epoch 1/1000 
	 loss: 1280.4917, MinusLogProbMetric: 1280.4917, val_loss: 464.5739, val_MinusLogProbMetric: 464.5739

Epoch 1: val_loss improved from inf to 464.57394, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 242s - loss: 1280.4917 - MinusLogProbMetric: 1280.4917 - val_loss: 464.5739 - val_MinusLogProbMetric: 464.5739 - lr: 3.3333e-04 - 242s/epoch - 1s/step
Epoch 2/1000
2023-09-27 17:15:14.758 
Epoch 2/1000 
	 loss: 404.2346, MinusLogProbMetric: 404.2346, val_loss: 323.4494, val_MinusLogProbMetric: 323.4494

Epoch 2: val_loss improved from 464.57394 to 323.44943, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 82s - loss: 404.2346 - MinusLogProbMetric: 404.2346 - val_loss: 323.4494 - val_MinusLogProbMetric: 323.4494 - lr: 3.3333e-04 - 82s/epoch - 416ms/step
Epoch 3/1000
2023-09-27 17:16:36.170 
Epoch 3/1000 
	 loss: 319.3554, MinusLogProbMetric: 319.3554, val_loss: 281.9762, val_MinusLogProbMetric: 281.9762

Epoch 3: val_loss improved from 323.44943 to 281.97620, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 82s - loss: 319.3554 - MinusLogProbMetric: 319.3554 - val_loss: 281.9762 - val_MinusLogProbMetric: 281.9762 - lr: 3.3333e-04 - 82s/epoch - 416ms/step
Epoch 4/1000
2023-09-27 17:17:57.734 
Epoch 4/1000 
	 loss: 270.8224, MinusLogProbMetric: 270.8224, val_loss: 347.8183, val_MinusLogProbMetric: 347.8183

Epoch 4: val_loss did not improve from 281.97620
196/196 - 80s - loss: 270.8224 - MinusLogProbMetric: 270.8224 - val_loss: 347.8183 - val_MinusLogProbMetric: 347.8183 - lr: 3.3333e-04 - 80s/epoch - 409ms/step
Epoch 5/1000
2023-09-27 17:19:17.628 
Epoch 5/1000 
	 loss: 285.3484, MinusLogProbMetric: 285.3484, val_loss: 254.7153, val_MinusLogProbMetric: 254.7153

Epoch 5: val_loss improved from 281.97620 to 254.71527, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 81s - loss: 285.3484 - MinusLogProbMetric: 285.3484 - val_loss: 254.7153 - val_MinusLogProbMetric: 254.7153 - lr: 3.3333e-04 - 81s/epoch - 415ms/step
Epoch 6/1000
2023-09-27 17:20:38.651 
Epoch 6/1000 
	 loss: 216.4644, MinusLogProbMetric: 216.4644, val_loss: 217.2476, val_MinusLogProbMetric: 217.2476

Epoch 6: val_loss improved from 254.71527 to 217.24763, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 81s - loss: 216.4644 - MinusLogProbMetric: 216.4644 - val_loss: 217.2476 - val_MinusLogProbMetric: 217.2476 - lr: 3.3333e-04 - 81s/epoch - 412ms/step
Epoch 7/1000
2023-09-27 17:21:59.939 
Epoch 7/1000 
	 loss: 178.9647, MinusLogProbMetric: 178.9647, val_loss: 139.3690, val_MinusLogProbMetric: 139.3690

Epoch 7: val_loss improved from 217.24763 to 139.36902, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 82s - loss: 178.9647 - MinusLogProbMetric: 178.9647 - val_loss: 139.3690 - val_MinusLogProbMetric: 139.3690 - lr: 3.3333e-04 - 82s/epoch - 418ms/step
Epoch 8/1000
2023-09-27 17:23:21.710 
Epoch 8/1000 
	 loss: 126.0378, MinusLogProbMetric: 126.0378, val_loss: 114.8855, val_MinusLogProbMetric: 114.8855

Epoch 8: val_loss improved from 139.36902 to 114.88554, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 81s - loss: 126.0378 - MinusLogProbMetric: 126.0378 - val_loss: 114.8855 - val_MinusLogProbMetric: 114.8855 - lr: 3.3333e-04 - 81s/epoch - 415ms/step
Epoch 9/1000
2023-09-27 17:24:42.809 
Epoch 9/1000 
	 loss: 113.2252, MinusLogProbMetric: 113.2252, val_loss: 104.6500, val_MinusLogProbMetric: 104.6500

Epoch 9: val_loss improved from 114.88554 to 104.64996, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 81s - loss: 113.2252 - MinusLogProbMetric: 113.2252 - val_loss: 104.6500 - val_MinusLogProbMetric: 104.6500 - lr: 3.3333e-04 - 81s/epoch - 413ms/step
Epoch 10/1000
2023-09-27 17:26:03.768 
Epoch 10/1000 
	 loss: 98.4963, MinusLogProbMetric: 98.4963, val_loss: 98.4738, val_MinusLogProbMetric: 98.4738

Epoch 10: val_loss improved from 104.64996 to 98.47385, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 81s - loss: 98.4963 - MinusLogProbMetric: 98.4963 - val_loss: 98.4738 - val_MinusLogProbMetric: 98.4738 - lr: 3.3333e-04 - 81s/epoch - 413ms/step
Epoch 11/1000
2023-09-27 17:27:24.896 
Epoch 11/1000 
	 loss: 90.0410, MinusLogProbMetric: 90.0410, val_loss: 82.4809, val_MinusLogProbMetric: 82.4809

Epoch 11: val_loss improved from 98.47385 to 82.48089, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 81s - loss: 90.0410 - MinusLogProbMetric: 90.0410 - val_loss: 82.4809 - val_MinusLogProbMetric: 82.4809 - lr: 3.3333e-04 - 81s/epoch - 414ms/step
Epoch 12/1000
2023-09-27 17:28:45.600 
Epoch 12/1000 
	 loss: 85.7743, MinusLogProbMetric: 85.7743, val_loss: 103.5203, val_MinusLogProbMetric: 103.5203

Epoch 12: val_loss did not improve from 82.48089
196/196 - 79s - loss: 85.7743 - MinusLogProbMetric: 85.7743 - val_loss: 103.5203 - val_MinusLogProbMetric: 103.5203 - lr: 3.3333e-04 - 79s/epoch - 405ms/step
Epoch 13/1000
2023-09-27 17:30:05.896 
Epoch 13/1000 
	 loss: 83.9584, MinusLogProbMetric: 83.9584, val_loss: 76.2047, val_MinusLogProbMetric: 76.2047

Epoch 13: val_loss improved from 82.48089 to 76.20474, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 82s - loss: 83.9584 - MinusLogProbMetric: 83.9584 - val_loss: 76.2047 - val_MinusLogProbMetric: 76.2047 - lr: 3.3333e-04 - 82s/epoch - 417ms/step
Epoch 14/1000
2023-09-27 17:31:27.155 
Epoch 14/1000 
	 loss: 73.5917, MinusLogProbMetric: 73.5917, val_loss: 67.5334, val_MinusLogProbMetric: 67.5334

Epoch 14: val_loss improved from 76.20474 to 67.53343, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 81s - loss: 73.5917 - MinusLogProbMetric: 73.5917 - val_loss: 67.5334 - val_MinusLogProbMetric: 67.5334 - lr: 3.3333e-04 - 81s/epoch - 414ms/step
Epoch 15/1000
2023-09-27 17:32:48.378 
Epoch 15/1000 
	 loss: 66.1869, MinusLogProbMetric: 66.1869, val_loss: 64.5813, val_MinusLogProbMetric: 64.5813

Epoch 15: val_loss improved from 67.53343 to 64.58134, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 81s - loss: 66.1869 - MinusLogProbMetric: 66.1869 - val_loss: 64.5813 - val_MinusLogProbMetric: 64.5813 - lr: 3.3333e-04 - 81s/epoch - 414ms/step
Epoch 16/1000
2023-09-27 17:34:08.867 
Epoch 16/1000 
	 loss: 62.9145, MinusLogProbMetric: 62.9145, val_loss: 61.0239, val_MinusLogProbMetric: 61.0239

Epoch 16: val_loss improved from 64.58134 to 61.02388, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 81s - loss: 62.9145 - MinusLogProbMetric: 62.9145 - val_loss: 61.0239 - val_MinusLogProbMetric: 61.0239 - lr: 3.3333e-04 - 81s/epoch - 411ms/step
Epoch 17/1000
2023-09-27 17:35:29.881 
Epoch 17/1000 
	 loss: 60.1684, MinusLogProbMetric: 60.1684, val_loss: 62.2490, val_MinusLogProbMetric: 62.2490

Epoch 17: val_loss did not improve from 61.02388
196/196 - 80s - loss: 60.1684 - MinusLogProbMetric: 60.1684 - val_loss: 62.2490 - val_MinusLogProbMetric: 62.2490 - lr: 3.3333e-04 - 80s/epoch - 407ms/step
Epoch 18/1000
2023-09-27 17:36:49.704 
Epoch 18/1000 
	 loss: 57.4561, MinusLogProbMetric: 57.4561, val_loss: 58.3133, val_MinusLogProbMetric: 58.3133

Epoch 18: val_loss improved from 61.02388 to 58.31328, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 81s - loss: 57.4561 - MinusLogProbMetric: 57.4561 - val_loss: 58.3133 - val_MinusLogProbMetric: 58.3133 - lr: 3.3333e-04 - 81s/epoch - 413ms/step
Epoch 19/1000
2023-09-27 17:38:10.476 
Epoch 19/1000 
	 loss: 55.1345, MinusLogProbMetric: 55.1345, val_loss: 54.7632, val_MinusLogProbMetric: 54.7632

Epoch 19: val_loss improved from 58.31328 to 54.76320, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 81s - loss: 55.1345 - MinusLogProbMetric: 55.1345 - val_loss: 54.7632 - val_MinusLogProbMetric: 54.7632 - lr: 3.3333e-04 - 81s/epoch - 412ms/step
Epoch 20/1000
2023-09-27 17:39:31.320 
Epoch 20/1000 
	 loss: 52.3333, MinusLogProbMetric: 52.3333, val_loss: 52.9273, val_MinusLogProbMetric: 52.9273

Epoch 20: val_loss improved from 54.76320 to 52.92735, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 81s - loss: 52.3333 - MinusLogProbMetric: 52.3333 - val_loss: 52.9273 - val_MinusLogProbMetric: 52.9273 - lr: 3.3333e-04 - 81s/epoch - 413ms/step
Epoch 21/1000
2023-09-27 17:40:52.560 
Epoch 21/1000 
	 loss: 50.0847, MinusLogProbMetric: 50.0847, val_loss: 50.4854, val_MinusLogProbMetric: 50.4854

Epoch 21: val_loss improved from 52.92735 to 50.48542, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 81s - loss: 50.0847 - MinusLogProbMetric: 50.0847 - val_loss: 50.4854 - val_MinusLogProbMetric: 50.4854 - lr: 3.3333e-04 - 81s/epoch - 416ms/step
Epoch 22/1000
2023-09-27 17:42:13.732 
Epoch 22/1000 
	 loss: 77.4394, MinusLogProbMetric: 77.4394, val_loss: 61.5358, val_MinusLogProbMetric: 61.5358

Epoch 22: val_loss did not improve from 50.48542
196/196 - 80s - loss: 77.4394 - MinusLogProbMetric: 77.4394 - val_loss: 61.5358 - val_MinusLogProbMetric: 61.5358 - lr: 3.3333e-04 - 80s/epoch - 407ms/step
Epoch 23/1000
2023-09-27 17:43:33.507 
Epoch 23/1000 
	 loss: 58.2547, MinusLogProbMetric: 58.2547, val_loss: 56.0999, val_MinusLogProbMetric: 56.0999

Epoch 23: val_loss did not improve from 50.48542
196/196 - 80s - loss: 58.2547 - MinusLogProbMetric: 58.2547 - val_loss: 56.0999 - val_MinusLogProbMetric: 56.0999 - lr: 3.3333e-04 - 80s/epoch - 407ms/step
Epoch 24/1000
2023-09-27 17:44:53.789 
Epoch 24/1000 
	 loss: 52.5863, MinusLogProbMetric: 52.5863, val_loss: 51.4233, val_MinusLogProbMetric: 51.4233

Epoch 24: val_loss did not improve from 50.48542
196/196 - 80s - loss: 52.5863 - MinusLogProbMetric: 52.5863 - val_loss: 51.4233 - val_MinusLogProbMetric: 51.4233 - lr: 3.3333e-04 - 80s/epoch - 410ms/step
Epoch 25/1000
2023-09-27 17:46:13.640 
Epoch 25/1000 
	 loss: 50.0696, MinusLogProbMetric: 50.0696, val_loss: 48.9383, val_MinusLogProbMetric: 48.9383

Epoch 25: val_loss improved from 50.48542 to 48.93832, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 81s - loss: 50.0696 - MinusLogProbMetric: 50.0696 - val_loss: 48.9383 - val_MinusLogProbMetric: 48.9383 - lr: 3.3333e-04 - 81s/epoch - 413ms/step
Epoch 26/1000
2023-09-27 17:47:34.332 
Epoch 26/1000 
	 loss: 53.9385, MinusLogProbMetric: 53.9385, val_loss: 73.6700, val_MinusLogProbMetric: 73.6700

Epoch 26: val_loss did not improve from 48.93832
196/196 - 80s - loss: 53.9385 - MinusLogProbMetric: 53.9385 - val_loss: 73.6700 - val_MinusLogProbMetric: 73.6700 - lr: 3.3333e-04 - 80s/epoch - 406ms/step
Epoch 27/1000
2023-09-27 17:48:55.132 
Epoch 27/1000 
	 loss: 60.8345, MinusLogProbMetric: 60.8345, val_loss: 51.8718, val_MinusLogProbMetric: 51.8718

Epoch 27: val_loss did not improve from 48.93832
196/196 - 81s - loss: 60.8345 - MinusLogProbMetric: 60.8345 - val_loss: 51.8718 - val_MinusLogProbMetric: 51.8718 - lr: 3.3333e-04 - 81s/epoch - 412ms/step
Epoch 28/1000
2023-09-27 17:50:14.697 
Epoch 28/1000 
	 loss: 59.1077, MinusLogProbMetric: 59.1077, val_loss: 53.0613, val_MinusLogProbMetric: 53.0613

Epoch 28: val_loss did not improve from 48.93832
196/196 - 80s - loss: 59.1077 - MinusLogProbMetric: 59.1077 - val_loss: 53.0613 - val_MinusLogProbMetric: 53.0613 - lr: 3.3333e-04 - 80s/epoch - 406ms/step
Epoch 29/1000
2023-09-27 17:51:34.649 
Epoch 29/1000 
	 loss: 52.3229, MinusLogProbMetric: 52.3229, val_loss: 48.4391, val_MinusLogProbMetric: 48.4391

Epoch 29: val_loss improved from 48.93832 to 48.43909, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 81s - loss: 52.3229 - MinusLogProbMetric: 52.3229 - val_loss: 48.4391 - val_MinusLogProbMetric: 48.4391 - lr: 3.3333e-04 - 81s/epoch - 415ms/step
Epoch 30/1000
2023-09-27 17:52:56.214 
Epoch 30/1000 
	 loss: 47.1529, MinusLogProbMetric: 47.1529, val_loss: 47.9377, val_MinusLogProbMetric: 47.9377

Epoch 30: val_loss improved from 48.43909 to 47.93774, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 82s - loss: 47.1529 - MinusLogProbMetric: 47.1529 - val_loss: 47.9377 - val_MinusLogProbMetric: 47.9377 - lr: 3.3333e-04 - 82s/epoch - 417ms/step
Epoch 31/1000
2023-09-27 17:54:18.189 
Epoch 31/1000 
	 loss: 44.5565, MinusLogProbMetric: 44.5565, val_loss: 42.9432, val_MinusLogProbMetric: 42.9432

Epoch 31: val_loss improved from 47.93774 to 42.94324, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 82s - loss: 44.5565 - MinusLogProbMetric: 44.5565 - val_loss: 42.9432 - val_MinusLogProbMetric: 42.9432 - lr: 3.3333e-04 - 82s/epoch - 418ms/step
Epoch 32/1000
2023-09-27 17:55:39.619 
Epoch 32/1000 
	 loss: 42.8858, MinusLogProbMetric: 42.8858, val_loss: 40.8319, val_MinusLogProbMetric: 40.8319

Epoch 32: val_loss improved from 42.94324 to 40.83191, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 81s - loss: 42.8858 - MinusLogProbMetric: 42.8858 - val_loss: 40.8319 - val_MinusLogProbMetric: 40.8319 - lr: 3.3333e-04 - 81s/epoch - 416ms/step
Epoch 33/1000
2023-09-27 17:57:00.496 
Epoch 33/1000 
	 loss: 40.6480, MinusLogProbMetric: 40.6480, val_loss: 40.8660, val_MinusLogProbMetric: 40.8660

Epoch 33: val_loss did not improve from 40.83191
196/196 - 80s - loss: 40.6480 - MinusLogProbMetric: 40.6480 - val_loss: 40.8660 - val_MinusLogProbMetric: 40.8660 - lr: 3.3333e-04 - 80s/epoch - 406ms/step
Epoch 34/1000
2023-09-27 17:58:20.465 
Epoch 34/1000 
	 loss: 40.1752, MinusLogProbMetric: 40.1752, val_loss: 38.1725, val_MinusLogProbMetric: 38.1725

Epoch 34: val_loss improved from 40.83191 to 38.17248, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 81s - loss: 40.1752 - MinusLogProbMetric: 40.1752 - val_loss: 38.1725 - val_MinusLogProbMetric: 38.1725 - lr: 3.3333e-04 - 81s/epoch - 415ms/step
Epoch 35/1000
2023-09-27 17:59:42.251 
Epoch 35/1000 
	 loss: 39.7727, MinusLogProbMetric: 39.7727, val_loss: 38.7419, val_MinusLogProbMetric: 38.7419

Epoch 35: val_loss did not improve from 38.17248
196/196 - 80s - loss: 39.7727 - MinusLogProbMetric: 39.7727 - val_loss: 38.7419 - val_MinusLogProbMetric: 38.7419 - lr: 3.3333e-04 - 80s/epoch - 411ms/step
Epoch 36/1000
2023-09-27 18:01:02.849 
Epoch 36/1000 
	 loss: 37.9702, MinusLogProbMetric: 37.9702, val_loss: 37.2346, val_MinusLogProbMetric: 37.2346

Epoch 36: val_loss improved from 38.17248 to 37.23463, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 82s - loss: 37.9702 - MinusLogProbMetric: 37.9702 - val_loss: 37.2346 - val_MinusLogProbMetric: 37.2346 - lr: 3.3333e-04 - 82s/epoch - 417ms/step
Epoch 37/1000
2023-09-27 18:02:24.140 
Epoch 37/1000 
	 loss: 37.4118, MinusLogProbMetric: 37.4118, val_loss: 35.8759, val_MinusLogProbMetric: 35.8759

Epoch 37: val_loss improved from 37.23463 to 35.87590, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 81s - loss: 37.4118 - MinusLogProbMetric: 37.4118 - val_loss: 35.8759 - val_MinusLogProbMetric: 35.8759 - lr: 3.3333e-04 - 81s/epoch - 415ms/step
Epoch 38/1000
2023-09-27 18:03:45.278 
Epoch 38/1000 
	 loss: 35.6973, MinusLogProbMetric: 35.6973, val_loss: 36.3256, val_MinusLogProbMetric: 36.3256

Epoch 38: val_loss did not improve from 35.87590
196/196 - 80s - loss: 35.6973 - MinusLogProbMetric: 35.6973 - val_loss: 36.3256 - val_MinusLogProbMetric: 36.3256 - lr: 3.3333e-04 - 80s/epoch - 407ms/step
Epoch 39/1000
2023-09-27 18:05:04.964 
Epoch 39/1000 
	 loss: 35.7170, MinusLogProbMetric: 35.7170, val_loss: 36.3519, val_MinusLogProbMetric: 36.3519

Epoch 39: val_loss did not improve from 35.87590
196/196 - 80s - loss: 35.7170 - MinusLogProbMetric: 35.7170 - val_loss: 36.3519 - val_MinusLogProbMetric: 36.3519 - lr: 3.3333e-04 - 80s/epoch - 407ms/step
Epoch 40/1000
2023-09-27 18:06:24.820 
Epoch 40/1000 
	 loss: 34.2860, MinusLogProbMetric: 34.2860, val_loss: 34.2324, val_MinusLogProbMetric: 34.2324

Epoch 40: val_loss improved from 35.87590 to 34.23236, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 81s - loss: 34.2860 - MinusLogProbMetric: 34.2860 - val_loss: 34.2324 - val_MinusLogProbMetric: 34.2324 - lr: 3.3333e-04 - 81s/epoch - 415ms/step
Epoch 41/1000
2023-09-27 18:07:45.761 
Epoch 41/1000 
	 loss: 33.6454, MinusLogProbMetric: 33.6454, val_loss: 32.9088, val_MinusLogProbMetric: 32.9088

Epoch 41: val_loss improved from 34.23236 to 32.90883, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 81s - loss: 33.6454 - MinusLogProbMetric: 33.6454 - val_loss: 32.9088 - val_MinusLogProbMetric: 32.9088 - lr: 3.3333e-04 - 81s/epoch - 412ms/step
Epoch 42/1000
2023-09-27 18:09:06.714 
Epoch 42/1000 
	 loss: 33.0160, MinusLogProbMetric: 33.0160, val_loss: 31.8274, val_MinusLogProbMetric: 31.8274

Epoch 42: val_loss improved from 32.90883 to 31.82740, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 81s - loss: 33.0160 - MinusLogProbMetric: 33.0160 - val_loss: 31.8274 - val_MinusLogProbMetric: 31.8274 - lr: 3.3333e-04 - 81s/epoch - 414ms/step
Epoch 43/1000
2023-09-27 18:10:28.110 
Epoch 43/1000 
	 loss: 32.6943, MinusLogProbMetric: 32.6943, val_loss: 33.2167, val_MinusLogProbMetric: 33.2167

Epoch 43: val_loss did not improve from 31.82740
196/196 - 80s - loss: 32.6943 - MinusLogProbMetric: 32.6943 - val_loss: 33.2167 - val_MinusLogProbMetric: 33.2167 - lr: 3.3333e-04 - 80s/epoch - 408ms/step
Epoch 44/1000
2023-09-27 18:11:47.922 
Epoch 44/1000 
	 loss: 32.3557, MinusLogProbMetric: 32.3557, val_loss: 31.2928, val_MinusLogProbMetric: 31.2928

Epoch 44: val_loss improved from 31.82740 to 31.29285, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 81s - loss: 32.3557 - MinusLogProbMetric: 32.3557 - val_loss: 31.2928 - val_MinusLogProbMetric: 31.2928 - lr: 3.3333e-04 - 81s/epoch - 413ms/step
Epoch 45/1000
2023-09-27 18:13:08.938 
Epoch 45/1000 
	 loss: 31.2394, MinusLogProbMetric: 31.2394, val_loss: 31.3499, val_MinusLogProbMetric: 31.3499

Epoch 45: val_loss did not improve from 31.29285
196/196 - 80s - loss: 31.2394 - MinusLogProbMetric: 31.2394 - val_loss: 31.3499 - val_MinusLogProbMetric: 31.3499 - lr: 3.3333e-04 - 80s/epoch - 407ms/step
Epoch 46/1000
2023-09-27 18:14:28.942 
Epoch 46/1000 
	 loss: 31.1337, MinusLogProbMetric: 31.1337, val_loss: 32.2008, val_MinusLogProbMetric: 32.2008

Epoch 46: val_loss did not improve from 31.29285
196/196 - 80s - loss: 31.1337 - MinusLogProbMetric: 31.1337 - val_loss: 32.2008 - val_MinusLogProbMetric: 32.2008 - lr: 3.3333e-04 - 80s/epoch - 408ms/step
Epoch 47/1000
2023-09-27 18:15:48.708 
Epoch 47/1000 
	 loss: 30.2324, MinusLogProbMetric: 30.2324, val_loss: 30.0997, val_MinusLogProbMetric: 30.0997

Epoch 47: val_loss improved from 31.29285 to 30.09970, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 81s - loss: 30.2324 - MinusLogProbMetric: 30.2324 - val_loss: 30.0997 - val_MinusLogProbMetric: 30.0997 - lr: 3.3333e-04 - 81s/epoch - 413ms/step
Epoch 48/1000
2023-09-27 18:17:10.021 
Epoch 48/1000 
	 loss: 31.1235, MinusLogProbMetric: 31.1235, val_loss: 34.3787, val_MinusLogProbMetric: 34.3787

Epoch 48: val_loss did not improve from 30.09970
196/196 - 80s - loss: 31.1235 - MinusLogProbMetric: 31.1235 - val_loss: 34.3787 - val_MinusLogProbMetric: 34.3787 - lr: 3.3333e-04 - 80s/epoch - 408ms/step
Epoch 49/1000
2023-09-27 18:18:30.262 
Epoch 49/1000 
	 loss: 30.9663, MinusLogProbMetric: 30.9663, val_loss: 30.6104, val_MinusLogProbMetric: 30.6104

Epoch 49: val_loss did not improve from 30.09970
196/196 - 80s - loss: 30.9663 - MinusLogProbMetric: 30.9663 - val_loss: 30.6104 - val_MinusLogProbMetric: 30.6104 - lr: 3.3333e-04 - 80s/epoch - 409ms/step
Epoch 50/1000
2023-09-27 18:19:50.695 
Epoch 50/1000 
	 loss: 29.4211, MinusLogProbMetric: 29.4211, val_loss: 29.3420, val_MinusLogProbMetric: 29.3420

Epoch 50: val_loss improved from 30.09970 to 29.34203, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 82s - loss: 29.4211 - MinusLogProbMetric: 29.4211 - val_loss: 29.3420 - val_MinusLogProbMetric: 29.3420 - lr: 3.3333e-04 - 82s/epoch - 417ms/step
Epoch 51/1000
2023-09-27 18:21:12.203 
Epoch 51/1000 
	 loss: 29.6123, MinusLogProbMetric: 29.6123, val_loss: 28.6595, val_MinusLogProbMetric: 28.6595

Epoch 51: val_loss improved from 29.34203 to 28.65950, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 82s - loss: 29.6123 - MinusLogProbMetric: 29.6123 - val_loss: 28.6595 - val_MinusLogProbMetric: 28.6595 - lr: 3.3333e-04 - 82s/epoch - 417ms/step
Epoch 52/1000
2023-09-27 18:22:33.542 
Epoch 52/1000 
	 loss: 28.7718, MinusLogProbMetric: 28.7718, val_loss: 28.6284, val_MinusLogProbMetric: 28.6284

Epoch 52: val_loss improved from 28.65950 to 28.62843, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 81s - loss: 28.7718 - MinusLogProbMetric: 28.7718 - val_loss: 28.6284 - val_MinusLogProbMetric: 28.6284 - lr: 3.3333e-04 - 81s/epoch - 415ms/step
Epoch 53/1000
2023-09-27 18:23:55.649 
Epoch 53/1000 
	 loss: 29.5024, MinusLogProbMetric: 29.5024, val_loss: 28.7997, val_MinusLogProbMetric: 28.7997

Epoch 53: val_loss did not improve from 28.62843
196/196 - 81s - loss: 29.5024 - MinusLogProbMetric: 29.5024 - val_loss: 28.7997 - val_MinusLogProbMetric: 28.7997 - lr: 3.3333e-04 - 81s/epoch - 412ms/step
Epoch 54/1000
2023-09-27 18:25:16.464 
Epoch 54/1000 
	 loss: 28.2686, MinusLogProbMetric: 28.2686, val_loss: 28.4664, val_MinusLogProbMetric: 28.4664

Epoch 54: val_loss improved from 28.62843 to 28.46642, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 82s - loss: 28.2686 - MinusLogProbMetric: 28.2686 - val_loss: 28.4664 - val_MinusLogProbMetric: 28.4664 - lr: 3.3333e-04 - 82s/epoch - 418ms/step
Epoch 55/1000
2023-09-27 18:26:38.061 
Epoch 55/1000 
	 loss: 27.9386, MinusLogProbMetric: 27.9386, val_loss: 29.3217, val_MinusLogProbMetric: 29.3217

Epoch 55: val_loss did not improve from 28.46642
196/196 - 80s - loss: 27.9386 - MinusLogProbMetric: 27.9386 - val_loss: 29.3217 - val_MinusLogProbMetric: 29.3217 - lr: 3.3333e-04 - 80s/epoch - 410ms/step
Epoch 56/1000
2023-09-27 18:27:58.124 
Epoch 56/1000 
	 loss: 27.7068, MinusLogProbMetric: 27.7068, val_loss: 27.3314, val_MinusLogProbMetric: 27.3314

Epoch 56: val_loss improved from 28.46642 to 27.33145, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 81s - loss: 27.7068 - MinusLogProbMetric: 27.7068 - val_loss: 27.3314 - val_MinusLogProbMetric: 27.3314 - lr: 3.3333e-04 - 81s/epoch - 415ms/step
Epoch 57/1000
2023-09-27 18:29:19.520 
Epoch 57/1000 
	 loss: 28.1298, MinusLogProbMetric: 28.1298, val_loss: 29.1545, val_MinusLogProbMetric: 29.1545

Epoch 57: val_loss did not improve from 27.33145
196/196 - 80s - loss: 28.1298 - MinusLogProbMetric: 28.1298 - val_loss: 29.1545 - val_MinusLogProbMetric: 29.1545 - lr: 3.3333e-04 - 80s/epoch - 409ms/step
Epoch 58/1000
2023-09-27 18:30:39.842 
Epoch 58/1000 
	 loss: 27.2525, MinusLogProbMetric: 27.2525, val_loss: 29.9135, val_MinusLogProbMetric: 29.9135

Epoch 58: val_loss did not improve from 27.33145
196/196 - 80s - loss: 27.2525 - MinusLogProbMetric: 27.2525 - val_loss: 29.9135 - val_MinusLogProbMetric: 29.9135 - lr: 3.3333e-04 - 80s/epoch - 410ms/step
Epoch 59/1000
2023-09-27 18:32:00.367 
Epoch 59/1000 
	 loss: 27.1621, MinusLogProbMetric: 27.1621, val_loss: 27.0122, val_MinusLogProbMetric: 27.0122

Epoch 59: val_loss improved from 27.33145 to 27.01217, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 82s - loss: 27.1621 - MinusLogProbMetric: 27.1621 - val_loss: 27.0122 - val_MinusLogProbMetric: 27.0122 - lr: 3.3333e-04 - 82s/epoch - 417ms/step
Epoch 60/1000
2023-09-27 18:33:21.065 
Epoch 60/1000 
	 loss: 27.1230, MinusLogProbMetric: 27.1230, val_loss: 26.7503, val_MinusLogProbMetric: 26.7503

Epoch 60: val_loss improved from 27.01217 to 26.75030, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 81s - loss: 27.1230 - MinusLogProbMetric: 27.1230 - val_loss: 26.7503 - val_MinusLogProbMetric: 26.7503 - lr: 3.3333e-04 - 81s/epoch - 411ms/step
Epoch 61/1000
2023-09-27 18:34:36.838 
Epoch 61/1000 
	 loss: 26.5655, MinusLogProbMetric: 26.5655, val_loss: 26.8733, val_MinusLogProbMetric: 26.8733

Epoch 61: val_loss did not improve from 26.75030
196/196 - 75s - loss: 26.5655 - MinusLogProbMetric: 26.5655 - val_loss: 26.8733 - val_MinusLogProbMetric: 26.8733 - lr: 3.3333e-04 - 75s/epoch - 381ms/step
Epoch 62/1000
2023-09-27 18:35:45.534 
Epoch 62/1000 
	 loss: 26.4742, MinusLogProbMetric: 26.4742, val_loss: 26.0271, val_MinusLogProbMetric: 26.0271

Epoch 62: val_loss improved from 26.75030 to 26.02710, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 70s - loss: 26.4742 - MinusLogProbMetric: 26.4742 - val_loss: 26.0271 - val_MinusLogProbMetric: 26.0271 - lr: 3.3333e-04 - 70s/epoch - 356ms/step
Epoch 63/1000
2023-09-27 18:37:02.399 
Epoch 63/1000 
	 loss: 26.5800, MinusLogProbMetric: 26.5800, val_loss: 26.5886, val_MinusLogProbMetric: 26.5886

Epoch 63: val_loss did not improve from 26.02710
196/196 - 76s - loss: 26.5800 - MinusLogProbMetric: 26.5800 - val_loss: 26.5886 - val_MinusLogProbMetric: 26.5886 - lr: 3.3333e-04 - 76s/epoch - 386ms/step
Epoch 64/1000
2023-09-27 18:38:13.138 
Epoch 64/1000 
	 loss: 26.3399, MinusLogProbMetric: 26.3399, val_loss: 26.6241, val_MinusLogProbMetric: 26.6241

Epoch 64: val_loss did not improve from 26.02710
196/196 - 71s - loss: 26.3399 - MinusLogProbMetric: 26.3399 - val_loss: 26.6241 - val_MinusLogProbMetric: 26.6241 - lr: 3.3333e-04 - 71s/epoch - 361ms/step
Epoch 65/1000
2023-09-27 18:39:21.651 
Epoch 65/1000 
	 loss: 26.3964, MinusLogProbMetric: 26.3964, val_loss: 26.3728, val_MinusLogProbMetric: 26.3728

Epoch 65: val_loss did not improve from 26.02710
196/196 - 69s - loss: 26.3964 - MinusLogProbMetric: 26.3964 - val_loss: 26.3728 - val_MinusLogProbMetric: 26.3728 - lr: 3.3333e-04 - 69s/epoch - 350ms/step
Epoch 66/1000
2023-09-27 18:40:40.820 
Epoch 66/1000 
	 loss: 26.3986, MinusLogProbMetric: 26.3986, val_loss: 31.7098, val_MinusLogProbMetric: 31.7098

Epoch 66: val_loss did not improve from 26.02710
196/196 - 79s - loss: 26.3986 - MinusLogProbMetric: 26.3986 - val_loss: 31.7098 - val_MinusLogProbMetric: 31.7098 - lr: 3.3333e-04 - 79s/epoch - 404ms/step
Epoch 67/1000
2023-09-27 18:41:59.770 
Epoch 67/1000 
	 loss: 26.2077, MinusLogProbMetric: 26.2077, val_loss: 25.9799, val_MinusLogProbMetric: 25.9799

Epoch 67: val_loss improved from 26.02710 to 25.97990, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 80s - loss: 26.2077 - MinusLogProbMetric: 26.2077 - val_loss: 25.9799 - val_MinusLogProbMetric: 25.9799 - lr: 3.3333e-04 - 80s/epoch - 409ms/step
Epoch 68/1000
2023-09-27 18:43:20.047 
Epoch 68/1000 
	 loss: 25.6115, MinusLogProbMetric: 25.6115, val_loss: 26.2059, val_MinusLogProbMetric: 26.2059

Epoch 68: val_loss did not improve from 25.97990
196/196 - 79s - loss: 25.6115 - MinusLogProbMetric: 25.6115 - val_loss: 26.2059 - val_MinusLogProbMetric: 26.2059 - lr: 3.3333e-04 - 79s/epoch - 403ms/step
Epoch 69/1000
2023-09-27 18:44:39.601 
Epoch 69/1000 
	 loss: 25.9688, MinusLogProbMetric: 25.9688, val_loss: 26.9361, val_MinusLogProbMetric: 26.9361

Epoch 69: val_loss did not improve from 25.97990
196/196 - 80s - loss: 25.9688 - MinusLogProbMetric: 25.9688 - val_loss: 26.9361 - val_MinusLogProbMetric: 26.9361 - lr: 3.3333e-04 - 80s/epoch - 406ms/step
Epoch 70/1000
2023-09-27 18:45:58.992 
Epoch 70/1000 
	 loss: 25.5535, MinusLogProbMetric: 25.5535, val_loss: 25.4169, val_MinusLogProbMetric: 25.4169

Epoch 70: val_loss improved from 25.97990 to 25.41692, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 81s - loss: 25.5535 - MinusLogProbMetric: 25.5535 - val_loss: 25.4169 - val_MinusLogProbMetric: 25.4169 - lr: 3.3333e-04 - 81s/epoch - 412ms/step
Epoch 71/1000
2023-09-27 18:47:19.173 
Epoch 71/1000 
	 loss: 25.2771, MinusLogProbMetric: 25.2771, val_loss: 25.7436, val_MinusLogProbMetric: 25.7436

Epoch 71: val_loss did not improve from 25.41692
196/196 - 79s - loss: 25.2771 - MinusLogProbMetric: 25.2771 - val_loss: 25.7436 - val_MinusLogProbMetric: 25.7436 - lr: 3.3333e-04 - 79s/epoch - 402ms/step
Epoch 72/1000
2023-09-27 18:48:38.327 
Epoch 72/1000 
	 loss: 25.2415, MinusLogProbMetric: 25.2415, val_loss: 25.2248, val_MinusLogProbMetric: 25.2248

Epoch 72: val_loss improved from 25.41692 to 25.22484, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 81s - loss: 25.2415 - MinusLogProbMetric: 25.2415 - val_loss: 25.2248 - val_MinusLogProbMetric: 25.2248 - lr: 3.3333e-04 - 81s/epoch - 411ms/step
Epoch 73/1000
2023-09-27 18:49:59.397 
Epoch 73/1000 
	 loss: 25.2946, MinusLogProbMetric: 25.2946, val_loss: 25.0026, val_MinusLogProbMetric: 25.0026

Epoch 73: val_loss improved from 25.22484 to 25.00263, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 81s - loss: 25.2946 - MinusLogProbMetric: 25.2946 - val_loss: 25.0026 - val_MinusLogProbMetric: 25.0026 - lr: 3.3333e-04 - 81s/epoch - 413ms/step
Epoch 74/1000
2023-09-27 18:51:19.985 
Epoch 74/1000 
	 loss: 25.1763, MinusLogProbMetric: 25.1763, val_loss: 25.2372, val_MinusLogProbMetric: 25.2372

Epoch 74: val_loss did not improve from 25.00263
196/196 - 79s - loss: 25.1763 - MinusLogProbMetric: 25.1763 - val_loss: 25.2372 - val_MinusLogProbMetric: 25.2372 - lr: 3.3333e-04 - 79s/epoch - 404ms/step
Epoch 75/1000
2023-09-27 18:52:39.309 
Epoch 75/1000 
	 loss: 25.2025, MinusLogProbMetric: 25.2025, val_loss: 25.8059, val_MinusLogProbMetric: 25.8059

Epoch 75: val_loss did not improve from 25.00263
196/196 - 79s - loss: 25.2025 - MinusLogProbMetric: 25.2025 - val_loss: 25.8059 - val_MinusLogProbMetric: 25.8059 - lr: 3.3333e-04 - 79s/epoch - 405ms/step
Epoch 76/1000
2023-09-27 18:53:57.977 
Epoch 76/1000 
	 loss: 24.7400, MinusLogProbMetric: 24.7400, val_loss: 24.8729, val_MinusLogProbMetric: 24.8729

Epoch 76: val_loss improved from 25.00263 to 24.87295, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 80s - loss: 24.7400 - MinusLogProbMetric: 24.7400 - val_loss: 24.8729 - val_MinusLogProbMetric: 24.8729 - lr: 3.3333e-04 - 80s/epoch - 408ms/step
Epoch 77/1000
2023-09-27 18:55:18.690 
Epoch 77/1000 
	 loss: 24.8480, MinusLogProbMetric: 24.8480, val_loss: 25.5276, val_MinusLogProbMetric: 25.5276

Epoch 77: val_loss did not improve from 24.87295
196/196 - 79s - loss: 24.8480 - MinusLogProbMetric: 24.8480 - val_loss: 25.5276 - val_MinusLogProbMetric: 25.5276 - lr: 3.3333e-04 - 79s/epoch - 405ms/step
Epoch 78/1000
2023-09-27 18:56:37.993 
Epoch 78/1000 
	 loss: 24.6252, MinusLogProbMetric: 24.6252, val_loss: 24.7494, val_MinusLogProbMetric: 24.7494

Epoch 78: val_loss improved from 24.87295 to 24.74937, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 80s - loss: 24.6252 - MinusLogProbMetric: 24.6252 - val_loss: 24.7494 - val_MinusLogProbMetric: 24.7494 - lr: 3.3333e-04 - 80s/epoch - 410ms/step
Epoch 79/1000
2023-09-27 18:57:58.088 
Epoch 79/1000 
	 loss: 24.6589, MinusLogProbMetric: 24.6589, val_loss: 24.4771, val_MinusLogProbMetric: 24.4771

Epoch 79: val_loss improved from 24.74937 to 24.47715, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 80s - loss: 24.6589 - MinusLogProbMetric: 24.6589 - val_loss: 24.4771 - val_MinusLogProbMetric: 24.4771 - lr: 3.3333e-04 - 80s/epoch - 409ms/step
Epoch 80/1000
2023-09-27 18:59:18.295 
Epoch 80/1000 
	 loss: 24.4007, MinusLogProbMetric: 24.4007, val_loss: 24.9114, val_MinusLogProbMetric: 24.9114

Epoch 80: val_loss did not improve from 24.47715
196/196 - 79s - loss: 24.4007 - MinusLogProbMetric: 24.4007 - val_loss: 24.9114 - val_MinusLogProbMetric: 24.9114 - lr: 3.3333e-04 - 79s/epoch - 403ms/step
Epoch 81/1000
2023-09-27 19:00:37.234 
Epoch 81/1000 
	 loss: 24.4145, MinusLogProbMetric: 24.4145, val_loss: 24.6667, val_MinusLogProbMetric: 24.6667

Epoch 81: val_loss did not improve from 24.47715
196/196 - 79s - loss: 24.4145 - MinusLogProbMetric: 24.4145 - val_loss: 24.6667 - val_MinusLogProbMetric: 24.6667 - lr: 3.3333e-04 - 79s/epoch - 403ms/step
Epoch 82/1000
2023-09-27 19:01:55.979 
Epoch 82/1000 
	 loss: 24.2120, MinusLogProbMetric: 24.2120, val_loss: 24.3813, val_MinusLogProbMetric: 24.3813

Epoch 82: val_loss improved from 24.47715 to 24.38126, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 80s - loss: 24.2120 - MinusLogProbMetric: 24.2120 - val_loss: 24.3813 - val_MinusLogProbMetric: 24.3813 - lr: 3.3333e-04 - 80s/epoch - 408ms/step
Epoch 83/1000
2023-09-27 19:03:16.507 
Epoch 83/1000 
	 loss: 23.9520, MinusLogProbMetric: 23.9520, val_loss: 24.9152, val_MinusLogProbMetric: 24.9152

Epoch 83: val_loss did not improve from 24.38126
196/196 - 79s - loss: 23.9520 - MinusLogProbMetric: 23.9520 - val_loss: 24.9152 - val_MinusLogProbMetric: 24.9152 - lr: 3.3333e-04 - 79s/epoch - 404ms/step
Epoch 84/1000
2023-09-27 19:04:34.931 
Epoch 84/1000 
	 loss: 24.0435, MinusLogProbMetric: 24.0435, val_loss: 23.7388, val_MinusLogProbMetric: 23.7388

Epoch 84: val_loss improved from 24.38126 to 23.73882, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 80s - loss: 24.0435 - MinusLogProbMetric: 24.0435 - val_loss: 23.7388 - val_MinusLogProbMetric: 23.7388 - lr: 3.3333e-04 - 80s/epoch - 407ms/step
Epoch 85/1000
2023-09-27 19:05:55.465 
Epoch 85/1000 
	 loss: 23.7739, MinusLogProbMetric: 23.7739, val_loss: 24.0913, val_MinusLogProbMetric: 24.0913

Epoch 85: val_loss did not improve from 23.73882
196/196 - 79s - loss: 23.7739 - MinusLogProbMetric: 23.7739 - val_loss: 24.0913 - val_MinusLogProbMetric: 24.0913 - lr: 3.3333e-04 - 79s/epoch - 404ms/step
Epoch 86/1000
2023-09-27 19:07:14.768 
Epoch 86/1000 
	 loss: 24.2732, MinusLogProbMetric: 24.2732, val_loss: 24.3699, val_MinusLogProbMetric: 24.3699

Epoch 86: val_loss did not improve from 23.73882
196/196 - 79s - loss: 24.2732 - MinusLogProbMetric: 24.2732 - val_loss: 24.3699 - val_MinusLogProbMetric: 24.3699 - lr: 3.3333e-04 - 79s/epoch - 405ms/step
Epoch 87/1000
2023-09-27 19:08:33.786 
Epoch 87/1000 
	 loss: 24.0115, MinusLogProbMetric: 24.0115, val_loss: 23.8148, val_MinusLogProbMetric: 23.8148

Epoch 87: val_loss did not improve from 23.73882
196/196 - 79s - loss: 24.0115 - MinusLogProbMetric: 24.0115 - val_loss: 23.8148 - val_MinusLogProbMetric: 23.8148 - lr: 3.3333e-04 - 79s/epoch - 403ms/step
Epoch 88/1000
2023-09-27 19:09:52.769 
Epoch 88/1000 
	 loss: 24.0307, MinusLogProbMetric: 24.0307, val_loss: 23.5730, val_MinusLogProbMetric: 23.5730

Epoch 88: val_loss improved from 23.73882 to 23.57296, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 80s - loss: 24.0307 - MinusLogProbMetric: 24.0307 - val_loss: 23.5730 - val_MinusLogProbMetric: 23.5730 - lr: 3.3333e-04 - 80s/epoch - 410ms/step
Epoch 89/1000
2023-09-27 19:11:13.292 
Epoch 89/1000 
	 loss: 24.4006, MinusLogProbMetric: 24.4006, val_loss: 23.8205, val_MinusLogProbMetric: 23.8205

Epoch 89: val_loss did not improve from 23.57296
196/196 - 79s - loss: 24.4006 - MinusLogProbMetric: 24.4006 - val_loss: 23.8205 - val_MinusLogProbMetric: 23.8205 - lr: 3.3333e-04 - 79s/epoch - 404ms/step
Epoch 90/1000
2023-09-27 19:12:31.960 
Epoch 90/1000 
	 loss: 23.5339, MinusLogProbMetric: 23.5339, val_loss: 23.3255, val_MinusLogProbMetric: 23.3255

Epoch 90: val_loss improved from 23.57296 to 23.32553, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 80s - loss: 23.5339 - MinusLogProbMetric: 23.5339 - val_loss: 23.3255 - val_MinusLogProbMetric: 23.3255 - lr: 3.3333e-04 - 80s/epoch - 407ms/step
Epoch 91/1000
2023-09-27 19:13:52.651 
Epoch 91/1000 
	 loss: 23.5612, MinusLogProbMetric: 23.5612, val_loss: 24.5895, val_MinusLogProbMetric: 24.5895

Epoch 91: val_loss did not improve from 23.32553
196/196 - 80s - loss: 23.5612 - MinusLogProbMetric: 23.5612 - val_loss: 24.5895 - val_MinusLogProbMetric: 24.5895 - lr: 3.3333e-04 - 80s/epoch - 406ms/step
Epoch 92/1000
2023-09-27 19:15:11.689 
Epoch 92/1000 
	 loss: 24.0281, MinusLogProbMetric: 24.0281, val_loss: 28.5163, val_MinusLogProbMetric: 28.5163

Epoch 92: val_loss did not improve from 23.32553
196/196 - 79s - loss: 24.0281 - MinusLogProbMetric: 24.0281 - val_loss: 28.5163 - val_MinusLogProbMetric: 28.5163 - lr: 3.3333e-04 - 79s/epoch - 403ms/step
Epoch 93/1000
2023-09-27 19:16:30.968 
Epoch 93/1000 
	 loss: 23.5934, MinusLogProbMetric: 23.5934, val_loss: 23.9750, val_MinusLogProbMetric: 23.9750

Epoch 93: val_loss did not improve from 23.32553
196/196 - 79s - loss: 23.5934 - MinusLogProbMetric: 23.5934 - val_loss: 23.9750 - val_MinusLogProbMetric: 23.9750 - lr: 3.3333e-04 - 79s/epoch - 404ms/step
Epoch 94/1000
2023-09-27 19:17:49.844 
Epoch 94/1000 
	 loss: 23.5712, MinusLogProbMetric: 23.5712, val_loss: 23.0987, val_MinusLogProbMetric: 23.0987

Epoch 94: val_loss improved from 23.32553 to 23.09872, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 80s - loss: 23.5712 - MinusLogProbMetric: 23.5712 - val_loss: 23.0987 - val_MinusLogProbMetric: 23.0987 - lr: 3.3333e-04 - 80s/epoch - 409ms/step
Epoch 95/1000
2023-09-27 19:19:10.762 
Epoch 95/1000 
	 loss: 23.4888, MinusLogProbMetric: 23.4888, val_loss: 24.3492, val_MinusLogProbMetric: 24.3492

Epoch 95: val_loss did not improve from 23.09872
196/196 - 80s - loss: 23.4888 - MinusLogProbMetric: 23.4888 - val_loss: 24.3492 - val_MinusLogProbMetric: 24.3492 - lr: 3.3333e-04 - 80s/epoch - 406ms/step
Epoch 96/1000
2023-09-27 19:20:29.689 
Epoch 96/1000 
	 loss: 23.6431, MinusLogProbMetric: 23.6431, val_loss: 23.4313, val_MinusLogProbMetric: 23.4313

Epoch 96: val_loss did not improve from 23.09872
196/196 - 79s - loss: 23.6431 - MinusLogProbMetric: 23.6431 - val_loss: 23.4313 - val_MinusLogProbMetric: 23.4313 - lr: 3.3333e-04 - 79s/epoch - 403ms/step
Epoch 97/1000
2023-09-27 19:21:48.877 
Epoch 97/1000 
	 loss: 23.3653, MinusLogProbMetric: 23.3653, val_loss: 23.6517, val_MinusLogProbMetric: 23.6517

Epoch 97: val_loss did not improve from 23.09872
196/196 - 79s - loss: 23.3653 - MinusLogProbMetric: 23.3653 - val_loss: 23.6517 - val_MinusLogProbMetric: 23.6517 - lr: 3.3333e-04 - 79s/epoch - 404ms/step
Epoch 98/1000
2023-09-27 19:23:08.054 
Epoch 98/1000 
	 loss: 23.0935, MinusLogProbMetric: 23.0935, val_loss: 24.2488, val_MinusLogProbMetric: 24.2488

Epoch 98: val_loss did not improve from 23.09872
196/196 - 79s - loss: 23.0935 - MinusLogProbMetric: 23.0935 - val_loss: 24.2488 - val_MinusLogProbMetric: 24.2488 - lr: 3.3333e-04 - 79s/epoch - 404ms/step
Epoch 99/1000
2023-09-27 19:24:26.973 
Epoch 99/1000 
	 loss: 23.1361, MinusLogProbMetric: 23.1361, val_loss: 23.0984, val_MinusLogProbMetric: 23.0984

Epoch 99: val_loss improved from 23.09872 to 23.09836, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 80s - loss: 23.1361 - MinusLogProbMetric: 23.1361 - val_loss: 23.0984 - val_MinusLogProbMetric: 23.0984 - lr: 3.3333e-04 - 80s/epoch - 409ms/step
Epoch 100/1000
2023-09-27 19:25:47.616 
Epoch 100/1000 
	 loss: 22.9105, MinusLogProbMetric: 22.9105, val_loss: 23.0908, val_MinusLogProbMetric: 23.0908

Epoch 100: val_loss improved from 23.09836 to 23.09078, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 81s - loss: 22.9105 - MinusLogProbMetric: 22.9105 - val_loss: 23.0908 - val_MinusLogProbMetric: 23.0908 - lr: 3.3333e-04 - 81s/epoch - 412ms/step
Epoch 101/1000
2023-09-27 19:27:02.808 
Epoch 101/1000 
	 loss: 23.1220, MinusLogProbMetric: 23.1220, val_loss: 23.2688, val_MinusLogProbMetric: 23.2688

Epoch 101: val_loss did not improve from 23.09078
196/196 - 74s - loss: 23.1220 - MinusLogProbMetric: 23.1220 - val_loss: 23.2688 - val_MinusLogProbMetric: 23.2688 - lr: 3.3333e-04 - 74s/epoch - 377ms/step
Epoch 102/1000
2023-09-27 19:28:16.920 
Epoch 102/1000 
	 loss: 23.1100, MinusLogProbMetric: 23.1100, val_loss: 22.9868, val_MinusLogProbMetric: 22.9868

Epoch 102: val_loss improved from 23.09078 to 22.98683, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 75s - loss: 23.1100 - MinusLogProbMetric: 23.1100 - val_loss: 22.9868 - val_MinusLogProbMetric: 22.9868 - lr: 3.3333e-04 - 75s/epoch - 384ms/step
Epoch 103/1000
2023-09-27 19:29:37.482 
Epoch 103/1000 
	 loss: 22.8396, MinusLogProbMetric: 22.8396, val_loss: 23.1535, val_MinusLogProbMetric: 23.1535

Epoch 103: val_loss did not improve from 22.98683
196/196 - 79s - loss: 22.8396 - MinusLogProbMetric: 22.8396 - val_loss: 23.1535 - val_MinusLogProbMetric: 23.1535 - lr: 3.3333e-04 - 79s/epoch - 405ms/step
Epoch 104/1000
2023-09-27 19:30:56.619 
Epoch 104/1000 
	 loss: 22.6422, MinusLogProbMetric: 22.6422, val_loss: 23.1369, val_MinusLogProbMetric: 23.1369

Epoch 104: val_loss did not improve from 22.98683
196/196 - 79s - loss: 22.6422 - MinusLogProbMetric: 22.6422 - val_loss: 23.1369 - val_MinusLogProbMetric: 23.1369 - lr: 3.3333e-04 - 79s/epoch - 404ms/step
Epoch 105/1000
2023-09-27 19:32:15.867 
Epoch 105/1000 
	 loss: 23.4964, MinusLogProbMetric: 23.4964, val_loss: 22.8102, val_MinusLogProbMetric: 22.8102

Epoch 105: val_loss improved from 22.98683 to 22.81021, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 80s - loss: 23.4964 - MinusLogProbMetric: 23.4964 - val_loss: 22.8102 - val_MinusLogProbMetric: 22.8102 - lr: 3.3333e-04 - 80s/epoch - 410ms/step
Epoch 106/1000
2023-09-27 19:33:35.964 
Epoch 106/1000 
	 loss: 22.6830, MinusLogProbMetric: 22.6830, val_loss: 23.4538, val_MinusLogProbMetric: 23.4538

Epoch 106: val_loss did not improve from 22.81021
196/196 - 79s - loss: 22.6830 - MinusLogProbMetric: 22.6830 - val_loss: 23.4538 - val_MinusLogProbMetric: 23.4538 - lr: 3.3333e-04 - 79s/epoch - 403ms/step
Epoch 107/1000
2023-09-27 19:34:54.535 
Epoch 107/1000 
	 loss: 22.8432, MinusLogProbMetric: 22.8432, val_loss: 22.2160, val_MinusLogProbMetric: 22.2160

Epoch 107: val_loss improved from 22.81021 to 22.21599, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 80s - loss: 22.8432 - MinusLogProbMetric: 22.8432 - val_loss: 22.2160 - val_MinusLogProbMetric: 22.2160 - lr: 3.3333e-04 - 80s/epoch - 408ms/step
Epoch 108/1000
2023-09-27 19:36:15.454 
Epoch 108/1000 
	 loss: 22.7461, MinusLogProbMetric: 22.7461, val_loss: 22.9589, val_MinusLogProbMetric: 22.9589

Epoch 108: val_loss did not improve from 22.21599
196/196 - 79s - loss: 22.7461 - MinusLogProbMetric: 22.7461 - val_loss: 22.9589 - val_MinusLogProbMetric: 22.9589 - lr: 3.3333e-04 - 79s/epoch - 405ms/step
Epoch 109/1000
2023-09-27 19:37:34.767 
Epoch 109/1000 
	 loss: 23.4265, MinusLogProbMetric: 23.4265, val_loss: 22.9470, val_MinusLogProbMetric: 22.9470

Epoch 109: val_loss did not improve from 22.21599
196/196 - 79s - loss: 23.4265 - MinusLogProbMetric: 23.4265 - val_loss: 22.9470 - val_MinusLogProbMetric: 22.9470 - lr: 3.3333e-04 - 79s/epoch - 405ms/step
Epoch 110/1000
2023-09-27 19:38:53.948 
Epoch 110/1000 
	 loss: 22.6816, MinusLogProbMetric: 22.6816, val_loss: 22.3123, val_MinusLogProbMetric: 22.3123

Epoch 110: val_loss did not improve from 22.21599
196/196 - 79s - loss: 22.6816 - MinusLogProbMetric: 22.6816 - val_loss: 22.3123 - val_MinusLogProbMetric: 22.3123 - lr: 3.3333e-04 - 79s/epoch - 404ms/step
Epoch 111/1000
2023-09-27 19:40:12.952 
Epoch 111/1000 
	 loss: 22.4758, MinusLogProbMetric: 22.4758, val_loss: 22.3374, val_MinusLogProbMetric: 22.3374

Epoch 111: val_loss did not improve from 22.21599
196/196 - 79s - loss: 22.4758 - MinusLogProbMetric: 22.4758 - val_loss: 22.3374 - val_MinusLogProbMetric: 22.3374 - lr: 3.3333e-04 - 79s/epoch - 403ms/step
Epoch 112/1000
2023-09-27 19:41:31.916 
Epoch 112/1000 
	 loss: 22.5427, MinusLogProbMetric: 22.5427, val_loss: 22.4806, val_MinusLogProbMetric: 22.4806

Epoch 112: val_loss did not improve from 22.21599
196/196 - 79s - loss: 22.5427 - MinusLogProbMetric: 22.5427 - val_loss: 22.4806 - val_MinusLogProbMetric: 22.4806 - lr: 3.3333e-04 - 79s/epoch - 403ms/step
Epoch 113/1000
2023-09-27 19:42:50.396 
Epoch 113/1000 
	 loss: 22.2830, MinusLogProbMetric: 22.2830, val_loss: 22.5444, val_MinusLogProbMetric: 22.5444

Epoch 113: val_loss did not improve from 22.21599
196/196 - 78s - loss: 22.2830 - MinusLogProbMetric: 22.2830 - val_loss: 22.5444 - val_MinusLogProbMetric: 22.5444 - lr: 3.3333e-04 - 78s/epoch - 400ms/step
Epoch 114/1000
2023-09-27 19:44:10.062 
Epoch 114/1000 
	 loss: 22.2241, MinusLogProbMetric: 22.2241, val_loss: 22.6459, val_MinusLogProbMetric: 22.6459

Epoch 114: val_loss did not improve from 22.21599
196/196 - 80s - loss: 22.2241 - MinusLogProbMetric: 22.2241 - val_loss: 22.6459 - val_MinusLogProbMetric: 22.6459 - lr: 3.3333e-04 - 80s/epoch - 406ms/step
Epoch 115/1000
2023-09-27 19:45:28.844 
Epoch 115/1000 
	 loss: 23.8885, MinusLogProbMetric: 23.8885, val_loss: 22.6450, val_MinusLogProbMetric: 22.6450

Epoch 115: val_loss did not improve from 22.21599
196/196 - 79s - loss: 23.8885 - MinusLogProbMetric: 23.8885 - val_loss: 22.6450 - val_MinusLogProbMetric: 22.6450 - lr: 3.3333e-04 - 79s/epoch - 402ms/step
Epoch 116/1000
2023-09-27 19:46:48.272 
Epoch 116/1000 
	 loss: 22.3404, MinusLogProbMetric: 22.3404, val_loss: 23.1614, val_MinusLogProbMetric: 23.1614

Epoch 116: val_loss did not improve from 22.21599
196/196 - 79s - loss: 22.3404 - MinusLogProbMetric: 22.3404 - val_loss: 23.1614 - val_MinusLogProbMetric: 23.1614 - lr: 3.3333e-04 - 79s/epoch - 405ms/step
Epoch 117/1000
2023-09-27 19:48:06.942 
Epoch 117/1000 
	 loss: 22.0343, MinusLogProbMetric: 22.0343, val_loss: 22.0935, val_MinusLogProbMetric: 22.0935

Epoch 117: val_loss improved from 22.21599 to 22.09346, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 80s - loss: 22.0343 - MinusLogProbMetric: 22.0343 - val_loss: 22.0935 - val_MinusLogProbMetric: 22.0935 - lr: 3.3333e-04 - 80s/epoch - 408ms/step
Epoch 118/1000
2023-09-27 19:49:27.600 
Epoch 118/1000 
	 loss: 22.3320, MinusLogProbMetric: 22.3320, val_loss: 22.4766, val_MinusLogProbMetric: 22.4766

Epoch 118: val_loss did not improve from 22.09346
196/196 - 79s - loss: 22.3320 - MinusLogProbMetric: 22.3320 - val_loss: 22.4766 - val_MinusLogProbMetric: 22.4766 - lr: 3.3333e-04 - 79s/epoch - 405ms/step
Epoch 119/1000
2023-09-27 19:50:46.453 
Epoch 119/1000 
	 loss: 22.3506, MinusLogProbMetric: 22.3506, val_loss: 21.9634, val_MinusLogProbMetric: 21.9634

Epoch 119: val_loss improved from 22.09346 to 21.96337, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 80s - loss: 22.3506 - MinusLogProbMetric: 22.3506 - val_loss: 21.9634 - val_MinusLogProbMetric: 21.9634 - lr: 3.3333e-04 - 80s/epoch - 408ms/step
Epoch 120/1000
2023-09-27 19:52:06.803 
Epoch 120/1000 
	 loss: 22.0664, MinusLogProbMetric: 22.0664, val_loss: 21.6928, val_MinusLogProbMetric: 21.6928

Epoch 120: val_loss improved from 21.96337 to 21.69280, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 80s - loss: 22.0664 - MinusLogProbMetric: 22.0664 - val_loss: 21.6928 - val_MinusLogProbMetric: 21.6928 - lr: 3.3333e-04 - 80s/epoch - 410ms/step
Epoch 121/1000
2023-09-27 19:53:27.616 
Epoch 121/1000 
	 loss: 21.9179, MinusLogProbMetric: 21.9179, val_loss: 22.1256, val_MinusLogProbMetric: 22.1256

Epoch 121: val_loss did not improve from 21.69280
196/196 - 80s - loss: 21.9179 - MinusLogProbMetric: 21.9179 - val_loss: 22.1256 - val_MinusLogProbMetric: 22.1256 - lr: 3.3333e-04 - 80s/epoch - 406ms/step
Epoch 122/1000
2023-09-27 19:54:45.877 
Epoch 122/1000 
	 loss: 24.1604, MinusLogProbMetric: 24.1604, val_loss: 23.5929, val_MinusLogProbMetric: 23.5929

Epoch 122: val_loss did not improve from 21.69280
196/196 - 78s - loss: 24.1604 - MinusLogProbMetric: 24.1604 - val_loss: 23.5929 - val_MinusLogProbMetric: 23.5929 - lr: 3.3333e-04 - 78s/epoch - 399ms/step
Epoch 123/1000
2023-09-27 19:56:05.048 
Epoch 123/1000 
	 loss: 22.9166, MinusLogProbMetric: 22.9166, val_loss: 22.1571, val_MinusLogProbMetric: 22.1571

Epoch 123: val_loss did not improve from 21.69280
196/196 - 79s - loss: 22.9166 - MinusLogProbMetric: 22.9166 - val_loss: 22.1571 - val_MinusLogProbMetric: 22.1571 - lr: 3.3333e-04 - 79s/epoch - 404ms/step
Epoch 124/1000
2023-09-27 19:57:23.931 
Epoch 124/1000 
	 loss: 22.1206, MinusLogProbMetric: 22.1206, val_loss: 22.2611, val_MinusLogProbMetric: 22.2611

Epoch 124: val_loss did not improve from 21.69280
196/196 - 79s - loss: 22.1206 - MinusLogProbMetric: 22.1206 - val_loss: 22.2611 - val_MinusLogProbMetric: 22.2611 - lr: 3.3333e-04 - 79s/epoch - 402ms/step
Epoch 125/1000
2023-09-27 19:58:42.852 
Epoch 125/1000 
	 loss: 22.0165, MinusLogProbMetric: 22.0165, val_loss: 22.0571, val_MinusLogProbMetric: 22.0571

Epoch 125: val_loss did not improve from 21.69280
196/196 - 79s - loss: 22.0165 - MinusLogProbMetric: 22.0165 - val_loss: 22.0571 - val_MinusLogProbMetric: 22.0571 - lr: 3.3333e-04 - 79s/epoch - 403ms/step
Epoch 126/1000
2023-09-27 20:00:02.636 
Epoch 126/1000 
	 loss: 21.9711, MinusLogProbMetric: 21.9711, val_loss: 22.0011, val_MinusLogProbMetric: 22.0011

Epoch 126: val_loss did not improve from 21.69280
196/196 - 80s - loss: 21.9711 - MinusLogProbMetric: 21.9711 - val_loss: 22.0011 - val_MinusLogProbMetric: 22.0011 - lr: 3.3333e-04 - 80s/epoch - 407ms/step
Epoch 127/1000
2023-09-27 20:01:21.573 
Epoch 127/1000 
	 loss: 21.7359, MinusLogProbMetric: 21.7359, val_loss: 21.8810, val_MinusLogProbMetric: 21.8810

Epoch 127: val_loss did not improve from 21.69280
196/196 - 79s - loss: 21.7359 - MinusLogProbMetric: 21.7359 - val_loss: 21.8810 - val_MinusLogProbMetric: 21.8810 - lr: 3.3333e-04 - 79s/epoch - 403ms/step
Epoch 128/1000
2023-09-27 20:02:40.552 
Epoch 128/1000 
	 loss: 21.9134, MinusLogProbMetric: 21.9134, val_loss: 21.6677, val_MinusLogProbMetric: 21.6677

Epoch 128: val_loss improved from 21.69280 to 21.66775, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 80s - loss: 21.9134 - MinusLogProbMetric: 21.9134 - val_loss: 21.6677 - val_MinusLogProbMetric: 21.6677 - lr: 3.3333e-04 - 80s/epoch - 409ms/step
Epoch 129/1000
2023-09-27 20:04:01.246 
Epoch 129/1000 
	 loss: 21.9908, MinusLogProbMetric: 21.9908, val_loss: 22.1561, val_MinusLogProbMetric: 22.1561

Epoch 129: val_loss did not improve from 21.66775
196/196 - 80s - loss: 21.9908 - MinusLogProbMetric: 21.9908 - val_loss: 22.1561 - val_MinusLogProbMetric: 22.1561 - lr: 3.3333e-04 - 80s/epoch - 406ms/step
Epoch 130/1000
2023-09-27 20:05:20.907 
Epoch 130/1000 
	 loss: 21.6832, MinusLogProbMetric: 21.6832, val_loss: 21.9090, val_MinusLogProbMetric: 21.9090

Epoch 130: val_loss did not improve from 21.66775
196/196 - 80s - loss: 21.6832 - MinusLogProbMetric: 21.6832 - val_loss: 21.9090 - val_MinusLogProbMetric: 21.9090 - lr: 3.3333e-04 - 80s/epoch - 406ms/step
Epoch 131/1000
2023-09-27 20:06:40.604 
Epoch 131/1000 
	 loss: 21.6763, MinusLogProbMetric: 21.6763, val_loss: 21.4751, val_MinusLogProbMetric: 21.4751

Epoch 131: val_loss improved from 21.66775 to 21.47510, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 81s - loss: 21.6763 - MinusLogProbMetric: 21.6763 - val_loss: 21.4751 - val_MinusLogProbMetric: 21.4751 - lr: 3.3333e-04 - 81s/epoch - 412ms/step
Epoch 132/1000
2023-09-27 20:08:01.357 
Epoch 132/1000 
	 loss: 21.7455, MinusLogProbMetric: 21.7455, val_loss: 21.6527, val_MinusLogProbMetric: 21.6527

Epoch 132: val_loss did not improve from 21.47510
196/196 - 80s - loss: 21.7455 - MinusLogProbMetric: 21.7455 - val_loss: 21.6527 - val_MinusLogProbMetric: 21.6527 - lr: 3.3333e-04 - 80s/epoch - 406ms/step
Epoch 133/1000
2023-09-27 20:09:20.443 
Epoch 133/1000 
	 loss: 21.9318, MinusLogProbMetric: 21.9318, val_loss: 21.9314, val_MinusLogProbMetric: 21.9314

Epoch 133: val_loss did not improve from 21.47510
196/196 - 79s - loss: 21.9318 - MinusLogProbMetric: 21.9318 - val_loss: 21.9314 - val_MinusLogProbMetric: 21.9314 - lr: 3.3333e-04 - 79s/epoch - 403ms/step
Epoch 134/1000
2023-09-27 20:10:39.776 
Epoch 134/1000 
	 loss: 23.8059, MinusLogProbMetric: 23.8059, val_loss: 22.1351, val_MinusLogProbMetric: 22.1351

Epoch 134: val_loss did not improve from 21.47510
196/196 - 79s - loss: 23.8059 - MinusLogProbMetric: 23.8059 - val_loss: 22.1351 - val_MinusLogProbMetric: 22.1351 - lr: 3.3333e-04 - 79s/epoch - 405ms/step
Epoch 135/1000
2023-09-27 20:11:59.144 
Epoch 135/1000 
	 loss: 21.7968, MinusLogProbMetric: 21.7968, val_loss: 22.7547, val_MinusLogProbMetric: 22.7547

Epoch 135: val_loss did not improve from 21.47510
196/196 - 79s - loss: 21.7968 - MinusLogProbMetric: 21.7968 - val_loss: 22.7547 - val_MinusLogProbMetric: 22.7547 - lr: 3.3333e-04 - 79s/epoch - 405ms/step
Epoch 136/1000
2023-09-27 20:13:18.148 
Epoch 136/1000 
	 loss: 21.6546, MinusLogProbMetric: 21.6546, val_loss: 21.5618, val_MinusLogProbMetric: 21.5618

Epoch 136: val_loss did not improve from 21.47510
196/196 - 79s - loss: 21.6546 - MinusLogProbMetric: 21.6546 - val_loss: 21.5618 - val_MinusLogProbMetric: 21.5618 - lr: 3.3333e-04 - 79s/epoch - 403ms/step
Epoch 137/1000
2023-09-27 20:14:37.845 
Epoch 137/1000 
	 loss: 21.4918, MinusLogProbMetric: 21.4918, val_loss: 21.5463, val_MinusLogProbMetric: 21.5463

Epoch 137: val_loss did not improve from 21.47510
196/196 - 80s - loss: 21.4918 - MinusLogProbMetric: 21.4918 - val_loss: 21.5463 - val_MinusLogProbMetric: 21.5463 - lr: 3.3333e-04 - 80s/epoch - 407ms/step
Epoch 138/1000
2023-09-27 20:15:56.838 
Epoch 138/1000 
	 loss: 21.8525, MinusLogProbMetric: 21.8525, val_loss: 21.5985, val_MinusLogProbMetric: 21.5985

Epoch 138: val_loss did not improve from 21.47510
196/196 - 79s - loss: 21.8525 - MinusLogProbMetric: 21.8525 - val_loss: 21.5985 - val_MinusLogProbMetric: 21.5985 - lr: 3.3333e-04 - 79s/epoch - 403ms/step
Epoch 139/1000
2023-09-27 20:17:15.582 
Epoch 139/1000 
	 loss: 25.0806, MinusLogProbMetric: 25.0806, val_loss: 28.4597, val_MinusLogProbMetric: 28.4597

Epoch 139: val_loss did not improve from 21.47510
196/196 - 79s - loss: 25.0806 - MinusLogProbMetric: 25.0806 - val_loss: 28.4597 - val_MinusLogProbMetric: 28.4597 - lr: 3.3333e-04 - 79s/epoch - 402ms/step
Epoch 140/1000
2023-09-27 20:18:34.630 
Epoch 140/1000 
	 loss: 24.6619, MinusLogProbMetric: 24.6619, val_loss: 24.3267, val_MinusLogProbMetric: 24.3267

Epoch 140: val_loss did not improve from 21.47510
196/196 - 79s - loss: 24.6619 - MinusLogProbMetric: 24.6619 - val_loss: 24.3267 - val_MinusLogProbMetric: 24.3267 - lr: 3.3333e-04 - 79s/epoch - 403ms/step
Epoch 141/1000
2023-09-27 20:19:54.001 
Epoch 141/1000 
	 loss: 23.8228, MinusLogProbMetric: 23.8228, val_loss: 23.7631, val_MinusLogProbMetric: 23.7631

Epoch 141: val_loss did not improve from 21.47510
196/196 - 79s - loss: 23.8228 - MinusLogProbMetric: 23.8228 - val_loss: 23.7631 - val_MinusLogProbMetric: 23.7631 - lr: 3.3333e-04 - 79s/epoch - 405ms/step
Epoch 142/1000
2023-09-27 20:21:13.142 
Epoch 142/1000 
	 loss: 25.1983, MinusLogProbMetric: 25.1983, val_loss: 22.8559, val_MinusLogProbMetric: 22.8559

Epoch 142: val_loss did not improve from 21.47510
196/196 - 79s - loss: 25.1983 - MinusLogProbMetric: 25.1983 - val_loss: 22.8559 - val_MinusLogProbMetric: 22.8559 - lr: 3.3333e-04 - 79s/epoch - 404ms/step
Epoch 143/1000
2023-09-27 20:22:32.172 
Epoch 143/1000 
	 loss: 22.2000, MinusLogProbMetric: 22.2000, val_loss: 21.9612, val_MinusLogProbMetric: 21.9612

Epoch 143: val_loss did not improve from 21.47510
196/196 - 79s - loss: 22.2000 - MinusLogProbMetric: 22.2000 - val_loss: 21.9612 - val_MinusLogProbMetric: 21.9612 - lr: 3.3333e-04 - 79s/epoch - 403ms/step
Epoch 144/1000
2023-09-27 20:23:51.386 
Epoch 144/1000 
	 loss: 21.9753, MinusLogProbMetric: 21.9753, val_loss: 22.0099, val_MinusLogProbMetric: 22.0099

Epoch 144: val_loss did not improve from 21.47510
196/196 - 79s - loss: 21.9753 - MinusLogProbMetric: 21.9753 - val_loss: 22.0099 - val_MinusLogProbMetric: 22.0099 - lr: 3.3333e-04 - 79s/epoch - 404ms/step
Epoch 145/1000
2023-09-27 20:25:11.067 
Epoch 145/1000 
	 loss: 21.6209, MinusLogProbMetric: 21.6209, val_loss: 21.8227, val_MinusLogProbMetric: 21.8227

Epoch 145: val_loss did not improve from 21.47510
196/196 - 80s - loss: 21.6209 - MinusLogProbMetric: 21.6209 - val_loss: 21.8227 - val_MinusLogProbMetric: 21.8227 - lr: 3.3333e-04 - 80s/epoch - 407ms/step
Epoch 146/1000
2023-09-27 20:26:30.184 
Epoch 146/1000 
	 loss: 21.4100, MinusLogProbMetric: 21.4100, val_loss: 21.3152, val_MinusLogProbMetric: 21.3152

Epoch 146: val_loss improved from 21.47510 to 21.31519, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 80s - loss: 21.4100 - MinusLogProbMetric: 21.4100 - val_loss: 21.3152 - val_MinusLogProbMetric: 21.3152 - lr: 3.3333e-04 - 80s/epoch - 410ms/step
Epoch 147/1000
2023-09-27 20:27:50.876 
Epoch 147/1000 
	 loss: 21.2754, MinusLogProbMetric: 21.2754, val_loss: 21.5879, val_MinusLogProbMetric: 21.5879

Epoch 147: val_loss did not improve from 21.31519
196/196 - 80s - loss: 21.2754 - MinusLogProbMetric: 21.2754 - val_loss: 21.5879 - val_MinusLogProbMetric: 21.5879 - lr: 3.3333e-04 - 80s/epoch - 406ms/step
Epoch 148/1000
2023-09-27 20:29:09.972 
Epoch 148/1000 
	 loss: 22.4614, MinusLogProbMetric: 22.4614, val_loss: 21.6502, val_MinusLogProbMetric: 21.6502

Epoch 148: val_loss did not improve from 21.31519
196/196 - 79s - loss: 22.4614 - MinusLogProbMetric: 22.4614 - val_loss: 21.6502 - val_MinusLogProbMetric: 21.6502 - lr: 3.3333e-04 - 79s/epoch - 404ms/step
Epoch 149/1000
2023-09-27 20:30:29.498 
Epoch 149/1000 
	 loss: 21.4645, MinusLogProbMetric: 21.4645, val_loss: 21.5800, val_MinusLogProbMetric: 21.5800

Epoch 149: val_loss did not improve from 21.31519
196/196 - 80s - loss: 21.4645 - MinusLogProbMetric: 21.4645 - val_loss: 21.5800 - val_MinusLogProbMetric: 21.5800 - lr: 3.3333e-04 - 80s/epoch - 406ms/step
Epoch 150/1000
2023-09-27 20:31:48.851 
Epoch 150/1000 
	 loss: 21.2378, MinusLogProbMetric: 21.2378, val_loss: 21.2155, val_MinusLogProbMetric: 21.2155

Epoch 150: val_loss improved from 21.31519 to 21.21551, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 81s - loss: 21.2378 - MinusLogProbMetric: 21.2378 - val_loss: 21.2155 - val_MinusLogProbMetric: 21.2155 - lr: 3.3333e-04 - 81s/epoch - 412ms/step
Epoch 151/1000
2023-09-27 20:33:09.724 
Epoch 151/1000 
	 loss: 21.0980, MinusLogProbMetric: 21.0980, val_loss: 21.5724, val_MinusLogProbMetric: 21.5724

Epoch 151: val_loss did not improve from 21.21551
196/196 - 79s - loss: 21.0980 - MinusLogProbMetric: 21.0980 - val_loss: 21.5724 - val_MinusLogProbMetric: 21.5724 - lr: 3.3333e-04 - 79s/epoch - 405ms/step
Epoch 152/1000
2023-09-27 20:34:28.420 
Epoch 152/1000 
	 loss: 21.0580, MinusLogProbMetric: 21.0580, val_loss: 20.8697, val_MinusLogProbMetric: 20.8697

Epoch 152: val_loss improved from 21.21551 to 20.86968, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 80s - loss: 21.0580 - MinusLogProbMetric: 21.0580 - val_loss: 20.8697 - val_MinusLogProbMetric: 20.8697 - lr: 3.3333e-04 - 80s/epoch - 408ms/step
Epoch 153/1000
2023-09-27 20:35:48.667 
Epoch 153/1000 
	 loss: 21.1519, MinusLogProbMetric: 21.1519, val_loss: 20.9332, val_MinusLogProbMetric: 20.9332

Epoch 153: val_loss did not improve from 20.86968
196/196 - 79s - loss: 21.1519 - MinusLogProbMetric: 21.1519 - val_loss: 20.9332 - val_MinusLogProbMetric: 20.9332 - lr: 3.3333e-04 - 79s/epoch - 403ms/step
Epoch 154/1000
2023-09-27 20:37:07.780 
Epoch 154/1000 
	 loss: 20.9917, MinusLogProbMetric: 20.9917, val_loss: 21.1997, val_MinusLogProbMetric: 21.1997

Epoch 154: val_loss did not improve from 20.86968
196/196 - 79s - loss: 20.9917 - MinusLogProbMetric: 20.9917 - val_loss: 21.1997 - val_MinusLogProbMetric: 21.1997 - lr: 3.3333e-04 - 79s/epoch - 404ms/step
Epoch 155/1000
2023-09-27 20:38:26.859 
Epoch 155/1000 
	 loss: 25.2934, MinusLogProbMetric: 25.2934, val_loss: 24.5942, val_MinusLogProbMetric: 24.5942

Epoch 155: val_loss did not improve from 20.86968
196/196 - 79s - loss: 25.2934 - MinusLogProbMetric: 25.2934 - val_loss: 24.5942 - val_MinusLogProbMetric: 24.5942 - lr: 3.3333e-04 - 79s/epoch - 403ms/step
Epoch 156/1000
2023-09-27 20:39:46.573 
Epoch 156/1000 
	 loss: 23.4552, MinusLogProbMetric: 23.4552, val_loss: 23.3316, val_MinusLogProbMetric: 23.3316

Epoch 156: val_loss did not improve from 20.86968
196/196 - 80s - loss: 23.4552 - MinusLogProbMetric: 23.4552 - val_loss: 23.3316 - val_MinusLogProbMetric: 23.3316 - lr: 3.3333e-04 - 80s/epoch - 407ms/step
Epoch 157/1000
2023-09-27 20:41:05.846 
Epoch 157/1000 
	 loss: 21.9535, MinusLogProbMetric: 21.9535, val_loss: 21.4990, val_MinusLogProbMetric: 21.4990

Epoch 157: val_loss did not improve from 20.86968
196/196 - 79s - loss: 21.9535 - MinusLogProbMetric: 21.9535 - val_loss: 21.4990 - val_MinusLogProbMetric: 21.4990 - lr: 3.3333e-04 - 79s/epoch - 404ms/step
Epoch 158/1000
2023-09-27 20:42:24.395 
Epoch 158/1000 
	 loss: 21.3577, MinusLogProbMetric: 21.3577, val_loss: 21.1752, val_MinusLogProbMetric: 21.1752

Epoch 158: val_loss did not improve from 20.86968
196/196 - 79s - loss: 21.3577 - MinusLogProbMetric: 21.3577 - val_loss: 21.1752 - val_MinusLogProbMetric: 21.1752 - lr: 3.3333e-04 - 79s/epoch - 401ms/step
Epoch 159/1000
2023-09-27 20:43:34.854 
Epoch 159/1000 
	 loss: 21.1202, MinusLogProbMetric: 21.1202, val_loss: 20.9598, val_MinusLogProbMetric: 20.9598

Epoch 159: val_loss did not improve from 20.86968
196/196 - 70s - loss: 21.1202 - MinusLogProbMetric: 21.1202 - val_loss: 20.9598 - val_MinusLogProbMetric: 20.9598 - lr: 3.3333e-04 - 70s/epoch - 359ms/step
Epoch 160/1000
2023-09-27 20:44:54.037 
Epoch 160/1000 
	 loss: 20.9277, MinusLogProbMetric: 20.9277, val_loss: 20.6974, val_MinusLogProbMetric: 20.6974

Epoch 160: val_loss improved from 20.86968 to 20.69740, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 81s - loss: 20.9277 - MinusLogProbMetric: 20.9277 - val_loss: 20.6974 - val_MinusLogProbMetric: 20.6974 - lr: 3.3333e-04 - 81s/epoch - 411ms/step
Epoch 161/1000
2023-09-27 20:46:15.756 
Epoch 161/1000 
	 loss: 21.0355, MinusLogProbMetric: 21.0355, val_loss: 20.8170, val_MinusLogProbMetric: 20.8170

Epoch 161: val_loss did not improve from 20.69740
196/196 - 80s - loss: 21.0355 - MinusLogProbMetric: 21.0355 - val_loss: 20.8170 - val_MinusLogProbMetric: 20.8170 - lr: 3.3333e-04 - 80s/epoch - 410ms/step
Epoch 162/1000
2023-09-27 20:47:35.718 
Epoch 162/1000 
	 loss: 20.6841, MinusLogProbMetric: 20.6841, val_loss: 21.0832, val_MinusLogProbMetric: 21.0832

Epoch 162: val_loss did not improve from 20.69740
196/196 - 80s - loss: 20.6841 - MinusLogProbMetric: 20.6841 - val_loss: 21.0832 - val_MinusLogProbMetric: 21.0832 - lr: 3.3333e-04 - 80s/epoch - 408ms/step
Epoch 163/1000
2023-09-27 20:48:54.704 
Epoch 163/1000 
	 loss: 20.7225, MinusLogProbMetric: 20.7225, val_loss: 21.5883, val_MinusLogProbMetric: 21.5883

Epoch 163: val_loss did not improve from 20.69740
196/196 - 79s - loss: 20.7225 - MinusLogProbMetric: 20.7225 - val_loss: 21.5883 - val_MinusLogProbMetric: 21.5883 - lr: 3.3333e-04 - 79s/epoch - 403ms/step
Epoch 164/1000
2023-09-27 20:50:13.934 
Epoch 164/1000 
	 loss: 20.7040, MinusLogProbMetric: 20.7040, val_loss: 21.2610, val_MinusLogProbMetric: 21.2610

Epoch 164: val_loss did not improve from 20.69740
196/196 - 79s - loss: 20.7040 - MinusLogProbMetric: 20.7040 - val_loss: 21.2610 - val_MinusLogProbMetric: 21.2610 - lr: 3.3333e-04 - 79s/epoch - 404ms/step
Epoch 165/1000
2023-09-27 20:51:33.630 
Epoch 165/1000 
	 loss: 20.9342, MinusLogProbMetric: 20.9342, val_loss: 20.7810, val_MinusLogProbMetric: 20.7810

Epoch 165: val_loss did not improve from 20.69740
196/196 - 80s - loss: 20.9342 - MinusLogProbMetric: 20.9342 - val_loss: 20.7810 - val_MinusLogProbMetric: 20.7810 - lr: 3.3333e-04 - 80s/epoch - 407ms/step
Epoch 166/1000
2023-09-27 20:52:53.549 
Epoch 166/1000 
	 loss: 20.6811, MinusLogProbMetric: 20.6811, val_loss: 20.8254, val_MinusLogProbMetric: 20.8254

Epoch 166: val_loss did not improve from 20.69740
196/196 - 80s - loss: 20.6811 - MinusLogProbMetric: 20.6811 - val_loss: 20.8254 - val_MinusLogProbMetric: 20.8254 - lr: 3.3333e-04 - 80s/epoch - 408ms/step
Epoch 167/1000
2023-09-27 20:54:13.703 
Epoch 167/1000 
	 loss: 20.6171, MinusLogProbMetric: 20.6171, val_loss: 20.8820, val_MinusLogProbMetric: 20.8820

Epoch 167: val_loss did not improve from 20.69740
196/196 - 80s - loss: 20.6171 - MinusLogProbMetric: 20.6171 - val_loss: 20.8820 - val_MinusLogProbMetric: 20.8820 - lr: 3.3333e-04 - 80s/epoch - 409ms/step
Epoch 168/1000
2023-09-27 20:55:33.342 
Epoch 168/1000 
	 loss: 20.6700, MinusLogProbMetric: 20.6700, val_loss: 20.7591, val_MinusLogProbMetric: 20.7591

Epoch 168: val_loss did not improve from 20.69740
196/196 - 80s - loss: 20.6700 - MinusLogProbMetric: 20.6700 - val_loss: 20.7591 - val_MinusLogProbMetric: 20.7591 - lr: 3.3333e-04 - 80s/epoch - 406ms/step
Epoch 169/1000
2023-09-27 20:56:52.551 
Epoch 169/1000 
	 loss: 20.5640, MinusLogProbMetric: 20.5640, val_loss: 21.5602, val_MinusLogProbMetric: 21.5602

Epoch 169: val_loss did not improve from 20.69740
196/196 - 79s - loss: 20.5640 - MinusLogProbMetric: 20.5640 - val_loss: 21.5602 - val_MinusLogProbMetric: 21.5602 - lr: 3.3333e-04 - 79s/epoch - 404ms/step
Epoch 170/1000
2023-09-27 20:58:11.734 
Epoch 170/1000 
	 loss: 20.4795, MinusLogProbMetric: 20.4795, val_loss: 20.5819, val_MinusLogProbMetric: 20.5819

Epoch 170: val_loss improved from 20.69740 to 20.58191, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 81s - loss: 20.4795 - MinusLogProbMetric: 20.4795 - val_loss: 20.5819 - val_MinusLogProbMetric: 20.5819 - lr: 3.3333e-04 - 81s/epoch - 411ms/step
Epoch 171/1000
2023-09-27 20:59:32.926 
Epoch 171/1000 
	 loss: 20.4500, MinusLogProbMetric: 20.4500, val_loss: 20.6167, val_MinusLogProbMetric: 20.6167

Epoch 171: val_loss did not improve from 20.58191
196/196 - 80s - loss: 20.4500 - MinusLogProbMetric: 20.4500 - val_loss: 20.6167 - val_MinusLogProbMetric: 20.6167 - lr: 3.3333e-04 - 80s/epoch - 407ms/step
Epoch 172/1000
2023-09-27 21:00:52.918 
Epoch 172/1000 
	 loss: 20.4457, MinusLogProbMetric: 20.4457, val_loss: 20.4929, val_MinusLogProbMetric: 20.4929

Epoch 172: val_loss improved from 20.58191 to 20.49288, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 81s - loss: 20.4457 - MinusLogProbMetric: 20.4457 - val_loss: 20.4929 - val_MinusLogProbMetric: 20.4929 - lr: 3.3333e-04 - 81s/epoch - 416ms/step
Epoch 173/1000
2023-09-27 21:02:13.878 
Epoch 173/1000 
	 loss: 20.6579, MinusLogProbMetric: 20.6579, val_loss: 20.6283, val_MinusLogProbMetric: 20.6283

Epoch 173: val_loss did not improve from 20.49288
196/196 - 79s - loss: 20.6579 - MinusLogProbMetric: 20.6579 - val_loss: 20.6283 - val_MinusLogProbMetric: 20.6283 - lr: 3.3333e-04 - 79s/epoch - 405ms/step
Epoch 174/1000
2023-09-27 21:03:33.704 
Epoch 174/1000 
	 loss: 20.4308, MinusLogProbMetric: 20.4308, val_loss: 20.8488, val_MinusLogProbMetric: 20.8488

Epoch 174: val_loss did not improve from 20.49288
196/196 - 80s - loss: 20.4308 - MinusLogProbMetric: 20.4308 - val_loss: 20.8488 - val_MinusLogProbMetric: 20.8488 - lr: 3.3333e-04 - 80s/epoch - 407ms/step
Epoch 175/1000
2023-09-27 21:04:53.600 
Epoch 175/1000 
	 loss: 20.4322, MinusLogProbMetric: 20.4322, val_loss: 21.1349, val_MinusLogProbMetric: 21.1349

Epoch 175: val_loss did not improve from 20.49288
196/196 - 80s - loss: 20.4322 - MinusLogProbMetric: 20.4322 - val_loss: 21.1349 - val_MinusLogProbMetric: 21.1349 - lr: 3.3333e-04 - 80s/epoch - 408ms/step
Epoch 176/1000
2023-09-27 21:06:12.995 
Epoch 176/1000 
	 loss: 20.9390, MinusLogProbMetric: 20.9390, val_loss: 20.4351, val_MinusLogProbMetric: 20.4351

Epoch 176: val_loss improved from 20.49288 to 20.43511, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 81s - loss: 20.9390 - MinusLogProbMetric: 20.9390 - val_loss: 20.4351 - val_MinusLogProbMetric: 20.4351 - lr: 3.3333e-04 - 81s/epoch - 411ms/step
Epoch 177/1000
2023-09-27 21:07:33.312 
Epoch 177/1000 
	 loss: 20.2950, MinusLogProbMetric: 20.2950, val_loss: 20.3225, val_MinusLogProbMetric: 20.3225

Epoch 177: val_loss improved from 20.43511 to 20.32248, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 80s - loss: 20.2950 - MinusLogProbMetric: 20.2950 - val_loss: 20.3225 - val_MinusLogProbMetric: 20.3225 - lr: 3.3333e-04 - 80s/epoch - 410ms/step
Epoch 178/1000
2023-09-27 21:08:54.873 
Epoch 178/1000 
	 loss: 20.2787, MinusLogProbMetric: 20.2787, val_loss: 20.2739, val_MinusLogProbMetric: 20.2739

Epoch 178: val_loss improved from 20.32248 to 20.27394, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 82s - loss: 20.2787 - MinusLogProbMetric: 20.2787 - val_loss: 20.2739 - val_MinusLogProbMetric: 20.2739 - lr: 3.3333e-04 - 82s/epoch - 416ms/step
Epoch 179/1000
2023-09-27 21:10:15.902 
Epoch 179/1000 
	 loss: 20.3571, MinusLogProbMetric: 20.3571, val_loss: 20.3553, val_MinusLogProbMetric: 20.3553

Epoch 179: val_loss did not improve from 20.27394
196/196 - 80s - loss: 20.3571 - MinusLogProbMetric: 20.3571 - val_loss: 20.3553 - val_MinusLogProbMetric: 20.3553 - lr: 3.3333e-04 - 80s/epoch - 406ms/step
Epoch 180/1000
2023-09-27 21:11:35.577 
Epoch 180/1000 
	 loss: 20.2574, MinusLogProbMetric: 20.2574, val_loss: 19.9754, val_MinusLogProbMetric: 19.9754

Epoch 180: val_loss improved from 20.27394 to 19.97541, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 81s - loss: 20.2574 - MinusLogProbMetric: 20.2574 - val_loss: 19.9754 - val_MinusLogProbMetric: 19.9754 - lr: 3.3333e-04 - 81s/epoch - 414ms/step
Epoch 181/1000
2023-09-27 21:12:56.576 
Epoch 181/1000 
	 loss: 20.1955, MinusLogProbMetric: 20.1955, val_loss: 20.6506, val_MinusLogProbMetric: 20.6506

Epoch 181: val_loss did not improve from 19.97541
196/196 - 79s - loss: 20.1955 - MinusLogProbMetric: 20.1955 - val_loss: 20.6506 - val_MinusLogProbMetric: 20.6506 - lr: 3.3333e-04 - 79s/epoch - 405ms/step
Epoch 182/1000
2023-09-27 21:14:16.049 
Epoch 182/1000 
	 loss: 20.2711, MinusLogProbMetric: 20.2711, val_loss: 20.2285, val_MinusLogProbMetric: 20.2285

Epoch 182: val_loss did not improve from 19.97541
196/196 - 79s - loss: 20.2711 - MinusLogProbMetric: 20.2711 - val_loss: 20.2285 - val_MinusLogProbMetric: 20.2285 - lr: 3.3333e-04 - 79s/epoch - 405ms/step
Epoch 183/1000
2023-09-27 21:15:35.630 
Epoch 183/1000 
	 loss: 20.1484, MinusLogProbMetric: 20.1484, val_loss: 19.8937, val_MinusLogProbMetric: 19.8937

Epoch 183: val_loss improved from 19.97541 to 19.89370, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 81s - loss: 20.1484 - MinusLogProbMetric: 20.1484 - val_loss: 19.8937 - val_MinusLogProbMetric: 19.8937 - lr: 3.3333e-04 - 81s/epoch - 412ms/step
Epoch 184/1000
2023-09-27 21:16:56.244 
Epoch 184/1000 
	 loss: 20.2719, MinusLogProbMetric: 20.2719, val_loss: 20.5390, val_MinusLogProbMetric: 20.5390

Epoch 184: val_loss did not improve from 19.89370
196/196 - 79s - loss: 20.2719 - MinusLogProbMetric: 20.2719 - val_loss: 20.5390 - val_MinusLogProbMetric: 20.5390 - lr: 3.3333e-04 - 79s/epoch - 405ms/step
Epoch 185/1000
2023-09-27 21:18:15.984 
Epoch 185/1000 
	 loss: 20.1820, MinusLogProbMetric: 20.1820, val_loss: 20.1097, val_MinusLogProbMetric: 20.1097

Epoch 185: val_loss did not improve from 19.89370
196/196 - 80s - loss: 20.1820 - MinusLogProbMetric: 20.1820 - val_loss: 20.1097 - val_MinusLogProbMetric: 20.1097 - lr: 3.3333e-04 - 80s/epoch - 407ms/step
Epoch 186/1000
2023-09-27 21:19:35.347 
Epoch 186/1000 
	 loss: 20.0571, MinusLogProbMetric: 20.0571, val_loss: 20.5320, val_MinusLogProbMetric: 20.5320

Epoch 186: val_loss did not improve from 19.89370
196/196 - 79s - loss: 20.0571 - MinusLogProbMetric: 20.0571 - val_loss: 20.5320 - val_MinusLogProbMetric: 20.5320 - lr: 3.3333e-04 - 79s/epoch - 405ms/step
Epoch 187/1000
2023-09-27 21:20:54.671 
Epoch 187/1000 
	 loss: 20.2610, MinusLogProbMetric: 20.2610, val_loss: 20.1208, val_MinusLogProbMetric: 20.1208

Epoch 187: val_loss did not improve from 19.89370
196/196 - 79s - loss: 20.2610 - MinusLogProbMetric: 20.2610 - val_loss: 20.1208 - val_MinusLogProbMetric: 20.1208 - lr: 3.3333e-04 - 79s/epoch - 405ms/step
Epoch 188/1000
2023-09-27 21:22:14.331 
Epoch 188/1000 
	 loss: 20.1167, MinusLogProbMetric: 20.1167, val_loss: 20.5524, val_MinusLogProbMetric: 20.5524

Epoch 188: val_loss did not improve from 19.89370
196/196 - 80s - loss: 20.1167 - MinusLogProbMetric: 20.1167 - val_loss: 20.5524 - val_MinusLogProbMetric: 20.5524 - lr: 3.3333e-04 - 80s/epoch - 406ms/step
Epoch 189/1000
2023-09-27 21:23:33.906 
Epoch 189/1000 
	 loss: 20.1269, MinusLogProbMetric: 20.1269, val_loss: 20.1489, val_MinusLogProbMetric: 20.1489

Epoch 189: val_loss did not improve from 19.89370
196/196 - 80s - loss: 20.1269 - MinusLogProbMetric: 20.1269 - val_loss: 20.1489 - val_MinusLogProbMetric: 20.1489 - lr: 3.3333e-04 - 80s/epoch - 406ms/step
Epoch 190/1000
2023-09-27 21:24:53.651 
Epoch 190/1000 
	 loss: 20.1717, MinusLogProbMetric: 20.1717, val_loss: 20.4766, val_MinusLogProbMetric: 20.4766

Epoch 190: val_loss did not improve from 19.89370
196/196 - 80s - loss: 20.1717 - MinusLogProbMetric: 20.1717 - val_loss: 20.4766 - val_MinusLogProbMetric: 20.4766 - lr: 3.3333e-04 - 80s/epoch - 407ms/step
Epoch 191/1000
2023-09-27 21:26:13.226 
Epoch 191/1000 
	 loss: 19.9935, MinusLogProbMetric: 19.9935, val_loss: 19.8931, val_MinusLogProbMetric: 19.8931

Epoch 191: val_loss improved from 19.89370 to 19.89311, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 81s - loss: 19.9935 - MinusLogProbMetric: 19.9935 - val_loss: 19.8931 - val_MinusLogProbMetric: 19.8931 - lr: 3.3333e-04 - 81s/epoch - 412ms/step
Epoch 192/1000
2023-09-27 21:27:34.302 
Epoch 192/1000 
	 loss: 19.9468, MinusLogProbMetric: 19.9468, val_loss: 19.9809, val_MinusLogProbMetric: 19.9809

Epoch 192: val_loss did not improve from 19.89311
196/196 - 80s - loss: 19.9468 - MinusLogProbMetric: 19.9468 - val_loss: 19.9809 - val_MinusLogProbMetric: 19.9809 - lr: 3.3333e-04 - 80s/epoch - 407ms/step
Epoch 193/1000
2023-09-27 21:28:53.548 
Epoch 193/1000 
	 loss: 21.0635, MinusLogProbMetric: 21.0635, val_loss: 20.0780, val_MinusLogProbMetric: 20.0780

Epoch 193: val_loss did not improve from 19.89311
196/196 - 79s - loss: 21.0635 - MinusLogProbMetric: 21.0635 - val_loss: 20.0780 - val_MinusLogProbMetric: 20.0780 - lr: 3.3333e-04 - 79s/epoch - 404ms/step
Epoch 194/1000
2023-09-27 21:30:12.865 
Epoch 194/1000 
	 loss: 20.0248, MinusLogProbMetric: 20.0248, val_loss: 19.9565, val_MinusLogProbMetric: 19.9565

Epoch 194: val_loss did not improve from 19.89311
196/196 - 79s - loss: 20.0248 - MinusLogProbMetric: 20.0248 - val_loss: 19.9565 - val_MinusLogProbMetric: 19.9565 - lr: 3.3333e-04 - 79s/epoch - 405ms/step
Epoch 195/1000
2023-09-27 21:31:32.828 
Epoch 195/1000 
	 loss: 19.9845, MinusLogProbMetric: 19.9845, val_loss: 20.0188, val_MinusLogProbMetric: 20.0188

Epoch 195: val_loss did not improve from 19.89311
196/196 - 80s - loss: 19.9845 - MinusLogProbMetric: 19.9845 - val_loss: 20.0188 - val_MinusLogProbMetric: 20.0188 - lr: 3.3333e-04 - 80s/epoch - 408ms/step
Epoch 196/1000
2023-09-27 21:32:52.668 
Epoch 196/1000 
	 loss: 19.9029, MinusLogProbMetric: 19.9029, val_loss: 20.0666, val_MinusLogProbMetric: 20.0666

Epoch 196: val_loss did not improve from 19.89311
196/196 - 80s - loss: 19.9029 - MinusLogProbMetric: 19.9029 - val_loss: 20.0666 - val_MinusLogProbMetric: 20.0666 - lr: 3.3333e-04 - 80s/epoch - 407ms/step
Epoch 197/1000
2023-09-27 21:34:12.247 
Epoch 197/1000 
	 loss: 20.0423, MinusLogProbMetric: 20.0423, val_loss: 20.1542, val_MinusLogProbMetric: 20.1542

Epoch 197: val_loss did not improve from 19.89311
196/196 - 80s - loss: 20.0423 - MinusLogProbMetric: 20.0423 - val_loss: 20.1542 - val_MinusLogProbMetric: 20.1542 - lr: 3.3333e-04 - 80s/epoch - 406ms/step
Epoch 198/1000
2023-09-27 21:35:31.618 
Epoch 198/1000 
	 loss: 19.8424, MinusLogProbMetric: 19.8424, val_loss: 19.7177, val_MinusLogProbMetric: 19.7177

Epoch 198: val_loss improved from 19.89311 to 19.71774, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 81s - loss: 19.8424 - MinusLogProbMetric: 19.8424 - val_loss: 19.7177 - val_MinusLogProbMetric: 19.7177 - lr: 3.3333e-04 - 81s/epoch - 411ms/step
Epoch 199/1000
2023-09-27 21:36:52.530 
Epoch 199/1000 
	 loss: 19.8968, MinusLogProbMetric: 19.8968, val_loss: 21.0210, val_MinusLogProbMetric: 21.0210

Epoch 199: val_loss did not improve from 19.71774
196/196 - 80s - loss: 19.8968 - MinusLogProbMetric: 19.8968 - val_loss: 21.0210 - val_MinusLogProbMetric: 21.0210 - lr: 3.3333e-04 - 80s/epoch - 406ms/step
Epoch 200/1000
2023-09-27 21:38:11.865 
Epoch 200/1000 
	 loss: 19.9473, MinusLogProbMetric: 19.9473, val_loss: 19.8528, val_MinusLogProbMetric: 19.8528

Epoch 200: val_loss did not improve from 19.71774
196/196 - 79s - loss: 19.9473 - MinusLogProbMetric: 19.9473 - val_loss: 19.8528 - val_MinusLogProbMetric: 19.8528 - lr: 3.3333e-04 - 79s/epoch - 405ms/step
Epoch 201/1000
2023-09-27 21:39:30.959 
Epoch 201/1000 
	 loss: 19.8150, MinusLogProbMetric: 19.8150, val_loss: 19.8238, val_MinusLogProbMetric: 19.8238

Epoch 201: val_loss did not improve from 19.71774
196/196 - 79s - loss: 19.8150 - MinusLogProbMetric: 19.8150 - val_loss: 19.8238 - val_MinusLogProbMetric: 19.8238 - lr: 3.3333e-04 - 79s/epoch - 404ms/step
Epoch 202/1000
2023-09-27 21:40:50.715 
Epoch 202/1000 
	 loss: 19.8208, MinusLogProbMetric: 19.8208, val_loss: 20.0286, val_MinusLogProbMetric: 20.0286

Epoch 202: val_loss did not improve from 19.71774
196/196 - 80s - loss: 19.8208 - MinusLogProbMetric: 19.8208 - val_loss: 20.0286 - val_MinusLogProbMetric: 20.0286 - lr: 3.3333e-04 - 80s/epoch - 407ms/step
Epoch 203/1000
2023-09-27 21:42:10.363 
Epoch 203/1000 
	 loss: 19.9070, MinusLogProbMetric: 19.9070, val_loss: 19.8463, val_MinusLogProbMetric: 19.8463

Epoch 203: val_loss did not improve from 19.71774
196/196 - 80s - loss: 19.9070 - MinusLogProbMetric: 19.9070 - val_loss: 19.8463 - val_MinusLogProbMetric: 19.8463 - lr: 3.3333e-04 - 80s/epoch - 406ms/step
Epoch 204/1000
2023-09-27 21:43:29.979 
Epoch 204/1000 
	 loss: 19.6538, MinusLogProbMetric: 19.6538, val_loss: 19.6126, val_MinusLogProbMetric: 19.6126

Epoch 204: val_loss improved from 19.71774 to 19.61258, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 81s - loss: 19.6538 - MinusLogProbMetric: 19.6538 - val_loss: 19.6126 - val_MinusLogProbMetric: 19.6126 - lr: 3.3333e-04 - 81s/epoch - 413ms/step
Epoch 205/1000
2023-09-27 21:44:50.127 
Epoch 205/1000 
	 loss: 19.8510, MinusLogProbMetric: 19.8510, val_loss: 19.6497, val_MinusLogProbMetric: 19.6497

Epoch 205: val_loss did not improve from 19.61258
196/196 - 79s - loss: 19.8510 - MinusLogProbMetric: 19.8510 - val_loss: 19.6497 - val_MinusLogProbMetric: 19.6497 - lr: 3.3333e-04 - 79s/epoch - 403ms/step
Epoch 206/1000
2023-09-27 21:46:10.346 
Epoch 206/1000 
	 loss: 19.7463, MinusLogProbMetric: 19.7463, val_loss: 19.6571, val_MinusLogProbMetric: 19.6571

Epoch 206: val_loss did not improve from 19.61258
196/196 - 80s - loss: 19.7463 - MinusLogProbMetric: 19.7463 - val_loss: 19.6571 - val_MinusLogProbMetric: 19.6571 - lr: 3.3333e-04 - 80s/epoch - 409ms/step
Epoch 207/1000
2023-09-27 21:47:30.600 
Epoch 207/1000 
	 loss: 19.6513, MinusLogProbMetric: 19.6513, val_loss: 19.6031, val_MinusLogProbMetric: 19.6031

Epoch 207: val_loss improved from 19.61258 to 19.60310, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 81s - loss: 19.6513 - MinusLogProbMetric: 19.6513 - val_loss: 19.6031 - val_MinusLogProbMetric: 19.6031 - lr: 3.3333e-04 - 81s/epoch - 415ms/step
Epoch 208/1000
2023-09-27 21:48:51.528 
Epoch 208/1000 
	 loss: 19.7796, MinusLogProbMetric: 19.7796, val_loss: 19.7743, val_MinusLogProbMetric: 19.7743

Epoch 208: val_loss did not improve from 19.60310
196/196 - 80s - loss: 19.7796 - MinusLogProbMetric: 19.7796 - val_loss: 19.7743 - val_MinusLogProbMetric: 19.7743 - lr: 3.3333e-04 - 80s/epoch - 407ms/step
Epoch 209/1000
2023-09-27 21:50:11.768 
Epoch 209/1000 
	 loss: 19.8295, MinusLogProbMetric: 19.8295, val_loss: 20.1188, val_MinusLogProbMetric: 20.1188

Epoch 209: val_loss did not improve from 19.60310
196/196 - 80s - loss: 19.8295 - MinusLogProbMetric: 19.8295 - val_loss: 20.1188 - val_MinusLogProbMetric: 20.1188 - lr: 3.3333e-04 - 80s/epoch - 409ms/step
Epoch 210/1000
2023-09-27 21:51:31.738 
Epoch 210/1000 
	 loss: 19.6002, MinusLogProbMetric: 19.6002, val_loss: 19.5677, val_MinusLogProbMetric: 19.5677

Epoch 210: val_loss improved from 19.60310 to 19.56771, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 81s - loss: 19.6002 - MinusLogProbMetric: 19.6002 - val_loss: 19.5677 - val_MinusLogProbMetric: 19.5677 - lr: 3.3333e-04 - 81s/epoch - 414ms/step
Epoch 211/1000
2023-09-27 21:52:52.682 
Epoch 211/1000 
	 loss: 19.6654, MinusLogProbMetric: 19.6654, val_loss: 21.1799, val_MinusLogProbMetric: 21.1799

Epoch 211: val_loss did not improve from 19.56771
196/196 - 80s - loss: 19.6654 - MinusLogProbMetric: 19.6654 - val_loss: 21.1799 - val_MinusLogProbMetric: 21.1799 - lr: 3.3333e-04 - 80s/epoch - 407ms/step
Epoch 212/1000
2023-09-27 21:54:12.420 
Epoch 212/1000 
	 loss: 19.7784, MinusLogProbMetric: 19.7784, val_loss: 19.8322, val_MinusLogProbMetric: 19.8322

Epoch 212: val_loss did not improve from 19.56771
196/196 - 80s - loss: 19.7784 - MinusLogProbMetric: 19.7784 - val_loss: 19.8322 - val_MinusLogProbMetric: 19.8322 - lr: 3.3333e-04 - 80s/epoch - 407ms/step
Epoch 213/1000
2023-09-27 21:55:32.289 
Epoch 213/1000 
	 loss: 20.1596, MinusLogProbMetric: 20.1596, val_loss: 20.1362, val_MinusLogProbMetric: 20.1362

Epoch 213: val_loss did not improve from 19.56771
196/196 - 80s - loss: 20.1596 - MinusLogProbMetric: 20.1596 - val_loss: 20.1362 - val_MinusLogProbMetric: 20.1362 - lr: 3.3333e-04 - 80s/epoch - 407ms/step
Epoch 214/1000
2023-09-27 21:56:51.841 
Epoch 214/1000 
	 loss: 19.6963, MinusLogProbMetric: 19.6963, val_loss: 19.6624, val_MinusLogProbMetric: 19.6624

Epoch 214: val_loss did not improve from 19.56771
196/196 - 80s - loss: 19.6963 - MinusLogProbMetric: 19.6963 - val_loss: 19.6624 - val_MinusLogProbMetric: 19.6624 - lr: 3.3333e-04 - 80s/epoch - 406ms/step
Epoch 215/1000
2023-09-27 21:58:11.931 
Epoch 215/1000 
	 loss: 19.5518, MinusLogProbMetric: 19.5518, val_loss: 19.5970, val_MinusLogProbMetric: 19.5970

Epoch 215: val_loss did not improve from 19.56771
196/196 - 80s - loss: 19.5518 - MinusLogProbMetric: 19.5518 - val_loss: 19.5970 - val_MinusLogProbMetric: 19.5970 - lr: 3.3333e-04 - 80s/epoch - 409ms/step
Epoch 216/1000
2023-09-27 21:59:31.449 
Epoch 216/1000 
	 loss: 19.8039, MinusLogProbMetric: 19.8039, val_loss: 19.8644, val_MinusLogProbMetric: 19.8644

Epoch 216: val_loss did not improve from 19.56771
196/196 - 80s - loss: 19.8039 - MinusLogProbMetric: 19.8039 - val_loss: 19.8644 - val_MinusLogProbMetric: 19.8644 - lr: 3.3333e-04 - 80s/epoch - 406ms/step
Epoch 217/1000
2023-09-27 22:00:51.483 
Epoch 217/1000 
	 loss: 19.5690, MinusLogProbMetric: 19.5690, val_loss: 19.8041, val_MinusLogProbMetric: 19.8041

Epoch 217: val_loss did not improve from 19.56771
196/196 - 80s - loss: 19.5690 - MinusLogProbMetric: 19.5690 - val_loss: 19.8041 - val_MinusLogProbMetric: 19.8041 - lr: 3.3333e-04 - 80s/epoch - 408ms/step
Epoch 218/1000
2023-09-27 22:02:11.488 
Epoch 218/1000 
	 loss: 19.5876, MinusLogProbMetric: 19.5876, val_loss: 19.6134, val_MinusLogProbMetric: 19.6134

Epoch 218: val_loss did not improve from 19.56771
196/196 - 80s - loss: 19.5876 - MinusLogProbMetric: 19.5876 - val_loss: 19.6134 - val_MinusLogProbMetric: 19.6134 - lr: 3.3333e-04 - 80s/epoch - 408ms/step
Epoch 219/1000
2023-09-27 22:03:31.253 
Epoch 219/1000 
	 loss: 19.4619, MinusLogProbMetric: 19.4619, val_loss: 20.1081, val_MinusLogProbMetric: 20.1081

Epoch 219: val_loss did not improve from 19.56771
196/196 - 80s - loss: 19.4619 - MinusLogProbMetric: 19.4619 - val_loss: 20.1081 - val_MinusLogProbMetric: 20.1081 - lr: 3.3333e-04 - 80s/epoch - 407ms/step
Epoch 220/1000
2023-09-27 22:04:50.810 
Epoch 220/1000 
	 loss: 19.5163, MinusLogProbMetric: 19.5163, val_loss: 19.9948, val_MinusLogProbMetric: 19.9948

Epoch 220: val_loss did not improve from 19.56771
196/196 - 80s - loss: 19.5163 - MinusLogProbMetric: 19.5163 - val_loss: 19.9948 - val_MinusLogProbMetric: 19.9948 - lr: 3.3333e-04 - 80s/epoch - 406ms/step
Epoch 221/1000
2023-09-27 22:06:10.516 
Epoch 221/1000 
	 loss: 19.5527, MinusLogProbMetric: 19.5527, val_loss: 19.7247, val_MinusLogProbMetric: 19.7247

Epoch 221: val_loss did not improve from 19.56771
196/196 - 80s - loss: 19.5527 - MinusLogProbMetric: 19.5527 - val_loss: 19.7247 - val_MinusLogProbMetric: 19.7247 - lr: 3.3333e-04 - 80s/epoch - 407ms/step
Epoch 222/1000
2023-09-27 22:07:30.058 
Epoch 222/1000 
	 loss: 19.4579, MinusLogProbMetric: 19.4579, val_loss: 19.3215, val_MinusLogProbMetric: 19.3215

Epoch 222: val_loss improved from 19.56771 to 19.32154, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 81s - loss: 19.4579 - MinusLogProbMetric: 19.4579 - val_loss: 19.3215 - val_MinusLogProbMetric: 19.3215 - lr: 3.3333e-04 - 81s/epoch - 413ms/step
Epoch 223/1000
2023-09-27 22:08:49.713 
Epoch 223/1000 
	 loss: 19.4650, MinusLogProbMetric: 19.4650, val_loss: 19.7000, val_MinusLogProbMetric: 19.7000

Epoch 223: val_loss did not improve from 19.32154
196/196 - 78s - loss: 19.4650 - MinusLogProbMetric: 19.4650 - val_loss: 19.7000 - val_MinusLogProbMetric: 19.7000 - lr: 3.3333e-04 - 78s/epoch - 399ms/step
Epoch 224/1000
2023-09-27 22:09:57.956 
Epoch 224/1000 
	 loss: 19.6289, MinusLogProbMetric: 19.6289, val_loss: 19.7359, val_MinusLogProbMetric: 19.7359

Epoch 224: val_loss did not improve from 19.32154
196/196 - 68s - loss: 19.6289 - MinusLogProbMetric: 19.6289 - val_loss: 19.7359 - val_MinusLogProbMetric: 19.7359 - lr: 3.3333e-04 - 68s/epoch - 348ms/step
Epoch 225/1000
2023-09-27 22:11:13.417 
Epoch 225/1000 
	 loss: 19.4103, MinusLogProbMetric: 19.4103, val_loss: 19.7044, val_MinusLogProbMetric: 19.7044

Epoch 225: val_loss did not improve from 19.32154
196/196 - 75s - loss: 19.4103 - MinusLogProbMetric: 19.4103 - val_loss: 19.7044 - val_MinusLogProbMetric: 19.7044 - lr: 3.3333e-04 - 75s/epoch - 385ms/step
Epoch 226/1000
2023-09-27 22:12:33.397 
Epoch 226/1000 
	 loss: 19.5738, MinusLogProbMetric: 19.5738, val_loss: 19.2741, val_MinusLogProbMetric: 19.2741

Epoch 226: val_loss improved from 19.32154 to 19.27411, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 81s - loss: 19.5738 - MinusLogProbMetric: 19.5738 - val_loss: 19.2741 - val_MinusLogProbMetric: 19.2741 - lr: 3.3333e-04 - 81s/epoch - 414ms/step
Epoch 227/1000
2023-09-27 22:13:53.867 
Epoch 227/1000 
	 loss: 19.2966, MinusLogProbMetric: 19.2966, val_loss: 19.2893, val_MinusLogProbMetric: 19.2893

Epoch 227: val_loss did not improve from 19.27411
196/196 - 79s - loss: 19.2966 - MinusLogProbMetric: 19.2966 - val_loss: 19.2893 - val_MinusLogProbMetric: 19.2893 - lr: 3.3333e-04 - 79s/epoch - 404ms/step
Epoch 228/1000
2023-09-27 22:15:13.535 
Epoch 228/1000 
	 loss: 19.5094, MinusLogProbMetric: 19.5094, val_loss: 19.7080, val_MinusLogProbMetric: 19.7080

Epoch 228: val_loss did not improve from 19.27411
196/196 - 80s - loss: 19.5094 - MinusLogProbMetric: 19.5094 - val_loss: 19.7080 - val_MinusLogProbMetric: 19.7080 - lr: 3.3333e-04 - 80s/epoch - 406ms/step
Epoch 229/1000
2023-09-27 22:16:33.148 
Epoch 229/1000 
	 loss: 19.4990, MinusLogProbMetric: 19.4990, val_loss: 19.8938, val_MinusLogProbMetric: 19.8938

Epoch 229: val_loss did not improve from 19.27411
196/196 - 80s - loss: 19.4990 - MinusLogProbMetric: 19.4990 - val_loss: 19.8938 - val_MinusLogProbMetric: 19.8938 - lr: 3.3333e-04 - 80s/epoch - 406ms/step
Epoch 230/1000
2023-09-27 22:17:54.342 
Epoch 230/1000 
	 loss: 19.5305, MinusLogProbMetric: 19.5305, val_loss: 19.4846, val_MinusLogProbMetric: 19.4846

Epoch 230: val_loss did not improve from 19.27411
196/196 - 81s - loss: 19.5305 - MinusLogProbMetric: 19.5305 - val_loss: 19.4846 - val_MinusLogProbMetric: 19.4846 - lr: 3.3333e-04 - 81s/epoch - 414ms/step
Epoch 231/1000
2023-09-27 22:19:16.781 
Epoch 231/1000 
	 loss: 19.4223, MinusLogProbMetric: 19.4223, val_loss: 19.4511, val_MinusLogProbMetric: 19.4511

Epoch 231: val_loss did not improve from 19.27411
196/196 - 82s - loss: 19.4223 - MinusLogProbMetric: 19.4223 - val_loss: 19.4511 - val_MinusLogProbMetric: 19.4511 - lr: 3.3333e-04 - 82s/epoch - 421ms/step
Epoch 232/1000
2023-09-27 22:20:36.748 
Epoch 232/1000 
	 loss: 19.3404, MinusLogProbMetric: 19.3404, val_loss: 19.6784, val_MinusLogProbMetric: 19.6784

Epoch 232: val_loss did not improve from 19.27411
196/196 - 80s - loss: 19.3404 - MinusLogProbMetric: 19.3404 - val_loss: 19.6784 - val_MinusLogProbMetric: 19.6784 - lr: 3.3333e-04 - 80s/epoch - 408ms/step
Epoch 233/1000
2023-09-27 22:21:56.672 
Epoch 233/1000 
	 loss: 20.4999, MinusLogProbMetric: 20.4999, val_loss: 20.0857, val_MinusLogProbMetric: 20.0857

Epoch 233: val_loss did not improve from 19.27411
196/196 - 80s - loss: 20.4999 - MinusLogProbMetric: 20.4999 - val_loss: 20.0857 - val_MinusLogProbMetric: 20.0857 - lr: 3.3333e-04 - 80s/epoch - 408ms/step
Epoch 234/1000
2023-09-27 22:23:16.444 
Epoch 234/1000 
	 loss: 19.4385, MinusLogProbMetric: 19.4385, val_loss: 19.4758, val_MinusLogProbMetric: 19.4758

Epoch 234: val_loss did not improve from 19.27411
196/196 - 80s - loss: 19.4385 - MinusLogProbMetric: 19.4385 - val_loss: 19.4758 - val_MinusLogProbMetric: 19.4758 - lr: 3.3333e-04 - 80s/epoch - 407ms/step
Epoch 235/1000
2023-09-27 22:24:36.120 
Epoch 235/1000 
	 loss: 19.4713, MinusLogProbMetric: 19.4713, val_loss: 20.0407, val_MinusLogProbMetric: 20.0407

Epoch 235: val_loss did not improve from 19.27411
196/196 - 80s - loss: 19.4713 - MinusLogProbMetric: 19.4713 - val_loss: 20.0407 - val_MinusLogProbMetric: 20.0407 - lr: 3.3333e-04 - 80s/epoch - 406ms/step
Epoch 236/1000
2023-09-27 22:25:55.940 
Epoch 236/1000 
	 loss: 19.3003, MinusLogProbMetric: 19.3003, val_loss: 19.8931, val_MinusLogProbMetric: 19.8931

Epoch 236: val_loss did not improve from 19.27411
196/196 - 80s - loss: 19.3003 - MinusLogProbMetric: 19.3003 - val_loss: 19.8931 - val_MinusLogProbMetric: 19.8931 - lr: 3.3333e-04 - 80s/epoch - 407ms/step
Epoch 237/1000
2023-09-27 22:27:15.583 
Epoch 237/1000 
	 loss: 19.3642, MinusLogProbMetric: 19.3642, val_loss: 19.6805, val_MinusLogProbMetric: 19.6805

Epoch 237: val_loss did not improve from 19.27411
196/196 - 80s - loss: 19.3642 - MinusLogProbMetric: 19.3642 - val_loss: 19.6805 - val_MinusLogProbMetric: 19.6805 - lr: 3.3333e-04 - 80s/epoch - 406ms/step
Epoch 238/1000
2023-09-27 22:28:35.433 
Epoch 238/1000 
	 loss: 19.3098, MinusLogProbMetric: 19.3098, val_loss: 19.3799, val_MinusLogProbMetric: 19.3799

Epoch 238: val_loss did not improve from 19.27411
196/196 - 80s - loss: 19.3098 - MinusLogProbMetric: 19.3098 - val_loss: 19.3799 - val_MinusLogProbMetric: 19.3799 - lr: 3.3333e-04 - 80s/epoch - 407ms/step
Epoch 239/1000
2023-09-27 22:29:54.543 
Epoch 239/1000 
	 loss: 19.2971, MinusLogProbMetric: 19.2971, val_loss: 19.7025, val_MinusLogProbMetric: 19.7025

Epoch 239: val_loss did not improve from 19.27411
196/196 - 79s - loss: 19.2971 - MinusLogProbMetric: 19.2971 - val_loss: 19.7025 - val_MinusLogProbMetric: 19.7025 - lr: 3.3333e-04 - 79s/epoch - 404ms/step
Epoch 240/1000
2023-09-27 22:31:13.729 
Epoch 240/1000 
	 loss: 19.2327, MinusLogProbMetric: 19.2327, val_loss: 19.3089, val_MinusLogProbMetric: 19.3089

Epoch 240: val_loss did not improve from 19.27411
196/196 - 79s - loss: 19.2327 - MinusLogProbMetric: 19.2327 - val_loss: 19.3089 - val_MinusLogProbMetric: 19.3089 - lr: 3.3333e-04 - 79s/epoch - 404ms/step
Epoch 241/1000
2023-09-27 22:32:33.224 
Epoch 241/1000 
	 loss: 19.3695, MinusLogProbMetric: 19.3695, val_loss: 19.1708, val_MinusLogProbMetric: 19.1708

Epoch 241: val_loss improved from 19.27411 to 19.17079, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 81s - loss: 19.3695 - MinusLogProbMetric: 19.3695 - val_loss: 19.1708 - val_MinusLogProbMetric: 19.1708 - lr: 3.3333e-04 - 81s/epoch - 412ms/step
Epoch 242/1000
2023-09-27 22:33:54.413 
Epoch 242/1000 
	 loss: 19.3291, MinusLogProbMetric: 19.3291, val_loss: 19.2414, val_MinusLogProbMetric: 19.2414

Epoch 242: val_loss did not improve from 19.17079
196/196 - 80s - loss: 19.3291 - MinusLogProbMetric: 19.3291 - val_loss: 19.2414 - val_MinusLogProbMetric: 19.2414 - lr: 3.3333e-04 - 80s/epoch - 408ms/step
Epoch 243/1000
2023-09-27 22:35:13.411 
Epoch 243/1000 
	 loss: 19.2303, MinusLogProbMetric: 19.2303, val_loss: 19.2546, val_MinusLogProbMetric: 19.2546

Epoch 243: val_loss did not improve from 19.17079
196/196 - 79s - loss: 19.2303 - MinusLogProbMetric: 19.2303 - val_loss: 19.2546 - val_MinusLogProbMetric: 19.2546 - lr: 3.3333e-04 - 79s/epoch - 403ms/step
Epoch 244/1000
2023-09-27 22:36:33.274 
Epoch 244/1000 
	 loss: 19.4126, MinusLogProbMetric: 19.4126, val_loss: 19.4845, val_MinusLogProbMetric: 19.4845

Epoch 244: val_loss did not improve from 19.17079
196/196 - 80s - loss: 19.4126 - MinusLogProbMetric: 19.4126 - val_loss: 19.4845 - val_MinusLogProbMetric: 19.4845 - lr: 3.3333e-04 - 80s/epoch - 407ms/step
Epoch 245/1000
2023-09-27 22:37:52.686 
Epoch 245/1000 
	 loss: 19.2227, MinusLogProbMetric: 19.2227, val_loss: 19.1580, val_MinusLogProbMetric: 19.1580

Epoch 245: val_loss improved from 19.17079 to 19.15796, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 81s - loss: 19.2227 - MinusLogProbMetric: 19.2227 - val_loss: 19.1580 - val_MinusLogProbMetric: 19.1580 - lr: 3.3333e-04 - 81s/epoch - 412ms/step
Epoch 246/1000
2023-09-27 22:39:14.271 
Epoch 246/1000 
	 loss: 19.1476, MinusLogProbMetric: 19.1476, val_loss: 19.1059, val_MinusLogProbMetric: 19.1059

Epoch 246: val_loss improved from 19.15796 to 19.10588, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 82s - loss: 19.1476 - MinusLogProbMetric: 19.1476 - val_loss: 19.1059 - val_MinusLogProbMetric: 19.1059 - lr: 3.3333e-04 - 82s/epoch - 416ms/step
Epoch 247/1000
2023-09-27 22:40:35.295 
Epoch 247/1000 
	 loss: 19.2108, MinusLogProbMetric: 19.2108, val_loss: 20.2402, val_MinusLogProbMetric: 20.2402

Epoch 247: val_loss did not improve from 19.10588
196/196 - 80s - loss: 19.2108 - MinusLogProbMetric: 19.2108 - val_loss: 20.2402 - val_MinusLogProbMetric: 20.2402 - lr: 3.3333e-04 - 80s/epoch - 406ms/step
Epoch 248/1000
2023-09-27 22:41:55.245 
Epoch 248/1000 
	 loss: 19.2658, MinusLogProbMetric: 19.2658, val_loss: 19.4108, val_MinusLogProbMetric: 19.4108

Epoch 248: val_loss did not improve from 19.10588
196/196 - 80s - loss: 19.2658 - MinusLogProbMetric: 19.2658 - val_loss: 19.4108 - val_MinusLogProbMetric: 19.4108 - lr: 3.3333e-04 - 80s/epoch - 408ms/step
Epoch 249/1000
2023-09-27 22:43:14.814 
Epoch 249/1000 
	 loss: 19.0978, MinusLogProbMetric: 19.0978, val_loss: 20.5395, val_MinusLogProbMetric: 20.5395

Epoch 249: val_loss did not improve from 19.10588
196/196 - 80s - loss: 19.0978 - MinusLogProbMetric: 19.0978 - val_loss: 20.5395 - val_MinusLogProbMetric: 20.5395 - lr: 3.3333e-04 - 80s/epoch - 406ms/step
Epoch 250/1000
2023-09-27 22:44:34.213 
Epoch 250/1000 
	 loss: 19.3083, MinusLogProbMetric: 19.3083, val_loss: 19.1634, val_MinusLogProbMetric: 19.1634

Epoch 250: val_loss did not improve from 19.10588
196/196 - 79s - loss: 19.3083 - MinusLogProbMetric: 19.3083 - val_loss: 19.1634 - val_MinusLogProbMetric: 19.1634 - lr: 3.3333e-04 - 79s/epoch - 405ms/step
Epoch 251/1000
2023-09-27 22:45:53.771 
Epoch 251/1000 
	 loss: 19.2022, MinusLogProbMetric: 19.2022, val_loss: 19.3683, val_MinusLogProbMetric: 19.3683

Epoch 251: val_loss did not improve from 19.10588
196/196 - 80s - loss: 19.2022 - MinusLogProbMetric: 19.2022 - val_loss: 19.3683 - val_MinusLogProbMetric: 19.3683 - lr: 3.3333e-04 - 80s/epoch - 406ms/step
Epoch 252/1000
2023-09-27 22:47:13.379 
Epoch 252/1000 
	 loss: 19.1008, MinusLogProbMetric: 19.1008, val_loss: 19.1817, val_MinusLogProbMetric: 19.1817

Epoch 252: val_loss did not improve from 19.10588
196/196 - 80s - loss: 19.1008 - MinusLogProbMetric: 19.1008 - val_loss: 19.1817 - val_MinusLogProbMetric: 19.1817 - lr: 3.3333e-04 - 80s/epoch - 406ms/step
Epoch 253/1000
2023-09-27 22:48:33.860 
Epoch 253/1000 
	 loss: 19.1782, MinusLogProbMetric: 19.1782, val_loss: 19.2633, val_MinusLogProbMetric: 19.2633

Epoch 253: val_loss did not improve from 19.10588
196/196 - 80s - loss: 19.1782 - MinusLogProbMetric: 19.1782 - val_loss: 19.2633 - val_MinusLogProbMetric: 19.2633 - lr: 3.3333e-04 - 80s/epoch - 411ms/step
Epoch 254/1000
2023-09-27 22:49:53.241 
Epoch 254/1000 
	 loss: 19.1806, MinusLogProbMetric: 19.1806, val_loss: 19.1362, val_MinusLogProbMetric: 19.1362

Epoch 254: val_loss did not improve from 19.10588
196/196 - 79s - loss: 19.1806 - MinusLogProbMetric: 19.1806 - val_loss: 19.1362 - val_MinusLogProbMetric: 19.1362 - lr: 3.3333e-04 - 79s/epoch - 405ms/step
Epoch 255/1000
2023-09-27 22:51:12.865 
Epoch 255/1000 
	 loss: 19.1526, MinusLogProbMetric: 19.1526, val_loss: 19.6923, val_MinusLogProbMetric: 19.6923

Epoch 255: val_loss did not improve from 19.10588
196/196 - 80s - loss: 19.1526 - MinusLogProbMetric: 19.1526 - val_loss: 19.6923 - val_MinusLogProbMetric: 19.6923 - lr: 3.3333e-04 - 80s/epoch - 406ms/step
Epoch 256/1000
2023-09-27 22:52:32.313 
Epoch 256/1000 
	 loss: 19.1426, MinusLogProbMetric: 19.1426, val_loss: 19.0220, val_MinusLogProbMetric: 19.0220

Epoch 256: val_loss improved from 19.10588 to 19.02196, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 81s - loss: 19.1426 - MinusLogProbMetric: 19.1426 - val_loss: 19.0220 - val_MinusLogProbMetric: 19.0220 - lr: 3.3333e-04 - 81s/epoch - 412ms/step
Epoch 257/1000
2023-09-27 22:53:52.768 
Epoch 257/1000 
	 loss: 19.2213, MinusLogProbMetric: 19.2213, val_loss: 19.4037, val_MinusLogProbMetric: 19.4037

Epoch 257: val_loss did not improve from 19.02196
196/196 - 79s - loss: 19.2213 - MinusLogProbMetric: 19.2213 - val_loss: 19.4037 - val_MinusLogProbMetric: 19.4037 - lr: 3.3333e-04 - 79s/epoch - 404ms/step
Epoch 258/1000
2023-09-27 22:55:12.383 
Epoch 258/1000 
	 loss: 21.1195, MinusLogProbMetric: 21.1195, val_loss: 26.5150, val_MinusLogProbMetric: 26.5150

Epoch 258: val_loss did not improve from 19.02196
196/196 - 80s - loss: 21.1195 - MinusLogProbMetric: 21.1195 - val_loss: 26.5150 - val_MinusLogProbMetric: 26.5150 - lr: 3.3333e-04 - 80s/epoch - 406ms/step
Epoch 259/1000
2023-09-27 22:56:32.514 
Epoch 259/1000 
	 loss: 21.8985, MinusLogProbMetric: 21.8985, val_loss: 20.6119, val_MinusLogProbMetric: 20.6119

Epoch 259: val_loss did not improve from 19.02196
196/196 - 80s - loss: 21.8985 - MinusLogProbMetric: 21.8985 - val_loss: 20.6119 - val_MinusLogProbMetric: 20.6119 - lr: 3.3333e-04 - 80s/epoch - 409ms/step
Epoch 260/1000
2023-09-27 22:57:51.310 
Epoch 260/1000 
	 loss: 20.1855, MinusLogProbMetric: 20.1855, val_loss: 19.8248, val_MinusLogProbMetric: 19.8248

Epoch 260: val_loss did not improve from 19.02196
196/196 - 79s - loss: 20.1855 - MinusLogProbMetric: 20.1855 - val_loss: 19.8248 - val_MinusLogProbMetric: 19.8248 - lr: 3.3333e-04 - 79s/epoch - 402ms/step
Epoch 261/1000
2023-09-27 22:59:09.888 
Epoch 261/1000 
	 loss: 19.6862, MinusLogProbMetric: 19.6862, val_loss: 19.4997, val_MinusLogProbMetric: 19.4997

Epoch 261: val_loss did not improve from 19.02196
196/196 - 79s - loss: 19.6862 - MinusLogProbMetric: 19.6862 - val_loss: 19.4997 - val_MinusLogProbMetric: 19.4997 - lr: 3.3333e-04 - 79s/epoch - 401ms/step
Epoch 262/1000
2023-09-27 23:00:29.339 
Epoch 262/1000 
	 loss: 19.4526, MinusLogProbMetric: 19.4526, val_loss: 19.8387, val_MinusLogProbMetric: 19.8387

Epoch 262: val_loss did not improve from 19.02196
196/196 - 79s - loss: 19.4526 - MinusLogProbMetric: 19.4526 - val_loss: 19.8387 - val_MinusLogProbMetric: 19.8387 - lr: 3.3333e-04 - 79s/epoch - 405ms/step
Epoch 263/1000
2023-09-27 23:01:48.845 
Epoch 263/1000 
	 loss: 19.3562, MinusLogProbMetric: 19.3562, val_loss: 19.6289, val_MinusLogProbMetric: 19.6289

Epoch 263: val_loss did not improve from 19.02196
196/196 - 80s - loss: 19.3562 - MinusLogProbMetric: 19.3562 - val_loss: 19.6289 - val_MinusLogProbMetric: 19.6289 - lr: 3.3333e-04 - 80s/epoch - 406ms/step
Epoch 264/1000
2023-09-27 23:03:08.474 
Epoch 264/1000 
	 loss: 19.2940, MinusLogProbMetric: 19.2940, val_loss: 19.1231, val_MinusLogProbMetric: 19.1231

Epoch 264: val_loss did not improve from 19.02196
196/196 - 80s - loss: 19.2940 - MinusLogProbMetric: 19.2940 - val_loss: 19.1231 - val_MinusLogProbMetric: 19.1231 - lr: 3.3333e-04 - 80s/epoch - 406ms/step
Epoch 265/1000
2023-09-27 23:04:29.027 
Epoch 265/1000 
	 loss: 19.2442, MinusLogProbMetric: 19.2442, val_loss: 19.0423, val_MinusLogProbMetric: 19.0423

Epoch 265: val_loss did not improve from 19.02196
196/196 - 81s - loss: 19.2442 - MinusLogProbMetric: 19.2442 - val_loss: 19.0423 - val_MinusLogProbMetric: 19.0423 - lr: 3.3333e-04 - 81s/epoch - 411ms/step
Epoch 266/1000
2023-09-27 23:05:49.225 
Epoch 266/1000 
	 loss: 19.1432, MinusLogProbMetric: 19.1432, val_loss: 19.2210, val_MinusLogProbMetric: 19.2210

Epoch 266: val_loss did not improve from 19.02196
196/196 - 80s - loss: 19.1432 - MinusLogProbMetric: 19.1432 - val_loss: 19.2210 - val_MinusLogProbMetric: 19.2210 - lr: 3.3333e-04 - 80s/epoch - 409ms/step
Epoch 267/1000
2023-09-27 23:07:08.406 
Epoch 267/1000 
	 loss: 19.1668, MinusLogProbMetric: 19.1668, val_loss: 19.1154, val_MinusLogProbMetric: 19.1154

Epoch 267: val_loss did not improve from 19.02196
196/196 - 79s - loss: 19.1668 - MinusLogProbMetric: 19.1668 - val_loss: 19.1154 - val_MinusLogProbMetric: 19.1154 - lr: 3.3333e-04 - 79s/epoch - 404ms/step
Epoch 268/1000
2023-09-27 23:08:28.890 
Epoch 268/1000 
	 loss: 18.9885, MinusLogProbMetric: 18.9885, val_loss: 19.2935, val_MinusLogProbMetric: 19.2935

Epoch 268: val_loss did not improve from 19.02196
196/196 - 80s - loss: 18.9885 - MinusLogProbMetric: 18.9885 - val_loss: 19.2935 - val_MinusLogProbMetric: 19.2935 - lr: 3.3333e-04 - 80s/epoch - 411ms/step
Epoch 269/1000
2023-09-27 23:09:48.654 
Epoch 269/1000 
	 loss: 19.0205, MinusLogProbMetric: 19.0205, val_loss: 19.1109, val_MinusLogProbMetric: 19.1109

Epoch 269: val_loss did not improve from 19.02196
196/196 - 80s - loss: 19.0205 - MinusLogProbMetric: 19.0205 - val_loss: 19.1109 - val_MinusLogProbMetric: 19.1109 - lr: 3.3333e-04 - 80s/epoch - 407ms/step
Epoch 270/1000
2023-09-27 23:11:08.392 
Epoch 270/1000 
	 loss: 19.0259, MinusLogProbMetric: 19.0259, val_loss: 19.0920, val_MinusLogProbMetric: 19.0920

Epoch 270: val_loss did not improve from 19.02196
196/196 - 80s - loss: 19.0259 - MinusLogProbMetric: 19.0259 - val_loss: 19.0920 - val_MinusLogProbMetric: 19.0920 - lr: 3.3333e-04 - 80s/epoch - 407ms/step
Epoch 271/1000
2023-09-27 23:12:27.835 
Epoch 271/1000 
	 loss: 19.0154, MinusLogProbMetric: 19.0154, val_loss: 19.2270, val_MinusLogProbMetric: 19.2270

Epoch 271: val_loss did not improve from 19.02196
196/196 - 79s - loss: 19.0154 - MinusLogProbMetric: 19.0154 - val_loss: 19.2270 - val_MinusLogProbMetric: 19.2270 - lr: 3.3333e-04 - 79s/epoch - 405ms/step
Epoch 272/1000
2023-09-27 23:13:47.217 
Epoch 272/1000 
	 loss: 19.0539, MinusLogProbMetric: 19.0539, val_loss: 19.2650, val_MinusLogProbMetric: 19.2650

Epoch 272: val_loss did not improve from 19.02196
196/196 - 79s - loss: 19.0539 - MinusLogProbMetric: 19.0539 - val_loss: 19.2650 - val_MinusLogProbMetric: 19.2650 - lr: 3.3333e-04 - 79s/epoch - 405ms/step
Epoch 273/1000
2023-09-27 23:15:06.977 
Epoch 273/1000 
	 loss: 19.0122, MinusLogProbMetric: 19.0122, val_loss: 19.0090, val_MinusLogProbMetric: 19.0090

Epoch 273: val_loss improved from 19.02196 to 19.00897, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 81s - loss: 19.0122 - MinusLogProbMetric: 19.0122 - val_loss: 19.0090 - val_MinusLogProbMetric: 19.0090 - lr: 3.3333e-04 - 81s/epoch - 413ms/step
Epoch 274/1000
2023-09-27 23:16:27.725 
Epoch 274/1000 
	 loss: 18.9907, MinusLogProbMetric: 18.9907, val_loss: 19.2977, val_MinusLogProbMetric: 19.2977

Epoch 274: val_loss did not improve from 19.00897
196/196 - 80s - loss: 18.9907 - MinusLogProbMetric: 18.9907 - val_loss: 19.2977 - val_MinusLogProbMetric: 19.2977 - lr: 3.3333e-04 - 80s/epoch - 406ms/step
Epoch 275/1000
2023-09-27 23:17:47.240 
Epoch 275/1000 
	 loss: 18.9238, MinusLogProbMetric: 18.9238, val_loss: 19.2234, val_MinusLogProbMetric: 19.2234

Epoch 275: val_loss did not improve from 19.00897
196/196 - 80s - loss: 18.9238 - MinusLogProbMetric: 18.9238 - val_loss: 19.2234 - val_MinusLogProbMetric: 19.2234 - lr: 3.3333e-04 - 80s/epoch - 406ms/step
Epoch 276/1000
2023-09-27 23:19:06.785 
Epoch 276/1000 
	 loss: 19.0134, MinusLogProbMetric: 19.0134, val_loss: 19.2253, val_MinusLogProbMetric: 19.2253

Epoch 276: val_loss did not improve from 19.00897
196/196 - 80s - loss: 19.0134 - MinusLogProbMetric: 19.0134 - val_loss: 19.2253 - val_MinusLogProbMetric: 19.2253 - lr: 3.3333e-04 - 80s/epoch - 406ms/step
Epoch 277/1000
2023-09-27 23:20:26.392 
Epoch 277/1000 
	 loss: 18.9795, MinusLogProbMetric: 18.9795, val_loss: 18.9652, val_MinusLogProbMetric: 18.9652

Epoch 277: val_loss improved from 19.00897 to 18.96525, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 81s - loss: 18.9795 - MinusLogProbMetric: 18.9795 - val_loss: 18.9652 - val_MinusLogProbMetric: 18.9652 - lr: 3.3333e-04 - 81s/epoch - 413ms/step
Epoch 278/1000
2023-09-27 23:21:47.335 
Epoch 278/1000 
	 loss: 18.9085, MinusLogProbMetric: 18.9085, val_loss: 19.7124, val_MinusLogProbMetric: 19.7124

Epoch 278: val_loss did not improve from 18.96525
196/196 - 80s - loss: 18.9085 - MinusLogProbMetric: 18.9085 - val_loss: 19.7124 - val_MinusLogProbMetric: 19.7124 - lr: 3.3333e-04 - 80s/epoch - 407ms/step
Epoch 279/1000
2023-09-27 23:23:07.118 
Epoch 279/1000 
	 loss: 18.9548, MinusLogProbMetric: 18.9548, val_loss: 19.1891, val_MinusLogProbMetric: 19.1891

Epoch 279: val_loss did not improve from 18.96525
196/196 - 80s - loss: 18.9548 - MinusLogProbMetric: 18.9548 - val_loss: 19.1891 - val_MinusLogProbMetric: 19.1891 - lr: 3.3333e-04 - 80s/epoch - 407ms/step
Epoch 280/1000
2023-09-27 23:24:26.990 
Epoch 280/1000 
	 loss: 19.0370, MinusLogProbMetric: 19.0370, val_loss: 18.9497, val_MinusLogProbMetric: 18.9497

Epoch 280: val_loss improved from 18.96525 to 18.94971, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 81s - loss: 19.0370 - MinusLogProbMetric: 19.0370 - val_loss: 18.9497 - val_MinusLogProbMetric: 18.9497 - lr: 3.3333e-04 - 81s/epoch - 414ms/step
Epoch 281/1000
2023-09-27 23:25:46.941 
Epoch 281/1000 
	 loss: 18.8771, MinusLogProbMetric: 18.8771, val_loss: 18.9904, val_MinusLogProbMetric: 18.9904

Epoch 281: val_loss did not improve from 18.94971
196/196 - 79s - loss: 18.8771 - MinusLogProbMetric: 18.8771 - val_loss: 18.9904 - val_MinusLogProbMetric: 18.9904 - lr: 3.3333e-04 - 79s/epoch - 401ms/step
Epoch 282/1000
2023-09-27 23:26:59.169 
Epoch 282/1000 
	 loss: 18.8714, MinusLogProbMetric: 18.8714, val_loss: 19.7396, val_MinusLogProbMetric: 19.7396

Epoch 282: val_loss did not improve from 18.94971
196/196 - 72s - loss: 18.8714 - MinusLogProbMetric: 18.8714 - val_loss: 19.7396 - val_MinusLogProbMetric: 19.7396 - lr: 3.3333e-04 - 72s/epoch - 368ms/step
Epoch 283/1000
2023-09-27 23:28:18.105 
Epoch 283/1000 
	 loss: 19.3101, MinusLogProbMetric: 19.3101, val_loss: 19.1641, val_MinusLogProbMetric: 19.1641

Epoch 283: val_loss did not improve from 18.94971
196/196 - 79s - loss: 19.3101 - MinusLogProbMetric: 19.3101 - val_loss: 19.1641 - val_MinusLogProbMetric: 19.1641 - lr: 3.3333e-04 - 79s/epoch - 403ms/step
Epoch 284/1000
2023-09-27 23:29:37.774 
Epoch 284/1000 
	 loss: 18.9575, MinusLogProbMetric: 18.9575, val_loss: 19.0645, val_MinusLogProbMetric: 19.0645

Epoch 284: val_loss did not improve from 18.94971
196/196 - 80s - loss: 18.9575 - MinusLogProbMetric: 18.9575 - val_loss: 19.0645 - val_MinusLogProbMetric: 19.0645 - lr: 3.3333e-04 - 80s/epoch - 406ms/step
Epoch 285/1000
2023-09-27 23:30:56.709 
Epoch 285/1000 
	 loss: 18.9321, MinusLogProbMetric: 18.9321, val_loss: 18.7597, val_MinusLogProbMetric: 18.7597

Epoch 285: val_loss improved from 18.94971 to 18.75973, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 80s - loss: 18.9321 - MinusLogProbMetric: 18.9321 - val_loss: 18.7597 - val_MinusLogProbMetric: 18.7597 - lr: 3.3333e-04 - 80s/epoch - 410ms/step
Epoch 286/1000
2023-09-27 23:32:17.673 
Epoch 286/1000 
	 loss: 18.8873, MinusLogProbMetric: 18.8873, val_loss: 19.5917, val_MinusLogProbMetric: 19.5917

Epoch 286: val_loss did not improve from 18.75973
196/196 - 79s - loss: 18.8873 - MinusLogProbMetric: 18.8873 - val_loss: 19.5917 - val_MinusLogProbMetric: 19.5917 - lr: 3.3333e-04 - 79s/epoch - 405ms/step
Epoch 287/1000
2023-09-27 23:33:36.790 
Epoch 287/1000 
	 loss: 18.8795, MinusLogProbMetric: 18.8795, val_loss: 18.8777, val_MinusLogProbMetric: 18.8777

Epoch 287: val_loss did not improve from 18.75973
196/196 - 79s - loss: 18.8795 - MinusLogProbMetric: 18.8795 - val_loss: 18.8777 - val_MinusLogProbMetric: 18.8777 - lr: 3.3333e-04 - 79s/epoch - 404ms/step
Epoch 288/1000
2023-09-27 23:34:56.305 
Epoch 288/1000 
	 loss: 18.8164, MinusLogProbMetric: 18.8164, val_loss: 18.7449, val_MinusLogProbMetric: 18.7449

Epoch 288: val_loss improved from 18.75973 to 18.74490, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 81s - loss: 18.8164 - MinusLogProbMetric: 18.8164 - val_loss: 18.7449 - val_MinusLogProbMetric: 18.7449 - lr: 3.3333e-04 - 81s/epoch - 411ms/step
Epoch 289/1000
2023-09-27 23:36:16.747 
Epoch 289/1000 
	 loss: 18.8201, MinusLogProbMetric: 18.8201, val_loss: 19.5331, val_MinusLogProbMetric: 19.5331

Epoch 289: val_loss did not improve from 18.74490
196/196 - 79s - loss: 18.8201 - MinusLogProbMetric: 18.8201 - val_loss: 19.5331 - val_MinusLogProbMetric: 19.5331 - lr: 3.3333e-04 - 79s/epoch - 405ms/step
Epoch 290/1000
2023-09-27 23:37:36.470 
Epoch 290/1000 
	 loss: 18.9276, MinusLogProbMetric: 18.9276, val_loss: 19.1116, val_MinusLogProbMetric: 19.1116

Epoch 290: val_loss did not improve from 18.74490
196/196 - 80s - loss: 18.9276 - MinusLogProbMetric: 18.9276 - val_loss: 19.1116 - val_MinusLogProbMetric: 19.1116 - lr: 3.3333e-04 - 80s/epoch - 407ms/step
Epoch 291/1000
2023-09-27 23:38:55.880 
Epoch 291/1000 
	 loss: 18.8391, MinusLogProbMetric: 18.8391, val_loss: 18.8985, val_MinusLogProbMetric: 18.8985

Epoch 291: val_loss did not improve from 18.74490
196/196 - 79s - loss: 18.8391 - MinusLogProbMetric: 18.8391 - val_loss: 18.8985 - val_MinusLogProbMetric: 18.8985 - lr: 3.3333e-04 - 79s/epoch - 405ms/step
Epoch 292/1000
2023-09-27 23:40:15.242 
Epoch 292/1000 
	 loss: 18.7922, MinusLogProbMetric: 18.7922, val_loss: 18.8553, val_MinusLogProbMetric: 18.8553

Epoch 292: val_loss did not improve from 18.74490
196/196 - 79s - loss: 18.7922 - MinusLogProbMetric: 18.7922 - val_loss: 18.8553 - val_MinusLogProbMetric: 18.8553 - lr: 3.3333e-04 - 79s/epoch - 405ms/step
Epoch 293/1000
2023-09-27 23:41:34.828 
Epoch 293/1000 
	 loss: 18.7707, MinusLogProbMetric: 18.7707, val_loss: 18.8383, val_MinusLogProbMetric: 18.8383

Epoch 293: val_loss did not improve from 18.74490
196/196 - 80s - loss: 18.7707 - MinusLogProbMetric: 18.7707 - val_loss: 18.8383 - val_MinusLogProbMetric: 18.8383 - lr: 3.3333e-04 - 80s/epoch - 406ms/step
Epoch 294/1000
2023-09-27 23:42:54.858 
Epoch 294/1000 
	 loss: 18.9674, MinusLogProbMetric: 18.9674, val_loss: 18.8386, val_MinusLogProbMetric: 18.8386

Epoch 294: val_loss did not improve from 18.74490
196/196 - 80s - loss: 18.9674 - MinusLogProbMetric: 18.9674 - val_loss: 18.8386 - val_MinusLogProbMetric: 18.8386 - lr: 3.3333e-04 - 80s/epoch - 408ms/step
Epoch 295/1000
2023-09-27 23:44:14.304 
Epoch 295/1000 
	 loss: 18.8545, MinusLogProbMetric: 18.8545, val_loss: 19.3970, val_MinusLogProbMetric: 19.3970

Epoch 295: val_loss did not improve from 18.74490
196/196 - 79s - loss: 18.8545 - MinusLogProbMetric: 18.8545 - val_loss: 19.3970 - val_MinusLogProbMetric: 19.3970 - lr: 3.3333e-04 - 79s/epoch - 405ms/step
Epoch 296/1000
2023-09-27 23:45:33.660 
Epoch 296/1000 
	 loss: 18.8455, MinusLogProbMetric: 18.8455, val_loss: 19.4157, val_MinusLogProbMetric: 19.4157

Epoch 296: val_loss did not improve from 18.74490
196/196 - 79s - loss: 18.8455 - MinusLogProbMetric: 18.8455 - val_loss: 19.4157 - val_MinusLogProbMetric: 19.4157 - lr: 3.3333e-04 - 79s/epoch - 405ms/step
Epoch 297/1000
2023-09-27 23:46:53.246 
Epoch 297/1000 
	 loss: 18.7580, MinusLogProbMetric: 18.7580, val_loss: 18.8724, val_MinusLogProbMetric: 18.8724

Epoch 297: val_loss did not improve from 18.74490
196/196 - 80s - loss: 18.7580 - MinusLogProbMetric: 18.7580 - val_loss: 18.8724 - val_MinusLogProbMetric: 18.8724 - lr: 3.3333e-04 - 80s/epoch - 406ms/step
Epoch 298/1000
2023-09-27 23:48:12.387 
Epoch 298/1000 
	 loss: 18.8631, MinusLogProbMetric: 18.8631, val_loss: 18.7812, val_MinusLogProbMetric: 18.7812

Epoch 298: val_loss did not improve from 18.74490
196/196 - 79s - loss: 18.8631 - MinusLogProbMetric: 18.8631 - val_loss: 18.7812 - val_MinusLogProbMetric: 18.7812 - lr: 3.3333e-04 - 79s/epoch - 404ms/step
Epoch 299/1000
2023-09-27 23:49:32.271 
Epoch 299/1000 
	 loss: 18.7564, MinusLogProbMetric: 18.7564, val_loss: 19.1257, val_MinusLogProbMetric: 19.1257

Epoch 299: val_loss did not improve from 18.74490
196/196 - 80s - loss: 18.7564 - MinusLogProbMetric: 18.7564 - val_loss: 19.1257 - val_MinusLogProbMetric: 19.1257 - lr: 3.3333e-04 - 80s/epoch - 408ms/step
Epoch 300/1000
2023-09-27 23:50:51.006 
Epoch 300/1000 
	 loss: 18.7299, MinusLogProbMetric: 18.7299, val_loss: 18.8813, val_MinusLogProbMetric: 18.8813

Epoch 300: val_loss did not improve from 18.74490
196/196 - 79s - loss: 18.7299 - MinusLogProbMetric: 18.7299 - val_loss: 18.8813 - val_MinusLogProbMetric: 18.8813 - lr: 3.3333e-04 - 79s/epoch - 402ms/step
Epoch 301/1000
2023-09-27 23:52:10.108 
Epoch 301/1000 
	 loss: 18.8671, MinusLogProbMetric: 18.8671, val_loss: 19.2006, val_MinusLogProbMetric: 19.2006

Epoch 301: val_loss did not improve from 18.74490
196/196 - 79s - loss: 18.8671 - MinusLogProbMetric: 18.8671 - val_loss: 19.2006 - val_MinusLogProbMetric: 19.2006 - lr: 3.3333e-04 - 79s/epoch - 404ms/step
Epoch 302/1000
2023-09-27 23:53:29.209 
Epoch 302/1000 
	 loss: 18.6962, MinusLogProbMetric: 18.6962, val_loss: 18.7862, val_MinusLogProbMetric: 18.7862

Epoch 302: val_loss did not improve from 18.74490
196/196 - 79s - loss: 18.6962 - MinusLogProbMetric: 18.6962 - val_loss: 18.7862 - val_MinusLogProbMetric: 18.7862 - lr: 3.3333e-04 - 79s/epoch - 404ms/step
Epoch 303/1000
2023-09-27 23:54:48.500 
Epoch 303/1000 
	 loss: 18.7139, MinusLogProbMetric: 18.7139, val_loss: 19.0163, val_MinusLogProbMetric: 19.0163

Epoch 303: val_loss did not improve from 18.74490
196/196 - 79s - loss: 18.7139 - MinusLogProbMetric: 18.7139 - val_loss: 19.0163 - val_MinusLogProbMetric: 19.0163 - lr: 3.3333e-04 - 79s/epoch - 405ms/step
Epoch 304/1000
2023-09-27 23:56:07.703 
Epoch 304/1000 
	 loss: 18.7608, MinusLogProbMetric: 18.7608, val_loss: 18.8797, val_MinusLogProbMetric: 18.8797

Epoch 304: val_loss did not improve from 18.74490
196/196 - 79s - loss: 18.7608 - MinusLogProbMetric: 18.7608 - val_loss: 18.8797 - val_MinusLogProbMetric: 18.8797 - lr: 3.3333e-04 - 79s/epoch - 404ms/step
Epoch 305/1000
2023-09-27 23:57:27.584 
Epoch 305/1000 
	 loss: 18.7917, MinusLogProbMetric: 18.7917, val_loss: 19.1832, val_MinusLogProbMetric: 19.1832

Epoch 305: val_loss did not improve from 18.74490
196/196 - 80s - loss: 18.7917 - MinusLogProbMetric: 18.7917 - val_loss: 19.1832 - val_MinusLogProbMetric: 19.1832 - lr: 3.3333e-04 - 80s/epoch - 408ms/step
Epoch 306/1000
2023-09-27 23:58:47.246 
Epoch 306/1000 
	 loss: 18.7315, MinusLogProbMetric: 18.7315, val_loss: 19.1786, val_MinusLogProbMetric: 19.1786

Epoch 306: val_loss did not improve from 18.74490
196/196 - 80s - loss: 18.7315 - MinusLogProbMetric: 18.7315 - val_loss: 19.1786 - val_MinusLogProbMetric: 19.1786 - lr: 3.3333e-04 - 80s/epoch - 406ms/step
Epoch 307/1000
2023-09-28 00:00:06.839 
Epoch 307/1000 
	 loss: 18.7261, MinusLogProbMetric: 18.7261, val_loss: 18.7312, val_MinusLogProbMetric: 18.7312

Epoch 307: val_loss improved from 18.74490 to 18.73124, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 81s - loss: 18.7261 - MinusLogProbMetric: 18.7261 - val_loss: 18.7312 - val_MinusLogProbMetric: 18.7312 - lr: 3.3333e-04 - 81s/epoch - 414ms/step
Epoch 308/1000
2023-09-28 00:01:27.931 
Epoch 308/1000 
	 loss: 18.6703, MinusLogProbMetric: 18.6703, val_loss: 18.6321, val_MinusLogProbMetric: 18.6321

Epoch 308: val_loss improved from 18.73124 to 18.63209, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 81s - loss: 18.6703 - MinusLogProbMetric: 18.6703 - val_loss: 18.6321 - val_MinusLogProbMetric: 18.6321 - lr: 3.3333e-04 - 81s/epoch - 413ms/step
Epoch 309/1000
2023-09-28 00:02:47.934 
Epoch 309/1000 
	 loss: 18.7604, MinusLogProbMetric: 18.7604, val_loss: 18.6538, val_MinusLogProbMetric: 18.6538

Epoch 309: val_loss did not improve from 18.63209
196/196 - 79s - loss: 18.7604 - MinusLogProbMetric: 18.7604 - val_loss: 18.6538 - val_MinusLogProbMetric: 18.6538 - lr: 3.3333e-04 - 79s/epoch - 401ms/step
Epoch 310/1000
2023-09-28 00:04:07.726 
Epoch 310/1000 
	 loss: 18.7869, MinusLogProbMetric: 18.7869, val_loss: 18.6265, val_MinusLogProbMetric: 18.6265

Epoch 310: val_loss improved from 18.63209 to 18.62652, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 81s - loss: 18.7869 - MinusLogProbMetric: 18.7869 - val_loss: 18.6265 - val_MinusLogProbMetric: 18.6265 - lr: 3.3333e-04 - 81s/epoch - 413ms/step
Epoch 311/1000
2023-09-28 00:05:28.334 
Epoch 311/1000 
	 loss: 18.6790, MinusLogProbMetric: 18.6790, val_loss: 18.9179, val_MinusLogProbMetric: 18.9179

Epoch 311: val_loss did not improve from 18.62652
196/196 - 79s - loss: 18.6790 - MinusLogProbMetric: 18.6790 - val_loss: 18.9179 - val_MinusLogProbMetric: 18.9179 - lr: 3.3333e-04 - 79s/epoch - 405ms/step
Epoch 312/1000
2023-09-28 00:06:47.556 
Epoch 312/1000 
	 loss: 18.7197, MinusLogProbMetric: 18.7197, val_loss: 18.6882, val_MinusLogProbMetric: 18.6882

Epoch 312: val_loss did not improve from 18.62652
196/196 - 79s - loss: 18.7197 - MinusLogProbMetric: 18.7197 - val_loss: 18.6882 - val_MinusLogProbMetric: 18.6882 - lr: 3.3333e-04 - 79s/epoch - 404ms/step
Epoch 313/1000
2023-09-28 00:08:07.051 
Epoch 313/1000 
	 loss: 18.6508, MinusLogProbMetric: 18.6508, val_loss: 19.1706, val_MinusLogProbMetric: 19.1706

Epoch 313: val_loss did not improve from 18.62652
196/196 - 79s - loss: 18.6508 - MinusLogProbMetric: 18.6508 - val_loss: 19.1706 - val_MinusLogProbMetric: 19.1706 - lr: 3.3333e-04 - 79s/epoch - 406ms/step
Epoch 314/1000
2023-09-28 00:09:25.984 
Epoch 314/1000 
	 loss: 18.7127, MinusLogProbMetric: 18.7127, val_loss: 18.9183, val_MinusLogProbMetric: 18.9183

Epoch 314: val_loss did not improve from 18.62652
196/196 - 79s - loss: 18.7127 - MinusLogProbMetric: 18.7127 - val_loss: 18.9183 - val_MinusLogProbMetric: 18.9183 - lr: 3.3333e-04 - 79s/epoch - 403ms/step
Epoch 315/1000
2023-09-28 00:10:44.730 
Epoch 315/1000 
	 loss: 18.7272, MinusLogProbMetric: 18.7272, val_loss: 18.6009, val_MinusLogProbMetric: 18.6009

Epoch 315: val_loss improved from 18.62652 to 18.60085, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 80s - loss: 18.7272 - MinusLogProbMetric: 18.7272 - val_loss: 18.6009 - val_MinusLogProbMetric: 18.6009 - lr: 3.3333e-04 - 80s/epoch - 408ms/step
Epoch 316/1000
2023-09-28 00:12:05.047 
Epoch 316/1000 
	 loss: 18.7076, MinusLogProbMetric: 18.7076, val_loss: 19.1072, val_MinusLogProbMetric: 19.1072

Epoch 316: val_loss did not improve from 18.60085
196/196 - 79s - loss: 18.7076 - MinusLogProbMetric: 18.7076 - val_loss: 19.1072 - val_MinusLogProbMetric: 19.1072 - lr: 3.3333e-04 - 79s/epoch - 403ms/step
Epoch 317/1000
2023-09-28 00:13:24.905 
Epoch 317/1000 
	 loss: 18.6905, MinusLogProbMetric: 18.6905, val_loss: 18.6728, val_MinusLogProbMetric: 18.6728

Epoch 317: val_loss did not improve from 18.60085
196/196 - 80s - loss: 18.6905 - MinusLogProbMetric: 18.6905 - val_loss: 18.6728 - val_MinusLogProbMetric: 18.6728 - lr: 3.3333e-04 - 80s/epoch - 407ms/step
Epoch 318/1000
2023-09-28 00:14:44.429 
Epoch 318/1000 
	 loss: 18.6261, MinusLogProbMetric: 18.6261, val_loss: 19.0988, val_MinusLogProbMetric: 19.0988

Epoch 318: val_loss did not improve from 18.60085
196/196 - 80s - loss: 18.6261 - MinusLogProbMetric: 18.6261 - val_loss: 19.0988 - val_MinusLogProbMetric: 19.0988 - lr: 3.3333e-04 - 80s/epoch - 406ms/step
Epoch 319/1000
2023-09-28 00:16:03.915 
Epoch 319/1000 
	 loss: 18.7201, MinusLogProbMetric: 18.7201, val_loss: 18.8651, val_MinusLogProbMetric: 18.8651

Epoch 319: val_loss did not improve from 18.60085
196/196 - 79s - loss: 18.7201 - MinusLogProbMetric: 18.7201 - val_loss: 18.8651 - val_MinusLogProbMetric: 18.8651 - lr: 3.3333e-04 - 79s/epoch - 406ms/step
Epoch 320/1000
2023-09-28 00:17:22.550 
Epoch 320/1000 
	 loss: 18.7244, MinusLogProbMetric: 18.7244, val_loss: 19.1218, val_MinusLogProbMetric: 19.1218

Epoch 320: val_loss did not improve from 18.60085
196/196 - 79s - loss: 18.7244 - MinusLogProbMetric: 18.7244 - val_loss: 19.1218 - val_MinusLogProbMetric: 19.1218 - lr: 3.3333e-04 - 79s/epoch - 401ms/step
Epoch 321/1000
2023-09-28 00:18:42.271 
Epoch 321/1000 
	 loss: 18.7060, MinusLogProbMetric: 18.7060, val_loss: 18.9462, val_MinusLogProbMetric: 18.9462

Epoch 321: val_loss did not improve from 18.60085
196/196 - 80s - loss: 18.7060 - MinusLogProbMetric: 18.7060 - val_loss: 18.9462 - val_MinusLogProbMetric: 18.9462 - lr: 3.3333e-04 - 80s/epoch - 407ms/step
Epoch 322/1000
2023-09-28 00:20:01.107 
Epoch 322/1000 
	 loss: 18.6542, MinusLogProbMetric: 18.6542, val_loss: 18.8116, val_MinusLogProbMetric: 18.8116

Epoch 322: val_loss did not improve from 18.60085
196/196 - 79s - loss: 18.6542 - MinusLogProbMetric: 18.6542 - val_loss: 18.8116 - val_MinusLogProbMetric: 18.8116 - lr: 3.3333e-04 - 79s/epoch - 402ms/step
Epoch 323/1000
2023-09-28 00:21:20.300 
Epoch 323/1000 
	 loss: 18.6637, MinusLogProbMetric: 18.6637, val_loss: 18.9777, val_MinusLogProbMetric: 18.9777

Epoch 323: val_loss did not improve from 18.60085
196/196 - 79s - loss: 18.6637 - MinusLogProbMetric: 18.6637 - val_loss: 18.9777 - val_MinusLogProbMetric: 18.9777 - lr: 3.3333e-04 - 79s/epoch - 404ms/step
Epoch 324/1000
2023-09-28 00:22:39.838 
Epoch 324/1000 
	 loss: 18.6619, MinusLogProbMetric: 18.6619, val_loss: 19.2471, val_MinusLogProbMetric: 19.2471

Epoch 324: val_loss did not improve from 18.60085
196/196 - 80s - loss: 18.6619 - MinusLogProbMetric: 18.6619 - val_loss: 19.2471 - val_MinusLogProbMetric: 19.2471 - lr: 3.3333e-04 - 80s/epoch - 406ms/step
Epoch 325/1000
2023-09-28 00:23:59.201 
Epoch 325/1000 
	 loss: 18.6190, MinusLogProbMetric: 18.6190, val_loss: 19.0262, val_MinusLogProbMetric: 19.0262

Epoch 325: val_loss did not improve from 18.60085
196/196 - 79s - loss: 18.6190 - MinusLogProbMetric: 18.6190 - val_loss: 19.0262 - val_MinusLogProbMetric: 19.0262 - lr: 3.3333e-04 - 79s/epoch - 405ms/step
Epoch 326/1000
2023-09-28 00:25:18.221 
Epoch 326/1000 
	 loss: 18.7413, MinusLogProbMetric: 18.7413, val_loss: 18.7429, val_MinusLogProbMetric: 18.7429

Epoch 326: val_loss did not improve from 18.60085
196/196 - 79s - loss: 18.7413 - MinusLogProbMetric: 18.7413 - val_loss: 18.7429 - val_MinusLogProbMetric: 18.7429 - lr: 3.3333e-04 - 79s/epoch - 403ms/step
Epoch 327/1000
2023-09-28 00:26:37.673 
Epoch 327/1000 
	 loss: 18.5533, MinusLogProbMetric: 18.5533, val_loss: 18.9222, val_MinusLogProbMetric: 18.9222

Epoch 327: val_loss did not improve from 18.60085
196/196 - 79s - loss: 18.5533 - MinusLogProbMetric: 18.5533 - val_loss: 18.9222 - val_MinusLogProbMetric: 18.9222 - lr: 3.3333e-04 - 79s/epoch - 405ms/step
Epoch 328/1000
2023-09-28 00:27:57.122 
Epoch 328/1000 
	 loss: 18.6034, MinusLogProbMetric: 18.6034, val_loss: 18.9366, val_MinusLogProbMetric: 18.9366

Epoch 328: val_loss did not improve from 18.60085
196/196 - 79s - loss: 18.6034 - MinusLogProbMetric: 18.6034 - val_loss: 18.9366 - val_MinusLogProbMetric: 18.9366 - lr: 3.3333e-04 - 79s/epoch - 405ms/step
Epoch 329/1000
2023-09-28 00:29:16.612 
Epoch 329/1000 
	 loss: 18.6169, MinusLogProbMetric: 18.6169, val_loss: 20.6428, val_MinusLogProbMetric: 20.6428

Epoch 329: val_loss did not improve from 18.60085
196/196 - 79s - loss: 18.6169 - MinusLogProbMetric: 18.6169 - val_loss: 20.6428 - val_MinusLogProbMetric: 20.6428 - lr: 3.3333e-04 - 79s/epoch - 406ms/step
Epoch 330/1000
2023-09-28 00:30:36.151 
Epoch 330/1000 
	 loss: 19.6326, MinusLogProbMetric: 19.6326, val_loss: 18.9347, val_MinusLogProbMetric: 18.9347

Epoch 330: val_loss did not improve from 18.60085
196/196 - 80s - loss: 19.6326 - MinusLogProbMetric: 19.6326 - val_loss: 18.9347 - val_MinusLogProbMetric: 18.9347 - lr: 3.3333e-04 - 80s/epoch - 406ms/step
Epoch 331/1000
2023-09-28 00:31:55.254 
Epoch 331/1000 
	 loss: 18.7006, MinusLogProbMetric: 18.7006, val_loss: 18.5739, val_MinusLogProbMetric: 18.5739

Epoch 331: val_loss improved from 18.60085 to 18.57389, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 80s - loss: 18.7006 - MinusLogProbMetric: 18.7006 - val_loss: 18.5739 - val_MinusLogProbMetric: 18.5739 - lr: 3.3333e-04 - 80s/epoch - 410ms/step
Epoch 332/1000
2023-09-28 00:33:15.835 
Epoch 332/1000 
	 loss: 18.6550, MinusLogProbMetric: 18.6550, val_loss: 18.5835, val_MinusLogProbMetric: 18.5835

Epoch 332: val_loss did not improve from 18.57389
196/196 - 79s - loss: 18.6550 - MinusLogProbMetric: 18.6550 - val_loss: 18.5835 - val_MinusLogProbMetric: 18.5835 - lr: 3.3333e-04 - 79s/epoch - 405ms/step
Epoch 333/1000
2023-09-28 00:34:34.761 
Epoch 333/1000 
	 loss: 18.6243, MinusLogProbMetric: 18.6243, val_loss: 18.7216, val_MinusLogProbMetric: 18.7216

Epoch 333: val_loss did not improve from 18.57389
196/196 - 79s - loss: 18.6243 - MinusLogProbMetric: 18.6243 - val_loss: 18.7216 - val_MinusLogProbMetric: 18.7216 - lr: 3.3333e-04 - 79s/epoch - 403ms/step
Epoch 334/1000
2023-09-28 00:35:54.405 
Epoch 334/1000 
	 loss: 18.5861, MinusLogProbMetric: 18.5861, val_loss: 18.8847, val_MinusLogProbMetric: 18.8847

Epoch 334: val_loss did not improve from 18.57389
196/196 - 80s - loss: 18.5861 - MinusLogProbMetric: 18.5861 - val_loss: 18.8847 - val_MinusLogProbMetric: 18.8847 - lr: 3.3333e-04 - 80s/epoch - 406ms/step
Epoch 335/1000
2023-09-28 00:37:11.682 
Epoch 335/1000 
	 loss: 18.5850, MinusLogProbMetric: 18.5850, val_loss: 18.7766, val_MinusLogProbMetric: 18.7766

Epoch 335: val_loss did not improve from 18.57389
196/196 - 77s - loss: 18.5850 - MinusLogProbMetric: 18.5850 - val_loss: 18.7766 - val_MinusLogProbMetric: 18.7766 - lr: 3.3333e-04 - 77s/epoch - 394ms/step
Epoch 336/1000
2023-09-28 00:38:21.023 
Epoch 336/1000 
	 loss: 18.5826, MinusLogProbMetric: 18.5826, val_loss: 18.7737, val_MinusLogProbMetric: 18.7737

Epoch 336: val_loss did not improve from 18.57389
196/196 - 69s - loss: 18.5826 - MinusLogProbMetric: 18.5826 - val_loss: 18.7737 - val_MinusLogProbMetric: 18.7737 - lr: 3.3333e-04 - 69s/epoch - 354ms/step
Epoch 337/1000
2023-09-28 00:39:37.190 
Epoch 337/1000 
	 loss: 18.6332, MinusLogProbMetric: 18.6332, val_loss: 18.7959, val_MinusLogProbMetric: 18.7959

Epoch 337: val_loss did not improve from 18.57389
196/196 - 76s - loss: 18.6332 - MinusLogProbMetric: 18.6332 - val_loss: 18.7959 - val_MinusLogProbMetric: 18.7959 - lr: 3.3333e-04 - 76s/epoch - 389ms/step
Epoch 338/1000
2023-09-28 00:40:45.414 
Epoch 338/1000 
	 loss: 18.6233, MinusLogProbMetric: 18.6233, val_loss: 18.7259, val_MinusLogProbMetric: 18.7259

Epoch 338: val_loss did not improve from 18.57389
196/196 - 68s - loss: 18.6233 - MinusLogProbMetric: 18.6233 - val_loss: 18.7259 - val_MinusLogProbMetric: 18.7259 - lr: 3.3333e-04 - 68s/epoch - 348ms/step
Epoch 339/1000
2023-09-28 00:42:03.884 
Epoch 339/1000 
	 loss: 18.5394, MinusLogProbMetric: 18.5394, val_loss: 18.8531, val_MinusLogProbMetric: 18.8531

Epoch 339: val_loss did not improve from 18.57389
196/196 - 78s - loss: 18.5394 - MinusLogProbMetric: 18.5394 - val_loss: 18.8531 - val_MinusLogProbMetric: 18.8531 - lr: 3.3333e-04 - 78s/epoch - 400ms/step
Epoch 340/1000
2023-09-28 00:43:22.701 
Epoch 340/1000 
	 loss: 18.6151, MinusLogProbMetric: 18.6151, val_loss: 18.8787, val_MinusLogProbMetric: 18.8787

Epoch 340: val_loss did not improve from 18.57389
196/196 - 79s - loss: 18.6151 - MinusLogProbMetric: 18.6151 - val_loss: 18.8787 - val_MinusLogProbMetric: 18.8787 - lr: 3.3333e-04 - 79s/epoch - 402ms/step
Epoch 341/1000
2023-09-28 00:44:41.704 
Epoch 341/1000 
	 loss: 18.6103, MinusLogProbMetric: 18.6103, val_loss: 18.7564, val_MinusLogProbMetric: 18.7564

Epoch 341: val_loss did not improve from 18.57389
196/196 - 79s - loss: 18.6103 - MinusLogProbMetric: 18.6103 - val_loss: 18.7564 - val_MinusLogProbMetric: 18.7564 - lr: 3.3333e-04 - 79s/epoch - 403ms/step
Epoch 342/1000
2023-09-28 00:46:01.293 
Epoch 342/1000 
	 loss: 18.4828, MinusLogProbMetric: 18.4828, val_loss: 19.0263, val_MinusLogProbMetric: 19.0263

Epoch 342: val_loss did not improve from 18.57389
196/196 - 80s - loss: 18.4828 - MinusLogProbMetric: 18.4828 - val_loss: 19.0263 - val_MinusLogProbMetric: 19.0263 - lr: 3.3333e-04 - 80s/epoch - 406ms/step
Epoch 343/1000
2023-09-28 00:47:21.042 
Epoch 343/1000 
	 loss: 18.5996, MinusLogProbMetric: 18.5996, val_loss: 18.6798, val_MinusLogProbMetric: 18.6798

Epoch 343: val_loss did not improve from 18.57389
196/196 - 80s - loss: 18.5996 - MinusLogProbMetric: 18.5996 - val_loss: 18.6798 - val_MinusLogProbMetric: 18.6798 - lr: 3.3333e-04 - 80s/epoch - 407ms/step
Epoch 344/1000
2023-09-28 00:48:40.454 
Epoch 344/1000 
	 loss: 18.5506, MinusLogProbMetric: 18.5506, val_loss: 18.4175, val_MinusLogProbMetric: 18.4175

Epoch 344: val_loss improved from 18.57389 to 18.41747, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 81s - loss: 18.5506 - MinusLogProbMetric: 18.5506 - val_loss: 18.4175 - val_MinusLogProbMetric: 18.4175 - lr: 3.3333e-04 - 81s/epoch - 412ms/step
Epoch 345/1000
2023-09-28 00:50:01.788 
Epoch 345/1000 
	 loss: 18.4917, MinusLogProbMetric: 18.4917, val_loss: 19.0098, val_MinusLogProbMetric: 19.0098

Epoch 345: val_loss did not improve from 18.41747
196/196 - 80s - loss: 18.4917 - MinusLogProbMetric: 18.4917 - val_loss: 19.0098 - val_MinusLogProbMetric: 19.0098 - lr: 3.3333e-04 - 80s/epoch - 409ms/step
Epoch 346/1000
2023-09-28 00:51:21.359 
Epoch 346/1000 
	 loss: 18.6153, MinusLogProbMetric: 18.6153, val_loss: 18.5720, val_MinusLogProbMetric: 18.5720

Epoch 346: val_loss did not improve from 18.41747
196/196 - 80s - loss: 18.6153 - MinusLogProbMetric: 18.6153 - val_loss: 18.5720 - val_MinusLogProbMetric: 18.5720 - lr: 3.3333e-04 - 80s/epoch - 406ms/step
Epoch 347/1000
2023-09-28 00:52:40.703 
Epoch 347/1000 
	 loss: 18.5461, MinusLogProbMetric: 18.5461, val_loss: 18.5109, val_MinusLogProbMetric: 18.5109

Epoch 347: val_loss did not improve from 18.41747
196/196 - 79s - loss: 18.5461 - MinusLogProbMetric: 18.5461 - val_loss: 18.5109 - val_MinusLogProbMetric: 18.5109 - lr: 3.3333e-04 - 79s/epoch - 405ms/step
Epoch 348/1000
2023-09-28 00:54:00.258 
Epoch 348/1000 
	 loss: 18.5342, MinusLogProbMetric: 18.5342, val_loss: 19.5229, val_MinusLogProbMetric: 19.5229

Epoch 348: val_loss did not improve from 18.41747
196/196 - 80s - loss: 18.5342 - MinusLogProbMetric: 18.5342 - val_loss: 19.5229 - val_MinusLogProbMetric: 19.5229 - lr: 3.3333e-04 - 80s/epoch - 406ms/step
Epoch 349/1000
2023-09-28 00:55:19.985 
Epoch 349/1000 
	 loss: 18.5562, MinusLogProbMetric: 18.5562, val_loss: 18.6014, val_MinusLogProbMetric: 18.6014

Epoch 349: val_loss did not improve from 18.41747
196/196 - 80s - loss: 18.5562 - MinusLogProbMetric: 18.5562 - val_loss: 18.6014 - val_MinusLogProbMetric: 18.6014 - lr: 3.3333e-04 - 80s/epoch - 407ms/step
Epoch 350/1000
2023-09-28 00:56:40.005 
Epoch 350/1000 
	 loss: 18.5487, MinusLogProbMetric: 18.5487, val_loss: 18.5006, val_MinusLogProbMetric: 18.5006

Epoch 350: val_loss did not improve from 18.41747
196/196 - 80s - loss: 18.5487 - MinusLogProbMetric: 18.5487 - val_loss: 18.5006 - val_MinusLogProbMetric: 18.5006 - lr: 3.3333e-04 - 80s/epoch - 408ms/step
Epoch 351/1000
2023-09-28 00:57:59.242 
Epoch 351/1000 
	 loss: 18.5197, MinusLogProbMetric: 18.5197, val_loss: 18.6014, val_MinusLogProbMetric: 18.6014

Epoch 351: val_loss did not improve from 18.41747
196/196 - 79s - loss: 18.5197 - MinusLogProbMetric: 18.5197 - val_loss: 18.6014 - val_MinusLogProbMetric: 18.6014 - lr: 3.3333e-04 - 79s/epoch - 404ms/step
Epoch 352/1000
2023-09-28 00:59:18.441 
Epoch 352/1000 
	 loss: 18.4982, MinusLogProbMetric: 18.4982, val_loss: 18.7710, val_MinusLogProbMetric: 18.7710

Epoch 352: val_loss did not improve from 18.41747
196/196 - 79s - loss: 18.4982 - MinusLogProbMetric: 18.4982 - val_loss: 18.7710 - val_MinusLogProbMetric: 18.7710 - lr: 3.3333e-04 - 79s/epoch - 404ms/step
Epoch 353/1000
2023-09-28 01:00:38.099 
Epoch 353/1000 
	 loss: 18.5059, MinusLogProbMetric: 18.5059, val_loss: 18.5056, val_MinusLogProbMetric: 18.5056

Epoch 353: val_loss did not improve from 18.41747
196/196 - 80s - loss: 18.5059 - MinusLogProbMetric: 18.5059 - val_loss: 18.5056 - val_MinusLogProbMetric: 18.5056 - lr: 3.3333e-04 - 80s/epoch - 406ms/step
Epoch 354/1000
2023-09-28 01:01:57.581 
Epoch 354/1000 
	 loss: 18.5341, MinusLogProbMetric: 18.5341, val_loss: 18.5731, val_MinusLogProbMetric: 18.5731

Epoch 354: val_loss did not improve from 18.41747
196/196 - 79s - loss: 18.5341 - MinusLogProbMetric: 18.5341 - val_loss: 18.5731 - val_MinusLogProbMetric: 18.5731 - lr: 3.3333e-04 - 79s/epoch - 406ms/step
Epoch 355/1000
2023-09-28 01:03:16.565 
Epoch 355/1000 
	 loss: 18.3975, MinusLogProbMetric: 18.3975, val_loss: 18.5525, val_MinusLogProbMetric: 18.5525

Epoch 355: val_loss did not improve from 18.41747
196/196 - 79s - loss: 18.3975 - MinusLogProbMetric: 18.3975 - val_loss: 18.5525 - val_MinusLogProbMetric: 18.5525 - lr: 3.3333e-04 - 79s/epoch - 403ms/step
Epoch 356/1000
2023-09-28 01:04:36.026 
Epoch 356/1000 
	 loss: 18.4655, MinusLogProbMetric: 18.4655, val_loss: 18.6370, val_MinusLogProbMetric: 18.6370

Epoch 356: val_loss did not improve from 18.41747
196/196 - 79s - loss: 18.4655 - MinusLogProbMetric: 18.4655 - val_loss: 18.6370 - val_MinusLogProbMetric: 18.6370 - lr: 3.3333e-04 - 79s/epoch - 405ms/step
Epoch 357/1000
2023-09-28 01:05:55.801 
Epoch 357/1000 
	 loss: 26.8317, MinusLogProbMetric: 26.8317, val_loss: 23.2664, val_MinusLogProbMetric: 23.2664

Epoch 357: val_loss did not improve from 18.41747
196/196 - 80s - loss: 26.8317 - MinusLogProbMetric: 26.8317 - val_loss: 23.2664 - val_MinusLogProbMetric: 23.2664 - lr: 3.3333e-04 - 80s/epoch - 407ms/step
Epoch 358/1000
2023-09-28 01:07:15.027 
Epoch 358/1000 
	 loss: 21.2950, MinusLogProbMetric: 21.2950, val_loss: 20.6885, val_MinusLogProbMetric: 20.6885

Epoch 358: val_loss did not improve from 18.41747
196/196 - 79s - loss: 21.2950 - MinusLogProbMetric: 21.2950 - val_loss: 20.6885 - val_MinusLogProbMetric: 20.6885 - lr: 3.3333e-04 - 79s/epoch - 404ms/step
Epoch 359/1000
2023-09-28 01:08:34.576 
Epoch 359/1000 
	 loss: 20.3131, MinusLogProbMetric: 20.3131, val_loss: 20.0356, val_MinusLogProbMetric: 20.0356

Epoch 359: val_loss did not improve from 18.41747
196/196 - 80s - loss: 20.3131 - MinusLogProbMetric: 20.3131 - val_loss: 20.0356 - val_MinusLogProbMetric: 20.0356 - lr: 3.3333e-04 - 80s/epoch - 406ms/step
Epoch 360/1000
2023-09-28 01:09:53.670 
Epoch 360/1000 
	 loss: 19.7259, MinusLogProbMetric: 19.7259, val_loss: 19.6189, val_MinusLogProbMetric: 19.6189

Epoch 360: val_loss did not improve from 18.41747
196/196 - 79s - loss: 19.7259 - MinusLogProbMetric: 19.7259 - val_loss: 19.6189 - val_MinusLogProbMetric: 19.6189 - lr: 3.3333e-04 - 79s/epoch - 404ms/step
Epoch 361/1000
2023-09-28 01:11:13.653 
Epoch 361/1000 
	 loss: 19.1351, MinusLogProbMetric: 19.1351, val_loss: 19.1015, val_MinusLogProbMetric: 19.1015

Epoch 361: val_loss did not improve from 18.41747
196/196 - 80s - loss: 19.1351 - MinusLogProbMetric: 19.1351 - val_loss: 19.1015 - val_MinusLogProbMetric: 19.1015 - lr: 3.3333e-04 - 80s/epoch - 408ms/step
Epoch 362/1000
2023-09-28 01:12:33.126 
Epoch 362/1000 
	 loss: 19.0503, MinusLogProbMetric: 19.0503, val_loss: 19.4496, val_MinusLogProbMetric: 19.4496

Epoch 362: val_loss did not improve from 18.41747
196/196 - 79s - loss: 19.0503 - MinusLogProbMetric: 19.0503 - val_loss: 19.4496 - val_MinusLogProbMetric: 19.4496 - lr: 3.3333e-04 - 79s/epoch - 405ms/step
Epoch 363/1000
2023-09-28 01:13:52.665 
Epoch 363/1000 
	 loss: 19.1137, MinusLogProbMetric: 19.1137, val_loss: 18.9122, val_MinusLogProbMetric: 18.9122

Epoch 363: val_loss did not improve from 18.41747
196/196 - 80s - loss: 19.1137 - MinusLogProbMetric: 19.1137 - val_loss: 18.9122 - val_MinusLogProbMetric: 18.9122 - lr: 3.3333e-04 - 80s/epoch - 406ms/step
Epoch 364/1000
2023-09-28 01:15:12.210 
Epoch 364/1000 
	 loss: 18.7933, MinusLogProbMetric: 18.7933, val_loss: 18.6835, val_MinusLogProbMetric: 18.6835

Epoch 364: val_loss did not improve from 18.41747
196/196 - 80s - loss: 18.7933 - MinusLogProbMetric: 18.7933 - val_loss: 18.6835 - val_MinusLogProbMetric: 18.6835 - lr: 3.3333e-04 - 80s/epoch - 406ms/step
Epoch 365/1000
2023-09-28 01:16:31.297 
Epoch 365/1000 
	 loss: 18.9769, MinusLogProbMetric: 18.9769, val_loss: 18.6438, val_MinusLogProbMetric: 18.6438

Epoch 365: val_loss did not improve from 18.41747
196/196 - 79s - loss: 18.9769 - MinusLogProbMetric: 18.9769 - val_loss: 18.6438 - val_MinusLogProbMetric: 18.6438 - lr: 3.3333e-04 - 79s/epoch - 404ms/step
Epoch 366/1000
2023-09-28 01:17:51.200 
Epoch 366/1000 
	 loss: 19.0441, MinusLogProbMetric: 19.0441, val_loss: 18.8461, val_MinusLogProbMetric: 18.8461

Epoch 366: val_loss did not improve from 18.41747
196/196 - 80s - loss: 19.0441 - MinusLogProbMetric: 19.0441 - val_loss: 18.8461 - val_MinusLogProbMetric: 18.8461 - lr: 3.3333e-04 - 80s/epoch - 408ms/step
Epoch 367/1000
2023-09-28 01:19:10.825 
Epoch 367/1000 
	 loss: 18.6758, MinusLogProbMetric: 18.6758, val_loss: 19.1085, val_MinusLogProbMetric: 19.1085

Epoch 367: val_loss did not improve from 18.41747
196/196 - 80s - loss: 18.6758 - MinusLogProbMetric: 18.6758 - val_loss: 19.1085 - val_MinusLogProbMetric: 19.1085 - lr: 3.3333e-04 - 80s/epoch - 406ms/step
Epoch 368/1000
2023-09-28 01:20:30.639 
Epoch 368/1000 
	 loss: 18.8094, MinusLogProbMetric: 18.8094, val_loss: 18.7461, val_MinusLogProbMetric: 18.7461

Epoch 368: val_loss did not improve from 18.41747
196/196 - 80s - loss: 18.8094 - MinusLogProbMetric: 18.8094 - val_loss: 18.7461 - val_MinusLogProbMetric: 18.7461 - lr: 3.3333e-04 - 80s/epoch - 407ms/step
Epoch 369/1000
2023-09-28 01:21:49.548 
Epoch 369/1000 
	 loss: 18.7340, MinusLogProbMetric: 18.7340, val_loss: 18.5516, val_MinusLogProbMetric: 18.5516

Epoch 369: val_loss did not improve from 18.41747
196/196 - 79s - loss: 18.7340 - MinusLogProbMetric: 18.7340 - val_loss: 18.5516 - val_MinusLogProbMetric: 18.5516 - lr: 3.3333e-04 - 79s/epoch - 403ms/step
Epoch 370/1000
2023-09-28 01:23:08.974 
Epoch 370/1000 
	 loss: 18.6492, MinusLogProbMetric: 18.6492, val_loss: 18.5851, val_MinusLogProbMetric: 18.5851

Epoch 370: val_loss did not improve from 18.41747
196/196 - 79s - loss: 18.6492 - MinusLogProbMetric: 18.6492 - val_loss: 18.5851 - val_MinusLogProbMetric: 18.5851 - lr: 3.3333e-04 - 79s/epoch - 405ms/step
Epoch 371/1000
2023-09-28 01:24:27.668 
Epoch 371/1000 
	 loss: 18.7788, MinusLogProbMetric: 18.7788, val_loss: 18.9459, val_MinusLogProbMetric: 18.9459

Epoch 371: val_loss did not improve from 18.41747
196/196 - 79s - loss: 18.7788 - MinusLogProbMetric: 18.7788 - val_loss: 18.9459 - val_MinusLogProbMetric: 18.9459 - lr: 3.3333e-04 - 79s/epoch - 401ms/step
Epoch 372/1000
2023-09-28 01:25:47.020 
Epoch 372/1000 
	 loss: 18.7151, MinusLogProbMetric: 18.7151, val_loss: 18.6871, val_MinusLogProbMetric: 18.6871

Epoch 372: val_loss did not improve from 18.41747
196/196 - 79s - loss: 18.7151 - MinusLogProbMetric: 18.7151 - val_loss: 18.6871 - val_MinusLogProbMetric: 18.6871 - lr: 3.3333e-04 - 79s/epoch - 405ms/step
Epoch 373/1000
2023-09-28 01:27:06.495 
Epoch 373/1000 
	 loss: 18.7914, MinusLogProbMetric: 18.7914, val_loss: 18.7766, val_MinusLogProbMetric: 18.7766

Epoch 373: val_loss did not improve from 18.41747
196/196 - 79s - loss: 18.7914 - MinusLogProbMetric: 18.7914 - val_loss: 18.7766 - val_MinusLogProbMetric: 18.7766 - lr: 3.3333e-04 - 79s/epoch - 405ms/step
Epoch 374/1000
2023-09-28 01:28:25.577 
Epoch 374/1000 
	 loss: 18.5947, MinusLogProbMetric: 18.5947, val_loss: 18.7672, val_MinusLogProbMetric: 18.7672

Epoch 374: val_loss did not improve from 18.41747
196/196 - 79s - loss: 18.5947 - MinusLogProbMetric: 18.5947 - val_loss: 18.7672 - val_MinusLogProbMetric: 18.7672 - lr: 3.3333e-04 - 79s/epoch - 403ms/step
Epoch 375/1000
2023-09-28 01:29:45.059 
Epoch 375/1000 
	 loss: 18.5237, MinusLogProbMetric: 18.5237, val_loss: 18.6793, val_MinusLogProbMetric: 18.6793

Epoch 375: val_loss did not improve from 18.41747
196/196 - 79s - loss: 18.5237 - MinusLogProbMetric: 18.5237 - val_loss: 18.6793 - val_MinusLogProbMetric: 18.6793 - lr: 3.3333e-04 - 79s/epoch - 405ms/step
Epoch 376/1000
2023-09-28 01:31:04.407 
Epoch 376/1000 
	 loss: 18.6035, MinusLogProbMetric: 18.6035, val_loss: 18.8573, val_MinusLogProbMetric: 18.8573

Epoch 376: val_loss did not improve from 18.41747
196/196 - 79s - loss: 18.6035 - MinusLogProbMetric: 18.6035 - val_loss: 18.8573 - val_MinusLogProbMetric: 18.8573 - lr: 3.3333e-04 - 79s/epoch - 405ms/step
Epoch 377/1000
2023-09-28 01:32:23.285 
Epoch 377/1000 
	 loss: 18.4979, MinusLogProbMetric: 18.4979, val_loss: 18.6326, val_MinusLogProbMetric: 18.6326

Epoch 377: val_loss did not improve from 18.41747
196/196 - 79s - loss: 18.4979 - MinusLogProbMetric: 18.4979 - val_loss: 18.6326 - val_MinusLogProbMetric: 18.6326 - lr: 3.3333e-04 - 79s/epoch - 402ms/step
Epoch 378/1000
2023-09-28 01:33:42.768 
Epoch 378/1000 
	 loss: 18.5079, MinusLogProbMetric: 18.5079, val_loss: 19.4290, val_MinusLogProbMetric: 19.4290

Epoch 378: val_loss did not improve from 18.41747
196/196 - 79s - loss: 18.5079 - MinusLogProbMetric: 18.5079 - val_loss: 19.4290 - val_MinusLogProbMetric: 19.4290 - lr: 3.3333e-04 - 79s/epoch - 406ms/step
Epoch 379/1000
2023-09-28 01:35:02.064 
Epoch 379/1000 
	 loss: 18.5715, MinusLogProbMetric: 18.5715, val_loss: 18.4630, val_MinusLogProbMetric: 18.4630

Epoch 379: val_loss did not improve from 18.41747
196/196 - 79s - loss: 18.5715 - MinusLogProbMetric: 18.5715 - val_loss: 18.4630 - val_MinusLogProbMetric: 18.4630 - lr: 3.3333e-04 - 79s/epoch - 405ms/step
Epoch 380/1000
2023-09-28 01:36:21.398 
Epoch 380/1000 
	 loss: 18.5726, MinusLogProbMetric: 18.5726, val_loss: 19.1342, val_MinusLogProbMetric: 19.1342

Epoch 380: val_loss did not improve from 18.41747
196/196 - 79s - loss: 18.5726 - MinusLogProbMetric: 18.5726 - val_loss: 19.1342 - val_MinusLogProbMetric: 19.1342 - lr: 3.3333e-04 - 79s/epoch - 405ms/step
Epoch 381/1000
2023-09-28 01:37:40.634 
Epoch 381/1000 
	 loss: 18.4959, MinusLogProbMetric: 18.4959, val_loss: 18.7022, val_MinusLogProbMetric: 18.7022

Epoch 381: val_loss did not improve from 18.41747
196/196 - 79s - loss: 18.4959 - MinusLogProbMetric: 18.4959 - val_loss: 18.7022 - val_MinusLogProbMetric: 18.7022 - lr: 3.3333e-04 - 79s/epoch - 404ms/step
Epoch 382/1000
2023-09-28 01:38:59.619 
Epoch 382/1000 
	 loss: 18.5307, MinusLogProbMetric: 18.5307, val_loss: 18.6396, val_MinusLogProbMetric: 18.6396

Epoch 382: val_loss did not improve from 18.41747
196/196 - 79s - loss: 18.5307 - MinusLogProbMetric: 18.5307 - val_loss: 18.6396 - val_MinusLogProbMetric: 18.6396 - lr: 3.3333e-04 - 79s/epoch - 403ms/step
Epoch 383/1000
2023-09-28 01:40:18.854 
Epoch 383/1000 
	 loss: 18.5554, MinusLogProbMetric: 18.5554, val_loss: 18.6709, val_MinusLogProbMetric: 18.6709

Epoch 383: val_loss did not improve from 18.41747
196/196 - 79s - loss: 18.5554 - MinusLogProbMetric: 18.5554 - val_loss: 18.6709 - val_MinusLogProbMetric: 18.6709 - lr: 3.3333e-04 - 79s/epoch - 404ms/step
Epoch 384/1000
2023-09-28 01:41:26.298 
Epoch 384/1000 
	 loss: 18.4486, MinusLogProbMetric: 18.4486, val_loss: 18.4173, val_MinusLogProbMetric: 18.4173

Epoch 384: val_loss improved from 18.41747 to 18.41734, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 69s - loss: 18.4486 - MinusLogProbMetric: 18.4486 - val_loss: 18.4173 - val_MinusLogProbMetric: 18.4173 - lr: 3.3333e-04 - 69s/epoch - 350ms/step
Epoch 385/1000
2023-09-28 01:42:42.365 
Epoch 385/1000 
	 loss: 18.4996, MinusLogProbMetric: 18.4996, val_loss: 18.5621, val_MinusLogProbMetric: 18.5621

Epoch 385: val_loss did not improve from 18.41734
196/196 - 75s - loss: 18.4996 - MinusLogProbMetric: 18.4996 - val_loss: 18.5621 - val_MinusLogProbMetric: 18.5621 - lr: 3.3333e-04 - 75s/epoch - 383ms/step
Epoch 386/1000
2023-09-28 01:43:49.637 
Epoch 386/1000 
	 loss: 18.4814, MinusLogProbMetric: 18.4814, val_loss: 19.0669, val_MinusLogProbMetric: 19.0669

Epoch 386: val_loss did not improve from 18.41734
196/196 - 67s - loss: 18.4814 - MinusLogProbMetric: 18.4814 - val_loss: 19.0669 - val_MinusLogProbMetric: 19.0669 - lr: 3.3333e-04 - 67s/epoch - 343ms/step
Epoch 387/1000
2023-09-28 01:45:05.501 
Epoch 387/1000 
	 loss: 18.3964, MinusLogProbMetric: 18.3964, val_loss: 18.7709, val_MinusLogProbMetric: 18.7709

Epoch 387: val_loss did not improve from 18.41734
196/196 - 76s - loss: 18.3964 - MinusLogProbMetric: 18.3964 - val_loss: 18.7709 - val_MinusLogProbMetric: 18.7709 - lr: 3.3333e-04 - 76s/epoch - 387ms/step
Epoch 388/1000
2023-09-28 01:46:24.051 
Epoch 388/1000 
	 loss: 18.5199, MinusLogProbMetric: 18.5199, val_loss: 18.5742, val_MinusLogProbMetric: 18.5742

Epoch 388: val_loss did not improve from 18.41734
196/196 - 79s - loss: 18.5199 - MinusLogProbMetric: 18.5199 - val_loss: 18.5742 - val_MinusLogProbMetric: 18.5742 - lr: 3.3333e-04 - 79s/epoch - 401ms/step
Epoch 389/1000
2023-09-28 01:47:42.806 
Epoch 389/1000 
	 loss: 18.5028, MinusLogProbMetric: 18.5028, val_loss: 18.4848, val_MinusLogProbMetric: 18.4848

Epoch 389: val_loss did not improve from 18.41734
196/196 - 79s - loss: 18.5028 - MinusLogProbMetric: 18.5028 - val_loss: 18.4848 - val_MinusLogProbMetric: 18.4848 - lr: 3.3333e-04 - 79s/epoch - 402ms/step
Epoch 390/1000
2023-09-28 01:49:01.924 
Epoch 390/1000 
	 loss: 18.4217, MinusLogProbMetric: 18.4217, val_loss: 18.7773, val_MinusLogProbMetric: 18.7773

Epoch 390: val_loss did not improve from 18.41734
196/196 - 79s - loss: 18.4217 - MinusLogProbMetric: 18.4217 - val_loss: 18.7773 - val_MinusLogProbMetric: 18.7773 - lr: 3.3333e-04 - 79s/epoch - 404ms/step
Epoch 391/1000
2023-09-28 01:50:21.319 
Epoch 391/1000 
	 loss: 18.4951, MinusLogProbMetric: 18.4951, val_loss: 18.6365, val_MinusLogProbMetric: 18.6365

Epoch 391: val_loss did not improve from 18.41734
196/196 - 79s - loss: 18.4951 - MinusLogProbMetric: 18.4951 - val_loss: 18.6365 - val_MinusLogProbMetric: 18.6365 - lr: 3.3333e-04 - 79s/epoch - 405ms/step
Epoch 392/1000
2023-09-28 01:51:40.418 
Epoch 392/1000 
	 loss: 18.4345, MinusLogProbMetric: 18.4345, val_loss: 18.8305, val_MinusLogProbMetric: 18.8305

Epoch 392: val_loss did not improve from 18.41734
196/196 - 79s - loss: 18.4345 - MinusLogProbMetric: 18.4345 - val_loss: 18.8305 - val_MinusLogProbMetric: 18.8305 - lr: 3.3333e-04 - 79s/epoch - 404ms/step
Epoch 393/1000
2023-09-28 01:52:59.804 
Epoch 393/1000 
	 loss: 18.4518, MinusLogProbMetric: 18.4518, val_loss: 18.7321, val_MinusLogProbMetric: 18.7321

Epoch 393: val_loss did not improve from 18.41734
196/196 - 79s - loss: 18.4518 - MinusLogProbMetric: 18.4518 - val_loss: 18.7321 - val_MinusLogProbMetric: 18.7321 - lr: 3.3333e-04 - 79s/epoch - 405ms/step
Epoch 394/1000
2023-09-28 01:54:18.767 
Epoch 394/1000 
	 loss: 18.3133, MinusLogProbMetric: 18.3133, val_loss: 18.5976, val_MinusLogProbMetric: 18.5976

Epoch 394: val_loss did not improve from 18.41734
196/196 - 79s - loss: 18.3133 - MinusLogProbMetric: 18.3133 - val_loss: 18.5976 - val_MinusLogProbMetric: 18.5976 - lr: 3.3333e-04 - 79s/epoch - 403ms/step
Epoch 395/1000
2023-09-28 01:55:38.756 
Epoch 395/1000 
	 loss: 18.3752, MinusLogProbMetric: 18.3752, val_loss: 18.5330, val_MinusLogProbMetric: 18.5330

Epoch 395: val_loss did not improve from 18.41734
196/196 - 80s - loss: 18.3752 - MinusLogProbMetric: 18.3752 - val_loss: 18.5330 - val_MinusLogProbMetric: 18.5330 - lr: 3.3333e-04 - 80s/epoch - 408ms/step
Epoch 396/1000
2023-09-28 01:56:57.519 
Epoch 396/1000 
	 loss: 18.4332, MinusLogProbMetric: 18.4332, val_loss: 18.7199, val_MinusLogProbMetric: 18.7199

Epoch 396: val_loss did not improve from 18.41734
196/196 - 79s - loss: 18.4332 - MinusLogProbMetric: 18.4332 - val_loss: 18.7199 - val_MinusLogProbMetric: 18.7199 - lr: 3.3333e-04 - 79s/epoch - 402ms/step
Epoch 397/1000
2023-09-28 01:58:16.968 
Epoch 397/1000 
	 loss: 18.4093, MinusLogProbMetric: 18.4093, val_loss: 18.3844, val_MinusLogProbMetric: 18.3844

Epoch 397: val_loss improved from 18.41734 to 18.38438, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 81s - loss: 18.4093 - MinusLogProbMetric: 18.4093 - val_loss: 18.3844 - val_MinusLogProbMetric: 18.3844 - lr: 3.3333e-04 - 81s/epoch - 412ms/step
Epoch 398/1000
2023-09-28 01:59:37.440 
Epoch 398/1000 
	 loss: 18.4284, MinusLogProbMetric: 18.4284, val_loss: 18.3260, val_MinusLogProbMetric: 18.3260

Epoch 398: val_loss improved from 18.38438 to 18.32597, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 80s - loss: 18.4284 - MinusLogProbMetric: 18.4284 - val_loss: 18.3260 - val_MinusLogProbMetric: 18.3260 - lr: 3.3333e-04 - 80s/epoch - 411ms/step
Epoch 399/1000
2023-09-28 02:00:57.939 
Epoch 399/1000 
	 loss: 18.4278, MinusLogProbMetric: 18.4278, val_loss: 18.5064, val_MinusLogProbMetric: 18.5064

Epoch 399: val_loss did not improve from 18.32597
196/196 - 79s - loss: 18.4278 - MinusLogProbMetric: 18.4278 - val_loss: 18.5064 - val_MinusLogProbMetric: 18.5064 - lr: 3.3333e-04 - 79s/epoch - 404ms/step
Epoch 400/1000
2023-09-28 02:02:17.588 
Epoch 400/1000 
	 loss: 18.3920, MinusLogProbMetric: 18.3920, val_loss: 18.8711, val_MinusLogProbMetric: 18.8711

Epoch 400: val_loss did not improve from 18.32597
196/196 - 80s - loss: 18.3920 - MinusLogProbMetric: 18.3920 - val_loss: 18.8711 - val_MinusLogProbMetric: 18.8711 - lr: 3.3333e-04 - 80s/epoch - 406ms/step
Epoch 401/1000
2023-09-28 02:03:36.752 
Epoch 401/1000 
	 loss: 18.3990, MinusLogProbMetric: 18.3990, val_loss: 18.5065, val_MinusLogProbMetric: 18.5065

Epoch 401: val_loss did not improve from 18.32597
196/196 - 79s - loss: 18.3990 - MinusLogProbMetric: 18.3990 - val_loss: 18.5065 - val_MinusLogProbMetric: 18.5065 - lr: 3.3333e-04 - 79s/epoch - 404ms/step
Epoch 402/1000
2023-09-28 02:04:55.882 
Epoch 402/1000 
	 loss: 18.4003, MinusLogProbMetric: 18.4003, val_loss: 18.3685, val_MinusLogProbMetric: 18.3685

Epoch 402: val_loss did not improve from 18.32597
196/196 - 79s - loss: 18.4003 - MinusLogProbMetric: 18.4003 - val_loss: 18.3685 - val_MinusLogProbMetric: 18.3685 - lr: 3.3333e-04 - 79s/epoch - 404ms/step
Epoch 403/1000
2023-09-28 02:06:14.200 
Epoch 403/1000 
	 loss: 18.3345, MinusLogProbMetric: 18.3345, val_loss: 18.8337, val_MinusLogProbMetric: 18.8337

Epoch 403: val_loss did not improve from 18.32597
196/196 - 78s - loss: 18.3345 - MinusLogProbMetric: 18.3345 - val_loss: 18.8337 - val_MinusLogProbMetric: 18.8337 - lr: 3.3333e-04 - 78s/epoch - 400ms/step
Epoch 404/1000
2023-09-28 02:07:33.171 
Epoch 404/1000 
	 loss: 18.4442, MinusLogProbMetric: 18.4442, val_loss: 18.3210, val_MinusLogProbMetric: 18.3210

Epoch 404: val_loss improved from 18.32597 to 18.32103, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 80s - loss: 18.4442 - MinusLogProbMetric: 18.4442 - val_loss: 18.3210 - val_MinusLogProbMetric: 18.3210 - lr: 3.3333e-04 - 80s/epoch - 409ms/step
Epoch 405/1000
2023-09-28 02:08:53.443 
Epoch 405/1000 
	 loss: 18.3045, MinusLogProbMetric: 18.3045, val_loss: 18.5314, val_MinusLogProbMetric: 18.5314

Epoch 405: val_loss did not improve from 18.32103
196/196 - 79s - loss: 18.3045 - MinusLogProbMetric: 18.3045 - val_loss: 18.5314 - val_MinusLogProbMetric: 18.5314 - lr: 3.3333e-04 - 79s/epoch - 403ms/step
Epoch 406/1000
2023-09-28 02:10:13.270 
Epoch 406/1000 
	 loss: 18.2904, MinusLogProbMetric: 18.2904, val_loss: 18.4412, val_MinusLogProbMetric: 18.4412

Epoch 406: val_loss did not improve from 18.32103
196/196 - 80s - loss: 18.2904 - MinusLogProbMetric: 18.2904 - val_loss: 18.4412 - val_MinusLogProbMetric: 18.4412 - lr: 3.3333e-04 - 80s/epoch - 407ms/step
Epoch 407/1000
2023-09-28 02:11:33.103 
Epoch 407/1000 
	 loss: 18.3348, MinusLogProbMetric: 18.3348, val_loss: 18.2880, val_MinusLogProbMetric: 18.2880

Epoch 407: val_loss improved from 18.32103 to 18.28801, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 81s - loss: 18.3348 - MinusLogProbMetric: 18.3348 - val_loss: 18.2880 - val_MinusLogProbMetric: 18.2880 - lr: 3.3333e-04 - 81s/epoch - 414ms/step
Epoch 408/1000
2023-09-28 02:12:53.641 
Epoch 408/1000 
	 loss: 18.3014, MinusLogProbMetric: 18.3014, val_loss: 18.3058, val_MinusLogProbMetric: 18.3058

Epoch 408: val_loss did not improve from 18.28801
196/196 - 79s - loss: 18.3014 - MinusLogProbMetric: 18.3014 - val_loss: 18.3058 - val_MinusLogProbMetric: 18.3058 - lr: 3.3333e-04 - 79s/epoch - 404ms/step
Epoch 409/1000
2023-09-28 02:14:13.134 
Epoch 409/1000 
	 loss: 18.3213, MinusLogProbMetric: 18.3213, val_loss: 19.1004, val_MinusLogProbMetric: 19.1004

Epoch 409: val_loss did not improve from 18.28801
196/196 - 79s - loss: 18.3213 - MinusLogProbMetric: 18.3213 - val_loss: 19.1004 - val_MinusLogProbMetric: 19.1004 - lr: 3.3333e-04 - 79s/epoch - 406ms/step
Epoch 410/1000
2023-09-28 02:15:32.306 
Epoch 410/1000 
	 loss: 18.4941, MinusLogProbMetric: 18.4941, val_loss: 18.2421, val_MinusLogProbMetric: 18.2421

Epoch 410: val_loss improved from 18.28801 to 18.24209, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 80s - loss: 18.4941 - MinusLogProbMetric: 18.4941 - val_loss: 18.2421 - val_MinusLogProbMetric: 18.2421 - lr: 3.3333e-04 - 80s/epoch - 410ms/step
Epoch 411/1000
2023-09-28 02:16:53.327 
Epoch 411/1000 
	 loss: 18.2401, MinusLogProbMetric: 18.2401, val_loss: 18.4144, val_MinusLogProbMetric: 18.4144

Epoch 411: val_loss did not improve from 18.24209
196/196 - 80s - loss: 18.2401 - MinusLogProbMetric: 18.2401 - val_loss: 18.4144 - val_MinusLogProbMetric: 18.4144 - lr: 3.3333e-04 - 80s/epoch - 407ms/step
Epoch 412/1000
2023-09-28 02:18:12.533 
Epoch 412/1000 
	 loss: 18.3352, MinusLogProbMetric: 18.3352, val_loss: 18.6355, val_MinusLogProbMetric: 18.6355

Epoch 412: val_loss did not improve from 18.24209
196/196 - 79s - loss: 18.3352 - MinusLogProbMetric: 18.3352 - val_loss: 18.6355 - val_MinusLogProbMetric: 18.6355 - lr: 3.3333e-04 - 79s/epoch - 404ms/step
Epoch 413/1000
2023-09-28 02:19:31.843 
Epoch 413/1000 
	 loss: 18.3313, MinusLogProbMetric: 18.3313, val_loss: 19.3775, val_MinusLogProbMetric: 19.3775

Epoch 413: val_loss did not improve from 18.24209
196/196 - 79s - loss: 18.3313 - MinusLogProbMetric: 18.3313 - val_loss: 19.3775 - val_MinusLogProbMetric: 19.3775 - lr: 3.3333e-04 - 79s/epoch - 405ms/step
Epoch 414/1000
2023-09-28 02:20:51.440 
Epoch 414/1000 
	 loss: 18.2852, MinusLogProbMetric: 18.2852, val_loss: 18.7527, val_MinusLogProbMetric: 18.7527

Epoch 414: val_loss did not improve from 18.24209
196/196 - 80s - loss: 18.2852 - MinusLogProbMetric: 18.2852 - val_loss: 18.7527 - val_MinusLogProbMetric: 18.7527 - lr: 3.3333e-04 - 80s/epoch - 406ms/step
Epoch 415/1000
2023-09-28 02:22:10.338 
Epoch 415/1000 
	 loss: 18.2883, MinusLogProbMetric: 18.2883, val_loss: 18.5081, val_MinusLogProbMetric: 18.5081

Epoch 415: val_loss did not improve from 18.24209
196/196 - 79s - loss: 18.2883 - MinusLogProbMetric: 18.2883 - val_loss: 18.5081 - val_MinusLogProbMetric: 18.5081 - lr: 3.3333e-04 - 79s/epoch - 403ms/step
Epoch 416/1000
2023-09-28 02:23:29.929 
Epoch 416/1000 
	 loss: 18.2602, MinusLogProbMetric: 18.2602, val_loss: 18.7408, val_MinusLogProbMetric: 18.7408

Epoch 416: val_loss did not improve from 18.24209
196/196 - 80s - loss: 18.2602 - MinusLogProbMetric: 18.2602 - val_loss: 18.7408 - val_MinusLogProbMetric: 18.7408 - lr: 3.3333e-04 - 80s/epoch - 406ms/step
Epoch 417/1000
2023-09-28 02:24:48.974 
Epoch 417/1000 
	 loss: 18.3138, MinusLogProbMetric: 18.3138, val_loss: 18.4649, val_MinusLogProbMetric: 18.4649

Epoch 417: val_loss did not improve from 18.24209
196/196 - 79s - loss: 18.3138 - MinusLogProbMetric: 18.3138 - val_loss: 18.4649 - val_MinusLogProbMetric: 18.4649 - lr: 3.3333e-04 - 79s/epoch - 403ms/step
Epoch 418/1000
2023-09-28 02:26:07.979 
Epoch 418/1000 
	 loss: 18.2798, MinusLogProbMetric: 18.2798, val_loss: 18.4291, val_MinusLogProbMetric: 18.4291

Epoch 418: val_loss did not improve from 18.24209
196/196 - 79s - loss: 18.2798 - MinusLogProbMetric: 18.2798 - val_loss: 18.4291 - val_MinusLogProbMetric: 18.4291 - lr: 3.3333e-04 - 79s/epoch - 403ms/step
Epoch 419/1000
2023-09-28 02:27:27.242 
Epoch 419/1000 
	 loss: 18.3129, MinusLogProbMetric: 18.3129, val_loss: 18.5769, val_MinusLogProbMetric: 18.5769

Epoch 419: val_loss did not improve from 18.24209
196/196 - 79s - loss: 18.3129 - MinusLogProbMetric: 18.3129 - val_loss: 18.5769 - val_MinusLogProbMetric: 18.5769 - lr: 3.3333e-04 - 79s/epoch - 404ms/step
Epoch 420/1000
2023-09-28 02:28:46.317 
Epoch 420/1000 
	 loss: 18.3596, MinusLogProbMetric: 18.3596, val_loss: 18.4388, val_MinusLogProbMetric: 18.4388

Epoch 420: val_loss did not improve from 18.24209
196/196 - 79s - loss: 18.3596 - MinusLogProbMetric: 18.3596 - val_loss: 18.4388 - val_MinusLogProbMetric: 18.4388 - lr: 3.3333e-04 - 79s/epoch - 403ms/step
Epoch 421/1000
2023-09-28 02:30:05.552 
Epoch 421/1000 
	 loss: 18.3196, MinusLogProbMetric: 18.3196, val_loss: 18.4901, val_MinusLogProbMetric: 18.4901

Epoch 421: val_loss did not improve from 18.24209
196/196 - 79s - loss: 18.3196 - MinusLogProbMetric: 18.3196 - val_loss: 18.4901 - val_MinusLogProbMetric: 18.4901 - lr: 3.3333e-04 - 79s/epoch - 404ms/step
Epoch 422/1000
2023-09-28 02:31:24.432 
Epoch 422/1000 
	 loss: 18.3155, MinusLogProbMetric: 18.3155, val_loss: 18.4821, val_MinusLogProbMetric: 18.4821

Epoch 422: val_loss did not improve from 18.24209
196/196 - 79s - loss: 18.3155 - MinusLogProbMetric: 18.3155 - val_loss: 18.4821 - val_MinusLogProbMetric: 18.4821 - lr: 3.3333e-04 - 79s/epoch - 402ms/step
Epoch 423/1000
2023-09-28 02:32:43.889 
Epoch 423/1000 
	 loss: 18.2535, MinusLogProbMetric: 18.2535, val_loss: 18.6013, val_MinusLogProbMetric: 18.6013

Epoch 423: val_loss did not improve from 18.24209
196/196 - 79s - loss: 18.2535 - MinusLogProbMetric: 18.2535 - val_loss: 18.6013 - val_MinusLogProbMetric: 18.6013 - lr: 3.3333e-04 - 79s/epoch - 405ms/step
Epoch 424/1000
2023-09-28 02:34:03.800 
Epoch 424/1000 
	 loss: 18.2305, MinusLogProbMetric: 18.2305, val_loss: 18.4714, val_MinusLogProbMetric: 18.4714

Epoch 424: val_loss did not improve from 18.24209
196/196 - 80s - loss: 18.2305 - MinusLogProbMetric: 18.2305 - val_loss: 18.4714 - val_MinusLogProbMetric: 18.4714 - lr: 3.3333e-04 - 80s/epoch - 408ms/step
Epoch 425/1000
2023-09-28 02:35:23.211 
Epoch 425/1000 
	 loss: 18.2465, MinusLogProbMetric: 18.2465, val_loss: 18.2071, val_MinusLogProbMetric: 18.2071

Epoch 425: val_loss improved from 18.24209 to 18.20707, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 81s - loss: 18.2465 - MinusLogProbMetric: 18.2465 - val_loss: 18.2071 - val_MinusLogProbMetric: 18.2071 - lr: 3.3333e-04 - 81s/epoch - 413ms/step
Epoch 426/1000
2023-09-28 02:36:43.785 
Epoch 426/1000 
	 loss: 18.2168, MinusLogProbMetric: 18.2168, val_loss: 18.6167, val_MinusLogProbMetric: 18.6167

Epoch 426: val_loss did not improve from 18.20707
196/196 - 79s - loss: 18.2168 - MinusLogProbMetric: 18.2168 - val_loss: 18.6167 - val_MinusLogProbMetric: 18.6167 - lr: 3.3333e-04 - 79s/epoch - 404ms/step
Epoch 427/1000
2023-09-28 02:38:02.943 
Epoch 427/1000 
	 loss: 18.2734, MinusLogProbMetric: 18.2734, val_loss: 18.6216, val_MinusLogProbMetric: 18.6216

Epoch 427: val_loss did not improve from 18.20707
196/196 - 79s - loss: 18.2734 - MinusLogProbMetric: 18.2734 - val_loss: 18.6216 - val_MinusLogProbMetric: 18.6216 - lr: 3.3333e-04 - 79s/epoch - 404ms/step
Epoch 428/1000
2023-09-28 02:39:22.533 
Epoch 428/1000 
	 loss: 18.2328, MinusLogProbMetric: 18.2328, val_loss: 18.8654, val_MinusLogProbMetric: 18.8654

Epoch 428: val_loss did not improve from 18.20707
196/196 - 80s - loss: 18.2328 - MinusLogProbMetric: 18.2328 - val_loss: 18.8654 - val_MinusLogProbMetric: 18.8654 - lr: 3.3333e-04 - 80s/epoch - 406ms/step
Epoch 429/1000
2023-09-28 02:40:41.460 
Epoch 429/1000 
	 loss: 18.2425, MinusLogProbMetric: 18.2425, val_loss: 18.4882, val_MinusLogProbMetric: 18.4882

Epoch 429: val_loss did not improve from 18.20707
196/196 - 79s - loss: 18.2425 - MinusLogProbMetric: 18.2425 - val_loss: 18.4882 - val_MinusLogProbMetric: 18.4882 - lr: 3.3333e-04 - 79s/epoch - 403ms/step
Epoch 430/1000
2023-09-28 02:42:00.545 
Epoch 430/1000 
	 loss: 18.4875, MinusLogProbMetric: 18.4875, val_loss: 18.5016, val_MinusLogProbMetric: 18.5016

Epoch 430: val_loss did not improve from 18.20707
196/196 - 79s - loss: 18.4875 - MinusLogProbMetric: 18.4875 - val_loss: 18.5016 - val_MinusLogProbMetric: 18.5016 - lr: 3.3333e-04 - 79s/epoch - 403ms/step
Epoch 431/1000
2023-09-28 02:43:19.484 
Epoch 431/1000 
	 loss: 18.1953, MinusLogProbMetric: 18.1953, val_loss: 18.3624, val_MinusLogProbMetric: 18.3624

Epoch 431: val_loss did not improve from 18.20707
196/196 - 79s - loss: 18.1953 - MinusLogProbMetric: 18.1953 - val_loss: 18.3624 - val_MinusLogProbMetric: 18.3624 - lr: 3.3333e-04 - 79s/epoch - 403ms/step
Epoch 432/1000
2023-09-28 02:44:38.682 
Epoch 432/1000 
	 loss: 18.2611, MinusLogProbMetric: 18.2611, val_loss: 18.3584, val_MinusLogProbMetric: 18.3584

Epoch 432: val_loss did not improve from 18.20707
196/196 - 79s - loss: 18.2611 - MinusLogProbMetric: 18.2611 - val_loss: 18.3584 - val_MinusLogProbMetric: 18.3584 - lr: 3.3333e-04 - 79s/epoch - 404ms/step
Epoch 433/1000
2023-09-28 02:45:57.558 
Epoch 433/1000 
	 loss: 18.2219, MinusLogProbMetric: 18.2219, val_loss: 18.2455, val_MinusLogProbMetric: 18.2455

Epoch 433: val_loss did not improve from 18.20707
196/196 - 79s - loss: 18.2219 - MinusLogProbMetric: 18.2219 - val_loss: 18.2455 - val_MinusLogProbMetric: 18.2455 - lr: 3.3333e-04 - 79s/epoch - 402ms/step
Epoch 434/1000
2023-09-28 02:47:17.424 
Epoch 434/1000 
	 loss: 18.2663, MinusLogProbMetric: 18.2663, val_loss: 18.4901, val_MinusLogProbMetric: 18.4901

Epoch 434: val_loss did not improve from 18.20707
196/196 - 80s - loss: 18.2663 - MinusLogProbMetric: 18.2663 - val_loss: 18.4901 - val_MinusLogProbMetric: 18.4901 - lr: 3.3333e-04 - 80s/epoch - 407ms/step
Epoch 435/1000
2023-09-28 02:48:37.002 
Epoch 435/1000 
	 loss: 18.2167, MinusLogProbMetric: 18.2167, val_loss: 19.0320, val_MinusLogProbMetric: 19.0320

Epoch 435: val_loss did not improve from 18.20707
196/196 - 80s - loss: 18.2167 - MinusLogProbMetric: 18.2167 - val_loss: 19.0320 - val_MinusLogProbMetric: 19.0320 - lr: 3.3333e-04 - 80s/epoch - 406ms/step
Epoch 436/1000
2023-09-28 02:49:55.827 
Epoch 436/1000 
	 loss: 18.1932, MinusLogProbMetric: 18.1932, val_loss: 18.3605, val_MinusLogProbMetric: 18.3605

Epoch 436: val_loss did not improve from 18.20707
196/196 - 79s - loss: 18.1932 - MinusLogProbMetric: 18.1932 - val_loss: 18.3605 - val_MinusLogProbMetric: 18.3605 - lr: 3.3333e-04 - 79s/epoch - 402ms/step
Epoch 437/1000
2023-09-28 02:51:14.674 
Epoch 437/1000 
	 loss: 18.1855, MinusLogProbMetric: 18.1855, val_loss: 18.1516, val_MinusLogProbMetric: 18.1516

Epoch 437: val_loss improved from 18.20707 to 18.15161, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 80s - loss: 18.1855 - MinusLogProbMetric: 18.1855 - val_loss: 18.1516 - val_MinusLogProbMetric: 18.1516 - lr: 3.3333e-04 - 80s/epoch - 408ms/step
Epoch 438/1000
2023-09-28 02:52:35.017 
Epoch 438/1000 
	 loss: 18.2058, MinusLogProbMetric: 18.2058, val_loss: 18.2158, val_MinusLogProbMetric: 18.2158

Epoch 438: val_loss did not improve from 18.15161
196/196 - 79s - loss: 18.2058 - MinusLogProbMetric: 18.2058 - val_loss: 18.2158 - val_MinusLogProbMetric: 18.2158 - lr: 3.3333e-04 - 79s/epoch - 404ms/step
Epoch 439/1000
2023-09-28 02:53:54.737 
Epoch 439/1000 
	 loss: 18.2391, MinusLogProbMetric: 18.2391, val_loss: 18.2060, val_MinusLogProbMetric: 18.2060

Epoch 439: val_loss did not improve from 18.15161
196/196 - 80s - loss: 18.2391 - MinusLogProbMetric: 18.2391 - val_loss: 18.2060 - val_MinusLogProbMetric: 18.2060 - lr: 3.3333e-04 - 80s/epoch - 407ms/step
Epoch 440/1000
2023-09-28 02:55:13.625 
Epoch 440/1000 
	 loss: 18.2030, MinusLogProbMetric: 18.2030, val_loss: 18.7025, val_MinusLogProbMetric: 18.7025

Epoch 440: val_loss did not improve from 18.15161
196/196 - 79s - loss: 18.2030 - MinusLogProbMetric: 18.2030 - val_loss: 18.7025 - val_MinusLogProbMetric: 18.7025 - lr: 3.3333e-04 - 79s/epoch - 402ms/step
Epoch 441/1000
2023-09-28 02:56:33.277 
Epoch 441/1000 
	 loss: 18.2062, MinusLogProbMetric: 18.2062, val_loss: 18.4487, val_MinusLogProbMetric: 18.4487

Epoch 441: val_loss did not improve from 18.15161
196/196 - 80s - loss: 18.2062 - MinusLogProbMetric: 18.2062 - val_loss: 18.4487 - val_MinusLogProbMetric: 18.4487 - lr: 3.3333e-04 - 80s/epoch - 406ms/step
Epoch 442/1000
2023-09-28 02:57:52.944 
Epoch 442/1000 
	 loss: 18.2087, MinusLogProbMetric: 18.2087, val_loss: 18.4738, val_MinusLogProbMetric: 18.4738

Epoch 442: val_loss did not improve from 18.15161
196/196 - 80s - loss: 18.2087 - MinusLogProbMetric: 18.2087 - val_loss: 18.4738 - val_MinusLogProbMetric: 18.4738 - lr: 3.3333e-04 - 80s/epoch - 406ms/step
Epoch 443/1000
2023-09-28 02:59:11.950 
Epoch 443/1000 
	 loss: 18.3050, MinusLogProbMetric: 18.3050, val_loss: 18.5789, val_MinusLogProbMetric: 18.5789

Epoch 443: val_loss did not improve from 18.15161
196/196 - 79s - loss: 18.3050 - MinusLogProbMetric: 18.3050 - val_loss: 18.5789 - val_MinusLogProbMetric: 18.5789 - lr: 3.3333e-04 - 79s/epoch - 403ms/step
Epoch 444/1000
2023-09-28 03:00:31.270 
Epoch 444/1000 
	 loss: 18.1998, MinusLogProbMetric: 18.1998, val_loss: 18.4412, val_MinusLogProbMetric: 18.4412

Epoch 444: val_loss did not improve from 18.15161
196/196 - 79s - loss: 18.1998 - MinusLogProbMetric: 18.1998 - val_loss: 18.4412 - val_MinusLogProbMetric: 18.4412 - lr: 3.3333e-04 - 79s/epoch - 405ms/step
Epoch 445/1000
2023-09-28 03:01:50.361 
Epoch 445/1000 
	 loss: 18.1941, MinusLogProbMetric: 18.1941, val_loss: 18.1418, val_MinusLogProbMetric: 18.1418

Epoch 445: val_loss improved from 18.15161 to 18.14178, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 80s - loss: 18.1941 - MinusLogProbMetric: 18.1941 - val_loss: 18.1418 - val_MinusLogProbMetric: 18.1418 - lr: 3.3333e-04 - 80s/epoch - 410ms/step
Epoch 446/1000
2023-09-28 03:03:10.215 
Epoch 446/1000 
	 loss: 18.1568, MinusLogProbMetric: 18.1568, val_loss: 18.2840, val_MinusLogProbMetric: 18.2840

Epoch 446: val_loss did not improve from 18.14178
196/196 - 79s - loss: 18.1568 - MinusLogProbMetric: 18.1568 - val_loss: 18.2840 - val_MinusLogProbMetric: 18.2840 - lr: 3.3333e-04 - 79s/epoch - 401ms/step
Epoch 447/1000
2023-09-28 03:04:29.529 
Epoch 447/1000 
	 loss: 18.1430, MinusLogProbMetric: 18.1430, val_loss: 18.3243, val_MinusLogProbMetric: 18.3243

Epoch 447: val_loss did not improve from 18.14178
196/196 - 79s - loss: 18.1430 - MinusLogProbMetric: 18.1430 - val_loss: 18.3243 - val_MinusLogProbMetric: 18.3243 - lr: 3.3333e-04 - 79s/epoch - 405ms/step
Epoch 448/1000
2023-09-28 03:05:48.713 
Epoch 448/1000 
	 loss: 18.1711, MinusLogProbMetric: 18.1711, val_loss: 18.7954, val_MinusLogProbMetric: 18.7954

Epoch 448: val_loss did not improve from 18.14178
196/196 - 79s - loss: 18.1711 - MinusLogProbMetric: 18.1711 - val_loss: 18.7954 - val_MinusLogProbMetric: 18.7954 - lr: 3.3333e-04 - 79s/epoch - 404ms/step
Epoch 449/1000
2023-09-28 03:07:08.304 
Epoch 449/1000 
	 loss: 18.1199, MinusLogProbMetric: 18.1199, val_loss: 18.2573, val_MinusLogProbMetric: 18.2573

Epoch 449: val_loss did not improve from 18.14178
196/196 - 80s - loss: 18.1199 - MinusLogProbMetric: 18.1199 - val_loss: 18.2573 - val_MinusLogProbMetric: 18.2573 - lr: 3.3333e-04 - 80s/epoch - 406ms/step
Epoch 450/1000
2023-09-28 03:08:27.755 
Epoch 450/1000 
	 loss: 19.6229, MinusLogProbMetric: 19.6229, val_loss: 18.6099, val_MinusLogProbMetric: 18.6099

Epoch 450: val_loss did not improve from 18.14178
196/196 - 79s - loss: 19.6229 - MinusLogProbMetric: 19.6229 - val_loss: 18.6099 - val_MinusLogProbMetric: 18.6099 - lr: 3.3333e-04 - 79s/epoch - 405ms/step
Epoch 451/1000
2023-09-28 03:09:46.855 
Epoch 451/1000 
	 loss: 19.7814, MinusLogProbMetric: 19.7814, val_loss: 20.8487, val_MinusLogProbMetric: 20.8487

Epoch 451: val_loss did not improve from 18.14178
196/196 - 79s - loss: 19.7814 - MinusLogProbMetric: 19.7814 - val_loss: 20.8487 - val_MinusLogProbMetric: 20.8487 - lr: 3.3333e-04 - 79s/epoch - 404ms/step
Epoch 452/1000
2023-09-28 03:11:06.122 
Epoch 452/1000 
	 loss: 19.7597, MinusLogProbMetric: 19.7597, val_loss: 19.3183, val_MinusLogProbMetric: 19.3183

Epoch 452: val_loss did not improve from 18.14178
196/196 - 79s - loss: 19.7597 - MinusLogProbMetric: 19.7597 - val_loss: 19.3183 - val_MinusLogProbMetric: 19.3183 - lr: 3.3333e-04 - 79s/epoch - 404ms/step
Epoch 453/1000
2023-09-28 03:12:25.441 
Epoch 453/1000 
	 loss: 18.8742, MinusLogProbMetric: 18.8742, val_loss: 18.4502, val_MinusLogProbMetric: 18.4502

Epoch 453: val_loss did not improve from 18.14178
196/196 - 79s - loss: 18.8742 - MinusLogProbMetric: 18.8742 - val_loss: 18.4502 - val_MinusLogProbMetric: 18.4502 - lr: 3.3333e-04 - 79s/epoch - 405ms/step
Epoch 454/1000
2023-09-28 03:13:44.633 
Epoch 454/1000 
	 loss: 18.2952, MinusLogProbMetric: 18.2952, val_loss: 18.5505, val_MinusLogProbMetric: 18.5505

Epoch 454: val_loss did not improve from 18.14178
196/196 - 79s - loss: 18.2952 - MinusLogProbMetric: 18.2952 - val_loss: 18.5505 - val_MinusLogProbMetric: 18.5505 - lr: 3.3333e-04 - 79s/epoch - 404ms/step
Epoch 455/1000
2023-09-28 03:15:03.324 
Epoch 455/1000 
	 loss: 18.3133, MinusLogProbMetric: 18.3133, val_loss: 18.9813, val_MinusLogProbMetric: 18.9813

Epoch 455: val_loss did not improve from 18.14178
196/196 - 79s - loss: 18.3133 - MinusLogProbMetric: 18.3133 - val_loss: 18.9813 - val_MinusLogProbMetric: 18.9813 - lr: 3.3333e-04 - 79s/epoch - 401ms/step
Epoch 456/1000
2023-09-28 03:16:22.089 
Epoch 456/1000 
	 loss: 18.2392, MinusLogProbMetric: 18.2392, val_loss: 18.4640, val_MinusLogProbMetric: 18.4640

Epoch 456: val_loss did not improve from 18.14178
196/196 - 79s - loss: 18.2392 - MinusLogProbMetric: 18.2392 - val_loss: 18.4640 - val_MinusLogProbMetric: 18.4640 - lr: 3.3333e-04 - 79s/epoch - 402ms/step
Epoch 457/1000
2023-09-28 03:17:41.177 
Epoch 457/1000 
	 loss: 18.2111, MinusLogProbMetric: 18.2111, val_loss: 18.5193, val_MinusLogProbMetric: 18.5193

Epoch 457: val_loss did not improve from 18.14178
196/196 - 79s - loss: 18.2111 - MinusLogProbMetric: 18.2111 - val_loss: 18.5193 - val_MinusLogProbMetric: 18.5193 - lr: 3.3333e-04 - 79s/epoch - 403ms/step
Epoch 458/1000
2023-09-28 03:19:00.545 
Epoch 458/1000 
	 loss: 18.2661, MinusLogProbMetric: 18.2661, val_loss: 18.4984, val_MinusLogProbMetric: 18.4984

Epoch 458: val_loss did not improve from 18.14178
196/196 - 79s - loss: 18.2661 - MinusLogProbMetric: 18.2661 - val_loss: 18.4984 - val_MinusLogProbMetric: 18.4984 - lr: 3.3333e-04 - 79s/epoch - 405ms/step
Epoch 459/1000
2023-09-28 03:20:20.205 
Epoch 459/1000 
	 loss: 18.1414, MinusLogProbMetric: 18.1414, val_loss: 18.4107, val_MinusLogProbMetric: 18.4107

Epoch 459: val_loss did not improve from 18.14178
196/196 - 80s - loss: 18.1414 - MinusLogProbMetric: 18.1414 - val_loss: 18.4107 - val_MinusLogProbMetric: 18.4107 - lr: 3.3333e-04 - 80s/epoch - 406ms/step
Epoch 460/1000
2023-09-28 03:21:39.293 
Epoch 460/1000 
	 loss: 18.1892, MinusLogProbMetric: 18.1892, val_loss: 18.7534, val_MinusLogProbMetric: 18.7534

Epoch 460: val_loss did not improve from 18.14178
196/196 - 79s - loss: 18.1892 - MinusLogProbMetric: 18.1892 - val_loss: 18.7534 - val_MinusLogProbMetric: 18.7534 - lr: 3.3333e-04 - 79s/epoch - 403ms/step
Epoch 461/1000
2023-09-28 03:22:59.089 
Epoch 461/1000 
	 loss: 18.1506, MinusLogProbMetric: 18.1506, val_loss: 18.3558, val_MinusLogProbMetric: 18.3558

Epoch 461: val_loss did not improve from 18.14178
196/196 - 80s - loss: 18.1506 - MinusLogProbMetric: 18.1506 - val_loss: 18.3558 - val_MinusLogProbMetric: 18.3558 - lr: 3.3333e-04 - 80s/epoch - 407ms/step
Epoch 462/1000
2023-09-28 03:24:18.850 
Epoch 462/1000 
	 loss: 18.2240, MinusLogProbMetric: 18.2240, val_loss: 18.5383, val_MinusLogProbMetric: 18.5383

Epoch 462: val_loss did not improve from 18.14178
196/196 - 80s - loss: 18.2240 - MinusLogProbMetric: 18.2240 - val_loss: 18.5383 - val_MinusLogProbMetric: 18.5383 - lr: 3.3333e-04 - 80s/epoch - 407ms/step
Epoch 463/1000
2023-09-28 03:25:38.103 
Epoch 463/1000 
	 loss: 18.1383, MinusLogProbMetric: 18.1383, val_loss: 18.3995, val_MinusLogProbMetric: 18.3995

Epoch 463: val_loss did not improve from 18.14178
196/196 - 79s - loss: 18.1383 - MinusLogProbMetric: 18.1383 - val_loss: 18.3995 - val_MinusLogProbMetric: 18.3995 - lr: 3.3333e-04 - 79s/epoch - 404ms/step
Epoch 464/1000
2023-09-28 03:26:57.448 
Epoch 464/1000 
	 loss: 18.1766, MinusLogProbMetric: 18.1766, val_loss: 18.2448, val_MinusLogProbMetric: 18.2448

Epoch 464: val_loss did not improve from 18.14178
196/196 - 79s - loss: 18.1766 - MinusLogProbMetric: 18.1766 - val_loss: 18.2448 - val_MinusLogProbMetric: 18.2448 - lr: 3.3333e-04 - 79s/epoch - 405ms/step
Epoch 465/1000
2023-09-28 03:28:16.399 
Epoch 465/1000 
	 loss: 18.1554, MinusLogProbMetric: 18.1554, val_loss: 18.2613, val_MinusLogProbMetric: 18.2613

Epoch 465: val_loss did not improve from 18.14178
196/196 - 79s - loss: 18.1554 - MinusLogProbMetric: 18.1554 - val_loss: 18.2613 - val_MinusLogProbMetric: 18.2613 - lr: 3.3333e-04 - 79s/epoch - 403ms/step
Epoch 466/1000
2023-09-28 03:29:35.544 
Epoch 466/1000 
	 loss: 18.2034, MinusLogProbMetric: 18.2034, val_loss: 18.3186, val_MinusLogProbMetric: 18.3186

Epoch 466: val_loss did not improve from 18.14178
196/196 - 79s - loss: 18.2034 - MinusLogProbMetric: 18.2034 - val_loss: 18.3186 - val_MinusLogProbMetric: 18.3186 - lr: 3.3333e-04 - 79s/epoch - 404ms/step
Epoch 467/1000
2023-09-28 03:30:54.871 
Epoch 467/1000 
	 loss: 18.1192, MinusLogProbMetric: 18.1192, val_loss: 18.5452, val_MinusLogProbMetric: 18.5452

Epoch 467: val_loss did not improve from 18.14178
196/196 - 79s - loss: 18.1192 - MinusLogProbMetric: 18.1192 - val_loss: 18.5452 - val_MinusLogProbMetric: 18.5452 - lr: 3.3333e-04 - 79s/epoch - 405ms/step
Epoch 468/1000
2023-09-28 03:32:14.030 
Epoch 468/1000 
	 loss: 18.1223, MinusLogProbMetric: 18.1223, val_loss: 18.4282, val_MinusLogProbMetric: 18.4282

Epoch 468: val_loss did not improve from 18.14178
196/196 - 79s - loss: 18.1223 - MinusLogProbMetric: 18.1223 - val_loss: 18.4282 - val_MinusLogProbMetric: 18.4282 - lr: 3.3333e-04 - 79s/epoch - 404ms/step
Epoch 469/1000
2023-09-28 03:33:33.171 
Epoch 469/1000 
	 loss: 18.1103, MinusLogProbMetric: 18.1103, val_loss: 18.5994, val_MinusLogProbMetric: 18.5994

Epoch 469: val_loss did not improve from 18.14178
196/196 - 79s - loss: 18.1103 - MinusLogProbMetric: 18.1103 - val_loss: 18.5994 - val_MinusLogProbMetric: 18.5994 - lr: 3.3333e-04 - 79s/epoch - 404ms/step
Epoch 470/1000
2023-09-28 03:34:52.732 
Epoch 470/1000 
	 loss: 18.1492, MinusLogProbMetric: 18.1492, val_loss: 18.7283, val_MinusLogProbMetric: 18.7283

Epoch 470: val_loss did not improve from 18.14178
196/196 - 80s - loss: 18.1492 - MinusLogProbMetric: 18.1492 - val_loss: 18.7283 - val_MinusLogProbMetric: 18.7283 - lr: 3.3333e-04 - 80s/epoch - 406ms/step
Epoch 471/1000
2023-09-28 03:36:11.938 
Epoch 471/1000 
	 loss: 18.0670, MinusLogProbMetric: 18.0670, val_loss: 18.7641, val_MinusLogProbMetric: 18.7641

Epoch 471: val_loss did not improve from 18.14178
196/196 - 79s - loss: 18.0670 - MinusLogProbMetric: 18.0670 - val_loss: 18.7641 - val_MinusLogProbMetric: 18.7641 - lr: 3.3333e-04 - 79s/epoch - 404ms/step
Epoch 472/1000
2023-09-28 03:37:31.345 
Epoch 472/1000 
	 loss: 18.0883, MinusLogProbMetric: 18.0883, val_loss: 18.5531, val_MinusLogProbMetric: 18.5531

Epoch 472: val_loss did not improve from 18.14178
196/196 - 79s - loss: 18.0883 - MinusLogProbMetric: 18.0883 - val_loss: 18.5531 - val_MinusLogProbMetric: 18.5531 - lr: 3.3333e-04 - 79s/epoch - 405ms/step
Epoch 473/1000
2023-09-28 03:38:50.960 
Epoch 473/1000 
	 loss: 18.1555, MinusLogProbMetric: 18.1555, val_loss: 18.2393, val_MinusLogProbMetric: 18.2393

Epoch 473: val_loss did not improve from 18.14178
196/196 - 80s - loss: 18.1555 - MinusLogProbMetric: 18.1555 - val_loss: 18.2393 - val_MinusLogProbMetric: 18.2393 - lr: 3.3333e-04 - 80s/epoch - 406ms/step
Epoch 474/1000
2023-09-28 03:40:10.098 
Epoch 474/1000 
	 loss: 18.0966, MinusLogProbMetric: 18.0966, val_loss: 18.5128, val_MinusLogProbMetric: 18.5128

Epoch 474: val_loss did not improve from 18.14178
196/196 - 79s - loss: 18.0966 - MinusLogProbMetric: 18.0966 - val_loss: 18.5128 - val_MinusLogProbMetric: 18.5128 - lr: 3.3333e-04 - 79s/epoch - 404ms/step
Epoch 475/1000
2023-09-28 03:41:29.193 
Epoch 475/1000 
	 loss: 18.1027, MinusLogProbMetric: 18.1027, val_loss: 18.3827, val_MinusLogProbMetric: 18.3827

Epoch 475: val_loss did not improve from 18.14178
196/196 - 79s - loss: 18.1027 - MinusLogProbMetric: 18.1027 - val_loss: 18.3827 - val_MinusLogProbMetric: 18.3827 - lr: 3.3333e-04 - 79s/epoch - 404ms/step
Epoch 476/1000
2023-09-28 03:42:47.953 
Epoch 476/1000 
	 loss: 18.2642, MinusLogProbMetric: 18.2642, val_loss: 18.0777, val_MinusLogProbMetric: 18.0777

Epoch 476: val_loss improved from 18.14178 to 18.07771, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 80s - loss: 18.2642 - MinusLogProbMetric: 18.2642 - val_loss: 18.0777 - val_MinusLogProbMetric: 18.0777 - lr: 3.3333e-04 - 80s/epoch - 408ms/step
Epoch 477/1000
2023-09-28 03:44:09.172 
Epoch 477/1000 
	 loss: 18.2098, MinusLogProbMetric: 18.2098, val_loss: 18.2168, val_MinusLogProbMetric: 18.2168

Epoch 477: val_loss did not improve from 18.07771
196/196 - 80s - loss: 18.2098 - MinusLogProbMetric: 18.2098 - val_loss: 18.2168 - val_MinusLogProbMetric: 18.2168 - lr: 3.3333e-04 - 80s/epoch - 408ms/step
Epoch 478/1000
2023-09-28 03:45:28.674 
Epoch 478/1000 
	 loss: 18.1032, MinusLogProbMetric: 18.1032, val_loss: 18.3823, val_MinusLogProbMetric: 18.3823

Epoch 478: val_loss did not improve from 18.07771
196/196 - 79s - loss: 18.1032 - MinusLogProbMetric: 18.1032 - val_loss: 18.3823 - val_MinusLogProbMetric: 18.3823 - lr: 3.3333e-04 - 79s/epoch - 406ms/step
Epoch 479/1000
2023-09-28 03:46:47.419 
Epoch 479/1000 
	 loss: 18.1270, MinusLogProbMetric: 18.1270, val_loss: 18.2796, val_MinusLogProbMetric: 18.2796

Epoch 479: val_loss did not improve from 18.07771
196/196 - 79s - loss: 18.1270 - MinusLogProbMetric: 18.1270 - val_loss: 18.2796 - val_MinusLogProbMetric: 18.2796 - lr: 3.3333e-04 - 79s/epoch - 402ms/step
Epoch 480/1000
2023-09-28 03:48:06.921 
Epoch 480/1000 
	 loss: 18.1016, MinusLogProbMetric: 18.1016, val_loss: 18.2141, val_MinusLogProbMetric: 18.2141

Epoch 480: val_loss did not improve from 18.07771
196/196 - 79s - loss: 18.1016 - MinusLogProbMetric: 18.1016 - val_loss: 18.2141 - val_MinusLogProbMetric: 18.2141 - lr: 3.3333e-04 - 79s/epoch - 406ms/step
Epoch 481/1000
2023-09-28 03:49:26.322 
Epoch 481/1000 
	 loss: 18.0562, MinusLogProbMetric: 18.0562, val_loss: 18.1980, val_MinusLogProbMetric: 18.1980

Epoch 481: val_loss did not improve from 18.07771
196/196 - 79s - loss: 18.0562 - MinusLogProbMetric: 18.0562 - val_loss: 18.1980 - val_MinusLogProbMetric: 18.1980 - lr: 3.3333e-04 - 79s/epoch - 405ms/step
Epoch 482/1000
2023-09-28 03:50:45.550 
Epoch 482/1000 
	 loss: 18.1765, MinusLogProbMetric: 18.1765, val_loss: 19.3706, val_MinusLogProbMetric: 19.3706

Epoch 482: val_loss did not improve from 18.07771
196/196 - 79s - loss: 18.1765 - MinusLogProbMetric: 18.1765 - val_loss: 19.3706 - val_MinusLogProbMetric: 19.3706 - lr: 3.3333e-04 - 79s/epoch - 404ms/step
Epoch 483/1000
2023-09-28 03:52:04.169 
Epoch 483/1000 
	 loss: 18.0887, MinusLogProbMetric: 18.0887, val_loss: 18.6520, val_MinusLogProbMetric: 18.6520

Epoch 483: val_loss did not improve from 18.07771
196/196 - 79s - loss: 18.0887 - MinusLogProbMetric: 18.0887 - val_loss: 18.6520 - val_MinusLogProbMetric: 18.6520 - lr: 3.3333e-04 - 79s/epoch - 401ms/step
Epoch 484/1000
2023-09-28 03:53:23.352 
Epoch 484/1000 
	 loss: 18.1079, MinusLogProbMetric: 18.1079, val_loss: 18.2055, val_MinusLogProbMetric: 18.2055

Epoch 484: val_loss did not improve from 18.07771
196/196 - 79s - loss: 18.1079 - MinusLogProbMetric: 18.1079 - val_loss: 18.2055 - val_MinusLogProbMetric: 18.2055 - lr: 3.3333e-04 - 79s/epoch - 404ms/step
Epoch 485/1000
2023-09-28 03:54:42.346 
Epoch 485/1000 
	 loss: 18.0808, MinusLogProbMetric: 18.0808, val_loss: 17.8949, val_MinusLogProbMetric: 17.8949

Epoch 485: val_loss improved from 18.07771 to 17.89487, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 80s - loss: 18.0808 - MinusLogProbMetric: 18.0808 - val_loss: 17.8949 - val_MinusLogProbMetric: 17.8949 - lr: 3.3333e-04 - 80s/epoch - 409ms/step
Epoch 486/1000
2023-09-28 03:56:02.373 
Epoch 486/1000 
	 loss: 18.0276, MinusLogProbMetric: 18.0276, val_loss: 18.0549, val_MinusLogProbMetric: 18.0549

Epoch 486: val_loss did not improve from 17.89487
196/196 - 79s - loss: 18.0276 - MinusLogProbMetric: 18.0276 - val_loss: 18.0549 - val_MinusLogProbMetric: 18.0549 - lr: 3.3333e-04 - 79s/epoch - 403ms/step
Epoch 487/1000
2023-09-28 03:57:21.526 
Epoch 487/1000 
	 loss: 18.0558, MinusLogProbMetric: 18.0558, val_loss: 18.0889, val_MinusLogProbMetric: 18.0889

Epoch 487: val_loss did not improve from 17.89487
196/196 - 79s - loss: 18.0558 - MinusLogProbMetric: 18.0558 - val_loss: 18.0889 - val_MinusLogProbMetric: 18.0889 - lr: 3.3333e-04 - 79s/epoch - 404ms/step
Epoch 488/1000
2023-09-28 03:58:40.823 
Epoch 488/1000 
	 loss: 18.0640, MinusLogProbMetric: 18.0640, val_loss: 18.1609, val_MinusLogProbMetric: 18.1609

Epoch 488: val_loss did not improve from 17.89487
196/196 - 79s - loss: 18.0640 - MinusLogProbMetric: 18.0640 - val_loss: 18.1609 - val_MinusLogProbMetric: 18.1609 - lr: 3.3333e-04 - 79s/epoch - 405ms/step
Epoch 489/1000
2023-09-28 03:59:59.934 
Epoch 489/1000 
	 loss: 18.0197, MinusLogProbMetric: 18.0197, val_loss: 18.5586, val_MinusLogProbMetric: 18.5586

Epoch 489: val_loss did not improve from 17.89487
196/196 - 79s - loss: 18.0197 - MinusLogProbMetric: 18.0197 - val_loss: 18.5586 - val_MinusLogProbMetric: 18.5586 - lr: 3.3333e-04 - 79s/epoch - 404ms/step
Epoch 490/1000
2023-09-28 04:01:18.987 
Epoch 490/1000 
	 loss: 19.2282, MinusLogProbMetric: 19.2282, val_loss: 19.1110, val_MinusLogProbMetric: 19.1110

Epoch 490: val_loss did not improve from 17.89487
196/196 - 79s - loss: 19.2282 - MinusLogProbMetric: 19.2282 - val_loss: 19.1110 - val_MinusLogProbMetric: 19.1110 - lr: 3.3333e-04 - 79s/epoch - 403ms/step
Epoch 491/1000
2023-09-28 04:02:37.522 
Epoch 491/1000 
	 loss: 18.2049, MinusLogProbMetric: 18.2049, val_loss: 18.2249, val_MinusLogProbMetric: 18.2249

Epoch 491: val_loss did not improve from 17.89487
196/196 - 79s - loss: 18.2049 - MinusLogProbMetric: 18.2049 - val_loss: 18.2249 - val_MinusLogProbMetric: 18.2249 - lr: 3.3333e-04 - 79s/epoch - 401ms/step
Epoch 492/1000
2023-09-28 04:03:56.534 
Epoch 492/1000 
	 loss: 18.1545, MinusLogProbMetric: 18.1545, val_loss: 18.0978, val_MinusLogProbMetric: 18.0978

Epoch 492: val_loss did not improve from 17.89487
196/196 - 79s - loss: 18.1545 - MinusLogProbMetric: 18.1545 - val_loss: 18.0978 - val_MinusLogProbMetric: 18.0978 - lr: 3.3333e-04 - 79s/epoch - 403ms/step
Epoch 493/1000
2023-09-28 04:05:15.889 
Epoch 493/1000 
	 loss: 18.1073, MinusLogProbMetric: 18.1073, val_loss: 18.2621, val_MinusLogProbMetric: 18.2621

Epoch 493: val_loss did not improve from 17.89487
196/196 - 79s - loss: 18.1073 - MinusLogProbMetric: 18.1073 - val_loss: 18.2621 - val_MinusLogProbMetric: 18.2621 - lr: 3.3333e-04 - 79s/epoch - 405ms/step
Epoch 494/1000
2023-09-28 04:06:35.110 
Epoch 494/1000 
	 loss: 18.1397, MinusLogProbMetric: 18.1397, val_loss: 18.1342, val_MinusLogProbMetric: 18.1342

Epoch 494: val_loss did not improve from 17.89487
196/196 - 79s - loss: 18.1397 - MinusLogProbMetric: 18.1397 - val_loss: 18.1342 - val_MinusLogProbMetric: 18.1342 - lr: 3.3333e-04 - 79s/epoch - 404ms/step
Epoch 495/1000
2023-09-28 04:07:54.455 
Epoch 495/1000 
	 loss: 18.0750, MinusLogProbMetric: 18.0750, val_loss: 18.2919, val_MinusLogProbMetric: 18.2919

Epoch 495: val_loss did not improve from 17.89487
196/196 - 79s - loss: 18.0750 - MinusLogProbMetric: 18.0750 - val_loss: 18.2919 - val_MinusLogProbMetric: 18.2919 - lr: 3.3333e-04 - 79s/epoch - 405ms/step
Epoch 496/1000
2023-09-28 04:09:13.933 
Epoch 496/1000 
	 loss: 18.1261, MinusLogProbMetric: 18.1261, val_loss: 17.9674, val_MinusLogProbMetric: 17.9674

Epoch 496: val_loss did not improve from 17.89487
196/196 - 79s - loss: 18.1261 - MinusLogProbMetric: 18.1261 - val_loss: 17.9674 - val_MinusLogProbMetric: 17.9674 - lr: 3.3333e-04 - 79s/epoch - 405ms/step
Epoch 497/1000
2023-09-28 04:10:33.423 
Epoch 497/1000 
	 loss: 18.0713, MinusLogProbMetric: 18.0713, val_loss: 18.0353, val_MinusLogProbMetric: 18.0353

Epoch 497: val_loss did not improve from 17.89487
196/196 - 79s - loss: 18.0713 - MinusLogProbMetric: 18.0713 - val_loss: 18.0353 - val_MinusLogProbMetric: 18.0353 - lr: 3.3333e-04 - 79s/epoch - 406ms/step
Epoch 498/1000
2023-09-28 04:11:52.472 
Epoch 498/1000 
	 loss: 18.0105, MinusLogProbMetric: 18.0105, val_loss: 18.1296, val_MinusLogProbMetric: 18.1296

Epoch 498: val_loss did not improve from 17.89487
196/196 - 79s - loss: 18.0105 - MinusLogProbMetric: 18.0105 - val_loss: 18.1296 - val_MinusLogProbMetric: 18.1296 - lr: 3.3333e-04 - 79s/epoch - 403ms/step
Epoch 499/1000
2023-09-28 04:13:12.149 
Epoch 499/1000 
	 loss: 18.0461, MinusLogProbMetric: 18.0461, val_loss: 18.2188, val_MinusLogProbMetric: 18.2188

Epoch 499: val_loss did not improve from 17.89487
196/196 - 80s - loss: 18.0461 - MinusLogProbMetric: 18.0461 - val_loss: 18.2188 - val_MinusLogProbMetric: 18.2188 - lr: 3.3333e-04 - 80s/epoch - 406ms/step
Epoch 500/1000
2023-09-28 04:14:31.622 
Epoch 500/1000 
	 loss: 17.9875, MinusLogProbMetric: 17.9875, val_loss: 18.1505, val_MinusLogProbMetric: 18.1505

Epoch 500: val_loss did not improve from 17.89487
196/196 - 79s - loss: 17.9875 - MinusLogProbMetric: 17.9875 - val_loss: 18.1505 - val_MinusLogProbMetric: 18.1505 - lr: 3.3333e-04 - 79s/epoch - 405ms/step
Epoch 501/1000
2023-09-28 04:15:50.127 
Epoch 501/1000 
	 loss: 18.1011, MinusLogProbMetric: 18.1011, val_loss: 18.1584, val_MinusLogProbMetric: 18.1584

Epoch 501: val_loss did not improve from 17.89487
196/196 - 79s - loss: 18.1011 - MinusLogProbMetric: 18.1011 - val_loss: 18.1584 - val_MinusLogProbMetric: 18.1584 - lr: 3.3333e-04 - 79s/epoch - 401ms/step
Epoch 502/1000
2023-09-28 04:17:09.230 
Epoch 502/1000 
	 loss: 18.0267, MinusLogProbMetric: 18.0267, val_loss: 18.1617, val_MinusLogProbMetric: 18.1617

Epoch 502: val_loss did not improve from 17.89487
196/196 - 79s - loss: 18.0267 - MinusLogProbMetric: 18.0267 - val_loss: 18.1617 - val_MinusLogProbMetric: 18.1617 - lr: 3.3333e-04 - 79s/epoch - 404ms/step
Epoch 503/1000
2023-09-28 04:18:27.956 
Epoch 503/1000 
	 loss: 17.9980, MinusLogProbMetric: 17.9980, val_loss: 18.3216, val_MinusLogProbMetric: 18.3216

Epoch 503: val_loss did not improve from 17.89487
196/196 - 79s - loss: 17.9980 - MinusLogProbMetric: 17.9980 - val_loss: 18.3216 - val_MinusLogProbMetric: 18.3216 - lr: 3.3333e-04 - 79s/epoch - 402ms/step
Epoch 504/1000
2023-09-28 04:19:47.205 
Epoch 504/1000 
	 loss: 18.0834, MinusLogProbMetric: 18.0834, val_loss: 18.3182, val_MinusLogProbMetric: 18.3182

Epoch 504: val_loss did not improve from 17.89487
196/196 - 79s - loss: 18.0834 - MinusLogProbMetric: 18.0834 - val_loss: 18.3182 - val_MinusLogProbMetric: 18.3182 - lr: 3.3333e-04 - 79s/epoch - 404ms/step
Epoch 505/1000
2023-09-28 04:21:06.430 
Epoch 505/1000 
	 loss: 18.1003, MinusLogProbMetric: 18.1003, val_loss: 18.0273, val_MinusLogProbMetric: 18.0273

Epoch 505: val_loss did not improve from 17.89487
196/196 - 79s - loss: 18.1003 - MinusLogProbMetric: 18.1003 - val_loss: 18.0273 - val_MinusLogProbMetric: 18.0273 - lr: 3.3333e-04 - 79s/epoch - 404ms/step
Epoch 506/1000
2023-09-28 04:22:25.346 
Epoch 506/1000 
	 loss: 18.0203, MinusLogProbMetric: 18.0203, val_loss: 18.2811, val_MinusLogProbMetric: 18.2811

Epoch 506: val_loss did not improve from 17.89487
196/196 - 79s - loss: 18.0203 - MinusLogProbMetric: 18.0203 - val_loss: 18.2811 - val_MinusLogProbMetric: 18.2811 - lr: 3.3333e-04 - 79s/epoch - 403ms/step
Epoch 507/1000
2023-09-28 04:23:44.967 
Epoch 507/1000 
	 loss: 17.9824, MinusLogProbMetric: 17.9824, val_loss: 18.2731, val_MinusLogProbMetric: 18.2731

Epoch 507: val_loss did not improve from 17.89487
196/196 - 80s - loss: 17.9824 - MinusLogProbMetric: 17.9824 - val_loss: 18.2731 - val_MinusLogProbMetric: 18.2731 - lr: 3.3333e-04 - 80s/epoch - 406ms/step
Epoch 508/1000
2023-09-28 04:25:04.437 
Epoch 508/1000 
	 loss: 18.0621, MinusLogProbMetric: 18.0621, val_loss: 18.0342, val_MinusLogProbMetric: 18.0342

Epoch 508: val_loss did not improve from 17.89487
196/196 - 79s - loss: 18.0621 - MinusLogProbMetric: 18.0621 - val_loss: 18.0342 - val_MinusLogProbMetric: 18.0342 - lr: 3.3333e-04 - 79s/epoch - 405ms/step
Epoch 509/1000
2023-09-28 04:26:23.034 
Epoch 509/1000 
	 loss: 17.9913, MinusLogProbMetric: 17.9913, val_loss: 18.3221, val_MinusLogProbMetric: 18.3221

Epoch 509: val_loss did not improve from 17.89487
196/196 - 79s - loss: 17.9913 - MinusLogProbMetric: 17.9913 - val_loss: 18.3221 - val_MinusLogProbMetric: 18.3221 - lr: 3.3333e-04 - 79s/epoch - 401ms/step
Epoch 510/1000
2023-09-28 04:27:42.329 
Epoch 510/1000 
	 loss: 18.0413, MinusLogProbMetric: 18.0413, val_loss: 18.0980, val_MinusLogProbMetric: 18.0980

Epoch 510: val_loss did not improve from 17.89487
196/196 - 79s - loss: 18.0413 - MinusLogProbMetric: 18.0413 - val_loss: 18.0980 - val_MinusLogProbMetric: 18.0980 - lr: 3.3333e-04 - 79s/epoch - 405ms/step
Epoch 511/1000
2023-09-28 04:29:01.108 
Epoch 511/1000 
	 loss: 18.0125, MinusLogProbMetric: 18.0125, val_loss: 18.1613, val_MinusLogProbMetric: 18.1613

Epoch 511: val_loss did not improve from 17.89487
196/196 - 79s - loss: 18.0125 - MinusLogProbMetric: 18.0125 - val_loss: 18.1613 - val_MinusLogProbMetric: 18.1613 - lr: 3.3333e-04 - 79s/epoch - 402ms/step
Epoch 512/1000
2023-09-28 04:30:20.416 
Epoch 512/1000 
	 loss: 18.0435, MinusLogProbMetric: 18.0435, val_loss: 18.2516, val_MinusLogProbMetric: 18.2516

Epoch 512: val_loss did not improve from 17.89487
196/196 - 79s - loss: 18.0435 - MinusLogProbMetric: 18.0435 - val_loss: 18.2516 - val_MinusLogProbMetric: 18.2516 - lr: 3.3333e-04 - 79s/epoch - 405ms/step
Epoch 513/1000
2023-09-28 04:31:39.539 
Epoch 513/1000 
	 loss: 17.9446, MinusLogProbMetric: 17.9446, val_loss: 18.3317, val_MinusLogProbMetric: 18.3317

Epoch 513: val_loss did not improve from 17.89487
196/196 - 79s - loss: 17.9446 - MinusLogProbMetric: 17.9446 - val_loss: 18.3317 - val_MinusLogProbMetric: 18.3317 - lr: 3.3333e-04 - 79s/epoch - 404ms/step
Epoch 514/1000
2023-09-28 04:32:58.570 
Epoch 514/1000 
	 loss: 18.0774, MinusLogProbMetric: 18.0774, val_loss: 17.9618, val_MinusLogProbMetric: 17.9618

Epoch 514: val_loss did not improve from 17.89487
196/196 - 79s - loss: 18.0774 - MinusLogProbMetric: 18.0774 - val_loss: 17.9618 - val_MinusLogProbMetric: 17.9618 - lr: 3.3333e-04 - 79s/epoch - 403ms/step
Epoch 515/1000
2023-09-28 04:34:18.061 
Epoch 515/1000 
	 loss: 18.0318, MinusLogProbMetric: 18.0318, val_loss: 18.2623, val_MinusLogProbMetric: 18.2623

Epoch 515: val_loss did not improve from 17.89487
196/196 - 79s - loss: 18.0318 - MinusLogProbMetric: 18.0318 - val_loss: 18.2623 - val_MinusLogProbMetric: 18.2623 - lr: 3.3333e-04 - 79s/epoch - 406ms/step
Epoch 516/1000
2023-09-28 04:35:36.934 
Epoch 516/1000 
	 loss: 18.0079, MinusLogProbMetric: 18.0079, val_loss: 18.1448, val_MinusLogProbMetric: 18.1448

Epoch 516: val_loss did not improve from 17.89487
196/196 - 79s - loss: 18.0079 - MinusLogProbMetric: 18.0079 - val_loss: 18.1448 - val_MinusLogProbMetric: 18.1448 - lr: 3.3333e-04 - 79s/epoch - 402ms/step
Epoch 517/1000
2023-09-28 04:36:56.408 
Epoch 517/1000 
	 loss: 18.7008, MinusLogProbMetric: 18.7008, val_loss: 18.1299, val_MinusLogProbMetric: 18.1299

Epoch 517: val_loss did not improve from 17.89487
196/196 - 79s - loss: 18.7008 - MinusLogProbMetric: 18.7008 - val_loss: 18.1299 - val_MinusLogProbMetric: 18.1299 - lr: 3.3333e-04 - 79s/epoch - 405ms/step
Epoch 518/1000
2023-09-28 04:38:15.842 
Epoch 518/1000 
	 loss: 18.0582, MinusLogProbMetric: 18.0582, val_loss: 18.4220, val_MinusLogProbMetric: 18.4220

Epoch 518: val_loss did not improve from 17.89487
196/196 - 79s - loss: 18.0582 - MinusLogProbMetric: 18.0582 - val_loss: 18.4220 - val_MinusLogProbMetric: 18.4220 - lr: 3.3333e-04 - 79s/epoch - 405ms/step
Epoch 519/1000
2023-09-28 04:39:34.994 
Epoch 519/1000 
	 loss: 18.0016, MinusLogProbMetric: 18.0016, val_loss: 18.3151, val_MinusLogProbMetric: 18.3151

Epoch 519: val_loss did not improve from 17.89487
196/196 - 79s - loss: 18.0016 - MinusLogProbMetric: 18.0016 - val_loss: 18.3151 - val_MinusLogProbMetric: 18.3151 - lr: 3.3333e-04 - 79s/epoch - 404ms/step
Epoch 520/1000
2023-09-28 04:40:54.137 
Epoch 520/1000 
	 loss: 18.0362, MinusLogProbMetric: 18.0362, val_loss: 18.1577, val_MinusLogProbMetric: 18.1577

Epoch 520: val_loss did not improve from 17.89487
196/196 - 79s - loss: 18.0362 - MinusLogProbMetric: 18.0362 - val_loss: 18.1577 - val_MinusLogProbMetric: 18.1577 - lr: 3.3333e-04 - 79s/epoch - 404ms/step
Epoch 521/1000
2023-09-28 04:42:13.227 
Epoch 521/1000 
	 loss: 17.9837, MinusLogProbMetric: 17.9837, val_loss: 18.3325, val_MinusLogProbMetric: 18.3325

Epoch 521: val_loss did not improve from 17.89487
196/196 - 79s - loss: 17.9837 - MinusLogProbMetric: 17.9837 - val_loss: 18.3325 - val_MinusLogProbMetric: 18.3325 - lr: 3.3333e-04 - 79s/epoch - 404ms/step
Epoch 522/1000
2023-09-28 04:43:32.515 
Epoch 522/1000 
	 loss: 17.9651, MinusLogProbMetric: 17.9651, val_loss: 18.4991, val_MinusLogProbMetric: 18.4991

Epoch 522: val_loss did not improve from 17.89487
196/196 - 79s - loss: 17.9651 - MinusLogProbMetric: 17.9651 - val_loss: 18.4991 - val_MinusLogProbMetric: 18.4991 - lr: 3.3333e-04 - 79s/epoch - 405ms/step
Epoch 523/1000
2023-09-28 04:44:51.487 
Epoch 523/1000 
	 loss: 18.0276, MinusLogProbMetric: 18.0276, val_loss: 18.2963, val_MinusLogProbMetric: 18.2963

Epoch 523: val_loss did not improve from 17.89487
196/196 - 79s - loss: 18.0276 - MinusLogProbMetric: 18.0276 - val_loss: 18.2963 - val_MinusLogProbMetric: 18.2963 - lr: 3.3333e-04 - 79s/epoch - 403ms/step
Epoch 524/1000
2023-09-28 04:46:10.900 
Epoch 524/1000 
	 loss: 18.0020, MinusLogProbMetric: 18.0020, val_loss: 18.6337, val_MinusLogProbMetric: 18.6337

Epoch 524: val_loss did not improve from 17.89487
196/196 - 79s - loss: 18.0020 - MinusLogProbMetric: 18.0020 - val_loss: 18.6337 - val_MinusLogProbMetric: 18.6337 - lr: 3.3333e-04 - 79s/epoch - 405ms/step
Epoch 525/1000
2023-09-28 04:47:30.055 
Epoch 525/1000 
	 loss: 17.9433, MinusLogProbMetric: 17.9433, val_loss: 17.9581, val_MinusLogProbMetric: 17.9581

Epoch 525: val_loss did not improve from 17.89487
196/196 - 79s - loss: 17.9433 - MinusLogProbMetric: 17.9433 - val_loss: 17.9581 - val_MinusLogProbMetric: 17.9581 - lr: 3.3333e-04 - 79s/epoch - 404ms/step
Epoch 526/1000
2023-09-28 04:48:49.193 
Epoch 526/1000 
	 loss: 18.0268, MinusLogProbMetric: 18.0268, val_loss: 18.2980, val_MinusLogProbMetric: 18.2980

Epoch 526: val_loss did not improve from 17.89487
196/196 - 79s - loss: 18.0268 - MinusLogProbMetric: 18.0268 - val_loss: 18.2980 - val_MinusLogProbMetric: 18.2980 - lr: 3.3333e-04 - 79s/epoch - 404ms/step
Epoch 527/1000
2023-09-28 04:50:08.087 
Epoch 527/1000 
	 loss: 18.0774, MinusLogProbMetric: 18.0774, val_loss: 18.0999, val_MinusLogProbMetric: 18.0999

Epoch 527: val_loss did not improve from 17.89487
196/196 - 79s - loss: 18.0774 - MinusLogProbMetric: 18.0774 - val_loss: 18.0999 - val_MinusLogProbMetric: 18.0999 - lr: 3.3333e-04 - 79s/epoch - 403ms/step
Epoch 528/1000
2023-09-28 04:51:27.340 
Epoch 528/1000 
	 loss: 17.9761, MinusLogProbMetric: 17.9761, val_loss: 18.1757, val_MinusLogProbMetric: 18.1757

Epoch 528: val_loss did not improve from 17.89487
196/196 - 79s - loss: 17.9761 - MinusLogProbMetric: 17.9761 - val_loss: 18.1757 - val_MinusLogProbMetric: 18.1757 - lr: 3.3333e-04 - 79s/epoch - 404ms/step
Epoch 529/1000
2023-09-28 04:52:46.672 
Epoch 529/1000 
	 loss: 17.9573, MinusLogProbMetric: 17.9573, val_loss: 17.9341, val_MinusLogProbMetric: 17.9341

Epoch 529: val_loss did not improve from 17.89487
196/196 - 79s - loss: 17.9573 - MinusLogProbMetric: 17.9573 - val_loss: 17.9341 - val_MinusLogProbMetric: 17.9341 - lr: 3.3333e-04 - 79s/epoch - 405ms/step
Epoch 530/1000
2023-09-28 04:54:06.343 
Epoch 530/1000 
	 loss: 17.9621, MinusLogProbMetric: 17.9621, val_loss: 17.9130, val_MinusLogProbMetric: 17.9130

Epoch 530: val_loss did not improve from 17.89487
196/196 - 80s - loss: 17.9621 - MinusLogProbMetric: 17.9621 - val_loss: 17.9130 - val_MinusLogProbMetric: 17.9130 - lr: 3.3333e-04 - 80s/epoch - 406ms/step
Epoch 531/1000
2023-09-28 04:55:25.250 
Epoch 531/1000 
	 loss: 17.9895, MinusLogProbMetric: 17.9895, val_loss: 18.0539, val_MinusLogProbMetric: 18.0539

Epoch 531: val_loss did not improve from 17.89487
196/196 - 79s - loss: 17.9895 - MinusLogProbMetric: 17.9895 - val_loss: 18.0539 - val_MinusLogProbMetric: 18.0539 - lr: 3.3333e-04 - 79s/epoch - 403ms/step
Epoch 532/1000
2023-09-28 04:56:44.662 
Epoch 532/1000 
	 loss: 17.9436, MinusLogProbMetric: 17.9436, val_loss: 18.6150, val_MinusLogProbMetric: 18.6150

Epoch 532: val_loss did not improve from 17.89487
196/196 - 79s - loss: 17.9436 - MinusLogProbMetric: 17.9436 - val_loss: 18.6150 - val_MinusLogProbMetric: 18.6150 - lr: 3.3333e-04 - 79s/epoch - 405ms/step
Epoch 533/1000
2023-09-28 04:58:03.628 
Epoch 533/1000 
	 loss: 17.9283, MinusLogProbMetric: 17.9283, val_loss: 18.0463, val_MinusLogProbMetric: 18.0463

Epoch 533: val_loss did not improve from 17.89487
196/196 - 79s - loss: 17.9283 - MinusLogProbMetric: 17.9283 - val_loss: 18.0463 - val_MinusLogProbMetric: 18.0463 - lr: 3.3333e-04 - 79s/epoch - 403ms/step
Epoch 534/1000
2023-09-28 04:59:22.506 
Epoch 534/1000 
	 loss: 17.9491, MinusLogProbMetric: 17.9491, val_loss: 18.2604, val_MinusLogProbMetric: 18.2604

Epoch 534: val_loss did not improve from 17.89487
196/196 - 79s - loss: 17.9491 - MinusLogProbMetric: 17.9491 - val_loss: 18.2604 - val_MinusLogProbMetric: 18.2604 - lr: 3.3333e-04 - 79s/epoch - 402ms/step
Epoch 535/1000
2023-09-28 05:00:41.665 
Epoch 535/1000 
	 loss: 17.9590, MinusLogProbMetric: 17.9590, val_loss: 18.2852, val_MinusLogProbMetric: 18.2852

Epoch 535: val_loss did not improve from 17.89487
196/196 - 79s - loss: 17.9590 - MinusLogProbMetric: 17.9590 - val_loss: 18.2852 - val_MinusLogProbMetric: 18.2852 - lr: 3.3333e-04 - 79s/epoch - 404ms/step
Epoch 536/1000
2023-09-28 05:02:01.103 
Epoch 536/1000 
	 loss: 17.6410, MinusLogProbMetric: 17.6410, val_loss: 17.6796, val_MinusLogProbMetric: 17.6796

Epoch 536: val_loss improved from 17.89487 to 17.67958, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 81s - loss: 17.6410 - MinusLogProbMetric: 17.6410 - val_loss: 17.6796 - val_MinusLogProbMetric: 17.6796 - lr: 1.6667e-04 - 81s/epoch - 412ms/step
Epoch 537/1000
2023-09-28 05:03:21.159 
Epoch 537/1000 
	 loss: 17.6227, MinusLogProbMetric: 17.6227, val_loss: 17.7773, val_MinusLogProbMetric: 17.7773

Epoch 537: val_loss did not improve from 17.67958
196/196 - 79s - loss: 17.6227 - MinusLogProbMetric: 17.6227 - val_loss: 17.7773 - val_MinusLogProbMetric: 17.7773 - lr: 1.6667e-04 - 79s/epoch - 402ms/step
Epoch 538/1000
2023-09-28 05:04:40.081 
Epoch 538/1000 
	 loss: 17.6349, MinusLogProbMetric: 17.6349, val_loss: 17.7987, val_MinusLogProbMetric: 17.7987

Epoch 538: val_loss did not improve from 17.67958
196/196 - 79s - loss: 17.6349 - MinusLogProbMetric: 17.6349 - val_loss: 17.7987 - val_MinusLogProbMetric: 17.7987 - lr: 1.6667e-04 - 79s/epoch - 403ms/step
Epoch 539/1000
2023-09-28 05:05:59.814 
Epoch 539/1000 
	 loss: 17.6696, MinusLogProbMetric: 17.6696, val_loss: 17.7596, val_MinusLogProbMetric: 17.7596

Epoch 539: val_loss did not improve from 17.67958
196/196 - 80s - loss: 17.6696 - MinusLogProbMetric: 17.6696 - val_loss: 17.7596 - val_MinusLogProbMetric: 17.7596 - lr: 1.6667e-04 - 80s/epoch - 407ms/step
Epoch 540/1000
2023-09-28 05:07:18.745 
Epoch 540/1000 
	 loss: 17.6453, MinusLogProbMetric: 17.6453, val_loss: 17.7278, val_MinusLogProbMetric: 17.7278

Epoch 540: val_loss did not improve from 17.67958
196/196 - 79s - loss: 17.6453 - MinusLogProbMetric: 17.6453 - val_loss: 17.7278 - val_MinusLogProbMetric: 17.7278 - lr: 1.6667e-04 - 79s/epoch - 403ms/step
Epoch 541/1000
2023-09-28 05:08:37.600 
Epoch 541/1000 
	 loss: 17.6395, MinusLogProbMetric: 17.6395, val_loss: 17.8343, val_MinusLogProbMetric: 17.8343

Epoch 541: val_loss did not improve from 17.67958
196/196 - 79s - loss: 17.6395 - MinusLogProbMetric: 17.6395 - val_loss: 17.8343 - val_MinusLogProbMetric: 17.8343 - lr: 1.6667e-04 - 79s/epoch - 402ms/step
Epoch 542/1000
2023-09-28 05:09:56.990 
Epoch 542/1000 
	 loss: 17.6282, MinusLogProbMetric: 17.6282, val_loss: 17.8252, val_MinusLogProbMetric: 17.8252

Epoch 542: val_loss did not improve from 17.67958
196/196 - 79s - loss: 17.6282 - MinusLogProbMetric: 17.6282 - val_loss: 17.8252 - val_MinusLogProbMetric: 17.8252 - lr: 1.6667e-04 - 79s/epoch - 405ms/step
Epoch 543/1000
2023-09-28 05:11:16.943 
Epoch 543/1000 
	 loss: 17.6604, MinusLogProbMetric: 17.6604, val_loss: 17.8088, val_MinusLogProbMetric: 17.8088

Epoch 543: val_loss did not improve from 17.67958
196/196 - 80s - loss: 17.6604 - MinusLogProbMetric: 17.6604 - val_loss: 17.8088 - val_MinusLogProbMetric: 17.8088 - lr: 1.6667e-04 - 80s/epoch - 408ms/step
Epoch 544/1000
2023-09-28 05:12:36.189 
Epoch 544/1000 
	 loss: 17.6436, MinusLogProbMetric: 17.6436, val_loss: 17.8832, val_MinusLogProbMetric: 17.8832

Epoch 544: val_loss did not improve from 17.67958
196/196 - 79s - loss: 17.6436 - MinusLogProbMetric: 17.6436 - val_loss: 17.8832 - val_MinusLogProbMetric: 17.8832 - lr: 1.6667e-04 - 79s/epoch - 404ms/step
Epoch 545/1000
2023-09-28 05:13:56.050 
Epoch 545/1000 
	 loss: 17.6214, MinusLogProbMetric: 17.6214, val_loss: 17.8427, val_MinusLogProbMetric: 17.8427

Epoch 545: val_loss did not improve from 17.67958
196/196 - 80s - loss: 17.6214 - MinusLogProbMetric: 17.6214 - val_loss: 17.8427 - val_MinusLogProbMetric: 17.8427 - lr: 1.6667e-04 - 80s/epoch - 407ms/step
Epoch 546/1000
2023-09-28 05:15:15.271 
Epoch 546/1000 
	 loss: 17.6168, MinusLogProbMetric: 17.6168, val_loss: 17.7159, val_MinusLogProbMetric: 17.7159

Epoch 546: val_loss did not improve from 17.67958
196/196 - 79s - loss: 17.6168 - MinusLogProbMetric: 17.6168 - val_loss: 17.7159 - val_MinusLogProbMetric: 17.7159 - lr: 1.6667e-04 - 79s/epoch - 404ms/step
Epoch 547/1000
2023-09-28 05:16:35.192 
Epoch 547/1000 
	 loss: 17.6493, MinusLogProbMetric: 17.6493, val_loss: 17.7337, val_MinusLogProbMetric: 17.7337

Epoch 547: val_loss did not improve from 17.67958
196/196 - 80s - loss: 17.6493 - MinusLogProbMetric: 17.6493 - val_loss: 17.7337 - val_MinusLogProbMetric: 17.7337 - lr: 1.6667e-04 - 80s/epoch - 408ms/step
Epoch 548/1000
2023-09-28 05:17:53.980 
Epoch 548/1000 
	 loss: 17.6247, MinusLogProbMetric: 17.6247, val_loss: 17.8123, val_MinusLogProbMetric: 17.8123

Epoch 548: val_loss did not improve from 17.67958
196/196 - 79s - loss: 17.6247 - MinusLogProbMetric: 17.6247 - val_loss: 17.8123 - val_MinusLogProbMetric: 17.8123 - lr: 1.6667e-04 - 79s/epoch - 402ms/step
Epoch 549/1000
2023-09-28 05:19:12.860 
Epoch 549/1000 
	 loss: 17.6397, MinusLogProbMetric: 17.6397, val_loss: 17.8269, val_MinusLogProbMetric: 17.8269

Epoch 549: val_loss did not improve from 17.67958
196/196 - 79s - loss: 17.6397 - MinusLogProbMetric: 17.6397 - val_loss: 17.8269 - val_MinusLogProbMetric: 17.8269 - lr: 1.6667e-04 - 79s/epoch - 402ms/step
Epoch 550/1000
2023-09-28 05:20:31.936 
Epoch 550/1000 
	 loss: 17.6265, MinusLogProbMetric: 17.6265, val_loss: 17.7788, val_MinusLogProbMetric: 17.7788

Epoch 550: val_loss did not improve from 17.67958
196/196 - 79s - loss: 17.6265 - MinusLogProbMetric: 17.6265 - val_loss: 17.7788 - val_MinusLogProbMetric: 17.7788 - lr: 1.6667e-04 - 79s/epoch - 403ms/step
Epoch 551/1000
2023-09-28 05:21:51.098 
Epoch 551/1000 
	 loss: 17.6334, MinusLogProbMetric: 17.6334, val_loss: 17.6527, val_MinusLogProbMetric: 17.6527

Epoch 551: val_loss improved from 17.67958 to 17.65270, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 81s - loss: 17.6334 - MinusLogProbMetric: 17.6334 - val_loss: 17.6527 - val_MinusLogProbMetric: 17.6527 - lr: 1.6667e-04 - 81s/epoch - 411ms/step
Epoch 552/1000
2023-09-28 05:23:11.499 
Epoch 552/1000 
	 loss: 17.6188, MinusLogProbMetric: 17.6188, val_loss: 17.7997, val_MinusLogProbMetric: 17.7997

Epoch 552: val_loss did not improve from 17.65270
196/196 - 79s - loss: 17.6188 - MinusLogProbMetric: 17.6188 - val_loss: 17.7997 - val_MinusLogProbMetric: 17.7997 - lr: 1.6667e-04 - 79s/epoch - 403ms/step
Epoch 553/1000
2023-09-28 05:24:30.618 
Epoch 553/1000 
	 loss: 17.6496, MinusLogProbMetric: 17.6496, val_loss: 17.6950, val_MinusLogProbMetric: 17.6950

Epoch 553: val_loss did not improve from 17.65270
196/196 - 79s - loss: 17.6496 - MinusLogProbMetric: 17.6496 - val_loss: 17.6950 - val_MinusLogProbMetric: 17.6950 - lr: 1.6667e-04 - 79s/epoch - 404ms/step
Epoch 554/1000
2023-09-28 05:25:50.099 
Epoch 554/1000 
	 loss: 17.6349, MinusLogProbMetric: 17.6349, val_loss: 17.8093, val_MinusLogProbMetric: 17.8093

Epoch 554: val_loss did not improve from 17.65270
196/196 - 79s - loss: 17.6349 - MinusLogProbMetric: 17.6349 - val_loss: 17.8093 - val_MinusLogProbMetric: 17.8093 - lr: 1.6667e-04 - 79s/epoch - 405ms/step
Epoch 555/1000
2023-09-28 05:27:09.626 
Epoch 555/1000 
	 loss: 17.6501, MinusLogProbMetric: 17.6501, val_loss: 17.7943, val_MinusLogProbMetric: 17.7943

Epoch 555: val_loss did not improve from 17.65270
196/196 - 80s - loss: 17.6501 - MinusLogProbMetric: 17.6501 - val_loss: 17.7943 - val_MinusLogProbMetric: 17.7943 - lr: 1.6667e-04 - 80s/epoch - 406ms/step
Epoch 556/1000
2023-09-28 05:28:28.998 
Epoch 556/1000 
	 loss: 17.6088, MinusLogProbMetric: 17.6088, val_loss: 17.7106, val_MinusLogProbMetric: 17.7106

Epoch 556: val_loss did not improve from 17.65270
196/196 - 79s - loss: 17.6088 - MinusLogProbMetric: 17.6088 - val_loss: 17.7106 - val_MinusLogProbMetric: 17.7106 - lr: 1.6667e-04 - 79s/epoch - 405ms/step
Epoch 557/1000
2023-09-28 05:29:48.013 
Epoch 557/1000 
	 loss: 17.6220, MinusLogProbMetric: 17.6220, val_loss: 17.6978, val_MinusLogProbMetric: 17.6978

Epoch 557: val_loss did not improve from 17.65270
196/196 - 79s - loss: 17.6220 - MinusLogProbMetric: 17.6220 - val_loss: 17.6978 - val_MinusLogProbMetric: 17.6978 - lr: 1.6667e-04 - 79s/epoch - 403ms/step
Epoch 558/1000
2023-09-28 05:31:07.500 
Epoch 558/1000 
	 loss: 17.5981, MinusLogProbMetric: 17.5981, val_loss: 17.7235, val_MinusLogProbMetric: 17.7235

Epoch 558: val_loss did not improve from 17.65270
196/196 - 79s - loss: 17.5981 - MinusLogProbMetric: 17.5981 - val_loss: 17.7235 - val_MinusLogProbMetric: 17.7235 - lr: 1.6667e-04 - 79s/epoch - 406ms/step
Epoch 559/1000
2023-09-28 05:32:26.837 
Epoch 559/1000 
	 loss: 17.6216, MinusLogProbMetric: 17.6216, val_loss: 17.7829, val_MinusLogProbMetric: 17.7829

Epoch 559: val_loss did not improve from 17.65270
196/196 - 79s - loss: 17.6216 - MinusLogProbMetric: 17.6216 - val_loss: 17.7829 - val_MinusLogProbMetric: 17.7829 - lr: 1.6667e-04 - 79s/epoch - 405ms/step
Epoch 560/1000
2023-09-28 05:33:46.337 
Epoch 560/1000 
	 loss: 17.6407, MinusLogProbMetric: 17.6407, val_loss: 17.8403, val_MinusLogProbMetric: 17.8403

Epoch 560: val_loss did not improve from 17.65270
196/196 - 79s - loss: 17.6407 - MinusLogProbMetric: 17.6407 - val_loss: 17.8403 - val_MinusLogProbMetric: 17.8403 - lr: 1.6667e-04 - 79s/epoch - 406ms/step
Epoch 561/1000
2023-09-28 05:35:05.767 
Epoch 561/1000 
	 loss: 17.6283, MinusLogProbMetric: 17.6283, val_loss: 17.8349, val_MinusLogProbMetric: 17.8349

Epoch 561: val_loss did not improve from 17.65270
196/196 - 79s - loss: 17.6283 - MinusLogProbMetric: 17.6283 - val_loss: 17.8349 - val_MinusLogProbMetric: 17.8349 - lr: 1.6667e-04 - 79s/epoch - 405ms/step
Epoch 562/1000
2023-09-28 05:36:25.111 
Epoch 562/1000 
	 loss: 17.6162, MinusLogProbMetric: 17.6162, val_loss: 17.8604, val_MinusLogProbMetric: 17.8604

Epoch 562: val_loss did not improve from 17.65270
196/196 - 79s - loss: 17.6162 - MinusLogProbMetric: 17.6162 - val_loss: 17.8604 - val_MinusLogProbMetric: 17.8604 - lr: 1.6667e-04 - 79s/epoch - 405ms/step
Epoch 563/1000
2023-09-28 05:37:43.782 
Epoch 563/1000 
	 loss: 17.6105, MinusLogProbMetric: 17.6105, val_loss: 17.7176, val_MinusLogProbMetric: 17.7176

Epoch 563: val_loss did not improve from 17.65270
196/196 - 79s - loss: 17.6105 - MinusLogProbMetric: 17.6105 - val_loss: 17.7176 - val_MinusLogProbMetric: 17.7176 - lr: 1.6667e-04 - 79s/epoch - 401ms/step
Epoch 564/1000
2023-09-28 05:39:03.117 
Epoch 564/1000 
	 loss: 17.6149, MinusLogProbMetric: 17.6149, val_loss: 17.8514, val_MinusLogProbMetric: 17.8514

Epoch 564: val_loss did not improve from 17.65270
196/196 - 79s - loss: 17.6149 - MinusLogProbMetric: 17.6149 - val_loss: 17.8514 - val_MinusLogProbMetric: 17.8514 - lr: 1.6667e-04 - 79s/epoch - 405ms/step
Epoch 565/1000
2023-09-28 05:40:21.604 
Epoch 565/1000 
	 loss: 17.6065, MinusLogProbMetric: 17.6065, val_loss: 17.9388, val_MinusLogProbMetric: 17.9388

Epoch 565: val_loss did not improve from 17.65270
196/196 - 78s - loss: 17.6065 - MinusLogProbMetric: 17.6065 - val_loss: 17.9388 - val_MinusLogProbMetric: 17.9388 - lr: 1.6667e-04 - 78s/epoch - 400ms/step
Epoch 566/1000
2023-09-28 05:41:40.596 
Epoch 566/1000 
	 loss: 17.6223, MinusLogProbMetric: 17.6223, val_loss: 17.7729, val_MinusLogProbMetric: 17.7729

Epoch 566: val_loss did not improve from 17.65270
196/196 - 79s - loss: 17.6223 - MinusLogProbMetric: 17.6223 - val_loss: 17.7729 - val_MinusLogProbMetric: 17.7729 - lr: 1.6667e-04 - 79s/epoch - 403ms/step
Epoch 567/1000
2023-09-28 05:43:00.376 
Epoch 567/1000 
	 loss: 17.6144, MinusLogProbMetric: 17.6144, val_loss: 17.7642, val_MinusLogProbMetric: 17.7642

Epoch 567: val_loss did not improve from 17.65270
196/196 - 80s - loss: 17.6144 - MinusLogProbMetric: 17.6144 - val_loss: 17.7642 - val_MinusLogProbMetric: 17.7642 - lr: 1.6667e-04 - 80s/epoch - 407ms/step
Epoch 568/1000
2023-09-28 05:44:19.564 
Epoch 568/1000 
	 loss: 17.8121, MinusLogProbMetric: 17.8121, val_loss: 18.3364, val_MinusLogProbMetric: 18.3364

Epoch 568: val_loss did not improve from 17.65270
196/196 - 79s - loss: 17.8121 - MinusLogProbMetric: 17.8121 - val_loss: 18.3364 - val_MinusLogProbMetric: 18.3364 - lr: 1.6667e-04 - 79s/epoch - 404ms/step
Epoch 569/1000
2023-09-28 05:45:39.223 
Epoch 569/1000 
	 loss: 17.6613, MinusLogProbMetric: 17.6613, val_loss: 17.9082, val_MinusLogProbMetric: 17.9082

Epoch 569: val_loss did not improve from 17.65270
196/196 - 80s - loss: 17.6613 - MinusLogProbMetric: 17.6613 - val_loss: 17.9082 - val_MinusLogProbMetric: 17.9082 - lr: 1.6667e-04 - 80s/epoch - 406ms/step
Epoch 570/1000
2023-09-28 05:46:58.705 
Epoch 570/1000 
	 loss: 17.6287, MinusLogProbMetric: 17.6287, val_loss: 17.7713, val_MinusLogProbMetric: 17.7713

Epoch 570: val_loss did not improve from 17.65270
196/196 - 79s - loss: 17.6287 - MinusLogProbMetric: 17.6287 - val_loss: 17.7713 - val_MinusLogProbMetric: 17.7713 - lr: 1.6667e-04 - 79s/epoch - 405ms/step
Epoch 571/1000
2023-09-28 05:48:17.744 
Epoch 571/1000 
	 loss: 17.6196, MinusLogProbMetric: 17.6196, val_loss: 17.6725, val_MinusLogProbMetric: 17.6725

Epoch 571: val_loss did not improve from 17.65270
196/196 - 79s - loss: 17.6196 - MinusLogProbMetric: 17.6196 - val_loss: 17.6725 - val_MinusLogProbMetric: 17.6725 - lr: 1.6667e-04 - 79s/epoch - 403ms/step
Epoch 572/1000
2023-09-28 05:49:36.520 
Epoch 572/1000 
	 loss: 17.5903, MinusLogProbMetric: 17.5903, val_loss: 17.9267, val_MinusLogProbMetric: 17.9267

Epoch 572: val_loss did not improve from 17.65270
196/196 - 79s - loss: 17.5903 - MinusLogProbMetric: 17.5903 - val_loss: 17.9267 - val_MinusLogProbMetric: 17.9267 - lr: 1.6667e-04 - 79s/epoch - 402ms/step
Epoch 573/1000
2023-09-28 05:50:55.196 
Epoch 573/1000 
	 loss: 17.6184, MinusLogProbMetric: 17.6184, val_loss: 17.7195, val_MinusLogProbMetric: 17.7195

Epoch 573: val_loss did not improve from 17.65270
196/196 - 79s - loss: 17.6184 - MinusLogProbMetric: 17.6184 - val_loss: 17.7195 - val_MinusLogProbMetric: 17.7195 - lr: 1.6667e-04 - 79s/epoch - 401ms/step
Epoch 574/1000
2023-09-28 05:52:14.164 
Epoch 574/1000 
	 loss: 17.6217, MinusLogProbMetric: 17.6217, val_loss: 17.6995, val_MinusLogProbMetric: 17.6995

Epoch 574: val_loss did not improve from 17.65270
196/196 - 79s - loss: 17.6217 - MinusLogProbMetric: 17.6217 - val_loss: 17.6995 - val_MinusLogProbMetric: 17.6995 - lr: 1.6667e-04 - 79s/epoch - 403ms/step
Epoch 575/1000
2023-09-28 05:53:33.305 
Epoch 575/1000 
	 loss: 17.5919, MinusLogProbMetric: 17.5919, val_loss: 17.7247, val_MinusLogProbMetric: 17.7247

Epoch 575: val_loss did not improve from 17.65270
196/196 - 79s - loss: 17.5919 - MinusLogProbMetric: 17.5919 - val_loss: 17.7247 - val_MinusLogProbMetric: 17.7247 - lr: 1.6667e-04 - 79s/epoch - 404ms/step
Epoch 576/1000
2023-09-28 05:54:52.905 
Epoch 576/1000 
	 loss: 17.6127, MinusLogProbMetric: 17.6127, val_loss: 17.8445, val_MinusLogProbMetric: 17.8445

Epoch 576: val_loss did not improve from 17.65270
196/196 - 80s - loss: 17.6127 - MinusLogProbMetric: 17.6127 - val_loss: 17.8445 - val_MinusLogProbMetric: 17.8445 - lr: 1.6667e-04 - 80s/epoch - 406ms/step
Epoch 577/1000
2023-09-28 05:56:12.117 
Epoch 577/1000 
	 loss: 17.6125, MinusLogProbMetric: 17.6125, val_loss: 17.8284, val_MinusLogProbMetric: 17.8284

Epoch 577: val_loss did not improve from 17.65270
196/196 - 79s - loss: 17.6125 - MinusLogProbMetric: 17.6125 - val_loss: 17.8284 - val_MinusLogProbMetric: 17.8284 - lr: 1.6667e-04 - 79s/epoch - 404ms/step
Epoch 578/1000
2023-09-28 05:57:31.677 
Epoch 578/1000 
	 loss: 17.5791, MinusLogProbMetric: 17.5791, val_loss: 17.6924, val_MinusLogProbMetric: 17.6924

Epoch 578: val_loss did not improve from 17.65270
196/196 - 80s - loss: 17.5791 - MinusLogProbMetric: 17.5791 - val_loss: 17.6924 - val_MinusLogProbMetric: 17.6924 - lr: 1.6667e-04 - 80s/epoch - 406ms/step
Epoch 579/1000
2023-09-28 05:58:51.264 
Epoch 579/1000 
	 loss: 17.6145, MinusLogProbMetric: 17.6145, val_loss: 17.6885, val_MinusLogProbMetric: 17.6885

Epoch 579: val_loss did not improve from 17.65270
196/196 - 80s - loss: 17.6145 - MinusLogProbMetric: 17.6145 - val_loss: 17.6885 - val_MinusLogProbMetric: 17.6885 - lr: 1.6667e-04 - 80s/epoch - 406ms/step
Epoch 580/1000
2023-09-28 06:00:10.979 
Epoch 580/1000 
	 loss: 17.5880, MinusLogProbMetric: 17.5880, val_loss: 17.6736, val_MinusLogProbMetric: 17.6736

Epoch 580: val_loss did not improve from 17.65270
196/196 - 80s - loss: 17.5880 - MinusLogProbMetric: 17.5880 - val_loss: 17.6736 - val_MinusLogProbMetric: 17.6736 - lr: 1.6667e-04 - 80s/epoch - 407ms/step
Epoch 581/1000
2023-09-28 06:01:30.557 
Epoch 581/1000 
	 loss: 17.5917, MinusLogProbMetric: 17.5917, val_loss: 17.7358, val_MinusLogProbMetric: 17.7358

Epoch 581: val_loss did not improve from 17.65270
196/196 - 80s - loss: 17.5917 - MinusLogProbMetric: 17.5917 - val_loss: 17.7358 - val_MinusLogProbMetric: 17.7358 - lr: 1.6667e-04 - 80s/epoch - 406ms/step
Epoch 582/1000
2023-09-28 06:02:49.874 
Epoch 582/1000 
	 loss: 17.5791, MinusLogProbMetric: 17.5791, val_loss: 17.6668, val_MinusLogProbMetric: 17.6668

Epoch 582: val_loss did not improve from 17.65270
196/196 - 79s - loss: 17.5791 - MinusLogProbMetric: 17.5791 - val_loss: 17.6668 - val_MinusLogProbMetric: 17.6668 - lr: 1.6667e-04 - 79s/epoch - 405ms/step
Epoch 583/1000
2023-09-28 06:04:09.692 
Epoch 583/1000 
	 loss: 17.6061, MinusLogProbMetric: 17.6061, val_loss: 17.7567, val_MinusLogProbMetric: 17.7567

Epoch 583: val_loss did not improve from 17.65270
196/196 - 80s - loss: 17.6061 - MinusLogProbMetric: 17.6061 - val_loss: 17.7567 - val_MinusLogProbMetric: 17.7567 - lr: 1.6667e-04 - 80s/epoch - 407ms/step
Epoch 584/1000
2023-09-28 06:05:28.935 
Epoch 584/1000 
	 loss: 17.5961, MinusLogProbMetric: 17.5961, val_loss: 17.7483, val_MinusLogProbMetric: 17.7483

Epoch 584: val_loss did not improve from 17.65270
196/196 - 79s - loss: 17.5961 - MinusLogProbMetric: 17.5961 - val_loss: 17.7483 - val_MinusLogProbMetric: 17.7483 - lr: 1.6667e-04 - 79s/epoch - 404ms/step
Epoch 585/1000
2023-09-28 06:06:47.949 
Epoch 585/1000 
	 loss: 17.5939, MinusLogProbMetric: 17.5939, val_loss: 17.7334, val_MinusLogProbMetric: 17.7334

Epoch 585: val_loss did not improve from 17.65270
196/196 - 79s - loss: 17.5939 - MinusLogProbMetric: 17.5939 - val_loss: 17.7334 - val_MinusLogProbMetric: 17.7334 - lr: 1.6667e-04 - 79s/epoch - 403ms/step
Epoch 586/1000
2023-09-28 06:08:07.156 
Epoch 586/1000 
	 loss: 17.5900, MinusLogProbMetric: 17.5900, val_loss: 17.8215, val_MinusLogProbMetric: 17.8215

Epoch 586: val_loss did not improve from 17.65270
196/196 - 79s - loss: 17.5900 - MinusLogProbMetric: 17.5900 - val_loss: 17.8215 - val_MinusLogProbMetric: 17.8215 - lr: 1.6667e-04 - 79s/epoch - 404ms/step
Epoch 587/1000
2023-09-28 06:09:26.200 
Epoch 587/1000 
	 loss: 17.6171, MinusLogProbMetric: 17.6171, val_loss: 17.6730, val_MinusLogProbMetric: 17.6730

Epoch 587: val_loss did not improve from 17.65270
196/196 - 79s - loss: 17.6171 - MinusLogProbMetric: 17.6171 - val_loss: 17.6730 - val_MinusLogProbMetric: 17.6730 - lr: 1.6667e-04 - 79s/epoch - 403ms/step
Epoch 588/1000
2023-09-28 06:10:44.913 
Epoch 588/1000 
	 loss: 17.5810, MinusLogProbMetric: 17.5810, val_loss: 17.7803, val_MinusLogProbMetric: 17.7803

Epoch 588: val_loss did not improve from 17.65270
196/196 - 79s - loss: 17.5810 - MinusLogProbMetric: 17.5810 - val_loss: 17.7803 - val_MinusLogProbMetric: 17.7803 - lr: 1.6667e-04 - 79s/epoch - 402ms/step
Epoch 589/1000
2023-09-28 06:12:04.273 
Epoch 589/1000 
	 loss: 17.5767, MinusLogProbMetric: 17.5767, val_loss: 17.6867, val_MinusLogProbMetric: 17.6867

Epoch 589: val_loss did not improve from 17.65270
196/196 - 79s - loss: 17.5767 - MinusLogProbMetric: 17.5767 - val_loss: 17.6867 - val_MinusLogProbMetric: 17.6867 - lr: 1.6667e-04 - 79s/epoch - 405ms/step
Epoch 590/1000
2023-09-28 06:13:23.165 
Epoch 590/1000 
	 loss: 17.5766, MinusLogProbMetric: 17.5766, val_loss: 18.0065, val_MinusLogProbMetric: 18.0065

Epoch 590: val_loss did not improve from 17.65270
196/196 - 79s - loss: 17.5766 - MinusLogProbMetric: 17.5766 - val_loss: 18.0065 - val_MinusLogProbMetric: 18.0065 - lr: 1.6667e-04 - 79s/epoch - 402ms/step
Epoch 591/1000
2023-09-28 06:14:41.813 
Epoch 591/1000 
	 loss: 17.5991, MinusLogProbMetric: 17.5991, val_loss: 17.8472, val_MinusLogProbMetric: 17.8472

Epoch 591: val_loss did not improve from 17.65270
196/196 - 79s - loss: 17.5991 - MinusLogProbMetric: 17.5991 - val_loss: 17.8472 - val_MinusLogProbMetric: 17.8472 - lr: 1.6667e-04 - 79s/epoch - 401ms/step
Epoch 592/1000
2023-09-28 06:16:00.746 
Epoch 592/1000 
	 loss: 17.5755, MinusLogProbMetric: 17.5755, val_loss: 17.8428, val_MinusLogProbMetric: 17.8428

Epoch 592: val_loss did not improve from 17.65270
196/196 - 79s - loss: 17.5755 - MinusLogProbMetric: 17.5755 - val_loss: 17.8428 - val_MinusLogProbMetric: 17.8428 - lr: 1.6667e-04 - 79s/epoch - 403ms/step
Epoch 593/1000
2023-09-28 06:17:19.781 
Epoch 593/1000 
	 loss: 17.6033, MinusLogProbMetric: 17.6033, val_loss: 17.9217, val_MinusLogProbMetric: 17.9217

Epoch 593: val_loss did not improve from 17.65270
196/196 - 79s - loss: 17.6033 - MinusLogProbMetric: 17.6033 - val_loss: 17.9217 - val_MinusLogProbMetric: 17.9217 - lr: 1.6667e-04 - 79s/epoch - 403ms/step
Epoch 594/1000
2023-09-28 06:18:38.983 
Epoch 594/1000 
	 loss: 17.5946, MinusLogProbMetric: 17.5946, val_loss: 17.6967, val_MinusLogProbMetric: 17.6967

Epoch 594: val_loss did not improve from 17.65270
196/196 - 79s - loss: 17.5946 - MinusLogProbMetric: 17.5946 - val_loss: 17.6967 - val_MinusLogProbMetric: 17.6967 - lr: 1.6667e-04 - 79s/epoch - 404ms/step
Epoch 595/1000
2023-09-28 06:19:58.704 
Epoch 595/1000 
	 loss: 17.5771, MinusLogProbMetric: 17.5771, val_loss: 17.7990, val_MinusLogProbMetric: 17.7990

Epoch 595: val_loss did not improve from 17.65270
196/196 - 80s - loss: 17.5771 - MinusLogProbMetric: 17.5771 - val_loss: 17.7990 - val_MinusLogProbMetric: 17.7990 - lr: 1.6667e-04 - 80s/epoch - 407ms/step
Epoch 596/1000
2023-09-28 06:21:18.095 
Epoch 596/1000 
	 loss: 17.6086, MinusLogProbMetric: 17.6086, val_loss: 17.7024, val_MinusLogProbMetric: 17.7024

Epoch 596: val_loss did not improve from 17.65270
196/196 - 79s - loss: 17.6086 - MinusLogProbMetric: 17.6086 - val_loss: 17.7024 - val_MinusLogProbMetric: 17.7024 - lr: 1.6667e-04 - 79s/epoch - 405ms/step
Epoch 597/1000
2023-09-28 06:22:37.834 
Epoch 597/1000 
	 loss: 17.5831, MinusLogProbMetric: 17.5831, val_loss: 17.9266, val_MinusLogProbMetric: 17.9266

Epoch 597: val_loss did not improve from 17.65270
196/196 - 80s - loss: 17.5831 - MinusLogProbMetric: 17.5831 - val_loss: 17.9266 - val_MinusLogProbMetric: 17.9266 - lr: 1.6667e-04 - 80s/epoch - 407ms/step
Epoch 598/1000
2023-09-28 06:23:56.135 
Epoch 598/1000 
	 loss: 17.6075, MinusLogProbMetric: 17.6075, val_loss: 17.6937, val_MinusLogProbMetric: 17.6937

Epoch 598: val_loss did not improve from 17.65270
196/196 - 78s - loss: 17.6075 - MinusLogProbMetric: 17.6075 - val_loss: 17.6937 - val_MinusLogProbMetric: 17.6937 - lr: 1.6667e-04 - 78s/epoch - 399ms/step
Epoch 599/1000
2023-09-28 06:25:15.323 
Epoch 599/1000 
	 loss: 17.5691, MinusLogProbMetric: 17.5691, val_loss: 17.6456, val_MinusLogProbMetric: 17.6456

Epoch 599: val_loss improved from 17.65270 to 17.64561, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 80s - loss: 17.5691 - MinusLogProbMetric: 17.5691 - val_loss: 17.6456 - val_MinusLogProbMetric: 17.6456 - lr: 1.6667e-04 - 80s/epoch - 411ms/step
Epoch 600/1000
2023-09-28 06:26:31.652 
Epoch 600/1000 
	 loss: 17.5904, MinusLogProbMetric: 17.5904, val_loss: 17.7366, val_MinusLogProbMetric: 17.7366

Epoch 600: val_loss did not improve from 17.64561
196/196 - 75s - loss: 17.5904 - MinusLogProbMetric: 17.5904 - val_loss: 17.7366 - val_MinusLogProbMetric: 17.7366 - lr: 1.6667e-04 - 75s/epoch - 383ms/step
Epoch 601/1000
2023-09-28 06:27:40.944 
Epoch 601/1000 
	 loss: 17.5714, MinusLogProbMetric: 17.5714, val_loss: 17.8765, val_MinusLogProbMetric: 17.8765

Epoch 601: val_loss did not improve from 17.64561
196/196 - 69s - loss: 17.5714 - MinusLogProbMetric: 17.5714 - val_loss: 17.8765 - val_MinusLogProbMetric: 17.8765 - lr: 1.6667e-04 - 69s/epoch - 354ms/step
Epoch 602/1000
2023-09-28 06:28:45.195 
Epoch 602/1000 
	 loss: 17.5978, MinusLogProbMetric: 17.5978, val_loss: 18.0170, val_MinusLogProbMetric: 18.0170

Epoch 602: val_loss did not improve from 17.64561
196/196 - 64s - loss: 17.5978 - MinusLogProbMetric: 17.5978 - val_loss: 18.0170 - val_MinusLogProbMetric: 18.0170 - lr: 1.6667e-04 - 64s/epoch - 328ms/step
Epoch 603/1000
2023-09-28 06:29:59.026 
Epoch 603/1000 
	 loss: 17.5677, MinusLogProbMetric: 17.5677, val_loss: 17.7974, val_MinusLogProbMetric: 17.7974

Epoch 603: val_loss did not improve from 17.64561
196/196 - 74s - loss: 17.5677 - MinusLogProbMetric: 17.5677 - val_loss: 17.7974 - val_MinusLogProbMetric: 17.7974 - lr: 1.6667e-04 - 74s/epoch - 377ms/step
Epoch 604/1000
2023-09-28 06:31:08.237 
Epoch 604/1000 
	 loss: 17.5539, MinusLogProbMetric: 17.5539, val_loss: 17.7137, val_MinusLogProbMetric: 17.7137

Epoch 604: val_loss did not improve from 17.64561
196/196 - 69s - loss: 17.5539 - MinusLogProbMetric: 17.5539 - val_loss: 17.7137 - val_MinusLogProbMetric: 17.7137 - lr: 1.6667e-04 - 69s/epoch - 353ms/step
Epoch 605/1000
2023-09-28 06:32:12.614 
Epoch 605/1000 
	 loss: 17.5942, MinusLogProbMetric: 17.5942, val_loss: 17.6833, val_MinusLogProbMetric: 17.6833

Epoch 605: val_loss did not improve from 17.64561
196/196 - 64s - loss: 17.5942 - MinusLogProbMetric: 17.5942 - val_loss: 17.6833 - val_MinusLogProbMetric: 17.6833 - lr: 1.6667e-04 - 64s/epoch - 328ms/step
Epoch 606/1000
2023-09-28 06:33:28.026 
Epoch 606/1000 
	 loss: 17.5878, MinusLogProbMetric: 17.5878, val_loss: 17.9405, val_MinusLogProbMetric: 17.9405

Epoch 606: val_loss did not improve from 17.64561
196/196 - 75s - loss: 17.5878 - MinusLogProbMetric: 17.5878 - val_loss: 17.9405 - val_MinusLogProbMetric: 17.9405 - lr: 1.6667e-04 - 75s/epoch - 385ms/step
Epoch 607/1000
2023-09-28 06:34:35.118 
Epoch 607/1000 
	 loss: 17.5751, MinusLogProbMetric: 17.5751, val_loss: 17.6161, val_MinusLogProbMetric: 17.6161

Epoch 607: val_loss improved from 17.64561 to 17.61609, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 68s - loss: 17.5751 - MinusLogProbMetric: 17.5751 - val_loss: 17.6161 - val_MinusLogProbMetric: 17.6161 - lr: 1.6667e-04 - 68s/epoch - 348ms/step
Epoch 608/1000
2023-09-28 06:35:43.409 
Epoch 608/1000 
	 loss: 17.5637, MinusLogProbMetric: 17.5637, val_loss: 17.7277, val_MinusLogProbMetric: 17.7277

Epoch 608: val_loss did not improve from 17.61609
196/196 - 67s - loss: 17.5637 - MinusLogProbMetric: 17.5637 - val_loss: 17.7277 - val_MinusLogProbMetric: 17.7277 - lr: 1.6667e-04 - 67s/epoch - 343ms/step
Epoch 609/1000
2023-09-28 06:36:58.014 
Epoch 609/1000 
	 loss: 17.5786, MinusLogProbMetric: 17.5786, val_loss: 17.6531, val_MinusLogProbMetric: 17.6531

Epoch 609: val_loss did not improve from 17.61609
196/196 - 75s - loss: 17.5786 - MinusLogProbMetric: 17.5786 - val_loss: 17.6531 - val_MinusLogProbMetric: 17.6531 - lr: 1.6667e-04 - 75s/epoch - 381ms/step
Epoch 610/1000
2023-09-28 06:38:03.005 
Epoch 610/1000 
	 loss: 17.5517, MinusLogProbMetric: 17.5517, val_loss: 17.6811, val_MinusLogProbMetric: 17.6811

Epoch 610: val_loss did not improve from 17.61609
196/196 - 65s - loss: 17.5517 - MinusLogProbMetric: 17.5517 - val_loss: 17.6811 - val_MinusLogProbMetric: 17.6811 - lr: 1.6667e-04 - 65s/epoch - 332ms/step
Epoch 611/1000
2023-09-28 06:39:14.476 
Epoch 611/1000 
	 loss: 17.5660, MinusLogProbMetric: 17.5660, val_loss: 17.8269, val_MinusLogProbMetric: 17.8269

Epoch 611: val_loss did not improve from 17.61609
196/196 - 71s - loss: 17.5660 - MinusLogProbMetric: 17.5660 - val_loss: 17.8269 - val_MinusLogProbMetric: 17.8269 - lr: 1.6667e-04 - 71s/epoch - 365ms/step
Epoch 612/1000
2023-09-28 06:40:23.504 
Epoch 612/1000 
	 loss: 17.5831, MinusLogProbMetric: 17.5831, val_loss: 17.7207, val_MinusLogProbMetric: 17.7207

Epoch 612: val_loss did not improve from 17.61609
196/196 - 69s - loss: 17.5831 - MinusLogProbMetric: 17.5831 - val_loss: 17.7207 - val_MinusLogProbMetric: 17.7207 - lr: 1.6667e-04 - 69s/epoch - 352ms/step
Epoch 613/1000
2023-09-28 06:41:27.997 
Epoch 613/1000 
	 loss: 17.5568, MinusLogProbMetric: 17.5568, val_loss: 17.6905, val_MinusLogProbMetric: 17.6905

Epoch 613: val_loss did not improve from 17.61609
196/196 - 64s - loss: 17.5568 - MinusLogProbMetric: 17.5568 - val_loss: 17.6905 - val_MinusLogProbMetric: 17.6905 - lr: 1.6667e-04 - 64s/epoch - 329ms/step
Epoch 614/1000
2023-09-28 06:42:42.008 
Epoch 614/1000 
	 loss: 17.5831, MinusLogProbMetric: 17.5831, val_loss: 17.6660, val_MinusLogProbMetric: 17.6660

Epoch 614: val_loss did not improve from 17.61609
196/196 - 74s - loss: 17.5831 - MinusLogProbMetric: 17.5831 - val_loss: 17.6660 - val_MinusLogProbMetric: 17.6660 - lr: 1.6667e-04 - 74s/epoch - 378ms/step
Epoch 615/1000
2023-09-28 06:43:47.956 
Epoch 615/1000 
	 loss: 17.5751, MinusLogProbMetric: 17.5751, val_loss: 17.9553, val_MinusLogProbMetric: 17.9553

Epoch 615: val_loss did not improve from 17.61609
196/196 - 66s - loss: 17.5751 - MinusLogProbMetric: 17.5751 - val_loss: 17.9553 - val_MinusLogProbMetric: 17.9553 - lr: 1.6667e-04 - 66s/epoch - 336ms/step
Epoch 616/1000
2023-09-28 06:44:54.826 
Epoch 616/1000 
	 loss: 17.5719, MinusLogProbMetric: 17.5719, val_loss: 17.8983, val_MinusLogProbMetric: 17.8983

Epoch 616: val_loss did not improve from 17.61609
196/196 - 67s - loss: 17.5719 - MinusLogProbMetric: 17.5719 - val_loss: 17.8983 - val_MinusLogProbMetric: 17.8983 - lr: 1.6667e-04 - 67s/epoch - 341ms/step
Epoch 617/1000
2023-09-28 06:46:10.735 
Epoch 617/1000 
	 loss: 17.5539, MinusLogProbMetric: 17.5539, val_loss: 17.6934, val_MinusLogProbMetric: 17.6934

Epoch 617: val_loss did not improve from 17.61609
196/196 - 76s - loss: 17.5539 - MinusLogProbMetric: 17.5539 - val_loss: 17.6934 - val_MinusLogProbMetric: 17.6934 - lr: 1.6667e-04 - 76s/epoch - 387ms/step
Epoch 618/1000
2023-09-28 06:47:13.593 
Epoch 618/1000 
	 loss: 17.5574, MinusLogProbMetric: 17.5574, val_loss: 17.7244, val_MinusLogProbMetric: 17.7244

Epoch 618: val_loss did not improve from 17.61609
196/196 - 63s - loss: 17.5574 - MinusLogProbMetric: 17.5574 - val_loss: 17.7244 - val_MinusLogProbMetric: 17.7244 - lr: 1.6667e-04 - 63s/epoch - 321ms/step
Epoch 619/1000
2023-09-28 06:48:20.041 
Epoch 619/1000 
	 loss: 17.5703, MinusLogProbMetric: 17.5703, val_loss: 17.7852, val_MinusLogProbMetric: 17.7852

Epoch 619: val_loss did not improve from 17.61609
196/196 - 66s - loss: 17.5703 - MinusLogProbMetric: 17.5703 - val_loss: 17.7852 - val_MinusLogProbMetric: 17.7852 - lr: 1.6667e-04 - 66s/epoch - 339ms/step
Epoch 620/1000
2023-09-28 06:49:32.450 
Epoch 620/1000 
	 loss: 17.7006, MinusLogProbMetric: 17.7006, val_loss: 17.6578, val_MinusLogProbMetric: 17.6578

Epoch 620: val_loss did not improve from 17.61609
196/196 - 72s - loss: 17.7006 - MinusLogProbMetric: 17.7006 - val_loss: 17.6578 - val_MinusLogProbMetric: 17.6578 - lr: 1.6667e-04 - 72s/epoch - 369ms/step
Epoch 621/1000
2023-09-28 06:50:35.417 
Epoch 621/1000 
	 loss: 17.5667, MinusLogProbMetric: 17.5667, val_loss: 17.6836, val_MinusLogProbMetric: 17.6836

Epoch 621: val_loss did not improve from 17.61609
196/196 - 63s - loss: 17.5667 - MinusLogProbMetric: 17.5667 - val_loss: 17.6836 - val_MinusLogProbMetric: 17.6836 - lr: 1.6667e-04 - 63s/epoch - 321ms/step
Epoch 622/1000
2023-09-28 06:51:42.444 
Epoch 622/1000 
	 loss: 17.5833, MinusLogProbMetric: 17.5833, val_loss: 17.7186, val_MinusLogProbMetric: 17.7186

Epoch 622: val_loss did not improve from 17.61609
196/196 - 67s - loss: 17.5833 - MinusLogProbMetric: 17.5833 - val_loss: 17.7186 - val_MinusLogProbMetric: 17.7186 - lr: 1.6667e-04 - 67s/epoch - 342ms/step
Epoch 623/1000
2023-09-28 06:52:56.557 
Epoch 623/1000 
	 loss: 17.5622, MinusLogProbMetric: 17.5622, val_loss: 17.6865, val_MinusLogProbMetric: 17.6865

Epoch 623: val_loss did not improve from 17.61609
196/196 - 74s - loss: 17.5622 - MinusLogProbMetric: 17.5622 - val_loss: 17.6865 - val_MinusLogProbMetric: 17.6865 - lr: 1.6667e-04 - 74s/epoch - 378ms/step
Epoch 624/1000
2023-09-28 06:53:59.519 
Epoch 624/1000 
	 loss: 17.5576, MinusLogProbMetric: 17.5576, val_loss: 17.7131, val_MinusLogProbMetric: 17.7131

Epoch 624: val_loss did not improve from 17.61609
196/196 - 63s - loss: 17.5576 - MinusLogProbMetric: 17.5576 - val_loss: 17.7131 - val_MinusLogProbMetric: 17.7131 - lr: 1.6667e-04 - 63s/epoch - 321ms/step
Epoch 625/1000
2023-09-28 06:55:06.858 
Epoch 625/1000 
	 loss: 17.5516, MinusLogProbMetric: 17.5516, val_loss: 17.8061, val_MinusLogProbMetric: 17.8061

Epoch 625: val_loss did not improve from 17.61609
196/196 - 67s - loss: 17.5516 - MinusLogProbMetric: 17.5516 - val_loss: 17.8061 - val_MinusLogProbMetric: 17.8061 - lr: 1.6667e-04 - 67s/epoch - 344ms/step
Epoch 626/1000
2023-09-28 06:56:19.659 
Epoch 626/1000 
	 loss: 17.5433, MinusLogProbMetric: 17.5433, val_loss: 17.6528, val_MinusLogProbMetric: 17.6528

Epoch 626: val_loss did not improve from 17.61609
196/196 - 73s - loss: 17.5433 - MinusLogProbMetric: 17.5433 - val_loss: 17.6528 - val_MinusLogProbMetric: 17.6528 - lr: 1.6667e-04 - 73s/epoch - 371ms/step
Epoch 627/1000
2023-09-28 06:57:23.731 
Epoch 627/1000 
	 loss: 17.5431, MinusLogProbMetric: 17.5431, val_loss: 17.6741, val_MinusLogProbMetric: 17.6741

Epoch 627: val_loss did not improve from 17.61609
196/196 - 64s - loss: 17.5431 - MinusLogProbMetric: 17.5431 - val_loss: 17.6741 - val_MinusLogProbMetric: 17.6741 - lr: 1.6667e-04 - 64s/epoch - 327ms/step
Epoch 628/1000
2023-09-28 06:58:35.636 
Epoch 628/1000 
	 loss: 17.5551, MinusLogProbMetric: 17.5551, val_loss: 17.6998, val_MinusLogProbMetric: 17.6998

Epoch 628: val_loss did not improve from 17.61609
196/196 - 72s - loss: 17.5551 - MinusLogProbMetric: 17.5551 - val_loss: 17.6998 - val_MinusLogProbMetric: 17.6998 - lr: 1.6667e-04 - 72s/epoch - 367ms/step
Epoch 629/1000
2023-09-28 06:59:45.417 
Epoch 629/1000 
	 loss: 17.5374, MinusLogProbMetric: 17.5374, val_loss: 17.7630, val_MinusLogProbMetric: 17.7630

Epoch 629: val_loss did not improve from 17.61609
196/196 - 70s - loss: 17.5374 - MinusLogProbMetric: 17.5374 - val_loss: 17.7630 - val_MinusLogProbMetric: 17.7630 - lr: 1.6667e-04 - 70s/epoch - 356ms/step
Epoch 630/1000
2023-09-28 07:00:48.619 
Epoch 630/1000 
	 loss: 17.5406, MinusLogProbMetric: 17.5406, val_loss: 17.7110, val_MinusLogProbMetric: 17.7110

Epoch 630: val_loss did not improve from 17.61609
196/196 - 63s - loss: 17.5406 - MinusLogProbMetric: 17.5406 - val_loss: 17.7110 - val_MinusLogProbMetric: 17.7110 - lr: 1.6667e-04 - 63s/epoch - 322ms/step
Epoch 631/1000
2023-09-28 07:02:02.889 
Epoch 631/1000 
	 loss: 17.5472, MinusLogProbMetric: 17.5472, val_loss: 17.7072, val_MinusLogProbMetric: 17.7072

Epoch 631: val_loss did not improve from 17.61609
196/196 - 74s - loss: 17.5472 - MinusLogProbMetric: 17.5472 - val_loss: 17.7072 - val_MinusLogProbMetric: 17.7072 - lr: 1.6667e-04 - 74s/epoch - 379ms/step
Epoch 632/1000
2023-09-28 07:03:18.286 
Epoch 632/1000 
	 loss: 17.5458, MinusLogProbMetric: 17.5458, val_loss: 17.8136, val_MinusLogProbMetric: 17.8136

Epoch 632: val_loss did not improve from 17.61609
196/196 - 75s - loss: 17.5458 - MinusLogProbMetric: 17.5458 - val_loss: 17.8136 - val_MinusLogProbMetric: 17.8136 - lr: 1.6667e-04 - 75s/epoch - 385ms/step
Epoch 633/1000
2023-09-28 07:04:32.422 
Epoch 633/1000 
	 loss: 17.5470, MinusLogProbMetric: 17.5470, val_loss: 17.6224, val_MinusLogProbMetric: 17.6224

Epoch 633: val_loss did not improve from 17.61609
196/196 - 74s - loss: 17.5470 - MinusLogProbMetric: 17.5470 - val_loss: 17.6224 - val_MinusLogProbMetric: 17.6224 - lr: 1.6667e-04 - 74s/epoch - 378ms/step
Epoch 634/1000
2023-09-28 07:05:52.192 
Epoch 634/1000 
	 loss: 17.5501, MinusLogProbMetric: 17.5501, val_loss: 17.6815, val_MinusLogProbMetric: 17.6815

Epoch 634: val_loss did not improve from 17.61609
196/196 - 80s - loss: 17.5501 - MinusLogProbMetric: 17.5501 - val_loss: 17.6815 - val_MinusLogProbMetric: 17.6815 - lr: 1.6667e-04 - 80s/epoch - 407ms/step
Epoch 635/1000
2023-09-28 07:07:11.754 
Epoch 635/1000 
	 loss: 17.5523, MinusLogProbMetric: 17.5523, val_loss: 17.7750, val_MinusLogProbMetric: 17.7750

Epoch 635: val_loss did not improve from 17.61609
196/196 - 80s - loss: 17.5523 - MinusLogProbMetric: 17.5523 - val_loss: 17.7750 - val_MinusLogProbMetric: 17.7750 - lr: 1.6667e-04 - 80s/epoch - 406ms/step
Epoch 636/1000
2023-09-28 07:08:32.731 
Epoch 636/1000 
	 loss: 17.5358, MinusLogProbMetric: 17.5358, val_loss: 17.7254, val_MinusLogProbMetric: 17.7254

Epoch 636: val_loss did not improve from 17.61609
196/196 - 81s - loss: 17.5358 - MinusLogProbMetric: 17.5358 - val_loss: 17.7254 - val_MinusLogProbMetric: 17.7254 - lr: 1.6667e-04 - 81s/epoch - 413ms/step
Epoch 637/1000
2023-09-28 07:09:46.620 
Epoch 637/1000 
	 loss: 17.5278, MinusLogProbMetric: 17.5278, val_loss: 17.7338, val_MinusLogProbMetric: 17.7338

Epoch 637: val_loss did not improve from 17.61609
196/196 - 74s - loss: 17.5278 - MinusLogProbMetric: 17.5278 - val_loss: 17.7338 - val_MinusLogProbMetric: 17.7338 - lr: 1.6667e-04 - 74s/epoch - 377ms/step
Epoch 638/1000
2023-09-28 07:11:01.699 
Epoch 638/1000 
	 loss: 17.5608, MinusLogProbMetric: 17.5608, val_loss: 17.6143, val_MinusLogProbMetric: 17.6143

Epoch 638: val_loss improved from 17.61609 to 17.61427, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 76s - loss: 17.5608 - MinusLogProbMetric: 17.5608 - val_loss: 17.6143 - val_MinusLogProbMetric: 17.6143 - lr: 1.6667e-04 - 76s/epoch - 390ms/step
Epoch 639/1000
2023-09-28 07:12:23.454 
Epoch 639/1000 
	 loss: 17.5281, MinusLogProbMetric: 17.5281, val_loss: 17.6910, val_MinusLogProbMetric: 17.6910

Epoch 639: val_loss did not improve from 17.61427
196/196 - 80s - loss: 17.5281 - MinusLogProbMetric: 17.5281 - val_loss: 17.6910 - val_MinusLogProbMetric: 17.6910 - lr: 1.6667e-04 - 80s/epoch - 410ms/step
Epoch 640/1000
2023-09-28 07:13:43.895 
Epoch 640/1000 
	 loss: 17.5486, MinusLogProbMetric: 17.5486, val_loss: 17.9321, val_MinusLogProbMetric: 17.9321

Epoch 640: val_loss did not improve from 17.61427
196/196 - 80s - loss: 17.5486 - MinusLogProbMetric: 17.5486 - val_loss: 17.9321 - val_MinusLogProbMetric: 17.9321 - lr: 1.6667e-04 - 80s/epoch - 410ms/step
Epoch 641/1000
2023-09-28 07:15:04.291 
Epoch 641/1000 
	 loss: 17.5873, MinusLogProbMetric: 17.5873, val_loss: 17.8062, val_MinusLogProbMetric: 17.8062

Epoch 641: val_loss did not improve from 17.61427
196/196 - 80s - loss: 17.5873 - MinusLogProbMetric: 17.5873 - val_loss: 17.8062 - val_MinusLogProbMetric: 17.8062 - lr: 1.6667e-04 - 80s/epoch - 410ms/step
Epoch 642/1000
2023-09-28 07:16:24.455 
Epoch 642/1000 
	 loss: 17.5515, MinusLogProbMetric: 17.5515, val_loss: 17.7653, val_MinusLogProbMetric: 17.7653

Epoch 642: val_loss did not improve from 17.61427
196/196 - 80s - loss: 17.5515 - MinusLogProbMetric: 17.5515 - val_loss: 17.7653 - val_MinusLogProbMetric: 17.7653 - lr: 1.6667e-04 - 80s/epoch - 409ms/step
Epoch 643/1000
2023-09-28 07:17:45.130 
Epoch 643/1000 
	 loss: 17.5317, MinusLogProbMetric: 17.5317, val_loss: 17.8355, val_MinusLogProbMetric: 17.8355

Epoch 643: val_loss did not improve from 17.61427
196/196 - 81s - loss: 17.5317 - MinusLogProbMetric: 17.5317 - val_loss: 17.8355 - val_MinusLogProbMetric: 17.8355 - lr: 1.6667e-04 - 81s/epoch - 412ms/step
Epoch 644/1000
2023-09-28 07:19:04.473 
Epoch 644/1000 
	 loss: 17.5175, MinusLogProbMetric: 17.5175, val_loss: 17.8901, val_MinusLogProbMetric: 17.8901

Epoch 644: val_loss did not improve from 17.61427
196/196 - 79s - loss: 17.5175 - MinusLogProbMetric: 17.5175 - val_loss: 17.8901 - val_MinusLogProbMetric: 17.8901 - lr: 1.6667e-04 - 79s/epoch - 405ms/step
Epoch 645/1000
2023-09-28 07:20:25.117 
Epoch 645/1000 
	 loss: 17.5472, MinusLogProbMetric: 17.5472, val_loss: 17.6549, val_MinusLogProbMetric: 17.6549

Epoch 645: val_loss did not improve from 17.61427
196/196 - 81s - loss: 17.5472 - MinusLogProbMetric: 17.5472 - val_loss: 17.6549 - val_MinusLogProbMetric: 17.6549 - lr: 1.6667e-04 - 81s/epoch - 411ms/step
Epoch 646/1000
2023-09-28 07:21:46.015 
Epoch 646/1000 
	 loss: 17.5482, MinusLogProbMetric: 17.5482, val_loss: 17.8222, val_MinusLogProbMetric: 17.8222

Epoch 646: val_loss did not improve from 17.61427
196/196 - 81s - loss: 17.5482 - MinusLogProbMetric: 17.5482 - val_loss: 17.8222 - val_MinusLogProbMetric: 17.8222 - lr: 1.6667e-04 - 81s/epoch - 413ms/step
Epoch 647/1000
2023-09-28 07:23:06.471 
Epoch 647/1000 
	 loss: 17.5403, MinusLogProbMetric: 17.5403, val_loss: 17.6843, val_MinusLogProbMetric: 17.6843

Epoch 647: val_loss did not improve from 17.61427
196/196 - 80s - loss: 17.5403 - MinusLogProbMetric: 17.5403 - val_loss: 17.6843 - val_MinusLogProbMetric: 17.6843 - lr: 1.6667e-04 - 80s/epoch - 410ms/step
Epoch 648/1000
2023-09-28 07:24:27.094 
Epoch 648/1000 
	 loss: 17.5498, MinusLogProbMetric: 17.5498, val_loss: 17.7580, val_MinusLogProbMetric: 17.7580

Epoch 648: val_loss did not improve from 17.61427
196/196 - 81s - loss: 17.5498 - MinusLogProbMetric: 17.5498 - val_loss: 17.7580 - val_MinusLogProbMetric: 17.7580 - lr: 1.6667e-04 - 81s/epoch - 411ms/step
Epoch 649/1000
2023-09-28 07:25:47.955 
Epoch 649/1000 
	 loss: 17.5374, MinusLogProbMetric: 17.5374, val_loss: 17.7364, val_MinusLogProbMetric: 17.7364

Epoch 649: val_loss did not improve from 17.61427
196/196 - 81s - loss: 17.5374 - MinusLogProbMetric: 17.5374 - val_loss: 17.7364 - val_MinusLogProbMetric: 17.7364 - lr: 1.6667e-04 - 81s/epoch - 413ms/step
Epoch 650/1000
2023-09-28 07:27:08.852 
Epoch 650/1000 
	 loss: 17.5383, MinusLogProbMetric: 17.5383, val_loss: 17.8684, val_MinusLogProbMetric: 17.8684

Epoch 650: val_loss did not improve from 17.61427
196/196 - 81s - loss: 17.5383 - MinusLogProbMetric: 17.5383 - val_loss: 17.8684 - val_MinusLogProbMetric: 17.8684 - lr: 1.6667e-04 - 81s/epoch - 413ms/step
Epoch 651/1000
2023-09-28 07:28:29.093 
Epoch 651/1000 
	 loss: 17.5564, MinusLogProbMetric: 17.5564, val_loss: 17.6791, val_MinusLogProbMetric: 17.6791

Epoch 651: val_loss did not improve from 17.61427
196/196 - 80s - loss: 17.5564 - MinusLogProbMetric: 17.5564 - val_loss: 17.6791 - val_MinusLogProbMetric: 17.6791 - lr: 1.6667e-04 - 80s/epoch - 409ms/step
Epoch 652/1000
2023-09-28 07:29:49.175 
Epoch 652/1000 
	 loss: 17.5372, MinusLogProbMetric: 17.5372, val_loss: 17.6538, val_MinusLogProbMetric: 17.6538

Epoch 652: val_loss did not improve from 17.61427
196/196 - 80s - loss: 17.5372 - MinusLogProbMetric: 17.5372 - val_loss: 17.6538 - val_MinusLogProbMetric: 17.6538 - lr: 1.6667e-04 - 80s/epoch - 409ms/step
Epoch 653/1000
2023-09-28 07:31:09.801 
Epoch 653/1000 
	 loss: 17.5256, MinusLogProbMetric: 17.5256, val_loss: 17.7255, val_MinusLogProbMetric: 17.7255

Epoch 653: val_loss did not improve from 17.61427
196/196 - 81s - loss: 17.5256 - MinusLogProbMetric: 17.5256 - val_loss: 17.7255 - val_MinusLogProbMetric: 17.7255 - lr: 1.6667e-04 - 81s/epoch - 411ms/step
Epoch 654/1000
2023-09-28 07:32:30.407 
Epoch 654/1000 
	 loss: 17.5509, MinusLogProbMetric: 17.5509, val_loss: 17.6307, val_MinusLogProbMetric: 17.6307

Epoch 654: val_loss did not improve from 17.61427
196/196 - 81s - loss: 17.5509 - MinusLogProbMetric: 17.5509 - val_loss: 17.6307 - val_MinusLogProbMetric: 17.6307 - lr: 1.6667e-04 - 81s/epoch - 411ms/step
Epoch 655/1000
2023-09-28 07:33:50.296 
Epoch 655/1000 
	 loss: 17.5508, MinusLogProbMetric: 17.5508, val_loss: 17.9811, val_MinusLogProbMetric: 17.9811

Epoch 655: val_loss did not improve from 17.61427
196/196 - 80s - loss: 17.5508 - MinusLogProbMetric: 17.5508 - val_loss: 17.9811 - val_MinusLogProbMetric: 17.9811 - lr: 1.6667e-04 - 80s/epoch - 408ms/step
Epoch 656/1000
2023-09-28 07:35:10.939 
Epoch 656/1000 
	 loss: 17.5638, MinusLogProbMetric: 17.5638, val_loss: 17.7111, val_MinusLogProbMetric: 17.7111

Epoch 656: val_loss did not improve from 17.61427
196/196 - 81s - loss: 17.5638 - MinusLogProbMetric: 17.5638 - val_loss: 17.7111 - val_MinusLogProbMetric: 17.7111 - lr: 1.6667e-04 - 81s/epoch - 411ms/step
Epoch 657/1000
2023-09-28 07:36:31.690 
Epoch 657/1000 
	 loss: 17.5595, MinusLogProbMetric: 17.5595, val_loss: 17.6560, val_MinusLogProbMetric: 17.6560

Epoch 657: val_loss did not improve from 17.61427
196/196 - 81s - loss: 17.5595 - MinusLogProbMetric: 17.5595 - val_loss: 17.6560 - val_MinusLogProbMetric: 17.6560 - lr: 1.6667e-04 - 81s/epoch - 412ms/step
Epoch 658/1000
2023-09-28 07:37:52.674 
Epoch 658/1000 
	 loss: 17.5507, MinusLogProbMetric: 17.5507, val_loss: 17.7054, val_MinusLogProbMetric: 17.7054

Epoch 658: val_loss did not improve from 17.61427
196/196 - 81s - loss: 17.5507 - MinusLogProbMetric: 17.5507 - val_loss: 17.7054 - val_MinusLogProbMetric: 17.7054 - lr: 1.6667e-04 - 81s/epoch - 413ms/step
Epoch 659/1000
2023-09-28 07:39:12.716 
Epoch 659/1000 
	 loss: 17.5440, MinusLogProbMetric: 17.5440, val_loss: 17.7245, val_MinusLogProbMetric: 17.7245

Epoch 659: val_loss did not improve from 17.61427
196/196 - 80s - loss: 17.5440 - MinusLogProbMetric: 17.5440 - val_loss: 17.7245 - val_MinusLogProbMetric: 17.7245 - lr: 1.6667e-04 - 80s/epoch - 408ms/step
Epoch 660/1000
2023-09-28 07:40:33.608 
Epoch 660/1000 
	 loss: 17.5212, MinusLogProbMetric: 17.5212, val_loss: 17.6467, val_MinusLogProbMetric: 17.6467

Epoch 660: val_loss did not improve from 17.61427
196/196 - 81s - loss: 17.5212 - MinusLogProbMetric: 17.5212 - val_loss: 17.6467 - val_MinusLogProbMetric: 17.6467 - lr: 1.6667e-04 - 81s/epoch - 413ms/step
Epoch 661/1000
2023-09-28 07:41:54.412 
Epoch 661/1000 
	 loss: 17.5231, MinusLogProbMetric: 17.5231, val_loss: 17.6727, val_MinusLogProbMetric: 17.6727

Epoch 661: val_loss did not improve from 17.61427
196/196 - 81s - loss: 17.5231 - MinusLogProbMetric: 17.5231 - val_loss: 17.6727 - val_MinusLogProbMetric: 17.6727 - lr: 1.6667e-04 - 81s/epoch - 412ms/step
Epoch 662/1000
2023-09-28 07:43:15.113 
Epoch 662/1000 
	 loss: 17.5617, MinusLogProbMetric: 17.5617, val_loss: 17.7168, val_MinusLogProbMetric: 17.7168

Epoch 662: val_loss did not improve from 17.61427
196/196 - 81s - loss: 17.5617 - MinusLogProbMetric: 17.5617 - val_loss: 17.7168 - val_MinusLogProbMetric: 17.7168 - lr: 1.6667e-04 - 81s/epoch - 412ms/step
Epoch 663/1000
2023-09-28 07:44:36.083 
Epoch 663/1000 
	 loss: 17.5248, MinusLogProbMetric: 17.5248, val_loss: 17.6016, val_MinusLogProbMetric: 17.6016

Epoch 663: val_loss improved from 17.61427 to 17.60156, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 83s - loss: 17.5248 - MinusLogProbMetric: 17.5248 - val_loss: 17.6016 - val_MinusLogProbMetric: 17.6016 - lr: 1.6667e-04 - 83s/epoch - 422ms/step
Epoch 664/1000
2023-09-28 07:45:58.175 
Epoch 664/1000 
	 loss: 17.5234, MinusLogProbMetric: 17.5234, val_loss: 17.7050, val_MinusLogProbMetric: 17.7050

Epoch 664: val_loss did not improve from 17.60156
196/196 - 80s - loss: 17.5234 - MinusLogProbMetric: 17.5234 - val_loss: 17.7050 - val_MinusLogProbMetric: 17.7050 - lr: 1.6667e-04 - 80s/epoch - 410ms/step
Epoch 665/1000
2023-09-28 07:47:19.224 
Epoch 665/1000 
	 loss: 17.5144, MinusLogProbMetric: 17.5144, val_loss: 17.7007, val_MinusLogProbMetric: 17.7007

Epoch 665: val_loss did not improve from 17.60156
196/196 - 81s - loss: 17.5144 - MinusLogProbMetric: 17.5144 - val_loss: 17.7007 - val_MinusLogProbMetric: 17.7007 - lr: 1.6667e-04 - 81s/epoch - 414ms/step
Epoch 666/1000
2023-09-28 07:48:39.289 
Epoch 666/1000 
	 loss: 17.5295, MinusLogProbMetric: 17.5295, val_loss: 17.6055, val_MinusLogProbMetric: 17.6055

Epoch 666: val_loss did not improve from 17.60156
196/196 - 80s - loss: 17.5295 - MinusLogProbMetric: 17.5295 - val_loss: 17.6055 - val_MinusLogProbMetric: 17.6055 - lr: 1.6667e-04 - 80s/epoch - 408ms/step
Epoch 667/1000
2023-09-28 07:49:59.591 
Epoch 667/1000 
	 loss: 17.5087, MinusLogProbMetric: 17.5087, val_loss: 17.6896, val_MinusLogProbMetric: 17.6896

Epoch 667: val_loss did not improve from 17.60156
196/196 - 80s - loss: 17.5087 - MinusLogProbMetric: 17.5087 - val_loss: 17.6896 - val_MinusLogProbMetric: 17.6896 - lr: 1.6667e-04 - 80s/epoch - 410ms/step
Epoch 668/1000
2023-09-28 07:51:19.396 
Epoch 668/1000 
	 loss: 17.5344, MinusLogProbMetric: 17.5344, val_loss: 17.6841, val_MinusLogProbMetric: 17.6841

Epoch 668: val_loss did not improve from 17.60156
196/196 - 80s - loss: 17.5344 - MinusLogProbMetric: 17.5344 - val_loss: 17.6841 - val_MinusLogProbMetric: 17.6841 - lr: 1.6667e-04 - 80s/epoch - 407ms/step
Epoch 669/1000
2023-09-28 07:52:40.410 
Epoch 669/1000 
	 loss: 17.5320, MinusLogProbMetric: 17.5320, val_loss: 17.6134, val_MinusLogProbMetric: 17.6134

Epoch 669: val_loss did not improve from 17.60156
196/196 - 81s - loss: 17.5320 - MinusLogProbMetric: 17.5320 - val_loss: 17.6134 - val_MinusLogProbMetric: 17.6134 - lr: 1.6667e-04 - 81s/epoch - 413ms/step
Epoch 670/1000
2023-09-28 07:54:00.655 
Epoch 670/1000 
	 loss: 17.5167, MinusLogProbMetric: 17.5167, val_loss: 17.6952, val_MinusLogProbMetric: 17.6952

Epoch 670: val_loss did not improve from 17.60156
196/196 - 80s - loss: 17.5167 - MinusLogProbMetric: 17.5167 - val_loss: 17.6952 - val_MinusLogProbMetric: 17.6952 - lr: 1.6667e-04 - 80s/epoch - 409ms/step
Epoch 671/1000
2023-09-28 07:55:20.974 
Epoch 671/1000 
	 loss: 17.6680, MinusLogProbMetric: 17.6680, val_loss: 26.0778, val_MinusLogProbMetric: 26.0778

Epoch 671: val_loss did not improve from 17.60156
196/196 - 80s - loss: 17.6680 - MinusLogProbMetric: 17.6680 - val_loss: 26.0778 - val_MinusLogProbMetric: 26.0778 - lr: 1.6667e-04 - 80s/epoch - 410ms/step
Epoch 672/1000
2023-09-28 07:56:41.237 
Epoch 672/1000 
	 loss: 18.0613, MinusLogProbMetric: 18.0613, val_loss: 17.7895, val_MinusLogProbMetric: 17.7895

Epoch 672: val_loss did not improve from 17.60156
196/196 - 80s - loss: 18.0613 - MinusLogProbMetric: 18.0613 - val_loss: 17.7895 - val_MinusLogProbMetric: 17.7895 - lr: 1.6667e-04 - 80s/epoch - 409ms/step
Epoch 673/1000
2023-09-28 07:58:01.883 
Epoch 673/1000 
	 loss: 17.5951, MinusLogProbMetric: 17.5951, val_loss: 17.6901, val_MinusLogProbMetric: 17.6901

Epoch 673: val_loss did not improve from 17.60156
196/196 - 81s - loss: 17.5951 - MinusLogProbMetric: 17.5951 - val_loss: 17.6901 - val_MinusLogProbMetric: 17.6901 - lr: 1.6667e-04 - 81s/epoch - 411ms/step
Epoch 674/1000
2023-09-28 07:59:22.791 
Epoch 674/1000 
	 loss: 17.5635, MinusLogProbMetric: 17.5635, val_loss: 17.6811, val_MinusLogProbMetric: 17.6811

Epoch 674: val_loss did not improve from 17.60156
196/196 - 81s - loss: 17.5635 - MinusLogProbMetric: 17.5635 - val_loss: 17.6811 - val_MinusLogProbMetric: 17.6811 - lr: 1.6667e-04 - 81s/epoch - 413ms/step
Epoch 675/1000
2023-09-28 08:00:43.413 
Epoch 675/1000 
	 loss: 17.5447, MinusLogProbMetric: 17.5447, val_loss: 17.6125, val_MinusLogProbMetric: 17.6125

Epoch 675: val_loss did not improve from 17.60156
196/196 - 81s - loss: 17.5447 - MinusLogProbMetric: 17.5447 - val_loss: 17.6125 - val_MinusLogProbMetric: 17.6125 - lr: 1.6667e-04 - 81s/epoch - 411ms/step
Epoch 676/1000
2023-09-28 08:02:03.863 
Epoch 676/1000 
	 loss: 17.5232, MinusLogProbMetric: 17.5232, val_loss: 17.6117, val_MinusLogProbMetric: 17.6117

Epoch 676: val_loss did not improve from 17.60156
196/196 - 80s - loss: 17.5232 - MinusLogProbMetric: 17.5232 - val_loss: 17.6117 - val_MinusLogProbMetric: 17.6117 - lr: 1.6667e-04 - 80s/epoch - 410ms/step
Epoch 677/1000
2023-09-28 08:03:23.616 
Epoch 677/1000 
	 loss: 17.4964, MinusLogProbMetric: 17.4964, val_loss: 17.8973, val_MinusLogProbMetric: 17.8973

Epoch 677: val_loss did not improve from 17.60156
196/196 - 80s - loss: 17.4964 - MinusLogProbMetric: 17.4964 - val_loss: 17.8973 - val_MinusLogProbMetric: 17.8973 - lr: 1.6667e-04 - 80s/epoch - 407ms/step
Epoch 678/1000
2023-09-28 08:04:44.013 
Epoch 678/1000 
	 loss: 17.5136, MinusLogProbMetric: 17.5136, val_loss: 17.6072, val_MinusLogProbMetric: 17.6072

Epoch 678: val_loss did not improve from 17.60156
196/196 - 80s - loss: 17.5136 - MinusLogProbMetric: 17.5136 - val_loss: 17.6072 - val_MinusLogProbMetric: 17.6072 - lr: 1.6667e-04 - 80s/epoch - 410ms/step
Epoch 679/1000
2023-09-28 08:06:04.202 
Epoch 679/1000 
	 loss: 17.5147, MinusLogProbMetric: 17.5147, val_loss: 17.6148, val_MinusLogProbMetric: 17.6148

Epoch 679: val_loss did not improve from 17.60156
196/196 - 80s - loss: 17.5147 - MinusLogProbMetric: 17.5147 - val_loss: 17.6148 - val_MinusLogProbMetric: 17.6148 - lr: 1.6667e-04 - 80s/epoch - 409ms/step
Epoch 680/1000
2023-09-28 08:07:24.391 
Epoch 680/1000 
	 loss: 17.5427, MinusLogProbMetric: 17.5427, val_loss: 17.6650, val_MinusLogProbMetric: 17.6650

Epoch 680: val_loss did not improve from 17.60156
196/196 - 80s - loss: 17.5427 - MinusLogProbMetric: 17.5427 - val_loss: 17.6650 - val_MinusLogProbMetric: 17.6650 - lr: 1.6667e-04 - 80s/epoch - 409ms/step
Epoch 681/1000
2023-09-28 08:08:44.834 
Epoch 681/1000 
	 loss: 17.5019, MinusLogProbMetric: 17.5019, val_loss: 17.7196, val_MinusLogProbMetric: 17.7196

Epoch 681: val_loss did not improve from 17.60156
196/196 - 80s - loss: 17.5019 - MinusLogProbMetric: 17.5019 - val_loss: 17.7196 - val_MinusLogProbMetric: 17.7196 - lr: 1.6667e-04 - 80s/epoch - 410ms/step
Epoch 682/1000
2023-09-28 08:10:05.433 
Epoch 682/1000 
	 loss: 17.5137, MinusLogProbMetric: 17.5137, val_loss: 17.6757, val_MinusLogProbMetric: 17.6757

Epoch 682: val_loss did not improve from 17.60156
196/196 - 81s - loss: 17.5137 - MinusLogProbMetric: 17.5137 - val_loss: 17.6757 - val_MinusLogProbMetric: 17.6757 - lr: 1.6667e-04 - 81s/epoch - 411ms/step
Epoch 683/1000
2023-09-28 08:11:25.419 
Epoch 683/1000 
	 loss: 17.5301, MinusLogProbMetric: 17.5301, val_loss: 17.5943, val_MinusLogProbMetric: 17.5943

Epoch 683: val_loss improved from 17.60156 to 17.59430, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 81s - loss: 17.5301 - MinusLogProbMetric: 17.5301 - val_loss: 17.5943 - val_MinusLogProbMetric: 17.5943 - lr: 1.6667e-04 - 81s/epoch - 415ms/step
Epoch 684/1000
2023-09-28 08:12:46.935 
Epoch 684/1000 
	 loss: 17.5113, MinusLogProbMetric: 17.5113, val_loss: 17.6443, val_MinusLogProbMetric: 17.6443

Epoch 684: val_loss did not improve from 17.59430
196/196 - 80s - loss: 17.5113 - MinusLogProbMetric: 17.5113 - val_loss: 17.6443 - val_MinusLogProbMetric: 17.6443 - lr: 1.6667e-04 - 80s/epoch - 409ms/step
Epoch 685/1000
2023-09-28 08:14:07.312 
Epoch 685/1000 
	 loss: 17.4895, MinusLogProbMetric: 17.4895, val_loss: 17.6362, val_MinusLogProbMetric: 17.6362

Epoch 685: val_loss did not improve from 17.59430
196/196 - 80s - loss: 17.4895 - MinusLogProbMetric: 17.4895 - val_loss: 17.6362 - val_MinusLogProbMetric: 17.6362 - lr: 1.6667e-04 - 80s/epoch - 410ms/step
Epoch 686/1000
2023-09-28 08:15:28.253 
Epoch 686/1000 
	 loss: 17.5345, MinusLogProbMetric: 17.5345, val_loss: 17.6140, val_MinusLogProbMetric: 17.6140

Epoch 686: val_loss did not improve from 17.59430
196/196 - 81s - loss: 17.5345 - MinusLogProbMetric: 17.5345 - val_loss: 17.6140 - val_MinusLogProbMetric: 17.6140 - lr: 1.6667e-04 - 81s/epoch - 413ms/step
Epoch 687/1000
2023-09-28 08:16:48.063 
Epoch 687/1000 
	 loss: 17.5084, MinusLogProbMetric: 17.5084, val_loss: 17.7167, val_MinusLogProbMetric: 17.7167

Epoch 687: val_loss did not improve from 17.59430
196/196 - 80s - loss: 17.5084 - MinusLogProbMetric: 17.5084 - val_loss: 17.7167 - val_MinusLogProbMetric: 17.7167 - lr: 1.6667e-04 - 80s/epoch - 407ms/step
Epoch 688/1000
2023-09-28 08:18:08.858 
Epoch 688/1000 
	 loss: 17.5239, MinusLogProbMetric: 17.5239, val_loss: 17.8395, val_MinusLogProbMetric: 17.8395

Epoch 688: val_loss did not improve from 17.59430
196/196 - 81s - loss: 17.5239 - MinusLogProbMetric: 17.5239 - val_loss: 17.8395 - val_MinusLogProbMetric: 17.8395 - lr: 1.6667e-04 - 81s/epoch - 412ms/step
Epoch 689/1000
2023-09-28 08:19:29.324 
Epoch 689/1000 
	 loss: 17.5153, MinusLogProbMetric: 17.5153, val_loss: 17.7593, val_MinusLogProbMetric: 17.7593

Epoch 689: val_loss did not improve from 17.59430
196/196 - 80s - loss: 17.5153 - MinusLogProbMetric: 17.5153 - val_loss: 17.7593 - val_MinusLogProbMetric: 17.7593 - lr: 1.6667e-04 - 80s/epoch - 411ms/step
Epoch 690/1000
2023-09-28 08:20:49.644 
Epoch 690/1000 
	 loss: 17.5055, MinusLogProbMetric: 17.5055, val_loss: 17.6339, val_MinusLogProbMetric: 17.6339

Epoch 690: val_loss did not improve from 17.59430
196/196 - 80s - loss: 17.5055 - MinusLogProbMetric: 17.5055 - val_loss: 17.6339 - val_MinusLogProbMetric: 17.6339 - lr: 1.6667e-04 - 80s/epoch - 410ms/step
Epoch 691/1000
2023-09-28 08:22:09.959 
Epoch 691/1000 
	 loss: 17.5190, MinusLogProbMetric: 17.5190, val_loss: 17.7297, val_MinusLogProbMetric: 17.7297

Epoch 691: val_loss did not improve from 17.59430
196/196 - 80s - loss: 17.5190 - MinusLogProbMetric: 17.5190 - val_loss: 17.7297 - val_MinusLogProbMetric: 17.7297 - lr: 1.6667e-04 - 80s/epoch - 410ms/step
Epoch 692/1000
2023-09-28 08:23:30.165 
Epoch 692/1000 
	 loss: 17.5050, MinusLogProbMetric: 17.5050, val_loss: 17.6528, val_MinusLogProbMetric: 17.6528

Epoch 692: val_loss did not improve from 17.59430
196/196 - 80s - loss: 17.5050 - MinusLogProbMetric: 17.5050 - val_loss: 17.6528 - val_MinusLogProbMetric: 17.6528 - lr: 1.6667e-04 - 80s/epoch - 409ms/step
Epoch 693/1000
2023-09-28 08:24:50.209 
Epoch 693/1000 
	 loss: 17.4789, MinusLogProbMetric: 17.4789, val_loss: 17.5880, val_MinusLogProbMetric: 17.5880

Epoch 693: val_loss improved from 17.59430 to 17.58803, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 81s - loss: 17.4789 - MinusLogProbMetric: 17.4789 - val_loss: 17.5880 - val_MinusLogProbMetric: 17.5880 - lr: 1.6667e-04 - 81s/epoch - 415ms/step
Epoch 694/1000
2023-09-28 08:26:11.129 
Epoch 694/1000 
	 loss: 17.5130, MinusLogProbMetric: 17.5130, val_loss: 17.6743, val_MinusLogProbMetric: 17.6743

Epoch 694: val_loss did not improve from 17.58803
196/196 - 80s - loss: 17.5130 - MinusLogProbMetric: 17.5130 - val_loss: 17.6743 - val_MinusLogProbMetric: 17.6743 - lr: 1.6667e-04 - 80s/epoch - 406ms/step
Epoch 695/1000
2023-09-28 08:27:30.828 
Epoch 695/1000 
	 loss: 17.5020, MinusLogProbMetric: 17.5020, val_loss: 17.8663, val_MinusLogProbMetric: 17.8663

Epoch 695: val_loss did not improve from 17.58803
196/196 - 80s - loss: 17.5020 - MinusLogProbMetric: 17.5020 - val_loss: 17.8663 - val_MinusLogProbMetric: 17.8663 - lr: 1.6667e-04 - 80s/epoch - 407ms/step
Epoch 696/1000
2023-09-28 08:28:51.003 
Epoch 696/1000 
	 loss: 17.5218, MinusLogProbMetric: 17.5218, val_loss: 17.5981, val_MinusLogProbMetric: 17.5981

Epoch 696: val_loss did not improve from 17.58803
196/196 - 80s - loss: 17.5218 - MinusLogProbMetric: 17.5218 - val_loss: 17.5981 - val_MinusLogProbMetric: 17.5981 - lr: 1.6667e-04 - 80s/epoch - 409ms/step
Epoch 697/1000
2023-09-28 08:30:10.545 
Epoch 697/1000 
	 loss: 17.4955, MinusLogProbMetric: 17.4955, val_loss: 17.5990, val_MinusLogProbMetric: 17.5990

Epoch 697: val_loss did not improve from 17.58803
196/196 - 80s - loss: 17.4955 - MinusLogProbMetric: 17.4955 - val_loss: 17.5990 - val_MinusLogProbMetric: 17.5990 - lr: 1.6667e-04 - 80s/epoch - 406ms/step
Epoch 698/1000
2023-09-28 08:31:30.654 
Epoch 698/1000 
	 loss: 17.4905, MinusLogProbMetric: 17.4905, val_loss: 17.7161, val_MinusLogProbMetric: 17.7161

Epoch 698: val_loss did not improve from 17.58803
196/196 - 80s - loss: 17.4905 - MinusLogProbMetric: 17.4905 - val_loss: 17.7161 - val_MinusLogProbMetric: 17.7161 - lr: 1.6667e-04 - 80s/epoch - 409ms/step
Epoch 699/1000
2023-09-28 08:32:50.536 
Epoch 699/1000 
	 loss: 17.5034, MinusLogProbMetric: 17.5034, val_loss: 17.6668, val_MinusLogProbMetric: 17.6668

Epoch 699: val_loss did not improve from 17.58803
196/196 - 80s - loss: 17.5034 - MinusLogProbMetric: 17.5034 - val_loss: 17.6668 - val_MinusLogProbMetric: 17.6668 - lr: 1.6667e-04 - 80s/epoch - 408ms/step
Epoch 700/1000
2023-09-28 08:34:11.080 
Epoch 700/1000 
	 loss: 17.5095, MinusLogProbMetric: 17.5095, val_loss: 17.6518, val_MinusLogProbMetric: 17.6518

Epoch 700: val_loss did not improve from 17.58803
196/196 - 81s - loss: 17.5095 - MinusLogProbMetric: 17.5095 - val_loss: 17.6518 - val_MinusLogProbMetric: 17.6518 - lr: 1.6667e-04 - 81s/epoch - 411ms/step
Epoch 701/1000
2023-09-28 08:35:31.725 
Epoch 701/1000 
	 loss: 17.5047, MinusLogProbMetric: 17.5047, val_loss: 17.5043, val_MinusLogProbMetric: 17.5043

Epoch 701: val_loss improved from 17.58803 to 17.50433, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 82s - loss: 17.5047 - MinusLogProbMetric: 17.5047 - val_loss: 17.5043 - val_MinusLogProbMetric: 17.5043 - lr: 1.6667e-04 - 82s/epoch - 417ms/step
Epoch 702/1000
2023-09-28 08:36:53.032 
Epoch 702/1000 
	 loss: 17.5023, MinusLogProbMetric: 17.5023, val_loss: 17.6019, val_MinusLogProbMetric: 17.6019

Epoch 702: val_loss did not improve from 17.50433
196/196 - 80s - loss: 17.5023 - MinusLogProbMetric: 17.5023 - val_loss: 17.6019 - val_MinusLogProbMetric: 17.6019 - lr: 1.6667e-04 - 80s/epoch - 409ms/step
Epoch 703/1000
2023-09-28 08:38:13.076 
Epoch 703/1000 
	 loss: 17.4872, MinusLogProbMetric: 17.4872, val_loss: 17.7782, val_MinusLogProbMetric: 17.7782

Epoch 703: val_loss did not improve from 17.50433
196/196 - 80s - loss: 17.4872 - MinusLogProbMetric: 17.4872 - val_loss: 17.7782 - val_MinusLogProbMetric: 17.7782 - lr: 1.6667e-04 - 80s/epoch - 408ms/step
Epoch 704/1000
2023-09-28 08:39:33.191 
Epoch 704/1000 
	 loss: 17.5153, MinusLogProbMetric: 17.5153, val_loss: 18.0460, val_MinusLogProbMetric: 18.0460

Epoch 704: val_loss did not improve from 17.50433
196/196 - 80s - loss: 17.5153 - MinusLogProbMetric: 17.5153 - val_loss: 18.0460 - val_MinusLogProbMetric: 18.0460 - lr: 1.6667e-04 - 80s/epoch - 409ms/step
Epoch 705/1000
2023-09-28 08:40:53.411 
Epoch 705/1000 
	 loss: 17.5125, MinusLogProbMetric: 17.5125, val_loss: 17.6675, val_MinusLogProbMetric: 17.6675

Epoch 705: val_loss did not improve from 17.50433
196/196 - 80s - loss: 17.5125 - MinusLogProbMetric: 17.5125 - val_loss: 17.6675 - val_MinusLogProbMetric: 17.6675 - lr: 1.6667e-04 - 80s/epoch - 409ms/step
Epoch 706/1000
2023-09-28 08:42:13.796 
Epoch 706/1000 
	 loss: 17.5034, MinusLogProbMetric: 17.5034, val_loss: 17.9446, val_MinusLogProbMetric: 17.9446

Epoch 706: val_loss did not improve from 17.50433
196/196 - 80s - loss: 17.5034 - MinusLogProbMetric: 17.5034 - val_loss: 17.9446 - val_MinusLogProbMetric: 17.9446 - lr: 1.6667e-04 - 80s/epoch - 410ms/step
Epoch 707/1000
2023-09-28 08:43:33.527 
Epoch 707/1000 
	 loss: 17.4955, MinusLogProbMetric: 17.4955, val_loss: 17.7687, val_MinusLogProbMetric: 17.7687

Epoch 707: val_loss did not improve from 17.50433
196/196 - 80s - loss: 17.4955 - MinusLogProbMetric: 17.4955 - val_loss: 17.7687 - val_MinusLogProbMetric: 17.7687 - lr: 1.6667e-04 - 80s/epoch - 407ms/step
Epoch 708/1000
2023-09-28 08:44:55.126 
Epoch 708/1000 
	 loss: 17.5031, MinusLogProbMetric: 17.5031, val_loss: 17.6801, val_MinusLogProbMetric: 17.6801

Epoch 708: val_loss did not improve from 17.50433
196/196 - 82s - loss: 17.5031 - MinusLogProbMetric: 17.5031 - val_loss: 17.6801 - val_MinusLogProbMetric: 17.6801 - lr: 1.6667e-04 - 82s/epoch - 416ms/step
Epoch 709/1000
2023-09-28 08:46:17.531 
Epoch 709/1000 
	 loss: 17.4760, MinusLogProbMetric: 17.4760, val_loss: 17.6590, val_MinusLogProbMetric: 17.6590

Epoch 709: val_loss did not improve from 17.50433
196/196 - 82s - loss: 17.4760 - MinusLogProbMetric: 17.4760 - val_loss: 17.6590 - val_MinusLogProbMetric: 17.6590 - lr: 1.6667e-04 - 82s/epoch - 420ms/step
Epoch 710/1000
2023-09-28 08:47:39.709 
Epoch 710/1000 
	 loss: 17.5267, MinusLogProbMetric: 17.5267, val_loss: 17.8072, val_MinusLogProbMetric: 17.8072

Epoch 710: val_loss did not improve from 17.50433
196/196 - 82s - loss: 17.5267 - MinusLogProbMetric: 17.5267 - val_loss: 17.8072 - val_MinusLogProbMetric: 17.8072 - lr: 1.6667e-04 - 82s/epoch - 419ms/step
Epoch 711/1000
2023-09-28 08:48:53.675 
Epoch 711/1000 
	 loss: 17.5075, MinusLogProbMetric: 17.5075, val_loss: 17.7416, val_MinusLogProbMetric: 17.7416

Epoch 711: val_loss did not improve from 17.50433
196/196 - 74s - loss: 17.5075 - MinusLogProbMetric: 17.5075 - val_loss: 17.7416 - val_MinusLogProbMetric: 17.7416 - lr: 1.6667e-04 - 74s/epoch - 377ms/step
Epoch 712/1000
2023-09-28 08:50:12.022 
Epoch 712/1000 
	 loss: 17.5156, MinusLogProbMetric: 17.5156, val_loss: 17.6887, val_MinusLogProbMetric: 17.6887

Epoch 712: val_loss did not improve from 17.50433
196/196 - 78s - loss: 17.5156 - MinusLogProbMetric: 17.5156 - val_loss: 17.6887 - val_MinusLogProbMetric: 17.6887 - lr: 1.6667e-04 - 78s/epoch - 400ms/step
Epoch 713/1000
2023-09-28 08:51:35.441 
Epoch 713/1000 
	 loss: 17.5162, MinusLogProbMetric: 17.5162, val_loss: 17.6415, val_MinusLogProbMetric: 17.6415

Epoch 713: val_loss did not improve from 17.50433
196/196 - 83s - loss: 17.5162 - MinusLogProbMetric: 17.5162 - val_loss: 17.6415 - val_MinusLogProbMetric: 17.6415 - lr: 1.6667e-04 - 83s/epoch - 425ms/step
Epoch 714/1000
2023-09-28 08:52:59.595 
Epoch 714/1000 
	 loss: 17.4745, MinusLogProbMetric: 17.4745, val_loss: 17.6246, val_MinusLogProbMetric: 17.6246

Epoch 714: val_loss did not improve from 17.50433
196/196 - 84s - loss: 17.4745 - MinusLogProbMetric: 17.4745 - val_loss: 17.6246 - val_MinusLogProbMetric: 17.6246 - lr: 1.6667e-04 - 84s/epoch - 429ms/step
Epoch 715/1000
2023-09-28 08:54:23.780 
Epoch 715/1000 
	 loss: 17.5026, MinusLogProbMetric: 17.5026, val_loss: 17.5873, val_MinusLogProbMetric: 17.5873

Epoch 715: val_loss did not improve from 17.50433
196/196 - 84s - loss: 17.5026 - MinusLogProbMetric: 17.5026 - val_loss: 17.5873 - val_MinusLogProbMetric: 17.5873 - lr: 1.6667e-04 - 84s/epoch - 430ms/step
Epoch 716/1000
2023-09-28 08:55:48.528 
Epoch 716/1000 
	 loss: 17.4853, MinusLogProbMetric: 17.4853, val_loss: 17.8499, val_MinusLogProbMetric: 17.8499

Epoch 716: val_loss did not improve from 17.50433
196/196 - 85s - loss: 17.4853 - MinusLogProbMetric: 17.4853 - val_loss: 17.8499 - val_MinusLogProbMetric: 17.8499 - lr: 1.6667e-04 - 85s/epoch - 432ms/step
Epoch 717/1000
2023-09-28 08:57:12.398 
Epoch 717/1000 
	 loss: 17.4774, MinusLogProbMetric: 17.4774, val_loss: 17.8165, val_MinusLogProbMetric: 17.8165

Epoch 717: val_loss did not improve from 17.50433
196/196 - 84s - loss: 17.4774 - MinusLogProbMetric: 17.4774 - val_loss: 17.8165 - val_MinusLogProbMetric: 17.8165 - lr: 1.6667e-04 - 84s/epoch - 428ms/step
Epoch 718/1000
2023-09-28 08:58:37.030 
Epoch 718/1000 
	 loss: 17.4790, MinusLogProbMetric: 17.4790, val_loss: 17.5725, val_MinusLogProbMetric: 17.5725

Epoch 718: val_loss did not improve from 17.50433
196/196 - 85s - loss: 17.4790 - MinusLogProbMetric: 17.4790 - val_loss: 17.5725 - val_MinusLogProbMetric: 17.5725 - lr: 1.6667e-04 - 85s/epoch - 432ms/step
Epoch 719/1000
2023-09-28 09:00:01.708 
Epoch 719/1000 
	 loss: 17.5219, MinusLogProbMetric: 17.5219, val_loss: 17.6470, val_MinusLogProbMetric: 17.6470

Epoch 719: val_loss did not improve from 17.50433
196/196 - 85s - loss: 17.5219 - MinusLogProbMetric: 17.5219 - val_loss: 17.6470 - val_MinusLogProbMetric: 17.6470 - lr: 1.6667e-04 - 85s/epoch - 432ms/step
Epoch 720/1000
2023-09-28 09:01:25.807 
Epoch 720/1000 
	 loss: 17.4902, MinusLogProbMetric: 17.4902, val_loss: 17.6427, val_MinusLogProbMetric: 17.6427

Epoch 720: val_loss did not improve from 17.50433
196/196 - 84s - loss: 17.4902 - MinusLogProbMetric: 17.4902 - val_loss: 17.6427 - val_MinusLogProbMetric: 17.6427 - lr: 1.6667e-04 - 84s/epoch - 429ms/step
Epoch 721/1000
2023-09-28 09:02:50.413 
Epoch 721/1000 
	 loss: 17.4827, MinusLogProbMetric: 17.4827, val_loss: 17.5755, val_MinusLogProbMetric: 17.5755

Epoch 721: val_loss did not improve from 17.50433
196/196 - 85s - loss: 17.4827 - MinusLogProbMetric: 17.4827 - val_loss: 17.5755 - val_MinusLogProbMetric: 17.5755 - lr: 1.6667e-04 - 85s/epoch - 432ms/step
Epoch 722/1000
2023-09-28 09:04:14.490 
Epoch 722/1000 
	 loss: 17.4830, MinusLogProbMetric: 17.4830, val_loss: 17.7644, val_MinusLogProbMetric: 17.7644

Epoch 722: val_loss did not improve from 17.50433
196/196 - 84s - loss: 17.4830 - MinusLogProbMetric: 17.4830 - val_loss: 17.7644 - val_MinusLogProbMetric: 17.7644 - lr: 1.6667e-04 - 84s/epoch - 429ms/step
Epoch 723/1000
2023-09-28 09:05:37.865 
Epoch 723/1000 
	 loss: 17.5256, MinusLogProbMetric: 17.5256, val_loss: 17.6976, val_MinusLogProbMetric: 17.6976

Epoch 723: val_loss did not improve from 17.50433
196/196 - 83s - loss: 17.5256 - MinusLogProbMetric: 17.5256 - val_loss: 17.6976 - val_MinusLogProbMetric: 17.6976 - lr: 1.6667e-04 - 83s/epoch - 425ms/step
Epoch 724/1000
2023-09-28 09:07:02.122 
Epoch 724/1000 
	 loss: 17.4986, MinusLogProbMetric: 17.4986, val_loss: 17.9122, val_MinusLogProbMetric: 17.9122

Epoch 724: val_loss did not improve from 17.50433
196/196 - 84s - loss: 17.4986 - MinusLogProbMetric: 17.4986 - val_loss: 17.9122 - val_MinusLogProbMetric: 17.9122 - lr: 1.6667e-04 - 84s/epoch - 430ms/step
Epoch 725/1000
2023-09-28 09:08:26.081 
Epoch 725/1000 
	 loss: 17.4863, MinusLogProbMetric: 17.4863, val_loss: 17.5398, val_MinusLogProbMetric: 17.5398

Epoch 725: val_loss did not improve from 17.50433
196/196 - 84s - loss: 17.4863 - MinusLogProbMetric: 17.4863 - val_loss: 17.5398 - val_MinusLogProbMetric: 17.5398 - lr: 1.6667e-04 - 84s/epoch - 428ms/step
Epoch 726/1000
2023-09-28 09:09:50.473 
Epoch 726/1000 
	 loss: 17.5063, MinusLogProbMetric: 17.5063, val_loss: 18.0712, val_MinusLogProbMetric: 18.0712

Epoch 726: val_loss did not improve from 17.50433
196/196 - 84s - loss: 17.5063 - MinusLogProbMetric: 17.5063 - val_loss: 18.0712 - val_MinusLogProbMetric: 18.0712 - lr: 1.6667e-04 - 84s/epoch - 431ms/step
Epoch 727/1000
2023-09-28 09:11:14.511 
Epoch 727/1000 
	 loss: 17.5125, MinusLogProbMetric: 17.5125, val_loss: 17.6308, val_MinusLogProbMetric: 17.6308

Epoch 727: val_loss did not improve from 17.50433
196/196 - 84s - loss: 17.5125 - MinusLogProbMetric: 17.5125 - val_loss: 17.6308 - val_MinusLogProbMetric: 17.6308 - lr: 1.6667e-04 - 84s/epoch - 429ms/step
Epoch 728/1000
2023-09-28 09:12:39.399 
Epoch 728/1000 
	 loss: 17.4717, MinusLogProbMetric: 17.4717, val_loss: 17.6194, val_MinusLogProbMetric: 17.6194

Epoch 728: val_loss did not improve from 17.50433
196/196 - 85s - loss: 17.4717 - MinusLogProbMetric: 17.4717 - val_loss: 17.6194 - val_MinusLogProbMetric: 17.6194 - lr: 1.6667e-04 - 85s/epoch - 433ms/step
Epoch 729/1000
2023-09-28 09:14:03.999 
Epoch 729/1000 
	 loss: 17.4857, MinusLogProbMetric: 17.4857, val_loss: 17.5995, val_MinusLogProbMetric: 17.5995

Epoch 729: val_loss did not improve from 17.50433
196/196 - 85s - loss: 17.4857 - MinusLogProbMetric: 17.4857 - val_loss: 17.5995 - val_MinusLogProbMetric: 17.5995 - lr: 1.6667e-04 - 85s/epoch - 432ms/step
Epoch 730/1000
2023-09-28 09:15:27.828 
Epoch 730/1000 
	 loss: 17.4724, MinusLogProbMetric: 17.4724, val_loss: 17.7237, val_MinusLogProbMetric: 17.7237

Epoch 730: val_loss did not improve from 17.50433
196/196 - 84s - loss: 17.4724 - MinusLogProbMetric: 17.4724 - val_loss: 17.7237 - val_MinusLogProbMetric: 17.7237 - lr: 1.6667e-04 - 84s/epoch - 428ms/step
Epoch 731/1000
2023-09-28 09:16:52.477 
Epoch 731/1000 
	 loss: 17.4667, MinusLogProbMetric: 17.4667, val_loss: 17.6493, val_MinusLogProbMetric: 17.6493

Epoch 731: val_loss did not improve from 17.50433
196/196 - 85s - loss: 17.4667 - MinusLogProbMetric: 17.4667 - val_loss: 17.6493 - val_MinusLogProbMetric: 17.6493 - lr: 1.6667e-04 - 85s/epoch - 432ms/step
Epoch 732/1000
2023-09-28 09:18:17.520 
Epoch 732/1000 
	 loss: 17.4958, MinusLogProbMetric: 17.4958, val_loss: 17.8455, val_MinusLogProbMetric: 17.8455

Epoch 732: val_loss did not improve from 17.50433
196/196 - 85s - loss: 17.4958 - MinusLogProbMetric: 17.4958 - val_loss: 17.8455 - val_MinusLogProbMetric: 17.8455 - lr: 1.6667e-04 - 85s/epoch - 434ms/step
Epoch 733/1000
2023-09-28 09:19:41.801 
Epoch 733/1000 
	 loss: 17.4885, MinusLogProbMetric: 17.4885, val_loss: 17.6847, val_MinusLogProbMetric: 17.6847

Epoch 733: val_loss did not improve from 17.50433
196/196 - 84s - loss: 17.4885 - MinusLogProbMetric: 17.4885 - val_loss: 17.6847 - val_MinusLogProbMetric: 17.6847 - lr: 1.6667e-04 - 84s/epoch - 430ms/step
Epoch 734/1000
2023-09-28 09:21:06.732 
Epoch 734/1000 
	 loss: 17.4994, MinusLogProbMetric: 17.4994, val_loss: 17.6386, val_MinusLogProbMetric: 17.6386

Epoch 734: val_loss did not improve from 17.50433
196/196 - 85s - loss: 17.4994 - MinusLogProbMetric: 17.4994 - val_loss: 17.6386 - val_MinusLogProbMetric: 17.6386 - lr: 1.6667e-04 - 85s/epoch - 433ms/step
Epoch 735/1000
2023-09-28 09:22:31.099 
Epoch 735/1000 
	 loss: 17.4763, MinusLogProbMetric: 17.4763, val_loss: 17.5883, val_MinusLogProbMetric: 17.5883

Epoch 735: val_loss did not improve from 17.50433
196/196 - 84s - loss: 17.4763 - MinusLogProbMetric: 17.4763 - val_loss: 17.5883 - val_MinusLogProbMetric: 17.5883 - lr: 1.6667e-04 - 84s/epoch - 430ms/step
Epoch 736/1000
2023-09-28 09:23:55.613 
Epoch 736/1000 
	 loss: 17.4653, MinusLogProbMetric: 17.4653, val_loss: 17.7091, val_MinusLogProbMetric: 17.7091

Epoch 736: val_loss did not improve from 17.50433
196/196 - 85s - loss: 17.4653 - MinusLogProbMetric: 17.4653 - val_loss: 17.7091 - val_MinusLogProbMetric: 17.7091 - lr: 1.6667e-04 - 85s/epoch - 431ms/step
Epoch 737/1000
2023-09-28 09:25:19.822 
Epoch 737/1000 
	 loss: 17.5391, MinusLogProbMetric: 17.5391, val_loss: 17.7866, val_MinusLogProbMetric: 17.7866

Epoch 737: val_loss did not improve from 17.50433
196/196 - 84s - loss: 17.5391 - MinusLogProbMetric: 17.5391 - val_loss: 17.7866 - val_MinusLogProbMetric: 17.7866 - lr: 1.6667e-04 - 84s/epoch - 430ms/step
Epoch 738/1000
2023-09-28 09:26:44.200 
Epoch 738/1000 
	 loss: 17.5236, MinusLogProbMetric: 17.5236, val_loss: 17.6917, val_MinusLogProbMetric: 17.6917

Epoch 738: val_loss did not improve from 17.50433
196/196 - 84s - loss: 17.5236 - MinusLogProbMetric: 17.5236 - val_loss: 17.6917 - val_MinusLogProbMetric: 17.6917 - lr: 1.6667e-04 - 84s/epoch - 430ms/step
Epoch 739/1000
2023-09-28 09:28:08.526 
Epoch 739/1000 
	 loss: 17.4820, MinusLogProbMetric: 17.4820, val_loss: 17.5402, val_MinusLogProbMetric: 17.5402

Epoch 739: val_loss did not improve from 17.50433
196/196 - 84s - loss: 17.4820 - MinusLogProbMetric: 17.4820 - val_loss: 17.5402 - val_MinusLogProbMetric: 17.5402 - lr: 1.6667e-04 - 84s/epoch - 430ms/step
Epoch 740/1000
2023-09-28 09:29:32.751 
Epoch 740/1000 
	 loss: 17.4810, MinusLogProbMetric: 17.4810, val_loss: 17.6366, val_MinusLogProbMetric: 17.6366

Epoch 740: val_loss did not improve from 17.50433
196/196 - 84s - loss: 17.4810 - MinusLogProbMetric: 17.4810 - val_loss: 17.6366 - val_MinusLogProbMetric: 17.6366 - lr: 1.6667e-04 - 84s/epoch - 430ms/step
Epoch 741/1000
2023-09-28 09:30:56.876 
Epoch 741/1000 
	 loss: 17.4933, MinusLogProbMetric: 17.4933, val_loss: 17.5828, val_MinusLogProbMetric: 17.5828

Epoch 741: val_loss did not improve from 17.50433
196/196 - 84s - loss: 17.4933 - MinusLogProbMetric: 17.4933 - val_loss: 17.5828 - val_MinusLogProbMetric: 17.5828 - lr: 1.6667e-04 - 84s/epoch - 429ms/step
Epoch 742/1000
2023-09-28 09:32:20.190 
Epoch 742/1000 
	 loss: 17.4653, MinusLogProbMetric: 17.4653, val_loss: 17.7676, val_MinusLogProbMetric: 17.7676

Epoch 742: val_loss did not improve from 17.50433
196/196 - 83s - loss: 17.4653 - MinusLogProbMetric: 17.4653 - val_loss: 17.7676 - val_MinusLogProbMetric: 17.7676 - lr: 1.6667e-04 - 83s/epoch - 425ms/step
Epoch 743/1000
2023-09-28 09:33:42.749 
Epoch 743/1000 
	 loss: 17.4820, MinusLogProbMetric: 17.4820, val_loss: 17.5111, val_MinusLogProbMetric: 17.5111

Epoch 743: val_loss did not improve from 17.50433
196/196 - 83s - loss: 17.4820 - MinusLogProbMetric: 17.4820 - val_loss: 17.5111 - val_MinusLogProbMetric: 17.5111 - lr: 1.6667e-04 - 83s/epoch - 421ms/step
Epoch 744/1000
2023-09-28 09:35:02.202 
Epoch 744/1000 
	 loss: 17.4804, MinusLogProbMetric: 17.4804, val_loss: 17.6838, val_MinusLogProbMetric: 17.6838

Epoch 744: val_loss did not improve from 17.50433
196/196 - 79s - loss: 17.4804 - MinusLogProbMetric: 17.4804 - val_loss: 17.6838 - val_MinusLogProbMetric: 17.6838 - lr: 1.6667e-04 - 79s/epoch - 405ms/step
Epoch 745/1000
2023-09-28 09:36:22.345 
Epoch 745/1000 
	 loss: 17.4759, MinusLogProbMetric: 17.4759, val_loss: 17.6500, val_MinusLogProbMetric: 17.6500

Epoch 745: val_loss did not improve from 17.50433
196/196 - 80s - loss: 17.4759 - MinusLogProbMetric: 17.4759 - val_loss: 17.6500 - val_MinusLogProbMetric: 17.6500 - lr: 1.6667e-04 - 80s/epoch - 409ms/step
Epoch 746/1000
2023-09-28 09:37:43.456 
Epoch 746/1000 
	 loss: 17.4755, MinusLogProbMetric: 17.4755, val_loss: 17.5943, val_MinusLogProbMetric: 17.5943

Epoch 746: val_loss did not improve from 17.50433
196/196 - 81s - loss: 17.4755 - MinusLogProbMetric: 17.4755 - val_loss: 17.5943 - val_MinusLogProbMetric: 17.5943 - lr: 1.6667e-04 - 81s/epoch - 414ms/step
Epoch 747/1000
2023-09-28 09:39:05.188 
Epoch 747/1000 
	 loss: 17.4894, MinusLogProbMetric: 17.4894, val_loss: 17.5918, val_MinusLogProbMetric: 17.5918

Epoch 747: val_loss did not improve from 17.50433
196/196 - 82s - loss: 17.4894 - MinusLogProbMetric: 17.4894 - val_loss: 17.5918 - val_MinusLogProbMetric: 17.5918 - lr: 1.6667e-04 - 82s/epoch - 417ms/step
Epoch 748/1000
2023-09-28 09:40:28.877 
Epoch 748/1000 
	 loss: 17.5061, MinusLogProbMetric: 17.5061, val_loss: 17.6001, val_MinusLogProbMetric: 17.6001

Epoch 748: val_loss did not improve from 17.50433
196/196 - 84s - loss: 17.5061 - MinusLogProbMetric: 17.5061 - val_loss: 17.6001 - val_MinusLogProbMetric: 17.6001 - lr: 1.6667e-04 - 84s/epoch - 427ms/step
Epoch 749/1000
2023-09-28 09:41:52.349 
Epoch 749/1000 
	 loss: 17.4836, MinusLogProbMetric: 17.4836, val_loss: 17.6938, val_MinusLogProbMetric: 17.6938

Epoch 749: val_loss did not improve from 17.50433
196/196 - 83s - loss: 17.4836 - MinusLogProbMetric: 17.4836 - val_loss: 17.6938 - val_MinusLogProbMetric: 17.6938 - lr: 1.6667e-04 - 83s/epoch - 426ms/step
Epoch 750/1000
2023-09-28 09:43:16.312 
Epoch 750/1000 
	 loss: 17.4728, MinusLogProbMetric: 17.4728, val_loss: 17.7387, val_MinusLogProbMetric: 17.7387

Epoch 750: val_loss did not improve from 17.50433
196/196 - 84s - loss: 17.4728 - MinusLogProbMetric: 17.4728 - val_loss: 17.7387 - val_MinusLogProbMetric: 17.7387 - lr: 1.6667e-04 - 84s/epoch - 428ms/step
Epoch 751/1000
2023-09-28 09:44:39.277 
Epoch 751/1000 
	 loss: 17.4779, MinusLogProbMetric: 17.4779, val_loss: 17.5285, val_MinusLogProbMetric: 17.5285

Epoch 751: val_loss did not improve from 17.50433
196/196 - 83s - loss: 17.4779 - MinusLogProbMetric: 17.4779 - val_loss: 17.5285 - val_MinusLogProbMetric: 17.5285 - lr: 1.6667e-04 - 83s/epoch - 423ms/step
Epoch 752/1000
2023-09-28 09:46:02.055 
Epoch 752/1000 
	 loss: 17.3410, MinusLogProbMetric: 17.3410, val_loss: 17.4707, val_MinusLogProbMetric: 17.4707

Epoch 752: val_loss improved from 17.50433 to 17.47067, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 84s - loss: 17.3410 - MinusLogProbMetric: 17.3410 - val_loss: 17.4707 - val_MinusLogProbMetric: 17.4707 - lr: 8.3333e-05 - 84s/epoch - 429ms/step
Epoch 753/1000
2023-09-28 09:47:26.518 
Epoch 753/1000 
	 loss: 17.3318, MinusLogProbMetric: 17.3318, val_loss: 17.4604, val_MinusLogProbMetric: 17.4604

Epoch 753: val_loss improved from 17.47067 to 17.46036, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 84s - loss: 17.3318 - MinusLogProbMetric: 17.3318 - val_loss: 17.4604 - val_MinusLogProbMetric: 17.4604 - lr: 8.3333e-05 - 84s/epoch - 430ms/step
Epoch 754/1000
2023-09-28 09:48:50.075 
Epoch 754/1000 
	 loss: 17.3361, MinusLogProbMetric: 17.3361, val_loss: 17.4754, val_MinusLogProbMetric: 17.4754

Epoch 754: val_loss did not improve from 17.46036
196/196 - 82s - loss: 17.3361 - MinusLogProbMetric: 17.3361 - val_loss: 17.4754 - val_MinusLogProbMetric: 17.4754 - lr: 8.3333e-05 - 82s/epoch - 420ms/step
Epoch 755/1000
2023-09-28 09:50:13.043 
Epoch 755/1000 
	 loss: 17.3362, MinusLogProbMetric: 17.3362, val_loss: 17.4524, val_MinusLogProbMetric: 17.4524

Epoch 755: val_loss improved from 17.46036 to 17.45241, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 84s - loss: 17.3362 - MinusLogProbMetric: 17.3362 - val_loss: 17.4524 - val_MinusLogProbMetric: 17.4524 - lr: 8.3333e-05 - 84s/epoch - 430ms/step
Epoch 756/1000
2023-09-28 09:51:36.341 
Epoch 756/1000 
	 loss: 17.3495, MinusLogProbMetric: 17.3495, val_loss: 17.5232, val_MinusLogProbMetric: 17.5232

Epoch 756: val_loss did not improve from 17.45241
196/196 - 82s - loss: 17.3495 - MinusLogProbMetric: 17.3495 - val_loss: 17.5232 - val_MinusLogProbMetric: 17.5232 - lr: 8.3333e-05 - 82s/epoch - 418ms/step
Epoch 757/1000
2023-09-28 09:52:59.733 
Epoch 757/1000 
	 loss: 17.3333, MinusLogProbMetric: 17.3333, val_loss: 17.4618, val_MinusLogProbMetric: 17.4618

Epoch 757: val_loss did not improve from 17.45241
196/196 - 83s - loss: 17.3333 - MinusLogProbMetric: 17.3333 - val_loss: 17.4618 - val_MinusLogProbMetric: 17.4618 - lr: 8.3333e-05 - 83s/epoch - 425ms/step
Epoch 758/1000
2023-09-28 09:54:22.556 
Epoch 758/1000 
	 loss: 17.3493, MinusLogProbMetric: 17.3493, val_loss: 17.4641, val_MinusLogProbMetric: 17.4641

Epoch 758: val_loss did not improve from 17.45241
196/196 - 83s - loss: 17.3493 - MinusLogProbMetric: 17.3493 - val_loss: 17.4641 - val_MinusLogProbMetric: 17.4641 - lr: 8.3333e-05 - 83s/epoch - 423ms/step
Epoch 759/1000
2023-09-28 09:55:45.887 
Epoch 759/1000 
	 loss: 17.3356, MinusLogProbMetric: 17.3356, val_loss: 17.4448, val_MinusLogProbMetric: 17.4448

Epoch 759: val_loss improved from 17.45241 to 17.44479, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 85s - loss: 17.3356 - MinusLogProbMetric: 17.3356 - val_loss: 17.4448 - val_MinusLogProbMetric: 17.4448 - lr: 8.3333e-05 - 85s/epoch - 432ms/step
Epoch 760/1000
2023-09-28 09:57:10.904 
Epoch 760/1000 
	 loss: 17.3590, MinusLogProbMetric: 17.3590, val_loss: 17.4402, val_MinusLogProbMetric: 17.4402

Epoch 760: val_loss improved from 17.44479 to 17.44023, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 85s - loss: 17.3590 - MinusLogProbMetric: 17.3590 - val_loss: 17.4402 - val_MinusLogProbMetric: 17.4402 - lr: 8.3333e-05 - 85s/epoch - 433ms/step
Epoch 761/1000
2023-09-28 09:58:35.887 
Epoch 761/1000 
	 loss: 17.3493, MinusLogProbMetric: 17.3493, val_loss: 17.4753, val_MinusLogProbMetric: 17.4753

Epoch 761: val_loss did not improve from 17.44023
196/196 - 84s - loss: 17.3493 - MinusLogProbMetric: 17.3493 - val_loss: 17.4753 - val_MinusLogProbMetric: 17.4753 - lr: 8.3333e-05 - 84s/epoch - 428ms/step
Epoch 762/1000
2023-09-28 09:59:59.889 
Epoch 762/1000 
	 loss: 17.3504, MinusLogProbMetric: 17.3504, val_loss: 17.4698, val_MinusLogProbMetric: 17.4698

Epoch 762: val_loss did not improve from 17.44023
196/196 - 84s - loss: 17.3504 - MinusLogProbMetric: 17.3504 - val_loss: 17.4698 - val_MinusLogProbMetric: 17.4698 - lr: 8.3333e-05 - 84s/epoch - 429ms/step
Epoch 763/1000
2023-09-28 10:01:23.500 
Epoch 763/1000 
	 loss: 17.3616, MinusLogProbMetric: 17.3616, val_loss: 17.4840, val_MinusLogProbMetric: 17.4840

Epoch 763: val_loss did not improve from 17.44023
196/196 - 84s - loss: 17.3616 - MinusLogProbMetric: 17.3616 - val_loss: 17.4840 - val_MinusLogProbMetric: 17.4840 - lr: 8.3333e-05 - 84s/epoch - 427ms/step
Epoch 764/1000
2023-09-28 10:02:47.205 
Epoch 764/1000 
	 loss: 17.3384, MinusLogProbMetric: 17.3384, val_loss: 17.4404, val_MinusLogProbMetric: 17.4404

Epoch 764: val_loss did not improve from 17.44023
196/196 - 84s - loss: 17.3384 - MinusLogProbMetric: 17.3384 - val_loss: 17.4404 - val_MinusLogProbMetric: 17.4404 - lr: 8.3333e-05 - 84s/epoch - 427ms/step
Epoch 765/1000
2023-09-28 10:04:10.756 
Epoch 765/1000 
	 loss: 17.3397, MinusLogProbMetric: 17.3397, val_loss: 17.4622, val_MinusLogProbMetric: 17.4622

Epoch 765: val_loss did not improve from 17.44023
196/196 - 84s - loss: 17.3397 - MinusLogProbMetric: 17.3397 - val_loss: 17.4622 - val_MinusLogProbMetric: 17.4622 - lr: 8.3333e-05 - 84s/epoch - 426ms/step
Epoch 766/1000
2023-09-28 10:05:34.135 
Epoch 766/1000 
	 loss: 17.3494, MinusLogProbMetric: 17.3494, val_loss: 17.4607, val_MinusLogProbMetric: 17.4607

Epoch 766: val_loss did not improve from 17.44023
196/196 - 83s - loss: 17.3494 - MinusLogProbMetric: 17.3494 - val_loss: 17.4607 - val_MinusLogProbMetric: 17.4607 - lr: 8.3333e-05 - 83s/epoch - 425ms/step
Epoch 767/1000
2023-09-28 10:06:56.907 
Epoch 767/1000 
	 loss: 17.3351, MinusLogProbMetric: 17.3351, val_loss: 17.6676, val_MinusLogProbMetric: 17.6676

Epoch 767: val_loss did not improve from 17.44023
196/196 - 83s - loss: 17.3351 - MinusLogProbMetric: 17.3351 - val_loss: 17.6676 - val_MinusLogProbMetric: 17.6676 - lr: 8.3333e-05 - 83s/epoch - 422ms/step
Epoch 768/1000
2023-09-28 10:08:18.803 
Epoch 768/1000 
	 loss: 17.3359, MinusLogProbMetric: 17.3359, val_loss: 17.4174, val_MinusLogProbMetric: 17.4174

Epoch 768: val_loss improved from 17.44023 to 17.41740, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 83s - loss: 17.3359 - MinusLogProbMetric: 17.3359 - val_loss: 17.4174 - val_MinusLogProbMetric: 17.4174 - lr: 8.3333e-05 - 83s/epoch - 425ms/step
Epoch 769/1000
2023-09-28 10:09:40.657 
Epoch 769/1000 
	 loss: 17.3293, MinusLogProbMetric: 17.3293, val_loss: 17.4628, val_MinusLogProbMetric: 17.4628

Epoch 769: val_loss did not improve from 17.41740
196/196 - 81s - loss: 17.3293 - MinusLogProbMetric: 17.3293 - val_loss: 17.4628 - val_MinusLogProbMetric: 17.4628 - lr: 8.3333e-05 - 81s/epoch - 411ms/step
Epoch 770/1000
2023-09-28 10:11:01.960 
Epoch 770/1000 
	 loss: 17.3308, MinusLogProbMetric: 17.3308, val_loss: 17.5177, val_MinusLogProbMetric: 17.5177

Epoch 770: val_loss did not improve from 17.41740
196/196 - 81s - loss: 17.3308 - MinusLogProbMetric: 17.3308 - val_loss: 17.5177 - val_MinusLogProbMetric: 17.5177 - lr: 8.3333e-05 - 81s/epoch - 415ms/step
Epoch 771/1000
2023-09-28 10:12:24.600 
Epoch 771/1000 
	 loss: 17.3344, MinusLogProbMetric: 17.3344, val_loss: 17.4456, val_MinusLogProbMetric: 17.4456

Epoch 771: val_loss did not improve from 17.41740
196/196 - 83s - loss: 17.3344 - MinusLogProbMetric: 17.3344 - val_loss: 17.4456 - val_MinusLogProbMetric: 17.4456 - lr: 8.3333e-05 - 83s/epoch - 422ms/step
Epoch 772/1000
2023-09-28 10:13:47.999 
Epoch 772/1000 
	 loss: 17.3341, MinusLogProbMetric: 17.3341, val_loss: 17.4473, val_MinusLogProbMetric: 17.4473

Epoch 772: val_loss did not improve from 17.41740
196/196 - 83s - loss: 17.3341 - MinusLogProbMetric: 17.3341 - val_loss: 17.4473 - val_MinusLogProbMetric: 17.4473 - lr: 8.3333e-05 - 83s/epoch - 425ms/step
Epoch 773/1000
2023-09-28 10:15:11.143 
Epoch 773/1000 
	 loss: 17.3379, MinusLogProbMetric: 17.3379, val_loss: 17.4692, val_MinusLogProbMetric: 17.4692

Epoch 773: val_loss did not improve from 17.41740
196/196 - 83s - loss: 17.3379 - MinusLogProbMetric: 17.3379 - val_loss: 17.4692 - val_MinusLogProbMetric: 17.4692 - lr: 8.3333e-05 - 83s/epoch - 424ms/step
Epoch 774/1000
2023-09-28 10:16:34.921 
Epoch 774/1000 
	 loss: 17.3300, MinusLogProbMetric: 17.3300, val_loss: 17.4508, val_MinusLogProbMetric: 17.4508

Epoch 774: val_loss did not improve from 17.41740
196/196 - 84s - loss: 17.3300 - MinusLogProbMetric: 17.3300 - val_loss: 17.4508 - val_MinusLogProbMetric: 17.4508 - lr: 8.3333e-05 - 84s/epoch - 427ms/step
Epoch 775/1000
2023-09-28 10:17:58.950 
Epoch 775/1000 
	 loss: 17.3272, MinusLogProbMetric: 17.3272, val_loss: 17.4633, val_MinusLogProbMetric: 17.4633

Epoch 775: val_loss did not improve from 17.41740
196/196 - 84s - loss: 17.3272 - MinusLogProbMetric: 17.3272 - val_loss: 17.4633 - val_MinusLogProbMetric: 17.4633 - lr: 8.3333e-05 - 84s/epoch - 429ms/step
Epoch 776/1000
2023-09-28 10:19:20.546 
Epoch 776/1000 
	 loss: 17.3246, MinusLogProbMetric: 17.3246, val_loss: 17.4630, val_MinusLogProbMetric: 17.4630

Epoch 776: val_loss did not improve from 17.41740
196/196 - 82s - loss: 17.3246 - MinusLogProbMetric: 17.3246 - val_loss: 17.4630 - val_MinusLogProbMetric: 17.4630 - lr: 8.3333e-05 - 82s/epoch - 416ms/step
Epoch 777/1000
2023-09-28 10:20:41.036 
Epoch 777/1000 
	 loss: 17.3337, MinusLogProbMetric: 17.3337, val_loss: 17.5934, val_MinusLogProbMetric: 17.5934

Epoch 777: val_loss did not improve from 17.41740
196/196 - 80s - loss: 17.3337 - MinusLogProbMetric: 17.3337 - val_loss: 17.5934 - val_MinusLogProbMetric: 17.5934 - lr: 8.3333e-05 - 80s/epoch - 411ms/step
Epoch 778/1000
2023-09-28 10:22:02.281 
Epoch 778/1000 
	 loss: 17.3453, MinusLogProbMetric: 17.3453, val_loss: 17.4562, val_MinusLogProbMetric: 17.4562

Epoch 778: val_loss did not improve from 17.41740
196/196 - 81s - loss: 17.3453 - MinusLogProbMetric: 17.3453 - val_loss: 17.4562 - val_MinusLogProbMetric: 17.4562 - lr: 8.3333e-05 - 81s/epoch - 414ms/step
Epoch 779/1000
2023-09-28 10:23:23.560 
Epoch 779/1000 
	 loss: 17.3320, MinusLogProbMetric: 17.3320, val_loss: 17.4644, val_MinusLogProbMetric: 17.4644

Epoch 779: val_loss did not improve from 17.41740
196/196 - 81s - loss: 17.3320 - MinusLogProbMetric: 17.3320 - val_loss: 17.4644 - val_MinusLogProbMetric: 17.4644 - lr: 8.3333e-05 - 81s/epoch - 415ms/step
Epoch 780/1000
2023-09-28 10:24:44.126 
Epoch 780/1000 
	 loss: 17.3335, MinusLogProbMetric: 17.3335, val_loss: 17.4778, val_MinusLogProbMetric: 17.4778

Epoch 780: val_loss did not improve from 17.41740
196/196 - 81s - loss: 17.3335 - MinusLogProbMetric: 17.3335 - val_loss: 17.4778 - val_MinusLogProbMetric: 17.4778 - lr: 8.3333e-05 - 81s/epoch - 411ms/step
Epoch 781/1000
2023-09-28 10:26:04.093 
Epoch 781/1000 
	 loss: 17.3320, MinusLogProbMetric: 17.3320, val_loss: 17.4473, val_MinusLogProbMetric: 17.4473

Epoch 781: val_loss did not improve from 17.41740
196/196 - 80s - loss: 17.3320 - MinusLogProbMetric: 17.3320 - val_loss: 17.4473 - val_MinusLogProbMetric: 17.4473 - lr: 8.3333e-05 - 80s/epoch - 408ms/step
Epoch 782/1000
2023-09-28 10:27:25.332 
Epoch 782/1000 
	 loss: 17.3380, MinusLogProbMetric: 17.3380, val_loss: 17.4479, val_MinusLogProbMetric: 17.4479

Epoch 782: val_loss did not improve from 17.41740
196/196 - 81s - loss: 17.3380 - MinusLogProbMetric: 17.3380 - val_loss: 17.4479 - val_MinusLogProbMetric: 17.4479 - lr: 8.3333e-05 - 81s/epoch - 414ms/step
Epoch 783/1000
2023-09-28 10:28:46.978 
Epoch 783/1000 
	 loss: 17.3249, MinusLogProbMetric: 17.3249, val_loss: 17.4622, val_MinusLogProbMetric: 17.4622

Epoch 783: val_loss did not improve from 17.41740
196/196 - 82s - loss: 17.3249 - MinusLogProbMetric: 17.3249 - val_loss: 17.4622 - val_MinusLogProbMetric: 17.4622 - lr: 8.3333e-05 - 82s/epoch - 417ms/step
Epoch 784/1000
2023-09-28 10:30:10.237 
Epoch 784/1000 
	 loss: 17.3266, MinusLogProbMetric: 17.3266, val_loss: 17.4783, val_MinusLogProbMetric: 17.4783

Epoch 784: val_loss did not improve from 17.41740
196/196 - 83s - loss: 17.3266 - MinusLogProbMetric: 17.3266 - val_loss: 17.4783 - val_MinusLogProbMetric: 17.4783 - lr: 8.3333e-05 - 83s/epoch - 425ms/step
Epoch 785/1000
2023-09-28 10:31:32.951 
Epoch 785/1000 
	 loss: 17.3487, MinusLogProbMetric: 17.3487, val_loss: 17.4527, val_MinusLogProbMetric: 17.4527

Epoch 785: val_loss did not improve from 17.41740
196/196 - 83s - loss: 17.3487 - MinusLogProbMetric: 17.3487 - val_loss: 17.4527 - val_MinusLogProbMetric: 17.4527 - lr: 8.3333e-05 - 83s/epoch - 422ms/step
Epoch 786/1000
2023-09-28 10:32:55.234 
Epoch 786/1000 
	 loss: 17.3260, MinusLogProbMetric: 17.3260, val_loss: 17.4428, val_MinusLogProbMetric: 17.4428

Epoch 786: val_loss did not improve from 17.41740
196/196 - 82s - loss: 17.3260 - MinusLogProbMetric: 17.3260 - val_loss: 17.4428 - val_MinusLogProbMetric: 17.4428 - lr: 8.3333e-05 - 82s/epoch - 420ms/step
Epoch 787/1000
2023-09-28 10:34:16.771 
Epoch 787/1000 
	 loss: 17.3270, MinusLogProbMetric: 17.3270, val_loss: 17.4705, val_MinusLogProbMetric: 17.4705

Epoch 787: val_loss did not improve from 17.41740
196/196 - 82s - loss: 17.3270 - MinusLogProbMetric: 17.3270 - val_loss: 17.4705 - val_MinusLogProbMetric: 17.4705 - lr: 8.3333e-05 - 82s/epoch - 416ms/step
Epoch 788/1000
2023-09-28 10:35:37.340 
Epoch 788/1000 
	 loss: 17.3311, MinusLogProbMetric: 17.3311, val_loss: 17.4361, val_MinusLogProbMetric: 17.4361

Epoch 788: val_loss did not improve from 17.41740
196/196 - 81s - loss: 17.3311 - MinusLogProbMetric: 17.3311 - val_loss: 17.4361 - val_MinusLogProbMetric: 17.4361 - lr: 8.3333e-05 - 81s/epoch - 411ms/step
Epoch 789/1000
2023-09-28 10:37:01.791 
Epoch 789/1000 
	 loss: 17.3284, MinusLogProbMetric: 17.3284, val_loss: 17.5276, val_MinusLogProbMetric: 17.5276

Epoch 789: val_loss did not improve from 17.41740
196/196 - 84s - loss: 17.3284 - MinusLogProbMetric: 17.3284 - val_loss: 17.5276 - val_MinusLogProbMetric: 17.5276 - lr: 8.3333e-05 - 84s/epoch - 431ms/step
Epoch 790/1000
2023-09-28 10:38:26.024 
Epoch 790/1000 
	 loss: 17.3332, MinusLogProbMetric: 17.3332, val_loss: 17.4767, val_MinusLogProbMetric: 17.4767

Epoch 790: val_loss did not improve from 17.41740
196/196 - 84s - loss: 17.3332 - MinusLogProbMetric: 17.3332 - val_loss: 17.4767 - val_MinusLogProbMetric: 17.4767 - lr: 8.3333e-05 - 84s/epoch - 430ms/step
Epoch 791/1000
2023-09-28 10:39:48.364 
Epoch 791/1000 
	 loss: 17.3421, MinusLogProbMetric: 17.3421, val_loss: 17.4479, val_MinusLogProbMetric: 17.4479

Epoch 791: val_loss did not improve from 17.41740
196/196 - 82s - loss: 17.3421 - MinusLogProbMetric: 17.3421 - val_loss: 17.4479 - val_MinusLogProbMetric: 17.4479 - lr: 8.3333e-05 - 82s/epoch - 420ms/step
Epoch 792/1000
2023-09-28 10:41:30.179 
Epoch 792/1000 
	 loss: 17.3279, MinusLogProbMetric: 17.3279, val_loss: 17.4627, val_MinusLogProbMetric: 17.4627

Epoch 792: val_loss did not improve from 17.41740
196/196 - 102s - loss: 17.3279 - MinusLogProbMetric: 17.3279 - val_loss: 17.4627 - val_MinusLogProbMetric: 17.4627 - lr: 8.3333e-05 - 102s/epoch - 519ms/step
Epoch 793/1000
2023-09-28 10:43:10.746 
Epoch 793/1000 
	 loss: 17.3213, MinusLogProbMetric: 17.3213, val_loss: 17.4812, val_MinusLogProbMetric: 17.4812

Epoch 793: val_loss did not improve from 17.41740
196/196 - 101s - loss: 17.3213 - MinusLogProbMetric: 17.3213 - val_loss: 17.4812 - val_MinusLogProbMetric: 17.4812 - lr: 8.3333e-05 - 101s/epoch - 513ms/step
Epoch 794/1000
2023-09-28 10:44:51.893 
Epoch 794/1000 
	 loss: 17.3309, MinusLogProbMetric: 17.3309, val_loss: 17.4899, val_MinusLogProbMetric: 17.4899

Epoch 794: val_loss did not improve from 17.41740
196/196 - 101s - loss: 17.3309 - MinusLogProbMetric: 17.3309 - val_loss: 17.4899 - val_MinusLogProbMetric: 17.4899 - lr: 8.3333e-05 - 101s/epoch - 516ms/step
Epoch 795/1000
2023-09-28 10:46:26.658 
Epoch 795/1000 
	 loss: 17.3229, MinusLogProbMetric: 17.3229, val_loss: 17.4611, val_MinusLogProbMetric: 17.4611

Epoch 795: val_loss did not improve from 17.41740
196/196 - 95s - loss: 17.3229 - MinusLogProbMetric: 17.3229 - val_loss: 17.4611 - val_MinusLogProbMetric: 17.4611 - lr: 8.3333e-05 - 95s/epoch - 483ms/step
Epoch 796/1000
2023-09-28 10:48:02.972 
Epoch 796/1000 
	 loss: 17.3210, MinusLogProbMetric: 17.3210, val_loss: 17.4468, val_MinusLogProbMetric: 17.4468

Epoch 796: val_loss did not improve from 17.41740
196/196 - 96s - loss: 17.3210 - MinusLogProbMetric: 17.3210 - val_loss: 17.4468 - val_MinusLogProbMetric: 17.4468 - lr: 8.3333e-05 - 96s/epoch - 491ms/step
Epoch 797/1000
2023-09-28 10:49:38.431 
Epoch 797/1000 
	 loss: 17.3249, MinusLogProbMetric: 17.3249, val_loss: 17.4474, val_MinusLogProbMetric: 17.4474

Epoch 797: val_loss did not improve from 17.41740
196/196 - 95s - loss: 17.3249 - MinusLogProbMetric: 17.3249 - val_loss: 17.4474 - val_MinusLogProbMetric: 17.4474 - lr: 8.3333e-05 - 95s/epoch - 487ms/step
Epoch 798/1000
2023-09-28 10:51:17.011 
Epoch 798/1000 
	 loss: 17.3197, MinusLogProbMetric: 17.3197, val_loss: 17.5444, val_MinusLogProbMetric: 17.5444

Epoch 798: val_loss did not improve from 17.41740
196/196 - 99s - loss: 17.3197 - MinusLogProbMetric: 17.3197 - val_loss: 17.5444 - val_MinusLogProbMetric: 17.5444 - lr: 8.3333e-05 - 99s/epoch - 503ms/step
Epoch 799/1000
2023-09-28 10:52:55.585 
Epoch 799/1000 
	 loss: 17.3276, MinusLogProbMetric: 17.3276, val_loss: 17.4490, val_MinusLogProbMetric: 17.4490

Epoch 799: val_loss did not improve from 17.41740
196/196 - 99s - loss: 17.3276 - MinusLogProbMetric: 17.3276 - val_loss: 17.4490 - val_MinusLogProbMetric: 17.4490 - lr: 8.3333e-05 - 99s/epoch - 503ms/step
Epoch 800/1000
2023-09-28 10:54:32.318 
Epoch 800/1000 
	 loss: 17.3505, MinusLogProbMetric: 17.3505, val_loss: 17.4766, val_MinusLogProbMetric: 17.4766

Epoch 800: val_loss did not improve from 17.41740
196/196 - 97s - loss: 17.3505 - MinusLogProbMetric: 17.3505 - val_loss: 17.4766 - val_MinusLogProbMetric: 17.4766 - lr: 8.3333e-05 - 97s/epoch - 493ms/step
Epoch 801/1000
2023-09-28 10:56:06.644 
Epoch 801/1000 
	 loss: 17.3310, MinusLogProbMetric: 17.3310, val_loss: 17.4672, val_MinusLogProbMetric: 17.4672

Epoch 801: val_loss did not improve from 17.41740
196/196 - 94s - loss: 17.3310 - MinusLogProbMetric: 17.3310 - val_loss: 17.4672 - val_MinusLogProbMetric: 17.4672 - lr: 8.3333e-05 - 94s/epoch - 481ms/step
Epoch 802/1000
2023-09-28 10:57:42.980 
Epoch 802/1000 
	 loss: 17.3255, MinusLogProbMetric: 17.3255, val_loss: 17.4354, val_MinusLogProbMetric: 17.4354

Epoch 802: val_loss did not improve from 17.41740
196/196 - 96s - loss: 17.3255 - MinusLogProbMetric: 17.3255 - val_loss: 17.4354 - val_MinusLogProbMetric: 17.4354 - lr: 8.3333e-05 - 96s/epoch - 491ms/step
Epoch 803/1000
2023-09-28 10:59:16.570 
Epoch 803/1000 
	 loss: 17.3327, MinusLogProbMetric: 17.3327, val_loss: 17.4126, val_MinusLogProbMetric: 17.4126

Epoch 803: val_loss improved from 17.41740 to 17.41260, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 95s - loss: 17.3327 - MinusLogProbMetric: 17.3327 - val_loss: 17.4126 - val_MinusLogProbMetric: 17.4126 - lr: 8.3333e-05 - 95s/epoch - 484ms/step
Epoch 804/1000
2023-09-28 11:00:52.835 
Epoch 804/1000 
	 loss: 17.3261, MinusLogProbMetric: 17.3261, val_loss: 17.5084, val_MinusLogProbMetric: 17.5084

Epoch 804: val_loss did not improve from 17.41260
196/196 - 95s - loss: 17.3261 - MinusLogProbMetric: 17.3261 - val_loss: 17.5084 - val_MinusLogProbMetric: 17.5084 - lr: 8.3333e-05 - 95s/epoch - 484ms/step
Epoch 805/1000
2023-09-28 11:02:29.075 
Epoch 805/1000 
	 loss: 17.3396, MinusLogProbMetric: 17.3396, val_loss: 17.4659, val_MinusLogProbMetric: 17.4659

Epoch 805: val_loss did not improve from 17.41260
196/196 - 96s - loss: 17.3396 - MinusLogProbMetric: 17.3396 - val_loss: 17.4659 - val_MinusLogProbMetric: 17.4659 - lr: 8.3333e-05 - 96s/epoch - 491ms/step
Epoch 806/1000
2023-09-28 11:04:04.537 
Epoch 806/1000 
	 loss: 17.3151, MinusLogProbMetric: 17.3151, val_loss: 17.4775, val_MinusLogProbMetric: 17.4775

Epoch 806: val_loss did not improve from 17.41260
196/196 - 95s - loss: 17.3151 - MinusLogProbMetric: 17.3151 - val_loss: 17.4775 - val_MinusLogProbMetric: 17.4775 - lr: 8.3333e-05 - 95s/epoch - 487ms/step
Epoch 807/1000
2023-09-28 11:05:40.316 
Epoch 807/1000 
	 loss: 17.3197, MinusLogProbMetric: 17.3197, val_loss: 17.4446, val_MinusLogProbMetric: 17.4446

Epoch 807: val_loss did not improve from 17.41260
196/196 - 96s - loss: 17.3197 - MinusLogProbMetric: 17.3197 - val_loss: 17.4446 - val_MinusLogProbMetric: 17.4446 - lr: 8.3333e-05 - 96s/epoch - 489ms/step
Epoch 808/1000
2023-09-28 11:07:19.152 
Epoch 808/1000 
	 loss: 17.3197, MinusLogProbMetric: 17.3197, val_loss: 17.5004, val_MinusLogProbMetric: 17.5004

Epoch 808: val_loss did not improve from 17.41260
196/196 - 99s - loss: 17.3197 - MinusLogProbMetric: 17.3197 - val_loss: 17.5004 - val_MinusLogProbMetric: 17.5004 - lr: 8.3333e-05 - 99s/epoch - 504ms/step
Epoch 809/1000
2023-09-28 11:08:53.856 
Epoch 809/1000 
	 loss: 17.3328, MinusLogProbMetric: 17.3328, val_loss: 17.5706, val_MinusLogProbMetric: 17.5706

Epoch 809: val_loss did not improve from 17.41260
196/196 - 95s - loss: 17.3328 - MinusLogProbMetric: 17.3328 - val_loss: 17.5706 - val_MinusLogProbMetric: 17.5706 - lr: 8.3333e-05 - 95s/epoch - 483ms/step
Epoch 810/1000
2023-09-28 11:10:28.950 
Epoch 810/1000 
	 loss: 17.3269, MinusLogProbMetric: 17.3269, val_loss: 17.4536, val_MinusLogProbMetric: 17.4536

Epoch 810: val_loss did not improve from 17.41260
196/196 - 95s - loss: 17.3269 - MinusLogProbMetric: 17.3269 - val_loss: 17.4536 - val_MinusLogProbMetric: 17.4536 - lr: 8.3333e-05 - 95s/epoch - 485ms/step
Epoch 811/1000
2023-09-28 11:12:02.974 
Epoch 811/1000 
	 loss: 17.3321, MinusLogProbMetric: 17.3321, val_loss: 17.5009, val_MinusLogProbMetric: 17.5009

Epoch 811: val_loss did not improve from 17.41260
196/196 - 94s - loss: 17.3321 - MinusLogProbMetric: 17.3321 - val_loss: 17.5009 - val_MinusLogProbMetric: 17.5009 - lr: 8.3333e-05 - 94s/epoch - 480ms/step
Epoch 812/1000
2023-09-28 11:13:43.156 
Epoch 812/1000 
	 loss: 17.3287, MinusLogProbMetric: 17.3287, val_loss: 17.5215, val_MinusLogProbMetric: 17.5215

Epoch 812: val_loss did not improve from 17.41260
196/196 - 100s - loss: 17.3287 - MinusLogProbMetric: 17.3287 - val_loss: 17.5215 - val_MinusLogProbMetric: 17.5215 - lr: 8.3333e-05 - 100s/epoch - 511ms/step
Epoch 813/1000
2023-09-28 11:15:25.937 
Epoch 813/1000 
	 loss: 17.3290, MinusLogProbMetric: 17.3290, val_loss: 17.5798, val_MinusLogProbMetric: 17.5798

Epoch 813: val_loss did not improve from 17.41260
196/196 - 103s - loss: 17.3290 - MinusLogProbMetric: 17.3290 - val_loss: 17.5798 - val_MinusLogProbMetric: 17.5798 - lr: 8.3333e-05 - 103s/epoch - 524ms/step
Epoch 814/1000
2023-09-28 11:17:05.462 
Epoch 814/1000 
	 loss: 17.3272, MinusLogProbMetric: 17.3272, val_loss: 17.4564, val_MinusLogProbMetric: 17.4564

Epoch 814: val_loss did not improve from 17.41260
196/196 - 100s - loss: 17.3272 - MinusLogProbMetric: 17.3272 - val_loss: 17.4564 - val_MinusLogProbMetric: 17.4564 - lr: 8.3333e-05 - 100s/epoch - 508ms/step
Epoch 815/1000
2023-09-28 11:18:49.324 
Epoch 815/1000 
	 loss: 17.3182, MinusLogProbMetric: 17.3182, val_loss: 17.5201, val_MinusLogProbMetric: 17.5201

Epoch 815: val_loss did not improve from 17.41260
196/196 - 104s - loss: 17.3182 - MinusLogProbMetric: 17.3182 - val_loss: 17.5201 - val_MinusLogProbMetric: 17.5201 - lr: 8.3333e-05 - 104s/epoch - 530ms/step
Epoch 816/1000
2023-09-28 11:20:31.405 
Epoch 816/1000 
	 loss: 17.3260, MinusLogProbMetric: 17.3260, val_loss: 17.4652, val_MinusLogProbMetric: 17.4652

Epoch 816: val_loss did not improve from 17.41260
196/196 - 102s - loss: 17.3260 - MinusLogProbMetric: 17.3260 - val_loss: 17.4652 - val_MinusLogProbMetric: 17.4652 - lr: 8.3333e-05 - 102s/epoch - 521ms/step
Epoch 817/1000
2023-09-28 11:22:13.408 
Epoch 817/1000 
	 loss: 17.3274, MinusLogProbMetric: 17.3274, val_loss: 17.4999, val_MinusLogProbMetric: 17.4999

Epoch 817: val_loss did not improve from 17.41260
196/196 - 102s - loss: 17.3274 - MinusLogProbMetric: 17.3274 - val_loss: 17.4999 - val_MinusLogProbMetric: 17.4999 - lr: 8.3333e-05 - 102s/epoch - 520ms/step
Epoch 818/1000
2023-09-28 11:23:56.563 
Epoch 818/1000 
	 loss: 17.3170, MinusLogProbMetric: 17.3170, val_loss: 17.4712, val_MinusLogProbMetric: 17.4712

Epoch 818: val_loss did not improve from 17.41260
196/196 - 103s - loss: 17.3170 - MinusLogProbMetric: 17.3170 - val_loss: 17.4712 - val_MinusLogProbMetric: 17.4712 - lr: 8.3333e-05 - 103s/epoch - 526ms/step
Epoch 819/1000
2023-09-28 11:25:40.828 
Epoch 819/1000 
	 loss: 17.3244, MinusLogProbMetric: 17.3244, val_loss: 17.4478, val_MinusLogProbMetric: 17.4478

Epoch 819: val_loss did not improve from 17.41260
196/196 - 104s - loss: 17.3244 - MinusLogProbMetric: 17.3244 - val_loss: 17.4478 - val_MinusLogProbMetric: 17.4478 - lr: 8.3333e-05 - 104s/epoch - 532ms/step
Epoch 820/1000
2023-09-28 11:27:24.145 
Epoch 820/1000 
	 loss: 17.3199, MinusLogProbMetric: 17.3199, val_loss: 17.4469, val_MinusLogProbMetric: 17.4469

Epoch 820: val_loss did not improve from 17.41260
196/196 - 103s - loss: 17.3199 - MinusLogProbMetric: 17.3199 - val_loss: 17.4469 - val_MinusLogProbMetric: 17.4469 - lr: 8.3333e-05 - 103s/epoch - 527ms/step
Epoch 821/1000
2023-09-28 11:29:09.389 
Epoch 821/1000 
	 loss: 17.3271, MinusLogProbMetric: 17.3271, val_loss: 17.4958, val_MinusLogProbMetric: 17.4958

Epoch 821: val_loss did not improve from 17.41260
196/196 - 105s - loss: 17.3271 - MinusLogProbMetric: 17.3271 - val_loss: 17.4958 - val_MinusLogProbMetric: 17.4958 - lr: 8.3333e-05 - 105s/epoch - 537ms/step
Epoch 822/1000
2023-09-28 11:30:54.247 
Epoch 822/1000 
	 loss: 17.3336, MinusLogProbMetric: 17.3336, val_loss: 17.4670, val_MinusLogProbMetric: 17.4670

Epoch 822: val_loss did not improve from 17.41260
196/196 - 105s - loss: 17.3336 - MinusLogProbMetric: 17.3336 - val_loss: 17.4670 - val_MinusLogProbMetric: 17.4670 - lr: 8.3333e-05 - 105s/epoch - 535ms/step
Epoch 823/1000
2023-09-28 11:32:39.505 
Epoch 823/1000 
	 loss: 17.3139, MinusLogProbMetric: 17.3139, val_loss: 17.4585, val_MinusLogProbMetric: 17.4585

Epoch 823: val_loss did not improve from 17.41260
196/196 - 105s - loss: 17.3139 - MinusLogProbMetric: 17.3139 - val_loss: 17.4585 - val_MinusLogProbMetric: 17.4585 - lr: 8.3333e-05 - 105s/epoch - 537ms/step
Epoch 824/1000
2023-09-28 11:34:24.938 
Epoch 824/1000 
	 loss: 17.3298, MinusLogProbMetric: 17.3298, val_loss: 17.4863, val_MinusLogProbMetric: 17.4863

Epoch 824: val_loss did not improve from 17.41260
196/196 - 105s - loss: 17.3298 - MinusLogProbMetric: 17.3298 - val_loss: 17.4863 - val_MinusLogProbMetric: 17.4863 - lr: 8.3333e-05 - 105s/epoch - 538ms/step
Epoch 825/1000
2023-09-28 11:36:08.189 
Epoch 825/1000 
	 loss: 17.3165, MinusLogProbMetric: 17.3165, val_loss: 17.4726, val_MinusLogProbMetric: 17.4726

Epoch 825: val_loss did not improve from 17.41260
196/196 - 103s - loss: 17.3165 - MinusLogProbMetric: 17.3165 - val_loss: 17.4726 - val_MinusLogProbMetric: 17.4726 - lr: 8.3333e-05 - 103s/epoch - 527ms/step
Epoch 826/1000
2023-09-28 11:37:48.399 
Epoch 826/1000 
	 loss: 17.3903, MinusLogProbMetric: 17.3903, val_loss: 17.5389, val_MinusLogProbMetric: 17.5389

Epoch 826: val_loss did not improve from 17.41260
196/196 - 100s - loss: 17.3903 - MinusLogProbMetric: 17.3903 - val_loss: 17.5389 - val_MinusLogProbMetric: 17.5389 - lr: 8.3333e-05 - 100s/epoch - 511ms/step
Epoch 827/1000
2023-09-28 11:39:27.095 
Epoch 827/1000 
	 loss: 17.3344, MinusLogProbMetric: 17.3344, val_loss: 17.4437, val_MinusLogProbMetric: 17.4437

Epoch 827: val_loss did not improve from 17.41260
196/196 - 99s - loss: 17.3344 - MinusLogProbMetric: 17.3344 - val_loss: 17.4437 - val_MinusLogProbMetric: 17.4437 - lr: 8.3333e-05 - 99s/epoch - 504ms/step
Epoch 828/1000
2023-09-28 11:41:07.468 
Epoch 828/1000 
	 loss: 17.3171, MinusLogProbMetric: 17.3171, val_loss: 17.4269, val_MinusLogProbMetric: 17.4269

Epoch 828: val_loss did not improve from 17.41260
196/196 - 100s - loss: 17.3171 - MinusLogProbMetric: 17.3171 - val_loss: 17.4269 - val_MinusLogProbMetric: 17.4269 - lr: 8.3333e-05 - 100s/epoch - 512ms/step
Epoch 829/1000
2023-09-28 11:42:51.705 
Epoch 829/1000 
	 loss: 17.3194, MinusLogProbMetric: 17.3194, val_loss: 17.4261, val_MinusLogProbMetric: 17.4261

Epoch 829: val_loss did not improve from 17.41260
196/196 - 104s - loss: 17.3194 - MinusLogProbMetric: 17.3194 - val_loss: 17.4261 - val_MinusLogProbMetric: 17.4261 - lr: 8.3333e-05 - 104s/epoch - 532ms/step
Epoch 830/1000
2023-09-28 11:44:35.435 
Epoch 830/1000 
	 loss: 17.3198, MinusLogProbMetric: 17.3198, val_loss: 17.4648, val_MinusLogProbMetric: 17.4648

Epoch 830: val_loss did not improve from 17.41260
196/196 - 104s - loss: 17.3198 - MinusLogProbMetric: 17.3198 - val_loss: 17.4648 - val_MinusLogProbMetric: 17.4648 - lr: 8.3333e-05 - 104s/epoch - 529ms/step
Epoch 831/1000
2023-09-28 11:46:19.168 
Epoch 831/1000 
	 loss: 17.3223, MinusLogProbMetric: 17.3223, val_loss: 17.4425, val_MinusLogProbMetric: 17.4425

Epoch 831: val_loss did not improve from 17.41260
196/196 - 104s - loss: 17.3223 - MinusLogProbMetric: 17.3223 - val_loss: 17.4425 - val_MinusLogProbMetric: 17.4425 - lr: 8.3333e-05 - 104s/epoch - 529ms/step
Epoch 832/1000
2023-09-28 11:47:55.427 
Epoch 832/1000 
	 loss: 17.3063, MinusLogProbMetric: 17.3063, val_loss: 17.4194, val_MinusLogProbMetric: 17.4194

Epoch 832: val_loss did not improve from 17.41260
196/196 - 96s - loss: 17.3063 - MinusLogProbMetric: 17.3063 - val_loss: 17.4194 - val_MinusLogProbMetric: 17.4194 - lr: 8.3333e-05 - 96s/epoch - 491ms/step
Epoch 833/1000
2023-09-28 11:49:24.791 
Epoch 833/1000 
	 loss: 17.3344, MinusLogProbMetric: 17.3344, val_loss: 17.4224, val_MinusLogProbMetric: 17.4224

Epoch 833: val_loss did not improve from 17.41260
196/196 - 89s - loss: 17.3344 - MinusLogProbMetric: 17.3344 - val_loss: 17.4224 - val_MinusLogProbMetric: 17.4224 - lr: 8.3333e-05 - 89s/epoch - 456ms/step
Epoch 834/1000
2023-09-28 11:51:01.463 
Epoch 834/1000 
	 loss: 17.3142, MinusLogProbMetric: 17.3142, val_loss: 17.4244, val_MinusLogProbMetric: 17.4244

Epoch 834: val_loss did not improve from 17.41260
196/196 - 97s - loss: 17.3142 - MinusLogProbMetric: 17.3142 - val_loss: 17.4244 - val_MinusLogProbMetric: 17.4244 - lr: 8.3333e-05 - 97s/epoch - 493ms/step
Epoch 835/1000
2023-09-28 11:52:31.134 
Epoch 835/1000 
	 loss: 17.3231, MinusLogProbMetric: 17.3231, val_loss: 17.4027, val_MinusLogProbMetric: 17.4027

Epoch 835: val_loss improved from 17.41260 to 17.40269, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 91s - loss: 17.3231 - MinusLogProbMetric: 17.3231 - val_loss: 17.4027 - val_MinusLogProbMetric: 17.4027 - lr: 8.3333e-05 - 91s/epoch - 463ms/step
Epoch 836/1000
2023-09-28 11:54:07.351 
Epoch 836/1000 
	 loss: 17.3185, MinusLogProbMetric: 17.3185, val_loss: 17.4178, val_MinusLogProbMetric: 17.4178

Epoch 836: val_loss did not improve from 17.40269
196/196 - 95s - loss: 17.3185 - MinusLogProbMetric: 17.3185 - val_loss: 17.4178 - val_MinusLogProbMetric: 17.4178 - lr: 8.3333e-05 - 95s/epoch - 485ms/step
Epoch 837/1000
2023-09-28 11:55:42.880 
Epoch 837/1000 
	 loss: 17.3151, MinusLogProbMetric: 17.3151, val_loss: 17.4111, val_MinusLogProbMetric: 17.4111

Epoch 837: val_loss did not improve from 17.40269
196/196 - 96s - loss: 17.3151 - MinusLogProbMetric: 17.3151 - val_loss: 17.4111 - val_MinusLogProbMetric: 17.4111 - lr: 8.3333e-05 - 96s/epoch - 487ms/step
Epoch 838/1000
2023-09-28 11:57:17.635 
Epoch 838/1000 
	 loss: 17.3174, MinusLogProbMetric: 17.3174, val_loss: 17.4765, val_MinusLogProbMetric: 17.4765

Epoch 838: val_loss did not improve from 17.40269
196/196 - 95s - loss: 17.3174 - MinusLogProbMetric: 17.3174 - val_loss: 17.4765 - val_MinusLogProbMetric: 17.4765 - lr: 8.3333e-05 - 95s/epoch - 483ms/step
Epoch 839/1000
2023-09-28 11:59:03.320 
Epoch 839/1000 
	 loss: 17.3203, MinusLogProbMetric: 17.3203, val_loss: 17.4643, val_MinusLogProbMetric: 17.4643

Epoch 839: val_loss did not improve from 17.40269
196/196 - 106s - loss: 17.3203 - MinusLogProbMetric: 17.3203 - val_loss: 17.4643 - val_MinusLogProbMetric: 17.4643 - lr: 8.3333e-05 - 106s/epoch - 539ms/step
Epoch 840/1000
2023-09-28 12:00:48.010 
Epoch 840/1000 
	 loss: 17.3198, MinusLogProbMetric: 17.3198, val_loss: 17.5217, val_MinusLogProbMetric: 17.5217

Epoch 840: val_loss did not improve from 17.40269
196/196 - 105s - loss: 17.3198 - MinusLogProbMetric: 17.3198 - val_loss: 17.5217 - val_MinusLogProbMetric: 17.5217 - lr: 8.3333e-05 - 105s/epoch - 534ms/step
Epoch 841/1000
2023-09-28 12:02:31.750 
Epoch 841/1000 
	 loss: 17.3319, MinusLogProbMetric: 17.3319, val_loss: 17.4313, val_MinusLogProbMetric: 17.4313

Epoch 841: val_loss did not improve from 17.40269
196/196 - 104s - loss: 17.3319 - MinusLogProbMetric: 17.3319 - val_loss: 17.4313 - val_MinusLogProbMetric: 17.4313 - lr: 8.3333e-05 - 104s/epoch - 529ms/step
Epoch 842/1000
2023-09-28 12:04:12.255 
Epoch 842/1000 
	 loss: 17.3131, MinusLogProbMetric: 17.3131, val_loss: 17.3980, val_MinusLogProbMetric: 17.3980

Epoch 842: val_loss improved from 17.40269 to 17.39799, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 102s - loss: 17.3131 - MinusLogProbMetric: 17.3131 - val_loss: 17.3980 - val_MinusLogProbMetric: 17.3980 - lr: 8.3333e-05 - 102s/epoch - 519ms/step
Epoch 843/1000
2023-09-28 12:05:54.249 
Epoch 843/1000 
	 loss: 17.3107, MinusLogProbMetric: 17.3107, val_loss: 17.4550, val_MinusLogProbMetric: 17.4550

Epoch 843: val_loss did not improve from 17.39799
196/196 - 101s - loss: 17.3107 - MinusLogProbMetric: 17.3107 - val_loss: 17.4550 - val_MinusLogProbMetric: 17.4550 - lr: 8.3333e-05 - 101s/epoch - 514ms/step
Epoch 844/1000
2023-09-28 12:07:29.257 
Epoch 844/1000 
	 loss: 17.3098, MinusLogProbMetric: 17.3098, val_loss: 17.5166, val_MinusLogProbMetric: 17.5166

Epoch 844: val_loss did not improve from 17.39799
196/196 - 95s - loss: 17.3098 - MinusLogProbMetric: 17.3098 - val_loss: 17.5166 - val_MinusLogProbMetric: 17.5166 - lr: 8.3333e-05 - 95s/epoch - 485ms/step
Epoch 845/1000
2023-09-28 12:09:02.116 
Epoch 845/1000 
	 loss: 17.3240, MinusLogProbMetric: 17.3240, val_loss: 17.5458, val_MinusLogProbMetric: 17.5458

Epoch 845: val_loss did not improve from 17.39799
196/196 - 93s - loss: 17.3240 - MinusLogProbMetric: 17.3240 - val_loss: 17.5458 - val_MinusLogProbMetric: 17.5458 - lr: 8.3333e-05 - 93s/epoch - 474ms/step
Epoch 846/1000
2023-09-28 12:10:37.069 
Epoch 846/1000 
	 loss: 17.3178, MinusLogProbMetric: 17.3178, val_loss: 17.4046, val_MinusLogProbMetric: 17.4046

Epoch 846: val_loss did not improve from 17.39799
196/196 - 95s - loss: 17.3178 - MinusLogProbMetric: 17.3178 - val_loss: 17.4046 - val_MinusLogProbMetric: 17.4046 - lr: 8.3333e-05 - 95s/epoch - 484ms/step
Epoch 847/1000
2023-09-28 12:12:10.558 
Epoch 847/1000 
	 loss: 17.3172, MinusLogProbMetric: 17.3172, val_loss: 17.4480, val_MinusLogProbMetric: 17.4480

Epoch 847: val_loss did not improve from 17.39799
196/196 - 93s - loss: 17.3172 - MinusLogProbMetric: 17.3172 - val_loss: 17.4480 - val_MinusLogProbMetric: 17.4480 - lr: 8.3333e-05 - 93s/epoch - 477ms/step
Epoch 848/1000
2023-09-28 12:13:46.810 
Epoch 848/1000 
	 loss: 17.3127, MinusLogProbMetric: 17.3127, val_loss: 17.4575, val_MinusLogProbMetric: 17.4575

Epoch 848: val_loss did not improve from 17.39799
196/196 - 96s - loss: 17.3127 - MinusLogProbMetric: 17.3127 - val_loss: 17.4575 - val_MinusLogProbMetric: 17.4575 - lr: 8.3333e-05 - 96s/epoch - 491ms/step
Epoch 849/1000
2023-09-28 12:15:23.995 
Epoch 849/1000 
	 loss: 17.3190, MinusLogProbMetric: 17.3190, val_loss: 17.4753, val_MinusLogProbMetric: 17.4753

Epoch 849: val_loss did not improve from 17.39799
196/196 - 97s - loss: 17.3190 - MinusLogProbMetric: 17.3190 - val_loss: 17.4753 - val_MinusLogProbMetric: 17.4753 - lr: 8.3333e-05 - 97s/epoch - 496ms/step
Epoch 850/1000
2023-09-28 12:16:57.068 
Epoch 850/1000 
	 loss: 17.3175, MinusLogProbMetric: 17.3175, val_loss: 17.4335, val_MinusLogProbMetric: 17.4335

Epoch 850: val_loss did not improve from 17.39799
196/196 - 93s - loss: 17.3175 - MinusLogProbMetric: 17.3175 - val_loss: 17.4335 - val_MinusLogProbMetric: 17.4335 - lr: 8.3333e-05 - 93s/epoch - 475ms/step
Epoch 851/1000
2023-09-28 12:18:34.547 
Epoch 851/1000 
	 loss: 17.3011, MinusLogProbMetric: 17.3011, val_loss: 17.5087, val_MinusLogProbMetric: 17.5087

Epoch 851: val_loss did not improve from 17.39799
196/196 - 97s - loss: 17.3011 - MinusLogProbMetric: 17.3011 - val_loss: 17.5087 - val_MinusLogProbMetric: 17.5087 - lr: 8.3333e-05 - 97s/epoch - 497ms/step
Epoch 852/1000
2023-09-28 12:20:13.105 
Epoch 852/1000 
	 loss: 17.3286, MinusLogProbMetric: 17.3286, val_loss: 17.4191, val_MinusLogProbMetric: 17.4191

Epoch 852: val_loss did not improve from 17.39799
196/196 - 99s - loss: 17.3286 - MinusLogProbMetric: 17.3286 - val_loss: 17.4191 - val_MinusLogProbMetric: 17.4191 - lr: 8.3333e-05 - 99s/epoch - 503ms/step
Epoch 853/1000
2023-09-28 12:21:50.932 
Epoch 853/1000 
	 loss: 17.3107, MinusLogProbMetric: 17.3107, val_loss: 17.4195, val_MinusLogProbMetric: 17.4195

Epoch 853: val_loss did not improve from 17.39799
196/196 - 98s - loss: 17.3107 - MinusLogProbMetric: 17.3107 - val_loss: 17.4195 - val_MinusLogProbMetric: 17.4195 - lr: 8.3333e-05 - 98s/epoch - 499ms/step
Epoch 854/1000
2023-09-28 12:23:34.792 
Epoch 854/1000 
	 loss: 17.3095, MinusLogProbMetric: 17.3095, val_loss: 17.5505, val_MinusLogProbMetric: 17.5505

Epoch 854: val_loss did not improve from 17.39799
196/196 - 104s - loss: 17.3095 - MinusLogProbMetric: 17.3095 - val_loss: 17.5505 - val_MinusLogProbMetric: 17.5505 - lr: 8.3333e-05 - 104s/epoch - 530ms/step
Epoch 855/1000
2023-09-28 12:25:17.441 
Epoch 855/1000 
	 loss: 17.3166, MinusLogProbMetric: 17.3166, val_loss: 17.4647, val_MinusLogProbMetric: 17.4647

Epoch 855: val_loss did not improve from 17.39799
196/196 - 103s - loss: 17.3166 - MinusLogProbMetric: 17.3166 - val_loss: 17.4647 - val_MinusLogProbMetric: 17.4647 - lr: 8.3333e-05 - 103s/epoch - 524ms/step
Epoch 856/1000
2023-09-28 12:26:57.871 
Epoch 856/1000 
	 loss: 17.3050, MinusLogProbMetric: 17.3050, val_loss: 17.4222, val_MinusLogProbMetric: 17.4222

Epoch 856: val_loss did not improve from 17.39799
196/196 - 100s - loss: 17.3050 - MinusLogProbMetric: 17.3050 - val_loss: 17.4222 - val_MinusLogProbMetric: 17.4222 - lr: 8.3333e-05 - 100s/epoch - 512ms/step
Epoch 857/1000
2023-09-28 12:28:40.930 
Epoch 857/1000 
	 loss: 17.3036, MinusLogProbMetric: 17.3036, val_loss: 17.4098, val_MinusLogProbMetric: 17.4098

Epoch 857: val_loss did not improve from 17.39799
196/196 - 103s - loss: 17.3036 - MinusLogProbMetric: 17.3036 - val_loss: 17.4098 - val_MinusLogProbMetric: 17.4098 - lr: 8.3333e-05 - 103s/epoch - 526ms/step
Epoch 858/1000
2023-09-28 12:30:25.293 
Epoch 858/1000 
	 loss: 17.3072, MinusLogProbMetric: 17.3072, val_loss: 17.4257, val_MinusLogProbMetric: 17.4257

Epoch 858: val_loss did not improve from 17.39799
196/196 - 104s - loss: 17.3072 - MinusLogProbMetric: 17.3072 - val_loss: 17.4257 - val_MinusLogProbMetric: 17.4257 - lr: 8.3333e-05 - 104s/epoch - 532ms/step
Epoch 859/1000
2023-09-28 12:32:07.822 
Epoch 859/1000 
	 loss: 17.3296, MinusLogProbMetric: 17.3296, val_loss: 17.4998, val_MinusLogProbMetric: 17.4998

Epoch 859: val_loss did not improve from 17.39799
196/196 - 103s - loss: 17.3296 - MinusLogProbMetric: 17.3296 - val_loss: 17.4998 - val_MinusLogProbMetric: 17.4998 - lr: 8.3333e-05 - 103s/epoch - 523ms/step
Epoch 860/1000
2023-09-28 12:33:52.966 
Epoch 860/1000 
	 loss: 17.3117, MinusLogProbMetric: 17.3117, val_loss: 17.4262, val_MinusLogProbMetric: 17.4262

Epoch 860: val_loss did not improve from 17.39799
196/196 - 105s - loss: 17.3117 - MinusLogProbMetric: 17.3117 - val_loss: 17.4262 - val_MinusLogProbMetric: 17.4262 - lr: 8.3333e-05 - 105s/epoch - 536ms/step
Epoch 861/1000
2023-09-28 12:35:38.792 
Epoch 861/1000 
	 loss: 17.3076, MinusLogProbMetric: 17.3076, val_loss: 17.4997, val_MinusLogProbMetric: 17.4997

Epoch 861: val_loss did not improve from 17.39799
196/196 - 106s - loss: 17.3076 - MinusLogProbMetric: 17.3076 - val_loss: 17.4997 - val_MinusLogProbMetric: 17.4997 - lr: 8.3333e-05 - 106s/epoch - 540ms/step
Epoch 862/1000
2023-09-28 12:37:23.089 
Epoch 862/1000 
	 loss: 17.3062, MinusLogProbMetric: 17.3062, val_loss: 17.4207, val_MinusLogProbMetric: 17.4207

Epoch 862: val_loss did not improve from 17.39799
196/196 - 104s - loss: 17.3062 - MinusLogProbMetric: 17.3062 - val_loss: 17.4207 - val_MinusLogProbMetric: 17.4207 - lr: 8.3333e-05 - 104s/epoch - 532ms/step
Epoch 863/1000
2023-09-28 12:39:08.085 
Epoch 863/1000 
	 loss: 17.2976, MinusLogProbMetric: 17.2976, val_loss: 17.4352, val_MinusLogProbMetric: 17.4352

Epoch 863: val_loss did not improve from 17.39799
196/196 - 105s - loss: 17.2976 - MinusLogProbMetric: 17.2976 - val_loss: 17.4352 - val_MinusLogProbMetric: 17.4352 - lr: 8.3333e-05 - 105s/epoch - 536ms/step
Epoch 864/1000
2023-09-28 12:40:53.374 
Epoch 864/1000 
	 loss: 17.3324, MinusLogProbMetric: 17.3324, val_loss: 17.4778, val_MinusLogProbMetric: 17.4778

Epoch 864: val_loss did not improve from 17.39799
196/196 - 105s - loss: 17.3324 - MinusLogProbMetric: 17.3324 - val_loss: 17.4778 - val_MinusLogProbMetric: 17.4778 - lr: 8.3333e-05 - 105s/epoch - 537ms/step
Epoch 865/1000
2023-09-28 12:42:38.070 
Epoch 865/1000 
	 loss: 17.2992, MinusLogProbMetric: 17.2992, val_loss: 17.4354, val_MinusLogProbMetric: 17.4354

Epoch 865: val_loss did not improve from 17.39799
196/196 - 105s - loss: 17.2992 - MinusLogProbMetric: 17.2992 - val_loss: 17.4354 - val_MinusLogProbMetric: 17.4354 - lr: 8.3333e-05 - 105s/epoch - 534ms/step
Epoch 866/1000
2023-09-28 12:44:23.735 
Epoch 866/1000 
	 loss: 17.3049, MinusLogProbMetric: 17.3049, val_loss: 17.5321, val_MinusLogProbMetric: 17.5321

Epoch 866: val_loss did not improve from 17.39799
196/196 - 106s - loss: 17.3049 - MinusLogProbMetric: 17.3049 - val_loss: 17.5321 - val_MinusLogProbMetric: 17.5321 - lr: 8.3333e-05 - 106s/epoch - 539ms/step
Epoch 867/1000
2023-09-28 12:46:09.226 
Epoch 867/1000 
	 loss: 17.3083, MinusLogProbMetric: 17.3083, val_loss: 17.4636, val_MinusLogProbMetric: 17.4636

Epoch 867: val_loss did not improve from 17.39799
196/196 - 105s - loss: 17.3083 - MinusLogProbMetric: 17.3083 - val_loss: 17.4636 - val_MinusLogProbMetric: 17.4636 - lr: 8.3333e-05 - 105s/epoch - 538ms/step
Epoch 868/1000
2023-09-28 12:47:54.436 
Epoch 868/1000 
	 loss: 17.3233, MinusLogProbMetric: 17.3233, val_loss: 17.4492, val_MinusLogProbMetric: 17.4492

Epoch 868: val_loss did not improve from 17.39799
196/196 - 105s - loss: 17.3233 - MinusLogProbMetric: 17.3233 - val_loss: 17.4492 - val_MinusLogProbMetric: 17.4492 - lr: 8.3333e-05 - 105s/epoch - 537ms/step
Epoch 869/1000
2023-09-28 12:49:38.634 
Epoch 869/1000 
	 loss: 17.2978, MinusLogProbMetric: 17.2978, val_loss: 17.4320, val_MinusLogProbMetric: 17.4320

Epoch 869: val_loss did not improve from 17.39799
196/196 - 104s - loss: 17.2978 - MinusLogProbMetric: 17.2978 - val_loss: 17.4320 - val_MinusLogProbMetric: 17.4320 - lr: 8.3333e-05 - 104s/epoch - 532ms/step
Epoch 870/1000
2023-09-28 12:51:21.497 
Epoch 870/1000 
	 loss: 17.3111, MinusLogProbMetric: 17.3111, val_loss: 17.4931, val_MinusLogProbMetric: 17.4931

Epoch 870: val_loss did not improve from 17.39799
196/196 - 103s - loss: 17.3111 - MinusLogProbMetric: 17.3111 - val_loss: 17.4931 - val_MinusLogProbMetric: 17.4931 - lr: 8.3333e-05 - 103s/epoch - 525ms/step
Epoch 871/1000
2023-09-28 12:53:05.592 
Epoch 871/1000 
	 loss: 17.3121, MinusLogProbMetric: 17.3121, val_loss: 17.4453, val_MinusLogProbMetric: 17.4453

Epoch 871: val_loss did not improve from 17.39799
196/196 - 104s - loss: 17.3121 - MinusLogProbMetric: 17.3121 - val_loss: 17.4453 - val_MinusLogProbMetric: 17.4453 - lr: 8.3333e-05 - 104s/epoch - 531ms/step
Epoch 872/1000
2023-09-28 12:54:50.275 
Epoch 872/1000 
	 loss: 17.3010, MinusLogProbMetric: 17.3010, val_loss: 17.4215, val_MinusLogProbMetric: 17.4215

Epoch 872: val_loss did not improve from 17.39799
196/196 - 105s - loss: 17.3010 - MinusLogProbMetric: 17.3010 - val_loss: 17.4215 - val_MinusLogProbMetric: 17.4215 - lr: 8.3333e-05 - 105s/epoch - 534ms/step
Epoch 873/1000
2023-09-28 12:56:35.600 
Epoch 873/1000 
	 loss: 17.2938, MinusLogProbMetric: 17.2938, val_loss: 17.4004, val_MinusLogProbMetric: 17.4004

Epoch 873: val_loss did not improve from 17.39799
196/196 - 105s - loss: 17.2938 - MinusLogProbMetric: 17.2938 - val_loss: 17.4004 - val_MinusLogProbMetric: 17.4004 - lr: 8.3333e-05 - 105s/epoch - 537ms/step
Epoch 874/1000
2023-09-28 12:58:20.597 
Epoch 874/1000 
	 loss: 17.3031, MinusLogProbMetric: 17.3031, val_loss: 17.4208, val_MinusLogProbMetric: 17.4208

Epoch 874: val_loss did not improve from 17.39799
196/196 - 105s - loss: 17.3031 - MinusLogProbMetric: 17.3031 - val_loss: 17.4208 - val_MinusLogProbMetric: 17.4208 - lr: 8.3333e-05 - 105s/epoch - 536ms/step
Epoch 875/1000
2023-09-28 13:00:05.961 
Epoch 875/1000 
	 loss: 17.3059, MinusLogProbMetric: 17.3059, val_loss: 17.4272, val_MinusLogProbMetric: 17.4272

Epoch 875: val_loss did not improve from 17.39799
196/196 - 105s - loss: 17.3059 - MinusLogProbMetric: 17.3059 - val_loss: 17.4272 - val_MinusLogProbMetric: 17.4272 - lr: 8.3333e-05 - 105s/epoch - 538ms/step
Epoch 876/1000
2023-09-28 13:01:50.063 
Epoch 876/1000 
	 loss: 17.3260, MinusLogProbMetric: 17.3260, val_loss: 17.4494, val_MinusLogProbMetric: 17.4494

Epoch 876: val_loss did not improve from 17.39799
196/196 - 104s - loss: 17.3260 - MinusLogProbMetric: 17.3260 - val_loss: 17.4494 - val_MinusLogProbMetric: 17.4494 - lr: 8.3333e-05 - 104s/epoch - 531ms/step
Epoch 877/1000
2023-09-28 13:03:34.824 
Epoch 877/1000 
	 loss: 17.2969, MinusLogProbMetric: 17.2969, val_loss: 17.4302, val_MinusLogProbMetric: 17.4302

Epoch 877: val_loss did not improve from 17.39799
196/196 - 105s - loss: 17.2969 - MinusLogProbMetric: 17.2969 - val_loss: 17.4302 - val_MinusLogProbMetric: 17.4302 - lr: 8.3333e-05 - 105s/epoch - 534ms/step
Epoch 878/1000
2023-09-28 13:05:19.630 
Epoch 878/1000 
	 loss: 17.2965, MinusLogProbMetric: 17.2965, val_loss: 17.4353, val_MinusLogProbMetric: 17.4353

Epoch 878: val_loss did not improve from 17.39799
196/196 - 105s - loss: 17.2965 - MinusLogProbMetric: 17.2965 - val_loss: 17.4353 - val_MinusLogProbMetric: 17.4353 - lr: 8.3333e-05 - 105s/epoch - 535ms/step
Epoch 879/1000
2023-09-28 13:07:02.638 
Epoch 879/1000 
	 loss: 17.3173, MinusLogProbMetric: 17.3173, val_loss: 17.5158, val_MinusLogProbMetric: 17.5158

Epoch 879: val_loss did not improve from 17.39799
196/196 - 103s - loss: 17.3173 - MinusLogProbMetric: 17.3173 - val_loss: 17.5158 - val_MinusLogProbMetric: 17.5158 - lr: 8.3333e-05 - 103s/epoch - 526ms/step
Epoch 880/1000
2023-09-28 13:08:44.019 
Epoch 880/1000 
	 loss: 17.3090, MinusLogProbMetric: 17.3090, val_loss: 17.4037, val_MinusLogProbMetric: 17.4037

Epoch 880: val_loss did not improve from 17.39799
196/196 - 101s - loss: 17.3090 - MinusLogProbMetric: 17.3090 - val_loss: 17.4037 - val_MinusLogProbMetric: 17.4037 - lr: 8.3333e-05 - 101s/epoch - 517ms/step
Epoch 881/1000
2023-09-28 13:10:26.424 
Epoch 881/1000 
	 loss: 17.2995, MinusLogProbMetric: 17.2995, val_loss: 17.4418, val_MinusLogProbMetric: 17.4418

Epoch 881: val_loss did not improve from 17.39799
196/196 - 102s - loss: 17.2995 - MinusLogProbMetric: 17.2995 - val_loss: 17.4418 - val_MinusLogProbMetric: 17.4418 - lr: 8.3333e-05 - 102s/epoch - 522ms/step
Epoch 882/1000
2023-09-28 13:12:06.435 
Epoch 882/1000 
	 loss: 17.3005, MinusLogProbMetric: 17.3005, val_loss: 17.5099, val_MinusLogProbMetric: 17.5099

Epoch 882: val_loss did not improve from 17.39799
196/196 - 100s - loss: 17.3005 - MinusLogProbMetric: 17.3005 - val_loss: 17.5099 - val_MinusLogProbMetric: 17.5099 - lr: 8.3333e-05 - 100s/epoch - 510ms/step
Epoch 883/1000
2023-09-28 13:13:48.110 
Epoch 883/1000 
	 loss: 17.2942, MinusLogProbMetric: 17.2942, val_loss: 17.4760, val_MinusLogProbMetric: 17.4760

Epoch 883: val_loss did not improve from 17.39799
196/196 - 102s - loss: 17.2942 - MinusLogProbMetric: 17.2942 - val_loss: 17.4760 - val_MinusLogProbMetric: 17.4760 - lr: 8.3333e-05 - 102s/epoch - 519ms/step
Epoch 884/1000
2023-09-28 13:15:30.691 
Epoch 884/1000 
	 loss: 17.2978, MinusLogProbMetric: 17.2978, val_loss: 17.4396, val_MinusLogProbMetric: 17.4396

Epoch 884: val_loss did not improve from 17.39799
196/196 - 103s - loss: 17.2978 - MinusLogProbMetric: 17.2978 - val_loss: 17.4396 - val_MinusLogProbMetric: 17.4396 - lr: 8.3333e-05 - 103s/epoch - 523ms/step
Epoch 885/1000
2023-09-28 13:17:13.971 
Epoch 885/1000 
	 loss: 17.3014, MinusLogProbMetric: 17.3014, val_loss: 17.4356, val_MinusLogProbMetric: 17.4356

Epoch 885: val_loss did not improve from 17.39799
196/196 - 103s - loss: 17.3014 - MinusLogProbMetric: 17.3014 - val_loss: 17.4356 - val_MinusLogProbMetric: 17.4356 - lr: 8.3333e-05 - 103s/epoch - 527ms/step
Epoch 886/1000
2023-09-28 13:18:55.040 
Epoch 886/1000 
	 loss: 17.3014, MinusLogProbMetric: 17.3014, val_loss: 17.4045, val_MinusLogProbMetric: 17.4045

Epoch 886: val_loss did not improve from 17.39799
196/196 - 101s - loss: 17.3014 - MinusLogProbMetric: 17.3014 - val_loss: 17.4045 - val_MinusLogProbMetric: 17.4045 - lr: 8.3333e-05 - 101s/epoch - 516ms/step
Epoch 887/1000
2023-09-28 13:20:37.688 
Epoch 887/1000 
	 loss: 17.3038, MinusLogProbMetric: 17.3038, val_loss: 17.4413, val_MinusLogProbMetric: 17.4413

Epoch 887: val_loss did not improve from 17.39799
196/196 - 103s - loss: 17.3038 - MinusLogProbMetric: 17.3038 - val_loss: 17.4413 - val_MinusLogProbMetric: 17.4413 - lr: 8.3333e-05 - 103s/epoch - 524ms/step
Epoch 888/1000
2023-09-28 13:22:19.756 
Epoch 888/1000 
	 loss: 17.2922, MinusLogProbMetric: 17.2922, val_loss: 17.4232, val_MinusLogProbMetric: 17.4232

Epoch 888: val_loss did not improve from 17.39799
196/196 - 102s - loss: 17.2922 - MinusLogProbMetric: 17.2922 - val_loss: 17.4232 - val_MinusLogProbMetric: 17.4232 - lr: 8.3333e-05 - 102s/epoch - 521ms/step
Epoch 889/1000
2023-09-28 13:24:06.971 
Epoch 889/1000 
	 loss: 17.3089, MinusLogProbMetric: 17.3089, val_loss: 17.3919, val_MinusLogProbMetric: 17.3919

Epoch 889: val_loss improved from 17.39799 to 17.39188, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 108s - loss: 17.3089 - MinusLogProbMetric: 17.3089 - val_loss: 17.3919 - val_MinusLogProbMetric: 17.3919 - lr: 8.3333e-05 - 108s/epoch - 552ms/step
Epoch 890/1000
2023-09-28 13:25:50.278 
Epoch 890/1000 
	 loss: 17.2950, MinusLogProbMetric: 17.2950, val_loss: 17.3938, val_MinusLogProbMetric: 17.3938

Epoch 890: val_loss did not improve from 17.39188
196/196 - 102s - loss: 17.2950 - MinusLogProbMetric: 17.2950 - val_loss: 17.3938 - val_MinusLogProbMetric: 17.3938 - lr: 8.3333e-05 - 102s/epoch - 522ms/step
Epoch 891/1000
2023-09-28 13:27:36.303 
Epoch 891/1000 
	 loss: 17.2939, MinusLogProbMetric: 17.2939, val_loss: 17.4232, val_MinusLogProbMetric: 17.4232

Epoch 891: val_loss did not improve from 17.39188
196/196 - 106s - loss: 17.2939 - MinusLogProbMetric: 17.2939 - val_loss: 17.4232 - val_MinusLogProbMetric: 17.4232 - lr: 8.3333e-05 - 106s/epoch - 541ms/step
Epoch 892/1000
2023-09-28 13:29:22.450 
Epoch 892/1000 
	 loss: 17.2966, MinusLogProbMetric: 17.2966, val_loss: 17.4985, val_MinusLogProbMetric: 17.4985

Epoch 892: val_loss did not improve from 17.39188
196/196 - 106s - loss: 17.2966 - MinusLogProbMetric: 17.2966 - val_loss: 17.4985 - val_MinusLogProbMetric: 17.4985 - lr: 8.3333e-05 - 106s/epoch - 542ms/step
Epoch 893/1000
2023-09-28 13:31:08.199 
Epoch 893/1000 
	 loss: 17.3012, MinusLogProbMetric: 17.3012, val_loss: 17.4315, val_MinusLogProbMetric: 17.4315

Epoch 893: val_loss did not improve from 17.39188
196/196 - 106s - loss: 17.3012 - MinusLogProbMetric: 17.3012 - val_loss: 17.4315 - val_MinusLogProbMetric: 17.4315 - lr: 8.3333e-05 - 106s/epoch - 539ms/step
Epoch 894/1000
2023-09-28 13:32:52.934 
Epoch 894/1000 
	 loss: 17.2912, MinusLogProbMetric: 17.2912, val_loss: 17.4592, val_MinusLogProbMetric: 17.4592

Epoch 894: val_loss did not improve from 17.39188
196/196 - 105s - loss: 17.2912 - MinusLogProbMetric: 17.2912 - val_loss: 17.4592 - val_MinusLogProbMetric: 17.4592 - lr: 8.3333e-05 - 105s/epoch - 534ms/step
Epoch 895/1000
2023-09-28 13:34:35.874 
Epoch 895/1000 
	 loss: 17.2981, MinusLogProbMetric: 17.2981, val_loss: 17.4503, val_MinusLogProbMetric: 17.4503

Epoch 895: val_loss did not improve from 17.39188
196/196 - 103s - loss: 17.2981 - MinusLogProbMetric: 17.2981 - val_loss: 17.4503 - val_MinusLogProbMetric: 17.4503 - lr: 8.3333e-05 - 103s/epoch - 525ms/step
Epoch 896/1000
2023-09-28 13:36:17.406 
Epoch 896/1000 
	 loss: 17.3139, MinusLogProbMetric: 17.3139, val_loss: 17.4734, val_MinusLogProbMetric: 17.4734

Epoch 896: val_loss did not improve from 17.39188
196/196 - 102s - loss: 17.3139 - MinusLogProbMetric: 17.3139 - val_loss: 17.4734 - val_MinusLogProbMetric: 17.4734 - lr: 8.3333e-05 - 102s/epoch - 518ms/step
Epoch 897/1000
2023-09-28 13:37:57.553 
Epoch 897/1000 
	 loss: 17.3003, MinusLogProbMetric: 17.3003, val_loss: 17.4437, val_MinusLogProbMetric: 17.4437

Epoch 897: val_loss did not improve from 17.39188
196/196 - 100s - loss: 17.3003 - MinusLogProbMetric: 17.3003 - val_loss: 17.4437 - val_MinusLogProbMetric: 17.4437 - lr: 8.3333e-05 - 100s/epoch - 511ms/step
Epoch 898/1000
2023-09-28 13:39:40.223 
Epoch 898/1000 
	 loss: 17.2955, MinusLogProbMetric: 17.2955, val_loss: 17.4694, val_MinusLogProbMetric: 17.4694

Epoch 898: val_loss did not improve from 17.39188
196/196 - 103s - loss: 17.2955 - MinusLogProbMetric: 17.2955 - val_loss: 17.4694 - val_MinusLogProbMetric: 17.4694 - lr: 8.3333e-05 - 103s/epoch - 524ms/step
Epoch 899/1000
2023-09-28 13:41:21.476 
Epoch 899/1000 
	 loss: 17.2967, MinusLogProbMetric: 17.2967, val_loss: 17.4527, val_MinusLogProbMetric: 17.4527

Epoch 899: val_loss did not improve from 17.39188
196/196 - 101s - loss: 17.2967 - MinusLogProbMetric: 17.2967 - val_loss: 17.4527 - val_MinusLogProbMetric: 17.4527 - lr: 8.3333e-05 - 101s/epoch - 517ms/step
Epoch 900/1000
2023-09-28 13:43:04.321 
Epoch 900/1000 
	 loss: 17.2866, MinusLogProbMetric: 17.2866, val_loss: 17.4346, val_MinusLogProbMetric: 17.4346

Epoch 900: val_loss did not improve from 17.39188
196/196 - 103s - loss: 17.2866 - MinusLogProbMetric: 17.2866 - val_loss: 17.4346 - val_MinusLogProbMetric: 17.4346 - lr: 8.3333e-05 - 103s/epoch - 525ms/step
Epoch 901/1000
2023-09-28 13:44:45.380 
Epoch 901/1000 
	 loss: 17.2913, MinusLogProbMetric: 17.2913, val_loss: 17.4199, val_MinusLogProbMetric: 17.4199

Epoch 901: val_loss did not improve from 17.39188
196/196 - 101s - loss: 17.2913 - MinusLogProbMetric: 17.2913 - val_loss: 17.4199 - val_MinusLogProbMetric: 17.4199 - lr: 8.3333e-05 - 101s/epoch - 516ms/step
Epoch 902/1000
2023-09-28 13:46:26.146 
Epoch 902/1000 
	 loss: 17.3000, MinusLogProbMetric: 17.3000, val_loss: 17.4312, val_MinusLogProbMetric: 17.4312

Epoch 902: val_loss did not improve from 17.39188
196/196 - 101s - loss: 17.3000 - MinusLogProbMetric: 17.3000 - val_loss: 17.4312 - val_MinusLogProbMetric: 17.4312 - lr: 8.3333e-05 - 101s/epoch - 514ms/step
Epoch 903/1000
2023-09-28 13:48:09.879 
Epoch 903/1000 
	 loss: 17.3070, MinusLogProbMetric: 17.3070, val_loss: 17.5280, val_MinusLogProbMetric: 17.5280

Epoch 903: val_loss did not improve from 17.39188
196/196 - 104s - loss: 17.3070 - MinusLogProbMetric: 17.3070 - val_loss: 17.5280 - val_MinusLogProbMetric: 17.5280 - lr: 8.3333e-05 - 104s/epoch - 529ms/step
Epoch 904/1000
2023-09-28 13:49:53.910 
Epoch 904/1000 
	 loss: 17.2916, MinusLogProbMetric: 17.2916, val_loss: 17.4444, val_MinusLogProbMetric: 17.4444

Epoch 904: val_loss did not improve from 17.39188
196/196 - 104s - loss: 17.2916 - MinusLogProbMetric: 17.2916 - val_loss: 17.4444 - val_MinusLogProbMetric: 17.4444 - lr: 8.3333e-05 - 104s/epoch - 531ms/step
Epoch 905/1000
2023-09-28 13:51:38.767 
Epoch 905/1000 
	 loss: 17.2917, MinusLogProbMetric: 17.2917, val_loss: 17.4887, val_MinusLogProbMetric: 17.4887

Epoch 905: val_loss did not improve from 17.39188
196/196 - 105s - loss: 17.2917 - MinusLogProbMetric: 17.2917 - val_loss: 17.4887 - val_MinusLogProbMetric: 17.4887 - lr: 8.3333e-05 - 105s/epoch - 535ms/step
Epoch 906/1000
2023-09-28 13:53:23.997 
Epoch 906/1000 
	 loss: 17.2904, MinusLogProbMetric: 17.2904, val_loss: 17.4212, val_MinusLogProbMetric: 17.4212

Epoch 906: val_loss did not improve from 17.39188
196/196 - 105s - loss: 17.2904 - MinusLogProbMetric: 17.2904 - val_loss: 17.4212 - val_MinusLogProbMetric: 17.4212 - lr: 8.3333e-05 - 105s/epoch - 537ms/step
Epoch 907/1000
2023-09-28 13:55:08.538 
Epoch 907/1000 
	 loss: 17.2879, MinusLogProbMetric: 17.2879, val_loss: 17.5156, val_MinusLogProbMetric: 17.5156

Epoch 907: val_loss did not improve from 17.39188
196/196 - 105s - loss: 17.2879 - MinusLogProbMetric: 17.2879 - val_loss: 17.5156 - val_MinusLogProbMetric: 17.5156 - lr: 8.3333e-05 - 105s/epoch - 533ms/step
Epoch 908/1000
2023-09-28 13:56:52.296 
Epoch 908/1000 
	 loss: 17.3027, MinusLogProbMetric: 17.3027, val_loss: 17.4930, val_MinusLogProbMetric: 17.4930

Epoch 908: val_loss did not improve from 17.39188
196/196 - 104s - loss: 17.3027 - MinusLogProbMetric: 17.3027 - val_loss: 17.4930 - val_MinusLogProbMetric: 17.4930 - lr: 8.3333e-05 - 104s/epoch - 529ms/step
Epoch 909/1000
2023-09-28 13:58:36.168 
Epoch 909/1000 
	 loss: 17.2901, MinusLogProbMetric: 17.2901, val_loss: 17.3798, val_MinusLogProbMetric: 17.3798

Epoch 909: val_loss improved from 17.39188 to 17.37976, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 105s - loss: 17.2901 - MinusLogProbMetric: 17.2901 - val_loss: 17.3798 - val_MinusLogProbMetric: 17.3798 - lr: 8.3333e-05 - 105s/epoch - 538ms/step
Epoch 910/1000
2023-09-28 14:00:22.910 
Epoch 910/1000 
	 loss: 17.3045, MinusLogProbMetric: 17.3045, val_loss: 17.5903, val_MinusLogProbMetric: 17.5903

Epoch 910: val_loss did not improve from 17.37976
196/196 - 105s - loss: 17.3045 - MinusLogProbMetric: 17.3045 - val_loss: 17.5903 - val_MinusLogProbMetric: 17.5903 - lr: 8.3333e-05 - 105s/epoch - 537ms/step
Epoch 911/1000
2023-09-28 14:02:08.073 
Epoch 911/1000 
	 loss: 17.3090, MinusLogProbMetric: 17.3090, val_loss: 17.4240, val_MinusLogProbMetric: 17.4240

Epoch 911: val_loss did not improve from 17.37976
196/196 - 105s - loss: 17.3090 - MinusLogProbMetric: 17.3090 - val_loss: 17.4240 - val_MinusLogProbMetric: 17.4240 - lr: 8.3333e-05 - 105s/epoch - 537ms/step
Epoch 912/1000
2023-09-28 14:03:53.085 
Epoch 912/1000 
	 loss: 17.2906, MinusLogProbMetric: 17.2906, val_loss: 17.4580, val_MinusLogProbMetric: 17.4580

Epoch 912: val_loss did not improve from 17.37976
196/196 - 105s - loss: 17.2906 - MinusLogProbMetric: 17.2906 - val_loss: 17.4580 - val_MinusLogProbMetric: 17.4580 - lr: 8.3333e-05 - 105s/epoch - 536ms/step
Epoch 913/1000
2023-09-28 14:05:37.604 
Epoch 913/1000 
	 loss: 17.2850, MinusLogProbMetric: 17.2850, val_loss: 17.3700, val_MinusLogProbMetric: 17.3700

Epoch 913: val_loss improved from 17.37976 to 17.37000, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 106s - loss: 17.2850 - MinusLogProbMetric: 17.2850 - val_loss: 17.3700 - val_MinusLogProbMetric: 17.3700 - lr: 8.3333e-05 - 106s/epoch - 543ms/step
Epoch 914/1000
2023-09-28 14:07:22.811 
Epoch 914/1000 
	 loss: 17.2862, MinusLogProbMetric: 17.2862, val_loss: 17.3929, val_MinusLogProbMetric: 17.3929

Epoch 914: val_loss did not improve from 17.37000
196/196 - 103s - loss: 17.2862 - MinusLogProbMetric: 17.2862 - val_loss: 17.3929 - val_MinusLogProbMetric: 17.3929 - lr: 8.3333e-05 - 103s/epoch - 527ms/step
Epoch 915/1000
2023-09-28 14:09:07.018 
Epoch 915/1000 
	 loss: 17.2874, MinusLogProbMetric: 17.2874, val_loss: 17.4051, val_MinusLogProbMetric: 17.4051

Epoch 915: val_loss did not improve from 17.37000
196/196 - 104s - loss: 17.2874 - MinusLogProbMetric: 17.2874 - val_loss: 17.4051 - val_MinusLogProbMetric: 17.4051 - lr: 8.3333e-05 - 104s/epoch - 532ms/step
Epoch 916/1000
2023-09-28 14:10:53.332 
Epoch 916/1000 
	 loss: 17.2900, MinusLogProbMetric: 17.2900, val_loss: 17.4274, val_MinusLogProbMetric: 17.4274

Epoch 916: val_loss did not improve from 17.37000
196/196 - 106s - loss: 17.2900 - MinusLogProbMetric: 17.2900 - val_loss: 17.4274 - val_MinusLogProbMetric: 17.4274 - lr: 8.3333e-05 - 106s/epoch - 542ms/step
Epoch 917/1000
2023-09-28 14:12:39.129 
Epoch 917/1000 
	 loss: 17.2982, MinusLogProbMetric: 17.2982, val_loss: 17.4717, val_MinusLogProbMetric: 17.4717

Epoch 917: val_loss did not improve from 17.37000
196/196 - 106s - loss: 17.2982 - MinusLogProbMetric: 17.2982 - val_loss: 17.4717 - val_MinusLogProbMetric: 17.4717 - lr: 8.3333e-05 - 106s/epoch - 540ms/step
Epoch 918/1000
2023-09-28 14:14:25.157 
Epoch 918/1000 
	 loss: 17.3052, MinusLogProbMetric: 17.3052, val_loss: 17.4059, val_MinusLogProbMetric: 17.4059

Epoch 918: val_loss did not improve from 17.37000
196/196 - 106s - loss: 17.3052 - MinusLogProbMetric: 17.3052 - val_loss: 17.4059 - val_MinusLogProbMetric: 17.4059 - lr: 8.3333e-05 - 106s/epoch - 541ms/step
Epoch 919/1000
2023-09-28 14:16:11.462 
Epoch 919/1000 
	 loss: 17.2895, MinusLogProbMetric: 17.2895, val_loss: 17.4861, val_MinusLogProbMetric: 17.4861

Epoch 919: val_loss did not improve from 17.37000
196/196 - 106s - loss: 17.2895 - MinusLogProbMetric: 17.2895 - val_loss: 17.4861 - val_MinusLogProbMetric: 17.4861 - lr: 8.3333e-05 - 106s/epoch - 542ms/step
Epoch 920/1000
2023-09-28 14:17:57.080 
Epoch 920/1000 
	 loss: 17.2916, MinusLogProbMetric: 17.2916, val_loss: 17.4173, val_MinusLogProbMetric: 17.4173

Epoch 920: val_loss did not improve from 17.37000
196/196 - 106s - loss: 17.2916 - MinusLogProbMetric: 17.2916 - val_loss: 17.4173 - val_MinusLogProbMetric: 17.4173 - lr: 8.3333e-05 - 106s/epoch - 539ms/step
Epoch 921/1000
2023-09-28 14:19:41.959 
Epoch 921/1000 
	 loss: 17.2769, MinusLogProbMetric: 17.2769, val_loss: 17.3872, val_MinusLogProbMetric: 17.3872

Epoch 921: val_loss did not improve from 17.37000
196/196 - 105s - loss: 17.2769 - MinusLogProbMetric: 17.2769 - val_loss: 17.3872 - val_MinusLogProbMetric: 17.3872 - lr: 8.3333e-05 - 105s/epoch - 535ms/step
Epoch 922/1000
2023-09-28 14:21:26.436 
Epoch 922/1000 
	 loss: 17.2992, MinusLogProbMetric: 17.2992, val_loss: 17.3949, val_MinusLogProbMetric: 17.3949

Epoch 922: val_loss did not improve from 17.37000
196/196 - 104s - loss: 17.2992 - MinusLogProbMetric: 17.2992 - val_loss: 17.3949 - val_MinusLogProbMetric: 17.3949 - lr: 8.3333e-05 - 104s/epoch - 533ms/step
Epoch 923/1000
2023-09-28 14:23:06.452 
Epoch 923/1000 
	 loss: 17.3014, MinusLogProbMetric: 17.3014, val_loss: 17.4361, val_MinusLogProbMetric: 17.4361

Epoch 923: val_loss did not improve from 17.37000
196/196 - 100s - loss: 17.3014 - MinusLogProbMetric: 17.3014 - val_loss: 17.4361 - val_MinusLogProbMetric: 17.4361 - lr: 8.3333e-05 - 100s/epoch - 510ms/step
Epoch 924/1000
2023-09-28 14:24:47.318 
Epoch 924/1000 
	 loss: 17.3110, MinusLogProbMetric: 17.3110, val_loss: 17.5427, val_MinusLogProbMetric: 17.5427

Epoch 924: val_loss did not improve from 17.37000
196/196 - 101s - loss: 17.3110 - MinusLogProbMetric: 17.3110 - val_loss: 17.5427 - val_MinusLogProbMetric: 17.5427 - lr: 8.3333e-05 - 101s/epoch - 515ms/step
Epoch 925/1000
2023-09-28 14:26:24.889 
Epoch 925/1000 
	 loss: 17.2957, MinusLogProbMetric: 17.2957, val_loss: 17.4869, val_MinusLogProbMetric: 17.4869

Epoch 925: val_loss did not improve from 17.37000
196/196 - 98s - loss: 17.2957 - MinusLogProbMetric: 17.2957 - val_loss: 17.4869 - val_MinusLogProbMetric: 17.4869 - lr: 8.3333e-05 - 98s/epoch - 498ms/step
Epoch 926/1000
2023-09-28 14:28:04.981 
Epoch 926/1000 
	 loss: 17.2867, MinusLogProbMetric: 17.2867, val_loss: 17.4421, val_MinusLogProbMetric: 17.4421

Epoch 926: val_loss did not improve from 17.37000
196/196 - 100s - loss: 17.2867 - MinusLogProbMetric: 17.2867 - val_loss: 17.4421 - val_MinusLogProbMetric: 17.4421 - lr: 8.3333e-05 - 100s/epoch - 511ms/step
Epoch 927/1000
2023-09-28 14:29:45.434 
Epoch 927/1000 
	 loss: 17.3070, MinusLogProbMetric: 17.3070, val_loss: 17.4159, val_MinusLogProbMetric: 17.4159

Epoch 927: val_loss did not improve from 17.37000
196/196 - 100s - loss: 17.3070 - MinusLogProbMetric: 17.3070 - val_loss: 17.4159 - val_MinusLogProbMetric: 17.4159 - lr: 8.3333e-05 - 100s/epoch - 512ms/step
Epoch 928/1000
2023-09-28 14:31:20.518 
Epoch 928/1000 
	 loss: 17.2805, MinusLogProbMetric: 17.2805, val_loss: 17.4649, val_MinusLogProbMetric: 17.4649

Epoch 928: val_loss did not improve from 17.37000
196/196 - 95s - loss: 17.2805 - MinusLogProbMetric: 17.2805 - val_loss: 17.4649 - val_MinusLogProbMetric: 17.4649 - lr: 8.3333e-05 - 95s/epoch - 485ms/step
Epoch 929/1000
2023-09-28 14:32:54.827 
Epoch 929/1000 
	 loss: 17.2939, MinusLogProbMetric: 17.2939, val_loss: 17.3978, val_MinusLogProbMetric: 17.3978

Epoch 929: val_loss did not improve from 17.37000
196/196 - 94s - loss: 17.2939 - MinusLogProbMetric: 17.2939 - val_loss: 17.3978 - val_MinusLogProbMetric: 17.3978 - lr: 8.3333e-05 - 94s/epoch - 481ms/step
Epoch 930/1000
2023-09-28 14:34:33.378 
Epoch 930/1000 
	 loss: 17.2909, MinusLogProbMetric: 17.2909, val_loss: 17.3877, val_MinusLogProbMetric: 17.3877

Epoch 930: val_loss did not improve from 17.37000
196/196 - 99s - loss: 17.2909 - MinusLogProbMetric: 17.2909 - val_loss: 17.3877 - val_MinusLogProbMetric: 17.3877 - lr: 8.3333e-05 - 99s/epoch - 503ms/step
Epoch 931/1000
2023-09-28 14:36:07.556 
Epoch 931/1000 
	 loss: 17.2894, MinusLogProbMetric: 17.2894, val_loss: 17.6185, val_MinusLogProbMetric: 17.6185

Epoch 931: val_loss did not improve from 17.37000
196/196 - 94s - loss: 17.2894 - MinusLogProbMetric: 17.2894 - val_loss: 17.6185 - val_MinusLogProbMetric: 17.6185 - lr: 8.3333e-05 - 94s/epoch - 480ms/step
Epoch 932/1000
2023-09-28 14:37:41.706 
Epoch 932/1000 
	 loss: 17.2906, MinusLogProbMetric: 17.2906, val_loss: 17.4062, val_MinusLogProbMetric: 17.4062

Epoch 932: val_loss did not improve from 17.37000
196/196 - 94s - loss: 17.2906 - MinusLogProbMetric: 17.2906 - val_loss: 17.4062 - val_MinusLogProbMetric: 17.4062 - lr: 8.3333e-05 - 94s/epoch - 480ms/step
Epoch 933/1000
2023-09-28 14:39:18.020 
Epoch 933/1000 
	 loss: 17.3479, MinusLogProbMetric: 17.3479, val_loss: 17.5010, val_MinusLogProbMetric: 17.5010

Epoch 933: val_loss did not improve from 17.37000
196/196 - 96s - loss: 17.3479 - MinusLogProbMetric: 17.3479 - val_loss: 17.5010 - val_MinusLogProbMetric: 17.5010 - lr: 8.3333e-05 - 96s/epoch - 491ms/step
Epoch 934/1000
2023-09-28 14:40:57.426 
Epoch 934/1000 
	 loss: 17.2962, MinusLogProbMetric: 17.2962, val_loss: 17.3616, val_MinusLogProbMetric: 17.3616

Epoch 934: val_loss improved from 17.37000 to 17.36158, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 101s - loss: 17.2962 - MinusLogProbMetric: 17.2962 - val_loss: 17.3616 - val_MinusLogProbMetric: 17.3616 - lr: 8.3333e-05 - 101s/epoch - 513ms/step
Epoch 935/1000
2023-09-28 14:42:39.168 
Epoch 935/1000 
	 loss: 17.2875, MinusLogProbMetric: 17.2875, val_loss: 17.3869, val_MinusLogProbMetric: 17.3869

Epoch 935: val_loss did not improve from 17.36158
196/196 - 101s - loss: 17.2875 - MinusLogProbMetric: 17.2875 - val_loss: 17.3869 - val_MinusLogProbMetric: 17.3869 - lr: 8.3333e-05 - 101s/epoch - 513ms/step
Epoch 936/1000
2023-09-28 14:44:17.204 
Epoch 936/1000 
	 loss: 17.2905, MinusLogProbMetric: 17.2905, val_loss: 17.4387, val_MinusLogProbMetric: 17.4387

Epoch 936: val_loss did not improve from 17.36158
196/196 - 98s - loss: 17.2905 - MinusLogProbMetric: 17.2905 - val_loss: 17.4387 - val_MinusLogProbMetric: 17.4387 - lr: 8.3333e-05 - 98s/epoch - 500ms/step
Epoch 937/1000
2023-09-28 14:46:00.361 
Epoch 937/1000 
	 loss: 17.2900, MinusLogProbMetric: 17.2900, val_loss: 17.3879, val_MinusLogProbMetric: 17.3879

Epoch 937: val_loss did not improve from 17.36158
196/196 - 103s - loss: 17.2900 - MinusLogProbMetric: 17.2900 - val_loss: 17.3879 - val_MinusLogProbMetric: 17.3879 - lr: 8.3333e-05 - 103s/epoch - 526ms/step
Epoch 938/1000
2023-09-28 14:47:42.995 
Epoch 938/1000 
	 loss: 17.2839, MinusLogProbMetric: 17.2839, val_loss: 17.5994, val_MinusLogProbMetric: 17.5994

Epoch 938: val_loss did not improve from 17.36158
196/196 - 103s - loss: 17.2839 - MinusLogProbMetric: 17.2839 - val_loss: 17.5994 - val_MinusLogProbMetric: 17.5994 - lr: 8.3333e-05 - 103s/epoch - 524ms/step
Epoch 939/1000
2023-09-28 14:49:24.491 
Epoch 939/1000 
	 loss: 17.2910, MinusLogProbMetric: 17.2910, val_loss: 17.4059, val_MinusLogProbMetric: 17.4059

Epoch 939: val_loss did not improve from 17.36158
196/196 - 101s - loss: 17.2910 - MinusLogProbMetric: 17.2910 - val_loss: 17.4059 - val_MinusLogProbMetric: 17.4059 - lr: 8.3333e-05 - 101s/epoch - 518ms/step
Epoch 940/1000
2023-09-28 14:51:07.294 
Epoch 940/1000 
	 loss: 17.2852, MinusLogProbMetric: 17.2852, val_loss: 17.3950, val_MinusLogProbMetric: 17.3950

Epoch 940: val_loss did not improve from 17.36158
196/196 - 103s - loss: 17.2852 - MinusLogProbMetric: 17.2852 - val_loss: 17.3950 - val_MinusLogProbMetric: 17.3950 - lr: 8.3333e-05 - 103s/epoch - 524ms/step
Epoch 941/1000
2023-09-28 14:52:50.707 
Epoch 941/1000 
	 loss: 17.2688, MinusLogProbMetric: 17.2688, val_loss: 17.4580, val_MinusLogProbMetric: 17.4580

Epoch 941: val_loss did not improve from 17.36158
196/196 - 103s - loss: 17.2688 - MinusLogProbMetric: 17.2688 - val_loss: 17.4580 - val_MinusLogProbMetric: 17.4580 - lr: 8.3333e-05 - 103s/epoch - 528ms/step
Epoch 942/1000
2023-09-28 14:54:35.492 
Epoch 942/1000 
	 loss: 17.2843, MinusLogProbMetric: 17.2843, val_loss: 17.5476, val_MinusLogProbMetric: 17.5476

Epoch 942: val_loss did not improve from 17.36158
196/196 - 105s - loss: 17.2843 - MinusLogProbMetric: 17.2843 - val_loss: 17.5476 - val_MinusLogProbMetric: 17.5476 - lr: 8.3333e-05 - 105s/epoch - 535ms/step
Epoch 943/1000
2023-09-28 14:56:19.480 
Epoch 943/1000 
	 loss: 17.2931, MinusLogProbMetric: 17.2931, val_loss: 17.5296, val_MinusLogProbMetric: 17.5296

Epoch 943: val_loss did not improve from 17.36158
196/196 - 104s - loss: 17.2931 - MinusLogProbMetric: 17.2931 - val_loss: 17.5296 - val_MinusLogProbMetric: 17.5296 - lr: 8.3333e-05 - 104s/epoch - 531ms/step
Epoch 944/1000
2023-09-28 14:58:03.183 
Epoch 944/1000 
	 loss: 17.2866, MinusLogProbMetric: 17.2866, val_loss: 17.3892, val_MinusLogProbMetric: 17.3892

Epoch 944: val_loss did not improve from 17.36158
196/196 - 104s - loss: 17.2866 - MinusLogProbMetric: 17.2866 - val_loss: 17.3892 - val_MinusLogProbMetric: 17.3892 - lr: 8.3333e-05 - 104s/epoch - 529ms/step
Epoch 945/1000
2023-09-28 14:59:47.084 
Epoch 945/1000 
	 loss: 17.2897, MinusLogProbMetric: 17.2897, val_loss: 17.3919, val_MinusLogProbMetric: 17.3919

Epoch 945: val_loss did not improve from 17.36158
196/196 - 104s - loss: 17.2897 - MinusLogProbMetric: 17.2897 - val_loss: 17.3919 - val_MinusLogProbMetric: 17.3919 - lr: 8.3333e-05 - 104s/epoch - 530ms/step
Epoch 946/1000
2023-09-28 15:01:30.607 
Epoch 946/1000 
	 loss: 17.2807, MinusLogProbMetric: 17.2807, val_loss: 17.4959, val_MinusLogProbMetric: 17.4959

Epoch 946: val_loss did not improve from 17.36158
196/196 - 104s - loss: 17.2807 - MinusLogProbMetric: 17.2807 - val_loss: 17.4959 - val_MinusLogProbMetric: 17.4959 - lr: 8.3333e-05 - 104s/epoch - 528ms/step
Epoch 947/1000
2023-09-28 15:03:14.342 
Epoch 947/1000 
	 loss: 17.2847, MinusLogProbMetric: 17.2847, val_loss: 17.4185, val_MinusLogProbMetric: 17.4185

Epoch 947: val_loss did not improve from 17.36158
196/196 - 104s - loss: 17.2847 - MinusLogProbMetric: 17.2847 - val_loss: 17.4185 - val_MinusLogProbMetric: 17.4185 - lr: 8.3333e-05 - 104s/epoch - 529ms/step
Epoch 948/1000
2023-09-28 15:04:56.920 
Epoch 948/1000 
	 loss: 17.2816, MinusLogProbMetric: 17.2816, val_loss: 17.4412, val_MinusLogProbMetric: 17.4412

Epoch 948: val_loss did not improve from 17.36158
196/196 - 103s - loss: 17.2816 - MinusLogProbMetric: 17.2816 - val_loss: 17.4412 - val_MinusLogProbMetric: 17.4412 - lr: 8.3333e-05 - 103s/epoch - 523ms/step
Epoch 949/1000
2023-09-28 15:06:40.639 
Epoch 949/1000 
	 loss: 17.2804, MinusLogProbMetric: 17.2804, val_loss: 17.3828, val_MinusLogProbMetric: 17.3828

Epoch 949: val_loss did not improve from 17.36158
196/196 - 104s - loss: 17.2804 - MinusLogProbMetric: 17.2804 - val_loss: 17.3828 - val_MinusLogProbMetric: 17.3828 - lr: 8.3333e-05 - 104s/epoch - 529ms/step
Epoch 950/1000
2023-09-28 15:08:23.976 
Epoch 950/1000 
	 loss: 17.2759, MinusLogProbMetric: 17.2759, val_loss: 17.3888, val_MinusLogProbMetric: 17.3888

Epoch 950: val_loss did not improve from 17.36158
196/196 - 103s - loss: 17.2759 - MinusLogProbMetric: 17.2759 - val_loss: 17.3888 - val_MinusLogProbMetric: 17.3888 - lr: 8.3333e-05 - 103s/epoch - 527ms/step
Epoch 951/1000
2023-09-28 15:10:06.955 
Epoch 951/1000 
	 loss: 17.2761, MinusLogProbMetric: 17.2761, val_loss: 17.3864, val_MinusLogProbMetric: 17.3864

Epoch 951: val_loss did not improve from 17.36158
196/196 - 103s - loss: 17.2761 - MinusLogProbMetric: 17.2761 - val_loss: 17.3864 - val_MinusLogProbMetric: 17.3864 - lr: 8.3333e-05 - 103s/epoch - 525ms/step
Epoch 952/1000
2023-09-28 15:11:52.102 
Epoch 952/1000 
	 loss: 17.2825, MinusLogProbMetric: 17.2825, val_loss: 17.4185, val_MinusLogProbMetric: 17.4185

Epoch 952: val_loss did not improve from 17.36158
196/196 - 105s - loss: 17.2825 - MinusLogProbMetric: 17.2825 - val_loss: 17.4185 - val_MinusLogProbMetric: 17.4185 - lr: 8.3333e-05 - 105s/epoch - 536ms/step
Epoch 953/1000
2023-09-28 15:13:35.716 
Epoch 953/1000 
	 loss: 17.2661, MinusLogProbMetric: 17.2661, val_loss: 17.4065, val_MinusLogProbMetric: 17.4065

Epoch 953: val_loss did not improve from 17.36158
196/196 - 104s - loss: 17.2661 - MinusLogProbMetric: 17.2661 - val_loss: 17.4065 - val_MinusLogProbMetric: 17.4065 - lr: 8.3333e-05 - 104s/epoch - 529ms/step
Epoch 954/1000
2023-09-28 15:15:19.770 
Epoch 954/1000 
	 loss: 17.2833, MinusLogProbMetric: 17.2833, val_loss: 17.5310, val_MinusLogProbMetric: 17.5310

Epoch 954: val_loss did not improve from 17.36158
196/196 - 104s - loss: 17.2833 - MinusLogProbMetric: 17.2833 - val_loss: 17.5310 - val_MinusLogProbMetric: 17.5310 - lr: 8.3333e-05 - 104s/epoch - 531ms/step
Epoch 955/1000
2023-09-28 15:17:03.630 
Epoch 955/1000 
	 loss: 17.3422, MinusLogProbMetric: 17.3422, val_loss: 17.4219, val_MinusLogProbMetric: 17.4219

Epoch 955: val_loss did not improve from 17.36158
196/196 - 104s - loss: 17.3422 - MinusLogProbMetric: 17.3422 - val_loss: 17.4219 - val_MinusLogProbMetric: 17.4219 - lr: 8.3333e-05 - 104s/epoch - 530ms/step
Epoch 956/1000
2023-09-28 15:18:46.484 
Epoch 956/1000 
	 loss: 17.2819, MinusLogProbMetric: 17.2819, val_loss: 17.3613, val_MinusLogProbMetric: 17.3613

Epoch 956: val_loss improved from 17.36158 to 17.36131, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 104s - loss: 17.2819 - MinusLogProbMetric: 17.2819 - val_loss: 17.3613 - val_MinusLogProbMetric: 17.3613 - lr: 8.3333e-05 - 104s/epoch - 532ms/step
Epoch 957/1000
2023-09-28 15:20:31.569 
Epoch 957/1000 
	 loss: 17.2895, MinusLogProbMetric: 17.2895, val_loss: 17.3622, val_MinusLogProbMetric: 17.3622

Epoch 957: val_loss did not improve from 17.36131
196/196 - 104s - loss: 17.2895 - MinusLogProbMetric: 17.2895 - val_loss: 17.3622 - val_MinusLogProbMetric: 17.3622 - lr: 8.3333e-05 - 104s/epoch - 529ms/step
Epoch 958/1000
2023-09-28 15:22:16.279 
Epoch 958/1000 
	 loss: 17.2908, MinusLogProbMetric: 17.2908, val_loss: 17.4085, val_MinusLogProbMetric: 17.4085

Epoch 958: val_loss did not improve from 17.36131
196/196 - 105s - loss: 17.2908 - MinusLogProbMetric: 17.2908 - val_loss: 17.4085 - val_MinusLogProbMetric: 17.4085 - lr: 8.3333e-05 - 105s/epoch - 534ms/step
Epoch 959/1000
2023-09-28 15:24:01.036 
Epoch 959/1000 
	 loss: 17.2731, MinusLogProbMetric: 17.2731, val_loss: 17.4837, val_MinusLogProbMetric: 17.4837

Epoch 959: val_loss did not improve from 17.36131
196/196 - 105s - loss: 17.2731 - MinusLogProbMetric: 17.2731 - val_loss: 17.4837 - val_MinusLogProbMetric: 17.4837 - lr: 8.3333e-05 - 105s/epoch - 534ms/step
Epoch 960/1000
2023-09-28 15:25:41.135 
Epoch 960/1000 
	 loss: 17.2817, MinusLogProbMetric: 17.2817, val_loss: 17.4164, val_MinusLogProbMetric: 17.4164

Epoch 960: val_loss did not improve from 17.36131
196/196 - 100s - loss: 17.2817 - MinusLogProbMetric: 17.2817 - val_loss: 17.4164 - val_MinusLogProbMetric: 17.4164 - lr: 8.3333e-05 - 100s/epoch - 511ms/step
Epoch 961/1000
2023-09-28 15:27:24.794 
Epoch 961/1000 
	 loss: 17.2850, MinusLogProbMetric: 17.2850, val_loss: 17.3989, val_MinusLogProbMetric: 17.3989

Epoch 961: val_loss did not improve from 17.36131
196/196 - 104s - loss: 17.2850 - MinusLogProbMetric: 17.2850 - val_loss: 17.3989 - val_MinusLogProbMetric: 17.3989 - lr: 8.3333e-05 - 104s/epoch - 529ms/step
Epoch 962/1000
2023-09-28 15:29:06.632 
Epoch 962/1000 
	 loss: 17.2759, MinusLogProbMetric: 17.2759, val_loss: 17.3901, val_MinusLogProbMetric: 17.3901

Epoch 962: val_loss did not improve from 17.36131
196/196 - 102s - loss: 17.2759 - MinusLogProbMetric: 17.2759 - val_loss: 17.3901 - val_MinusLogProbMetric: 17.3901 - lr: 8.3333e-05 - 102s/epoch - 520ms/step
Epoch 963/1000
2023-09-28 15:30:49.975 
Epoch 963/1000 
	 loss: 17.2735, MinusLogProbMetric: 17.2735, val_loss: 17.3629, val_MinusLogProbMetric: 17.3629

Epoch 963: val_loss did not improve from 17.36131
196/196 - 103s - loss: 17.2735 - MinusLogProbMetric: 17.2735 - val_loss: 17.3629 - val_MinusLogProbMetric: 17.3629 - lr: 8.3333e-05 - 103s/epoch - 527ms/step
Epoch 964/1000
2023-09-28 15:32:31.573 
Epoch 964/1000 
	 loss: 17.2827, MinusLogProbMetric: 17.2827, val_loss: 17.4007, val_MinusLogProbMetric: 17.4007

Epoch 964: val_loss did not improve from 17.36131
196/196 - 102s - loss: 17.2827 - MinusLogProbMetric: 17.2827 - val_loss: 17.4007 - val_MinusLogProbMetric: 17.4007 - lr: 8.3333e-05 - 102s/epoch - 518ms/step
Epoch 965/1000
2023-09-28 15:34:14.005 
Epoch 965/1000 
	 loss: 17.2860, MinusLogProbMetric: 17.2860, val_loss: 17.4018, val_MinusLogProbMetric: 17.4018

Epoch 965: val_loss did not improve from 17.36131
196/196 - 102s - loss: 17.2860 - MinusLogProbMetric: 17.2860 - val_loss: 17.4018 - val_MinusLogProbMetric: 17.4018 - lr: 8.3333e-05 - 102s/epoch - 523ms/step
Epoch 966/1000
2023-09-28 15:35:57.352 
Epoch 966/1000 
	 loss: 17.2797, MinusLogProbMetric: 17.2797, val_loss: 17.5777, val_MinusLogProbMetric: 17.5777

Epoch 966: val_loss did not improve from 17.36131
196/196 - 103s - loss: 17.2797 - MinusLogProbMetric: 17.2797 - val_loss: 17.5777 - val_MinusLogProbMetric: 17.5777 - lr: 8.3333e-05 - 103s/epoch - 527ms/step
Epoch 967/1000
2023-09-28 15:37:39.241 
Epoch 967/1000 
	 loss: 17.2807, MinusLogProbMetric: 17.2807, val_loss: 17.3367, val_MinusLogProbMetric: 17.3367

Epoch 967: val_loss improved from 17.36131 to 17.33669, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_311/weights/best_weights.h5
196/196 - 104s - loss: 17.2807 - MinusLogProbMetric: 17.2807 - val_loss: 17.3367 - val_MinusLogProbMetric: 17.3367 - lr: 8.3333e-05 - 104s/epoch - 529ms/step
Epoch 968/1000
2023-09-28 15:39:24.735 
Epoch 968/1000 
	 loss: 17.2742, MinusLogProbMetric: 17.2742, val_loss: 17.4271, val_MinusLogProbMetric: 17.4271

Epoch 968: val_loss did not improve from 17.33669
196/196 - 104s - loss: 17.2742 - MinusLogProbMetric: 17.2742 - val_loss: 17.4271 - val_MinusLogProbMetric: 17.4271 - lr: 8.3333e-05 - 104s/epoch - 528ms/step
Epoch 969/1000
2023-09-28 15:41:04.582 
Epoch 969/1000 
	 loss: 17.2637, MinusLogProbMetric: 17.2637, val_loss: 17.4073, val_MinusLogProbMetric: 17.4073

Epoch 969: val_loss did not improve from 17.33669
196/196 - 100s - loss: 17.2637 - MinusLogProbMetric: 17.2637 - val_loss: 17.4073 - val_MinusLogProbMetric: 17.4073 - lr: 8.3333e-05 - 100s/epoch - 509ms/step
Epoch 970/1000
2023-09-28 15:42:48.861 
Epoch 970/1000 
	 loss: 17.2714, MinusLogProbMetric: 17.2714, val_loss: 17.3885, val_MinusLogProbMetric: 17.3885

Epoch 970: val_loss did not improve from 17.33669
196/196 - 104s - loss: 17.2714 - MinusLogProbMetric: 17.2714 - val_loss: 17.3885 - val_MinusLogProbMetric: 17.3885 - lr: 8.3333e-05 - 104s/epoch - 532ms/step
Epoch 971/1000
2023-09-28 15:44:33.527 
Epoch 971/1000 
	 loss: 17.2835, MinusLogProbMetric: 17.2835, val_loss: 17.4816, val_MinusLogProbMetric: 17.4816

Epoch 971: val_loss did not improve from 17.33669
196/196 - 105s - loss: 17.2835 - MinusLogProbMetric: 17.2835 - val_loss: 17.4816 - val_MinusLogProbMetric: 17.4816 - lr: 8.3333e-05 - 105s/epoch - 534ms/step
Epoch 972/1000
2023-09-28 15:46:19.708 
Epoch 972/1000 
	 loss: 17.2795, MinusLogProbMetric: 17.2795, val_loss: 17.4533, val_MinusLogProbMetric: 17.4533

Epoch 972: val_loss did not improve from 17.33669
196/196 - 106s - loss: 17.2795 - MinusLogProbMetric: 17.2795 - val_loss: 17.4533 - val_MinusLogProbMetric: 17.4533 - lr: 8.3333e-05 - 106s/epoch - 542ms/step
Epoch 973/1000
2023-09-28 15:48:05.211 
Epoch 973/1000 
	 loss: 17.2679, MinusLogProbMetric: 17.2679, val_loss: 17.4575, val_MinusLogProbMetric: 17.4575

Epoch 973: val_loss did not improve from 17.33669
196/196 - 105s - loss: 17.2679 - MinusLogProbMetric: 17.2679 - val_loss: 17.4575 - val_MinusLogProbMetric: 17.4575 - lr: 8.3333e-05 - 105s/epoch - 538ms/step
Epoch 974/1000
2023-09-28 15:49:49.246 
Epoch 974/1000 
	 loss: 17.2688, MinusLogProbMetric: 17.2688, val_loss: 17.4249, val_MinusLogProbMetric: 17.4249

Epoch 974: val_loss did not improve from 17.33669
196/196 - 104s - loss: 17.2688 - MinusLogProbMetric: 17.2688 - val_loss: 17.4249 - val_MinusLogProbMetric: 17.4249 - lr: 8.3333e-05 - 104s/epoch - 531ms/step
Epoch 975/1000
2023-09-28 15:51:32.292 
Epoch 975/1000 
	 loss: 17.2796, MinusLogProbMetric: 17.2796, val_loss: 17.5361, val_MinusLogProbMetric: 17.5361

Epoch 975: val_loss did not improve from 17.33669
196/196 - 103s - loss: 17.2796 - MinusLogProbMetric: 17.2796 - val_loss: 17.5361 - val_MinusLogProbMetric: 17.5361 - lr: 8.3333e-05 - 103s/epoch - 526ms/step
Epoch 976/1000
2023-09-28 15:53:16.009 
Epoch 976/1000 
	 loss: 17.2898, MinusLogProbMetric: 17.2898, val_loss: 17.3774, val_MinusLogProbMetric: 17.3774

Epoch 976: val_loss did not improve from 17.33669
196/196 - 104s - loss: 17.2898 - MinusLogProbMetric: 17.2898 - val_loss: 17.3774 - val_MinusLogProbMetric: 17.3774 - lr: 8.3333e-05 - 104s/epoch - 529ms/step
Epoch 977/1000
2023-09-28 15:55:00.754 
Epoch 977/1000 
	 loss: 17.2680, MinusLogProbMetric: 17.2680, val_loss: 17.4178, val_MinusLogProbMetric: 17.4178

Epoch 977: val_loss did not improve from 17.33669
196/196 - 105s - loss: 17.2680 - MinusLogProbMetric: 17.2680 - val_loss: 17.4178 - val_MinusLogProbMetric: 17.4178 - lr: 8.3333e-05 - 105s/epoch - 534ms/step
Epoch 978/1000
2023-09-28 15:56:45.284 
Epoch 978/1000 
	 loss: 17.2731, MinusLogProbMetric: 17.2731, val_loss: 17.5183, val_MinusLogProbMetric: 17.5183

Epoch 978: val_loss did not improve from 17.33669
196/196 - 105s - loss: 17.2731 - MinusLogProbMetric: 17.2731 - val_loss: 17.5183 - val_MinusLogProbMetric: 17.5183 - lr: 8.3333e-05 - 105s/epoch - 533ms/step
Epoch 979/1000
2023-09-28 15:58:28.412 
Epoch 979/1000 
	 loss: 17.2724, MinusLogProbMetric: 17.2724, val_loss: 17.4209, val_MinusLogProbMetric: 17.4209

Epoch 979: val_loss did not improve from 17.33669
196/196 - 103s - loss: 17.2724 - MinusLogProbMetric: 17.2724 - val_loss: 17.4209 - val_MinusLogProbMetric: 17.4209 - lr: 8.3333e-05 - 103s/epoch - 526ms/step
Epoch 980/1000
2023-09-28 16:00:08.340 
Epoch 980/1000 
	 loss: 17.2701, MinusLogProbMetric: 17.2701, val_loss: 17.4093, val_MinusLogProbMetric: 17.4093

Epoch 980: val_loss did not improve from 17.33669
196/196 - 100s - loss: 17.2701 - MinusLogProbMetric: 17.2701 - val_loss: 17.4093 - val_MinusLogProbMetric: 17.4093 - lr: 8.3333e-05 - 100s/epoch - 510ms/step
Epoch 981/1000
2023-09-28 16:01:47.639 
Epoch 981/1000 
	 loss: 17.2674, MinusLogProbMetric: 17.2674, val_loss: 17.3979, val_MinusLogProbMetric: 17.3979

Epoch 981: val_loss did not improve from 17.33669
196/196 - 99s - loss: 17.2674 - MinusLogProbMetric: 17.2674 - val_loss: 17.3979 - val_MinusLogProbMetric: 17.3979 - lr: 8.3333e-05 - 99s/epoch - 507ms/step
Epoch 982/1000
2023-09-28 16:03:28.778 
Epoch 982/1000 
	 loss: 17.2677, MinusLogProbMetric: 17.2677, val_loss: 17.3846, val_MinusLogProbMetric: 17.3846

Epoch 982: val_loss did not improve from 17.33669
196/196 - 101s - loss: 17.2677 - MinusLogProbMetric: 17.2677 - val_loss: 17.3846 - val_MinusLogProbMetric: 17.3846 - lr: 8.3333e-05 - 101s/epoch - 516ms/step
Epoch 983/1000
2023-09-28 16:05:09.323 
Epoch 983/1000 
	 loss: 17.2966, MinusLogProbMetric: 17.2966, val_loss: 17.4771, val_MinusLogProbMetric: 17.4771

Epoch 983: val_loss did not improve from 17.33669
196/196 - 101s - loss: 17.2966 - MinusLogProbMetric: 17.2966 - val_loss: 17.4771 - val_MinusLogProbMetric: 17.4771 - lr: 8.3333e-05 - 101s/epoch - 513ms/step
Epoch 984/1000
2023-09-28 16:06:50.862 
Epoch 984/1000 
	 loss: 17.2765, MinusLogProbMetric: 17.2765, val_loss: 17.3753, val_MinusLogProbMetric: 17.3753

Epoch 984: val_loss did not improve from 17.33669
196/196 - 102s - loss: 17.2765 - MinusLogProbMetric: 17.2765 - val_loss: 17.3753 - val_MinusLogProbMetric: 17.3753 - lr: 8.3333e-05 - 102s/epoch - 518ms/step
Epoch 985/1000
2023-09-28 16:08:33.263 
Epoch 985/1000 
	 loss: 17.2737, MinusLogProbMetric: 17.2737, val_loss: 17.3944, val_MinusLogProbMetric: 17.3944

Epoch 985: val_loss did not improve from 17.33669
196/196 - 102s - loss: 17.2737 - MinusLogProbMetric: 17.2737 - val_loss: 17.3944 - val_MinusLogProbMetric: 17.3944 - lr: 8.3333e-05 - 102s/epoch - 522ms/step
Epoch 986/1000
2023-09-28 16:10:14.760 
Epoch 986/1000 
	 loss: 17.2785, MinusLogProbMetric: 17.2785, val_loss: 17.3827, val_MinusLogProbMetric: 17.3827

Epoch 986: val_loss did not improve from 17.33669
196/196 - 101s - loss: 17.2785 - MinusLogProbMetric: 17.2785 - val_loss: 17.3827 - val_MinusLogProbMetric: 17.3827 - lr: 8.3333e-05 - 101s/epoch - 518ms/step
Epoch 987/1000
2023-09-28 16:11:55.540 
Epoch 987/1000 
	 loss: 17.2721, MinusLogProbMetric: 17.2721, val_loss: 17.3588, val_MinusLogProbMetric: 17.3588

Epoch 987: val_loss did not improve from 17.33669
196/196 - 101s - loss: 17.2721 - MinusLogProbMetric: 17.2721 - val_loss: 17.3588 - val_MinusLogProbMetric: 17.3588 - lr: 8.3333e-05 - 101s/epoch - 514ms/step
Epoch 988/1000
2023-09-28 16:13:34.614 
Epoch 988/1000 
	 loss: 17.2687, MinusLogProbMetric: 17.2687, val_loss: 17.4805, val_MinusLogProbMetric: 17.4805

Epoch 988: val_loss did not improve from 17.33669
196/196 - 99s - loss: 17.2687 - MinusLogProbMetric: 17.2687 - val_loss: 17.4805 - val_MinusLogProbMetric: 17.4805 - lr: 8.3333e-05 - 99s/epoch - 505ms/step
Epoch 989/1000
2023-09-28 16:15:16.168 
Epoch 989/1000 
	 loss: 17.2767, MinusLogProbMetric: 17.2767, val_loss: 17.5557, val_MinusLogProbMetric: 17.5557

Epoch 989: val_loss did not improve from 17.33669
196/196 - 102s - loss: 17.2767 - MinusLogProbMetric: 17.2767 - val_loss: 17.5557 - val_MinusLogProbMetric: 17.5557 - lr: 8.3333e-05 - 102s/epoch - 518ms/step
Epoch 990/1000
2023-09-28 16:17:00.325 
Epoch 990/1000 
	 loss: 17.2795, MinusLogProbMetric: 17.2795, val_loss: 17.3678, val_MinusLogProbMetric: 17.3678

Epoch 990: val_loss did not improve from 17.33669
196/196 - 104s - loss: 17.2795 - MinusLogProbMetric: 17.2795 - val_loss: 17.3678 - val_MinusLogProbMetric: 17.3678 - lr: 8.3333e-05 - 104s/epoch - 532ms/step
Epoch 991/1000
2023-09-28 16:18:44.755 
Epoch 991/1000 
	 loss: 17.2645, MinusLogProbMetric: 17.2645, val_loss: 17.3756, val_MinusLogProbMetric: 17.3756

Epoch 991: val_loss did not improve from 17.33669
196/196 - 104s - loss: 17.2645 - MinusLogProbMetric: 17.2645 - val_loss: 17.3756 - val_MinusLogProbMetric: 17.3756 - lr: 8.3333e-05 - 104s/epoch - 533ms/step
Epoch 992/1000
2023-09-28 16:20:29.676 
Epoch 992/1000 
	 loss: 17.2782, MinusLogProbMetric: 17.2782, val_loss: 17.3813, val_MinusLogProbMetric: 17.3813

Epoch 992: val_loss did not improve from 17.33669
196/196 - 105s - loss: 17.2782 - MinusLogProbMetric: 17.2782 - val_loss: 17.3813 - val_MinusLogProbMetric: 17.3813 - lr: 8.3333e-05 - 105s/epoch - 535ms/step
Epoch 993/1000
2023-09-28 16:22:13.646 
Epoch 993/1000 
	 loss: 17.2595, MinusLogProbMetric: 17.2595, val_loss: 17.4060, val_MinusLogProbMetric: 17.4060

Epoch 993: val_loss did not improve from 17.33669
196/196 - 104s - loss: 17.2595 - MinusLogProbMetric: 17.2595 - val_loss: 17.4060 - val_MinusLogProbMetric: 17.4060 - lr: 8.3333e-05 - 104s/epoch - 530ms/step
Epoch 994/1000
2023-09-28 16:23:55.806 
Epoch 994/1000 
	 loss: 17.2608, MinusLogProbMetric: 17.2608, val_loss: 17.4191, val_MinusLogProbMetric: 17.4191

Epoch 994: val_loss did not improve from 17.33669
196/196 - 102s - loss: 17.2608 - MinusLogProbMetric: 17.2608 - val_loss: 17.4191 - val_MinusLogProbMetric: 17.4191 - lr: 8.3333e-05 - 102s/epoch - 521ms/step
Epoch 995/1000
2023-09-28 16:25:37.297 
Epoch 995/1000 
	 loss: 17.2856, MinusLogProbMetric: 17.2856, val_loss: 17.3944, val_MinusLogProbMetric: 17.3944

Epoch 995: val_loss did not improve from 17.33669
196/196 - 101s - loss: 17.2856 - MinusLogProbMetric: 17.2856 - val_loss: 17.3944 - val_MinusLogProbMetric: 17.3944 - lr: 8.3333e-05 - 101s/epoch - 518ms/step
Epoch 996/1000
2023-09-28 16:27:17.980 
Epoch 996/1000 
	 loss: 17.2684, MinusLogProbMetric: 17.2684, val_loss: 17.3870, val_MinusLogProbMetric: 17.3870

Epoch 996: val_loss did not improve from 17.33669
196/196 - 101s - loss: 17.2684 - MinusLogProbMetric: 17.2684 - val_loss: 17.3870 - val_MinusLogProbMetric: 17.3870 - lr: 8.3333e-05 - 101s/epoch - 514ms/step
Epoch 997/1000
2023-09-28 16:28:59.668 
Epoch 997/1000 
	 loss: 17.2643, MinusLogProbMetric: 17.2643, val_loss: 17.4792, val_MinusLogProbMetric: 17.4792

Epoch 997: val_loss did not improve from 17.33669
196/196 - 102s - loss: 17.2643 - MinusLogProbMetric: 17.2643 - val_loss: 17.4792 - val_MinusLogProbMetric: 17.4792 - lr: 8.3333e-05 - 102s/epoch - 519ms/step
Epoch 998/1000
2023-09-28 16:30:41.716 
Epoch 998/1000 
	 loss: 17.2729, MinusLogProbMetric: 17.2729, val_loss: 17.3816, val_MinusLogProbMetric: 17.3816

Epoch 998: val_loss did not improve from 17.33669
196/196 - 102s - loss: 17.2729 - MinusLogProbMetric: 17.2729 - val_loss: 17.3816 - val_MinusLogProbMetric: 17.3816 - lr: 8.3333e-05 - 102s/epoch - 521ms/step
Epoch 999/1000
2023-09-28 16:32:19.734 
Epoch 999/1000 
	 loss: 17.2702, MinusLogProbMetric: 17.2702, val_loss: 17.5075, val_MinusLogProbMetric: 17.5075

Epoch 999: val_loss did not improve from 17.33669
196/196 - 98s - loss: 17.2702 - MinusLogProbMetric: 17.2702 - val_loss: 17.5075 - val_MinusLogProbMetric: 17.5075 - lr: 8.3333e-05 - 98s/epoch - 500ms/step
Epoch 1000/1000
2023-09-28 16:34:03.535 
Epoch 1000/1000 
	 loss: 17.2791, MinusLogProbMetric: 17.2791, val_loss: 17.3912, val_MinusLogProbMetric: 17.3912

Epoch 1000: val_loss did not improve from 17.33669
196/196 - 104s - loss: 17.2791 - MinusLogProbMetric: 17.2791 - val_loss: 17.3912 - val_MinusLogProbMetric: 17.3912 - lr: 8.3333e-05 - 104s/epoch - 530ms/step
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.

------------------------------------------
Starting LR metric calculation...
Running TF LR calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
Warning: Removed a fraction of alternative samples due to non-finite log probabilities.
Warning: Removed a fraction of alternative samples due to non-finite log probabilities.
Warning: Removed a fraction of alternative samples due to non-finite log probabilities.
Warning: Removed a fraction of alternative samples due to non-finite log probabilities.
Warning: Removed a fraction of alternative samples due to non-finite log probabilities.
Warning: Removed a fraction of alternative samples due to non-finite log probabilities.
Warning: Removed a fraction of alternative samples due to non-finite log probabilities.
Warning: Removed a fraction of alternative samples due to non-finite log probabilities.
Warning: Removed a fraction of alternative samples due to non-finite log probabilities.
Warning: Removed a fraction of alternative samples due to non-finite log probabilities.
LR metric calculation completed in 70.72521244402742 seconds.

------------------------------------------
Starting KS tests calculation...
Running TF KS tests...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
KS tests calculation completed in 30.618679646984674 seconds.

------------------------------------------
Starting SWD metric calculation...
Running TF SWD calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
SWD metric calculation completed in 26.271852260048036 seconds.

------------------------------------------
Starting FN metric calculation...
Running TF FN calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
FN metric calculation completed in 26.48926871200092 seconds.
Training succeeded with seed 926.
Model trained in 84252.55 s.

===========
Computing predictions
===========

Computing metrics...
Metrics computed in 156.77 s.
                ===========
                print("===========
Failed to plot
===========
")
                Exception type: ValueError
                Exception message: Number of rows must be a positive integer, not 0
                Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN.py , Line : 469, Func.Name : prediction_function, Message : Plotters.cornerplotter(X_data_test.numpy(),X_data_nf.numpy(),path_to_results,ndims,norm=True) # type: ignore', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/Plotters.py , Line : 156, Func.Name : cornerplotter, Message : figure=corner.corner(target_samples,color=\'red\',bins=n_bins,labels=[r"%s" % s for s in labels],normalize1d=True)', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/corner.py , Line : 235, Func.Name : corner, Message : fig, axes = pl.subplots(K, K, figsize=(dim, dim))', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/pyplot.py , Line : 1502, Func.Name : subplots, Message : axs = fig.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 905, Func.Name : subplots, Message : gs = self.add_gridspec(nrows, ncols, figure=self, **gridspec_kw)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 1527, Func.Name : add_gridspec, Message : gs = GridSpec(nrows=nrows, ncols=ncols, figure=self, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 379, Func.Name : __init__, Message : super().__init__(nrows, ncols,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 49, Func.Name : __init__, Message : raise ValueError(']
                ===========

results.txt saved
results.json saved
Results log saved
Model predictions computed in 157.02 s.
===========
Run 311/720 done in 84599.27 s.
===========

Directory ../../results/CsplineN_new/run_312/ already exists.
Skipping it.
===========
Run 312/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_313/ already exists.
Skipping it.
===========
Run 313/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_314/ already exists.
Skipping it.
===========
Run 314/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_315/ already exists.
Skipping it.
===========
Run 315/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_316/ already exists.
Skipping it.
===========
Run 316/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_317/ already exists.
Skipping it.
===========
Run 317/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_318/ already exists.
Skipping it.
===========
Run 318/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_319/ already exists.
Skipping it.
===========
Run 319/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_320/ already exists.
Skipping it.
===========
Run 320/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_321/ already exists.
Skipping it.
===========
Run 321/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_322/ already exists.
Skipping it.
===========
Run 322/720 already exists. Skipping it.
===========

===========
Generating train data for run 323.
===========
Train data generated in 0.23 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_323/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_323/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_323/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_323
self.data_kwargs: {'seed': 0}
self.x_data: [[ 6.6721983   2.7232099   6.150583   ...  3.6440284   4.565682
   2.645935  ]
 [ 1.86333     2.9602146   9.095053   ...  6.366897   -0.51058275
   3.6708503 ]
 [ 5.5981846   7.771953    4.990416   ...  1.6793485   6.8585963
   1.4719678 ]
 ...
 [ 1.690249    4.096276    8.408299   ...  5.907632    0.7324666
   3.17104   ]
 [ 1.5949699   2.976067    6.189886   ...  6.072563    1.3667407
   3.7841148 ]
 [ 6.6605377   2.9536047   6.141097   ...  3.8587723   3.7946694
   1.5606189 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_66"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_67 (InputLayer)       [(None, 64)]              0         
                                                                 
 log_prob_layer_6 (LogProbLa  (None,)                  908640    
 yer)                                                            
                                                                 
=================================================================
Total params: 908,640
Trainable params: 908,640
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_6/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_6'")
self.model: <keras.engine.functional.Functional object at 0x7f7c28223520>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f7bcc738790>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f7bcc738790>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f7d00307a90>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f7d28adda20>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_323/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f7d28add7b0>, <keras.callbacks.ModelCheckpoint object at 0x7f7d28add8a0>, <keras.callbacks.EarlyStopping object at 0x7f7d28adf160>, <keras.callbacks.ReduceLROnPlateau object at 0x7f7d28ade7d0>, <keras.callbacks.TerminateOnNaN object at 0x7f7d28add2a0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_323/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 323/720 with hyperparameters:
timestamp = 2023-09-28 16:36:47.284767
ndims = 64
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 908640
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 6.6721983   2.7232099   6.150583    4.580923    1.5652926   2.6565456
  7.1790314   4.9339914   5.7110434   6.7498174   6.386396    3.9689803
  8.906224    3.806384    4.1225185   9.359665    7.9177184   7.261933
  0.1027934   9.199222    7.196588   10.08044     2.3912797   8.714102
  2.4626331   6.1591697   1.6348177   8.815128    7.787716    5.5076594
  3.3412566   0.7007286   7.2919664   4.3464866   6.6338673   7.1474853
  9.7648      8.511822   -0.5374965   5.0141335   7.365311    0.6638582
  5.9037857   0.7669884   1.383983    0.33108917  8.1157875   2.689867
  4.5051117   9.846115    7.6886387   0.35349095  2.5627694   4.5428576
  5.8595724   2.7118683   9.446846    6.2642756   4.7844443   6.0548897
  8.103386    3.6440284   4.565682    2.645935  ]
Epoch 1/1000
2023-09-28 16:38:55.771 
Epoch 1/1000 
	 loss: 1159.5690, MinusLogProbMetric: 1159.5690, val_loss: 288.7306, val_MinusLogProbMetric: 288.7306

Epoch 1: val_loss improved from inf to 288.73062, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_323/weights/best_weights.h5
196/196 - 129s - loss: 1159.5690 - MinusLogProbMetric: 1159.5690 - val_loss: 288.7306 - val_MinusLogProbMetric: 288.7306 - lr: 0.0010 - 129s/epoch - 659ms/step
Epoch 2/1000
2023-09-28 16:39:51.919 
Epoch 2/1000 
	 loss: 216.1196, MinusLogProbMetric: 216.1196, val_loss: 165.5085, val_MinusLogProbMetric: 165.5085

Epoch 2: val_loss improved from 288.73062 to 165.50845, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_323/weights/best_weights.h5
196/196 - 56s - loss: 216.1196 - MinusLogProbMetric: 216.1196 - val_loss: 165.5085 - val_MinusLogProbMetric: 165.5085 - lr: 0.0010 - 56s/epoch - 285ms/step
Epoch 3/1000
2023-09-28 16:40:48.030 
Epoch 3/1000 
	 loss: 152.4064, MinusLogProbMetric: 152.4064, val_loss: 142.3388, val_MinusLogProbMetric: 142.3388

Epoch 3: val_loss improved from 165.50845 to 142.33876, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_323/weights/best_weights.h5
196/196 - 56s - loss: 152.4064 - MinusLogProbMetric: 152.4064 - val_loss: 142.3388 - val_MinusLogProbMetric: 142.3388 - lr: 0.0010 - 56s/epoch - 285ms/step
Epoch 4/1000
2023-09-28 16:41:40.134 
Epoch 4/1000 
	 loss: 183.1556, MinusLogProbMetric: 183.1556, val_loss: 265.3049, val_MinusLogProbMetric: 265.3049

Epoch 4: val_loss did not improve from 142.33876
196/196 - 52s - loss: 183.1556 - MinusLogProbMetric: 183.1556 - val_loss: 265.3049 - val_MinusLogProbMetric: 265.3049 - lr: 0.0010 - 52s/epoch - 263ms/step
Epoch 5/1000
2023-09-28 16:42:30.498 
Epoch 5/1000 
	 loss: 189.7565, MinusLogProbMetric: 189.7565, val_loss: 112.8364, val_MinusLogProbMetric: 112.8364

Epoch 5: val_loss improved from 142.33876 to 112.83641, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_323/weights/best_weights.h5
196/196 - 51s - loss: 189.7565 - MinusLogProbMetric: 189.7565 - val_loss: 112.8364 - val_MinusLogProbMetric: 112.8364 - lr: 0.0010 - 51s/epoch - 261ms/step
Epoch 6/1000
2023-09-28 16:43:24.503 
Epoch 6/1000 
	 loss: 99.3281, MinusLogProbMetric: 99.3281, val_loss: 90.5772, val_MinusLogProbMetric: 90.5772

Epoch 6: val_loss improved from 112.83641 to 90.57719, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_323/weights/best_weights.h5
196/196 - 54s - loss: 99.3281 - MinusLogProbMetric: 99.3281 - val_loss: 90.5772 - val_MinusLogProbMetric: 90.5772 - lr: 0.0010 - 54s/epoch - 275ms/step
Epoch 7/1000
2023-09-28 16:44:21.498 
Epoch 7/1000 
	 loss: 93.6322, MinusLogProbMetric: 93.6322, val_loss: 82.1682, val_MinusLogProbMetric: 82.1682

Epoch 7: val_loss improved from 90.57719 to 82.16820, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_323/weights/best_weights.h5
196/196 - 57s - loss: 93.6322 - MinusLogProbMetric: 93.6322 - val_loss: 82.1682 - val_MinusLogProbMetric: 82.1682 - lr: 0.0010 - 57s/epoch - 291ms/step
Epoch 8/1000
2023-09-28 16:45:17.765 
Epoch 8/1000 
	 loss: 77.4606, MinusLogProbMetric: 77.4606, val_loss: 72.9096, val_MinusLogProbMetric: 72.9096

Epoch 8: val_loss improved from 82.16820 to 72.90963, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_323/weights/best_weights.h5
196/196 - 56s - loss: 77.4606 - MinusLogProbMetric: 77.4606 - val_loss: 72.9096 - val_MinusLogProbMetric: 72.9096 - lr: 0.0010 - 56s/epoch - 287ms/step
Epoch 9/1000
2023-09-28 16:46:13.275 
Epoch 9/1000 
	 loss: 71.2628, MinusLogProbMetric: 71.2628, val_loss: 69.2404, val_MinusLogProbMetric: 69.2404

Epoch 9: val_loss improved from 72.90963 to 69.24037, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_323/weights/best_weights.h5
196/196 - 56s - loss: 71.2628 - MinusLogProbMetric: 71.2628 - val_loss: 69.2404 - val_MinusLogProbMetric: 69.2404 - lr: 0.0010 - 56s/epoch - 283ms/step
Epoch 10/1000
2023-09-28 16:47:09.368 
Epoch 10/1000 
	 loss: 65.7993, MinusLogProbMetric: 65.7993, val_loss: 63.4172, val_MinusLogProbMetric: 63.4172

Epoch 10: val_loss improved from 69.24037 to 63.41719, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_323/weights/best_weights.h5
196/196 - 56s - loss: 65.7993 - MinusLogProbMetric: 65.7993 - val_loss: 63.4172 - val_MinusLogProbMetric: 63.4172 - lr: 0.0010 - 56s/epoch - 287ms/step
Epoch 11/1000
2023-09-28 16:48:05.998 
Epoch 11/1000 
	 loss: 61.0896, MinusLogProbMetric: 61.0896, val_loss: 58.6239, val_MinusLogProbMetric: 58.6239

Epoch 11: val_loss improved from 63.41719 to 58.62389, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_323/weights/best_weights.h5
196/196 - 56s - loss: 61.0896 - MinusLogProbMetric: 61.0896 - val_loss: 58.6239 - val_MinusLogProbMetric: 58.6239 - lr: 0.0010 - 56s/epoch - 288ms/step
Epoch 12/1000
2023-09-28 16:49:02.320 
Epoch 12/1000 
	 loss: 60.0592, MinusLogProbMetric: 60.0592, val_loss: 57.7063, val_MinusLogProbMetric: 57.7063

Epoch 12: val_loss improved from 58.62389 to 57.70633, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_323/weights/best_weights.h5
196/196 - 56s - loss: 60.0592 - MinusLogProbMetric: 60.0592 - val_loss: 57.7063 - val_MinusLogProbMetric: 57.7063 - lr: 0.0010 - 56s/epoch - 287ms/step
Epoch 13/1000
2023-09-28 16:49:58.637 
Epoch 13/1000 
	 loss: 55.7020, MinusLogProbMetric: 55.7020, val_loss: 55.0444, val_MinusLogProbMetric: 55.0444

Epoch 13: val_loss improved from 57.70633 to 55.04444, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_323/weights/best_weights.h5
196/196 - 56s - loss: 55.7020 - MinusLogProbMetric: 55.7020 - val_loss: 55.0444 - val_MinusLogProbMetric: 55.0444 - lr: 0.0010 - 56s/epoch - 288ms/step
Epoch 14/1000
2023-09-28 16:50:55.017 
Epoch 14/1000 
	 loss: 53.6789, MinusLogProbMetric: 53.6789, val_loss: 53.6715, val_MinusLogProbMetric: 53.6715

Epoch 14: val_loss improved from 55.04444 to 53.67150, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_323/weights/best_weights.h5
196/196 - 56s - loss: 53.6789 - MinusLogProbMetric: 53.6789 - val_loss: 53.6715 - val_MinusLogProbMetric: 53.6715 - lr: 0.0010 - 56s/epoch - 287ms/step
Epoch 15/1000
2023-09-28 16:51:50.931 
Epoch 15/1000 
	 loss: 51.5475, MinusLogProbMetric: 51.5475, val_loss: 52.2060, val_MinusLogProbMetric: 52.2060

Epoch 15: val_loss improved from 53.67150 to 52.20595, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_323/weights/best_weights.h5
196/196 - 56s - loss: 51.5475 - MinusLogProbMetric: 51.5475 - val_loss: 52.2060 - val_MinusLogProbMetric: 52.2060 - lr: 0.0010 - 56s/epoch - 286ms/step
Epoch 16/1000
2023-09-28 16:52:47.129 
Epoch 16/1000 
	 loss: 50.0255, MinusLogProbMetric: 50.0255, val_loss: 49.2331, val_MinusLogProbMetric: 49.2331

Epoch 16: val_loss improved from 52.20595 to 49.23310, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_323/weights/best_weights.h5
196/196 - 56s - loss: 50.0255 - MinusLogProbMetric: 50.0255 - val_loss: 49.2331 - val_MinusLogProbMetric: 49.2331 - lr: 0.0010 - 56s/epoch - 286ms/step
Epoch 17/1000
2023-09-28 16:53:43.599 
Epoch 17/1000 
	 loss: 48.6682, MinusLogProbMetric: 48.6682, val_loss: 47.1522, val_MinusLogProbMetric: 47.1522

Epoch 17: val_loss improved from 49.23310 to 47.15219, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_323/weights/best_weights.h5
196/196 - 57s - loss: 48.6682 - MinusLogProbMetric: 48.6682 - val_loss: 47.1522 - val_MinusLogProbMetric: 47.1522 - lr: 0.0010 - 57s/epoch - 289ms/step
Epoch 18/1000
2023-09-28 16:54:40.102 
Epoch 18/1000 
	 loss: 47.6321, MinusLogProbMetric: 47.6321, val_loss: 47.4617, val_MinusLogProbMetric: 47.4617

Epoch 18: val_loss did not improve from 47.15219
196/196 - 56s - loss: 47.6321 - MinusLogProbMetric: 47.6321 - val_loss: 47.4617 - val_MinusLogProbMetric: 47.4617 - lr: 0.0010 - 56s/epoch - 284ms/step
Epoch 19/1000
2023-09-28 16:55:35.379 
Epoch 19/1000 
	 loss: 46.5818, MinusLogProbMetric: 46.5818, val_loss: 45.9592, val_MinusLogProbMetric: 45.9592

Epoch 19: val_loss improved from 47.15219 to 45.95916, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_323/weights/best_weights.h5
196/196 - 56s - loss: 46.5818 - MinusLogProbMetric: 46.5818 - val_loss: 45.9592 - val_MinusLogProbMetric: 45.9592 - lr: 0.0010 - 56s/epoch - 286ms/step
Epoch 20/1000
2023-09-28 16:56:31.450 
Epoch 20/1000 
	 loss: 45.2982, MinusLogProbMetric: 45.2982, val_loss: 45.8151, val_MinusLogProbMetric: 45.8151

Epoch 20: val_loss improved from 45.95916 to 45.81513, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_323/weights/best_weights.h5
196/196 - 56s - loss: 45.2982 - MinusLogProbMetric: 45.2982 - val_loss: 45.8151 - val_MinusLogProbMetric: 45.8151 - lr: 0.0010 - 56s/epoch - 286ms/step
Epoch 21/1000
2023-09-28 16:57:27.189 
Epoch 21/1000 
	 loss: 44.7154, MinusLogProbMetric: 44.7154, val_loss: 43.7254, val_MinusLogProbMetric: 43.7254

Epoch 21: val_loss improved from 45.81513 to 43.72537, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_323/weights/best_weights.h5
196/196 - 56s - loss: 44.7154 - MinusLogProbMetric: 44.7154 - val_loss: 43.7254 - val_MinusLogProbMetric: 43.7254 - lr: 0.0010 - 56s/epoch - 284ms/step
Epoch 22/1000
2023-09-28 16:58:22.980 
Epoch 22/1000 
	 loss: 44.1450, MinusLogProbMetric: 44.1450, val_loss: 43.2868, val_MinusLogProbMetric: 43.2868

Epoch 22: val_loss improved from 43.72537 to 43.28676, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_323/weights/best_weights.h5
196/196 - 56s - loss: 44.1450 - MinusLogProbMetric: 44.1450 - val_loss: 43.2868 - val_MinusLogProbMetric: 43.2868 - lr: 0.0010 - 56s/epoch - 285ms/step
Epoch 23/1000
2023-09-28 16:59:18.741 
Epoch 23/1000 
	 loss: 42.7933, MinusLogProbMetric: 42.7933, val_loss: 49.8822, val_MinusLogProbMetric: 49.8822

Epoch 23: val_loss did not improve from 43.28676
196/196 - 55s - loss: 42.7933 - MinusLogProbMetric: 42.7933 - val_loss: 49.8822 - val_MinusLogProbMetric: 49.8822 - lr: 0.0010 - 55s/epoch - 280ms/step
Epoch 24/1000
2023-09-28 17:00:12.457 
Epoch 24/1000 
	 loss: 42.9609, MinusLogProbMetric: 42.9609, val_loss: 43.8554, val_MinusLogProbMetric: 43.8554

Epoch 24: val_loss did not improve from 43.28676
196/196 - 54s - loss: 42.9609 - MinusLogProbMetric: 42.9609 - val_loss: 43.8554 - val_MinusLogProbMetric: 43.8554 - lr: 0.0010 - 54s/epoch - 274ms/step
Epoch 25/1000
2023-09-28 17:01:07.058 
Epoch 25/1000 
	 loss: 42.0578, MinusLogProbMetric: 42.0578, val_loss: 66.7495, val_MinusLogProbMetric: 66.7495

Epoch 25: val_loss did not improve from 43.28676
196/196 - 55s - loss: 42.0578 - MinusLogProbMetric: 42.0578 - val_loss: 66.7495 - val_MinusLogProbMetric: 66.7495 - lr: 0.0010 - 55s/epoch - 279ms/step
Epoch 26/1000
2023-09-28 17:02:01.463 
Epoch 26/1000 
	 loss: 43.8053, MinusLogProbMetric: 43.8053, val_loss: 45.5497, val_MinusLogProbMetric: 45.5497

Epoch 26: val_loss did not improve from 43.28676
196/196 - 54s - loss: 43.8053 - MinusLogProbMetric: 43.8053 - val_loss: 45.5497 - val_MinusLogProbMetric: 45.5497 - lr: 0.0010 - 54s/epoch - 278ms/step
Epoch 27/1000
2023-09-28 17:02:55.884 
Epoch 27/1000 
	 loss: 41.1087, MinusLogProbMetric: 41.1087, val_loss: 40.2656, val_MinusLogProbMetric: 40.2656

Epoch 27: val_loss improved from 43.28676 to 40.26562, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_323/weights/best_weights.h5
196/196 - 55s - loss: 41.1087 - MinusLogProbMetric: 41.1087 - val_loss: 40.2656 - val_MinusLogProbMetric: 40.2656 - lr: 0.0010 - 55s/epoch - 281ms/step
Epoch 28/1000
2023-09-28 17:03:50.746 
Epoch 28/1000 
	 loss: 40.7819, MinusLogProbMetric: 40.7819, val_loss: 39.9443, val_MinusLogProbMetric: 39.9443

Epoch 28: val_loss improved from 40.26562 to 39.94429, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_323/weights/best_weights.h5
196/196 - 55s - loss: 40.7819 - MinusLogProbMetric: 40.7819 - val_loss: 39.9443 - val_MinusLogProbMetric: 39.9443 - lr: 0.0010 - 55s/epoch - 280ms/step
Epoch 29/1000
2023-09-28 17:04:45.479 
Epoch 29/1000 
	 loss: 40.5160, MinusLogProbMetric: 40.5160, val_loss: 39.7499, val_MinusLogProbMetric: 39.7499

Epoch 29: val_loss improved from 39.94429 to 39.74987, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_323/weights/best_weights.h5
196/196 - 55s - loss: 40.5160 - MinusLogProbMetric: 40.5160 - val_loss: 39.7499 - val_MinusLogProbMetric: 39.7499 - lr: 0.0010 - 55s/epoch - 278ms/step
Epoch 30/1000
2023-09-28 17:05:40.991 
Epoch 30/1000 
	 loss: 40.7800, MinusLogProbMetric: 40.7800, val_loss: 42.3879, val_MinusLogProbMetric: 42.3879

Epoch 30: val_loss did not improve from 39.74987
196/196 - 55s - loss: 40.7800 - MinusLogProbMetric: 40.7800 - val_loss: 42.3879 - val_MinusLogProbMetric: 42.3879 - lr: 0.0010 - 55s/epoch - 280ms/step
Epoch 31/1000
2023-09-28 17:06:35.516 
Epoch 31/1000 
	 loss: 39.6092, MinusLogProbMetric: 39.6092, val_loss: 41.1355, val_MinusLogProbMetric: 41.1355

Epoch 31: val_loss did not improve from 39.74987
196/196 - 55s - loss: 39.6092 - MinusLogProbMetric: 39.6092 - val_loss: 41.1355 - val_MinusLogProbMetric: 41.1355 - lr: 0.0010 - 55s/epoch - 278ms/step
Epoch 32/1000
2023-09-28 17:07:28.197 
Epoch 32/1000 
	 loss: 39.1684, MinusLogProbMetric: 39.1684, val_loss: 41.9059, val_MinusLogProbMetric: 41.9059

Epoch 32: val_loss did not improve from 39.74987
196/196 - 53s - loss: 39.1684 - MinusLogProbMetric: 39.1684 - val_loss: 41.9059 - val_MinusLogProbMetric: 41.9059 - lr: 0.0010 - 53s/epoch - 269ms/step
Epoch 33/1000
2023-09-28 17:08:22.717 
Epoch 33/1000 
	 loss: 62.8328, MinusLogProbMetric: 62.8328, val_loss: 48.3631, val_MinusLogProbMetric: 48.3631

Epoch 33: val_loss did not improve from 39.74987
196/196 - 55s - loss: 62.8328 - MinusLogProbMetric: 62.8328 - val_loss: 48.3631 - val_MinusLogProbMetric: 48.3631 - lr: 0.0010 - 55s/epoch - 278ms/step
Epoch 34/1000
2023-09-28 17:09:17.200 
Epoch 34/1000 
	 loss: 43.3289, MinusLogProbMetric: 43.3289, val_loss: 41.8119, val_MinusLogProbMetric: 41.8119

Epoch 34: val_loss did not improve from 39.74987
196/196 - 54s - loss: 43.3289 - MinusLogProbMetric: 43.3289 - val_loss: 41.8119 - val_MinusLogProbMetric: 41.8119 - lr: 0.0010 - 54s/epoch - 278ms/step
Epoch 35/1000
2023-09-28 17:10:12.041 
Epoch 35/1000 
	 loss: 41.3174, MinusLogProbMetric: 41.3174, val_loss: 41.7869, val_MinusLogProbMetric: 41.7869

Epoch 35: val_loss did not improve from 39.74987
196/196 - 55s - loss: 41.3174 - MinusLogProbMetric: 41.3174 - val_loss: 41.7869 - val_MinusLogProbMetric: 41.7869 - lr: 0.0010 - 55s/epoch - 280ms/step
Epoch 36/1000
2023-09-28 17:11:06.737 
Epoch 36/1000 
	 loss: 40.1470, MinusLogProbMetric: 40.1470, val_loss: 39.2221, val_MinusLogProbMetric: 39.2221

Epoch 36: val_loss improved from 39.74987 to 39.22210, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_323/weights/best_weights.h5
196/196 - 55s - loss: 40.1470 - MinusLogProbMetric: 40.1470 - val_loss: 39.2221 - val_MinusLogProbMetric: 39.2221 - lr: 0.0010 - 55s/epoch - 283ms/step
Epoch 37/1000
2023-09-28 17:12:00.723 
Epoch 37/1000 
	 loss: 39.3145, MinusLogProbMetric: 39.3145, val_loss: 39.0533, val_MinusLogProbMetric: 39.0533

Epoch 37: val_loss improved from 39.22210 to 39.05332, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_323/weights/best_weights.h5
196/196 - 54s - loss: 39.3145 - MinusLogProbMetric: 39.3145 - val_loss: 39.0533 - val_MinusLogProbMetric: 39.0533 - lr: 0.0010 - 54s/epoch - 275ms/step
Epoch 38/1000
2023-09-28 17:12:56.273 
Epoch 38/1000 
	 loss: 39.0731, MinusLogProbMetric: 39.0731, val_loss: 40.3080, val_MinusLogProbMetric: 40.3080

Epoch 38: val_loss did not improve from 39.05332
196/196 - 55s - loss: 39.0731 - MinusLogProbMetric: 39.0731 - val_loss: 40.3080 - val_MinusLogProbMetric: 40.3080 - lr: 0.0010 - 55s/epoch - 280ms/step
Epoch 39/1000
2023-09-28 17:13:50.920 
Epoch 39/1000 
	 loss: 38.5258, MinusLogProbMetric: 38.5258, val_loss: 39.7872, val_MinusLogProbMetric: 39.7872

Epoch 39: val_loss did not improve from 39.05332
196/196 - 55s - loss: 38.5258 - MinusLogProbMetric: 38.5258 - val_loss: 39.7872 - val_MinusLogProbMetric: 39.7872 - lr: 0.0010 - 55s/epoch - 279ms/step
Epoch 40/1000
2023-09-28 17:14:43.718 
Epoch 40/1000 
	 loss: 38.5003, MinusLogProbMetric: 38.5003, val_loss: 37.9398, val_MinusLogProbMetric: 37.9398

Epoch 40: val_loss improved from 39.05332 to 37.93978, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_323/weights/best_weights.h5
196/196 - 54s - loss: 38.5003 - MinusLogProbMetric: 38.5003 - val_loss: 37.9398 - val_MinusLogProbMetric: 37.9398 - lr: 0.0010 - 54s/epoch - 273ms/step
Epoch 41/1000
2023-09-28 17:15:37.892 
Epoch 41/1000 
	 loss: 37.9025, MinusLogProbMetric: 37.9025, val_loss: 37.1597, val_MinusLogProbMetric: 37.1597

Epoch 41: val_loss improved from 37.93978 to 37.15970, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_323/weights/best_weights.h5
196/196 - 54s - loss: 37.9025 - MinusLogProbMetric: 37.9025 - val_loss: 37.1597 - val_MinusLogProbMetric: 37.1597 - lr: 0.0010 - 54s/epoch - 276ms/step
Epoch 42/1000
2023-09-28 17:16:29.944 
Epoch 42/1000 
	 loss: 37.9193, MinusLogProbMetric: 37.9193, val_loss: 39.3288, val_MinusLogProbMetric: 39.3288

Epoch 42: val_loss did not improve from 37.15970
196/196 - 51s - loss: 37.9193 - MinusLogProbMetric: 37.9193 - val_loss: 39.3288 - val_MinusLogProbMetric: 39.3288 - lr: 0.0010 - 51s/epoch - 262ms/step
Epoch 43/1000
2023-09-28 17:17:24.343 
Epoch 43/1000 
	 loss: 37.6650, MinusLogProbMetric: 37.6650, val_loss: 39.3896, val_MinusLogProbMetric: 39.3896

Epoch 43: val_loss did not improve from 37.15970
196/196 - 54s - loss: 37.6650 - MinusLogProbMetric: 37.6650 - val_loss: 39.3896 - val_MinusLogProbMetric: 39.3896 - lr: 0.0010 - 54s/epoch - 277ms/step
Epoch 44/1000
2023-09-28 17:18:19.879 
Epoch 44/1000 
	 loss: 37.5582, MinusLogProbMetric: 37.5582, val_loss: 37.5479, val_MinusLogProbMetric: 37.5479

Epoch 44: val_loss did not improve from 37.15970
196/196 - 56s - loss: 37.5582 - MinusLogProbMetric: 37.5582 - val_loss: 37.5479 - val_MinusLogProbMetric: 37.5479 - lr: 0.0010 - 56s/epoch - 283ms/step
Epoch 45/1000
2023-09-28 17:19:15.225 
Epoch 45/1000 
	 loss: 37.1568, MinusLogProbMetric: 37.1568, val_loss: 36.3336, val_MinusLogProbMetric: 36.3336

Epoch 45: val_loss improved from 37.15970 to 36.33356, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_323/weights/best_weights.h5
196/196 - 56s - loss: 37.1568 - MinusLogProbMetric: 37.1568 - val_loss: 36.3336 - val_MinusLogProbMetric: 36.3336 - lr: 0.0010 - 56s/epoch - 286ms/step
Epoch 46/1000
2023-09-28 17:20:09.912 
Epoch 46/1000 
	 loss: 36.9185, MinusLogProbMetric: 36.9185, val_loss: 36.6470, val_MinusLogProbMetric: 36.6470

Epoch 46: val_loss did not improve from 36.33356
196/196 - 54s - loss: 36.9185 - MinusLogProbMetric: 36.9185 - val_loss: 36.6470 - val_MinusLogProbMetric: 36.6470 - lr: 0.0010 - 54s/epoch - 275ms/step
Epoch 47/1000
2023-09-28 17:21:05.360 
Epoch 47/1000 
	 loss: 36.8177, MinusLogProbMetric: 36.8177, val_loss: 36.4988, val_MinusLogProbMetric: 36.4988

Epoch 47: val_loss did not improve from 36.33356
196/196 - 55s - loss: 36.8177 - MinusLogProbMetric: 36.8177 - val_loss: 36.4988 - val_MinusLogProbMetric: 36.4988 - lr: 0.0010 - 55s/epoch - 283ms/step
Epoch 48/1000
2023-09-28 17:21:59.540 
Epoch 48/1000 
	 loss: 36.5902, MinusLogProbMetric: 36.5902, val_loss: 38.0496, val_MinusLogProbMetric: 38.0496

Epoch 48: val_loss did not improve from 36.33356
196/196 - 54s - loss: 36.5902 - MinusLogProbMetric: 36.5902 - val_loss: 38.0496 - val_MinusLogProbMetric: 38.0496 - lr: 0.0010 - 54s/epoch - 276ms/step
Epoch 49/1000
2023-09-28 17:22:52.525 
Epoch 49/1000 
	 loss: 36.2921, MinusLogProbMetric: 36.2921, val_loss: 35.9456, val_MinusLogProbMetric: 35.9456

Epoch 49: val_loss improved from 36.33356 to 35.94561, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_323/weights/best_weights.h5
196/196 - 54s - loss: 36.2921 - MinusLogProbMetric: 36.2921 - val_loss: 35.9456 - val_MinusLogProbMetric: 35.9456 - lr: 0.0010 - 54s/epoch - 275ms/step
Epoch 50/1000
2023-09-28 17:23:47.600 
Epoch 50/1000 
	 loss: 36.4404, MinusLogProbMetric: 36.4404, val_loss: 36.0264, val_MinusLogProbMetric: 36.0264

Epoch 50: val_loss did not improve from 35.94561
196/196 - 54s - loss: 36.4404 - MinusLogProbMetric: 36.4404 - val_loss: 36.0264 - val_MinusLogProbMetric: 36.0264 - lr: 0.0010 - 54s/epoch - 276ms/step
Epoch 51/1000
2023-09-28 17:24:41.942 
Epoch 51/1000 
	 loss: 37.0113, MinusLogProbMetric: 37.0113, val_loss: 36.3268, val_MinusLogProbMetric: 36.3268

Epoch 51: val_loss did not improve from 35.94561
196/196 - 54s - loss: 37.0113 - MinusLogProbMetric: 37.0113 - val_loss: 36.3268 - val_MinusLogProbMetric: 36.3268 - lr: 0.0010 - 54s/epoch - 277ms/step
Epoch 52/1000
2023-09-28 17:25:34.874 
Epoch 52/1000 
	 loss: 36.0007, MinusLogProbMetric: 36.0007, val_loss: 35.5280, val_MinusLogProbMetric: 35.5280

Epoch 52: val_loss improved from 35.94561 to 35.52801, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_323/weights/best_weights.h5
196/196 - 54s - loss: 36.0007 - MinusLogProbMetric: 36.0007 - val_loss: 35.5280 - val_MinusLogProbMetric: 35.5280 - lr: 0.0010 - 54s/epoch - 275ms/step
Epoch 53/1000
2023-09-28 17:26:30.801 
Epoch 53/1000 
	 loss: 36.1909, MinusLogProbMetric: 36.1909, val_loss: 40.8616, val_MinusLogProbMetric: 40.8616

Epoch 53: val_loss did not improve from 35.52801
196/196 - 55s - loss: 36.1909 - MinusLogProbMetric: 36.1909 - val_loss: 40.8616 - val_MinusLogProbMetric: 40.8616 - lr: 0.0010 - 55s/epoch - 281ms/step
Epoch 54/1000
2023-09-28 17:27:24.952 
Epoch 54/1000 
	 loss: 36.0029, MinusLogProbMetric: 36.0029, val_loss: 36.0442, val_MinusLogProbMetric: 36.0442

Epoch 54: val_loss did not improve from 35.52801
196/196 - 54s - loss: 36.0029 - MinusLogProbMetric: 36.0029 - val_loss: 36.0442 - val_MinusLogProbMetric: 36.0442 - lr: 0.0010 - 54s/epoch - 276ms/step
Epoch 55/1000
2023-09-28 17:28:18.232 
Epoch 55/1000 
	 loss: 36.1051, MinusLogProbMetric: 36.1051, val_loss: 35.6389, val_MinusLogProbMetric: 35.6389

Epoch 55: val_loss did not improve from 35.52801
196/196 - 53s - loss: 36.1051 - MinusLogProbMetric: 36.1051 - val_loss: 35.6389 - val_MinusLogProbMetric: 35.6389 - lr: 0.0010 - 53s/epoch - 272ms/step
Epoch 56/1000
2023-09-28 17:29:11.421 
Epoch 56/1000 
	 loss: 35.6867, MinusLogProbMetric: 35.6867, val_loss: 34.6735, val_MinusLogProbMetric: 34.6735

Epoch 56: val_loss improved from 35.52801 to 34.67347, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_323/weights/best_weights.h5
196/196 - 54s - loss: 35.6867 - MinusLogProbMetric: 35.6867 - val_loss: 34.6735 - val_MinusLogProbMetric: 34.6735 - lr: 0.0010 - 54s/epoch - 276ms/step
Epoch 57/1000
2023-09-28 17:30:06.114 
Epoch 57/1000 
	 loss: 35.8839, MinusLogProbMetric: 35.8839, val_loss: 37.0158, val_MinusLogProbMetric: 37.0158

Epoch 57: val_loss did not improve from 34.67347
196/196 - 54s - loss: 35.8839 - MinusLogProbMetric: 35.8839 - val_loss: 37.0158 - val_MinusLogProbMetric: 37.0158 - lr: 0.0010 - 54s/epoch - 274ms/step
Epoch 58/1000
2023-09-28 17:31:00.992 
Epoch 58/1000 
	 loss: 35.9545, MinusLogProbMetric: 35.9545, val_loss: 50.5343, val_MinusLogProbMetric: 50.5343

Epoch 58: val_loss did not improve from 34.67347
196/196 - 55s - loss: 35.9545 - MinusLogProbMetric: 35.9545 - val_loss: 50.5343 - val_MinusLogProbMetric: 50.5343 - lr: 0.0010 - 55s/epoch - 280ms/step
Epoch 59/1000
2023-09-28 17:31:53.321 
Epoch 59/1000 
	 loss: 36.4495, MinusLogProbMetric: 36.4495, val_loss: 35.1564, val_MinusLogProbMetric: 35.1564

Epoch 59: val_loss did not improve from 34.67347
196/196 - 52s - loss: 36.4495 - MinusLogProbMetric: 36.4495 - val_loss: 35.1564 - val_MinusLogProbMetric: 35.1564 - lr: 0.0010 - 52s/epoch - 267ms/step
Epoch 60/1000
2023-09-28 17:32:46.642 
Epoch 60/1000 
	 loss: 35.1524, MinusLogProbMetric: 35.1524, val_loss: 35.1977, val_MinusLogProbMetric: 35.1977

Epoch 60: val_loss did not improve from 34.67347
196/196 - 53s - loss: 35.1524 - MinusLogProbMetric: 35.1524 - val_loss: 35.1977 - val_MinusLogProbMetric: 35.1977 - lr: 0.0010 - 53s/epoch - 272ms/step
Epoch 61/1000
2023-09-28 17:33:42.150 
Epoch 61/1000 
	 loss: 35.3241, MinusLogProbMetric: 35.3241, val_loss: 34.9336, val_MinusLogProbMetric: 34.9336

Epoch 61: val_loss did not improve from 34.67347
196/196 - 56s - loss: 35.3241 - MinusLogProbMetric: 35.3241 - val_loss: 34.9336 - val_MinusLogProbMetric: 34.9336 - lr: 0.0010 - 56s/epoch - 283ms/step
Epoch 62/1000
2023-09-28 17:34:38.865 
Epoch 62/1000 
	 loss: 35.1815, MinusLogProbMetric: 35.1815, val_loss: 34.8479, val_MinusLogProbMetric: 34.8479

Epoch 62: val_loss did not improve from 34.67347
196/196 - 57s - loss: 35.1815 - MinusLogProbMetric: 35.1815 - val_loss: 34.8479 - val_MinusLogProbMetric: 34.8479 - lr: 0.0010 - 57s/epoch - 289ms/step
Epoch 63/1000
2023-09-28 17:35:34.512 
Epoch 63/1000 
	 loss: 35.0953, MinusLogProbMetric: 35.0953, val_loss: 34.3908, val_MinusLogProbMetric: 34.3908

Epoch 63: val_loss improved from 34.67347 to 34.39076, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_323/weights/best_weights.h5
196/196 - 56s - loss: 35.0953 - MinusLogProbMetric: 35.0953 - val_loss: 34.3908 - val_MinusLogProbMetric: 34.3908 - lr: 0.0010 - 56s/epoch - 287ms/step
Epoch 64/1000
2023-09-28 17:36:30.914 
Epoch 64/1000 
	 loss: 34.8559, MinusLogProbMetric: 34.8559, val_loss: 35.3906, val_MinusLogProbMetric: 35.3906

Epoch 64: val_loss did not improve from 34.39076
196/196 - 56s - loss: 34.8559 - MinusLogProbMetric: 34.8559 - val_loss: 35.3906 - val_MinusLogProbMetric: 35.3906 - lr: 0.0010 - 56s/epoch - 285ms/step
Epoch 65/1000
2023-09-28 17:37:27.441 
Epoch 65/1000 
	 loss: 35.2005, MinusLogProbMetric: 35.2005, val_loss: 35.3502, val_MinusLogProbMetric: 35.3502

Epoch 65: val_loss did not improve from 34.39076
196/196 - 57s - loss: 35.2005 - MinusLogProbMetric: 35.2005 - val_loss: 35.3502 - val_MinusLogProbMetric: 35.3502 - lr: 0.0010 - 57s/epoch - 288ms/step
Epoch 66/1000
2023-09-28 17:38:22.718 
Epoch 66/1000 
	 loss: 34.7956, MinusLogProbMetric: 34.7956, val_loss: 34.1840, val_MinusLogProbMetric: 34.1840

Epoch 66: val_loss improved from 34.39076 to 34.18399, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_323/weights/best_weights.h5
196/196 - 56s - loss: 34.7956 - MinusLogProbMetric: 34.7956 - val_loss: 34.1840 - val_MinusLogProbMetric: 34.1840 - lr: 0.0010 - 56s/epoch - 287ms/step
Epoch 67/1000
2023-09-28 17:39:19.364 
Epoch 67/1000 
	 loss: 34.5562, MinusLogProbMetric: 34.5562, val_loss: 34.6378, val_MinusLogProbMetric: 34.6378

Epoch 67: val_loss did not improve from 34.18399
196/196 - 56s - loss: 34.5562 - MinusLogProbMetric: 34.5562 - val_loss: 34.6378 - val_MinusLogProbMetric: 34.6378 - lr: 0.0010 - 56s/epoch - 284ms/step
Epoch 68/1000
2023-09-28 17:40:14.713 
Epoch 68/1000 
	 loss: 34.4565, MinusLogProbMetric: 34.4565, val_loss: 33.6900, val_MinusLogProbMetric: 33.6900

Epoch 68: val_loss improved from 34.18399 to 33.68995, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_323/weights/best_weights.h5
196/196 - 56s - loss: 34.4565 - MinusLogProbMetric: 34.4565 - val_loss: 33.6900 - val_MinusLogProbMetric: 33.6900 - lr: 0.0010 - 56s/epoch - 286ms/step
Epoch 69/1000
2023-09-28 17:41:10.613 
Epoch 69/1000 
	 loss: 34.7431, MinusLogProbMetric: 34.7431, val_loss: 34.5182, val_MinusLogProbMetric: 34.5182

Epoch 69: val_loss did not improve from 33.68995
196/196 - 55s - loss: 34.7431 - MinusLogProbMetric: 34.7431 - val_loss: 34.5182 - val_MinusLogProbMetric: 34.5182 - lr: 0.0010 - 55s/epoch - 282ms/step
Epoch 70/1000
2023-09-28 17:42:05.964 
Epoch 70/1000 
	 loss: 34.3789, MinusLogProbMetric: 34.3789, val_loss: 33.9282, val_MinusLogProbMetric: 33.9282

Epoch 70: val_loss did not improve from 33.68995
196/196 - 55s - loss: 34.3789 - MinusLogProbMetric: 34.3789 - val_loss: 33.9282 - val_MinusLogProbMetric: 33.9282 - lr: 0.0010 - 55s/epoch - 282ms/step
Epoch 71/1000
2023-09-28 17:43:00.376 
Epoch 71/1000 
	 loss: 34.4444, MinusLogProbMetric: 34.4444, val_loss: 34.5500, val_MinusLogProbMetric: 34.5500

Epoch 71: val_loss did not improve from 33.68995
196/196 - 54s - loss: 34.4444 - MinusLogProbMetric: 34.4444 - val_loss: 34.5500 - val_MinusLogProbMetric: 34.5500 - lr: 0.0010 - 54s/epoch - 278ms/step
Epoch 72/1000
2023-09-28 17:43:55.985 
Epoch 72/1000 
	 loss: 34.4486, MinusLogProbMetric: 34.4486, val_loss: 35.7845, val_MinusLogProbMetric: 35.7845

Epoch 72: val_loss did not improve from 33.68995
196/196 - 56s - loss: 34.4486 - MinusLogProbMetric: 34.4486 - val_loss: 35.7845 - val_MinusLogProbMetric: 35.7845 - lr: 0.0010 - 56s/epoch - 284ms/step
Epoch 73/1000
2023-09-28 17:44:51.250 
Epoch 73/1000 
	 loss: 34.5961, MinusLogProbMetric: 34.5961, val_loss: 33.7677, val_MinusLogProbMetric: 33.7677

Epoch 73: val_loss did not improve from 33.68995
196/196 - 55s - loss: 34.5961 - MinusLogProbMetric: 34.5961 - val_loss: 33.7677 - val_MinusLogProbMetric: 33.7677 - lr: 0.0010 - 55s/epoch - 282ms/step
Epoch 74/1000
2023-09-28 17:45:46.218 
Epoch 74/1000 
	 loss: 34.2614, MinusLogProbMetric: 34.2614, val_loss: 33.7554, val_MinusLogProbMetric: 33.7554

Epoch 74: val_loss did not improve from 33.68995
196/196 - 55s - loss: 34.2614 - MinusLogProbMetric: 34.2614 - val_loss: 33.7554 - val_MinusLogProbMetric: 33.7554 - lr: 0.0010 - 55s/epoch - 280ms/step
Epoch 75/1000
2023-09-28 17:46:41.758 
Epoch 75/1000 
	 loss: 34.3561, MinusLogProbMetric: 34.3561, val_loss: 33.7621, val_MinusLogProbMetric: 33.7621

Epoch 75: val_loss did not improve from 33.68995
196/196 - 56s - loss: 34.3561 - MinusLogProbMetric: 34.3561 - val_loss: 33.7621 - val_MinusLogProbMetric: 33.7621 - lr: 0.0010 - 56s/epoch - 283ms/step
Epoch 76/1000
2023-09-28 17:47:36.710 
Epoch 76/1000 
	 loss: 34.0946, MinusLogProbMetric: 34.0946, val_loss: 37.3024, val_MinusLogProbMetric: 37.3024

Epoch 76: val_loss did not improve from 33.68995
196/196 - 55s - loss: 34.0946 - MinusLogProbMetric: 34.0946 - val_loss: 37.3024 - val_MinusLogProbMetric: 37.3024 - lr: 0.0010 - 55s/epoch - 280ms/step
Epoch 77/1000
2023-09-28 17:48:31.866 
Epoch 77/1000 
	 loss: 34.0336, MinusLogProbMetric: 34.0336, val_loss: 34.1163, val_MinusLogProbMetric: 34.1163

Epoch 77: val_loss did not improve from 33.68995
196/196 - 55s - loss: 34.0336 - MinusLogProbMetric: 34.0336 - val_loss: 34.1163 - val_MinusLogProbMetric: 34.1163 - lr: 0.0010 - 55s/epoch - 281ms/step
Epoch 78/1000
2023-09-28 17:49:27.159 
Epoch 78/1000 
	 loss: 34.0451, MinusLogProbMetric: 34.0451, val_loss: 35.6126, val_MinusLogProbMetric: 35.6126

Epoch 78: val_loss did not improve from 33.68995
196/196 - 55s - loss: 34.0451 - MinusLogProbMetric: 34.0451 - val_loss: 35.6126 - val_MinusLogProbMetric: 35.6126 - lr: 0.0010 - 55s/epoch - 282ms/step
Epoch 79/1000
2023-09-28 17:50:22.192 
Epoch 79/1000 
	 loss: 34.0615, MinusLogProbMetric: 34.0615, val_loss: 34.0711, val_MinusLogProbMetric: 34.0711

Epoch 79: val_loss did not improve from 33.68995
196/196 - 55s - loss: 34.0615 - MinusLogProbMetric: 34.0615 - val_loss: 34.0711 - val_MinusLogProbMetric: 34.0711 - lr: 0.0010 - 55s/epoch - 281ms/step
Epoch 80/1000
2023-09-28 17:51:17.255 
Epoch 80/1000 
	 loss: 33.8036, MinusLogProbMetric: 33.8036, val_loss: 34.0365, val_MinusLogProbMetric: 34.0365

Epoch 80: val_loss did not improve from 33.68995
196/196 - 55s - loss: 33.8036 - MinusLogProbMetric: 33.8036 - val_loss: 34.0365 - val_MinusLogProbMetric: 34.0365 - lr: 0.0010 - 55s/epoch - 281ms/step
Epoch 81/1000
2023-09-28 17:52:12.793 
Epoch 81/1000 
	 loss: 33.8529, MinusLogProbMetric: 33.8529, val_loss: 33.2561, val_MinusLogProbMetric: 33.2561

Epoch 81: val_loss improved from 33.68995 to 33.25613, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_323/weights/best_weights.h5
196/196 - 56s - loss: 33.8529 - MinusLogProbMetric: 33.8529 - val_loss: 33.2561 - val_MinusLogProbMetric: 33.2561 - lr: 0.0010 - 56s/epoch - 287ms/step
Epoch 82/1000
2023-09-28 17:53:08.429 
Epoch 82/1000 
	 loss: 33.8605, MinusLogProbMetric: 33.8605, val_loss: 34.4700, val_MinusLogProbMetric: 34.4700

Epoch 82: val_loss did not improve from 33.25613
196/196 - 55s - loss: 33.8605 - MinusLogProbMetric: 33.8605 - val_loss: 34.4700 - val_MinusLogProbMetric: 34.4700 - lr: 0.0010 - 55s/epoch - 280ms/step
Epoch 83/1000
2023-09-28 17:54:03.478 
Epoch 83/1000 
	 loss: 33.5015, MinusLogProbMetric: 33.5015, val_loss: 33.1813, val_MinusLogProbMetric: 33.1813

Epoch 83: val_loss improved from 33.25613 to 33.18130, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_323/weights/best_weights.h5
196/196 - 56s - loss: 33.5015 - MinusLogProbMetric: 33.5015 - val_loss: 33.1813 - val_MinusLogProbMetric: 33.1813 - lr: 0.0010 - 56s/epoch - 285ms/step
Epoch 84/1000
2023-09-28 17:54:59.313 
Epoch 84/1000 
	 loss: 33.9799, MinusLogProbMetric: 33.9799, val_loss: 33.2797, val_MinusLogProbMetric: 33.2797

Epoch 84: val_loss did not improve from 33.18130
196/196 - 55s - loss: 33.9799 - MinusLogProbMetric: 33.9799 - val_loss: 33.2797 - val_MinusLogProbMetric: 33.2797 - lr: 0.0010 - 55s/epoch - 281ms/step
Epoch 85/1000
2023-09-28 17:55:54.122 
Epoch 85/1000 
	 loss: 33.6100, MinusLogProbMetric: 33.6100, val_loss: 33.0702, val_MinusLogProbMetric: 33.0702

Epoch 85: val_loss improved from 33.18130 to 33.07019, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_323/weights/best_weights.h5
196/196 - 56s - loss: 33.6100 - MinusLogProbMetric: 33.6100 - val_loss: 33.0702 - val_MinusLogProbMetric: 33.0702 - lr: 0.0010 - 56s/epoch - 283ms/step
Epoch 86/1000
2023-09-28 17:56:50.503 
Epoch 86/1000 
	 loss: 33.4013, MinusLogProbMetric: 33.4013, val_loss: 33.9807, val_MinusLogProbMetric: 33.9807

Epoch 86: val_loss did not improve from 33.07019
196/196 - 56s - loss: 33.4013 - MinusLogProbMetric: 33.4013 - val_loss: 33.9807 - val_MinusLogProbMetric: 33.9807 - lr: 0.0010 - 56s/epoch - 284ms/step
Epoch 87/1000
2023-09-28 17:57:45.215 
Epoch 87/1000 
	 loss: 33.5239, MinusLogProbMetric: 33.5239, val_loss: 36.7598, val_MinusLogProbMetric: 36.7598

Epoch 87: val_loss did not improve from 33.07019
196/196 - 55s - loss: 33.5239 - MinusLogProbMetric: 33.5239 - val_loss: 36.7598 - val_MinusLogProbMetric: 36.7598 - lr: 0.0010 - 55s/epoch - 279ms/step
Epoch 88/1000
2023-09-28 17:58:39.683 
Epoch 88/1000 
	 loss: 33.8945, MinusLogProbMetric: 33.8945, val_loss: 33.4148, val_MinusLogProbMetric: 33.4148

Epoch 88: val_loss did not improve from 33.07019
196/196 - 54s - loss: 33.8945 - MinusLogProbMetric: 33.8945 - val_loss: 33.4148 - val_MinusLogProbMetric: 33.4148 - lr: 0.0010 - 54s/epoch - 278ms/step
Epoch 89/1000
2023-09-28 17:59:30.780 
Epoch 89/1000 
	 loss: 33.4520, MinusLogProbMetric: 33.4520, val_loss: 36.2815, val_MinusLogProbMetric: 36.2815

Epoch 89: val_loss did not improve from 33.07019
196/196 - 51s - loss: 33.4520 - MinusLogProbMetric: 33.4520 - val_loss: 36.2815 - val_MinusLogProbMetric: 36.2815 - lr: 0.0010 - 51s/epoch - 261ms/step
Epoch 90/1000
2023-09-28 18:00:22.826 
Epoch 90/1000 
	 loss: 33.7007, MinusLogProbMetric: 33.7007, val_loss: 32.8649, val_MinusLogProbMetric: 32.8649

Epoch 90: val_loss improved from 33.07019 to 32.86492, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_323/weights/best_weights.h5
196/196 - 53s - loss: 33.7007 - MinusLogProbMetric: 33.7007 - val_loss: 32.8649 - val_MinusLogProbMetric: 32.8649 - lr: 0.0010 - 53s/epoch - 270ms/step
Epoch 91/1000
2023-09-28 18:01:16.176 
Epoch 91/1000 
	 loss: 33.3140, MinusLogProbMetric: 33.3140, val_loss: 32.5840, val_MinusLogProbMetric: 32.5840

Epoch 91: val_loss improved from 32.86492 to 32.58401, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_323/weights/best_weights.h5
196/196 - 53s - loss: 33.3140 - MinusLogProbMetric: 33.3140 - val_loss: 32.5840 - val_MinusLogProbMetric: 32.5840 - lr: 0.0010 - 53s/epoch - 272ms/step
Epoch 92/1000
2023-09-28 18:02:07.554 
Epoch 92/1000 
	 loss: 33.1026, MinusLogProbMetric: 33.1026, val_loss: 36.3766, val_MinusLogProbMetric: 36.3766

Epoch 92: val_loss did not improve from 32.58401
196/196 - 51s - loss: 33.1026 - MinusLogProbMetric: 33.1026 - val_loss: 36.3766 - val_MinusLogProbMetric: 36.3766 - lr: 0.0010 - 51s/epoch - 259ms/step
Epoch 93/1000
2023-09-28 18:02:59.053 
Epoch 93/1000 
	 loss: 33.6019, MinusLogProbMetric: 33.6019, val_loss: 33.9159, val_MinusLogProbMetric: 33.9159

Epoch 93: val_loss did not improve from 32.58401
196/196 - 51s - loss: 33.6019 - MinusLogProbMetric: 33.6019 - val_loss: 33.9159 - val_MinusLogProbMetric: 33.9159 - lr: 0.0010 - 51s/epoch - 263ms/step
Epoch 94/1000
2023-09-28 18:03:49.267 
Epoch 94/1000 
	 loss: 33.5171, MinusLogProbMetric: 33.5171, val_loss: 33.2347, val_MinusLogProbMetric: 33.2347

Epoch 94: val_loss did not improve from 32.58401
196/196 - 50s - loss: 33.5171 - MinusLogProbMetric: 33.5171 - val_loss: 33.2347 - val_MinusLogProbMetric: 33.2347 - lr: 0.0010 - 50s/epoch - 256ms/step
Epoch 95/1000
2023-09-28 18:04:40.422 
Epoch 95/1000 
	 loss: 33.2220, MinusLogProbMetric: 33.2220, val_loss: 34.5950, val_MinusLogProbMetric: 34.5950

Epoch 95: val_loss did not improve from 32.58401
196/196 - 51s - loss: 33.2220 - MinusLogProbMetric: 33.2220 - val_loss: 34.5950 - val_MinusLogProbMetric: 34.5950 - lr: 0.0010 - 51s/epoch - 261ms/step
Epoch 96/1000
2023-09-28 18:05:25.141 
Epoch 96/1000 
	 loss: 33.2819, MinusLogProbMetric: 33.2819, val_loss: 33.2239, val_MinusLogProbMetric: 33.2239

Epoch 96: val_loss did not improve from 32.58401
196/196 - 45s - loss: 33.2819 - MinusLogProbMetric: 33.2819 - val_loss: 33.2239 - val_MinusLogProbMetric: 33.2239 - lr: 0.0010 - 45s/epoch - 228ms/step
Epoch 97/1000
2023-09-28 18:06:11.458 
Epoch 97/1000 
	 loss: 36.0257, MinusLogProbMetric: 36.0257, val_loss: 35.0358, val_MinusLogProbMetric: 35.0358

Epoch 97: val_loss did not improve from 32.58401
196/196 - 46s - loss: 36.0257 - MinusLogProbMetric: 36.0257 - val_loss: 35.0358 - val_MinusLogProbMetric: 35.0358 - lr: 0.0010 - 46s/epoch - 236ms/step
Epoch 98/1000
2023-09-28 18:07:01.635 
Epoch 98/1000 
	 loss: 33.2865, MinusLogProbMetric: 33.2865, val_loss: 34.6741, val_MinusLogProbMetric: 34.6741

Epoch 98: val_loss did not improve from 32.58401
196/196 - 50s - loss: 33.2865 - MinusLogProbMetric: 33.2865 - val_loss: 34.6741 - val_MinusLogProbMetric: 34.6741 - lr: 0.0010 - 50s/epoch - 256ms/step
Epoch 99/1000
2023-09-28 18:07:53.774 
Epoch 99/1000 
	 loss: 33.1022, MinusLogProbMetric: 33.1022, val_loss: 35.1910, val_MinusLogProbMetric: 35.1910

Epoch 99: val_loss did not improve from 32.58401
196/196 - 52s - loss: 33.1022 - MinusLogProbMetric: 33.1022 - val_loss: 35.1910 - val_MinusLogProbMetric: 35.1910 - lr: 0.0010 - 52s/epoch - 266ms/step
Epoch 100/1000
2023-09-28 18:08:44.156 
Epoch 100/1000 
	 loss: 33.0243, MinusLogProbMetric: 33.0243, val_loss: 32.7684, val_MinusLogProbMetric: 32.7684

Epoch 100: val_loss did not improve from 32.58401
196/196 - 50s - loss: 33.0243 - MinusLogProbMetric: 33.0243 - val_loss: 32.7684 - val_MinusLogProbMetric: 32.7684 - lr: 0.0010 - 50s/epoch - 257ms/step
Epoch 101/1000
2023-09-28 18:09:33.704 
Epoch 101/1000 
	 loss: 33.0557, MinusLogProbMetric: 33.0557, val_loss: 32.7648, val_MinusLogProbMetric: 32.7648

Epoch 101: val_loss did not improve from 32.58401
196/196 - 50s - loss: 33.0557 - MinusLogProbMetric: 33.0557 - val_loss: 32.7648 - val_MinusLogProbMetric: 32.7648 - lr: 0.0010 - 50s/epoch - 253ms/step
Epoch 102/1000
2023-09-28 18:10:26.126 
Epoch 102/1000 
	 loss: 32.8966, MinusLogProbMetric: 32.8966, val_loss: 34.7659, val_MinusLogProbMetric: 34.7659

Epoch 102: val_loss did not improve from 32.58401
196/196 - 52s - loss: 32.8966 - MinusLogProbMetric: 32.8966 - val_loss: 34.7659 - val_MinusLogProbMetric: 34.7659 - lr: 0.0010 - 52s/epoch - 267ms/step
Epoch 103/1000
2023-09-28 18:11:18.365 
Epoch 103/1000 
	 loss: 33.1587, MinusLogProbMetric: 33.1587, val_loss: 34.4151, val_MinusLogProbMetric: 34.4151

Epoch 103: val_loss did not improve from 32.58401
196/196 - 52s - loss: 33.1587 - MinusLogProbMetric: 33.1587 - val_loss: 34.4151 - val_MinusLogProbMetric: 34.4151 - lr: 0.0010 - 52s/epoch - 266ms/step
Epoch 104/1000
2023-09-28 18:12:09.993 
Epoch 104/1000 
	 loss: 33.0346, MinusLogProbMetric: 33.0346, val_loss: 33.8817, val_MinusLogProbMetric: 33.8817

Epoch 104: val_loss did not improve from 32.58401
196/196 - 52s - loss: 33.0346 - MinusLogProbMetric: 33.0346 - val_loss: 33.8817 - val_MinusLogProbMetric: 33.8817 - lr: 0.0010 - 52s/epoch - 263ms/step
Epoch 105/1000
2023-09-28 18:13:03.083 
Epoch 105/1000 
	 loss: 32.8197, MinusLogProbMetric: 32.8197, val_loss: 34.3496, val_MinusLogProbMetric: 34.3496

Epoch 105: val_loss did not improve from 32.58401
196/196 - 53s - loss: 32.8197 - MinusLogProbMetric: 32.8197 - val_loss: 34.3496 - val_MinusLogProbMetric: 34.3496 - lr: 0.0010 - 53s/epoch - 271ms/step
Epoch 106/1000
2023-09-28 18:13:53.449 
Epoch 106/1000 
	 loss: 32.9149, MinusLogProbMetric: 32.9149, val_loss: 32.4110, val_MinusLogProbMetric: 32.4110

Epoch 106: val_loss improved from 32.58401 to 32.41103, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_323/weights/best_weights.h5
196/196 - 51s - loss: 32.9149 - MinusLogProbMetric: 32.9149 - val_loss: 32.4110 - val_MinusLogProbMetric: 32.4110 - lr: 0.0010 - 51s/epoch - 260ms/step
Epoch 107/1000
2023-09-28 18:14:43.904 
Epoch 107/1000 
	 loss: 32.7435, MinusLogProbMetric: 32.7435, val_loss: 32.8694, val_MinusLogProbMetric: 32.8694

Epoch 107: val_loss did not improve from 32.41103
196/196 - 50s - loss: 32.7435 - MinusLogProbMetric: 32.7435 - val_loss: 32.8694 - val_MinusLogProbMetric: 32.8694 - lr: 0.0010 - 50s/epoch - 254ms/step
Epoch 108/1000
2023-09-28 18:15:35.599 
Epoch 108/1000 
	 loss: 32.7262, MinusLogProbMetric: 32.7262, val_loss: 33.6491, val_MinusLogProbMetric: 33.6491

Epoch 108: val_loss did not improve from 32.41103
196/196 - 52s - loss: 32.7262 - MinusLogProbMetric: 32.7262 - val_loss: 33.6491 - val_MinusLogProbMetric: 33.6491 - lr: 0.0010 - 52s/epoch - 264ms/step
Epoch 109/1000
2023-09-28 18:16:26.875 
Epoch 109/1000 
	 loss: 32.6658, MinusLogProbMetric: 32.6658, val_loss: 32.1221, val_MinusLogProbMetric: 32.1221

Epoch 109: val_loss improved from 32.41103 to 32.12205, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_323/weights/best_weights.h5
196/196 - 52s - loss: 32.6658 - MinusLogProbMetric: 32.6658 - val_loss: 32.1221 - val_MinusLogProbMetric: 32.1221 - lr: 0.0010 - 52s/epoch - 264ms/step
Epoch 110/1000
2023-09-28 18:17:16.319 
Epoch 110/1000 
	 loss: 32.8591, MinusLogProbMetric: 32.8591, val_loss: 34.1593, val_MinusLogProbMetric: 34.1593

Epoch 110: val_loss did not improve from 32.12205
196/196 - 49s - loss: 32.8591 - MinusLogProbMetric: 32.8591 - val_loss: 34.1593 - val_MinusLogProbMetric: 34.1593 - lr: 0.0010 - 49s/epoch - 249ms/step
Epoch 111/1000
2023-09-28 18:18:08.817 
Epoch 111/1000 
	 loss: 32.6095, MinusLogProbMetric: 32.6095, val_loss: 31.9343, val_MinusLogProbMetric: 31.9343

Epoch 111: val_loss improved from 32.12205 to 31.93432, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_323/weights/best_weights.h5
196/196 - 53s - loss: 32.6095 - MinusLogProbMetric: 32.6095 - val_loss: 31.9343 - val_MinusLogProbMetric: 31.9343 - lr: 0.0010 - 53s/epoch - 272ms/step
Epoch 112/1000
2023-09-28 18:18:58.618 
Epoch 112/1000 
	 loss: 32.6688, MinusLogProbMetric: 32.6688, val_loss: 32.4361, val_MinusLogProbMetric: 32.4361

Epoch 112: val_loss did not improve from 31.93432
196/196 - 49s - loss: 32.6688 - MinusLogProbMetric: 32.6688 - val_loss: 32.4361 - val_MinusLogProbMetric: 32.4361 - lr: 0.0010 - 49s/epoch - 250ms/step
Epoch 113/1000
2023-09-28 18:19:46.264 
Epoch 113/1000 
	 loss: 32.6150, MinusLogProbMetric: 32.6150, val_loss: 33.2779, val_MinusLogProbMetric: 33.2779

Epoch 113: val_loss did not improve from 31.93432
196/196 - 48s - loss: 32.6150 - MinusLogProbMetric: 32.6150 - val_loss: 33.2779 - val_MinusLogProbMetric: 33.2779 - lr: 0.0010 - 48s/epoch - 243ms/step
Epoch 114/1000
2023-09-28 18:20:37.610 
Epoch 114/1000 
	 loss: 32.5716, MinusLogProbMetric: 32.5716, val_loss: 32.8402, val_MinusLogProbMetric: 32.8402

Epoch 114: val_loss did not improve from 31.93432
196/196 - 51s - loss: 32.5716 - MinusLogProbMetric: 32.5716 - val_loss: 32.8402 - val_MinusLogProbMetric: 32.8402 - lr: 0.0010 - 51s/epoch - 262ms/step
Epoch 115/1000
2023-09-28 18:21:27.275 
Epoch 115/1000 
	 loss: 32.7253, MinusLogProbMetric: 32.7253, val_loss: 32.7246, val_MinusLogProbMetric: 32.7246

Epoch 115: val_loss did not improve from 31.93432
196/196 - 50s - loss: 32.7253 - MinusLogProbMetric: 32.7253 - val_loss: 32.7246 - val_MinusLogProbMetric: 32.7246 - lr: 0.0010 - 50s/epoch - 253ms/step
Epoch 116/1000
2023-09-28 18:22:16.452 
Epoch 116/1000 
	 loss: 32.5641, MinusLogProbMetric: 32.5641, val_loss: 32.5081, val_MinusLogProbMetric: 32.5081

Epoch 116: val_loss did not improve from 31.93432
196/196 - 49s - loss: 32.5641 - MinusLogProbMetric: 32.5641 - val_loss: 32.5081 - val_MinusLogProbMetric: 32.5081 - lr: 0.0010 - 49s/epoch - 251ms/step
Epoch 117/1000
2023-09-28 18:23:07.067 
Epoch 117/1000 
	 loss: 32.4899, MinusLogProbMetric: 32.4899, val_loss: 32.3200, val_MinusLogProbMetric: 32.3200

Epoch 117: val_loss did not improve from 31.93432
196/196 - 51s - loss: 32.4899 - MinusLogProbMetric: 32.4899 - val_loss: 32.3200 - val_MinusLogProbMetric: 32.3200 - lr: 0.0010 - 51s/epoch - 258ms/step
Epoch 118/1000
2023-09-28 18:23:56.737 
Epoch 118/1000 
	 loss: 32.5500, MinusLogProbMetric: 32.5500, val_loss: 34.2471, val_MinusLogProbMetric: 34.2471

Epoch 118: val_loss did not improve from 31.93432
196/196 - 50s - loss: 32.5500 - MinusLogProbMetric: 32.5500 - val_loss: 34.2471 - val_MinusLogProbMetric: 34.2471 - lr: 0.0010 - 50s/epoch - 253ms/step
Epoch 119/1000
2023-09-28 18:24:46.978 
Epoch 119/1000 
	 loss: 32.5032, MinusLogProbMetric: 32.5032, val_loss: 34.1079, val_MinusLogProbMetric: 34.1079

Epoch 119: val_loss did not improve from 31.93432
196/196 - 50s - loss: 32.5032 - MinusLogProbMetric: 32.5032 - val_loss: 34.1079 - val_MinusLogProbMetric: 34.1079 - lr: 0.0010 - 50s/epoch - 256ms/step
Epoch 120/1000
2023-09-28 18:25:37.016 
Epoch 120/1000 
	 loss: 32.2694, MinusLogProbMetric: 32.2694, val_loss: 32.2397, val_MinusLogProbMetric: 32.2397

Epoch 120: val_loss did not improve from 31.93432
196/196 - 50s - loss: 32.2694 - MinusLogProbMetric: 32.2694 - val_loss: 32.2397 - val_MinusLogProbMetric: 32.2397 - lr: 0.0010 - 50s/epoch - 255ms/step
Epoch 121/1000
2023-09-28 18:26:28.009 
Epoch 121/1000 
	 loss: 32.3218, MinusLogProbMetric: 32.3218, val_loss: 32.1643, val_MinusLogProbMetric: 32.1643

Epoch 121: val_loss did not improve from 31.93432
196/196 - 51s - loss: 32.3218 - MinusLogProbMetric: 32.3218 - val_loss: 32.1643 - val_MinusLogProbMetric: 32.1643 - lr: 0.0010 - 51s/epoch - 260ms/step
Epoch 122/1000
2023-09-28 18:27:17.375 
Epoch 122/1000 
	 loss: 32.3852, MinusLogProbMetric: 32.3852, val_loss: 32.4410, val_MinusLogProbMetric: 32.4410

Epoch 122: val_loss did not improve from 31.93432
196/196 - 49s - loss: 32.3852 - MinusLogProbMetric: 32.3852 - val_loss: 32.4410 - val_MinusLogProbMetric: 32.4410 - lr: 0.0010 - 49s/epoch - 252ms/step
Epoch 123/1000
2023-09-28 18:28:08.584 
Epoch 123/1000 
	 loss: 32.3381, MinusLogProbMetric: 32.3381, val_loss: 34.3541, val_MinusLogProbMetric: 34.3541

Epoch 123: val_loss did not improve from 31.93432
196/196 - 51s - loss: 32.3381 - MinusLogProbMetric: 32.3381 - val_loss: 34.3541 - val_MinusLogProbMetric: 34.3541 - lr: 0.0010 - 51s/epoch - 261ms/step
Epoch 124/1000
2023-09-28 18:29:02.613 
Epoch 124/1000 
	 loss: 32.2902, MinusLogProbMetric: 32.2902, val_loss: 31.3896, val_MinusLogProbMetric: 31.3896

Epoch 124: val_loss improved from 31.93432 to 31.38964, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_323/weights/best_weights.h5
196/196 - 55s - loss: 32.2902 - MinusLogProbMetric: 32.2902 - val_loss: 31.3896 - val_MinusLogProbMetric: 31.3896 - lr: 0.0010 - 55s/epoch - 279ms/step
Epoch 125/1000
2023-09-28 18:29:54.529 
Epoch 125/1000 
	 loss: 32.1815, MinusLogProbMetric: 32.1815, val_loss: 35.2367, val_MinusLogProbMetric: 35.2367

Epoch 125: val_loss did not improve from 31.38964
196/196 - 51s - loss: 32.1815 - MinusLogProbMetric: 32.1815 - val_loss: 35.2367 - val_MinusLogProbMetric: 35.2367 - lr: 0.0010 - 51s/epoch - 261ms/step
Epoch 126/1000
2023-09-28 18:30:40.400 
Epoch 126/1000 
	 loss: 32.5485, MinusLogProbMetric: 32.5485, val_loss: 31.9431, val_MinusLogProbMetric: 31.9431

Epoch 126: val_loss did not improve from 31.38964
196/196 - 46s - loss: 32.5485 - MinusLogProbMetric: 32.5485 - val_loss: 31.9431 - val_MinusLogProbMetric: 31.9431 - lr: 0.0010 - 46s/epoch - 234ms/step
Epoch 127/1000
2023-09-28 18:31:30.745 
Epoch 127/1000 
	 loss: 32.1889, MinusLogProbMetric: 32.1889, val_loss: 32.2920, val_MinusLogProbMetric: 32.2920

Epoch 127: val_loss did not improve from 31.38964
196/196 - 50s - loss: 32.1889 - MinusLogProbMetric: 32.1889 - val_loss: 32.2920 - val_MinusLogProbMetric: 32.2920 - lr: 0.0010 - 50s/epoch - 257ms/step
Epoch 128/1000
2023-09-28 18:32:18.522 
Epoch 128/1000 
	 loss: 32.0844, MinusLogProbMetric: 32.0844, val_loss: 32.7310, val_MinusLogProbMetric: 32.7310

Epoch 128: val_loss did not improve from 31.38964
196/196 - 48s - loss: 32.0844 - MinusLogProbMetric: 32.0844 - val_loss: 32.7310 - val_MinusLogProbMetric: 32.7310 - lr: 0.0010 - 48s/epoch - 244ms/step
Epoch 129/1000
2023-09-28 18:33:04.253 
Epoch 129/1000 
	 loss: 32.2495, MinusLogProbMetric: 32.2495, val_loss: 31.7792, val_MinusLogProbMetric: 31.7792

Epoch 129: val_loss did not improve from 31.38964
196/196 - 46s - loss: 32.2495 - MinusLogProbMetric: 32.2495 - val_loss: 31.7792 - val_MinusLogProbMetric: 31.7792 - lr: 0.0010 - 46s/epoch - 233ms/step
Epoch 130/1000
2023-09-28 18:33:56.949 
Epoch 130/1000 
	 loss: 32.1609, MinusLogProbMetric: 32.1609, val_loss: 32.8252, val_MinusLogProbMetric: 32.8252

Epoch 130: val_loss did not improve from 31.38964
196/196 - 53s - loss: 32.1609 - MinusLogProbMetric: 32.1609 - val_loss: 32.8252 - val_MinusLogProbMetric: 32.8252 - lr: 0.0010 - 53s/epoch - 269ms/step
Epoch 131/1000
2023-09-28 18:34:49.978 
Epoch 131/1000 
	 loss: 32.3062, MinusLogProbMetric: 32.3062, val_loss: 31.3936, val_MinusLogProbMetric: 31.3936

Epoch 131: val_loss did not improve from 31.38964
196/196 - 53s - loss: 32.3062 - MinusLogProbMetric: 32.3062 - val_loss: 31.3936 - val_MinusLogProbMetric: 31.3936 - lr: 0.0010 - 53s/epoch - 270ms/step
Epoch 132/1000
2023-09-28 18:35:44.825 
Epoch 132/1000 
	 loss: 31.9468, MinusLogProbMetric: 31.9468, val_loss: 32.0684, val_MinusLogProbMetric: 32.0684

Epoch 132: val_loss did not improve from 31.38964
196/196 - 55s - loss: 31.9468 - MinusLogProbMetric: 31.9468 - val_loss: 32.0684 - val_MinusLogProbMetric: 32.0684 - lr: 0.0010 - 55s/epoch - 280ms/step
Epoch 133/1000
2023-09-28 18:36:37.012 
Epoch 133/1000 
	 loss: 31.9181, MinusLogProbMetric: 31.9181, val_loss: 32.1183, val_MinusLogProbMetric: 32.1183

Epoch 133: val_loss did not improve from 31.38964
196/196 - 52s - loss: 31.9181 - MinusLogProbMetric: 31.9181 - val_loss: 32.1183 - val_MinusLogProbMetric: 32.1183 - lr: 0.0010 - 52s/epoch - 266ms/step
Epoch 134/1000
2023-09-28 18:37:30.194 
Epoch 134/1000 
	 loss: 32.3354, MinusLogProbMetric: 32.3354, val_loss: 31.7972, val_MinusLogProbMetric: 31.7972

Epoch 134: val_loss did not improve from 31.38964
196/196 - 53s - loss: 32.3354 - MinusLogProbMetric: 32.3354 - val_loss: 31.7972 - val_MinusLogProbMetric: 31.7972 - lr: 0.0010 - 53s/epoch - 271ms/step
Epoch 135/1000
2023-09-28 18:38:23.717 
Epoch 135/1000 
	 loss: 31.9436, MinusLogProbMetric: 31.9436, val_loss: 34.4341, val_MinusLogProbMetric: 34.4341

Epoch 135: val_loss did not improve from 31.38964
196/196 - 54s - loss: 31.9436 - MinusLogProbMetric: 31.9436 - val_loss: 34.4341 - val_MinusLogProbMetric: 34.4341 - lr: 0.0010 - 54s/epoch - 273ms/step
Epoch 136/1000
2023-09-28 18:39:17.750 
Epoch 136/1000 
	 loss: 32.0946, MinusLogProbMetric: 32.0946, val_loss: 31.7847, val_MinusLogProbMetric: 31.7847

Epoch 136: val_loss did not improve from 31.38964
196/196 - 54s - loss: 32.0946 - MinusLogProbMetric: 32.0946 - val_loss: 31.7847 - val_MinusLogProbMetric: 31.7847 - lr: 0.0010 - 54s/epoch - 276ms/step
Epoch 137/1000
2023-09-28 18:40:09.777 
Epoch 137/1000 
	 loss: 31.8401, MinusLogProbMetric: 31.8401, val_loss: 31.6499, val_MinusLogProbMetric: 31.6499

Epoch 137: val_loss did not improve from 31.38964
196/196 - 52s - loss: 31.8401 - MinusLogProbMetric: 31.8401 - val_loss: 31.6499 - val_MinusLogProbMetric: 31.6499 - lr: 0.0010 - 52s/epoch - 265ms/step
Epoch 138/1000
2023-09-28 18:41:02.935 
Epoch 138/1000 
	 loss: 32.1337, MinusLogProbMetric: 32.1337, val_loss: 32.3320, val_MinusLogProbMetric: 32.3320

Epoch 138: val_loss did not improve from 31.38964
196/196 - 53s - loss: 32.1337 - MinusLogProbMetric: 32.1337 - val_loss: 32.3320 - val_MinusLogProbMetric: 32.3320 - lr: 0.0010 - 53s/epoch - 271ms/step
Epoch 139/1000
2023-09-28 18:41:55.606 
Epoch 139/1000 
	 loss: 32.0585, MinusLogProbMetric: 32.0585, val_loss: 32.8586, val_MinusLogProbMetric: 32.8586

Epoch 139: val_loss did not improve from 31.38964
196/196 - 53s - loss: 32.0585 - MinusLogProbMetric: 32.0585 - val_loss: 32.8586 - val_MinusLogProbMetric: 32.8586 - lr: 0.0010 - 53s/epoch - 269ms/step
Epoch 140/1000
2023-09-28 18:42:49.928 
Epoch 140/1000 
	 loss: 31.7970, MinusLogProbMetric: 31.7970, val_loss: 32.4388, val_MinusLogProbMetric: 32.4388

Epoch 140: val_loss did not improve from 31.38964
196/196 - 54s - loss: 31.7970 - MinusLogProbMetric: 31.7970 - val_loss: 32.4388 - val_MinusLogProbMetric: 32.4388 - lr: 0.0010 - 54s/epoch - 277ms/step
Epoch 141/1000
2023-09-28 18:43:42.522 
Epoch 141/1000 
	 loss: 31.8741, MinusLogProbMetric: 31.8741, val_loss: 31.8027, val_MinusLogProbMetric: 31.8027

Epoch 141: val_loss did not improve from 31.38964
196/196 - 53s - loss: 31.8741 - MinusLogProbMetric: 31.8741 - val_loss: 31.8027 - val_MinusLogProbMetric: 31.8027 - lr: 0.0010 - 53s/epoch - 268ms/step
Epoch 142/1000
2023-09-28 18:44:36.967 
Epoch 142/1000 
	 loss: 32.1497, MinusLogProbMetric: 32.1497, val_loss: 31.2802, val_MinusLogProbMetric: 31.2802

Epoch 142: val_loss improved from 31.38964 to 31.28024, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_323/weights/best_weights.h5
196/196 - 55s - loss: 32.1497 - MinusLogProbMetric: 32.1497 - val_loss: 31.2802 - val_MinusLogProbMetric: 31.2802 - lr: 0.0010 - 55s/epoch - 281ms/step
Epoch 143/1000
2023-09-28 18:45:32.875 
Epoch 143/1000 
	 loss: 31.8640, MinusLogProbMetric: 31.8640, val_loss: 31.9896, val_MinusLogProbMetric: 31.9896

Epoch 143: val_loss did not improve from 31.28024
196/196 - 55s - loss: 31.8640 - MinusLogProbMetric: 31.8640 - val_loss: 31.9896 - val_MinusLogProbMetric: 31.9896 - lr: 0.0010 - 55s/epoch - 282ms/step
Epoch 144/1000
2023-09-28 18:46:27.441 
Epoch 144/1000 
	 loss: 32.0015, MinusLogProbMetric: 32.0015, val_loss: 34.4082, val_MinusLogProbMetric: 34.4082

Epoch 144: val_loss did not improve from 31.28024
196/196 - 55s - loss: 32.0015 - MinusLogProbMetric: 32.0015 - val_loss: 34.4082 - val_MinusLogProbMetric: 34.4082 - lr: 0.0010 - 55s/epoch - 278ms/step
Epoch 145/1000
2023-09-28 18:47:22.735 
Epoch 145/1000 
	 loss: 31.7826, MinusLogProbMetric: 31.7826, val_loss: 32.8207, val_MinusLogProbMetric: 32.8207

Epoch 145: val_loss did not improve from 31.28024
196/196 - 55s - loss: 31.7826 - MinusLogProbMetric: 31.7826 - val_loss: 32.8207 - val_MinusLogProbMetric: 32.8207 - lr: 0.0010 - 55s/epoch - 282ms/step
Epoch 146/1000
2023-09-28 18:48:17.746 
Epoch 146/1000 
	 loss: 31.9519, MinusLogProbMetric: 31.9519, val_loss: 31.2664, val_MinusLogProbMetric: 31.2664

Epoch 146: val_loss improved from 31.28024 to 31.26643, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_323/weights/best_weights.h5
196/196 - 56s - loss: 31.9519 - MinusLogProbMetric: 31.9519 - val_loss: 31.2664 - val_MinusLogProbMetric: 31.2664 - lr: 0.0010 - 56s/epoch - 285ms/step
Epoch 147/1000
2023-09-28 18:49:13.183 
Epoch 147/1000 
	 loss: 31.6568, MinusLogProbMetric: 31.6568, val_loss: 33.3134, val_MinusLogProbMetric: 33.3134

Epoch 147: val_loss did not improve from 31.26643
196/196 - 55s - loss: 31.6568 - MinusLogProbMetric: 31.6568 - val_loss: 33.3134 - val_MinusLogProbMetric: 33.3134 - lr: 0.0010 - 55s/epoch - 279ms/step
Epoch 148/1000
2023-09-28 18:50:08.622 
Epoch 148/1000 
	 loss: 31.8352, MinusLogProbMetric: 31.8352, val_loss: 32.1320, val_MinusLogProbMetric: 32.1320

Epoch 148: val_loss did not improve from 31.26643
196/196 - 55s - loss: 31.8352 - MinusLogProbMetric: 31.8352 - val_loss: 32.1320 - val_MinusLogProbMetric: 32.1320 - lr: 0.0010 - 55s/epoch - 283ms/step
Epoch 149/1000
2023-09-28 18:51:03.598 
Epoch 149/1000 
	 loss: 31.7756, MinusLogProbMetric: 31.7756, val_loss: 31.4401, val_MinusLogProbMetric: 31.4401

Epoch 149: val_loss did not improve from 31.26643
196/196 - 55s - loss: 31.7756 - MinusLogProbMetric: 31.7756 - val_loss: 31.4401 - val_MinusLogProbMetric: 31.4401 - lr: 0.0010 - 55s/epoch - 280ms/step
Epoch 150/1000
2023-09-28 18:51:57.906 
Epoch 150/1000 
	 loss: 31.6527, MinusLogProbMetric: 31.6527, val_loss: 31.2617, val_MinusLogProbMetric: 31.2617

Epoch 150: val_loss improved from 31.26643 to 31.26172, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_323/weights/best_weights.h5
196/196 - 55s - loss: 31.6527 - MinusLogProbMetric: 31.6527 - val_loss: 31.2617 - val_MinusLogProbMetric: 31.2617 - lr: 0.0010 - 55s/epoch - 281ms/step
Epoch 151/1000
2023-09-28 18:52:51.600 
Epoch 151/1000 
	 loss: 31.6631, MinusLogProbMetric: 31.6631, val_loss: 32.9761, val_MinusLogProbMetric: 32.9761

Epoch 151: val_loss did not improve from 31.26172
196/196 - 53s - loss: 31.6631 - MinusLogProbMetric: 31.6631 - val_loss: 32.9761 - val_MinusLogProbMetric: 32.9761 - lr: 0.0010 - 53s/epoch - 270ms/step
Epoch 152/1000
2023-09-28 18:53:43.975 
Epoch 152/1000 
	 loss: 31.6660, MinusLogProbMetric: 31.6660, val_loss: 31.8232, val_MinusLogProbMetric: 31.8232

Epoch 152: val_loss did not improve from 31.26172
196/196 - 52s - loss: 31.6660 - MinusLogProbMetric: 31.6660 - val_loss: 31.8232 - val_MinusLogProbMetric: 31.8232 - lr: 0.0010 - 52s/epoch - 267ms/step
Epoch 153/1000
2023-09-28 18:54:37.376 
Epoch 153/1000 
	 loss: 31.9863, MinusLogProbMetric: 31.9863, val_loss: 31.1596, val_MinusLogProbMetric: 31.1596

Epoch 153: val_loss improved from 31.26172 to 31.15962, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_323/weights/best_weights.h5
196/196 - 54s - loss: 31.9863 - MinusLogProbMetric: 31.9863 - val_loss: 31.1596 - val_MinusLogProbMetric: 31.1596 - lr: 0.0010 - 54s/epoch - 276ms/step
Epoch 154/1000
2023-09-28 18:55:29.799 
Epoch 154/1000 
	 loss: 31.5904, MinusLogProbMetric: 31.5904, val_loss: 33.1993, val_MinusLogProbMetric: 33.1993

Epoch 154: val_loss did not improve from 31.15962
196/196 - 52s - loss: 31.5904 - MinusLogProbMetric: 31.5904 - val_loss: 33.1993 - val_MinusLogProbMetric: 33.1993 - lr: 0.0010 - 52s/epoch - 264ms/step
Epoch 155/1000
2023-09-28 18:56:19.706 
Epoch 155/1000 
	 loss: 31.6019, MinusLogProbMetric: 31.6019, val_loss: 31.8616, val_MinusLogProbMetric: 31.8616

Epoch 155: val_loss did not improve from 31.15962
196/196 - 50s - loss: 31.6019 - MinusLogProbMetric: 31.6019 - val_loss: 31.8616 - val_MinusLogProbMetric: 31.8616 - lr: 0.0010 - 50s/epoch - 255ms/step
Epoch 156/1000
2023-09-28 18:57:12.746 
Epoch 156/1000 
	 loss: 31.6758, MinusLogProbMetric: 31.6758, val_loss: 31.4262, val_MinusLogProbMetric: 31.4262

Epoch 156: val_loss did not improve from 31.15962
196/196 - 53s - loss: 31.6758 - MinusLogProbMetric: 31.6758 - val_loss: 31.4262 - val_MinusLogProbMetric: 31.4262 - lr: 0.0010 - 53s/epoch - 271ms/step
Epoch 157/1000
2023-09-28 18:58:07.884 
Epoch 157/1000 
	 loss: 31.5851, MinusLogProbMetric: 31.5851, val_loss: 31.5062, val_MinusLogProbMetric: 31.5062

Epoch 157: val_loss did not improve from 31.15962
196/196 - 55s - loss: 31.5851 - MinusLogProbMetric: 31.5851 - val_loss: 31.5062 - val_MinusLogProbMetric: 31.5062 - lr: 0.0010 - 55s/epoch - 281ms/step
Epoch 158/1000
2023-09-28 18:59:02.576 
Epoch 158/1000 
	 loss: 31.6336, MinusLogProbMetric: 31.6336, val_loss: 32.6341, val_MinusLogProbMetric: 32.6341

Epoch 158: val_loss did not improve from 31.15962
196/196 - 55s - loss: 31.6336 - MinusLogProbMetric: 31.6336 - val_loss: 32.6341 - val_MinusLogProbMetric: 32.6341 - lr: 0.0010 - 55s/epoch - 279ms/step
Epoch 159/1000
2023-09-28 18:59:56.989 
Epoch 159/1000 
	 loss: 32.0608, MinusLogProbMetric: 32.0608, val_loss: 32.4288, val_MinusLogProbMetric: 32.4288

Epoch 159: val_loss did not improve from 31.15962
196/196 - 54s - loss: 32.0608 - MinusLogProbMetric: 32.0608 - val_loss: 32.4288 - val_MinusLogProbMetric: 32.4288 - lr: 0.0010 - 54s/epoch - 278ms/step
Epoch 160/1000
2023-09-28 19:00:51.630 
Epoch 160/1000 
	 loss: 31.7308, MinusLogProbMetric: 31.7308, val_loss: 32.6115, val_MinusLogProbMetric: 32.6115

Epoch 160: val_loss did not improve from 31.15962
196/196 - 55s - loss: 31.7308 - MinusLogProbMetric: 31.7308 - val_loss: 32.6115 - val_MinusLogProbMetric: 32.6115 - lr: 0.0010 - 55s/epoch - 279ms/step
Epoch 161/1000
2023-09-28 19:01:47.937 
Epoch 161/1000 
	 loss: 31.6421, MinusLogProbMetric: 31.6421, val_loss: 31.9239, val_MinusLogProbMetric: 31.9239

Epoch 161: val_loss did not improve from 31.15962
196/196 - 56s - loss: 31.6421 - MinusLogProbMetric: 31.6421 - val_loss: 31.9239 - val_MinusLogProbMetric: 31.9239 - lr: 0.0010 - 56s/epoch - 287ms/step
Epoch 162/1000
2023-09-28 19:02:43.636 
Epoch 162/1000 
	 loss: 31.4032, MinusLogProbMetric: 31.4032, val_loss: 30.9808, val_MinusLogProbMetric: 30.9808

Epoch 162: val_loss improved from 31.15962 to 30.98080, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_323/weights/best_weights.h5
196/196 - 56s - loss: 31.4032 - MinusLogProbMetric: 31.4032 - val_loss: 30.9808 - val_MinusLogProbMetric: 30.9808 - lr: 0.0010 - 56s/epoch - 288ms/step
Epoch 163/1000
2023-09-28 19:03:40.323 
Epoch 163/1000 
	 loss: 31.3991, MinusLogProbMetric: 31.3991, val_loss: 32.8239, val_MinusLogProbMetric: 32.8239

Epoch 163: val_loss did not improve from 30.98080
196/196 - 56s - loss: 31.3991 - MinusLogProbMetric: 31.3991 - val_loss: 32.8239 - val_MinusLogProbMetric: 32.8239 - lr: 0.0010 - 56s/epoch - 285ms/step
Epoch 164/1000
2023-09-28 19:04:34.982 
Epoch 164/1000 
	 loss: 31.3549, MinusLogProbMetric: 31.3549, val_loss: 31.1216, val_MinusLogProbMetric: 31.1216

Epoch 164: val_loss did not improve from 30.98080
196/196 - 55s - loss: 31.3549 - MinusLogProbMetric: 31.3549 - val_loss: 31.1216 - val_MinusLogProbMetric: 31.1216 - lr: 0.0010 - 55s/epoch - 279ms/step
Epoch 165/1000
2023-09-28 19:05:31.476 
Epoch 165/1000 
	 loss: 31.3870, MinusLogProbMetric: 31.3870, val_loss: 30.8724, val_MinusLogProbMetric: 30.8724

Epoch 165: val_loss improved from 30.98080 to 30.87236, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_323/weights/best_weights.h5
196/196 - 57s - loss: 31.3870 - MinusLogProbMetric: 31.3870 - val_loss: 30.8724 - val_MinusLogProbMetric: 30.8724 - lr: 0.0010 - 57s/epoch - 293ms/step
Epoch 166/1000
2023-09-28 19:06:27.047 
Epoch 166/1000 
	 loss: 31.3049, MinusLogProbMetric: 31.3049, val_loss: 31.2531, val_MinusLogProbMetric: 31.2531

Epoch 166: val_loss did not improve from 30.87236
196/196 - 55s - loss: 31.3049 - MinusLogProbMetric: 31.3049 - val_loss: 31.2531 - val_MinusLogProbMetric: 31.2531 - lr: 0.0010 - 55s/epoch - 279ms/step
Epoch 167/1000
2023-09-28 19:07:21.605 
Epoch 167/1000 
	 loss: 31.3967, MinusLogProbMetric: 31.3967, val_loss: 32.0145, val_MinusLogProbMetric: 32.0145

Epoch 167: val_loss did not improve from 30.87236
196/196 - 55s - loss: 31.3967 - MinusLogProbMetric: 31.3967 - val_loss: 32.0145 - val_MinusLogProbMetric: 32.0145 - lr: 0.0010 - 55s/epoch - 278ms/step
Epoch 168/1000
2023-09-28 19:08:16.030 
Epoch 168/1000 
	 loss: 31.3920, MinusLogProbMetric: 31.3920, val_loss: 32.5170, val_MinusLogProbMetric: 32.5170

Epoch 168: val_loss did not improve from 30.87236
196/196 - 54s - loss: 31.3920 - MinusLogProbMetric: 31.3920 - val_loss: 32.5170 - val_MinusLogProbMetric: 32.5170 - lr: 0.0010 - 54s/epoch - 278ms/step
Epoch 169/1000
2023-09-28 19:09:11.165 
Epoch 169/1000 
	 loss: 31.2141, MinusLogProbMetric: 31.2141, val_loss: 31.0788, val_MinusLogProbMetric: 31.0788

Epoch 169: val_loss did not improve from 30.87236
196/196 - 55s - loss: 31.2141 - MinusLogProbMetric: 31.2141 - val_loss: 31.0788 - val_MinusLogProbMetric: 31.0788 - lr: 0.0010 - 55s/epoch - 281ms/step
Epoch 170/1000
2023-09-28 19:10:05.157 
Epoch 170/1000 
	 loss: 31.3943, MinusLogProbMetric: 31.3943, val_loss: 32.0781, val_MinusLogProbMetric: 32.0781

Epoch 170: val_loss did not improve from 30.87236
196/196 - 54s - loss: 31.3943 - MinusLogProbMetric: 31.3943 - val_loss: 32.0781 - val_MinusLogProbMetric: 32.0781 - lr: 0.0010 - 54s/epoch - 275ms/step
Epoch 171/1000
2023-09-28 19:10:59.476 
Epoch 171/1000 
	 loss: 31.3330, MinusLogProbMetric: 31.3330, val_loss: 31.3179, val_MinusLogProbMetric: 31.3179

Epoch 171: val_loss did not improve from 30.87236
196/196 - 54s - loss: 31.3330 - MinusLogProbMetric: 31.3330 - val_loss: 31.3179 - val_MinusLogProbMetric: 31.3179 - lr: 0.0010 - 54s/epoch - 277ms/step
Epoch 172/1000
2023-09-28 19:11:53.960 
Epoch 172/1000 
	 loss: 31.2438, MinusLogProbMetric: 31.2438, val_loss: 32.5232, val_MinusLogProbMetric: 32.5232

Epoch 172: val_loss did not improve from 30.87236
196/196 - 54s - loss: 31.2438 - MinusLogProbMetric: 31.2438 - val_loss: 32.5232 - val_MinusLogProbMetric: 32.5232 - lr: 0.0010 - 54s/epoch - 278ms/step
Epoch 173/1000
2023-09-28 19:12:48.269 
Epoch 173/1000 
	 loss: 31.2674, MinusLogProbMetric: 31.2674, val_loss: 30.6660, val_MinusLogProbMetric: 30.6660

Epoch 173: val_loss improved from 30.87236 to 30.66603, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_323/weights/best_weights.h5
196/196 - 55s - loss: 31.2674 - MinusLogProbMetric: 31.2674 - val_loss: 30.6660 - val_MinusLogProbMetric: 30.6660 - lr: 0.0010 - 55s/epoch - 282ms/step
Epoch 174/1000
2023-09-28 19:13:43.540 
Epoch 174/1000 
	 loss: 31.3897, MinusLogProbMetric: 31.3897, val_loss: 31.6696, val_MinusLogProbMetric: 31.6696

Epoch 174: val_loss did not improve from 30.66603
196/196 - 54s - loss: 31.3897 - MinusLogProbMetric: 31.3897 - val_loss: 31.6696 - val_MinusLogProbMetric: 31.6696 - lr: 0.0010 - 54s/epoch - 277ms/step
Epoch 175/1000
2023-09-28 19:14:37.688 
Epoch 175/1000 
	 loss: 31.0887, MinusLogProbMetric: 31.0887, val_loss: 31.4317, val_MinusLogProbMetric: 31.4317

Epoch 175: val_loss did not improve from 30.66603
196/196 - 54s - loss: 31.0887 - MinusLogProbMetric: 31.0887 - val_loss: 31.4317 - val_MinusLogProbMetric: 31.4317 - lr: 0.0010 - 54s/epoch - 276ms/step
Epoch 176/1000
2023-09-28 19:15:32.055 
Epoch 176/1000 
	 loss: 31.0876, MinusLogProbMetric: 31.0876, val_loss: 30.9490, val_MinusLogProbMetric: 30.9490

Epoch 176: val_loss did not improve from 30.66603
196/196 - 54s - loss: 31.0876 - MinusLogProbMetric: 31.0876 - val_loss: 30.9490 - val_MinusLogProbMetric: 30.9490 - lr: 0.0010 - 54s/epoch - 277ms/step
Epoch 177/1000
2023-09-28 19:16:26.806 
Epoch 177/1000 
	 loss: 31.3313, MinusLogProbMetric: 31.3313, val_loss: 31.4237, val_MinusLogProbMetric: 31.4237

Epoch 177: val_loss did not improve from 30.66603
196/196 - 55s - loss: 31.3313 - MinusLogProbMetric: 31.3313 - val_loss: 31.4237 - val_MinusLogProbMetric: 31.4237 - lr: 0.0010 - 55s/epoch - 279ms/step
Epoch 178/1000
2023-09-28 19:17:20.765 
Epoch 178/1000 
	 loss: 31.1755, MinusLogProbMetric: 31.1755, val_loss: 30.5222, val_MinusLogProbMetric: 30.5222

Epoch 178: val_loss improved from 30.66603 to 30.52217, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_323/weights/best_weights.h5
196/196 - 55s - loss: 31.1755 - MinusLogProbMetric: 31.1755 - val_loss: 30.5222 - val_MinusLogProbMetric: 30.5222 - lr: 0.0010 - 55s/epoch - 279ms/step
Epoch 179/1000
2023-09-28 19:18:15.393 
Epoch 179/1000 
	 loss: 31.1179, MinusLogProbMetric: 31.1179, val_loss: 31.9488, val_MinusLogProbMetric: 31.9488

Epoch 179: val_loss did not improve from 30.52217
196/196 - 54s - loss: 31.1179 - MinusLogProbMetric: 31.1179 - val_loss: 31.9488 - val_MinusLogProbMetric: 31.9488 - lr: 0.0010 - 54s/epoch - 275ms/step
Epoch 180/1000
2023-09-28 19:19:09.348 
Epoch 180/1000 
	 loss: 31.0605, MinusLogProbMetric: 31.0605, val_loss: 31.2217, val_MinusLogProbMetric: 31.2217

Epoch 180: val_loss did not improve from 30.52217
196/196 - 54s - loss: 31.0605 - MinusLogProbMetric: 31.0605 - val_loss: 31.2217 - val_MinusLogProbMetric: 31.2217 - lr: 0.0010 - 54s/epoch - 275ms/step
Epoch 181/1000
2023-09-28 19:20:03.561 
Epoch 181/1000 
	 loss: 31.3815, MinusLogProbMetric: 31.3815, val_loss: 31.4910, val_MinusLogProbMetric: 31.4910

Epoch 181: val_loss did not improve from 30.52217
196/196 - 54s - loss: 31.3815 - MinusLogProbMetric: 31.3815 - val_loss: 31.4910 - val_MinusLogProbMetric: 31.4910 - lr: 0.0010 - 54s/epoch - 277ms/step
Epoch 182/1000
2023-09-28 19:20:58.136 
Epoch 182/1000 
	 loss: 31.1864, MinusLogProbMetric: 31.1864, val_loss: 31.0172, val_MinusLogProbMetric: 31.0172

Epoch 182: val_loss did not improve from 30.52217
196/196 - 55s - loss: 31.1864 - MinusLogProbMetric: 31.1864 - val_loss: 31.0172 - val_MinusLogProbMetric: 31.0172 - lr: 0.0010 - 55s/epoch - 278ms/step
Epoch 183/1000
2023-09-28 19:21:52.761 
Epoch 183/1000 
	 loss: 30.9834, MinusLogProbMetric: 30.9834, val_loss: 30.9622, val_MinusLogProbMetric: 30.9622

Epoch 183: val_loss did not improve from 30.52217
196/196 - 55s - loss: 30.9834 - MinusLogProbMetric: 30.9834 - val_loss: 30.9622 - val_MinusLogProbMetric: 30.9622 - lr: 0.0010 - 55s/epoch - 279ms/step
Epoch 184/1000
2023-09-28 19:22:48.762 
Epoch 184/1000 
	 loss: 31.1920, MinusLogProbMetric: 31.1920, val_loss: 31.7742, val_MinusLogProbMetric: 31.7742

Epoch 184: val_loss did not improve from 30.52217
196/196 - 56s - loss: 31.1920 - MinusLogProbMetric: 31.1920 - val_loss: 31.7742 - val_MinusLogProbMetric: 31.7742 - lr: 0.0010 - 56s/epoch - 286ms/step
Epoch 185/1000
2023-09-28 19:23:44.227 
Epoch 185/1000 
	 loss: 31.1279, MinusLogProbMetric: 31.1279, val_loss: 31.5234, val_MinusLogProbMetric: 31.5234

Epoch 185: val_loss did not improve from 30.52217
196/196 - 55s - loss: 31.1279 - MinusLogProbMetric: 31.1279 - val_loss: 31.5234 - val_MinusLogProbMetric: 31.5234 - lr: 0.0010 - 55s/epoch - 283ms/step
Epoch 186/1000
2023-09-28 19:24:39.840 
Epoch 186/1000 
	 loss: 31.0043, MinusLogProbMetric: 31.0043, val_loss: 30.7205, val_MinusLogProbMetric: 30.7205

Epoch 186: val_loss did not improve from 30.52217
196/196 - 56s - loss: 31.0043 - MinusLogProbMetric: 31.0043 - val_loss: 30.7205 - val_MinusLogProbMetric: 30.7205 - lr: 0.0010 - 56s/epoch - 284ms/step
Epoch 187/1000
2023-09-28 19:25:34.606 
Epoch 187/1000 
	 loss: 31.0600, MinusLogProbMetric: 31.0600, val_loss: 30.7095, val_MinusLogProbMetric: 30.7095

Epoch 187: val_loss did not improve from 30.52217
196/196 - 55s - loss: 31.0600 - MinusLogProbMetric: 31.0600 - val_loss: 30.7095 - val_MinusLogProbMetric: 30.7095 - lr: 0.0010 - 55s/epoch - 279ms/step
Epoch 188/1000
2023-09-28 19:26:29.055 
Epoch 188/1000 
	 loss: 31.3065, MinusLogProbMetric: 31.3065, val_loss: 31.0546, val_MinusLogProbMetric: 31.0546

Epoch 188: val_loss did not improve from 30.52217
196/196 - 54s - loss: 31.3065 - MinusLogProbMetric: 31.3065 - val_loss: 31.0546 - val_MinusLogProbMetric: 31.0546 - lr: 0.0010 - 54s/epoch - 278ms/step
Epoch 189/1000
2023-09-28 19:27:24.408 
Epoch 189/1000 
	 loss: 30.8758, MinusLogProbMetric: 30.8758, val_loss: 30.6020, val_MinusLogProbMetric: 30.6020

Epoch 189: val_loss did not improve from 30.52217
196/196 - 55s - loss: 30.8758 - MinusLogProbMetric: 30.8758 - val_loss: 30.6020 - val_MinusLogProbMetric: 30.6020 - lr: 0.0010 - 55s/epoch - 282ms/step
Epoch 190/1000
2023-09-28 19:28:19.458 
Epoch 190/1000 
	 loss: 30.8251, MinusLogProbMetric: 30.8251, val_loss: 32.5650, val_MinusLogProbMetric: 32.5650

Epoch 190: val_loss did not improve from 30.52217
196/196 - 55s - loss: 30.8251 - MinusLogProbMetric: 30.8251 - val_loss: 32.5650 - val_MinusLogProbMetric: 32.5650 - lr: 0.0010 - 55s/epoch - 281ms/step
Epoch 191/1000
2023-09-28 19:29:13.365 
Epoch 191/1000 
	 loss: 31.0995, MinusLogProbMetric: 31.0995, val_loss: 30.8601, val_MinusLogProbMetric: 30.8601

Epoch 191: val_loss did not improve from 30.52217
196/196 - 54s - loss: 31.0995 - MinusLogProbMetric: 31.0995 - val_loss: 30.8601 - val_MinusLogProbMetric: 30.8601 - lr: 0.0010 - 54s/epoch - 275ms/step
Epoch 192/1000
2023-09-28 19:30:08.957 
Epoch 192/1000 
	 loss: 31.0246, MinusLogProbMetric: 31.0246, val_loss: 32.2963, val_MinusLogProbMetric: 32.2963

Epoch 192: val_loss did not improve from 30.52217
196/196 - 56s - loss: 31.0246 - MinusLogProbMetric: 31.0246 - val_loss: 32.2963 - val_MinusLogProbMetric: 32.2963 - lr: 0.0010 - 56s/epoch - 284ms/step
Epoch 193/1000
2023-09-28 19:31:03.407 
Epoch 193/1000 
	 loss: 31.0251, MinusLogProbMetric: 31.0251, val_loss: 30.8990, val_MinusLogProbMetric: 30.8990

Epoch 193: val_loss did not improve from 30.52217
196/196 - 54s - loss: 31.0251 - MinusLogProbMetric: 31.0251 - val_loss: 30.8990 - val_MinusLogProbMetric: 30.8990 - lr: 0.0010 - 54s/epoch - 278ms/step
Epoch 194/1000
2023-09-28 19:31:58.035 
Epoch 194/1000 
	 loss: 30.7207, MinusLogProbMetric: 30.7207, val_loss: 33.9000, val_MinusLogProbMetric: 33.9000

Epoch 194: val_loss did not improve from 30.52217
196/196 - 55s - loss: 30.7207 - MinusLogProbMetric: 30.7207 - val_loss: 33.9000 - val_MinusLogProbMetric: 33.9000 - lr: 0.0010 - 55s/epoch - 279ms/step
Epoch 195/1000
2023-09-28 19:32:53.734 
Epoch 195/1000 
	 loss: 30.9780, MinusLogProbMetric: 30.9780, val_loss: 33.0519, val_MinusLogProbMetric: 33.0519

Epoch 195: val_loss did not improve from 30.52217
196/196 - 56s - loss: 30.9780 - MinusLogProbMetric: 30.9780 - val_loss: 33.0519 - val_MinusLogProbMetric: 33.0519 - lr: 0.0010 - 56s/epoch - 284ms/step
Epoch 196/1000
2023-09-28 19:33:49.766 
Epoch 196/1000 
	 loss: 30.8687, MinusLogProbMetric: 30.8687, val_loss: 30.5925, val_MinusLogProbMetric: 30.5925

Epoch 196: val_loss did not improve from 30.52217
196/196 - 56s - loss: 30.8687 - MinusLogProbMetric: 30.8687 - val_loss: 30.5925 - val_MinusLogProbMetric: 30.5925 - lr: 0.0010 - 56s/epoch - 286ms/step
Epoch 197/1000
2023-09-28 19:34:42.568 
Epoch 197/1000 
	 loss: 30.8323, MinusLogProbMetric: 30.8323, val_loss: 30.9192, val_MinusLogProbMetric: 30.9192

Epoch 197: val_loss did not improve from 30.52217
196/196 - 53s - loss: 30.8323 - MinusLogProbMetric: 30.8323 - val_loss: 30.9192 - val_MinusLogProbMetric: 30.9192 - lr: 0.0010 - 53s/epoch - 269ms/step
Epoch 198/1000
2023-09-28 19:35:37.321 
Epoch 198/1000 
	 loss: 30.9552, MinusLogProbMetric: 30.9552, val_loss: 30.5933, val_MinusLogProbMetric: 30.5933

Epoch 198: val_loss did not improve from 30.52217
196/196 - 55s - loss: 30.9552 - MinusLogProbMetric: 30.9552 - val_loss: 30.5933 - val_MinusLogProbMetric: 30.5933 - lr: 0.0010 - 55s/epoch - 279ms/step
Epoch 199/1000
2023-09-28 19:36:31.648 
Epoch 199/1000 
	 loss: 30.8984, MinusLogProbMetric: 30.8984, val_loss: 31.5824, val_MinusLogProbMetric: 31.5824

Epoch 199: val_loss did not improve from 30.52217
196/196 - 54s - loss: 30.8984 - MinusLogProbMetric: 30.8984 - val_loss: 31.5824 - val_MinusLogProbMetric: 31.5824 - lr: 0.0010 - 54s/epoch - 277ms/step
Epoch 200/1000
2023-09-28 19:37:26.183 
Epoch 200/1000 
	 loss: 30.7915, MinusLogProbMetric: 30.7915, val_loss: 31.5468, val_MinusLogProbMetric: 31.5468

Epoch 200: val_loss did not improve from 30.52217
196/196 - 55s - loss: 30.7915 - MinusLogProbMetric: 30.7915 - val_loss: 31.5468 - val_MinusLogProbMetric: 31.5468 - lr: 0.0010 - 55s/epoch - 278ms/step
Epoch 201/1000
2023-09-28 19:38:19.769 
Epoch 201/1000 
	 loss: 30.8479, MinusLogProbMetric: 30.8479, val_loss: 30.7145, val_MinusLogProbMetric: 30.7145

Epoch 201: val_loss did not improve from 30.52217
196/196 - 54s - loss: 30.8479 - MinusLogProbMetric: 30.8479 - val_loss: 30.7145 - val_MinusLogProbMetric: 30.7145 - lr: 0.0010 - 54s/epoch - 273ms/step
Epoch 202/1000
2023-09-28 19:39:10.352 
Epoch 202/1000 
	 loss: 30.7954, MinusLogProbMetric: 30.7954, val_loss: 31.6406, val_MinusLogProbMetric: 31.6406

Epoch 202: val_loss did not improve from 30.52217
196/196 - 51s - loss: 30.7954 - MinusLogProbMetric: 30.7954 - val_loss: 31.6406 - val_MinusLogProbMetric: 31.6406 - lr: 0.0010 - 51s/epoch - 258ms/step
Epoch 203/1000
2023-09-28 19:40:02.494 
Epoch 203/1000 
	 loss: 30.8269, MinusLogProbMetric: 30.8269, val_loss: 31.3008, val_MinusLogProbMetric: 31.3008

Epoch 203: val_loss did not improve from 30.52217
196/196 - 52s - loss: 30.8269 - MinusLogProbMetric: 30.8269 - val_loss: 31.3008 - val_MinusLogProbMetric: 31.3008 - lr: 0.0010 - 52s/epoch - 266ms/step
Epoch 204/1000
2023-09-28 19:40:52.310 
Epoch 204/1000 
	 loss: 30.6234, MinusLogProbMetric: 30.6234, val_loss: 31.6964, val_MinusLogProbMetric: 31.6964

Epoch 204: val_loss did not improve from 30.52217
196/196 - 50s - loss: 30.6234 - MinusLogProbMetric: 30.6234 - val_loss: 31.6964 - val_MinusLogProbMetric: 31.6964 - lr: 0.0010 - 50s/epoch - 254ms/step
Epoch 205/1000
2023-09-28 19:41:42.441 
Epoch 205/1000 
	 loss: 30.8140, MinusLogProbMetric: 30.8140, val_loss: 30.4840, val_MinusLogProbMetric: 30.4840

Epoch 205: val_loss improved from 30.52217 to 30.48401, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_323/weights/best_weights.h5
196/196 - 51s - loss: 30.8140 - MinusLogProbMetric: 30.8140 - val_loss: 30.4840 - val_MinusLogProbMetric: 30.4840 - lr: 0.0010 - 51s/epoch - 259ms/step
Epoch 206/1000
2023-09-28 19:42:36.205 
Epoch 206/1000 
	 loss: 30.7200, MinusLogProbMetric: 30.7200, val_loss: 31.1636, val_MinusLogProbMetric: 31.1636

Epoch 206: val_loss did not improve from 30.48401
196/196 - 53s - loss: 30.7200 - MinusLogProbMetric: 30.7200 - val_loss: 31.1636 - val_MinusLogProbMetric: 31.1636 - lr: 0.0010 - 53s/epoch - 271ms/step
Epoch 207/1000
2023-09-28 19:43:26.635 
Epoch 207/1000 
	 loss: 30.6694, MinusLogProbMetric: 30.6694, val_loss: 30.5423, val_MinusLogProbMetric: 30.5423

Epoch 207: val_loss did not improve from 30.48401
196/196 - 50s - loss: 30.6694 - MinusLogProbMetric: 30.6694 - val_loss: 30.5423 - val_MinusLogProbMetric: 30.5423 - lr: 0.0010 - 50s/epoch - 257ms/step
Epoch 208/1000
2023-09-28 19:44:17.302 
Epoch 208/1000 
	 loss: 30.6346, MinusLogProbMetric: 30.6346, val_loss: 32.3400, val_MinusLogProbMetric: 32.3400

Epoch 208: val_loss did not improve from 30.48401
196/196 - 51s - loss: 30.6346 - MinusLogProbMetric: 30.6346 - val_loss: 32.3400 - val_MinusLogProbMetric: 32.3400 - lr: 0.0010 - 51s/epoch - 258ms/step
Epoch 209/1000
2023-09-28 19:45:07.287 
Epoch 209/1000 
	 loss: 30.6309, MinusLogProbMetric: 30.6309, val_loss: 31.2915, val_MinusLogProbMetric: 31.2915

Epoch 209: val_loss did not improve from 30.48401
196/196 - 50s - loss: 30.6309 - MinusLogProbMetric: 30.6309 - val_loss: 31.2915 - val_MinusLogProbMetric: 31.2915 - lr: 0.0010 - 50s/epoch - 255ms/step
Epoch 210/1000
2023-09-28 19:46:01.468 
Epoch 210/1000 
	 loss: 30.7092, MinusLogProbMetric: 30.7092, val_loss: 30.6650, val_MinusLogProbMetric: 30.6650

Epoch 210: val_loss did not improve from 30.48401
196/196 - 54s - loss: 30.7092 - MinusLogProbMetric: 30.7092 - val_loss: 30.6650 - val_MinusLogProbMetric: 30.6650 - lr: 0.0010 - 54s/epoch - 276ms/step
Epoch 211/1000
2023-09-28 19:46:54.211 
Epoch 211/1000 
	 loss: 30.6506, MinusLogProbMetric: 30.6506, val_loss: 31.7836, val_MinusLogProbMetric: 31.7836

Epoch 211: val_loss did not improve from 30.48401
196/196 - 53s - loss: 30.6506 - MinusLogProbMetric: 30.6506 - val_loss: 31.7836 - val_MinusLogProbMetric: 31.7836 - lr: 0.0010 - 53s/epoch - 269ms/step
Epoch 212/1000
2023-09-28 19:47:46.433 
Epoch 212/1000 
	 loss: 30.5995, MinusLogProbMetric: 30.5995, val_loss: 32.1355, val_MinusLogProbMetric: 32.1355

Epoch 212: val_loss did not improve from 30.48401
196/196 - 52s - loss: 30.5995 - MinusLogProbMetric: 30.5995 - val_loss: 32.1355 - val_MinusLogProbMetric: 32.1355 - lr: 0.0010 - 52s/epoch - 266ms/step
Epoch 213/1000
2023-09-28 19:48:39.155 
Epoch 213/1000 
	 loss: 30.6604, MinusLogProbMetric: 30.6604, val_loss: 30.8404, val_MinusLogProbMetric: 30.8404

Epoch 213: val_loss did not improve from 30.48401
196/196 - 53s - loss: 30.6604 - MinusLogProbMetric: 30.6604 - val_loss: 30.8404 - val_MinusLogProbMetric: 30.8404 - lr: 0.0010 - 53s/epoch - 269ms/step
Epoch 214/1000
2023-09-28 19:49:30.625 
Epoch 214/1000 
	 loss: 30.5769, MinusLogProbMetric: 30.5769, val_loss: 30.7438, val_MinusLogProbMetric: 30.7438

Epoch 214: val_loss did not improve from 30.48401
196/196 - 51s - loss: 30.5769 - MinusLogProbMetric: 30.5769 - val_loss: 30.7438 - val_MinusLogProbMetric: 30.7438 - lr: 0.0010 - 51s/epoch - 263ms/step
Epoch 215/1000
2023-09-28 19:50:22.397 
Epoch 215/1000 
	 loss: 30.6372, MinusLogProbMetric: 30.6372, val_loss: 31.4018, val_MinusLogProbMetric: 31.4018

Epoch 215: val_loss did not improve from 30.48401
196/196 - 52s - loss: 30.6372 - MinusLogProbMetric: 30.6372 - val_loss: 31.4018 - val_MinusLogProbMetric: 31.4018 - lr: 0.0010 - 52s/epoch - 264ms/step
Epoch 216/1000
2023-09-28 19:51:14.628 
Epoch 216/1000 
	 loss: 30.5886, MinusLogProbMetric: 30.5886, val_loss: 31.8409, val_MinusLogProbMetric: 31.8409

Epoch 216: val_loss did not improve from 30.48401
196/196 - 52s - loss: 30.5886 - MinusLogProbMetric: 30.5886 - val_loss: 31.8409 - val_MinusLogProbMetric: 31.8409 - lr: 0.0010 - 52s/epoch - 266ms/step
Epoch 217/1000
2023-09-28 19:52:07.904 
Epoch 217/1000 
	 loss: 30.6282, MinusLogProbMetric: 30.6282, val_loss: 30.7194, val_MinusLogProbMetric: 30.7194

Epoch 217: val_loss did not improve from 30.48401
196/196 - 53s - loss: 30.6282 - MinusLogProbMetric: 30.6282 - val_loss: 30.7194 - val_MinusLogProbMetric: 30.7194 - lr: 0.0010 - 53s/epoch - 272ms/step
Epoch 218/1000
2023-09-28 19:52:59.646 
Epoch 218/1000 
	 loss: 30.5193, MinusLogProbMetric: 30.5193, val_loss: 32.3045, val_MinusLogProbMetric: 32.3045

Epoch 218: val_loss did not improve from 30.48401
196/196 - 52s - loss: 30.5193 - MinusLogProbMetric: 30.5193 - val_loss: 32.3045 - val_MinusLogProbMetric: 32.3045 - lr: 0.0010 - 52s/epoch - 264ms/step
Epoch 219/1000
2023-09-28 19:53:53.630 
Epoch 219/1000 
	 loss: 30.6308, MinusLogProbMetric: 30.6308, val_loss: 31.3095, val_MinusLogProbMetric: 31.3095

Epoch 219: val_loss did not improve from 30.48401
196/196 - 54s - loss: 30.6308 - MinusLogProbMetric: 30.6308 - val_loss: 31.3095 - val_MinusLogProbMetric: 31.3095 - lr: 0.0010 - 54s/epoch - 275ms/step
Epoch 220/1000
2023-09-28 19:54:44.811 
Epoch 220/1000 
	 loss: 30.6664, MinusLogProbMetric: 30.6664, val_loss: 30.0347, val_MinusLogProbMetric: 30.0347

Epoch 220: val_loss improved from 30.48401 to 30.03468, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_323/weights/best_weights.h5
196/196 - 52s - loss: 30.6664 - MinusLogProbMetric: 30.6664 - val_loss: 30.0347 - val_MinusLogProbMetric: 30.0347 - lr: 0.0010 - 52s/epoch - 264ms/step
Epoch 221/1000
2023-09-28 19:55:37.299 
Epoch 221/1000 
	 loss: 30.6596, MinusLogProbMetric: 30.6596, val_loss: 30.1040, val_MinusLogProbMetric: 30.1040

Epoch 221: val_loss did not improve from 30.03468
196/196 - 52s - loss: 30.6596 - MinusLogProbMetric: 30.6596 - val_loss: 30.1040 - val_MinusLogProbMetric: 30.1040 - lr: 0.0010 - 52s/epoch - 264ms/step
Epoch 222/1000
2023-09-28 19:56:33.038 
Epoch 222/1000 
	 loss: 30.5514, MinusLogProbMetric: 30.5514, val_loss: 31.5874, val_MinusLogProbMetric: 31.5874

Epoch 222: val_loss did not improve from 30.03468
196/196 - 56s - loss: 30.5514 - MinusLogProbMetric: 30.5514 - val_loss: 31.5874 - val_MinusLogProbMetric: 31.5874 - lr: 0.0010 - 56s/epoch - 284ms/step
Epoch 223/1000
2023-09-28 19:57:25.523 
Epoch 223/1000 
	 loss: 30.5674, MinusLogProbMetric: 30.5674, val_loss: 30.5083, val_MinusLogProbMetric: 30.5083

Epoch 223: val_loss did not improve from 30.03468
196/196 - 52s - loss: 30.5674 - MinusLogProbMetric: 30.5674 - val_loss: 30.5083 - val_MinusLogProbMetric: 30.5083 - lr: 0.0010 - 52s/epoch - 268ms/step
Epoch 224/1000
2023-09-28 19:58:18.331 
Epoch 224/1000 
	 loss: 30.5522, MinusLogProbMetric: 30.5522, val_loss: 30.7230, val_MinusLogProbMetric: 30.7230

Epoch 224: val_loss did not improve from 30.03468
196/196 - 53s - loss: 30.5522 - MinusLogProbMetric: 30.5522 - val_loss: 30.7230 - val_MinusLogProbMetric: 30.7230 - lr: 0.0010 - 53s/epoch - 269ms/step
Epoch 225/1000
2023-09-28 19:59:11.459 
Epoch 225/1000 
	 loss: 30.5592, MinusLogProbMetric: 30.5592, val_loss: 30.2406, val_MinusLogProbMetric: 30.2406

Epoch 225: val_loss did not improve from 30.03468
196/196 - 53s - loss: 30.5592 - MinusLogProbMetric: 30.5592 - val_loss: 30.2406 - val_MinusLogProbMetric: 30.2406 - lr: 0.0010 - 53s/epoch - 271ms/step
Epoch 226/1000
2023-09-28 20:00:03.956 
Epoch 226/1000 
	 loss: 30.3886, MinusLogProbMetric: 30.3886, val_loss: 30.1892, val_MinusLogProbMetric: 30.1892

Epoch 226: val_loss did not improve from 30.03468
196/196 - 52s - loss: 30.3886 - MinusLogProbMetric: 30.3886 - val_loss: 30.1892 - val_MinusLogProbMetric: 30.1892 - lr: 0.0010 - 52s/epoch - 268ms/step
Epoch 227/1000
2023-09-28 20:00:58.212 
Epoch 227/1000 
	 loss: 30.6518, MinusLogProbMetric: 30.6518, val_loss: 31.4593, val_MinusLogProbMetric: 31.4593

Epoch 227: val_loss did not improve from 30.03468
196/196 - 54s - loss: 30.6518 - MinusLogProbMetric: 30.6518 - val_loss: 31.4593 - val_MinusLogProbMetric: 31.4593 - lr: 0.0010 - 54s/epoch - 277ms/step
Epoch 228/1000
2023-09-28 20:01:52.395 
Epoch 228/1000 
	 loss: 30.4456, MinusLogProbMetric: 30.4456, val_loss: 31.2613, val_MinusLogProbMetric: 31.2613

Epoch 228: val_loss did not improve from 30.03468
196/196 - 54s - loss: 30.4456 - MinusLogProbMetric: 30.4456 - val_loss: 31.2613 - val_MinusLogProbMetric: 31.2613 - lr: 0.0010 - 54s/epoch - 276ms/step
Epoch 229/1000
2023-09-28 20:02:44.206 
Epoch 229/1000 
	 loss: 30.3867, MinusLogProbMetric: 30.3867, val_loss: 30.1582, val_MinusLogProbMetric: 30.1582

Epoch 229: val_loss did not improve from 30.03468
196/196 - 52s - loss: 30.3867 - MinusLogProbMetric: 30.3867 - val_loss: 30.1582 - val_MinusLogProbMetric: 30.1582 - lr: 0.0010 - 52s/epoch - 264ms/step
Epoch 230/1000
2023-09-28 20:03:38.582 
Epoch 230/1000 
	 loss: 30.5987, MinusLogProbMetric: 30.5987, val_loss: 31.2988, val_MinusLogProbMetric: 31.2988

Epoch 230: val_loss did not improve from 30.03468
196/196 - 54s - loss: 30.5987 - MinusLogProbMetric: 30.5987 - val_loss: 31.2988 - val_MinusLogProbMetric: 31.2988 - lr: 0.0010 - 54s/epoch - 277ms/step
Epoch 231/1000
2023-09-28 20:04:30.187 
Epoch 231/1000 
	 loss: 30.3543, MinusLogProbMetric: 30.3543, val_loss: 30.7473, val_MinusLogProbMetric: 30.7473

Epoch 231: val_loss did not improve from 30.03468
196/196 - 52s - loss: 30.3543 - MinusLogProbMetric: 30.3543 - val_loss: 30.7473 - val_MinusLogProbMetric: 30.7473 - lr: 0.0010 - 52s/epoch - 263ms/step
Epoch 232/1000
2023-09-28 20:05:25.847 
Epoch 232/1000 
	 loss: 30.4241, MinusLogProbMetric: 30.4241, val_loss: 30.8135, val_MinusLogProbMetric: 30.8135

Epoch 232: val_loss did not improve from 30.03468
196/196 - 56s - loss: 30.4241 - MinusLogProbMetric: 30.4241 - val_loss: 30.8135 - val_MinusLogProbMetric: 30.8135 - lr: 0.0010 - 56s/epoch - 284ms/step
Epoch 233/1000
2023-09-28 20:06:19.485 
Epoch 233/1000 
	 loss: 30.5351, MinusLogProbMetric: 30.5351, val_loss: 30.4346, val_MinusLogProbMetric: 30.4346

Epoch 233: val_loss did not improve from 30.03468
196/196 - 54s - loss: 30.5351 - MinusLogProbMetric: 30.5351 - val_loss: 30.4346 - val_MinusLogProbMetric: 30.4346 - lr: 0.0010 - 54s/epoch - 274ms/step
Epoch 234/1000
2023-09-28 20:07:14.037 
Epoch 234/1000 
	 loss: 30.3784, MinusLogProbMetric: 30.3784, val_loss: 30.2733, val_MinusLogProbMetric: 30.2733

Epoch 234: val_loss did not improve from 30.03468
196/196 - 55s - loss: 30.3784 - MinusLogProbMetric: 30.3784 - val_loss: 30.2733 - val_MinusLogProbMetric: 30.2733 - lr: 0.0010 - 55s/epoch - 278ms/step
Epoch 235/1000
2023-09-28 20:08:08.481 
Epoch 235/1000 
	 loss: 30.3458, MinusLogProbMetric: 30.3458, val_loss: 30.1198, val_MinusLogProbMetric: 30.1198

Epoch 235: val_loss did not improve from 30.03468
196/196 - 54s - loss: 30.3458 - MinusLogProbMetric: 30.3458 - val_loss: 30.1198 - val_MinusLogProbMetric: 30.1198 - lr: 0.0010 - 54s/epoch - 278ms/step
Epoch 236/1000
2023-09-28 20:09:02.226 
Epoch 236/1000 
	 loss: 30.6047, MinusLogProbMetric: 30.6047, val_loss: 30.2571, val_MinusLogProbMetric: 30.2571

Epoch 236: val_loss did not improve from 30.03468
196/196 - 54s - loss: 30.6047 - MinusLogProbMetric: 30.6047 - val_loss: 30.2571 - val_MinusLogProbMetric: 30.2571 - lr: 0.0010 - 54s/epoch - 274ms/step
Epoch 237/1000
2023-09-28 20:09:56.509 
Epoch 237/1000 
	 loss: 30.2069, MinusLogProbMetric: 30.2069, val_loss: 31.4521, val_MinusLogProbMetric: 31.4521

Epoch 237: val_loss did not improve from 30.03468
196/196 - 54s - loss: 30.2069 - MinusLogProbMetric: 30.2069 - val_loss: 31.4521 - val_MinusLogProbMetric: 31.4521 - lr: 0.0010 - 54s/epoch - 277ms/step
Epoch 238/1000
2023-09-28 20:10:50.788 
Epoch 238/1000 
	 loss: 30.4552, MinusLogProbMetric: 30.4552, val_loss: 29.9389, val_MinusLogProbMetric: 29.9389

Epoch 238: val_loss improved from 30.03468 to 29.93894, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_323/weights/best_weights.h5
196/196 - 55s - loss: 30.4552 - MinusLogProbMetric: 30.4552 - val_loss: 29.9389 - val_MinusLogProbMetric: 29.9389 - lr: 0.0010 - 55s/epoch - 280ms/step
Epoch 239/1000
2023-09-28 20:11:45.670 
Epoch 239/1000 
	 loss: 30.3497, MinusLogProbMetric: 30.3497, val_loss: 31.0888, val_MinusLogProbMetric: 31.0888

Epoch 239: val_loss did not improve from 29.93894
196/196 - 54s - loss: 30.3497 - MinusLogProbMetric: 30.3497 - val_loss: 31.0888 - val_MinusLogProbMetric: 31.0888 - lr: 0.0010 - 54s/epoch - 277ms/step
Epoch 240/1000
2023-09-28 20:12:39.832 
Epoch 240/1000 
	 loss: 30.4576, MinusLogProbMetric: 30.4576, val_loss: 30.0321, val_MinusLogProbMetric: 30.0321

Epoch 240: val_loss did not improve from 29.93894
196/196 - 54s - loss: 30.4576 - MinusLogProbMetric: 30.4576 - val_loss: 30.0321 - val_MinusLogProbMetric: 30.0321 - lr: 0.0010 - 54s/epoch - 276ms/step
Epoch 241/1000
2023-09-28 20:13:33.471 
Epoch 241/1000 
	 loss: 30.5062, MinusLogProbMetric: 30.5062, val_loss: 30.1865, val_MinusLogProbMetric: 30.1865

Epoch 241: val_loss did not improve from 29.93894
196/196 - 54s - loss: 30.5062 - MinusLogProbMetric: 30.5062 - val_loss: 30.1865 - val_MinusLogProbMetric: 30.1865 - lr: 0.0010 - 54s/epoch - 274ms/step
Epoch 242/1000
2023-09-28 20:14:28.114 
Epoch 242/1000 
	 loss: 30.4090, MinusLogProbMetric: 30.4090, val_loss: 29.9212, val_MinusLogProbMetric: 29.9212

Epoch 242: val_loss improved from 29.93894 to 29.92124, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_323/weights/best_weights.h5
196/196 - 55s - loss: 30.4090 - MinusLogProbMetric: 30.4090 - val_loss: 29.9212 - val_MinusLogProbMetric: 29.9212 - lr: 0.0010 - 55s/epoch - 282ms/step
Epoch 243/1000
2023-09-28 20:15:22.563 
Epoch 243/1000 
	 loss: 30.4635, MinusLogProbMetric: 30.4635, val_loss: 32.1360, val_MinusLogProbMetric: 32.1360

Epoch 243: val_loss did not improve from 29.92124
196/196 - 54s - loss: 30.4635 - MinusLogProbMetric: 30.4635 - val_loss: 32.1360 - val_MinusLogProbMetric: 32.1360 - lr: 0.0010 - 54s/epoch - 274ms/step
Epoch 244/1000
2023-09-28 20:16:15.795 
Epoch 244/1000 
	 loss: 30.4804, MinusLogProbMetric: 30.4804, val_loss: 30.1413, val_MinusLogProbMetric: 30.1413

Epoch 244: val_loss did not improve from 29.92124
196/196 - 53s - loss: 30.4804 - MinusLogProbMetric: 30.4804 - val_loss: 30.1413 - val_MinusLogProbMetric: 30.1413 - lr: 0.0010 - 53s/epoch - 272ms/step
Epoch 245/1000
2023-09-28 20:17:09.932 
Epoch 245/1000 
	 loss: 30.3122, MinusLogProbMetric: 30.3122, val_loss: 30.0509, val_MinusLogProbMetric: 30.0509

Epoch 245: val_loss did not improve from 29.92124
196/196 - 54s - loss: 30.3122 - MinusLogProbMetric: 30.3122 - val_loss: 30.0509 - val_MinusLogProbMetric: 30.0509 - lr: 0.0010 - 54s/epoch - 276ms/step
Epoch 246/1000
2023-09-28 20:18:04.434 
Epoch 246/1000 
	 loss: 30.2739, MinusLogProbMetric: 30.2739, val_loss: 30.0109, val_MinusLogProbMetric: 30.0109

Epoch 246: val_loss did not improve from 29.92124
196/196 - 54s - loss: 30.2739 - MinusLogProbMetric: 30.2739 - val_loss: 30.0109 - val_MinusLogProbMetric: 30.0109 - lr: 0.0010 - 54s/epoch - 278ms/step
Epoch 247/1000
2023-09-28 20:18:57.756 
Epoch 247/1000 
	 loss: 30.3703, MinusLogProbMetric: 30.3703, val_loss: 30.6836, val_MinusLogProbMetric: 30.6836

Epoch 247: val_loss did not improve from 29.92124
196/196 - 53s - loss: 30.3703 - MinusLogProbMetric: 30.3703 - val_loss: 30.6836 - val_MinusLogProbMetric: 30.6836 - lr: 0.0010 - 53s/epoch - 272ms/step
Epoch 248/1000
2023-09-28 20:19:51.774 
Epoch 248/1000 
	 loss: 30.2576, MinusLogProbMetric: 30.2576, val_loss: 30.0794, val_MinusLogProbMetric: 30.0794

Epoch 248: val_loss did not improve from 29.92124
196/196 - 54s - loss: 30.2576 - MinusLogProbMetric: 30.2576 - val_loss: 30.0794 - val_MinusLogProbMetric: 30.0794 - lr: 0.0010 - 54s/epoch - 276ms/step
Epoch 249/1000
2023-09-28 20:20:46.188 
Epoch 249/1000 
	 loss: 30.2617, MinusLogProbMetric: 30.2617, val_loss: 29.7967, val_MinusLogProbMetric: 29.7967

Epoch 249: val_loss improved from 29.92124 to 29.79669, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_323/weights/best_weights.h5
196/196 - 55s - loss: 30.2617 - MinusLogProbMetric: 30.2617 - val_loss: 29.7967 - val_MinusLogProbMetric: 29.7967 - lr: 0.0010 - 55s/epoch - 281ms/step
Epoch 250/1000
2023-09-28 20:21:40.326 
Epoch 250/1000 
	 loss: 30.2417, MinusLogProbMetric: 30.2417, val_loss: 30.6864, val_MinusLogProbMetric: 30.6864

Epoch 250: val_loss did not improve from 29.79669
196/196 - 53s - loss: 30.2417 - MinusLogProbMetric: 30.2417 - val_loss: 30.6864 - val_MinusLogProbMetric: 30.6864 - lr: 0.0010 - 53s/epoch - 272ms/step
Epoch 251/1000
2023-09-28 20:22:34.733 
Epoch 251/1000 
	 loss: 30.1805, MinusLogProbMetric: 30.1805, val_loss: 30.2332, val_MinusLogProbMetric: 30.2332

Epoch 251: val_loss did not improve from 29.79669
196/196 - 54s - loss: 30.1805 - MinusLogProbMetric: 30.1805 - val_loss: 30.2332 - val_MinusLogProbMetric: 30.2332 - lr: 0.0010 - 54s/epoch - 278ms/step
Epoch 252/1000
2023-09-28 20:23:28.356 
Epoch 252/1000 
	 loss: 30.1694, MinusLogProbMetric: 30.1694, val_loss: 29.6484, val_MinusLogProbMetric: 29.6484

Epoch 252: val_loss improved from 29.79669 to 29.64843, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_323/weights/best_weights.h5
196/196 - 55s - loss: 30.1694 - MinusLogProbMetric: 30.1694 - val_loss: 29.6484 - val_MinusLogProbMetric: 29.6484 - lr: 0.0010 - 55s/epoch - 278ms/step
Epoch 253/1000
2023-09-28 20:24:21.038 
Epoch 253/1000 
	 loss: 30.4594, MinusLogProbMetric: 30.4594, val_loss: 30.3205, val_MinusLogProbMetric: 30.3205

Epoch 253: val_loss did not improve from 29.64843
196/196 - 52s - loss: 30.4594 - MinusLogProbMetric: 30.4594 - val_loss: 30.3205 - val_MinusLogProbMetric: 30.3205 - lr: 0.0010 - 52s/epoch - 264ms/step
Epoch 254/1000
2023-09-28 20:25:15.255 
Epoch 254/1000 
	 loss: 30.2519, MinusLogProbMetric: 30.2519, val_loss: 31.5911, val_MinusLogProbMetric: 31.5911

Epoch 254: val_loss did not improve from 29.64843
196/196 - 54s - loss: 30.2519 - MinusLogProbMetric: 30.2519 - val_loss: 31.5911 - val_MinusLogProbMetric: 31.5911 - lr: 0.0010 - 54s/epoch - 277ms/step
Epoch 255/1000
2023-09-28 20:26:10.455 
Epoch 255/1000 
	 loss: 30.1949, MinusLogProbMetric: 30.1949, val_loss: 31.5475, val_MinusLogProbMetric: 31.5475

Epoch 255: val_loss did not improve from 29.64843
196/196 - 55s - loss: 30.1949 - MinusLogProbMetric: 30.1949 - val_loss: 31.5475 - val_MinusLogProbMetric: 31.5475 - lr: 0.0010 - 55s/epoch - 282ms/step
Epoch 256/1000
2023-09-28 20:27:05.409 
Epoch 256/1000 
	 loss: 30.1510, MinusLogProbMetric: 30.1510, val_loss: 30.6394, val_MinusLogProbMetric: 30.6394

Epoch 256: val_loss did not improve from 29.64843
196/196 - 55s - loss: 30.1510 - MinusLogProbMetric: 30.1510 - val_loss: 30.6394 - val_MinusLogProbMetric: 30.6394 - lr: 0.0010 - 55s/epoch - 280ms/step
Epoch 257/1000
2023-09-28 20:28:00.049 
Epoch 257/1000 
	 loss: 30.3179, MinusLogProbMetric: 30.3179, val_loss: 30.4149, val_MinusLogProbMetric: 30.4149

Epoch 257: val_loss did not improve from 29.64843
196/196 - 55s - loss: 30.3179 - MinusLogProbMetric: 30.3179 - val_loss: 30.4149 - val_MinusLogProbMetric: 30.4149 - lr: 0.0010 - 55s/epoch - 279ms/step
Epoch 258/1000
2023-09-28 20:28:55.396 
Epoch 258/1000 
	 loss: 30.1091, MinusLogProbMetric: 30.1091, val_loss: 29.9084, val_MinusLogProbMetric: 29.9084

Epoch 258: val_loss did not improve from 29.64843
196/196 - 55s - loss: 30.1091 - MinusLogProbMetric: 30.1091 - val_loss: 29.9084 - val_MinusLogProbMetric: 29.9084 - lr: 0.0010 - 55s/epoch - 282ms/step
Epoch 259/1000
2023-09-28 20:29:50.203 
Epoch 259/1000 
	 loss: 30.2466, MinusLogProbMetric: 30.2466, val_loss: 31.3892, val_MinusLogProbMetric: 31.3892

Epoch 259: val_loss did not improve from 29.64843
196/196 - 55s - loss: 30.2466 - MinusLogProbMetric: 30.2466 - val_loss: 31.3892 - val_MinusLogProbMetric: 31.3892 - lr: 0.0010 - 55s/epoch - 280ms/step
Epoch 260/1000
2023-09-28 20:30:45.265 
Epoch 260/1000 
	 loss: 30.2640, MinusLogProbMetric: 30.2640, val_loss: 30.6055, val_MinusLogProbMetric: 30.6055

Epoch 260: val_loss did not improve from 29.64843
196/196 - 55s - loss: 30.2640 - MinusLogProbMetric: 30.2640 - val_loss: 30.6055 - val_MinusLogProbMetric: 30.6055 - lr: 0.0010 - 55s/epoch - 281ms/step
Epoch 261/1000
2023-09-28 20:31:40.010 
Epoch 261/1000 
	 loss: 30.1950, MinusLogProbMetric: 30.1950, val_loss: 30.4256, val_MinusLogProbMetric: 30.4256

Epoch 261: val_loss did not improve from 29.64843
196/196 - 55s - loss: 30.1950 - MinusLogProbMetric: 30.1950 - val_loss: 30.4256 - val_MinusLogProbMetric: 30.4256 - lr: 0.0010 - 55s/epoch - 279ms/step
Epoch 262/1000
2023-09-28 20:32:34.690 
Epoch 262/1000 
	 loss: 30.3604, MinusLogProbMetric: 30.3604, val_loss: 30.0941, val_MinusLogProbMetric: 30.0941

Epoch 262: val_loss did not improve from 29.64843
196/196 - 55s - loss: 30.3604 - MinusLogProbMetric: 30.3604 - val_loss: 30.0941 - val_MinusLogProbMetric: 30.0941 - lr: 0.0010 - 55s/epoch - 279ms/step
Epoch 263/1000
2023-09-28 20:33:29.637 
Epoch 263/1000 
	 loss: 30.2017, MinusLogProbMetric: 30.2017, val_loss: 30.3867, val_MinusLogProbMetric: 30.3867

Epoch 263: val_loss did not improve from 29.64843
196/196 - 55s - loss: 30.2017 - MinusLogProbMetric: 30.2017 - val_loss: 30.3867 - val_MinusLogProbMetric: 30.3867 - lr: 0.0010 - 55s/epoch - 280ms/step
Epoch 264/1000
2023-09-28 20:34:24.663 
Epoch 264/1000 
	 loss: 30.1557, MinusLogProbMetric: 30.1557, val_loss: 30.1748, val_MinusLogProbMetric: 30.1748

Epoch 264: val_loss did not improve from 29.64843
196/196 - 55s - loss: 30.1557 - MinusLogProbMetric: 30.1557 - val_loss: 30.1748 - val_MinusLogProbMetric: 30.1748 - lr: 0.0010 - 55s/epoch - 281ms/step
Epoch 265/1000
2023-09-28 20:35:19.197 
Epoch 265/1000 
	 loss: 30.1956, MinusLogProbMetric: 30.1956, val_loss: 30.5807, val_MinusLogProbMetric: 30.5807

Epoch 265: val_loss did not improve from 29.64843
196/196 - 55s - loss: 30.1956 - MinusLogProbMetric: 30.1956 - val_loss: 30.5807 - val_MinusLogProbMetric: 30.5807 - lr: 0.0010 - 55s/epoch - 278ms/step
Epoch 266/1000
2023-09-28 20:36:14.414 
Epoch 266/1000 
	 loss: 30.1593, MinusLogProbMetric: 30.1593, val_loss: 30.2281, val_MinusLogProbMetric: 30.2281

Epoch 266: val_loss did not improve from 29.64843
196/196 - 55s - loss: 30.1593 - MinusLogProbMetric: 30.1593 - val_loss: 30.2281 - val_MinusLogProbMetric: 30.2281 - lr: 0.0010 - 55s/epoch - 282ms/step
Epoch 267/1000
2023-09-28 20:37:09.074 
Epoch 267/1000 
	 loss: 30.0785, MinusLogProbMetric: 30.0785, val_loss: 29.8924, val_MinusLogProbMetric: 29.8924

Epoch 267: val_loss did not improve from 29.64843
196/196 - 55s - loss: 30.0785 - MinusLogProbMetric: 30.0785 - val_loss: 29.8924 - val_MinusLogProbMetric: 29.8924 - lr: 0.0010 - 55s/epoch - 279ms/step
Epoch 268/1000
2023-09-28 20:38:02.987 
Epoch 268/1000 
	 loss: 30.1737, MinusLogProbMetric: 30.1737, val_loss: 30.0415, val_MinusLogProbMetric: 30.0415

Epoch 268: val_loss did not improve from 29.64843
196/196 - 54s - loss: 30.1737 - MinusLogProbMetric: 30.1737 - val_loss: 30.0415 - val_MinusLogProbMetric: 30.0415 - lr: 0.0010 - 54s/epoch - 275ms/step
Epoch 269/1000
2023-09-28 20:38:57.428 
Epoch 269/1000 
	 loss: 30.2469, MinusLogProbMetric: 30.2469, val_loss: 30.0856, val_MinusLogProbMetric: 30.0856

Epoch 269: val_loss did not improve from 29.64843
196/196 - 54s - loss: 30.2469 - MinusLogProbMetric: 30.2469 - val_loss: 30.0856 - val_MinusLogProbMetric: 30.0856 - lr: 0.0010 - 54s/epoch - 278ms/step
Epoch 270/1000
2023-09-28 20:39:51.747 
Epoch 270/1000 
	 loss: 30.2223, MinusLogProbMetric: 30.2223, val_loss: 31.1230, val_MinusLogProbMetric: 31.1230

Epoch 270: val_loss did not improve from 29.64843
196/196 - 54s - loss: 30.2223 - MinusLogProbMetric: 30.2223 - val_loss: 31.1230 - val_MinusLogProbMetric: 31.1230 - lr: 0.0010 - 54s/epoch - 277ms/step
Epoch 271/1000
2023-09-28 20:40:46.318 
Epoch 271/1000 
	 loss: 30.0487, MinusLogProbMetric: 30.0487, val_loss: 30.0834, val_MinusLogProbMetric: 30.0834

Epoch 271: val_loss did not improve from 29.64843
196/196 - 55s - loss: 30.0487 - MinusLogProbMetric: 30.0487 - val_loss: 30.0834 - val_MinusLogProbMetric: 30.0834 - lr: 0.0010 - 55s/epoch - 278ms/step
Epoch 272/1000
2023-09-28 20:41:40.940 
Epoch 272/1000 
	 loss: 30.0464, MinusLogProbMetric: 30.0464, val_loss: 30.0330, val_MinusLogProbMetric: 30.0330

Epoch 272: val_loss did not improve from 29.64843
196/196 - 55s - loss: 30.0464 - MinusLogProbMetric: 30.0464 - val_loss: 30.0330 - val_MinusLogProbMetric: 30.0330 - lr: 0.0010 - 55s/epoch - 279ms/step
Epoch 273/1000
2023-09-28 20:42:33.676 
Epoch 273/1000 
	 loss: 30.0970, MinusLogProbMetric: 30.0970, val_loss: 30.3059, val_MinusLogProbMetric: 30.3059

Epoch 273: val_loss did not improve from 29.64843
196/196 - 53s - loss: 30.0970 - MinusLogProbMetric: 30.0970 - val_loss: 30.3059 - val_MinusLogProbMetric: 30.3059 - lr: 0.0010 - 53s/epoch - 269ms/step
Epoch 274/1000
2023-09-28 20:43:28.173 
Epoch 274/1000 
	 loss: 30.1084, MinusLogProbMetric: 30.1084, val_loss: 29.9935, val_MinusLogProbMetric: 29.9935

Epoch 274: val_loss did not improve from 29.64843
196/196 - 54s - loss: 30.1084 - MinusLogProbMetric: 30.1084 - val_loss: 29.9935 - val_MinusLogProbMetric: 29.9935 - lr: 0.0010 - 54s/epoch - 278ms/step
Epoch 275/1000
2023-09-28 20:44:20.987 
Epoch 275/1000 
	 loss: 30.2030, MinusLogProbMetric: 30.2030, val_loss: 31.2442, val_MinusLogProbMetric: 31.2442

Epoch 275: val_loss did not improve from 29.64843
196/196 - 53s - loss: 30.2030 - MinusLogProbMetric: 30.2030 - val_loss: 31.2442 - val_MinusLogProbMetric: 31.2442 - lr: 0.0010 - 53s/epoch - 269ms/step
Epoch 276/1000
2023-09-28 20:45:15.269 
Epoch 276/1000 
	 loss: 30.1225, MinusLogProbMetric: 30.1225, val_loss: 29.9030, val_MinusLogProbMetric: 29.9030

Epoch 276: val_loss did not improve from 29.64843
196/196 - 54s - loss: 30.1225 - MinusLogProbMetric: 30.1225 - val_loss: 29.9030 - val_MinusLogProbMetric: 29.9030 - lr: 0.0010 - 54s/epoch - 277ms/step
Epoch 277/1000
2023-09-28 20:46:09.165 
Epoch 277/1000 
	 loss: 29.9782, MinusLogProbMetric: 29.9782, val_loss: 29.8615, val_MinusLogProbMetric: 29.8615

Epoch 277: val_loss did not improve from 29.64843
196/196 - 54s - loss: 29.9782 - MinusLogProbMetric: 29.9782 - val_loss: 29.8615 - val_MinusLogProbMetric: 29.8615 - lr: 0.0010 - 54s/epoch - 275ms/step
Epoch 278/1000
2023-09-28 20:47:03.071 
Epoch 278/1000 
	 loss: 30.2809, MinusLogProbMetric: 30.2809, val_loss: 31.1557, val_MinusLogProbMetric: 31.1557

Epoch 278: val_loss did not improve from 29.64843
196/196 - 54s - loss: 30.2809 - MinusLogProbMetric: 30.2809 - val_loss: 31.1557 - val_MinusLogProbMetric: 31.1557 - lr: 0.0010 - 54s/epoch - 275ms/step
Epoch 279/1000
2023-09-28 20:47:57.170 
Epoch 279/1000 
	 loss: 30.0618, MinusLogProbMetric: 30.0618, val_loss: 30.2638, val_MinusLogProbMetric: 30.2638

Epoch 279: val_loss did not improve from 29.64843
196/196 - 54s - loss: 30.0618 - MinusLogProbMetric: 30.0618 - val_loss: 30.2638 - val_MinusLogProbMetric: 30.2638 - lr: 0.0010 - 54s/epoch - 276ms/step
Epoch 280/1000
2023-09-28 20:48:51.306 
Epoch 280/1000 
	 loss: 30.0468, MinusLogProbMetric: 30.0468, val_loss: 30.0653, val_MinusLogProbMetric: 30.0653

Epoch 280: val_loss did not improve from 29.64843
196/196 - 54s - loss: 30.0468 - MinusLogProbMetric: 30.0468 - val_loss: 30.0653 - val_MinusLogProbMetric: 30.0653 - lr: 0.0010 - 54s/epoch - 276ms/step
Epoch 281/1000
2023-09-28 20:49:44.767 
Epoch 281/1000 
	 loss: 30.1886, MinusLogProbMetric: 30.1886, val_loss: 29.9842, val_MinusLogProbMetric: 29.9842

Epoch 281: val_loss did not improve from 29.64843
196/196 - 53s - loss: 30.1886 - MinusLogProbMetric: 30.1886 - val_loss: 29.9842 - val_MinusLogProbMetric: 29.9842 - lr: 0.0010 - 53s/epoch - 273ms/step
Epoch 282/1000
2023-09-28 20:50:38.914 
Epoch 282/1000 
	 loss: 30.0600, MinusLogProbMetric: 30.0600, val_loss: 29.9457, val_MinusLogProbMetric: 29.9457

Epoch 282: val_loss did not improve from 29.64843
196/196 - 54s - loss: 30.0600 - MinusLogProbMetric: 30.0600 - val_loss: 29.9457 - val_MinusLogProbMetric: 29.9457 - lr: 0.0010 - 54s/epoch - 276ms/step
Epoch 283/1000
2023-09-28 20:51:32.602 
Epoch 283/1000 
	 loss: 30.0319, MinusLogProbMetric: 30.0319, val_loss: 30.0944, val_MinusLogProbMetric: 30.0944

Epoch 283: val_loss did not improve from 29.64843
196/196 - 54s - loss: 30.0319 - MinusLogProbMetric: 30.0319 - val_loss: 30.0944 - val_MinusLogProbMetric: 30.0944 - lr: 0.0010 - 54s/epoch - 274ms/step
Epoch 284/1000
2023-09-28 20:52:27.227 
Epoch 284/1000 
	 loss: 30.0196, MinusLogProbMetric: 30.0196, val_loss: 29.7000, val_MinusLogProbMetric: 29.7000

Epoch 284: val_loss did not improve from 29.64843
196/196 - 55s - loss: 30.0196 - MinusLogProbMetric: 30.0196 - val_loss: 29.7000 - val_MinusLogProbMetric: 29.7000 - lr: 0.0010 - 55s/epoch - 279ms/step
Epoch 285/1000
2023-09-28 20:53:21.858 
Epoch 285/1000 
	 loss: 29.9715, MinusLogProbMetric: 29.9715, val_loss: 30.3300, val_MinusLogProbMetric: 30.3300

Epoch 285: val_loss did not improve from 29.64843
196/196 - 55s - loss: 29.9715 - MinusLogProbMetric: 29.9715 - val_loss: 30.3300 - val_MinusLogProbMetric: 30.3300 - lr: 0.0010 - 55s/epoch - 279ms/step
Epoch 286/1000
2023-09-28 20:54:16.098 
Epoch 286/1000 
	 loss: 30.0114, MinusLogProbMetric: 30.0114, val_loss: 29.7416, val_MinusLogProbMetric: 29.7416

Epoch 286: val_loss did not improve from 29.64843
196/196 - 54s - loss: 30.0114 - MinusLogProbMetric: 30.0114 - val_loss: 29.7416 - val_MinusLogProbMetric: 29.7416 - lr: 0.0010 - 54s/epoch - 277ms/step
Epoch 287/1000
2023-09-28 20:55:10.264 
Epoch 287/1000 
	 loss: 30.0085, MinusLogProbMetric: 30.0085, val_loss: 30.6154, val_MinusLogProbMetric: 30.6154

Epoch 287: val_loss did not improve from 29.64843
196/196 - 54s - loss: 30.0085 - MinusLogProbMetric: 30.0085 - val_loss: 30.6154 - val_MinusLogProbMetric: 30.6154 - lr: 0.0010 - 54s/epoch - 276ms/step
Epoch 288/1000
2023-09-28 20:56:03.576 
Epoch 288/1000 
	 loss: 30.1730, MinusLogProbMetric: 30.1730, val_loss: 29.7408, val_MinusLogProbMetric: 29.7408

Epoch 288: val_loss did not improve from 29.64843
196/196 - 53s - loss: 30.1730 - MinusLogProbMetric: 30.1730 - val_loss: 29.7408 - val_MinusLogProbMetric: 29.7408 - lr: 0.0010 - 53s/epoch - 272ms/step
Epoch 289/1000
2023-09-28 20:56:55.637 
Epoch 289/1000 
	 loss: 29.8481, MinusLogProbMetric: 29.8481, val_loss: 29.8928, val_MinusLogProbMetric: 29.8928

Epoch 289: val_loss did not improve from 29.64843
196/196 - 52s - loss: 29.8481 - MinusLogProbMetric: 29.8481 - val_loss: 29.8928 - val_MinusLogProbMetric: 29.8928 - lr: 0.0010 - 52s/epoch - 266ms/step
Epoch 290/1000
2023-09-28 20:57:50.545 
Epoch 290/1000 
	 loss: 30.1093, MinusLogProbMetric: 30.1093, val_loss: 29.8077, val_MinusLogProbMetric: 29.8077

Epoch 290: val_loss did not improve from 29.64843
196/196 - 55s - loss: 30.1093 - MinusLogProbMetric: 30.1093 - val_loss: 29.8077 - val_MinusLogProbMetric: 29.8077 - lr: 0.0010 - 55s/epoch - 280ms/step
Epoch 291/1000
2023-09-28 20:58:42.613 
Epoch 291/1000 
	 loss: 30.0080, MinusLogProbMetric: 30.0080, val_loss: 30.5057, val_MinusLogProbMetric: 30.5057

Epoch 291: val_loss did not improve from 29.64843
196/196 - 52s - loss: 30.0080 - MinusLogProbMetric: 30.0080 - val_loss: 30.5057 - val_MinusLogProbMetric: 30.5057 - lr: 0.0010 - 52s/epoch - 266ms/step
Epoch 292/1000
2023-09-28 20:59:37.455 
Epoch 292/1000 
	 loss: 29.9889, MinusLogProbMetric: 29.9889, val_loss: 29.9088, val_MinusLogProbMetric: 29.9088

Epoch 292: val_loss did not improve from 29.64843
196/196 - 55s - loss: 29.9889 - MinusLogProbMetric: 29.9889 - val_loss: 29.9088 - val_MinusLogProbMetric: 29.9088 - lr: 0.0010 - 55s/epoch - 280ms/step
Epoch 293/1000
2023-09-28 21:00:32.486 
Epoch 293/1000 
	 loss: 29.9489, MinusLogProbMetric: 29.9489, val_loss: 30.7405, val_MinusLogProbMetric: 30.7405

Epoch 293: val_loss did not improve from 29.64843
196/196 - 55s - loss: 29.9489 - MinusLogProbMetric: 29.9489 - val_loss: 30.7405 - val_MinusLogProbMetric: 30.7405 - lr: 0.0010 - 55s/epoch - 281ms/step
Epoch 294/1000
2023-09-28 21:01:34.537 
Epoch 294/1000 
	 loss: 29.9177, MinusLogProbMetric: 29.9177, val_loss: 30.4797, val_MinusLogProbMetric: 30.4797

Epoch 294: val_loss did not improve from 29.64843
196/196 - 62s - loss: 29.9177 - MinusLogProbMetric: 29.9177 - val_loss: 30.4797 - val_MinusLogProbMetric: 30.4797 - lr: 0.0010 - 62s/epoch - 317ms/step
Epoch 295/1000
2023-09-28 21:02:35.651 
Epoch 295/1000 
	 loss: 29.8846, MinusLogProbMetric: 29.8846, val_loss: 29.9549, val_MinusLogProbMetric: 29.9549

Epoch 295: val_loss did not improve from 29.64843
196/196 - 61s - loss: 29.8846 - MinusLogProbMetric: 29.8846 - val_loss: 29.9549 - val_MinusLogProbMetric: 29.9549 - lr: 0.0010 - 61s/epoch - 312ms/step
Epoch 296/1000
2023-09-28 21:03:29.469 
Epoch 296/1000 
	 loss: 29.9404, MinusLogProbMetric: 29.9404, val_loss: 29.9079, val_MinusLogProbMetric: 29.9079

Epoch 296: val_loss did not improve from 29.64843
196/196 - 54s - loss: 29.9404 - MinusLogProbMetric: 29.9404 - val_loss: 29.9079 - val_MinusLogProbMetric: 29.9079 - lr: 0.0010 - 54s/epoch - 275ms/step
Epoch 297/1000
2023-09-28 21:04:16.939 
Epoch 297/1000 
	 loss: 29.9029, MinusLogProbMetric: 29.9029, val_loss: 29.4237, val_MinusLogProbMetric: 29.4237

Epoch 297: val_loss improved from 29.64843 to 29.42371, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_323/weights/best_weights.h5
196/196 - 48s - loss: 29.9029 - MinusLogProbMetric: 29.9029 - val_loss: 29.4237 - val_MinusLogProbMetric: 29.4237 - lr: 0.0010 - 48s/epoch - 246ms/step
Epoch 298/1000
2023-09-28 21:05:08.490 
Epoch 298/1000 
	 loss: 30.2443, MinusLogProbMetric: 30.2443, val_loss: 30.0498, val_MinusLogProbMetric: 30.0498

Epoch 298: val_loss did not improve from 29.42371
196/196 - 51s - loss: 30.2443 - MinusLogProbMetric: 30.2443 - val_loss: 30.0498 - val_MinusLogProbMetric: 30.0498 - lr: 0.0010 - 51s/epoch - 259ms/step
Epoch 299/1000
2023-09-28 21:06:01.100 
Epoch 299/1000 
	 loss: 30.0300, MinusLogProbMetric: 30.0300, val_loss: 30.6147, val_MinusLogProbMetric: 30.6147

Epoch 299: val_loss did not improve from 29.42371
196/196 - 53s - loss: 30.0300 - MinusLogProbMetric: 30.0300 - val_loss: 30.6147 - val_MinusLogProbMetric: 30.6147 - lr: 0.0010 - 53s/epoch - 268ms/step
Epoch 300/1000
2023-09-28 21:06:49.342 
Epoch 300/1000 
	 loss: 29.8865, MinusLogProbMetric: 29.8865, val_loss: 30.0148, val_MinusLogProbMetric: 30.0148

Epoch 300: val_loss did not improve from 29.42371
196/196 - 48s - loss: 29.8865 - MinusLogProbMetric: 29.8865 - val_loss: 30.0148 - val_MinusLogProbMetric: 30.0148 - lr: 0.0010 - 48s/epoch - 246ms/step
Epoch 301/1000
2023-09-28 21:07:37.670 
Epoch 301/1000 
	 loss: 29.8105, MinusLogProbMetric: 29.8105, val_loss: 30.3225, val_MinusLogProbMetric: 30.3225

Epoch 301: val_loss did not improve from 29.42371
196/196 - 48s - loss: 29.8105 - MinusLogProbMetric: 29.8105 - val_loss: 30.3225 - val_MinusLogProbMetric: 30.3225 - lr: 0.0010 - 48s/epoch - 247ms/step
Epoch 302/1000
2023-09-28 21:08:27.227 
Epoch 302/1000 
	 loss: 29.9563, MinusLogProbMetric: 29.9563, val_loss: 29.4332, val_MinusLogProbMetric: 29.4332

Epoch 302: val_loss did not improve from 29.42371
196/196 - 50s - loss: 29.9563 - MinusLogProbMetric: 29.9563 - val_loss: 29.4332 - val_MinusLogProbMetric: 29.4332 - lr: 0.0010 - 50s/epoch - 253ms/step
Epoch 303/1000
2023-09-28 21:09:08.928 
Epoch 303/1000 
	 loss: 29.9268, MinusLogProbMetric: 29.9268, val_loss: 30.1274, val_MinusLogProbMetric: 30.1274

Epoch 303: val_loss did not improve from 29.42371
196/196 - 42s - loss: 29.9268 - MinusLogProbMetric: 29.9268 - val_loss: 30.1274 - val_MinusLogProbMetric: 30.1274 - lr: 0.0010 - 42s/epoch - 213ms/step
Epoch 304/1000
2023-09-28 21:09:54.931 
Epoch 304/1000 
	 loss: 29.9499, MinusLogProbMetric: 29.9499, val_loss: 31.2519, val_MinusLogProbMetric: 31.2519

Epoch 304: val_loss did not improve from 29.42371
196/196 - 46s - loss: 29.9499 - MinusLogProbMetric: 29.9499 - val_loss: 31.2519 - val_MinusLogProbMetric: 31.2519 - lr: 0.0010 - 46s/epoch - 235ms/step
Epoch 305/1000
2023-09-28 21:10:42.205 
Epoch 305/1000 
	 loss: 29.9427, MinusLogProbMetric: 29.9427, val_loss: 30.1212, val_MinusLogProbMetric: 30.1212

Epoch 305: val_loss did not improve from 29.42371
196/196 - 47s - loss: 29.9427 - MinusLogProbMetric: 29.9427 - val_loss: 30.1212 - val_MinusLogProbMetric: 30.1212 - lr: 0.0010 - 47s/epoch - 241ms/step
Epoch 306/1000
2023-09-28 21:11:34.191 
Epoch 306/1000 
	 loss: 29.8193, MinusLogProbMetric: 29.8193, val_loss: 29.7138, val_MinusLogProbMetric: 29.7138

Epoch 306: val_loss did not improve from 29.42371
196/196 - 52s - loss: 29.8193 - MinusLogProbMetric: 29.8193 - val_loss: 29.7138 - val_MinusLogProbMetric: 29.7138 - lr: 0.0010 - 52s/epoch - 265ms/step
Epoch 307/1000
2023-09-28 21:12:25.724 
Epoch 307/1000 
	 loss: 29.8961, MinusLogProbMetric: 29.8961, val_loss: 31.3743, val_MinusLogProbMetric: 31.3743

Epoch 307: val_loss did not improve from 29.42371
196/196 - 52s - loss: 29.8961 - MinusLogProbMetric: 29.8961 - val_loss: 31.3743 - val_MinusLogProbMetric: 31.3743 - lr: 0.0010 - 52s/epoch - 263ms/step
Epoch 308/1000
2023-09-28 21:13:13.060 
Epoch 308/1000 
	 loss: 29.9732, MinusLogProbMetric: 29.9732, val_loss: 29.8561, val_MinusLogProbMetric: 29.8561

Epoch 308: val_loss did not improve from 29.42371
196/196 - 47s - loss: 29.9732 - MinusLogProbMetric: 29.9732 - val_loss: 29.8561 - val_MinusLogProbMetric: 29.8561 - lr: 0.0010 - 47s/epoch - 241ms/step
Epoch 309/1000
2023-09-28 21:14:00.419 
Epoch 309/1000 
	 loss: 29.9259, MinusLogProbMetric: 29.9259, val_loss: 30.0950, val_MinusLogProbMetric: 30.0950

Epoch 309: val_loss did not improve from 29.42371
196/196 - 47s - loss: 29.9259 - MinusLogProbMetric: 29.9259 - val_loss: 30.0950 - val_MinusLogProbMetric: 30.0950 - lr: 0.0010 - 47s/epoch - 242ms/step
Epoch 310/1000
2023-09-28 21:14:51.773 
Epoch 310/1000 
	 loss: 29.8600, MinusLogProbMetric: 29.8600, val_loss: 29.7571, val_MinusLogProbMetric: 29.7571

Epoch 310: val_loss did not improve from 29.42371
196/196 - 51s - loss: 29.8600 - MinusLogProbMetric: 29.8600 - val_loss: 29.7571 - val_MinusLogProbMetric: 29.7571 - lr: 0.0010 - 51s/epoch - 262ms/step
Epoch 311/1000
2023-09-28 21:15:42.096 
Epoch 311/1000 
	 loss: 30.0357, MinusLogProbMetric: 30.0357, val_loss: 30.5290, val_MinusLogProbMetric: 30.5290

Epoch 311: val_loss did not improve from 29.42371
196/196 - 50s - loss: 30.0357 - MinusLogProbMetric: 30.0357 - val_loss: 30.5290 - val_MinusLogProbMetric: 30.5290 - lr: 0.0010 - 50s/epoch - 257ms/step
Epoch 312/1000
2023-09-28 21:16:30.145 
Epoch 312/1000 
	 loss: 29.8658, MinusLogProbMetric: 29.8658, val_loss: 30.2495, val_MinusLogProbMetric: 30.2495

Epoch 312: val_loss did not improve from 29.42371
196/196 - 48s - loss: 29.8658 - MinusLogProbMetric: 29.8658 - val_loss: 30.2495 - val_MinusLogProbMetric: 30.2495 - lr: 0.0010 - 48s/epoch - 245ms/step
Epoch 313/1000
2023-09-28 21:17:18.082 
Epoch 313/1000 
	 loss: 29.7366, MinusLogProbMetric: 29.7366, val_loss: 30.7914, val_MinusLogProbMetric: 30.7914

Epoch 313: val_loss did not improve from 29.42371
196/196 - 48s - loss: 29.7366 - MinusLogProbMetric: 29.7366 - val_loss: 30.7914 - val_MinusLogProbMetric: 30.7914 - lr: 0.0010 - 48s/epoch - 245ms/step
Epoch 314/1000
2023-09-28 21:18:09.851 
Epoch 314/1000 
	 loss: 29.7166, MinusLogProbMetric: 29.7166, val_loss: 30.4947, val_MinusLogProbMetric: 30.4947

Epoch 314: val_loss did not improve from 29.42371
196/196 - 52s - loss: 29.7166 - MinusLogProbMetric: 29.7166 - val_loss: 30.4947 - val_MinusLogProbMetric: 30.4947 - lr: 0.0010 - 52s/epoch - 264ms/step
Epoch 315/1000
2023-09-28 21:18:58.931 
Epoch 315/1000 
	 loss: 29.8822, MinusLogProbMetric: 29.8822, val_loss: 30.2433, val_MinusLogProbMetric: 30.2433

Epoch 315: val_loss did not improve from 29.42371
196/196 - 49s - loss: 29.8822 - MinusLogProbMetric: 29.8822 - val_loss: 30.2433 - val_MinusLogProbMetric: 30.2433 - lr: 0.0010 - 49s/epoch - 250ms/step
Epoch 316/1000
2023-09-28 21:19:46.379 
Epoch 316/1000 
	 loss: 29.7192, MinusLogProbMetric: 29.7192, val_loss: 30.2991, val_MinusLogProbMetric: 30.2991

Epoch 316: val_loss did not improve from 29.42371
196/196 - 47s - loss: 29.7192 - MinusLogProbMetric: 29.7192 - val_loss: 30.2991 - val_MinusLogProbMetric: 30.2991 - lr: 0.0010 - 47s/epoch - 242ms/step
Epoch 317/1000
2023-09-28 21:20:34.282 
Epoch 317/1000 
	 loss: 29.8238, MinusLogProbMetric: 29.8238, val_loss: 30.4160, val_MinusLogProbMetric: 30.4160

Epoch 317: val_loss did not improve from 29.42371
196/196 - 48s - loss: 29.8238 - MinusLogProbMetric: 29.8238 - val_loss: 30.4160 - val_MinusLogProbMetric: 30.4160 - lr: 0.0010 - 48s/epoch - 244ms/step
Epoch 318/1000
2023-09-28 21:21:26.974 
Epoch 318/1000 
	 loss: 29.8583, MinusLogProbMetric: 29.8583, val_loss: 29.6822, val_MinusLogProbMetric: 29.6822

Epoch 318: val_loss did not improve from 29.42371
196/196 - 53s - loss: 29.8583 - MinusLogProbMetric: 29.8583 - val_loss: 29.6822 - val_MinusLogProbMetric: 29.6822 - lr: 0.0010 - 53s/epoch - 269ms/step
Epoch 319/1000
2023-09-28 21:22:15.461 
Epoch 319/1000 
	 loss: 29.7058, MinusLogProbMetric: 29.7058, val_loss: 31.0192, val_MinusLogProbMetric: 31.0192

Epoch 319: val_loss did not improve from 29.42371
196/196 - 48s - loss: 29.7058 - MinusLogProbMetric: 29.7058 - val_loss: 31.0192 - val_MinusLogProbMetric: 31.0192 - lr: 0.0010 - 48s/epoch - 247ms/step
Epoch 320/1000
2023-09-28 21:23:02.844 
Epoch 320/1000 
	 loss: 29.9040, MinusLogProbMetric: 29.9040, val_loss: 31.2513, val_MinusLogProbMetric: 31.2513

Epoch 320: val_loss did not improve from 29.42371
196/196 - 47s - loss: 29.9040 - MinusLogProbMetric: 29.9040 - val_loss: 31.2513 - val_MinusLogProbMetric: 31.2513 - lr: 0.0010 - 47s/epoch - 242ms/step
Epoch 321/1000
2023-09-28 21:23:51.991 
Epoch 321/1000 
	 loss: 29.7487, MinusLogProbMetric: 29.7487, val_loss: 29.7335, val_MinusLogProbMetric: 29.7335

Epoch 321: val_loss did not improve from 29.42371
196/196 - 49s - loss: 29.7487 - MinusLogProbMetric: 29.7487 - val_loss: 29.7335 - val_MinusLogProbMetric: 29.7335 - lr: 0.0010 - 49s/epoch - 251ms/step
Epoch 322/1000
2023-09-28 21:24:43.173 
Epoch 322/1000 
	 loss: 30.0036, MinusLogProbMetric: 30.0036, val_loss: 30.2017, val_MinusLogProbMetric: 30.2017

Epoch 322: val_loss did not improve from 29.42371
196/196 - 51s - loss: 30.0036 - MinusLogProbMetric: 30.0036 - val_loss: 30.2017 - val_MinusLogProbMetric: 30.2017 - lr: 0.0010 - 51s/epoch - 261ms/step
Epoch 323/1000
2023-09-28 21:25:30.704 
Epoch 323/1000 
	 loss: 29.7366, MinusLogProbMetric: 29.7366, val_loss: 30.3143, val_MinusLogProbMetric: 30.3143

Epoch 323: val_loss did not improve from 29.42371
196/196 - 48s - loss: 29.7366 - MinusLogProbMetric: 29.7366 - val_loss: 30.3143 - val_MinusLogProbMetric: 30.3143 - lr: 0.0010 - 48s/epoch - 242ms/step
Epoch 324/1000
2023-09-28 21:26:18.322 
Epoch 324/1000 
	 loss: 29.7859, MinusLogProbMetric: 29.7859, val_loss: 30.5569, val_MinusLogProbMetric: 30.5569

Epoch 324: val_loss did not improve from 29.42371
196/196 - 48s - loss: 29.7859 - MinusLogProbMetric: 29.7859 - val_loss: 30.5569 - val_MinusLogProbMetric: 30.5569 - lr: 0.0010 - 48s/epoch - 243ms/step
Epoch 325/1000
2023-09-28 21:27:09.763 
Epoch 325/1000 
	 loss: 29.8992, MinusLogProbMetric: 29.8992, val_loss: 29.6905, val_MinusLogProbMetric: 29.6905

Epoch 325: val_loss did not improve from 29.42371
196/196 - 51s - loss: 29.8992 - MinusLogProbMetric: 29.8992 - val_loss: 29.6905 - val_MinusLogProbMetric: 29.6905 - lr: 0.0010 - 51s/epoch - 262ms/step
Epoch 326/1000
2023-09-28 21:28:00.710 
Epoch 326/1000 
	 loss: 29.8521, MinusLogProbMetric: 29.8521, val_loss: 29.4578, val_MinusLogProbMetric: 29.4578

Epoch 326: val_loss did not improve from 29.42371
196/196 - 51s - loss: 29.8521 - MinusLogProbMetric: 29.8521 - val_loss: 29.4578 - val_MinusLogProbMetric: 29.4578 - lr: 0.0010 - 51s/epoch - 260ms/step
Epoch 327/1000
2023-09-28 21:28:48.215 
Epoch 327/1000 
	 loss: 29.6686, MinusLogProbMetric: 29.6686, val_loss: 29.8060, val_MinusLogProbMetric: 29.8060

Epoch 327: val_loss did not improve from 29.42371
196/196 - 48s - loss: 29.6686 - MinusLogProbMetric: 29.6686 - val_loss: 29.8060 - val_MinusLogProbMetric: 29.8060 - lr: 0.0010 - 48s/epoch - 242ms/step
Epoch 328/1000
2023-09-28 21:29:35.833 
Epoch 328/1000 
	 loss: 29.7530, MinusLogProbMetric: 29.7530, val_loss: 29.7864, val_MinusLogProbMetric: 29.7864

Epoch 328: val_loss did not improve from 29.42371
196/196 - 48s - loss: 29.7530 - MinusLogProbMetric: 29.7530 - val_loss: 29.7864 - val_MinusLogProbMetric: 29.7864 - lr: 0.0010 - 48s/epoch - 243ms/step
Epoch 329/1000
2023-09-28 21:30:24.792 
Epoch 329/1000 
	 loss: 29.7143, MinusLogProbMetric: 29.7143, val_loss: 30.7737, val_MinusLogProbMetric: 30.7737

Epoch 329: val_loss did not improve from 29.42371
196/196 - 49s - loss: 29.7143 - MinusLogProbMetric: 29.7143 - val_loss: 30.7737 - val_MinusLogProbMetric: 30.7737 - lr: 0.0010 - 49s/epoch - 250ms/step
Epoch 330/1000
2023-09-28 21:31:14.690 
Epoch 330/1000 
	 loss: 30.0016, MinusLogProbMetric: 30.0016, val_loss: 29.8673, val_MinusLogProbMetric: 29.8673

Epoch 330: val_loss did not improve from 29.42371
196/196 - 50s - loss: 30.0016 - MinusLogProbMetric: 30.0016 - val_loss: 29.8673 - val_MinusLogProbMetric: 29.8673 - lr: 0.0010 - 50s/epoch - 255ms/step
Epoch 331/1000
2023-09-28 21:32:01.569 
Epoch 331/1000 
	 loss: 29.7035, MinusLogProbMetric: 29.7035, val_loss: 30.3022, val_MinusLogProbMetric: 30.3022

Epoch 331: val_loss did not improve from 29.42371
196/196 - 47s - loss: 29.7035 - MinusLogProbMetric: 29.7035 - val_loss: 30.3022 - val_MinusLogProbMetric: 30.3022 - lr: 0.0010 - 47s/epoch - 239ms/step
Epoch 332/1000
2023-09-28 21:32:49.388 
Epoch 332/1000 
	 loss: 29.7707, MinusLogProbMetric: 29.7707, val_loss: 29.6881, val_MinusLogProbMetric: 29.6881

Epoch 332: val_loss did not improve from 29.42371
196/196 - 48s - loss: 29.7707 - MinusLogProbMetric: 29.7707 - val_loss: 29.6881 - val_MinusLogProbMetric: 29.6881 - lr: 0.0010 - 48s/epoch - 244ms/step
Epoch 333/1000
2023-09-28 21:33:37.179 
Epoch 333/1000 
	 loss: 29.7537, MinusLogProbMetric: 29.7537, val_loss: 30.3714, val_MinusLogProbMetric: 30.3714

Epoch 333: val_loss did not improve from 29.42371
196/196 - 48s - loss: 29.7537 - MinusLogProbMetric: 29.7537 - val_loss: 30.3714 - val_MinusLogProbMetric: 30.3714 - lr: 0.0010 - 48s/epoch - 244ms/step
Epoch 334/1000
2023-09-28 21:34:25.461 
Epoch 334/1000 
	 loss: 29.8082, MinusLogProbMetric: 29.8082, val_loss: 30.0224, val_MinusLogProbMetric: 30.0224

Epoch 334: val_loss did not improve from 29.42371
196/196 - 48s - loss: 29.8082 - MinusLogProbMetric: 29.8082 - val_loss: 30.0224 - val_MinusLogProbMetric: 30.0224 - lr: 0.0010 - 48s/epoch - 246ms/step
Epoch 335/1000
2023-09-28 21:35:12.462 
Epoch 335/1000 
	 loss: 29.6529, MinusLogProbMetric: 29.6529, val_loss: 29.9445, val_MinusLogProbMetric: 29.9445

Epoch 335: val_loss did not improve from 29.42371
196/196 - 47s - loss: 29.6529 - MinusLogProbMetric: 29.6529 - val_loss: 29.9445 - val_MinusLogProbMetric: 29.9445 - lr: 0.0010 - 47s/epoch - 240ms/step
Epoch 336/1000
2023-09-28 21:36:00.093 
Epoch 336/1000 
	 loss: 29.6399, MinusLogProbMetric: 29.6399, val_loss: 30.6708, val_MinusLogProbMetric: 30.6708

Epoch 336: val_loss did not improve from 29.42371
196/196 - 48s - loss: 29.6399 - MinusLogProbMetric: 29.6399 - val_loss: 30.6708 - val_MinusLogProbMetric: 30.6708 - lr: 0.0010 - 48s/epoch - 243ms/step
Epoch 337/1000
2023-09-28 21:36:48.254 
Epoch 337/1000 
	 loss: 29.7254, MinusLogProbMetric: 29.7254, val_loss: 29.6742, val_MinusLogProbMetric: 29.6742

Epoch 337: val_loss did not improve from 29.42371
196/196 - 48s - loss: 29.7254 - MinusLogProbMetric: 29.7254 - val_loss: 29.6742 - val_MinusLogProbMetric: 29.6742 - lr: 0.0010 - 48s/epoch - 246ms/step
Epoch 338/1000
2023-09-28 21:37:36.135 
Epoch 338/1000 
	 loss: 29.8122, MinusLogProbMetric: 29.8122, val_loss: 29.4717, val_MinusLogProbMetric: 29.4717

Epoch 338: val_loss did not improve from 29.42371
196/196 - 48s - loss: 29.8122 - MinusLogProbMetric: 29.8122 - val_loss: 29.4717 - val_MinusLogProbMetric: 29.4717 - lr: 0.0010 - 48s/epoch - 244ms/step
Epoch 339/1000
2023-09-28 21:38:23.424 
Epoch 339/1000 
	 loss: 29.6035, MinusLogProbMetric: 29.6035, val_loss: 31.0592, val_MinusLogProbMetric: 31.0592

Epoch 339: val_loss did not improve from 29.42371
196/196 - 47s - loss: 29.6035 - MinusLogProbMetric: 29.6035 - val_loss: 31.0592 - val_MinusLogProbMetric: 31.0592 - lr: 0.0010 - 47s/epoch - 241ms/step
Epoch 340/1000
2023-09-28 21:39:11.887 
Epoch 340/1000 
	 loss: 30.3041, MinusLogProbMetric: 30.3041, val_loss: 29.7594, val_MinusLogProbMetric: 29.7594

Epoch 340: val_loss did not improve from 29.42371
196/196 - 48s - loss: 30.3041 - MinusLogProbMetric: 30.3041 - val_loss: 29.7594 - val_MinusLogProbMetric: 29.7594 - lr: 0.0010 - 48s/epoch - 247ms/step
Epoch 341/1000
2023-09-28 21:40:01.354 
Epoch 341/1000 
	 loss: 29.7533, MinusLogProbMetric: 29.7533, val_loss: 31.0755, val_MinusLogProbMetric: 31.0755

Epoch 341: val_loss did not improve from 29.42371
196/196 - 49s - loss: 29.7533 - MinusLogProbMetric: 29.7533 - val_loss: 31.0755 - val_MinusLogProbMetric: 31.0755 - lr: 0.0010 - 49s/epoch - 252ms/step
Epoch 342/1000
2023-09-28 21:40:48.121 
Epoch 342/1000 
	 loss: 29.7386, MinusLogProbMetric: 29.7386, val_loss: 29.6476, val_MinusLogProbMetric: 29.6476

Epoch 342: val_loss did not improve from 29.42371
196/196 - 47s - loss: 29.7386 - MinusLogProbMetric: 29.7386 - val_loss: 29.6476 - val_MinusLogProbMetric: 29.6476 - lr: 0.0010 - 47s/epoch - 239ms/step
Epoch 343/1000
2023-09-28 21:41:35.272 
Epoch 343/1000 
	 loss: 29.7659, MinusLogProbMetric: 29.7659, val_loss: 31.4003, val_MinusLogProbMetric: 31.4003

Epoch 343: val_loss did not improve from 29.42371
196/196 - 47s - loss: 29.7659 - MinusLogProbMetric: 29.7659 - val_loss: 31.4003 - val_MinusLogProbMetric: 31.4003 - lr: 0.0010 - 47s/epoch - 241ms/step
Epoch 344/1000
2023-09-28 21:42:22.235 
Epoch 344/1000 
	 loss: 29.6372, MinusLogProbMetric: 29.6372, val_loss: 29.4412, val_MinusLogProbMetric: 29.4412

Epoch 344: val_loss did not improve from 29.42371
196/196 - 47s - loss: 29.6372 - MinusLogProbMetric: 29.6372 - val_loss: 29.4412 - val_MinusLogProbMetric: 29.4412 - lr: 0.0010 - 47s/epoch - 240ms/step
Epoch 345/1000
2023-09-28 21:43:11.182 
Epoch 345/1000 
	 loss: 29.5938, MinusLogProbMetric: 29.5938, val_loss: 29.5411, val_MinusLogProbMetric: 29.5411

Epoch 345: val_loss did not improve from 29.42371
196/196 - 49s - loss: 29.5938 - MinusLogProbMetric: 29.5938 - val_loss: 29.5411 - val_MinusLogProbMetric: 29.5411 - lr: 0.0010 - 49s/epoch - 250ms/step
Epoch 346/1000
2023-09-28 21:43:58.433 
Epoch 346/1000 
	 loss: 29.7411, MinusLogProbMetric: 29.7411, val_loss: 30.6803, val_MinusLogProbMetric: 30.6803

Epoch 346: val_loss did not improve from 29.42371
196/196 - 47s - loss: 29.7411 - MinusLogProbMetric: 29.7411 - val_loss: 30.6803 - val_MinusLogProbMetric: 30.6803 - lr: 0.0010 - 47s/epoch - 241ms/step
Epoch 347/1000
2023-09-28 21:44:45.797 
Epoch 347/1000 
	 loss: 29.5830, MinusLogProbMetric: 29.5830, val_loss: 29.6865, val_MinusLogProbMetric: 29.6865

Epoch 347: val_loss did not improve from 29.42371
196/196 - 47s - loss: 29.5830 - MinusLogProbMetric: 29.5830 - val_loss: 29.6865 - val_MinusLogProbMetric: 29.6865 - lr: 0.0010 - 47s/epoch - 242ms/step
Epoch 348/1000
2023-09-28 21:45:33.967 
Epoch 348/1000 
	 loss: 28.7073, MinusLogProbMetric: 28.7073, val_loss: 29.0100, val_MinusLogProbMetric: 29.0100

Epoch 348: val_loss improved from 29.42371 to 29.00998, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_323/weights/best_weights.h5
196/196 - 49s - loss: 28.7073 - MinusLogProbMetric: 28.7073 - val_loss: 29.0100 - val_MinusLogProbMetric: 29.0100 - lr: 5.0000e-04 - 49s/epoch - 250ms/step
Epoch 349/1000
2023-09-28 21:46:28.341 
Epoch 349/1000 
	 loss: 28.7194, MinusLogProbMetric: 28.7194, val_loss: 28.7330, val_MinusLogProbMetric: 28.7330

Epoch 349: val_loss improved from 29.00998 to 28.73296, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_323/weights/best_weights.h5
196/196 - 54s - loss: 28.7194 - MinusLogProbMetric: 28.7194 - val_loss: 28.7330 - val_MinusLogProbMetric: 28.7330 - lr: 5.0000e-04 - 54s/epoch - 277ms/step
Epoch 350/1000
2023-09-28 21:47:19.423 
Epoch 350/1000 
	 loss: 28.7437, MinusLogProbMetric: 28.7437, val_loss: 28.9082, val_MinusLogProbMetric: 28.9082

Epoch 350: val_loss did not improve from 28.73296
196/196 - 50s - loss: 28.7437 - MinusLogProbMetric: 28.7437 - val_loss: 28.9082 - val_MinusLogProbMetric: 28.9082 - lr: 5.0000e-04 - 50s/epoch - 257ms/step
Epoch 351/1000
2023-09-28 21:48:07.058 
Epoch 351/1000 
	 loss: 28.7971, MinusLogProbMetric: 28.7971, val_loss: 29.8252, val_MinusLogProbMetric: 29.8252

Epoch 351: val_loss did not improve from 28.73296
196/196 - 48s - loss: 28.7971 - MinusLogProbMetric: 28.7971 - val_loss: 29.8252 - val_MinusLogProbMetric: 29.8252 - lr: 5.0000e-04 - 48s/epoch - 243ms/step
Epoch 352/1000
2023-09-28 21:48:54.930 
Epoch 352/1000 
	 loss: 28.7698, MinusLogProbMetric: 28.7698, val_loss: 28.7978, val_MinusLogProbMetric: 28.7978

Epoch 352: val_loss did not improve from 28.73296
196/196 - 48s - loss: 28.7698 - MinusLogProbMetric: 28.7698 - val_loss: 28.7978 - val_MinusLogProbMetric: 28.7978 - lr: 5.0000e-04 - 48s/epoch - 244ms/step
Epoch 353/1000
2023-09-28 21:49:48.147 
Epoch 353/1000 
	 loss: 28.7613, MinusLogProbMetric: 28.7613, val_loss: 29.0091, val_MinusLogProbMetric: 29.0091

Epoch 353: val_loss did not improve from 28.73296
196/196 - 53s - loss: 28.7613 - MinusLogProbMetric: 28.7613 - val_loss: 29.0091 - val_MinusLogProbMetric: 29.0091 - lr: 5.0000e-04 - 53s/epoch - 271ms/step
Epoch 354/1000
2023-09-28 21:50:40.390 
Epoch 354/1000 
	 loss: 28.7624, MinusLogProbMetric: 28.7624, val_loss: 28.8345, val_MinusLogProbMetric: 28.8345

Epoch 354: val_loss did not improve from 28.73296
196/196 - 52s - loss: 28.7624 - MinusLogProbMetric: 28.7624 - val_loss: 28.8345 - val_MinusLogProbMetric: 28.8345 - lr: 5.0000e-04 - 52s/epoch - 267ms/step
Epoch 355/1000
2023-09-28 21:51:31.473 
Epoch 355/1000 
	 loss: 28.8111, MinusLogProbMetric: 28.8111, val_loss: 28.9651, val_MinusLogProbMetric: 28.9651

Epoch 355: val_loss did not improve from 28.73296
196/196 - 51s - loss: 28.8111 - MinusLogProbMetric: 28.8111 - val_loss: 28.9651 - val_MinusLogProbMetric: 28.9651 - lr: 5.0000e-04 - 51s/epoch - 261ms/step
Epoch 356/1000
2023-09-28 21:52:26.862 
Epoch 356/1000 
	 loss: 28.7595, MinusLogProbMetric: 28.7595, val_loss: 28.8271, val_MinusLogProbMetric: 28.8271

Epoch 356: val_loss did not improve from 28.73296
196/196 - 55s - loss: 28.7595 - MinusLogProbMetric: 28.7595 - val_loss: 28.8271 - val_MinusLogProbMetric: 28.8271 - lr: 5.0000e-04 - 55s/epoch - 283ms/step
Epoch 357/1000
2023-09-28 21:53:15.477 
Epoch 357/1000 
	 loss: 28.7111, MinusLogProbMetric: 28.7111, val_loss: 29.0960, val_MinusLogProbMetric: 29.0960

Epoch 357: val_loss did not improve from 28.73296
196/196 - 49s - loss: 28.7111 - MinusLogProbMetric: 28.7111 - val_loss: 29.0960 - val_MinusLogProbMetric: 29.0960 - lr: 5.0000e-04 - 49s/epoch - 248ms/step
Epoch 358/1000
2023-09-28 21:54:03.691 
Epoch 358/1000 
	 loss: 28.7491, MinusLogProbMetric: 28.7491, val_loss: 28.8878, val_MinusLogProbMetric: 28.8878

Epoch 358: val_loss did not improve from 28.73296
196/196 - 48s - loss: 28.7491 - MinusLogProbMetric: 28.7491 - val_loss: 28.8878 - val_MinusLogProbMetric: 28.8878 - lr: 5.0000e-04 - 48s/epoch - 246ms/step
Epoch 359/1000
2023-09-28 21:54:51.606 
Epoch 359/1000 
	 loss: 28.7753, MinusLogProbMetric: 28.7753, val_loss: 28.7447, val_MinusLogProbMetric: 28.7447

Epoch 359: val_loss did not improve from 28.73296
196/196 - 48s - loss: 28.7753 - MinusLogProbMetric: 28.7753 - val_loss: 28.7447 - val_MinusLogProbMetric: 28.7447 - lr: 5.0000e-04 - 48s/epoch - 244ms/step
Epoch 360/1000
2023-09-28 21:55:43.379 
Epoch 360/1000 
	 loss: 28.7969, MinusLogProbMetric: 28.7969, val_loss: 30.1674, val_MinusLogProbMetric: 30.1674

Epoch 360: val_loss did not improve from 28.73296
196/196 - 52s - loss: 28.7969 - MinusLogProbMetric: 28.7969 - val_loss: 30.1674 - val_MinusLogProbMetric: 30.1674 - lr: 5.0000e-04 - 52s/epoch - 264ms/step
Epoch 361/1000
2023-09-28 21:56:36.085 
Epoch 361/1000 
	 loss: 28.7275, MinusLogProbMetric: 28.7275, val_loss: 28.9654, val_MinusLogProbMetric: 28.9654

Epoch 361: val_loss did not improve from 28.73296
196/196 - 53s - loss: 28.7275 - MinusLogProbMetric: 28.7275 - val_loss: 28.9654 - val_MinusLogProbMetric: 28.9654 - lr: 5.0000e-04 - 53s/epoch - 269ms/step
Epoch 362/1000
2023-09-28 21:57:27.784 
Epoch 362/1000 
	 loss: 28.7516, MinusLogProbMetric: 28.7516, val_loss: 29.0797, val_MinusLogProbMetric: 29.0797

Epoch 362: val_loss did not improve from 28.73296
196/196 - 52s - loss: 28.7516 - MinusLogProbMetric: 28.7516 - val_loss: 29.0797 - val_MinusLogProbMetric: 29.0797 - lr: 5.0000e-04 - 52s/epoch - 264ms/step
Epoch 363/1000
2023-09-28 21:58:23.272 
Epoch 363/1000 
	 loss: 28.6796, MinusLogProbMetric: 28.6796, val_loss: 28.9092, val_MinusLogProbMetric: 28.9092

Epoch 363: val_loss did not improve from 28.73296
196/196 - 55s - loss: 28.6796 - MinusLogProbMetric: 28.6796 - val_loss: 28.9092 - val_MinusLogProbMetric: 28.9092 - lr: 5.0000e-04 - 55s/epoch - 283ms/step
Epoch 364/1000
2023-09-28 21:59:15.197 
Epoch 364/1000 
	 loss: 28.7501, MinusLogProbMetric: 28.7501, val_loss: 29.4641, val_MinusLogProbMetric: 29.4641

Epoch 364: val_loss did not improve from 28.73296
196/196 - 52s - loss: 28.7501 - MinusLogProbMetric: 28.7501 - val_loss: 29.4641 - val_MinusLogProbMetric: 29.4641 - lr: 5.0000e-04 - 52s/epoch - 265ms/step
Epoch 365/1000
2023-09-28 22:00:09.027 
Epoch 365/1000 
	 loss: 28.7025, MinusLogProbMetric: 28.7025, val_loss: 28.9003, val_MinusLogProbMetric: 28.9003

Epoch 365: val_loss did not improve from 28.73296
196/196 - 54s - loss: 28.7025 - MinusLogProbMetric: 28.7025 - val_loss: 28.9003 - val_MinusLogProbMetric: 28.9003 - lr: 5.0000e-04 - 54s/epoch - 275ms/step
Epoch 366/1000
2023-09-28 22:01:02.195 
Epoch 366/1000 
	 loss: 28.7075, MinusLogProbMetric: 28.7075, val_loss: 29.2931, val_MinusLogProbMetric: 29.2931

Epoch 366: val_loss did not improve from 28.73296
196/196 - 53s - loss: 28.7075 - MinusLogProbMetric: 28.7075 - val_loss: 29.2931 - val_MinusLogProbMetric: 29.2931 - lr: 5.0000e-04 - 53s/epoch - 271ms/step
Epoch 367/1000
2023-09-28 22:01:55.561 
Epoch 367/1000 
	 loss: 28.7388, MinusLogProbMetric: 28.7388, val_loss: 29.0241, val_MinusLogProbMetric: 29.0241

Epoch 367: val_loss did not improve from 28.73296
196/196 - 53s - loss: 28.7388 - MinusLogProbMetric: 28.7388 - val_loss: 29.0241 - val_MinusLogProbMetric: 29.0241 - lr: 5.0000e-04 - 53s/epoch - 272ms/step
Epoch 368/1000
2023-09-28 22:02:48.430 
Epoch 368/1000 
	 loss: 28.7683, MinusLogProbMetric: 28.7683, val_loss: 28.8419, val_MinusLogProbMetric: 28.8419

Epoch 368: val_loss did not improve from 28.73296
196/196 - 53s - loss: 28.7683 - MinusLogProbMetric: 28.7683 - val_loss: 28.8419 - val_MinusLogProbMetric: 28.8419 - lr: 5.0000e-04 - 53s/epoch - 270ms/step
Epoch 369/1000
2023-09-28 22:03:43.093 
Epoch 369/1000 
	 loss: 28.6765, MinusLogProbMetric: 28.6765, val_loss: 28.9951, val_MinusLogProbMetric: 28.9951

Epoch 369: val_loss did not improve from 28.73296
196/196 - 55s - loss: 28.6765 - MinusLogProbMetric: 28.6765 - val_loss: 28.9951 - val_MinusLogProbMetric: 28.9951 - lr: 5.0000e-04 - 55s/epoch - 279ms/step
Epoch 370/1000
2023-09-28 22:04:36.182 
Epoch 370/1000 
	 loss: 28.7442, MinusLogProbMetric: 28.7442, val_loss: 28.6534, val_MinusLogProbMetric: 28.6534

Epoch 370: val_loss improved from 28.73296 to 28.65342, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_323/weights/best_weights.h5
196/196 - 54s - loss: 28.7442 - MinusLogProbMetric: 28.7442 - val_loss: 28.6534 - val_MinusLogProbMetric: 28.6534 - lr: 5.0000e-04 - 54s/epoch - 274ms/step
Epoch 371/1000
2023-09-28 22:05:31.205 
Epoch 371/1000 
	 loss: 28.7189, MinusLogProbMetric: 28.7189, val_loss: 29.6153, val_MinusLogProbMetric: 29.6153

Epoch 371: val_loss did not improve from 28.65342
196/196 - 54s - loss: 28.7189 - MinusLogProbMetric: 28.7189 - val_loss: 29.6153 - val_MinusLogProbMetric: 29.6153 - lr: 5.0000e-04 - 54s/epoch - 278ms/step
Epoch 372/1000
2023-09-28 22:06:25.959 
Epoch 372/1000 
	 loss: 28.6974, MinusLogProbMetric: 28.6974, val_loss: 29.2689, val_MinusLogProbMetric: 29.2689

Epoch 372: val_loss did not improve from 28.65342
196/196 - 55s - loss: 28.6974 - MinusLogProbMetric: 28.6974 - val_loss: 29.2689 - val_MinusLogProbMetric: 29.2689 - lr: 5.0000e-04 - 55s/epoch - 279ms/step
Epoch 373/1000
2023-09-28 22:07:20.133 
Epoch 373/1000 
	 loss: 28.6789, MinusLogProbMetric: 28.6789, val_loss: 28.7772, val_MinusLogProbMetric: 28.7772

Epoch 373: val_loss did not improve from 28.65342
196/196 - 54s - loss: 28.6789 - MinusLogProbMetric: 28.6789 - val_loss: 28.7772 - val_MinusLogProbMetric: 28.7772 - lr: 5.0000e-04 - 54s/epoch - 276ms/step
Epoch 374/1000
2023-09-28 22:08:14.423 
Epoch 374/1000 
	 loss: 28.7112, MinusLogProbMetric: 28.7112, val_loss: 29.1890, val_MinusLogProbMetric: 29.1890

Epoch 374: val_loss did not improve from 28.65342
196/196 - 54s - loss: 28.7112 - MinusLogProbMetric: 28.7112 - val_loss: 29.1890 - val_MinusLogProbMetric: 29.1890 - lr: 5.0000e-04 - 54s/epoch - 277ms/step
Epoch 375/1000
2023-09-28 22:09:08.390 
Epoch 375/1000 
	 loss: 28.7413, MinusLogProbMetric: 28.7413, val_loss: 28.9008, val_MinusLogProbMetric: 28.9008

Epoch 375: val_loss did not improve from 28.65342
196/196 - 54s - loss: 28.7413 - MinusLogProbMetric: 28.7413 - val_loss: 28.9008 - val_MinusLogProbMetric: 28.9008 - lr: 5.0000e-04 - 54s/epoch - 275ms/step
Epoch 376/1000
2023-09-28 22:10:01.729 
Epoch 376/1000 
	 loss: 28.7518, MinusLogProbMetric: 28.7518, val_loss: 28.8930, val_MinusLogProbMetric: 28.8930

Epoch 376: val_loss did not improve from 28.65342
196/196 - 53s - loss: 28.7518 - MinusLogProbMetric: 28.7518 - val_loss: 28.8930 - val_MinusLogProbMetric: 28.8930 - lr: 5.0000e-04 - 53s/epoch - 272ms/step
Epoch 377/1000
2023-09-28 22:10:52.579 
Epoch 377/1000 
	 loss: 28.6959, MinusLogProbMetric: 28.6959, val_loss: 28.9683, val_MinusLogProbMetric: 28.9683

Epoch 377: val_loss did not improve from 28.65342
196/196 - 51s - loss: 28.6959 - MinusLogProbMetric: 28.6959 - val_loss: 28.9683 - val_MinusLogProbMetric: 28.9683 - lr: 5.0000e-04 - 51s/epoch - 259ms/step
Epoch 378/1000
2023-09-28 22:11:38.874 
Epoch 378/1000 
	 loss: 28.6392, MinusLogProbMetric: 28.6392, val_loss: 28.7014, val_MinusLogProbMetric: 28.7014

Epoch 378: val_loss did not improve from 28.65342
196/196 - 46s - loss: 28.6392 - MinusLogProbMetric: 28.6392 - val_loss: 28.7014 - val_MinusLogProbMetric: 28.7014 - lr: 5.0000e-04 - 46s/epoch - 236ms/step
Epoch 379/1000
2023-09-28 22:12:25.961 
Epoch 379/1000 
	 loss: 28.7691, MinusLogProbMetric: 28.7691, val_loss: 29.1040, val_MinusLogProbMetric: 29.1040

Epoch 379: val_loss did not improve from 28.65342
196/196 - 47s - loss: 28.7691 - MinusLogProbMetric: 28.7691 - val_loss: 29.1040 - val_MinusLogProbMetric: 29.1040 - lr: 5.0000e-04 - 47s/epoch - 240ms/step
Epoch 380/1000
2023-09-28 22:13:15.626 
Epoch 380/1000 
	 loss: 28.6388, MinusLogProbMetric: 28.6388, val_loss: 28.8731, val_MinusLogProbMetric: 28.8731

Epoch 380: val_loss did not improve from 28.65342
196/196 - 50s - loss: 28.6388 - MinusLogProbMetric: 28.6388 - val_loss: 28.8731 - val_MinusLogProbMetric: 28.8731 - lr: 5.0000e-04 - 50s/epoch - 253ms/step
Epoch 381/1000
2023-09-28 22:14:10.501 
Epoch 381/1000 
	 loss: 28.6736, MinusLogProbMetric: 28.6736, val_loss: 28.6101, val_MinusLogProbMetric: 28.6101

Epoch 381: val_loss improved from 28.65342 to 28.61005, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_323/weights/best_weights.h5
196/196 - 56s - loss: 28.6736 - MinusLogProbMetric: 28.6736 - val_loss: 28.6101 - val_MinusLogProbMetric: 28.6101 - lr: 5.0000e-04 - 56s/epoch - 283ms/step
Epoch 382/1000
2023-09-28 22:15:03.892 
Epoch 382/1000 
	 loss: 28.7883, MinusLogProbMetric: 28.7883, val_loss: 28.8996, val_MinusLogProbMetric: 28.8996

Epoch 382: val_loss did not improve from 28.61005
196/196 - 53s - loss: 28.7883 - MinusLogProbMetric: 28.7883 - val_loss: 28.8996 - val_MinusLogProbMetric: 28.8996 - lr: 5.0000e-04 - 53s/epoch - 269ms/step
Epoch 383/1000
2023-09-28 22:15:58.790 
Epoch 383/1000 
	 loss: 28.6382, MinusLogProbMetric: 28.6382, val_loss: 29.2007, val_MinusLogProbMetric: 29.2007

Epoch 383: val_loss did not improve from 28.61005
196/196 - 55s - loss: 28.6382 - MinusLogProbMetric: 28.6382 - val_loss: 29.2007 - val_MinusLogProbMetric: 29.2007 - lr: 5.0000e-04 - 55s/epoch - 280ms/step
Epoch 384/1000
2023-09-28 22:16:53.503 
Epoch 384/1000 
	 loss: 28.6834, MinusLogProbMetric: 28.6834, val_loss: 29.4355, val_MinusLogProbMetric: 29.4355

Epoch 384: val_loss did not improve from 28.61005
196/196 - 55s - loss: 28.6834 - MinusLogProbMetric: 28.6834 - val_loss: 29.4355 - val_MinusLogProbMetric: 29.4355 - lr: 5.0000e-04 - 55s/epoch - 279ms/step
Epoch 385/1000
2023-09-28 22:17:48.320 
Epoch 385/1000 
	 loss: 28.7644, MinusLogProbMetric: 28.7644, val_loss: 28.7716, val_MinusLogProbMetric: 28.7716

Epoch 385: val_loss did not improve from 28.61005
196/196 - 55s - loss: 28.7644 - MinusLogProbMetric: 28.7644 - val_loss: 28.7716 - val_MinusLogProbMetric: 28.7716 - lr: 5.0000e-04 - 55s/epoch - 280ms/step
Epoch 386/1000
2023-09-28 22:18:41.093 
Epoch 386/1000 
	 loss: 28.6857, MinusLogProbMetric: 28.6857, val_loss: 29.2350, val_MinusLogProbMetric: 29.2350

Epoch 386: val_loss did not improve from 28.61005
196/196 - 53s - loss: 28.6857 - MinusLogProbMetric: 28.6857 - val_loss: 29.2350 - val_MinusLogProbMetric: 29.2350 - lr: 5.0000e-04 - 53s/epoch - 269ms/step
Epoch 387/1000
2023-09-28 22:19:34.946 
Epoch 387/1000 
	 loss: 28.7653, MinusLogProbMetric: 28.7653, val_loss: 28.8631, val_MinusLogProbMetric: 28.8631

Epoch 387: val_loss did not improve from 28.61005
196/196 - 54s - loss: 28.7653 - MinusLogProbMetric: 28.7653 - val_loss: 28.8631 - val_MinusLogProbMetric: 28.8631 - lr: 5.0000e-04 - 54s/epoch - 275ms/step
Epoch 388/1000
2023-09-28 22:20:29.295 
Epoch 388/1000 
	 loss: 28.6604, MinusLogProbMetric: 28.6604, val_loss: 28.8643, val_MinusLogProbMetric: 28.8643

Epoch 388: val_loss did not improve from 28.61005
196/196 - 54s - loss: 28.6604 - MinusLogProbMetric: 28.6604 - val_loss: 28.8643 - val_MinusLogProbMetric: 28.8643 - lr: 5.0000e-04 - 54s/epoch - 277ms/step
Epoch 389/1000
2023-09-28 22:21:23.979 
Epoch 389/1000 
	 loss: 28.7719, MinusLogProbMetric: 28.7719, val_loss: 29.3459, val_MinusLogProbMetric: 29.3459

Epoch 389: val_loss did not improve from 28.61005
196/196 - 55s - loss: 28.7719 - MinusLogProbMetric: 28.7719 - val_loss: 29.3459 - val_MinusLogProbMetric: 29.3459 - lr: 5.0000e-04 - 55s/epoch - 279ms/step
Epoch 390/1000
2023-09-28 22:22:18.880 
Epoch 390/1000 
	 loss: 28.7724, MinusLogProbMetric: 28.7724, val_loss: 28.7149, val_MinusLogProbMetric: 28.7149

Epoch 390: val_loss did not improve from 28.61005
196/196 - 55s - loss: 28.7724 - MinusLogProbMetric: 28.7724 - val_loss: 28.7149 - val_MinusLogProbMetric: 28.7149 - lr: 5.0000e-04 - 55s/epoch - 280ms/step
Epoch 391/1000
2023-09-28 22:23:13.517 
Epoch 391/1000 
	 loss: 28.6601, MinusLogProbMetric: 28.6601, val_loss: 29.2333, val_MinusLogProbMetric: 29.2333

Epoch 391: val_loss did not improve from 28.61005
196/196 - 55s - loss: 28.6601 - MinusLogProbMetric: 28.6601 - val_loss: 29.2333 - val_MinusLogProbMetric: 29.2333 - lr: 5.0000e-04 - 55s/epoch - 279ms/step
Epoch 392/1000
2023-09-28 22:24:08.157 
Epoch 392/1000 
	 loss: 28.8078, MinusLogProbMetric: 28.8078, val_loss: 28.7018, val_MinusLogProbMetric: 28.7018

Epoch 392: val_loss did not improve from 28.61005
196/196 - 55s - loss: 28.8078 - MinusLogProbMetric: 28.8078 - val_loss: 28.7018 - val_MinusLogProbMetric: 28.7018 - lr: 5.0000e-04 - 55s/epoch - 279ms/step
Epoch 393/1000
2023-09-28 22:25:02.170 
Epoch 393/1000 
	 loss: 28.6795, MinusLogProbMetric: 28.6795, val_loss: 30.3544, val_MinusLogProbMetric: 30.3544

Epoch 393: val_loss did not improve from 28.61005
196/196 - 54s - loss: 28.6795 - MinusLogProbMetric: 28.6795 - val_loss: 30.3544 - val_MinusLogProbMetric: 30.3544 - lr: 5.0000e-04 - 54s/epoch - 276ms/step
Epoch 394/1000
2023-09-28 22:25:56.888 
Epoch 394/1000 
	 loss: 28.7393, MinusLogProbMetric: 28.7393, val_loss: 28.7324, val_MinusLogProbMetric: 28.7324

Epoch 394: val_loss did not improve from 28.61005
196/196 - 55s - loss: 28.7393 - MinusLogProbMetric: 28.7393 - val_loss: 28.7324 - val_MinusLogProbMetric: 28.7324 - lr: 5.0000e-04 - 55s/epoch - 279ms/step
Epoch 395/1000
2023-09-28 22:26:51.490 
Epoch 395/1000 
	 loss: 28.6119, MinusLogProbMetric: 28.6119, val_loss: 28.8203, val_MinusLogProbMetric: 28.8203

Epoch 395: val_loss did not improve from 28.61005
196/196 - 55s - loss: 28.6119 - MinusLogProbMetric: 28.6119 - val_loss: 28.8203 - val_MinusLogProbMetric: 28.8203 - lr: 5.0000e-04 - 55s/epoch - 279ms/step
Epoch 396/1000
2023-09-28 22:27:46.030 
Epoch 396/1000 
	 loss: 28.6184, MinusLogProbMetric: 28.6184, val_loss: 28.6007, val_MinusLogProbMetric: 28.6007

Epoch 396: val_loss improved from 28.61005 to 28.60072, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_323/weights/best_weights.h5
196/196 - 55s - loss: 28.6184 - MinusLogProbMetric: 28.6184 - val_loss: 28.6007 - val_MinusLogProbMetric: 28.6007 - lr: 5.0000e-04 - 55s/epoch - 282ms/step
Epoch 397/1000
2023-09-28 22:28:41.453 
Epoch 397/1000 
	 loss: 28.7114, MinusLogProbMetric: 28.7114, val_loss: 28.8318, val_MinusLogProbMetric: 28.8318

Epoch 397: val_loss did not improve from 28.60072
196/196 - 55s - loss: 28.7114 - MinusLogProbMetric: 28.7114 - val_loss: 28.8318 - val_MinusLogProbMetric: 28.8318 - lr: 5.0000e-04 - 55s/epoch - 279ms/step
Epoch 398/1000
2023-09-28 22:29:36.426 
Epoch 398/1000 
	 loss: 28.7064, MinusLogProbMetric: 28.7064, val_loss: 28.6480, val_MinusLogProbMetric: 28.6480

Epoch 398: val_loss did not improve from 28.60072
196/196 - 55s - loss: 28.7064 - MinusLogProbMetric: 28.7064 - val_loss: 28.6480 - val_MinusLogProbMetric: 28.6480 - lr: 5.0000e-04 - 55s/epoch - 280ms/step
Epoch 399/1000
2023-09-28 22:30:31.320 
Epoch 399/1000 
	 loss: 28.7654, MinusLogProbMetric: 28.7654, val_loss: 28.8223, val_MinusLogProbMetric: 28.8223

Epoch 399: val_loss did not improve from 28.60072
196/196 - 55s - loss: 28.7654 - MinusLogProbMetric: 28.7654 - val_loss: 28.8223 - val_MinusLogProbMetric: 28.8223 - lr: 5.0000e-04 - 55s/epoch - 280ms/step
Epoch 400/1000
2023-09-28 22:31:26.466 
Epoch 400/1000 
	 loss: 28.6818, MinusLogProbMetric: 28.6818, val_loss: 29.0357, val_MinusLogProbMetric: 29.0357

Epoch 400: val_loss did not improve from 28.60072
196/196 - 55s - loss: 28.6818 - MinusLogProbMetric: 28.6818 - val_loss: 29.0357 - val_MinusLogProbMetric: 29.0357 - lr: 5.0000e-04 - 55s/epoch - 281ms/step
Epoch 401/1000
2023-09-28 22:32:20.799 
Epoch 401/1000 
	 loss: 28.6395, MinusLogProbMetric: 28.6395, val_loss: 29.3163, val_MinusLogProbMetric: 29.3163

Epoch 401: val_loss did not improve from 28.60072
196/196 - 54s - loss: 28.6395 - MinusLogProbMetric: 28.6395 - val_loss: 29.3163 - val_MinusLogProbMetric: 29.3163 - lr: 5.0000e-04 - 54s/epoch - 277ms/step
Epoch 402/1000
2023-09-28 22:33:15.477 
Epoch 402/1000 
	 loss: 28.7518, MinusLogProbMetric: 28.7518, val_loss: 28.8039, val_MinusLogProbMetric: 28.8039

Epoch 402: val_loss did not improve from 28.60072
196/196 - 55s - loss: 28.7518 - MinusLogProbMetric: 28.7518 - val_loss: 28.8039 - val_MinusLogProbMetric: 28.8039 - lr: 5.0000e-04 - 55s/epoch - 279ms/step
Epoch 403/1000
2023-09-28 22:34:10.854 
Epoch 403/1000 
	 loss: 28.7203, MinusLogProbMetric: 28.7203, val_loss: 28.6509, val_MinusLogProbMetric: 28.6509

Epoch 403: val_loss did not improve from 28.60072
196/196 - 55s - loss: 28.7203 - MinusLogProbMetric: 28.7203 - val_loss: 28.6509 - val_MinusLogProbMetric: 28.6509 - lr: 5.0000e-04 - 55s/epoch - 282ms/step
Epoch 404/1000
2023-09-28 22:35:04.544 
Epoch 404/1000 
	 loss: 28.6322, MinusLogProbMetric: 28.6322, val_loss: 28.9075, val_MinusLogProbMetric: 28.9075

Epoch 404: val_loss did not improve from 28.60072
196/196 - 54s - loss: 28.6322 - MinusLogProbMetric: 28.6322 - val_loss: 28.9075 - val_MinusLogProbMetric: 28.9075 - lr: 5.0000e-04 - 54s/epoch - 274ms/step
Epoch 405/1000
2023-09-28 22:35:59.390 
Epoch 405/1000 
	 loss: 28.6983, MinusLogProbMetric: 28.6983, val_loss: 29.1427, val_MinusLogProbMetric: 29.1427

Epoch 405: val_loss did not improve from 28.60072
196/196 - 55s - loss: 28.6983 - MinusLogProbMetric: 28.6983 - val_loss: 29.1427 - val_MinusLogProbMetric: 29.1427 - lr: 5.0000e-04 - 55s/epoch - 280ms/step
Epoch 406/1000
2023-09-28 22:36:53.607 
Epoch 406/1000 
	 loss: 28.6988, MinusLogProbMetric: 28.6988, val_loss: 28.7877, val_MinusLogProbMetric: 28.7877

Epoch 406: val_loss did not improve from 28.60072
196/196 - 54s - loss: 28.6988 - MinusLogProbMetric: 28.6988 - val_loss: 28.7877 - val_MinusLogProbMetric: 28.7877 - lr: 5.0000e-04 - 54s/epoch - 277ms/step
Epoch 407/1000
2023-09-28 22:37:45.966 
Epoch 407/1000 
	 loss: 28.6769, MinusLogProbMetric: 28.6769, val_loss: 28.6578, val_MinusLogProbMetric: 28.6578

Epoch 407: val_loss did not improve from 28.60072
196/196 - 52s - loss: 28.6769 - MinusLogProbMetric: 28.6769 - val_loss: 28.6578 - val_MinusLogProbMetric: 28.6578 - lr: 5.0000e-04 - 52s/epoch - 267ms/step
Epoch 408/1000
2023-09-28 22:38:40.407 
Epoch 408/1000 
	 loss: 28.5986, MinusLogProbMetric: 28.5986, val_loss: 29.2351, val_MinusLogProbMetric: 29.2351

Epoch 408: val_loss did not improve from 28.60072
196/196 - 54s - loss: 28.5986 - MinusLogProbMetric: 28.5986 - val_loss: 29.2351 - val_MinusLogProbMetric: 29.2351 - lr: 5.0000e-04 - 54s/epoch - 278ms/step
Epoch 409/1000
2023-09-28 22:39:32.704 
Epoch 409/1000 
	 loss: 28.6760, MinusLogProbMetric: 28.6760, val_loss: 29.0489, val_MinusLogProbMetric: 29.0489

Epoch 409: val_loss did not improve from 28.60072
196/196 - 52s - loss: 28.6760 - MinusLogProbMetric: 28.6760 - val_loss: 29.0489 - val_MinusLogProbMetric: 29.0489 - lr: 5.0000e-04 - 52s/epoch - 267ms/step
Epoch 410/1000
2023-09-28 22:40:27.315 
Epoch 410/1000 
	 loss: 28.7358, MinusLogProbMetric: 28.7358, val_loss: 28.5922, val_MinusLogProbMetric: 28.5922

Epoch 410: val_loss improved from 28.60072 to 28.59224, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_323/weights/best_weights.h5
196/196 - 55s - loss: 28.7358 - MinusLogProbMetric: 28.7358 - val_loss: 28.5922 - val_MinusLogProbMetric: 28.5922 - lr: 5.0000e-04 - 55s/epoch - 282ms/step
Epoch 411/1000
2023-09-28 22:41:22.328 
Epoch 411/1000 
	 loss: 28.6499, MinusLogProbMetric: 28.6499, val_loss: 29.1826, val_MinusLogProbMetric: 29.1826

Epoch 411: val_loss did not improve from 28.59224
196/196 - 54s - loss: 28.6499 - MinusLogProbMetric: 28.6499 - val_loss: 29.1826 - val_MinusLogProbMetric: 29.1826 - lr: 5.0000e-04 - 54s/epoch - 277ms/step
Epoch 412/1000
2023-09-28 22:42:16.110 
Epoch 412/1000 
	 loss: 28.6426, MinusLogProbMetric: 28.6426, val_loss: 29.3877, val_MinusLogProbMetric: 29.3877

Epoch 412: val_loss did not improve from 28.59224
196/196 - 54s - loss: 28.6426 - MinusLogProbMetric: 28.6426 - val_loss: 29.3877 - val_MinusLogProbMetric: 29.3877 - lr: 5.0000e-04 - 54s/epoch - 274ms/step
Epoch 413/1000
2023-09-28 22:43:10.453 
Epoch 413/1000 
	 loss: 28.6955, MinusLogProbMetric: 28.6955, val_loss: 29.4389, val_MinusLogProbMetric: 29.4389

Epoch 413: val_loss did not improve from 28.59224
196/196 - 54s - loss: 28.6955 - MinusLogProbMetric: 28.6955 - val_loss: 29.4389 - val_MinusLogProbMetric: 29.4389 - lr: 5.0000e-04 - 54s/epoch - 277ms/step
Epoch 414/1000
2023-09-28 22:44:05.174 
Epoch 414/1000 
	 loss: 28.7542, MinusLogProbMetric: 28.7542, val_loss: 28.9029, val_MinusLogProbMetric: 28.9029

Epoch 414: val_loss did not improve from 28.59224
196/196 - 55s - loss: 28.7542 - MinusLogProbMetric: 28.7542 - val_loss: 28.9029 - val_MinusLogProbMetric: 28.9029 - lr: 5.0000e-04 - 55s/epoch - 279ms/step
Epoch 415/1000
2023-09-28 22:44:59.911 
Epoch 415/1000 
	 loss: 28.6198, MinusLogProbMetric: 28.6198, val_loss: 28.8208, val_MinusLogProbMetric: 28.8208

Epoch 415: val_loss did not improve from 28.59224
196/196 - 55s - loss: 28.6198 - MinusLogProbMetric: 28.6198 - val_loss: 28.8208 - val_MinusLogProbMetric: 28.8208 - lr: 5.0000e-04 - 55s/epoch - 279ms/step
Epoch 416/1000
2023-09-28 22:45:54.524 
Epoch 416/1000 
	 loss: 28.6966, MinusLogProbMetric: 28.6966, val_loss: 28.5984, val_MinusLogProbMetric: 28.5984

Epoch 416: val_loss did not improve from 28.59224
196/196 - 55s - loss: 28.6966 - MinusLogProbMetric: 28.6966 - val_loss: 28.5984 - val_MinusLogProbMetric: 28.5984 - lr: 5.0000e-04 - 55s/epoch - 279ms/step
Epoch 417/1000
2023-09-28 22:46:48.913 
Epoch 417/1000 
	 loss: 28.7000, MinusLogProbMetric: 28.7000, val_loss: 28.8316, val_MinusLogProbMetric: 28.8316

Epoch 417: val_loss did not improve from 28.59224
196/196 - 54s - loss: 28.7000 - MinusLogProbMetric: 28.7000 - val_loss: 28.8316 - val_MinusLogProbMetric: 28.8316 - lr: 5.0000e-04 - 54s/epoch - 277ms/step
Epoch 418/1000
2023-09-28 22:47:43.994 
Epoch 418/1000 
	 loss: 28.7114, MinusLogProbMetric: 28.7114, val_loss: 29.0656, val_MinusLogProbMetric: 29.0656

Epoch 418: val_loss did not improve from 28.59224
196/196 - 55s - loss: 28.7114 - MinusLogProbMetric: 28.7114 - val_loss: 29.0656 - val_MinusLogProbMetric: 29.0656 - lr: 5.0000e-04 - 55s/epoch - 281ms/step
Epoch 419/1000
2023-09-28 22:48:36.729 
Epoch 419/1000 
	 loss: 28.6028, MinusLogProbMetric: 28.6028, val_loss: 28.7933, val_MinusLogProbMetric: 28.7933

Epoch 419: val_loss did not improve from 28.59224
196/196 - 53s - loss: 28.6028 - MinusLogProbMetric: 28.6028 - val_loss: 28.7933 - val_MinusLogProbMetric: 28.7933 - lr: 5.0000e-04 - 53s/epoch - 269ms/step
Epoch 420/1000
2023-09-28 22:49:30.559 
Epoch 420/1000 
	 loss: 28.5888, MinusLogProbMetric: 28.5888, val_loss: 28.8252, val_MinusLogProbMetric: 28.8252

Epoch 420: val_loss did not improve from 28.59224
196/196 - 54s - loss: 28.5888 - MinusLogProbMetric: 28.5888 - val_loss: 28.8252 - val_MinusLogProbMetric: 28.8252 - lr: 5.0000e-04 - 54s/epoch - 275ms/step
Epoch 421/1000
2023-09-28 22:50:24.912 
Epoch 421/1000 
	 loss: 28.6726, MinusLogProbMetric: 28.6726, val_loss: 28.6329, val_MinusLogProbMetric: 28.6329

Epoch 421: val_loss did not improve from 28.59224
196/196 - 54s - loss: 28.6726 - MinusLogProbMetric: 28.6726 - val_loss: 28.6329 - val_MinusLogProbMetric: 28.6329 - lr: 5.0000e-04 - 54s/epoch - 277ms/step
Epoch 422/1000
2023-09-28 22:51:19.334 
Epoch 422/1000 
	 loss: 28.7979, MinusLogProbMetric: 28.7979, val_loss: 28.6948, val_MinusLogProbMetric: 28.6948

Epoch 422: val_loss did not improve from 28.59224
196/196 - 54s - loss: 28.7979 - MinusLogProbMetric: 28.7979 - val_loss: 28.6948 - val_MinusLogProbMetric: 28.6948 - lr: 5.0000e-04 - 54s/epoch - 278ms/step
Epoch 423/1000
2023-09-28 22:52:13.495 
Epoch 423/1000 
	 loss: 28.6141, MinusLogProbMetric: 28.6141, val_loss: 29.6581, val_MinusLogProbMetric: 29.6581

Epoch 423: val_loss did not improve from 28.59224
196/196 - 54s - loss: 28.6141 - MinusLogProbMetric: 28.6141 - val_loss: 29.6581 - val_MinusLogProbMetric: 29.6581 - lr: 5.0000e-04 - 54s/epoch - 276ms/step
Epoch 424/1000
2023-09-28 22:53:05.707 
Epoch 424/1000 
	 loss: 28.5787, MinusLogProbMetric: 28.5787, val_loss: 28.7746, val_MinusLogProbMetric: 28.7746

Epoch 424: val_loss did not improve from 28.59224
196/196 - 52s - loss: 28.5787 - MinusLogProbMetric: 28.5787 - val_loss: 28.7746 - val_MinusLogProbMetric: 28.7746 - lr: 5.0000e-04 - 52s/epoch - 266ms/step
Epoch 425/1000
2023-09-28 22:53:59.480 
Epoch 425/1000 
	 loss: 28.7183, MinusLogProbMetric: 28.7183, val_loss: 28.8473, val_MinusLogProbMetric: 28.8473

Epoch 425: val_loss did not improve from 28.59224
196/196 - 54s - loss: 28.7183 - MinusLogProbMetric: 28.7183 - val_loss: 28.8473 - val_MinusLogProbMetric: 28.8473 - lr: 5.0000e-04 - 54s/epoch - 274ms/step
Epoch 426/1000
2023-09-28 22:54:51.660 
Epoch 426/1000 
	 loss: 28.6519, MinusLogProbMetric: 28.6519, val_loss: 28.6907, val_MinusLogProbMetric: 28.6907

Epoch 426: val_loss did not improve from 28.59224
196/196 - 52s - loss: 28.6519 - MinusLogProbMetric: 28.6519 - val_loss: 28.6907 - val_MinusLogProbMetric: 28.6907 - lr: 5.0000e-04 - 52s/epoch - 266ms/step
Epoch 427/1000
2023-09-28 22:55:44.501 
Epoch 427/1000 
	 loss: 28.5941, MinusLogProbMetric: 28.5941, val_loss: 29.1045, val_MinusLogProbMetric: 29.1045

Epoch 427: val_loss did not improve from 28.59224
196/196 - 53s - loss: 28.5941 - MinusLogProbMetric: 28.5941 - val_loss: 29.1045 - val_MinusLogProbMetric: 29.1045 - lr: 5.0000e-04 - 53s/epoch - 270ms/step
Epoch 428/1000
2023-09-28 22:56:38.373 
Epoch 428/1000 
	 loss: 28.6979, MinusLogProbMetric: 28.6979, val_loss: 28.7459, val_MinusLogProbMetric: 28.7459

Epoch 428: val_loss did not improve from 28.59224
196/196 - 54s - loss: 28.6979 - MinusLogProbMetric: 28.6979 - val_loss: 28.7459 - val_MinusLogProbMetric: 28.7459 - lr: 5.0000e-04 - 54s/epoch - 275ms/step
Epoch 429/1000
2023-09-28 22:57:30.290 
Epoch 429/1000 
	 loss: 28.6569, MinusLogProbMetric: 28.6569, val_loss: 29.0296, val_MinusLogProbMetric: 29.0296

Epoch 429: val_loss did not improve from 28.59224
196/196 - 52s - loss: 28.6569 - MinusLogProbMetric: 28.6569 - val_loss: 29.0296 - val_MinusLogProbMetric: 29.0296 - lr: 5.0000e-04 - 52s/epoch - 265ms/step
Epoch 430/1000
2023-09-28 22:58:21.928 
Epoch 430/1000 
	 loss: 28.6512, MinusLogProbMetric: 28.6512, val_loss: 28.6801, val_MinusLogProbMetric: 28.6801

Epoch 430: val_loss did not improve from 28.59224
196/196 - 52s - loss: 28.6512 - MinusLogProbMetric: 28.6512 - val_loss: 28.6801 - val_MinusLogProbMetric: 28.6801 - lr: 5.0000e-04 - 52s/epoch - 263ms/step
Epoch 431/1000
2023-09-28 22:59:14.759 
Epoch 431/1000 
	 loss: 28.6559, MinusLogProbMetric: 28.6559, val_loss: 28.7158, val_MinusLogProbMetric: 28.7158

Epoch 431: val_loss did not improve from 28.59224
196/196 - 53s - loss: 28.6559 - MinusLogProbMetric: 28.6559 - val_loss: 28.7158 - val_MinusLogProbMetric: 28.7158 - lr: 5.0000e-04 - 53s/epoch - 270ms/step
Epoch 432/1000
2023-09-28 23:00:06.596 
Epoch 432/1000 
	 loss: 28.6474, MinusLogProbMetric: 28.6474, val_loss: 29.7156, val_MinusLogProbMetric: 29.7156

Epoch 432: val_loss did not improve from 28.59224
196/196 - 52s - loss: 28.6474 - MinusLogProbMetric: 28.6474 - val_loss: 29.7156 - val_MinusLogProbMetric: 29.7156 - lr: 5.0000e-04 - 52s/epoch - 264ms/step
Epoch 433/1000
2023-09-28 23:00:57.740 
Epoch 433/1000 
	 loss: 28.7040, MinusLogProbMetric: 28.7040, val_loss: 28.7091, val_MinusLogProbMetric: 28.7091

Epoch 433: val_loss did not improve from 28.59224
196/196 - 51s - loss: 28.7040 - MinusLogProbMetric: 28.7040 - val_loss: 28.7091 - val_MinusLogProbMetric: 28.7091 - lr: 5.0000e-04 - 51s/epoch - 261ms/step
Epoch 434/1000
2023-09-28 23:01:46.633 
Epoch 434/1000 
	 loss: 28.5811, MinusLogProbMetric: 28.5811, val_loss: 28.5476, val_MinusLogProbMetric: 28.5476

Epoch 434: val_loss improved from 28.59224 to 28.54761, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_323/weights/best_weights.h5
196/196 - 49s - loss: 28.5811 - MinusLogProbMetric: 28.5811 - val_loss: 28.5476 - val_MinusLogProbMetric: 28.5476 - lr: 5.0000e-04 - 49s/epoch - 253ms/step
Epoch 435/1000
2023-09-28 23:02:41.163 
Epoch 435/1000 
	 loss: 28.6555, MinusLogProbMetric: 28.6555, val_loss: 28.6955, val_MinusLogProbMetric: 28.6955

Epoch 435: val_loss did not improve from 28.54761
196/196 - 54s - loss: 28.6555 - MinusLogProbMetric: 28.6555 - val_loss: 28.6955 - val_MinusLogProbMetric: 28.6955 - lr: 5.0000e-04 - 54s/epoch - 275ms/step
Epoch 436/1000
2023-09-28 23:03:34.369 
Epoch 436/1000 
	 loss: 28.5704, MinusLogProbMetric: 28.5704, val_loss: 28.9888, val_MinusLogProbMetric: 28.9888

Epoch 436: val_loss did not improve from 28.54761
196/196 - 53s - loss: 28.5704 - MinusLogProbMetric: 28.5704 - val_loss: 28.9888 - val_MinusLogProbMetric: 28.9888 - lr: 5.0000e-04 - 53s/epoch - 271ms/step
Epoch 437/1000
2023-09-28 23:04:26.936 
Epoch 437/1000 
	 loss: 28.7225, MinusLogProbMetric: 28.7225, val_loss: 28.6113, val_MinusLogProbMetric: 28.6113

Epoch 437: val_loss did not improve from 28.54761
196/196 - 53s - loss: 28.7225 - MinusLogProbMetric: 28.7225 - val_loss: 28.6113 - val_MinusLogProbMetric: 28.6113 - lr: 5.0000e-04 - 53s/epoch - 268ms/step
Epoch 438/1000
2023-09-28 23:05:19.133 
Epoch 438/1000 
	 loss: 28.6377, MinusLogProbMetric: 28.6377, val_loss: 28.7144, val_MinusLogProbMetric: 28.7144

Epoch 438: val_loss did not improve from 28.54761
196/196 - 52s - loss: 28.6377 - MinusLogProbMetric: 28.6377 - val_loss: 28.7144 - val_MinusLogProbMetric: 28.7144 - lr: 5.0000e-04 - 52s/epoch - 266ms/step
Epoch 439/1000
2023-09-28 23:06:10.513 
Epoch 439/1000 
	 loss: 28.6315, MinusLogProbMetric: 28.6315, val_loss: 28.5984, val_MinusLogProbMetric: 28.5984

Epoch 439: val_loss did not improve from 28.54761
196/196 - 51s - loss: 28.6315 - MinusLogProbMetric: 28.6315 - val_loss: 28.5984 - val_MinusLogProbMetric: 28.5984 - lr: 5.0000e-04 - 51s/epoch - 262ms/step
Epoch 440/1000
2023-09-28 23:07:02.164 
Epoch 440/1000 
	 loss: 28.6540, MinusLogProbMetric: 28.6540, val_loss: 28.5144, val_MinusLogProbMetric: 28.5144

Epoch 440: val_loss improved from 28.54761 to 28.51442, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_323/weights/best_weights.h5
196/196 - 52s - loss: 28.6540 - MinusLogProbMetric: 28.6540 - val_loss: 28.5144 - val_MinusLogProbMetric: 28.5144 - lr: 5.0000e-04 - 52s/epoch - 267ms/step
Epoch 441/1000
2023-09-28 23:07:54.324 
Epoch 441/1000 
	 loss: 28.6831, MinusLogProbMetric: 28.6831, val_loss: 28.8352, val_MinusLogProbMetric: 28.8352

Epoch 441: val_loss did not improve from 28.51442
196/196 - 51s - loss: 28.6831 - MinusLogProbMetric: 28.6831 - val_loss: 28.8352 - val_MinusLogProbMetric: 28.8352 - lr: 5.0000e-04 - 51s/epoch - 262ms/step
Epoch 442/1000
2023-09-28 23:08:47.929 
Epoch 442/1000 
	 loss: 28.6989, MinusLogProbMetric: 28.6989, val_loss: 28.7539, val_MinusLogProbMetric: 28.7539

Epoch 442: val_loss did not improve from 28.51442
196/196 - 54s - loss: 28.6989 - MinusLogProbMetric: 28.6989 - val_loss: 28.7539 - val_MinusLogProbMetric: 28.7539 - lr: 5.0000e-04 - 54s/epoch - 273ms/step
Epoch 443/1000
2023-09-28 23:09:38.437 
Epoch 443/1000 
	 loss: 28.6300, MinusLogProbMetric: 28.6300, val_loss: 28.6297, val_MinusLogProbMetric: 28.6297

Epoch 443: val_loss did not improve from 28.51442
196/196 - 50s - loss: 28.6300 - MinusLogProbMetric: 28.6300 - val_loss: 28.6297 - val_MinusLogProbMetric: 28.6297 - lr: 5.0000e-04 - 50s/epoch - 258ms/step
Epoch 444/1000
2023-09-28 23:10:29.728 
Epoch 444/1000 
	 loss: 28.6498, MinusLogProbMetric: 28.6498, val_loss: 29.1534, val_MinusLogProbMetric: 29.1534

Epoch 444: val_loss did not improve from 28.51442
196/196 - 51s - loss: 28.6498 - MinusLogProbMetric: 28.6498 - val_loss: 29.1534 - val_MinusLogProbMetric: 29.1534 - lr: 5.0000e-04 - 51s/epoch - 262ms/step
Epoch 445/1000
2023-09-28 23:11:23.478 
Epoch 445/1000 
	 loss: 28.6954, MinusLogProbMetric: 28.6954, val_loss: 28.9897, val_MinusLogProbMetric: 28.9897

Epoch 445: val_loss did not improve from 28.51442
196/196 - 54s - loss: 28.6954 - MinusLogProbMetric: 28.6954 - val_loss: 28.9897 - val_MinusLogProbMetric: 28.9897 - lr: 5.0000e-04 - 54s/epoch - 274ms/step
Epoch 446/1000
2023-09-28 23:12:17.817 
Epoch 446/1000 
	 loss: 28.7059, MinusLogProbMetric: 28.7059, val_loss: 29.0978, val_MinusLogProbMetric: 29.0978

Epoch 446: val_loss did not improve from 28.51442
196/196 - 54s - loss: 28.7059 - MinusLogProbMetric: 28.7059 - val_loss: 29.0978 - val_MinusLogProbMetric: 29.0978 - lr: 5.0000e-04 - 54s/epoch - 277ms/step
Epoch 447/1000
2023-09-28 23:13:10.725 
Epoch 447/1000 
	 loss: 28.7157, MinusLogProbMetric: 28.7157, val_loss: 28.8607, val_MinusLogProbMetric: 28.8607

Epoch 447: val_loss did not improve from 28.51442
196/196 - 53s - loss: 28.7157 - MinusLogProbMetric: 28.7157 - val_loss: 28.8607 - val_MinusLogProbMetric: 28.8607 - lr: 5.0000e-04 - 53s/epoch - 270ms/step
Epoch 448/1000
2023-09-28 23:14:04.439 
Epoch 448/1000 
	 loss: 28.6464, MinusLogProbMetric: 28.6464, val_loss: 28.6441, val_MinusLogProbMetric: 28.6441

Epoch 448: val_loss did not improve from 28.51442
196/196 - 54s - loss: 28.6464 - MinusLogProbMetric: 28.6464 - val_loss: 28.6441 - val_MinusLogProbMetric: 28.6441 - lr: 5.0000e-04 - 54s/epoch - 274ms/step
Epoch 449/1000
2023-09-28 23:14:57.298 
Epoch 449/1000 
	 loss: 28.6621, MinusLogProbMetric: 28.6621, val_loss: 29.3499, val_MinusLogProbMetric: 29.3499

Epoch 449: val_loss did not improve from 28.51442
196/196 - 53s - loss: 28.6621 - MinusLogProbMetric: 28.6621 - val_loss: 29.3499 - val_MinusLogProbMetric: 29.3499 - lr: 5.0000e-04 - 53s/epoch - 270ms/step
Epoch 450/1000
2023-09-28 23:15:47.640 
Epoch 450/1000 
	 loss: 28.6380, MinusLogProbMetric: 28.6380, val_loss: 28.7839, val_MinusLogProbMetric: 28.7839

Epoch 450: val_loss did not improve from 28.51442
196/196 - 50s - loss: 28.6380 - MinusLogProbMetric: 28.6380 - val_loss: 28.7839 - val_MinusLogProbMetric: 28.7839 - lr: 5.0000e-04 - 50s/epoch - 257ms/step
Epoch 451/1000
2023-09-28 23:16:40.399 
Epoch 451/1000 
	 loss: 28.6528, MinusLogProbMetric: 28.6528, val_loss: 29.3080, val_MinusLogProbMetric: 29.3080

Epoch 451: val_loss did not improve from 28.51442
196/196 - 53s - loss: 28.6528 - MinusLogProbMetric: 28.6528 - val_loss: 29.3080 - val_MinusLogProbMetric: 29.3080 - lr: 5.0000e-04 - 53s/epoch - 269ms/step
Epoch 452/1000
2023-09-28 23:17:31.343 
Epoch 452/1000 
	 loss: 28.6401, MinusLogProbMetric: 28.6401, val_loss: 28.8401, val_MinusLogProbMetric: 28.8401

Epoch 452: val_loss did not improve from 28.51442
196/196 - 51s - loss: 28.6401 - MinusLogProbMetric: 28.6401 - val_loss: 28.8401 - val_MinusLogProbMetric: 28.8401 - lr: 5.0000e-04 - 51s/epoch - 260ms/step
Epoch 453/1000
2023-09-28 23:18:24.242 
Epoch 453/1000 
	 loss: 28.6168, MinusLogProbMetric: 28.6168, val_loss: 29.1453, val_MinusLogProbMetric: 29.1453

Epoch 453: val_loss did not improve from 28.51442
196/196 - 53s - loss: 28.6168 - MinusLogProbMetric: 28.6168 - val_loss: 29.1453 - val_MinusLogProbMetric: 29.1453 - lr: 5.0000e-04 - 53s/epoch - 270ms/step
Epoch 454/1000
2023-09-28 23:19:16.245 
Epoch 454/1000 
	 loss: 28.6206, MinusLogProbMetric: 28.6206, val_loss: 28.6788, val_MinusLogProbMetric: 28.6788

Epoch 454: val_loss did not improve from 28.51442
196/196 - 52s - loss: 28.6206 - MinusLogProbMetric: 28.6206 - val_loss: 28.6788 - val_MinusLogProbMetric: 28.6788 - lr: 5.0000e-04 - 52s/epoch - 265ms/step
Epoch 455/1000
2023-09-28 23:20:07.333 
Epoch 455/1000 
	 loss: 28.5793, MinusLogProbMetric: 28.5793, val_loss: 28.7476, val_MinusLogProbMetric: 28.7476

Epoch 455: val_loss did not improve from 28.51442
196/196 - 51s - loss: 28.5793 - MinusLogProbMetric: 28.5793 - val_loss: 28.7476 - val_MinusLogProbMetric: 28.7476 - lr: 5.0000e-04 - 51s/epoch - 261ms/step
Epoch 456/1000
2023-09-28 23:20:57.708 
Epoch 456/1000 
	 loss: 28.7014, MinusLogProbMetric: 28.7014, val_loss: 28.7533, val_MinusLogProbMetric: 28.7533

Epoch 456: val_loss did not improve from 28.51442
196/196 - 50s - loss: 28.7014 - MinusLogProbMetric: 28.7014 - val_loss: 28.7533 - val_MinusLogProbMetric: 28.7533 - lr: 5.0000e-04 - 50s/epoch - 257ms/step
Epoch 457/1000
2023-09-28 23:21:48.016 
Epoch 457/1000 
	 loss: 28.5540, MinusLogProbMetric: 28.5540, val_loss: 28.9810, val_MinusLogProbMetric: 28.9810

Epoch 457: val_loss did not improve from 28.51442
196/196 - 50s - loss: 28.5540 - MinusLogProbMetric: 28.5540 - val_loss: 28.9810 - val_MinusLogProbMetric: 28.9810 - lr: 5.0000e-04 - 50s/epoch - 257ms/step
Epoch 458/1000
2023-09-28 23:22:37.274 
Epoch 458/1000 
	 loss: 28.6161, MinusLogProbMetric: 28.6161, val_loss: 28.6375, val_MinusLogProbMetric: 28.6375

Epoch 458: val_loss did not improve from 28.51442
196/196 - 49s - loss: 28.6161 - MinusLogProbMetric: 28.6161 - val_loss: 28.6375 - val_MinusLogProbMetric: 28.6375 - lr: 5.0000e-04 - 49s/epoch - 251ms/step
Epoch 459/1000
2023-09-28 23:23:25.844 
Epoch 459/1000 
	 loss: 28.6110, MinusLogProbMetric: 28.6110, val_loss: 28.9226, val_MinusLogProbMetric: 28.9226

Epoch 459: val_loss did not improve from 28.51442
196/196 - 49s - loss: 28.6110 - MinusLogProbMetric: 28.6110 - val_loss: 28.9226 - val_MinusLogProbMetric: 28.9226 - lr: 5.0000e-04 - 49s/epoch - 248ms/step
Epoch 460/1000
2023-09-28 23:24:17.618 
Epoch 460/1000 
	 loss: 28.5440, MinusLogProbMetric: 28.5440, val_loss: 28.6718, val_MinusLogProbMetric: 28.6718

Epoch 460: val_loss did not improve from 28.51442
196/196 - 52s - loss: 28.5440 - MinusLogProbMetric: 28.5440 - val_loss: 28.6718 - val_MinusLogProbMetric: 28.6718 - lr: 5.0000e-04 - 52s/epoch - 264ms/step
Epoch 461/1000
2023-09-28 23:25:09.916 
Epoch 461/1000 
	 loss: 28.6092, MinusLogProbMetric: 28.6092, val_loss: 28.7458, val_MinusLogProbMetric: 28.7458

Epoch 461: val_loss did not improve from 28.51442
196/196 - 52s - loss: 28.6092 - MinusLogProbMetric: 28.6092 - val_loss: 28.7458 - val_MinusLogProbMetric: 28.7458 - lr: 5.0000e-04 - 52s/epoch - 267ms/step
Epoch 462/1000
2023-09-28 23:25:57.539 
Epoch 462/1000 
	 loss: 28.5576, MinusLogProbMetric: 28.5576, val_loss: 30.3589, val_MinusLogProbMetric: 30.3589

Epoch 462: val_loss did not improve from 28.51442
196/196 - 48s - loss: 28.5576 - MinusLogProbMetric: 28.5576 - val_loss: 30.3589 - val_MinusLogProbMetric: 30.3589 - lr: 5.0000e-04 - 48s/epoch - 243ms/step
Epoch 463/1000
2023-09-28 23:26:48.330 
Epoch 463/1000 
	 loss: 28.7608, MinusLogProbMetric: 28.7608, val_loss: 28.7192, val_MinusLogProbMetric: 28.7192

Epoch 463: val_loss did not improve from 28.51442
196/196 - 51s - loss: 28.7608 - MinusLogProbMetric: 28.7608 - val_loss: 28.7192 - val_MinusLogProbMetric: 28.7192 - lr: 5.0000e-04 - 51s/epoch - 259ms/step
Epoch 464/1000
2023-09-28 23:27:36.969 
Epoch 464/1000 
	 loss: 28.5777, MinusLogProbMetric: 28.5777, val_loss: 28.8172, val_MinusLogProbMetric: 28.8172

Epoch 464: val_loss did not improve from 28.51442
196/196 - 49s - loss: 28.5777 - MinusLogProbMetric: 28.5777 - val_loss: 28.8172 - val_MinusLogProbMetric: 28.8172 - lr: 5.0000e-04 - 49s/epoch - 248ms/step
Epoch 465/1000
2023-09-28 23:28:27.709 
Epoch 465/1000 
	 loss: 28.6706, MinusLogProbMetric: 28.6706, val_loss: 28.4567, val_MinusLogProbMetric: 28.4567

Epoch 465: val_loss improved from 28.51442 to 28.45669, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_323/weights/best_weights.h5
196/196 - 51s - loss: 28.6706 - MinusLogProbMetric: 28.6706 - val_loss: 28.4567 - val_MinusLogProbMetric: 28.4567 - lr: 5.0000e-04 - 51s/epoch - 262ms/step
Epoch 466/1000
2023-09-28 23:29:19.903 
Epoch 466/1000 
	 loss: 28.6004, MinusLogProbMetric: 28.6004, val_loss: 28.5607, val_MinusLogProbMetric: 28.5607

Epoch 466: val_loss did not improve from 28.45669
196/196 - 52s - loss: 28.6004 - MinusLogProbMetric: 28.6004 - val_loss: 28.5607 - val_MinusLogProbMetric: 28.5607 - lr: 5.0000e-04 - 52s/epoch - 263ms/step
Epoch 467/1000
2023-09-28 23:30:13.908 
Epoch 467/1000 
	 loss: 28.5639, MinusLogProbMetric: 28.5639, val_loss: 28.5900, val_MinusLogProbMetric: 28.5900

Epoch 467: val_loss did not improve from 28.45669
196/196 - 54s - loss: 28.5639 - MinusLogProbMetric: 28.5639 - val_loss: 28.5900 - val_MinusLogProbMetric: 28.5900 - lr: 5.0000e-04 - 54s/epoch - 275ms/step
Epoch 468/1000
2023-09-28 23:31:07.897 
Epoch 468/1000 
	 loss: 28.6770, MinusLogProbMetric: 28.6770, val_loss: 29.0445, val_MinusLogProbMetric: 29.0445

Epoch 468: val_loss did not improve from 28.45669
196/196 - 54s - loss: 28.6770 - MinusLogProbMetric: 28.6770 - val_loss: 29.0445 - val_MinusLogProbMetric: 29.0445 - lr: 5.0000e-04 - 54s/epoch - 275ms/step
Epoch 469/1000
2023-09-28 23:31:58.762 
Epoch 469/1000 
	 loss: 28.5904, MinusLogProbMetric: 28.5904, val_loss: 28.8525, val_MinusLogProbMetric: 28.8525

Epoch 469: val_loss did not improve from 28.45669
196/196 - 51s - loss: 28.5904 - MinusLogProbMetric: 28.5904 - val_loss: 28.8525 - val_MinusLogProbMetric: 28.8525 - lr: 5.0000e-04 - 51s/epoch - 259ms/step
Epoch 470/1000
2023-09-28 23:32:53.551 
Epoch 470/1000 
	 loss: 28.5651, MinusLogProbMetric: 28.5651, val_loss: 29.5101, val_MinusLogProbMetric: 29.5101

Epoch 470: val_loss did not improve from 28.45669
196/196 - 55s - loss: 28.5651 - MinusLogProbMetric: 28.5651 - val_loss: 29.5101 - val_MinusLogProbMetric: 29.5101 - lr: 5.0000e-04 - 55s/epoch - 279ms/step
Epoch 471/1000
2023-09-28 23:33:48.001 
Epoch 471/1000 
	 loss: 28.5872, MinusLogProbMetric: 28.5872, val_loss: 28.6198, val_MinusLogProbMetric: 28.6198

Epoch 471: val_loss did not improve from 28.45669
196/196 - 54s - loss: 28.5872 - MinusLogProbMetric: 28.5872 - val_loss: 28.6198 - val_MinusLogProbMetric: 28.6198 - lr: 5.0000e-04 - 54s/epoch - 278ms/step
Epoch 472/1000
2023-09-28 23:34:41.732 
Epoch 472/1000 
	 loss: 28.4804, MinusLogProbMetric: 28.4804, val_loss: 29.8993, val_MinusLogProbMetric: 29.8993

Epoch 472: val_loss did not improve from 28.45669
196/196 - 54s - loss: 28.4804 - MinusLogProbMetric: 28.4804 - val_loss: 29.8993 - val_MinusLogProbMetric: 29.8993 - lr: 5.0000e-04 - 54s/epoch - 274ms/step
Epoch 473/1000
2023-09-28 23:35:35.910 
Epoch 473/1000 
	 loss: 28.7920, MinusLogProbMetric: 28.7920, val_loss: 29.5160, val_MinusLogProbMetric: 29.5160

Epoch 473: val_loss did not improve from 28.45669
196/196 - 54s - loss: 28.7920 - MinusLogProbMetric: 28.7920 - val_loss: 29.5160 - val_MinusLogProbMetric: 29.5160 - lr: 5.0000e-04 - 54s/epoch - 276ms/step
Epoch 474/1000
2023-09-28 23:36:30.460 
Epoch 474/1000 
	 loss: 28.6285, MinusLogProbMetric: 28.6285, val_loss: 29.1596, val_MinusLogProbMetric: 29.1596

Epoch 474: val_loss did not improve from 28.45669
196/196 - 55s - loss: 28.6285 - MinusLogProbMetric: 28.6285 - val_loss: 29.1596 - val_MinusLogProbMetric: 29.1596 - lr: 5.0000e-04 - 55s/epoch - 278ms/step
Epoch 475/1000
2023-09-28 23:37:24.404 
Epoch 475/1000 
	 loss: 28.7321, MinusLogProbMetric: 28.7321, val_loss: 28.7330, val_MinusLogProbMetric: 28.7330

Epoch 475: val_loss did not improve from 28.45669
196/196 - 54s - loss: 28.7321 - MinusLogProbMetric: 28.7321 - val_loss: 28.7330 - val_MinusLogProbMetric: 28.7330 - lr: 5.0000e-04 - 54s/epoch - 275ms/step
Epoch 476/1000
2023-09-28 23:38:18.291 
Epoch 476/1000 
	 loss: 28.5951, MinusLogProbMetric: 28.5951, val_loss: 28.8469, val_MinusLogProbMetric: 28.8469

Epoch 476: val_loss did not improve from 28.45669
196/196 - 54s - loss: 28.5951 - MinusLogProbMetric: 28.5951 - val_loss: 28.8469 - val_MinusLogProbMetric: 28.8469 - lr: 5.0000e-04 - 54s/epoch - 275ms/step
Epoch 477/1000
2023-09-28 23:39:13.001 
Epoch 477/1000 
	 loss: 28.5187, MinusLogProbMetric: 28.5187, val_loss: 28.5836, val_MinusLogProbMetric: 28.5836

Epoch 477: val_loss did not improve from 28.45669
196/196 - 55s - loss: 28.5187 - MinusLogProbMetric: 28.5187 - val_loss: 28.5836 - val_MinusLogProbMetric: 28.5836 - lr: 5.0000e-04 - 55s/epoch - 279ms/step
Epoch 478/1000
2023-09-28 23:40:07.489 
Epoch 478/1000 
	 loss: 28.6360, MinusLogProbMetric: 28.6360, val_loss: 28.4629, val_MinusLogProbMetric: 28.4629

Epoch 478: val_loss did not improve from 28.45669
196/196 - 54s - loss: 28.6360 - MinusLogProbMetric: 28.6360 - val_loss: 28.4629 - val_MinusLogProbMetric: 28.4629 - lr: 5.0000e-04 - 54s/epoch - 278ms/step
Epoch 479/1000
2023-09-28 23:41:02.474 
Epoch 479/1000 
	 loss: 28.5306, MinusLogProbMetric: 28.5306, val_loss: 28.5372, val_MinusLogProbMetric: 28.5372

Epoch 479: val_loss did not improve from 28.45669
196/196 - 55s - loss: 28.5306 - MinusLogProbMetric: 28.5306 - val_loss: 28.5372 - val_MinusLogProbMetric: 28.5372 - lr: 5.0000e-04 - 55s/epoch - 280ms/step
Epoch 480/1000
2023-09-28 23:41:57.023 
Epoch 480/1000 
	 loss: 28.5840, MinusLogProbMetric: 28.5840, val_loss: 28.5542, val_MinusLogProbMetric: 28.5542

Epoch 480: val_loss did not improve from 28.45669
196/196 - 55s - loss: 28.5840 - MinusLogProbMetric: 28.5840 - val_loss: 28.5542 - val_MinusLogProbMetric: 28.5542 - lr: 5.0000e-04 - 55s/epoch - 278ms/step
Epoch 481/1000
2023-09-28 23:42:51.632 
Epoch 481/1000 
	 loss: 28.6610, MinusLogProbMetric: 28.6610, val_loss: 30.2107, val_MinusLogProbMetric: 30.2107

Epoch 481: val_loss did not improve from 28.45669
196/196 - 55s - loss: 28.6610 - MinusLogProbMetric: 28.6610 - val_loss: 30.2107 - val_MinusLogProbMetric: 30.2107 - lr: 5.0000e-04 - 55s/epoch - 279ms/step
Epoch 482/1000
2023-09-28 23:43:45.406 
Epoch 482/1000 
	 loss: 28.6776, MinusLogProbMetric: 28.6776, val_loss: 28.6757, val_MinusLogProbMetric: 28.6757

Epoch 482: val_loss did not improve from 28.45669
196/196 - 54s - loss: 28.6776 - MinusLogProbMetric: 28.6776 - val_loss: 28.6757 - val_MinusLogProbMetric: 28.6757 - lr: 5.0000e-04 - 54s/epoch - 274ms/step
Epoch 483/1000
2023-09-28 23:44:39.303 
Epoch 483/1000 
	 loss: 28.6015, MinusLogProbMetric: 28.6015, val_loss: 28.6830, val_MinusLogProbMetric: 28.6830

Epoch 483: val_loss did not improve from 28.45669
196/196 - 54s - loss: 28.6015 - MinusLogProbMetric: 28.6015 - val_loss: 28.6830 - val_MinusLogProbMetric: 28.6830 - lr: 5.0000e-04 - 54s/epoch - 275ms/step
Epoch 484/1000
2023-09-28 23:45:33.599 
Epoch 484/1000 
	 loss: 28.5870, MinusLogProbMetric: 28.5870, val_loss: 29.3495, val_MinusLogProbMetric: 29.3495

Epoch 484: val_loss did not improve from 28.45669
196/196 - 54s - loss: 28.5870 - MinusLogProbMetric: 28.5870 - val_loss: 29.3495 - val_MinusLogProbMetric: 29.3495 - lr: 5.0000e-04 - 54s/epoch - 277ms/step
Epoch 485/1000
2023-09-28 23:46:28.080 
Epoch 485/1000 
	 loss: 28.5687, MinusLogProbMetric: 28.5687, val_loss: 28.6346, val_MinusLogProbMetric: 28.6346

Epoch 485: val_loss did not improve from 28.45669
196/196 - 54s - loss: 28.5687 - MinusLogProbMetric: 28.5687 - val_loss: 28.6346 - val_MinusLogProbMetric: 28.6346 - lr: 5.0000e-04 - 54s/epoch - 278ms/step
Epoch 486/1000
2023-09-28 23:47:22.114 
Epoch 486/1000 
	 loss: 28.6568, MinusLogProbMetric: 28.6568, val_loss: 28.4767, val_MinusLogProbMetric: 28.4767

Epoch 486: val_loss did not improve from 28.45669
196/196 - 54s - loss: 28.6568 - MinusLogProbMetric: 28.6568 - val_loss: 28.4767 - val_MinusLogProbMetric: 28.4767 - lr: 5.0000e-04 - 54s/epoch - 276ms/step
Epoch 487/1000
2023-09-28 23:48:16.447 
Epoch 487/1000 
	 loss: 28.6064, MinusLogProbMetric: 28.6064, val_loss: 29.1309, val_MinusLogProbMetric: 29.1309

Epoch 487: val_loss did not improve from 28.45669
196/196 - 54s - loss: 28.6064 - MinusLogProbMetric: 28.6064 - val_loss: 29.1309 - val_MinusLogProbMetric: 29.1309 - lr: 5.0000e-04 - 54s/epoch - 277ms/step
Epoch 488/1000
2023-09-28 23:49:10.706 
Epoch 488/1000 
	 loss: 28.5963, MinusLogProbMetric: 28.5963, val_loss: 28.7492, val_MinusLogProbMetric: 28.7492

Epoch 488: val_loss did not improve from 28.45669
196/196 - 54s - loss: 28.5963 - MinusLogProbMetric: 28.5963 - val_loss: 28.7492 - val_MinusLogProbMetric: 28.7492 - lr: 5.0000e-04 - 54s/epoch - 277ms/step
Epoch 489/1000
2023-09-28 23:50:04.512 
Epoch 489/1000 
	 loss: 28.5630, MinusLogProbMetric: 28.5630, val_loss: 28.9128, val_MinusLogProbMetric: 28.9128

Epoch 489: val_loss did not improve from 28.45669
196/196 - 54s - loss: 28.5630 - MinusLogProbMetric: 28.5630 - val_loss: 28.9128 - val_MinusLogProbMetric: 28.9128 - lr: 5.0000e-04 - 54s/epoch - 274ms/step
Epoch 490/1000
2023-09-28 23:50:59.009 
Epoch 490/1000 
	 loss: 28.6234, MinusLogProbMetric: 28.6234, val_loss: 28.6647, val_MinusLogProbMetric: 28.6647

Epoch 490: val_loss did not improve from 28.45669
196/196 - 54s - loss: 28.6234 - MinusLogProbMetric: 28.6234 - val_loss: 28.6647 - val_MinusLogProbMetric: 28.6647 - lr: 5.0000e-04 - 54s/epoch - 278ms/step
Epoch 491/1000
2023-09-28 23:51:51.812 
Epoch 491/1000 
	 loss: 28.6188, MinusLogProbMetric: 28.6188, val_loss: 29.4685, val_MinusLogProbMetric: 29.4685

Epoch 491: val_loss did not improve from 28.45669
196/196 - 53s - loss: 28.6188 - MinusLogProbMetric: 28.6188 - val_loss: 29.4685 - val_MinusLogProbMetric: 29.4685 - lr: 5.0000e-04 - 53s/epoch - 269ms/step
Epoch 492/1000
2023-09-28 23:52:46.002 
Epoch 492/1000 
	 loss: 28.5876, MinusLogProbMetric: 28.5876, val_loss: 28.9118, val_MinusLogProbMetric: 28.9118

Epoch 492: val_loss did not improve from 28.45669
196/196 - 54s - loss: 28.5876 - MinusLogProbMetric: 28.5876 - val_loss: 28.9118 - val_MinusLogProbMetric: 28.9118 - lr: 5.0000e-04 - 54s/epoch - 276ms/step
Epoch 493/1000
2023-09-28 23:53:39.708 
Epoch 493/1000 
	 loss: 28.6283, MinusLogProbMetric: 28.6283, val_loss: 28.4485, val_MinusLogProbMetric: 28.4485

Epoch 493: val_loss improved from 28.45669 to 28.44851, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_323/weights/best_weights.h5
196/196 - 55s - loss: 28.6283 - MinusLogProbMetric: 28.6283 - val_loss: 28.4485 - val_MinusLogProbMetric: 28.4485 - lr: 5.0000e-04 - 55s/epoch - 278ms/step
Epoch 494/1000
2023-09-28 23:54:34.470 
Epoch 494/1000 
	 loss: 28.5144, MinusLogProbMetric: 28.5144, val_loss: 28.9308, val_MinusLogProbMetric: 28.9308

Epoch 494: val_loss did not improve from 28.44851
196/196 - 54s - loss: 28.5144 - MinusLogProbMetric: 28.5144 - val_loss: 28.9308 - val_MinusLogProbMetric: 28.9308 - lr: 5.0000e-04 - 54s/epoch - 275ms/step
Epoch 495/1000
2023-09-28 23:55:28.235 
Epoch 495/1000 
	 loss: 28.6155, MinusLogProbMetric: 28.6155, val_loss: 28.9868, val_MinusLogProbMetric: 28.9868

Epoch 495: val_loss did not improve from 28.44851
196/196 - 54s - loss: 28.6155 - MinusLogProbMetric: 28.6155 - val_loss: 28.9868 - val_MinusLogProbMetric: 28.9868 - lr: 5.0000e-04 - 54s/epoch - 274ms/step
Epoch 496/1000
2023-09-28 23:56:22.747 
Epoch 496/1000 
	 loss: 28.6160, MinusLogProbMetric: 28.6160, val_loss: 28.6905, val_MinusLogProbMetric: 28.6905

Epoch 496: val_loss did not improve from 28.44851
196/196 - 54s - loss: 28.6160 - MinusLogProbMetric: 28.6160 - val_loss: 28.6905 - val_MinusLogProbMetric: 28.6905 - lr: 5.0000e-04 - 54s/epoch - 278ms/step
Epoch 497/1000
2023-09-28 23:57:16.670 
Epoch 497/1000 
	 loss: 28.4960, MinusLogProbMetric: 28.4960, val_loss: 28.6780, val_MinusLogProbMetric: 28.6780

Epoch 497: val_loss did not improve from 28.44851
196/196 - 54s - loss: 28.4960 - MinusLogProbMetric: 28.4960 - val_loss: 28.6780 - val_MinusLogProbMetric: 28.6780 - lr: 5.0000e-04 - 54s/epoch - 275ms/step
Epoch 498/1000
2023-09-28 23:58:11.439 
Epoch 498/1000 
	 loss: 28.5190, MinusLogProbMetric: 28.5190, val_loss: 28.4986, val_MinusLogProbMetric: 28.4986

Epoch 498: val_loss did not improve from 28.44851
196/196 - 55s - loss: 28.5190 - MinusLogProbMetric: 28.5190 - val_loss: 28.4986 - val_MinusLogProbMetric: 28.4986 - lr: 5.0000e-04 - 55s/epoch - 279ms/step
Epoch 499/1000
2023-09-28 23:59:05.677 
Epoch 499/1000 
	 loss: 28.5886, MinusLogProbMetric: 28.5886, val_loss: 28.8232, val_MinusLogProbMetric: 28.8232

Epoch 499: val_loss did not improve from 28.44851
196/196 - 54s - loss: 28.5886 - MinusLogProbMetric: 28.5886 - val_loss: 28.8232 - val_MinusLogProbMetric: 28.8232 - lr: 5.0000e-04 - 54s/epoch - 277ms/step
Epoch 500/1000
2023-09-29 00:00:00.250 
Epoch 500/1000 
	 loss: 28.5389, MinusLogProbMetric: 28.5389, val_loss: 28.6026, val_MinusLogProbMetric: 28.6026

Epoch 500: val_loss did not improve from 28.44851
196/196 - 55s - loss: 28.5389 - MinusLogProbMetric: 28.5389 - val_loss: 28.6026 - val_MinusLogProbMetric: 28.6026 - lr: 5.0000e-04 - 55s/epoch - 278ms/step
Epoch 501/1000
2023-09-29 00:00:54.894 
Epoch 501/1000 
	 loss: 28.6352, MinusLogProbMetric: 28.6352, val_loss: 28.8711, val_MinusLogProbMetric: 28.8711

Epoch 501: val_loss did not improve from 28.44851
196/196 - 55s - loss: 28.6352 - MinusLogProbMetric: 28.6352 - val_loss: 28.8711 - val_MinusLogProbMetric: 28.8711 - lr: 5.0000e-04 - 55s/epoch - 279ms/step
Epoch 502/1000
2023-09-29 00:01:49.925 
Epoch 502/1000 
	 loss: 28.5361, MinusLogProbMetric: 28.5361, val_loss: 29.5471, val_MinusLogProbMetric: 29.5471

Epoch 502: val_loss did not improve from 28.44851
196/196 - 55s - loss: 28.5361 - MinusLogProbMetric: 28.5361 - val_loss: 29.5471 - val_MinusLogProbMetric: 29.5471 - lr: 5.0000e-04 - 55s/epoch - 281ms/step
Epoch 503/1000
2023-09-29 00:02:44.481 
Epoch 503/1000 
	 loss: 28.5477, MinusLogProbMetric: 28.5477, val_loss: 28.7571, val_MinusLogProbMetric: 28.7571

Epoch 503: val_loss did not improve from 28.44851
196/196 - 55s - loss: 28.5477 - MinusLogProbMetric: 28.5477 - val_loss: 28.7571 - val_MinusLogProbMetric: 28.7571 - lr: 5.0000e-04 - 55s/epoch - 278ms/step
Epoch 504/1000
2023-09-29 00:03:39.136 
Epoch 504/1000 
	 loss: 28.5638, MinusLogProbMetric: 28.5638, val_loss: 29.4569, val_MinusLogProbMetric: 29.4569

Epoch 504: val_loss did not improve from 28.44851
196/196 - 55s - loss: 28.5638 - MinusLogProbMetric: 28.5638 - val_loss: 29.4569 - val_MinusLogProbMetric: 29.4569 - lr: 5.0000e-04 - 55s/epoch - 279ms/step
Epoch 505/1000
2023-09-29 00:04:33.977 
Epoch 505/1000 
	 loss: 28.5616, MinusLogProbMetric: 28.5616, val_loss: 28.7486, val_MinusLogProbMetric: 28.7486

Epoch 505: val_loss did not improve from 28.44851
196/196 - 55s - loss: 28.5616 - MinusLogProbMetric: 28.5616 - val_loss: 28.7486 - val_MinusLogProbMetric: 28.7486 - lr: 5.0000e-04 - 55s/epoch - 280ms/step
Epoch 506/1000
2023-09-29 00:05:27.886 
Epoch 506/1000 
	 loss: 28.5414, MinusLogProbMetric: 28.5414, val_loss: 28.5223, val_MinusLogProbMetric: 28.5223

Epoch 506: val_loss did not improve from 28.44851
196/196 - 54s - loss: 28.5414 - MinusLogProbMetric: 28.5414 - val_loss: 28.5223 - val_MinusLogProbMetric: 28.5223 - lr: 5.0000e-04 - 54s/epoch - 275ms/step
Epoch 507/1000
2023-09-29 00:06:22.538 
Epoch 507/1000 
	 loss: 28.5115, MinusLogProbMetric: 28.5115, val_loss: 28.9669, val_MinusLogProbMetric: 28.9669

Epoch 507: val_loss did not improve from 28.44851
196/196 - 55s - loss: 28.5115 - MinusLogProbMetric: 28.5115 - val_loss: 28.9669 - val_MinusLogProbMetric: 28.9669 - lr: 5.0000e-04 - 55s/epoch - 279ms/step
Epoch 508/1000
2023-09-29 00:07:17.167 
Epoch 508/1000 
	 loss: 28.5786, MinusLogProbMetric: 28.5786, val_loss: 28.6434, val_MinusLogProbMetric: 28.6434

Epoch 508: val_loss did not improve from 28.44851
196/196 - 55s - loss: 28.5786 - MinusLogProbMetric: 28.5786 - val_loss: 28.6434 - val_MinusLogProbMetric: 28.6434 - lr: 5.0000e-04 - 55s/epoch - 279ms/step
Epoch 509/1000
2023-09-29 00:08:10.054 
Epoch 509/1000 
	 loss: 28.6430, MinusLogProbMetric: 28.6430, val_loss: 28.5662, val_MinusLogProbMetric: 28.5662

Epoch 509: val_loss did not improve from 28.44851
196/196 - 53s - loss: 28.6430 - MinusLogProbMetric: 28.6430 - val_loss: 28.5662 - val_MinusLogProbMetric: 28.5662 - lr: 5.0000e-04 - 53s/epoch - 270ms/step
Epoch 510/1000
2023-09-29 00:09:04.059 
Epoch 510/1000 
	 loss: 28.5423, MinusLogProbMetric: 28.5423, val_loss: 28.5278, val_MinusLogProbMetric: 28.5278

Epoch 510: val_loss did not improve from 28.44851
196/196 - 54s - loss: 28.5423 - MinusLogProbMetric: 28.5423 - val_loss: 28.5278 - val_MinusLogProbMetric: 28.5278 - lr: 5.0000e-04 - 54s/epoch - 276ms/step
Epoch 511/1000
2023-09-29 00:09:58.383 
Epoch 511/1000 
	 loss: 28.6196, MinusLogProbMetric: 28.6196, val_loss: 29.1246, val_MinusLogProbMetric: 29.1246

Epoch 511: val_loss did not improve from 28.44851
196/196 - 54s - loss: 28.6196 - MinusLogProbMetric: 28.6196 - val_loss: 29.1246 - val_MinusLogProbMetric: 29.1246 - lr: 5.0000e-04 - 54s/epoch - 277ms/step
Epoch 512/1000
2023-09-29 00:10:52.077 
Epoch 512/1000 
	 loss: 28.6855, MinusLogProbMetric: 28.6855, val_loss: 29.0248, val_MinusLogProbMetric: 29.0248

Epoch 512: val_loss did not improve from 28.44851
196/196 - 54s - loss: 28.6855 - MinusLogProbMetric: 28.6855 - val_loss: 29.0248 - val_MinusLogProbMetric: 29.0248 - lr: 5.0000e-04 - 54s/epoch - 274ms/step
Epoch 513/1000
2023-09-29 00:11:46.857 
Epoch 513/1000 
	 loss: 28.5782, MinusLogProbMetric: 28.5782, val_loss: 28.7504, val_MinusLogProbMetric: 28.7504

Epoch 513: val_loss did not improve from 28.44851
196/196 - 55s - loss: 28.5782 - MinusLogProbMetric: 28.5782 - val_loss: 28.7504 - val_MinusLogProbMetric: 28.7504 - lr: 5.0000e-04 - 55s/epoch - 279ms/step
Epoch 514/1000
2023-09-29 00:12:41.499 
Epoch 514/1000 
	 loss: 28.5603, MinusLogProbMetric: 28.5603, val_loss: 28.7404, val_MinusLogProbMetric: 28.7404

Epoch 514: val_loss did not improve from 28.44851
196/196 - 55s - loss: 28.5603 - MinusLogProbMetric: 28.5603 - val_loss: 28.7404 - val_MinusLogProbMetric: 28.7404 - lr: 5.0000e-04 - 55s/epoch - 279ms/step
Epoch 515/1000
2023-09-29 00:13:36.016 
Epoch 515/1000 
	 loss: 28.5794, MinusLogProbMetric: 28.5794, val_loss: 28.8824, val_MinusLogProbMetric: 28.8824

Epoch 515: val_loss did not improve from 28.44851
196/196 - 55s - loss: 28.5794 - MinusLogProbMetric: 28.5794 - val_loss: 28.8824 - val_MinusLogProbMetric: 28.8824 - lr: 5.0000e-04 - 55s/epoch - 278ms/step
Epoch 516/1000
2023-09-29 00:14:30.131 
Epoch 516/1000 
	 loss: 28.5963, MinusLogProbMetric: 28.5963, val_loss: 28.4360, val_MinusLogProbMetric: 28.4360

Epoch 516: val_loss improved from 28.44851 to 28.43599, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_323/weights/best_weights.h5
196/196 - 55s - loss: 28.5963 - MinusLogProbMetric: 28.5963 - val_loss: 28.4360 - val_MinusLogProbMetric: 28.4360 - lr: 5.0000e-04 - 55s/epoch - 281ms/step
Epoch 517/1000
2023-09-29 00:15:25.763 
Epoch 517/1000 
	 loss: 28.4966, MinusLogProbMetric: 28.4966, val_loss: 28.9766, val_MinusLogProbMetric: 28.9766

Epoch 517: val_loss did not improve from 28.43599
196/196 - 55s - loss: 28.4966 - MinusLogProbMetric: 28.4966 - val_loss: 28.9766 - val_MinusLogProbMetric: 28.9766 - lr: 5.0000e-04 - 55s/epoch - 279ms/step
Epoch 518/1000
2023-09-29 00:16:19.952 
Epoch 518/1000 
	 loss: 28.5490, MinusLogProbMetric: 28.5490, val_loss: 28.9976, val_MinusLogProbMetric: 28.9976

Epoch 518: val_loss did not improve from 28.43599
196/196 - 54s - loss: 28.5490 - MinusLogProbMetric: 28.5490 - val_loss: 28.9976 - val_MinusLogProbMetric: 28.9976 - lr: 5.0000e-04 - 54s/epoch - 276ms/step
Epoch 519/1000
2023-09-29 00:17:14.758 
Epoch 519/1000 
	 loss: 28.6175, MinusLogProbMetric: 28.6175, val_loss: 28.9601, val_MinusLogProbMetric: 28.9601

Epoch 519: val_loss did not improve from 28.43599
196/196 - 55s - loss: 28.6175 - MinusLogProbMetric: 28.6175 - val_loss: 28.9601 - val_MinusLogProbMetric: 28.9601 - lr: 5.0000e-04 - 55s/epoch - 280ms/step
Epoch 520/1000
2023-09-29 00:18:07.759 
Epoch 520/1000 
	 loss: 28.5846, MinusLogProbMetric: 28.5846, val_loss: 28.6250, val_MinusLogProbMetric: 28.6250

Epoch 520: val_loss did not improve from 28.43599
196/196 - 53s - loss: 28.5846 - MinusLogProbMetric: 28.5846 - val_loss: 28.6250 - val_MinusLogProbMetric: 28.6250 - lr: 5.0000e-04 - 53s/epoch - 270ms/step
Epoch 521/1000
2023-09-29 00:19:01.246 
Epoch 521/1000 
	 loss: 28.6048, MinusLogProbMetric: 28.6048, val_loss: 29.1921, val_MinusLogProbMetric: 29.1921

Epoch 521: val_loss did not improve from 28.43599
196/196 - 53s - loss: 28.6048 - MinusLogProbMetric: 28.6048 - val_loss: 29.1921 - val_MinusLogProbMetric: 29.1921 - lr: 5.0000e-04 - 53s/epoch - 273ms/step
Epoch 522/1000
2023-09-29 00:19:55.289 
Epoch 522/1000 
	 loss: 28.4644, MinusLogProbMetric: 28.4644, val_loss: 28.4487, val_MinusLogProbMetric: 28.4487

Epoch 522: val_loss did not improve from 28.43599
196/196 - 54s - loss: 28.4644 - MinusLogProbMetric: 28.4644 - val_loss: 28.4487 - val_MinusLogProbMetric: 28.4487 - lr: 5.0000e-04 - 54s/epoch - 276ms/step
Epoch 523/1000
2023-09-29 00:20:50.053 
Epoch 523/1000 
	 loss: 28.7164, MinusLogProbMetric: 28.7164, val_loss: 28.6276, val_MinusLogProbMetric: 28.6276

Epoch 523: val_loss did not improve from 28.43599
196/196 - 55s - loss: 28.7164 - MinusLogProbMetric: 28.7164 - val_loss: 28.6276 - val_MinusLogProbMetric: 28.6276 - lr: 5.0000e-04 - 55s/epoch - 279ms/step
Epoch 524/1000
2023-09-29 00:21:42.677 
Epoch 524/1000 
	 loss: 28.4860, MinusLogProbMetric: 28.4860, val_loss: 28.6341, val_MinusLogProbMetric: 28.6341

Epoch 524: val_loss did not improve from 28.43599
196/196 - 53s - loss: 28.4860 - MinusLogProbMetric: 28.4860 - val_loss: 28.6341 - val_MinusLogProbMetric: 28.6341 - lr: 5.0000e-04 - 53s/epoch - 268ms/step
Epoch 525/1000
2023-09-29 00:22:36.963 
Epoch 525/1000 
	 loss: 28.5744, MinusLogProbMetric: 28.5744, val_loss: 28.5088, val_MinusLogProbMetric: 28.5088

Epoch 525: val_loss did not improve from 28.43599
196/196 - 54s - loss: 28.5744 - MinusLogProbMetric: 28.5744 - val_loss: 28.5088 - val_MinusLogProbMetric: 28.5088 - lr: 5.0000e-04 - 54s/epoch - 277ms/step
Epoch 526/1000
2023-09-29 00:23:29.076 
Epoch 526/1000 
	 loss: 28.5482, MinusLogProbMetric: 28.5482, val_loss: 28.5677, val_MinusLogProbMetric: 28.5677

Epoch 526: val_loss did not improve from 28.43599
196/196 - 52s - loss: 28.5482 - MinusLogProbMetric: 28.5482 - val_loss: 28.5677 - val_MinusLogProbMetric: 28.5677 - lr: 5.0000e-04 - 52s/epoch - 266ms/step
Epoch 527/1000
2023-09-29 00:24:22.404 
Epoch 527/1000 
	 loss: 28.5203, MinusLogProbMetric: 28.5203, val_loss: 30.0256, val_MinusLogProbMetric: 30.0256

Epoch 527: val_loss did not improve from 28.43599
196/196 - 53s - loss: 28.5203 - MinusLogProbMetric: 28.5203 - val_loss: 30.0256 - val_MinusLogProbMetric: 30.0256 - lr: 5.0000e-04 - 53s/epoch - 272ms/step
Epoch 528/1000
2023-09-29 00:25:14.197 
Epoch 528/1000 
	 loss: 28.6169, MinusLogProbMetric: 28.6169, val_loss: 28.7168, val_MinusLogProbMetric: 28.7168

Epoch 528: val_loss did not improve from 28.43599
196/196 - 52s - loss: 28.6169 - MinusLogProbMetric: 28.6169 - val_loss: 28.7168 - val_MinusLogProbMetric: 28.7168 - lr: 5.0000e-04 - 52s/epoch - 264ms/step
Epoch 529/1000
2023-09-29 00:26:07.235 
Epoch 529/1000 
	 loss: 28.5197, MinusLogProbMetric: 28.5197, val_loss: 29.0724, val_MinusLogProbMetric: 29.0724

Epoch 529: val_loss did not improve from 28.43599
196/196 - 53s - loss: 28.5197 - MinusLogProbMetric: 28.5197 - val_loss: 29.0724 - val_MinusLogProbMetric: 29.0724 - lr: 5.0000e-04 - 53s/epoch - 271ms/step
Epoch 530/1000
2023-09-29 00:26:59.205 
Epoch 530/1000 
	 loss: 28.6356, MinusLogProbMetric: 28.6356, val_loss: 28.6589, val_MinusLogProbMetric: 28.6589

Epoch 530: val_loss did not improve from 28.43599
196/196 - 52s - loss: 28.6356 - MinusLogProbMetric: 28.6356 - val_loss: 28.6589 - val_MinusLogProbMetric: 28.6589 - lr: 5.0000e-04 - 52s/epoch - 265ms/step
Epoch 531/1000
2023-09-29 00:27:52.256 
Epoch 531/1000 
	 loss: 28.5078, MinusLogProbMetric: 28.5078, val_loss: 28.8618, val_MinusLogProbMetric: 28.8618

Epoch 531: val_loss did not improve from 28.43599
196/196 - 53s - loss: 28.5078 - MinusLogProbMetric: 28.5078 - val_loss: 28.8618 - val_MinusLogProbMetric: 28.8618 - lr: 5.0000e-04 - 53s/epoch - 271ms/step
Epoch 532/1000
2023-09-29 00:28:44.658 
Epoch 532/1000 
	 loss: 28.5435, MinusLogProbMetric: 28.5435, val_loss: 28.6291, val_MinusLogProbMetric: 28.6291

Epoch 532: val_loss did not improve from 28.43599
196/196 - 52s - loss: 28.5435 - MinusLogProbMetric: 28.5435 - val_loss: 28.6291 - val_MinusLogProbMetric: 28.6291 - lr: 5.0000e-04 - 52s/epoch - 267ms/step
Epoch 533/1000
2023-09-29 00:29:37.008 
Epoch 533/1000 
	 loss: 28.5382, MinusLogProbMetric: 28.5382, val_loss: 28.6588, val_MinusLogProbMetric: 28.6588

Epoch 533: val_loss did not improve from 28.43599
196/196 - 52s - loss: 28.5382 - MinusLogProbMetric: 28.5382 - val_loss: 28.6588 - val_MinusLogProbMetric: 28.6588 - lr: 5.0000e-04 - 52s/epoch - 267ms/step
Epoch 534/1000
2023-09-29 00:30:28.631 
Epoch 534/1000 
	 loss: 28.5633, MinusLogProbMetric: 28.5633, val_loss: 28.7391, val_MinusLogProbMetric: 28.7391

Epoch 534: val_loss did not improve from 28.43599
196/196 - 52s - loss: 28.5633 - MinusLogProbMetric: 28.5633 - val_loss: 28.7391 - val_MinusLogProbMetric: 28.7391 - lr: 5.0000e-04 - 52s/epoch - 263ms/step
Epoch 535/1000
2023-09-29 00:31:21.722 
Epoch 535/1000 
	 loss: 28.5315, MinusLogProbMetric: 28.5315, val_loss: 28.7035, val_MinusLogProbMetric: 28.7035

Epoch 535: val_loss did not improve from 28.43599
196/196 - 53s - loss: 28.5315 - MinusLogProbMetric: 28.5315 - val_loss: 28.7035 - val_MinusLogProbMetric: 28.7035 - lr: 5.0000e-04 - 53s/epoch - 271ms/step
Epoch 536/1000
2023-09-29 00:32:14.537 
Epoch 536/1000 
	 loss: 28.4979, MinusLogProbMetric: 28.4979, val_loss: 28.5960, val_MinusLogProbMetric: 28.5960

Epoch 536: val_loss did not improve from 28.43599
196/196 - 53s - loss: 28.4979 - MinusLogProbMetric: 28.4979 - val_loss: 28.5960 - val_MinusLogProbMetric: 28.5960 - lr: 5.0000e-04 - 53s/epoch - 269ms/step
Epoch 537/1000
2023-09-29 00:33:05.943 
Epoch 537/1000 
	 loss: 28.5155, MinusLogProbMetric: 28.5155, val_loss: 28.5431, val_MinusLogProbMetric: 28.5431

Epoch 537: val_loss did not improve from 28.43599
196/196 - 51s - loss: 28.5155 - MinusLogProbMetric: 28.5155 - val_loss: 28.5431 - val_MinusLogProbMetric: 28.5431 - lr: 5.0000e-04 - 51s/epoch - 262ms/step
Epoch 538/1000
2023-09-29 00:33:56.916 
Epoch 538/1000 
	 loss: 28.5456, MinusLogProbMetric: 28.5456, val_loss: 28.7688, val_MinusLogProbMetric: 28.7688

Epoch 538: val_loss did not improve from 28.43599
196/196 - 51s - loss: 28.5456 - MinusLogProbMetric: 28.5456 - val_loss: 28.7688 - val_MinusLogProbMetric: 28.7688 - lr: 5.0000e-04 - 51s/epoch - 260ms/step
Epoch 539/1000
2023-09-29 00:34:50.072 
Epoch 539/1000 
	 loss: 28.5644, MinusLogProbMetric: 28.5644, val_loss: 28.7638, val_MinusLogProbMetric: 28.7638

Epoch 539: val_loss did not improve from 28.43599
196/196 - 53s - loss: 28.5644 - MinusLogProbMetric: 28.5644 - val_loss: 28.7638 - val_MinusLogProbMetric: 28.7638 - lr: 5.0000e-04 - 53s/epoch - 271ms/step
Epoch 540/1000
2023-09-29 00:35:44.082 
Epoch 540/1000 
	 loss: 28.5088, MinusLogProbMetric: 28.5088, val_loss: 29.4070, val_MinusLogProbMetric: 29.4070

Epoch 540: val_loss did not improve from 28.43599
196/196 - 54s - loss: 28.5088 - MinusLogProbMetric: 28.5088 - val_loss: 29.4070 - val_MinusLogProbMetric: 29.4070 - lr: 5.0000e-04 - 54s/epoch - 276ms/step
Epoch 541/1000
2023-09-29 00:36:37.736 
Epoch 541/1000 
	 loss: 28.5849, MinusLogProbMetric: 28.5849, val_loss: 28.5728, val_MinusLogProbMetric: 28.5728

Epoch 541: val_loss did not improve from 28.43599
196/196 - 54s - loss: 28.5849 - MinusLogProbMetric: 28.5849 - val_loss: 28.5728 - val_MinusLogProbMetric: 28.5728 - lr: 5.0000e-04 - 54s/epoch - 274ms/step
Epoch 542/1000
2023-09-29 00:37:31.006 
Epoch 542/1000 
	 loss: 28.5443, MinusLogProbMetric: 28.5443, val_loss: 29.1724, val_MinusLogProbMetric: 29.1724

Epoch 542: val_loss did not improve from 28.43599
196/196 - 53s - loss: 28.5443 - MinusLogProbMetric: 28.5443 - val_loss: 29.1724 - val_MinusLogProbMetric: 29.1724 - lr: 5.0000e-04 - 53s/epoch - 272ms/step
Epoch 543/1000
2023-09-29 00:38:22.060 
Epoch 543/1000 
	 loss: 28.5468, MinusLogProbMetric: 28.5468, val_loss: 28.7430, val_MinusLogProbMetric: 28.7430

Epoch 543: val_loss did not improve from 28.43599
196/196 - 51s - loss: 28.5468 - MinusLogProbMetric: 28.5468 - val_loss: 28.7430 - val_MinusLogProbMetric: 28.7430 - lr: 5.0000e-04 - 51s/epoch - 260ms/step
Epoch 544/1000
2023-09-29 00:39:13.547 
Epoch 544/1000 
	 loss: 28.5008, MinusLogProbMetric: 28.5008, val_loss: 28.5326, val_MinusLogProbMetric: 28.5326

Epoch 544: val_loss did not improve from 28.43599
196/196 - 51s - loss: 28.5008 - MinusLogProbMetric: 28.5008 - val_loss: 28.5326 - val_MinusLogProbMetric: 28.5326 - lr: 5.0000e-04 - 51s/epoch - 263ms/step
Epoch 545/1000
2023-09-29 00:40:08.035 
Epoch 545/1000 
	 loss: 28.5760, MinusLogProbMetric: 28.5760, val_loss: 28.9282, val_MinusLogProbMetric: 28.9282

Epoch 545: val_loss did not improve from 28.43599
196/196 - 54s - loss: 28.5760 - MinusLogProbMetric: 28.5760 - val_loss: 28.9282 - val_MinusLogProbMetric: 28.9282 - lr: 5.0000e-04 - 54s/epoch - 278ms/step
Epoch 546/1000
2023-09-29 00:41:01.414 
Epoch 546/1000 
	 loss: 28.5593, MinusLogProbMetric: 28.5593, val_loss: 28.5981, val_MinusLogProbMetric: 28.5981

Epoch 546: val_loss did not improve from 28.43599
196/196 - 53s - loss: 28.5593 - MinusLogProbMetric: 28.5593 - val_loss: 28.5981 - val_MinusLogProbMetric: 28.5981 - lr: 5.0000e-04 - 53s/epoch - 272ms/step
Epoch 547/1000
2023-09-29 00:41:52.816 
Epoch 547/1000 
	 loss: 28.5207, MinusLogProbMetric: 28.5207, val_loss: 28.5296, val_MinusLogProbMetric: 28.5296

Epoch 547: val_loss did not improve from 28.43599
196/196 - 51s - loss: 28.5207 - MinusLogProbMetric: 28.5207 - val_loss: 28.5296 - val_MinusLogProbMetric: 28.5296 - lr: 5.0000e-04 - 51s/epoch - 262ms/step
Epoch 548/1000
2023-09-29 00:42:45.247 
Epoch 548/1000 
	 loss: 28.6622, MinusLogProbMetric: 28.6622, val_loss: 28.7695, val_MinusLogProbMetric: 28.7695

Epoch 548: val_loss did not improve from 28.43599
196/196 - 52s - loss: 28.6622 - MinusLogProbMetric: 28.6622 - val_loss: 28.7695 - val_MinusLogProbMetric: 28.7695 - lr: 5.0000e-04 - 52s/epoch - 267ms/step
Epoch 549/1000
2023-09-29 00:43:38.391 
Epoch 549/1000 
	 loss: 28.5434, MinusLogProbMetric: 28.5434, val_loss: 28.6266, val_MinusLogProbMetric: 28.6266

Epoch 549: val_loss did not improve from 28.43599
196/196 - 53s - loss: 28.5434 - MinusLogProbMetric: 28.5434 - val_loss: 28.6266 - val_MinusLogProbMetric: 28.6266 - lr: 5.0000e-04 - 53s/epoch - 271ms/step
Epoch 550/1000
2023-09-29 00:44:30.537 
Epoch 550/1000 
	 loss: 28.5069, MinusLogProbMetric: 28.5069, val_loss: 28.5739, val_MinusLogProbMetric: 28.5739

Epoch 550: val_loss did not improve from 28.43599
196/196 - 52s - loss: 28.5069 - MinusLogProbMetric: 28.5069 - val_loss: 28.5739 - val_MinusLogProbMetric: 28.5739 - lr: 5.0000e-04 - 52s/epoch - 266ms/step
Epoch 551/1000
2023-09-29 00:45:18.796 
Epoch 551/1000 
	 loss: 28.5096, MinusLogProbMetric: 28.5096, val_loss: 29.4127, val_MinusLogProbMetric: 29.4127

Epoch 551: val_loss did not improve from 28.43599
196/196 - 48s - loss: 28.5096 - MinusLogProbMetric: 28.5096 - val_loss: 29.4127 - val_MinusLogProbMetric: 29.4127 - lr: 5.0000e-04 - 48s/epoch - 246ms/step
Epoch 552/1000
2023-09-29 00:46:02.997 
Epoch 552/1000 
	 loss: 28.4631, MinusLogProbMetric: 28.4631, val_loss: 28.3619, val_MinusLogProbMetric: 28.3619

Epoch 552: val_loss improved from 28.43599 to 28.36186, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_323/weights/best_weights.h5
196/196 - 45s - loss: 28.4631 - MinusLogProbMetric: 28.4631 - val_loss: 28.3619 - val_MinusLogProbMetric: 28.3619 - lr: 5.0000e-04 - 45s/epoch - 231ms/step
Epoch 553/1000
2023-09-29 00:46:47.825 
Epoch 553/1000 
	 loss: 28.6270, MinusLogProbMetric: 28.6270, val_loss: 28.4804, val_MinusLogProbMetric: 28.4804

Epoch 553: val_loss did not improve from 28.36186
196/196 - 44s - loss: 28.6270 - MinusLogProbMetric: 28.6270 - val_loss: 28.4804 - val_MinusLogProbMetric: 28.4804 - lr: 5.0000e-04 - 44s/epoch - 223ms/step
Epoch 554/1000
2023-09-29 00:47:42.229 
Epoch 554/1000 
	 loss: 28.4726, MinusLogProbMetric: 28.4726, val_loss: 28.6236, val_MinusLogProbMetric: 28.6236

Epoch 554: val_loss did not improve from 28.36186
196/196 - 54s - loss: 28.4726 - MinusLogProbMetric: 28.4726 - val_loss: 28.6236 - val_MinusLogProbMetric: 28.6236 - lr: 5.0000e-04 - 54s/epoch - 278ms/step
Epoch 555/1000
2023-09-29 00:48:37.251 
Epoch 555/1000 
	 loss: 28.4732, MinusLogProbMetric: 28.4732, val_loss: 28.5541, val_MinusLogProbMetric: 28.5541

Epoch 555: val_loss did not improve from 28.36186
196/196 - 55s - loss: 28.4732 - MinusLogProbMetric: 28.4732 - val_loss: 28.5541 - val_MinusLogProbMetric: 28.5541 - lr: 5.0000e-04 - 55s/epoch - 281ms/step
Epoch 556/1000
2023-09-29 00:49:32.294 
Epoch 556/1000 
	 loss: 28.5135, MinusLogProbMetric: 28.5135, val_loss: 28.5858, val_MinusLogProbMetric: 28.5858

Epoch 556: val_loss did not improve from 28.36186
196/196 - 55s - loss: 28.5135 - MinusLogProbMetric: 28.5135 - val_loss: 28.5858 - val_MinusLogProbMetric: 28.5858 - lr: 5.0000e-04 - 55s/epoch - 281ms/step
Epoch 557/1000
2023-09-29 00:50:26.462 
Epoch 557/1000 
	 loss: 28.5136, MinusLogProbMetric: 28.5136, val_loss: 28.6836, val_MinusLogProbMetric: 28.6836

Epoch 557: val_loss did not improve from 28.36186
196/196 - 54s - loss: 28.5136 - MinusLogProbMetric: 28.5136 - val_loss: 28.6836 - val_MinusLogProbMetric: 28.6836 - lr: 5.0000e-04 - 54s/epoch - 276ms/step
Epoch 558/1000
2023-09-29 00:51:21.099 
Epoch 558/1000 
	 loss: 28.6081, MinusLogProbMetric: 28.6081, val_loss: 28.6345, val_MinusLogProbMetric: 28.6345

Epoch 558: val_loss did not improve from 28.36186
196/196 - 55s - loss: 28.6081 - MinusLogProbMetric: 28.6081 - val_loss: 28.6345 - val_MinusLogProbMetric: 28.6345 - lr: 5.0000e-04 - 55s/epoch - 279ms/step
Epoch 559/1000
2023-09-29 00:52:14.404 
Epoch 559/1000 
	 loss: 28.5365, MinusLogProbMetric: 28.5365, val_loss: 28.4809, val_MinusLogProbMetric: 28.4809

Epoch 559: val_loss did not improve from 28.36186
196/196 - 53s - loss: 28.5365 - MinusLogProbMetric: 28.5365 - val_loss: 28.4809 - val_MinusLogProbMetric: 28.4809 - lr: 5.0000e-04 - 53s/epoch - 272ms/step
Epoch 560/1000
2023-09-29 00:53:08.479 
Epoch 560/1000 
	 loss: 28.5549, MinusLogProbMetric: 28.5549, val_loss: 28.8182, val_MinusLogProbMetric: 28.8182

Epoch 560: val_loss did not improve from 28.36186
196/196 - 54s - loss: 28.5549 - MinusLogProbMetric: 28.5549 - val_loss: 28.8182 - val_MinusLogProbMetric: 28.8182 - lr: 5.0000e-04 - 54s/epoch - 276ms/step
Epoch 561/1000
2023-09-29 00:54:01.211 
Epoch 561/1000 
	 loss: 28.4776, MinusLogProbMetric: 28.4776, val_loss: 29.5245, val_MinusLogProbMetric: 29.5245

Epoch 561: val_loss did not improve from 28.36186
196/196 - 53s - loss: 28.4776 - MinusLogProbMetric: 28.4776 - val_loss: 29.5245 - val_MinusLogProbMetric: 29.5245 - lr: 5.0000e-04 - 53s/epoch - 269ms/step
Epoch 562/1000
2023-09-29 00:54:54.586 
Epoch 562/1000 
	 loss: 28.5697, MinusLogProbMetric: 28.5697, val_loss: 28.6379, val_MinusLogProbMetric: 28.6379

Epoch 562: val_loss did not improve from 28.36186
196/196 - 53s - loss: 28.5697 - MinusLogProbMetric: 28.5697 - val_loss: 28.6379 - val_MinusLogProbMetric: 28.6379 - lr: 5.0000e-04 - 53s/epoch - 272ms/step
Epoch 563/1000
2023-09-29 00:55:47.529 
Epoch 563/1000 
	 loss: 28.4316, MinusLogProbMetric: 28.4316, val_loss: 28.4819, val_MinusLogProbMetric: 28.4819

Epoch 563: val_loss did not improve from 28.36186
196/196 - 53s - loss: 28.4316 - MinusLogProbMetric: 28.4316 - val_loss: 28.4819 - val_MinusLogProbMetric: 28.4819 - lr: 5.0000e-04 - 53s/epoch - 270ms/step
Epoch 564/1000
2023-09-29 00:56:40.718 
Epoch 564/1000 
	 loss: 28.5623, MinusLogProbMetric: 28.5623, val_loss: 28.5570, val_MinusLogProbMetric: 28.5570

Epoch 564: val_loss did not improve from 28.36186
196/196 - 53s - loss: 28.5623 - MinusLogProbMetric: 28.5623 - val_loss: 28.5570 - val_MinusLogProbMetric: 28.5570 - lr: 5.0000e-04 - 53s/epoch - 271ms/step
Epoch 565/1000
2023-09-29 00:57:34.695 
Epoch 565/1000 
	 loss: 28.5009, MinusLogProbMetric: 28.5009, val_loss: 28.5865, val_MinusLogProbMetric: 28.5865

Epoch 565: val_loss did not improve from 28.36186
196/196 - 54s - loss: 28.5009 - MinusLogProbMetric: 28.5009 - val_loss: 28.5865 - val_MinusLogProbMetric: 28.5865 - lr: 5.0000e-04 - 54s/epoch - 275ms/step
Epoch 566/1000
2023-09-29 00:58:27.494 
Epoch 566/1000 
	 loss: 28.5164, MinusLogProbMetric: 28.5164, val_loss: 29.2765, val_MinusLogProbMetric: 29.2765

Epoch 566: val_loss did not improve from 28.36186
196/196 - 53s - loss: 28.5164 - MinusLogProbMetric: 28.5164 - val_loss: 29.2765 - val_MinusLogProbMetric: 29.2765 - lr: 5.0000e-04 - 53s/epoch - 269ms/step
Epoch 567/1000
2023-09-29 00:59:19.012 
Epoch 567/1000 
	 loss: 28.5010, MinusLogProbMetric: 28.5010, val_loss: 28.9215, val_MinusLogProbMetric: 28.9215

Epoch 567: val_loss did not improve from 28.36186
196/196 - 52s - loss: 28.5010 - MinusLogProbMetric: 28.5010 - val_loss: 28.9215 - val_MinusLogProbMetric: 28.9215 - lr: 5.0000e-04 - 52s/epoch - 263ms/step
Epoch 568/1000
2023-09-29 01:00:08.974 
Epoch 568/1000 
	 loss: 28.5533, MinusLogProbMetric: 28.5533, val_loss: 28.5451, val_MinusLogProbMetric: 28.5451

Epoch 568: val_loss did not improve from 28.36186
196/196 - 50s - loss: 28.5533 - MinusLogProbMetric: 28.5533 - val_loss: 28.5451 - val_MinusLogProbMetric: 28.5451 - lr: 5.0000e-04 - 50s/epoch - 255ms/step
Epoch 569/1000
2023-09-29 01:01:02.945 
Epoch 569/1000 
	 loss: 28.5901, MinusLogProbMetric: 28.5901, val_loss: 29.0818, val_MinusLogProbMetric: 29.0818

Epoch 569: val_loss did not improve from 28.36186
196/196 - 54s - loss: 28.5901 - MinusLogProbMetric: 28.5901 - val_loss: 29.0818 - val_MinusLogProbMetric: 29.0818 - lr: 5.0000e-04 - 54s/epoch - 275ms/step
Epoch 570/1000
2023-09-29 01:01:55.785 
Epoch 570/1000 
	 loss: 28.5547, MinusLogProbMetric: 28.5547, val_loss: 28.5680, val_MinusLogProbMetric: 28.5680

Epoch 570: val_loss did not improve from 28.36186
196/196 - 53s - loss: 28.5547 - MinusLogProbMetric: 28.5547 - val_loss: 28.5680 - val_MinusLogProbMetric: 28.5680 - lr: 5.0000e-04 - 53s/epoch - 270ms/step
Epoch 571/1000
2023-09-29 01:02:50.827 
Epoch 571/1000 
	 loss: 28.4970, MinusLogProbMetric: 28.4970, val_loss: 28.6823, val_MinusLogProbMetric: 28.6823

Epoch 571: val_loss did not improve from 28.36186
196/196 - 55s - loss: 28.4970 - MinusLogProbMetric: 28.4970 - val_loss: 28.6823 - val_MinusLogProbMetric: 28.6823 - lr: 5.0000e-04 - 55s/epoch - 281ms/step
Epoch 572/1000
2023-09-29 01:03:45.714 
Epoch 572/1000 
	 loss: 28.4925, MinusLogProbMetric: 28.4925, val_loss: 29.1549, val_MinusLogProbMetric: 29.1549

Epoch 572: val_loss did not improve from 28.36186
196/196 - 55s - loss: 28.4925 - MinusLogProbMetric: 28.4925 - val_loss: 29.1549 - val_MinusLogProbMetric: 29.1549 - lr: 5.0000e-04 - 55s/epoch - 280ms/step
Epoch 573/1000
2023-09-29 01:04:39.877 
Epoch 573/1000 
	 loss: 28.5903, MinusLogProbMetric: 28.5903, val_loss: 28.9832, val_MinusLogProbMetric: 28.9832

Epoch 573: val_loss did not improve from 28.36186
196/196 - 54s - loss: 28.5903 - MinusLogProbMetric: 28.5903 - val_loss: 28.9832 - val_MinusLogProbMetric: 28.9832 - lr: 5.0000e-04 - 54s/epoch - 276ms/step
Epoch 574/1000
2023-09-29 01:05:34.800 
Epoch 574/1000 
	 loss: 28.4792, MinusLogProbMetric: 28.4792, val_loss: 28.5838, val_MinusLogProbMetric: 28.5838

Epoch 574: val_loss did not improve from 28.36186
196/196 - 55s - loss: 28.4792 - MinusLogProbMetric: 28.4792 - val_loss: 28.5838 - val_MinusLogProbMetric: 28.5838 - lr: 5.0000e-04 - 55s/epoch - 280ms/step
Epoch 575/1000
2023-09-29 01:06:29.576 
Epoch 575/1000 
	 loss: 28.5494, MinusLogProbMetric: 28.5494, val_loss: 29.0368, val_MinusLogProbMetric: 29.0368

Epoch 575: val_loss did not improve from 28.36186
196/196 - 55s - loss: 28.5494 - MinusLogProbMetric: 28.5494 - val_loss: 29.0368 - val_MinusLogProbMetric: 29.0368 - lr: 5.0000e-04 - 55s/epoch - 279ms/step
Epoch 576/1000
2023-09-29 01:07:23.705 
Epoch 576/1000 
	 loss: 28.4883, MinusLogProbMetric: 28.4883, val_loss: 28.6905, val_MinusLogProbMetric: 28.6905

Epoch 576: val_loss did not improve from 28.36186
196/196 - 54s - loss: 28.4883 - MinusLogProbMetric: 28.4883 - val_loss: 28.6905 - val_MinusLogProbMetric: 28.6905 - lr: 5.0000e-04 - 54s/epoch - 276ms/step
Epoch 577/1000
2023-09-29 01:08:17.455 
Epoch 577/1000 
	 loss: 28.5314, MinusLogProbMetric: 28.5314, val_loss: 28.4661, val_MinusLogProbMetric: 28.4661

Epoch 577: val_loss did not improve from 28.36186
196/196 - 54s - loss: 28.5314 - MinusLogProbMetric: 28.5314 - val_loss: 28.4661 - val_MinusLogProbMetric: 28.4661 - lr: 5.0000e-04 - 54s/epoch - 274ms/step
Epoch 578/1000
2023-09-29 01:09:09.338 
Epoch 578/1000 
	 loss: 28.5778, MinusLogProbMetric: 28.5778, val_loss: 28.5566, val_MinusLogProbMetric: 28.5566

Epoch 578: val_loss did not improve from 28.36186
196/196 - 52s - loss: 28.5778 - MinusLogProbMetric: 28.5778 - val_loss: 28.5566 - val_MinusLogProbMetric: 28.5566 - lr: 5.0000e-04 - 52s/epoch - 265ms/step
Epoch 579/1000
2023-09-29 01:09:59.789 
Epoch 579/1000 
	 loss: 28.5373, MinusLogProbMetric: 28.5373, val_loss: 28.7550, val_MinusLogProbMetric: 28.7550

Epoch 579: val_loss did not improve from 28.36186
196/196 - 50s - loss: 28.5373 - MinusLogProbMetric: 28.5373 - val_loss: 28.7550 - val_MinusLogProbMetric: 28.7550 - lr: 5.0000e-04 - 50s/epoch - 257ms/step
Epoch 580/1000
2023-09-29 01:10:50.842 
Epoch 580/1000 
	 loss: 28.4933, MinusLogProbMetric: 28.4933, val_loss: 29.8207, val_MinusLogProbMetric: 29.8207

Epoch 580: val_loss did not improve from 28.36186
196/196 - 51s - loss: 28.4933 - MinusLogProbMetric: 28.4933 - val_loss: 29.8207 - val_MinusLogProbMetric: 29.8207 - lr: 5.0000e-04 - 51s/epoch - 260ms/step
Epoch 581/1000
2023-09-29 01:11:40.893 
Epoch 581/1000 
	 loss: 28.5849, MinusLogProbMetric: 28.5849, val_loss: 28.4272, val_MinusLogProbMetric: 28.4272

Epoch 581: val_loss did not improve from 28.36186
196/196 - 50s - loss: 28.5849 - MinusLogProbMetric: 28.5849 - val_loss: 28.4272 - val_MinusLogProbMetric: 28.4272 - lr: 5.0000e-04 - 50s/epoch - 255ms/step
Epoch 582/1000
2023-09-29 01:12:30.624 
Epoch 582/1000 
	 loss: 28.4601, MinusLogProbMetric: 28.4601, val_loss: 28.8237, val_MinusLogProbMetric: 28.8237

Epoch 582: val_loss did not improve from 28.36186
196/196 - 50s - loss: 28.4601 - MinusLogProbMetric: 28.4601 - val_loss: 28.8237 - val_MinusLogProbMetric: 28.8237 - lr: 5.0000e-04 - 50s/epoch - 254ms/step
Epoch 583/1000
2023-09-29 01:13:20.150 
Epoch 583/1000 
	 loss: 28.6072, MinusLogProbMetric: 28.6072, val_loss: 28.6131, val_MinusLogProbMetric: 28.6131

Epoch 583: val_loss did not improve from 28.36186
196/196 - 50s - loss: 28.6072 - MinusLogProbMetric: 28.6072 - val_loss: 28.6131 - val_MinusLogProbMetric: 28.6131 - lr: 5.0000e-04 - 50s/epoch - 253ms/step
Epoch 584/1000
2023-09-29 01:14:11.304 
Epoch 584/1000 
	 loss: 28.5230, MinusLogProbMetric: 28.5230, val_loss: 28.7666, val_MinusLogProbMetric: 28.7666

Epoch 584: val_loss did not improve from 28.36186
196/196 - 51s - loss: 28.5230 - MinusLogProbMetric: 28.5230 - val_loss: 28.7666 - val_MinusLogProbMetric: 28.7666 - lr: 5.0000e-04 - 51s/epoch - 261ms/step
Epoch 585/1000
2023-09-29 01:15:02.607 
Epoch 585/1000 
	 loss: 28.5839, MinusLogProbMetric: 28.5839, val_loss: 28.5276, val_MinusLogProbMetric: 28.5276

Epoch 585: val_loss did not improve from 28.36186
196/196 - 51s - loss: 28.5839 - MinusLogProbMetric: 28.5839 - val_loss: 28.5276 - val_MinusLogProbMetric: 28.5276 - lr: 5.0000e-04 - 51s/epoch - 262ms/step
Epoch 586/1000
2023-09-29 01:15:51.982 
Epoch 586/1000 
	 loss: 28.4466, MinusLogProbMetric: 28.4466, val_loss: 28.7987, val_MinusLogProbMetric: 28.7987

Epoch 586: val_loss did not improve from 28.36186
196/196 - 49s - loss: 28.4466 - MinusLogProbMetric: 28.4466 - val_loss: 28.7987 - val_MinusLogProbMetric: 28.7987 - lr: 5.0000e-04 - 49s/epoch - 252ms/step
Epoch 587/1000
2023-09-29 01:16:42.017 
Epoch 587/1000 
	 loss: 28.4789, MinusLogProbMetric: 28.4789, val_loss: 28.5732, val_MinusLogProbMetric: 28.5732

Epoch 587: val_loss did not improve from 28.36186
196/196 - 50s - loss: 28.4789 - MinusLogProbMetric: 28.4789 - val_loss: 28.5732 - val_MinusLogProbMetric: 28.5732 - lr: 5.0000e-04 - 50s/epoch - 255ms/step
Epoch 588/1000
2023-09-29 01:17:32.438 
Epoch 588/1000 
	 loss: 28.4019, MinusLogProbMetric: 28.4019, val_loss: 28.6200, val_MinusLogProbMetric: 28.6200

Epoch 588: val_loss did not improve from 28.36186
196/196 - 50s - loss: 28.4019 - MinusLogProbMetric: 28.4019 - val_loss: 28.6200 - val_MinusLogProbMetric: 28.6200 - lr: 5.0000e-04 - 50s/epoch - 257ms/step
Epoch 589/1000
2023-09-29 01:18:22.288 
Epoch 589/1000 
	 loss: 28.5071, MinusLogProbMetric: 28.5071, val_loss: 28.9977, val_MinusLogProbMetric: 28.9977

Epoch 589: val_loss did not improve from 28.36186
196/196 - 50s - loss: 28.5071 - MinusLogProbMetric: 28.5071 - val_loss: 28.9977 - val_MinusLogProbMetric: 28.9977 - lr: 5.0000e-04 - 50s/epoch - 254ms/step
Epoch 590/1000
2023-09-29 01:19:13.694 
Epoch 590/1000 
	 loss: 28.4035, MinusLogProbMetric: 28.4035, val_loss: 28.4081, val_MinusLogProbMetric: 28.4081

Epoch 590: val_loss did not improve from 28.36186
196/196 - 51s - loss: 28.4035 - MinusLogProbMetric: 28.4035 - val_loss: 28.4081 - val_MinusLogProbMetric: 28.4081 - lr: 5.0000e-04 - 51s/epoch - 262ms/step
Epoch 591/1000
2023-09-29 01:20:04.043 
Epoch 591/1000 
	 loss: 28.6953, MinusLogProbMetric: 28.6953, val_loss: 28.7005, val_MinusLogProbMetric: 28.7005

Epoch 591: val_loss did not improve from 28.36186
196/196 - 50s - loss: 28.6953 - MinusLogProbMetric: 28.6953 - val_loss: 28.7005 - val_MinusLogProbMetric: 28.7005 - lr: 5.0000e-04 - 50s/epoch - 257ms/step
Epoch 592/1000
2023-09-29 01:20:53.467 
Epoch 592/1000 
	 loss: 28.5498, MinusLogProbMetric: 28.5498, val_loss: 28.5765, val_MinusLogProbMetric: 28.5765

Epoch 592: val_loss did not improve from 28.36186
196/196 - 49s - loss: 28.5498 - MinusLogProbMetric: 28.5498 - val_loss: 28.5765 - val_MinusLogProbMetric: 28.5765 - lr: 5.0000e-04 - 49s/epoch - 252ms/step
Epoch 593/1000
2023-09-29 01:21:42.992 
Epoch 593/1000 
	 loss: 28.4748, MinusLogProbMetric: 28.4748, val_loss: 28.3883, val_MinusLogProbMetric: 28.3883

Epoch 593: val_loss did not improve from 28.36186
196/196 - 50s - loss: 28.4748 - MinusLogProbMetric: 28.4748 - val_loss: 28.3883 - val_MinusLogProbMetric: 28.3883 - lr: 5.0000e-04 - 50s/epoch - 253ms/step
Epoch 594/1000
2023-09-29 01:22:32.660 
Epoch 594/1000 
	 loss: 28.5385, MinusLogProbMetric: 28.5385, val_loss: 28.7853, val_MinusLogProbMetric: 28.7853

Epoch 594: val_loss did not improve from 28.36186
196/196 - 50s - loss: 28.5385 - MinusLogProbMetric: 28.5385 - val_loss: 28.7853 - val_MinusLogProbMetric: 28.7853 - lr: 5.0000e-04 - 50s/epoch - 253ms/step
Epoch 595/1000
2023-09-29 01:23:25.906 
Epoch 595/1000 
	 loss: 28.4453, MinusLogProbMetric: 28.4453, val_loss: 28.8767, val_MinusLogProbMetric: 28.8767

Epoch 595: val_loss did not improve from 28.36186
196/196 - 53s - loss: 28.4453 - MinusLogProbMetric: 28.4453 - val_loss: 28.8767 - val_MinusLogProbMetric: 28.8767 - lr: 5.0000e-04 - 53s/epoch - 272ms/step
Epoch 596/1000
2023-09-29 01:24:15.596 
Epoch 596/1000 
	 loss: 28.5103, MinusLogProbMetric: 28.5103, val_loss: 28.9688, val_MinusLogProbMetric: 28.9688

Epoch 596: val_loss did not improve from 28.36186
196/196 - 50s - loss: 28.5103 - MinusLogProbMetric: 28.5103 - val_loss: 28.9688 - val_MinusLogProbMetric: 28.9688 - lr: 5.0000e-04 - 50s/epoch - 253ms/step
Epoch 597/1000
2023-09-29 01:25:07.913 
Epoch 597/1000 
	 loss: 28.5389, MinusLogProbMetric: 28.5389, val_loss: 28.5017, val_MinusLogProbMetric: 28.5017

Epoch 597: val_loss did not improve from 28.36186
196/196 - 52s - loss: 28.5389 - MinusLogProbMetric: 28.5389 - val_loss: 28.5017 - val_MinusLogProbMetric: 28.5017 - lr: 5.0000e-04 - 52s/epoch - 267ms/step
Epoch 598/1000
2023-09-29 01:25:58.839 
Epoch 598/1000 
	 loss: 28.5221, MinusLogProbMetric: 28.5221, val_loss: 29.1438, val_MinusLogProbMetric: 29.1438

Epoch 598: val_loss did not improve from 28.36186
196/196 - 51s - loss: 28.5221 - MinusLogProbMetric: 28.5221 - val_loss: 29.1438 - val_MinusLogProbMetric: 29.1438 - lr: 5.0000e-04 - 51s/epoch - 260ms/step
Epoch 599/1000
2023-09-29 01:26:48.177 
Epoch 599/1000 
	 loss: 28.5128, MinusLogProbMetric: 28.5128, val_loss: 28.9154, val_MinusLogProbMetric: 28.9154

Epoch 599: val_loss did not improve from 28.36186
196/196 - 49s - loss: 28.5128 - MinusLogProbMetric: 28.5128 - val_loss: 28.9154 - val_MinusLogProbMetric: 28.9154 - lr: 5.0000e-04 - 49s/epoch - 252ms/step
Epoch 600/1000
2023-09-29 01:27:38.679 
Epoch 600/1000 
	 loss: 28.4934, MinusLogProbMetric: 28.4934, val_loss: 28.5385, val_MinusLogProbMetric: 28.5385

Epoch 600: val_loss did not improve from 28.36186
196/196 - 50s - loss: 28.4934 - MinusLogProbMetric: 28.4934 - val_loss: 28.5385 - val_MinusLogProbMetric: 28.5385 - lr: 5.0000e-04 - 50s/epoch - 258ms/step
Epoch 601/1000
2023-09-29 01:28:27.926 
Epoch 601/1000 
	 loss: 28.4314, MinusLogProbMetric: 28.4314, val_loss: 28.4359, val_MinusLogProbMetric: 28.4359

Epoch 601: val_loss did not improve from 28.36186
196/196 - 49s - loss: 28.4314 - MinusLogProbMetric: 28.4314 - val_loss: 28.4359 - val_MinusLogProbMetric: 28.4359 - lr: 5.0000e-04 - 49s/epoch - 251ms/step
Epoch 602/1000
2023-09-29 01:29:18.626 
Epoch 602/1000 
	 loss: 28.4732, MinusLogProbMetric: 28.4732, val_loss: 28.8096, val_MinusLogProbMetric: 28.8096

Epoch 602: val_loss did not improve from 28.36186
196/196 - 51s - loss: 28.4732 - MinusLogProbMetric: 28.4732 - val_loss: 28.8096 - val_MinusLogProbMetric: 28.8096 - lr: 5.0000e-04 - 51s/epoch - 259ms/step
Epoch 603/1000
2023-09-29 01:30:08.104 
Epoch 603/1000 
	 loss: 28.0616, MinusLogProbMetric: 28.0616, val_loss: 28.4561, val_MinusLogProbMetric: 28.4561

Epoch 603: val_loss did not improve from 28.36186
196/196 - 49s - loss: 28.0616 - MinusLogProbMetric: 28.0616 - val_loss: 28.4561 - val_MinusLogProbMetric: 28.4561 - lr: 2.5000e-04 - 49s/epoch - 252ms/step
Epoch 604/1000
2023-09-29 01:30:58.003 
Epoch 604/1000 
	 loss: 28.0586, MinusLogProbMetric: 28.0586, val_loss: 28.2855, val_MinusLogProbMetric: 28.2855

Epoch 604: val_loss improved from 28.36186 to 28.28554, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_323/weights/best_weights.h5
196/196 - 51s - loss: 28.0586 - MinusLogProbMetric: 28.0586 - val_loss: 28.2855 - val_MinusLogProbMetric: 28.2855 - lr: 2.5000e-04 - 51s/epoch - 260ms/step
Epoch 605/1000
2023-09-29 01:31:50.604 
Epoch 605/1000 
	 loss: 28.0581, MinusLogProbMetric: 28.0581, val_loss: 28.4519, val_MinusLogProbMetric: 28.4519

Epoch 605: val_loss did not improve from 28.28554
196/196 - 52s - loss: 28.0581 - MinusLogProbMetric: 28.0581 - val_loss: 28.4519 - val_MinusLogProbMetric: 28.4519 - lr: 2.5000e-04 - 52s/epoch - 263ms/step
Epoch 606/1000
2023-09-29 01:32:41.946 
Epoch 606/1000 
	 loss: 28.0742, MinusLogProbMetric: 28.0742, val_loss: 28.2229, val_MinusLogProbMetric: 28.2229

Epoch 606: val_loss improved from 28.28554 to 28.22290, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_323/weights/best_weights.h5
196/196 - 52s - loss: 28.0742 - MinusLogProbMetric: 28.0742 - val_loss: 28.2229 - val_MinusLogProbMetric: 28.2229 - lr: 2.5000e-04 - 52s/epoch - 266ms/step
Epoch 607/1000
2023-09-29 01:33:32.441 
Epoch 607/1000 
	 loss: 28.0672, MinusLogProbMetric: 28.0672, val_loss: 28.3002, val_MinusLogProbMetric: 28.3002

Epoch 607: val_loss did not improve from 28.22290
196/196 - 50s - loss: 28.0672 - MinusLogProbMetric: 28.0672 - val_loss: 28.3002 - val_MinusLogProbMetric: 28.3002 - lr: 2.5000e-04 - 50s/epoch - 254ms/step
Epoch 608/1000
2023-09-29 01:34:22.826 
Epoch 608/1000 
	 loss: 28.0576, MinusLogProbMetric: 28.0576, val_loss: 28.2664, val_MinusLogProbMetric: 28.2664

Epoch 608: val_loss did not improve from 28.22290
196/196 - 50s - loss: 28.0576 - MinusLogProbMetric: 28.0576 - val_loss: 28.2664 - val_MinusLogProbMetric: 28.2664 - lr: 2.5000e-04 - 50s/epoch - 257ms/step
Epoch 609/1000
2023-09-29 01:35:13.638 
Epoch 609/1000 
	 loss: 28.0752, MinusLogProbMetric: 28.0752, val_loss: 28.4031, val_MinusLogProbMetric: 28.4031

Epoch 609: val_loss did not improve from 28.22290
196/196 - 51s - loss: 28.0752 - MinusLogProbMetric: 28.0752 - val_loss: 28.4031 - val_MinusLogProbMetric: 28.4031 - lr: 2.5000e-04 - 51s/epoch - 259ms/step
Epoch 610/1000
2023-09-29 01:36:03.019 
Epoch 610/1000 
	 loss: 28.0466, MinusLogProbMetric: 28.0466, val_loss: 28.3526, val_MinusLogProbMetric: 28.3526

Epoch 610: val_loss did not improve from 28.22290
196/196 - 49s - loss: 28.0466 - MinusLogProbMetric: 28.0466 - val_loss: 28.3526 - val_MinusLogProbMetric: 28.3526 - lr: 2.5000e-04 - 49s/epoch - 252ms/step
Epoch 611/1000
2023-09-29 01:36:54.694 
Epoch 611/1000 
	 loss: 28.1090, MinusLogProbMetric: 28.1090, val_loss: 28.3124, val_MinusLogProbMetric: 28.3124

Epoch 611: val_loss did not improve from 28.22290
196/196 - 52s - loss: 28.1090 - MinusLogProbMetric: 28.1090 - val_loss: 28.3124 - val_MinusLogProbMetric: 28.3124 - lr: 2.5000e-04 - 52s/epoch - 264ms/step
Epoch 612/1000
2023-09-29 01:37:44.512 
Epoch 612/1000 
	 loss: 28.0840, MinusLogProbMetric: 28.0840, val_loss: 28.5711, val_MinusLogProbMetric: 28.5711

Epoch 612: val_loss did not improve from 28.22290
196/196 - 50s - loss: 28.0840 - MinusLogProbMetric: 28.0840 - val_loss: 28.5711 - val_MinusLogProbMetric: 28.5711 - lr: 2.5000e-04 - 50s/epoch - 254ms/step
Epoch 613/1000
2023-09-29 01:38:35.184 
Epoch 613/1000 
	 loss: 28.2067, MinusLogProbMetric: 28.2067, val_loss: 28.3419, val_MinusLogProbMetric: 28.3419

Epoch 613: val_loss did not improve from 28.22290
196/196 - 51s - loss: 28.2067 - MinusLogProbMetric: 28.2067 - val_loss: 28.3419 - val_MinusLogProbMetric: 28.3419 - lr: 2.5000e-04 - 51s/epoch - 258ms/step
Epoch 614/1000
2023-09-29 01:39:25.770 
Epoch 614/1000 
	 loss: 28.0669, MinusLogProbMetric: 28.0669, val_loss: 28.2566, val_MinusLogProbMetric: 28.2566

Epoch 614: val_loss did not improve from 28.22290
196/196 - 51s - loss: 28.0669 - MinusLogProbMetric: 28.0669 - val_loss: 28.2566 - val_MinusLogProbMetric: 28.2566 - lr: 2.5000e-04 - 51s/epoch - 258ms/step
Epoch 615/1000
2023-09-29 01:40:15.989 
Epoch 615/1000 
	 loss: 28.1012, MinusLogProbMetric: 28.1012, val_loss: 28.3721, val_MinusLogProbMetric: 28.3721

Epoch 615: val_loss did not improve from 28.22290
196/196 - 50s - loss: 28.1012 - MinusLogProbMetric: 28.1012 - val_loss: 28.3721 - val_MinusLogProbMetric: 28.3721 - lr: 2.5000e-04 - 50s/epoch - 256ms/step
Epoch 616/1000
2023-09-29 01:41:08.986 
Epoch 616/1000 
	 loss: 28.0697, MinusLogProbMetric: 28.0697, val_loss: 28.3037, val_MinusLogProbMetric: 28.3037

Epoch 616: val_loss did not improve from 28.22290
196/196 - 53s - loss: 28.0697 - MinusLogProbMetric: 28.0697 - val_loss: 28.3037 - val_MinusLogProbMetric: 28.3037 - lr: 2.5000e-04 - 53s/epoch - 270ms/step
Epoch 617/1000
2023-09-29 01:42:00.563 
Epoch 617/1000 
	 loss: 28.0819, MinusLogProbMetric: 28.0819, val_loss: 28.3638, val_MinusLogProbMetric: 28.3638

Epoch 617: val_loss did not improve from 28.22290
196/196 - 52s - loss: 28.0819 - MinusLogProbMetric: 28.0819 - val_loss: 28.3638 - val_MinusLogProbMetric: 28.3638 - lr: 2.5000e-04 - 52s/epoch - 263ms/step
Epoch 618/1000
2023-09-29 01:42:51.335 
Epoch 618/1000 
	 loss: 28.0932, MinusLogProbMetric: 28.0932, val_loss: 28.2254, val_MinusLogProbMetric: 28.2254

Epoch 618: val_loss did not improve from 28.22290
196/196 - 51s - loss: 28.0932 - MinusLogProbMetric: 28.0932 - val_loss: 28.2254 - val_MinusLogProbMetric: 28.2254 - lr: 2.5000e-04 - 51s/epoch - 259ms/step
Epoch 619/1000
2023-09-29 01:43:41.642 
Epoch 619/1000 
	 loss: 28.0346, MinusLogProbMetric: 28.0346, val_loss: 28.3322, val_MinusLogProbMetric: 28.3322

Epoch 619: val_loss did not improve from 28.22290
196/196 - 50s - loss: 28.0346 - MinusLogProbMetric: 28.0346 - val_loss: 28.3322 - val_MinusLogProbMetric: 28.3322 - lr: 2.5000e-04 - 50s/epoch - 257ms/step
Epoch 620/1000
2023-09-29 01:44:33.873 
Epoch 620/1000 
	 loss: 28.0530, MinusLogProbMetric: 28.0530, val_loss: 28.3359, val_MinusLogProbMetric: 28.3359

Epoch 620: val_loss did not improve from 28.22290
196/196 - 52s - loss: 28.0530 - MinusLogProbMetric: 28.0530 - val_loss: 28.3359 - val_MinusLogProbMetric: 28.3359 - lr: 2.5000e-04 - 52s/epoch - 266ms/step
Epoch 621/1000
2023-09-29 01:45:25.471 
Epoch 621/1000 
	 loss: 28.0438, MinusLogProbMetric: 28.0438, val_loss: 28.5594, val_MinusLogProbMetric: 28.5594

Epoch 621: val_loss did not improve from 28.22290
196/196 - 52s - loss: 28.0438 - MinusLogProbMetric: 28.0438 - val_loss: 28.5594 - val_MinusLogProbMetric: 28.5594 - lr: 2.5000e-04 - 52s/epoch - 263ms/step
Epoch 622/1000
2023-09-29 01:46:15.898 
Epoch 622/1000 
	 loss: 28.0481, MinusLogProbMetric: 28.0481, val_loss: 28.3677, val_MinusLogProbMetric: 28.3677

Epoch 622: val_loss did not improve from 28.22290
196/196 - 50s - loss: 28.0481 - MinusLogProbMetric: 28.0481 - val_loss: 28.3677 - val_MinusLogProbMetric: 28.3677 - lr: 2.5000e-04 - 50s/epoch - 257ms/step
Epoch 623/1000
2023-09-29 01:47:06.886 
Epoch 623/1000 
	 loss: 28.0632, MinusLogProbMetric: 28.0632, val_loss: 28.2352, val_MinusLogProbMetric: 28.2352

Epoch 623: val_loss did not improve from 28.22290
196/196 - 51s - loss: 28.0632 - MinusLogProbMetric: 28.0632 - val_loss: 28.2352 - val_MinusLogProbMetric: 28.2352 - lr: 2.5000e-04 - 51s/epoch - 260ms/step
Epoch 624/1000
2023-09-29 01:47:58.676 
Epoch 624/1000 
	 loss: 28.0413, MinusLogProbMetric: 28.0413, val_loss: 28.1911, val_MinusLogProbMetric: 28.1911

Epoch 624: val_loss improved from 28.22290 to 28.19110, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_323/weights/best_weights.h5
196/196 - 52s - loss: 28.0413 - MinusLogProbMetric: 28.0413 - val_loss: 28.1911 - val_MinusLogProbMetric: 28.1911 - lr: 2.5000e-04 - 52s/epoch - 267ms/step
Epoch 625/1000
2023-09-29 01:48:51.866 
Epoch 625/1000 
	 loss: 28.0213, MinusLogProbMetric: 28.0213, val_loss: 28.3445, val_MinusLogProbMetric: 28.3445

Epoch 625: val_loss did not improve from 28.19110
196/196 - 53s - loss: 28.0213 - MinusLogProbMetric: 28.0213 - val_loss: 28.3445 - val_MinusLogProbMetric: 28.3445 - lr: 2.5000e-04 - 53s/epoch - 268ms/step
Epoch 626/1000
2023-09-29 01:49:42.053 
Epoch 626/1000 
	 loss: 28.0324, MinusLogProbMetric: 28.0324, val_loss: 28.2790, val_MinusLogProbMetric: 28.2790

Epoch 626: val_loss did not improve from 28.19110
196/196 - 50s - loss: 28.0324 - MinusLogProbMetric: 28.0324 - val_loss: 28.2790 - val_MinusLogProbMetric: 28.2790 - lr: 2.5000e-04 - 50s/epoch - 256ms/step
Epoch 627/1000
2023-09-29 01:50:33.759 
Epoch 627/1000 
	 loss: 28.0534, MinusLogProbMetric: 28.0534, val_loss: 28.3007, val_MinusLogProbMetric: 28.3007

Epoch 627: val_loss did not improve from 28.19110
196/196 - 52s - loss: 28.0534 - MinusLogProbMetric: 28.0534 - val_loss: 28.3007 - val_MinusLogProbMetric: 28.3007 - lr: 2.5000e-04 - 52s/epoch - 264ms/step
Epoch 628/1000
2023-09-29 01:51:23.791 
Epoch 628/1000 
	 loss: 28.0931, MinusLogProbMetric: 28.0931, val_loss: 28.3251, val_MinusLogProbMetric: 28.3251

Epoch 628: val_loss did not improve from 28.19110
196/196 - 50s - loss: 28.0931 - MinusLogProbMetric: 28.0931 - val_loss: 28.3251 - val_MinusLogProbMetric: 28.3251 - lr: 2.5000e-04 - 50s/epoch - 255ms/step
Epoch 629/1000
2023-09-29 01:52:14.180 
Epoch 629/1000 
	 loss: 28.0540, MinusLogProbMetric: 28.0540, val_loss: 28.3018, val_MinusLogProbMetric: 28.3018

Epoch 629: val_loss did not improve from 28.19110
196/196 - 50s - loss: 28.0540 - MinusLogProbMetric: 28.0540 - val_loss: 28.3018 - val_MinusLogProbMetric: 28.3018 - lr: 2.5000e-04 - 50s/epoch - 257ms/step
Epoch 630/1000
2023-09-29 01:53:04.980 
Epoch 630/1000 
	 loss: 28.0434, MinusLogProbMetric: 28.0434, val_loss: 28.1694, val_MinusLogProbMetric: 28.1694

Epoch 630: val_loss improved from 28.19110 to 28.16942, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_323/weights/best_weights.h5
196/196 - 52s - loss: 28.0434 - MinusLogProbMetric: 28.0434 - val_loss: 28.1694 - val_MinusLogProbMetric: 28.1694 - lr: 2.5000e-04 - 52s/epoch - 263ms/step
Epoch 631/1000
2023-09-29 01:53:55.801 
Epoch 631/1000 
	 loss: 28.0433, MinusLogProbMetric: 28.0433, val_loss: 28.3177, val_MinusLogProbMetric: 28.3177

Epoch 631: val_loss did not improve from 28.16942
196/196 - 50s - loss: 28.0433 - MinusLogProbMetric: 28.0433 - val_loss: 28.3177 - val_MinusLogProbMetric: 28.3177 - lr: 2.5000e-04 - 50s/epoch - 255ms/step
Epoch 632/1000
2023-09-29 01:54:46.194 
Epoch 632/1000 
	 loss: 28.0716, MinusLogProbMetric: 28.0716, val_loss: 28.2159, val_MinusLogProbMetric: 28.2159

Epoch 632: val_loss did not improve from 28.16942
196/196 - 50s - loss: 28.0716 - MinusLogProbMetric: 28.0716 - val_loss: 28.2159 - val_MinusLogProbMetric: 28.2159 - lr: 2.5000e-04 - 50s/epoch - 257ms/step
Epoch 633/1000
2023-09-29 01:55:36.123 
Epoch 633/1000 
	 loss: 28.0450, MinusLogProbMetric: 28.0450, val_loss: 28.2747, val_MinusLogProbMetric: 28.2747

Epoch 633: val_loss did not improve from 28.16942
196/196 - 50s - loss: 28.0450 - MinusLogProbMetric: 28.0450 - val_loss: 28.2747 - val_MinusLogProbMetric: 28.2747 - lr: 2.5000e-04 - 50s/epoch - 255ms/step
Epoch 634/1000
2023-09-29 01:56:25.429 
Epoch 634/1000 
	 loss: 28.0380, MinusLogProbMetric: 28.0380, val_loss: 28.3051, val_MinusLogProbMetric: 28.3051

Epoch 634: val_loss did not improve from 28.16942
196/196 - 49s - loss: 28.0380 - MinusLogProbMetric: 28.0380 - val_loss: 28.3051 - val_MinusLogProbMetric: 28.3051 - lr: 2.5000e-04 - 49s/epoch - 252ms/step
Epoch 635/1000
2023-09-29 01:57:15.215 
Epoch 635/1000 
	 loss: 28.0182, MinusLogProbMetric: 28.0182, val_loss: 28.1883, val_MinusLogProbMetric: 28.1883

Epoch 635: val_loss did not improve from 28.16942
196/196 - 50s - loss: 28.0182 - MinusLogProbMetric: 28.0182 - val_loss: 28.1883 - val_MinusLogProbMetric: 28.1883 - lr: 2.5000e-04 - 50s/epoch - 254ms/step
Epoch 636/1000
2023-09-29 01:58:03.754 
Epoch 636/1000 
	 loss: 28.0370, MinusLogProbMetric: 28.0370, val_loss: 28.3467, val_MinusLogProbMetric: 28.3467

Epoch 636: val_loss did not improve from 28.16942
196/196 - 49s - loss: 28.0370 - MinusLogProbMetric: 28.0370 - val_loss: 28.3467 - val_MinusLogProbMetric: 28.3467 - lr: 2.5000e-04 - 49s/epoch - 248ms/step
Epoch 637/1000
2023-09-29 01:58:53.525 
Epoch 637/1000 
	 loss: 28.0701, MinusLogProbMetric: 28.0701, val_loss: 28.4261, val_MinusLogProbMetric: 28.4261

Epoch 637: val_loss did not improve from 28.16942
196/196 - 50s - loss: 28.0701 - MinusLogProbMetric: 28.0701 - val_loss: 28.4261 - val_MinusLogProbMetric: 28.4261 - lr: 2.5000e-04 - 50s/epoch - 254ms/step
Epoch 638/1000
2023-09-29 01:59:45.923 
Epoch 638/1000 
	 loss: 28.0448, MinusLogProbMetric: 28.0448, val_loss: 28.2509, val_MinusLogProbMetric: 28.2509

Epoch 638: val_loss did not improve from 28.16942
196/196 - 52s - loss: 28.0448 - MinusLogProbMetric: 28.0448 - val_loss: 28.2509 - val_MinusLogProbMetric: 28.2509 - lr: 2.5000e-04 - 52s/epoch - 267ms/step
Epoch 639/1000
2023-09-29 02:00:35.847 
Epoch 639/1000 
	 loss: 28.0470, MinusLogProbMetric: 28.0470, val_loss: 28.4522, val_MinusLogProbMetric: 28.4522

Epoch 639: val_loss did not improve from 28.16942
196/196 - 50s - loss: 28.0470 - MinusLogProbMetric: 28.0470 - val_loss: 28.4522 - val_MinusLogProbMetric: 28.4522 - lr: 2.5000e-04 - 50s/epoch - 255ms/step
Epoch 640/1000
2023-09-29 02:01:27.854 
Epoch 640/1000 
	 loss: 28.0303, MinusLogProbMetric: 28.0303, val_loss: 28.4652, val_MinusLogProbMetric: 28.4652

Epoch 640: val_loss did not improve from 28.16942
196/196 - 52s - loss: 28.0303 - MinusLogProbMetric: 28.0303 - val_loss: 28.4652 - val_MinusLogProbMetric: 28.4652 - lr: 2.5000e-04 - 52s/epoch - 265ms/step
Epoch 641/1000
2023-09-29 02:02:16.817 
Epoch 641/1000 
	 loss: 28.0959, MinusLogProbMetric: 28.0959, val_loss: 28.2246, val_MinusLogProbMetric: 28.2246

Epoch 641: val_loss did not improve from 28.16942
196/196 - 49s - loss: 28.0959 - MinusLogProbMetric: 28.0959 - val_loss: 28.2246 - val_MinusLogProbMetric: 28.2246 - lr: 2.5000e-04 - 49s/epoch - 250ms/step
Epoch 642/1000
2023-09-29 02:03:07.582 
Epoch 642/1000 
	 loss: 28.0501, MinusLogProbMetric: 28.0501, val_loss: 28.2861, val_MinusLogProbMetric: 28.2861

Epoch 642: val_loss did not improve from 28.16942
196/196 - 51s - loss: 28.0501 - MinusLogProbMetric: 28.0501 - val_loss: 28.2861 - val_MinusLogProbMetric: 28.2861 - lr: 2.5000e-04 - 51s/epoch - 259ms/step
Epoch 643/1000
2023-09-29 02:03:57.069 
Epoch 643/1000 
	 loss: 28.0843, MinusLogProbMetric: 28.0843, val_loss: 28.3396, val_MinusLogProbMetric: 28.3396

Epoch 643: val_loss did not improve from 28.16942
196/196 - 49s - loss: 28.0843 - MinusLogProbMetric: 28.0843 - val_loss: 28.3396 - val_MinusLogProbMetric: 28.3396 - lr: 2.5000e-04 - 49s/epoch - 252ms/step
Epoch 644/1000
2023-09-29 02:04:46.455 
Epoch 644/1000 
	 loss: 28.0426, MinusLogProbMetric: 28.0426, val_loss: 28.4251, val_MinusLogProbMetric: 28.4251

Epoch 644: val_loss did not improve from 28.16942
196/196 - 49s - loss: 28.0426 - MinusLogProbMetric: 28.0426 - val_loss: 28.4251 - val_MinusLogProbMetric: 28.4251 - lr: 2.5000e-04 - 49s/epoch - 252ms/step
Epoch 645/1000
2023-09-29 02:05:36.474 
Epoch 645/1000 
	 loss: 28.0622, MinusLogProbMetric: 28.0622, val_loss: 28.2378, val_MinusLogProbMetric: 28.2378

Epoch 645: val_loss did not improve from 28.16942
196/196 - 50s - loss: 28.0622 - MinusLogProbMetric: 28.0622 - val_loss: 28.2378 - val_MinusLogProbMetric: 28.2378 - lr: 2.5000e-04 - 50s/epoch - 255ms/step
Epoch 646/1000
2023-09-29 02:06:27.117 
Epoch 646/1000 
	 loss: 28.0318, MinusLogProbMetric: 28.0318, val_loss: 28.3277, val_MinusLogProbMetric: 28.3277

Epoch 646: val_loss did not improve from 28.16942
196/196 - 51s - loss: 28.0318 - MinusLogProbMetric: 28.0318 - val_loss: 28.3277 - val_MinusLogProbMetric: 28.3277 - lr: 2.5000e-04 - 51s/epoch - 258ms/step
Epoch 647/1000
2023-09-29 02:07:16.602 
Epoch 647/1000 
	 loss: 28.0527, MinusLogProbMetric: 28.0527, val_loss: 28.2311, val_MinusLogProbMetric: 28.2311

Epoch 647: val_loss did not improve from 28.16942
196/196 - 49s - loss: 28.0527 - MinusLogProbMetric: 28.0527 - val_loss: 28.2311 - val_MinusLogProbMetric: 28.2311 - lr: 2.5000e-04 - 49s/epoch - 252ms/step
Epoch 648/1000
2023-09-29 02:08:07.289 
Epoch 648/1000 
	 loss: 28.0344, MinusLogProbMetric: 28.0344, val_loss: 28.3143, val_MinusLogProbMetric: 28.3143

Epoch 648: val_loss did not improve from 28.16942
196/196 - 51s - loss: 28.0344 - MinusLogProbMetric: 28.0344 - val_loss: 28.3143 - val_MinusLogProbMetric: 28.3143 - lr: 2.5000e-04 - 51s/epoch - 259ms/step
Epoch 649/1000
2023-09-29 02:08:58.825 
Epoch 649/1000 
	 loss: 28.0485, MinusLogProbMetric: 28.0485, val_loss: 28.1913, val_MinusLogProbMetric: 28.1913

Epoch 649: val_loss did not improve from 28.16942
196/196 - 52s - loss: 28.0485 - MinusLogProbMetric: 28.0485 - val_loss: 28.1913 - val_MinusLogProbMetric: 28.1913 - lr: 2.5000e-04 - 52s/epoch - 263ms/step
Epoch 650/1000
2023-09-29 02:09:48.242 
Epoch 650/1000 
	 loss: 28.0897, MinusLogProbMetric: 28.0897, val_loss: 28.3298, val_MinusLogProbMetric: 28.3298

Epoch 650: val_loss did not improve from 28.16942
196/196 - 49s - loss: 28.0897 - MinusLogProbMetric: 28.0897 - val_loss: 28.3298 - val_MinusLogProbMetric: 28.3298 - lr: 2.5000e-04 - 49s/epoch - 252ms/step
Epoch 651/1000
2023-09-29 02:10:39.602 
Epoch 651/1000 
	 loss: 28.0325, MinusLogProbMetric: 28.0325, val_loss: 28.2379, val_MinusLogProbMetric: 28.2379

Epoch 651: val_loss did not improve from 28.16942
196/196 - 51s - loss: 28.0325 - MinusLogProbMetric: 28.0325 - val_loss: 28.2379 - val_MinusLogProbMetric: 28.2379 - lr: 2.5000e-04 - 51s/epoch - 262ms/step
Epoch 652/1000
2023-09-29 02:11:32.252 
Epoch 652/1000 
	 loss: 28.0683, MinusLogProbMetric: 28.0683, val_loss: 28.3220, val_MinusLogProbMetric: 28.3220

Epoch 652: val_loss did not improve from 28.16942
196/196 - 53s - loss: 28.0683 - MinusLogProbMetric: 28.0683 - val_loss: 28.3220 - val_MinusLogProbMetric: 28.3220 - lr: 2.5000e-04 - 53s/epoch - 269ms/step
Epoch 653/1000
2023-09-29 02:12:25.613 
Epoch 653/1000 
	 loss: 28.0884, MinusLogProbMetric: 28.0884, val_loss: 28.3417, val_MinusLogProbMetric: 28.3417

Epoch 653: val_loss did not improve from 28.16942
196/196 - 53s - loss: 28.0884 - MinusLogProbMetric: 28.0884 - val_loss: 28.3417 - val_MinusLogProbMetric: 28.3417 - lr: 2.5000e-04 - 53s/epoch - 272ms/step
Epoch 654/1000
2023-09-29 02:13:17.339 
Epoch 654/1000 
	 loss: 28.0436, MinusLogProbMetric: 28.0436, val_loss: 28.2669, val_MinusLogProbMetric: 28.2669

Epoch 654: val_loss did not improve from 28.16942
196/196 - 52s - loss: 28.0436 - MinusLogProbMetric: 28.0436 - val_loss: 28.2669 - val_MinusLogProbMetric: 28.2669 - lr: 2.5000e-04 - 52s/epoch - 264ms/step
Epoch 655/1000
2023-09-29 02:14:09.979 
Epoch 655/1000 
	 loss: 28.0592, MinusLogProbMetric: 28.0592, val_loss: 28.3895, val_MinusLogProbMetric: 28.3895

Epoch 655: val_loss did not improve from 28.16942
196/196 - 53s - loss: 28.0592 - MinusLogProbMetric: 28.0592 - val_loss: 28.3895 - val_MinusLogProbMetric: 28.3895 - lr: 2.5000e-04 - 53s/epoch - 269ms/step
Epoch 656/1000
2023-09-29 02:15:01.067 
Epoch 656/1000 
	 loss: 28.0356, MinusLogProbMetric: 28.0356, val_loss: 28.3610, val_MinusLogProbMetric: 28.3610

Epoch 656: val_loss did not improve from 28.16942
196/196 - 51s - loss: 28.0356 - MinusLogProbMetric: 28.0356 - val_loss: 28.3610 - val_MinusLogProbMetric: 28.3610 - lr: 2.5000e-04 - 51s/epoch - 261ms/step
Epoch 657/1000
2023-09-29 02:15:48.429 
Epoch 657/1000 
	 loss: 28.0362, MinusLogProbMetric: 28.0362, val_loss: 28.2333, val_MinusLogProbMetric: 28.2333

Epoch 657: val_loss did not improve from 28.16942
196/196 - 47s - loss: 28.0362 - MinusLogProbMetric: 28.0362 - val_loss: 28.2333 - val_MinusLogProbMetric: 28.2333 - lr: 2.5000e-04 - 47s/epoch - 242ms/step
Epoch 658/1000
2023-09-29 02:16:37.398 
Epoch 658/1000 
	 loss: 28.0297, MinusLogProbMetric: 28.0297, val_loss: 28.3684, val_MinusLogProbMetric: 28.3684

Epoch 658: val_loss did not improve from 28.16942
196/196 - 49s - loss: 28.0297 - MinusLogProbMetric: 28.0297 - val_loss: 28.3684 - val_MinusLogProbMetric: 28.3684 - lr: 2.5000e-04 - 49s/epoch - 250ms/step
Epoch 659/1000
2023-09-29 02:17:27.616 
Epoch 659/1000 
	 loss: 28.0129, MinusLogProbMetric: 28.0129, val_loss: 28.3979, val_MinusLogProbMetric: 28.3979

Epoch 659: val_loss did not improve from 28.16942
196/196 - 50s - loss: 28.0129 - MinusLogProbMetric: 28.0129 - val_loss: 28.3979 - val_MinusLogProbMetric: 28.3979 - lr: 2.5000e-04 - 50s/epoch - 256ms/step
Epoch 660/1000
2023-09-29 02:18:19.356 
Epoch 660/1000 
	 loss: 28.0620, MinusLogProbMetric: 28.0620, val_loss: 28.2562, val_MinusLogProbMetric: 28.2562

Epoch 660: val_loss did not improve from 28.16942
196/196 - 52s - loss: 28.0620 - MinusLogProbMetric: 28.0620 - val_loss: 28.2562 - val_MinusLogProbMetric: 28.2562 - lr: 2.5000e-04 - 52s/epoch - 264ms/step
Epoch 661/1000
2023-09-29 02:19:11.139 
Epoch 661/1000 
	 loss: 28.0270, MinusLogProbMetric: 28.0270, val_loss: 28.3381, val_MinusLogProbMetric: 28.3381

Epoch 661: val_loss did not improve from 28.16942
196/196 - 52s - loss: 28.0270 - MinusLogProbMetric: 28.0270 - val_loss: 28.3381 - val_MinusLogProbMetric: 28.3381 - lr: 2.5000e-04 - 52s/epoch - 264ms/step
Epoch 662/1000
2023-09-29 02:20:02.740 
Epoch 662/1000 
	 loss: 28.0418, MinusLogProbMetric: 28.0418, val_loss: 28.2104, val_MinusLogProbMetric: 28.2104

Epoch 662: val_loss did not improve from 28.16942
196/196 - 52s - loss: 28.0418 - MinusLogProbMetric: 28.0418 - val_loss: 28.2104 - val_MinusLogProbMetric: 28.2104 - lr: 2.5000e-04 - 52s/epoch - 263ms/step
Epoch 663/1000
2023-09-29 02:20:53.232 
Epoch 663/1000 
	 loss: 28.0119, MinusLogProbMetric: 28.0119, val_loss: 28.2221, val_MinusLogProbMetric: 28.2221

Epoch 663: val_loss did not improve from 28.16942
196/196 - 50s - loss: 28.0119 - MinusLogProbMetric: 28.0119 - val_loss: 28.2221 - val_MinusLogProbMetric: 28.2221 - lr: 2.5000e-04 - 50s/epoch - 258ms/step
Epoch 664/1000
2023-09-29 02:21:44.463 
Epoch 664/1000 
	 loss: 28.0224, MinusLogProbMetric: 28.0224, val_loss: 28.2773, val_MinusLogProbMetric: 28.2773

Epoch 664: val_loss did not improve from 28.16942
196/196 - 51s - loss: 28.0224 - MinusLogProbMetric: 28.0224 - val_loss: 28.2773 - val_MinusLogProbMetric: 28.2773 - lr: 2.5000e-04 - 51s/epoch - 261ms/step
Epoch 665/1000
2023-09-29 02:22:30.797 
Epoch 665/1000 
	 loss: 28.0324, MinusLogProbMetric: 28.0324, val_loss: 28.3821, val_MinusLogProbMetric: 28.3821

Epoch 665: val_loss did not improve from 28.16942
196/196 - 46s - loss: 28.0324 - MinusLogProbMetric: 28.0324 - val_loss: 28.3821 - val_MinusLogProbMetric: 28.3821 - lr: 2.5000e-04 - 46s/epoch - 236ms/step
Epoch 666/1000
2023-09-29 02:23:22.557 
Epoch 666/1000 
	 loss: 28.0543, MinusLogProbMetric: 28.0543, val_loss: 28.5053, val_MinusLogProbMetric: 28.5053

Epoch 666: val_loss did not improve from 28.16942
196/196 - 52s - loss: 28.0543 - MinusLogProbMetric: 28.0543 - val_loss: 28.5053 - val_MinusLogProbMetric: 28.5053 - lr: 2.5000e-04 - 52s/epoch - 264ms/step
Epoch 667/1000
2023-09-29 02:24:13.647 
Epoch 667/1000 
	 loss: 28.0419, MinusLogProbMetric: 28.0419, val_loss: 28.3414, val_MinusLogProbMetric: 28.3414

Epoch 667: val_loss did not improve from 28.16942
196/196 - 51s - loss: 28.0419 - MinusLogProbMetric: 28.0419 - val_loss: 28.3414 - val_MinusLogProbMetric: 28.3414 - lr: 2.5000e-04 - 51s/epoch - 261ms/step
Epoch 668/1000
2023-09-29 02:25:03.704 
Epoch 668/1000 
	 loss: 28.0599, MinusLogProbMetric: 28.0599, val_loss: 28.2516, val_MinusLogProbMetric: 28.2516

Epoch 668: val_loss did not improve from 28.16942
196/196 - 50s - loss: 28.0599 - MinusLogProbMetric: 28.0599 - val_loss: 28.2516 - val_MinusLogProbMetric: 28.2516 - lr: 2.5000e-04 - 50s/epoch - 255ms/step
Epoch 669/1000
2023-09-29 02:25:53.023 
Epoch 669/1000 
	 loss: 28.0338, MinusLogProbMetric: 28.0338, val_loss: 28.4713, val_MinusLogProbMetric: 28.4713

Epoch 669: val_loss did not improve from 28.16942
196/196 - 49s - loss: 28.0338 - MinusLogProbMetric: 28.0338 - val_loss: 28.4713 - val_MinusLogProbMetric: 28.4713 - lr: 2.5000e-04 - 49s/epoch - 252ms/step
Epoch 670/1000
2023-09-29 02:26:42.374 
Epoch 670/1000 
	 loss: 28.0377, MinusLogProbMetric: 28.0377, val_loss: 28.9408, val_MinusLogProbMetric: 28.9408

Epoch 670: val_loss did not improve from 28.16942
196/196 - 49s - loss: 28.0377 - MinusLogProbMetric: 28.0377 - val_loss: 28.9408 - val_MinusLogProbMetric: 28.9408 - lr: 2.5000e-04 - 49s/epoch - 252ms/step
Epoch 671/1000
2023-09-29 02:27:35.557 
Epoch 671/1000 
	 loss: 28.0656, MinusLogProbMetric: 28.0656, val_loss: 28.4143, val_MinusLogProbMetric: 28.4143

Epoch 671: val_loss did not improve from 28.16942
196/196 - 53s - loss: 28.0656 - MinusLogProbMetric: 28.0656 - val_loss: 28.4143 - val_MinusLogProbMetric: 28.4143 - lr: 2.5000e-04 - 53s/epoch - 271ms/step
Epoch 672/1000
2023-09-29 02:28:26.935 
Epoch 672/1000 
	 loss: 28.0533, MinusLogProbMetric: 28.0533, val_loss: 28.3102, val_MinusLogProbMetric: 28.3102

Epoch 672: val_loss did not improve from 28.16942
196/196 - 51s - loss: 28.0533 - MinusLogProbMetric: 28.0533 - val_loss: 28.3102 - val_MinusLogProbMetric: 28.3102 - lr: 2.5000e-04 - 51s/epoch - 262ms/step
Epoch 673/1000
2023-09-29 02:29:17.345 
Epoch 673/1000 
	 loss: 28.0501, MinusLogProbMetric: 28.0501, val_loss: 28.3550, val_MinusLogProbMetric: 28.3550

Epoch 673: val_loss did not improve from 28.16942
196/196 - 50s - loss: 28.0501 - MinusLogProbMetric: 28.0501 - val_loss: 28.3550 - val_MinusLogProbMetric: 28.3550 - lr: 2.5000e-04 - 50s/epoch - 257ms/step
Epoch 674/1000
2023-09-29 02:30:11.138 
Epoch 674/1000 
	 loss: 28.0341, MinusLogProbMetric: 28.0341, val_loss: 28.3253, val_MinusLogProbMetric: 28.3253

Epoch 674: val_loss did not improve from 28.16942
196/196 - 54s - loss: 28.0341 - MinusLogProbMetric: 28.0341 - val_loss: 28.3253 - val_MinusLogProbMetric: 28.3253 - lr: 2.5000e-04 - 54s/epoch - 274ms/step
Epoch 675/1000
2023-09-29 02:31:00.500 
Epoch 675/1000 
	 loss: 28.0396, MinusLogProbMetric: 28.0396, val_loss: 28.3264, val_MinusLogProbMetric: 28.3264

Epoch 675: val_loss did not improve from 28.16942
196/196 - 49s - loss: 28.0396 - MinusLogProbMetric: 28.0396 - val_loss: 28.3264 - val_MinusLogProbMetric: 28.3264 - lr: 2.5000e-04 - 49s/epoch - 252ms/step
Epoch 676/1000
2023-09-29 02:31:50.936 
Epoch 676/1000 
	 loss: 28.0730, MinusLogProbMetric: 28.0730, val_loss: 28.5479, val_MinusLogProbMetric: 28.5479

Epoch 676: val_loss did not improve from 28.16942
196/196 - 50s - loss: 28.0730 - MinusLogProbMetric: 28.0730 - val_loss: 28.5479 - val_MinusLogProbMetric: 28.5479 - lr: 2.5000e-04 - 50s/epoch - 257ms/step
Epoch 677/1000
2023-09-29 02:32:37.749 
Epoch 677/1000 
	 loss: 28.0388, MinusLogProbMetric: 28.0388, val_loss: 28.4138, val_MinusLogProbMetric: 28.4138

Epoch 677: val_loss did not improve from 28.16942
196/196 - 47s - loss: 28.0388 - MinusLogProbMetric: 28.0388 - val_loss: 28.4138 - val_MinusLogProbMetric: 28.4138 - lr: 2.5000e-04 - 47s/epoch - 239ms/step
Epoch 678/1000
2023-09-29 02:33:29.125 
Epoch 678/1000 
	 loss: 28.0393, MinusLogProbMetric: 28.0393, val_loss: 28.4532, val_MinusLogProbMetric: 28.4532

Epoch 678: val_loss did not improve from 28.16942
196/196 - 51s - loss: 28.0393 - MinusLogProbMetric: 28.0393 - val_loss: 28.4532 - val_MinusLogProbMetric: 28.4532 - lr: 2.5000e-04 - 51s/epoch - 262ms/step
Epoch 679/1000
2023-09-29 02:34:18.514 
Epoch 679/1000 
	 loss: 28.0371, MinusLogProbMetric: 28.0371, val_loss: 28.3404, val_MinusLogProbMetric: 28.3404

Epoch 679: val_loss did not improve from 28.16942
196/196 - 49s - loss: 28.0371 - MinusLogProbMetric: 28.0371 - val_loss: 28.3404 - val_MinusLogProbMetric: 28.3404 - lr: 2.5000e-04 - 49s/epoch - 252ms/step
Epoch 680/1000
2023-09-29 02:35:06.921 
Epoch 680/1000 
	 loss: 28.0535, MinusLogProbMetric: 28.0535, val_loss: 28.4306, val_MinusLogProbMetric: 28.4306

Epoch 680: val_loss did not improve from 28.16942
196/196 - 48s - loss: 28.0535 - MinusLogProbMetric: 28.0535 - val_loss: 28.4306 - val_MinusLogProbMetric: 28.4306 - lr: 2.5000e-04 - 48s/epoch - 247ms/step
Epoch 681/1000
2023-09-29 02:35:57.777 
Epoch 681/1000 
	 loss: 27.9157, MinusLogProbMetric: 27.9157, val_loss: 28.1336, val_MinusLogProbMetric: 28.1336

Epoch 681: val_loss improved from 28.16942 to 28.13363, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_323/weights/best_weights.h5
196/196 - 52s - loss: 27.9157 - MinusLogProbMetric: 27.9157 - val_loss: 28.1336 - val_MinusLogProbMetric: 28.1336 - lr: 1.2500e-04 - 52s/epoch - 264ms/step
Epoch 682/1000
2023-09-29 02:36:53.336 
Epoch 682/1000 
	 loss: 27.9081, MinusLogProbMetric: 27.9081, val_loss: 28.1282, val_MinusLogProbMetric: 28.1282

Epoch 682: val_loss improved from 28.13363 to 28.12820, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_323/weights/best_weights.h5
196/196 - 56s - loss: 27.9081 - MinusLogProbMetric: 27.9081 - val_loss: 28.1282 - val_MinusLogProbMetric: 28.1282 - lr: 1.2500e-04 - 56s/epoch - 284ms/step
Epoch 683/1000
2023-09-29 02:37:45.580 
Epoch 683/1000 
	 loss: 27.8996, MinusLogProbMetric: 27.8996, val_loss: 28.1702, val_MinusLogProbMetric: 28.1702

Epoch 683: val_loss did not improve from 28.12820
196/196 - 51s - loss: 27.8996 - MinusLogProbMetric: 27.8996 - val_loss: 28.1702 - val_MinusLogProbMetric: 28.1702 - lr: 1.2500e-04 - 51s/epoch - 261ms/step
Epoch 684/1000
2023-09-29 02:38:37.894 
Epoch 684/1000 
	 loss: 27.9046, MinusLogProbMetric: 27.9046, val_loss: 28.1352, val_MinusLogProbMetric: 28.1352

Epoch 684: val_loss did not improve from 28.12820
196/196 - 52s - loss: 27.9046 - MinusLogProbMetric: 27.9046 - val_loss: 28.1352 - val_MinusLogProbMetric: 28.1352 - lr: 1.2500e-04 - 52s/epoch - 267ms/step
Epoch 685/1000
2023-09-29 02:39:30.506 
Epoch 685/1000 
	 loss: 27.9152, MinusLogProbMetric: 27.9152, val_loss: 28.1755, val_MinusLogProbMetric: 28.1755

Epoch 685: val_loss did not improve from 28.12820
196/196 - 53s - loss: 27.9152 - MinusLogProbMetric: 27.9152 - val_loss: 28.1755 - val_MinusLogProbMetric: 28.1755 - lr: 1.2500e-04 - 53s/epoch - 268ms/step
Epoch 686/1000
2023-09-29 02:40:21.560 
Epoch 686/1000 
	 loss: 27.9307, MinusLogProbMetric: 27.9307, val_loss: 28.0988, val_MinusLogProbMetric: 28.0988

Epoch 686: val_loss improved from 28.12820 to 28.09881, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_323/weights/best_weights.h5
196/196 - 52s - loss: 27.9307 - MinusLogProbMetric: 27.9307 - val_loss: 28.0988 - val_MinusLogProbMetric: 28.0988 - lr: 1.2500e-04 - 52s/epoch - 264ms/step
Epoch 687/1000
2023-09-29 02:41:12.533 
Epoch 687/1000 
	 loss: 27.9049, MinusLogProbMetric: 27.9049, val_loss: 28.1463, val_MinusLogProbMetric: 28.1463

Epoch 687: val_loss did not improve from 28.09881
196/196 - 50s - loss: 27.9049 - MinusLogProbMetric: 27.9049 - val_loss: 28.1463 - val_MinusLogProbMetric: 28.1463 - lr: 1.2500e-04 - 50s/epoch - 256ms/step
Epoch 688/1000
2023-09-29 02:42:01.303 
Epoch 688/1000 
	 loss: 27.8975, MinusLogProbMetric: 27.8975, val_loss: 28.1029, val_MinusLogProbMetric: 28.1029

Epoch 688: val_loss did not improve from 28.09881
196/196 - 49s - loss: 27.8975 - MinusLogProbMetric: 27.8975 - val_loss: 28.1029 - val_MinusLogProbMetric: 28.1029 - lr: 1.2500e-04 - 49s/epoch - 249ms/step
Epoch 689/1000
2023-09-29 02:42:49.248 
Epoch 689/1000 
	 loss: 27.8981, MinusLogProbMetric: 27.8981, val_loss: 28.1404, val_MinusLogProbMetric: 28.1404

Epoch 689: val_loss did not improve from 28.09881
196/196 - 48s - loss: 27.8981 - MinusLogProbMetric: 27.8981 - val_loss: 28.1404 - val_MinusLogProbMetric: 28.1404 - lr: 1.2500e-04 - 48s/epoch - 245ms/step
Epoch 690/1000
2023-09-29 02:43:38.539 
Epoch 690/1000 
	 loss: 27.9042, MinusLogProbMetric: 27.9042, val_loss: 28.1127, val_MinusLogProbMetric: 28.1127

Epoch 690: val_loss did not improve from 28.09881
196/196 - 49s - loss: 27.9042 - MinusLogProbMetric: 27.9042 - val_loss: 28.1127 - val_MinusLogProbMetric: 28.1127 - lr: 1.2500e-04 - 49s/epoch - 251ms/step
Epoch 691/1000
2023-09-29 02:44:25.805 
Epoch 691/1000 
	 loss: 27.9098, MinusLogProbMetric: 27.9098, val_loss: 28.1390, val_MinusLogProbMetric: 28.1390

Epoch 691: val_loss did not improve from 28.09881
196/196 - 47s - loss: 27.9098 - MinusLogProbMetric: 27.9098 - val_loss: 28.1390 - val_MinusLogProbMetric: 28.1390 - lr: 1.2500e-04 - 47s/epoch - 241ms/step
Epoch 692/1000
2023-09-29 02:45:18.865 
Epoch 692/1000 
	 loss: 27.9044, MinusLogProbMetric: 27.9044, val_loss: 28.1074, val_MinusLogProbMetric: 28.1074

Epoch 692: val_loss did not improve from 28.09881
196/196 - 53s - loss: 27.9044 - MinusLogProbMetric: 27.9044 - val_loss: 28.1074 - val_MinusLogProbMetric: 28.1074 - lr: 1.2500e-04 - 53s/epoch - 271ms/step
Epoch 693/1000
2023-09-29 02:46:14.052 
Epoch 693/1000 
	 loss: 27.9132, MinusLogProbMetric: 27.9132, val_loss: 28.1120, val_MinusLogProbMetric: 28.1120

Epoch 693: val_loss did not improve from 28.09881
196/196 - 55s - loss: 27.9132 - MinusLogProbMetric: 27.9132 - val_loss: 28.1120 - val_MinusLogProbMetric: 28.1120 - lr: 1.2500e-04 - 55s/epoch - 282ms/step
Epoch 694/1000
2023-09-29 02:47:06.397 
Epoch 694/1000 
	 loss: 27.8890, MinusLogProbMetric: 27.8890, val_loss: 28.1546, val_MinusLogProbMetric: 28.1546

Epoch 694: val_loss did not improve from 28.09881
196/196 - 52s - loss: 27.8890 - MinusLogProbMetric: 27.8890 - val_loss: 28.1546 - val_MinusLogProbMetric: 28.1546 - lr: 1.2500e-04 - 52s/epoch - 267ms/step
Epoch 695/1000
2023-09-29 02:47:58.614 
Epoch 695/1000 
	 loss: 27.9136, MinusLogProbMetric: 27.9136, val_loss: 28.0988, val_MinusLogProbMetric: 28.0988

Epoch 695: val_loss improved from 28.09881 to 28.09879, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_323/weights/best_weights.h5
196/196 - 53s - loss: 27.9136 - MinusLogProbMetric: 27.9136 - val_loss: 28.0988 - val_MinusLogProbMetric: 28.0988 - lr: 1.2500e-04 - 53s/epoch - 269ms/step
Epoch 696/1000
2023-09-29 02:48:51.563 
Epoch 696/1000 
	 loss: 27.8969, MinusLogProbMetric: 27.8969, val_loss: 28.2093, val_MinusLogProbMetric: 28.2093

Epoch 696: val_loss did not improve from 28.09879
196/196 - 52s - loss: 27.8969 - MinusLogProbMetric: 27.8969 - val_loss: 28.2093 - val_MinusLogProbMetric: 28.2093 - lr: 1.2500e-04 - 52s/epoch - 268ms/step
Epoch 697/1000
2023-09-29 02:49:38.783 
Epoch 697/1000 
	 loss: 27.9179, MinusLogProbMetric: 27.9179, val_loss: 28.0914, val_MinusLogProbMetric: 28.0914

Epoch 697: val_loss improved from 28.09879 to 28.09140, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_323/weights/best_weights.h5
196/196 - 48s - loss: 27.9179 - MinusLogProbMetric: 27.9179 - val_loss: 28.0914 - val_MinusLogProbMetric: 28.0914 - lr: 1.2500e-04 - 48s/epoch - 245ms/step
Epoch 698/1000
2023-09-29 02:50:28.491 
Epoch 698/1000 
	 loss: 27.9009, MinusLogProbMetric: 27.9009, val_loss: 28.1061, val_MinusLogProbMetric: 28.1061

Epoch 698: val_loss did not improve from 28.09140
196/196 - 49s - loss: 27.9009 - MinusLogProbMetric: 27.9009 - val_loss: 28.1061 - val_MinusLogProbMetric: 28.1061 - lr: 1.2500e-04 - 49s/epoch - 250ms/step
Epoch 699/1000
2023-09-29 02:51:17.146 
Epoch 699/1000 
	 loss: 27.8907, MinusLogProbMetric: 27.8907, val_loss: 28.0958, val_MinusLogProbMetric: 28.0958

Epoch 699: val_loss did not improve from 28.09140
196/196 - 49s - loss: 27.8907 - MinusLogProbMetric: 27.8907 - val_loss: 28.0958 - val_MinusLogProbMetric: 28.0958 - lr: 1.2500e-04 - 49s/epoch - 248ms/step
Epoch 700/1000
2023-09-29 02:52:07.170 
Epoch 700/1000 
	 loss: 27.9074, MinusLogProbMetric: 27.9074, val_loss: 28.1179, val_MinusLogProbMetric: 28.1179

Epoch 700: val_loss did not improve from 28.09140
196/196 - 50s - loss: 27.9074 - MinusLogProbMetric: 27.9074 - val_loss: 28.1179 - val_MinusLogProbMetric: 28.1179 - lr: 1.2500e-04 - 50s/epoch - 255ms/step
Epoch 701/1000
2023-09-29 02:52:56.565 
Epoch 701/1000 
	 loss: 27.8928, MinusLogProbMetric: 27.8928, val_loss: 28.1643, val_MinusLogProbMetric: 28.1643

Epoch 701: val_loss did not improve from 28.09140
196/196 - 49s - loss: 27.8928 - MinusLogProbMetric: 27.8928 - val_loss: 28.1643 - val_MinusLogProbMetric: 28.1643 - lr: 1.2500e-04 - 49s/epoch - 252ms/step
Epoch 702/1000
2023-09-29 02:53:47.614 
Epoch 702/1000 
	 loss: 27.8989, MinusLogProbMetric: 27.8989, val_loss: 28.0847, val_MinusLogProbMetric: 28.0847

Epoch 702: val_loss improved from 28.09140 to 28.08475, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_323/weights/best_weights.h5
196/196 - 52s - loss: 27.8989 - MinusLogProbMetric: 27.8989 - val_loss: 28.0847 - val_MinusLogProbMetric: 28.0847 - lr: 1.2500e-04 - 52s/epoch - 266ms/step
Epoch 703/1000
2023-09-29 02:54:42.727 
Epoch 703/1000 
	 loss: 27.8953, MinusLogProbMetric: 27.8953, val_loss: 28.1251, val_MinusLogProbMetric: 28.1251

Epoch 703: val_loss did not improve from 28.08475
196/196 - 54s - loss: 27.8953 - MinusLogProbMetric: 27.8953 - val_loss: 28.1251 - val_MinusLogProbMetric: 28.1251 - lr: 1.2500e-04 - 54s/epoch - 276ms/step
Epoch 704/1000
2023-09-29 02:55:36.671 
Epoch 704/1000 
	 loss: 27.8981, MinusLogProbMetric: 27.8981, val_loss: 28.1389, val_MinusLogProbMetric: 28.1389

Epoch 704: val_loss did not improve from 28.08475
196/196 - 54s - loss: 27.8981 - MinusLogProbMetric: 27.8981 - val_loss: 28.1389 - val_MinusLogProbMetric: 28.1389 - lr: 1.2500e-04 - 54s/epoch - 275ms/step
Epoch 705/1000
2023-09-29 02:56:29.102 
Epoch 705/1000 
	 loss: 27.8964, MinusLogProbMetric: 27.8964, val_loss: 28.1894, val_MinusLogProbMetric: 28.1894

Epoch 705: val_loss did not improve from 28.08475
196/196 - 52s - loss: 27.8964 - MinusLogProbMetric: 27.8964 - val_loss: 28.1894 - val_MinusLogProbMetric: 28.1894 - lr: 1.2500e-04 - 52s/epoch - 267ms/step
Epoch 706/1000
2023-09-29 02:57:23.425 
Epoch 706/1000 
	 loss: 27.9196, MinusLogProbMetric: 27.9196, val_loss: 28.1513, val_MinusLogProbMetric: 28.1513

Epoch 706: val_loss did not improve from 28.08475
196/196 - 54s - loss: 27.9196 - MinusLogProbMetric: 27.9196 - val_loss: 28.1513 - val_MinusLogProbMetric: 28.1513 - lr: 1.2500e-04 - 54s/epoch - 277ms/step
Epoch 707/1000
2023-09-29 02:58:18.231 
Epoch 707/1000 
	 loss: 27.9183, MinusLogProbMetric: 27.9183, val_loss: 28.1234, val_MinusLogProbMetric: 28.1234

Epoch 707: val_loss did not improve from 28.08475
196/196 - 55s - loss: 27.9183 - MinusLogProbMetric: 27.9183 - val_loss: 28.1234 - val_MinusLogProbMetric: 28.1234 - lr: 1.2500e-04 - 55s/epoch - 280ms/step
Epoch 708/1000
2023-09-29 02:59:12.036 
Epoch 708/1000 
	 loss: 27.9086, MinusLogProbMetric: 27.9086, val_loss: 28.1523, val_MinusLogProbMetric: 28.1523

Epoch 708: val_loss did not improve from 28.08475
196/196 - 54s - loss: 27.9086 - MinusLogProbMetric: 27.9086 - val_loss: 28.1523 - val_MinusLogProbMetric: 28.1523 - lr: 1.2500e-04 - 54s/epoch - 274ms/step
Epoch 709/1000
2023-09-29 03:00:03.444 
Epoch 709/1000 
	 loss: 27.8979, MinusLogProbMetric: 27.8979, val_loss: 28.1806, val_MinusLogProbMetric: 28.1806

Epoch 709: val_loss did not improve from 28.08475
196/196 - 51s - loss: 27.8979 - MinusLogProbMetric: 27.8979 - val_loss: 28.1806 - val_MinusLogProbMetric: 28.1806 - lr: 1.2500e-04 - 51s/epoch - 262ms/step
Epoch 710/1000
2023-09-29 03:00:55.121 
Epoch 710/1000 
	 loss: 27.9013, MinusLogProbMetric: 27.9013, val_loss: 28.1549, val_MinusLogProbMetric: 28.1549

Epoch 710: val_loss did not improve from 28.08475
196/196 - 52s - loss: 27.9013 - MinusLogProbMetric: 27.9013 - val_loss: 28.1549 - val_MinusLogProbMetric: 28.1549 - lr: 1.2500e-04 - 52s/epoch - 264ms/step
Epoch 711/1000
2023-09-29 03:01:49.253 
Epoch 711/1000 
	 loss: 27.8900, MinusLogProbMetric: 27.8900, val_loss: 28.1359, val_MinusLogProbMetric: 28.1359

Epoch 711: val_loss did not improve from 28.08475
196/196 - 54s - loss: 27.8900 - MinusLogProbMetric: 27.8900 - val_loss: 28.1359 - val_MinusLogProbMetric: 28.1359 - lr: 1.2500e-04 - 54s/epoch - 276ms/step
Epoch 712/1000
2023-09-29 03:02:42.114 
Epoch 712/1000 
	 loss: 27.8994, MinusLogProbMetric: 27.8994, val_loss: 28.1266, val_MinusLogProbMetric: 28.1266

Epoch 712: val_loss did not improve from 28.08475
196/196 - 53s - loss: 27.8994 - MinusLogProbMetric: 27.8994 - val_loss: 28.1266 - val_MinusLogProbMetric: 28.1266 - lr: 1.2500e-04 - 53s/epoch - 270ms/step
Epoch 713/1000
2023-09-29 03:03:34.468 
Epoch 713/1000 
	 loss: 27.8924, MinusLogProbMetric: 27.8924, val_loss: 28.1559, val_MinusLogProbMetric: 28.1559

Epoch 713: val_loss did not improve from 28.08475
196/196 - 52s - loss: 27.8924 - MinusLogProbMetric: 27.8924 - val_loss: 28.1559 - val_MinusLogProbMetric: 28.1559 - lr: 1.2500e-04 - 52s/epoch - 267ms/step
Epoch 714/1000
2023-09-29 03:04:28.687 
Epoch 714/1000 
	 loss: 27.8951, MinusLogProbMetric: 27.8951, val_loss: 28.1445, val_MinusLogProbMetric: 28.1445

Epoch 714: val_loss did not improve from 28.08475
196/196 - 54s - loss: 27.8951 - MinusLogProbMetric: 27.8951 - val_loss: 28.1445 - val_MinusLogProbMetric: 28.1445 - lr: 1.2500e-04 - 54s/epoch - 277ms/step
Epoch 715/1000
2023-09-29 03:05:23.434 
Epoch 715/1000 
	 loss: 27.8923, MinusLogProbMetric: 27.8923, val_loss: 28.1383, val_MinusLogProbMetric: 28.1383

Epoch 715: val_loss did not improve from 28.08475
196/196 - 55s - loss: 27.8923 - MinusLogProbMetric: 27.8923 - val_loss: 28.1383 - val_MinusLogProbMetric: 28.1383 - lr: 1.2500e-04 - 55s/epoch - 279ms/step
Epoch 716/1000
2023-09-29 03:06:17.969 
Epoch 716/1000 
	 loss: 27.8995, MinusLogProbMetric: 27.8995, val_loss: 28.1848, val_MinusLogProbMetric: 28.1848

Epoch 716: val_loss did not improve from 28.08475
196/196 - 55s - loss: 27.8995 - MinusLogProbMetric: 27.8995 - val_loss: 28.1848 - val_MinusLogProbMetric: 28.1848 - lr: 1.2500e-04 - 55s/epoch - 278ms/step
Epoch 717/1000
2023-09-29 03:07:10.371 
Epoch 717/1000 
	 loss: 27.9000, MinusLogProbMetric: 27.9000, val_loss: 28.1029, val_MinusLogProbMetric: 28.1029

Epoch 717: val_loss did not improve from 28.08475
196/196 - 52s - loss: 27.9000 - MinusLogProbMetric: 27.9000 - val_loss: 28.1029 - val_MinusLogProbMetric: 28.1029 - lr: 1.2500e-04 - 52s/epoch - 267ms/step
Epoch 718/1000
2023-09-29 03:08:03.199 
Epoch 718/1000 
	 loss: 27.8963, MinusLogProbMetric: 27.8963, val_loss: 28.0937, val_MinusLogProbMetric: 28.0937

Epoch 718: val_loss did not improve from 28.08475
196/196 - 53s - loss: 27.8963 - MinusLogProbMetric: 27.8963 - val_loss: 28.0937 - val_MinusLogProbMetric: 28.0937 - lr: 1.2500e-04 - 53s/epoch - 269ms/step
Epoch 719/1000
2023-09-29 03:08:56.327 
Epoch 719/1000 
	 loss: 27.8990, MinusLogProbMetric: 27.8990, val_loss: 28.1445, val_MinusLogProbMetric: 28.1445

Epoch 719: val_loss did not improve from 28.08475
196/196 - 53s - loss: 27.8990 - MinusLogProbMetric: 27.8990 - val_loss: 28.1445 - val_MinusLogProbMetric: 28.1445 - lr: 1.2500e-04 - 53s/epoch - 271ms/step
Epoch 720/1000
2023-09-29 03:09:49.748 
Epoch 720/1000 
	 loss: 27.9314, MinusLogProbMetric: 27.9314, val_loss: 28.1408, val_MinusLogProbMetric: 28.1408

Epoch 720: val_loss did not improve from 28.08475
196/196 - 53s - loss: 27.9314 - MinusLogProbMetric: 27.9314 - val_loss: 28.1408 - val_MinusLogProbMetric: 28.1408 - lr: 1.2500e-04 - 53s/epoch - 273ms/step
Epoch 721/1000
2023-09-29 03:10:42.837 
Epoch 721/1000 
	 loss: 27.9007, MinusLogProbMetric: 27.9007, val_loss: 28.1656, val_MinusLogProbMetric: 28.1656

Epoch 721: val_loss did not improve from 28.08475
196/196 - 53s - loss: 27.9007 - MinusLogProbMetric: 27.9007 - val_loss: 28.1656 - val_MinusLogProbMetric: 28.1656 - lr: 1.2500e-04 - 53s/epoch - 271ms/step
Epoch 722/1000
2023-09-29 03:11:36.625 
Epoch 722/1000 
	 loss: 27.8984, MinusLogProbMetric: 27.8984, val_loss: 28.0996, val_MinusLogProbMetric: 28.0996

Epoch 722: val_loss did not improve from 28.08475
196/196 - 54s - loss: 27.8984 - MinusLogProbMetric: 27.8984 - val_loss: 28.0996 - val_MinusLogProbMetric: 28.0996 - lr: 1.2500e-04 - 54s/epoch - 274ms/step
Epoch 723/1000
2023-09-29 03:12:31.089 
Epoch 723/1000 
	 loss: 27.8901, MinusLogProbMetric: 27.8901, val_loss: 28.1097, val_MinusLogProbMetric: 28.1097

Epoch 723: val_loss did not improve from 28.08475
196/196 - 54s - loss: 27.8901 - MinusLogProbMetric: 27.8901 - val_loss: 28.1097 - val_MinusLogProbMetric: 28.1097 - lr: 1.2500e-04 - 54s/epoch - 278ms/step
Epoch 724/1000
2023-09-29 03:13:25.247 
Epoch 724/1000 
	 loss: 27.8970, MinusLogProbMetric: 27.8970, val_loss: 28.2190, val_MinusLogProbMetric: 28.2190

Epoch 724: val_loss did not improve from 28.08475
196/196 - 54s - loss: 27.8970 - MinusLogProbMetric: 27.8970 - val_loss: 28.2190 - val_MinusLogProbMetric: 28.2190 - lr: 1.2500e-04 - 54s/epoch - 276ms/step
Epoch 725/1000
2023-09-29 03:14:19.751 
Epoch 725/1000 
	 loss: 27.9193, MinusLogProbMetric: 27.9193, val_loss: 28.1160, val_MinusLogProbMetric: 28.1160

Epoch 725: val_loss did not improve from 28.08475
196/196 - 54s - loss: 27.9193 - MinusLogProbMetric: 27.9193 - val_loss: 28.1160 - val_MinusLogProbMetric: 28.1160 - lr: 1.2500e-04 - 54s/epoch - 278ms/step
Epoch 726/1000
2023-09-29 03:15:13.666 
Epoch 726/1000 
	 loss: 27.8947, MinusLogProbMetric: 27.8947, val_loss: 28.1284, val_MinusLogProbMetric: 28.1284

Epoch 726: val_loss did not improve from 28.08475
196/196 - 54s - loss: 27.8947 - MinusLogProbMetric: 27.8947 - val_loss: 28.1284 - val_MinusLogProbMetric: 28.1284 - lr: 1.2500e-04 - 54s/epoch - 275ms/step
Epoch 727/1000
2023-09-29 03:16:07.349 
Epoch 727/1000 
	 loss: 27.8977, MinusLogProbMetric: 27.8977, val_loss: 28.1130, val_MinusLogProbMetric: 28.1130

Epoch 727: val_loss did not improve from 28.08475
196/196 - 54s - loss: 27.8977 - MinusLogProbMetric: 27.8977 - val_loss: 28.1130 - val_MinusLogProbMetric: 28.1130 - lr: 1.2500e-04 - 54s/epoch - 274ms/step
Epoch 728/1000
2023-09-29 03:17:00.007 
Epoch 728/1000 
	 loss: 27.8951, MinusLogProbMetric: 27.8951, val_loss: 28.1086, val_MinusLogProbMetric: 28.1086

Epoch 728: val_loss did not improve from 28.08475
196/196 - 53s - loss: 27.8951 - MinusLogProbMetric: 27.8951 - val_loss: 28.1086 - val_MinusLogProbMetric: 28.1086 - lr: 1.2500e-04 - 53s/epoch - 269ms/step
Epoch 729/1000
2023-09-29 03:17:52.919 
Epoch 729/1000 
	 loss: 27.8908, MinusLogProbMetric: 27.8908, val_loss: 28.1632, val_MinusLogProbMetric: 28.1632

Epoch 729: val_loss did not improve from 28.08475
196/196 - 53s - loss: 27.8908 - MinusLogProbMetric: 27.8908 - val_loss: 28.1632 - val_MinusLogProbMetric: 28.1632 - lr: 1.2500e-04 - 53s/epoch - 270ms/step
Epoch 730/1000
2023-09-29 03:18:47.568 
Epoch 730/1000 
	 loss: 27.9026, MinusLogProbMetric: 27.9026, val_loss: 28.1994, val_MinusLogProbMetric: 28.1994

Epoch 730: val_loss did not improve from 28.08475
196/196 - 55s - loss: 27.9026 - MinusLogProbMetric: 27.9026 - val_loss: 28.1994 - val_MinusLogProbMetric: 28.1994 - lr: 1.2500e-04 - 55s/epoch - 279ms/step
Epoch 731/1000
2023-09-29 03:19:38.857 
Epoch 731/1000 
	 loss: 27.8929, MinusLogProbMetric: 27.8929, val_loss: 28.0980, val_MinusLogProbMetric: 28.0980

Epoch 731: val_loss did not improve from 28.08475
196/196 - 51s - loss: 27.8929 - MinusLogProbMetric: 27.8929 - val_loss: 28.0980 - val_MinusLogProbMetric: 28.0980 - lr: 1.2500e-04 - 51s/epoch - 262ms/step
Epoch 732/1000
2023-09-29 03:20:31.726 
Epoch 732/1000 
	 loss: 27.8982, MinusLogProbMetric: 27.8982, val_loss: 28.1191, val_MinusLogProbMetric: 28.1191

Epoch 732: val_loss did not improve from 28.08475
196/196 - 53s - loss: 27.8982 - MinusLogProbMetric: 27.8982 - val_loss: 28.1191 - val_MinusLogProbMetric: 28.1191 - lr: 1.2500e-04 - 53s/epoch - 270ms/step
Epoch 733/1000
2023-09-29 03:21:26.306 
Epoch 733/1000 
	 loss: 27.9073, MinusLogProbMetric: 27.9073, val_loss: 28.1079, val_MinusLogProbMetric: 28.1079

Epoch 733: val_loss did not improve from 28.08475
196/196 - 55s - loss: 27.9073 - MinusLogProbMetric: 27.9073 - val_loss: 28.1079 - val_MinusLogProbMetric: 28.1079 - lr: 1.2500e-04 - 55s/epoch - 278ms/step
Epoch 734/1000
2023-09-29 03:22:20.205 
Epoch 734/1000 
	 loss: 27.8971, MinusLogProbMetric: 27.8971, val_loss: 28.1303, val_MinusLogProbMetric: 28.1303

Epoch 734: val_loss did not improve from 28.08475
196/196 - 54s - loss: 27.8971 - MinusLogProbMetric: 27.8971 - val_loss: 28.1303 - val_MinusLogProbMetric: 28.1303 - lr: 1.2500e-04 - 54s/epoch - 275ms/step
Epoch 735/1000
2023-09-29 03:23:13.892 
Epoch 735/1000 
	 loss: 27.8982, MinusLogProbMetric: 27.8982, val_loss: 28.1693, val_MinusLogProbMetric: 28.1693

Epoch 735: val_loss did not improve from 28.08475
196/196 - 54s - loss: 27.8982 - MinusLogProbMetric: 27.8982 - val_loss: 28.1693 - val_MinusLogProbMetric: 28.1693 - lr: 1.2500e-04 - 54s/epoch - 274ms/step
Epoch 736/1000
2023-09-29 03:24:07.753 
Epoch 736/1000 
	 loss: 27.9016, MinusLogProbMetric: 27.9016, val_loss: 28.1634, val_MinusLogProbMetric: 28.1634

Epoch 736: val_loss did not improve from 28.08475
196/196 - 54s - loss: 27.9016 - MinusLogProbMetric: 27.9016 - val_loss: 28.1634 - val_MinusLogProbMetric: 28.1634 - lr: 1.2500e-04 - 54s/epoch - 275ms/step
Epoch 737/1000
2023-09-29 03:25:00.099 
Epoch 737/1000 
	 loss: 27.8996, MinusLogProbMetric: 27.8996, val_loss: 28.1456, val_MinusLogProbMetric: 28.1456

Epoch 737: val_loss did not improve from 28.08475
196/196 - 52s - loss: 27.8996 - MinusLogProbMetric: 27.8996 - val_loss: 28.1456 - val_MinusLogProbMetric: 28.1456 - lr: 1.2500e-04 - 52s/epoch - 267ms/step
Epoch 738/1000
2023-09-29 03:26:15.289 
Epoch 738/1000 
	 loss: 27.8789, MinusLogProbMetric: 27.8789, val_loss: 28.1126, val_MinusLogProbMetric: 28.1126

Epoch 738: val_loss did not improve from 28.08475
196/196 - 75s - loss: 27.8789 - MinusLogProbMetric: 27.8789 - val_loss: 28.1126 - val_MinusLogProbMetric: 28.1126 - lr: 1.2500e-04 - 75s/epoch - 384ms/step
Epoch 739/1000
2023-09-29 03:27:22.255 
Epoch 739/1000 
	 loss: 27.8958, MinusLogProbMetric: 27.8958, val_loss: 28.2030, val_MinusLogProbMetric: 28.2030

Epoch 739: val_loss did not improve from 28.08475
196/196 - 67s - loss: 27.8958 - MinusLogProbMetric: 27.8958 - val_loss: 28.2030 - val_MinusLogProbMetric: 28.2030 - lr: 1.2500e-04 - 67s/epoch - 342ms/step
Epoch 740/1000
2023-09-29 03:28:22.488 
Epoch 740/1000 
	 loss: 27.8979, MinusLogProbMetric: 27.8979, val_loss: 28.2515, val_MinusLogProbMetric: 28.2515

Epoch 740: val_loss did not improve from 28.08475
196/196 - 60s - loss: 27.8979 - MinusLogProbMetric: 27.8979 - val_loss: 28.2515 - val_MinusLogProbMetric: 28.2515 - lr: 1.2500e-04 - 60s/epoch - 307ms/step
Epoch 741/1000
2023-09-29 03:29:18.951 
Epoch 741/1000 
	 loss: 27.9118, MinusLogProbMetric: 27.9118, val_loss: 28.2977, val_MinusLogProbMetric: 28.2977

Epoch 741: val_loss did not improve from 28.08475
196/196 - 56s - loss: 27.9118 - MinusLogProbMetric: 27.9118 - val_loss: 28.2977 - val_MinusLogProbMetric: 28.2977 - lr: 1.2500e-04 - 56s/epoch - 288ms/step
Epoch 742/1000
2023-09-29 03:30:07.131 
Epoch 742/1000 
	 loss: 27.9070, MinusLogProbMetric: 27.9070, val_loss: 28.0795, val_MinusLogProbMetric: 28.0795

Epoch 742: val_loss improved from 28.08475 to 28.07950, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_323/weights/best_weights.h5
196/196 - 49s - loss: 27.9070 - MinusLogProbMetric: 27.9070 - val_loss: 28.0795 - val_MinusLogProbMetric: 28.0795 - lr: 1.2500e-04 - 49s/epoch - 251ms/step
Epoch 743/1000
2023-09-29 03:31:03.128 
Epoch 743/1000 
	 loss: 27.8856, MinusLogProbMetric: 27.8856, val_loss: 28.0895, val_MinusLogProbMetric: 28.0895

Epoch 743: val_loss did not improve from 28.07950
196/196 - 55s - loss: 27.8856 - MinusLogProbMetric: 27.8856 - val_loss: 28.0895 - val_MinusLogProbMetric: 28.0895 - lr: 1.2500e-04 - 55s/epoch - 281ms/step
Epoch 744/1000
2023-09-29 03:31:57.064 
Epoch 744/1000 
	 loss: 27.8958, MinusLogProbMetric: 27.8958, val_loss: 28.1314, val_MinusLogProbMetric: 28.1314

Epoch 744: val_loss did not improve from 28.07950
196/196 - 54s - loss: 27.8958 - MinusLogProbMetric: 27.8958 - val_loss: 28.1314 - val_MinusLogProbMetric: 28.1314 - lr: 1.2500e-04 - 54s/epoch - 275ms/step
Epoch 745/1000
2023-09-29 03:32:51.475 
Epoch 745/1000 
	 loss: 27.8903, MinusLogProbMetric: 27.8903, val_loss: 28.1323, val_MinusLogProbMetric: 28.1323

Epoch 745: val_loss did not improve from 28.07950
196/196 - 54s - loss: 27.8903 - MinusLogProbMetric: 27.8903 - val_loss: 28.1323 - val_MinusLogProbMetric: 28.1323 - lr: 1.2500e-04 - 54s/epoch - 278ms/step
Epoch 746/1000
2023-09-29 03:33:46.268 
Epoch 746/1000 
	 loss: 27.8987, MinusLogProbMetric: 27.8987, val_loss: 28.0876, val_MinusLogProbMetric: 28.0876

Epoch 746: val_loss did not improve from 28.07950
196/196 - 55s - loss: 27.8987 - MinusLogProbMetric: 27.8987 - val_loss: 28.0876 - val_MinusLogProbMetric: 28.0876 - lr: 1.2500e-04 - 55s/epoch - 280ms/step
Epoch 747/1000
2023-09-29 03:34:40.951 
Epoch 747/1000 
	 loss: 27.8957, MinusLogProbMetric: 27.8957, val_loss: 28.1558, val_MinusLogProbMetric: 28.1558

Epoch 747: val_loss did not improve from 28.07950
196/196 - 55s - loss: 27.8957 - MinusLogProbMetric: 27.8957 - val_loss: 28.1558 - val_MinusLogProbMetric: 28.1558 - lr: 1.2500e-04 - 55s/epoch - 279ms/step
Epoch 748/1000
2023-09-29 03:35:34.872 
Epoch 748/1000 
	 loss: 27.9039, MinusLogProbMetric: 27.9039, val_loss: 28.1434, val_MinusLogProbMetric: 28.1434

Epoch 748: val_loss did not improve from 28.07950
196/196 - 54s - loss: 27.9039 - MinusLogProbMetric: 27.9039 - val_loss: 28.1434 - val_MinusLogProbMetric: 28.1434 - lr: 1.2500e-04 - 54s/epoch - 275ms/step
Epoch 749/1000
2023-09-29 03:36:26.731 
Epoch 749/1000 
	 loss: 27.8984, MinusLogProbMetric: 27.8984, val_loss: 28.1750, val_MinusLogProbMetric: 28.1750

Epoch 749: val_loss did not improve from 28.07950
196/196 - 52s - loss: 27.8984 - MinusLogProbMetric: 27.8984 - val_loss: 28.1750 - val_MinusLogProbMetric: 28.1750 - lr: 1.2500e-04 - 52s/epoch - 265ms/step
Epoch 750/1000
2023-09-29 03:37:17.719 
Epoch 750/1000 
	 loss: 27.8889, MinusLogProbMetric: 27.8889, val_loss: 28.1960, val_MinusLogProbMetric: 28.1960

Epoch 750: val_loss did not improve from 28.07950
196/196 - 51s - loss: 27.8889 - MinusLogProbMetric: 27.8889 - val_loss: 28.1960 - val_MinusLogProbMetric: 28.1960 - lr: 1.2500e-04 - 51s/epoch - 260ms/step
Epoch 751/1000
2023-09-29 03:38:12.829 
Epoch 751/1000 
	 loss: 27.9094, MinusLogProbMetric: 27.9094, val_loss: 28.1314, val_MinusLogProbMetric: 28.1314

Epoch 751: val_loss did not improve from 28.07950
196/196 - 55s - loss: 27.9094 - MinusLogProbMetric: 27.9094 - val_loss: 28.1314 - val_MinusLogProbMetric: 28.1314 - lr: 1.2500e-04 - 55s/epoch - 281ms/step
Epoch 752/1000
2023-09-29 03:39:07.512 
Epoch 752/1000 
	 loss: 27.8909, MinusLogProbMetric: 27.8909, val_loss: 28.0783, val_MinusLogProbMetric: 28.0783

Epoch 752: val_loss improved from 28.07950 to 28.07825, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_323/weights/best_weights.h5
196/196 - 56s - loss: 27.8909 - MinusLogProbMetric: 27.8909 - val_loss: 28.0783 - val_MinusLogProbMetric: 28.0783 - lr: 1.2500e-04 - 56s/epoch - 284ms/step
Epoch 753/1000
2023-09-29 03:40:03.202 
Epoch 753/1000 
	 loss: 27.8915, MinusLogProbMetric: 27.8915, val_loss: 28.1289, val_MinusLogProbMetric: 28.1289

Epoch 753: val_loss did not improve from 28.07825
196/196 - 55s - loss: 27.8915 - MinusLogProbMetric: 27.8915 - val_loss: 28.1289 - val_MinusLogProbMetric: 28.1289 - lr: 1.2500e-04 - 55s/epoch - 279ms/step
Epoch 754/1000
2023-09-29 03:40:57.902 
Epoch 754/1000 
	 loss: 27.9024, MinusLogProbMetric: 27.9024, val_loss: 28.1903, val_MinusLogProbMetric: 28.1903

Epoch 754: val_loss did not improve from 28.07825
196/196 - 55s - loss: 27.9024 - MinusLogProbMetric: 27.9024 - val_loss: 28.1903 - val_MinusLogProbMetric: 28.1903 - lr: 1.2500e-04 - 55s/epoch - 279ms/step
Epoch 755/1000
2023-09-29 03:41:52.609 
Epoch 755/1000 
	 loss: 27.9002, MinusLogProbMetric: 27.9002, val_loss: 28.1157, val_MinusLogProbMetric: 28.1157

Epoch 755: val_loss did not improve from 28.07825
196/196 - 55s - loss: 27.9002 - MinusLogProbMetric: 27.9002 - val_loss: 28.1157 - val_MinusLogProbMetric: 28.1157 - lr: 1.2500e-04 - 55s/epoch - 279ms/step
Epoch 756/1000
2023-09-29 03:42:48.246 
Epoch 756/1000 
	 loss: 27.9020, MinusLogProbMetric: 27.9020, val_loss: 28.1793, val_MinusLogProbMetric: 28.1793

Epoch 756: val_loss did not improve from 28.07825
196/196 - 56s - loss: 27.9020 - MinusLogProbMetric: 27.9020 - val_loss: 28.1793 - val_MinusLogProbMetric: 28.1793 - lr: 1.2500e-04 - 56s/epoch - 284ms/step
Epoch 757/1000
2023-09-29 03:43:43.157 
Epoch 757/1000 
	 loss: 27.8916, MinusLogProbMetric: 27.8916, val_loss: 28.1401, val_MinusLogProbMetric: 28.1401

Epoch 757: val_loss did not improve from 28.07825
196/196 - 55s - loss: 27.8916 - MinusLogProbMetric: 27.8916 - val_loss: 28.1401 - val_MinusLogProbMetric: 28.1401 - lr: 1.2500e-04 - 55s/epoch - 280ms/step
Epoch 758/1000
2023-09-29 03:44:38.386 
Epoch 758/1000 
	 loss: 27.9002, MinusLogProbMetric: 27.9002, val_loss: 28.1173, val_MinusLogProbMetric: 28.1173

Epoch 758: val_loss did not improve from 28.07825
196/196 - 55s - loss: 27.9002 - MinusLogProbMetric: 27.9002 - val_loss: 28.1173 - val_MinusLogProbMetric: 28.1173 - lr: 1.2500e-04 - 55s/epoch - 282ms/step
Epoch 759/1000
2023-09-29 03:45:34.101 
Epoch 759/1000 
	 loss: 27.8849, MinusLogProbMetric: 27.8849, val_loss: 28.2304, val_MinusLogProbMetric: 28.2304

Epoch 759: val_loss did not improve from 28.07825
196/196 - 56s - loss: 27.8849 - MinusLogProbMetric: 27.8849 - val_loss: 28.2304 - val_MinusLogProbMetric: 28.2304 - lr: 1.2500e-04 - 56s/epoch - 284ms/step
Epoch 760/1000
2023-09-29 03:46:28.984 
Epoch 760/1000 
	 loss: 27.8918, MinusLogProbMetric: 27.8918, val_loss: 28.1447, val_MinusLogProbMetric: 28.1447

Epoch 760: val_loss did not improve from 28.07825
196/196 - 55s - loss: 27.8918 - MinusLogProbMetric: 27.8918 - val_loss: 28.1447 - val_MinusLogProbMetric: 28.1447 - lr: 1.2500e-04 - 55s/epoch - 280ms/step
Epoch 761/1000
2023-09-29 03:47:24.096 
Epoch 761/1000 
	 loss: 27.8916, MinusLogProbMetric: 27.8916, val_loss: 28.2576, val_MinusLogProbMetric: 28.2576

Epoch 761: val_loss did not improve from 28.07825
196/196 - 55s - loss: 27.8916 - MinusLogProbMetric: 27.8916 - val_loss: 28.2576 - val_MinusLogProbMetric: 28.2576 - lr: 1.2500e-04 - 55s/epoch - 281ms/step
Epoch 762/1000
2023-09-29 03:48:16.863 
Epoch 762/1000 
	 loss: 27.8903, MinusLogProbMetric: 27.8903, val_loss: 28.0786, val_MinusLogProbMetric: 28.0786

Epoch 762: val_loss did not improve from 28.07825
196/196 - 53s - loss: 27.8903 - MinusLogProbMetric: 27.8903 - val_loss: 28.0786 - val_MinusLogProbMetric: 28.0786 - lr: 1.2500e-04 - 53s/epoch - 269ms/step
Epoch 763/1000
2023-09-29 03:49:11.036 
Epoch 763/1000 
	 loss: 27.8938, MinusLogProbMetric: 27.8938, val_loss: 28.0829, val_MinusLogProbMetric: 28.0829

Epoch 763: val_loss did not improve from 28.07825
196/196 - 54s - loss: 27.8938 - MinusLogProbMetric: 27.8938 - val_loss: 28.0829 - val_MinusLogProbMetric: 28.0829 - lr: 1.2500e-04 - 54s/epoch - 276ms/step
Epoch 764/1000
2023-09-29 03:50:05.094 
Epoch 764/1000 
	 loss: 27.9053, MinusLogProbMetric: 27.9053, val_loss: 28.1389, val_MinusLogProbMetric: 28.1389

Epoch 764: val_loss did not improve from 28.07825
196/196 - 54s - loss: 27.9053 - MinusLogProbMetric: 27.9053 - val_loss: 28.1389 - val_MinusLogProbMetric: 28.1389 - lr: 1.2500e-04 - 54s/epoch - 276ms/step
Epoch 765/1000
2023-09-29 03:50:59.254 
Epoch 765/1000 
	 loss: 27.8867, MinusLogProbMetric: 27.8867, val_loss: 28.1307, val_MinusLogProbMetric: 28.1307

Epoch 765: val_loss did not improve from 28.07825
196/196 - 54s - loss: 27.8867 - MinusLogProbMetric: 27.8867 - val_loss: 28.1307 - val_MinusLogProbMetric: 28.1307 - lr: 1.2500e-04 - 54s/epoch - 276ms/step
Epoch 766/1000
2023-09-29 03:51:52.975 
Epoch 766/1000 
	 loss: 27.8961, MinusLogProbMetric: 27.8961, val_loss: 28.0937, val_MinusLogProbMetric: 28.0937

Epoch 766: val_loss did not improve from 28.07825
196/196 - 54s - loss: 27.8961 - MinusLogProbMetric: 27.8961 - val_loss: 28.0937 - val_MinusLogProbMetric: 28.0937 - lr: 1.2500e-04 - 54s/epoch - 274ms/step
Epoch 767/1000
2023-09-29 03:52:46.715 
Epoch 767/1000 
	 loss: 27.8960, MinusLogProbMetric: 27.8960, val_loss: 28.1068, val_MinusLogProbMetric: 28.1068

Epoch 767: val_loss did not improve from 28.07825
196/196 - 54s - loss: 27.8960 - MinusLogProbMetric: 27.8960 - val_loss: 28.1068 - val_MinusLogProbMetric: 28.1068 - lr: 1.2500e-04 - 54s/epoch - 274ms/step
Epoch 768/1000
2023-09-29 03:53:39.617 
Epoch 768/1000 
	 loss: 27.8861, MinusLogProbMetric: 27.8861, val_loss: 28.0831, val_MinusLogProbMetric: 28.0831

Epoch 768: val_loss did not improve from 28.07825
196/196 - 53s - loss: 27.8861 - MinusLogProbMetric: 27.8861 - val_loss: 28.0831 - val_MinusLogProbMetric: 28.0831 - lr: 1.2500e-04 - 53s/epoch - 270ms/step
Epoch 769/1000
2023-09-29 03:54:34.395 
Epoch 769/1000 
	 loss: 27.8945, MinusLogProbMetric: 27.8945, val_loss: 28.1660, val_MinusLogProbMetric: 28.1660

Epoch 769: val_loss did not improve from 28.07825
196/196 - 55s - loss: 27.8945 - MinusLogProbMetric: 27.8945 - val_loss: 28.1660 - val_MinusLogProbMetric: 28.1660 - lr: 1.2500e-04 - 55s/epoch - 279ms/step
Epoch 770/1000
2023-09-29 03:55:27.842 
Epoch 770/1000 
	 loss: 27.9036, MinusLogProbMetric: 27.9036, val_loss: 28.0784, val_MinusLogProbMetric: 28.0784

Epoch 770: val_loss did not improve from 28.07825
196/196 - 53s - loss: 27.9036 - MinusLogProbMetric: 27.9036 - val_loss: 28.0784 - val_MinusLogProbMetric: 28.0784 - lr: 1.2500e-04 - 53s/epoch - 273ms/step
Epoch 771/1000
2023-09-29 03:56:20.714 
Epoch 771/1000 
	 loss: 27.8834, MinusLogProbMetric: 27.8834, val_loss: 28.0920, val_MinusLogProbMetric: 28.0920

Epoch 771: val_loss did not improve from 28.07825
196/196 - 53s - loss: 27.8834 - MinusLogProbMetric: 27.8834 - val_loss: 28.0920 - val_MinusLogProbMetric: 28.0920 - lr: 1.2500e-04 - 53s/epoch - 270ms/step
Epoch 772/1000
2023-09-29 03:57:15.570 
Epoch 772/1000 
	 loss: 27.8954, MinusLogProbMetric: 27.8954, val_loss: 28.1886, val_MinusLogProbMetric: 28.1886

Epoch 772: val_loss did not improve from 28.07825
196/196 - 55s - loss: 27.8954 - MinusLogProbMetric: 27.8954 - val_loss: 28.1886 - val_MinusLogProbMetric: 28.1886 - lr: 1.2500e-04 - 55s/epoch - 280ms/step
Epoch 773/1000
2023-09-29 03:58:09.981 
Epoch 773/1000 
	 loss: 27.8852, MinusLogProbMetric: 27.8852, val_loss: 28.2101, val_MinusLogProbMetric: 28.2101

Epoch 773: val_loss did not improve from 28.07825
196/196 - 54s - loss: 27.8852 - MinusLogProbMetric: 27.8852 - val_loss: 28.2101 - val_MinusLogProbMetric: 28.2101 - lr: 1.2500e-04 - 54s/epoch - 278ms/step
Epoch 774/1000
2023-09-29 03:59:04.597 
Epoch 774/1000 
	 loss: 27.8891, MinusLogProbMetric: 27.8891, val_loss: 28.1345, val_MinusLogProbMetric: 28.1345

Epoch 774: val_loss did not improve from 28.07825
196/196 - 55s - loss: 27.8891 - MinusLogProbMetric: 27.8891 - val_loss: 28.1345 - val_MinusLogProbMetric: 28.1345 - lr: 1.2500e-04 - 55s/epoch - 279ms/step
Epoch 775/1000
2023-09-29 03:59:57.800 
Epoch 775/1000 
	 loss: 27.8778, MinusLogProbMetric: 27.8778, val_loss: 28.1057, val_MinusLogProbMetric: 28.1057

Epoch 775: val_loss did not improve from 28.07825
196/196 - 53s - loss: 27.8778 - MinusLogProbMetric: 27.8778 - val_loss: 28.1057 - val_MinusLogProbMetric: 28.1057 - lr: 1.2500e-04 - 53s/epoch - 271ms/step
Epoch 776/1000
2023-09-29 04:00:52.638 
Epoch 776/1000 
	 loss: 27.8793, MinusLogProbMetric: 27.8793, val_loss: 28.1082, val_MinusLogProbMetric: 28.1082

Epoch 776: val_loss did not improve from 28.07825
196/196 - 55s - loss: 27.8793 - MinusLogProbMetric: 27.8793 - val_loss: 28.1082 - val_MinusLogProbMetric: 28.1082 - lr: 1.2500e-04 - 55s/epoch - 280ms/step
Epoch 777/1000
2023-09-29 04:01:46.686 
Epoch 777/1000 
	 loss: 27.8969, MinusLogProbMetric: 27.8969, val_loss: 28.1265, val_MinusLogProbMetric: 28.1265

Epoch 777: val_loss did not improve from 28.07825
196/196 - 54s - loss: 27.8969 - MinusLogProbMetric: 27.8969 - val_loss: 28.1265 - val_MinusLogProbMetric: 28.1265 - lr: 1.2500e-04 - 54s/epoch - 276ms/step
Epoch 778/1000
2023-09-29 04:02:41.910 
Epoch 778/1000 
	 loss: 27.8785, MinusLogProbMetric: 27.8785, val_loss: 28.0988, val_MinusLogProbMetric: 28.0988

Epoch 778: val_loss did not improve from 28.07825
196/196 - 55s - loss: 27.8785 - MinusLogProbMetric: 27.8785 - val_loss: 28.0988 - val_MinusLogProbMetric: 28.0988 - lr: 1.2500e-04 - 55s/epoch - 282ms/step
Epoch 779/1000
2023-09-29 04:03:36.877 
Epoch 779/1000 
	 loss: 27.8690, MinusLogProbMetric: 27.8690, val_loss: 28.1306, val_MinusLogProbMetric: 28.1306

Epoch 779: val_loss did not improve from 28.07825
196/196 - 55s - loss: 27.8690 - MinusLogProbMetric: 27.8690 - val_loss: 28.1306 - val_MinusLogProbMetric: 28.1306 - lr: 1.2500e-04 - 55s/epoch - 280ms/step
Epoch 780/1000
2023-09-29 04:04:31.234 
Epoch 780/1000 
	 loss: 27.8746, MinusLogProbMetric: 27.8746, val_loss: 28.1078, val_MinusLogProbMetric: 28.1078

Epoch 780: val_loss did not improve from 28.07825
196/196 - 54s - loss: 27.8746 - MinusLogProbMetric: 27.8746 - val_loss: 28.1078 - val_MinusLogProbMetric: 28.1078 - lr: 1.2500e-04 - 54s/epoch - 277ms/step
Epoch 781/1000
2023-09-29 04:05:26.559 
Epoch 781/1000 
	 loss: 27.8691, MinusLogProbMetric: 27.8691, val_loss: 28.0767, val_MinusLogProbMetric: 28.0767

Epoch 781: val_loss improved from 28.07825 to 28.07672, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_323/weights/best_weights.h5
196/196 - 56s - loss: 27.8691 - MinusLogProbMetric: 27.8691 - val_loss: 28.0767 - val_MinusLogProbMetric: 28.0767 - lr: 1.2500e-04 - 56s/epoch - 286ms/step
Epoch 782/1000
2023-09-29 04:06:22.647 
Epoch 782/1000 
	 loss: 27.8769, MinusLogProbMetric: 27.8769, val_loss: 28.0946, val_MinusLogProbMetric: 28.0946

Epoch 782: val_loss did not improve from 28.07672
196/196 - 55s - loss: 27.8769 - MinusLogProbMetric: 27.8769 - val_loss: 28.0946 - val_MinusLogProbMetric: 28.0946 - lr: 1.2500e-04 - 55s/epoch - 282ms/step
Epoch 783/1000
2023-09-29 04:07:17.456 
Epoch 783/1000 
	 loss: 27.8824, MinusLogProbMetric: 27.8824, val_loss: 28.1230, val_MinusLogProbMetric: 28.1230

Epoch 783: val_loss did not improve from 28.07672
196/196 - 55s - loss: 27.8824 - MinusLogProbMetric: 27.8824 - val_loss: 28.1230 - val_MinusLogProbMetric: 28.1230 - lr: 1.2500e-04 - 55s/epoch - 280ms/step
Epoch 784/1000
2023-09-29 04:08:11.954 
Epoch 784/1000 
	 loss: 27.8856, MinusLogProbMetric: 27.8856, val_loss: 28.1563, val_MinusLogProbMetric: 28.1563

Epoch 784: val_loss did not improve from 28.07672
196/196 - 54s - loss: 27.8856 - MinusLogProbMetric: 27.8856 - val_loss: 28.1563 - val_MinusLogProbMetric: 28.1563 - lr: 1.2500e-04 - 54s/epoch - 278ms/step
Epoch 785/1000
2023-09-29 04:09:07.208 
Epoch 785/1000 
	 loss: 27.8722, MinusLogProbMetric: 27.8722, val_loss: 28.1428, val_MinusLogProbMetric: 28.1428

Epoch 785: val_loss did not improve from 28.07672
196/196 - 55s - loss: 27.8722 - MinusLogProbMetric: 27.8722 - val_loss: 28.1428 - val_MinusLogProbMetric: 28.1428 - lr: 1.2500e-04 - 55s/epoch - 282ms/step
Epoch 786/1000
2023-09-29 04:10:00.109 
Epoch 786/1000 
	 loss: 27.8742, MinusLogProbMetric: 27.8742, val_loss: 28.0896, val_MinusLogProbMetric: 28.0896

Epoch 786: val_loss did not improve from 28.07672
196/196 - 53s - loss: 27.8742 - MinusLogProbMetric: 27.8742 - val_loss: 28.0896 - val_MinusLogProbMetric: 28.0896 - lr: 1.2500e-04 - 53s/epoch - 270ms/step
Epoch 787/1000
2023-09-29 04:10:55.666 
Epoch 787/1000 
	 loss: 27.8864, MinusLogProbMetric: 27.8864, val_loss: 28.0990, val_MinusLogProbMetric: 28.0990

Epoch 787: val_loss did not improve from 28.07672
196/196 - 56s - loss: 27.8864 - MinusLogProbMetric: 27.8864 - val_loss: 28.0990 - val_MinusLogProbMetric: 28.0990 - lr: 1.2500e-04 - 56s/epoch - 283ms/step
Epoch 788/1000
2023-09-29 04:11:48.937 
Epoch 788/1000 
	 loss: 27.8808, MinusLogProbMetric: 27.8808, val_loss: 28.0672, val_MinusLogProbMetric: 28.0672

Epoch 788: val_loss improved from 28.07672 to 28.06721, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_323/weights/best_weights.h5
196/196 - 54s - loss: 27.8808 - MinusLogProbMetric: 27.8808 - val_loss: 28.0672 - val_MinusLogProbMetric: 28.0672 - lr: 1.2500e-04 - 54s/epoch - 277ms/step
Epoch 789/1000
2023-09-29 04:12:43.715 
Epoch 789/1000 
	 loss: 27.8866, MinusLogProbMetric: 27.8866, val_loss: 28.1339, val_MinusLogProbMetric: 28.1339

Epoch 789: val_loss did not improve from 28.06721
196/196 - 54s - loss: 27.8866 - MinusLogProbMetric: 27.8866 - val_loss: 28.1339 - val_MinusLogProbMetric: 28.1339 - lr: 1.2500e-04 - 54s/epoch - 274ms/step
Epoch 790/1000
2023-09-29 04:13:37.539 
Epoch 790/1000 
	 loss: 27.8789, MinusLogProbMetric: 27.8789, val_loss: 28.0670, val_MinusLogProbMetric: 28.0670

Epoch 790: val_loss improved from 28.06721 to 28.06698, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_323/weights/best_weights.h5
196/196 - 55s - loss: 27.8789 - MinusLogProbMetric: 27.8789 - val_loss: 28.0670 - val_MinusLogProbMetric: 28.0670 - lr: 1.2500e-04 - 55s/epoch - 280ms/step
Epoch 791/1000
2023-09-29 04:14:33.074 
Epoch 791/1000 
	 loss: 27.8777, MinusLogProbMetric: 27.8777, val_loss: 28.1300, val_MinusLogProbMetric: 28.1300

Epoch 791: val_loss did not improve from 28.06698
196/196 - 55s - loss: 27.8777 - MinusLogProbMetric: 27.8777 - val_loss: 28.1300 - val_MinusLogProbMetric: 28.1300 - lr: 1.2500e-04 - 55s/epoch - 278ms/step
Epoch 792/1000
2023-09-29 04:15:26.979 
Epoch 792/1000 
	 loss: 27.8887, MinusLogProbMetric: 27.8887, val_loss: 28.1209, val_MinusLogProbMetric: 28.1209

Epoch 792: val_loss did not improve from 28.06698
196/196 - 54s - loss: 27.8887 - MinusLogProbMetric: 27.8887 - val_loss: 28.1209 - val_MinusLogProbMetric: 28.1209 - lr: 1.2500e-04 - 54s/epoch - 275ms/step
Epoch 793/1000
2023-09-29 04:16:20.365 
Epoch 793/1000 
	 loss: 27.8695, MinusLogProbMetric: 27.8695, val_loss: 28.1283, val_MinusLogProbMetric: 28.1283

Epoch 793: val_loss did not improve from 28.06698
196/196 - 53s - loss: 27.8695 - MinusLogProbMetric: 27.8695 - val_loss: 28.1283 - val_MinusLogProbMetric: 28.1283 - lr: 1.2500e-04 - 53s/epoch - 272ms/step
Epoch 794/1000
2023-09-29 04:17:15.951 
Epoch 794/1000 
	 loss: 27.8724, MinusLogProbMetric: 27.8724, val_loss: 28.1360, val_MinusLogProbMetric: 28.1360

Epoch 794: val_loss did not improve from 28.06698
196/196 - 56s - loss: 27.8724 - MinusLogProbMetric: 27.8724 - val_loss: 28.1360 - val_MinusLogProbMetric: 28.1360 - lr: 1.2500e-04 - 56s/epoch - 284ms/step
Epoch 795/1000
2023-09-29 04:18:11.284 
Epoch 795/1000 
	 loss: 27.8868, MinusLogProbMetric: 27.8868, val_loss: 28.0473, val_MinusLogProbMetric: 28.0473

Epoch 795: val_loss improved from 28.06698 to 28.04732, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_323/weights/best_weights.h5
196/196 - 56s - loss: 27.8868 - MinusLogProbMetric: 27.8868 - val_loss: 28.0473 - val_MinusLogProbMetric: 28.0473 - lr: 1.2500e-04 - 56s/epoch - 287ms/step
Epoch 796/1000
2023-09-29 04:19:07.959 
Epoch 796/1000 
	 loss: 27.8832, MinusLogProbMetric: 27.8832, val_loss: 28.1146, val_MinusLogProbMetric: 28.1146

Epoch 796: val_loss did not improve from 28.04732
196/196 - 56s - loss: 27.8832 - MinusLogProbMetric: 27.8832 - val_loss: 28.1146 - val_MinusLogProbMetric: 28.1146 - lr: 1.2500e-04 - 56s/epoch - 285ms/step
Epoch 797/1000
2023-09-29 04:20:01.314 
Epoch 797/1000 
	 loss: 27.8824, MinusLogProbMetric: 27.8824, val_loss: 28.1579, val_MinusLogProbMetric: 28.1579

Epoch 797: val_loss did not improve from 28.04732
196/196 - 53s - loss: 27.8824 - MinusLogProbMetric: 27.8824 - val_loss: 28.1579 - val_MinusLogProbMetric: 28.1579 - lr: 1.2500e-04 - 53s/epoch - 272ms/step
Epoch 798/1000
2023-09-29 04:20:56.389 
Epoch 798/1000 
	 loss: 27.8822, MinusLogProbMetric: 27.8822, val_loss: 28.1641, val_MinusLogProbMetric: 28.1641

Epoch 798: val_loss did not improve from 28.04732
196/196 - 55s - loss: 27.8822 - MinusLogProbMetric: 27.8822 - val_loss: 28.1641 - val_MinusLogProbMetric: 28.1641 - lr: 1.2500e-04 - 55s/epoch - 281ms/step
Epoch 799/1000
2023-09-29 04:21:51.155 
Epoch 799/1000 
	 loss: 27.8808, MinusLogProbMetric: 27.8808, val_loss: 28.0867, val_MinusLogProbMetric: 28.0867

Epoch 799: val_loss did not improve from 28.04732
196/196 - 55s - loss: 27.8808 - MinusLogProbMetric: 27.8808 - val_loss: 28.0867 - val_MinusLogProbMetric: 28.0867 - lr: 1.2500e-04 - 55s/epoch - 279ms/step
Epoch 800/1000
2023-09-29 04:22:47.019 
Epoch 800/1000 
	 loss: 27.8881, MinusLogProbMetric: 27.8881, val_loss: 28.0779, val_MinusLogProbMetric: 28.0779

Epoch 800: val_loss did not improve from 28.04732
196/196 - 56s - loss: 27.8881 - MinusLogProbMetric: 27.8881 - val_loss: 28.0779 - val_MinusLogProbMetric: 28.0779 - lr: 1.2500e-04 - 56s/epoch - 285ms/step
Epoch 801/1000
2023-09-29 04:23:41.455 
Epoch 801/1000 
	 loss: 27.8889, MinusLogProbMetric: 27.8889, val_loss: 28.0854, val_MinusLogProbMetric: 28.0854

Epoch 801: val_loss did not improve from 28.04732
196/196 - 54s - loss: 27.8889 - MinusLogProbMetric: 27.8889 - val_loss: 28.0854 - val_MinusLogProbMetric: 28.0854 - lr: 1.2500e-04 - 54s/epoch - 278ms/step
Epoch 802/1000
2023-09-29 04:24:35.989 
Epoch 802/1000 
	 loss: 27.8804, MinusLogProbMetric: 27.8804, val_loss: 28.1386, val_MinusLogProbMetric: 28.1386

Epoch 802: val_loss did not improve from 28.04732
196/196 - 55s - loss: 27.8804 - MinusLogProbMetric: 27.8804 - val_loss: 28.1386 - val_MinusLogProbMetric: 28.1386 - lr: 1.2500e-04 - 55s/epoch - 278ms/step
Epoch 803/1000
2023-09-29 04:25:28.688 
Epoch 803/1000 
	 loss: 27.8687, MinusLogProbMetric: 27.8687, val_loss: 28.1414, val_MinusLogProbMetric: 28.1414

Epoch 803: val_loss did not improve from 28.04732
196/196 - 53s - loss: 27.8687 - MinusLogProbMetric: 27.8687 - val_loss: 28.1414 - val_MinusLogProbMetric: 28.1414 - lr: 1.2500e-04 - 53s/epoch - 269ms/step
Epoch 804/1000
2023-09-29 04:26:23.202 
Epoch 804/1000 
	 loss: 27.8810, MinusLogProbMetric: 27.8810, val_loss: 28.0862, val_MinusLogProbMetric: 28.0862

Epoch 804: val_loss did not improve from 28.04732
196/196 - 55s - loss: 27.8810 - MinusLogProbMetric: 27.8810 - val_loss: 28.0862 - val_MinusLogProbMetric: 28.0862 - lr: 1.2500e-04 - 55s/epoch - 278ms/step
Epoch 805/1000
2023-09-29 04:27:17.537 
Epoch 805/1000 
	 loss: 27.8755, MinusLogProbMetric: 27.8755, val_loss: 28.1133, val_MinusLogProbMetric: 28.1133

Epoch 805: val_loss did not improve from 28.04732
196/196 - 54s - loss: 27.8755 - MinusLogProbMetric: 27.8755 - val_loss: 28.1133 - val_MinusLogProbMetric: 28.1133 - lr: 1.2500e-04 - 54s/epoch - 277ms/step
Epoch 806/1000
2023-09-29 04:28:11.687 
Epoch 806/1000 
	 loss: 27.8683, MinusLogProbMetric: 27.8683, val_loss: 28.1024, val_MinusLogProbMetric: 28.1024

Epoch 806: val_loss did not improve from 28.04732
196/196 - 54s - loss: 27.8683 - MinusLogProbMetric: 27.8683 - val_loss: 28.1024 - val_MinusLogProbMetric: 28.1024 - lr: 1.2500e-04 - 54s/epoch - 276ms/step
Epoch 807/1000
2023-09-29 04:29:05.434 
Epoch 807/1000 
	 loss: 27.8709, MinusLogProbMetric: 27.8709, val_loss: 28.0709, val_MinusLogProbMetric: 28.0709

Epoch 807: val_loss did not improve from 28.04732
196/196 - 54s - loss: 27.8709 - MinusLogProbMetric: 27.8709 - val_loss: 28.0709 - val_MinusLogProbMetric: 28.0709 - lr: 1.2500e-04 - 54s/epoch - 274ms/step
Epoch 808/1000
2023-09-29 04:29:58.592 
Epoch 808/1000 
	 loss: 27.8737, MinusLogProbMetric: 27.8737, val_loss: 28.2321, val_MinusLogProbMetric: 28.2321

Epoch 808: val_loss did not improve from 28.04732
196/196 - 53s - loss: 27.8737 - MinusLogProbMetric: 27.8737 - val_loss: 28.2321 - val_MinusLogProbMetric: 28.2321 - lr: 1.2500e-04 - 53s/epoch - 271ms/step
Epoch 809/1000
2023-09-29 04:30:47.499 
Epoch 809/1000 
	 loss: 27.8778, MinusLogProbMetric: 27.8778, val_loss: 28.1584, val_MinusLogProbMetric: 28.1584

Epoch 809: val_loss did not improve from 28.04732
196/196 - 49s - loss: 27.8778 - MinusLogProbMetric: 27.8778 - val_loss: 28.1584 - val_MinusLogProbMetric: 28.1584 - lr: 1.2500e-04 - 49s/epoch - 249ms/step
Epoch 810/1000
2023-09-29 04:31:38.798 
Epoch 810/1000 
	 loss: 27.8930, MinusLogProbMetric: 27.8930, val_loss: 28.0975, val_MinusLogProbMetric: 28.0975

Epoch 810: val_loss did not improve from 28.04732
196/196 - 51s - loss: 27.8930 - MinusLogProbMetric: 27.8930 - val_loss: 28.0975 - val_MinusLogProbMetric: 28.0975 - lr: 1.2500e-04 - 51s/epoch - 262ms/step
Epoch 811/1000
2023-09-29 04:32:30.724 
Epoch 811/1000 
	 loss: 27.8732, MinusLogProbMetric: 27.8732, val_loss: 28.0861, val_MinusLogProbMetric: 28.0861

Epoch 811: val_loss did not improve from 28.04732
196/196 - 52s - loss: 27.8732 - MinusLogProbMetric: 27.8732 - val_loss: 28.0861 - val_MinusLogProbMetric: 28.0861 - lr: 1.2500e-04 - 52s/epoch - 265ms/step
Epoch 812/1000
2023-09-29 04:33:20.662 
Epoch 812/1000 
	 loss: 27.8713, MinusLogProbMetric: 27.8713, val_loss: 28.1243, val_MinusLogProbMetric: 28.1243

Epoch 812: val_loss did not improve from 28.04732
196/196 - 50s - loss: 27.8713 - MinusLogProbMetric: 27.8713 - val_loss: 28.1243 - val_MinusLogProbMetric: 28.1243 - lr: 1.2500e-04 - 50s/epoch - 255ms/step
Epoch 813/1000
2023-09-29 04:34:11.195 
Epoch 813/1000 
	 loss: 27.8946, MinusLogProbMetric: 27.8946, val_loss: 28.1481, val_MinusLogProbMetric: 28.1481

Epoch 813: val_loss did not improve from 28.04732
196/196 - 51s - loss: 27.8946 - MinusLogProbMetric: 27.8946 - val_loss: 28.1481 - val_MinusLogProbMetric: 28.1481 - lr: 1.2500e-04 - 51s/epoch - 258ms/step
Epoch 814/1000
2023-09-29 04:35:01.253 
Epoch 814/1000 
	 loss: 27.8865, MinusLogProbMetric: 27.8865, val_loss: 28.1521, val_MinusLogProbMetric: 28.1521

Epoch 814: val_loss did not improve from 28.04732
196/196 - 50s - loss: 27.8865 - MinusLogProbMetric: 27.8865 - val_loss: 28.1521 - val_MinusLogProbMetric: 28.1521 - lr: 1.2500e-04 - 50s/epoch - 255ms/step
Epoch 815/1000
2023-09-29 04:35:51.385 
Epoch 815/1000 
	 loss: 27.8842, MinusLogProbMetric: 27.8842, val_loss: 28.0742, val_MinusLogProbMetric: 28.0742

Epoch 815: val_loss did not improve from 28.04732
196/196 - 50s - loss: 27.8842 - MinusLogProbMetric: 27.8842 - val_loss: 28.0742 - val_MinusLogProbMetric: 28.0742 - lr: 1.2500e-04 - 50s/epoch - 256ms/step
Epoch 816/1000
2023-09-29 04:36:41.035 
Epoch 816/1000 
	 loss: 27.8838, MinusLogProbMetric: 27.8838, val_loss: 28.0896, val_MinusLogProbMetric: 28.0896

Epoch 816: val_loss did not improve from 28.04732
196/196 - 50s - loss: 27.8838 - MinusLogProbMetric: 27.8838 - val_loss: 28.0896 - val_MinusLogProbMetric: 28.0896 - lr: 1.2500e-04 - 50s/epoch - 253ms/step
Epoch 817/1000
2023-09-29 04:37:34.172 
Epoch 817/1000 
	 loss: 27.8768, MinusLogProbMetric: 27.8768, val_loss: 28.1113, val_MinusLogProbMetric: 28.1113

Epoch 817: val_loss did not improve from 28.04732
196/196 - 53s - loss: 27.8768 - MinusLogProbMetric: 27.8768 - val_loss: 28.1113 - val_MinusLogProbMetric: 28.1113 - lr: 1.2500e-04 - 53s/epoch - 271ms/step
Epoch 818/1000
2023-09-29 04:38:25.708 
Epoch 818/1000 
	 loss: 27.8712, MinusLogProbMetric: 27.8712, val_loss: 28.0979, val_MinusLogProbMetric: 28.0979

Epoch 818: val_loss did not improve from 28.04732
196/196 - 52s - loss: 27.8712 - MinusLogProbMetric: 27.8712 - val_loss: 28.0979 - val_MinusLogProbMetric: 28.0979 - lr: 1.2500e-04 - 52s/epoch - 263ms/step
Epoch 819/1000
2023-09-29 04:39:18.006 
Epoch 819/1000 
	 loss: 27.8709, MinusLogProbMetric: 27.8709, val_loss: 28.1718, val_MinusLogProbMetric: 28.1718

Epoch 819: val_loss did not improve from 28.04732
196/196 - 52s - loss: 27.8709 - MinusLogProbMetric: 27.8709 - val_loss: 28.1718 - val_MinusLogProbMetric: 28.1718 - lr: 1.2500e-04 - 52s/epoch - 267ms/step
Epoch 820/1000
2023-09-29 04:40:10.529 
Epoch 820/1000 
	 loss: 27.8783, MinusLogProbMetric: 27.8783, val_loss: 28.1006, val_MinusLogProbMetric: 28.1006

Epoch 820: val_loss did not improve from 28.04732
196/196 - 53s - loss: 27.8783 - MinusLogProbMetric: 27.8783 - val_loss: 28.1006 - val_MinusLogProbMetric: 28.1006 - lr: 1.2500e-04 - 53s/epoch - 268ms/step
Epoch 821/1000
2023-09-29 04:41:00.743 
Epoch 821/1000 
	 loss: 27.8794, MinusLogProbMetric: 27.8794, val_loss: 28.0773, val_MinusLogProbMetric: 28.0773

Epoch 821: val_loss did not improve from 28.04732
196/196 - 50s - loss: 27.8794 - MinusLogProbMetric: 27.8794 - val_loss: 28.0773 - val_MinusLogProbMetric: 28.0773 - lr: 1.2500e-04 - 50s/epoch - 256ms/step
Epoch 822/1000
2023-09-29 04:41:53.805 
Epoch 822/1000 
	 loss: 27.8847, MinusLogProbMetric: 27.8847, val_loss: 28.1285, val_MinusLogProbMetric: 28.1285

Epoch 822: val_loss did not improve from 28.04732
196/196 - 53s - loss: 27.8847 - MinusLogProbMetric: 27.8847 - val_loss: 28.1285 - val_MinusLogProbMetric: 28.1285 - lr: 1.2500e-04 - 53s/epoch - 271ms/step
Epoch 823/1000
2023-09-29 04:42:44.250 
Epoch 823/1000 
	 loss: 27.8838, MinusLogProbMetric: 27.8838, val_loss: 28.1164, val_MinusLogProbMetric: 28.1164

Epoch 823: val_loss did not improve from 28.04732
196/196 - 50s - loss: 27.8838 - MinusLogProbMetric: 27.8838 - val_loss: 28.1164 - val_MinusLogProbMetric: 28.1164 - lr: 1.2500e-04 - 50s/epoch - 257ms/step
Epoch 824/1000
2023-09-29 04:43:35.736 
Epoch 824/1000 
	 loss: 27.8711, MinusLogProbMetric: 27.8711, val_loss: 28.0896, val_MinusLogProbMetric: 28.0896

Epoch 824: val_loss did not improve from 28.04732
196/196 - 51s - loss: 27.8711 - MinusLogProbMetric: 27.8711 - val_loss: 28.0896 - val_MinusLogProbMetric: 28.0896 - lr: 1.2500e-04 - 51s/epoch - 263ms/step
Epoch 825/1000
2023-09-29 04:44:25.646 
Epoch 825/1000 
	 loss: 27.8737, MinusLogProbMetric: 27.8737, val_loss: 28.1820, val_MinusLogProbMetric: 28.1820

Epoch 825: val_loss did not improve from 28.04732
196/196 - 50s - loss: 27.8737 - MinusLogProbMetric: 27.8737 - val_loss: 28.1820 - val_MinusLogProbMetric: 28.1820 - lr: 1.2500e-04 - 50s/epoch - 255ms/step
Epoch 826/1000
2023-09-29 04:45:15.980 
Epoch 826/1000 
	 loss: 27.8758, MinusLogProbMetric: 27.8758, val_loss: 28.1346, val_MinusLogProbMetric: 28.1346

Epoch 826: val_loss did not improve from 28.04732
196/196 - 50s - loss: 27.8758 - MinusLogProbMetric: 27.8758 - val_loss: 28.1346 - val_MinusLogProbMetric: 28.1346 - lr: 1.2500e-04 - 50s/epoch - 257ms/step
Epoch 827/1000
2023-09-29 04:46:05.565 
Epoch 827/1000 
	 loss: 27.8738, MinusLogProbMetric: 27.8738, val_loss: 28.1326, val_MinusLogProbMetric: 28.1326

Epoch 827: val_loss did not improve from 28.04732
196/196 - 50s - loss: 27.8738 - MinusLogProbMetric: 27.8738 - val_loss: 28.1326 - val_MinusLogProbMetric: 28.1326 - lr: 1.2500e-04 - 50s/epoch - 253ms/step
Epoch 828/1000
2023-09-29 04:46:57.297 
Epoch 828/1000 
	 loss: 27.8820, MinusLogProbMetric: 27.8820, val_loss: 28.2308, val_MinusLogProbMetric: 28.2308

Epoch 828: val_loss did not improve from 28.04732
196/196 - 52s - loss: 27.8820 - MinusLogProbMetric: 27.8820 - val_loss: 28.2308 - val_MinusLogProbMetric: 28.2308 - lr: 1.2500e-04 - 52s/epoch - 264ms/step
Epoch 829/1000
2023-09-29 04:47:48.242 
Epoch 829/1000 
	 loss: 27.8887, MinusLogProbMetric: 27.8887, val_loss: 28.1160, val_MinusLogProbMetric: 28.1160

Epoch 829: val_loss did not improve from 28.04732
196/196 - 51s - loss: 27.8887 - MinusLogProbMetric: 27.8887 - val_loss: 28.1160 - val_MinusLogProbMetric: 28.1160 - lr: 1.2500e-04 - 51s/epoch - 260ms/step
Epoch 830/1000
2023-09-29 04:48:41.158 
Epoch 830/1000 
	 loss: 27.8706, MinusLogProbMetric: 27.8706, val_loss: 28.1143, val_MinusLogProbMetric: 28.1143

Epoch 830: val_loss did not improve from 28.04732
196/196 - 53s - loss: 27.8706 - MinusLogProbMetric: 27.8706 - val_loss: 28.1143 - val_MinusLogProbMetric: 28.1143 - lr: 1.2500e-04 - 53s/epoch - 270ms/step
Epoch 831/1000
2023-09-29 04:49:33.494 
Epoch 831/1000 
	 loss: 27.8663, MinusLogProbMetric: 27.8663, val_loss: 28.1561, val_MinusLogProbMetric: 28.1561

Epoch 831: val_loss did not improve from 28.04732
196/196 - 52s - loss: 27.8663 - MinusLogProbMetric: 27.8663 - val_loss: 28.1561 - val_MinusLogProbMetric: 28.1561 - lr: 1.2500e-04 - 52s/epoch - 267ms/step
Epoch 832/1000
2023-09-29 04:50:26.686 
Epoch 832/1000 
	 loss: 27.8929, MinusLogProbMetric: 27.8929, val_loss: 28.2223, val_MinusLogProbMetric: 28.2223

Epoch 832: val_loss did not improve from 28.04732
196/196 - 53s - loss: 27.8929 - MinusLogProbMetric: 27.8929 - val_loss: 28.2223 - val_MinusLogProbMetric: 28.2223 - lr: 1.2500e-04 - 53s/epoch - 271ms/step
Epoch 833/1000
2023-09-29 04:51:18.259 
Epoch 833/1000 
	 loss: 27.8820, MinusLogProbMetric: 27.8820, val_loss: 28.1831, val_MinusLogProbMetric: 28.1831

Epoch 833: val_loss did not improve from 28.04732
196/196 - 52s - loss: 27.8820 - MinusLogProbMetric: 27.8820 - val_loss: 28.1831 - val_MinusLogProbMetric: 28.1831 - lr: 1.2500e-04 - 52s/epoch - 263ms/step
Epoch 834/1000
2023-09-29 04:52:08.065 
Epoch 834/1000 
	 loss: 27.8739, MinusLogProbMetric: 27.8739, val_loss: 28.1874, val_MinusLogProbMetric: 28.1874

Epoch 834: val_loss did not improve from 28.04732
196/196 - 50s - loss: 27.8739 - MinusLogProbMetric: 27.8739 - val_loss: 28.1874 - val_MinusLogProbMetric: 28.1874 - lr: 1.2500e-04 - 50s/epoch - 254ms/step
Epoch 835/1000
2023-09-29 04:52:59.996 
Epoch 835/1000 
	 loss: 27.8729, MinusLogProbMetric: 27.8729, val_loss: 28.1764, val_MinusLogProbMetric: 28.1764

Epoch 835: val_loss did not improve from 28.04732
196/196 - 52s - loss: 27.8729 - MinusLogProbMetric: 27.8729 - val_loss: 28.1764 - val_MinusLogProbMetric: 28.1764 - lr: 1.2500e-04 - 52s/epoch - 265ms/step
Epoch 836/1000
2023-09-29 04:53:52.735 
Epoch 836/1000 
	 loss: 27.8974, MinusLogProbMetric: 27.8974, val_loss: 28.0658, val_MinusLogProbMetric: 28.0658

Epoch 836: val_loss did not improve from 28.04732
196/196 - 53s - loss: 27.8974 - MinusLogProbMetric: 27.8974 - val_loss: 28.0658 - val_MinusLogProbMetric: 28.0658 - lr: 1.2500e-04 - 53s/epoch - 269ms/step
Epoch 837/1000
2023-09-29 04:54:39.621 
Epoch 837/1000 
	 loss: 27.8766, MinusLogProbMetric: 27.8766, val_loss: 28.1461, val_MinusLogProbMetric: 28.1461

Epoch 837: val_loss did not improve from 28.04732
196/196 - 47s - loss: 27.8766 - MinusLogProbMetric: 27.8766 - val_loss: 28.1461 - val_MinusLogProbMetric: 28.1461 - lr: 1.2500e-04 - 47s/epoch - 239ms/step
Epoch 838/1000
2023-09-29 04:55:32.166 
Epoch 838/1000 
	 loss: 27.8704, MinusLogProbMetric: 27.8704, val_loss: 28.1268, val_MinusLogProbMetric: 28.1268

Epoch 838: val_loss did not improve from 28.04732
196/196 - 53s - loss: 27.8704 - MinusLogProbMetric: 27.8704 - val_loss: 28.1268 - val_MinusLogProbMetric: 28.1268 - lr: 1.2500e-04 - 53s/epoch - 268ms/step
Epoch 839/1000
2023-09-29 04:56:22.701 
Epoch 839/1000 
	 loss: 27.8870, MinusLogProbMetric: 27.8870, val_loss: 28.1206, val_MinusLogProbMetric: 28.1206

Epoch 839: val_loss did not improve from 28.04732
196/196 - 51s - loss: 27.8870 - MinusLogProbMetric: 27.8870 - val_loss: 28.1206 - val_MinusLogProbMetric: 28.1206 - lr: 1.2500e-04 - 51s/epoch - 258ms/step
Epoch 840/1000
2023-09-29 04:57:12.007 
Epoch 840/1000 
	 loss: 27.8615, MinusLogProbMetric: 27.8615, val_loss: 28.0783, val_MinusLogProbMetric: 28.0783

Epoch 840: val_loss did not improve from 28.04732
196/196 - 49s - loss: 27.8615 - MinusLogProbMetric: 27.8615 - val_loss: 28.0783 - val_MinusLogProbMetric: 28.0783 - lr: 1.2500e-04 - 49s/epoch - 252ms/step
Epoch 841/1000
2023-09-29 04:57:59.719 
Epoch 841/1000 
	 loss: 27.8962, MinusLogProbMetric: 27.8962, val_loss: 28.0806, val_MinusLogProbMetric: 28.0806

Epoch 841: val_loss did not improve from 28.04732
196/196 - 48s - loss: 27.8962 - MinusLogProbMetric: 27.8962 - val_loss: 28.0806 - val_MinusLogProbMetric: 28.0806 - lr: 1.2500e-04 - 48s/epoch - 243ms/step
Epoch 842/1000
2023-09-29 04:58:51.453 
Epoch 842/1000 
	 loss: 27.8774, MinusLogProbMetric: 27.8774, val_loss: 28.1233, val_MinusLogProbMetric: 28.1233

Epoch 842: val_loss did not improve from 28.04732
196/196 - 52s - loss: 27.8774 - MinusLogProbMetric: 27.8774 - val_loss: 28.1233 - val_MinusLogProbMetric: 28.1233 - lr: 1.2500e-04 - 52s/epoch - 264ms/step
Epoch 843/1000
2023-09-29 04:59:41.828 
Epoch 843/1000 
	 loss: 27.8846, MinusLogProbMetric: 27.8846, val_loss: 28.1145, val_MinusLogProbMetric: 28.1145

Epoch 843: val_loss did not improve from 28.04732
196/196 - 50s - loss: 27.8846 - MinusLogProbMetric: 27.8846 - val_loss: 28.1145 - val_MinusLogProbMetric: 28.1145 - lr: 1.2500e-04 - 50s/epoch - 257ms/step
Epoch 844/1000
2023-09-29 05:00:30.547 
Epoch 844/1000 
	 loss: 27.8769, MinusLogProbMetric: 27.8769, val_loss: 28.0943, val_MinusLogProbMetric: 28.0943

Epoch 844: val_loss did not improve from 28.04732
196/196 - 49s - loss: 27.8769 - MinusLogProbMetric: 27.8769 - val_loss: 28.0943 - val_MinusLogProbMetric: 28.0943 - lr: 1.2500e-04 - 49s/epoch - 249ms/step
Epoch 845/1000
2023-09-29 05:01:20.484 
Epoch 845/1000 
	 loss: 27.8713, MinusLogProbMetric: 27.8713, val_loss: 28.1249, val_MinusLogProbMetric: 28.1249

Epoch 845: val_loss did not improve from 28.04732
196/196 - 50s - loss: 27.8713 - MinusLogProbMetric: 27.8713 - val_loss: 28.1249 - val_MinusLogProbMetric: 28.1249 - lr: 1.2500e-04 - 50s/epoch - 255ms/step
Epoch 846/1000
2023-09-29 05:02:08.943 
Epoch 846/1000 
	 loss: 27.8242, MinusLogProbMetric: 27.8242, val_loss: 28.0398, val_MinusLogProbMetric: 28.0398

Epoch 846: val_loss improved from 28.04732 to 28.03977, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_323/weights/best_weights.h5
196/196 - 49s - loss: 27.8242 - MinusLogProbMetric: 27.8242 - val_loss: 28.0398 - val_MinusLogProbMetric: 28.0398 - lr: 6.2500e-05 - 49s/epoch - 251ms/step
Epoch 847/1000
2023-09-29 05:02:56.309 
Epoch 847/1000 
	 loss: 27.8179, MinusLogProbMetric: 27.8179, val_loss: 28.1041, val_MinusLogProbMetric: 28.1041

Epoch 847: val_loss did not improve from 28.03977
196/196 - 47s - loss: 27.8179 - MinusLogProbMetric: 27.8179 - val_loss: 28.1041 - val_MinusLogProbMetric: 28.1041 - lr: 6.2500e-05 - 47s/epoch - 238ms/step
Epoch 848/1000
2023-09-29 05:03:47.074 
Epoch 848/1000 
	 loss: 27.8218, MinusLogProbMetric: 27.8218, val_loss: 28.0670, val_MinusLogProbMetric: 28.0670

Epoch 848: val_loss did not improve from 28.03977
196/196 - 51s - loss: 27.8218 - MinusLogProbMetric: 27.8218 - val_loss: 28.0670 - val_MinusLogProbMetric: 28.0670 - lr: 6.2500e-05 - 51s/epoch - 259ms/step
Epoch 849/1000
2023-09-29 05:04:38.704 
Epoch 849/1000 
	 loss: 27.8246, MinusLogProbMetric: 27.8246, val_loss: 28.0650, val_MinusLogProbMetric: 28.0650

Epoch 849: val_loss did not improve from 28.03977
196/196 - 52s - loss: 27.8246 - MinusLogProbMetric: 27.8246 - val_loss: 28.0650 - val_MinusLogProbMetric: 28.0650 - lr: 6.2500e-05 - 52s/epoch - 263ms/step
Epoch 850/1000
2023-09-29 05:05:28.242 
Epoch 850/1000 
	 loss: 27.8267, MinusLogProbMetric: 27.8267, val_loss: 28.1175, val_MinusLogProbMetric: 28.1175

Epoch 850: val_loss did not improve from 28.03977
196/196 - 50s - loss: 27.8267 - MinusLogProbMetric: 27.8267 - val_loss: 28.1175 - val_MinusLogProbMetric: 28.1175 - lr: 6.2500e-05 - 50s/epoch - 253ms/step
Epoch 851/1000
2023-09-29 05:06:15.316 
Epoch 851/1000 
	 loss: 27.8261, MinusLogProbMetric: 27.8261, val_loss: 28.0471, val_MinusLogProbMetric: 28.0471

Epoch 851: val_loss did not improve from 28.03977
196/196 - 47s - loss: 27.8261 - MinusLogProbMetric: 27.8261 - val_loss: 28.0471 - val_MinusLogProbMetric: 28.0471 - lr: 6.2500e-05 - 47s/epoch - 240ms/step
Epoch 852/1000
2023-09-29 05:07:01.655 
Epoch 852/1000 
	 loss: 27.8223, MinusLogProbMetric: 27.8223, val_loss: 28.0564, val_MinusLogProbMetric: 28.0564

Epoch 852: val_loss did not improve from 28.03977
196/196 - 46s - loss: 27.8223 - MinusLogProbMetric: 27.8223 - val_loss: 28.0564 - val_MinusLogProbMetric: 28.0564 - lr: 6.2500e-05 - 46s/epoch - 236ms/step
Epoch 853/1000
2023-09-29 05:07:50.096 
Epoch 853/1000 
	 loss: 27.8254, MinusLogProbMetric: 27.8254, val_loss: 28.0606, val_MinusLogProbMetric: 28.0606

Epoch 853: val_loss did not improve from 28.03977
196/196 - 48s - loss: 27.8254 - MinusLogProbMetric: 27.8254 - val_loss: 28.0606 - val_MinusLogProbMetric: 28.0606 - lr: 6.2500e-05 - 48s/epoch - 247ms/step
Epoch 854/1000
2023-09-29 05:08:41.202 
Epoch 854/1000 
	 loss: 27.8200, MinusLogProbMetric: 27.8200, val_loss: 28.0580, val_MinusLogProbMetric: 28.0580

Epoch 854: val_loss did not improve from 28.03977
196/196 - 51s - loss: 27.8200 - MinusLogProbMetric: 27.8200 - val_loss: 28.0580 - val_MinusLogProbMetric: 28.0580 - lr: 6.2500e-05 - 51s/epoch - 261ms/step
Epoch 855/1000
2023-09-29 05:09:27.179 
Epoch 855/1000 
	 loss: 27.8232, MinusLogProbMetric: 27.8232, val_loss: 28.1074, val_MinusLogProbMetric: 28.1074

Epoch 855: val_loss did not improve from 28.03977
196/196 - 46s - loss: 27.8232 - MinusLogProbMetric: 27.8232 - val_loss: 28.1074 - val_MinusLogProbMetric: 28.1074 - lr: 6.2500e-05 - 46s/epoch - 235ms/step
Epoch 856/1000
2023-09-29 05:10:17.238 
Epoch 856/1000 
	 loss: 27.8241, MinusLogProbMetric: 27.8241, val_loss: 28.0716, val_MinusLogProbMetric: 28.0716

Epoch 856: val_loss did not improve from 28.03977
196/196 - 50s - loss: 27.8241 - MinusLogProbMetric: 27.8241 - val_loss: 28.0716 - val_MinusLogProbMetric: 28.0716 - lr: 6.2500e-05 - 50s/epoch - 255ms/step
Epoch 857/1000
2023-09-29 05:11:03.408 
Epoch 857/1000 
	 loss: 27.8264, MinusLogProbMetric: 27.8264, val_loss: 28.0506, val_MinusLogProbMetric: 28.0506

Epoch 857: val_loss did not improve from 28.03977
196/196 - 46s - loss: 27.8264 - MinusLogProbMetric: 27.8264 - val_loss: 28.0506 - val_MinusLogProbMetric: 28.0506 - lr: 6.2500e-05 - 46s/epoch - 236ms/step
Epoch 858/1000
2023-09-29 05:11:51.941 
Epoch 858/1000 
	 loss: 27.8220, MinusLogProbMetric: 27.8220, val_loss: 28.0675, val_MinusLogProbMetric: 28.0675

Epoch 858: val_loss did not improve from 28.03977
196/196 - 49s - loss: 27.8220 - MinusLogProbMetric: 27.8220 - val_loss: 28.0675 - val_MinusLogProbMetric: 28.0675 - lr: 6.2500e-05 - 49s/epoch - 248ms/step
Epoch 859/1000
2023-09-29 05:12:42.417 
Epoch 859/1000 
	 loss: 27.8244, MinusLogProbMetric: 27.8244, val_loss: 28.0544, val_MinusLogProbMetric: 28.0544

Epoch 859: val_loss did not improve from 28.03977
196/196 - 50s - loss: 27.8244 - MinusLogProbMetric: 27.8244 - val_loss: 28.0544 - val_MinusLogProbMetric: 28.0544 - lr: 6.2500e-05 - 50s/epoch - 257ms/step
Epoch 860/1000
2023-09-29 05:13:32.434 
Epoch 860/1000 
	 loss: 27.8289, MinusLogProbMetric: 27.8289, val_loss: 28.0617, val_MinusLogProbMetric: 28.0617

Epoch 860: val_loss did not improve from 28.03977
196/196 - 50s - loss: 27.8289 - MinusLogProbMetric: 27.8289 - val_loss: 28.0617 - val_MinusLogProbMetric: 28.0617 - lr: 6.2500e-05 - 50s/epoch - 255ms/step
Epoch 861/1000
2023-09-29 05:14:22.497 
Epoch 861/1000 
	 loss: 27.8193, MinusLogProbMetric: 27.8193, val_loss: 28.0697, val_MinusLogProbMetric: 28.0697

Epoch 861: val_loss did not improve from 28.03977
196/196 - 50s - loss: 27.8193 - MinusLogProbMetric: 27.8193 - val_loss: 28.0697 - val_MinusLogProbMetric: 28.0697 - lr: 6.2500e-05 - 50s/epoch - 255ms/step
Epoch 862/1000
2023-09-29 05:15:14.409 
Epoch 862/1000 
	 loss: 27.8216, MinusLogProbMetric: 27.8216, val_loss: 28.0679, val_MinusLogProbMetric: 28.0679

Epoch 862: val_loss did not improve from 28.03977
196/196 - 52s - loss: 27.8216 - MinusLogProbMetric: 27.8216 - val_loss: 28.0679 - val_MinusLogProbMetric: 28.0679 - lr: 6.2500e-05 - 52s/epoch - 265ms/step
Epoch 863/1000
2023-09-29 05:16:03.720 
Epoch 863/1000 
	 loss: 27.8205, MinusLogProbMetric: 27.8205, val_loss: 28.0629, val_MinusLogProbMetric: 28.0629

Epoch 863: val_loss did not improve from 28.03977
196/196 - 49s - loss: 27.8205 - MinusLogProbMetric: 27.8205 - val_loss: 28.0629 - val_MinusLogProbMetric: 28.0629 - lr: 6.2500e-05 - 49s/epoch - 252ms/step
Epoch 864/1000
2023-09-29 05:16:55.730 
Epoch 864/1000 
	 loss: 27.8232, MinusLogProbMetric: 27.8232, val_loss: 28.0437, val_MinusLogProbMetric: 28.0437

Epoch 864: val_loss did not improve from 28.03977
196/196 - 52s - loss: 27.8232 - MinusLogProbMetric: 27.8232 - val_loss: 28.0437 - val_MinusLogProbMetric: 28.0437 - lr: 6.2500e-05 - 52s/epoch - 265ms/step
Epoch 865/1000
2023-09-29 05:17:46.276 
Epoch 865/1000 
	 loss: 27.8184, MinusLogProbMetric: 27.8184, val_loss: 28.0578, val_MinusLogProbMetric: 28.0578

Epoch 865: val_loss did not improve from 28.03977
196/196 - 51s - loss: 27.8184 - MinusLogProbMetric: 27.8184 - val_loss: 28.0578 - val_MinusLogProbMetric: 28.0578 - lr: 6.2500e-05 - 51s/epoch - 258ms/step
Epoch 866/1000
2023-09-29 05:18:39.120 
Epoch 866/1000 
	 loss: 27.8197, MinusLogProbMetric: 27.8197, val_loss: 28.0556, val_MinusLogProbMetric: 28.0556

Epoch 866: val_loss did not improve from 28.03977
196/196 - 53s - loss: 27.8197 - MinusLogProbMetric: 27.8197 - val_loss: 28.0556 - val_MinusLogProbMetric: 28.0556 - lr: 6.2500e-05 - 53s/epoch - 270ms/step
Epoch 867/1000
2023-09-29 05:19:30.131 
Epoch 867/1000 
	 loss: 27.8163, MinusLogProbMetric: 27.8163, val_loss: 28.0430, val_MinusLogProbMetric: 28.0430

Epoch 867: val_loss did not improve from 28.03977
196/196 - 51s - loss: 27.8163 - MinusLogProbMetric: 27.8163 - val_loss: 28.0430 - val_MinusLogProbMetric: 28.0430 - lr: 6.2500e-05 - 51s/epoch - 260ms/step
Epoch 868/1000
2023-09-29 05:20:23.008 
Epoch 868/1000 
	 loss: 27.8177, MinusLogProbMetric: 27.8177, val_loss: 28.0341, val_MinusLogProbMetric: 28.0341

Epoch 868: val_loss improved from 28.03977 to 28.03407, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_323/weights/best_weights.h5
196/196 - 54s - loss: 27.8177 - MinusLogProbMetric: 27.8177 - val_loss: 28.0341 - val_MinusLogProbMetric: 28.0341 - lr: 6.2500e-05 - 54s/epoch - 273ms/step
Epoch 869/1000
2023-09-29 05:21:15.277 
Epoch 869/1000 
	 loss: 27.8209, MinusLogProbMetric: 27.8209, val_loss: 28.0576, val_MinusLogProbMetric: 28.0576

Epoch 869: val_loss did not improve from 28.03407
196/196 - 52s - loss: 27.8209 - MinusLogProbMetric: 27.8209 - val_loss: 28.0576 - val_MinusLogProbMetric: 28.0576 - lr: 6.2500e-05 - 52s/epoch - 263ms/step
Epoch 870/1000
2023-09-29 05:22:03.999 
Epoch 870/1000 
	 loss: 27.8163, MinusLogProbMetric: 27.8163, val_loss: 28.0414, val_MinusLogProbMetric: 28.0414

Epoch 870: val_loss did not improve from 28.03407
196/196 - 49s - loss: 27.8163 - MinusLogProbMetric: 27.8163 - val_loss: 28.0414 - val_MinusLogProbMetric: 28.0414 - lr: 6.2500e-05 - 49s/epoch - 249ms/step
Epoch 871/1000
2023-09-29 05:22:51.361 
Epoch 871/1000 
	 loss: 27.8194, MinusLogProbMetric: 27.8194, val_loss: 28.0399, val_MinusLogProbMetric: 28.0399

Epoch 871: val_loss did not improve from 28.03407
196/196 - 47s - loss: 27.8194 - MinusLogProbMetric: 27.8194 - val_loss: 28.0399 - val_MinusLogProbMetric: 28.0399 - lr: 6.2500e-05 - 47s/epoch - 242ms/step
Epoch 872/1000
2023-09-29 05:23:41.200 
Epoch 872/1000 
	 loss: 27.8178, MinusLogProbMetric: 27.8178, val_loss: 28.0490, val_MinusLogProbMetric: 28.0490

Epoch 872: val_loss did not improve from 28.03407
196/196 - 50s - loss: 27.8178 - MinusLogProbMetric: 27.8178 - val_loss: 28.0490 - val_MinusLogProbMetric: 28.0490 - lr: 6.2500e-05 - 50s/epoch - 254ms/step
Epoch 873/1000
2023-09-29 05:24:29.307 
Epoch 873/1000 
	 loss: 27.8188, MinusLogProbMetric: 27.8188, val_loss: 28.0466, val_MinusLogProbMetric: 28.0466

Epoch 873: val_loss did not improve from 28.03407
196/196 - 48s - loss: 27.8188 - MinusLogProbMetric: 27.8188 - val_loss: 28.0466 - val_MinusLogProbMetric: 28.0466 - lr: 6.2500e-05 - 48s/epoch - 245ms/step
Epoch 874/1000
2023-09-29 05:25:17.022 
Epoch 874/1000 
	 loss: 27.8172, MinusLogProbMetric: 27.8172, val_loss: 28.0570, val_MinusLogProbMetric: 28.0570

Epoch 874: val_loss did not improve from 28.03407
196/196 - 48s - loss: 27.8172 - MinusLogProbMetric: 27.8172 - val_loss: 28.0570 - val_MinusLogProbMetric: 28.0570 - lr: 6.2500e-05 - 48s/epoch - 243ms/step
Epoch 875/1000
2023-09-29 05:26:07.329 
Epoch 875/1000 
	 loss: 27.8124, MinusLogProbMetric: 27.8124, val_loss: 28.0346, val_MinusLogProbMetric: 28.0346

Epoch 875: val_loss did not improve from 28.03407
196/196 - 50s - loss: 27.8124 - MinusLogProbMetric: 27.8124 - val_loss: 28.0346 - val_MinusLogProbMetric: 28.0346 - lr: 6.2500e-05 - 50s/epoch - 257ms/step
Epoch 876/1000
2023-09-29 05:26:58.741 
Epoch 876/1000 
	 loss: 27.8142, MinusLogProbMetric: 27.8142, val_loss: 28.0640, val_MinusLogProbMetric: 28.0640

Epoch 876: val_loss did not improve from 28.03407
196/196 - 51s - loss: 27.8142 - MinusLogProbMetric: 27.8142 - val_loss: 28.0640 - val_MinusLogProbMetric: 28.0640 - lr: 6.2500e-05 - 51s/epoch - 262ms/step
Epoch 877/1000
2023-09-29 05:27:48.719 
Epoch 877/1000 
	 loss: 27.8193, MinusLogProbMetric: 27.8193, val_loss: 28.0550, val_MinusLogProbMetric: 28.0550

Epoch 877: val_loss did not improve from 28.03407
196/196 - 50s - loss: 27.8193 - MinusLogProbMetric: 27.8193 - val_loss: 28.0550 - val_MinusLogProbMetric: 28.0550 - lr: 6.2500e-05 - 50s/epoch - 255ms/step
Epoch 878/1000
2023-09-29 05:28:37.656 
Epoch 878/1000 
	 loss: 27.8160, MinusLogProbMetric: 27.8160, val_loss: 28.0375, val_MinusLogProbMetric: 28.0375

Epoch 878: val_loss did not improve from 28.03407
196/196 - 49s - loss: 27.8160 - MinusLogProbMetric: 27.8160 - val_loss: 28.0375 - val_MinusLogProbMetric: 28.0375 - lr: 6.2500e-05 - 49s/epoch - 250ms/step
Epoch 879/1000
2023-09-29 05:29:29.009 
Epoch 879/1000 
	 loss: 27.8135, MinusLogProbMetric: 27.8135, val_loss: 28.0427, val_MinusLogProbMetric: 28.0427

Epoch 879: val_loss did not improve from 28.03407
196/196 - 51s - loss: 27.8135 - MinusLogProbMetric: 27.8135 - val_loss: 28.0427 - val_MinusLogProbMetric: 28.0427 - lr: 6.2500e-05 - 51s/epoch - 262ms/step
Epoch 880/1000
2023-09-29 05:30:20.727 
Epoch 880/1000 
	 loss: 27.8132, MinusLogProbMetric: 27.8132, val_loss: 28.0427, val_MinusLogProbMetric: 28.0427

Epoch 880: val_loss did not improve from 28.03407
196/196 - 52s - loss: 27.8132 - MinusLogProbMetric: 27.8132 - val_loss: 28.0427 - val_MinusLogProbMetric: 28.0427 - lr: 6.2500e-05 - 52s/epoch - 264ms/step
Epoch 881/1000
2023-09-29 05:31:12.068 
Epoch 881/1000 
	 loss: 27.8136, MinusLogProbMetric: 27.8136, val_loss: 28.0439, val_MinusLogProbMetric: 28.0439

Epoch 881: val_loss did not improve from 28.03407
196/196 - 51s - loss: 27.8136 - MinusLogProbMetric: 27.8136 - val_loss: 28.0439 - val_MinusLogProbMetric: 28.0439 - lr: 6.2500e-05 - 51s/epoch - 262ms/step
Epoch 882/1000
2023-09-29 05:32:04.125 
Epoch 882/1000 
	 loss: 27.8160, MinusLogProbMetric: 27.8160, val_loss: 28.0561, val_MinusLogProbMetric: 28.0561

Epoch 882: val_loss did not improve from 28.03407
196/196 - 52s - loss: 27.8160 - MinusLogProbMetric: 27.8160 - val_loss: 28.0561 - val_MinusLogProbMetric: 28.0561 - lr: 6.2500e-05 - 52s/epoch - 266ms/step
Epoch 883/1000
2023-09-29 05:32:53.551 
Epoch 883/1000 
	 loss: 27.8148, MinusLogProbMetric: 27.8148, val_loss: 28.0671, val_MinusLogProbMetric: 28.0671

Epoch 883: val_loss did not improve from 28.03407
196/196 - 49s - loss: 27.8148 - MinusLogProbMetric: 27.8148 - val_loss: 28.0671 - val_MinusLogProbMetric: 28.0671 - lr: 6.2500e-05 - 49s/epoch - 252ms/step
Epoch 884/1000
2023-09-29 05:33:43.636 
Epoch 884/1000 
	 loss: 27.8218, MinusLogProbMetric: 27.8218, val_loss: 28.0450, val_MinusLogProbMetric: 28.0450

Epoch 884: val_loss did not improve from 28.03407
196/196 - 50s - loss: 27.8218 - MinusLogProbMetric: 27.8218 - val_loss: 28.0450 - val_MinusLogProbMetric: 28.0450 - lr: 6.2500e-05 - 50s/epoch - 256ms/step
Epoch 885/1000
2023-09-29 05:34:33.294 
Epoch 885/1000 
	 loss: 27.8174, MinusLogProbMetric: 27.8174, val_loss: 28.0396, val_MinusLogProbMetric: 28.0396

Epoch 885: val_loss did not improve from 28.03407
196/196 - 50s - loss: 27.8174 - MinusLogProbMetric: 27.8174 - val_loss: 28.0396 - val_MinusLogProbMetric: 28.0396 - lr: 6.2500e-05 - 50s/epoch - 253ms/step
Epoch 886/1000
2023-09-29 05:35:25.781 
Epoch 886/1000 
	 loss: 27.8183, MinusLogProbMetric: 27.8183, val_loss: 28.0309, val_MinusLogProbMetric: 28.0309

Epoch 886: val_loss improved from 28.03407 to 28.03090, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_323/weights/best_weights.h5
196/196 - 53s - loss: 27.8183 - MinusLogProbMetric: 27.8183 - val_loss: 28.0309 - val_MinusLogProbMetric: 28.0309 - lr: 6.2500e-05 - 53s/epoch - 271ms/step
Epoch 887/1000
2023-09-29 05:36:16.202 
Epoch 887/1000 
	 loss: 27.8119, MinusLogProbMetric: 27.8119, val_loss: 28.0487, val_MinusLogProbMetric: 28.0487

Epoch 887: val_loss did not improve from 28.03090
196/196 - 50s - loss: 27.8119 - MinusLogProbMetric: 27.8119 - val_loss: 28.0487 - val_MinusLogProbMetric: 28.0487 - lr: 6.2500e-05 - 50s/epoch - 254ms/step
Epoch 888/1000
2023-09-29 05:37:05.458 
Epoch 888/1000 
	 loss: 27.8206, MinusLogProbMetric: 27.8206, val_loss: 28.0480, val_MinusLogProbMetric: 28.0480

Epoch 888: val_loss did not improve from 28.03090
196/196 - 49s - loss: 27.8206 - MinusLogProbMetric: 27.8206 - val_loss: 28.0480 - val_MinusLogProbMetric: 28.0480 - lr: 6.2500e-05 - 49s/epoch - 251ms/step
Epoch 889/1000
2023-09-29 05:37:53.096 
Epoch 889/1000 
	 loss: 27.8152, MinusLogProbMetric: 27.8152, val_loss: 28.0308, val_MinusLogProbMetric: 28.0308

Epoch 889: val_loss improved from 28.03090 to 28.03082, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_323/weights/best_weights.h5
196/196 - 48s - loss: 27.8152 - MinusLogProbMetric: 27.8152 - val_loss: 28.0308 - val_MinusLogProbMetric: 28.0308 - lr: 6.2500e-05 - 48s/epoch - 246ms/step
Epoch 890/1000
2023-09-29 05:38:44.775 
Epoch 890/1000 
	 loss: 27.8140, MinusLogProbMetric: 27.8140, val_loss: 28.0590, val_MinusLogProbMetric: 28.0590

Epoch 890: val_loss did not improve from 28.03082
196/196 - 51s - loss: 27.8140 - MinusLogProbMetric: 27.8140 - val_loss: 28.0590 - val_MinusLogProbMetric: 28.0590 - lr: 6.2500e-05 - 51s/epoch - 260ms/step
Epoch 891/1000
2023-09-29 05:39:34.887 
Epoch 891/1000 
	 loss: 27.8133, MinusLogProbMetric: 27.8133, val_loss: 28.0423, val_MinusLogProbMetric: 28.0423

Epoch 891: val_loss did not improve from 28.03082
196/196 - 50s - loss: 27.8133 - MinusLogProbMetric: 27.8133 - val_loss: 28.0423 - val_MinusLogProbMetric: 28.0423 - lr: 6.2500e-05 - 50s/epoch - 256ms/step
Epoch 892/1000
2023-09-29 05:40:24.561 
Epoch 892/1000 
	 loss: 27.8191, MinusLogProbMetric: 27.8191, val_loss: 28.0517, val_MinusLogProbMetric: 28.0517

Epoch 892: val_loss did not improve from 28.03082
196/196 - 50s - loss: 27.8191 - MinusLogProbMetric: 27.8191 - val_loss: 28.0517 - val_MinusLogProbMetric: 28.0517 - lr: 6.2500e-05 - 50s/epoch - 253ms/step
Epoch 893/1000
2023-09-29 05:41:16.497 
Epoch 893/1000 
	 loss: 27.8141, MinusLogProbMetric: 27.8141, val_loss: 28.0459, val_MinusLogProbMetric: 28.0459

Epoch 893: val_loss did not improve from 28.03082
196/196 - 52s - loss: 27.8141 - MinusLogProbMetric: 27.8141 - val_loss: 28.0459 - val_MinusLogProbMetric: 28.0459 - lr: 6.2500e-05 - 52s/epoch - 265ms/step
Epoch 894/1000
2023-09-29 05:42:08.128 
Epoch 894/1000 
	 loss: 27.8174, MinusLogProbMetric: 27.8174, val_loss: 28.0610, val_MinusLogProbMetric: 28.0610

Epoch 894: val_loss did not improve from 28.03082
196/196 - 52s - loss: 27.8174 - MinusLogProbMetric: 27.8174 - val_loss: 28.0610 - val_MinusLogProbMetric: 28.0610 - lr: 6.2500e-05 - 52s/epoch - 263ms/step
Epoch 895/1000
2023-09-29 05:42:57.671 
Epoch 895/1000 
	 loss: 27.8180, MinusLogProbMetric: 27.8180, val_loss: 28.0373, val_MinusLogProbMetric: 28.0373

Epoch 895: val_loss did not improve from 28.03082
196/196 - 50s - loss: 27.8180 - MinusLogProbMetric: 27.8180 - val_loss: 28.0373 - val_MinusLogProbMetric: 28.0373 - lr: 6.2500e-05 - 50s/epoch - 253ms/step
Epoch 896/1000
2023-09-29 05:43:47.638 
Epoch 896/1000 
	 loss: 27.8154, MinusLogProbMetric: 27.8154, val_loss: 28.0490, val_MinusLogProbMetric: 28.0490

Epoch 896: val_loss did not improve from 28.03082
196/196 - 50s - loss: 27.8154 - MinusLogProbMetric: 27.8154 - val_loss: 28.0490 - val_MinusLogProbMetric: 28.0490 - lr: 6.2500e-05 - 50s/epoch - 255ms/step
Epoch 897/1000
2023-09-29 05:44:37.408 
Epoch 897/1000 
	 loss: 27.8139, MinusLogProbMetric: 27.8139, val_loss: 28.0434, val_MinusLogProbMetric: 28.0434

Epoch 897: val_loss did not improve from 28.03082
196/196 - 50s - loss: 27.8139 - MinusLogProbMetric: 27.8139 - val_loss: 28.0434 - val_MinusLogProbMetric: 28.0434 - lr: 6.2500e-05 - 50s/epoch - 254ms/step
Epoch 898/1000
2023-09-29 05:45:28.603 
Epoch 898/1000 
	 loss: 27.8182, MinusLogProbMetric: 27.8182, val_loss: 28.0505, val_MinusLogProbMetric: 28.0505

Epoch 898: val_loss did not improve from 28.03082
196/196 - 51s - loss: 27.8182 - MinusLogProbMetric: 27.8182 - val_loss: 28.0505 - val_MinusLogProbMetric: 28.0505 - lr: 6.2500e-05 - 51s/epoch - 261ms/step
Epoch 899/1000
2023-09-29 05:46:19.738 
Epoch 899/1000 
	 loss: 27.8196, MinusLogProbMetric: 27.8196, val_loss: 28.0496, val_MinusLogProbMetric: 28.0496

Epoch 899: val_loss did not improve from 28.03082
196/196 - 51s - loss: 27.8196 - MinusLogProbMetric: 27.8196 - val_loss: 28.0496 - val_MinusLogProbMetric: 28.0496 - lr: 6.2500e-05 - 51s/epoch - 261ms/step
Epoch 900/1000
2023-09-29 05:47:11.164 
Epoch 900/1000 
	 loss: 27.8218, MinusLogProbMetric: 27.8218, val_loss: 28.1030, val_MinusLogProbMetric: 28.1030

Epoch 900: val_loss did not improve from 28.03082
196/196 - 51s - loss: 27.8218 - MinusLogProbMetric: 27.8218 - val_loss: 28.1030 - val_MinusLogProbMetric: 28.1030 - lr: 6.2500e-05 - 51s/epoch - 262ms/step
Epoch 901/1000
2023-09-29 05:48:02.191 
Epoch 901/1000 
	 loss: 27.8213, MinusLogProbMetric: 27.8213, val_loss: 28.0577, val_MinusLogProbMetric: 28.0577

Epoch 901: val_loss did not improve from 28.03082
196/196 - 51s - loss: 27.8213 - MinusLogProbMetric: 27.8213 - val_loss: 28.0577 - val_MinusLogProbMetric: 28.0577 - lr: 6.2500e-05 - 51s/epoch - 260ms/step
Epoch 902/1000
2023-09-29 05:48:51.021 
Epoch 902/1000 
	 loss: 27.8200, MinusLogProbMetric: 27.8200, val_loss: 28.0525, val_MinusLogProbMetric: 28.0525

Epoch 902: val_loss did not improve from 28.03082
196/196 - 49s - loss: 27.8200 - MinusLogProbMetric: 27.8200 - val_loss: 28.0525 - val_MinusLogProbMetric: 28.0525 - lr: 6.2500e-05 - 49s/epoch - 249ms/step
Epoch 903/1000
2023-09-29 05:49:40.202 
Epoch 903/1000 
	 loss: 27.8224, MinusLogProbMetric: 27.8224, val_loss: 28.0418, val_MinusLogProbMetric: 28.0418

Epoch 903: val_loss did not improve from 28.03082
196/196 - 49s - loss: 27.8224 - MinusLogProbMetric: 27.8224 - val_loss: 28.0418 - val_MinusLogProbMetric: 28.0418 - lr: 6.2500e-05 - 49s/epoch - 251ms/step
Epoch 904/1000
2023-09-29 05:50:30.673 
Epoch 904/1000 
	 loss: 27.8171, MinusLogProbMetric: 27.8171, val_loss: 28.0361, val_MinusLogProbMetric: 28.0361

Epoch 904: val_loss did not improve from 28.03082
196/196 - 50s - loss: 27.8171 - MinusLogProbMetric: 27.8171 - val_loss: 28.0361 - val_MinusLogProbMetric: 28.0361 - lr: 6.2500e-05 - 50s/epoch - 257ms/step
Epoch 905/1000
2023-09-29 05:51:20.657 
Epoch 905/1000 
	 loss: 27.8173, MinusLogProbMetric: 27.8173, val_loss: 28.0344, val_MinusLogProbMetric: 28.0344

Epoch 905: val_loss did not improve from 28.03082
196/196 - 50s - loss: 27.8173 - MinusLogProbMetric: 27.8173 - val_loss: 28.0344 - val_MinusLogProbMetric: 28.0344 - lr: 6.2500e-05 - 50s/epoch - 255ms/step
Epoch 906/1000
2023-09-29 05:52:11.800 
Epoch 906/1000 
	 loss: 27.8202, MinusLogProbMetric: 27.8202, val_loss: 28.0429, val_MinusLogProbMetric: 28.0429

Epoch 906: val_loss did not improve from 28.03082
196/196 - 51s - loss: 27.8202 - MinusLogProbMetric: 27.8202 - val_loss: 28.0429 - val_MinusLogProbMetric: 28.0429 - lr: 6.2500e-05 - 51s/epoch - 261ms/step
Epoch 907/1000
2023-09-29 05:53:01.047 
Epoch 907/1000 
	 loss: 27.8171, MinusLogProbMetric: 27.8171, val_loss: 28.0374, val_MinusLogProbMetric: 28.0374

Epoch 907: val_loss did not improve from 28.03082
196/196 - 49s - loss: 27.8171 - MinusLogProbMetric: 27.8171 - val_loss: 28.0374 - val_MinusLogProbMetric: 28.0374 - lr: 6.2500e-05 - 49s/epoch - 251ms/step
Epoch 908/1000
2023-09-29 05:53:51.809 
Epoch 908/1000 
	 loss: 27.8158, MinusLogProbMetric: 27.8158, val_loss: 28.0661, val_MinusLogProbMetric: 28.0661

Epoch 908: val_loss did not improve from 28.03082
196/196 - 51s - loss: 27.8158 - MinusLogProbMetric: 27.8158 - val_loss: 28.0661 - val_MinusLogProbMetric: 28.0661 - lr: 6.2500e-05 - 51s/epoch - 259ms/step
Epoch 909/1000
2023-09-29 05:54:41.817 
Epoch 909/1000 
	 loss: 27.8239, MinusLogProbMetric: 27.8239, val_loss: 28.1170, val_MinusLogProbMetric: 28.1170

Epoch 909: val_loss did not improve from 28.03082
196/196 - 50s - loss: 27.8239 - MinusLogProbMetric: 27.8239 - val_loss: 28.1170 - val_MinusLogProbMetric: 28.1170 - lr: 6.2500e-05 - 50s/epoch - 255ms/step
Epoch 910/1000
2023-09-29 05:55:29.537 
Epoch 910/1000 
	 loss: 27.8219, MinusLogProbMetric: 27.8219, val_loss: 28.0767, val_MinusLogProbMetric: 28.0767

Epoch 910: val_loss did not improve from 28.03082
196/196 - 48s - loss: 27.8219 - MinusLogProbMetric: 27.8219 - val_loss: 28.0767 - val_MinusLogProbMetric: 28.0767 - lr: 6.2500e-05 - 48s/epoch - 243ms/step
Epoch 911/1000
2023-09-29 05:56:17.916 
Epoch 911/1000 
	 loss: 27.8226, MinusLogProbMetric: 27.8226, val_loss: 28.0513, val_MinusLogProbMetric: 28.0513

Epoch 911: val_loss did not improve from 28.03082
196/196 - 48s - loss: 27.8226 - MinusLogProbMetric: 27.8226 - val_loss: 28.0513 - val_MinusLogProbMetric: 28.0513 - lr: 6.2500e-05 - 48s/epoch - 247ms/step
Epoch 912/1000
2023-09-29 05:57:05.970 
Epoch 912/1000 
	 loss: 27.8158, MinusLogProbMetric: 27.8158, val_loss: 28.0420, val_MinusLogProbMetric: 28.0420

Epoch 912: val_loss did not improve from 28.03082
196/196 - 48s - loss: 27.8158 - MinusLogProbMetric: 27.8158 - val_loss: 28.0420 - val_MinusLogProbMetric: 28.0420 - lr: 6.2500e-05 - 48s/epoch - 245ms/step
Epoch 913/1000
2023-09-29 05:57:54.999 
Epoch 913/1000 
	 loss: 27.8151, MinusLogProbMetric: 27.8151, val_loss: 28.0558, val_MinusLogProbMetric: 28.0558

Epoch 913: val_loss did not improve from 28.03082
196/196 - 49s - loss: 27.8151 - MinusLogProbMetric: 27.8151 - val_loss: 28.0558 - val_MinusLogProbMetric: 28.0558 - lr: 6.2500e-05 - 49s/epoch - 250ms/step
Epoch 914/1000
2023-09-29 05:58:46.461 
Epoch 914/1000 
	 loss: 27.8195, MinusLogProbMetric: 27.8195, val_loss: 28.0646, val_MinusLogProbMetric: 28.0646

Epoch 914: val_loss did not improve from 28.03082
196/196 - 51s - loss: 27.8195 - MinusLogProbMetric: 27.8195 - val_loss: 28.0646 - val_MinusLogProbMetric: 28.0646 - lr: 6.2500e-05 - 51s/epoch - 263ms/step
Epoch 915/1000
2023-09-29 05:59:37.495 
Epoch 915/1000 
	 loss: 27.8162, MinusLogProbMetric: 27.8162, val_loss: 28.0267, val_MinusLogProbMetric: 28.0267

Epoch 915: val_loss improved from 28.03082 to 28.02665, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_323/weights/best_weights.h5
196/196 - 52s - loss: 27.8162 - MinusLogProbMetric: 27.8162 - val_loss: 28.0267 - val_MinusLogProbMetric: 28.0267 - lr: 6.2500e-05 - 52s/epoch - 264ms/step
Epoch 916/1000
2023-09-29 06:00:29.632 
Epoch 916/1000 
	 loss: 27.8172, MinusLogProbMetric: 27.8172, val_loss: 28.0491, val_MinusLogProbMetric: 28.0491

Epoch 916: val_loss did not improve from 28.02665
196/196 - 51s - loss: 27.8172 - MinusLogProbMetric: 27.8172 - val_loss: 28.0491 - val_MinusLogProbMetric: 28.0491 - lr: 6.2500e-05 - 51s/epoch - 262ms/step
Epoch 917/1000
2023-09-29 06:01:20.917 
Epoch 917/1000 
	 loss: 27.8149, MinusLogProbMetric: 27.8149, val_loss: 28.0423, val_MinusLogProbMetric: 28.0423

Epoch 917: val_loss did not improve from 28.02665
196/196 - 51s - loss: 27.8149 - MinusLogProbMetric: 27.8149 - val_loss: 28.0423 - val_MinusLogProbMetric: 28.0423 - lr: 6.2500e-05 - 51s/epoch - 262ms/step
Epoch 918/1000
2023-09-29 06:02:10.124 
Epoch 918/1000 
	 loss: 27.8181, MinusLogProbMetric: 27.8181, val_loss: 28.0461, val_MinusLogProbMetric: 28.0461

Epoch 918: val_loss did not improve from 28.02665
196/196 - 49s - loss: 27.8181 - MinusLogProbMetric: 27.8181 - val_loss: 28.0461 - val_MinusLogProbMetric: 28.0461 - lr: 6.2500e-05 - 49s/epoch - 251ms/step
Epoch 919/1000
2023-09-29 06:03:00.685 
Epoch 919/1000 
	 loss: 27.8183, MinusLogProbMetric: 27.8183, val_loss: 28.0405, val_MinusLogProbMetric: 28.0405

Epoch 919: val_loss did not improve from 28.02665
196/196 - 51s - loss: 27.8183 - MinusLogProbMetric: 27.8183 - val_loss: 28.0405 - val_MinusLogProbMetric: 28.0405 - lr: 6.2500e-05 - 51s/epoch - 258ms/step
Epoch 920/1000
2023-09-29 06:03:50.033 
Epoch 920/1000 
	 loss: 27.8155, MinusLogProbMetric: 27.8155, val_loss: 28.0391, val_MinusLogProbMetric: 28.0391

Epoch 920: val_loss did not improve from 28.02665
196/196 - 49s - loss: 27.8155 - MinusLogProbMetric: 27.8155 - val_loss: 28.0391 - val_MinusLogProbMetric: 28.0391 - lr: 6.2500e-05 - 49s/epoch - 252ms/step
Epoch 921/1000
2023-09-29 06:04:39.559 
Epoch 921/1000 
	 loss: 27.8182, MinusLogProbMetric: 27.8182, val_loss: 28.0485, val_MinusLogProbMetric: 28.0485

Epoch 921: val_loss did not improve from 28.02665
196/196 - 50s - loss: 27.8182 - MinusLogProbMetric: 27.8182 - val_loss: 28.0485 - val_MinusLogProbMetric: 28.0485 - lr: 6.2500e-05 - 50s/epoch - 253ms/step
Epoch 922/1000
2023-09-29 06:05:27.631 
Epoch 922/1000 
	 loss: 27.8100, MinusLogProbMetric: 27.8100, val_loss: 28.0498, val_MinusLogProbMetric: 28.0498

Epoch 922: val_loss did not improve from 28.02665
196/196 - 48s - loss: 27.8100 - MinusLogProbMetric: 27.8100 - val_loss: 28.0498 - val_MinusLogProbMetric: 28.0498 - lr: 6.2500e-05 - 48s/epoch - 245ms/step
Epoch 923/1000
2023-09-29 06:06:18.032 
Epoch 923/1000 
	 loss: 27.8133, MinusLogProbMetric: 27.8133, val_loss: 28.0467, val_MinusLogProbMetric: 28.0467

Epoch 923: val_loss did not improve from 28.02665
196/196 - 50s - loss: 27.8133 - MinusLogProbMetric: 27.8133 - val_loss: 28.0467 - val_MinusLogProbMetric: 28.0467 - lr: 6.2500e-05 - 50s/epoch - 257ms/step
Epoch 924/1000
2023-09-29 06:07:09.284 
Epoch 924/1000 
	 loss: 27.8128, MinusLogProbMetric: 27.8128, val_loss: 28.0771, val_MinusLogProbMetric: 28.0771

Epoch 924: val_loss did not improve from 28.02665
196/196 - 51s - loss: 27.8128 - MinusLogProbMetric: 27.8128 - val_loss: 28.0771 - val_MinusLogProbMetric: 28.0771 - lr: 6.2500e-05 - 51s/epoch - 261ms/step
Epoch 925/1000
2023-09-29 06:07:56.280 
Epoch 925/1000 
	 loss: 27.8133, MinusLogProbMetric: 27.8133, val_loss: 28.0402, val_MinusLogProbMetric: 28.0402

Epoch 925: val_loss did not improve from 28.02665
196/196 - 47s - loss: 27.8133 - MinusLogProbMetric: 27.8133 - val_loss: 28.0402 - val_MinusLogProbMetric: 28.0402 - lr: 6.2500e-05 - 47s/epoch - 240ms/step
Epoch 926/1000
2023-09-29 06:08:44.935 
Epoch 926/1000 
	 loss: 27.8146, MinusLogProbMetric: 27.8146, val_loss: 28.0545, val_MinusLogProbMetric: 28.0545

Epoch 926: val_loss did not improve from 28.02665
196/196 - 49s - loss: 27.8146 - MinusLogProbMetric: 27.8146 - val_loss: 28.0545 - val_MinusLogProbMetric: 28.0545 - lr: 6.2500e-05 - 49s/epoch - 248ms/step
Epoch 927/1000
2023-09-29 06:09:34.788 
Epoch 927/1000 
	 loss: 27.8229, MinusLogProbMetric: 27.8229, val_loss: 28.0380, val_MinusLogProbMetric: 28.0380

Epoch 927: val_loss did not improve from 28.02665
196/196 - 50s - loss: 27.8229 - MinusLogProbMetric: 27.8229 - val_loss: 28.0380 - val_MinusLogProbMetric: 28.0380 - lr: 6.2500e-05 - 50s/epoch - 254ms/step
Epoch 928/1000
2023-09-29 06:10:25.896 
Epoch 928/1000 
	 loss: 27.8160, MinusLogProbMetric: 27.8160, val_loss: 28.0720, val_MinusLogProbMetric: 28.0720

Epoch 928: val_loss did not improve from 28.02665
196/196 - 51s - loss: 27.8160 - MinusLogProbMetric: 27.8160 - val_loss: 28.0720 - val_MinusLogProbMetric: 28.0720 - lr: 6.2500e-05 - 51s/epoch - 261ms/step
Epoch 929/1000
2023-09-29 06:11:15.608 
Epoch 929/1000 
	 loss: 27.8146, MinusLogProbMetric: 27.8146, val_loss: 28.1044, val_MinusLogProbMetric: 28.1044

Epoch 929: val_loss did not improve from 28.02665
196/196 - 50s - loss: 27.8146 - MinusLogProbMetric: 27.8146 - val_loss: 28.1044 - val_MinusLogProbMetric: 28.1044 - lr: 6.2500e-05 - 50s/epoch - 254ms/step
Epoch 930/1000
2023-09-29 06:12:02.316 
Epoch 930/1000 
	 loss: 27.8200, MinusLogProbMetric: 27.8200, val_loss: 28.0313, val_MinusLogProbMetric: 28.0313

Epoch 930: val_loss did not improve from 28.02665
196/196 - 47s - loss: 27.8200 - MinusLogProbMetric: 27.8200 - val_loss: 28.0313 - val_MinusLogProbMetric: 28.0313 - lr: 6.2500e-05 - 47s/epoch - 238ms/step
Epoch 931/1000
2023-09-29 06:12:50.488 
Epoch 931/1000 
	 loss: 27.8187, MinusLogProbMetric: 27.8187, val_loss: 28.0308, val_MinusLogProbMetric: 28.0308

Epoch 931: val_loss did not improve from 28.02665
196/196 - 48s - loss: 27.8187 - MinusLogProbMetric: 27.8187 - val_loss: 28.0308 - val_MinusLogProbMetric: 28.0308 - lr: 6.2500e-05 - 48s/epoch - 246ms/step
Epoch 932/1000
2023-09-29 06:13:43.408 
Epoch 932/1000 
	 loss: 27.8135, MinusLogProbMetric: 27.8135, val_loss: 28.1091, val_MinusLogProbMetric: 28.1091

Epoch 932: val_loss did not improve from 28.02665
196/196 - 53s - loss: 27.8135 - MinusLogProbMetric: 27.8135 - val_loss: 28.1091 - val_MinusLogProbMetric: 28.1091 - lr: 6.2500e-05 - 53s/epoch - 270ms/step
Epoch 933/1000
2023-09-29 06:14:36.617 
Epoch 933/1000 
	 loss: 27.8184, MinusLogProbMetric: 27.8184, val_loss: 28.0755, val_MinusLogProbMetric: 28.0755

Epoch 933: val_loss did not improve from 28.02665
196/196 - 53s - loss: 27.8184 - MinusLogProbMetric: 27.8184 - val_loss: 28.0755 - val_MinusLogProbMetric: 28.0755 - lr: 6.2500e-05 - 53s/epoch - 271ms/step
Epoch 934/1000
2023-09-29 06:15:31.022 
Epoch 934/1000 
	 loss: 27.8184, MinusLogProbMetric: 27.8184, val_loss: 28.0534, val_MinusLogProbMetric: 28.0534

Epoch 934: val_loss did not improve from 28.02665
196/196 - 54s - loss: 27.8184 - MinusLogProbMetric: 27.8184 - val_loss: 28.0534 - val_MinusLogProbMetric: 28.0534 - lr: 6.2500e-05 - 54s/epoch - 278ms/step
Epoch 935/1000
2023-09-29 06:16:25.060 
Epoch 935/1000 
	 loss: 27.8190, MinusLogProbMetric: 27.8190, val_loss: 28.0795, val_MinusLogProbMetric: 28.0795

Epoch 935: val_loss did not improve from 28.02665
196/196 - 54s - loss: 27.8190 - MinusLogProbMetric: 27.8190 - val_loss: 28.0795 - val_MinusLogProbMetric: 28.0795 - lr: 6.2500e-05 - 54s/epoch - 276ms/step
Epoch 936/1000
2023-09-29 06:17:15.602 
Epoch 936/1000 
	 loss: 27.8218, MinusLogProbMetric: 27.8218, val_loss: 28.0647, val_MinusLogProbMetric: 28.0647

Epoch 936: val_loss did not improve from 28.02665
196/196 - 51s - loss: 27.8218 - MinusLogProbMetric: 27.8218 - val_loss: 28.0647 - val_MinusLogProbMetric: 28.0647 - lr: 6.2500e-05 - 51s/epoch - 258ms/step
Epoch 937/1000
2023-09-29 06:18:08.972 
Epoch 937/1000 
	 loss: 27.8252, MinusLogProbMetric: 27.8252, val_loss: 28.0507, val_MinusLogProbMetric: 28.0507

Epoch 937: val_loss did not improve from 28.02665
196/196 - 53s - loss: 27.8252 - MinusLogProbMetric: 27.8252 - val_loss: 28.0507 - val_MinusLogProbMetric: 28.0507 - lr: 6.2500e-05 - 53s/epoch - 272ms/step
Epoch 938/1000
2023-09-29 06:19:02.874 
Epoch 938/1000 
	 loss: 27.8225, MinusLogProbMetric: 27.8225, val_loss: 28.0852, val_MinusLogProbMetric: 28.0852

Epoch 938: val_loss did not improve from 28.02665
196/196 - 54s - loss: 27.8225 - MinusLogProbMetric: 27.8225 - val_loss: 28.0852 - val_MinusLogProbMetric: 28.0852 - lr: 6.2500e-05 - 54s/epoch - 275ms/step
Epoch 939/1000
2023-09-29 06:19:55.789 
Epoch 939/1000 
	 loss: 27.8167, MinusLogProbMetric: 27.8167, val_loss: 28.0501, val_MinusLogProbMetric: 28.0501

Epoch 939: val_loss did not improve from 28.02665
196/196 - 53s - loss: 27.8167 - MinusLogProbMetric: 27.8167 - val_loss: 28.0501 - val_MinusLogProbMetric: 28.0501 - lr: 6.2500e-05 - 53s/epoch - 270ms/step
Epoch 940/1000
2023-09-29 06:20:47.097 
Epoch 940/1000 
	 loss: 27.8137, MinusLogProbMetric: 27.8137, val_loss: 28.0555, val_MinusLogProbMetric: 28.0555

Epoch 940: val_loss did not improve from 28.02665
196/196 - 51s - loss: 27.8137 - MinusLogProbMetric: 27.8137 - val_loss: 28.0555 - val_MinusLogProbMetric: 28.0555 - lr: 6.2500e-05 - 51s/epoch - 262ms/step
Epoch 941/1000
2023-09-29 06:21:40.310 
Epoch 941/1000 
	 loss: 27.8150, MinusLogProbMetric: 27.8150, val_loss: 28.0322, val_MinusLogProbMetric: 28.0322

Epoch 941: val_loss did not improve from 28.02665
196/196 - 53s - loss: 27.8150 - MinusLogProbMetric: 27.8150 - val_loss: 28.0322 - val_MinusLogProbMetric: 28.0322 - lr: 6.2500e-05 - 53s/epoch - 271ms/step
Epoch 942/1000
2023-09-29 06:22:32.535 
Epoch 942/1000 
	 loss: 27.8149, MinusLogProbMetric: 27.8149, val_loss: 28.0667, val_MinusLogProbMetric: 28.0667

Epoch 942: val_loss did not improve from 28.02665
196/196 - 52s - loss: 27.8149 - MinusLogProbMetric: 27.8149 - val_loss: 28.0667 - val_MinusLogProbMetric: 28.0667 - lr: 6.2500e-05 - 52s/epoch - 266ms/step
Epoch 943/1000
2023-09-29 06:23:25.391 
Epoch 943/1000 
	 loss: 27.8211, MinusLogProbMetric: 27.8211, val_loss: 28.0436, val_MinusLogProbMetric: 28.0436

Epoch 943: val_loss did not improve from 28.02665
196/196 - 53s - loss: 27.8211 - MinusLogProbMetric: 27.8211 - val_loss: 28.0436 - val_MinusLogProbMetric: 28.0436 - lr: 6.2500e-05 - 53s/epoch - 270ms/step
Epoch 944/1000
2023-09-29 06:24:19.447 
Epoch 944/1000 
	 loss: 27.8167, MinusLogProbMetric: 27.8167, val_loss: 28.0388, val_MinusLogProbMetric: 28.0388

Epoch 944: val_loss did not improve from 28.02665
196/196 - 54s - loss: 27.8167 - MinusLogProbMetric: 27.8167 - val_loss: 28.0388 - val_MinusLogProbMetric: 28.0388 - lr: 6.2500e-05 - 54s/epoch - 276ms/step
Epoch 945/1000
2023-09-29 06:25:14.334 
Epoch 945/1000 
	 loss: 27.8133, MinusLogProbMetric: 27.8133, val_loss: 28.0383, val_MinusLogProbMetric: 28.0383

Epoch 945: val_loss did not improve from 28.02665
196/196 - 55s - loss: 27.8133 - MinusLogProbMetric: 27.8133 - val_loss: 28.0383 - val_MinusLogProbMetric: 28.0383 - lr: 6.2500e-05 - 55s/epoch - 280ms/step
Epoch 946/1000
2023-09-29 06:26:09.244 
Epoch 946/1000 
	 loss: 27.8152, MinusLogProbMetric: 27.8152, val_loss: 28.0393, val_MinusLogProbMetric: 28.0393

Epoch 946: val_loss did not improve from 28.02665
196/196 - 55s - loss: 27.8152 - MinusLogProbMetric: 27.8152 - val_loss: 28.0393 - val_MinusLogProbMetric: 28.0393 - lr: 6.2500e-05 - 55s/epoch - 280ms/step
Epoch 947/1000
2023-09-29 06:27:04.091 
Epoch 947/1000 
	 loss: 27.8222, MinusLogProbMetric: 27.8222, val_loss: 28.0784, val_MinusLogProbMetric: 28.0784

Epoch 947: val_loss did not improve from 28.02665
196/196 - 55s - loss: 27.8222 - MinusLogProbMetric: 27.8222 - val_loss: 28.0784 - val_MinusLogProbMetric: 28.0784 - lr: 6.2500e-05 - 55s/epoch - 280ms/step
Epoch 948/1000
2023-09-29 06:27:58.278 
Epoch 948/1000 
	 loss: 27.8157, MinusLogProbMetric: 27.8157, val_loss: 28.0942, val_MinusLogProbMetric: 28.0942

Epoch 948: val_loss did not improve from 28.02665
196/196 - 54s - loss: 27.8157 - MinusLogProbMetric: 27.8157 - val_loss: 28.0942 - val_MinusLogProbMetric: 28.0942 - lr: 6.2500e-05 - 54s/epoch - 276ms/step
Epoch 949/1000
2023-09-29 06:28:53.064 
Epoch 949/1000 
	 loss: 27.8152, MinusLogProbMetric: 27.8152, val_loss: 28.0411, val_MinusLogProbMetric: 28.0411

Epoch 949: val_loss did not improve from 28.02665
196/196 - 55s - loss: 27.8152 - MinusLogProbMetric: 27.8152 - val_loss: 28.0411 - val_MinusLogProbMetric: 28.0411 - lr: 6.2500e-05 - 55s/epoch - 279ms/step
Epoch 950/1000
2023-09-29 06:29:47.702 
Epoch 950/1000 
	 loss: 27.8126, MinusLogProbMetric: 27.8126, val_loss: 28.0377, val_MinusLogProbMetric: 28.0377

Epoch 950: val_loss did not improve from 28.02665
196/196 - 55s - loss: 27.8126 - MinusLogProbMetric: 27.8126 - val_loss: 28.0377 - val_MinusLogProbMetric: 28.0377 - lr: 6.2500e-05 - 55s/epoch - 279ms/step
Epoch 951/1000
2023-09-29 06:30:42.771 
Epoch 951/1000 
	 loss: 27.8097, MinusLogProbMetric: 27.8097, val_loss: 28.0310, val_MinusLogProbMetric: 28.0310

Epoch 951: val_loss did not improve from 28.02665
196/196 - 55s - loss: 27.8097 - MinusLogProbMetric: 27.8097 - val_loss: 28.0310 - val_MinusLogProbMetric: 28.0310 - lr: 6.2500e-05 - 55s/epoch - 281ms/step
Epoch 952/1000
2023-09-29 06:31:37.822 
Epoch 952/1000 
	 loss: 27.8128, MinusLogProbMetric: 27.8128, val_loss: 28.0515, val_MinusLogProbMetric: 28.0515

Epoch 952: val_loss did not improve from 28.02665
196/196 - 55s - loss: 27.8128 - MinusLogProbMetric: 27.8128 - val_loss: 28.0515 - val_MinusLogProbMetric: 28.0515 - lr: 6.2500e-05 - 55s/epoch - 281ms/step
Epoch 953/1000
2023-09-29 06:32:32.701 
Epoch 953/1000 
	 loss: 27.8184, MinusLogProbMetric: 27.8184, val_loss: 28.0713, val_MinusLogProbMetric: 28.0713

Epoch 953: val_loss did not improve from 28.02665
196/196 - 55s - loss: 27.8184 - MinusLogProbMetric: 27.8184 - val_loss: 28.0713 - val_MinusLogProbMetric: 28.0713 - lr: 6.2500e-05 - 55s/epoch - 280ms/step
Epoch 954/1000
2023-09-29 06:33:27.718 
Epoch 954/1000 
	 loss: 27.8085, MinusLogProbMetric: 27.8085, val_loss: 28.0355, val_MinusLogProbMetric: 28.0355

Epoch 954: val_loss did not improve from 28.02665
196/196 - 55s - loss: 27.8085 - MinusLogProbMetric: 27.8085 - val_loss: 28.0355 - val_MinusLogProbMetric: 28.0355 - lr: 6.2500e-05 - 55s/epoch - 281ms/step
Epoch 955/1000
2023-09-29 06:34:22.809 
Epoch 955/1000 
	 loss: 27.8107, MinusLogProbMetric: 27.8107, val_loss: 28.0436, val_MinusLogProbMetric: 28.0436

Epoch 955: val_loss did not improve from 28.02665
196/196 - 55s - loss: 27.8107 - MinusLogProbMetric: 27.8107 - val_loss: 28.0436 - val_MinusLogProbMetric: 28.0436 - lr: 6.2500e-05 - 55s/epoch - 281ms/step
Epoch 956/1000
2023-09-29 06:35:17.980 
Epoch 956/1000 
	 loss: 27.8164, MinusLogProbMetric: 27.8164, val_loss: 28.0425, val_MinusLogProbMetric: 28.0425

Epoch 956: val_loss did not improve from 28.02665
196/196 - 55s - loss: 27.8164 - MinusLogProbMetric: 27.8164 - val_loss: 28.0425 - val_MinusLogProbMetric: 28.0425 - lr: 6.2500e-05 - 55s/epoch - 281ms/step
Epoch 957/1000
2023-09-29 06:36:13.083 
Epoch 957/1000 
	 loss: 27.8132, MinusLogProbMetric: 27.8132, val_loss: 28.0548, val_MinusLogProbMetric: 28.0548

Epoch 957: val_loss did not improve from 28.02665
196/196 - 55s - loss: 27.8132 - MinusLogProbMetric: 27.8132 - val_loss: 28.0548 - val_MinusLogProbMetric: 28.0548 - lr: 6.2500e-05 - 55s/epoch - 281ms/step
Epoch 958/1000
2023-09-29 06:37:08.104 
Epoch 958/1000 
	 loss: 27.8164, MinusLogProbMetric: 27.8164, val_loss: 28.0702, val_MinusLogProbMetric: 28.0702

Epoch 958: val_loss did not improve from 28.02665
196/196 - 55s - loss: 27.8164 - MinusLogProbMetric: 27.8164 - val_loss: 28.0702 - val_MinusLogProbMetric: 28.0702 - lr: 6.2500e-05 - 55s/epoch - 281ms/step
Epoch 959/1000
2023-09-29 06:38:03.188 
Epoch 959/1000 
	 loss: 27.8202, MinusLogProbMetric: 27.8202, val_loss: 28.0386, val_MinusLogProbMetric: 28.0386

Epoch 959: val_loss did not improve from 28.02665
196/196 - 55s - loss: 27.8202 - MinusLogProbMetric: 27.8202 - val_loss: 28.0386 - val_MinusLogProbMetric: 28.0386 - lr: 6.2500e-05 - 55s/epoch - 281ms/step
Epoch 960/1000
2023-09-29 06:38:57.840 
Epoch 960/1000 
	 loss: 27.8135, MinusLogProbMetric: 27.8135, val_loss: 28.0368, val_MinusLogProbMetric: 28.0368

Epoch 960: val_loss did not improve from 28.02665
196/196 - 55s - loss: 27.8135 - MinusLogProbMetric: 27.8135 - val_loss: 28.0368 - val_MinusLogProbMetric: 28.0368 - lr: 6.2500e-05 - 55s/epoch - 279ms/step
Epoch 961/1000
2023-09-29 06:39:53.114 
Epoch 961/1000 
	 loss: 27.8126, MinusLogProbMetric: 27.8126, val_loss: 28.0396, val_MinusLogProbMetric: 28.0396

Epoch 961: val_loss did not improve from 28.02665
196/196 - 55s - loss: 27.8126 - MinusLogProbMetric: 27.8126 - val_loss: 28.0396 - val_MinusLogProbMetric: 28.0396 - lr: 6.2500e-05 - 55s/epoch - 282ms/step
Epoch 962/1000
2023-09-29 06:40:48.177 
Epoch 962/1000 
	 loss: 27.8102, MinusLogProbMetric: 27.8102, val_loss: 28.0383, val_MinusLogProbMetric: 28.0383

Epoch 962: val_loss did not improve from 28.02665
196/196 - 55s - loss: 27.8102 - MinusLogProbMetric: 27.8102 - val_loss: 28.0383 - val_MinusLogProbMetric: 28.0383 - lr: 6.2500e-05 - 55s/epoch - 281ms/step
Epoch 963/1000
2023-09-29 06:41:43.299 
Epoch 963/1000 
	 loss: 27.8245, MinusLogProbMetric: 27.8245, val_loss: 28.0376, val_MinusLogProbMetric: 28.0376

Epoch 963: val_loss did not improve from 28.02665
196/196 - 55s - loss: 27.8245 - MinusLogProbMetric: 27.8245 - val_loss: 28.0376 - val_MinusLogProbMetric: 28.0376 - lr: 6.2500e-05 - 55s/epoch - 281ms/step
Epoch 964/1000
2023-09-29 06:42:38.348 
Epoch 964/1000 
	 loss: 27.8132, MinusLogProbMetric: 27.8132, val_loss: 28.0347, val_MinusLogProbMetric: 28.0347

Epoch 964: val_loss did not improve from 28.02665
196/196 - 55s - loss: 27.8132 - MinusLogProbMetric: 27.8132 - val_loss: 28.0347 - val_MinusLogProbMetric: 28.0347 - lr: 6.2500e-05 - 55s/epoch - 281ms/step
Epoch 965/1000
2023-09-29 06:43:33.440 
Epoch 965/1000 
	 loss: 27.8134, MinusLogProbMetric: 27.8134, val_loss: 28.0982, val_MinusLogProbMetric: 28.0982

Epoch 965: val_loss did not improve from 28.02665
196/196 - 55s - loss: 27.8134 - MinusLogProbMetric: 27.8134 - val_loss: 28.0982 - val_MinusLogProbMetric: 28.0982 - lr: 6.2500e-05 - 55s/epoch - 281ms/step
Epoch 966/1000
2023-09-29 06:44:28.476 
Epoch 966/1000 
	 loss: 27.7912, MinusLogProbMetric: 27.7912, val_loss: 28.0346, val_MinusLogProbMetric: 28.0346

Epoch 966: val_loss did not improve from 28.02665
196/196 - 55s - loss: 27.7912 - MinusLogProbMetric: 27.7912 - val_loss: 28.0346 - val_MinusLogProbMetric: 28.0346 - lr: 3.1250e-05 - 55s/epoch - 281ms/step
Epoch 967/1000
2023-09-29 06:45:23.658 
Epoch 967/1000 
	 loss: 27.7901, MinusLogProbMetric: 27.7901, val_loss: 28.0182, val_MinusLogProbMetric: 28.0182

Epoch 967: val_loss improved from 28.02665 to 28.01815, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_323/weights/best_weights.h5
196/196 - 56s - loss: 27.7901 - MinusLogProbMetric: 27.7901 - val_loss: 28.0182 - val_MinusLogProbMetric: 28.0182 - lr: 3.1250e-05 - 56s/epoch - 287ms/step
Epoch 968/1000
2023-09-29 06:46:10.136 
Epoch 968/1000 
	 loss: 27.7918, MinusLogProbMetric: 27.7918, val_loss: 28.0203, val_MinusLogProbMetric: 28.0203

Epoch 968: val_loss did not improve from 28.01815
196/196 - 45s - loss: 27.7918 - MinusLogProbMetric: 27.7918 - val_loss: 28.0203 - val_MinusLogProbMetric: 28.0203 - lr: 3.1250e-05 - 45s/epoch - 231ms/step
Epoch 969/1000
2023-09-29 06:46:54.181 
Epoch 969/1000 
	 loss: 27.7905, MinusLogProbMetric: 27.7905, val_loss: 28.0315, val_MinusLogProbMetric: 28.0315

Epoch 969: val_loss did not improve from 28.01815
196/196 - 44s - loss: 27.7905 - MinusLogProbMetric: 27.7905 - val_loss: 28.0315 - val_MinusLogProbMetric: 28.0315 - lr: 3.1250e-05 - 44s/epoch - 225ms/step
Epoch 970/1000
2023-09-29 06:47:39.234 
Epoch 970/1000 
	 loss: 27.7900, MinusLogProbMetric: 27.7900, val_loss: 28.0500, val_MinusLogProbMetric: 28.0500

Epoch 970: val_loss did not improve from 28.01815
196/196 - 45s - loss: 27.7900 - MinusLogProbMetric: 27.7900 - val_loss: 28.0500 - val_MinusLogProbMetric: 28.0500 - lr: 3.1250e-05 - 45s/epoch - 230ms/step
Epoch 971/1000
2023-09-29 06:48:33.605 
Epoch 971/1000 
	 loss: 27.7924, MinusLogProbMetric: 27.7924, val_loss: 28.0208, val_MinusLogProbMetric: 28.0208

Epoch 971: val_loss did not improve from 28.01815
196/196 - 54s - loss: 27.7924 - MinusLogProbMetric: 27.7924 - val_loss: 28.0208 - val_MinusLogProbMetric: 28.0208 - lr: 3.1250e-05 - 54s/epoch - 277ms/step
Epoch 972/1000
2023-09-29 06:49:28.239 
Epoch 972/1000 
	 loss: 27.7914, MinusLogProbMetric: 27.7914, val_loss: 28.0204, val_MinusLogProbMetric: 28.0204

Epoch 972: val_loss did not improve from 28.01815
196/196 - 55s - loss: 27.7914 - MinusLogProbMetric: 27.7914 - val_loss: 28.0204 - val_MinusLogProbMetric: 28.0204 - lr: 3.1250e-05 - 55s/epoch - 279ms/step
Epoch 973/1000
2023-09-29 06:50:22.634 
Epoch 973/1000 
	 loss: 27.7903, MinusLogProbMetric: 27.7903, val_loss: 28.0299, val_MinusLogProbMetric: 28.0299

Epoch 973: val_loss did not improve from 28.01815
196/196 - 54s - loss: 27.7903 - MinusLogProbMetric: 27.7903 - val_loss: 28.0299 - val_MinusLogProbMetric: 28.0299 - lr: 3.1250e-05 - 54s/epoch - 278ms/step
Epoch 974/1000
2023-09-29 06:51:18.715 
Epoch 974/1000 
	 loss: 27.7944, MinusLogProbMetric: 27.7944, val_loss: 28.0196, val_MinusLogProbMetric: 28.0196

Epoch 974: val_loss did not improve from 28.01815
196/196 - 56s - loss: 27.7944 - MinusLogProbMetric: 27.7944 - val_loss: 28.0196 - val_MinusLogProbMetric: 28.0196 - lr: 3.1250e-05 - 56s/epoch - 286ms/step
Epoch 975/1000
2023-09-29 06:52:13.643 
Epoch 975/1000 
	 loss: 27.7889, MinusLogProbMetric: 27.7889, val_loss: 28.0281, val_MinusLogProbMetric: 28.0281

Epoch 975: val_loss did not improve from 28.01815
196/196 - 55s - loss: 27.7889 - MinusLogProbMetric: 27.7889 - val_loss: 28.0281 - val_MinusLogProbMetric: 28.0281 - lr: 3.1250e-05 - 55s/epoch - 280ms/step
Epoch 976/1000
2023-09-29 06:53:08.331 
Epoch 976/1000 
	 loss: 27.7901, MinusLogProbMetric: 27.7901, val_loss: 28.0153, val_MinusLogProbMetric: 28.0153

Epoch 976: val_loss improved from 28.01815 to 28.01534, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_323/weights/best_weights.h5
196/196 - 56s - loss: 27.7901 - MinusLogProbMetric: 27.7901 - val_loss: 28.0153 - val_MinusLogProbMetric: 28.0153 - lr: 3.1250e-05 - 56s/epoch - 283ms/step
Epoch 977/1000
2023-09-29 06:54:04.817 
Epoch 977/1000 
	 loss: 27.7900, MinusLogProbMetric: 27.7900, val_loss: 28.0409, val_MinusLogProbMetric: 28.0409

Epoch 977: val_loss did not improve from 28.01534
196/196 - 56s - loss: 27.7900 - MinusLogProbMetric: 27.7900 - val_loss: 28.0409 - val_MinusLogProbMetric: 28.0409 - lr: 3.1250e-05 - 56s/epoch - 284ms/step
Epoch 978/1000
2023-09-29 06:55:00.608 
Epoch 978/1000 
	 loss: 27.7917, MinusLogProbMetric: 27.7917, val_loss: 28.0563, val_MinusLogProbMetric: 28.0563

Epoch 978: val_loss did not improve from 28.01534
196/196 - 56s - loss: 27.7917 - MinusLogProbMetric: 27.7917 - val_loss: 28.0563 - val_MinusLogProbMetric: 28.0563 - lr: 3.1250e-05 - 56s/epoch - 285ms/step
Epoch 979/1000
2023-09-29 06:55:55.970 
Epoch 979/1000 
	 loss: 27.7913, MinusLogProbMetric: 27.7913, val_loss: 28.0462, val_MinusLogProbMetric: 28.0462

Epoch 979: val_loss did not improve from 28.01534
196/196 - 55s - loss: 27.7913 - MinusLogProbMetric: 27.7913 - val_loss: 28.0462 - val_MinusLogProbMetric: 28.0462 - lr: 3.1250e-05 - 55s/epoch - 282ms/step
Epoch 980/1000
2023-09-29 06:56:50.803 
Epoch 980/1000 
	 loss: 27.7880, MinusLogProbMetric: 27.7880, val_loss: 28.0190, val_MinusLogProbMetric: 28.0190

Epoch 980: val_loss did not improve from 28.01534
196/196 - 55s - loss: 27.7880 - MinusLogProbMetric: 27.7880 - val_loss: 28.0190 - val_MinusLogProbMetric: 28.0190 - lr: 3.1250e-05 - 55s/epoch - 280ms/step
Epoch 981/1000
2023-09-29 06:57:45.604 
Epoch 981/1000 
	 loss: 27.7928, MinusLogProbMetric: 27.7928, val_loss: 28.0274, val_MinusLogProbMetric: 28.0274

Epoch 981: val_loss did not improve from 28.01534
196/196 - 55s - loss: 27.7928 - MinusLogProbMetric: 27.7928 - val_loss: 28.0274 - val_MinusLogProbMetric: 28.0274 - lr: 3.1250e-05 - 55s/epoch - 280ms/step
Epoch 982/1000
2023-09-29 06:58:40.188 
Epoch 982/1000 
	 loss: 27.7929, MinusLogProbMetric: 27.7929, val_loss: 28.0169, val_MinusLogProbMetric: 28.0169

Epoch 982: val_loss did not improve from 28.01534
196/196 - 55s - loss: 27.7929 - MinusLogProbMetric: 27.7929 - val_loss: 28.0169 - val_MinusLogProbMetric: 28.0169 - lr: 3.1250e-05 - 55s/epoch - 278ms/step
Epoch 983/1000
2023-09-29 06:59:35.588 
Epoch 983/1000 
	 loss: 27.7884, MinusLogProbMetric: 27.7884, val_loss: 28.0233, val_MinusLogProbMetric: 28.0233

Epoch 983: val_loss did not improve from 28.01534
196/196 - 55s - loss: 27.7884 - MinusLogProbMetric: 27.7884 - val_loss: 28.0233 - val_MinusLogProbMetric: 28.0233 - lr: 3.1250e-05 - 55s/epoch - 283ms/step
Epoch 984/1000
2023-09-29 07:00:30.224 
Epoch 984/1000 
	 loss: 27.7907, MinusLogProbMetric: 27.7907, val_loss: 28.0278, val_MinusLogProbMetric: 28.0278

Epoch 984: val_loss did not improve from 28.01534
196/196 - 55s - loss: 27.7907 - MinusLogProbMetric: 27.7907 - val_loss: 28.0278 - val_MinusLogProbMetric: 28.0278 - lr: 3.1250e-05 - 55s/epoch - 279ms/step
Epoch 985/1000
2023-09-29 07:01:25.328 
Epoch 985/1000 
	 loss: 27.7905, MinusLogProbMetric: 27.7905, val_loss: 28.0226, val_MinusLogProbMetric: 28.0226

Epoch 985: val_loss did not improve from 28.01534
196/196 - 55s - loss: 27.7905 - MinusLogProbMetric: 27.7905 - val_loss: 28.0226 - val_MinusLogProbMetric: 28.0226 - lr: 3.1250e-05 - 55s/epoch - 281ms/step
Epoch 986/1000
2023-09-29 07:02:16.691 
Epoch 986/1000 
	 loss: 27.7923, MinusLogProbMetric: 27.7923, val_loss: 28.0402, val_MinusLogProbMetric: 28.0402

Epoch 986: val_loss did not improve from 28.01534
196/196 - 51s - loss: 27.7923 - MinusLogProbMetric: 27.7923 - val_loss: 28.0402 - val_MinusLogProbMetric: 28.0402 - lr: 3.1250e-05 - 51s/epoch - 262ms/step
Epoch 987/1000
2023-09-29 07:03:08.393 
Epoch 987/1000 
	 loss: 27.7926, MinusLogProbMetric: 27.7926, val_loss: 28.0252, val_MinusLogProbMetric: 28.0252

Epoch 987: val_loss did not improve from 28.01534
196/196 - 52s - loss: 27.7926 - MinusLogProbMetric: 27.7926 - val_loss: 28.0252 - val_MinusLogProbMetric: 28.0252 - lr: 3.1250e-05 - 52s/epoch - 264ms/step
Epoch 988/1000
2023-09-29 07:04:03.658 
Epoch 988/1000 
	 loss: 27.7889, MinusLogProbMetric: 27.7889, val_loss: 28.0437, val_MinusLogProbMetric: 28.0437

Epoch 988: val_loss did not improve from 28.01534
196/196 - 55s - loss: 27.7889 - MinusLogProbMetric: 27.7889 - val_loss: 28.0437 - val_MinusLogProbMetric: 28.0437 - lr: 3.1250e-05 - 55s/epoch - 282ms/step
Epoch 989/1000
2023-09-29 07:04:58.785 
Epoch 989/1000 
	 loss: 27.7895, MinusLogProbMetric: 27.7895, val_loss: 28.0172, val_MinusLogProbMetric: 28.0172

Epoch 989: val_loss did not improve from 28.01534
196/196 - 55s - loss: 27.7895 - MinusLogProbMetric: 27.7895 - val_loss: 28.0172 - val_MinusLogProbMetric: 28.0172 - lr: 3.1250e-05 - 55s/epoch - 281ms/step
Epoch 990/1000
2023-09-29 07:05:53.575 
Epoch 990/1000 
	 loss: 27.7882, MinusLogProbMetric: 27.7882, val_loss: 28.0196, val_MinusLogProbMetric: 28.0196

Epoch 990: val_loss did not improve from 28.01534
196/196 - 55s - loss: 27.7882 - MinusLogProbMetric: 27.7882 - val_loss: 28.0196 - val_MinusLogProbMetric: 28.0196 - lr: 3.1250e-05 - 55s/epoch - 280ms/step
Epoch 991/1000
2023-09-29 07:06:48.017 
Epoch 991/1000 
	 loss: 27.7873, MinusLogProbMetric: 27.7873, val_loss: 28.0228, val_MinusLogProbMetric: 28.0228

Epoch 991: val_loss did not improve from 28.01534
196/196 - 54s - loss: 27.7873 - MinusLogProbMetric: 27.7873 - val_loss: 28.0228 - val_MinusLogProbMetric: 28.0228 - lr: 3.1250e-05 - 54s/epoch - 278ms/step
Epoch 992/1000
2023-09-29 07:07:42.868 
Epoch 992/1000 
	 loss: 27.7892, MinusLogProbMetric: 27.7892, val_loss: 28.0147, val_MinusLogProbMetric: 28.0147

Epoch 992: val_loss improved from 28.01534 to 28.01475, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_323/weights/best_weights.h5
196/196 - 56s - loss: 27.7892 - MinusLogProbMetric: 27.7892 - val_loss: 28.0147 - val_MinusLogProbMetric: 28.0147 - lr: 3.1250e-05 - 56s/epoch - 285ms/step
Epoch 993/1000
2023-09-29 07:08:37.419 
Epoch 993/1000 
	 loss: 27.7889, MinusLogProbMetric: 27.7889, val_loss: 28.0154, val_MinusLogProbMetric: 28.0154

Epoch 993: val_loss did not improve from 28.01475
196/196 - 54s - loss: 27.7889 - MinusLogProbMetric: 27.7889 - val_loss: 28.0154 - val_MinusLogProbMetric: 28.0154 - lr: 3.1250e-05 - 54s/epoch - 273ms/step
Epoch 994/1000
2023-09-29 07:09:31.896 
Epoch 994/1000 
	 loss: 27.7885, MinusLogProbMetric: 27.7885, val_loss: 28.0240, val_MinusLogProbMetric: 28.0240

Epoch 994: val_loss did not improve from 28.01475
196/196 - 54s - loss: 27.7885 - MinusLogProbMetric: 27.7885 - val_loss: 28.0240 - val_MinusLogProbMetric: 28.0240 - lr: 3.1250e-05 - 54s/epoch - 278ms/step
Epoch 995/1000
2023-09-29 07:10:27.195 
Epoch 995/1000 
	 loss: 27.7878, MinusLogProbMetric: 27.7878, val_loss: 28.0175, val_MinusLogProbMetric: 28.0175

Epoch 995: val_loss did not improve from 28.01475
196/196 - 55s - loss: 27.7878 - MinusLogProbMetric: 27.7878 - val_loss: 28.0175 - val_MinusLogProbMetric: 28.0175 - lr: 3.1250e-05 - 55s/epoch - 282ms/step
Epoch 996/1000
2023-09-29 07:11:20.728 
Epoch 996/1000 
	 loss: 27.7882, MinusLogProbMetric: 27.7882, val_loss: 28.0296, val_MinusLogProbMetric: 28.0296

Epoch 996: val_loss did not improve from 28.01475
196/196 - 54s - loss: 27.7882 - MinusLogProbMetric: 27.7882 - val_loss: 28.0296 - val_MinusLogProbMetric: 28.0296 - lr: 3.1250e-05 - 54s/epoch - 273ms/step
Epoch 997/1000
2023-09-29 07:12:16.029 
Epoch 997/1000 
	 loss: 27.7879, MinusLogProbMetric: 27.7879, val_loss: 28.0166, val_MinusLogProbMetric: 28.0166

Epoch 997: val_loss did not improve from 28.01475
196/196 - 55s - loss: 27.7879 - MinusLogProbMetric: 27.7879 - val_loss: 28.0166 - val_MinusLogProbMetric: 28.0166 - lr: 3.1250e-05 - 55s/epoch - 282ms/step
Epoch 998/1000
2023-09-29 07:13:09.132 
Epoch 998/1000 
	 loss: 27.7878, MinusLogProbMetric: 27.7878, val_loss: 28.0420, val_MinusLogProbMetric: 28.0420

Epoch 998: val_loss did not improve from 28.01475
196/196 - 53s - loss: 27.7878 - MinusLogProbMetric: 27.7878 - val_loss: 28.0420 - val_MinusLogProbMetric: 28.0420 - lr: 3.1250e-05 - 53s/epoch - 271ms/step
Epoch 999/1000
2023-09-29 07:14:04.938 
Epoch 999/1000 
	 loss: 27.7892, MinusLogProbMetric: 27.7892, val_loss: 28.0360, val_MinusLogProbMetric: 28.0360

Epoch 999: val_loss did not improve from 28.01475
196/196 - 56s - loss: 27.7892 - MinusLogProbMetric: 27.7892 - val_loss: 28.0360 - val_MinusLogProbMetric: 28.0360 - lr: 3.1250e-05 - 56s/epoch - 285ms/step
Epoch 1000/1000
2023-09-29 07:14:59.457 
Epoch 1000/1000 
	 loss: 27.7896, MinusLogProbMetric: 27.7896, val_loss: 28.0231, val_MinusLogProbMetric: 28.0231

Epoch 1000: val_loss did not improve from 28.01475
196/196 - 55s - loss: 27.7896 - MinusLogProbMetric: 27.7896 - val_loss: 28.0231 - val_MinusLogProbMetric: 28.0231 - lr: 3.1250e-05 - 55s/epoch - 278ms/step
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.

------------------------------------------
Starting LR metric calculation...
Running TF LR calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
LR metric calculation completed in 57.010995944961905 seconds.

------------------------------------------
Starting KS tests calculation...
Running TF KS tests...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.

------------------------------------------
Starting LR metric calculation...
Running TF LR calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
WARNING:tensorflow:5 out of the last 5 calls to <function LRMetric.Test_tf.<locals>.compute_test at 0x7f7b5caadf30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
LR metric calculation completed in 55.23500335699646 seconds.

------------------------------------------
Starting KS tests calculation...
Running TF KS tests...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
nchunks = 10
Iterating from 0 to 1 out of 10 .
Iterating from 1 to 2 out of 10 .
Iterating from 2 to 3 out of 10 .
Iterating from 3 to 4 out of 10 .
Iterating from 4 to 5 out of 10 .
Iterating from 5 to 6 out of 10 .
Iterating from 6 to 7 out of 10 .
Iterating from 7 to 8 out of 10 .
Iterating from 8 to 9 out of 10 .
Iterating from 9 to 10 out of 10 .
KS tests calculation completed in 18.702977775014006 seconds.

------------------------------------------
Starting SWD metric calculation...
Running TF SWD calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
SWD metric calculation completed in 16.942251187982038 seconds.

------------------------------------------
Starting FN metric calculation...
Running TF FN calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
nchunks = 10
Iterating from 0 to 1 out of 10 .
Iterating from 1 to 2 out of 10 .
Iterating from 2 to 3 out of 10 .
Iterating from 3 to 4 out of 10 .
Iterating from 4 to 5 out of 10 .
Iterating from 5 to 6 out of 10 .
Iterating from 6 to 7 out of 10 .
Iterating from 7 to 8 out of 10 .
Iterating from 8 to 9 out of 10 .
Iterating from 9 to 10 out of 10 .
FN metric calculation completed in 18.6739400580409 seconds.
Training succeeded with seed 0.
Model trained in 52692.30 s.

===========
Computing predictions
===========

Computing metrics...
===========
Failed on GPU, re-trying on CPU
===========

Computing metrics...
Metrics computed in 110.70 s.
                ===========
                print("===========
Failed to plot
===========
")
                Exception type: ValueError
                Exception message: Number of rows must be a positive integer, not 0
                Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN.py , Line : 469, Func.Name : prediction_function, Message : Plotters.cornerplotter(X_data_test.numpy(),X_data_nf.numpy(),path_to_results,ndims,norm=True) # type: ignore', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/Plotters.py , Line : 156, Func.Name : cornerplotter, Message : figure=corner.corner(target_samples,color=\'red\',bins=n_bins,labels=[r"%s" % s for s in labels],normalize1d=True)', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/corner.py , Line : 235, Func.Name : corner, Message : fig, axes = pl.subplots(K, K, figsize=(dim, dim))', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/pyplot.py , Line : 1502, Func.Name : subplots, Message : axs = fig.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 905, Func.Name : subplots, Message : gs = self.add_gridspec(nrows, ncols, figure=self, **gridspec_kw)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 1527, Func.Name : add_gridspec, Message : gs = GridSpec(nrows=nrows, ncols=ncols, figure=self, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 379, Func.Name : __init__, Message : super().__init__(nrows, ncols,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 49, Func.Name : __init__, Message : raise ValueError(']
                ===========

results.txt saved
results.json saved
Results log saved
Model predictions computed in 186.98 s.
===========
Run 323/720 done in 52885.73 s.
===========

Directory ../../results/CsplineN_new/run_324/ already exists.
Skipping it.
===========
Run 324/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_325/ already exists.
Skipping it.
===========
Run 325/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_326/ already exists.
Skipping it.
===========
Run 326/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_327/ already exists.
Skipping it.
===========
Run 327/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_328/ already exists.
Skipping it.
===========
Run 328/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_329/ already exists.
Skipping it.
===========
Run 329/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_330/ already exists.
Skipping it.
===========
Run 330/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_331/ already exists.
Skipping it.
===========
Run 331/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_332/ already exists.
Skipping it.
===========
Run 332/720 already exists. Skipping it.
===========

===========
Generating train data for run 333.
===========
Train data generated in 0.19 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_333/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_333/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_333/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_333
self.data_kwargs: {'seed': 187}
self.x_data: [[ 6.8153725   2.8430347   6.1778684  ...  2.16058     2.2525268
   1.7267085 ]
 [ 2.1836705   1.800204    9.035551   ...  6.5503054   1.5567943
   2.9656742 ]
 [ 0.87054014  3.1855702   7.8237348  ...  6.3044343  -0.7088827
   2.3190107 ]
 ...
 [ 0.35442734  3.573855   10.922302   ...  5.8448343   0.8037275
   2.171598  ]
 [ 1.4908957   3.8775673   8.757837   ...  5.7039022  -0.88000226
   2.3087878 ]
 [ 1.7291393   3.7802503   9.770119   ...  5.5898924   0.22995539
   4.1106014 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_77"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_78 (InputLayer)       [(None, 64)]              0         
                                                                 
 log_prob_layer_7 (LogProbLa  (None,)                  1321920   
 yer)                                                            
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_7/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_7'")
self.model: <keras.engine.functional.Functional object at 0x7f7cd3c1c4f0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f7cd37b9ab0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f7cd37b9ab0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f8059e74ac0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f7cd36e1120>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_333/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f7cd36e1690>, <keras.callbacks.ModelCheckpoint object at 0x7f7cd36e1750>, <keras.callbacks.EarlyStopping object at 0x7f7cd36e19c0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f7cd36e19f0>, <keras.callbacks.TerminateOnNaN object at 0x7f7cd36e1630>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_333/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 333/720 with hyperparameters:
timestamp = 2023-09-29 07:18:15.983553
ndims = 64
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 6.8153725e+00  2.8430347e+00  6.1778684e+00  4.2452879e+00
  1.6284513e+00  3.3207846e+00  7.0928354e+00  6.4340367e+00
  6.0301833e+00  6.4328456e+00  6.5495849e+00  4.9321003e+00
  8.9532204e+00  3.4481030e+00  4.8284545e+00  8.0901794e+00
  8.1549883e+00  7.3900299e+00  1.3510420e+00  9.5149851e+00
  7.5267930e+00  1.0017762e+01  1.1636026e+00  8.6080103e+00
  2.2276068e+00  6.2524571e+00  1.5594521e+00  8.2755499e+00
  8.5135918e+00  6.3978677e+00  3.8566861e+00  6.8423492e-01
  7.5210962e+00  5.4144373e+00  6.8548636e+00  8.7448330e+00
  9.6529083e+00  8.6500130e+00  9.1009712e-01  4.1672401e+00
  7.6439004e+00  9.0885794e-01  5.2691178e+00  8.8166595e-03
  7.4780917e-01 -4.8575500e-01  7.9515800e+00  2.7509699e+00
  4.8898096e+00  1.0073138e+01  7.4751706e+00  6.5633142e-01
  1.5831217e+00  6.0002384e+00  5.6322212e+00  2.2999794e+00
  9.1524439e+00  5.5740438e+00  5.8525505e+00  6.6422234e+00
  6.9292612e+00  2.1605799e+00  2.2525268e+00  1.7267085e+00]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-29 07:20:57.690 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7292.4321, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 162s - loss: nan - MinusLogProbMetric: 7292.4321 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 162s/epoch - 824ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 0.0003333333333333333.
===========
Generating train data for run 333.
===========
Train data generated in 0.43 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_333/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_333/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_333/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_333
self.data_kwargs: {'seed': 187}
self.x_data: [[ 6.8153725   2.8430347   6.1778684  ...  2.16058     2.2525268
   1.7267085 ]
 [ 2.1836705   1.800204    9.035551   ...  6.5503054   1.5567943
   2.9656742 ]
 [ 0.87054014  3.1855702   7.8237348  ...  6.3044343  -0.7088827
   2.3190107 ]
 ...
 [ 0.35442734  3.573855   10.922302   ...  5.8448343   0.8037275
   2.171598  ]
 [ 1.4908957   3.8775673   8.757837   ...  5.7039022  -0.88000226
   2.3087878 ]
 [ 1.7291393   3.7802503   9.770119   ...  5.5898924   0.22995539
   4.1106014 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_88"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_89 (InputLayer)       [(None, 64)]              0         
                                                                 
 log_prob_layer_8 (LogProbLa  (None,)                  1321920   
 yer)                                                            
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_8/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_8'")
self.model: <keras.engine.functional.Functional object at 0x7f7e48522110>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f7d0b9a11e0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f7d0b9a11e0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f7d0b80beb0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f83af791b40>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_333/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f83af7920b0>, <keras.callbacks.ModelCheckpoint object at 0x7f83af792170>, <keras.callbacks.EarlyStopping object at 0x7f83af7923e0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f83af792410>, <keras.callbacks.TerminateOnNaN object at 0x7f83af792050>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_333/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 333/720 with hyperparameters:
timestamp = 2023-09-29 07:21:09.036973
ndims = 64
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 6.8153725e+00  2.8430347e+00  6.1778684e+00  4.2452879e+00
  1.6284513e+00  3.3207846e+00  7.0928354e+00  6.4340367e+00
  6.0301833e+00  6.4328456e+00  6.5495849e+00  4.9321003e+00
  8.9532204e+00  3.4481030e+00  4.8284545e+00  8.0901794e+00
  8.1549883e+00  7.3900299e+00  1.3510420e+00  9.5149851e+00
  7.5267930e+00  1.0017762e+01  1.1636026e+00  8.6080103e+00
  2.2276068e+00  6.2524571e+00  1.5594521e+00  8.2755499e+00
  8.5135918e+00  6.3978677e+00  3.8566861e+00  6.8423492e-01
  7.5210962e+00  5.4144373e+00  6.8548636e+00  8.7448330e+00
  9.6529083e+00  8.6500130e+00  9.1009712e-01  4.1672401e+00
  7.6439004e+00  9.0885794e-01  5.2691178e+00  8.8166595e-03
  7.4780917e-01 -4.8575500e-01  7.9515800e+00  2.7509699e+00
  4.8898096e+00  1.0073138e+01  7.4751706e+00  6.5633142e-01
  1.5831217e+00  6.0002384e+00  5.6322212e+00  2.2999794e+00
  9.1524439e+00  5.5740438e+00  5.8525505e+00  6.6422234e+00
  6.9292612e+00  2.1605799e+00  2.2525268e+00  1.7267085e+00]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-29 07:23:43.766 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7292.4321, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 155s - loss: nan - MinusLogProbMetric: 7292.4321 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 155s/epoch - 789ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 0.0001111111111111111.
===========
Generating train data for run 333.
===========
Train data generated in 0.45 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_333/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_333/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_333/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_333
self.data_kwargs: {'seed': 187}
self.x_data: [[ 6.8153725   2.8430347   6.1778684  ...  2.16058     2.2525268
   1.7267085 ]
 [ 2.1836705   1.800204    9.035551   ...  6.5503054   1.5567943
   2.9656742 ]
 [ 0.87054014  3.1855702   7.8237348  ...  6.3044343  -0.7088827
   2.3190107 ]
 ...
 [ 0.35442734  3.573855   10.922302   ...  5.8448343   0.8037275
   2.171598  ]
 [ 1.4908957   3.8775673   8.757837   ...  5.7039022  -0.88000226
   2.3087878 ]
 [ 1.7291393   3.7802503   9.770119   ...  5.5898924   0.22995539
   4.1106014 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_99"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_100 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_9 (LogProbLa  (None,)                  1321920   
 yer)                                                            
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_9/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_9'")
self.model: <keras.engine.functional.Functional object at 0x7f7cd089be20>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f7f8c152140>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f7f8c152140>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f83845b6290>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f7f2c338340>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_333/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f7f2c3388b0>, <keras.callbacks.ModelCheckpoint object at 0x7f7f2c338970>, <keras.callbacks.EarlyStopping object at 0x7f7f2c338be0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f7f2c338c10>, <keras.callbacks.TerminateOnNaN object at 0x7f7f2c338850>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_333/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 333/720 with hyperparameters:
timestamp = 2023-09-29 07:23:53.758701
ndims = 64
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 6.8153725e+00  2.8430347e+00  6.1778684e+00  4.2452879e+00
  1.6284513e+00  3.3207846e+00  7.0928354e+00  6.4340367e+00
  6.0301833e+00  6.4328456e+00  6.5495849e+00  4.9321003e+00
  8.9532204e+00  3.4481030e+00  4.8284545e+00  8.0901794e+00
  8.1549883e+00  7.3900299e+00  1.3510420e+00  9.5149851e+00
  7.5267930e+00  1.0017762e+01  1.1636026e+00  8.6080103e+00
  2.2276068e+00  6.2524571e+00  1.5594521e+00  8.2755499e+00
  8.5135918e+00  6.3978677e+00  3.8566861e+00  6.8423492e-01
  7.5210962e+00  5.4144373e+00  6.8548636e+00  8.7448330e+00
  9.6529083e+00  8.6500130e+00  9.1009712e-01  4.1672401e+00
  7.6439004e+00  9.0885794e-01  5.2691178e+00  8.8166595e-03
  7.4780917e-01 -4.8575500e-01  7.9515800e+00  2.7509699e+00
  4.8898096e+00  1.0073138e+01  7.4751706e+00  6.5633142e-01
  1.5831217e+00  6.0002384e+00  5.6322212e+00  2.2999794e+00
  9.1524439e+00  5.5740438e+00  5.8525505e+00  6.6422234e+00
  6.9292612e+00  2.1605799e+00  2.2525268e+00  1.7267085e+00]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-29 07:26:27.192 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7292.4321, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 153s - loss: nan - MinusLogProbMetric: 7292.4321 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.1111e-04 - 153s/epoch - 781ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 3.703703703703703e-05.
===========
Generating train data for run 333.
===========
Train data generated in 0.31 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_333/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_333/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_333/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_333
self.data_kwargs: {'seed': 187}
self.x_data: [[ 6.8153725   2.8430347   6.1778684  ...  2.16058     2.2525268
   1.7267085 ]
 [ 2.1836705   1.800204    9.035551   ...  6.5503054   1.5567943
   2.9656742 ]
 [ 0.87054014  3.1855702   7.8237348  ...  6.3044343  -0.7088827
   2.3190107 ]
 ...
 [ 0.35442734  3.573855   10.922302   ...  5.8448343   0.8037275
   2.171598  ]
 [ 1.4908957   3.8775673   8.757837   ...  5.7039022  -0.88000226
   2.3087878 ]
 [ 1.7291393   3.7802503   9.770119   ...  5.5898924   0.22995539
   4.1106014 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_110"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_111 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_10 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_10/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_10'")
self.model: <keras.engine.functional.Functional object at 0x7f83af713ca0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f800c331690>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f800c331690>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f7f085b5c30>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f7fe461d210>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_333/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f7fe461d780>, <keras.callbacks.ModelCheckpoint object at 0x7f7fe461d840>, <keras.callbacks.EarlyStopping object at 0x7f7fe461dab0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f7fe461dae0>, <keras.callbacks.TerminateOnNaN object at 0x7f7fe461d720>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_333/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 333/720 with hyperparameters:
timestamp = 2023-09-29 07:26:37.877299
ndims = 64
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 3.703703703703703e-05...
Train first sample: [ 6.8153725e+00  2.8430347e+00  6.1778684e+00  4.2452879e+00
  1.6284513e+00  3.3207846e+00  7.0928354e+00  6.4340367e+00
  6.0301833e+00  6.4328456e+00  6.5495849e+00  4.9321003e+00
  8.9532204e+00  3.4481030e+00  4.8284545e+00  8.0901794e+00
  8.1549883e+00  7.3900299e+00  1.3510420e+00  9.5149851e+00
  7.5267930e+00  1.0017762e+01  1.1636026e+00  8.6080103e+00
  2.2276068e+00  6.2524571e+00  1.5594521e+00  8.2755499e+00
  8.5135918e+00  6.3978677e+00  3.8566861e+00  6.8423492e-01
  7.5210962e+00  5.4144373e+00  6.8548636e+00  8.7448330e+00
  9.6529083e+00  8.6500130e+00  9.1009712e-01  4.1672401e+00
  7.6439004e+00  9.0885794e-01  5.2691178e+00  8.8166595e-03
  7.4780917e-01 -4.8575500e-01  7.9515800e+00  2.7509699e+00
  4.8898096e+00  1.0073138e+01  7.4751706e+00  6.5633142e-01
  1.5831217e+00  6.0002384e+00  5.6322212e+00  2.2999794e+00
  9.1524439e+00  5.5740438e+00  5.8525505e+00  6.6422234e+00
  6.9292612e+00  2.1605799e+00  2.2525268e+00  1.7267085e+00]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
WARNING:tensorflow:5 out of the last 196007 calls to <function Model.make_train_function.<locals>.train_function at 0x7f7d288e9900> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-29 07:29:12.405 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7292.4321, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 154s - loss: nan - MinusLogProbMetric: 7292.4321 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.7037e-05 - 154s/epoch - 787ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 1.2345679012345677e-05.
===========
Generating train data for run 333.
===========
Train data generated in 0.40 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_333/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_333/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_333/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_333
self.data_kwargs: {'seed': 187}
self.x_data: [[ 6.8153725   2.8430347   6.1778684  ...  2.16058     2.2525268
   1.7267085 ]
 [ 2.1836705   1.800204    9.035551   ...  6.5503054   1.5567943
   2.9656742 ]
 [ 0.87054014  3.1855702   7.8237348  ...  6.3044343  -0.7088827
   2.3190107 ]
 ...
 [ 0.35442734  3.573855   10.922302   ...  5.8448343   0.8037275
   2.171598  ]
 [ 1.4908957   3.8775673   8.757837   ...  5.7039022  -0.88000226
   2.3087878 ]
 [ 1.7291393   3.7802503   9.770119   ...  5.5898924   0.22995539
   4.1106014 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_121"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_122 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_11 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_11/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_11'")
self.model: <keras.engine.functional.Functional object at 0x7f7afccbaaa0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f83846e59f0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f83846e59f0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f7af473e650>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f788e6a0d90>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_333/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f788e6a1300>, <keras.callbacks.ModelCheckpoint object at 0x7f788e6a13c0>, <keras.callbacks.EarlyStopping object at 0x7f788e6a1630>, <keras.callbacks.ReduceLROnPlateau object at 0x7f788e6a1660>, <keras.callbacks.TerminateOnNaN object at 0x7f788e6a12a0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_333/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 333/720 with hyperparameters:
timestamp = 2023-09-29 07:29:24.577543
ndims = 64
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.2345679012345677e-05...
Train first sample: [ 6.8153725e+00  2.8430347e+00  6.1778684e+00  4.2452879e+00
  1.6284513e+00  3.3207846e+00  7.0928354e+00  6.4340367e+00
  6.0301833e+00  6.4328456e+00  6.5495849e+00  4.9321003e+00
  8.9532204e+00  3.4481030e+00  4.8284545e+00  8.0901794e+00
  8.1549883e+00  7.3900299e+00  1.3510420e+00  9.5149851e+00
  7.5267930e+00  1.0017762e+01  1.1636026e+00  8.6080103e+00
  2.2276068e+00  6.2524571e+00  1.5594521e+00  8.2755499e+00
  8.5135918e+00  6.3978677e+00  3.8566861e+00  6.8423492e-01
  7.5210962e+00  5.4144373e+00  6.8548636e+00  8.7448330e+00
  9.6529083e+00  8.6500130e+00  9.1009712e-01  4.1672401e+00
  7.6439004e+00  9.0885794e-01  5.2691178e+00  8.8166595e-03
  7.4780917e-01 -4.8575500e-01  7.9515800e+00  2.7509699e+00
  4.8898096e+00  1.0073138e+01  7.4751706e+00  6.5633142e-01
  1.5831217e+00  6.0002384e+00  5.6322212e+00  2.2999794e+00
  9.1524439e+00  5.5740438e+00  5.8525505e+00  6.6422234e+00
  6.9292612e+00  2.1605799e+00  2.2525268e+00  1.7267085e+00]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
WARNING:tensorflow:6 out of the last 196009 calls to <function Model.make_train_function.<locals>.train_function at 0x7f7d0b4db5b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-29 07:32:07.323 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7292.4321, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 162s - loss: nan - MinusLogProbMetric: 7292.4321 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.2346e-05 - 162s/epoch - 829ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 4.115226337448558e-06.
===========
Generating train data for run 333.
===========
Train data generated in 0.47 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_333/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_333/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_333/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_333
self.data_kwargs: {'seed': 187}
self.x_data: [[ 6.8153725   2.8430347   6.1778684  ...  2.16058     2.2525268
   1.7267085 ]
 [ 2.1836705   1.800204    9.035551   ...  6.5503054   1.5567943
   2.9656742 ]
 [ 0.87054014  3.1855702   7.8237348  ...  6.3044343  -0.7088827
   2.3190107 ]
 ...
 [ 0.35442734  3.573855   10.922302   ...  5.8448343   0.8037275
   2.171598  ]
 [ 1.4908957   3.8775673   8.757837   ...  5.7039022  -0.88000226
   2.3087878 ]
 [ 1.7291393   3.7802503   9.770119   ...  5.5898924   0.22995539
   4.1106014 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_132"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_133 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_12 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_12/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_12'")
self.model: <keras.engine.functional.Functional object at 0x7f80504f66e0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f7c2862b4c0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f7c2862b4c0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f7f08468fd0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f7afc8a4bb0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_333/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f7afc8a42e0>, <keras.callbacks.ModelCheckpoint object at 0x7f7afc8a4490>, <keras.callbacks.EarlyStopping object at 0x7f7afc8a5e70>, <keras.callbacks.ReduceLROnPlateau object at 0x7f7afc8a6ec0>, <keras.callbacks.TerminateOnNaN object at 0x7f7afc8a4be0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_333/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 333/720 with hyperparameters:
timestamp = 2023-09-29 07:32:19.557814
ndims = 64
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.115226337448558e-06...
Train first sample: [ 6.8153725e+00  2.8430347e+00  6.1778684e+00  4.2452879e+00
  1.6284513e+00  3.3207846e+00  7.0928354e+00  6.4340367e+00
  6.0301833e+00  6.4328456e+00  6.5495849e+00  4.9321003e+00
  8.9532204e+00  3.4481030e+00  4.8284545e+00  8.0901794e+00
  8.1549883e+00  7.3900299e+00  1.3510420e+00  9.5149851e+00
  7.5267930e+00  1.0017762e+01  1.1636026e+00  8.6080103e+00
  2.2276068e+00  6.2524571e+00  1.5594521e+00  8.2755499e+00
  8.5135918e+00  6.3978677e+00  3.8566861e+00  6.8423492e-01
  7.5210962e+00  5.4144373e+00  6.8548636e+00  8.7448330e+00
  9.6529083e+00  8.6500130e+00  9.1009712e-01  4.1672401e+00
  7.6439004e+00  9.0885794e-01  5.2691178e+00  8.8166595e-03
  7.4780917e-01 -4.8575500e-01  7.9515800e+00  2.7509699e+00
  4.8898096e+00  1.0073138e+01  7.4751706e+00  6.5633142e-01
  1.5831217e+00  6.0002384e+00  5.6322212e+00  2.2999794e+00
  9.1524439e+00  5.5740438e+00  5.8525505e+00  6.6422234e+00
  6.9292612e+00  2.1605799e+00  2.2525268e+00  1.7267085e+00]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-29 07:34:45.816 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7292.4321, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 146s - loss: nan - MinusLogProbMetric: 7292.4321 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.1152e-06 - 146s/epoch - 745ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 1.3717421124828526e-06.
===========
Generating train data for run 333.
===========
Train data generated in 0.36 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_333/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_333/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_333/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_333
self.data_kwargs: {'seed': 187}
self.x_data: [[ 6.8153725   2.8430347   6.1778684  ...  2.16058     2.2525268
   1.7267085 ]
 [ 2.1836705   1.800204    9.035551   ...  6.5503054   1.5567943
   2.9656742 ]
 [ 0.87054014  3.1855702   7.8237348  ...  6.3044343  -0.7088827
   2.3190107 ]
 ...
 [ 0.35442734  3.573855   10.922302   ...  5.8448343   0.8037275
   2.171598  ]
 [ 1.4908957   3.8775673   8.757837   ...  5.7039022  -0.88000226
   2.3087878 ]
 [ 1.7291393   3.7802503   9.770119   ...  5.5898924   0.22995539
   4.1106014 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_143"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_144 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_13 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_13/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_13'")
self.model: <keras.engine.functional.Functional object at 0x7f8384092320>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f79886ef640>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f79886ef640>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f8394f0b1c0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f798865fb50>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_333/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f7afced6f50>, <keras.callbacks.ModelCheckpoint object at 0x7f798848a920>, <keras.callbacks.EarlyStopping object at 0x7f7988489990>, <keras.callbacks.ReduceLROnPlateau object at 0x7f79884899f0>, <keras.callbacks.TerminateOnNaN object at 0x7f798848baf0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_333/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 333/720 with hyperparameters:
timestamp = 2023-09-29 07:34:55.157320
ndims = 64
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.3717421124828526e-06...
Train first sample: [ 6.8153725e+00  2.8430347e+00  6.1778684e+00  4.2452879e+00
  1.6284513e+00  3.3207846e+00  7.0928354e+00  6.4340367e+00
  6.0301833e+00  6.4328456e+00  6.5495849e+00  4.9321003e+00
  8.9532204e+00  3.4481030e+00  4.8284545e+00  8.0901794e+00
  8.1549883e+00  7.3900299e+00  1.3510420e+00  9.5149851e+00
  7.5267930e+00  1.0017762e+01  1.1636026e+00  8.6080103e+00
  2.2276068e+00  6.2524571e+00  1.5594521e+00  8.2755499e+00
  8.5135918e+00  6.3978677e+00  3.8566861e+00  6.8423492e-01
  7.5210962e+00  5.4144373e+00  6.8548636e+00  8.7448330e+00
  9.6529083e+00  8.6500130e+00  9.1009712e-01  4.1672401e+00
  7.6439004e+00  9.0885794e-01  5.2691178e+00  8.8166595e-03
  7.4780917e-01 -4.8575500e-01  7.9515800e+00  2.7509699e+00
  4.8898096e+00  1.0073138e+01  7.4751706e+00  6.5633142e-01
  1.5831217e+00  6.0002384e+00  5.6322212e+00  2.2999794e+00
  9.1524439e+00  5.5740438e+00  5.8525505e+00  6.6422234e+00
  6.9292612e+00  2.1605799e+00  2.2525268e+00  1.7267085e+00]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-29 07:37:06.605 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7292.4321, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 131s - loss: nan - MinusLogProbMetric: 7292.4321 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.3717e-06 - 131s/epoch - 669ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 4.572473708276175e-07.
===========
Generating train data for run 333.
===========
Train data generated in 0.35 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_333/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_333/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_333/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_333
self.data_kwargs: {'seed': 187}
self.x_data: [[ 6.8153725   2.8430347   6.1778684  ...  2.16058     2.2525268
   1.7267085 ]
 [ 2.1836705   1.800204    9.035551   ...  6.5503054   1.5567943
   2.9656742 ]
 [ 0.87054014  3.1855702   7.8237348  ...  6.3044343  -0.7088827
   2.3190107 ]
 ...
 [ 0.35442734  3.573855   10.922302   ...  5.8448343   0.8037275
   2.171598  ]
 [ 1.4908957   3.8775673   8.757837   ...  5.7039022  -0.88000226
   2.3087878 ]
 [ 1.7291393   3.7802503   9.770119   ...  5.5898924   0.22995539
   4.1106014 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_154"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_155 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_14 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_14/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_14'")
self.model: <keras.engine.functional.Functional object at 0x7f7cd305e7d0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f7eac6884c0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f7eac6884c0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f7aec5168c0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f7cd29272e0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_333/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f7cd2927850>, <keras.callbacks.ModelCheckpoint object at 0x7f7cd2927910>, <keras.callbacks.EarlyStopping object at 0x7f7cd2927b80>, <keras.callbacks.ReduceLROnPlateau object at 0x7f7cd2927bb0>, <keras.callbacks.TerminateOnNaN object at 0x7f7cd29277f0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_333/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 333/720 with hyperparameters:
timestamp = 2023-09-29 07:37:16.581273
ndims = 64
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.572473708276175e-07...
Train first sample: [ 6.8153725e+00  2.8430347e+00  6.1778684e+00  4.2452879e+00
  1.6284513e+00  3.3207846e+00  7.0928354e+00  6.4340367e+00
  6.0301833e+00  6.4328456e+00  6.5495849e+00  4.9321003e+00
  8.9532204e+00  3.4481030e+00  4.8284545e+00  8.0901794e+00
  8.1549883e+00  7.3900299e+00  1.3510420e+00  9.5149851e+00
  7.5267930e+00  1.0017762e+01  1.1636026e+00  8.6080103e+00
  2.2276068e+00  6.2524571e+00  1.5594521e+00  8.2755499e+00
  8.5135918e+00  6.3978677e+00  3.8566861e+00  6.8423492e-01
  7.5210962e+00  5.4144373e+00  6.8548636e+00  8.7448330e+00
  9.6529083e+00  8.6500130e+00  9.1009712e-01  4.1672401e+00
  7.6439004e+00  9.0885794e-01  5.2691178e+00  8.8166595e-03
  7.4780917e-01 -4.8575500e-01  7.9515800e+00  2.7509699e+00
  4.8898096e+00  1.0073138e+01  7.4751706e+00  6.5633142e-01
  1.5831217e+00  6.0002384e+00  5.6322212e+00  2.2999794e+00
  9.1524439e+00  5.5740438e+00  5.8525505e+00  6.6422234e+00
  6.9292612e+00  2.1605799e+00  2.2525268e+00  1.7267085e+00]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-29 07:39:45.828 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7292.4321, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 149s - loss: nan - MinusLogProbMetric: 7292.4321 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.5725e-07 - 149s/epoch - 760ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 1.524157902758725e-07.
===========
Generating train data for run 333.
===========
Train data generated in 0.42 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_333/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_333/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_333/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_333
self.data_kwargs: {'seed': 187}
self.x_data: [[ 6.8153725   2.8430347   6.1778684  ...  2.16058     2.2525268
   1.7267085 ]
 [ 2.1836705   1.800204    9.035551   ...  6.5503054   1.5567943
   2.9656742 ]
 [ 0.87054014  3.1855702   7.8237348  ...  6.3044343  -0.7088827
   2.3190107 ]
 ...
 [ 0.35442734  3.573855   10.922302   ...  5.8448343   0.8037275
   2.171598  ]
 [ 1.4908957   3.8775673   8.757837   ...  5.7039022  -0.88000226
   2.3087878 ]
 [ 1.7291393   3.7802503   9.770119   ...  5.5898924   0.22995539
   4.1106014 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_165"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_166 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_15 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_15/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_15'")
self.model: <keras.engine.functional.Functional object at 0x7f7af430e530>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f7aec47fd90>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f7aec47fd90>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f77400ab700>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f7748ef6080>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_333/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f7748ef43d0>, <keras.callbacks.ModelCheckpoint object at 0x7f7748ef4730>, <keras.callbacks.EarlyStopping object at 0x7f7748eac340>, <keras.callbacks.ReduceLROnPlateau object at 0x7f7748eac3a0>, <keras.callbacks.TerminateOnNaN object at 0x7f7748eac370>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_333/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 333/720 with hyperparameters:
timestamp = 2023-09-29 07:39:56.403210
ndims = 64
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.524157902758725e-07...
Train first sample: [ 6.8153725e+00  2.8430347e+00  6.1778684e+00  4.2452879e+00
  1.6284513e+00  3.3207846e+00  7.0928354e+00  6.4340367e+00
  6.0301833e+00  6.4328456e+00  6.5495849e+00  4.9321003e+00
  8.9532204e+00  3.4481030e+00  4.8284545e+00  8.0901794e+00
  8.1549883e+00  7.3900299e+00  1.3510420e+00  9.5149851e+00
  7.5267930e+00  1.0017762e+01  1.1636026e+00  8.6080103e+00
  2.2276068e+00  6.2524571e+00  1.5594521e+00  8.2755499e+00
  8.5135918e+00  6.3978677e+00  3.8566861e+00  6.8423492e-01
  7.5210962e+00  5.4144373e+00  6.8548636e+00  8.7448330e+00
  9.6529083e+00  8.6500130e+00  9.1009712e-01  4.1672401e+00
  7.6439004e+00  9.0885794e-01  5.2691178e+00  8.8166595e-03
  7.4780917e-01 -4.8575500e-01  7.9515800e+00  2.7509699e+00
  4.8898096e+00  1.0073138e+01  7.4751706e+00  6.5633142e-01
  1.5831217e+00  6.0002384e+00  5.6322212e+00  2.2999794e+00
  9.1524439e+00  5.5740438e+00  5.8525505e+00  6.6422234e+00
  6.9292612e+00  2.1605799e+00  2.2525268e+00  1.7267085e+00]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-29 07:42:21.123 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7292.4321, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 144s - loss: nan - MinusLogProbMetric: 7292.4321 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.5242e-07 - 144s/epoch - 737ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 5.0805263425290834e-08.
===========
Generating train data for run 333.
===========
Train data generated in 0.33 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_333/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_333/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_333/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_333
self.data_kwargs: {'seed': 187}
self.x_data: [[ 6.8153725   2.8430347   6.1778684  ...  2.16058     2.2525268
   1.7267085 ]
 [ 2.1836705   1.800204    9.035551   ...  6.5503054   1.5567943
   2.9656742 ]
 [ 0.87054014  3.1855702   7.8237348  ...  6.3044343  -0.7088827
   2.3190107 ]
 ...
 [ 0.35442734  3.573855   10.922302   ...  5.8448343   0.8037275
   2.171598  ]
 [ 1.4908957   3.8775673   8.757837   ...  5.7039022  -0.88000226
   2.3087878 ]
 [ 1.7291393   3.7802503   9.770119   ...  5.5898924   0.22995539
   4.1106014 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_176"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_177 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_16 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_16/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_16'")
self.model: <keras.engine.functional.Functional object at 0x7f7afcc517e0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f7b5cdc3280>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f7b5cdc3280>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f7fac208730>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f7b687469e0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_333/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f7b68746f50>, <keras.callbacks.ModelCheckpoint object at 0x7f7b68747010>, <keras.callbacks.EarlyStopping object at 0x7f7b68747280>, <keras.callbacks.ReduceLROnPlateau object at 0x7f7b687472b0>, <keras.callbacks.TerminateOnNaN object at 0x7f7b68746ef0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_333/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 333/720 with hyperparameters:
timestamp = 2023-09-29 07:42:30.449532
ndims = 64
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 5.0805263425290834e-08...
Train first sample: [ 6.8153725e+00  2.8430347e+00  6.1778684e+00  4.2452879e+00
  1.6284513e+00  3.3207846e+00  7.0928354e+00  6.4340367e+00
  6.0301833e+00  6.4328456e+00  6.5495849e+00  4.9321003e+00
  8.9532204e+00  3.4481030e+00  4.8284545e+00  8.0901794e+00
  8.1549883e+00  7.3900299e+00  1.3510420e+00  9.5149851e+00
  7.5267930e+00  1.0017762e+01  1.1636026e+00  8.6080103e+00
  2.2276068e+00  6.2524571e+00  1.5594521e+00  8.2755499e+00
  8.5135918e+00  6.3978677e+00  3.8566861e+00  6.8423492e-01
  7.5210962e+00  5.4144373e+00  6.8548636e+00  8.7448330e+00
  9.6529083e+00  8.6500130e+00  9.1009712e-01  4.1672401e+00
  7.6439004e+00  9.0885794e-01  5.2691178e+00  8.8166595e-03
  7.4780917e-01 -4.8575500e-01  7.9515800e+00  2.7509699e+00
  4.8898096e+00  1.0073138e+01  7.4751706e+00  6.5633142e-01
  1.5831217e+00  6.0002384e+00  5.6322212e+00  2.2999794e+00
  9.1524439e+00  5.5740438e+00  5.8525505e+00  6.6422234e+00
  6.9292612e+00  2.1605799e+00  2.2525268e+00  1.7267085e+00]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-29 07:44:58.517 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7292.4321, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 148s - loss: nan - MinusLogProbMetric: 7292.4321 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 5.0805e-08 - 148s/epoch - 754ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 1.6935087808430278e-08.
===========
Generating train data for run 333.
===========
Train data generated in 0.37 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_333/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_333/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_333/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_333
self.data_kwargs: {'seed': 187}
self.x_data: [[ 6.8153725   2.8430347   6.1778684  ...  2.16058     2.2525268
   1.7267085 ]
 [ 2.1836705   1.800204    9.035551   ...  6.5503054   1.5567943
   2.9656742 ]
 [ 0.87054014  3.1855702   7.8237348  ...  6.3044343  -0.7088827
   2.3190107 ]
 ...
 [ 0.35442734  3.573855   10.922302   ...  5.8448343   0.8037275
   2.171598  ]
 [ 1.4908957   3.8775673   8.757837   ...  5.7039022  -0.88000226
   2.3087878 ]
 [ 1.7291393   3.7802503   9.770119   ...  5.5898924   0.22995539
   4.1106014 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_187"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_188 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_17 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_17/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_17'")
self.model: <keras.engine.functional.Functional object at 0x7f7750294820>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f77505dfc70>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f77505dfc70>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f7e80217b50>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f77502b3820>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_333/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f77502b3130>, <keras.callbacks.ModelCheckpoint object at 0x7f77502b2f50>, <keras.callbacks.EarlyStopping object at 0x7f77502b1660>, <keras.callbacks.ReduceLROnPlateau object at 0x7f77502b1750>, <keras.callbacks.TerminateOnNaN object at 0x7f77502b3070>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_333/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 333/720 with hyperparameters:
timestamp = 2023-09-29 07:45:09.134850
ndims = 64
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.6935087808430278e-08...
Train first sample: [ 6.8153725e+00  2.8430347e+00  6.1778684e+00  4.2452879e+00
  1.6284513e+00  3.3207846e+00  7.0928354e+00  6.4340367e+00
  6.0301833e+00  6.4328456e+00  6.5495849e+00  4.9321003e+00
  8.9532204e+00  3.4481030e+00  4.8284545e+00  8.0901794e+00
  8.1549883e+00  7.3900299e+00  1.3510420e+00  9.5149851e+00
  7.5267930e+00  1.0017762e+01  1.1636026e+00  8.6080103e+00
  2.2276068e+00  6.2524571e+00  1.5594521e+00  8.2755499e+00
  8.5135918e+00  6.3978677e+00  3.8566861e+00  6.8423492e-01
  7.5210962e+00  5.4144373e+00  6.8548636e+00  8.7448330e+00
  9.6529083e+00  8.6500130e+00  9.1009712e-01  4.1672401e+00
  7.6439004e+00  9.0885794e-01  5.2691178e+00  8.8166595e-03
  7.4780917e-01 -4.8575500e-01  7.9515800e+00  2.7509699e+00
  4.8898096e+00  1.0073138e+01  7.4751706e+00  6.5633142e-01
  1.5831217e+00  6.0002384e+00  5.6322212e+00  2.2999794e+00
  9.1524439e+00  5.5740438e+00  5.8525505e+00  6.6422234e+00
  6.9292612e+00  2.1605799e+00  2.2525268e+00  1.7267085e+00]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-29 07:47:25.560 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7292.4321, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 136s - loss: nan - MinusLogProbMetric: 7292.4321 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.6935e-08 - 136s/epoch - 695ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 5.645029269476759e-09.
===========
Run 333/720 failed.
Exception type: Exception
Exception message: Training failed for the maximum number of retry.
Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN.py , Line : 637, Func.Name : <module>, Message : hyperparams_dict, NFObject, seed_train, training_time = train_function(seeds = [seed_train, seed_test, seed_dist, seed_metrics],', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN.py , Line : 313, Func.Name : train_function, Message : raise Exception("Training failed for the maximum number of retry.")']
===========

Directory ../../results/CsplineN_new/run_334/ already exists.
Skipping it.
===========
Run 334/720 already exists. Skipping it.
===========

===========
Generating train data for run 335.
===========
Train data generated in 0.38 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_335/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_335/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_335/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_335
self.data_kwargs: {'seed': 187}
self.x_data: [[ 6.8153725   2.8430347   6.1778684  ...  2.16058     2.2525268
   1.7267085 ]
 [ 2.1836705   1.800204    9.035551   ...  6.5503054   1.5567943
   2.9656742 ]
 [ 0.87054014  3.1855702   7.8237348  ...  6.3044343  -0.7088827
   2.3190107 ]
 ...
 [ 0.35442734  3.573855   10.922302   ...  5.8448343   0.8037275
   2.171598  ]
 [ 1.4908957   3.8775673   8.757837   ...  5.7039022  -0.88000226
   2.3087878 ]
 [ 1.7291393   3.7802503   9.770119   ...  5.5898924   0.22995539
   4.1106014 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_198"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_199 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_18 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_18/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_18'")
self.model: <keras.engine.functional.Functional object at 0x7f7b887d9180>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f7e806817b0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f7e806817b0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f7f6424f790>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f8384460bb0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_335/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f8384461120>, <keras.callbacks.ModelCheckpoint object at 0x7f83844611e0>, <keras.callbacks.EarlyStopping object at 0x7f8384461450>, <keras.callbacks.ReduceLROnPlateau object at 0x7f8384461480>, <keras.callbacks.TerminateOnNaN object at 0x7f83844610c0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_335/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 335/720 with hyperparameters:
timestamp = 2023-09-29 07:47:38.746709
ndims = 64
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 6.8153725e+00  2.8430347e+00  6.1778684e+00  4.2452879e+00
  1.6284513e+00  3.3207846e+00  7.0928354e+00  6.4340367e+00
  6.0301833e+00  6.4328456e+00  6.5495849e+00  4.9321003e+00
  8.9532204e+00  3.4481030e+00  4.8284545e+00  8.0901794e+00
  8.1549883e+00  7.3900299e+00  1.3510420e+00  9.5149851e+00
  7.5267930e+00  1.0017762e+01  1.1636026e+00  8.6080103e+00
  2.2276068e+00  6.2524571e+00  1.5594521e+00  8.2755499e+00
  8.5135918e+00  6.3978677e+00  3.8566861e+00  6.8423492e-01
  7.5210962e+00  5.4144373e+00  6.8548636e+00  8.7448330e+00
  9.6529083e+00  8.6500130e+00  9.1009712e-01  4.1672401e+00
  7.6439004e+00  9.0885794e-01  5.2691178e+00  8.8166595e-03
  7.4780917e-01 -4.8575500e-01  7.9515800e+00  2.7509699e+00
  4.8898096e+00  1.0073138e+01  7.4751706e+00  6.5633142e-01
  1.5831217e+00  6.0002384e+00  5.6322212e+00  2.2999794e+00
  9.1524439e+00  5.5740438e+00  5.8525505e+00  6.6422234e+00
  6.9292612e+00  2.1605799e+00  2.2525268e+00  1.7267085e+00]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-29 07:50:55.433 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7498.4800, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 196s - loss: nan - MinusLogProbMetric: 7498.4800 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 196s/epoch - 1s/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 0.0003333333333333333.
===========
Generating train data for run 335.
===========
Train data generated in 0.29 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_335/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_335/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_335/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_335
self.data_kwargs: {'seed': 187}
self.x_data: [[ 6.8153725   2.8430347   6.1778684  ...  2.16058     2.2525268
   1.7267085 ]
 [ 2.1836705   1.800204    9.035551   ...  6.5503054   1.5567943
   2.9656742 ]
 [ 0.87054014  3.1855702   7.8237348  ...  6.3044343  -0.7088827
   2.3190107 ]
 ...
 [ 0.35442734  3.573855   10.922302   ...  5.8448343   0.8037275
   2.171598  ]
 [ 1.4908957   3.8775673   8.757837   ...  5.7039022  -0.88000226
   2.3087878 ]
 [ 1.7291393   3.7802503   9.770119   ...  5.5898924   0.22995539
   4.1106014 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_209"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_210 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_19 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_19/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_19'")
self.model: <keras.engine.functional.Functional object at 0x7f773976e4d0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f7740ca9720>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f7740ca9720>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f77285170a0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f7cd1aa1900>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_335/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f7cd1aa1e70>, <keras.callbacks.ModelCheckpoint object at 0x7f7cd1aa1f30>, <keras.callbacks.EarlyStopping object at 0x7f7cd1aa21a0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f7cd1aa21d0>, <keras.callbacks.TerminateOnNaN object at 0x7f7cd1aa1e10>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_335/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 335/720 with hyperparameters:
timestamp = 2023-09-29 07:51:07.312083
ndims = 64
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 6.8153725e+00  2.8430347e+00  6.1778684e+00  4.2452879e+00
  1.6284513e+00  3.3207846e+00  7.0928354e+00  6.4340367e+00
  6.0301833e+00  6.4328456e+00  6.5495849e+00  4.9321003e+00
  8.9532204e+00  3.4481030e+00  4.8284545e+00  8.0901794e+00
  8.1549883e+00  7.3900299e+00  1.3510420e+00  9.5149851e+00
  7.5267930e+00  1.0017762e+01  1.1636026e+00  8.6080103e+00
  2.2276068e+00  6.2524571e+00  1.5594521e+00  8.2755499e+00
  8.5135918e+00  6.3978677e+00  3.8566861e+00  6.8423492e-01
  7.5210962e+00  5.4144373e+00  6.8548636e+00  8.7448330e+00
  9.6529083e+00  8.6500130e+00  9.1009712e-01  4.1672401e+00
  7.6439004e+00  9.0885794e-01  5.2691178e+00  8.8166595e-03
  7.4780917e-01 -4.8575500e-01  7.9515800e+00  2.7509699e+00
  4.8898096e+00  1.0073138e+01  7.4751706e+00  6.5633142e-01
  1.5831217e+00  6.0002384e+00  5.6322212e+00  2.2999794e+00
  9.1524439e+00  5.5740438e+00  5.8525505e+00  6.6422234e+00
  6.9292612e+00  2.1605799e+00  2.2525268e+00  1.7267085e+00]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-29 07:54:09.155 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7498.4800, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 182s - loss: nan - MinusLogProbMetric: 7498.4800 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 182s/epoch - 927ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 0.0001111111111111111.
===========
Generating train data for run 335.
===========
Train data generated in 0.31 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_335/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_335/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_335/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_335
self.data_kwargs: {'seed': 187}
self.x_data: [[ 6.8153725   2.8430347   6.1778684  ...  2.16058     2.2525268
   1.7267085 ]
 [ 2.1836705   1.800204    9.035551   ...  6.5503054   1.5567943
   2.9656742 ]
 [ 0.87054014  3.1855702   7.8237348  ...  6.3044343  -0.7088827
   2.3190107 ]
 ...
 [ 0.35442734  3.573855   10.922302   ...  5.8448343   0.8037275
   2.171598  ]
 [ 1.4908957   3.8775673   8.757837   ...  5.7039022  -0.88000226
   2.3087878 ]
 [ 1.7291393   3.7802503   9.770119   ...  5.5898924   0.22995539
   4.1106014 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_220"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_221 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_20 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_20/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_20'")
self.model: <keras.engine.functional.Functional object at 0x7f7740bfb9d0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f7739dc9720>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f7739dc9720>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f7af4217820>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f7740805ba0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_335/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f7740805180>, <keras.callbacks.ModelCheckpoint object at 0x7f7740805150>, <keras.callbacks.EarlyStopping object at 0x7f7740804c70>, <keras.callbacks.ReduceLROnPlateau object at 0x7f7740804730>, <keras.callbacks.TerminateOnNaN object at 0x7f7740805480>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_335/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 335/720 with hyperparameters:
timestamp = 2023-09-29 07:54:22.228784
ndims = 64
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 6.8153725e+00  2.8430347e+00  6.1778684e+00  4.2452879e+00
  1.6284513e+00  3.3207846e+00  7.0928354e+00  6.4340367e+00
  6.0301833e+00  6.4328456e+00  6.5495849e+00  4.9321003e+00
  8.9532204e+00  3.4481030e+00  4.8284545e+00  8.0901794e+00
  8.1549883e+00  7.3900299e+00  1.3510420e+00  9.5149851e+00
  7.5267930e+00  1.0017762e+01  1.1636026e+00  8.6080103e+00
  2.2276068e+00  6.2524571e+00  1.5594521e+00  8.2755499e+00
  8.5135918e+00  6.3978677e+00  3.8566861e+00  6.8423492e-01
  7.5210962e+00  5.4144373e+00  6.8548636e+00  8.7448330e+00
  9.6529083e+00  8.6500130e+00  9.1009712e-01  4.1672401e+00
  7.6439004e+00  9.0885794e-01  5.2691178e+00  8.8166595e-03
  7.4780917e-01 -4.8575500e-01  7.9515800e+00  2.7509699e+00
  4.8898096e+00  1.0073138e+01  7.4751706e+00  6.5633142e-01
  1.5831217e+00  6.0002384e+00  5.6322212e+00  2.2999794e+00
  9.1524439e+00  5.5740438e+00  5.8525505e+00  6.6422234e+00
  6.9292612e+00  2.1605799e+00  2.2525268e+00  1.7267085e+00]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-29 07:57:38.755 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7498.4800, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 196s - loss: nan - MinusLogProbMetric: 7498.4800 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.1111e-04 - 196s/epoch - 1s/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 3.703703703703703e-05.
===========
Generating train data for run 335.
===========
Train data generated in 0.37 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_335/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_335/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_335/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_335
self.data_kwargs: {'seed': 187}
self.x_data: [[ 6.8153725   2.8430347   6.1778684  ...  2.16058     2.2525268
   1.7267085 ]
 [ 2.1836705   1.800204    9.035551   ...  6.5503054   1.5567943
   2.9656742 ]
 [ 0.87054014  3.1855702   7.8237348  ...  6.3044343  -0.7088827
   2.3190107 ]
 ...
 [ 0.35442734  3.573855   10.922302   ...  5.8448343   0.8037275
   2.171598  ]
 [ 1.4908957   3.8775673   8.757837   ...  5.7039022  -0.88000226
   2.3087878 ]
 [ 1.7291393   3.7802503   9.770119   ...  5.5898924   0.22995539
   4.1106014 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_231"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_232 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_21 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_21/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_21'")
self.model: <keras.engine.functional.Functional object at 0x7f77428420b0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f7afc291750>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f7afc291750>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f7728a4fb20>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f7749e029e0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_335/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f7749e02f50>, <keras.callbacks.ModelCheckpoint object at 0x7f7749e03010>, <keras.callbacks.EarlyStopping object at 0x7f7749e03280>, <keras.callbacks.ReduceLROnPlateau object at 0x7f7749e032b0>, <keras.callbacks.TerminateOnNaN object at 0x7f7749e02ef0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_335/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 335/720 with hyperparameters:
timestamp = 2023-09-29 07:57:51.683425
ndims = 64
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 3.703703703703703e-05...
Train first sample: [ 6.8153725e+00  2.8430347e+00  6.1778684e+00  4.2452879e+00
  1.6284513e+00  3.3207846e+00  7.0928354e+00  6.4340367e+00
  6.0301833e+00  6.4328456e+00  6.5495849e+00  4.9321003e+00
  8.9532204e+00  3.4481030e+00  4.8284545e+00  8.0901794e+00
  8.1549883e+00  7.3900299e+00  1.3510420e+00  9.5149851e+00
  7.5267930e+00  1.0017762e+01  1.1636026e+00  8.6080103e+00
  2.2276068e+00  6.2524571e+00  1.5594521e+00  8.2755499e+00
  8.5135918e+00  6.3978677e+00  3.8566861e+00  6.8423492e-01
  7.5210962e+00  5.4144373e+00  6.8548636e+00  8.7448330e+00
  9.6529083e+00  8.6500130e+00  9.1009712e-01  4.1672401e+00
  7.6439004e+00  9.0885794e-01  5.2691178e+00  8.8166595e-03
  7.4780917e-01 -4.8575500e-01  7.9515800e+00  2.7509699e+00
  4.8898096e+00  1.0073138e+01  7.4751706e+00  6.5633142e-01
  1.5831217e+00  6.0002384e+00  5.6322212e+00  2.2999794e+00
  9.1524439e+00  5.5740438e+00  5.8525505e+00  6.6422234e+00
  6.9292612e+00  2.1605799e+00  2.2525268e+00  1.7267085e+00]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-29 08:01:03.625 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7498.4800, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 192s - loss: nan - MinusLogProbMetric: 7498.4800 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.7037e-05 - 192s/epoch - 978ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 1.2345679012345677e-05.
===========
Generating train data for run 335.
===========
Train data generated in 0.35 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_335/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_335/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_335/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_335
self.data_kwargs: {'seed': 187}
self.x_data: [[ 6.8153725   2.8430347   6.1778684  ...  2.16058     2.2525268
   1.7267085 ]
 [ 2.1836705   1.800204    9.035551   ...  6.5503054   1.5567943
   2.9656742 ]
 [ 0.87054014  3.1855702   7.8237348  ...  6.3044343  -0.7088827
   2.3190107 ]
 ...
 [ 0.35442734  3.573855   10.922302   ...  5.8448343   0.8037275
   2.171598  ]
 [ 1.4908957   3.8775673   8.757837   ...  5.7039022  -0.88000226
   2.3087878 ]
 [ 1.7291393   3.7802503   9.770119   ...  5.5898924   0.22995539
   4.1106014 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_242"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_243 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_22 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_22/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_22'")
self.model: <keras.engine.functional.Functional object at 0x7f7aec156830>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f772884a890>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f772884a890>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f7740c039d0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f77520b6110>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_335/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f77520b6680>, <keras.callbacks.ModelCheckpoint object at 0x7f77520b6740>, <keras.callbacks.EarlyStopping object at 0x7f77520b69b0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f77520b69e0>, <keras.callbacks.TerminateOnNaN object at 0x7f77520b6620>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_335/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 335/720 with hyperparameters:
timestamp = 2023-09-29 08:01:17.166191
ndims = 64
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.2345679012345677e-05...
Train first sample: [ 6.8153725e+00  2.8430347e+00  6.1778684e+00  4.2452879e+00
  1.6284513e+00  3.3207846e+00  7.0928354e+00  6.4340367e+00
  6.0301833e+00  6.4328456e+00  6.5495849e+00  4.9321003e+00
  8.9532204e+00  3.4481030e+00  4.8284545e+00  8.0901794e+00
  8.1549883e+00  7.3900299e+00  1.3510420e+00  9.5149851e+00
  7.5267930e+00  1.0017762e+01  1.1636026e+00  8.6080103e+00
  2.2276068e+00  6.2524571e+00  1.5594521e+00  8.2755499e+00
  8.5135918e+00  6.3978677e+00  3.8566861e+00  6.8423492e-01
  7.5210962e+00  5.4144373e+00  6.8548636e+00  8.7448330e+00
  9.6529083e+00  8.6500130e+00  9.1009712e-01  4.1672401e+00
  7.6439004e+00  9.0885794e-01  5.2691178e+00  8.8166595e-03
  7.4780917e-01 -4.8575500e-01  7.9515800e+00  2.7509699e+00
  4.8898096e+00  1.0073138e+01  7.4751706e+00  6.5633142e-01
  1.5831217e+00  6.0002384e+00  5.6322212e+00  2.2999794e+00
  9.1524439e+00  5.5740438e+00  5.8525505e+00  6.6422234e+00
  6.9292612e+00  2.1605799e+00  2.2525268e+00  1.7267085e+00]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-29 08:04:37.040 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7498.4800, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 200s - loss: nan - MinusLogProbMetric: 7498.4800 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.2346e-05 - 200s/epoch - 1s/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 4.115226337448558e-06.
===========
Generating train data for run 335.
===========
Train data generated in 0.34 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_335/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_335/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_335/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_335
self.data_kwargs: {'seed': 187}
self.x_data: [[ 6.8153725   2.8430347   6.1778684  ...  2.16058     2.2525268
   1.7267085 ]
 [ 2.1836705   1.800204    9.035551   ...  6.5503054   1.5567943
   2.9656742 ]
 [ 0.87054014  3.1855702   7.8237348  ...  6.3044343  -0.7088827
   2.3190107 ]
 ...
 [ 0.35442734  3.573855   10.922302   ...  5.8448343   0.8037275
   2.171598  ]
 [ 1.4908957   3.8775673   8.757837   ...  5.7039022  -0.88000226
   2.3087878 ]
 [ 1.7291393   3.7802503   9.770119   ...  5.5898924   0.22995539
   4.1106014 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_253"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_254 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_23 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_23/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_23'")
self.model: <keras.engine.functional.Functional object at 0x7f7739f9bf70>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f7af5902980>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f7af5902980>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f7721619960>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f77216ce7a0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_335/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f77216ced10>, <keras.callbacks.ModelCheckpoint object at 0x7f77216cedd0>, <keras.callbacks.EarlyStopping object at 0x7f77216cf040>, <keras.callbacks.ReduceLROnPlateau object at 0x7f77216cf070>, <keras.callbacks.TerminateOnNaN object at 0x7f77216cecb0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_335/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 335/720 with hyperparameters:
timestamp = 2023-09-29 08:04:50.416694
ndims = 64
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.115226337448558e-06...
Train first sample: [ 6.8153725e+00  2.8430347e+00  6.1778684e+00  4.2452879e+00
  1.6284513e+00  3.3207846e+00  7.0928354e+00  6.4340367e+00
  6.0301833e+00  6.4328456e+00  6.5495849e+00  4.9321003e+00
  8.9532204e+00  3.4481030e+00  4.8284545e+00  8.0901794e+00
  8.1549883e+00  7.3900299e+00  1.3510420e+00  9.5149851e+00
  7.5267930e+00  1.0017762e+01  1.1636026e+00  8.6080103e+00
  2.2276068e+00  6.2524571e+00  1.5594521e+00  8.2755499e+00
  8.5135918e+00  6.3978677e+00  3.8566861e+00  6.8423492e-01
  7.5210962e+00  5.4144373e+00  6.8548636e+00  8.7448330e+00
  9.6529083e+00  8.6500130e+00  9.1009712e-01  4.1672401e+00
  7.6439004e+00  9.0885794e-01  5.2691178e+00  8.8166595e-03
  7.4780917e-01 -4.8575500e-01  7.9515800e+00  2.7509699e+00
  4.8898096e+00  1.0073138e+01  7.4751706e+00  6.5633142e-01
  1.5831217e+00  6.0002384e+00  5.6322212e+00  2.2999794e+00
  9.1524439e+00  5.5740438e+00  5.8525505e+00  6.6422234e+00
  6.9292612e+00  2.1605799e+00  2.2525268e+00  1.7267085e+00]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-29 08:07:45.245 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7498.4800, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 175s - loss: nan - MinusLogProbMetric: 7498.4800 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.1152e-06 - 175s/epoch - 891ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 1.3717421124828526e-06.
===========
Generating train data for run 335.
===========
Train data generated in 0.27 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_335/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_335/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_335/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_335
self.data_kwargs: {'seed': 187}
self.x_data: [[ 6.8153725   2.8430347   6.1778684  ...  2.16058     2.2525268
   1.7267085 ]
 [ 2.1836705   1.800204    9.035551   ...  6.5503054   1.5567943
   2.9656742 ]
 [ 0.87054014  3.1855702   7.8237348  ...  6.3044343  -0.7088827
   2.3190107 ]
 ...
 [ 0.35442734  3.573855   10.922302   ...  5.8448343   0.8037275
   2.171598  ]
 [ 1.4908957   3.8775673   8.757837   ...  5.7039022  -0.88000226
   2.3087878 ]
 [ 1.7291393   3.7802503   9.770119   ...  5.5898924   0.22995539
   4.1106014 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_264"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_265 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_24 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_24/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_24'")
self.model: <keras.engine.functional.Functional object at 0x7f7c939d2e90>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f7722cc4550>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f7722cc4550>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f7c93b68670>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f7c939fd5d0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_335/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f7c939fdb40>, <keras.callbacks.ModelCheckpoint object at 0x7f7c939fdc00>, <keras.callbacks.EarlyStopping object at 0x7f7c939fde70>, <keras.callbacks.ReduceLROnPlateau object at 0x7f7c939fdea0>, <keras.callbacks.TerminateOnNaN object at 0x7f7c939fdae0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_335/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 335/720 with hyperparameters:
timestamp = 2023-09-29 08:07:59.458641
ndims = 64
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.3717421124828526e-06...
Train first sample: [ 6.8153725e+00  2.8430347e+00  6.1778684e+00  4.2452879e+00
  1.6284513e+00  3.3207846e+00  7.0928354e+00  6.4340367e+00
  6.0301833e+00  6.4328456e+00  6.5495849e+00  4.9321003e+00
  8.9532204e+00  3.4481030e+00  4.8284545e+00  8.0901794e+00
  8.1549883e+00  7.3900299e+00  1.3510420e+00  9.5149851e+00
  7.5267930e+00  1.0017762e+01  1.1636026e+00  8.6080103e+00
  2.2276068e+00  6.2524571e+00  1.5594521e+00  8.2755499e+00
  8.5135918e+00  6.3978677e+00  3.8566861e+00  6.8423492e-01
  7.5210962e+00  5.4144373e+00  6.8548636e+00  8.7448330e+00
  9.6529083e+00  8.6500130e+00  9.1009712e-01  4.1672401e+00
  7.6439004e+00  9.0885794e-01  5.2691178e+00  8.8166595e-03
  7.4780917e-01 -4.8575500e-01  7.9515800e+00  2.7509699e+00
  4.8898096e+00  1.0073138e+01  7.4751706e+00  6.5633142e-01
  1.5831217e+00  6.0002384e+00  5.6322212e+00  2.2999794e+00
  9.1524439e+00  5.5740438e+00  5.8525505e+00  6.6422234e+00
  6.9292612e+00  2.1605799e+00  2.2525268e+00  1.7267085e+00]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-29 08:11:18.053 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7498.4800, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 198s - loss: nan - MinusLogProbMetric: 7498.4800 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.3717e-06 - 198s/epoch - 1s/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 4.572473708276175e-07.
===========
Generating train data for run 335.
===========
Train data generated in 0.39 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_335/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_335/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_335/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_335
self.data_kwargs: {'seed': 187}
self.x_data: [[ 6.8153725   2.8430347   6.1778684  ...  2.16058     2.2525268
   1.7267085 ]
 [ 2.1836705   1.800204    9.035551   ...  6.5503054   1.5567943
   2.9656742 ]
 [ 0.87054014  3.1855702   7.8237348  ...  6.3044343  -0.7088827
   2.3190107 ]
 ...
 [ 0.35442734  3.573855   10.922302   ...  5.8448343   0.8037275
   2.171598  ]
 [ 1.4908957   3.8775673   8.757837   ...  5.7039022  -0.88000226
   2.3087878 ]
 [ 1.7291393   3.7802503   9.770119   ...  5.5898924   0.22995539
   4.1106014 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_275"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_276 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_25 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_25/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_25'")
self.model: <keras.engine.functional.Functional object at 0x7f7720fe7790>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f7729dbcb20>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f7729dbcb20>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f7720fa4ee0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f7720ffa7d0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_335/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f7720ffad40>, <keras.callbacks.ModelCheckpoint object at 0x7f7720ffae00>, <keras.callbacks.EarlyStopping object at 0x7f7720ffb070>, <keras.callbacks.ReduceLROnPlateau object at 0x7f7720ffb0a0>, <keras.callbacks.TerminateOnNaN object at 0x7f7720fface0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_335/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 335/720 with hyperparameters:
timestamp = 2023-09-29 08:11:31.753913
ndims = 64
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.572473708276175e-07...
Train first sample: [ 6.8153725e+00  2.8430347e+00  6.1778684e+00  4.2452879e+00
  1.6284513e+00  3.3207846e+00  7.0928354e+00  6.4340367e+00
  6.0301833e+00  6.4328456e+00  6.5495849e+00  4.9321003e+00
  8.9532204e+00  3.4481030e+00  4.8284545e+00  8.0901794e+00
  8.1549883e+00  7.3900299e+00  1.3510420e+00  9.5149851e+00
  7.5267930e+00  1.0017762e+01  1.1636026e+00  8.6080103e+00
  2.2276068e+00  6.2524571e+00  1.5594521e+00  8.2755499e+00
  8.5135918e+00  6.3978677e+00  3.8566861e+00  6.8423492e-01
  7.5210962e+00  5.4144373e+00  6.8548636e+00  8.7448330e+00
  9.6529083e+00  8.6500130e+00  9.1009712e-01  4.1672401e+00
  7.6439004e+00  9.0885794e-01  5.2691178e+00  8.8166595e-03
  7.4780917e-01 -4.8575500e-01  7.9515800e+00  2.7509699e+00
  4.8898096e+00  1.0073138e+01  7.4751706e+00  6.5633142e-01
  1.5831217e+00  6.0002384e+00  5.6322212e+00  2.2999794e+00
  9.1524439e+00  5.5740438e+00  5.8525505e+00  6.6422234e+00
  6.9292612e+00  2.1605799e+00  2.2525268e+00  1.7267085e+00]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-29 08:14:45.551 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7498.4800, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 194s - loss: nan - MinusLogProbMetric: 7498.4800 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.5725e-07 - 194s/epoch - 988ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 1.524157902758725e-07.
===========
Generating train data for run 335.
===========
Train data generated in 0.27 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_335/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_335/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_335/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_335
self.data_kwargs: {'seed': 187}
self.x_data: [[ 6.8153725   2.8430347   6.1778684  ...  2.16058     2.2525268
   1.7267085 ]
 [ 2.1836705   1.800204    9.035551   ...  6.5503054   1.5567943
   2.9656742 ]
 [ 0.87054014  3.1855702   7.8237348  ...  6.3044343  -0.7088827
   2.3190107 ]
 ...
 [ 0.35442734  3.573855   10.922302   ...  5.8448343   0.8037275
   2.171598  ]
 [ 1.4908957   3.8775673   8.757837   ...  5.7039022  -0.88000226
   2.3087878 ]
 [ 1.7291393   3.7802503   9.770119   ...  5.5898924   0.22995539
   4.1106014 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_286"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_287 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_26 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_26/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_26'")
self.model: <keras.engine.functional.Functional object at 0x7f7719741540>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f770a5ced70>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f770a5ced70>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f7721005cf0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f77210737c0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_335/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f7721073d30>, <keras.callbacks.ModelCheckpoint object at 0x7f7721073df0>, <keras.callbacks.EarlyStopping object at 0x7f7721073fa0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f7721073d00>, <keras.callbacks.TerminateOnNaN object at 0x7f7721073f40>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_335/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 335/720 with hyperparameters:
timestamp = 2023-09-29 08:14:58.152693
ndims = 64
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.524157902758725e-07...
Train first sample: [ 6.8153725e+00  2.8430347e+00  6.1778684e+00  4.2452879e+00
  1.6284513e+00  3.3207846e+00  7.0928354e+00  6.4340367e+00
  6.0301833e+00  6.4328456e+00  6.5495849e+00  4.9321003e+00
  8.9532204e+00  3.4481030e+00  4.8284545e+00  8.0901794e+00
  8.1549883e+00  7.3900299e+00  1.3510420e+00  9.5149851e+00
  7.5267930e+00  1.0017762e+01  1.1636026e+00  8.6080103e+00
  2.2276068e+00  6.2524571e+00  1.5594521e+00  8.2755499e+00
  8.5135918e+00  6.3978677e+00  3.8566861e+00  6.8423492e-01
  7.5210962e+00  5.4144373e+00  6.8548636e+00  8.7448330e+00
  9.6529083e+00  8.6500130e+00  9.1009712e-01  4.1672401e+00
  7.6439004e+00  9.0885794e-01  5.2691178e+00  8.8166595e-03
  7.4780917e-01 -4.8575500e-01  7.9515800e+00  2.7509699e+00
  4.8898096e+00  1.0073138e+01  7.4751706e+00  6.5633142e-01
  1.5831217e+00  6.0002384e+00  5.6322212e+00  2.2999794e+00
  9.1524439e+00  5.5740438e+00  5.8525505e+00  6.6422234e+00
  6.9292612e+00  2.1605799e+00  2.2525268e+00  1.7267085e+00]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-29 08:18:13.827 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7498.4800, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 195s - loss: nan - MinusLogProbMetric: 7498.4800 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.5242e-07 - 195s/epoch - 997ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 5.0805263425290834e-08.
===========
Generating train data for run 335.
===========
Train data generated in 0.33 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_335/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_335/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_335/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_335
self.data_kwargs: {'seed': 187}
self.x_data: [[ 6.8153725   2.8430347   6.1778684  ...  2.16058     2.2525268
   1.7267085 ]
 [ 2.1836705   1.800204    9.035551   ...  6.5503054   1.5567943
   2.9656742 ]
 [ 0.87054014  3.1855702   7.8237348  ...  6.3044343  -0.7088827
   2.3190107 ]
 ...
 [ 0.35442734  3.573855   10.922302   ...  5.8448343   0.8037275
   2.171598  ]
 [ 1.4908957   3.8775673   8.757837   ...  5.7039022  -0.88000226
   2.3087878 ]
 [ 1.7291393   3.7802503   9.770119   ...  5.5898924   0.22995539
   4.1106014 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_297"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_298 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_27 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_27/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_27'")
self.model: <keras.engine.functional.Functional object at 0x7f7cd28ca2f0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f775139b730>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f775139b730>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f775373dc30>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f7758d1dc90>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_335/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f7758d1d600>, <keras.callbacks.ModelCheckpoint object at 0x7f7758d1d1b0>, <keras.callbacks.EarlyStopping object at 0x7f7758d1d060>, <keras.callbacks.ReduceLROnPlateau object at 0x7f7758d1c520>, <keras.callbacks.TerminateOnNaN object at 0x7f7758d1d780>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_335/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 335/720 with hyperparameters:
timestamp = 2023-09-29 08:18:26.778466
ndims = 64
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 5.0805263425290834e-08...
Train first sample: [ 6.8153725e+00  2.8430347e+00  6.1778684e+00  4.2452879e+00
  1.6284513e+00  3.3207846e+00  7.0928354e+00  6.4340367e+00
  6.0301833e+00  6.4328456e+00  6.5495849e+00  4.9321003e+00
  8.9532204e+00  3.4481030e+00  4.8284545e+00  8.0901794e+00
  8.1549883e+00  7.3900299e+00  1.3510420e+00  9.5149851e+00
  7.5267930e+00  1.0017762e+01  1.1636026e+00  8.6080103e+00
  2.2276068e+00  6.2524571e+00  1.5594521e+00  8.2755499e+00
  8.5135918e+00  6.3978677e+00  3.8566861e+00  6.8423492e-01
  7.5210962e+00  5.4144373e+00  6.8548636e+00  8.7448330e+00
  9.6529083e+00  8.6500130e+00  9.1009712e-01  4.1672401e+00
  7.6439004e+00  9.0885794e-01  5.2691178e+00  8.8166595e-03
  7.4780917e-01 -4.8575500e-01  7.9515800e+00  2.7509699e+00
  4.8898096e+00  1.0073138e+01  7.4751706e+00  6.5633142e-01
  1.5831217e+00  6.0002384e+00  5.6322212e+00  2.2999794e+00
  9.1524439e+00  5.5740438e+00  5.8525505e+00  6.6422234e+00
  6.9292612e+00  2.1605799e+00  2.2525268e+00  1.7267085e+00]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-29 08:21:30.873 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7498.4800, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 184s - loss: nan - MinusLogProbMetric: 7498.4800 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 5.0805e-08 - 184s/epoch - 938ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 1.6935087808430278e-08.
===========
Generating train data for run 335.
===========
Train data generated in 0.24 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_335/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_335/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_335/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_335
self.data_kwargs: {'seed': 187}
self.x_data: [[ 6.8153725   2.8430347   6.1778684  ...  2.16058     2.2525268
   1.7267085 ]
 [ 2.1836705   1.800204    9.035551   ...  6.5503054   1.5567943
   2.9656742 ]
 [ 0.87054014  3.1855702   7.8237348  ...  6.3044343  -0.7088827
   2.3190107 ]
 ...
 [ 0.35442734  3.573855   10.922302   ...  5.8448343   0.8037275
   2.171598  ]
 [ 1.4908957   3.8775673   8.757837   ...  5.7039022  -0.88000226
   2.3087878 ]
 [ 1.7291393   3.7802503   9.770119   ...  5.5898924   0.22995539
   4.1106014 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_308"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_309 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_28 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_28/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_28'")
self.model: <keras.engine.functional.Functional object at 0x7f7c9338f670>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f7c92e363b0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f7c92e363b0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f7758d1caf0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f7c92adb1f0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_335/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f7c92adb760>, <keras.callbacks.ModelCheckpoint object at 0x7f7c92adb820>, <keras.callbacks.EarlyStopping object at 0x7f7c92adba90>, <keras.callbacks.ReduceLROnPlateau object at 0x7f7c92adbac0>, <keras.callbacks.TerminateOnNaN object at 0x7f7c92adb700>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_335/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 335/720 with hyperparameters:
timestamp = 2023-09-29 08:21:43.358754
ndims = 64
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.6935087808430278e-08...
Train first sample: [ 6.8153725e+00  2.8430347e+00  6.1778684e+00  4.2452879e+00
  1.6284513e+00  3.3207846e+00  7.0928354e+00  6.4340367e+00
  6.0301833e+00  6.4328456e+00  6.5495849e+00  4.9321003e+00
  8.9532204e+00  3.4481030e+00  4.8284545e+00  8.0901794e+00
  8.1549883e+00  7.3900299e+00  1.3510420e+00  9.5149851e+00
  7.5267930e+00  1.0017762e+01  1.1636026e+00  8.6080103e+00
  2.2276068e+00  6.2524571e+00  1.5594521e+00  8.2755499e+00
  8.5135918e+00  6.3978677e+00  3.8566861e+00  6.8423492e-01
  7.5210962e+00  5.4144373e+00  6.8548636e+00  8.7448330e+00
  9.6529083e+00  8.6500130e+00  9.1009712e-01  4.1672401e+00
  7.6439004e+00  9.0885794e-01  5.2691178e+00  8.8166595e-03
  7.4780917e-01 -4.8575500e-01  7.9515800e+00  2.7509699e+00
  4.8898096e+00  1.0073138e+01  7.4751706e+00  6.5633142e-01
  1.5831217e+00  6.0002384e+00  5.6322212e+00  2.2999794e+00
  9.1524439e+00  5.5740438e+00  5.8525505e+00  6.6422234e+00
  6.9292612e+00  2.1605799e+00  2.2525268e+00  1.7267085e+00]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-29 08:25:04.240 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7498.4800, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 201s - loss: nan - MinusLogProbMetric: 7498.4800 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.6935e-08 - 201s/epoch - 1s/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 5.645029269476759e-09.
===========
Run 335/720 failed.
Exception type: Exception
Exception message: Training failed for the maximum number of retry.
Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN.py , Line : 637, Func.Name : <module>, Message : hyperparams_dict, NFObject, seed_train, training_time = train_function(seeds = [seed_train, seed_test, seed_dist, seed_metrics],', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN.py , Line : 313, Func.Name : train_function, Message : raise Exception("Training failed for the maximum number of retry.")']
===========

===========
Generating train data for run 336.
===========
Train data generated in 0.25 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_336/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_336/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_336/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_336
self.data_kwargs: {'seed': 187}
self.x_data: [[ 6.8153725   2.8430347   6.1778684  ...  2.16058     2.2525268
   1.7267085 ]
 [ 2.1836705   1.800204    9.035551   ...  6.5503054   1.5567943
   2.9656742 ]
 [ 0.87054014  3.1855702   7.8237348  ...  6.3044343  -0.7088827
   2.3190107 ]
 ...
 [ 0.35442734  3.573855   10.922302   ...  5.8448343   0.8037275
   2.171598  ]
 [ 1.4908957   3.8775673   8.757837   ...  5.7039022  -0.88000226
   2.3087878 ]
 [ 1.7291393   3.7802503   9.770119   ...  5.5898924   0.22995539
   4.1106014 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_319"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_320 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_29 (LogProbL  (None,)                  4278720   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,278,720
Trainable params: 4,278,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_29/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_29'")
self.model: <keras.engine.functional.Functional object at 0x7f77420b68f0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f77420c79a0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f77420c79a0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f77389872b0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f7738ac4400>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f7738ac4970>, <keras.callbacks.ModelCheckpoint object at 0x7f7738ac4a30>, <keras.callbacks.EarlyStopping object at 0x7f7738ac4ca0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f7738ac4cd0>, <keras.callbacks.TerminateOnNaN object at 0x7f7738ac4910>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_336/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 336/720 with hyperparameters:
timestamp = 2023-09-29 08:25:17.078444
ndims = 64
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4278720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 6.8153725e+00  2.8430347e+00  6.1778684e+00  4.2452879e+00
  1.6284513e+00  3.3207846e+00  7.0928354e+00  6.4340367e+00
  6.0301833e+00  6.4328456e+00  6.5495849e+00  4.9321003e+00
  8.9532204e+00  3.4481030e+00  4.8284545e+00  8.0901794e+00
  8.1549883e+00  7.3900299e+00  1.3510420e+00  9.5149851e+00
  7.5267930e+00  1.0017762e+01  1.1636026e+00  8.6080103e+00
  2.2276068e+00  6.2524571e+00  1.5594521e+00  8.2755499e+00
  8.5135918e+00  6.3978677e+00  3.8566861e+00  6.8423492e-01
  7.5210962e+00  5.4144373e+00  6.8548636e+00  8.7448330e+00
  9.6529083e+00  8.6500130e+00  9.1009712e-01  4.1672401e+00
  7.6439004e+00  9.0885794e-01  5.2691178e+00  8.8166595e-03
  7.4780917e-01 -4.8575500e-01  7.9515800e+00  2.7509699e+00
  4.8898096e+00  1.0073138e+01  7.4751706e+00  6.5633142e-01
  1.5831217e+00  6.0002384e+00  5.6322212e+00  2.2999794e+00
  9.1524439e+00  5.5740438e+00  5.8525505e+00  6.6422234e+00
  6.9292612e+00  2.1605799e+00  2.2525268e+00  1.7267085e+00]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 6: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-29 08:28:33.811 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6190.7603, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 196s - loss: nan - MinusLogProbMetric: 6190.7603 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 196s/epoch - 1s/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 0.0003333333333333333.
===========
Generating train data for run 336.
===========
Train data generated in 0.40 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_336/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_336/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_336/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_336
self.data_kwargs: {'seed': 187}
self.x_data: [[ 6.8153725   2.8430347   6.1778684  ...  2.16058     2.2525268
   1.7267085 ]
 [ 2.1836705   1.800204    9.035551   ...  6.5503054   1.5567943
   2.9656742 ]
 [ 0.87054014  3.1855702   7.8237348  ...  6.3044343  -0.7088827
   2.3190107 ]
 ...
 [ 0.35442734  3.573855   10.922302   ...  5.8448343   0.8037275
   2.171598  ]
 [ 1.4908957   3.8775673   8.757837   ...  5.7039022  -0.88000226
   2.3087878 ]
 [ 1.7291393   3.7802503   9.770119   ...  5.5898924   0.22995539
   4.1106014 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_330"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_331 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_30 (LogProbL  (None,)                  4278720   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,278,720
Trainable params: 4,278,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_30/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_30'")
self.model: <keras.engine.functional.Functional object at 0x7f7f6c2aa3b0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f83846d6a40>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f83846d6a40>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f7eac240370>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f7d0a1a3ac0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f7d0a1a2d70>, <keras.callbacks.ModelCheckpoint object at 0x7f7d0a1a3730>, <keras.callbacks.EarlyStopping object at 0x7f7d0a1a08e0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f7d0a1a1630>, <keras.callbacks.TerminateOnNaN object at 0x7f7d0a1a3af0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_336/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 336/720 with hyperparameters:
timestamp = 2023-09-29 08:28:47.453521
ndims = 64
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4278720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 6.8153725e+00  2.8430347e+00  6.1778684e+00  4.2452879e+00
  1.6284513e+00  3.3207846e+00  7.0928354e+00  6.4340367e+00
  6.0301833e+00  6.4328456e+00  6.5495849e+00  4.9321003e+00
  8.9532204e+00  3.4481030e+00  4.8284545e+00  8.0901794e+00
  8.1549883e+00  7.3900299e+00  1.3510420e+00  9.5149851e+00
  7.5267930e+00  1.0017762e+01  1.1636026e+00  8.6080103e+00
  2.2276068e+00  6.2524571e+00  1.5594521e+00  8.2755499e+00
  8.5135918e+00  6.3978677e+00  3.8566861e+00  6.8423492e-01
  7.5210962e+00  5.4144373e+00  6.8548636e+00  8.7448330e+00
  9.6529083e+00  8.6500130e+00  9.1009712e-01  4.1672401e+00
  7.6439004e+00  9.0885794e-01  5.2691178e+00  8.8166595e-03
  7.4780917e-01 -4.8575500e-01  7.9515800e+00  2.7509699e+00
  4.8898096e+00  1.0073138e+01  7.4751706e+00  6.5633142e-01
  1.5831217e+00  6.0002384e+00  5.6322212e+00  2.2999794e+00
  9.1524439e+00  5.5740438e+00  5.8525505e+00  6.6422234e+00
  6.9292612e+00  2.1605799e+00  2.2525268e+00  1.7267085e+00]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 45: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-29 08:31:58.335 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 4621.6743, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 191s - loss: nan - MinusLogProbMetric: 4621.6743 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 191s/epoch - 973ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 0.0001111111111111111.
===========
Generating train data for run 336.
===========
Train data generated in 0.37 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_336/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_336/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_336/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_336
self.data_kwargs: {'seed': 187}
self.x_data: [[ 6.8153725   2.8430347   6.1778684  ...  2.16058     2.2525268
   1.7267085 ]
 [ 2.1836705   1.800204    9.035551   ...  6.5503054   1.5567943
   2.9656742 ]
 [ 0.87054014  3.1855702   7.8237348  ...  6.3044343  -0.7088827
   2.3190107 ]
 ...
 [ 0.35442734  3.573855   10.922302   ...  5.8448343   0.8037275
   2.171598  ]
 [ 1.4908957   3.8775673   8.757837   ...  5.7039022  -0.88000226
   2.3087878 ]
 [ 1.7291393   3.7802503   9.770119   ...  5.5898924   0.22995539
   4.1106014 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_341"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_342 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_31 (LogProbL  (None,)                  4278720   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,278,720
Trainable params: 4,278,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_31/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_31'")
self.model: <keras.engine.functional.Functional object at 0x7f835086f100>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f83501bfb50>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f83501bfb50>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f7c60e6e890>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f83500b5330>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f83500b58a0>, <keras.callbacks.ModelCheckpoint object at 0x7f83500b5960>, <keras.callbacks.EarlyStopping object at 0x7f83500b5bd0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f83500b5c00>, <keras.callbacks.TerminateOnNaN object at 0x7f83500b5840>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_336/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 336/720 with hyperparameters:
timestamp = 2023-09-29 08:32:13.352069
ndims = 64
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4278720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 6.8153725e+00  2.8430347e+00  6.1778684e+00  4.2452879e+00
  1.6284513e+00  3.3207846e+00  7.0928354e+00  6.4340367e+00
  6.0301833e+00  6.4328456e+00  6.5495849e+00  4.9321003e+00
  8.9532204e+00  3.4481030e+00  4.8284545e+00  8.0901794e+00
  8.1549883e+00  7.3900299e+00  1.3510420e+00  9.5149851e+00
  7.5267930e+00  1.0017762e+01  1.1636026e+00  8.6080103e+00
  2.2276068e+00  6.2524571e+00  1.5594521e+00  8.2755499e+00
  8.5135918e+00  6.3978677e+00  3.8566861e+00  6.8423492e-01
  7.5210962e+00  5.4144373e+00  6.8548636e+00  8.7448330e+00
  9.6529083e+00  8.6500130e+00  9.1009712e-01  4.1672401e+00
  7.6439004e+00  9.0885794e-01  5.2691178e+00  8.8166595e-03
  7.4780917e-01 -4.8575500e-01  7.9515800e+00  2.7509699e+00
  4.8898096e+00  1.0073138e+01  7.4751706e+00  6.5633142e-01
  1.5831217e+00  6.0002384e+00  5.6322212e+00  2.2999794e+00
  9.1524439e+00  5.5740438e+00  5.8525505e+00  6.6422234e+00
  6.9292612e+00  2.1605799e+00  2.2525268e+00  1.7267085e+00]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 135: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-29 08:36:27.548 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 4641.5562, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 254s - loss: nan - MinusLogProbMetric: 4641.5562 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.1111e-04 - 254s/epoch - 1s/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 3.703703703703703e-05.
===========
Generating train data for run 336.
===========
Train data generated in 0.48 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_336/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_336/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_336/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_336
self.data_kwargs: {'seed': 187}
self.x_data: [[ 6.8153725   2.8430347   6.1778684  ...  2.16058     2.2525268
   1.7267085 ]
 [ 2.1836705   1.800204    9.035551   ...  6.5503054   1.5567943
   2.9656742 ]
 [ 0.87054014  3.1855702   7.8237348  ...  6.3044343  -0.7088827
   2.3190107 ]
 ...
 [ 0.35442734  3.573855   10.922302   ...  5.8448343   0.8037275
   2.171598  ]
 [ 1.4908957   3.8775673   8.757837   ...  5.7039022  -0.88000226
   2.3087878 ]
 [ 1.7291393   3.7802503   9.770119   ...  5.5898924   0.22995539
   4.1106014 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_352"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_353 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_32 (LogProbL  (None,)                  4278720   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,278,720
Trainable params: 4,278,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_32/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_32'")
self.model: <keras.engine.functional.Functional object at 0x7f7750eeace0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f7e8853f6a0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f7e8853f6a0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f7719963610>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f7719924970>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f7719924ee0>, <keras.callbacks.ModelCheckpoint object at 0x7f7719924fa0>, <keras.callbacks.EarlyStopping object at 0x7f7719925210>, <keras.callbacks.ReduceLROnPlateau object at 0x7f7719925240>, <keras.callbacks.TerminateOnNaN object at 0x7f7719924e80>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_336/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 336/720 with hyperparameters:
timestamp = 2023-09-29 08:36:42.019828
ndims = 64
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4278720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 3.703703703703703e-05...
Train first sample: [ 6.8153725e+00  2.8430347e+00  6.1778684e+00  4.2452879e+00
  1.6284513e+00  3.3207846e+00  7.0928354e+00  6.4340367e+00
  6.0301833e+00  6.4328456e+00  6.5495849e+00  4.9321003e+00
  8.9532204e+00  3.4481030e+00  4.8284545e+00  8.0901794e+00
  8.1549883e+00  7.3900299e+00  1.3510420e+00  9.5149851e+00
  7.5267930e+00  1.0017762e+01  1.1636026e+00  8.6080103e+00
  2.2276068e+00  6.2524571e+00  1.5594521e+00  8.2755499e+00
  8.5135918e+00  6.3978677e+00  3.8566861e+00  6.8423492e-01
  7.5210962e+00  5.4144373e+00  6.8548636e+00  8.7448330e+00
  9.6529083e+00  8.6500130e+00  9.1009712e-01  4.1672401e+00
  7.6439004e+00  9.0885794e-01  5.2691178e+00  8.8166595e-03
  7.4780917e-01 -4.8575500e-01  7.9515800e+00  2.7509699e+00
  4.8898096e+00  1.0073138e+01  7.4751706e+00  6.5633142e-01
  1.5831217e+00  6.0002384e+00  5.6322212e+00  2.2999794e+00
  9.1524439e+00  5.5740438e+00  5.8525505e+00  6.6422234e+00
  6.9292612e+00  2.1605799e+00  2.2525268e+00  1.7267085e+00]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 67: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-29 08:40:23.388 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6213.6211, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 221s - loss: nan - MinusLogProbMetric: 6213.6211 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.7037e-05 - 221s/epoch - 1s/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 1.2345679012345677e-05.
===========
Generating train data for run 336.
===========
Train data generated in 0.44 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_336/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_336/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_336/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_336
self.data_kwargs: {'seed': 187}
self.x_data: [[ 6.8153725   2.8430347   6.1778684  ...  2.16058     2.2525268
   1.7267085 ]
 [ 2.1836705   1.800204    9.035551   ...  6.5503054   1.5567943
   2.9656742 ]
 [ 0.87054014  3.1855702   7.8237348  ...  6.3044343  -0.7088827
   2.3190107 ]
 ...
 [ 0.35442734  3.573855   10.922302   ...  5.8448343   0.8037275
   2.171598  ]
 [ 1.4908957   3.8775673   8.757837   ...  5.7039022  -0.88000226
   2.3087878 ]
 [ 1.7291393   3.7802503   9.770119   ...  5.5898924   0.22995539
   4.1106014 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_363"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_364 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_33 (LogProbL  (None,)                  4278720   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,278,720
Trainable params: 4,278,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_33/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_33'")
self.model: <keras.engine.functional.Functional object at 0x7f7b38539d80>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f7b38539600>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f7b38539600>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f7b385103d0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f7f6c6d9f90>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f7f6c6dacb0>, <keras.callbacks.ModelCheckpoint object at 0x7f7f6c6d8c10>, <keras.callbacks.EarlyStopping object at 0x7f7f6c6db1c0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f7f6c6d8a00>, <keras.callbacks.TerminateOnNaN object at 0x7f7f6c6dbf40>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_336/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 336/720 with hyperparameters:
timestamp = 2023-09-29 08:40:38.583145
ndims = 64
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4278720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.2345679012345677e-05...
Train first sample: [ 6.8153725e+00  2.8430347e+00  6.1778684e+00  4.2452879e+00
  1.6284513e+00  3.3207846e+00  7.0928354e+00  6.4340367e+00
  6.0301833e+00  6.4328456e+00  6.5495849e+00  4.9321003e+00
  8.9532204e+00  3.4481030e+00  4.8284545e+00  8.0901794e+00
  8.1549883e+00  7.3900299e+00  1.3510420e+00  9.5149851e+00
  7.5267930e+00  1.0017762e+01  1.1636026e+00  8.6080103e+00
  2.2276068e+00  6.2524571e+00  1.5594521e+00  8.2755499e+00
  8.5135918e+00  6.3978677e+00  3.8566861e+00  6.8423492e-01
  7.5210962e+00  5.4144373e+00  6.8548636e+00  8.7448330e+00
  9.6529083e+00  8.6500130e+00  9.1009712e-01  4.1672401e+00
  7.6439004e+00  9.0885794e-01  5.2691178e+00  8.8166595e-03
  7.4780917e-01 -4.8575500e-01  7.9515800e+00  2.7509699e+00
  4.8898096e+00  1.0073138e+01  7.4751706e+00  6.5633142e-01
  1.5831217e+00  6.0002384e+00  5.6322212e+00  2.2999794e+00
  9.1524439e+00  5.5740438e+00  5.8525505e+00  6.6422234e+00
  6.9292612e+00  2.1605799e+00  2.2525268e+00  1.7267085e+00]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 30: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-29 08:43:57.296 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6725.5649, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 199s - loss: nan - MinusLogProbMetric: 6725.5649 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.2346e-05 - 199s/epoch - 1s/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 4.115226337448558e-06.
===========
Generating train data for run 336.
===========
Train data generated in 0.30 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_336/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_336/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_336/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_336
self.data_kwargs: {'seed': 187}
self.x_data: [[ 6.8153725   2.8430347   6.1778684  ...  2.16058     2.2525268
   1.7267085 ]
 [ 2.1836705   1.800204    9.035551   ...  6.5503054   1.5567943
   2.9656742 ]
 [ 0.87054014  3.1855702   7.8237348  ...  6.3044343  -0.7088827
   2.3190107 ]
 ...
 [ 0.35442734  3.573855   10.922302   ...  5.8448343   0.8037275
   2.171598  ]
 [ 1.4908957   3.8775673   8.757837   ...  5.7039022  -0.88000226
   2.3087878 ]
 [ 1.7291393   3.7802503   9.770119   ...  5.5898924   0.22995539
   4.1106014 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_374"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_375 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_34 (LogProbL  (None,)                  4278720   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,278,720
Trainable params: 4,278,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_34/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_34'")
self.model: <keras.engine.functional.Functional object at 0x7f8316d0b2b0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f7c2bff8610>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f7c2bff8610>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f77222ddb40>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f8316b419f0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f8316b41f60>, <keras.callbacks.ModelCheckpoint object at 0x7f8316b42020>, <keras.callbacks.EarlyStopping object at 0x7f8316b42290>, <keras.callbacks.ReduceLROnPlateau object at 0x7f8316b422c0>, <keras.callbacks.TerminateOnNaN object at 0x7f8316b41f00>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_336/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 336/720 with hyperparameters:
timestamp = 2023-09-29 08:44:11.897399
ndims = 64
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4278720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.115226337448558e-06...
Train first sample: [ 6.8153725e+00  2.8430347e+00  6.1778684e+00  4.2452879e+00
  1.6284513e+00  3.3207846e+00  7.0928354e+00  6.4340367e+00
  6.0301833e+00  6.4328456e+00  6.5495849e+00  4.9321003e+00
  8.9532204e+00  3.4481030e+00  4.8284545e+00  8.0901794e+00
  8.1549883e+00  7.3900299e+00  1.3510420e+00  9.5149851e+00
  7.5267930e+00  1.0017762e+01  1.1636026e+00  8.6080103e+00
  2.2276068e+00  6.2524571e+00  1.5594521e+00  8.2755499e+00
  8.5135918e+00  6.3978677e+00  3.8566861e+00  6.8423492e-01
  7.5210962e+00  5.4144373e+00  6.8548636e+00  8.7448330e+00
  9.6529083e+00  8.6500130e+00  9.1009712e-01  4.1672401e+00
  7.6439004e+00  9.0885794e-01  5.2691178e+00  8.8166595e-03
  7.4780917e-01 -4.8575500e-01  7.9515800e+00  2.7509699e+00
  4.8898096e+00  1.0073138e+01  7.4751706e+00  6.5633142e-01
  1.5831217e+00  6.0002384e+00  5.6322212e+00  2.2999794e+00
  9.1524439e+00  5.5740438e+00  5.8525505e+00  6.6422234e+00
  6.9292612e+00  2.1605799e+00  2.2525268e+00  1.7267085e+00]
Epoch 1/1000
2023-09-29 08:48:58.249 
Epoch 1/1000 
	 loss: 6624.4102, MinusLogProbMetric: 6624.4102, val_loss: 6457.2773, val_MinusLogProbMetric: 6457.2773

Epoch 1: val_loss improved from inf to 6457.27734, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 288s - loss: 6624.4102 - MinusLogProbMetric: 6624.4102 - val_loss: 6457.2773 - val_MinusLogProbMetric: 6457.2773 - lr: 4.1152e-06 - 288s/epoch - 1s/step
Epoch 2/1000
2023-09-29 08:50:47.196 
Epoch 2/1000 
	 loss: 6361.2896, MinusLogProbMetric: 6361.2896, val_loss: 6195.4741, val_MinusLogProbMetric: 6195.4741

Epoch 2: val_loss improved from 6457.27734 to 6195.47412, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 109s - loss: 6361.2896 - MinusLogProbMetric: 6361.2896 - val_loss: 6195.4741 - val_MinusLogProbMetric: 6195.4741 - lr: 4.1152e-06 - 109s/epoch - 554ms/step
Epoch 3/1000
2023-09-29 08:52:36.695 
Epoch 3/1000 
	 loss: 6079.3115, MinusLogProbMetric: 6079.3115, val_loss: 5933.1152, val_MinusLogProbMetric: 5933.1152

Epoch 3: val_loss improved from 6195.47412 to 5933.11523, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 110s - loss: 6079.3115 - MinusLogProbMetric: 6079.3115 - val_loss: 5933.1152 - val_MinusLogProbMetric: 5933.1152 - lr: 4.1152e-06 - 110s/epoch - 560ms/step
Epoch 4/1000
2023-09-29 08:54:27.323 
Epoch 4/1000 
	 loss: 5795.9541, MinusLogProbMetric: 5795.9541, val_loss: 5679.2832, val_MinusLogProbMetric: 5679.2832

Epoch 4: val_loss improved from 5933.11523 to 5679.28320, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 110s - loss: 5795.9541 - MinusLogProbMetric: 5795.9541 - val_loss: 5679.2832 - val_MinusLogProbMetric: 5679.2832 - lr: 4.1152e-06 - 110s/epoch - 561ms/step
Epoch 5/1000
2023-09-29 08:56:18.688 
Epoch 5/1000 
	 loss: 5543.0488, MinusLogProbMetric: 5543.0488, val_loss: 5413.1118, val_MinusLogProbMetric: 5413.1118

Epoch 5: val_loss improved from 5679.28320 to 5413.11182, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 111s - loss: 5543.0488 - MinusLogProbMetric: 5543.0488 - val_loss: 5413.1118 - val_MinusLogProbMetric: 5413.1118 - lr: 4.1152e-06 - 111s/epoch - 569ms/step
Epoch 6/1000
2023-09-29 08:58:11.203 
Epoch 6/1000 
	 loss: 5318.4863, MinusLogProbMetric: 5318.4863, val_loss: 5205.8560, val_MinusLogProbMetric: 5205.8560

Epoch 6: val_loss improved from 5413.11182 to 5205.85596, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 113s - loss: 5318.4863 - MinusLogProbMetric: 5318.4863 - val_loss: 5205.8560 - val_MinusLogProbMetric: 5205.8560 - lr: 4.1152e-06 - 113s/epoch - 575ms/step
Epoch 7/1000
2023-09-29 08:59:55.456 
Epoch 7/1000 
	 loss: 5094.3960, MinusLogProbMetric: 5094.3960, val_loss: 5004.8828, val_MinusLogProbMetric: 5004.8828

Epoch 7: val_loss improved from 5205.85596 to 5004.88281, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 104s - loss: 5094.3960 - MinusLogProbMetric: 5094.3960 - val_loss: 5004.8828 - val_MinusLogProbMetric: 5004.8828 - lr: 4.1152e-06 - 104s/epoch - 530ms/step
Epoch 8/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 63: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-29 09:00:35.780 
Epoch 8/1000 
	 loss: nan, MinusLogProbMetric: 4969.5063, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 8: val_loss did not improve from 5004.88281
196/196 - 39s - loss: nan - MinusLogProbMetric: 4969.5063 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.1152e-06 - 39s/epoch - 200ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 1.3717421124828526e-06.
===========
Generating train data for run 336.
===========
Train data generated in 0.37 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_336/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_336/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_336/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_336
self.data_kwargs: {'seed': 187}
self.x_data: [[ 6.8153725   2.8430347   6.1778684  ...  2.16058     2.2525268
   1.7267085 ]
 [ 2.1836705   1.800204    9.035551   ...  6.5503054   1.5567943
   2.9656742 ]
 [ 0.87054014  3.1855702   7.8237348  ...  6.3044343  -0.7088827
   2.3190107 ]
 ...
 [ 0.35442734  3.573855   10.922302   ...  5.8448343   0.8037275
   2.171598  ]
 [ 1.4908957   3.8775673   8.757837   ...  5.7039022  -0.88000226
   2.3087878 ]
 [ 1.7291393   3.7802503   9.770119   ...  5.5898924   0.22995539
   4.1106014 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_385"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_386 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_35 (LogProbL  (None,)                  4278720   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,278,720
Trainable params: 4,278,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_35/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_35'")
self.model: <keras.engine.functional.Functional object at 0x7f76f8c2fd90>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f7709a0af50>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f7709a0af50>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f7cd351e0b0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f76f0231840>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f76f0231db0>, <keras.callbacks.ModelCheckpoint object at 0x7f76f0231e70>, <keras.callbacks.EarlyStopping object at 0x7f76f02320e0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f76f0232110>, <keras.callbacks.TerminateOnNaN object at 0x7f76f0231d50>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 336/720 with hyperparameters:
timestamp = 2023-09-29 09:00:50.218834
ndims = 64
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4278720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.3717421124828526e-06...
Train first sample: [ 6.8153725e+00  2.8430347e+00  6.1778684e+00  4.2452879e+00
  1.6284513e+00  3.3207846e+00  7.0928354e+00  6.4340367e+00
  6.0301833e+00  6.4328456e+00  6.5495849e+00  4.9321003e+00
  8.9532204e+00  3.4481030e+00  4.8284545e+00  8.0901794e+00
  8.1549883e+00  7.3900299e+00  1.3510420e+00  9.5149851e+00
  7.5267930e+00  1.0017762e+01  1.1636026e+00  8.6080103e+00
  2.2276068e+00  6.2524571e+00  1.5594521e+00  8.2755499e+00
  8.5135918e+00  6.3978677e+00  3.8566861e+00  6.8423492e-01
  7.5210962e+00  5.4144373e+00  6.8548636e+00  8.7448330e+00
  9.6529083e+00  8.6500130e+00  9.1009712e-01  4.1672401e+00
  7.6439004e+00  9.0885794e-01  5.2691178e+00  8.8166595e-03
  7.4780917e-01 -4.8575500e-01  7.9515800e+00  2.7509699e+00
  4.8898096e+00  1.0073138e+01  7.4751706e+00  6.5633142e-01
  1.5831217e+00  6.0002384e+00  5.6322212e+00  2.2999794e+00
  9.1524439e+00  5.5740438e+00  5.8525505e+00  6.6422234e+00
  6.9292612e+00  2.1605799e+00  2.2525268e+00  1.7267085e+00]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 168: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-29 09:05:01.796 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 4922.6890, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 251s - loss: nan - MinusLogProbMetric: 4922.6890 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.3717e-06 - 251s/epoch - 1s/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 4.572473708276175e-07.
===========
Generating train data for run 336.
===========
Train data generated in 0.39 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_336/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_336/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_336/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_336
self.data_kwargs: {'seed': 187}
self.x_data: [[ 6.8153725   2.8430347   6.1778684  ...  2.16058     2.2525268
   1.7267085 ]
 [ 2.1836705   1.800204    9.035551   ...  6.5503054   1.5567943
   2.9656742 ]
 [ 0.87054014  3.1855702   7.8237348  ...  6.3044343  -0.7088827
   2.3190107 ]
 ...
 [ 0.35442734  3.573855   10.922302   ...  5.8448343   0.8037275
   2.171598  ]
 [ 1.4908957   3.8775673   8.757837   ...  5.7039022  -0.88000226
   2.3087878 ]
 [ 1.7291393   3.7802503   9.770119   ...  5.5898924   0.22995539
   4.1106014 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_396"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_397 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_36 (LogProbL  (None,)                  4278720   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,278,720
Trainable params: 4,278,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_36/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_36'")
self.model: <keras.engine.functional.Functional object at 0x7f7720c8f940>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f837369ba90>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f837369ba90>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f76f03ae050>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f7720ca5c60>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f7720ca61d0>, <keras.callbacks.ModelCheckpoint object at 0x7f7720ca6290>, <keras.callbacks.EarlyStopping object at 0x7f7720ca6500>, <keras.callbacks.ReduceLROnPlateau object at 0x7f7720ca6530>, <keras.callbacks.TerminateOnNaN object at 0x7f7720ca6170>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 336/720 with hyperparameters:
timestamp = 2023-09-29 09:05:18.381398
ndims = 64
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4278720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.572473708276175e-07...
Train first sample: [ 6.8153725e+00  2.8430347e+00  6.1778684e+00  4.2452879e+00
  1.6284513e+00  3.3207846e+00  7.0928354e+00  6.4340367e+00
  6.0301833e+00  6.4328456e+00  6.5495849e+00  4.9321003e+00
  8.9532204e+00  3.4481030e+00  4.8284545e+00  8.0901794e+00
  8.1549883e+00  7.3900299e+00  1.3510420e+00  9.5149851e+00
  7.5267930e+00  1.0017762e+01  1.1636026e+00  8.6080103e+00
  2.2276068e+00  6.2524571e+00  1.5594521e+00  8.2755499e+00
  8.5135918e+00  6.3978677e+00  3.8566861e+00  6.8423492e-01
  7.5210962e+00  5.4144373e+00  6.8548636e+00  8.7448330e+00
  9.6529083e+00  8.6500130e+00  9.1009712e-01  4.1672401e+00
  7.6439004e+00  9.0885794e-01  5.2691178e+00  8.8166595e-03
  7.4780917e-01 -4.8575500e-01  7.9515800e+00  2.7509699e+00
  4.8898096e+00  1.0073138e+01  7.4751706e+00  6.5633142e-01
  1.5831217e+00  6.0002384e+00  5.6322212e+00  2.2999794e+00
  9.1524439e+00  5.5740438e+00  5.8525505e+00  6.6422234e+00
  6.9292612e+00  2.1605799e+00  2.2525268e+00  1.7267085e+00]
Epoch 1/1000
2023-09-29 09:10:01.280 
Epoch 1/1000 
	 loss: 4962.5522, MinusLogProbMetric: 4962.5522, val_loss: 4931.1577, val_MinusLogProbMetric: 4931.1577

Epoch 1: val_loss improved from inf to 4931.15771, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 284s - loss: 4962.5522 - MinusLogProbMetric: 4962.5522 - val_loss: 4931.1577 - val_MinusLogProbMetric: 4931.1577 - lr: 4.5725e-07 - 284s/epoch - 1s/step
Epoch 2/1000
2023-09-29 09:11:47.682 
Epoch 2/1000 
	 loss: 4910.8589, MinusLogProbMetric: 4910.8589, val_loss: 4896.1641, val_MinusLogProbMetric: 4896.1641

Epoch 2: val_loss improved from 4931.15771 to 4896.16406, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 106s - loss: 4910.8589 - MinusLogProbMetric: 4910.8589 - val_loss: 4896.1641 - val_MinusLogProbMetric: 4896.1641 - lr: 4.5725e-07 - 106s/epoch - 542ms/step
Epoch 3/1000
2023-09-29 09:13:29.866 
Epoch 3/1000 
	 loss: 4880.4004, MinusLogProbMetric: 4880.4004, val_loss: 4864.3184, val_MinusLogProbMetric: 4864.3184

Epoch 3: val_loss improved from 4896.16406 to 4864.31836, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 102s - loss: 4880.4004 - MinusLogProbMetric: 4880.4004 - val_loss: 4864.3184 - val_MinusLogProbMetric: 4864.3184 - lr: 4.5725e-07 - 102s/epoch - 520ms/step
Epoch 4/1000
2023-09-29 09:15:15.847 
Epoch 4/1000 
	 loss: 4844.3521, MinusLogProbMetric: 4844.3521, val_loss: 4842.2061, val_MinusLogProbMetric: 4842.2061

Epoch 4: val_loss improved from 4864.31836 to 4842.20605, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 106s - loss: 4844.3521 - MinusLogProbMetric: 4844.3521 - val_loss: 4842.2061 - val_MinusLogProbMetric: 4842.2061 - lr: 4.5725e-07 - 106s/epoch - 541ms/step
Epoch 5/1000
2023-09-29 09:17:01.245 
Epoch 5/1000 
	 loss: 4819.9214, MinusLogProbMetric: 4819.9214, val_loss: 4802.5391, val_MinusLogProbMetric: 4802.5391

Epoch 5: val_loss improved from 4842.20605 to 4802.53906, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 106s - loss: 4819.9214 - MinusLogProbMetric: 4819.9214 - val_loss: 4802.5391 - val_MinusLogProbMetric: 4802.5391 - lr: 4.5725e-07 - 106s/epoch - 539ms/step
Epoch 6/1000
2023-09-29 09:18:47.967 
Epoch 6/1000 
	 loss: 4784.3423, MinusLogProbMetric: 4784.3423, val_loss: 4770.3311, val_MinusLogProbMetric: 4770.3311

Epoch 6: val_loss improved from 4802.53906 to 4770.33105, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 107s - loss: 4784.3423 - MinusLogProbMetric: 4784.3423 - val_loss: 4770.3311 - val_MinusLogProbMetric: 4770.3311 - lr: 4.5725e-07 - 107s/epoch - 544ms/step
Epoch 7/1000
2023-09-29 09:20:36.166 
Epoch 7/1000 
	 loss: 4749.5225, MinusLogProbMetric: 4749.5225, val_loss: 4737.9629, val_MinusLogProbMetric: 4737.9629

Epoch 7: val_loss improved from 4770.33105 to 4737.96289, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 108s - loss: 4749.5225 - MinusLogProbMetric: 4749.5225 - val_loss: 4737.9629 - val_MinusLogProbMetric: 4737.9629 - lr: 4.5725e-07 - 108s/epoch - 553ms/step
Epoch 8/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 48: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-29 09:21:10.667 
Epoch 8/1000 
	 loss: nan, MinusLogProbMetric: 4731.5386, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 8: val_loss did not improve from 4737.96289
196/196 - 33s - loss: nan - MinusLogProbMetric: 4731.5386 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.5725e-07 - 33s/epoch - 167ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 1.524157902758725e-07.
===========
Generating train data for run 336.
===========
Train data generated in 0.39 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_336/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_336/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_336/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_336
self.data_kwargs: {'seed': 187}
self.x_data: [[ 6.8153725   2.8430347   6.1778684  ...  2.16058     2.2525268
   1.7267085 ]
 [ 2.1836705   1.800204    9.035551   ...  6.5503054   1.5567943
   2.9656742 ]
 [ 0.87054014  3.1855702   7.8237348  ...  6.3044343  -0.7088827
   2.3190107 ]
 ...
 [ 0.35442734  3.573855   10.922302   ...  5.8448343   0.8037275
   2.171598  ]
 [ 1.4908957   3.8775673   8.757837   ...  5.7039022  -0.88000226
   2.3087878 ]
 [ 1.7291393   3.7802503   9.770119   ...  5.5898924   0.22995539
   4.1106014 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_407"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_408 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_37 (LogProbL  (None,)                  4278720   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,278,720
Trainable params: 4,278,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_37/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_37'")
self.model: <keras.engine.functional.Functional object at 0x7f83733626b0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f77235009a0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f77235009a0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f7750fec580>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f83733c4df0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f83733c5360>, <keras.callbacks.ModelCheckpoint object at 0x7f83733c5420>, <keras.callbacks.EarlyStopping object at 0x7f83733c5690>, <keras.callbacks.ReduceLROnPlateau object at 0x7f83733c56c0>, <keras.callbacks.TerminateOnNaN object at 0x7f83733c5300>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 336/720 with hyperparameters:
timestamp = 2023-09-29 09:21:28.643002
ndims = 64
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4278720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.524157902758725e-07...
Train first sample: [ 6.8153725e+00  2.8430347e+00  6.1778684e+00  4.2452879e+00
  1.6284513e+00  3.3207846e+00  7.0928354e+00  6.4340367e+00
  6.0301833e+00  6.4328456e+00  6.5495849e+00  4.9321003e+00
  8.9532204e+00  3.4481030e+00  4.8284545e+00  8.0901794e+00
  8.1549883e+00  7.3900299e+00  1.3510420e+00  9.5149851e+00
  7.5267930e+00  1.0017762e+01  1.1636026e+00  8.6080103e+00
  2.2276068e+00  6.2524571e+00  1.5594521e+00  8.2755499e+00
  8.5135918e+00  6.3978677e+00  3.8566861e+00  6.8423492e-01
  7.5210962e+00  5.4144373e+00  6.8548636e+00  8.7448330e+00
  9.6529083e+00  8.6500130e+00  9.1009712e-01  4.1672401e+00
  7.6439004e+00  9.0885794e-01  5.2691178e+00  8.8166595e-03
  7.4780917e-01 -4.8575500e-01  7.9515800e+00  2.7509699e+00
  4.8898096e+00  1.0073138e+01  7.4751706e+00  6.5633142e-01
  1.5831217e+00  6.0002384e+00  5.6322212e+00  2.2999794e+00
  9.1524439e+00  5.5740438e+00  5.8525505e+00  6.6422234e+00
  6.9292612e+00  2.1605799e+00  2.2525268e+00  1.7267085e+00]
Epoch 1/1000
2023-09-29 09:26:06.452 
Epoch 1/1000 
	 loss: 4724.1699, MinusLogProbMetric: 4724.1699, val_loss: 4718.1499, val_MinusLogProbMetric: 4718.1499

Epoch 1: val_loss improved from inf to 4718.14990, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 279s - loss: 4724.1699 - MinusLogProbMetric: 4724.1699 - val_loss: 4718.1499 - val_MinusLogProbMetric: 4718.1499 - lr: 1.5242e-07 - 279s/epoch - 1s/step
Epoch 2/1000
2023-09-29 09:27:54.877 
Epoch 2/1000 
	 loss: 4706.9185, MinusLogProbMetric: 4706.9185, val_loss: 4702.4702, val_MinusLogProbMetric: 4702.4702

Epoch 2: val_loss improved from 4718.14990 to 4702.47021, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 108s - loss: 4706.9185 - MinusLogProbMetric: 4706.9185 - val_loss: 4702.4702 - val_MinusLogProbMetric: 4702.4702 - lr: 1.5242e-07 - 108s/epoch - 550ms/step
Epoch 3/1000
2023-09-29 09:29:43.479 
Epoch 3/1000 
	 loss: 4689.8267, MinusLogProbMetric: 4689.8267, val_loss: 4686.8940, val_MinusLogProbMetric: 4686.8940

Epoch 3: val_loss improved from 4702.47021 to 4686.89404, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 109s - loss: 4689.8267 - MinusLogProbMetric: 4689.8267 - val_loss: 4686.8940 - val_MinusLogProbMetric: 4686.8940 - lr: 1.5242e-07 - 109s/epoch - 555ms/step
Epoch 4/1000
2023-09-29 09:31:30.800 
Epoch 4/1000 
	 loss: 4677.8955, MinusLogProbMetric: 4677.8955, val_loss: 4679.2319, val_MinusLogProbMetric: 4679.2319

Epoch 4: val_loss improved from 4686.89404 to 4679.23193, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 107s - loss: 4677.8955 - MinusLogProbMetric: 4677.8955 - val_loss: 4679.2319 - val_MinusLogProbMetric: 4679.2319 - lr: 1.5242e-07 - 107s/epoch - 545ms/step
Epoch 5/1000
2023-09-29 09:33:14.442 
Epoch 5/1000 
	 loss: 4667.9507, MinusLogProbMetric: 4667.9507, val_loss: 4666.2095, val_MinusLogProbMetric: 4666.2095

Epoch 5: val_loss improved from 4679.23193 to 4666.20947, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 104s - loss: 4667.9507 - MinusLogProbMetric: 4667.9507 - val_loss: 4666.2095 - val_MinusLogProbMetric: 4666.2095 - lr: 1.5242e-07 - 104s/epoch - 529ms/step
Epoch 6/1000
2023-09-29 09:34:57.094 
Epoch 6/1000 
	 loss: 4656.7031, MinusLogProbMetric: 4656.7031, val_loss: 4656.0122, val_MinusLogProbMetric: 4656.0122

Epoch 6: val_loss improved from 4666.20947 to 4656.01221, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 103s - loss: 4656.7031 - MinusLogProbMetric: 4656.7031 - val_loss: 4656.0122 - val_MinusLogProbMetric: 4656.0122 - lr: 1.5242e-07 - 103s/epoch - 525ms/step
Epoch 7/1000
2023-09-29 09:36:41.884 
Epoch 7/1000 
	 loss: 4645.7378, MinusLogProbMetric: 4645.7378, val_loss: 4642.1323, val_MinusLogProbMetric: 4642.1323

Epoch 7: val_loss improved from 4656.01221 to 4642.13232, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 105s - loss: 4645.7378 - MinusLogProbMetric: 4645.7378 - val_loss: 4642.1323 - val_MinusLogProbMetric: 4642.1323 - lr: 1.5242e-07 - 105s/epoch - 535ms/step
Epoch 8/1000
2023-09-29 09:38:28.048 
Epoch 8/1000 
	 loss: 4633.9390, MinusLogProbMetric: 4633.9390, val_loss: 4630.9644, val_MinusLogProbMetric: 4630.9644

Epoch 8: val_loss improved from 4642.13232 to 4630.96436, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 106s - loss: 4633.9390 - MinusLogProbMetric: 4633.9390 - val_loss: 4630.9644 - val_MinusLogProbMetric: 4630.9644 - lr: 1.5242e-07 - 106s/epoch - 541ms/step
Epoch 9/1000
2023-09-29 09:40:15.292 
Epoch 9/1000 
	 loss: 4622.2168, MinusLogProbMetric: 4622.2168, val_loss: 4622.4780, val_MinusLogProbMetric: 4622.4780

Epoch 9: val_loss improved from 4630.96436 to 4622.47803, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 107s - loss: 4622.2168 - MinusLogProbMetric: 4622.2168 - val_loss: 4622.4780 - val_MinusLogProbMetric: 4622.4780 - lr: 1.5242e-07 - 107s/epoch - 547ms/step
Epoch 10/1000
2023-09-29 09:42:03.115 
Epoch 10/1000 
	 loss: 4614.4697, MinusLogProbMetric: 4614.4697, val_loss: 4614.3140, val_MinusLogProbMetric: 4614.3140

Epoch 10: val_loss improved from 4622.47803 to 4614.31396, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 108s - loss: 4614.4697 - MinusLogProbMetric: 4614.4697 - val_loss: 4614.3140 - val_MinusLogProbMetric: 4614.3140 - lr: 1.5242e-07 - 108s/epoch - 550ms/step
Epoch 11/1000
2023-09-29 09:43:45.711 
Epoch 11/1000 
	 loss: 4604.9795, MinusLogProbMetric: 4604.9795, val_loss: 4604.6128, val_MinusLogProbMetric: 4604.6128

Epoch 11: val_loss improved from 4614.31396 to 4604.61279, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 102s - loss: 4604.9795 - MinusLogProbMetric: 4604.9795 - val_loss: 4604.6128 - val_MinusLogProbMetric: 4604.6128 - lr: 1.5242e-07 - 102s/epoch - 522ms/step
Epoch 12/1000
2023-09-29 09:45:30.718 
Epoch 12/1000 
	 loss: 4597.5815, MinusLogProbMetric: 4597.5815, val_loss: 4598.0991, val_MinusLogProbMetric: 4598.0991

Epoch 12: val_loss improved from 4604.61279 to 4598.09912, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 106s - loss: 4597.5815 - MinusLogProbMetric: 4597.5815 - val_loss: 4598.0991 - val_MinusLogProbMetric: 4598.0991 - lr: 1.5242e-07 - 106s/epoch - 539ms/step
Epoch 13/1000
2023-09-29 09:47:17.483 
Epoch 13/1000 
	 loss: 4590.9556, MinusLogProbMetric: 4590.9556, val_loss: 4590.8047, val_MinusLogProbMetric: 4590.8047

Epoch 13: val_loss improved from 4598.09912 to 4590.80469, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 107s - loss: 4590.9556 - MinusLogProbMetric: 4590.9556 - val_loss: 4590.8047 - val_MinusLogProbMetric: 4590.8047 - lr: 1.5242e-07 - 107s/epoch - 544ms/step
Epoch 14/1000
2023-09-29 09:49:04.749 
Epoch 14/1000 
	 loss: 4583.7710, MinusLogProbMetric: 4583.7710, val_loss: 4580.3716, val_MinusLogProbMetric: 4580.3716

Epoch 14: val_loss improved from 4590.80469 to 4580.37158, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 107s - loss: 4583.7710 - MinusLogProbMetric: 4583.7710 - val_loss: 4580.3716 - val_MinusLogProbMetric: 4580.3716 - lr: 1.5242e-07 - 107s/epoch - 547ms/step
Epoch 15/1000
2023-09-29 09:50:51.811 
Epoch 15/1000 
	 loss: 4573.6938, MinusLogProbMetric: 4573.6938, val_loss: 4572.1567, val_MinusLogProbMetric: 4572.1567

Epoch 15: val_loss improved from 4580.37158 to 4572.15674, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 107s - loss: 4573.6938 - MinusLogProbMetric: 4573.6938 - val_loss: 4572.1567 - val_MinusLogProbMetric: 4572.1567 - lr: 1.5242e-07 - 107s/epoch - 545ms/step
Epoch 16/1000
2023-09-29 09:52:39.911 
Epoch 16/1000 
	 loss: 4566.2158, MinusLogProbMetric: 4566.2158, val_loss: 4562.4067, val_MinusLogProbMetric: 4562.4067

Epoch 16: val_loss improved from 4572.15674 to 4562.40674, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 109s - loss: 4566.2158 - MinusLogProbMetric: 4566.2158 - val_loss: 4562.4067 - val_MinusLogProbMetric: 4562.4067 - lr: 1.5242e-07 - 109s/epoch - 554ms/step
Epoch 17/1000
2023-09-29 09:54:21.142 
Epoch 17/1000 
	 loss: 4556.1128, MinusLogProbMetric: 4556.1128, val_loss: 4553.1714, val_MinusLogProbMetric: 4553.1714

Epoch 17: val_loss improved from 4562.40674 to 4553.17139, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 101s - loss: 4556.1128 - MinusLogProbMetric: 4556.1128 - val_loss: 4553.1714 - val_MinusLogProbMetric: 4553.1714 - lr: 1.5242e-07 - 101s/epoch - 513ms/step
Epoch 18/1000
2023-09-29 09:56:04.130 
Epoch 18/1000 
	 loss: 4547.5552, MinusLogProbMetric: 4547.5552, val_loss: 4545.8247, val_MinusLogProbMetric: 4545.8247

Epoch 18: val_loss improved from 4553.17139 to 4545.82471, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 103s - loss: 4547.5552 - MinusLogProbMetric: 4547.5552 - val_loss: 4545.8247 - val_MinusLogProbMetric: 4545.8247 - lr: 1.5242e-07 - 103s/epoch - 526ms/step
Epoch 19/1000
2023-09-29 09:57:44.683 
Epoch 19/1000 
	 loss: 4539.7192, MinusLogProbMetric: 4539.7192, val_loss: 4540.2319, val_MinusLogProbMetric: 4540.2319

Epoch 19: val_loss improved from 4545.82471 to 4540.23193, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 101s - loss: 4539.7192 - MinusLogProbMetric: 4539.7192 - val_loss: 4540.2319 - val_MinusLogProbMetric: 4540.2319 - lr: 1.5242e-07 - 101s/epoch - 515ms/step
Epoch 20/1000
2023-09-29 09:59:31.691 
Epoch 20/1000 
	 loss: 4534.3711, MinusLogProbMetric: 4534.3711, val_loss: 4532.5820, val_MinusLogProbMetric: 4532.5820

Epoch 20: val_loss improved from 4540.23193 to 4532.58203, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 107s - loss: 4534.3711 - MinusLogProbMetric: 4534.3711 - val_loss: 4532.5820 - val_MinusLogProbMetric: 4532.5820 - lr: 1.5242e-07 - 107s/epoch - 546ms/step
Epoch 21/1000
2023-09-29 10:01:20.795 
Epoch 21/1000 
	 loss: 4524.8120, MinusLogProbMetric: 4524.8120, val_loss: 4523.2974, val_MinusLogProbMetric: 4523.2974

Epoch 21: val_loss improved from 4532.58203 to 4523.29736, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 109s - loss: 4524.8120 - MinusLogProbMetric: 4524.8120 - val_loss: 4523.2974 - val_MinusLogProbMetric: 4523.2974 - lr: 1.5242e-07 - 109s/epoch - 555ms/step
Epoch 22/1000
2023-09-29 10:03:05.441 
Epoch 22/1000 
	 loss: 4518.0649, MinusLogProbMetric: 4518.0649, val_loss: 4519.5391, val_MinusLogProbMetric: 4519.5391

Epoch 22: val_loss improved from 4523.29736 to 4519.53906, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 105s - loss: 4518.0649 - MinusLogProbMetric: 4518.0649 - val_loss: 4519.5391 - val_MinusLogProbMetric: 4519.5391 - lr: 1.5242e-07 - 105s/epoch - 536ms/step
Epoch 23/1000
2023-09-29 10:04:47.759 
Epoch 23/1000 
	 loss: 4515.0962, MinusLogProbMetric: 4515.0962, val_loss: 4510.4785, val_MinusLogProbMetric: 4510.4785

Epoch 23: val_loss improved from 4519.53906 to 4510.47852, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 102s - loss: 4515.0962 - MinusLogProbMetric: 4515.0962 - val_loss: 4510.4785 - val_MinusLogProbMetric: 4510.4785 - lr: 1.5242e-07 - 102s/epoch - 521ms/step
Epoch 24/1000
2023-09-29 10:06:33.685 
Epoch 24/1000 
	 loss: 4508.2954, MinusLogProbMetric: 4508.2954, val_loss: 4506.1230, val_MinusLogProbMetric: 4506.1230

Epoch 24: val_loss improved from 4510.47852 to 4506.12305, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 106s - loss: 4508.2954 - MinusLogProbMetric: 4508.2954 - val_loss: 4506.1230 - val_MinusLogProbMetric: 4506.1230 - lr: 1.5242e-07 - 106s/epoch - 539ms/step
Epoch 25/1000
2023-09-29 10:08:15.160 
Epoch 25/1000 
	 loss: 4502.3569, MinusLogProbMetric: 4502.3569, val_loss: 4500.7490, val_MinusLogProbMetric: 4500.7490

Epoch 25: val_loss improved from 4506.12305 to 4500.74902, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 102s - loss: 4502.3569 - MinusLogProbMetric: 4502.3569 - val_loss: 4500.7490 - val_MinusLogProbMetric: 4500.7490 - lr: 1.5242e-07 - 102s/epoch - 518ms/step
Epoch 26/1000
2023-09-29 10:10:00.531 
Epoch 26/1000 
	 loss: 4496.6963, MinusLogProbMetric: 4496.6963, val_loss: 4495.5435, val_MinusLogProbMetric: 4495.5435

Epoch 26: val_loss improved from 4500.74902 to 4495.54346, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 106s - loss: 4496.6963 - MinusLogProbMetric: 4496.6963 - val_loss: 4495.5435 - val_MinusLogProbMetric: 4495.5435 - lr: 1.5242e-07 - 106s/epoch - 538ms/step
Epoch 27/1000
2023-09-29 10:11:51.290 
Epoch 27/1000 
	 loss: 4491.6274, MinusLogProbMetric: 4491.6274, val_loss: 4490.9502, val_MinusLogProbMetric: 4490.9502

Epoch 27: val_loss improved from 4495.54346 to 4490.95020, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 111s - loss: 4491.6274 - MinusLogProbMetric: 4491.6274 - val_loss: 4490.9502 - val_MinusLogProbMetric: 4490.9502 - lr: 1.5242e-07 - 111s/epoch - 565ms/step
Epoch 28/1000
2023-09-29 10:13:40.315 
Epoch 28/1000 
	 loss: 4485.1191, MinusLogProbMetric: 4485.1191, val_loss: 4483.1675, val_MinusLogProbMetric: 4483.1675

Epoch 28: val_loss improved from 4490.95020 to 4483.16748, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 109s - loss: 4485.1191 - MinusLogProbMetric: 4485.1191 - val_loss: 4483.1675 - val_MinusLogProbMetric: 4483.1675 - lr: 1.5242e-07 - 109s/epoch - 556ms/step
Epoch 29/1000
2023-09-29 10:15:25.830 
Epoch 29/1000 
	 loss: 4479.0630, MinusLogProbMetric: 4479.0630, val_loss: 4479.0405, val_MinusLogProbMetric: 4479.0405

Epoch 29: val_loss improved from 4483.16748 to 4479.04053, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 105s - loss: 4479.0630 - MinusLogProbMetric: 4479.0630 - val_loss: 4479.0405 - val_MinusLogProbMetric: 4479.0405 - lr: 1.5242e-07 - 105s/epoch - 537ms/step
Epoch 30/1000
2023-09-29 10:17:06.037 
Epoch 30/1000 
	 loss: 4475.4849, MinusLogProbMetric: 4475.4849, val_loss: 4474.0742, val_MinusLogProbMetric: 4474.0742

Epoch 30: val_loss improved from 4479.04053 to 4474.07422, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 100s - loss: 4475.4849 - MinusLogProbMetric: 4475.4849 - val_loss: 4474.0742 - val_MinusLogProbMetric: 4474.0742 - lr: 1.5242e-07 - 100s/epoch - 512ms/step
Epoch 31/1000
2023-09-29 10:18:46.235 
Epoch 31/1000 
	 loss: 4469.1201, MinusLogProbMetric: 4469.1201, val_loss: 4467.8369, val_MinusLogProbMetric: 4467.8369

Epoch 31: val_loss improved from 4474.07422 to 4467.83691, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 100s - loss: 4469.1201 - MinusLogProbMetric: 4469.1201 - val_loss: 4467.8369 - val_MinusLogProbMetric: 4467.8369 - lr: 1.5242e-07 - 100s/epoch - 513ms/step
Epoch 32/1000
2023-09-29 10:20:30.048 
Epoch 32/1000 
	 loss: 4466.2964, MinusLogProbMetric: 4466.2964, val_loss: 4464.6523, val_MinusLogProbMetric: 4464.6523

Epoch 32: val_loss improved from 4467.83691 to 4464.65234, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 103s - loss: 4466.2964 - MinusLogProbMetric: 4466.2964 - val_loss: 4464.6523 - val_MinusLogProbMetric: 4464.6523 - lr: 1.5242e-07 - 103s/epoch - 528ms/step
Epoch 33/1000
2023-09-29 10:22:14.840 
Epoch 33/1000 
	 loss: 4459.9111, MinusLogProbMetric: 4459.9111, val_loss: 4459.3984, val_MinusLogProbMetric: 4459.3984

Epoch 33: val_loss improved from 4464.65234 to 4459.39844, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 105s - loss: 4459.9111 - MinusLogProbMetric: 4459.9111 - val_loss: 4459.3984 - val_MinusLogProbMetric: 4459.3984 - lr: 1.5242e-07 - 105s/epoch - 535ms/step
Epoch 34/1000
2023-09-29 10:23:59.900 
Epoch 34/1000 
	 loss: 4454.5830, MinusLogProbMetric: 4454.5830, val_loss: 4454.1929, val_MinusLogProbMetric: 4454.1929

Epoch 34: val_loss improved from 4459.39844 to 4454.19287, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 105s - loss: 4454.5830 - MinusLogProbMetric: 4454.5830 - val_loss: 4454.1929 - val_MinusLogProbMetric: 4454.1929 - lr: 1.5242e-07 - 105s/epoch - 536ms/step
Epoch 35/1000
2023-09-29 10:25:45.454 
Epoch 35/1000 
	 loss: 4448.8037, MinusLogProbMetric: 4448.8037, val_loss: 4450.4258, val_MinusLogProbMetric: 4450.4258

Epoch 35: val_loss improved from 4454.19287 to 4450.42578, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 105s - loss: 4448.8037 - MinusLogProbMetric: 4448.8037 - val_loss: 4450.4258 - val_MinusLogProbMetric: 4450.4258 - lr: 1.5242e-07 - 105s/epoch - 538ms/step
Epoch 36/1000
2023-09-29 10:27:34.769 
Epoch 36/1000 
	 loss: 4445.8760, MinusLogProbMetric: 4445.8760, val_loss: 4443.9585, val_MinusLogProbMetric: 4443.9585

Epoch 36: val_loss improved from 4450.42578 to 4443.95850, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 109s - loss: 4445.8760 - MinusLogProbMetric: 4445.8760 - val_loss: 4443.9585 - val_MinusLogProbMetric: 4443.9585 - lr: 1.5242e-07 - 109s/epoch - 557ms/step
Epoch 37/1000
2023-09-29 10:29:17.029 
Epoch 37/1000 
	 loss: 4440.6553, MinusLogProbMetric: 4440.6553, val_loss: 4439.7437, val_MinusLogProbMetric: 4439.7437

Epoch 37: val_loss improved from 4443.95850 to 4439.74365, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 102s - loss: 4440.6553 - MinusLogProbMetric: 4440.6553 - val_loss: 4439.7437 - val_MinusLogProbMetric: 4439.7437 - lr: 1.5242e-07 - 102s/epoch - 522ms/step
Epoch 38/1000
2023-09-29 10:31:03.508 
Epoch 38/1000 
	 loss: 4436.7202, MinusLogProbMetric: 4436.7202, val_loss: 4435.5249, val_MinusLogProbMetric: 4435.5249

Epoch 38: val_loss improved from 4439.74365 to 4435.52490, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 107s - loss: 4436.7202 - MinusLogProbMetric: 4436.7202 - val_loss: 4435.5249 - val_MinusLogProbMetric: 4435.5249 - lr: 1.5242e-07 - 107s/epoch - 544ms/step
Epoch 39/1000
2023-09-29 10:32:45.187 
Epoch 39/1000 
	 loss: 4431.6914, MinusLogProbMetric: 4431.6914, val_loss: 4431.0864, val_MinusLogProbMetric: 4431.0864

Epoch 39: val_loss improved from 4435.52490 to 4431.08643, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 102s - loss: 4431.6914 - MinusLogProbMetric: 4431.6914 - val_loss: 4431.0864 - val_MinusLogProbMetric: 4431.0864 - lr: 1.5242e-07 - 102s/epoch - 520ms/step
Epoch 40/1000
2023-09-29 10:34:30.700 
Epoch 40/1000 
	 loss: 4426.2676, MinusLogProbMetric: 4426.2676, val_loss: 4425.2544, val_MinusLogProbMetric: 4425.2544

Epoch 40: val_loss improved from 4431.08643 to 4425.25439, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 105s - loss: 4426.2676 - MinusLogProbMetric: 4426.2676 - val_loss: 4425.2544 - val_MinusLogProbMetric: 4425.2544 - lr: 1.5242e-07 - 105s/epoch - 537ms/step
Epoch 41/1000
2023-09-29 10:36:14.823 
Epoch 41/1000 
	 loss: 4421.7500, MinusLogProbMetric: 4421.7500, val_loss: 4421.8193, val_MinusLogProbMetric: 4421.8193

Epoch 41: val_loss improved from 4425.25439 to 4421.81934, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 104s - loss: 4421.7500 - MinusLogProbMetric: 4421.7500 - val_loss: 4421.8193 - val_MinusLogProbMetric: 4421.8193 - lr: 1.5242e-07 - 104s/epoch - 532ms/step
Epoch 42/1000
2023-09-29 10:37:51.805 
Epoch 42/1000 
	 loss: 4416.9443, MinusLogProbMetric: 4416.9443, val_loss: 4417.4189, val_MinusLogProbMetric: 4417.4189

Epoch 42: val_loss improved from 4421.81934 to 4417.41895, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 97s - loss: 4416.9443 - MinusLogProbMetric: 4416.9443 - val_loss: 4417.4189 - val_MinusLogProbMetric: 4417.4189 - lr: 1.5242e-07 - 97s/epoch - 494ms/step
Epoch 43/1000
2023-09-29 10:39:33.776 
Epoch 43/1000 
	 loss: 4413.1030, MinusLogProbMetric: 4413.1030, val_loss: 4413.0117, val_MinusLogProbMetric: 4413.0117

Epoch 43: val_loss improved from 4417.41895 to 4413.01172, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 102s - loss: 4413.1030 - MinusLogProbMetric: 4413.1030 - val_loss: 4413.0117 - val_MinusLogProbMetric: 4413.0117 - lr: 1.5242e-07 - 102s/epoch - 520ms/step
Epoch 44/1000
2023-09-29 10:41:17.682 
Epoch 44/1000 
	 loss: 4409.4194, MinusLogProbMetric: 4409.4194, val_loss: 4407.9316, val_MinusLogProbMetric: 4407.9316

Epoch 44: val_loss improved from 4413.01172 to 4407.93164, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 104s - loss: 4409.4194 - MinusLogProbMetric: 4409.4194 - val_loss: 4407.9316 - val_MinusLogProbMetric: 4407.9316 - lr: 1.5242e-07 - 104s/epoch - 532ms/step
Epoch 45/1000
2023-09-29 10:43:00.812 
Epoch 45/1000 
	 loss: 4404.6743, MinusLogProbMetric: 4404.6743, val_loss: 4403.5215, val_MinusLogProbMetric: 4403.5215

Epoch 45: val_loss improved from 4407.93164 to 4403.52148, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 103s - loss: 4404.6743 - MinusLogProbMetric: 4404.6743 - val_loss: 4403.5215 - val_MinusLogProbMetric: 4403.5215 - lr: 1.5242e-07 - 103s/epoch - 525ms/step
Epoch 46/1000
2023-09-29 10:44:48.135 
Epoch 46/1000 
	 loss: 4399.8774, MinusLogProbMetric: 4399.8774, val_loss: 4398.5688, val_MinusLogProbMetric: 4398.5688

Epoch 46: val_loss improved from 4403.52148 to 4398.56885, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 107s - loss: 4399.8774 - MinusLogProbMetric: 4399.8774 - val_loss: 4398.5688 - val_MinusLogProbMetric: 4398.5688 - lr: 1.5242e-07 - 107s/epoch - 547ms/step
Epoch 47/1000
2023-09-29 10:46:35.577 
Epoch 47/1000 
	 loss: 4394.4976, MinusLogProbMetric: 4394.4976, val_loss: 4393.6646, val_MinusLogProbMetric: 4393.6646

Epoch 47: val_loss improved from 4398.56885 to 4393.66455, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 108s - loss: 4394.4976 - MinusLogProbMetric: 4394.4976 - val_loss: 4393.6646 - val_MinusLogProbMetric: 4393.6646 - lr: 1.5242e-07 - 108s/epoch - 550ms/step
Epoch 48/1000
2023-09-29 10:48:22.328 
Epoch 48/1000 
	 loss: 4390.7593, MinusLogProbMetric: 4390.7593, val_loss: 4389.1147, val_MinusLogProbMetric: 4389.1147

Epoch 48: val_loss improved from 4393.66455 to 4389.11475, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 106s - loss: 4390.7593 - MinusLogProbMetric: 4390.7593 - val_loss: 4389.1147 - val_MinusLogProbMetric: 4389.1147 - lr: 1.5242e-07 - 106s/epoch - 543ms/step
Epoch 49/1000
2023-09-29 10:50:03.631 
Epoch 49/1000 
	 loss: 4385.2617, MinusLogProbMetric: 4385.2617, val_loss: 4385.1333, val_MinusLogProbMetric: 4385.1333

Epoch 49: val_loss improved from 4389.11475 to 4385.13330, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 102s - loss: 4385.2617 - MinusLogProbMetric: 4385.2617 - val_loss: 4385.1333 - val_MinusLogProbMetric: 4385.1333 - lr: 1.5242e-07 - 102s/epoch - 519ms/step
Epoch 50/1000
2023-09-29 10:51:53.073 
Epoch 50/1000 
	 loss: 4380.9727, MinusLogProbMetric: 4380.9727, val_loss: 4379.9927, val_MinusLogProbMetric: 4379.9927

Epoch 50: val_loss improved from 4385.13330 to 4379.99268, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 109s - loss: 4380.9727 - MinusLogProbMetric: 4380.9727 - val_loss: 4379.9927 - val_MinusLogProbMetric: 4379.9927 - lr: 1.5242e-07 - 109s/epoch - 556ms/step
Epoch 51/1000
2023-09-29 10:53:42.674 
Epoch 51/1000 
	 loss: 4375.4341, MinusLogProbMetric: 4375.4341, val_loss: 4374.9116, val_MinusLogProbMetric: 4374.9116

Epoch 51: val_loss improved from 4379.99268 to 4374.91162, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 110s - loss: 4375.4341 - MinusLogProbMetric: 4375.4341 - val_loss: 4374.9116 - val_MinusLogProbMetric: 4374.9116 - lr: 1.5242e-07 - 110s/epoch - 560ms/step
Epoch 52/1000
2023-09-29 10:55:23.944 
Epoch 52/1000 
	 loss: 4371.6968, MinusLogProbMetric: 4371.6968, val_loss: 4373.7197, val_MinusLogProbMetric: 4373.7197

Epoch 52: val_loss improved from 4374.91162 to 4373.71973, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 102s - loss: 4371.6968 - MinusLogProbMetric: 4371.6968 - val_loss: 4373.7197 - val_MinusLogProbMetric: 4373.7197 - lr: 1.5242e-07 - 102s/epoch - 518ms/step
Epoch 53/1000
2023-09-29 10:57:06.134 
Epoch 53/1000 
	 loss: 4370.5073, MinusLogProbMetric: 4370.5073, val_loss: 4369.6211, val_MinusLogProbMetric: 4369.6211

Epoch 53: val_loss improved from 4373.71973 to 4369.62109, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 102s - loss: 4370.5073 - MinusLogProbMetric: 4370.5073 - val_loss: 4369.6211 - val_MinusLogProbMetric: 4369.6211 - lr: 1.5242e-07 - 102s/epoch - 519ms/step
Epoch 54/1000
2023-09-29 10:58:43.373 
Epoch 54/1000 
	 loss: 4366.0010, MinusLogProbMetric: 4366.0010, val_loss: 4365.5532, val_MinusLogProbMetric: 4365.5532

Epoch 54: val_loss improved from 4369.62109 to 4365.55322, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 97s - loss: 4366.0010 - MinusLogProbMetric: 4366.0010 - val_loss: 4365.5532 - val_MinusLogProbMetric: 4365.5532 - lr: 1.5242e-07 - 97s/epoch - 496ms/step
Epoch 55/1000
2023-09-29 11:00:25.869 
Epoch 55/1000 
	 loss: 4361.4692, MinusLogProbMetric: 4361.4692, val_loss: 4361.1167, val_MinusLogProbMetric: 4361.1167

Epoch 55: val_loss improved from 4365.55322 to 4361.11670, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 103s - loss: 4361.4692 - MinusLogProbMetric: 4361.4692 - val_loss: 4361.1167 - val_MinusLogProbMetric: 4361.1167 - lr: 1.5242e-07 - 103s/epoch - 523ms/step
Epoch 56/1000
2023-09-29 11:02:10.977 
Epoch 56/1000 
	 loss: 4355.9971, MinusLogProbMetric: 4355.9971, val_loss: 4357.3022, val_MinusLogProbMetric: 4357.3022

Epoch 56: val_loss improved from 4361.11670 to 4357.30225, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 106s - loss: 4355.9971 - MinusLogProbMetric: 4355.9971 - val_loss: 4357.3022 - val_MinusLogProbMetric: 4357.3022 - lr: 1.5242e-07 - 106s/epoch - 539ms/step
Epoch 57/1000
2023-09-29 11:03:59.876 
Epoch 57/1000 
	 loss: 4350.3271, MinusLogProbMetric: 4350.3271, val_loss: 4353.2881, val_MinusLogProbMetric: 4353.2881

Epoch 57: val_loss improved from 4357.30225 to 4353.28809, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 108s - loss: 4350.3271 - MinusLogProbMetric: 4350.3271 - val_loss: 4353.2881 - val_MinusLogProbMetric: 4353.2881 - lr: 1.5242e-07 - 108s/epoch - 553ms/step
Epoch 58/1000
2023-09-29 11:05:40.080 
Epoch 58/1000 
	 loss: 4347.3818, MinusLogProbMetric: 4347.3818, val_loss: 4349.4204, val_MinusLogProbMetric: 4349.4204

Epoch 58: val_loss improved from 4353.28809 to 4349.42041, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 100s - loss: 4347.3818 - MinusLogProbMetric: 4347.3818 - val_loss: 4349.4204 - val_MinusLogProbMetric: 4349.4204 - lr: 1.5242e-07 - 100s/epoch - 511ms/step
Epoch 59/1000
2023-09-29 11:07:30.409 
Epoch 59/1000 
	 loss: 4343.6348, MinusLogProbMetric: 4343.6348, val_loss: 4346.0835, val_MinusLogProbMetric: 4346.0835

Epoch 59: val_loss improved from 4349.42041 to 4346.08350, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 111s - loss: 4343.6348 - MinusLogProbMetric: 4343.6348 - val_loss: 4346.0835 - val_MinusLogProbMetric: 4346.0835 - lr: 1.5242e-07 - 111s/epoch - 565ms/step
Epoch 60/1000
2023-09-29 11:09:15.194 
Epoch 60/1000 
	 loss: 4340.3652, MinusLogProbMetric: 4340.3652, val_loss: 4341.7773, val_MinusLogProbMetric: 4341.7773

Epoch 60: val_loss improved from 4346.08350 to 4341.77734, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 105s - loss: 4340.3652 - MinusLogProbMetric: 4340.3652 - val_loss: 4341.7773 - val_MinusLogProbMetric: 4341.7773 - lr: 1.5242e-07 - 105s/epoch - 534ms/step
Epoch 61/1000
2023-09-29 11:10:58.643 
Epoch 61/1000 
	 loss: 4335.2422, MinusLogProbMetric: 4335.2422, val_loss: 4337.1680, val_MinusLogProbMetric: 4337.1680

Epoch 61: val_loss improved from 4341.77734 to 4337.16797, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 103s - loss: 4335.2422 - MinusLogProbMetric: 4335.2422 - val_loss: 4337.1680 - val_MinusLogProbMetric: 4337.1680 - lr: 1.5242e-07 - 103s/epoch - 527ms/step
Epoch 62/1000
2023-09-29 11:12:40.389 
Epoch 62/1000 
	 loss: 4330.3188, MinusLogProbMetric: 4330.3188, val_loss: 4333.4302, val_MinusLogProbMetric: 4333.4302

Epoch 62: val_loss improved from 4337.16797 to 4333.43018, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 102s - loss: 4330.3188 - MinusLogProbMetric: 4330.3188 - val_loss: 4333.4302 - val_MinusLogProbMetric: 4333.4302 - lr: 1.5242e-07 - 102s/epoch - 519ms/step
Epoch 63/1000
2023-09-29 11:14:19.187 
Epoch 63/1000 
	 loss: 4325.9980, MinusLogProbMetric: 4325.9980, val_loss: 4329.4849, val_MinusLogProbMetric: 4329.4849

Epoch 63: val_loss improved from 4333.43018 to 4329.48486, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 99s - loss: 4325.9980 - MinusLogProbMetric: 4325.9980 - val_loss: 4329.4849 - val_MinusLogProbMetric: 4329.4849 - lr: 1.5242e-07 - 99s/epoch - 506ms/step
Epoch 64/1000
2023-09-29 11:16:06.024 
Epoch 64/1000 
	 loss: 4323.4819, MinusLogProbMetric: 4323.4819, val_loss: 4324.8818, val_MinusLogProbMetric: 4324.8818

Epoch 64: val_loss improved from 4329.48486 to 4324.88184, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 106s - loss: 4323.4819 - MinusLogProbMetric: 4323.4819 - val_loss: 4324.8818 - val_MinusLogProbMetric: 4324.8818 - lr: 1.5242e-07 - 106s/epoch - 543ms/step
Epoch 65/1000
2023-09-29 11:17:47.469 
Epoch 65/1000 
	 loss: 4320.3530, MinusLogProbMetric: 4320.3530, val_loss: 4321.9448, val_MinusLogProbMetric: 4321.9448

Epoch 65: val_loss improved from 4324.88184 to 4321.94482, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 101s - loss: 4320.3530 - MinusLogProbMetric: 4320.3530 - val_loss: 4321.9448 - val_MinusLogProbMetric: 4321.9448 - lr: 1.5242e-07 - 101s/epoch - 517ms/step
Epoch 66/1000
2023-09-29 11:19:29.205 
Epoch 66/1000 
	 loss: 4317.2642, MinusLogProbMetric: 4317.2642, val_loss: 4318.7856, val_MinusLogProbMetric: 4318.7856

Epoch 66: val_loss improved from 4321.94482 to 4318.78564, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 102s - loss: 4317.2642 - MinusLogProbMetric: 4317.2642 - val_loss: 4318.7856 - val_MinusLogProbMetric: 4318.7856 - lr: 1.5242e-07 - 102s/epoch - 522ms/step
Epoch 67/1000
2023-09-29 11:21:10.976 
Epoch 67/1000 
	 loss: 4313.6973, MinusLogProbMetric: 4313.6973, val_loss: 4314.7603, val_MinusLogProbMetric: 4314.7603

Epoch 67: val_loss improved from 4318.78564 to 4314.76025, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 101s - loss: 4313.6973 - MinusLogProbMetric: 4313.6973 - val_loss: 4314.7603 - val_MinusLogProbMetric: 4314.7603 - lr: 1.5242e-07 - 101s/epoch - 517ms/step
Epoch 68/1000
2023-09-29 11:22:53.492 
Epoch 68/1000 
	 loss: 4308.9053, MinusLogProbMetric: 4308.9053, val_loss: 4310.2764, val_MinusLogProbMetric: 4310.2764

Epoch 68: val_loss improved from 4314.76025 to 4310.27637, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 103s - loss: 4308.9053 - MinusLogProbMetric: 4308.9053 - val_loss: 4310.2764 - val_MinusLogProbMetric: 4310.2764 - lr: 1.5242e-07 - 103s/epoch - 525ms/step
Epoch 69/1000
2023-09-29 11:24:35.699 
Epoch 69/1000 
	 loss: 4303.7759, MinusLogProbMetric: 4303.7759, val_loss: 4304.0542, val_MinusLogProbMetric: 4304.0542

Epoch 69: val_loss improved from 4310.27637 to 4304.05420, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 102s - loss: 4303.7759 - MinusLogProbMetric: 4303.7759 - val_loss: 4304.0542 - val_MinusLogProbMetric: 4304.0542 - lr: 1.5242e-07 - 102s/epoch - 519ms/step
Epoch 70/1000
2023-09-29 11:26:16.379 
Epoch 70/1000 
	 loss: 4299.6226, MinusLogProbMetric: 4299.6226, val_loss: 4301.0444, val_MinusLogProbMetric: 4301.0444

Epoch 70: val_loss improved from 4304.05420 to 4301.04443, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 101s - loss: 4299.6226 - MinusLogProbMetric: 4299.6226 - val_loss: 4301.0444 - val_MinusLogProbMetric: 4301.0444 - lr: 1.5242e-07 - 101s/epoch - 515ms/step
Epoch 71/1000
2023-09-29 11:28:05.962 
Epoch 71/1000 
	 loss: 4296.0708, MinusLogProbMetric: 4296.0708, val_loss: 4296.8491, val_MinusLogProbMetric: 4296.8491

Epoch 71: val_loss improved from 4301.04443 to 4296.84912, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 110s - loss: 4296.0708 - MinusLogProbMetric: 4296.0708 - val_loss: 4296.8491 - val_MinusLogProbMetric: 4296.8491 - lr: 1.5242e-07 - 110s/epoch - 561ms/step
Epoch 72/1000
2023-09-29 11:29:50.955 
Epoch 72/1000 
	 loss: 4292.1680, MinusLogProbMetric: 4292.1680, val_loss: 4292.2261, val_MinusLogProbMetric: 4292.2261

Epoch 72: val_loss improved from 4296.84912 to 4292.22607, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 105s - loss: 4292.1680 - MinusLogProbMetric: 4292.1680 - val_loss: 4292.2261 - val_MinusLogProbMetric: 4292.2261 - lr: 1.5242e-07 - 105s/epoch - 536ms/step
Epoch 73/1000
2023-09-29 11:31:34.110 
Epoch 73/1000 
	 loss: 4287.7671, MinusLogProbMetric: 4287.7671, val_loss: 4287.8877, val_MinusLogProbMetric: 4287.8877

Epoch 73: val_loss improved from 4292.22607 to 4287.88770, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 103s - loss: 4287.7671 - MinusLogProbMetric: 4287.7671 - val_loss: 4287.8877 - val_MinusLogProbMetric: 4287.8877 - lr: 1.5242e-07 - 103s/epoch - 525ms/step
Epoch 74/1000
2023-09-29 11:33:15.014 
Epoch 74/1000 
	 loss: 4283.7437, MinusLogProbMetric: 4283.7437, val_loss: 4283.2598, val_MinusLogProbMetric: 4283.2598

Epoch 74: val_loss improved from 4287.88770 to 4283.25977, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 101s - loss: 4283.7437 - MinusLogProbMetric: 4283.7437 - val_loss: 4283.2598 - val_MinusLogProbMetric: 4283.2598 - lr: 1.5242e-07 - 101s/epoch - 513ms/step
Epoch 75/1000
2023-09-29 11:34:56.965 
Epoch 75/1000 
	 loss: 4280.4619, MinusLogProbMetric: 4280.4619, val_loss: 4281.0679, val_MinusLogProbMetric: 4281.0679

Epoch 75: val_loss improved from 4283.25977 to 4281.06787, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 103s - loss: 4280.4619 - MinusLogProbMetric: 4280.4619 - val_loss: 4281.0679 - val_MinusLogProbMetric: 4281.0679 - lr: 1.5242e-07 - 103s/epoch - 524ms/step
Epoch 76/1000
2023-09-29 11:36:41.995 
Epoch 76/1000 
	 loss: 4275.9248, MinusLogProbMetric: 4275.9248, val_loss: 4276.7886, val_MinusLogProbMetric: 4276.7886

Epoch 76: val_loss improved from 4281.06787 to 4276.78857, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 105s - loss: 4275.9248 - MinusLogProbMetric: 4275.9248 - val_loss: 4276.7886 - val_MinusLogProbMetric: 4276.7886 - lr: 1.5242e-07 - 105s/epoch - 534ms/step
Epoch 77/1000
2023-09-29 11:38:28.703 
Epoch 77/1000 
	 loss: 4272.5532, MinusLogProbMetric: 4272.5532, val_loss: 4272.9014, val_MinusLogProbMetric: 4272.9014

Epoch 77: val_loss improved from 4276.78857 to 4272.90137, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 106s - loss: 4272.5532 - MinusLogProbMetric: 4272.5532 - val_loss: 4272.9014 - val_MinusLogProbMetric: 4272.9014 - lr: 1.5242e-07 - 106s/epoch - 543ms/step
Epoch 78/1000
2023-09-29 11:40:12.201 
Epoch 78/1000 
	 loss: 4268.2456, MinusLogProbMetric: 4268.2456, val_loss: 4268.6265, val_MinusLogProbMetric: 4268.6265

Epoch 78: val_loss improved from 4272.90137 to 4268.62646, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 104s - loss: 4268.2456 - MinusLogProbMetric: 4268.2456 - val_loss: 4268.6265 - val_MinusLogProbMetric: 4268.6265 - lr: 1.5242e-07 - 104s/epoch - 530ms/step
Epoch 79/1000
2023-09-29 11:41:55.944 
Epoch 79/1000 
	 loss: 4264.5288, MinusLogProbMetric: 4264.5288, val_loss: 4265.0195, val_MinusLogProbMetric: 4265.0195

Epoch 79: val_loss improved from 4268.62646 to 4265.01953, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 104s - loss: 4264.5288 - MinusLogProbMetric: 4264.5288 - val_loss: 4265.0195 - val_MinusLogProbMetric: 4265.0195 - lr: 1.5242e-07 - 104s/epoch - 529ms/step
Epoch 80/1000
2023-09-29 11:43:43.345 
Epoch 80/1000 
	 loss: 4260.4546, MinusLogProbMetric: 4260.4546, val_loss: 4261.8589, val_MinusLogProbMetric: 4261.8589

Epoch 80: val_loss improved from 4265.01953 to 4261.85889, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 107s - loss: 4260.4546 - MinusLogProbMetric: 4260.4546 - val_loss: 4261.8589 - val_MinusLogProbMetric: 4261.8589 - lr: 1.5242e-07 - 107s/epoch - 548ms/step
Epoch 81/1000
2023-09-29 11:45:27.368 
Epoch 81/1000 
	 loss: 4256.8379, MinusLogProbMetric: 4256.8379, val_loss: 4257.6509, val_MinusLogProbMetric: 4257.6509

Epoch 81: val_loss improved from 4261.85889 to 4257.65088, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 104s - loss: 4256.8379 - MinusLogProbMetric: 4256.8379 - val_loss: 4257.6509 - val_MinusLogProbMetric: 4257.6509 - lr: 1.5242e-07 - 104s/epoch - 528ms/step
Epoch 82/1000
2023-09-29 11:47:10.327 
Epoch 82/1000 
	 loss: 4253.5400, MinusLogProbMetric: 4253.5400, val_loss: 4253.9517, val_MinusLogProbMetric: 4253.9517

Epoch 82: val_loss improved from 4257.65088 to 4253.95166, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 103s - loss: 4253.5400 - MinusLogProbMetric: 4253.5400 - val_loss: 4253.9517 - val_MinusLogProbMetric: 4253.9517 - lr: 1.5242e-07 - 103s/epoch - 527ms/step
Epoch 83/1000
2023-09-29 11:48:55.377 
Epoch 83/1000 
	 loss: 4249.9707, MinusLogProbMetric: 4249.9707, val_loss: 4252.4473, val_MinusLogProbMetric: 4252.4473

Epoch 83: val_loss improved from 4253.95166 to 4252.44727, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 106s - loss: 4249.9707 - MinusLogProbMetric: 4249.9707 - val_loss: 4252.4473 - val_MinusLogProbMetric: 4252.4473 - lr: 1.5242e-07 - 106s/epoch - 539ms/step
Epoch 84/1000
2023-09-29 11:50:34.565 
Epoch 84/1000 
	 loss: 4248.7202, MinusLogProbMetric: 4248.7202, val_loss: 4249.9492, val_MinusLogProbMetric: 4249.9492

Epoch 84: val_loss improved from 4252.44727 to 4249.94922, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 99s - loss: 4248.7202 - MinusLogProbMetric: 4248.7202 - val_loss: 4249.9492 - val_MinusLogProbMetric: 4249.9492 - lr: 1.5242e-07 - 99s/epoch - 504ms/step
Epoch 85/1000
2023-09-29 11:52:15.391 
Epoch 85/1000 
	 loss: 4244.2544, MinusLogProbMetric: 4244.2544, val_loss: 4244.7368, val_MinusLogProbMetric: 4244.7368

Epoch 85: val_loss improved from 4249.94922 to 4244.73682, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 101s - loss: 4244.2544 - MinusLogProbMetric: 4244.2544 - val_loss: 4244.7368 - val_MinusLogProbMetric: 4244.7368 - lr: 1.5242e-07 - 101s/epoch - 513ms/step
Epoch 86/1000
2023-09-29 11:53:52.902 
Epoch 86/1000 
	 loss: 4240.0376, MinusLogProbMetric: 4240.0376, val_loss: 4241.2866, val_MinusLogProbMetric: 4241.2866

Epoch 86: val_loss improved from 4244.73682 to 4241.28662, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 98s - loss: 4240.0376 - MinusLogProbMetric: 4240.0376 - val_loss: 4241.2866 - val_MinusLogProbMetric: 4241.2866 - lr: 1.5242e-07 - 98s/epoch - 498ms/step
Epoch 87/1000
2023-09-29 11:55:35.289 
Epoch 87/1000 
	 loss: 4237.0083, MinusLogProbMetric: 4237.0083, val_loss: 4237.5859, val_MinusLogProbMetric: 4237.5859

Epoch 87: val_loss improved from 4241.28662 to 4237.58594, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 103s - loss: 4237.0083 - MinusLogProbMetric: 4237.0083 - val_loss: 4237.5859 - val_MinusLogProbMetric: 4237.5859 - lr: 1.5242e-07 - 103s/epoch - 523ms/step
Epoch 88/1000
2023-09-29 11:57:15.133 
Epoch 88/1000 
	 loss: 4233.4873, MinusLogProbMetric: 4233.4873, val_loss: 4233.4546, val_MinusLogProbMetric: 4233.4546

Epoch 88: val_loss improved from 4237.58594 to 4233.45459, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 99s - loss: 4233.4873 - MinusLogProbMetric: 4233.4873 - val_loss: 4233.4546 - val_MinusLogProbMetric: 4233.4546 - lr: 1.5242e-07 - 99s/epoch - 507ms/step
Epoch 89/1000
2023-09-29 11:58:59.034 
Epoch 89/1000 
	 loss: 4229.3369, MinusLogProbMetric: 4229.3369, val_loss: 4230.0127, val_MinusLogProbMetric: 4230.0127

Epoch 89: val_loss improved from 4233.45459 to 4230.01270, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 105s - loss: 4229.3369 - MinusLogProbMetric: 4229.3369 - val_loss: 4230.0127 - val_MinusLogProbMetric: 4230.0127 - lr: 1.5242e-07 - 105s/epoch - 537ms/step
Epoch 90/1000
2023-09-29 12:00:44.400 
Epoch 90/1000 
	 loss: 4225.2393, MinusLogProbMetric: 4225.2393, val_loss: 4225.5190, val_MinusLogProbMetric: 4225.5190

Epoch 90: val_loss improved from 4230.01270 to 4225.51904, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 104s - loss: 4225.2393 - MinusLogProbMetric: 4225.2393 - val_loss: 4225.5190 - val_MinusLogProbMetric: 4225.5190 - lr: 1.5242e-07 - 104s/epoch - 531ms/step
Epoch 91/1000
2023-09-29 12:02:27.440 
Epoch 91/1000 
	 loss: 4222.0967, MinusLogProbMetric: 4222.0967, val_loss: 4222.7788, val_MinusLogProbMetric: 4222.7788

Epoch 91: val_loss improved from 4225.51904 to 4222.77881, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 103s - loss: 4222.0967 - MinusLogProbMetric: 4222.0967 - val_loss: 4222.7788 - val_MinusLogProbMetric: 4222.7788 - lr: 1.5242e-07 - 103s/epoch - 527ms/step
Epoch 92/1000
2023-09-29 12:04:06.841 
Epoch 92/1000 
	 loss: 4218.6123, MinusLogProbMetric: 4218.6123, val_loss: 4219.4614, val_MinusLogProbMetric: 4219.4614

Epoch 92: val_loss improved from 4222.77881 to 4219.46143, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 100s - loss: 4218.6123 - MinusLogProbMetric: 4218.6123 - val_loss: 4219.4614 - val_MinusLogProbMetric: 4219.4614 - lr: 1.5242e-07 - 100s/epoch - 509ms/step
Epoch 93/1000
2023-09-29 12:05:50.047 
Epoch 93/1000 
	 loss: 4214.7754, MinusLogProbMetric: 4214.7754, val_loss: 4216.8877, val_MinusLogProbMetric: 4216.8877

Epoch 93: val_loss improved from 4219.46143 to 4216.88770, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 103s - loss: 4214.7754 - MinusLogProbMetric: 4214.7754 - val_loss: 4216.8877 - val_MinusLogProbMetric: 4216.8877 - lr: 1.5242e-07 - 103s/epoch - 528ms/step
Epoch 94/1000
2023-09-29 12:07:32.624 
Epoch 94/1000 
	 loss: 4208.8853, MinusLogProbMetric: 4208.8853, val_loss: 4210.4961, val_MinusLogProbMetric: 4210.4961

Epoch 94: val_loss improved from 4216.88770 to 4210.49609, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 102s - loss: 4208.8853 - MinusLogProbMetric: 4208.8853 - val_loss: 4210.4961 - val_MinusLogProbMetric: 4210.4961 - lr: 1.5242e-07 - 102s/epoch - 518ms/step
Epoch 95/1000
2023-09-29 12:09:10.743 
Epoch 95/1000 
	 loss: 4205.1870, MinusLogProbMetric: 4205.1870, val_loss: 4206.5063, val_MinusLogProbMetric: 4206.5063

Epoch 95: val_loss improved from 4210.49609 to 4206.50635, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 98s - loss: 4205.1870 - MinusLogProbMetric: 4205.1870 - val_loss: 4206.5063 - val_MinusLogProbMetric: 4206.5063 - lr: 1.5242e-07 - 98s/epoch - 502ms/step
Epoch 96/1000
2023-09-29 12:10:50.427 
Epoch 96/1000 
	 loss: 4201.7271, MinusLogProbMetric: 4201.7271, val_loss: 4203.1621, val_MinusLogProbMetric: 4203.1621

Epoch 96: val_loss improved from 4206.50635 to 4203.16211, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 100s - loss: 4201.7271 - MinusLogProbMetric: 4201.7271 - val_loss: 4203.1621 - val_MinusLogProbMetric: 4203.1621 - lr: 1.5242e-07 - 100s/epoch - 510ms/step
Epoch 97/1000
2023-09-29 12:12:35.293 
Epoch 97/1000 
	 loss: 4198.3032, MinusLogProbMetric: 4198.3032, val_loss: 4199.7798, val_MinusLogProbMetric: 4199.7798

Epoch 97: val_loss improved from 4203.16211 to 4199.77979, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 105s - loss: 4198.3032 - MinusLogProbMetric: 4198.3032 - val_loss: 4199.7798 - val_MinusLogProbMetric: 4199.7798 - lr: 1.5242e-07 - 105s/epoch - 534ms/step
Epoch 98/1000
2023-09-29 12:14:15.728 
Epoch 98/1000 
	 loss: 4195.0977, MinusLogProbMetric: 4195.0977, val_loss: 4196.0049, val_MinusLogProbMetric: 4196.0049

Epoch 98: val_loss improved from 4199.77979 to 4196.00488, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 101s - loss: 4195.0977 - MinusLogProbMetric: 4195.0977 - val_loss: 4196.0049 - val_MinusLogProbMetric: 4196.0049 - lr: 1.5242e-07 - 101s/epoch - 517ms/step
Epoch 99/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 160: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-29 12:15:40.766 
Epoch 99/1000 
	 loss: nan, MinusLogProbMetric: 4190.6299, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 99: val_loss did not improve from 4196.00488
196/196 - 83s - loss: nan - MinusLogProbMetric: 4190.6299 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.5242e-07 - 83s/epoch - 422ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 5.0805263425290834e-08.
===========
Generating train data for run 336.
===========
Train data generated in 0.43 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_336/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_336/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_336/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_336
self.data_kwargs: {'seed': 187}
self.x_data: [[ 6.8153725   2.8430347   6.1778684  ...  2.16058     2.2525268
   1.7267085 ]
 [ 2.1836705   1.800204    9.035551   ...  6.5503054   1.5567943
   2.9656742 ]
 [ 0.87054014  3.1855702   7.8237348  ...  6.3044343  -0.7088827
   2.3190107 ]
 ...
 [ 0.35442734  3.573855   10.922302   ...  5.8448343   0.8037275
   2.171598  ]
 [ 1.4908957   3.8775673   8.757837   ...  5.7039022  -0.88000226
   2.3087878 ]
 [ 1.7291393   3.7802503   9.770119   ...  5.5898924   0.22995539
   4.1106014 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_418"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_419 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_38 (LogProbL  (None,)                  4278720   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,278,720
Trainable params: 4,278,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_38/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_38'")
self.model: <keras.engine.functional.Functional object at 0x7f7f08743250>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f7753200190>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f7753200190>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f7e80455ed0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f788e6a2ec0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f788e6a2350>, <keras.callbacks.ModelCheckpoint object at 0x7f788e6a3640>, <keras.callbacks.EarlyStopping object at 0x7f788e6a2020>, <keras.callbacks.ReduceLROnPlateau object at 0x7f788e6a1510>, <keras.callbacks.TerminateOnNaN object at 0x7f788e6a3340>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 336/720 with hyperparameters:
timestamp = 2023-09-29 12:15:57.816406
ndims = 64
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4278720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 5.0805263425290834e-08...
Train first sample: [ 6.8153725e+00  2.8430347e+00  6.1778684e+00  4.2452879e+00
  1.6284513e+00  3.3207846e+00  7.0928354e+00  6.4340367e+00
  6.0301833e+00  6.4328456e+00  6.5495849e+00  4.9321003e+00
  8.9532204e+00  3.4481030e+00  4.8284545e+00  8.0901794e+00
  8.1549883e+00  7.3900299e+00  1.3510420e+00  9.5149851e+00
  7.5267930e+00  1.0017762e+01  1.1636026e+00  8.6080103e+00
  2.2276068e+00  6.2524571e+00  1.5594521e+00  8.2755499e+00
  8.5135918e+00  6.3978677e+00  3.8566861e+00  6.8423492e-01
  7.5210962e+00  5.4144373e+00  6.8548636e+00  8.7448330e+00
  9.6529083e+00  8.6500130e+00  9.1009712e-01  4.1672401e+00
  7.6439004e+00  9.0885794e-01  5.2691178e+00  8.8166595e-03
  7.4780917e-01 -4.8575500e-01  7.9515800e+00  2.7509699e+00
  4.8898096e+00  1.0073138e+01  7.4751706e+00  6.5633142e-01
  1.5831217e+00  6.0002384e+00  5.6322212e+00  2.2999794e+00
  9.1524439e+00  5.5740438e+00  5.8525505e+00  6.6422234e+00
  6.9292612e+00  2.1605799e+00  2.2525268e+00  1.7267085e+00]
Epoch 1/1000
2023-09-29 12:20:33.737 
Epoch 1/1000 
	 loss: 4189.7202, MinusLogProbMetric: 4189.7202, val_loss: 4190.6587, val_MinusLogProbMetric: 4190.6587

Epoch 1: val_loss improved from inf to 4190.65869, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 276s - loss: 4189.7202 - MinusLogProbMetric: 4189.7202 - val_loss: 4190.6587 - val_MinusLogProbMetric: 4190.6587 - lr: 5.0805e-08 - 276s/epoch - 1s/step
Epoch 2/1000
2023-09-29 12:22:14.410 
Epoch 2/1000 
	 loss: 4186.5444, MinusLogProbMetric: 4186.5444, val_loss: 4187.9980, val_MinusLogProbMetric: 4187.9980

Epoch 2: val_loss improved from 4190.65869 to 4187.99805, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 100s - loss: 4186.5444 - MinusLogProbMetric: 4186.5444 - val_loss: 4187.9980 - val_MinusLogProbMetric: 4187.9980 - lr: 5.0805e-08 - 100s/epoch - 513ms/step
Epoch 3/1000
2023-09-29 12:23:57.728 
Epoch 3/1000 
	 loss: 4183.5859, MinusLogProbMetric: 4183.5859, val_loss: 4185.1812, val_MinusLogProbMetric: 4185.1812

Epoch 3: val_loss improved from 4187.99805 to 4185.18115, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 104s - loss: 4183.5859 - MinusLogProbMetric: 4183.5859 - val_loss: 4185.1812 - val_MinusLogProbMetric: 4185.1812 - lr: 5.0805e-08 - 104s/epoch - 529ms/step
Epoch 4/1000
2023-09-29 12:25:49.948 
Epoch 4/1000 
	 loss: 4181.6270, MinusLogProbMetric: 4181.6270, val_loss: 4182.2725, val_MinusLogProbMetric: 4182.2725

Epoch 4: val_loss improved from 4185.18115 to 4182.27246, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 112s - loss: 4181.6270 - MinusLogProbMetric: 4181.6270 - val_loss: 4182.2725 - val_MinusLogProbMetric: 4182.2725 - lr: 5.0805e-08 - 112s/epoch - 571ms/step
Epoch 5/1000
2023-09-29 12:27:41.376 
Epoch 5/1000 
	 loss: 4177.8730, MinusLogProbMetric: 4177.8730, val_loss: 4181.6465, val_MinusLogProbMetric: 4181.6465

Epoch 5: val_loss improved from 4182.27246 to 4181.64648, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 111s - loss: 4177.8730 - MinusLogProbMetric: 4177.8730 - val_loss: 4181.6465 - val_MinusLogProbMetric: 4181.6465 - lr: 5.0805e-08 - 111s/epoch - 566ms/step
Epoch 6/1000
2023-09-29 12:29:31.325 
Epoch 6/1000 
	 loss: 4175.9727, MinusLogProbMetric: 4175.9727, val_loss: 4179.7549, val_MinusLogProbMetric: 4179.7549

Epoch 6: val_loss improved from 4181.64648 to 4179.75488, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 111s - loss: 4175.9727 - MinusLogProbMetric: 4175.9727 - val_loss: 4179.7549 - val_MinusLogProbMetric: 4179.7549 - lr: 5.0805e-08 - 111s/epoch - 565ms/step
Epoch 7/1000
2023-09-29 12:31:20.760 
Epoch 7/1000 
	 loss: 4173.5405, MinusLogProbMetric: 4173.5405, val_loss: 4176.5078, val_MinusLogProbMetric: 4176.5078

Epoch 7: val_loss improved from 4179.75488 to 4176.50781, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 109s - loss: 4173.5405 - MinusLogProbMetric: 4173.5405 - val_loss: 4176.5078 - val_MinusLogProbMetric: 4176.5078 - lr: 5.0805e-08 - 109s/epoch - 555ms/step
Epoch 8/1000
2023-09-29 12:33:04.596 
Epoch 8/1000 
	 loss: 4171.3779, MinusLogProbMetric: 4171.3779, val_loss: 4174.0181, val_MinusLogProbMetric: 4174.0181

Epoch 8: val_loss improved from 4176.50781 to 4174.01807, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 104s - loss: 4171.3779 - MinusLogProbMetric: 4171.3779 - val_loss: 4174.0181 - val_MinusLogProbMetric: 4174.0181 - lr: 5.0805e-08 - 104s/epoch - 528ms/step
Epoch 9/1000
2023-09-29 12:34:47.051 
Epoch 9/1000 
	 loss: 4168.7500, MinusLogProbMetric: 4168.7500, val_loss: 4171.6968, val_MinusLogProbMetric: 4171.6968

Epoch 9: val_loss improved from 4174.01807 to 4171.69678, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 102s - loss: 4168.7500 - MinusLogProbMetric: 4168.7500 - val_loss: 4171.6968 - val_MinusLogProbMetric: 4171.6968 - lr: 5.0805e-08 - 102s/epoch - 522ms/step
Epoch 10/1000
2023-09-29 12:36:32.059 
Epoch 10/1000 
	 loss: 4166.1753, MinusLogProbMetric: 4166.1753, val_loss: 4168.5718, val_MinusLogProbMetric: 4168.5718

Epoch 10: val_loss improved from 4171.69678 to 4168.57178, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 105s - loss: 4166.1753 - MinusLogProbMetric: 4166.1753 - val_loss: 4168.5718 - val_MinusLogProbMetric: 4168.5718 - lr: 5.0805e-08 - 105s/epoch - 536ms/step
Epoch 11/1000
2023-09-29 12:38:18.204 
Epoch 11/1000 
	 loss: 4163.5708, MinusLogProbMetric: 4163.5708, val_loss: 4166.1758, val_MinusLogProbMetric: 4166.1758

Epoch 11: val_loss improved from 4168.57178 to 4166.17578, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 107s - loss: 4163.5708 - MinusLogProbMetric: 4163.5708 - val_loss: 4166.1758 - val_MinusLogProbMetric: 4166.1758 - lr: 5.0805e-08 - 107s/epoch - 544ms/step
Epoch 12/1000
2023-09-29 12:40:03.066 
Epoch 12/1000 
	 loss: 4160.9976, MinusLogProbMetric: 4160.9976, val_loss: 4163.8940, val_MinusLogProbMetric: 4163.8940

Epoch 12: val_loss improved from 4166.17578 to 4163.89404, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 105s - loss: 4160.9976 - MinusLogProbMetric: 4160.9976 - val_loss: 4163.8940 - val_MinusLogProbMetric: 4163.8940 - lr: 5.0805e-08 - 105s/epoch - 535ms/step
Epoch 13/1000
2023-09-29 12:41:54.004 
Epoch 13/1000 
	 loss: 4159.3662, MinusLogProbMetric: 4159.3662, val_loss: 4161.6396, val_MinusLogProbMetric: 4161.6396

Epoch 13: val_loss improved from 4163.89404 to 4161.63965, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 111s - loss: 4159.3662 - MinusLogProbMetric: 4159.3662 - val_loss: 4161.6396 - val_MinusLogProbMetric: 4161.6396 - lr: 5.0805e-08 - 111s/epoch - 566ms/step
Epoch 14/1000
2023-09-29 12:43:46.398 
Epoch 14/1000 
	 loss: 4156.3579, MinusLogProbMetric: 4156.3579, val_loss: 4160.1074, val_MinusLogProbMetric: 4160.1074

Epoch 14: val_loss improved from 4161.63965 to 4160.10742, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 112s - loss: 4156.3579 - MinusLogProbMetric: 4156.3579 - val_loss: 4160.1074 - val_MinusLogProbMetric: 4160.1074 - lr: 5.0805e-08 - 112s/epoch - 571ms/step
Epoch 15/1000
2023-09-29 12:45:36.257 
Epoch 15/1000 
	 loss: 4154.3018, MinusLogProbMetric: 4154.3018, val_loss: 4158.0386, val_MinusLogProbMetric: 4158.0386

Epoch 15: val_loss improved from 4160.10742 to 4158.03857, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 110s - loss: 4154.3018 - MinusLogProbMetric: 4154.3018 - val_loss: 4158.0386 - val_MinusLogProbMetric: 4158.0386 - lr: 5.0805e-08 - 110s/epoch - 563ms/step
Epoch 16/1000
2023-09-29 12:47:28.813 
Epoch 16/1000 
	 loss: 4152.7505, MinusLogProbMetric: 4152.7505, val_loss: 4156.2085, val_MinusLogProbMetric: 4156.2085

Epoch 16: val_loss improved from 4158.03857 to 4156.20850, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 112s - loss: 4152.7505 - MinusLogProbMetric: 4152.7505 - val_loss: 4156.2085 - val_MinusLogProbMetric: 4156.2085 - lr: 5.0805e-08 - 112s/epoch - 573ms/step
Epoch 17/1000
2023-09-29 12:49:20.379 
Epoch 17/1000 
	 loss: 4150.3940, MinusLogProbMetric: 4150.3940, val_loss: 4153.7305, val_MinusLogProbMetric: 4153.7305

Epoch 17: val_loss improved from 4156.20850 to 4153.73047, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 112s - loss: 4150.3940 - MinusLogProbMetric: 4150.3940 - val_loss: 4153.7305 - val_MinusLogProbMetric: 4153.7305 - lr: 5.0805e-08 - 112s/epoch - 570ms/step
Epoch 18/1000
2023-09-29 12:51:12.344 
Epoch 18/1000 
	 loss: 4148.4180, MinusLogProbMetric: 4148.4180, val_loss: 4150.9692, val_MinusLogProbMetric: 4150.9692

Epoch 18: val_loss improved from 4153.73047 to 4150.96924, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 112s - loss: 4148.4180 - MinusLogProbMetric: 4148.4180 - val_loss: 4150.9692 - val_MinusLogProbMetric: 4150.9692 - lr: 5.0805e-08 - 112s/epoch - 572ms/step
Epoch 19/1000
2023-09-29 12:53:04.149 
Epoch 19/1000 
	 loss: 4146.8740, MinusLogProbMetric: 4146.8740, val_loss: 4149.2549, val_MinusLogProbMetric: 4149.2549

Epoch 19: val_loss improved from 4150.96924 to 4149.25488, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 111s - loss: 4146.8740 - MinusLogProbMetric: 4146.8740 - val_loss: 4149.2549 - val_MinusLogProbMetric: 4149.2549 - lr: 5.0805e-08 - 111s/epoch - 568ms/step
Epoch 20/1000
2023-09-29 12:54:55.034 
Epoch 20/1000 
	 loss: 4145.8071, MinusLogProbMetric: 4145.8071, val_loss: 4147.7900, val_MinusLogProbMetric: 4147.7900

Epoch 20: val_loss improved from 4149.25488 to 4147.79004, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 112s - loss: 4145.8071 - MinusLogProbMetric: 4145.8071 - val_loss: 4147.7900 - val_MinusLogProbMetric: 4147.7900 - lr: 5.0805e-08 - 112s/epoch - 569ms/step
Epoch 21/1000
2023-09-29 12:56:45.837 
Epoch 21/1000 
	 loss: 4143.5405, MinusLogProbMetric: 4143.5405, val_loss: 4144.7808, val_MinusLogProbMetric: 4144.7808

Epoch 21: val_loss improved from 4147.79004 to 4144.78076, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 110s - loss: 4143.5405 - MinusLogProbMetric: 4143.5405 - val_loss: 4144.7808 - val_MinusLogProbMetric: 4144.7808 - lr: 5.0805e-08 - 110s/epoch - 562ms/step
Epoch 22/1000
2023-09-29 12:58:36.557 
Epoch 22/1000 
	 loss: 4141.0171, MinusLogProbMetric: 4141.0171, val_loss: 4142.3608, val_MinusLogProbMetric: 4142.3608

Epoch 22: val_loss improved from 4144.78076 to 4142.36084, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 111s - loss: 4141.0171 - MinusLogProbMetric: 4141.0171 - val_loss: 4142.3608 - val_MinusLogProbMetric: 4142.3608 - lr: 5.0805e-08 - 111s/epoch - 565ms/step
Epoch 23/1000
2023-09-29 13:00:27.075 
Epoch 23/1000 
	 loss: 4139.2183, MinusLogProbMetric: 4139.2183, val_loss: 4140.8921, val_MinusLogProbMetric: 4140.8921

Epoch 23: val_loss improved from 4142.36084 to 4140.89209, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 110s - loss: 4139.2183 - MinusLogProbMetric: 4139.2183 - val_loss: 4140.8921 - val_MinusLogProbMetric: 4140.8921 - lr: 5.0805e-08 - 110s/epoch - 563ms/step
Epoch 24/1000
2023-09-29 13:02:17.836 
Epoch 24/1000 
	 loss: 4136.9302, MinusLogProbMetric: 4136.9302, val_loss: 4139.7627, val_MinusLogProbMetric: 4139.7627

Epoch 24: val_loss improved from 4140.89209 to 4139.76270, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 111s - loss: 4136.9302 - MinusLogProbMetric: 4136.9302 - val_loss: 4139.7627 - val_MinusLogProbMetric: 4139.7627 - lr: 5.0805e-08 - 111s/epoch - 566ms/step
Epoch 25/1000
2023-09-29 13:04:08.436 
Epoch 25/1000 
	 loss: 4135.0156, MinusLogProbMetric: 4135.0156, val_loss: 4137.1802, val_MinusLogProbMetric: 4137.1802

Epoch 25: val_loss improved from 4139.76270 to 4137.18018, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 111s - loss: 4135.0156 - MinusLogProbMetric: 4135.0156 - val_loss: 4137.1802 - val_MinusLogProbMetric: 4137.1802 - lr: 5.0805e-08 - 111s/epoch - 565ms/step
Epoch 26/1000
2023-09-29 13:06:00.809 
Epoch 26/1000 
	 loss: 4133.1909, MinusLogProbMetric: 4133.1909, val_loss: 4135.0840, val_MinusLogProbMetric: 4135.0840

Epoch 26: val_loss improved from 4137.18018 to 4135.08398, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 112s - loss: 4133.1909 - MinusLogProbMetric: 4133.1909 - val_loss: 4135.0840 - val_MinusLogProbMetric: 4135.0840 - lr: 5.0805e-08 - 112s/epoch - 574ms/step
Epoch 27/1000
2023-09-29 13:07:54.470 
Epoch 27/1000 
	 loss: 4130.9458, MinusLogProbMetric: 4130.9458, val_loss: 4132.7739, val_MinusLogProbMetric: 4132.7739

Epoch 27: val_loss improved from 4135.08398 to 4132.77393, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 114s - loss: 4130.9458 - MinusLogProbMetric: 4130.9458 - val_loss: 4132.7739 - val_MinusLogProbMetric: 4132.7739 - lr: 5.0805e-08 - 114s/epoch - 581ms/step
Epoch 28/1000
2023-09-29 13:09:45.102 
Epoch 28/1000 
	 loss: 4128.5825, MinusLogProbMetric: 4128.5825, val_loss: 4130.6069, val_MinusLogProbMetric: 4130.6069

Epoch 28: val_loss improved from 4132.77393 to 4130.60693, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 111s - loss: 4128.5825 - MinusLogProbMetric: 4128.5825 - val_loss: 4130.6069 - val_MinusLogProbMetric: 4130.6069 - lr: 5.0805e-08 - 111s/epoch - 565ms/step
Epoch 29/1000
2023-09-29 13:11:36.718 
Epoch 29/1000 
	 loss: 4127.2188, MinusLogProbMetric: 4127.2188, val_loss: 4128.8667, val_MinusLogProbMetric: 4128.8667

Epoch 29: val_loss improved from 4130.60693 to 4128.86670, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 111s - loss: 4127.2188 - MinusLogProbMetric: 4127.2188 - val_loss: 4128.8667 - val_MinusLogProbMetric: 4128.8667 - lr: 5.0805e-08 - 111s/epoch - 568ms/step
Epoch 30/1000
2023-09-29 13:13:28.019 
Epoch 30/1000 
	 loss: 4125.3940, MinusLogProbMetric: 4125.3940, val_loss: 4127.4038, val_MinusLogProbMetric: 4127.4038

Epoch 30: val_loss improved from 4128.86670 to 4127.40381, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 111s - loss: 4125.3940 - MinusLogProbMetric: 4125.3940 - val_loss: 4127.4038 - val_MinusLogProbMetric: 4127.4038 - lr: 5.0805e-08 - 111s/epoch - 568ms/step
Epoch 31/1000
2023-09-29 13:15:18.969 
Epoch 31/1000 
	 loss: 4123.2627, MinusLogProbMetric: 4123.2627, val_loss: 4125.1401, val_MinusLogProbMetric: 4125.1401

Epoch 31: val_loss improved from 4127.40381 to 4125.14014, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 111s - loss: 4123.2627 - MinusLogProbMetric: 4123.2627 - val_loss: 4125.1401 - val_MinusLogProbMetric: 4125.1401 - lr: 5.0805e-08 - 111s/epoch - 566ms/step
Epoch 32/1000
2023-09-29 13:17:10.562 
Epoch 32/1000 
	 loss: 4121.6177, MinusLogProbMetric: 4121.6177, val_loss: 4123.5728, val_MinusLogProbMetric: 4123.5728

Epoch 32: val_loss improved from 4125.14014 to 4123.57275, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 111s - loss: 4121.6177 - MinusLogProbMetric: 4121.6177 - val_loss: 4123.5728 - val_MinusLogProbMetric: 4123.5728 - lr: 5.0805e-08 - 111s/epoch - 569ms/step
Epoch 33/1000
2023-09-29 13:19:01.753 
Epoch 33/1000 
	 loss: 4119.9683, MinusLogProbMetric: 4119.9683, val_loss: 4122.7383, val_MinusLogProbMetric: 4122.7383

Epoch 33: val_loss improved from 4123.57275 to 4122.73828, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 111s - loss: 4119.9683 - MinusLogProbMetric: 4119.9683 - val_loss: 4122.7383 - val_MinusLogProbMetric: 4122.7383 - lr: 5.0805e-08 - 111s/epoch - 566ms/step
Epoch 34/1000
2023-09-29 13:20:52.651 
Epoch 34/1000 
	 loss: 4118.4375, MinusLogProbMetric: 4118.4375, val_loss: 4120.5083, val_MinusLogProbMetric: 4120.5083

Epoch 34: val_loss improved from 4122.73828 to 4120.50830, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 112s - loss: 4118.4375 - MinusLogProbMetric: 4118.4375 - val_loss: 4120.5083 - val_MinusLogProbMetric: 4120.5083 - lr: 5.0805e-08 - 112s/epoch - 571ms/step
Epoch 35/1000
2023-09-29 13:22:44.974 
Epoch 35/1000 
	 loss: 4116.5098, MinusLogProbMetric: 4116.5098, val_loss: 4118.9751, val_MinusLogProbMetric: 4118.9751

Epoch 35: val_loss improved from 4120.50830 to 4118.97510, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 112s - loss: 4116.5098 - MinusLogProbMetric: 4116.5098 - val_loss: 4118.9751 - val_MinusLogProbMetric: 4118.9751 - lr: 5.0805e-08 - 112s/epoch - 569ms/step
Epoch 36/1000
2023-09-29 13:24:36.366 
Epoch 36/1000 
	 loss: 4114.4272, MinusLogProbMetric: 4114.4272, val_loss: 4116.4355, val_MinusLogProbMetric: 4116.4355

Epoch 36: val_loss improved from 4118.97510 to 4116.43555, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 112s - loss: 4114.4272 - MinusLogProbMetric: 4114.4272 - val_loss: 4116.4355 - val_MinusLogProbMetric: 4116.4355 - lr: 5.0805e-08 - 112s/epoch - 570ms/step
Epoch 37/1000
2023-09-29 13:26:28.339 
Epoch 37/1000 
	 loss: 4112.6030, MinusLogProbMetric: 4112.6030, val_loss: 4115.2969, val_MinusLogProbMetric: 4115.2969

Epoch 37: val_loss improved from 4116.43555 to 4115.29688, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 111s - loss: 4112.6030 - MinusLogProbMetric: 4112.6030 - val_loss: 4115.2969 - val_MinusLogProbMetric: 4115.2969 - lr: 5.0805e-08 - 111s/epoch - 568ms/step
Epoch 38/1000
2023-09-29 13:28:19.089 
Epoch 38/1000 
	 loss: 4111.1157, MinusLogProbMetric: 4111.1157, val_loss: 4114.7842, val_MinusLogProbMetric: 4114.7842

Epoch 38: val_loss improved from 4115.29688 to 4114.78418, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 111s - loss: 4111.1157 - MinusLogProbMetric: 4111.1157 - val_loss: 4114.7842 - val_MinusLogProbMetric: 4114.7842 - lr: 5.0805e-08 - 111s/epoch - 565ms/step
Epoch 39/1000
2023-09-29 13:30:09.895 
Epoch 39/1000 
	 loss: 4109.2588, MinusLogProbMetric: 4109.2588, val_loss: 4111.8623, val_MinusLogProbMetric: 4111.8623

Epoch 39: val_loss improved from 4114.78418 to 4111.86230, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 112s - loss: 4109.2588 - MinusLogProbMetric: 4109.2588 - val_loss: 4111.8623 - val_MinusLogProbMetric: 4111.8623 - lr: 5.0805e-08 - 112s/epoch - 571ms/step
Epoch 40/1000
2023-09-29 13:32:02.370 
Epoch 40/1000 
	 loss: 4107.4507, MinusLogProbMetric: 4107.4507, val_loss: 4109.1143, val_MinusLogProbMetric: 4109.1143

Epoch 40: val_loss improved from 4111.86230 to 4109.11426, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 112s - loss: 4107.4507 - MinusLogProbMetric: 4107.4507 - val_loss: 4109.1143 - val_MinusLogProbMetric: 4109.1143 - lr: 5.0805e-08 - 112s/epoch - 574ms/step
Epoch 41/1000
2023-09-29 13:33:54.200 
Epoch 41/1000 
	 loss: 4105.0498, MinusLogProbMetric: 4105.0498, val_loss: 4107.8203, val_MinusLogProbMetric: 4107.8203

Epoch 41: val_loss improved from 4109.11426 to 4107.82031, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 112s - loss: 4105.0498 - MinusLogProbMetric: 4105.0498 - val_loss: 4107.8203 - val_MinusLogProbMetric: 4107.8203 - lr: 5.0805e-08 - 112s/epoch - 569ms/step
Epoch 42/1000
2023-09-29 13:35:45.313 
Epoch 42/1000 
	 loss: 4103.7236, MinusLogProbMetric: 4103.7236, val_loss: 4105.8306, val_MinusLogProbMetric: 4105.8306

Epoch 42: val_loss improved from 4107.82031 to 4105.83057, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 110s - loss: 4103.7236 - MinusLogProbMetric: 4103.7236 - val_loss: 4105.8306 - val_MinusLogProbMetric: 4105.8306 - lr: 5.0805e-08 - 110s/epoch - 563ms/step
Epoch 43/1000
2023-09-29 13:37:36.211 
Epoch 43/1000 
	 loss: 4102.1426, MinusLogProbMetric: 4102.1426, val_loss: 4103.8770, val_MinusLogProbMetric: 4103.8770

Epoch 43: val_loss improved from 4105.83057 to 4103.87695, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5
196/196 - 111s - loss: 4102.1426 - MinusLogProbMetric: 4102.1426 - val_loss: 4103.8770 - val_MinusLogProbMetric: 4103.8770 - lr: 5.0805e-08 - 111s/epoch - 566ms/step
Epoch 44/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 124: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-29 13:38:50.989 
Epoch 44/1000 
	 loss: nan, MinusLogProbMetric: 4100.4702, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 44: val_loss did not improve from 4103.87695
196/196 - 73s - loss: nan - MinusLogProbMetric: 4100.4702 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 5.0805e-08 - 73s/epoch - 374ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 1.6935087808430278e-08.
===========
Generating train data for run 336.
===========
Train data generated in 0.49 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_336/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_336/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_336/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_336
self.data_kwargs: {'seed': 187}
self.x_data: [[ 6.8153725   2.8430347   6.1778684  ...  2.16058     2.2525268
   1.7267085 ]
 [ 2.1836705   1.800204    9.035551   ...  6.5503054   1.5567943
   2.9656742 ]
 [ 0.87054014  3.1855702   7.8237348  ...  6.3044343  -0.7088827
   2.3190107 ]
 ...
 [ 0.35442734  3.573855   10.922302   ...  5.8448343   0.8037275
   2.171598  ]
 [ 1.4908957   3.8775673   8.757837   ...  5.7039022  -0.88000226
   2.3087878 ]
 [ 1.7291393   3.7802503   9.770119   ...  5.5898924   0.22995539
   4.1106014 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_429"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_430 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_39 (LogProbL  (None,)                  4278720   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,278,720
Trainable params: 4,278,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_39/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_39'")
self.model: <keras.engine.functional.Functional object at 0x7f831666f490>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f7748ae4040>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f7748ae4040>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f7c0b166d10>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f8305984970>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_336/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f8305984ee0>, <keras.callbacks.ModelCheckpoint object at 0x7f8305984fa0>, <keras.callbacks.EarlyStopping object at 0x7f8305985210>, <keras.callbacks.ReduceLROnPlateau object at 0x7f8305985240>, <keras.callbacks.TerminateOnNaN object at 0x7f8305984e80>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 336/720 with hyperparameters:
timestamp = 2023-09-29 13:39:09.215108
ndims = 64
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4278720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.6935087808430278e-08...
Train first sample: [ 6.8153725e+00  2.8430347e+00  6.1778684e+00  4.2452879e+00
  1.6284513e+00  3.3207846e+00  7.0928354e+00  6.4340367e+00
  6.0301833e+00  6.4328456e+00  6.5495849e+00  4.9321003e+00
  8.9532204e+00  3.4481030e+00  4.8284545e+00  8.0901794e+00
  8.1549883e+00  7.3900299e+00  1.3510420e+00  9.5149851e+00
  7.5267930e+00  1.0017762e+01  1.1636026e+00  8.6080103e+00
  2.2276068e+00  6.2524571e+00  1.5594521e+00  8.2755499e+00
  8.5135918e+00  6.3978677e+00  3.8566861e+00  6.8423492e-01
  7.5210962e+00  5.4144373e+00  6.8548636e+00  8.7448330e+00
  9.6529083e+00  8.6500130e+00  9.1009712e-01  4.1672401e+00
  7.6439004e+00  9.0885794e-01  5.2691178e+00  8.8166595e-03
  7.4780917e-01 -4.8575500e-01  7.9515800e+00  2.7509699e+00
  4.8898096e+00  1.0073138e+01  7.4751706e+00  6.5633142e-01
  1.5831217e+00  6.0002384e+00  5.6322212e+00  2.2999794e+00
  9.1524439e+00  5.5740438e+00  5.8525505e+00  6.6422234e+00
  6.9292612e+00  2.1605799e+00  2.2525268e+00  1.7267085e+00]
Epoch 1/1000
                                ===========
                                Run 336/720 failed.
                                Exception type: InternalError
                                Exception message: Graph execution error:

Detected at node 'StatefulPartitionedCall_746' defined at (most recent call last):
    File "/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN.py", line 637, in <module>
      hyperparams_dict, NFObject, seed_train, training_time = train_function(seeds = [seed_train, seed_test, seed_dist, seed_metrics],
    File "/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN.py", line 286, in train_function
      NFObject.train()
    File "/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/Trainer.py", line 1635, in train
      history: tf.keras.callbacks.History = self.model.fit(x=self.x_data,
    File "/local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/keras/utils/traceback_utils.py", line 65, in error_handler
      return fn(*args, **kwargs)
    File "/local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/keras/engine/training.py", line 1685, in fit
      tmp_logs = self.train_function(iterator)
    File "/local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/keras/engine/training.py", line 1284, in train_function
      return step_function(self, iterator)
    File "/local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/keras/engine/training.py", line 1268, in step_function
      outputs = model.distribute_strategy.run(run_step, args=(data,))
    File "/local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/keras/engine/training.py", line 1249, in run_step
      outputs = model.train_step(data)
    File "/local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/keras/engine/training.py", line 1054, in train_step
      self.optimizer.minimize(loss, self.trainable_variables, tape=tape)
    File "/local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/keras/optimizers/optimizer.py", line 543, in minimize
      self.apply_gradients(grads_and_vars)
    File "/local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/keras/optimizers/optimizer.py", line 1174, in apply_gradients
      return super().apply_gradients(grads_and_vars, name=name)
    File "/local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/keras/optimizers/optimizer.py", line 650, in apply_gradients
      iteration = self._internal_apply_gradients(grads_and_vars)
    File "/local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/keras/optimizers/optimizer.py", line 1200, in _internal_apply_gradients
      return tf.__internal__.distribute.interim.maybe_merge_call(
    File "/local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/keras/optimizers/optimizer.py", line 1250, in _distributed_apply_gradients_fn
      distribution.extended.update(
    File "/local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/keras/optimizers/optimizer.py", line 1245, in apply_grad_to_update_var
      return self._update_step_xla(grad, var, id(self._var_key(var)))
Node: 'StatefulPartitionedCall_746'
Failed to load in-memory CUBIN: CUDA_ERROR_OUT_OF_MEMORY: out of memory
	 [[{{node StatefulPartitionedCall_746}}]] [Op:__inference_train_function_12239818]
                                Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN.py , Line : 637, Func.Name : <module>, Message : hyperparams_dict, NFObject, seed_train, training_time = train_function(seeds = [seed_train, seed_test, seed_dist, seed_metrics],', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN.py , Line : 286, Func.Name : train_function, Message : NFObject.train()', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/Trainer.py , Line : 1635, Func.Name : train, Message : history: tf.keras.callbacks.History = self.model.fit(x=self.x_data,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/keras/utils/traceback_utils.py , Line : 70, Func.Name : error_handler, Message : raise e.with_traceback(filtered_tb) from None', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/eager/execute.py , Line : 52, Func.Name : quick_execute, Message : tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,']
                                ===========

Directory ../../results/CsplineN_new/run_337/ already exists.
Skipping it.
===========
Run 337/720 already exists. Skipping it.
===========

===========
Generating train data for run 338.
===========
Train data generated in 0.45 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_338/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_338/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_338/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_338
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.465355   7.122426   6.507682  ...  1.2216942  7.396178   1.4448237]
 [ 1.8683796  2.8760538  9.147447  ...  4.980697  -1.1366768  2.8016024]
 [ 5.6175413  8.642618   5.6734285 ... -0.9440901  7.0561414  1.3082856]
 ...
 [ 6.145307   2.8273122  6.191003  ...  2.964365   6.077158   2.5509653]
 [ 6.5595512  2.759307   6.0778823 ...  3.4800506  4.3951674  2.1941729]
 [ 5.586527   7.3216014  6.2487698 ...  1.292631   5.797888   1.2879843]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_435"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_436 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_40 (LogProbL  (None,)                  1645920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,645,920
Trainable params: 1,645,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_40/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_40'")
self.model: <keras.engine.functional.Functional object at 0x7f7742fd3130>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f7f6425d330>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f7f6425d330>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f7b4150f100>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f77398a2f80>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_338/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f77398a34f0>, <keras.callbacks.ModelCheckpoint object at 0x7f77398a35b0>, <keras.callbacks.EarlyStopping object at 0x7f77398a3820>, <keras.callbacks.ReduceLROnPlateau object at 0x7f77398a3850>, <keras.callbacks.TerminateOnNaN object at 0x7f77398a3490>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_338/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 338/720 with hyperparameters:
timestamp = 2023-09-29 13:41:56.300814
ndims = 64
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 1645920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 5.465355    7.122426    6.507682    5.369255    4.1136785   6.196197
  4.259451    8.702779    9.329599    4.155665    7.5761204   5.312057
  5.67009     9.304932    0.8909831   1.4673554  -0.03015512  8.003835
  8.704047    8.985184    9.595006    8.103893    4.62255     8.186032
  0.5885266   6.325719    0.92288995  9.5184145   6.6715336   3.3791308
  2.4301429   8.523114    4.619198    5.3705144   0.3660334   5.918427
  6.2292395   7.1572037   9.551745    6.67638     3.2475867   4.406022
  7.1114645   0.65166384  6.9876056   6.6356955   2.3022606   1.2872343
  3.532573    3.8445692   6.0782557   4.5467997   9.491786    0.11065191
  3.0255654   1.2933161   6.592196    2.150623    4.2476006   4.39
  1.805124    1.2216942   7.396178    1.4448237 ]
Epoch 1/1000
                                ===========
                                Run 338/720 failed.
                                Exception type: InternalError
                                Exception message: Graph execution error:

Detected at node 'StatefulPartitionedCall_214' defined at (most recent call last):
    File "/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN.py", line 637, in <module>
      hyperparams_dict, NFObject, seed_train, training_time = train_function(seeds = [seed_train, seed_test, seed_dist, seed_metrics],
    File "/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN.py", line 286, in train_function
      NFObject.train()
    File "/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/Trainer.py", line 1635, in train
      history: tf.keras.callbacks.History = self.model.fit(x=self.x_data,
    File "/local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/keras/utils/traceback_utils.py", line 65, in error_handler
      return fn(*args, **kwargs)
    File "/local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/keras/engine/training.py", line 1685, in fit
      tmp_logs = self.train_function(iterator)
    File "/local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/keras/engine/training.py", line 1284, in train_function
      return step_function(self, iterator)
    File "/local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/keras/engine/training.py", line 1268, in step_function
      outputs = model.distribute_strategy.run(run_step, args=(data,))
    File "/local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/keras/engine/training.py", line 1249, in run_step
      outputs = model.train_step(data)
    File "/local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/keras/engine/training.py", line 1054, in train_step
      self.optimizer.minimize(loss, self.trainable_variables, tape=tape)
    File "/local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/keras/optimizers/optimizer.py", line 543, in minimize
      self.apply_gradients(grads_and_vars)
    File "/local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/keras/optimizers/optimizer.py", line 1174, in apply_gradients
      return super().apply_gradients(grads_and_vars, name=name)
    File "/local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/keras/optimizers/optimizer.py", line 650, in apply_gradients
      iteration = self._internal_apply_gradients(grads_and_vars)
    File "/local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/keras/optimizers/optimizer.py", line 1200, in _internal_apply_gradients
      return tf.__internal__.distribute.interim.maybe_merge_call(
    File "/local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/keras/optimizers/optimizer.py", line 1250, in _distributed_apply_gradients_fn
      distribution.extended.update(
    File "/local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/keras/optimizers/optimizer.py", line 1245, in apply_grad_to_update_var
      return self._update_step_xla(grad, var, id(self._var_key(var)))
Node: 'StatefulPartitionedCall_214'
Failed to load in-memory CUBIN: CUDA_ERROR_OUT_OF_MEMORY: out of memory
	 [[{{node StatefulPartitionedCall_214}}]] [Op:__inference_train_function_12307585]
                                Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN.py , Line : 637, Func.Name : <module>, Message : hyperparams_dict, NFObject, seed_train, training_time = train_function(seeds = [seed_train, seed_test, seed_dist, seed_metrics],', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN.py , Line : 286, Func.Name : train_function, Message : NFObject.train()', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/Trainer.py , Line : 1635, Func.Name : train, Message : history: tf.keras.callbacks.History = self.model.fit(x=self.x_data,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/keras/utils/traceback_utils.py , Line : 70, Func.Name : error_handler, Message : raise e.with_traceback(filtered_tb) from None', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/eager/execute.py , Line : 52, Func.Name : quick_execute, Message : tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,']
                                ===========

===========
Generating train data for run 339.
===========
Train data generated in 0.40 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_339/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_339/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_339/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_339
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.465355   7.122426   6.507682  ...  1.2216942  7.396178   1.4448237]
 [ 1.8683796  2.8760538  9.147447  ...  4.980697  -1.1366768  2.8016024]
 [ 5.6175413  8.642618   5.6734285 ... -0.9440901  7.0561414  1.3082856]
 ...
 [ 6.145307   2.8273122  6.191003  ...  2.964365   6.077158   2.5509653]
 [ 6.5595512  2.759307   6.0778823 ...  3.4800506  4.3951674  2.1941729]
 [ 5.586527   7.3216014  6.2487698 ...  1.292631   5.797888   1.2879843]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_441"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_442 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_41 (LogProbL  (None,)                  908640    
 ayer)                                                           
                                                                 
=================================================================
Total params: 908,640
Trainable params: 908,640
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_41/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_41'")
self.model: <keras.engine.functional.Functional object at 0x7f7711872890>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f76f976a530>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f76f976a530>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f77135a04f0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f7728165bd0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f77281640d0>, <keras.callbacks.ModelCheckpoint object at 0x7f7728164be0>, <keras.callbacks.EarlyStopping object at 0x7f7728164340>, <keras.callbacks.ReduceLROnPlateau object at 0x7f7728164430>, <keras.callbacks.TerminateOnNaN object at 0x7f77281649d0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_339/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 339/720 with hyperparameters:
timestamp = 2023-09-29 13:42:49.734700
ndims = 64
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 908640
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 5.465355    7.122426    6.507682    5.369255    4.1136785   6.196197
  4.259451    8.702779    9.329599    4.155665    7.5761204   5.312057
  5.67009     9.304932    0.8909831   1.4673554  -0.03015512  8.003835
  8.704047    8.985184    9.595006    8.103893    4.62255     8.186032
  0.5885266   6.325719    0.92288995  9.5184145   6.6715336   3.3791308
  2.4301429   8.523114    4.619198    5.3705144   0.3660334   5.918427
  6.2292395   7.1572037   9.551745    6.67638     3.2475867   4.406022
  7.1114645   0.65166384  6.9876056   6.6356955   2.3022606   1.2872343
  3.532573    3.8445692   6.0782557   4.5467997   9.491786    0.11065191
  3.0255654   1.2933161   6.592196    2.150623    4.2476006   4.39
  1.805124    1.2216942   7.396178    1.4448237 ]
Epoch 1/1000
                                ===========
                                Run 339/720 failed.
                                Exception type: InternalError
                                Exception message: Graph execution error:

Detected at node 'StatefulPartitionedCall_310' defined at (most recent call last):
    File "/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN.py", line 637, in <module>
      hyperparams_dict, NFObject, seed_train, training_time = train_function(seeds = [seed_train, seed_test, seed_dist, seed_metrics],
    File "/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN.py", line 286, in train_function
      NFObject.train()
    File "/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/Trainer.py", line 1635, in train
      history: tf.keras.callbacks.History = self.model.fit(x=self.x_data,
    File "/local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/keras/utils/traceback_utils.py", line 65, in error_handler
      return fn(*args, **kwargs)
    File "/local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/keras/engine/training.py", line 1685, in fit
      tmp_logs = self.train_function(iterator)
    File "/local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/keras/engine/training.py", line 1284, in train_function
      return step_function(self, iterator)
    File "/local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/keras/engine/training.py", line 1268, in step_function
      outputs = model.distribute_strategy.run(run_step, args=(data,))
    File "/local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/keras/engine/training.py", line 1249, in run_step
      outputs = model.train_step(data)
    File "/local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/keras/engine/training.py", line 1054, in train_step
      self.optimizer.minimize(loss, self.trainable_variables, tape=tape)
    File "/local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/keras/optimizers/optimizer.py", line 543, in minimize
      self.apply_gradients(grads_and_vars)
    File "/local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/keras/optimizers/optimizer.py", line 1174, in apply_gradients
      return super().apply_gradients(grads_and_vars, name=name)
    File "/local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/keras/optimizers/optimizer.py", line 650, in apply_gradients
      iteration = self._internal_apply_gradients(grads_and_vars)
    File "/local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/keras/optimizers/optimizer.py", line 1200, in _internal_apply_gradients
      return tf.__internal__.distribute.interim.maybe_merge_call(
    File "/local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/keras/optimizers/optimizer.py", line 1250, in _distributed_apply_gradients_fn
      distribution.extended.update(
    File "/local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/keras/optimizers/optimizer.py", line 1245, in apply_grad_to_update_var
      return self._update_step_xla(grad, var, id(self._var_key(var)))
Node: 'StatefulPartitionedCall_310'
Failed to load in-memory CUBIN: CUDA_ERROR_OUT_OF_MEMORY: out of memory
	 [[{{node StatefulPartitionedCall_310}}]] [Op:__inference_train_function_12390652]
                                Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN.py , Line : 637, Func.Name : <module>, Message : hyperparams_dict, NFObject, seed_train, training_time = train_function(seeds = [seed_train, seed_test, seed_dist, seed_metrics],', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN.py , Line : 286, Func.Name : train_function, Message : NFObject.train()', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/Trainer.py , Line : 1635, Func.Name : train, Message : history: tf.keras.callbacks.History = self.model.fit(x=self.x_data,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/keras/utils/traceback_utils.py , Line : 70, Func.Name : error_handler, Message : raise e.with_traceback(filtered_tb) from None', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/eager/execute.py , Line : 52, Func.Name : quick_execute, Message : tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,']
                                ===========

===========
Generating train data for run 340.
===========
Train data generated in 0.33 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_340/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_340/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_340/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_340
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.465355   7.122426   6.507682  ...  1.2216942  7.396178   1.4448237]
 [ 1.8683796  2.8760538  9.147447  ...  4.980697  -1.1366768  2.8016024]
 [ 5.6175413  8.642618   5.6734285 ... -0.9440901  7.0561414  1.3082856]
 ...
 [ 6.145307   2.8273122  6.191003  ...  2.964365   6.077158   2.5509653]
 [ 6.5595512  2.759307   6.0778823 ...  3.4800506  4.3951674  2.1941729]
 [ 5.586527   7.3216014  6.2487698 ...  1.292631   5.797888   1.2879843]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_447"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_448 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_42 (LogProbL  (None,)                  2139360   
 ayer)                                                           
                                                                 
=================================================================
Total params: 2,139,360
Trainable params: 2,139,360
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_42/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_42'")
self.model: <keras.engine.functional.Functional object at 0x7f7c0a4ebe80>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f770adbefe0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f770adbefe0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f7c60bf8a30>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f7c629e0760>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_340/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f7c629e2050>, <keras.callbacks.ModelCheckpoint object at 0x7f7c629e3580>, <keras.callbacks.EarlyStopping object at 0x7f7c629e0700>, <keras.callbacks.ReduceLROnPlateau object at 0x7f7c629e0040>, <keras.callbacks.TerminateOnNaN object at 0x7f7c629e2380>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_340/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 340/720 with hyperparameters:
timestamp = 2023-09-29 13:43:53.315750
ndims = 64
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 2139360
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 5.465355    7.122426    6.507682    5.369255    4.1136785   6.196197
  4.259451    8.702779    9.329599    4.155665    7.5761204   5.312057
  5.67009     9.304932    0.8909831   1.4673554  -0.03015512  8.003835
  8.704047    8.985184    9.595006    8.103893    4.62255     8.186032
  0.5885266   6.325719    0.92288995  9.5184145   6.6715336   3.3791308
  2.4301429   8.523114    4.619198    5.3705144   0.3660334   5.918427
  6.2292395   7.1572037   9.551745    6.67638     3.2475867   4.406022
  7.1114645   0.65166384  6.9876056   6.6356955   2.3022606   1.2872343
  3.532573    3.8445692   6.0782557   4.5467997   9.491786    0.11065191
  3.0255654   1.2933161   6.592196    2.150623    4.2476006   4.39
  1.805124    1.2216942   7.396178    1.4448237 ]
Epoch 1/1000
2023-09-29 13:45:11.474147: F tensorflow/tsl/framework/bfc_allocator.cc:700] Check failed: h != kInvalidChunkHandle 
